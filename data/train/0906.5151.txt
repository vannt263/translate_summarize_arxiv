{
  "article_text": [
    "a prevalent and useful version of unsupervised learning arises when both the observed data and the latent variables are structured .",
    "examples range from hidden alignment variables in speech recognition @xcite and machine translation @xcite , to latent trees in unsupervised parsing @xcite , and to pose estimation in computer vision @xcite .",
    "these techniques are all based on probabilistic models .",
    "their applicability hinges on the tractability of ( approximately ) computing latent variable expectations , thus enabling the use of em @xcite . in this paper",
    "we show that a recently - developed _ search - based _ algorithm , searn  @xcite ( see section  [ sec : searn ] ) , can be utilized for unsupervised structured prediction ( section  [ sec : unsearn ] ) .",
    "we show : ( 1 ) that under an appropriate construction , searn  can imitate the expectation maximization ( section  [ sec : em ] ) ; ( 2 ) that unsupervised searn  can be used to obtain competitive performance on an unsupervised dependency parsing task ( section  [ sec : gi ] ) ; and ( 3 ) that unsupervised searn  naturally extends to a semi - supervised setting ( section  [ sec : semi ] ) . the key insight that enables",
    "this work is that we can consider the prediction of the ( observed ) input to be , itself , a structured prediction problem .",
    "the _ supervised _ structured prediction problem is the task of mapping inputs @xmath0 to complex structured outputs @xmath1 ( e.g. , sequences , trees , etc . ) .",
    "formally , let @xmath2 be an arbitrary input space and @xmath3 be structure output space .",
    "@xmath3 is typically assumed to _",
    "decompose _ over some smaller substructures ( e.g. , labels in a sequence ) .",
    "@xmath3 comes equipped with a loss function , often assumed to take the form of a hamming loss over the substructures .",
    "features are defined over pairs @xmath4 in such a way that they obey the substructures ( e.g. , one might have features over adjacent label pairs in a sequence ) . under strong assumptions on the structures , the loss function and the features ( essentially `` locality '' assumptions ) ,",
    "a number of learning algorithms can be employed : for example , conditional random fields @xcite or max - margin markov networks @xcite .",
    "a key difficulty in structured prediction occurs when the output space @xmath3 , the features , or the loss , does not decompose nicely .",
    "all of these issues can lead to intractable computations at either training or prediction time ( often both ) .",
    "an attractive approach for dealing with this intractability is to employ a search - based algorithm .",
    "the key idea in search - based structured prediction is to first decompose the output @xmath1 into a sequence of ( dependent ) smaller predictions @xmath5 .",
    "these may each be predicted in turn , with later predictions dependent of previous decisions .      a recently proposed algorithm for solving the structured prediction problem is searn  @xcite .",
    "searn  operates by considering each substructure prediction @xmath5 as a classification problem .",
    "a classifier @xmath6 is trained so that at time @xmath7 , given a feature vector , it predict the best value for @xmath8 .",
    "the feature vector can be based on any part of the input @xmath0 and any previous decision @xmath9 .",
    "this introduces a chicken - and - egg problem .",
    "@xmath6 should ideally be trained so that it makes the best decision for @xmath8 _ given _ that @xmath6 makes all past decisions @xmath9 and all future decisions @xmath10 . of course",
    ", at training time we do not have access to @xmath6 ( we are trying to construct it ) .",
    "the solution is to use an iterative scheme .",
    "the presentation we give here differs slightly from the original presentation of the searn  algorithm .",
    "our motivation for straying from the original formulation is because our presentation makes more clear the connection between our unsupervised variant of searn  and more standard unsupervised learning methods ( such as standard algorithms on hidden markov models ) .",
    "let @xmath11 denote a distribution over pairs @xmath4 drawn from @xmath12 , and let @xmath13 be the loss associated with predicting @xmath14 when the true answer is @xmath1 .",
    "we assume that @xmath15 can be decomposed into atomic predictions @xmath16 , where each @xmath8 is drawn from a discrete set @xmath17 .",
    "a _ policy _ ,",
    "@xmath18 , is a ( possibly stochastic ) function that maps tuples @xmath19 to atomic predictions @xmath8 .",
    "the key ingredient in searn  is to use the loss function @xmath20 and a `` current '' policy @xmath18 to turn @xmath11 into a distribution over cost - sensitive ( multiclass ) classification problems @xcite .",
    "a cost - sensitive classification example is given by an input @xmath0 and a cost vector @xmath21 , where @xmath22 is the cost of predicting class @xmath23 on input @xmath0 .",
    "define by searn@xmath24 a distribution over cost - sensitive classification problems derived as follows .",
    "to sample from this induced distribution , we first sample an example @xmath25 .",
    "we then sample @xmath7 uniformly from @xmath26 $ ] and run @xmath18 for @xmath27 steps on @xmath4 .",
    "this yields a partial prediction @xmath28 .",
    "the input for the cost sensitive classification problem is then the tuple @xmath29 .",
    "the costs are derived as follows . for each possible choice @xmath23 of @xmath30",
    ", we defined @xmath22 as the _ expected _ loss if @xmath18 were run , beginning at @xmath31 on input @xmath0 .",
    "formally : @xmath32 searn  assumes access to an `` initial policy '' @xmath33 ( sometimes called the `` optimal policy '' ) .",
    "given an input @xmath0 , a true output @xmath1 and a prefix of predictions @xmath34 , @xmath33 produces a best next - action , @xmath30 .",
    "it should be constructed so that the choice @xmath30 is optimal ( or close to optimal ) with respect to the problem - specific loss function . for example , if the loss function is hamming loss , the @xmath33 will always produce @xmath35 . for more complex loss functions ,",
    "computing @xmath33 may be more involved .    given these ingredients , searn  operates according the algorithm given in figure  [ fig : searn ] .",
    "operationally , the sampling step is typically implemented by generating _ every _ example from a fixed structured prediction training set . the costs ( expected losses ) are computed by sampling with tied randomness @xcite .",
    "if @xmath36 , one can show @xcite that after at most @xmath37 iterations , searn  is guaranteed to find a solution @xmath18 with structured prediction loss bounded as : @xmath38 where @xmath39 is the loss of the initial policy ( typically zero ) , @xmath40 is the length of the longest example , @xmath41 is the worse - case per - step loss and @xmath42 is the average multiclass classification loss .",
    "this shows that the structured prediction algorithm learned by searn  is guaranteed to be not - much - worse than that produced by the initial policy , _ provided _ that the created classification problems are easy ( i.e. , that @xmath42 is small ) .",
    "note that one can use _ any _ classification algorithm one likes .",
    "in unsupervised structured prediction , we no longer receive an pair @xmath4 but instead observes only an input @xmath0 .",
    "our job is to construct a classifier that produces @xmath1 , even though we have never observed it .      the key idea  one that underlies much work in unsupervised learning  is that a good @xmath1 is one that enables us to easily recover @xmath0",
    "this is precisely the intuition we build in to our model .",
    "the observation that makes this practical is that there is nothing in the theory or application of searn  that says that @xmath33 can not be stochastic .",
    "moreover , there is not requirement that the loss function depend on _ all _ components of the prediction .",
    "our model will essentially first predict @xmath1 and then predict @xmath0 based on @xmath1 .",
    "importantly , the loss function is agnostic to @xmath1 ( since we do not have true outputs ) .",
    "the general construction is as follows .",
    "let @xmath43 be a distribution over inputs @xmath44 and let @xmath3 be the space of desired latent structures ( e.g. , trees ) .",
    "we define a distribution @xmath45 over @xmath46 by defining a sampling procedure . to sample from @xmath45 , we first sample @xmath47 .",
    "we then sample uniformly from the set of all @xmath3 that are valid structures for @xmath0 .",
    "finally , we return the pair @xmath48 . we define a loss function @xmath49 by @xmath50 where @xmath51 is any loss function on the input space ( e.g. , hamming loss ) .",
    "we apply searn  to the supervised structured prediction problem @xmath45 , and implicitly learn latent structures .",
    "to gain insight into the operation of searn  in the unsupervised setting , it is useful to consider a sequence labeling example .",
    "that is , our input @xmath0 is a sequence of length @xmath40 and we desire a label sequence @xmath1 of length @xmath40 drawn from a label space of size @xmath52 . we convert this into a supervised learning problem by considering the `` true '' structured output to be a label sequence of length @xmath53 , with the first @xmath40 components drawn from the label space of size @xmath52 and the second @xmath40 components drawn from the input vocabulary",
    ". the loss function can then be anything that depends only on the last @xmath40 components . for simplicity",
    ", we can consider it to be hamming loss .",
    "the construction of the optimal policy in this case is straightforward .",
    "for the first @xmath40 components , @xmath33 may behave arbitrarily ( e.g. , it may produce a uniform distribution over the @xmath52 labels ) . for the second @xmath40 components , @xmath33 always predicts the true label ( which is known , because it is part of the input ) .",
    "an important aspect of the model is the construction of the feature vectors .",
    "it is most useful to consider this construction as having two parts .",
    "the first part has to do with predicting the hidden structure ( the first @xmath40 components ) .",
    "the second part has to do with predicting the observed structure ( the second @xmath40 components ) .",
    "for the first part , we are free to use whatever features we desire , so long as they can be computed based on the input @xmath0 and a partial output .",
    "for instance , in the hmm case , we could use the two most recent label predictions and windowed features from @xmath0 .",
    "the construction of the features for the second part is , however , also crucial .",
    "for instance , if the feature vector corresponding to `` predict the @xmath7th component of @xmath0 '' contains the @xmath7 component of @xmath0 , then this learning problem is trivial  but also renders the latent structure useless .",
    "the goal of the designer of the feature space is to construct features for predicting @xmath54 that crucially depend on getting the latent structure @xmath1 correct .",
    "that is , the ideal feature set is one for which you can predict @xmath54 accurately _ if an only if _ we have found the correct latent structure ( more on this in section  [ sec : guarantees ] ) .",
    "for instance , in the hmm case , we may predict @xmath54 based only on the corresponding label @xmath8 , or maybe on the basis of @xmath55 .",
    "( note that we are not limited to the markov assumption , as in the case of hmms . )    in the first iteration of searn , all costs for the prediction of the latent structure are computed with respect to the initial policy .",
    "recalling that the initial policy behaves randomly when predicting the latent labels and correctly when predicting the words , we can see that these costs are all _",
    "zero_. thus , for the latent structure actions , searn  will not induce any classification examples ( because the cost of all actions is equal ) .",
    "however , it will create example for predicting the @xmath0 component . for predicting the @xmath0s",
    ", the cost will be zero for the correct word and one for any incorrect word .",
    "these examples will have associated features : we will predict word @xmath54 based _ exclusively _ on @xmath8 .",
    "remember : @xmath8 was generated randomly by the initial policy .",
    "in the _ second _ iteration , the behavior is different .",
    "searnreturns to creating examples for the latent structure components",
    ". however , in this iteration , since the current policy is not longer optimal , the future cost estimates may be non - zero .",
    "consider generating an example corresponding to a ( latent ) state @xmath8 .",
    "for some small percentage ( as dictated by @xmath56 ) of the `` generate @xmath0 '' decisions , the previously learned classifier will fire .",
    "if this learned classifier does well , then the associated cost will be low .",
    "however , if the learned classifier does poorly , the the associated cost will be high . intuitively , the learned classifier will do well if and only if the action that labels @xmath8 is `` good '' ( i.e. , consistent with what was learned previously ) .",
    "this , in the second pass through the data , searn  _ does _ create classification examples specific to the latent decisions .    as searn  iterates ,",
    "more and more of the latent prediction decisions are made according to the learned classifiers and not with respect to the random policy .",
    "in this section , we show an equivalence between expectation maximization in directed probabilistic structures and unsupervised searn . we use mixture of multinomials as a motivating example ( primarily for simplicity ) , but the results easily extend to more complicated models ( e.g. , hmms : see section  [ sec : hmm ] ) .      in the mixture of multinomials problem , we are given @xmath57 documents @xmath58 , where @xmath59 is a vector of word counts over a vocabulary of size @xmath60 ; that is , @xmath61 is the number of times word @xmath62 appeared in document @xmath63 .",
    "the mixture of multinomials is a probabilistic clustering model , where we assume an underlying set of @xmath52 clusters ( multinomials ) that generated the documents .",
    "denote by @xmath64 the multinomial parameter associated with cluster @xmath23 , @xmath65 the prior probability of choosing cluster @xmath23 , and let @xmath66 be an indicator vector associating document @xmath63 with the unique cluster @xmath23 such that @xmath67 .",
    "the probabilistic model has the form : @xmath68^{z_{n , k}}\\ ] ] expectation maximization in this model involves first computing expectations over the @xmath69 vectors and then updating the model parameters @xmath70 :    @xmath71    in both cases , the constant of proportionality is chosen so that the variables sum to one over the last component .",
    "these updates are repeated until convergence of the incomplete data likelihood , eq  .",
    "now , we show how to construct an instance of unsupervised searn  that effectively mimics the behavior of em on the mixture of multinomials problem .",
    "the ingredients are as follows :    * the input space @xmath2 is the space of documents , represented as word count vectors . *",
    "the ( latent ) output space @xmath3 is a single discrete variable in the range @xmath72 $ ] that specifies the cluster .",
    "* the feature set for predicting @xmath1 ( document counts ) . * the feature set for predicting @xmath0 is the label @xmath1 and the total number of words in the document .",
    "the predictions for a document are estimated word probabilities , not the words themselves .",
    "* the loss function ignores the prediction @xmath1 and returns the log loss of the true document @xmath0 under the word probabilities predicted .",
    "* the cost - sensitive learning algorithm is different depending on whether the latent structure @xmath1 is being predicted or if the document @xmath0 is being predicted : * * structure : the base classifier is a multinomial nave bayes classifier , parameterized by ( say ) @xmath73 * * document : the base classifier is a collection of independent maximum likelihood multinomial estimators for each cluster .",
    "consider the behavior of this setup .",
    "in particular , consider the distribution searn@xmath24 .",
    "there are two `` types '' of examples drawn from this distribution : ( 1 ) latent structure examples and ( 2 ) document examples .",
    "the claim is that _ both _",
    "classifiers learned are identical to the mixture of multinomials model from section  [ sec : em - mm ] .",
    "consider the generation of a latent structure example .",
    "first , a document @xmath63 is sampled uniformly from the training set .",
    "then , for each possible label @xmath23 of this document , a cost @xmath74 is computed . by definition ,",
    "the @xmath75 that is computed is exactly the prediction according to the current multinomial estimator , @xmath73 . interpreting the multinomial estimator in terms of the em parameters , the costs are _",
    "precisely _ the @xmath76s from em ( see eq  ) .",
    "these latent structure examples are fed in to the multinomial nave bayes classifier , which re - estimates a model exactly as per the m - step in em ( eq  ) .",
    "next , consider the generation of the document examples .",
    "these examples are generated by @xmath18 first choosing a cluster according to the structure classifier .",
    "this cluster i d is then used as the ( only ) feature to the `` generate document '' multinomial . as we saw before , the probability that @xmath18 will select label @xmath23 for document @xmath63 is precisely @xmath76 from eq  .",
    "thus , the multinomial estimator will effectively receive weighted examples , weighted by these @xmath76s , thus making the maximum likelihood estimate exactly the same as the m - step from em ( eq  ) .      to demonstrate the advantages of the generality of searn , we report here the result of some experiments on synthetic data .",
    "we generate synthetic data according to two different hmms .",
    "the first hmm is a first - order model .",
    "the initial state probabilities , the transition probabilities , and the observation probabilities are all drawn uniformly .",
    "the second hmm is a second - order model , also will all probabilities drawn uniformly .",
    "the lengths of observations are given by a poisson with a fixed mean .",
    "[ cols=\"<,<,^,^,^,^\",options=\"header \" , ]     [ tab : dep ]    the baseline systems are : two random baselines ( one generative , one given by the searn  initial policy ) , klein and manning s model @xcite em - based model ( with and without clever initialization ) , and three variants of smith and eisner s model @xcite ( with random initialization , which seems to be better for most of their models ) .",
    "we also report an `` upper bound '' performance based on supervised training , for both the probabilistic ( smith+eisner model ) as well as supervised searn .",
    "the results are reported in table  [ tab : dep ] : accuracy on the training data , accuracy on the test data and the number of iterations required .",
    "these are all averaged over @xmath77 runs ; standard deviations are shown in small print .",
    "many of the results ( the non - searnresults ) are copied from @xcite . the stopping criteria for the em - based models",
    "is that the log likelihood changes by less than @xmath78 . for the searn - based methods ,",
    "the stopping criteria is that the development accuracy ceases to increase ( on the individual classification tasks , not on the structured prediction task ) .",
    "all learned algorithms outperform the random algorithms ( except klein+manning with random inits ) .",
    "k+m with smart initialization does slightly better than the worst of the s+e models , though the difference is not statistically significant .",
    "it does so needing only about a third of the number of iterations ( moreover , a single s+e iteration is slower than a single k+m iteration ) .",
    "the other two s+e models do roughly comparably in terms of performance ( strictly dominating the previous methods )",
    ". one of them ( `` delortrans1 '' ) requires about twice as many iterations as k+m ; the other ( `` trans1 '' ) requires about three times ( but has much high performance variance ) .",
    "unsupervised searn  performs halfway between the best k+m model and the best s+e model ( it is within the error bars for `` delortrans1 '' but not `` trans1 '' ) .",
    "nicely , it takes significantly fewer iterations to converge ( roughly @xmath79 ) .",
    "moreover , each iteration is quite fast in comparison to the em - based methods ( a complete run took roughly @xmath80 hours on a 3.8ghz opteron using searnshell ) . finally , we present results for the supervised case . here",
    ", we see that the searn - based method converges much more quickly to a better solution than the s+e model .",
    "note that this comparison is unfair since the searn - based model uses additional features ( though it is a nice property of the searn - based model that it _ can _ make use of additional features ) .",
    "nevertheless we provide it so as to give a sense of a reasonable upper - bound .",
    "we imagine that including more features would shift the upper - bound and the unsupervised algorithm performance up .",
    "the unsupervised learning algorithm described above naturally extends to the case where some labeled data is available .",
    "in fact , the only modification to the algorithm is to change the loss function . in the unsupervised case",
    ", the loss function completely ignores the latent structure , and returns a loss dependent only on the `` predict self '' task . in the semi - supervised version ,",
    "one plugs in a natural loss function for the `` latent '' structure prediction for the labeled subset of the data .    in figure",
    "[ fig : semisup ] , we present results on dependency parsing .",
    "we show learning curves for unsupervised , fully supervised and semi - supervised models .",
    "the x - axis shows the number of examples used ; in the unsupervised and supervised cases , this is the total number of examples ; in the semi - supervised case , it is the number of labeled examples .",
    "error bars are two standard deviations .",
    "somewhat surprisingly , with only five labeled examples , the semi - supervised approach achieves an accuracy of over @xmath81 , only about @xmath82 behind the fully supervised approach with @xmath83 labeled examples .",
    "eventually the supervised model catches up ( at about @xmath84 labeled examples ) .",
    "the performance of the unsupervised model continues to grow as more examples are provided , but never reaches anywhere close to the supervised or semi - supervised models .",
    "we have described the application of a search - based structured prediction algorithm , searn , to unsupervised learning .",
    "this answers positively an open question in the field of learning reductions @xcite : can unsupervised learning be reduced to supervised learning ? we have shown a near - equivalence between the resulting algorithm and the forward - backward algorithm in hidden markov models .",
    "we have shown an application of this algorithm to unsupervised dependency parsing in a shift - reduce framework .",
    "this provides the first example of unsupervised learning for dependency parsing in a non - probabilistic model and shows that unsupervised shift - reduce parsing is possible .",
    "one obvious extension of this work is to structured prediction problems with additional latent structure , such as in machine translation . instead of using the predict - self methodology",
    ", one could directly apply a predict - target methodology .",
    "the view of `` predict the input '' for unsupervised learning is implicit in many unsupervised learning approaches , including standard models such as restricted boltzmann machines and markov random fields .",
    "this is made most precise in the wake - sleep algorithm @xcite , which explicitly trains a neural network to reproduce its own input .",
    "the wake - sleep algorithm consists of two phases : the wake phase , where the latent layers are produced , and the sleep phase , where the input is ( re-)produced .",
    "these two phases are analogous to the predict - structure phase and the predict - words phase in unsupervised searn .",
    "thanks for ryan mcdonald and joakim nivre for discussions related to dependency parsing algorithms .",
    "comments from 5 ( ! ) anonymous reviewers were incredibly helpful .",
    "this was partially supported by nsf grant iis-0712764 ."
  ],
  "abstract_text": [
    "<S> we describe an adaptation and application of a search - based structured prediction algorithm `` searn '' to unsupervised learning problems . </S>",
    "<S> we show that it is possible to reduce unsupervised learning to supervised learning and demonstrate a high - quality unsupervised shift - reduce parsing model . </S>",
    "<S> we additionally show a close connection between unsupervised searn  and expectation maximization . </S>",
    "<S> finally , we demonstrate the efficacy of a semi - supervised extension . </S>",
    "<S> the key idea that enables this is an application of the _ predict - self _ idea for unsupervised learning . </S>"
  ]
}