{
  "article_text": [
    "motivated by applications of the viterbi training ( vt ) algorithm to estimate parameters of hidden markov models in speech recognition @xcite , natural language modeling @xcite , image analysis @xcite , bioinformatics @xcite , and by connections with constrained vector quantization @xcite , we are interested in improving accuracy of the vt estimators while preserving its essential computational advantages .",
    "+ let @xmath0 be the emission parameters of hmm with states @xmath1 .",
    "the standard method to compute a maximum likelihood estimator of @xmath2 is the em algorithm that in the hmm context is also known as the _ baum - welch _ or _ forward - backward algorithm _",
    "since em can in practice be computationally expensive , it is commonly replaced by _",
    "viterbi training_. vt effectively replaces the computationally costly expectation ( e ) step of em by an appropriate maximization step that is computationally less intensive .",
    "an important example of successful and elaborate application of vt in industry is philips speech recognition systems @xcite .",
    "+ there are also variations of vt that use more than one best alignment , or several perturbations of the best alignment @xcite .",
    "the improvements that we explore are , however , of a different nature : roughly , we increase estimation accuracy purely by means of analytic calculations and do not require computing more than one optimal alignment .",
    "+ vt is inferior to em in terms of accuracy because the vt estimators need not be ( local ) maximum likelihood estimators ( vt does not necessarily increase the likelihood ) ; which leads to bias and inconsistency (  [ sec : viterbi_alignment_training ] ) .",
    "+ given current parameter values , vt first finds a viterbi _ alignment _ that is a sequence of hidden states maximizing the likelihood of the observed data .",
    "observations assumed to have been emitted from state @xmath3 , are regarded as an _",
    "_ sample from @xmath4 , the corresponding emission distribution .",
    "these observations produce @xmath5 , the empirical version of @xmath4 , and ultimately @xmath6 , a maximum likelihood estimate of @xmath0 .",
    "@xmath7 is then used to find an alignment in the next step , and so forth .",
    "it can be shown that in general this procedure converges in finitely many steps ; also , it is usually much faster than em .",
    "+ in speech recognition , the same training procedure was already described by l. rabiner _",
    "_ in @xcite ( see also @xcite ) who considered his procedure as a variation of the _",
    "lloyd algorithm _ from vector quantization , and referred to it as _ segmential k - means training_. the analogy with vector quantization is especially pronounced when the underlying chain is a sequence of _ i.i.d .",
    "_ variables in which case the observations are simply an _ i.i.d . _",
    "sample from a mixture distribution (  [ sec : mixture ] ) . for such mixture models ,",
    "viterbi training was also described by r. gray _ et al . _ in @xcite , where the training algorithm was considered in the vector quantization context under the name of _ entropy constrained vector quantization ( ecvq)_. ( see also @xcite for more recent developments in this theory . ) + our goal is to alleviate the inconsistency of the vt estimators while preserving the fast convergence and computational feasibility of the baseline vt algorithm . to this end",
    ", we note that @xmath8 , the true parameters , are asymptotically a fixed point of em but not of vt  [ sec : viterbi_alignment_training ] ,  [ sec : mixture ] .",
    "we thus attempt to adjust vt in order to restore this property by studying the asymptotics of @xmath9 .",
    "thus , we discuss the existence of @xmath10 @xmath11 @xmath12 first in the general hmm context   [ sec : viterbi_alignment_training ] , and then in the special case of mixture models   [ sec : mixture ] . if such limiting measures exist , then under certain continuity assumptions , the estimators @xmath6 will converge to @xmath13 , where @xmath14 taking into account the difference between @xmath13 and the true parameter , the appropriate adjustment of the viterbi training can now be defined (  [ sec : viterbi_alignment_training ] ) .",
    "+ however , the asymptotic behavior of @xmath9 is not in general straightforward and its analysis requires an extension of the definition of viterbi alignment _ at infinitum _ @xcite . with the infinite alignment one can prove the existence of the limiting measures @xmath10 , which is essential for the general definition of the adjusted viterbi training .",
    "+ to implement these ideas in practice , a closed form of @xmath10 ( or @xmath15 ) as a function of the true parameters is necessary .",
    "the measures @xmath10 also depend on the transition as well as on the emission models , and computing @xmath10 can be very difficult .",
    "however , in the special case of mixture models  [ sec : mixture ] , the measures @xmath10 are easier to find .",
    "we are also motivated by the continuing interest of others in computational efficiency and accuracy of parameter estimation in mixture models @xcite . in  [ sec : mixture ] , we describe the adjusted viterbi training ( va1 ) for the mixture case , which we view as the main contribution of this paper : va1 _ recovers the asymptotic fixed point property _ and , since its adjustment function _ does not depend on data _ , each iteration of va1 enjoys the _ same order of computational complexity ( in terms of the sample size ) as vt_. moreover , for commonly used mixtures , such as , for example ( example [ normal ] ) , mixtures of multivariate normal distributions with unknown means and known covariances , the adjustment function is available in a _ closed form _ requiring integration with the mixture densities .",
    "depending on the dimension of the emission variates and on the number of components , and on the available computational resources , one can vary the accuracy of the adjustment .",
    "we reiterate that , unlike the computations of the em algorithm , computations of the adjustment _ do not involve evaluation and subsequent summation of the mixture density at every data point_. + we first introduce these ideas for the case of known mixture weights and then extend them in  [ sec : kaalud ] to the case of unknown weights .",
    "+ to test our theory , in ",
    "[ sec : simu ] we simulate a mixture of two univariate normal distributions with unit variance , unknown means , and unequal but comparable weights",
    ". the main goal of our simulations is to compare the performances of vt , va1 , and em in terms of the accuracy , convergence , amount of computations per iteration , and the total amount of computations .",
    "the simulations are performed under different conditions on the initial guess when the weights are assumed to be known ,  [ sec : known_weights ] , and unknown ,  [ sec : unknown_weights ] , and the results (  [ sec : results ] ) are consistently in favor of va1 .",
    "+ in  [ sec : second ] , we propose va2 , a more mathematically advanced correction to vt ; we verify its fast convergence and high accuracy on the simulated data in  [ sec : simu ] , and intend to elaborate on its computationally feasible implementations in future work . a concluding summary",
    "is given in ",
    "[ sec : conclusion ] .",
    "let @xmath16 be a markov chain with finite state space @xmath17 .",
    "we assume @xmath16 to be irreducible and aperiodic with transition matrix @xmath18 and initial distribution @xmath19 that is also the stationary distribution of @xmath16 . to every state @xmath11",
    "there corresponds an _ emission distribution _",
    "@xmath4 on ( @xmath20 ) , a separable metric space and the corresponding borel @xmath21-algebra .",
    "let @xmath22 , the density of @xmath4 with respect to some reference measure @xmath23 ( for instance , the lebesgue measure ) , be known up to the parametrization @xmath24 .",
    "when @xmath16 is in state @xmath3 , an observation according to @xmath25 and independent of everything else is emitted , with @xmath26 being the unknown true parameters .",
    "+ thus , for any @xmath27 , a realization of @xmath16 , there corresponds a sequence of independent random variables , @xmath28 , where @xmath29 has distribution @xmath30 .",
    "note that we only observe @xmath31 and the realization @xmath32 is unknown ( @xmath16 is hidden ) . + the distribution of @xmath33 is completely determined by the chain parameters @xmath34 and the emission distributions @xmath35 @xmath11 .",
    "the process @xmath33 is also _ mixing _ and , therefore , ergodic .",
    "we now recall viterbi alignment and training .",
    "+ let @xmath36 be first @xmath37 observations on @xmath33 .",
    "let @xmath38 be the likelihood function @xmath39 , @xmath40 .",
    "+ the _ viterbi alignment _ is any sequence of states @xmath41 that maximizes the likelihood of observing @xmath36 , @xmath2 being fixed .",
    "thus , for a fixed @xmath2 , the viterbi alignment is a maximum - likelihood estimator of _ the realization of _ @xmath42 given @xmath36 . in the following , the viterbi alignment will be referred to as the alignment . for each @xmath43 , let @xmath44 be the set of alignments : @xmath45 any map @xmath46 will also be called an alignment . further , unless explicitly specified , @xmath47 will denote an arbitrary element of @xmath48 .    1 .",
    "choose an initial value @xmath49 .",
    "2 .   given @xmath50 @xmath51 ,",
    "find the alignment @xmath52 and partition @xmath53 into @xmath54 subsamples , with @xmath55 going to the @xmath56 subsample if and only if @xmath57 .",
    "equivalently , define @xmath54 empirical measures @xmath58 where @xmath59 stands for the indicator function of set @xmath60 .",
    "3 .   for every subsample find the mle given by : @xmath61 and",
    "take @xmath62 .",
    "if for some @xmath11 , @xmath63 for any @xmath64 ( @xmath56 subsample is empty ) , then the empirical measure @xmath5 is formally undefined , in which case we take @xmath65 .",
    "we omit this exceptional case in the following discussion .",
    "vt can be interpreted as follows .",
    "suppose that at step @xmath66 , @xmath67 and hence @xmath68 is obtained using the true parameters .",
    "the training is then based on the assumption that the alignment @xmath69 is correct , i.e. , @xmath70 , @xmath71 . in this case",
    ", the empirical measures @xmath72 , @xmath11 would be obtained from the i.i.d .",
    "sample generated from @xmath25 , and the mle @xmath73 would be the natural estimator to use .",
    "clearly , under these assumptions @xmath74 a.s . and , provided that @xmath75 is a @xmath4-glivenko - cantelli class and @xmath76 is equipped with some suitable metric , @xmath77 a.s . hence , if @xmath37 is sufficiently large , then @xmath78 and @xmath79 , @xmath80 i.e. @xmath67 would be ( approximately ) a fixed point of the training algorithm .",
    "+ a weak point of the previous argument is that the alignment in general is not correct even when the parameters used to find it are , i.e. generally @xmath81 . in particular , this implies that the empirical measures @xmath82 are not obtained from an i.i.d .",
    "sample taken from @xmath25 .",
    "hence , we have no reason to believe that @xmath83 a.s . and @xmath84 a.s .",
    "moreover , we do not even know whether the sequences of empirical measures @xmath85 and mle estimators @xmath86 converge ( a.s . ) at all .",
    "+ in @xcite , we prove the existence of limiting probability measures @xmath87 , @xmath11 , that depend on @xmath2 , the parameters used to find the alignment @xmath88 , and on @xmath8 , the true parameters with which the random samples are generated .",
    "namely , these @xmath10 , @xmath11 will be such that for every @xmath3 @xmath89 suppose also that the parameter space @xmath76 is equipped with some metric . then , under certain consistency assumptions on classes @xmath90 , the convergence @xmath91 can be deduced from , where @xmath92 we also show that in general , for the baseline viterbi training @xmath93 , implying @xmath94 . in an attempt to reduce the bias @xmath95 , we next propose the _ adjusted viterbi training_. suppose and hold . based on",
    ", we now consider the mapping @xmath96 since this function is independent of the sample , we can define the following correction for the bias : @xmath97    1 .",
    "choose an initial value @xmath49 .",
    "2 .   given @xmath50 ,",
    "perform the alignment and define @xmath54 empirical measures @xmath72 as in .",
    "3 .   for every @xmath5 ,",
    "find @xmath98 as in and for each @xmath3 , define @xmath99 , where @xmath100 is defined @xmath101 in .",
    "note that , as desired , for @xmath37 sufficiently large , the adjusted training algorithm has @xmath8 as its ( approximately ) fixed point : indeed , suppose @xmath67 . from , @xmath102 , for all @xmath11 .",
    "hence , @xmath103",
    "in general , no closed form for the distribution @xmath104 in is available .",
    "therefore , the mapping may be impossible to determine exactly and approximations of @xmath10 should be used for the adjustments of viterbi training (  [ sec : viterbi_alignment_training ] ) .",
    "however , in the case of the mixture model , the distributions @xmath10 are straightforward to find and the adjusted viterbi training can therefore be fully specified . in this model , @xmath16 , the underlying markov chain , is a sequence of i.i.d . discrete random variables with the state space @xmath105 of _ mixture components_. thus , the transition probabilities are @xmath106 , @xmath107 , where @xmath108 are mixture weights .",
    "to each component @xmath11 there corresponds a probability distribution @xmath4 with density @xmath109 , where @xmath110 are the true parameters .",
    "unless explicitly stated otherwise , the mixture weights @xmath111 will be assumed to be known .",
    "such a model produces observations @xmath36 , that are regarded as an i.i.d .",
    "sample from the mixture distribution @xmath112 with density @xmath113 for any set of parameters @xmath114 , the alignment @xmath47 can be obtained via a _ voronoi partition _",
    "@xmath115 , where @xmath116 now , the alignment can be defined as follows : @xmath117 if and only if @xmath118 . in particular , given the voronoi partition @xmath119 , the empirical measures @xmath5",
    "are @xmath120 thus , given the same partition , @xmath121 , the sub - sample mle for component @xmath3 , becomes @xmath122 where @xmath123 is the ordinary empirical measure associated with the given random sample .",
    "the convergence then follows immediately from .",
    "indeed , for any @xmath2 , by virtue of the _ strong law of large numbers _ we have @xmath124 since @xmath125 is separable , it follows that @xmath126 a.s . , where @xmath127 are the densities of respective @xmath87 s .",
    "+ now it is clear that even when the partition @xmath128 is obtained using the true parameters @xmath8 , @xmath104 , the limiting distribution ( density @xmath129 ) , can be different from @xmath25 , the desired distribution ( density @xmath130 ) .",
    "likewise , @xmath131 can be different from @xmath132 in order to see this , note that and in the context of the mixture model specialize to @xmath133 respectively .",
    "we also emphasize that @xmath134 can be significant which justifies the adjustment .",
    "[ normal ] let @xmath135 where @xmath136 is the density of the @xmath137-variate normal distribution with identity covariance matrix and vector of means @xmath138 for @xmath139 . in this case , for each @xmath54-tuple of parameters @xmath140 , the decision - rule for the alignment is essentially as follows ( disregarding possible ties ) : @xmath141 if and only if @xmath142 .",
    "thus , the decision regions in this case correspond to the voronoi partition in its original sense , justifying our generalization of this term .",
    "now , it can be easily seen that for all @xmath143 : @xmath144    when @xmath137 and @xmath54 are large , the exact integration in still requires intensive computations , for which reason one may be interested in approximations of . in the context of the above example , one might think of the following approximations for @xmath145 :    # 1#2#3#4#5 @font    ( 7224,6939)(0,-10 ) ( 4062,3012 ) ( 4887,1437 ) ( 2412,2562 ) ( 6537,3462 ) ( 4962,5187 ) ( 3087,4362 ) ( 1587,4137 ) ( 2487,5712 ) ( 987,3987 ) ( 6837,3837 ) ( 2937,4512 ) ( 4212,3162 ) ( 4940,815 ) ( 1887,2037 ) ( 5037,5637 ) ( 2187,6237 ) ( 2412,3612)(12,2412 ) ( 3012,3312)(3462,1737 ) ( 3012,3312)(3462,1737 ) ( 3462,1737)(5412,2712 ) ( 3462,1737)(5412,2712 ) ( 3462,1737)(2712,12 ) ( 3462,1737)(2712,12 ) ( 5412,2712)(5112,3612 ) ( 5412,2712)(5112,3612 ) ( 5412,2712)(6912,1737 ) ( 5412,2712)(6912,1737 ) ( 4287,3912)(4287,3912 ) ( 4212,3912)(5112,3612 ) ( 4212,3912)(5112,3612 ) ( 2412,3612)(3012,3312 ) ( 2412,3612)(3012,3312 ) ( 3012,3312)(4212,3912 ) ( 3012,3312)(4212,3912 ) ( 5112,3612)(7212,5412 ) ( 5112,3612)(7212,5412 ) ( 4212,3912)(3687,5262 ) ( 4212,3912)(3687,5262 ) ( 2412,3612)(2262,4812 ) ( 2412,3612)(2262,4812 ) ( 2262,4812)(537,5862 ) ( 2262,4812)(537,5862 ) ( 2262,4812)(3687,5262 ) ( 2262,4812)(3687,5262 ) ( 3687,5262)(4212,6912 ) ( 3687,5262)(4212,6912 ) ( 4962,837)(4287,2187)(4287,2112 ) ( 4737,762)(4437,1362 ) ( 4530.915,1274.793)(4437.000,1362.000)(4450.416,1234.544)(4530.915,1274.793 ) ( 2637,3462)(1887,2037 ) ( 1887,2037)(1362,3087 ) ( 1887,2037)(3312,1362 ) ( 1887,2037)(3237,2412 ) ( 1737,2187)(1512,2637 ) ( 1605.915,2549.793)(1512.000,2637.000)(1525.416,2509.544)(1605.915,2549.793 ) ( 1887,2262)(2037,2562 ) ( 2023.584,2434.544)(2037.000,2562.000)(1943.085,2474.793)(2023.584,2434.544 ) ( 2112,2037)(2637,2187 ) ( 2533.980,2110.765)(2637.000,2187.000)(2509.255,2197.302)(2533.980,2110.765 ) ( 1962,1887)(2187,1737 ) ( 2062.192,1766.122)(2187.000,1737.000)(2112.115,1841.006)(2062.192,1766.122 ) ( 2388.392,5793.863)(2487.000,5712.000)(2466.534,5838.515)(2388.392,5793.863 ) ( 2487,5712)(2187,6237 ) ( 2285.608,6155.137)(2187.000,6237.000)(2207.466,6110.485)(2285.608,6155.137 ) ( 1092.503,4059.761)(987.000,3987.000)(1114.331,3972.448)(1092.503,4059.761 ) ( 987,3987)(1587,4137 ) ( 1481.497,4064.239)(1587.000,4137.000)(1459.669,4151.552)(1481.497,4064.239 ) ( 2637,5637)(0,0)[lb ] ( 2350,6250)(0,0)[lb ] ( 1062,2187)(0,0)[lb ] ( 1212,1137)(0,0)[lb ] ( 4287,312)(0,0)[lb ] ( 6612,2862)(0,0)[lb ] ( 5412,4887)(0,0)[lb ] ( 3312,6387)(0,0)[lb ] ( 687,4887)(0,0)[lb ] ( 3312,4662)(0,0)[lb ] ( 4062,3387)(0,0)[lb ] ( 3237,4062)(0,0)[lb ] ( 4287,2787)(0,0)[lb ] ( 1062,4212)(0,0)[lb ] ( 2050,5800)(0,0)[lb ]    1 .",
    "approximate @xmath146 in by @xmath147 , so @xmath148 this approximation is based on the limiting case when the components are `` infinitely '' far from each other .",
    "2 .   if @xmath149 , then some components are fully surrounded by others , namely , the partition cells corresponding to such `` internal '' components are bounded ( figure [ fig : voronoj ] ) .",
    "it is then conceivable that @xmath100 s that correspond to the bounded cells are less significant than the others , in which case one might only correct the estimators of the internal components .",
    "every voronoi cell is determined by several hyperplanes and every such hyperplane @xmath150 gives rise to @xmath151 , a component of @xmath100 in the direction perpendicular to @xmath150 and corresponding to the @xmath56 term in the sum in .",
    "thus , @xmath152 ( see , for example , @xmath153 cell in figure [ fig : voronoj ] ) . it may be reasonable to find only the `` main direction '' of correction , i.e. the largest @xmath154 for each @xmath3 ( see , for example , the @xmath155 cell in figure [ fig : voronoj ] ) .",
    "in example [ normal ] , the decision regions correspond to the voronoi partition in its original sense .",
    "moreover , it is easy to see that in this particular case , the viterbi training is none other than the well - known ( generalized ) lloyd algorithm designed for finding vector quantizers , which in this case are also called _ @xmath54-means _ ( see , e.g. @xcite ) . in this case",
    ", the estimators obtained by the viterbi training are empirical @xmath54-means .",
    "the latter estimators enjoy certain desirable properties , and in particular they are consistent with respect to the population @xmath54-means @xcite .",
    "however , they need not be consistent with respect to @xmath8 , our parameters of interest . in the mixture case ,",
    "the viterbi training can always be considered as the ( generalized ) lloyd algorithm , and the estimators obtained by viterbi training can be regarded as ( generalized ) empirical @xmath54-means @xcite .",
    "this observation links the study of viterbi training and related algorithms to the theory of vector quantization .",
    "the adjusted viterbi training is designed to asymptotically fix the true parameter @xmath8 , returning approximately the correct solution given this solution as the initial guess and given an infinitely large data sample : va1@xmath156 .",
    "va2 goes further and attempts to maximally expand @xmath157 , the set of parameter values that are asymptotically mapped to the true ones , to @xmath158 . specifically ,",
    "if the algorithm ever arrives at @xmath159 , the voronoi partition corresponding to the true parameters @xmath8 , then we would like to coerce the adjusted estimates to return @xmath8 .",
    "let us explain these ideas in more detail .",
    "+ let @xmath160 stand for @xmath161 , the true voronoi partition ( that also coincides with the bayes decision boundary ) .",
    "the mapping @xmath162 is generally many - to - one , hence the set @xmath163 generally contains more than one element .",
    "( this also means that guessing @xmath160 , i.e. guessing any element from @xmath164 , is generally easier than guessing @xmath8 . )",
    "we now introduce va2 : + note first that @xmath165 in , as well as the estimate @xmath166 in , depends on @xmath2 through @xmath167 only .",
    "however , the correction @xmath168 does depend on @xmath2 fully and hence would not generally work ( in the sense of ) for an arbitrary @xmath169 unless @xmath67 .",
    "we now attempt to improve the first type of adjustment that is based on adding @xmath170 to @xmath171 .",
    "namely , we propose the following iterative update for @xmath172 : next , define @xmath173 ( as function of @xmath2 only ) to be the restriction of @xmath174 to @xmath175 , and write @xmath176 in place of the more cumbersome @xmath173",
    ". let @xmath177 for any @xmath50 and @xmath8 , the event that @xmath178 belongs to the range of @xmath179 as a function of @xmath180 is of zero probability , as example [ normalii ] illustrates .",
    "hence , the introduction of the individual inverses @xmath181 @xmath172 is essential , although still not always effective : in some mixture models ( a mixture of normal distributions with unequal weights is one such example ) , for a fixed @xmath3 , the event that @xmath182 belongs in the range of @xmath183 ( as function of @xmath180 ) need not occur with probability one for all @xmath50 and @xmath8 .",
    "this , and also the fact that the inverses in general need not have a closed form , or may require intensive computations , may reduce the attractiveness of the suggested method .",
    "further discussion of the computational issues related to this method is outside the scope of this paper , except for mentioning the possibility of various , e.g. linear or quadratic , approximations of the above functions @xmath184 . + in order to better understand the meaning of the new adjustment , imagine that @xmath185 .",
    "we would then expect for @xmath172 : @xmath186 the above argument , of course , also depends on the regularity of the above inverses at @xmath187 @xmath172 , and in this regard our experiments in  [ sec : simu ] provide encouraging results for an important model similar to the model in the following example :    [ normalii ] let @xmath188 , where @xmath189 is the density of the standard normal distribution . in this case any voronoi partition is specified by a single parameter @xmath190 solving @xmath191 ( ties are evidently inessential in this context ) .",
    "the true voronoi partition corresponds to @xmath192 . given a voronoi partition @xmath193 , @xmath194 .",
    "hence , restricted to @xmath195 , the function @xmath196 depends on one parameter only : let @xmath197 be this parameter and define @xmath198 as follows : @xmath199 , @xmath200 , where @xmath201 is the distribution function of the standard normal distribution . after calculating @xmath202 from the data , the inversion equations of become @xmath203=\\hat{\\mu}_1 , \\quad   t+[a(1 - 2\\phi(-a))+2\\phi(a)]={\\hat\\mu}_2.\\ ] ]",
    "obviously has a ( unique ) solution if and only if @xmath204 , @xmath205 are symmetric with respect to @xmath206 and the probability of this latter event is clearly zero under the model .",
    "thus , as suggested in , we consider the equations separately : @xmath207 it can be shown that and have unique solutions , let us denote the latter by @xmath208 and @xmath209 , respectively .",
    "the points @xmath210 and @xmath211 will be now taken as the estimators of @xmath212 and @xmath213 for the next step of iterations .    1 .",
    "choose @xmath49 .",
    "2 .   given @xmath50 , find @xmath214 and define empirical measures @xmath72 as in .",
    "3 .   for every @xmath5 ,",
    "find @xmath215 as in .",
    "update @xmath216 in accordance with .",
    "we consider the case when the mixture weights @xmath111 are unknown , which corresponds to the case of the unknown transition parameters @xmath34 in the general hmm context .",
    "+ the voronoi partition depends on the weight - vector @xmath217 as well as on @xmath2 .",
    "hence , @xmath218 and the vector @xmath219 should be reestimated at each step along with @xmath2 .",
    "given a voronoi partition @xmath220 , the simplest way to estimate the weights @xmath111 is to take @xmath221 , the empirical measure of @xmath222 .",
    "hence all the algorithms considered so far can be modified accordingly to include the weight estimation as in . @xmath223 taking into account the asymptotics , it is easy to correct the estimators @xmath224 as well . indeed , suppose @xmath67 and @xmath225 , i.e. @xmath226 . if @xmath227 , then @xmath228 in general the latter differs from @xmath111 .",
    "the difference is @xmath229 .",
    "hence , by analogy with , we can define the weight correction @xmath230 as follows : @xmath231 which is also data independent .",
    "we now summarize the above by giving a formal definition of the adjusted viterbi training with the weight correction .",
    "the viterbi training and the second type of adjusted viterbi training with @xmath219 unknown can be defined similarly .    1",
    ".   choose @xmath49 and @xmath232 2 .",
    "given @xmath233 and @xmath234 define the voronoi partition @xmath235 as in and , and the empirical measures @xmath236 as in .",
    "3 .   put @xmath237 where @xmath238 is defined in .",
    "4 .   put @xmath239 .",
    "in order to support our theory of adjusted viterbi training we simulate 1000 i.i.d .",
    "random samples of size 1000 according to the following mixture : @xmath240 the true parameters in our experiments are @xmath241 and @xmath242 .",
    "the corresponding density is plotted in figure [ fig : mixture ] .",
    "note that for all such mixtures with @xmath243 and @xmath244 , @xmath245 ( @xmath246 in our case ) implies that the both means fall on one side of the decision boundary , which makes detection of the second component particularly difficult as is already becoming the case in our setting with @xmath247 @xmath248 .",
    "+ our main goal is to compare the performances of vt , va1 , and em in terms of the accuracy , convergence , amount of computations per iteration , and the total amount of computations .",
    "we implement these algorithms in matlab @xcite , providing a fair comparison of their computational intensities based on their execution times .",
    "our code is available for the reader s perusal @xcite and is fully - optimized for speed in the case of vt and em .",
    "consequently , our simulations possibly only overestimate the execution times for va1 . + additionally , we compare va2 with the above algorithms by the accuracy and convergence .",
    "we use a numerical solver to compute the adjustment function of va2 and presently make no effort to replace this by a computationally efficient approximation .",
    "hence , we do not discuss the computational intensity of va2 in this work . + in our experiments , the algorithms are instructed to terminate as soon as the @xmath249 distance between consecutive @xmath2 updates falls below @xmath250 .",
    "we also provide a high precision mle computed with a built - in matlab optimization function .",
    "the cases of known and unknown weights (  [ sec : kaalud ] ) are considered in ",
    "[ sec : known_weights ] and  [ sec : unknown_weights ] , respectively .",
    "we report the following statistics for each of the algorithms in the form : average@xmath251one standard deviation .",
    "* @xmath252 - the estimates of the means ; * @xmath219 - the estimate of the weight of the first component ; * @xmath253 - @xmath254- and @xmath249-normed distances between @xmath2 and the true parameters ; * @xmath37 - number of steps used by the algorithm ; * @xmath255 - total time in milliseconds to execute the entire algorithm ; * @xmath206 - time in milliseconds to execute one iteration of the algorithm ;      it is often the case in speech recognition models that the weights are assumed known .",
    "+ first , consider @xmath256 as an `` arbitrary '' initial guess for @xmath2 .",
    "table  [ tab : arbitrary ] presents the performance statistics based on the 1000 samples .",
    "the baseline viterbi method terminates quickly ( on average in @xmath257 steps ) , outperformed only by va2 , but is the least accurate among the considered methods .",
    "as expected , vt also requires fewest computations : 0.2 ms per iteration and 1.85 ms total .",
    "ranked from low to high , accuracies of va1 , va2 , and em appear similar , and are about three times superior to that of vt . in units of the vt execution time",
    ", em compares to va1 as 16.85:6.7 per iteration , and as 20.43:7.59 by the total execution times .",
    ".correct initial guess . [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]      va1 is consistently close in accuracy to em which is always superior to viterbi training : specifically , in estimating the means , the gain in accuracy is about three - fold as measured by @xmath254- and @xmath249-distances , and in estimating the weights , it is about one standard deviation .",
    "+ va1 always converges almost as fast as vt and noticeably ( by 30% in the case of unknown weights ) faster than em .",
    "+ when the weights are known , an iteration of va1 is about six times longer than that of vt and is more than twice as fast as that of em . by total execution ,",
    "va1 is at most eight times slower than vt and is more than two and a half times faster than em .",
    "+ when the weights are unknown , va1 is at most twice slower than vt and more than two and a half times faster than em , per iteration .",
    "it is also about 50% slower than than vt and more than four times faster than em in total times .",
    "+ accuracy of va2 is consistently between those of va1 and em , and va2 additionally converges faster than va1 .",
    "we have considered the problem of parameter estimation of the emission distribution in hidden markov models in connection with the two most common estimation algorithms : the em algorithm and the viterbi training algorithm .",
    "we have identified the sources of bias , or lack of consistency , in vt estimation in comparison with em ( mle ) estimation . in the case of hmm",
    ", em computes the mle , which is often consistent .",
    "trading the em s accuracy for the vt s ease of computations , one loses , among other things , the asymptotic fixed point property : vt no longer holds the true parameter values fixed , even asymptotically . in this work",
    ", we have restored this property , and consequently recovered a certain amount of the em s accuracy , without a significant increase in computations relative to viterbi training .",
    "specifically , we have derived two types of adjustments to the baseline viterbi training algorithm .",
    "our first algorithm , va1 , that we also present as the central contribution of this work , is a modification of vt that restores the asymptotic fixed point property .",
    "we also present evidence that , at least in the case of mixture models ( a special and important case of hmm ) , the price in extra computations for this increase in accuracy can be made reasonable .",
    "the second algorithm , va2 , in addition to restoring the fixed point property , also ensures that the true parameters are asymptotically found as soon as the true alignment ( i.e. voronoi partition ) has been found .",
    "this latter feature may require intensive computations , undermining viterbi training as a computationally feasible alternative to em .",
    "we intend to investigate feasible approximations to the correction function of va2 in future work .",
    "+ certainly , the final decision as to which algorithm to use is application dependent , and this work presents valuable information to facilitate such selection , especially in the context of the gaussian mixture models . for this special case , we have provided simulation studies based on 1000 large random samples which illustrate the key features of the adjusted algorithms in contrast with em and baseline viterbi training . in our simulations , va1 demonstrates a significant increase of accuracy ( three - fold and one standard deviation in in estimating the mixture means and weights , respectively ) relative to vt .",
    "in fact , the accuracy of va1 is already comparable to that of em .",
    "computation - wise , va1 in our studies is still several factors faster than em .",
    "we therefore suggest replacing vt by va1 in applications that can afford computing ( to a variable precision ) the correction function in appreciation of the increased accuracy .",
    "bilmes j. 1998 . a gentle tutorial of the em algorithm and its application to parameter estimation for gaussian mixture and hidden markov models .",
    "technical report 97021 , international computer science institute , berkeley , ca , usa .                          ney h. , steinbiss v. , haeb - umbach r. , tran b. , and essen u. 1994 .",
    "an overview of the philips research system for large vocabulary continuous speech recognition .",
    "international journal of pattern recognition and artificial intelligence , 8(1):3370 .",
    "och f. and ney h. 2000 .",
    "improved statistical alignment models . in : proceedings of the 38th annual meeting of the association for computational linguistics . a digital archive of research papers in computational linguistics : http://acl.ldc.upenn.edu/p/p00/p00-1056.pdf .",
    "steinbiss v. , ney h. , aubert x. , besling s. , dugast c. , essen u. , geller d. , haeb - umbach r. , kneser r. , meyer h. , oerder m. , and tran b. 1995 .",
    "the philips research system for continuous - speech recognition .",
    "philips journal of research , 49:317352 .",
    "strm n. , hetherington l. , hazen t. , sandness e. , and glass j. 1999 .",
    "acoustic modeling improvements in a segment - based speech recognizer . in : proc .",
    "ieee asru workshop keystone , co , usa . mit comp .",
    "sci . and ai lab .",
    ", spoken language systems http://www.sls.lcs.mit.edu/sls/publications/1999/asru99-strom.pdf ."
  ],
  "abstract_text": [
    "<S> we study modifications of the viterbi training ( vt ) algorithm to estimate emission parameters in hidden markov models ( hmm ) in general , and in mixure models in particular . </S>",
    "<S> motivated by applications of vt to hmm that are used in speech recognition , natural language modeling , image analysis , and bioinformatics , we investigate a possibility of alleviating the inconsistency of vt while controlling the amount of extra computations . </S>",
    "<S> specifically , we propose to enable vt to asymptotically fix the true values of the parameters as does the em algorithm . </S>",
    "<S> this relies on infinite viterbi alignment and an associated with it limiting probability distribution . </S>",
    "<S> this paper , however , focuses on mixture models , an important case of hmm , wherein the limiting distribution can always be computed exactly ; finding such limiting distribution for general hmm presents a more challenging task under our ongoing investigation .    </S>",
    "<S> a simulation of a univariate gaussian mixture shows that our central algorithm ( va1 ) can dramatically improve accuracy without much cost in computation time .    </S>",
    "<S> we also present va2 , a more mathematically advanced correction to vt , verify by simulation its fast convergence and high accuracy ; its computational feasibility remains to be investigated in future work .    </S>",
    "<S> _ keywords : _ viterbi training algorithm ; hidden markov models ; mixture models ; em algorithm ; consistency ; parameter estimation </S>"
  ]
}