{
  "article_text": [
    "in the early 1990s each instrument delivered to the united kingdom infrared telescope ( ukirt ) and the james clerk maxwell telescope ( jcmt ) came with its own distinct data reduction system that reused very little code from previous instruments . in part",
    "this was due to the rapid change in hardware and software technologies during the period , but it was also driven by the instrument projects being delivered by independent project teams with no standardisation requirements being imposed by the observatory .",
    "the observatories were required to support the delivered code and as operations budgets shrank the need to use a single infrastructure became more apparent .",
    "cgs4dr ( * ? ? ?",
    "* ; * ? ? ?",
    "* http://www.ascl.net/1406.013[ascl:1406.013 ] ) was the archetypal instrument - specific on - line data reduction system at ukirt .",
    "the move from vms to unix in the acquisition environment coupled with plans for rapid instrument development of ufti @xcite , michelle @xcite and uist @xcite , led to a decision to revamp the pipeline infrastructure at ukirt @xcite . in the same time period",
    "the scuba instrument @xcite was being delivered to the jcmt .",
    "scuba had an on - line data reduction system developed on vms that was difficult to modify and ultimately was capable solely of simple quick - look functionality .",
    "there was no explicit data reduction pipeline and this provided the opportunity to develop a truly instrument agnostic pipeline capable of supporting different imaging modes and wavelength regimes .",
    "the observatory reduction and acquisition control data reduction pipeline ( orac - dr ; * ? ? ?",
    "* ; * ? ? ?",
    "* http://www.ascl.net/1310.001[ascl:1310.001 ] ) was the resulting system . in the sections that follow",
    "we present an overview of the architectural design and then describe the pipeline implementation .",
    "we finish by detailing lessons learned during the lifetime of the project .",
    "the general architecture of the orac - dr  system has been described elsewhere @xcite . to summarize ,",
    "the system is split into discrete units with well - defined interfaces .",
    "the recipes define the processing steps that are required using abstract language and no obvious software code .",
    "these recipes are expanded into executable code by a parser and this code is executed with the current state of the input data file objects and calibration system .",
    "the recipes call out to external packages using a standardized calling interface and it is these applications that contain the detailed knowledge of how to process pixel data . in all the currently supported instruments the external algorithm code is from the starlink software collection ( * ? ? ?",
    "* http://www.ascl.net/1110.012[ascl:1110.012 ] ) and uses the adam messaging system @xcite , but",
    "this is not required by the orac - dr  design .",
    "there was a deliberate decision to separate the core pipeline functionality from the high - performance data processing applications so that one single application infrastructure was not locked in .",
    "a key part of the architecture is that the pipeline can function entirely in a data - driven manner .",
    "all information required to reduce the data correctly must be available in the metadata of the input data files .",
    "this requires a systems engineering approach to observatory operations where the metadata are treated as equals to the science pixel data ( see e.g. , * ? ? ?",
    "* for an overview of the jcmt and ukirt approach ) and all observing modes are designed with observation preparation and data reduction in mind .",
    "an overview of the pipeline process is show in fig .",
    "[ fig : flow ] .",
    "in this section we discuss the core components of the pipeline infrastructure .",
    "the algorithms themselves are pluggable parts of the architecture and are not considered further . the only requirement being that the algorithm code must be callable either directly from perl or over a messaging interface supported by perl .",
    "the first step in reducing data is determining which data should be processed .",
    "orac - dr  separates data detection from pipeline processing , allowing for a number of different schemes for locating files . in on - line mode the pipeline is set up to assume an incremental delivery of data throughout the period the pipeline is running .",
    "here we describe the most commonly - used options .",
    "the initial default scheme was to check whether a new file with the expected naming convention had appeared on disk . whilst this can work if the appearance of the data file is instantaneous ( for example , it is written to a temporary location and then renamed ) , it is all too easy to attempt to read a file that is being written to . modifying legacy acquisition systems to do atomic file",
    "renames proved to be difficult and instead a `` flag '' file system was used .",
    "a flag file was historically a zero - length file created as soon as the observation was completed and the raw data file was closed .",
    "the pipeline would look for the appearance of the flag file ( it would be able to use a heuristic to know the name of the file in advance and also look a few ahead in case the acquisition system had crashed ) and use that to trigger processing of the primary data file .    as more complex instruments arrived capable of writing multiple files for a single observation ( either in parallel ( scuba-2 ; * ? ? ? * ) or sequentially ( acsis ; * ? ? ? * ) ) the flag system was modified to allow the pipeline to monitor a single flag file but storing the names of the relevant data files inside the file ( one file per line ) . for the instruments writing files sequentially the pipeline is able to determine the new files that have been added to the file since the previous check .",
    "historically synchronization delays over nfs mounts caused difficulties when the flag file would appear but the actual data file had not yet appeared to the nfs client computer , but on modern systems this behavior no longer occurs .",
    "modern file event notification schemes ( such as ` inotify ` on linux ) do not generally help with the data detection problem since , in the current setup , the data reduction pipelines always mount the data disks from the acquisition computer over nfs .",
    "a more robust solution is to implement a publish / subscribe system whereby the pipeline monitors the acquisition computers for new data .",
    "such a scheme is discussed in the next section .",
    "the scuba-2 quick look pipeline @xcite had a requirement to be able to detect files taken at a rate of approximately 1hz for stare observations .",
    "this was impractical using a single - threaded data detection system embedded in the pipeline process and using the file system .",
    "therefore , for scuba-2 quick - look processing the pipeline uses a separate process that continually monitors the four data acquisition computers using the drama messaging system @xcite . when all four sub - arrays indicate that a matched data set is available the monitored data are written to disk and a flag file created .",
    "since these data are ephemeral there is a slight change to flag file behavior in that the pipeline will take ownership of data it finds by renaming the flag file .",
    "if that happens the pipeline will be responsible for cleaning up ; whereas if the pipeline does not handle the data before the next quick look image arrives the gathering process will remove the flag file and delete the data before making the new data available .",
    "once files have been found they are first sent to the format conversion library .",
    "the instrument infrastructure defines what the external format of each file is expected to be and also the internal format expected by the reduction system .",
    "the format conversion system knows how to convert the files to the necessary form .",
    "this does not always involve a change in low level format ( such as fits to ndf ) but can handle changes to instrument acquisition systems such as converting hds files spread across header and exposure files into a single hds container matching the modern ukirt layout .      a _ recipe _  is the top - level view of the data processing steps required to reduce some data .",
    "the requirements were that the recipe should be easily editable by an instrument scientist without having to understand the code , the _ recipe _  should be easily understandable by using plain language , and it should be possible to reorganize steps easily . furthermore , there was a need to allow _ recipes _  to be edited `` on the fly '' without having to restart the pipeline .",
    "the next data file to be picked up would be processed using the modified version of the _ recipe _  and this is very important during instrument commissioning .",
    "an example , simplified , imaging _ recipe _  is shown in fig.[fig : recipe ] .",
    "each of these steps can be given parameters to modify their behavior .",
    "the expectation was that these _ recipes _  would be loadable into a recipe editor gui tool , although such a tool was never implemented .",
    "_ subtract_dark _ _ divide_by_flat _ _ bias_correct_group _ _ apply_distortion_transformation _ _ generate_offsets_jitter _ _ make_mosaic _ fillbad=1 resample=1 ....    each of the steps in a _ recipe _  is known as a _",
    "primitive_. the _ primitives _  contain the perl source code and can themselves call other _ primitives _  if required .",
    "the parser s core job is to read the _ recipe _ , replace the mentions of _",
    "primitives_with subroutine calls to the source code for that primitive . for each",
    "_ primitive _  the parser keeps a cache containing the compiled form of the _ primitive _  as a code reference , the modification time associated with the _ primitive _  source file when it was last read , and the full text of the _ primitive _  for debugging purposes .",
    "whenever a _",
    "primitive_code reference is about to be executed the modification time is checked to decide whether the _ primitive _  needs to be re - read .",
    "the parser is also responsible for adding additional code at the start of the _ primitive _  to allow it to integrate into the general pipeline infrastructure .",
    "this code includes :    * handling of state objects that are passed through the subroutine argument stack and parsing of parameters passed to the _ primitive _ by the caller .",
    "these arguments are designed not be language - specific and use a simple ` keyword = value ` syntax and can not be handled directly by the perl interpreter . *",
    "trapping for _ primitive _",
    "call recursion .",
    "* debugging information such as timers to allow profile information be collected , and entry and exit log messages to indicate exactly when a routine is in use . *",
    "callbacks to gui code to indicate which _ primitive",
    "_  is currently active . *",
    "configuring the logging system so that all messages appearing will be associated with the correct primitive when they are written to the history blocks ( see sect .",
    "[ sec : prov ] for details ) .",
    "the design is such that adding new code to the entry and exit of each _ primitive _  can be done in a few lines with little overhead .",
    "in particular , use is made of the ` # line ` directive in perl that allows for the line number to be manipulated such that error messages reflect the line number in the original _ primitive _  and not the line number in the expanded _",
    "primitive_.    calling external packages is a very common occurrence and is also where most of the time is spent during _ recipe",
    "_  execution . in order to minimize repetitive coding for error conditions and to allow for profiling ,",
    "calls to external packages are surrounded by code to automatically handle these conditions .",
    "this allows the programmer to focus on the _ recipe _  logic and not have to understand all the failure modes for a particular package .",
    "the parser is designed such that if a particular error code is important ( for example there might be an error code indicating that a failure was due to there being too few stars in the image ) then the automated error handling is changed if the _ primitive _  writer is explicitly asking to check the return value from the external application .",
    "the general behavior of a recipe can be controlled by editing it and adjusting the parameters passed to the _ primitives_. a much more flexible scheme is available which allows the person running the pipeline to specify a _ recipe _  configuration file that can be used to control the behavior of _ recipe _",
    "selection and how a _ recipe _  behaves .",
    "the configuration file is a text file written in the ini format .",
    "although it is possible for the _ recipe _  to be specified on the command - line that _ recipe _  would be used for all the files being reduced in the same batch and this is not an efficient way to permanently change the _ recipe _  name . changing the file header is not always possible so",
    "the configuration file can be written to allow per - object selection of _",
    "recipes_. for example ,    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .... [ recipes_science ] object1=reduce_science object2=reduce_faint_source a.*=bright_compact .... _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    would select ` reduce_science ` whenever a _ science _ observation of object1 is encountered but choose ` reduce_faint_source ` whenever object2 is found .",
    "the third line is an example of a regular expression that can be used to select recipes based on a more general pattern match of the object name .",
    "this relies on header translation functioning to find the observation type and object name correctly .",
    "this sort of configuration is quite common when the observing tool has not been set up to switch recipes .",
    "once a _ recipe _  has been selected it can be configured as simple key - value pairs :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .... [ reduce_science ] param1 = value1 param2 = value2    [ reduce_science : a . * ] param1 = value3 .... _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    and here , again , the parameters selected can be controlled by a regular expression on the object name .",
    "the final set of parameters are made available to the primitives in a key - value lookup table .",
    "once a set of files have been found the header is read to determine how the data should be reduced .",
    "files from the same observation are read into what is known as a _ frame _  object .",
    "this object contains all the metadata and pipeline context and , given that the currently used applications require files to be written , the name of the currently active intermediate file ( or files for observations that either consist of multiple files or which generate multiple intermediate files ) . in some cases , such as for acsis , a single observation can generate multiple files that are independent and in these cases multiple _ frame _  objects are created and they are processed independently .",
    "there is also a _ group _",
    "object which contains the collection of _ frame _  objects that the pipeline should combine .",
    "the pipeline will have been initialized to expect a particular instrument and the resulting _ frame _  and _ group _  objects will be instrument - specific subclasses .",
    "the _ frame _  object contains sufficient information to allow the pipeline to work out which _ recipe _  should be used to reduce the data .",
    "the _ recipe _  itself is located by looking through a search path and modifiers can be specified to select recipe variants .",
    "for example , if the recipe would normally be ` reduce_science ` the pipeline can be configured to prefer a recipe suffix of ` _ ql ` to enable a quick - look version of a recipe to be selected at the summit whilst selecting the full recipe when running off - line . the top - level _ recipe _  is parsed and",
    "is then evaluated in the parent pipeline context using the perl ` eval ` function .",
    "the _ recipe _  is called with the relevant _ frame _ , and _ group _  objects along with other context .",
    "the reason we use ` eval ` rather than running the recipe in a distinct process is to allow the recipe to update the state . as discussed in sect .",
    "[ sec : onvoff ] , the pipeline is designed to function in an incremental mode where data are reduced as they arrive , with group co - adding either happening incrementally or waiting for a set cadence to complete .",
    "this requires that the group processing stage knows the current state of the _ group _  object and of the contributing _ frame _  objects .",
    "launching an external process to execute the recipe each time new data arrived would significantly complicate the architecture .    as noted in the previous section ,",
    "the _ recipe _  is parsed incrementally and the decision on whether to re - read a _ primitive _",
    "is deferred until that _ primitive _  is required .",
    "this is important for instruments such as michelle and uist which can observe in multiple modes ( spectroscopy , imaging , ifu ) , sometimes requiring a single recipe invocation to call _ primitives _",
    "optimized for the different modes .",
    "the execution environment handles this by allowing a caller to set the instrument mode and this dynamically adjusts the _ primitive _  selection code .",
    "as more instruments were added to orac - dr  it quickly became apparent that many of the _ primitives _",
    "were being adjusted to support different variants of fits headers through the use of repetitive if / then / else constructs .",
    "this was making it harder to support the code and it was decided to modify the _ primitives _  to use standardized headers .",
    "when a new _ frame _  object is created the headers are immediately translated to standard form and both the original and translated headers are available to _ primitive _  authors .",
    "the code to do the translation was felt to be fairly generic and was written to be a standalone module .",
    "each instrument header maps to a single translation class with a class hierarchy that allows , for example , jcmt instruments to inherit knowledge of shared jcmt headers without requiring that the translations be duplicated .",
    "each class is passed the input header and reports whether the class can process it , and it is an error for multiple classes to be able to process a single header .",
    "a method exists for each target generic header where , for example , the method to calculate the start airmass would be ` _",
    "to_airmass_start ` . the simple unit mappings ( where there is a one - to - one mapping of an instrument header to a generic header without requiring changes to units ) are defined as simple perl lookup tables but at compile - time the corresponding methods are generated so that there is no difference in interface for these cases .",
    "complex mappings that may involve multiple input fits headers , are written as explicit conversion methods .",
    "the header translation system can also reverse the mapping such that a set of generic headers can be converted back into instrument - specific form .",
    "this can be particularly useful when required to update a header during processing .      during _ frame _  processing it is necessary to make use of calibration frames or parameters derived from calibration observations .",
    "the early design focused entirely on how to solve the problem of selecting the most suitable calibration frame for a particular science observation without requiring the instrument scientist to write code or understand the internals of the pipeline .",
    "the solution that was adopted involves two distinct operations : filing calibration results and querying those results . when a calibration image is reduced ( using the same pipeline environment as science frames ) the results of the processing",
    "are registered with the calibration system .",
    "information such as the name of the file , the wavelength , and the observing mode are all stored in the _ index_. in the current system the _ index _  is a text file on disk that is cached by the pipeline but the design would be no different if an sql database was used instead ; no _ primitives _  would need to be modified to switch to an sql backend .",
    "the only requirement is that the _ index _  is persistent over pipeline restarts ( which may happen a lot during instrument commissioning ) .",
    "the second half of the problem was to provide a rules - based system .",
    "a calibration rule simply indicates how a header in the science data must relate to a header in the calibration database in order for the calibration to be flagged as suitable .",
    "the following is an excerpt from a rules file for an imaging instrument dark calibration :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .... obstype eq ' dark ' mode eq $ hdr{mode } exp_time = = $ hdr{exp_time } meancount .... _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    each row in the rules file is evaluated in turn by replacing the unadorned keyword with the corresponding calibration value read from the _ index _  and the ` $ hdr ` corresponding to the science header . in the above example the calibration would match if the exposure times and observing readout mode match and the calibration itself is a dark .",
    "these rules are evaluated using the perl ` eval ` command so the full perl interpreter is available .",
    "this allows for complex rules to be generated such as a rule that allows a calibration to expire if it is too old .",
    "the rules file itself represents the schema of the database in that for every line in the rules file , information from that calibration is stored in the _ index_. in the example above , ` meancount ` is not used in the rules processing but the presence of this item means that the corresponding value will be extracted from the header of the calibration image and registered in the calibration database .",
    "once an item is stored in the calibration database a calibration query will make that value available in addition to the name of the matching calibration file .",
    "it is therefore simple for the instrument scientist to add a new header for tracking , although this does require that the old _ index _  is removed and the data reprocessed to regenerate a new _ index _  in the correct form .",
    "the calibration selection system can behave differently in off - line mode as the full set of calibrations can be made available and calibrations taken after the current observation may be relevant .",
    "each instrument s calibration class can decide whether this is an appropriate behavior .",
    "the calibration system can also be modified by a command - line argument at run time to allow the user to decide which behavior to use .",
    "for example , with the scuba pipeline @xcite the user can decide which opacity calibration scheme they require from a number of options .",
    "one of the more controversial aspects of the calibration system was that the ukirt pipelines would stop and refuse to reduce data if no suitable calibration frame had been taken previously ( such as a dark taken in the wrong mode or with the wrong exposure ) .",
    "this sometimes led to people reporting that the pipeline had crashed ( and so was unstable ) but the purpose was to force the observer to stop and think about their observing run and ensure that they did not take many hours of data with their calibration observations being taken in a manner incompatible with the science data .",
    "a pro - active pipeline helped to prevent this and also made it easier to support flexible scheduling @xcite without fearing that the data were unreducible .    this hard - line approach to requiring fully calibrated observations , even if the pi s specific science goals did not require it , was adopted in anticipation of the emergence of science data archives as an important source of data for scientific papers . casting the pi not as the data owner , but rather as somebody who is being leased observatory data from the public domain for the length of their proprietary period , requires an observation as only being complete if fully calibratable . in that way",
    ", the telescope time s value is maximised by making the dataset useful to the widest range of its potential uses . to this end",
    ", the authors favor a model where for flexibly - scheduled pi - led facilities , calibration time is not deducted from the pi s allocation .      for the outputs from a data reduction pipeline",
    "it is important for astronomers to understand what was done to the data and how they can reproduce the processing steps .",
    "orac - dr  manages this provenance and history tracking in a number of different ways .",
    "the pipeline makes available to _ primitives _",
    "the commit i d ( sha1 ) of the pipeline software and the commit i d of the external application package .",
    "it is up to the _ recipe _  to determine whether use should be made of that information .",
    "for the _ recipes _  that run at the jcmt science archive @xcite there is code that inserts this information , and the _ recipe _  name , into data headers .",
    "summit processing _ recipes _  do not include this detail as the products are generally thought to be transient in nature as the _ recipes _  are optimized for speed and quality assurance tracking rather than absolute data quality .",
    "one caveat in this approach is that an end - user who modifies a _ recipe _  will not see any change as the commit i d will not have changed .",
    "this was thought to be of secondary importance compared to the major use case of archive processing but does need consideration before the reproducibility aspects of data reduction can be considered complete .",
    "detailed tracking of the individual steps of the processing are handled differently in that the pipeline is written with the assumption that the external applications will track provenance and history themselves .",
    "this is true for the starlink software where the ndf library , which already supported detailed history tracking , was updated to also support file provenance so that all ancestor files could be tracked ( see e.g. * ? ? ?",
    "* for details on the provenance algorithm ) .",
    "we took this approach because we felt it was far too complicated to require that the pipeline infrastructure and _ primitives _",
    "track what is being done to the data files . modifying the file",
    "i / o library meant that provenance tracking would be available to all users of the external packages ( in this case the starlink software applications ) and not just the pipeline users .",
    "the history information automatically logged by the external applications is augmented by code in the pipeline that logs the primitive name whenever header information is synchronized to a file , and , optionally , all text messages that are output by a _ primitive _  can be stored as history items in the files written by the _",
    "primitive_.      on - line pipelines are most useful when results are displayed to the observer .",
    "one complication with pipeline display is that different observers are interested in different intermediate data products or wish the final data products to be displayed in a particular way .",
    "display logic such as this can not be embedded directly in _ primitives _ ; all a _ primitive _  can do is indicate that a particular product _ could _ be displayed and leave it to a different system to decide _ whether _ the product should be displayed and how to do so .",
    "the display system uses the orac - dr  file naming convention to determine relevance .",
    "usually , the text after the last underscore , referred to as the file suffix , is used to indicate the reduction step that generated the file : ` mos ` for mosaic , ` dk ` for dark , etc . when a _ frame _  or _ group _  is passed to the display system the file suffix and , optionally a _ group _  versus _ frame _  indicator , are used to form an identifier which is compared with the entries in the display configuration file . for each row containing a matching identifier",
    "the files will be passed to the specific display tool .",
    "different plot types are available such as image , spectrum , histogram , and vector plot and also a specific mode for plotting a 1-dimensional dataset over a corresponding model .",
    "additional parameters can be used to control placement within a viewport and how auto - scaling is handled .",
    "the display system currently supports gaia ( * ? ? ? * http://www.ascl.net/1403.024[ascl:1403.024 ] ) and kappa ( * ? ? ? * http://www.ascl.net/1403.022[ascl:1403.022 ] ) as well as the historical p4 tool ( part of cgs4dr  @xcite and an important influence on the design ) .",
    "originally the display commands would be handled within the _ recipe_execution environment and would block the processing until the display was complete .",
    "this can take a non - negligible amount of time and for the scuba-2 pipeline to meet its performance goals this delay was unacceptable .",
    "the architecture was therefore modified to allow the display system running from within the _ recipe _  to register the display request but for a separate process to be monitoring these requests and triggering the display .      as well as",
    "the systems described above there are general support modules that provides standardized interfaces for message output , log files creation and temporary file handling .",
    "the message output layer is required to allow messages from the external packages and from the _ primitives_to be sent to the right location .",
    "this might be a gui , the terminal or a log file ( or all at once ) and supports different messaging levels to distinguish verbose messages , from normal messages and warnings .",
    "internally this is implemented as a tied object that emulates the file handle api and contains multiple objects to allow messages to be sent to multiple locations .",
    "log files are a standard requirement for storing information of interest to the scientist about the processing such as quality assurance parameters or photometry results .",
    "the pipeline controls the opening of these files in a standard way so that the primitive writer simply has to worry about he content .    with the current external applications",
    "there are many intermediate files and most of them are temporary .",
    "the allocation of filenames is handled by the infrastructure and they are cleaned up automatically unless the pipeline is configured in debugging mode to retain them .",
    "an important part of the orac - dr  philosophy is to make adding new instruments as painless as possible and re - use as much of the existing code as possible .",
    "the work required obviously depends on the type of instrument .",
    "an infrared array will be straightforward as many of the _ recipes _  will work with only minor adjustments .",
    "adding support for an x - ray telescope or radio interferometer would require significantly more work on the recipes .",
    "to add a new instrument the following items must be considered :    * how are new data presented to the pipeline ?",
    "orac - dr  supports a number of different data detection schemes but ca nt cover every option .",
    "* what is the file format ? all the current _ recipes _",
    "use starlink applications that require ndf @xcite and if fits files are detected the infrastructure converts them to ndf before handing them to the rest of the system .",
    "if the raw data are in hdf5 , or use a very complex data model on top of fits , new code will have to be written to support this . * how to map the metadata to the internal expectations of the pipeline ?",
    "a new module would be needed for ` astro::fits::hdrtrans ` .",
    "* does it need new _",
    "recipes_/_primitives _ ?",
    "this depends on how close the instrument is to an instrument already supported .",
    "the _ recipe _",
    "parser can be configured to search in instrument - specific sub - directories and , for example , the las cumbres observatory imaging recipes use the standard _ primitives _  in many case but also provide bespoke versions that handle the idiosyncrasies of their instrumentation .",
    "once this has been decided new subclasses will have to be written to encode specialist behavior for _ frame _  and _ group _  objects and the calibration system , along with the instrument initialization class that declares the supported calibrations and applications .",
    "in 1998 the best choice of dynamic `` scripting '' language for an astronomy project was still an open question with the main choices being between perl and tcl / tk with python being a distant third @xcite .",
    "tcl / tk had already been adopted by starlink @xcite , stsci @xcite , sdss @xcite and eso @xcite and would have been the safest choice , but at the time it was felt that the popularity of tcl / tk was peaking .",
    "perl was chosen as it was a language gaining in popularity and the development team were proficient in it in addition to developing the perl data language ( pdl ; * ? ? ?",
    "* ) promising easy handling of array data ; something tcl / tk was incapable of handling .    over the next decade and a half ,",
    "beginning with the advent of ` pyraf ` ( * ? ? ?",
    "* ; * ? ? ?",
    "* http://www.ascl.net/1207.010[ascl:1207.010 ] ) and culminating in astropy , python became the dominant language for astronomy , becoming the _ lingua franca _ for new students in astronomy and the default scripting interface for new data reductions systems such as those for alma @xcite and lsst @xcite .",
    "in this environment , whilst orac - dr  received much interest from other observatories , the use of perl rather than python became a deal - breaker given the skill sets of development groups . during this period",
    "only two additional observatories adopted the pipeline : the anglo - australian observatory for iris2 @xcite and las cumbres observatory for their imaging pipeline @xcite .",
    "the core design concepts were not at issue , indeed , gemini adopted the key features of the orac - dr  design in their gemini recipe system @xcite . with approximately 100,000 lines of perl code in orac - dr it is impractical to rewrite it all in python given that the system does work as designed .",
    "of course , a language must be chosen without the benefit of hindsight but it is instructive to see how the best choice for a particular moment can have significant consequences 15 years later .",
    "when orac - dr  was being designed the choice was between iraf ( * ? ? ? * http://www.ascl.net/9911.002[ascl:9911.002 ] ) and starlink for the external packages . at the time the answer was that starlink messaging and error reporting were significantly more robust and",
    "allowed the _ primitives _  to adjust their processing based on specific error states ( such as there being too few stars in the field to solve the mosaicking offsets ) . additionally , starlink supported variance propagation and a structured data format . from a software engineering perspective starlink was clearly the correct choice but it turned out to be yet another reason why orac - dr  could not be adopted by other telescopes . both these environments relied on each command reading data from a disk file , processing it in some way and then writing the results out to either the same or a new file .",
    "many of these routines were optimized for environments where the science data was comparable in size to the available ram and went to great lengths to read the data in chunks to minimize swapping .",
    "it was also not feasible to rewrite these algorithms ( that had been well - tested ) in the perl data language , or even turn the low - level libraries into perl function calls , and the penalty involved in continually reading and writing to the disk was deemed to be a good trade off .    as it turns out , the entire debate of starlink versus iraf is somewhat moot in the current funding climate and in an era where many pipeline environments ( e.g. , * ? ? ?",
    "* ) are abandoning intermediate files and doing all processing in memory for performance reasons , using , for example , ` numpy ` arrays or `` piddles '' . for instruments where the size of a single observation approaches 1 tb ( e.g. , swcam at ccat ; * ? ? ?",
    "* ) this presents a sizable challenge but it seems clear that this is the current trend and a newly written pipeline infrastructure would assume that all algorithm work would be in memory .",
    "initially , the intent was for _ recipes _  to be edited to suit different processing needs and for the processing to be entirely driven by the input data .",
    "this was driven strongly by the requirement that the pipeline should work at the telescope without requiring intervention from the observer .",
    "the initial design was meant to be that the astronomer would select their _ recipe _  when they prepared the observation and that this would be the _ recipe _  automatically picked up by the pipeline when the data were observed .",
    "eventually we realized that anything more than two or three recipes to choose from ( for example , is your object broad line or narrow line , or are your objects extremely faint point sources or bright extended structures ? ) in the observing tool became unwieldy and most people were nt sure how they wanted to optimize their data processing until they saw what the initial processing gave them .",
    "after many years of resistance a system was developed in 2009 for passing _ recipe _",
    "parameters from configuration files to the pipeline and this proved to be immensely popular .",
    "it is much simpler for people to tweak a small set of documented parameters than it is to edit recipes and it is also much easier to support many project - specific configuration files than it is to keep track of the differences between the equivalent number of bespoke recipes .",
    "when a processing job is submitted to the jcmt science archive any associated project - specific configuration file is automatically included , and these can be updated at any time based on feedback from the data products .",
    "it took far too long to add this functionality and this delay was partly driven by the overt focus on online functionality despite the shift to the pipeline being used predominantly in an offline setting .",
    "this is discussed further in the next section .",
    "orac - dr  was initially designed for on - line summit usage where data appear incrementally and where as much processing should be done on each frame whilst waiting for subsequent frames to arrive .",
    "as discussed previously ( sect .",
    "[ sec : exec ] ) , this led to the execution of the _ recipe _  within the main process so that context could be shared easily .    for off - line mode",
    "the environment is very different and you would ideally wish to first reduce all the calibration observations , then process all the individual science observations and finally do the group processing to generate mosaics and co - adds . when doing this the only context that would need to be passed between different _ recipe_executions would be the calibration information that is already persistent .",
    "indeed , the _ recipes _  themselves could be significantly simplified in that single observation _ recipes",
    "_  would not include any group processing instructions .",
    "this is not strictly possible in all cases . for the acsis data reduction _ recipes _",
    "@xcite the output of the frame processing depends on how well the group co - adding has been done ; the more measurements that are included , the better the baseline subtraction .",
    "as written , the recipes have to handle both on - line and off - line operation and this is achieved by the group _ primitives _  being configured to be no - ops if they realize that the _ frame _  object that is currently being processed is not the final member of the group . whilst the off - line restrictions can be annoying to someone reducing a night of data on their home machine , it is possible to deal with the problems by scanning through a set of observations and running the pipeline separately for each one",
    "this is exactly how the the healpix processing for the jcmt science archive is implemented @xcite .",
    "parallelization is therefore occurring at a level above the orac - dr  pipeline itself .",
    "currently , the ukirt science archive @xcite reduces data in two passes : first all the calibrations are reduced from a single night and then all the science data are reduced together .",
    "this is particularly important for the archival data taken before the orac software was released . in 2012",
    "las cumbres added the ability to sort the _ group _  objects before starting the processing in off - line mode and this allows them to reduce all the calibration observations before doing the science observations .",
    "the picard @xcite frontend to the infrastructure was developed to try to overcome some of the on - line bias in the data handling .",
    "the * * pi**peline for * * c**ombining and * * a**nalyzing * * r**educed * * d**ata , was introduced in 2007 @xcite specifically to allow the existing infrastructure to be leveraged in an off - line science archive environment .",
    "the orac - dr  layer was removed and replaced with a simplified application that accepts a list of files , works out what type of instrument they come from , and uses the same parser to read the specified recipe .      whilst the initial driver for dynamic recipe generation came directly out of the requirement for a readable _ recipe _ , it would have been simple to write the _ primitives _  as perl subroutines from the beginning and require the primitive writer to handle the subroutine arguments and return codes . in some sense , this would have been the obvious approach as that is how most people want to write code .",
    "it soon became clear that the benefits associated with running a parser over the _ primitives _",
    "were substantial .",
    "not only could we minimize the use of repetitive code but we could dynamically add in profiling code .",
    "in fact , this was critical to the ongoing development of the orac - dr  infrastructure as the current version of the parser is the third complete rewrite and none of the rewrites required any change to _ primitive _  code .",
    "the first version of the parser did not use subroutines at all and simply expanded the _",
    "recipe_into one long string for evaluation .",
    "perl does not really support multi - threaded operation and this has led to some problems with components of the system that use blocking i / o such as the display system or the file detection code . being able to monitor new data arriving whilst creating",
    "a thread for the current data to be processed seems attractive but would probably have created more complication than would be acceptable and using separate processes has worked despite it feeling `` kludgy '' .",
    "similarly , the way the messaging interface was implemented meant that it was not possible to send multiple messages concurrently .",
    "the ability to send four files to four separate , but identical , tasks in parallel , and waiting for them all to complete would have led to some efficiency games even if the tasks were all running on the single pipeline computer .",
    "we are again struggling with this issue as the smurf map - maker ( * ? ? ? * http://www.ascl.net/1310.007[ascl:1310.007 ] ) , currently supported by orac - dr , is being modified to support multi - node operation using mpi @xcite .",
    "it may be that implementing mpi at the task level is much easier than at the messaging level but this remains to be tested .",
    "orac - dr  has been in use since 1998 , running on four telescopes and ten instruments covering the submillimeter to the optical and imaging to ifus .",
    "prototype pipelines have also been developed for gemini and eso instruments to prove the inherent flexibility of the design @xcite and the ccat project are considering adopting it @xcite .    in an environment similar to that now promoted by the devops movement ( see e.g. , * ? ? ?",
    "* ) , we participated , together with our contributors , in the design , implementation , extension , operation , distribution and night - time telescope support of the pipeline as it was running on every night at ukirt , jcmt and later the jcmt science archive over a period of 16 years .",
    "we therefore had the opportunity to evaluate what features contributed to the easy extensibility and low maintenance cost of the codebase over a long period of time in a changing computing environment .",
    "we regard the following as key design successes of orac - dr , and hope to see them repeated in other astronomical data reduction infrastructures :    * the clean separation of recipes , primitives , instrument specification , logging , and external applications .",
    "the core design is unchanged from the beginning and we frequently reaped the benefits of a design that meant that the science code was well isolated from the infrastructure components that were in turn relatively agnostic about their implementation .",
    "thus we were able to engage in major pieces of refactoring such as rewriting the recipe parser or reorganizing the class hierarchy for _ frame _  and _ group _  classes without affecting the pipeline users . *",
    "reusing primitive code for new instruments was a key requirement and this has worked ; especially once header translation was added .",
    "for example , las cumbres observatory are adding additional instruments to the pipeline and can reuse the imaging _ primitives _ , only modifying code that defines the differences between instruments .",
    "a similar approach has been used by the lsst data management stack to allow it to support different camera geometries @xcite . * defining",
    "the items of interest in the calibration rules file guarantees that all the required information is available to the calibration system whilst also making it trivial to add additional information to the database .",
    "such techniques anticipate the evolving requirements of astronomical observing and build in a flexibility that requires no , or minimal , changes .",
    "it also externalizes to instrument scientists important aspects of the pipeline operations in a human readable ( and configurable ) way . *",
    "orac - dr  was used at ukirt and jcmt as a `` quick - look '' pipeline during observing ( even though the data reduction quality was higher than the term suggests ) and so was a vital part to the smooth functioning of the observatory . as is often the case , the pipeline was often the first system to alert the observers that something had gone awry with data taking , and so occasionally triggered night - time support",
    "calls reporting that `` there was something wrong with the pipeline '' , despite the fact that an extensive suite of engineering interfaces were also available to the operators .",
    "we conclude from this that despite the understandable tendency to focus on the scientific user of the data , data reduction systems are often the first port of call for investigating technical issues .",
    "we frequently utilized the completeness and readability of our file metadata to identify problems for our engineers , and so came to regard the data reduction infrastructure as a core technical element of a modern facility as well as a valuable scientific productivity tool .    ultimately , the design of the pipeline infrastructure succeeded in its goals of minimizing the amount of code needed for each new instrument delivery , re - using a significant amount of code available to the community and in making a system that can be tweaked easily by instrument scientists and interested astronomers",
    "orac - dr  was initially developed for ukirt as part of the orac project @xcite , and then further developed by the joint astronomy centre to support jcmt and ukirt instrumentation .",
    "we thank all the orac - dr  recipe developers who have helped make the pipeline successful , and in particular we thank malcolm currie , brad cavanagh , andy gibb , paul hirst , tim lister , stuart ryder and stephen todd for their significant contributions .",
    "we thank malcolm currie and andy gibb for comments on the draft manuscript .",
    "56 natexlab#1#1[1]`#1 ` [ 2]#2 [ 1]#1 [ 1]http://dx.doi.org/#1 [ ] [ 1]pmid:#1 [ ] [ 2]#2 , , , , , , , . , in : , ( eds . ) , , volume of _ _",
    ", , , . . ,",
    ". . , . , in : , , ( eds . ) , , volume   of _ _ . p. . ,",
    ". , http://arxiv.org/abs/1307.6212[arxiv:1307.6212 ] . , , , , . ,",
    "in : , ( eds . ) , , volume of _ _ . p. . .",
    ", in : ( ed . ) , , volume of _ _ .",
    ", , . , in : , , ( eds . ) , , volume of _ _ . p. .",
    ", , , , , a. , in : , ( eds . ) , , volume of _ _ . p. .",
    "bell , g.  s. et  al . ,",
    "b. , in : , ( eds . ) , , volume of _ _ . p. . .",
    ", , , , . , in : ( ed . ) , ,",
    "volume of _ _ .",
    ", t.  m. et  al . , . .",
    ", http://arxiv.org/abs/1305.2437[arxiv:1305.2437 ] . , j.  v. et  al . , .",
    ", http://arxiv.org/abs/0907.3610[arxiv:0907.3610 ] .",
    ", , , , , , , . , in : , , ( eds . ) , , volume of _ _ . p. .",
    ", , , , . . ,",
    ", , , , , , , , . . ,",
    ". , http://arxiv.org/abs/1301.3652[arxiv:1301.3652 ] . , , .",
    ", in : , , ( eds . ) , , volume   of _ _ . p.  . , . , in : ,",
    ", ( eds . ) , , volume of _ _ . p. .",
    "joint astronomy centre / starlink project .",
    ", , , , , , . , in : , ( eds . ) , , volume of _ _ .",
    "joint astronomy centre .",
    ", , , . , in : , , ( eds . ) , , volume   of _ _ .",
    ", . , in : ( ed . ) , ,",
    "volume of _ _ .",
    ", , , , . , in : , , ( eds . ) , , volume of _ _ . p. .",
    ", , , , , , . , in : , , ( eds . ) , , volume of _ _ .",
    ", , , , , . , in : , , ( eds . ) , , volume of _ _ . p. .",
    ", , , , , , , , . .",
    "http://arxiv.org/abs/1407.6463[arxiv:1407.6463 ] .",
    ", , , , , , , , . , in : , , ( eds . ) , , volume of _ _ . p. .",
    ", in : , , ( eds . ) , , volume of _ _ . p. .",
    "joint astronomy centre .",
    ", , , , , , . , in : , , ( eds . ) , , volume of _ _ . p. .",
    ", , , . , in : ( ed . ) , ,",
    "volume   of _ _ .",
    ", in : , , ( eds . ) , , volume of _ _ .",
    ", , . , in : , , ( eds . ) , , p. . , , , . , in : , ( eds . ) , , volume of _ _ . p. .",
    ", w.  s. et  al . , . .",
    ", . , http://arxiv.org/abs/1301.3650[arxiv:1301.3650 ] .",
    ", w.  s. et  al . , . .",
    ", http://arxiv.org/abs/astro-ph/9809122[arxiv:astro-ph/9809122 ] .",
    "jenness , t. et  al .",
    ", in : , , ( eds . ) , , volume of _ _ . p. .",
    ", , , , , , b. . .",
    ", , . , in : ,",
    ", ( eds . ) , , volume of _ _ . p. .",
    ", , . , in : gajadhar , s. et  al .",
    "( eds . ) , , p.  .",
    "http://arxiv.org/abs/1111.5855[arxiv:1111.5855 ] .",
    ", , , , , , , , . , in : , , ( eds . ) , , volume of _ _ . p. .",
    "jenness , t. et  al . ,",
    "c. , in : , ( eds . ) , , volume of _ _ . p. .",
    ", http://arxiv.org/abs/1406.1515[arxiv:1406.1515 ] .",
    ", , . , in : , , ( eds . ) , , volume of _ _ .",
    ", , , , . , in : , ( eds . ) , , volume of _ _ .",
    ", , , , , . , in : , ( eds . ) , , volume of _ _ .",
    "http://arxiv.org/abs/1405.0482[arxiv:1405.0482 ] .",
    ", , , , , . , in : , , ( eds . ) , , volume of _ _ . p. . , s.  k. et  al . , . , in : , ( eds . ) , , volume of _ _ .",
    ", p.  f. et  al .",
    ", . , in : , ( eds . ) , , volume of _ _ .",
    ". , g. et  al . , . ,",
    "in : , ( eds . ) , , volume of _ _ . p. .",
    ", et  al . , . , in : , ( eds . ) , , volume of _ _ . p. . .",
    ", , , , , .",
    ", in : , , ( eds . ) , , volume   of _ _ . p. .",
    ", . , in : ,",
    ", ( eds . ) , , volume   of _ _ . p. .",
    ", c.  g. et  al . , . , in : , ( eds . ) , , volume of _ _ ."
  ],
  "abstract_text": [
    "<S> orac - dr is a general purpose data reduction pipeline system designed to be instrument and observatory agnostic . </S>",
    "<S> the pipeline works with instruments as varied as infrared integral field units , imaging arrays and spectrographs , and sub - millimeter heterodyne arrays & continuum cameras . </S>",
    "<S> this paper describes the architecture of the pipeline system and the implementation of the core infrastructure . </S>",
    "<S> we finish by discussing the lessons learned since the initial deployment of the pipeline system in the late 1990s .    data reduction pipelines , techniques : miscellaneous , methods : data analysis </S>"
  ]
}