{
  "article_text": [
    "in the last few years , some very effective frameworks for image restoration have been proposed that exploit non - locality ( long - distance correlations ) in images , and/or use patches instead of pixels to robustly compare photometric similarities .",
    "the archetype algorithm in this regard is the non - local means ( nlm ) @xcite .",
    "the success of nlm triggered a huge amount of research , leading to state - of - the - art algorithms that exploit non - locality and/or the patch model in specialized ways ; e.g. , see @xcite , to name a few .",
    "we refer the interested reader to @xcite for detailed reviews",
    ". of these , the best performing method till date is perhaps the hybrid bm3d algorithm @xcite , which effectively combines the nlm framework with other classical algorithms .    to setup notations , we recall the working of nlm .",
    "let @xmath3 be some linear indexing of the input noisy image .",
    "the standard setting is that @xmath4 is the corrupted version of some clean image @xmath5 , @xmath6 where @xmath7 is iid @xmath8 .",
    "the goal is to estimate ( approximate ) @xmath9 from the noisy measurement @xmath4 , possibly given a good estimate of the noise floor @xmath10 .",
    "in nlm , the restored image @xmath11 is computed using the simple formula @xmath12 where @xmath13 is some weight ( affinity ) assigned to pixels @xmath14 and @xmath15 . here",
    "@xmath16 is the neighborhood of pixel @xmath14 over which the averaging is performed . to exploit non - local correlations",
    ", @xmath16 is ideally set to the whole image domain . in practice , however , one restricts @xmath16 to a geometric neighborhood , e.g. , to a sufficiently large window of size @xmath17 around @xmath14 @xcite .",
    "the other idea in nlm is to set the weights using image patches centered around each pixel . in particular , for a given pixel @xmath14 , let @xmath18 denote the restriction of @xmath4 to a square window around @xmath14 .",
    "letting @xmath19 be the length of this window , this associates every pixel @xmath14 with a point @xmath18 in @xmath20 ( the patch space ) .",
    "the weights in standard nlm are set to be @xmath21 where @xmath22 is the euclidean distance between @xmath18 and @xmath23 as points in @xmath20 , and @xmath24 is a smoothing parameter .",
    "along with non - locality , it is the use of patches that makes nlm more robust in comparison to pixel - based neighborhood filters @xcite .",
    "recently , it was demonstrated in @xcite that the denoising performance of nlm can be improved ( often substantially for images with sharp edges ) by replacing the @xmath0 regression in nlm with the more robust @xmath1 regression .",
    "more precisely , given weights @xmath13 , note that is equivalent to performing the following regression ( on the patch space ) : @xmath25 and then setting @xmath26 to be the center pixel in @xmath27 .",
    "indeed , this reduces to once we write the regression in terms of the center pixel @xmath26 .",
    "the idea in @xcite was to use @xmath1 regression instead , namely , to compute @xmath28 and then set @xmath26 to be the center pixel in @xmath27 .",
    "note that is a convex optimization , and the minimizer ( the euclidean median ) is unique when @xmath29 @xcite .",
    "the resulting estimator was called the non - local euclidean medians ( nlem ) .",
    "a numerical scheme was proposed in @xcite for computing the euclidean median using a sequence of weighted least - squares .",
    "it was demonstrated that nlem performed consistently better than nlm on a large class of synthetic and natural images , as soon as the noise was above a certain threshold .",
    "more specifically , it was shown that the bulk of the improvement in nlem came from pixels situated close to edges .",
    "an inlier - outlier model of the patch space around an edge was proposed , and the improvement was attributed to the robustness of in the presence of outliers @xcite .    in this paper , we show how a simple extension of the above idea can dramatically improve the denoising performance of nlm , and even that of nlem .",
    "this is the content of section ii . in particular ,",
    "a general optimization and algorithmic framework is provided that includes nlm and nlem as special cases .",
    "some numerical results on synthetic and natural images are provided in section iii to justify our claims .",
    "possible extensions of the present work are discussed in section iv .",
    "it is well - known that @xmath1 minimization is more robust to outliers than @xmath0 minimization .",
    "a simple argument is that the _ unsquared _ residuals @xmath30 in are better guarded against the aberrant data points compared to the squared residuals @xmath31 .",
    "the former tends to better suppress the large residuals that may result from outliers .",
    "this basic principle of robust statistics can be traced back to the works of von neumann , tukey @xcite , and huber @xcite , and lies at the heart of several recent work on the design of robust estimators ; e.g. , see @xcite , and the references therein .",
    "a natural question is what happens if we replace the @xmath1 regression in by @xmath32 regression ?",
    "in general , one could consider the following class of problems : @xmath33 the intuitive idea here is that , by taking smaller values of @xmath34 , we can better suppress the residuals @xmath30 induced by the outliers .",
    "this should make the regression even more robust to outliers , compared to what we get with @xmath35 .",
    "we note that a flip side of setting @xmath36 is that will no longer be convex ( this is essentially because @xmath37 is convex if and only if @xmath38 ) , and it is in general difficult to find the global minimizer of a non - convex functional .",
    "however , we do have a good chance of finding the global optimum if we can initialize the solver close to the global optimum .",
    "the purpose of this note is to numerically demonstrate that , for all sufficiently large @xmath10 , the @xmath39 obtained by solving ( and letting @xmath40 to be the center pixel in @xmath27 ) results in a more robust approximation of @xmath9 as @xmath41 , than what is obtained using nlm .",
    "henceforth , we will refer to as non - local patch regression ( nlpr ) , where @xmath34 is generally allowed to take values in the range @xmath42 $ ] .",
    "the usefulness of the above idea actually stems from the fact that there exists a simple iterative solver for .",
    "in fact , the idea was influenced by the well - known connection between ` sparsity ' and ` robustness ' , particularly the use of @xmath43 minimization for best - basis selection and exact sparse recovery @xcite .",
    "we were particularly motivated by the iteratively reweighted least squares ( irls ) approach of daubechies et al .",
    "@xcite , and a regularized version of irls developed by chartrand for non - convex optimization @xcite .",
    "we will adapt the regularized irls algorithm in @xcite for solving .",
    "the exact working of this iterative solver is as follows .",
    "we use the nlm estimate to initialize the algorithm , that is , we set @xmath44 then , at every iteration @xmath45 , we write @xmath46 in , and use the current estimate to approximate this by @xmath47 .",
    "this gives us the surrogate least - squares problem @xmath48 here @xmath49 is used as a guard against division by zero , and is gradually shrunk to zero as the iteration progresses .",
    "we refer the reader to @xcite for details .",
    "the solution of is explicitly given by @xmath50 where @xmath51 the minimizer of is taken to be the limit of the iterates , assuming that it exists . while we can not provide any guarantee on local convergence at this point , we note that can be expressed as a gradient descent step ( with appropriate step size ) of a smooth surrogate of .",
    "this interpretation leads to the well - known weiszfeld algorithm ( for the special case @xmath35 ) , which is known to converge linearly @xcite .",
    "alternatively , one could adapt more sophisticated irls algorithms ( e.g. , the one in @xcite ) , which come with proven guarantees on local convergence , to the case @xmath52 .",
    "the overall computational complexity of nlpr is @xmath53 per pixel , where @xmath54 is the average number of iterations . for nlm ,",
    "the complexity is @xmath55 per pixel . for a given convergence accuracy",
    ", we have noticed that @xmath54 increases as @xmath34 decreases . in particular ,",
    "a large number of iterations are required in the non - convex regime @xmath56 . in this case , we halt the computation after a sufficiently large number of iterations .    * input * : noisy image @xmath3 , and parameters @xmath57 .",
    "* return * : denoised image @xmath11 .",
    "( 1 ) extract patch @xmath18 of size @xmath58 at every pixel @xmath14 .",
    "( 2 ) for every pixel @xmath14 , do ( a ) set @xmath59 for every @xmath60 .",
    "( b ) sort @xmath61 in non - increasing order .",
    "( c ) let @xmath62 be the re - indexing of @xmath60 as per the above order .",
    "( d ) find patch @xmath63 that minimizes @xmath64 } w_{ij_{t } } \\lvert \\p - \\p_j \\rvert^p$ ] .",
    "( e ) set @xmath26 to be the center pixel in @xmath63 .",
    "we noticed in @xcite that a simple heuristic often provides a remarkable improvement in the performance of nlm . in",
    ", one considers all patches @xmath65 drawn from the geometric neighborhood of pixel @xmath14 .",
    "however , notice that when a patch is close to an edge , then roughly half of its neighboring patches are on one side ( the correct side ) of the edge .",
    "following this observation , we consider only the top @xmath66 of the the neighboring patches that have the largest weights .",
    "that is , the selected patches correspond to the @xmath67$]-nearest neighbors of @xmath18 in the patch space , where @xmath68 . while this tends to inhibit the diffusion at low noise levels ( in smooth regions ) , it was demonstrated in @xcite that it can significantly improve the robustness of nlm and nlem at large @xmath10 .",
    "we will also use this heuristic in nlpr .",
    "the overall scheme is summarized in algorithm [ algo1 ] .",
    "we use @xmath16 to denote a window of size @xmath17 centered at pixel @xmath14 in the algorithm .",
    "to understand the denoising performance of nlpr , we provide some limited results on synthetic and natural images",
    ". the main theme of our investigation would be to understand how the performance of nlpr changes with the regression index @xmath34 . for a quantitative comparison of the denoising results",
    ", we will use the standard peak - signal - to - noise ratio ( psnr ) . for an @xmath69-pixel image , with intensity",
    "scaled to @xmath70 $ ] , this is defined to be @xmath71 , where @xmath72 .",
    "we first consider the test image of _ checker _ used in @xcite .",
    "this serves as a good model for simultaneously testing the denoising quality in smooth regions and in the vicinity of edges .",
    "we corrupt _ checker _ as per the noise model in .",
    "we then compute the denoised image using algorithm [ algo1 ] , with the exception that we skip steps ( b ) and ( c ) , that is , we use the full neighborhood @xmath16 .",
    "we initialize the iterations of the irls solver using .",
    "for all the experiments in this paper , we fix the parameters to be @xmath73 and @xmath74 .",
    "these are the settings originally proposed in @xcite .",
    "the results obtained using these settings are not necessarily optimal , and other settings could have been used as well . the point is to fix all the parameters in algorithm [ algo1 ] , except @xmath34 .",
    "this means that the same @xmath13 are used for different @xmath34 .",
    "we now run the above denoising experiment for @xmath75 , and for @xmath76 .",
    "the results are shown in figure [ psnr_sigma_p ] .",
    "we notice that , beyond a certain noise level , nlpr performs better when @xmath34 is close to zero .",
    "in fact , the psnr increases gradually from @xmath77 to @xmath78 , for a fixed @xmath10 . at lower noise levels , the situation reverses completely , and nlpr tends to perform better around @xmath77 .",
    "a possible explanation is that the true neighbors in patch space are well identified at low noise levels , and since the noise is gaussian , @xmath0 regression gives statistically optimal results .",
    "an analysis of the above results shows us that , as @xmath41 , the bulk of the improvement comes from pixels situated in the vicinity of edges .",
    "a similar observation was also made in @xcite for nlem . to understand this better , we recall the ideal @xmath79-@xmath80 edge model used in @xcite .",
    "this is shown in figure [ em1 ] .",
    "we add noise of strength @xmath81 to the edge , and denoise it using nlpr .",
    "we examine the regression at a reference point situated just right to the edge ( cf .",
    "figure [ em2 ] ) .",
    "the patch space at this point is specified using @xmath82 and @xmath83 .",
    "the distribution of patches is shown in figure [ in - out ] .",
    "note that the patches are clustered around the centers @xmath84 and @xmath85 .",
    "for the reference point , the points around @xmath86 are the outliers , while the ones around @xmath87 are the inliers .",
    "we now perform @xmath88 regression on this distribution for @xmath89 and @xmath90 .",
    "the results obtained ( algorithm [ algo1 ] , steps ( b ) and ( c ) skipped ) from a single noise realization are shown in figure [ in - out ] .",
    "the exact values of the estimate in this case are @xmath91 ( @xmath77 ) , @xmath92 ( @xmath35 ) , and @xmath93 ( @xmath94 ) .",
    "the average estimate over @xmath95 noise realizations are @xmath96 ( @xmath77 ) , @xmath97 ( @xmath35 ) , and @xmath98 ( @xmath78 ) .",
    "we note that the working of the irls algorithm provides some insight into the robustness of @xmath88 regression .",
    "note that when @xmath77 ( nlm ) , the reconstruction in is linear ; the contribution of each noisy patch @xmath23 is controlled by the corresponding weight @xmath13 . on the other hand",
    ", the reconstruction is non - linear when @xmath99 .",
    "the contribution of each @xmath23 is controlled not only by the respective weights , but also by the multipliers @xmath100 .",
    "in particular , the limiting value of the multipliers dictate the contribution of each @xmath23 in the final reconstruction .",
    "figure gives the distribution of the sorted multipliers ( at convergence ) for the experiment described above . in this case",
    ", the large multipliers correspond to the inliers , and the small multipliers correspond to the outliers .",
    "notice that when @xmath101 , the tail part of the multipliers ( outliers ) has much smaller values ( close to zero ) compared to the leading part ( inliers ) .",
    "in a sense , the iterative algorithm gradually ` learns ' the outliers from the patch distribution as the iteration progresses , which are finally taken out of estimation .",
    ".comparison of nlm and nlpr ( @xmath78 ) at noise levels @xmath75 ( results averaged over @xmath95 noise realizations ) [ cols=\"<,^ , > , > , > , > , > , > , > , > , > , > \" , ]      we compare the psnrs obtained using nlpr ( @xmath78 ) with that of nlm for some standard natural images in table [ table1 ] . we notice that , for each of the images , nlpr consistently outperforms nlm at large noise levels .",
    "the gain in psnr is often as large as @xmath90 db . the results obtained for _ barbara _ using nlm and nlpr are compared in figure [ barbararesults ] .",
    "note that , as expected , robust regression provides a much better restoration of the sharp edges in the image than nlm .",
    "what is probably surprising is that the restoration is superior even in the textured regions .",
    "note , however , that nlm tends to perform better in the smooth regions .",
    "for example , we some more noise grains in the smooth regions in figure [ nlpr ] compared that in figure [ nlm ] .",
    "this suggests that an ` adaptive ' optimization framework , which combines @xmath0 regression ( in smooth regions ) and @xmath102 regression ( in the vicinity of edges ) , might possibly perform better than a fixed @xmath88 regression .",
    "some other possible extensions of the present work are as follows : ( i ) local convergence analysis of the present irls algorithm , and ways of improving it ; ( ii ) possibility of using more efficient numerical algorithms for solving ; ( iii ) finding better ways of estimating the denoised pixel @xmath26 from the estimated patch @xmath27 ( the projection method used here is probably the simplest ) ; ( iv ) use of ` better ' weights than the ones used in standard nlm @xcite ; and ( v ) formulation of a ` joint ' optimization framework for , where the optimization is performed with respect to @xmath13 and @xmath63 @xcite .",
    "m. aharon , m. elad , a. bruckstein , `` k - svd : an algorithm for designing overcomplete dictionaries for sparse representation , '' ieee transactions on signal processing , vol .",
    "4311 - 4322 , 2006 .",
    "i. daubechies , r. devore , m. fornasier , c. s. gunturk `` iteratively reweighted least squares minimization for sparse recovery , '' communications on pure and applied mathematics , vol .",
    "63 , pp . 1 - 38 , 2009"
  ],
  "abstract_text": [
    "<S> it was recently demonstrated in @xcite that the denoising performance of non - local means ( nlm ) can be improved at large noise levels by replacing the mean by the robust euclidean median . </S>",
    "<S> numerical experiments on synthetic and natural images showed that the latter consistently performed better than nlm beyond a certain noise level , and significantly so for images with sharp edges . </S>",
    "<S> the euclidean mean and median can be put into a common regression ( on the patch space ) framework , in which the @xmath0 norm of the residuals is considered in the former , while the @xmath1 norm is considered in the latter . </S>",
    "<S> the natural question then is what happens if we consider @xmath2 regression ? </S>",
    "<S> we investigate this possibility in this paper .    </S>",
    "<S> image denoising , non - local means , non - local euclidean medians , edges , inlier - outlier model , robustness , sparsity , non - convex optimization , iteratively reweighted least squares . </S>"
  ]
}