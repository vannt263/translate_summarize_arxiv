{
  "article_text": [
    "[ index : chaptertitle ]      the performance assessment of ( numerical ) optimization algorithms with the https://github.com/numbbo/coco[coco ] platform [ index : id1 ] is invariably based on the measurement of the _ runtime _ until a _ quality indicator _ reaches a predefined _ target value_. on each problem instance , several target values are defined and for each target value a runtime is measured ( or no runtime value is available if the indicator does not reach the target value ) [ index : id3 ] . in the single - objective , noise - free case , the assessed quality indicator is , at each given time step , the function value of the best solution the algorithm has obtained ( evaluated or recommended , see [ index : id4 ] ) before or at this time step .    in the bi- and multi - objective case ,",
    "e.g. on the biobjective test suite [ index : id5 ] , the assessed quality indicator at the given time step is a hypervolume indicator computed from _ all _ solutions obtained ( evaluated or recommended ) before or at this time step .",
    "_ function instance , problem _ : :    in the case of the bi - objective performance assessment within    https://github.com/numbbo/coco[coco ] , a problem is a 5-tuple of    +    * a _ parameterized function _",
    "@xmath0 , mapping the    decision variables of a solution @xmath1 to its    objective vector @xmath2    with @xmath3 and    @xmath4 being    parameterized ( single - objective ) functions themselves    * its concrete parameter value @xmath5    determining the so - called _ function instance _",
    "@xmath6 ,    * the _ problem dimension _ @xmath7 ,    * an underlying quality indicator @xmath8 , mapping a set of    solutions to its quality , and    * a _ target value",
    "_ @xmath9 of the underlying    quality indicator , see below for details .",
    "+    we call a problem _ solved _ by an optimization algorithm if the    algorithm reaches a quality indicator value at least as good as the    associated target value .",
    "the number of function evaluations needed to    surpass the target value for the first time is    https://github.com/numbbo/coco[coco]s central performance measure .",
    "[ index : id7 ] in case a single quality indicator is used for all    problems in a benchmark suite , we can drop the quality indicator and    refer to a problem as a quadruple    @xmath10 .",
    "note that typically    more than one problem for a _ function instance _ of    @xmath11 is defined by choosing more than one    target value .",
    "_ pareto set _ , _ pareto front _ , and _ pareto dominance _ : :    for a function instance , i.e. , a function    @xmath12 with given parameter value    @xmath13 and dimension @xmath7 , the pareto set is    the set of all ( pareto - optimal ) solutions for which no solutions in    the search space @xmath14 exist that    have either an improved @xmath15 or an improved    @xmath16 value while the other value is at least as good    ( or in other words , a _",
    "pareto - optimal _ solution in the pareto set has    no other solution that _ dominates _ it ) .",
    "the image of the pareto set in    the _ objective space _ is called the pareto front .",
    "we generalize the    standard pareto dominance relation to sets by saying solution set    @xmath17 dominates solution set    @xmath18 if and only if for all    @xmath19 there is at least one solution    @xmath20 that dominates it .",
    "_ ideal point _ : :    the ideal point ( in objective space ) is defined as the vector in    objective space that contains the optimal function value for each    objective _ independently _ , i.e. for the above concrete function    instance , the ideal point is given by    @xmath21 .",
    "_ nadir point _ : :    the nadir point ( in objective space ) consists in each objective of the    worst value obtained by any pareto - optimal solution .",
    "more precisely ,    if @xmath22 denotes the pareto set , the nadir point    satisfies    @xmath23 . _ archive _ : :    an external archive or simply an archive is the set of non - dominated    solutions , obtained over an algorithm run . at each point",
    "@xmath24 in time ( that is after @xmath24 function    evaluations ) , we consider the set of all mutually non - dominating    solutions that have been evaluated so far .",
    "we denote the archive after    @xmath24 function evaluations as @xmath25 and use it    to define the performance of the algorithm in terms of a ( quality )    indicator function    @xmath26 that might    depend on a problem s underlying parameterized function and its    dimension and instance .      for measuring the runtime on a given problem",
    ", we consider a quality indicator which is to be optimized ( minimized ) . in the noiseless single - objective case ,",
    "the quality indicator is the best so - far observed objective function value ( recommendations can replace previous observations ) . in the case of the test suite ,",
    "the quality indicator is based on the hypervolume indicator of the _ archive _ @xmath25 .",
    "the indicator @xmath27 to be mininized is either the negative hypervolume indicator of the archive with the nadir point as reference point or the distance to the region of interest @xmath28 $ ] after a normalization of the objective space : @xmath29 ) & \\text{if $ a_t$ dominates } \\{z_{\\text{nadir}}\\}\\\\      dist(a_t , [ z_{\\text{ideal } } , z_{\\text{nadir } } ] ) & \\text{otherwise }      \\end{array }      \\right.\\enspace .\\ ] ] where @xmath30\\times\\left[\\frac{f_\\beta(a)-z_{\\text{ideal } , \\beta}}{z_{\\text{nadir } , \\beta}-z_{\\text{ideal } , \\beta } } , 1\\right]\\right)\\ ] ] is the ( normalized ) hypervolume of archive @xmath25 with respect to the nadir point @xmath31 as reference point and where ( with division understood to be element - wise , hadamard division ) , @xmath32 ) = \\inf_{a\\in a_t , z\\in [ z_{\\text{ideal } } , z_{\\text{nadir } } ] } \\left\\|\\frac{f(a)-z}{z_{\\text{nadir}}-z_{\\text{ideal}}}\\right\\|\\ ] ] is the smallest ( normalized ) euclidean distance between a solution in the archive and the region of interest , see also the figures below for an illustration .              _ why using an archive ?",
    "_ : :    we believe using an archive to keep all non - dominated solutions is    relevant in practice in bi - objective real - world applications , in    particular when function evaluations are expensive . using an external    archive for the performance assessment",
    "has the additional advantage    that no populuation size needs to be prescribed and algorithms with    different or even changing population sizes can be easily compared . _",
    "why hypervolume ?",
    "_ : :    although , in principle , other quality indicators can be used in    replacement of the hypervolume , the monotonicity of the hypervolume is    a strong theoretical argument for using it in the performance    assessment : the hypervolume indicator value of the archive improves if    and only if a new non - dominated solution is generated [ index : id10 ] .",
    "* algorithm performance is measured via runtime until the quality of the archive of non - dominated solutions found so far surpasses a target value . *",
    "to compute the quality indicator , the objective space is normalized .",
    "the region of interest ( roi ) @xmath28 $ ] , defined by the ideal and nadir point , is mapped to @xmath33 ^ 2 $ ] . *",
    "if the nadir point is dominated by at least one point in the archive , the quality is computed as the negative hypervolume of the archive using the nadir point as hypervolume reference point .",
    "* if the nadir point is not dominated by the archive , the quality equals the distance of the archive to the roi .      *",
    "the quality indicator value of an archive that contains the nadir point as non - dominated point is @xmath34 . *",
    "the quality indicator value is bounded from below by @xmath35 , which is the quality of an archive that contains the ideal point , and * because the quality of an archive is used as performance criterion , no population size has to be prescribed to the algorithm .",
    "in particular , steady - state and generational algorithms can be compared directly as well as algorithms with varying population size and algorithms which carry along their external archive themselves .",
    "for each problem instance of the benchmark suite , consisting of a parameterized function , its dimension and its instance parameter @xmath36 , a set of quality indicator target values is chosen , eventually used to measure algorithm runtime to reach each of these targets .",
    "the target values are based on a target precision @xmath37 and a _ reference hypervolume indicator value _ , @xmath38 , which is an approximation of the @xmath27 indicator value of the pareto set .",
    "all target indicator values are computed in the form of @xmath38 @xmath39 from the instance dependent reference value @xmath38 and a target precision value @xmath37 . for the test suite , 58 target precisions @xmath37",
    "are chosen , identical for all problem instances , as @xmath40 negative target precisions are used because the reference indicator value , as defined in the next section , can be surpassed by an optimization algorithm . the runtimes to reach these target values are presented as empirical cumulative distribution function , ecdf [ index : id12 ] .",
    "runtimes to reach specific target precisions are presented as well .",
    "it is not uncommon however that the quality indicator value of the algorithm never surpasses some of these target values , which leads to missing runtime measurements .      unlike the single - objective test suite [ index : id15 ] ,",
    "the biobjective test suite does not provide analytic expressions of its optima . except for @xmath41 ,",
    "the pareto set and the pareto front are unknown .    instead of the unknown hypervolume of the true pareto set",
    ", we use the hypervolume of an approximation of the pareto set as reference hypervolume indicator value @xmath38 . to obtain the approximation ,",
    "several multi - objective optimization algorithms have been run and all non - dominated solutions over all runs have been recorded .",
    "the hypervolume indicator value of the obtained set of non - dominated solutions , also called _ non - dominated reference set _ , separately obtained for each problem instance in the benchmark suite , is then used as the reference hypervolume indicator value .      the standard procedure for an experiment on a benchmark suite , like the suite , prescribes to run the algorithm of choice once on each problem of the suite [ index : id27 ] . for the suite ,",
    "the postprocessing part of https://github.com/numbbo/coco[coco ] displays currently by default only 5 out of the 10 instances from each function - dimension pair .      having a good approximation of the pareto set / pareto front",
    "is crucial in assessing algorithm performance with the above suggested performance criterion . in order to allow",
    "the reference sets to approximate the pareto set / pareto front better and better over time , the https://github.com/numbbo/coco[coco ] platform records every non - dominated solution over the algorithm run .",
    "algorithm data sets , submitted through the https://github.com/numbbo/coco[coco ] platform s web page , can therefore be used to improve the quality of the reference set by adding all solutions to the reference set which are currently non - dominated to it .",
    "recording every new non - dominated solution within every algorithm run also allows to recover the algorithm runs after the experiment and to recalculate the corresponding hypervolume difference values if the reference set changes in the future . in order to be able to distinguish between different collections of reference sets that might have been used during the actual benchmarking experiment and",
    "the production of the graphical output , https://github.com/numbbo/coco[coco ] writes the absolute hypervolume reference values together with the performance data during the benchmarking experiment and displays a version number in the plots generated that allows to retrieve the used reference values from the https://github.com/numbbo/coco[github repository of coco ] .",
    "the authors would like to thank thanh - do tran for his contributions and assistance with the preliminary code of the bi - objective setting and for providing us with his extensive experimental data .",
    "we also thank tobias glasmachers , oswin krause , and ilya loshchilov for their bug reports , feature requests , code testing , and many valuable discussions .",
    "special thanks go to olaf mersmann for the inital rewriting of the coco platform without which the bi - objective extension of coco would not have happened ."
  ],
  "abstract_text": [
    "<S> this document details the rationales behind assessing the performance of numerical black - box optimizers on multi - objective problems within the https://github.com/numbbo/coco[coco ] platform and in particular on the biobjective test suite http://numbbo.github.io/coco-doc/bbob-biobj/functions [ ] . </S>",
    "<S> the evaluation is based on a hypervolume of all non - dominated solutions in the archive of candidate solutions and measures the runtime until the hypervolume value succeeds prescribed target values .    </S>",
    "<S> [ index::doc ] </S>"
  ]
}