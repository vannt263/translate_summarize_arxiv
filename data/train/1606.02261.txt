{
  "article_text": [
    "in many machine learning ( ml ) applications , one needs to compute the expected value of a quantity . in continuous spaces",
    ", this means estimating the value of an integral of the form @xmath3 = { { \\hat{f}}}= \\int f(x)p(x)\\,dx\\ ] ] where @xmath0 is the function of interest , and @xmath4 is the probability of the input @xmath2 . often in ml",
    "contexts this integral is not known analytically , and so must be estimated from a finite set of samples of the function  which requires an estimator algorithm for mapping that set to an estimate of the integral .",
    "when the samples are generated deterministically  as in quadrature  the quality of the estimator is just its error .",
    "when the samples are instead generated randomly  as in monte carlo ( mc ) algorithms  the quality of the estimator can be quantified as its expected squared error .",
    "one way to reduce the error of an estimator based on mc samples is by exploiting some side information .",
    "an important example of such information is a `` control variate '' , which is a function with known mean whose behavior is correlated with @xmath0 .",
    "the difference between the sample mean of the control variate and its actual mean provides information about the difference between the sample mean of @xmath1 and _ its _ true mean  information which can then be exploited to correct that sample mean to provide a better estimate of the integral .",
    "unfortunately , most problems of interest do not have a natural control variate to perform such regularization .",
    "the core idea of the stacked monte carlo ( stackmc ) algorithm @xcite is to _ construct _ a control variate by training a supervised learning algorithm on the available data samples .",
    "the supervised learning algorithm is chosen such that the expected value of the fit can be found analytically ( or cheaply through sampling ) .",
    "then , the fit is evaluated on held - out data samples , and the discovered performance is used to estimate the quality of the control variate .",
    "the original presentation of stackmc tested the algorithm under simple sampling , finding stackmc has expected error at least as low as the better of mc and the fitting algorithm alone .    here",
    "we provide a deeper theoretical understanding of stackmc and extend stackmc to more sophisticated mc techniques .",
    "first , we review the original presentation of stackmc and compare the estimation procedure with other algorithms .",
    "we examine some of the implicit assumptions , and find an improved estimator for the quality of the fit .",
    "we then present modifications that extend the algorithm to new regimes of interest , specifically when samples are generated from quasi - monte carlo , when samples are generated from importance sampling , and finally when the input domain is discrete instead of continuous .",
    "we find that with the appropriate modifications , the estimate under stackmc has error at least as low as the monte carlo estimate in almost all cases , and in most cases the stackmc error is also at least as low as the fitting algorithm used alone .",
    "the estimation error of an arbitrary estimator , @xmath5 $ ] , can be decomposed as @xmath6 , where @xmath7 is the variance of the estimator , @xmath8)^2]$ ] , and @xmath9 is the bias of the estimator , @xmath10 $ ] .",
    "an estimation algorithm that has small expected squared error is one that maintains both a low bias and a low variance .",
    "the original presentation of stackmc @xcite imagines that there exists a function @xmath11 that is correlated with @xmath1 and has @xmath12 as its known mean .",
    "this function is combined into , @xmath13 where @xmath14 is a constant .",
    "the function samples and corresponding @xmath11 values are used to estimate , @xmath15 at first glance it seems nothing has been accomplished since we have merely replaced one integral equation with another .",
    "however , since the control variate @xmath11 is correlated with @xmath1 by assumption , if @xmath14 is chosen properly then @xmath16 has lower variance than @xmath1 .",
    "the estimation error of monte carlo is directly proportional to the variance of the function whose mean is being estimated .",
    "this new integral has lower variance , and thus lower estimation error .",
    "in fact , the stronger the correlation of @xmath11 is correlated with @xmath1 , the more accurate the mc estimate of @xmath17 .",
    "the difficulty , of course , is how to find @xmath11 in the first place .",
    "one natural idea is to construct it directly from the pairs @xmath18 using a supervised learning algorithm that produces fits whose value of @xmath12 can be found through analytic integration or sampling . unfortunately this approach is subject to over - fitting , which typically introduces very large bias into the resultant integral estimator . to overcome this over - fitting problem ,",
    "it is natural to consider using the out - of - sample behavior of the supervised learning algorithm on the pairs @xmath18 to correct the integral estimate .",
    "sub - sampling techniques , such as cross - validation , frequently use out of sample behavior to decide upon a single best fit . in context , this would be choosing a single best @xmath11 to use in .",
    "however , just as one might expect a combination of features to produce a better fit than a single feature , one may also expect that a combination of fits to in - sample data would provide a better estimate than any single one of them .",
    "this idea of `` blending '' fits is the underlying idea of stacking  @xcite .",
    "stacking has a long history of success , being applied to regression @xcite , classification  @xcite probability density estimation @xcite , confidence interval estimation @xcite , and optimization @xcite .",
    "stackmc uses the predictions at out - of - sample locations to find the value of @xmath14 and set @xmath19 .",
    "in particular , stackmc partitions the data into @xmath20 sets ( as in @xmath20-fold validation ) .",
    "each of these sets is the held - out data for a single fold , and the corresponding held - in set is the complement set to the held - in set .",
    "the value of @xmath19 is taken to be the value predicted at @xmath21 when @xmath21 is in the held - out data ( with @xmath20-fold ensuring every data location is held - out exactly once ) .",
    "stackmc then uses these held - out predictions and their corresponding true function values @xmath22 to estimate @xmath14 using the equation from control variates , @xmath23 .",
    "finally , the expected value for each fold is corrected using the held - out samples , and @xmath24 is estimated as the average of the fold expected values , where @xmath25 is the number of held - out points in fold @xmath26 . @xmath27 } } \\quad \\text{.}\\ ] ]      there are a multitude of different monte carlo methods , each with its own way of reducing variance . for the purposes of this work ,",
    "there are two broad classes of mc algorithms : those where sampling from @xmath28 is easy but evaluating @xmath1 is difficult , and those where sampling from @xmath28 directly is difficult or impossible .",
    "this later regime contains the metropolis - hastings algorithm @xcite and its many variants such as gibbs sampling and hamiltonian monte carlo @xcite .",
    "regularizing @xmath1 with @xmath11 does not help if it is @xmath28 which is hard to acquire ( for example , @xmath12 could not be found ) , and combining stackmc with these methods is outside the scope of this work",
    ".    when @xmath1 is expensive to evaluate , one wants to be judicious in the number of samples required .",
    "many techniques seek to be smarter in how samples are allocated .",
    "for example , importance sampling @xcite generates samples from an alternate distribution @xmath29 to bias the samples into `` important '' regions .",
    "cross entropy methods @xcite are the adaptive version of importance sampling that try to actively modify @xmath29 to be close to the ideal importance sampling distribution .",
    "quasi - monte carlo methods seek to distribute samples evenly , while others use bayesian priors @xcite to intelligently place sample locations .",
    "in contrast to all of these techniques , stackmc is a post - processing method .",
    "stackmc does not change how the samples are generated , and so it can , at least in theory , be combined with any of the sampling methods mentioned above .",
    "in fact , several such combinations are demonstrated in this work .",
    "stackmc is most closely related to recent developments in control functionals @xcite , though these works focus on constructing good kernel - based fits .",
    "above we described stackmc as it was initially presented , and contrasted it with other variance reduction methods .",
    "this section first provides greater detail on the estimation error of stackmc , focusing on the estimation of @xmath14 .",
    "the section then presents modifications to the stackmc algorithm to apply it in sampling regimes beyond simple monte carlo . accompanying this analysis are computational results demonstrating variance reduction .",
    "these result plots depict the expected squared error for the stackmc algorithm being tested , for the monte carlo sampler , and for the fit alone .",
    "the squared error for the fit alone is computed by training the supervised learning algorithm on all of the available data samples and estimating @xmath24 as @xmath12 .",
    "the error bars are the error in the mean .      as described in sec .",
    "[ sec : smcback ] , stackmc estimates the value of @xmath24 as an average of the predictions from the held - out folds . using , it can be seen that the expected squared error of this estimator is @xmath30 & = e \\left [ \\left ( \\frac{1}{k } \\sum_k \\alpha { { \\hat{g}}}_k + \\frac{1}{n } \\sum_i f_i   - \\alpha g_i - { { \\hat{f}}}\\right)^2\\right]\\end{aligned}\\ ] ] where the expectation is over datasets , @xmath31 is the expected value from the held - in data in fold @xmath26 , and @xmath32 is the prediction made by the fitter to the fold for which @xmath21 is in the held - out dataset .",
    "the sum over @xmath33 and @xmath26 simplifies to the single sum over @xmath34 above under @xmath20-fold validation since @xmath25 is constant across @xmath26 and each @xmath21 is held - out exactly once . the above",
    "can also be written as @xmath35 = e\\left [ \\left ( \\left ( \\frac{1}{n } \\sum_i f_i - { { \\hat{f}}}\\right ) - \\alpha \\left ( \\frac{1}{n } \\sum_i g_i - \\frac{1}{k } { { \\hat{g}}}\\right ) \\right)^2 \\right]\\ ] ] for simplicity , make the following substitutions : @xmath36 , @xmath37 and @xmath38 for @xmath39 . @xmath40 = e \\left [ ( { { \\tilde{f}}}- { { \\hat{f}}})^2 - 2 \\alpha ( { { \\tilde{g}}}- { { \\hat{g}}})({{\\tilde{f}}}-{{\\hat{f } } } ) + \\alpha^2 ( { { \\tilde{g}}}- { { \\hat{g}}})^2   \\right ] \\\\ \\label{eq : smcprealpha } & = e \\left [ ( { { \\tilde{f}}}- { { \\hat{f}}})^2 \\right ]   -2 \\alpha e\\left [ \\left({{\\tilde{g}}}- { { \\hat{g}}}\\right ) \\left({{\\tilde{f}}}-{{\\hat{f}}}\\right ) \\right ] + \\alpha^2 e\\left [ \\left({{\\tilde{g}}}- { { \\hat{g}}}\\right)^2 \\right]\\end{aligned}\\ ] ] the first term in , @xmath41 $ ] , is the error under the sampling procedure . if @xmath14 is chosen such that @xmath42   > \\alpha^2 e\\left [ \\left({{\\tilde{g}}}- { { \\hat{g}}}\\right)^2 \\right]\\ ] ] then the error of this estimation procedure is lower than the original monte carlo estimate .",
    "this illuminates the importance of choosing @xmath14 properly .",
    "it is not the case that setting @xmath14 to a fixed constant , say @xmath43 , necessarily leads to variance reduction .",
    "in fact , a value of @xmath14 that is too large will actually _ increase _ the variance of the estimator .",
    "the value for @xmath14 which minimizes the variance is found by taking the derivative with respect to alpha and setting it to zero .",
    "the second derivative is always positive , so this estimator for @xmath14 is the unique global minimizer of @xmath14 : @xmath44}{e\\left [ \\left({{\\tilde{g}}}- { { \\hat{g}}}\\right)^2 \\right ] } = \\frac{cov({{\\tilde{g}}}- { { \\hat{g } } } , { { \\tilde{f } } } ) + b_g b_f}{var({{\\tilde{g}}}- { { \\hat{g } } } ) + { b_g}^2}\\end{aligned}\\ ] ] where @xmath45 $ ] and @xmath46 $ ] .",
    "this optimal value for @xmath14 can not be estimated from the samples , as there is only data from 1 @xmath20-fold partition . while one could make several @xmath20-fold partitions ,",
    "the estimate of @xmath38 is the same for all of them .",
    "however , one can approximate this equation by using the per - fold information by taking @xmath12 as simply the expected value for the fold , and estimate @xmath47 and @xmath38 using only the held - in samples .",
    "furthermore , it is impossible to directly estimate @xmath48 since of course @xmath24 is unknown , but many sampling distributions are known to be unbiased , and so @xmath49 . with these approximations ,",
    "the optimal estimator for @xmath14 becomes @xmath50    this equation for @xmath51 is , in general , different than @xmath52 , the estimator used in @xcite .",
    "these two estimates become the same when : 1 ) @xmath53 ( leave - one - out ) , 2 ) @xmath54 , and 3 ) @xmath12 is constant .",
    "the first assumption is reasonable , as it gives @xmath55 data points with which to compute @xmath14 instead of only @xmath20 .",
    "these latter assumptions characterize the difference between the two estimators . if the held - out samples are not an unbiased estimator of the mean of the fit , then @xmath56 .",
    "( this case is discussed further below . )",
    "when @xmath54 , this third assumption is similar to the statement @xmath57 , i.e. , the covariation of the fit sample mean is much larger than the covariation of the mean of the fit .",
    "this is a reasonable assumption when the number of samples is large , since the fit should stabilize as the number of samples grows large .",
    "however , if the sample size is small , this assumption may not hold . performance could be improved by using instead of the original estimator , or perhaps with the additional regularization of setting @xmath54 if it is known to be true .",
    "this equation for @xmath14 can be inserted back into to find the expected error reduction given optimal estimation for @xmath14 . performing this substitution one finds @xmath58 & = e[({{\\tilde{f}}}- { { \\hat{f}}})^2 ] -   \\frac{e \\left [ \\left ( { { \\tilde{g}}}- { { \\hat{g}}}\\right ) \\left ( { { \\tilde{f}}}- { { \\hat{f}}}\\right ) \\right]^2}{e \\left [ \\left ( { { \\tilde{g}}}- { { \\hat{g}}}\\right)^2 \\right ] } \\ ] ] having the optimal value of @xmath14 guarantees a reduction in expected squared error , as the monte carlo error is reduced by a strictly positive term .",
    "this equation is similar to the standard control variates result , but , again , differs in that the individual estimators @xmath38 and @xmath47 may be biased , and @xmath12 may fluctuate .",
    "if @xmath59 , and @xmath60 , then the standard control variates formula is recovered : @xmath35   = \\left ( 1 - \\rho^2   \\right ) var({{\\tilde{f}}})\\ ] ] where @xmath61 is the correlation between @xmath47 and @xmath38 .",
    "this highlights that a good fitting algorithm will produce a high correlation between @xmath38 and @xmath47 , while keeping the variation in the mean across fits small .",
    "as mentioned , this new estimate of @xmath51 is likely to be better when there is large variation of @xmath12 across the folds .",
    "this is likely to happen when the sample size is only slightly larger than the number of free parameters .",
    "we test this hypothesis by constructing a simple example .",
    "the function of interest is a simple quadratic , @xmath62 , and the sampling distribution is uniform over the unit interval .",
    "the fitting algorithm is a linear fit to the held - in data samples .",
    "the results of this test case are shown in fig.[fig : updatedalpha ] , and they confirm the hypothesis . at very small numbers of sample points ,",
    "there is significant improvement by including the fluctuations of @xmath12 in the estimate of @xmath14 .",
    "as the number of samples grows , this effect becomes negligible , and the two estimators have equivalent performance .",
    "frequently , one may consider multiple different fitting algorithms to train on the data . instead of choosing among them",
    ", they may all be used together to jointly reduce the estimation error .",
    "given a set of supervised learning algorithms , can be written as @xmath63 which introduces some number of control variates , @xmath64 .",
    "similar to the analysis above , the expected error of this estimator is @xmath65 - 2 \\sum_i \\alpha_i   e[({{\\tilde{g}}}_i - { { \\hat{g}}}_i)({{\\tilde{f}}}- { { \\hat{f } } } ) ] + \\sum_i \\sum_j \\alpha_i \\alpha_j e[({{\\tilde{g}}}_i - { { \\hat{g}}}_i)({{\\tilde{g}}}_j - { { \\hat{g}}}_j)]\\end{aligned}\\ ] ] let @xmath66 $ ] and @xmath67 $ ] , so that becomes @xmath68 = e[({{\\tilde{f}}}-{{\\hat{f}}})^2 ] - 2 u^t \\boldsymbol{\\alpha } + \\boldsymbol{\\alpha}^t w \\boldsymbol{\\alpha}\\end{aligned}\\ ] ] this is quadratic in @xmath69 , with minimizer @xmath70 .",
    "the estimation error using all of the fits is @xmath68 & =   e[({{\\tilde{f}}}-{{\\hat{f}}})^2 ]   - 2 u^t w^{-1 } u + ( w^{-1 } u)^t w ( w^{-1 } u ) \\\\ & = e[({{\\tilde{f}}}-{{\\hat{f}}})^2 ] - u^t w^{-1 } u\\end{aligned}\\ ] ]    the total error reduction from the joint set of fitting algorithms depends on how correlated the fits are with one another and the true function samples .",
    "consider the case where all fitters have equal covariance with the function samples , @xmath71 , and equal variances , @xmath72 .",
    "further , assume the off - diagonal terms are represented by @xmath73 . in the ideal case ,",
    "the fitters are uncorrelated , @xmath74 and the total variance reduction is @xmath75 . on the other hand ,",
    "if all of the fitters are perfectly correlated with one another , @xmath76 , and the error reduction is just @xmath77 , i.e. , there is no extra variance reduction from incorporating multiple fits .",
    "this shows that a set of fitting algorithms should be sought that each have strong coupling with the true function but weak coupling amongst themselves . in theory , adding a new fitting algorithm should always be beneficial ( or at least not harmful ) , though in practice errors in the estimation of @xmath14 could lead to degraded performance .      as a test for multiple fitting algorithms",
    ", we chose the rosenbrock function @xmath78 $ ] .",
    "the rosenbrock is widely used as a benchmark problem in optimization . in addition",
    ", it has features representative of many engineering problems , in that there is a shallow region of good performance with interesting structure , and performance degrades significantly outside that region .",
    "the uncertainty was assumed to be a uniform distribution over the [ -3,3 ] hypercube .",
    "two different supervised learning algorithms were used .",
    "first , a third - order polynomial with no cross terms , i.e. @xmath79 , and a fourier fitting algorithm @xmath80 . for both algorithms ,",
    "the @xmath81 parameters of this polynomial are trained using least squares .",
    "the data is in a 10-d space , and the data is partitioned into @xmath82-folds .",
    "the estimation error as a function of number of samples can be seen in fig.[fig : rosenunifpolyfourier10d ] .    for small sample sizes ,",
    "the fitting algorithms do quite poorly and the stackmc error equals the mc error . for intermediate sample sizes , the polynomial fit outperforms the other two , but its error is matched by stackmc . finally , at large sample sizes , the fourier fit is the best performer , and its error is also matched by stackmc .",
    "for all sample sizes , the expected squared error of stackmc is at least as low as the best individually performing algorithm .",
    "while it is true that stackmc does not decrease the error for any specific sample size , the best error is obtained without having to choose the best performing algorithm in advance .",
    "quasi - monte carlo techniques , such as latin - hypercube sampling and the halton sequence @xcite , seek to reduce the variance of monte carlo by explicitly spreading the sample locations evenly throughout the space .",
    "this sampling procedure is effective at reducing variance , but it also causes all of the data samples to be correlated with one another . in particular , imagine a particular fold @xmath11 trained on a specific set of held - in data",
    ". under iid sampling , the held - out samples still have a uniform distribution over the domain . however , under latin - hypercube , conditioned on knowing the held - in samples , the held - out samples have zero probability of being in large regions of the domain .",
    "these anti - correlations cause @xmath14 to be estimated poorly , and thus cause high error in the stackmc estimate .",
    "this effect is due to the fact that under quasi - mc , held - out data samples are at regular distances from held - in samples .",
    "the covariance between @xmath11 and @xmath1 is then typically measured at medium distances from the held - in data .",
    "in contrast , under iid sampling , the held - out samples are sometimes nearby to held - in points , and sometimes very far away , netting an overall unbiasedness in cov(@xmath11 , @xmath1 ) .",
    "furthermore , terms such as @xmath83 $ ] , which are zero under iid sampling , are non - zero on a per - fold basis with quasi - monte carlo sampling .",
    "this problem can be mitigated by removing the correlation between held - in and held - out samples . to keep the expected error small ,",
    "this must be done without introducing significant variance .",
    "one algorithm that meets these criteria first partitions the data into @xmath26 folds , and then replaces the held - in samples by doing a bootstap without replacement from the full dataset .",
    "this removes much of the correlation between the held - in and held - out samples . in theory ,",
    "one may be able to reduce correlations further by performing a bootstrap with replacement , but in practice this can cause numerical issues when too many of the same sample are in the held - in set . in practice , this estimator still has relatively high variance due to the in - sample procedure .",
    "this can be mitigated by performing this partition / bootstrap procedure several times and setting @xmath84 as the average over all of the runs .",
    "we tested this bootstrap procedure on two different cases . in both @xmath0",
    "is 10-d rosenbrock . in the first ,",
    "samples are generated with latin - hypercube sampling and a uniform distribution on [ -3 , 3 ] in each dimension . in the second case data drawn from the scrambled halton sequence according to a gaussian with @xmath85 in every dimension .",
    "different draws from the halton sequence were done with an initial random burn - in length .",
    "the results are seen in fig.[fig : rosenuniflatin ] and fig.[fig : rosengausshalton ] .    in both of these example cases",
    ", it is seen that the normal stackmc algorithm has much higher expected error than the monte carlo samples . in fig .",
    "[ fig : rosengausshalton ] , in fact , we see that this performance gap persists even as the number of samples grows . in both of the examples ,",
    "this bootstrap procedure reduces the estimation error , and when @xmath86 different samplings are combined , we see that an error at least as low as monte carlo is recovered . for certain regimes in fig . [",
    "fig : rosengausshalton ] , the error is in fact lower than the monte carlo error and the direct fit error .",
    "if the sample data are generated via importance sampling , the original stackmc equation no longer holds .",
    "the control variates equation is re - written as    @xmath87    where @xmath29 is the importance sampling distribution .",
    "the natural choice for the target of the supervised learning algorithm is no longer the true function values themselves , but instead the function values scaled by the importance sampling correction factor @xmath88 .",
    "like above , we consider the 10-rosenbrock and a uniform distribution , but this time the samples are generated under the importance sampling distribution @xmath89 $ ] .",
    "this distribution grows towards the edge of the hypercube like the rosenbrock function itself . in high dimensions , however , the polynomial fit can not be integrated analytically under the distribution as there are an exponential number of terms in the product @xmath90 .",
    "the fit must be estimated using monte carlo instead .",
    "this estimate is performed using @xmath91 samples generated from @xmath29 , representing a case where samples of @xmath92 are moderately expensive to produce .",
    "the results from this experiment can be seen in fig .",
    "[ fig : rosenunif_impsampquad30d ] .",
    "it is seen that this high sampling error causes the fit estimation error to be quite high , and yet in the medium range of samples stackmc has lower error than mc .",
    "in addition to demonstrating is - stackmc , this also highlights that @xmath12 does not need to be estimated exactly .              in some cases , one wants to use mc to estimate a sum instead of an integral , for example when @xmath2 is a boolean string , @xmath93 . a fitting algorithm for this space",
    "is found by noting that any function of a @xmath94-dimensional bit string can be represented as @xmath95 this is essentially a discrete fourier transform of @xmath1 using an orthonormal basis of the space @xmath96 given by the walsh functions @xmath97 .",
    "an approximation to @xmath0 can be created by truncating this expansion and only considering contributions from a subset of the walsh functions .",
    "this learning algorithm is a fast  off the shelf \" learner of functions from bit strings to the reals . as an example , we take @xmath0 to be the the four peaks function @xcite of baluja and caruana , and @xmath4 to be uniform .",
    "the fit ignores all terms with three or more components , and sets the rest using least - squares .",
    "this fitting algorithm has very nice property that its expected value is simply @xmath98 as all of the other terms have 0 expectation .",
    "the results from this experiment are shown in fig .",
    "[ fig : twinpeaks_walsh ] .",
    "we see that not only does stackmc have the the lowest squared error for all sample sizes , but for a range of sample sizes the error is much lower than either mc or the walsh fit on their own .",
    "stackmc uses supervised learning to construct a control variate from mc samples and then uses stacking to reduce the resultant overfitting problem .",
    "it is a purely post - processing technique , applicable to any mc estimator , and can significantly reduce the variance of monte carlo integral estimators without adding bias , thereby significantly improving accuracy .",
    "here we derive expressions for stackmc s expected error , and use it to motivate a new estimator of stackmc s parameter @xmath14 , which can reduce stackmc s error in extreme cases .",
    "we also extend stackmc to incorporate multiple control variates ; to use data generated with importance sampling and with quasi - monte carlo ; and to categorical sample spaces .",
    "we present experiments verifying the power of these extensions : applying them to data generated by an mc algorithm never results in higher error than the uncorrected mc estimate , never results in higher error than the fitter ( except for very small data sets ) , and frequently significantly outperforms both .",
    "we also find that stackmc may be more flexible than previously appreciated ; our results suggest that obtaining an accurate estimate of @xmath12 , the integral of the fit , may not be necessary just to improve over the original mc estimate .",
    "there are several major areas of future research .",
    "we intend to investigate the use of stackmc for mcmc methods .",
    "( mcmc methods create correlations among the samples similar to those of quasi - monte carlo , and so we may need to adapt stackmc for mcmc similarly to how we did it for quasi - mc . ) we also intend to extend stackmc to cases where the probability distribution defining the desired expectation value is unknown .",
    "( in theory , a control variate could be used to estimate the probability distribution itself , and stackmc style techniques applied with that control variate . )",
    "t.  gunter , m.  a. osborne , r.  garnett , p.  hennig , and s.  j. roberts . sampling for inference in probabilistic models with fast bayesian quadrature . in _ advances in neural information processing systems _ ,",
    "pages 27892797 , 2014 .",
    "g.  r. ravanbakhsh  siamak , poczos  barnabas .",
    "a cross entropy optimization method for partially decomposable problems . in d.",
    "p. m.  fox , editor , _ proceedings of the twenty - fourth aaai conference on artificial intelligence",
    ". special track on ai and bioinformatics _ , pages 12801286 , atlanta , usa , july 11  15 2010 .",
    "aaai press .",
    "p.  smyth and d.  wolpert .",
    "stacked density estimation . in _ proceedings of the 1997 conference on advances in neural information processing systems 10",
    "_ , nips 97 , pages 668674 , cambridge , ma , usa , 1998 . mit press ."
  ],
  "abstract_text": [
    "<S> monte carlo ( mc ) sampling algorithms are an extremely widely - used technique to estimate expectations of functions @xmath0 , especially in high dimensions . </S>",
    "<S> control variates are a very powerful technique to reduce the error of such estimates , but in their conventional form rely on having an accurate approximation of @xmath1 , _ a priori_. stacked monte carlo ( stackmc ) is a recently introduced technique designed to overcome this limitation by fitting a control variate to the data samples themselves . </S>",
    "<S> done naively , forming a control variate to the data would result in overfitting , typically _ worsening _ the mc algorithm s performance . </S>",
    "<S> stackmc uses in - sample / out - sample techniques to remove this overfitting . </S>",
    "<S> crucially , it is a post - processing technique , requiring no additional samples , and can be applied to data generated by _ any _ mc estimator . </S>",
    "<S> our preliminary experiments demonstrated that stackmc improved the estimates of expectations when it was used to post - process samples produces by a  simple sampling \" mc estimator . here </S>",
    "<S> we substantially extend this earlier work . </S>",
    "<S> we provide an in - depth analysis of the stackmc algorithm , which we use to construct an improved version of the original algorithm , with lower estimation error . </S>",
    "<S> we then perform experiments of stackmc on several additional kinds of mc estimators , demonstrating improved performance when the samples are generated via importance sampling , latin - hypercube sampling and quasi - monte carlo sampling . </S>",
    "<S> we also show how to extend stackmc to combine multiple fitting functions , and how to apply it to discrete input spaces @xmath2 . </S>"
  ]
}