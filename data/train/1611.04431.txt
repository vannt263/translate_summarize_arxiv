{
  "article_text": [
    "in the last decade we entered the data - intensive era of astrophysics , where the size of data has rapidly increased , reaching in many cases dimensions overcoming the human possibility to handle them in an efficient and comprehensible way . in a very close future petabytes of data",
    "will be the standard and , to deal with such amount of information , also the data analysis techniques and facilities must quickly evolve .",
    "for example the current exploration of petabyte - scale , multi - disciplinary astronomy and earth observation synergy , by taking the advantage from their similarities in data analytics , has issued the urgency to find and develop common strategies able to achieve solutions in the data mining algorithms , computer technologies , large scale distributed database management systems as well as parallel processing frameworks @xcite .",
    "astrophysics is one of the most involved research fields facing with this data explosion , where the data volumes from the ongoing and next generation multi - band and multi - epoch surveys are expected to be so huge that the ability of the astronomers to analyze , cross - correlate and extract knowledge from such data will represent a challenge for scientists and computer engineers . to quote just a few ,",
    "the esa euclid space mission will acquire and process about 100 gbday@xmath1 over at least 6 years , collecting a minimum amount of about @xmath2 tb of data @xcite ; pan - starrs @xcite is expected to produce more than @xmath3 tb of data ; the gaia space mission will build a @xmath4 map of the milky way galaxy , by collecting about one petabyte of data in five years @xcite ; the large synoptic survey telescope ( @xcite ) will provide about @xmath5tb / night of imaging data for ten years and petabytes / year of radio data products .",
    "many other planned instruments and already operative surveys will reach a huge scale during their operational lifetime , such as kids ( kilo - degree survey ; @xcite ) , des ( dark energy survey , @xcite ) , herschel - atlas @xcite , hi - gal @xcite , ska @xcite and e - elt @xcite .",
    "the growth and heterogeneity of data availability induce challenges on cross - correlation algorithms and methods .",
    "most of the interesting research fields are in fact based on the capability and efficiency to cross - correlate information among different surveys .",
    "this poses the consequent problem of transferring large volumes of data from / to data centers , _ de facto _ making almost inoperable any cross - reference analysis , unless to change the perspective , by moving software to the data @xcite .",
    "furthermore , observed data coming from different surveys , even if referred to a same sky region , are often archived and reduced by different systems and technologies .",
    "this implies that the resulting catalogs , containing billions of sources , may have very different formats , naming schemas , data structures and resolution , making the data analysis to be a not trivial challenge .",
    "some past attempts have been explored to propose standard solutions to introduce the uniformity of astronomical data quantities description , such as in the case of the uniform content descriptors of the virtual observatory @xcite .",
    "one of the most common techniques used in astrophysics and fundamental prerequisite for combining multi - band data , particularly sensible to the growing of the data sets dimensions , is the cross - match among heterogeneous catalogs , which consists in identifying and comparing sources belonging to different observations , performed at different wavelengths or under different conditions .",
    "this makes cross - matching one of the core steps of any standard modern pipeline of data reduction / analysis and one of the central components of the virtual observatory @xcite .",
    "the massive multi - band and multi - epoch information , foreseen to be available from the on - going and future surveys , will require efficient techniques and software solutions to be directly integrated into the reduction pipelines , making possible to cross - correlate in real time a large variety of parameters for billions of sky objects .",
    "important astrophysical questions , such as the evolution of star forming regions , the galaxy formation , the distribution of dark matter and the nature of dark energy , could be addressed by monitoring and correlating fluxes at different wavelengths , morphological and structural parameters at different epochs , as well as by opportunely determining their cosmological distances and by identifying and classifying peculiar objects . in such context ,",
    "an efficient , reliable and flexible cross - matching mechanism plays a crucial role . in this work we present @xmath0 ( _ command - line catalog cross - match tool and the user guide are available at the page http://dame.dsf.unina.it/c3.html.]_ , @xcite ) , a tool to perform efficient catalog cross - matching , based on the multi - thread paradigm , which can be easily integrated into an automatic data analysis pipeline and scientifically validated on some real case examples taken from public astronomical data archives .",
    "furthermore , one of major features of this tool is the possibility to choose shape , orientation and size of the cross - matching area , respectively , between elliptical and rectangular , clockwise and counterclockwise , fixed and parametric .",
    "this makes the @xmath0 tool easily tailored on the specific user needs .",
    "the paper is structured as follows : after a preliminary introduction , in sec .  [",
    "sec : techniques ] we perform a summary of main available techniques ; in sec .",
    "[ sect : c3design ] , the design and architecture of the @xmath0 tool is described ; in sections [ sect : config ] and [ sect : optimization ] , the procedure to correctly use @xmath0 is illustrated with particular reference to the optimization of its parameters ; some tests performed in order to evaluate @xmath0 performance are shown in sec .",
    "[ sect : performances ] ; finally , conclusions and future improvements are drawn in sec .",
    "[ sect : conclusion ] .",
    "cross - match can be used to find detections surrounding a given source or to perform one - to - one matches in order to combine physical properties or to study the temporal evolution of a set of sources .",
    "the primary criterion for cross - matching is the approximate coincidence of celestial coordinates ( positional cross - match ) .",
    "there are also other kinds of approach , which make use of the positional mechanism supplemented by statistical analysis used to select best candidates , like the bayesian statistics @xcite . in the positional cross - match , the only attributes under consideration are the spatial information .",
    "this kind of match is of fundamental importance in astronomy , due to the fact that the same object may have different coordinates in various catalogs , for several reasons : measurement errors , instrument sensitivities , calibration , physical constraints , etc .    in principle , at the base of any kind of catalog cross - match , each source of a first catalog should be compared with all counterparts contained in a second catalog .",
    "this procedure , if performed in the naive way , is extremely time consuming , due to the huge amount of sources .",
    "therefore different solutions to this problem have been proposed , taking advantage of the progress in computer science in the field of multi - processing and high performing techniques of sky partitioning .",
    "two different strategies to implement cross - matching tools basically exist : web and stand - alone applications .",
    "web applications , like openskyquery @xcite , or cds - xmatch @xcite , offer a portal to the astronomers , allowing to cross - match large astronomical data sets , either mirrored from worldwide distributed data centers or directly uploadable from the user local machine , through an intuitive user interface .",
    "the end - user has not the need to know how the data are treated , delegating all the computational choices to the backend software , in particular for what is concerning the data handling for the concurrent parallelization mechanism .",
    "other web applications , like arches @xcite , provide dedicated script languages which , on one hand , allow to perform complex cross - correlations while controlling the full process but , on the other hand , make experiment settings quite hard for an astronomer .",
    "basically , main limitation of a web - based approach is the impossibility to directly use the cross - matching tool in an automatic pipeline of data reduction / analysis . in other words , with such a tool the user can not design and implement a complete automatic procedure to deal with data .",
    "moreover , the management of concurrent jobs and the number of simultaneous users can limit the scalability of the tool . for example",
    ", a registered user of cds - xmatch has only @xmath6 mb disk space available to store his own data ( reduced to @xmath3 mb for unregistered users ) and all jobs are aborted if the computation time exceeds 100 minutes @xcite .",
    "finally , the choice of parameters and/or functional cases is often limited in order to guarantee a basic use by the end - users through short web forms ( for instance , in cds - xmatch only equatorial coordinate system is allowed ) .",
    "stand - alone applications are generally command - line tools that can be run on the end - user machine as well as on a distributed computing environment .",
    "a stand - alone application generally makes use of apis ( application programming interfaces ) , a set of routines , protocols and tools integrated in the code .",
    "there are several examples of available apis , implementing astronomical facilities , such as stil @xcite , and astroml @xcite , that can be integrated by an astronomer within its own source code .",
    "however , this requires the astronomer to be aware of strong programming skills .",
    "moreover , when the tools are executed on any local machine , it is evident that such applications may be not able to exploit the power of distributed computing , limiting the performance and requiring the storage of the catalogs on the hosting machine , besides the problem of platform dependency .    on the contrary ,",
    "a ready - to - use stand - alone tool , already conceived and implemented to embed the use of apis in the best way , will result an off - the - shelf product that the end - user has only to run .",
    "a local command - line tool can be put in a pipeline through easy system calls , thus giving the possibility to the end - user to create a custom data analysis / reduction procedure without writing or modifying any source code . moreover , being an all - in - one package , i.e including all the required libraries and routines , a stand - alone application can be easily used in a distributed computing environment , by simply uploading the code and the data on the working nodes of the available computing infrastructure .",
    "one of the most used stand - alone tools is stilts ( stil tool set , @xcite ) .",
    "it is not only a cross - matching software , but also a set of command - line tools based on the stil libraries , to process tabular data .",
    "it is written in pure java ( almost platform independent ) and contains a large number of facilities for table analysis , so being a very powerful instrument for the astronomers .",
    "on one hand , the general - purpose nature of stilts has the drawback to make hard the syntax for the composition of the command line ; on the other hand , it does not support the full range of cross - matching options provided by @xmath0 . in order to provide a more user - friendly tool to the astronomers ,",
    "it is also available its graphical counterpart , tool for operations on catalogs and tables ( topcat , @xcite ) , an interactive graphical viewer and editor for tabular data , based on stil apis and implementing the stilts functionalities , but with all the intrinsic limitations of the graphical tools , very similar to the web applications in terms of use .",
    "regardless the approach to cross - match the astronomical sources , the main problem is to minimize the computational time exploding with the increasing of the matching catalog size . in principle , the code can be designed according to multi - process and/or multi - thread paradigm , so exploiting the hosting machine features .",
    "for instance , @xcite evaluated to use a multi - gpu environment , designing and developing their own xmatch tool , @xcite .",
    "other studies are focused to efficiently cross - match large astronomical catalogs on clusters consisting of heterogeneous processors including both multi - core cpus and gpus , ( @xcite , @xcite ) .",
    "furthermore , it is possible to reduce the number of sources to be compared among catalogs , by opportunely partitioning the sky through indexing functions and determining only a specific area to be analyzed for each source .",
    "cds - xmatch and the tool described in @xcite use hierarchical equal area isolatitude pixelisation ( healpix , @xcite ) , to create such sky partition .",
    "@xcite , instead , proposed a combined method to speed up the cross - match by using htm ( hierarchical triangle mesh , @xcite ) , in combination with healpix and by submitting the analysis to a pool of threads .",
    "healpix is a genuinely curvilinear partition of the sphere into exactly equal area quadrilaterals of varying shape ( see fig .  3 in @xcite )",
    "the base - resolution comprises twelve pixels in three rings around the poles and equator .",
    "each pixel is partitioned into four smaller quadrilaterals in the next level .",
    "the strategy of htm is the same of healpix .",
    "the difference between the two spatial - indexing functions is that htm partitioning is based on triangles , starting with eight triangles , @xmath7 on the northern and @xmath7 on the southern hemisphere , each one partitioned into four smaller triangles at the next level ( see also fig .  2 in @xcite ) .",
    "by using one or both functions combined together , it is possible to reduce the number of comparisons among objects to ones lying in adjacent areas .",
    "finally openskyquery uses the _ zones _ indexing algorithm to efficiently support spatial queries on the sphere , @xcite .    the basic idea behind the _ zones _",
    "method is to map the sphere into stripes of a certain height @xmath8 , called zones .",
    "each object with coordinates ( @xmath9 , @xmath10 ) is assigned to a zone by using the formula :    @xmath11    a traditional b - tree index is then used to store objects within a zone , ordered by _ zoneid _ and right ascension . in this way",
    ", the spatial cross - matching can be performed by using bounding boxes ( b - tree ranges ) dynamically computed , thus reducing the number of comparisons ( fig .  1 in @xcite ) .",
    "finally , an additional and expensive test allows to discard false positives .",
    "all the cross - matching algorithms based on a sky partitioning have to deal with the so - called block - edge problem , illustrated in fig .",
    "[ fig : block - edge ] : the objects @xmath12 and @xmath13 in different catalogs correspond to the same object but , falling in different pieces of the sky partition , the cross - matching algorithm is not able to identify the match . to solve this issue , it is necessary to add further steps to the pipeline , inevitably increasing the computational time .",
    "for example , the zhao s tool , @xcite , expands a healpix block with an opportunely dimensioned border ; instead , the algorithm described by @xcite , combining healpix and htm virtual indexing function shapes , is able to reduce the block - edge problem , because the lost objects in a partition may be different from one to another .     and @xmath13 in two catalogs . even if corresponding to the same source , they can be discarded by the algorithm , since they belong to two different blocks of the sky partition.,title=\"fig:\",width=226 ] +",
    "@xmath0 is a command - line open - source python script , designed and developed to perform a wide range of cross - matching types among astrophysical catalogs .",
    "the tool is able to be easily executed as a stand - alone process or integrated within any generic data reduction / analysis pipeline .",
    "based on a specialized sky partitioning function , its high - performance capability is ensured by making use of the multi - core parallel processing paradigm . it is designed to deal with massive catalogs in different formats , with the maximum flexibility given to the end - user , in terms of catalog parameters , file formats , coordinates and cross - matching functions .    in @xmath0 different functional cases and matching criteria",
    "have been implemented , as well as the most used join function types .",
    "it also works with the most common catalog formats , with or without header : flexible image transport system ( fits , version tabular ) , american standard code for information interchange ( ascii , ordinary text , i.e. space separated values ) , comma separated values ( csv ) , virtual observatory table ( votable , xml based ) and with two kinds of coordinate system , equatorial and galactic , by using stilts in combination with some standard python libraries , namely _ _ numpy _ _",
    "@xcite , and _",
    "_ pyfits _ _ ] .",
    "+    despite the general purpose of the tool , reflected in a variety of possible functional cases , @xmath0 is easy to use and to configure through few lines in a single configuration file .",
    "main features of @xmath0 are the following :    1",
    ".   _ command line _ : @xmath0 is a command - line tool",
    ". it can be used as stand - alone process or integrated within more complex pipelines ; 2 .   _ python compatibility _ : compatible with python 2.7.x and 3.4.x ( up to the latest version currently available , @xmath15 ) ; 3 .   _",
    "multi - platform _ :",
    "@xmath0 has been tested on ubuntu linux @xmath16 , windows @xmath17 and @xmath18 , mac os and fedora ; 4 .   _ multi - process _ :",
    "the cross - matching process has been developed to run by using a multi - core parallel processing paradigm ; 5 .   _ user - friendliness _ : the tool is very simple to configure and to use ; it requires only a configuration file , described in sec .",
    "[ sect : config ] .",
    "the internal cross - matching mechanism is based on the sky partitioning into cells , whose dimensions are determined by the parameters used to match the catalogs . the sky partitioning procedure is described in [ sect : preproc ] .",
    "the fig .",
    "[ fig : flowchart ] shows the most relevant features of the @xmath0 processing flow and the user parameters available at each stage .      as mentioned before",
    ", the user can run @xmath0 to match two input catalogs by choosing among three different functional cases :    1 .",
    "_ sky _ : the cross - match is done within sky areas ( elliptical or rectangular ) defined by the celestial coordinates taken from catalog parameters ; 2 .",
    "_ exact value _ : two objects are matched if they have the same value for a pair of columns ( one for each catalog ) defined by the user ; 3 .   _ row - by - row _ : match done on a same row - id of the two catalogs .",
    "the only requirement here is that the input catalogs must have the same number of records .",
    "the positional cross - match strategy of the @xmath0 method is based on the same concept of the q - fulltree approach , an our tool introduced in @xcite and @xcite : for each object of the first input catalog , it is possible to define an elliptical , circular or rectangular region centered on its coordinates , whose dimensions are limited by a fixed value or defined by specific catalog parameters .",
    "for instance , the two full width at half maximum ( fwhm ) values in the catalog can define the two semi - axes of an ellipse or the couple width and height of a rectangular region .",
    "it is also possible to have a circular region , by defining an elliptical area having equal dimensions .",
    "once defined the region of interest , the next step is to search for sources of the second catalog within such region , by comparing their distance from the central object and the limits of the area ( for instance , in the elliptical cross - match the limits are defined by the analytical equation of the ellipse ) .",
    "+    in the _ sky _ functional case , the user can set additional parameters in order to characterize the matching region and the properties of the input catalogs .",
    "in particular , the user may define :    1 .",
    "the shape ( elliptical or rectangular ) of the matching area , i.e. the region , centered on one of the matching sources , in which to search the objects of the second catalog ; 2 .",
    "the dimensions of the searching area .",
    "they can be defined by fixed values ( in arcseconds ) or by parametric values coming from the catalog .",
    "moreover , the region can be rotated by a position angle ( defined as fixed value or by a specific column present in the catalog ) ; 3 .",
    "the coordinate system for each catalog ( galactic , icrs , fk4 , fk5 ) and its units ( degrees , radians , sexagesimal ) , as well as the columns containing information about position and designation of the sources .",
    "an example of graphical representation of an elliptical cross - match is shown in fig .",
    "[ fig : crossmatch ] .    in the _ exact value _ case , the user has to define only which columns ( one for each input catalog ) have to be matched , while in the most simple _ row - by - row _ case no particular configuration is needed .",
    "@xmath0 produces a file containing the results of the cross - match , consisting into a series of rows , corresponding to the matching objects . in the case of _ exact value _ and",
    "_ sky _ options , the user can define the conditions to be satisfied by the matched rows to be stored in the output .",
    "first , it is possible to retrieve , for each source , all the matches or only the best pairs ( in the sense of closest objects , according to the match selection criterion ) ; then , the user can choose different join possibilities ( in fig .  [",
    "fig : joins ] the graphical representation of available joins is shown ) :    @xmath19 and @xmath20 : :    only rows having an entry in both input catalogs , ( fig .",
    "[ fig : joins]a ) ; @xmath19 or @xmath20 : :    all rows , matched and unmatched , from both input catalogs ,    ( fig .",
    "[ fig : joins]b ) ; all from @xmath19 ( all from @xmath20 ) : :    all matched rows from catalog @xmath19 ( or @xmath20 ) ,    together with the unmatched rows from catalog @xmath19 ( or    @xmath20 ) , ( fig .",
    "[ fig : joins]c - d ) ; @xmath19 not @xmath20 ( @xmath20 not @xmath19 ) : :    all the rows of catalog @xmath19 ( or @xmath20 ) without    matches in the catalog @xmath20 ( or @xmath19 ) ,    ( fig .",
    "[ fig : joins]e - f ) ; @xmath19 xor @xmath20 : :    the `` exclusive or '' of the match - i.e. only rows from the catalog    @xmath19 not having matches in the catalog @xmath20 and    viceversa , ( fig .",
    "[ fig : joins]g ) .",
    "+   +      any experiment with the @xmath0 tool is based on two main phases ( see fig .",
    "[ fig : flowchart ] ) :    1 .",
    "_ pre - matching : _ this is the first task performed by @xmath0 during execution . the tool manipulates input catalogs to extract the required information and prepare them to the further analysis ; 2 .   _",
    "matching : _",
    "after data preparation , @xmath0 performs the matching according to the criteria defined in the configuration file .",
    "finally , the results are stored in a file , according to the match criterion described in sec .",
    "[ sect : join ] , and all the temporary data are automatically deleted .",
    "this is the preliminary task performed by @xmath0 execution . during the pre - matching phase",
    ", @xmath0 performs a series of preparatory manipulations on input data .",
    "first of all , a validity check of the configuration parameters and input files .",
    "then it is necessary to split the data sets in order to parallelize the matching phase and improve the performance . in the _ exact value _",
    "functional case only the first input catalog will be split , while in the _ sky _ case both data sets will be partitioned in subsets . in the latter case",
    ", @xmath0 makes always use of galactic coordinates expressed in degrees , thus converting them accordingly if expressed in different format .",
    "when required , the two catalogs are split in the following way : in the first catalog all the entries are divided in groups , whose number depends on the multi - processing settings ( see sec .  [ sect : config ] ) , since each process is assigned to one group ; in the second catalog the sky region defined by the data set is divided into square cells , by assigning a cell to each entry , according to its coordinates ( fig .",
    "[ fig : partitioning ] ) .",
    "we used the python multiprocess module to overcome the gil problem , by devoting particular care to the granularity of data to be handled in parallel .",
    "this implies that the concurrent processes do not need to share resources , since each process receives different files in input ( group of object of the 1st catalog and cells ) and produces its own output .",
    "finally the results are merged to produce the final output .",
    "the partitioning procedure on the second catalog is based on the dimensions of the matching areas : the size of the unit cell is defined by the maximum dimension that the elliptical matching regions can assume .",
    "if the `` size type '' is `` parametric '' , then the maximum value of the columns indicated in the configuration is used as cell size ; in the case of `` fixed '' values , the size of the cell will be the maximum of the two values defined in the configuration ( fig .",
    "[ fig : partitioning]a ) . in order to optimize the performance ,",
    "the size of the unit cell can not be less than a threshold value , namely the _ minimum partition cell size _ , which the user has to set through the configuration file .",
    "the threshold on the cell size is required in order to avoid the risk to divide the sky in too many small areas ( each one corresponding to a file stored on the disk ) , which could slow down the cross - matching phase performance . in sec .",
    "[ sect : optimization ] we illustrated a method to optimize such parameter as well as the number of processes to use , according to the hosting machine properties .    once the partitioning is defined , each object of the second catalog is assigned to one cell , according to its coordinates .",
    "having defined the cells , the boundaries of an elliptical region associated to an object can fall at maximum in the eight cells surrounding the one including the object , as shown in fig .",
    "[ fig : partitioning]b .",
    "this prevents the block - edge problem previously introduced .",
    "once the data have been properly re - arranged , the cross - match analysis can start . in the _ row - by - row _ case , each row of the first catalog",
    "is simply merged with the corresponding row of the second data set through a serial procedure . in the other functional cases ,",
    "the cross - matching procedure has been designed and implemented to run by using parallel processing , i.e. by assigning to each parallel process one group generated in the previous phase . in the _ exact value _",
    "case , each object of the group is compared with all the records of the second catalog and matched according to the conditions defined in the configuration file .    in the _ sky _ functional case ,",
    "the matching procedure is slightly more complex . as described in sec .",
    "[ sect : usecases ] , the cross - match at the basis of the @xmath0 method is based on the relative position of two objects : for each object of the first input catalog , @xmath0 defines the elliptical / rectangular region centered on its coordinates and dimensions .",
    "therefore a source of the second catalog is matched if it falls within such region .    in practice ,",
    "as explained in the pre - matching phase , having identified a specific cell for each object of a group , this information is used to define the minimum region around the object used for the matching analysis .",
    "the described choice to set the dimensions of the cells ensures that , if a source matches with the object , it must lie in the nine cells surrounding the object ( also known as moore s neighborhood , @xcite , see also fig .  [",
    "fig : partitioning]b ) .",
    "therefore it is sufficient to cross - match an object of a group only with the sources falling in nine cells .    in the _ sky _ functional case",
    ", @xmath0 performs a cross - matching of objects lying within an elliptical , circular or rectangular area , centered on the sources of the first input catalog .",
    "the matching area is characterized by @xmath21 configuration parameters defining its shape , dimensions and orientation . in fig .",
    "[ fig : pa ] is depicted a graphical representation of two matching areas ( elliptical and rectangular ) with the indication of its parameters .        in particular , to define the orientation of the matching area , @xmath0 requires two further parameters besides the offset and the value of the position angle , representing its orientation .",
    "the position angle , indeed , is referred , by default , to the greatest axis of the matching area with a clockwise orientation .",
    "the two additional parameters give the possibility to indicate , respectively , the correct orientation ( clockwise / counterclockwise ) and a shift angle ( in degrees ) .    finally , the results of the cross - matching are stored in a file , containing the concatenation of all the columns of the input catalogs referred to the matched rows . in the _ sky _",
    "functional case the column reporting the separation distance between the two matching objects is also included .",
    "the tool @xmath0 is interfaced with the user through a single configuration file , to be properly edited just before the execution of any experiment . if the catalogs do not contain the source s designation / id information , @xmath0 will automatically assign an incremental row - id to each entry as object designation .    for the _ sky _ functional case , assuming that both input catalogs contain the columns reporting the object coordinates , @xmath0 is able to work with galactic and equatorial ( icrs , fk4 , fk5 ) coordinate systems , expressed in the following units : degrees , radians or sexagesimal .",
    "if the user wants to use catalog information to define the matching region ( for instance , the fwhms or a radius defined by the instrumental resolution ) , obviously the first input catalog must contain such data .",
    "the position angle value / column is , on the contrary , an optional information ( default is 0@xmath22 , clockwise ) .",
    "@xmath0 is conceived for a community as wide as possible , hence it has been designed in order to satisfy the requirement of user - friendliness . therefore , the configuration phase is limited to the editing of a setup file , can also automatically generate a dummy configuration file that could be used as template . ] containing all the information required to run @xmath0 .",
    "this file is structured in sections , identified by square brackets : the first two are required , while the others depend on the particular use case . in particular , the user has to provide the following information :    1 .   the input files and their format ( fits , ascii , csv or votable ) ; 2 .",
    "the name and paths of the temporary , log and output files ; 3 .   the match criterion , corresponding to one of the functional cases ( _ sky , exact value , row - by - row _ ) .",
    "@xmath0 gives also the possibility to set the number of processes running in parallel , through an optional parameter which has as default the number of cores of the working machine ( minus one left available for system auxiliary tasks ) .",
    "the configuration for the _ sky _ functional case foresees the setup of specific parameters of the configuration file : those required to define the shape and dimensions of the matching area , the properties of the input catalogs already mentioned in sec .",
    "[ sect : usecases ] , coordinate system , units as well as the column indexes for source coordinates and designation .",
    "in addition , a parameter characterizing the sky partitioning has to be set ( see sec .",
    "[ sect : preproc ] for further information ) .",
    "the parameters useful to characterize the matching area are the following :    area shape : :    it can be elliptical or rectangular ( circular is a special elliptical    case ) ; size type : :    the valid entries are _ fixed _ or _",
    "parametric_. in the first case , a    fixed value will be used to determine the matching area ; in the    second , the dimensions and inclination of the matching area will be    calculated by using catalog parameters ; first and second dimensions of matching area : :    the axes of the ellipse or width and height of the rectangular area .    in case of fixed `` size type '' ,",
    "they are decimal values ( in arcsec ) ,    otherwise , they represent the index ( integer ) or name ( string ) of the    columns containing the information to be used ; parametric factor : :    it is required and used only in the case of parametric `` size type '' .",
    "it is a decimal number factor to be multiplied by the values used as    dimensions , in order to increase or decrease the matching region , as    well as useful to convert their format ; pa column / value : :    it is the position angle value ( in the `` fixed '' case , expressed in    degrees ) or the name / id of the column containing the position angle    information ( in the `` parametric '' case ) ; pa settings : :    the position angle , which in @xmath0 is referred , by    default , to the main axis of the matching area ( greatest ) with a    clockwise orientation .",
    "the two parameters defined here give the    possibility to indicate the correct orientation    ( clockwise / counterclockwise ) and a shift angle ( in degrees ) .",
    "the user has also to specify which rows must be included in the output file , by setting the two parameters indicating the match selection and the join type , as described in sec .",
    "[ sect : join ] .      for the _ exact value _",
    "functional case it is required to set the name or i d of the columns used for the match for both input files .",
    "the user has also to specify which rows must be included in the output file , by setting the two parameters indicating the match selection and the join type , as described in sec .",
    "[ sect : join ] .      for the _ row - by - row _ functional case ,",
    "no other settings are required .",
    "the only constrain is that both catalogs must have the same number of entries .",
    "as reflected from the description of @xmath0 , the choice of the best values for its internal parameters ( in particular the number of parallel processes and the minimum cell size , introduced in sec .",
    "[ sect : preproc ] ) , is crucial to obtain the best computational efficiency .",
    "this section is dedicated to show the importance of this choice , directly depending on the features of the hosting machine . in the following tests we used a computer equipped with an intel(r )",
    "core(tm ) @xmath23 , with one @xmath24 , @xmath25 cpu , @xmath26 gb of ram and hosting ubuntu linux @xmath16 as operative system ( os ) on a standard hard disk drive .",
    "we proceeded by performing two different kinds of tests :    1 .",
    "a series of tests with a fixed value for the minimum cell size ( @xmath27 ) and different values of the number of parallel processes ; 2 .   a second series by using the best value of number of parallel processes found at previous step and different values for the minimum cell size .",
    "the configuration parameters used in this set of tests are reported in table [ test1:settings ] .",
    "the input data sets are two identical catalogs ( csv format ) consisting of @xmath28 objects extracted from the ukidss gps public data @xcite , in the range of galactic coordinates @xmath29 $ ] , @xmath30 $ ] .",
    "each record is composed by @xmath31 columns . the choice to cross - match a catalog with itself represent the worst case in terms of cross - matching computational time , since each object matches at least with itself .    by setting `` match selection '' as best and",
    "`` join type '' as 1 and 2 ( see table  [ test1:settings ] ) , we obtained an output of @xmath28 objects matched with themselves as expected .",
    "we also performed all the tests by using a  random shuffled  version of the same input catalog , obtaining the same results .",
    "this demonstrates that the @xmath0 output is not affected by the particular order of data in the catalogs .",
    ".@xmath0 settings in the first set of tests performed to evaluate the impact of the number of parallel processes and the minimum cell size configuration parameters on the execution time .",
    "the choice of same dimensions for the ellipse axes was due to perform a fair comparison with stilts and cds - xmatch , which allow only circular cross - matching . [ cols=\"^,^\",options=\"header \" , ]",
    "the first input catalog has been extracted by the ukidss gps data in the range of galactic coordinates @xmath32 $ ] , @xmath30 $ ] , while the second input catalog has been extracted by the glimpse _",
    "data , ( @xcite and @xcite ) , in the same range of coordinates . from each catalog , different subsets with variable number of objects have been extracted .",
    "in particular , data sets with , respectively , @xmath33 , @xmath34 , @xmath28 , @xmath35 and @xmath36 objects have been created from the first catalog , while , from second catalog , data sets with @xmath33 , @xmath34 , @xmath28 and @xmath35 rows have been extracted .",
    "then , each subset of first catalog has been cross - matched with all the subsets of the second catalog . for uniformity of comparison , due to the limitations imposed by cds - xmatch in terms of available disk space , it has been necessary to limit to only @xmath37 the number of columns for all the subsets involved in the tests performed to compare c@xmath38 and cds - xmatch ( for instance , i d and galactic coordinates ) . for the same reason",
    ", the data set with @xmath39 rows has not been used in the comparison between c@xmath38 and cds - xmatch .",
    "the common internal configuration used in these tests is shown in table  [ test1:settings ] , except for the  match selection ",
    "there was , in fact , the necessity to set it to _ all _ for uniformity of comparison with the cds - xmatch tool ( which makes available only this option ) .",
    "then the _ best _ type has been used to compare @xmath0 with stilts and topcat .",
    "furthermore , in all the tests , the number of parallel processes was set to @xmath40 and the minimum cell size to @xmath27 , corresponding to the best conditions found in the optimization process of @xmath0 ( see sec .  [ sect : optimization ] ) . finally , we chose same dimensions of the ellipse axes in order to be aligned with other tools , which allow only circular cross - matching areas .    concerning the comparison among @xmath0 and the three mentioned tools , in the cases of both _ all _ and _ best _ types of matching selection , all tools provided exactly the same number of matches in the whole set of tests , thus confirming the reliability of @xmath0 with respect to other tools ( table  [ tab : matchres ] ) .",
    "rows has not been used . ]      in terms of computational efficiency , @xmath0 has been evaluated by comparing the computational time of its cross - matching phase with the other tools .",
    "the pre - matching and output creation steps have been excluded from the comparison , because strongly dependent on the host computing infrastructure .",
    "the other configuration parameters have been left unchanged ( table  [ test1:settings ] ) .",
    "the complete setup for the described experiments is reported in the appendix .    in fig .",
    "[ fig : c3vsstrows ] we show the computational time of the cross - matching phase for @xmath0 and stilts , as function of the incremental number of rows ( objects ) in the first catalog , and by varying the size of the second catalog in four cases , spanning from @xmath33 to @xmath35 rows . in all diagrams",
    ", it appears evident the difference between the two tools , becoming particularly relevant with increasing amounts of data .    in the second set of tests performed on the @xmath0 cross - matching phase and stilts ,",
    "the computational time has been evaluated as function of the incremental number of columns of the first catalog ( from the minimum required @xmath37 up to @xmath31 , the maximum number of columns of catalog 1 ) , and by fixing the number of columns of the second catalog in five cases , respectively , @xmath37 , @xmath5 , @xmath41 , @xmath42 and @xmath43 , which is the maximum number of columns for catalog 2 . in terms of number of rows , in all cases both catalogs",
    "were fixed to @xmath35 of entries . in fig .",
    "[ fig : c3vsstcols ] the results only for @xmath37 and @xmath43 columns of catalog 2 are reported , showing that @xmath0 is almost invariant to the increasing of columns , becoming indeed faster than stilts from a certain amount of columns .",
    "such trend is confirmed in all the other tests with different number of columns of the second catalog .",
    "this behavior appears particularly suitable in the case of massive catalogs .",
    "finally , in the case of two fits input files instead of csv files , stilts computational time as function of the number of columns is constant and slightly faster than @xmath14 .        in the last series of tests , we compared the computational efficiency between the @xmath0 cross - matching phase and cds - xmatch .",
    "in this case , due to the limitation of the catalog size imposed by cds - xmatch , the tests have been performed by varying only the number of rows from @xmath33 to @xmath35 as in the analogous tests with stilts ( except the test with @xmath36 rows ) , fixing the number of columns to @xmath37 . moreover , in this case , the cross - matching phase of @xmath0 has been compared with the duration of the phase _ execution _ of the cds - xmatch experiment , thus ignoring latency time due to the job submission , strongly depending on the network status and the state of the job queue , but taking into account the whole job execution . the results , reported in fig .  [",
    "fig : c3vsxmrows ] , show a better performance of @xmath0 , although less evident when both catalogs are highly increasing their dimensions , where the differences due to the different hardware features become more relevant .    at the end of the test campaign ,",
    "two other kinds of tests have been performed : ( i ) the verification of the portability of @xmath0 on different oss and ( ii ) an analysis of the impact of different disk technology on the computing time efficiency of the tool .    in the first case , we noted , as expected , a decreasing of @xmath0 overall time performance on the windows versions ( @xmath17 and @xmath18 ) , with respect to same tests executed on linux versions ( ubuntu and fedora ) and mac os . on",
    "average @xmath0 execution was @xmath44 times more efficient on linux and mac os than windows .",
    "this is most probably due to the different strategy of disk handling among various oss , particularly critical for applications , like cross - matching tools , which make an intensive use of disk accesses .",
    "this analysis induced us to compare two disk technologies : hdd ( hard disk drive ) vs ssd ( solid state disk ) . both kinds of disks have been used on a sample of the tests previously described , revealing on average a not negligible increasing of computing time performance in the ssd case of @xmath45 times with respect to hdd . for clarity ,",
    "all test results presented in the previous sections have been performed on the same hdd .",
    "in this paper we have introduced @xmath0 , a new scalable tool to cross - match astronomical data sets .",
    "it is a multi - platform command - line python script , designed to provide the maximum flexibility to the end users in terms of choice about catalog properties ( i / o formats and coordinates systems ) , shape and size of matching area and cross - matching type .",
    "nevertheless , it is easy to configure , by compiling a single configuration file , and to execute as a stand - alone process or integrated within any generic data reduction / analysis pipeline .    in order to ensure the high - performance capability ,",
    "the tool design has been based on the multi - core parallel processing paradigm and on a basic sky partitioning function to reduce the number of matches to check , thus decreasing the global computational time .",
    "moreover , in order to reach the best performance , the user can tune on the specific needs the shape and orientation of the matching region , as well as tailor the tool configuration to the features of the hosting machine , by properly setting the number of concurrent processes and the resolution of sky partitioning .",
    "although elliptical cross - match and the parametric handling of angular orientation and offset are known concepts in the astrophysical context , their availability in the presented command - line tool makes @xmath0 competitive in the context of public astronomical tools .",
    "a test campaign , done on real public data , has been performed to scientifically validate the @xmath0 tool , showing a perfect agreement with other publicly available tools .",
    "the computing time efficiency has been also measured by comparing our tool with other applications , representative of different paradigms , from stand - alone command - line ( stilts ) and graphical user interface ( topcat ) to web applications ( cds - xmatch ) . such tests revealed the full comparable performance , in particular when input catalogs increase their size and dimensions .    for the next release of the tool , the work will be mainly focused on the optimization of the pre - matching and output creation phases , by applying the parallel processing paradigm in a more intensive way .",
    "moreover , we are evaluating the possibility to improve the sky partitioning efficiency by optimizing the calculation of the minimum cell size , suitable also to avoid the block - edge problem .",
    "the @xmath0 tool , @xcite , and the user guide are available at the page http://dame.dsf.unina.it/c3.html .",
    "the authors would like to thank the anonymous referee for extremely valuable comments and suggestions .",
    "mb and sc acknowledge financial contribution from the agreement asi / inaf i/023/12/1 .",
    "mb , am and gr acknowledge financial contribution from the 7th european framework programme for research grant fp7-space-2013 - 1 , _ vialactea - the milky way as a star formation engine_. mb and am acknowledge the prin - inaf 2014 _ glittering kaleidoscopes in the sky : the multifaceted nature and role of galaxy clusters_.    99 agrafioti , i. 2012 , from the geosphere to the cosmos , synergies with astroparticle physics , astroparticle physics for europe ( aspera ) , contributed volume , http://www.aspera-eu.org annis , j.  t. , 2013 , in american astronomical society meeting abstracts # 221 , des survey strategy and expectations for early science , 221 , id.335.05 becciani , u. , bandieramonte , m. , brescia , m. , et al .",
    "2015 , in proc .",
    "adass xxv conf . ,",
    "advanced environment for knowledge discovery in the vialactea project , in press .",
    "( arxiv:1511.08619 )",
    "benjamin , r.  a. , churchwell , e. , babler , b.l .",
    "2003 , , 115 , 953 , doi : 10.1086/376696 boch , t. , pineau , f.  x. , & derriere , s. 2014 , cds xmatch service documentation , http://cdsxmatch.u-strasbg.fr/xmatch/doc/ braun , r. 2015 , in proc . of `` the many facets of extragalactic radio surveys : towards new scientific challenges '' ( extra - radsur2015 ) .",
    "20 - 23 october 2015 .",
    "bologna , italy .",
    "http://pos.sissa.it/cgi-bin/reader/conf.cgi?confid=267 , id.34 budavri , t. , & lee , m.  a. 2013 , xmatch : gpu enhanced astronomic catalog cross - matching , astrophysics source code library , record ascl:1303.021 budavri , t. , & szalay , a.  s. 2008 , , 679 , 301 cavuoti , s. , brescia , m. , longo , g. 2012 , proc .",
    "spie , 8451 , 845103 , doi : 10.1117/12.925321 churchwell , e. , babler , b.l . , meade , m.r .",
    "2009 , , 121 , 213 de jong , j.  t.  a. , verdoes kleijn , g.  a. , boxhoorn , d.  r. , et al .",
    "2015 , a&a , 582 , a62 douglas , j. , de bruijne , j. , oflaherty , k. , et al .",
    "2007 , esa bulletin , 132 , 26 du , p. , ren , j.  j. , pan , j.  c. , luo , a. 2014 , scpma , 57 , 577 gorski , k.  m. , hivon , e. , banday , a.  j. , et al .",
    "2005 , 622 , 759 gray , j. , nieto - santisteban , m.  a. & szalay , a.  s. 2006 , the zones algorithm for finding points - near - a - point or cross - matchin spatial datasetes , microsoft tech .",
    ": msr - tr-2006 - 52 gray , l. 2003 , not .",
    "50 , 200 ivezic , z. , 2009 , in aps april meeting abstracts , lsst : the physics of the dark universe , 54 , w4.003 , http://adsabs.harvard.edu/abs/2009aps..apr.w4003i ivoa recommendation 2005 , an ivoa standard for unified content decriptors version 1.1 ( http://adsabs.harvard.edu/abs/2005ivoa.spec.0819d ) jia , x. & luo , q. 2016 , in proc .",
    "conf . on scientific and statistical database management ( ssdbm 16 ) , ed .",
    "p. baumann et al .",
    "( new york , ny , acm ) , 12 , doi : 10.1145/2949689.2949705 jia , x. , luo , q. & fan , d. 2015 , in proc .",
    "ieee xxi int . conf . on parallel and distributed systems ( icpads ) , 617 ,",
    "doi : 10.1109/icpads.2015.83 kaiser , n. , 2004 , proc .",
    "spie , 5489 , 11 kunszt , p.  z. , szalay , a.  s. , thakar , a.  r. in proc .",
    "mpa / eso / mpe workshop , eds banday , a.  j. , zaroubi , s. , bartelmann , m. ( berlin : springer ) , 631 , doi : 10.1007/10849171_83 laureijs , r. , racca , g. , stagnaro , l. , et al .",
    "2014 , in proc .",
    "spie , 9143 , 91430h , doi : 10.1117/12.2054883 lee , m.  a. , & budavri , t. , 2013 , in asp conf .",
    "475 , proc .",
    "astronomical data analysis software and systems xxii conf . , cross - identification of astronomical catalogs on multiple gpus , ed .",
    "friedel , d.  n. , ( san francisco , ca : asp ) , 235 lucas , p.  w , hoare , m.  g , longmore , a. , et al .",
    "2008 , , 391 , 136 malkov , o. , dluzhnevskaya , o. , karpov , s. , et al .",
    "2012 , balta , 21 , 319 martins , c.  j.  a.  p. , leite , a.  c.  o. , pedrosa , p.  o.  j. , 2014 , in statistical challenges in 21st century cosmology , fundamental cosmology with the e - elt , proc . of the international astronomical union ,",
    "iau symposium , ed .",
    "heavens , a. , starck , j .- l . &",
    "krone - martins , a. , 306 , 385 - 387 , doi : 10.1017/s1743921314013441 molinari , s. , schisano , e. , elia , d. , et al .",
    "2016 , a&a 591 , a149 , doi : 10.1051/0004 - 6361/201526380 motch , c. , & arches consortium , 2015 , in astronomical data analysis software and systems xxiv , the arches project , ed .",
    "a.  r. taylor and e. rosolowsky ( san francisco : astronomical society of the pacific ) , 437 nieto - santisteban , m.  a. , thakar , a.  r. , szalay , a.  s. , 2006 , cross - matching very large datasets ( baltimore , md : johns hopkins university ) pineau , f.  x. , boch , t. , & derriere , s. 2011 , in asp conf .",
    "442 , proc astronomical data analysis software and systems xx , ed .",
    "evans , i.  n. , accomazzi , a. , mink , d.  j. , & rots , a.  h. ( san francisco , ca : asp ) , 85 riccio , g. , brescia , m. , cavuoti , s. , mercurio , a. 2016 , c3 : command - line catalog crossmatch for modern astronomical surveys , astrophysics source code library , record ascl:1610.006 sciacca , e. , vitello , f. , becciani , u. , et al . , 2016 , milky way analysis through a science gateway : workflows and resource monitoring , proceedings of 8th international workshop on science gateways , june 2016 , rome , italy , submitted to ceur - ws , http://ceur-ws.org , issn : 1613 - 0073 .",
    "taylor , m.  b. , 2005 , in asp conf . ser . 347 , astronomical data analysis software and systems xiv , ed .",
    "p. shopbell , m. britton , & r. ebert ( san francisco , ca : asp ) , 29 taylor , m.  b. , 2006 , in asp conf . ser . 351 , astronomical data analysis software and systems xv , ed .",
    "c. gabriel et al .",
    "( san francisco , ca : asp ) , 666 valiante , e. 2015 , in iau general assembly , meeting 29 , the herschel - atlas survey : main results and data release , 22 , 2257414 van der walt , s. , colbert , s.  c. & varoquaux , g. , 2011 , cse , 13 , 22 vanderplas , j.  t. , connolly , a.  j. , ivezi ,  & gray , a. 2012 , in proc .",
    "conf . on intelligent data understanding ( cidu ) ,",
    "introduction to astroml : machine learning for astrophysics , 47 - 54 , doi : 10.1109/cidu.2012.6382200 varga - verebelyi , e. , dobos , l. , budavari , t. , 2016 , in from interstellar clouds to star - forming galaxies : universal processes ?",
    ", iau symposium 315 , herschel footprint database and service , eprint arxiv:1602.01050 zhao , q. , sun , j. , yu , c. , et al . , 2009 , in algorithms and architectures for parallel processing , proc . of 9th international conference , ica3pp 2009 , a paralleled large - scale astronomical cross - matching function , eds .",
    "arrems , h. , chang , s .,-",
    "l . , 604 - 614 , isbn : 978 - 3 - 642 - 03095 - 6",
    "this appendix reports the configuration file as used in the example described in sec .",
    "[ sect : comparison ] .",
    "the text preceded by the semicolon is a comment .    ....",
    "\\textcolor{red}{[i / o files ] } input catalog 1 : \\textcolor{olive}{./input / ukidss.csv } format catalog 1 : \\textcolor{olive}{csv } \\textcolor{blue}{;csv , fits , votable or ascii } input catalog 2 : \\textcolor{olive}{./input / glimpse.csv } format catalog 2 : \\textcolor{olive}{csv } \\textcolor{blue}{;csv , fits , votable or ascii } output : \\textcolor{olive}{./output / out.csv } output format : \\textcolor{olive}{csv } \\textcolor{blue}{;csv , fits , votable or ascii } log file : \\textcolor{olive}{./output / out.log } stilts directory : \\textcolor{olive}{./libs } working directory : \\textcolor{olive}{./tmp } \\textcolor{blue}{;temporary directory , removed when completed }      \\textcolor{red}{[sky parameters ] } area shape : \\textcolor{olive}{ellipse } \\textcolor{blue}{;ellipse or rectangle } size type : \\textcolor{olive}{fixed } \\textcolor{blue}{;parametric or fixed } matching area first dimension : \\textcolor{olive}{5 } \\textcolor{blue}{;arcsec for fixed type - column name / number for parametric type } matching area second dimension : \\textcolor{olive}{5 } \\textcolor{blue}{;arcsec for fixed type - column name / number for parametric type } parametric factor : \\textcolor{olive}{1 } \\textcolor{blue}{;multiplicative factor for dimension columns - required for parametric type } pa column / value : \\textcolor{olive}{0 } \\textcolor{blue}{;degrees for fixed type - column name / number for parametric type } pa settings : \\textcolor{olive}{clock , 0 } \\textcolor{blue}{;orientation ( clock , counter ) , shift ( degrees ) -empty or default = clock,0 } catalog 2 minimum partition cell size : \\textcolor{olive}{100 } \\textcolor{blue}{;arcsec }    \\textcolor{red}{[catalog 1 properties ] } coordinate system : \\textcolor{olive}{galactic } \\textcolor{blue}{;galactic , icrs , fk4 , fk5 } coordinate units : \\textcolor{olive}{deg } \\textcolor{blue}{;degrees ( or deg ) , radians ( or rad ) , sexagesimal ( or sex ) } glon / ra column : \\textcolor{olive}{l } \\textcolor{blue}{;column number or name - required for sky algorithm } glat / dec column : \\textcolor{olive}{b } \\textcolor{blue}{;column number or name - required for sky algorithm } designation column : \\textcolor{olive}{sourceid } \\textcolor{blue}{;column number or name - -1 for none }    \\textcolor{red}{[catalog 2 properties ] } coordinate system : \\textcolor{olive}{galactic } \\textcolor{blue}{;galactic , icrs , fk4 , fk5 } coordinate units : \\textcolor{olive}{deg } \\textcolor{blue}{;degrees ( or deg ) , radians ( or rad ) , sexagesimal ( or sex ) } glon / ra column : \\textcolor{olive}{l } \\textcolor{blue}{;column number or name - required for sky algorithm } glat / dec column : \\textcolor{olive}{b } \\textcolor{blue}{;column number or name - required for sky algorithm } designation column : \\textcolor{olive}{designation } \\textcolor{blue}{;column number or name , -1 for none }      \\textcolor{red}{[output rows ] } match selection : \\textcolor{olive}{all } \\textcolor{blue}{;all or best } join type : \\textcolor{olive}{1 and 2 } \\textcolor{blue}{;1 and 2 , 1 or 2 , all from 1 , all from 2 , 1 not 2 , 2 not 1 , 1 xor 2 } ...."
  ],
  "abstract_text": [
    "<S> modern astrophysics is based on multi - wavelength data organized into large and heterogeneous catalogs . </S>",
    "<S> hence , the need for efficient , reliable and scalable catalog cross - matching methods plays a crucial role in the era of the petabyte scale . </S>",
    "<S> furthermore , multi - band data have often very different angular resolution , requiring the highest generality of cross - matching features , mainly in terms of region shape and resolution . in this work we present @xmath0 ( command - line catalog cross - match ) , a multi - platform application designed to efficiently cross - match massive catalogs . </S>",
    "<S> it is based on a multi - core parallel processing paradigm and conceived to be executed as a stand - alone command - line process or integrated within any generic data reduction / analysis pipeline , providing the maximum flexibility to the end - user , in terms of portability , parameter configuration , catalog formats , angular resolution , region shapes , coordinate units and cross - matching types . using real data , extracted from public surveys , </S>",
    "<S> we discuss the cross - matching capabilities and computing time efficiency also through a direct comparison with some publicly available tools , chosen among the most used within the community , and representative of different interface paradigms . </S>",
    "<S> we verified that the @xmath0 tool has excellent capabilities to perform an efficient and reliable cross - matching between large data sets . </S>",
    "<S> although the elliptical cross - match and the parametric handling of angular orientation and offset are known concepts in the astrophysical context , their availability in the presented command - line tool makes @xmath0 competitive in the context of public astronomical tools . </S>"
  ]
}