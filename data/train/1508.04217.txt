{
  "article_text": [
    "recent advancements in macroeconomic data collection have led to increased focus on high - dimensional time series analysis .",
    "a more efficient and precise analysis can thus be realized in various situations if we elicit information appropriately from a ( very ) large number of explanatory variables .",
    "however , a higher - dimensional model does not necessarily yield better performance ; in fact , performance varies depending on how large the dimensionality is and what estimation method is considered .",
    "if we estimate a large - dimensional model without appropriate dimension reduction , performance may be poor due to the accumulated estimation losses from unimportant variables .",
    "we may encounter such high - dimensionality when we wish to forecast macroeconomic variables .",
    "for example , dimensionality tends to be very large when forecasting a low - frequency ( e.g. , quarterly ) process , such as gdp , by using higher - frequency ( e.g. , monthly ) variables , since the conclusive matrix of predictors is made of ( skip - sampled ) high - frequency variables with their sufficiently large lags .",
    "this forecasting scheme is called mixed data sampling ( midas ) regression , and was proposed by ghysels et al .",
    "( 2004 , 2006 , 2007 ) ; see also andreou et al .",
    "( 2010a ) and foroni et al .",
    "( 2013 ) for a comprehensive survey .",
    "the midas regression is now one of the essential tools for forecasting a low - frequency variable because of its simpleness and usability .",
    "however , the `` basic '' midas regression is unable to cope with the case where the number of high - frequency variables is large although it can reduce the dimensionality that originates from the lags by a distributed lag structure with a few hyperparameters .",
    "recently , marcellino and schumatcher ( 2010 ) have introduced the factor - midas regression , which overcomes the high dimensionality by subtracting factors from a large number of high - frequency predictors with maintaining their information .    in this paper , we tackle such a high - dimensional problem from another viewpoint ; we adopt _ sparse _ modeling with a large number of predictors that can manage even ultrahigh dimensionality without a prominent cost .",
    "the underlying assumption is that the model is formulated as a linear regression with ultrahigh - dimensional regressors with sparse coefficients .",
    "the unknown sparsity can be recovered with desirable properties via _ folded - concave penalized regression_. there have been researches on forecasting using sparse modeling , such as bai and ng ( 2008 ) and marsilli ( 2014 ) , but there are only a few studies on ultrahigh dimensionality in both theoretical and empirical aspects in macroeconometric literature .",
    "moreover , their estimation strategy is limited to the lasso proposed by tibshirani ( 1996 ) , and they do not consider the possibility of using other folded - concave penalties , like the smoothly clipped absolute deviation ( scad ) penalty introduced by fan and li ( 2001 ) , or minimax concave penalty ( mcp ) by zhang ( 2010 ) , notwithstanding the possible model selection inconsistency of the lasso .    in the first half of this paper , we provide the comprehensive properties of the penalized regression estimator under suitable conditions for macroeconometrics .",
    "in fact , the theoretical aspects have been explored by many recent works on statistics ; see blmann and van de geer ( 2011 ) , fan and lv ( 2011 ) , and fan and lv ( 2013 ) , and the references therein .",
    "however , it is not sufficient for time series econometrics since the theories have been derived under somewhat stringent conditions such as an i.i.d .",
    "assumption and even deterministic covariates . specifically , we prove an upper bound of the prediction error called the _ oracle inequality_. this ensures that the forecasting value is reliable in the asymptotic sense .",
    "likewise , we also show the estimation precision of the regression coefficient and the model selection consistency known as the _ oracle property _ ; i.e. , it selects the correct subset of predictors and estimates the non - zero coefficients as efficiently as could be possible if we knew which variables were irrelevant .",
    "the oracle property provides another insight into the modeling of the variable of interest . in this regard",
    ", models can be selected by information criteria , like the aic and bic .",
    "these have become popular due to their tractability , however , they are limited when dealing with high - dimensional models since they demand an exhaustive search over all submodels . in contrast , a penalized regression with a scad - type penalty yields simultaneous estimation and model selection even in the ultrahigh - dimensional case .    in the second half of the paper , we shed light on the validity of sparse modeling in macroeconometrics by forecasting quarterly u.s .  real gdp growth using midas regression",
    "the regressor matrix is made from skip - sampled 124 monthly macroeconomic time series from the fred - md database provided by mccracken and ng ( 2015 ) , leading to the estimation of 1,117 parameters from 157218 observations .",
    "although the dimension is much bigger than the sample size , we can estimate the model and forecast u.s .",
    "gdp growth by penalized regression .",
    "we find that the forecasting performance is remarkably better than that of factor - midas , which is widely recognized as the most effective tool for forecasting with a large number of mixed frequency data . at the same time",
    ", we focus on what are  key \" predictors for forecasting u.s . gdp growth . unlike the factor - midas",
    ", we can interpret the estimated coefficients from our model since the penalized regression with folded - concave penalties excluding lasso has the oracle property .",
    "here , we find that five predictors ( real consumption expenditure , industrial production , all employees in total non - farm , s&p dividend yield , and federal funds rate ) are the most effective key predictors for forecasting u.s . gdp .",
    "it is noteworthy that the method is not limited to midas regression but applicable to a wide range of stationary time series regression models with a ( very ) large number of regressors .",
    "the remainder of the paper is organized as follows .",
    "section [ sec2 ] specifies an ultrahigh - dimensional time series regression model and the estimation scheme .",
    "the statistical validity of the method is confirmed in section [ sec3 ] by deriving the oracle inequality and the oracle property .",
    "section [ p - midas ] illustrates how we can apply the penalized regression model described in sections [ sec2 ] and [ sec3 ] for macroeconomic time series through the midas regression . in section [ sec5 ]",
    ", we forecast quarterly real u.s .",
    "gdp with a large number of monthly macroeconomic predictors using the p - midas and investigate how well p - midas works in macroeconometric analysis .",
    "section [ sec6 ] concludes .",
    "the proofs are collected in appendix .",
    "* notation . * for some vector @xmath0 and matrix @xmath1 , the @xmath2th and @xmath3th elements are written as @xmath4 and @xmath5 , respectively . the @xmath6th column ( @xmath2th row ) vector of @xmath1",
    "is similarly denoted as @xmath7 ( @xmath8 ) . @xmath9 and @xmath10 mean the minimum and maximum eigenvalues of @xmath1 , respectively .",
    "@xmath11 is the @xmath12-norm , meaning that @xmath13 gives the manhattan distance , and @xmath14 becomes the euclidean distance .",
    "@xmath15 is the largest element of @xmath0 in modulus .",
    "@xmath16 represents the spectral norm , that is , a square root of @xmath17 .",
    "@xmath18 refers to the operator norm induced by @xmath15 , or the largest absolute row sum .",
    "the regression model to be considered is @xmath19 where @xmath20 is a stationary covariate matrix composed of @xmath21 predictors with a finite second moment , @xmath22 is an error vector such that @xmath23 is a martingale difference sequence with respect to the @xmath24-field @xmath25 generated by @xmath26 , and @xmath27 is a @xmath21-dimensional parameter vector to be estimated .",
    "we denote by @xmath28 the @xmath29 matrix with @xmath30 being the @xmath21-vector given by @xmath31 and @xmath32 being the @xmath33-vector .",
    "we also denote by @xmath34 the @xmath33-vector given by @xmath35 with a slight abuse of notation .",
    "the objective is how to construct an efficient forecasting value of @xmath36 with an interpretable way of estimation when dimension @xmath21 is possibly much larger than @xmath33 . in such cases ,",
    "the matrix of predictors @xmath20 may contain many irrelevant columns , and the parameter vector @xmath27 should be assumed to be sparse ; that is , @xmath27 is filled with many zeros . to make the notation clear",
    ", the parameter vector @xmath37 is assumed to be decomposed into two subvectors .",
    "we denote by @xmath38 the set of indices @xmath39 and by @xmath40 the @xmath41-dimensional vector composed of nonzero elements @xmath42 . similarly ,",
    "letting @xmath43 , we denote by @xmath44 the @xmath45-dimensional zero vector . without loss of generality ,",
    "the vector is stacked as @xmath46 .",
    "further , let @xmath47 denote a corresponding decomposition .    in this paper",
    ", we consider an _ ultrahigh - dimensional _ case , meaning that @xmath21 diverges sub - exponantially ( non - polynomially ) ; see assumption [ assporder ] in the next section . at the same time , @xmath41 may also diverge , but @xmath48 must hold . in section [ p - midas ] , we will consider the forecasting regression model with mixed frequency data , where the model can be deemed an ultrahigh - dimensional model .    the estimation procedure should select a relevant model as well as",
    "consistently estimate the intrinsic parameter vector @xmath40 .",
    "this can be possible if we consider the _",
    "penalized regression_. the estimator @xmath49 is defined as the minimizer of the objective function @xmath50 over @xmath51 , where @xmath52 and @xmath53 , @xmath54 , is a penalty function indexed by a regularization parameter @xmath55 that converges to zero as @xmath33 tends to infinity .",
    "the penalty function @xmath56 takes such forms as the @xmath57-penalty ( lasso ) by tibshirani ( 1996 ) , scad penalty by fan and li ( 2001 ) , and mcp by zhang ( 2010 ) .",
    "these penalties belong to a family of so - called _ folded - concave penalties _ due to their functional forms ; see appendix [ subsecpen ] for their definition and figure [ mcpshape ] for an illustration .",
    "in this section , we establish two important theoretical results , the _ oracle inequality _ for a predicted value @xmath58 and the _ oracle property _ for the estimated coefficient @xmath49 .",
    "the existing results have been derived under i.i.d .",
    "conditions , but in the present paper we extend them to the results applicable for time series models .",
    "the oracle inequality gives an optimal upper - bound of the prediction error @xmath59 in the sense that the error is of the same order of magnitude as the prediction error we would have if we a priori knew the relevant variables ( blmann and van de geer ( 2011 ) ) .",
    "this result strongly supports the use of penalized regressions in terms of forecasting accuracy even in ultrahigh - dimensional spaces .",
    "meanwhile , we should remark that the inequality provides no information about model selection consistency @xmath60 ; that is , it is not clear whether the penalized regression correctly distinguishes the relevant variables contained in the true model from the irrelevant ones .",
    "this issue is then addressed by the establishment of the oracle property , which in turn states that @xmath49 exhibits model selection consistency . as long as a scad - type penalty",
    "is employed , a stronger result holds ; the estimator is asymptotically equivalent to the estimator that is obtained under the correct zero restrictions .",
    "it is a noteworthy that the lasso is difficult to satisfy a key condition for model selection consistency ; see section [ noteonlasso ] .    to derive these results",
    ", we make the following assumptions throughout the paper .",
    "[ assporder ] dimensionality satisfies @xmath61 for some constant @xmath62 .",
    "[ asspenalty ] penalty function @xmath53 is increasing and concave in @xmath63 with @xmath64 , and has a continuous derivative @xmath65 with @xmath66 .",
    "assumption [ assporder ] means that the dimensionality of the model , @xmath21 , diverges sub - exponentially as @xmath33 goes to infinity .",
    "this may be appropriate if we consider a regression under the midas setting , for instance .",
    "assumption [ asspenalty ] determines a family of folded - concave penalties that bridges @xmath67- and @xmath57-penalties and has been used in many articles , including lv and fan ( 2009 ) , fan and lv ( 2011 ) and fan and lv ( 2013 ) .",
    "the @xmath57-penalty satisfies this condition as the boundary of this class .",
    "we define the gradient vector and hessian matrix of @xmath68 as @xmath69 and @xmath70 , respectively .",
    "if we let @xmath71 , then we may write @xmath72      we derive an optimal bound for forecasting accuracy called the oracle inequality . in the literature , blmann and van de geer ( 2011 , ch .",
    "6 ) presented a complete guide for the inequality using the @xmath57-penalty .",
    "we extend the result to two directions .",
    "first , the inequality holds for stationary model ( [ model2 ] ) .",
    "it is found that penalized regression is a powerful tool for time series prediction in an ultrahigh dimension .",
    "second , we prove the asymptotic equivalence of @xmath57- and the other folded - concave penalties characterized by assumption [ asspenalty ] in the sense that they satisfy the same inequality .",
    "this indicates that the forecasting performance is asymptotically equivalent whatever a folded - concave penalty is used .",
    "[ assevent]for @xmath73 , the complement of event @xmath74 satisfies @xmath75 for some @xmath76 that can be sufficiently large for a large enough @xmath77 .",
    "[ asspenalty_add ] there exist a concave function @xmath78 and constant @xmath79 such that @xmath80 and @xmath81 hold .",
    "[ asscompatible ] for some @xmath82 and any @xmath51 such that @xmath83 , it holds that @xmath84 .",
    "assumption [ assevent ] was employed by bickel et al .",
    "( 2009 ) for the gaussian error , and by fan and lv ( 2013 ) for the case where the event occurs with high probability under the i.i.d .  assumption .",
    "we further study when it occurs with dependent errors and predictors ; see appendix [ subsecevent ] for the sufficient conditions .",
    "assumption [ asspenalty_add ] is new and is necessary to fill the gap between the @xmath57- and the other folded - concave penalties . in other words , a penalty with",
    "this condition arrives at the theory of lasso .",
    "the @xmath57-penalty obviously satisfies the condition .",
    "furthermore , it is easy to see that the scad and mcp as well as the @xmath57-penalty have such a decomposition .",
    "for example , the mcp is decomposed as @xmath85 , where @xmath86 a sufficient condition for the inequality of assumption [ asspenalty_add ] is @xmath87 since scad - type penalized estimators exhibit model selection consistency , @xmath88 for asymptotically all @xmath89 , and @xmath90 can be made large , this condition seems mild .",
    "assumption [ asscompatible ] is called the _ compatibility condition _ and",
    "is thoroughly examined by blmann and van de geer ( 2011 , ch .",
    "this condition is essentially similar to the bounded minimum eigenvalue condition from blow for @xmath91 , but is weaker as it restricts @xmath92 to the class that satisfies @xmath83 .",
    "the oracle inequality is derived in the next theorem .",
    "[ oracleineq ] let assumptions [ assporder][asscompatible ] hold .",
    "then , there exists a local minimizer @xmath49 of @xmath93 in ( [ obj ] ) such that with probability at least @xmath94 ,    * ( prediction loss ) @xmath95 . *",
    "( rate of convergence ) @xmath96",
    ".    fan and lv ( 2013 , theorems 1 and 2 ) proved similar and stronger results in terms of the rates for i.i.d .",
    "models . however , they reached the inequality via proving the model selection consistency while imposing a somewhat strong condition on the minimum nonzero coefficient , @xmath97 , for some positive constant @xmath98 satisfying @xmath99 .",
    "this assumption is pointed out to be restrictive in dealing with time series models as the effective coefficients of lagged variables may be close to zero even though they are effective ; see elliott et al .",
    "( 2013 ) , for instance . here",
    ", we do not take this approach to keep the conditions mild .",
    "result ( a ) exhibits an optimal bound for the prediction loss in the @xmath100-norm in the sense of bickel et al .",
    "( 2009 ) . from this result",
    ", it is justified to use any penalty function specified by assumption [ asspenalty ] when the aim is forecasting in the ultrahigh dimension . to understand the result",
    ", we consider a simplification in model ( [ model2 ] ) such that @xmath20 is deterministic , @xmath101 is i.i.d .  with a unit variance , and @xmath102 .",
    "then , the ols estimator is @xmath103 , and the squared risk becomes @xmath104 consider the case @xmath105 . if we were to know the true model @xmath38 , we could choose the correct @xmath41 variables from @xmath20 .",
    "then , the risk is @xmath106 .",
    "since @xmath38 is unknown , however , the additional factor @xmath107 , which is regarded as the price to pay for not knowing @xmath38 , is inserted . result ( b ) gives the consistency of the estimator , but is not so informative ; this does not account for any property of model selection consistency .",
    "we investigate the issue in the next two subsections .",
    "as far as forecasting is concerned , theorem [ oracleineq ] shows that the resulting performance does not depend on the choice of penalties .",
    "however , if we wish to know what variables should be selected , the situation changes . in this subsection",
    ", we argue that a key assumption for model selection consistency for the @xmath57-penalty ( lasso ) does not hold quite often especially under midas while a scad - type penalty does .",
    "zhao and yu ( 2006 ) studied a concept called sign consistency defined by @xmath108 , which is stronger than model selection consistency . under a deterministic covariate assumption , they show that the _ weak irrepresentable condition _",
    "@xmath109 is necessary for the sign consistency of lasso . to establish the model selection consistency of lasso",
    ", we usually need a stronger condition @xmath110 which was supposed by fan and lv ( 2011 ) .",
    "it seems difficult to prove model selection consistency for the lasso without this condition ; however , the condition may be easily violated .",
    "let @xmath111 , @xmath112 , be a column vector of @xmath113 .",
    "then , the left - hand side of the bound is @xmath114 where @xmath115 is regarded as the ols estimator of regression of an irrelevant variable @xmath116 on important variables @xmath117 . due to stationarity",
    ", this is @xmath118 provided that the regularity conditions for an asymptotic theory are satisfied .",
    "even when @xmath41 is finite , it is unrealistic for this value to be strictly bounded by one since macroeconomic data have cross - sectional dependence in general .",
    "when lagged variables are included in @xmath20 , the condition becomes more tight because @xmath38 and @xmath119 may share the same variable .",
    "this is truly emphasized when we consider midas regression because it inherently has a large number of autoregressive covariates .      as described in the previous subsection",
    ", the capacity of the lasso for model selection is quite limited .",
    "if we employ a scad - type penalty , however , a stronger and more desirable result can be obtained .",
    "this result is called the oracle property , as studied first by fan and li ( 2001 ) .",
    "the property admits @xmath120 to be asymptotically equivalent to the maximum likelihood estimate that is obtained under correct restriction @xmath121 . to derive it",
    ", we need a different set of conditions .",
    "define @xmath122 and @xmath123 .",
    "furthermore , set @xmath124 \\right):={\\mathop{\\rm e}\\nolimits}[\\v{u}\\v{u}^\\top | \\mathcal{f}_t]$ ] and @xmath125 $ ] .",
    "[ assevent2 ] the complement of event @xmath126 satisfies @xmath127 .",
    "[ asspen ] @xmath128 and @xmath129 for a sufficiently large @xmath33 .",
    "[ asslik0]for all @xmath33 , @xmath130 with high probability for a ( small ) constant @xmath131 .    [ asslik ] @xmath132 for a ( small ) constant @xmath133 .",
    "[ assliklln ] the hessian matrix admits a law of large numbers , @xmath134 .",
    "[ asslik_submat ] @xmath135 .",
    "the role of assumption [ assevent2 ] is the same as that of assumption [ assevent ] .",
    "the first condition in assumption [ asspen ] determines the convergence rates of @xmath136 and @xmath137 , and is a variant of the so - called _ beta - min condition _ in blmann and van de geer ( 2011 , ch .",
    "though it is stringent as mentioned before , this is necessary to distinguish the nonzero coefficient of relevant variables from zero .",
    "the second condition @xmath129 is key to achieve the oracle property .",
    "this is strong enough to exclude the @xmath57-penalty , which is on a boundary of assumption [ asspenalty ] .",
    "in fact , for the @xmath57-penalty , @xmath138 holds identically for all @xmath139 . for the scad and mcp",
    ", on the other hand , this holds for a sufficiently large @xmath33 as long as @xmath140 in the first condition is satisfied . in this sense",
    ", the class of the scad - type penalties is characterized by this assumption .",
    "assumptions [ asslik0][assliklln ] seem quite natural and are frequently used in stationary time series analysis .",
    "note that assumption [ asslik0 ] permits heteroskedastic disturbances , such as an arch disturbance .",
    "assumption [ asslik_submat ] restricts the asymptotic behavior of the lower - left @xmath141 submatrix of @xmath91 .",
    "this is essentially the same as condition ( 27 ) of fan and lv ( 2011 ) , but has a milder rate in exchange for the strong restriction on the class of penalties in assumption [ asspen ] .",
    "the next assumption is required to obtain asymptotic normality of the estimator in addition to the assumptions above .",
    "let @xmath142 denote the information matrix of the true model defined by @xmath143 $ ] , which is also written as @xmath144 $ ] by the tower property for conditional expectations .",
    "letting @xmath145 be such that @xmath146 , we further set @xmath147 and @xmath148 , which can easily be found to be a martingale difference sequence and martingale difference array , respectively .",
    "note that @xmath149 can also be written as @xmath150 .",
    "[ asslikasyn ] @xmath151 for some constant @xmath152 .    by davidson ( 1994 , ch .",
    "24 ) , this leads to a central limit theorem of a martingale difference sequence .",
    "if @xmath153 is ergodic stationary , the assumption is redundant by the result of billingsley ( 1961 ) .",
    "[ exist ] let assumptions [ assporder ] , [ asspenalty ] , and [ assevent2][asslik_submat ] hold .",
    "then , there exists a local minimizer @xmath154 of @xmath93 such that    * ( sparsity ) @xmath155 with probability approaching one ; * ( rate of convergence ) @xmath156 .",
    "in addition , suppose assumption [ asslikasyn ] holds .",
    "then , for any vector @xmath157 that satisfies @xmath158 , we have    * ( asymptotic normality ) @xmath159 .",
    "the oracle property means that the model selection is consistent in the sense that results ( a ) and ( b ) hold , which give richer information than result ( b ) of theorem [ oracleineq ] .",
    "moreover , as is understood by result ( c ) in addition to results ( a ) and ( b ) , the estimator has the same asymptotic efficiency as the ( infeasible ) mle obtained with advance knowledge of the true submodel .",
    "thanks to these results , we can estimate ultrahigh - dimensional models without irksome tests for zero restrictions on the parameters or exhaustive search by information criteria .",
    "in this section , we illustrate how we can apply the penalized regression model described in sections [ sec2 ] and [ sec3 ] for macroeconomic time series through the midas regression .",
    "the midas regression is now globally known as one of the most effective tools for forecasting ( especially nowcasting ) with mixed - frequency data .",
    "the midas regression has an advantage in describing a forecasting regression model in a simple and parsimonious way with a distributed lag structure of a few hyperparameters .",
    "however , the ",
    "basic \" midas regression would not be suitable for a situation where the number of predictors in the model is very large .",
    "for example , consider the midas regression model with @xmath160 hyperparameters and @xmath161 macroeconomic time series .",
    "then , the total number of parameters in the midas regression model remains @xmath162 including the constant term .",
    "thus , it invokes a serious efficiency loss if @xmath163 is large or even it makes the model inestimable if @xmath164 as in a standard linear regression model . on the other hand , the findings from section [ sec3 ] reveal that if we consider the midas regression model with a penalty function @xmath165 instead of the distributed lag structure , we can forecast and estimate the regression model with a large number of mixed - frequency predictors .",
    "furthermore , it has desirable properties : oracle inequality and oracle property .",
    "let @xmath166 be the real - valued midas process in line with andreou et al .",
    "( 2010b ) , where the scalar @xmath167 is the low - frequency variable observed at @xmath168 , and the @xmath161-dimensional vector @xmath169 is a set of higher - frequency variables observed @xmath170 times between @xmath171 and @xmath172 .",
    "for example , @xmath173 if we forecast quarterly variable with monthly predictors .",
    "we consider the following @xmath174-step ahead midas forecasting regression model @xmath175 where @xmath176 is a martingale difference sequence with respect to the @xmath24-field @xmath25 generated by @xmath177 with @xmath178 , @xmath179 with @xmath180 for @xmath181 and @xmath182 is the ( true ) parameter vector .",
    "note that we  nowcast \" @xmath183 if @xmath184 , in the sense that we forecast a low - frequency variable with the  latest \" high - frequency variables that are released between @xmath172 and @xmath171 : for example , if we consider a quarterly / monthly ( @xmath185 ) case , @xmath186 means that we forecast a quarterly variable in 2015q2 with monthly macroeconomic variables in june ( may ) 2015 or later .",
    "note that model ( [ midasmodel ] ) is the same as ( [ model2 ] ) with @xmath187 but differs from the basic midas regression model with distributed lag structure proposed by ghysels et al .",
    "( 2004 , 2006 , 2007 ) .",
    "ghysels et al .",
    "( 2004 , 2006 , 2007 ) employ @xmath188 instead of @xmath189 such that @xmath190 for @xmath191 where @xmath192 and @xmath193 to reduce @xmath21 to @xmath194 and make the model estimable .",
    "however , forecasting with the basic midas crucially depends on a distributed lag structure and this seems restrictive .",
    "moreover , the basic midas can not reduce @xmath21 effectively if @xmath161 is large as mentioned above .",
    "alternatively , we consider the penalized midas regression that minimizes @xmath195 so that we can directly estimate @xmath196 essentially only with the assumption of sparsity . since all monthly variables including their lags are not necessarily relevant predictors when we consider a large number of regressors , the sparsity assumption makes sense in macroeconomic forecasting literature .",
    "hereafter , we call the midas regression model with the penalty function  penalized midas regression \" and abbreviate it as  p - midas . \"",
    "according to the theoretical results given in the previous sections , p - midas has two desirable properties : the oracle inequality that exhibits an optimal bound for the prediction loss , and the oracle property that assures model selection consistency . in this section , we forecast quarterly real u.s .",
    "gdp with a large number of monthly macroeconomic predictors using the p - midas and investigate how well p - midas works in macroeconometric analysis .      the data of u.s .",
    "quarterly real gdp growth is from the fred database and the sample period is from 1959q4 to 2014q2 .",
    "we retrieve 124 u.s .",
    "monthly macroeconomic time series from the fred  md database provided by mccracken and ng ( 2015 ) and the series are appropriately differenced according to a guideline in mccracken and ng ( 2015 ) .",
    "note that although the fred  md database originally contains a total of 134 series , we remove 10 series .",
    "this is because 9 series ( umcsentx , twexmmth , andenox , acogno , permit , permitne , permitmw , permits , permitw ) have no observations from 1959 and our preliminary inspection found that 1 series ( nonborres ) contained extreme changes ( 283,995 times larger than its mean ) in february 2008 , which would contaminate our analysis .",
    "the sample period of the monthly series is from march , 1959 ( 1959:3 ) to july , 2014 ( 2014:7 ) .",
    "we evaluate the out - of - sample forecasting performance by mean squared forecast errors ( msfe ) in the evaluation period from 2000q1 to 2014q2 .",
    "the parameter estimates are obtained from each estimation period ; the initial period is 1959q41999q4 and the next one extends the end point to 2000q1 with the starting point 1959q4 being fixed . for example , the initial forecast error in 2002q1 is calculated using the estimates from the initial estimation period 1959q41999q4 , and the second forecast error in 2000q2 using the estimates from the second estimation period 1959q42000q1 . as a consequence ,",
    "we calculate the msfe from a total of @xmath197 squared errors .",
    "we suppose that the forecast regression consists of 9 lags ( @xmath198 ) , so that the total number of parameters for the forecasting regression to be estimated is @xmath199 , including a constant term .    to investigate the forecasting performance of the p - midas model with a variety of horizons , we examine the cases where @xmath200 in the same manner as clements and galvo ( 2008 ) and marcellino and schumacher ( 2010 ) . as mentioned in section [ p - midas ] , the cases @xmath201 and @xmath202 correspond to nowcasting in the sense that we forecast contemporaneous or very short - forecast - horizon quarterly gdp growth using monthly series before the official announcement of the gdp , while the case @xmath203 is a forecast with a relatively long horizon .",
    "note that the sample size of the estimation period @xmath33 gradually increases and varies depending on @xmath204 : for example , @xmath33 ranges from 161 to 218 if @xmath205 , and from 159 to 216 if @xmath206 .",
    "finally , we need to determine the values of tuning parameters , @xmath207 and @xmath137 , in advance of the p - midas regression . in the literature , @xmath208 has been frequently used for the scad , which was proposed by fan and li ( 2001 ) in case of the dimension being not greater relative to the sample size . however",
    ", a larger @xmath207 should be used when dealing with high - dimensional models as pointed out by breheny and huang ( 2011 , pp .",
    "19 and 21 ) .",
    "following their guidelines , we set @xmath209 for the scad and mcp based on our preliminary inspection with the whole samples , although the performance could be improved by a more careful choice in response to each estimation .",
    "the value of @xmath137 is selected by 10-fold cross - validation .",
    "the theoretical validity of this method for models with dependence is not clear as far as we know , but we confirmed by simulation that cross - validation worked well even in the dependent case as well as the i.i.d .",
    "case ; see uematsu and tanaka ( 2015 ) for details .",
    "another possibility of choosing @xmath137 is to rely on information criteria as fan and tang ( 2013 ) investigate .",
    "however , they focus on the i.i.d .",
    "case and the validity is also unknown under the dependent data . all estimations regarding penalized regression are conducted by r 3.2.1 with the ncvreg package of breheny and huang ( 2011 ) .",
    "we explore the empirical results from two aspects : ( @xmath2 ) the forecasting performances of the gdp in section [ forecastperformance ] and ( @xmath210 ) the variable selection in the estimated models in section [ variableselection ] .",
    "these correspond to the theories explored in sections [ subsecoracleineq ] and [ subsecoracle ] , respectively .      to measure the forecast performances appropriately",
    ", we consider the following three evaluation periods : ( @xmath2 ) overall ( 2000q12014q2 ) , ( @xmath210 ) 1st subsample ( 2000q12007q4 ) , and ( @xmath211 ) 2nd subsample ( 2008q12014q2 ) .",
    "this is because the unprecedented turmoil of the u.s .",
    "economy stemming from the subprime mortgage crisis and the ensuing collapse of lehman brothers in 2008 would introduce parameter instability that distorts the forecast evaluation .",
    "tables [ mseall][mse2 ] show the msfes of p - midas with the scad , mcp , and lasso penalties in the overall sample , 1st subsample , and 2nd subsample , respectively , as well as those of the ( basic ) f - midas proposed by marcellino and schumatcher ( 2010 ) .",
    "the f - midas estimates are obtained by the exponential almon lag structure with two hyperparameters .",
    "the number of factors to be extracted from 124 monthly series is assumed to be 7 since the information criterion @xmath212 proposed by bai and ng ( 2002 ) frequently selects 7 in the estimation periods .",
    "the values of tables are _ relative _ msfes compared to naive ar(4 ) gdp forecasts , such that a value less than 1 means that the corresponding forecasting method is more efficient than the ar(4 ) forecast .",
    "furthermore , the values in tables are highlighted in bold if the corresponding forecasting method has minimum msfe among others including the ar(4 ) forecast .",
    "first let us consider the nowcasting ( @xmath213 ) cases .",
    "tables [ mseall][mse2 ] yield that p - midas is much better than the naive ar(4 ) forecast and outperforms the f - midas with a few exceptions .",
    "in particular , it appears that the scad works better than the other penalties when the forecast horizon is 0 and 1/3 while mcp is better when @xmath214 .",
    "next , we turn to the forecast performance when @xmath215 .",
    "we confirm that here too , p - midas performs best when @xmath216 .",
    "however , we also find that relative efficiency becomes small as compared to in the nowcasting cases . on the other hand ,",
    "when the forecast horizon is longer than 1 quarter , the p - midas as well as the f - midas are beaten by ar(4 ) forecast .",
    "however , the results are not surprising because clements and galvo ( 2008 ) and marcellino and schumatcher ( 2010 ) also find that forecasts with midas are frequently beaten by naive forecasts when @xmath217 .    hence , our results show that p ",
    "midas has good forecast performance in a very short horizon , although it is not necessarily a primary tool for a forecast with a relatively long horizon .",
    "however , we can conclude that p - midas is definitely an effective tool since our main interest in forecasting with mixed - frequency data is the performance for nowcasting where low - frequency data is not available .      according to theorem [ exist ]  ( oracle property )",
    ", we expect that the p - midas estimates corresponding to irrelevant predictors with the scad - type penalties are _ exactly _ zero even when @xmath33 is finite but tolerably large .",
    "this means that we can use p - midas to identify the  key \" predictors for forecasting u.s .  gdp growth .",
    "then , we reestimate p - midas model and examine what are the key predictors .",
    "tables [ coeffigmcp][coeffiglasso ] give the series that have non - zero estimates among their leads and lags in @xmath218 , for the mcp , scad , and lasso , respectively . the estimation period is 1959q42014q2 .",
    "the  cells \" painted blue indicate that the corresponding non - zero values are _ positive _ while those filled red denote _ negative _ non - zero estimates .",
    "color intensity corresponds to how the estimates are distant from zero : deep - color cells are effective key predictors .",
    "we have the following three findings from tables [ coeffigmcp][coeffiglasso ] .",
    "first , the mcp , scad , and lasso select 26 , 31 , and 33 series out of 124 as the key predictors for u.s .",
    "gdp forecasting , respectively .",
    "that is , they evaluate a large amount ( 7580% ) of the series in the fred  md database as unimportant for u.s .  gdp forecasting .",
    "however , it should be mentioned that the lasso probably overestimates the number of key predictors since the irrepresentable condition is hard to satisfy under the lasso as we saw in section [ noteonlasso ] , while the scad - type penalties do not suffer from this problem .",
    "second , the monthly predictors of @xmath219 tend to be much more effective for forecasting gdp than those of @xmath220 as a whole .",
    "this is consistent with our intuition because we can interpret quarterly gdp as a composition of monthly series by construction so that movements of the monthly series in most recent months mainly contribute to those of the gdp in a recent quarter .",
    "this encourages us to apply nowcasting for low - frequency series forecasts .",
    "third , estimated coefficients far away from zero have almost the same value regardless of what penalty is chosen . in particular , our estimation results say that real personal consumption expenditure ( dpcera3m086sbea ) , industrial production index ( indpro ) , all employees in total non - farm ( payems ) , ism new orders index ( napmnoi ) , s&p dividend yield ratio ( s&p div yield ) , and federal funds rate ( fedfunds ) are the most important key predictors for forecasting u.s .",
    "gdp : pce , ip , new orders index , and all employees in total non - farm have strong positive signs : this is plausible since they are definitely proportional to the business cycles . the negative sign of the s&p div yield is somewhat controvertible , but this may be due to the fact that the stock price in the denominator of the s&p div yield positively relates to u.s . gdp . the strong negative sign in 9th lag of fedfunds is convincing and interesting because it can be interpreted as causality of the fed funds to u.s .",
    "gdp .      the findings from sections [ forecastperformance ] and [ variableselection ]",
    "motivate us to consider the following two - step forecasting procedure .",
    "first , we estimate the forecast model with a penalized regression and detect the regressors that have non - zero coefficients .",
    "second , we regress the gdp on the selected regressors by ols and obtain the forecast value . note that the two - step procedure using lasso is known as the _ ols post - lasso _ : it is proposed by belloni and chernozhukov ( 2013 ) .",
    "they show that a forecast through ols post - lasso can perform at least as well as the lasso and can be better in some cases . however , the result holds asymptotically and comes from quite restricted assumptions for macroeconometrics such as fixed regressors and i.i.d .  normal errors .",
    "thus , it is interesting to investigate whether the result holds under a finite sample situation with dependence .",
    "tables [ mseallzero][mse2zero ] show the msfes of the two - step procedure as well as those of p - midas when @xmath221 and @xmath222 , in the same manner as tables [ mseall][mse2 ] . from these tables , we find that the mses of the two - step procedure are worse than those of p - midas overall when @xmath205 or @xmath223 while they outperform p - midas when @xmath224 of @xmath225 .",
    "however , it seems that the good performance of the two - step procedure mainly comes from the 2nd subsample , implying that the difference between p - midas and the two - step procedure is quite small even when @xmath226 or @xmath222 .",
    "hence , the results inform us that the two - step procedure does not provide effective efficiency gains in our situation . a probable reason",
    "is that the total number of regressors in the second - step ols regression does not become effectively small when we assume a long - length lag structure in the model even if variable  screening \" is conducted in the first step .",
    "this would make the efficiency losses arising from estimating many parameters more serious than estimating p - midas directly .",
    "we have studied macroeconomic forecasting and variable selection by a folded - concave penalized regression with a very large number of predictors .",
    "the contributions include both theoretical and empirical results .",
    "the first half of the paper developed the theory for a folded - concave penalized regression when the model exhibits time series dependences .",
    "specifically , we have proved the oracle inequality and the oracle property under appropriate conditions for macroeconomic time series . in the latter half",
    ", the validity of the method has been confirmed through the forecasting of quarterly u.s .  real gdp growth with u.s .",
    "monthly macroeconomic time series from the fred  md database .",
    "we called this kind of mixed - frequency forecasting model penalized midas regression model ; it consisted of more than 1100 monthly covariates while the sample size was much smaller than the total number of covariates .",
    "the performance was remarkably good in comparison to the factor - based method ( f - midas ) proposed by marcellino and schumacher ( 2010 ) though the sample size was much smaller than the total number of parameters .",
    "furthermore , the penalized midas regression enabled us to identify what was the key predictors for forecasting u.s .",
    "we found that around 2025% of monthly series in the fred  md database were key predictors ; specifically , the real pce , ip , all employees in total non - farm , new orders index , s&p dividend yield , and fed funds rate were the most effective ones .",
    "we are grateful to kohei aono , akiyuki tonogi , yohei yamamoto , takahide yanagi and seminar participants at 6th seminar on time series and financial engineering for their valuable comments .",
    "uematsu acknowledges financial supports from a grant - in - aid for jsps fellows , 26 - 1905 .",
    "tanaka acknowledges financial support from joint usage and research center , institute of economic research , hitotsubashi university .",
    "all remaining errors are our own .",
    "* the @xmath227-penalty is given by @xmath228 , and we then obtain @xmath229 and @xmath230 . *",
    "the scad penalty is defined by @xmath231 its derivative is @xmath232 for some @xmath233 .",
    "then we have @xmath234 . *",
    "the mcp is defined by @xmath235 its derivative is @xmath236 for some @xmath237 .",
    "thus , we have @xmath238 .",
    "recall that @xmath239 .",
    "it follows from the bonferroni inequality that @xmath240 we consider two cases to bound the probability . in both cases , we assume that @xmath241 is for every @xmath6 a martingale difference sequence .",
    "this assumption is mild and is frequently used in time series analysis . in fact , it is clearly satisfied in our model .    *",
    "assume that @xmath242 a.s .  for all @xmath171 and for constants @xmath243 such that @xmath244 for some constant @xmath245 .",
    "then , by azuma - hoeffding s inequality , we have @xmath246 when @xmath247 , the bound becomes @xmath248 .",
    "thus ( [ bonineq ] ) is further bounded by @xmath249 , which goes to zero as long as @xmath250 is chosen large enough .",
    "* let @xmath251 $ ] and @xmath252 .",
    "assume that @xmath253 \\leq k !",
    "\\sigma_t^2 c_{p1}^{k-2}~~\\mbox{or}~~p(|x_{tj}u_t| \\leq c_{p1 } | \\mathcal{f}_{t-1})=1\\end{aligned}\\ ] ] for @xmath254 and for some @xmath255 .",
    "furthermore , assume that @xmath256 \\leq m$ ] with @xmath257 for some constant @xmath258 .",
    "then , by de la pea ( 1999 ) s inequality , we have @xmath259 when @xmath247 , the bound becomes @xmath260 .",
    "thus ( [ bonineq ] ) is further bounded by @xmath261 , which goes to zero as long as @xmath250 is chosen large enough .",
    "in lemma [ lemcharact ] below , let @xmath262 , a set of indices corresponding to all nonzero components of @xmath49 , and @xmath263 denote a subvector of @xmath49 formed by its restriction to @xmath264 .",
    "the other symbols are defined analogously .",
    "let @xmath265 denote the hadamard product .",
    "the sign function @xmath266 is applied coordinate - wise .",
    "define @xmath267 define the _ local concavity _ at @xmath268 with @xmath269 as @xmath270 .",
    "[ lemcharact ] suppose assumption [ asspenalty ] holds .",
    "then @xmath49 is a strict local minimizer of @xmath93 in ( [ obj ] ) if @xmath271 conversely , any local minimizer of @xmath93 must satisfy ( [ lem11 ] ) , ( [ lem12 ] ) , and ( [ lem13 ] ) with strict inequalities replaced by nonstrict ones .",
    "the proof was given by lv and fan ( 2009 , theorem 1 ) .",
    "consider the case where @xmath272 . under assumption [ asspen ]",
    ", it holds that @xmath273 for sufficiently large @xmath33 .",
    "thus , condition ( [ lem13 ] ) is satisfied as long as @xmath274 is bounded away from zero .",
    "* proof of theorem [ oracleineq ] * because @xmath49 minimizes @xmath93 , we have @xmath275 by model ( [ model2 ] ) and the property of norms , this can be rewritten and bounded as @xmath276 on the event @xmath277 , ( [ basic1 ] ) becomes @xmath278 where the equality holds from @xmath279 and assumption [ asspenalty ] .",
    "now we bound @xmath280 from below . by the triangle inequality",
    ", we have @xmath281 for the second term of the lower bound in ( [ bound1 ] ) , we have , by the mean value theorem and concavity of @xmath282 with assumption [ asspenalty ] , @xmath283 where @xmath284 lies between @xmath285 and @xmath286 . from ( [ bound1 ] ) and ( [ bound2 ] ) , we obtain @xmath287 plugging ( [ bound3 ] ) into ( [ basic2 ] ) and collecting terms , we obtain @xmath288 note that folded - concave penalties can be decomposed as @xmath289 , where @xmath290 is a concave function . by assumption [ asspenalty_add ] , we have @xmath291 for some positive constants satisfying @xmath292 . plugging ( [ bound4 ] ) into ( [ oracin1 ] ) and adding @xmath293 to the both sides",
    ", we get @xmath294 by assumption [ asscompatible ] and @xmath295 , ( [ oracin2 ] ) becomes @xmath296 implying the results .",
    "@xmath297      _ step 1 .",
    "_ we consider @xmath93 in the correctly constrained space @xmath298 , which is the @xmath41-dimensional subspace @xmath299 .",
    "the corresponding objective function is given by @xmath300 we now show the existence of a strict local minimizer @xmath301 of @xmath302 such that @xmath303 . to this end , it is sufficient to prove that , for a large constant @xmath304 , the event @xmath305 occurs with probability tending to one .",
    "this implies that , with probability tending to one , there is a local minimizer @xmath301 of @xmath302 in the ball @xmath306 .    by the definition of the objective function",
    ", we have @xmath307 first , we evaluate the two terms in ( [ q - q2 ] ) .",
    "the taylor expansion gives @xmath308 where @xmath309 lies between @xmath286 and @xmath310 , and the last inequality follows from the monotonicity of @xmath282 , @xmath311 , and the triangle inequality .",
    "eventually , the last term is zero by assumption [ asspen ] .",
    "next , we consider ( [ q - q1 ] ) . by the law of iterated expectations and",
    "assumption [ asslik0 ] , we have @xmath312 = t^{-2}{\\mathop{\\rm e}\\nolimits}[{\\mathop{\\rm e}\\nolimits}[{\\mathop{\\rm tr}\\nolimits}\\v{x}_a^\\top \\v{u}\\v{u}^\\top \\v{x}_a|\\mathcal{f}_t ] ] \\\\ & = t^{-2 } \\sum_{j \\in a } { \\mathop{\\rm e}\\nolimits } [ \\v{x}_j^\\top   \\v{\\sigma}_{ut } \\v{x}_j ] \\leq t^{-2 } c_u^{-1}\\sum_{j \\in",
    "a}{\\mathop{\\rm e}\\nolimits}\\|\\v{x}_j\\|_2 ^ 2 = o(q / t ) .",
    "\\end{aligned}\\ ] ] this together with the markov inequality implies that @xmath313 is @xmath314 .",
    "therefore , we obtain @xmath315 whereas , by assumptions [ asslik ] and [ assliklln ] , we get @xmath316 with high probability . because ( [ hhh ] ) dominates the other terms of @xmath317 when a large value of @xmath318 is taken , @xmath319 tends to positivity as @xmath33 grows large .",
    "thus , ( [ pr12 ] ) holds with probability approaching one .",
    "_ step 2 . _ to complete the proof of ( a ) and ( b ) , it remains to show that @xmath320 is indeed a strict local maximizer of @xmath93 in @xmath321 . from lemma [ lemcharact ] , it suffices to check conditions ( [ lem11 ] ) , ( [ lem12 ] ) , and ( [ lem13 ] ) with setting @xmath322 , but condition ( [ lem11 ] ) is clearly satisfied by assumptions [ asspen ] , [ asslik ] , [ assliklln ] , the monotonicity of @xmath282 , @xmath323 , and the argument above .",
    "in fact , we have @xmath324    we then check condition ( [ lem13 ] ) . by assumption [ asspen ] , we have @xmath325 , so that , for sufficiently large @xmath33 , @xmath326 implies @xmath327 . thus the condition is eventually satisfied by assumptions [ asslik ] and [ assliklln ] along with the comment after lemma [ lemcharact ] .    to verify ( [ lem12 ] ) , we work on event @xmath328 defined in assumption [ assevent2 ] . from assumption [ asslik_submat ] and @xmath329 in assumption [ asspenalty ]",
    ", we obtain @xmath330 thus , condition ( [ lem12 ] ) holds for a sufficiently large @xmath33 .",
    "this completes the proof of ( a ) and ( b ) by assumption [ assevent2 ] .    finally , we prove ( c ) .",
    "clearly we only need to show the asymptotic normality of @xmath120 .",
    "first wee see that from assumptions [ asslik0 ] and [ asslik ] @xmath331 \\right )   \\geq c_u \\lambda_{\\min } ( \\v{j}_{0aa } ) \\geq c_u c_j . \\end{aligned}\\",
    "] ] thus , @xmath142 is positive definite and @xmath332 is well - defined . on the event @xmath333 in ( [ pr12 ] ) , it has been shown that @xmath326 is a strict local minimizer of @xmath302 and @xmath334 .",
    "we thus obtain , for any vector @xmath335 such that @xmath158 , @xmath336 recall that @xmath337 and @xmath338 is a martingale difference array .",
    "we show the asymptotic normality of this part .",
    "it is not hard to say that @xmath339 assumption [ asslikasyn ] implies uniform integrability of @xmath340 .",
    "hence , by theorems 24.3 and 24.4 of davidson ( 1994 , ch .",
    "24 ) , we obtain @xmath341 .",
    "because the last term of ( [ focasyn ] ) is @xmath342 by the argument above , the result follows from the slutsky lemma and assumption [ assliklln ] .",
    "@xmath297    999 andreou , e. , e. ghysels and a. kourtellos ( 2010a ) , `` forecasting with mixed - frequency data , '' _ the oxford handbook of economic forecasting _",
    "( m. p. clements and d. f. hendry eds . ) , oxford university press .",
    "andreou , e. , e. ghysels and a. kourtellos ( 2010b ) , `` regression models with mixed sampling frequencies , '' _ journal of econometrics _ , * 158 * , 246261 .",
    "bai , j. and s. ng ( 2002 ) , `` determining the number of factors in approximate factor models , '' _ econometrica _ , * 70 * , 191221 .",
    "bai , j. and s. ng ( 2008 ) , `` forecasting economic time series using targeted predictors , '' _ journal of econometrics _ , * 146 * , 304317 .",
    "belloni , a. and v. chernozhukov ( 2013 ) , `` least squares after model selection in high - dimensional sparse models , '' _ bernoulli _ , * 19 * , 521547 .",
    "bickel p.  j. , y. ritov and a.  b. tsybakov ( 2010 ) , `` hierarchical selection of variables in sparse high - dimensional regression , '' _ borrowing strength : theory powering applications ",
    "a festschrift for lawrence d. brown .",
    "_ institute of mathematical statistics , 5669 .",
    "breheny , p. and j. huang ( 2011 ) , `` coordinate descent algorithm for nonconvex penalized regression , with applications to biological feature selection , '' _ annals of applied statistics _ , * 5 * , 232253 .",
    "clements , m. and a. b. galvo ( 2008 ) , ` macroeconomic forecasting with mixed - frequency data : forecasting output growth in the united states,' _ journal of business and economic statistics _ , * 26 * , 546554 .",
    "davidson , j. ( 1994 ) , _ stochastic limit theory _ , oxford university press .",
    "de la pea , v.  h. ( 1999 ) , `` a general class of exponential inequalities for martingales and ratios , '' _ annals of probability _ , * 27 * , 537564 .",
    "elliott , g. , a. gargano and a. timmermann ( 2013 ) , `` complete subset regressions , '' _ journal of econometrics _ ,",
    "* 177 * , 357373 .",
    "fan , j. and r. li ( 2001 ) , `` variable selection via nonconcave penalized likelihood and its oracle properties , '' _ journal of the american statistical association _ , * 96 * , 13481360 .",
    "fan , j. and j. lv ( 2011 ) , `` nonconcave penalized likelihood with np - dimensionality , '' _ ieee transactions on information theory _ , * 57 * , 54675484 .",
    "fan , j. , j. lv and l. qi ( 2011 ) , `` sparse high - dimensional models in economics , '' _ annual review of economics _ , * 3 * , 291317 .",
    "fan , y. and j. lv ( 2013 ) , `` asymptotic equivalence of regularization methods in thresholded parameter space , '' _ journal of the american statistical association _ , * 108 * , 10441061 .",
    "fan , y. and c.  y. tang ( 2013 ) , `` tuning parameter selection in high - dimensional penalized likelihood , '' _ journal of royal statistical society series b _ , * 75 * , 531552 .",
    "foroni , c. and m. marcellino ( 2013 ) , `` a survey of econometric methods for mixed - frequency data , '' _ norges bank working paper _ , * 201306*. ghysels , e. , p. santa - clara and r. valkanov ( 2004 ) , `` the midas touch : mixed data sampling regressions , '' mimeo .",
    "ghysels , e. , p. santa - clara and r. valkanov ( 2006 ) , `` predicting volatility : getting the most out of return data sampled at different frequencies , '' _ journal of econometrics _ , * 131 * , 5995 .",
    "ghysels , e. , a. sinko and r. valkanov ( 2007 ) , `` midas regressions : further results and new directions , '' _ econometric reviews _ , * 26 * , 5390 .",
    "lv , j. and y. fan ( 2009 ) , `` a unified approach to model selection and sparse recovery using regularized least squares , '' _ annals of statistics _ , * 37 * , 34983528 .",
    "marcellino , m.  h. and c. schumacher ( 2010 ) , `` factor midas for nowcasting and forecasting with ragged - edge data : a model comparison for german gdp , '' _ oxford bulletin of economics and statistics _ , * 72 * , 518550 .",
    "marsilli , c. ( 2014),``variable selection in predictive midas models , '' _ banque de france working paper _ , * 520*. mccracken , m. w. and s. ng ( 2015),``fred - md : a monthly database for macroeconomic research , '' _ federal reserve bank of st .",
    "louis working paper series _ , * 2015 - 012a*. tibshirani , r. ( 1996 ) , `` regression shrinkage and selection via the lasso , '' _ journal of royal statistical society series b _ , * 58 * , 267288 .",
    "uematsu , y. and s. tanaka ( 2015 ) , `` effect of dependent regressors on regularization parameter selection via cross - validation , '' mimeo .",
    "zhang , c.h .",
    "( 2010 ) , `` nearly unbiased variable selection under minimax concave penalty , '' _ annals of statistics _ ,",
    "* 38 * , 8949421 .",
    "zhao , p. and b. yu ( 2006 ) , `` on model selection consistency of lasso , '' _ journal of machine learning research _ ,",
    "* 7 * , 25412563 ."
  ],
  "abstract_text": [
    "<S> this paper studies macroeconomic forecasting and variable selection by a folded - concave penalized regression with a very large number of predictors . the penalized regression approach leads to sparse estimates of the regression coefficients and it is applicable even if the dimensionality of the model is much larger than the sample size . </S>",
    "<S> the first half of the paper discusses the theoretical aspects of a folded - concave penalized regression when the model exhibits time series dependences . </S>",
    "<S> specifically , we show the oracle inequality and the oracle property under mild conditions with time - dependent regressors . in the latter half </S>",
    "<S> , we show the validity of the penalized regression by forecasting u.s .  </S>",
    "<S> gdp growth with mixed frequency data . </S>",
    "<S> the model consists of more than 1,100 covariates in the mixed data sampling ( midas ) setting with the sample size at most being 220 . </S>",
    "<S> performance is remarkably good in comparison to the factor - midas proposed by marcellino and schumacher ( 2010 ) and we find the key predictors selected by the regression are quite conformable .    * keywords * : _ macroeconomic forecasting , variable selection , lasso , folded - concave penalty , ultrahigh - dimensional time series , mixed data sampling . _ + * jel classification * : c13 , c32 , c52 , c53 , c55 </S>"
  ]
}