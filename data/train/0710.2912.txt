{
  "article_text": [
    "the maxent method @xcite was designed to assign probabilities .",
    "this method has evolved to a more general method , the method of maximum ( relative ) entropy ( me ) @xcite which has the advantage of not only assigning probabilities but _ updating _ them when new information is given in the form of constraints on the family of allowed posteriors .",
    "the main purpose of this paper is to show both general and specific examples of how the me method can be applied using data and moment constraints .",
    "the two preeminent updating methods are the me method and bayes rule .",
    "the choice between the two methods has traditionally been dictated by the nature of the information being processed ( either constraints or observed data ) but questions about their compatibility are regularly raised .",
    "our first objective is to review how data is introduced into the me method .",
    "next we show a general example of updating with two different forms of information : moments and data .",
    "the solution resembles bayes rule .",
    "the difference between this solution and the traditional bayes form results from using the moment constraint .",
    "this constraint modifies the usual bayesian likelihood . in an effort to put some names to these pieces",
    "we will call the standard bayesian likelihood the _ likelihood _ and the part associated with the moment the _ likelihood modifier _ so that the product of the two yields the _ modified likelihood_. we extend this general example by solving a specific ill - behaved econometric problem in detail , which can then be used as a template for real world problems .",
    "numerical solutions are produced to explicitly illustrate the case .",
    "recently , ill - behaved problems have been solved using large deviation theory or information - theoretic approaches .",
    "all of these methods have a common premise : they rely on asymptotic arguments .",
    "the me method does not need such assumptions to work and therefore can process finite amounts of data well .",
    "however , when me is taken to asymptotic limits one recovers the same solutions that the information - theoretic methods produce .",
    "this is discussed by comparing the numerical solution to our specific example and the solution that is attained by the method of types @xcite .",
    "our first concern when using the me method to update from a prior to a posterior distribution is to define the space in which the search for the posterior will be conducted .",
    "we wish to infer something about the values of one or several quantities , @xmath0 , on the basis of three pieces of information : prior information about @xmath1 ( the prior ) , the known relationship between @xmath2 _ and _ @xmath1 ( the model ) , and the observed values of the data @xmath3 . since we are concerned with both @xmath2 _ and _ @xmath1 , the relevant space is neither @xmath4 nor @xmath5 but the product @xmath6 and our attention must be focused on the joint distribution @xmath7 .",
    "the selected joint posterior @xmath8 is that which maximizes the entropy,@xmath9=-\\int dxd\\theta ~p(x,\\theta ) \\log \\frac{p(x,\\theta ) } { p_{% \\text{old}}(x,\\theta ) } ~\\]]subject to the appropriate constraints .",
    "@xmath10 contains our prior information which we call the _ joint prior_. to be explicit,@xmath11where @xmath12 is the traditional bayesian prior and @xmath13 is the likelihood .",
    "it is important to note that they _ both _ contain prior information .",
    "the bayesian prior is defined as containing prior information .",
    "however , the likelihood is not traditionally thought of in terms of prior information . of course it is reasonable to see it as such because the likelihood represents the model ( the relationship between @xmath1 and @xmath14 that has already been established .",
    "thus we consider both pieces , the bayesian prior and the likelihood to be _ prior _ information .",
    "the new information is the _ observed data _ , @xmath15 , which in the me framework must be expressed in the form of a constraint on the allowed posteriors .",
    "the family of posteriors that reflects the fact that @xmath2 is now known to be @xmath15 is such that@xmath16this amounts to an _",
    "infinite _ number of constraints : there is one constraint on @xmath7 for each value of the variable @xmath2 and each constraint will require its own lagrange multiplier @xmath17 . furthermore , we impose the usual normalization constraint , @xmath18    maximize @xmath19 subject to these constraints , @xmath20 \\\\   + \\int dx\\lambda ( x)\\left [ \\int d\\theta p(x,\\theta ) -\\delta ( x - x% % tcimacro{\\u{b4}}% % beginexpansion { \\acute{}}% % endexpansion ) \\right]% \\end{array}% \\right\\ } = 0~,\\ ] ] and the selected posterior is @xmath21where the normalization @xmath22 is @xmath23and the multipliers @xmath17 are determined from ( [ data constraint ] ) , @xmath24therefore , substituting @xmath25 back into ( [ solution a ] ) , @xmath26    the new marginal distribution for @xmath1 is@xmath27this is the familiar bayes conditionalization rule . to summarize : @xmath28 is updated to @xmath29 with @xmath30 fixed by the observed data while @xmath31 remains unchanged .",
    "we see that in accordance with the minimal updating philosophy that drives the me method _ _ one only updates those aspects of one s beliefs for which corrective new evidence ( in his case , the data ) has been supplied _ _ function has been criticized in that by implementing it , the probability is completely constrained , thus it can not be updated by future information .",
    "this is certainly true ! however , imposing one constraint does not imply a revision of the other : an experiment , once performed and its outcome observed , can not be _ un - performed _ and its result can not be _ un - observed _ by subsequent experiments.]_. _",
    "in this general example , we extend our results from the previous section . again we wish to infer something about @xmath32 given some information .",
    "the information that we are given in this example is some observed data , @xmath15 and a constraint on the posterior in the form of a moment .",
    "here we apply the data constraint _ simultaneously _ with the moment constraint .",
    "note that this problem can not be solved by maxent or bayes .",
    "for this example , we assume the constraints , @xmath33which is our normalization constraint,@xmath34which represents some observable data,@xmath35which represents some additional information . maximizing the entropy given the constraints with respect to @xmath7 yields,@xmath36where @xmath22",
    "is determined by using ( [ normalization]),@xmath37and the lagrange multipliers @xmath17 are determined by using ( data)@xmath38the posterior now becomes@xmath39where @xmath40    the lagrange multiplier @xmath41 is determined by first substituting the posterior into ( [ expectation])@xmath42 f(\\theta ) = f~,\\]]which can be rewritten as@xmath43 \\delta ( x - x% % tcimacro{\\u{b4}}% % beginexpansion { \\acute{}}% % endexpansion ) = f~.\\]]integrating over @xmath2 yields,@xmath44where @xmath45 . now @xmath41 can be determined by@xmath46    the final step is to marginalize the posterior , @xmath8 to get our updated probability,@xmath47additionally , this result can be rewritten using the product rule as @xmath48where @xmath49 the right side resembles bayes theorem , where the term @xmath50 is the standard bayesian likelihood and @xmath12 is the prior .",
    "the exponential term is a _ modification _ to these two terms . in an effort to put some names to these pieces",
    "we will call the standard bayesian likelihood the _ likelihood _ and the exponential part the _ likelihood modifier _ so that the product of the two gives the _ modified likelihood_. the denominator is the normalization or _",
    "marginal modified likelihood_. could only be used when it does not contradict the data constraint ( [ data ] ) .",
    "therefore , it is redundant and the constraint would simply get absorbed when solving for @xmath17 . ]",
    "this is a general example of an ill - posed problem using the above method : a factory makes @xmath51 different kinds of bouncy balls . for reference , they assign each different kind with a number , @xmath52 .",
    "they ship large boxes of them out to stores .",
    "unfortunately , there is no mechanism that regulates how many of each ball goes into the boxes , therefore we do not know the amount of each kind of ball in each or all of the boxes .",
    "however , we are informed that the company does know the average amount of balls , @xmath53 in each of the boxes over the time that they have been in existence . what is the probability of getting a particular kind of ball in one of the boxes ? at this point one could use maxent to answer the question , assuming that the average could be substituted for the moment constraint .",
    "now let us complicate the problem by suggesting that we would like a better idea of how many balls are in each box ( perhaps for quality control or perhaps the customer would like more of one kind of ball than another ) . to do this we randomly select a few balls , @xmath54 from a particular box and count how many of each kind we get , @xmath55 ( or perhaps we simply open the box and look at the balls on the surface ) .",
    "now let us put the above example in a more mathematical format .",
    "let the set of possible outcomes be represented by , @xmath56 from a sample where the total number of balls , @xmath57 for the me method to work .",
    "we simply wish to use the description of the problem that is common in information - theoretic examples . ] and whose sample average is @xmath58 further , let us draw a _",
    "sample of size @xmath59 from the original sample whose outcomes are counted and represented as @xmath60 where @xmath61 .",
    "we would like to determine the probability of getting _ any _ particular outcome in one draw ( @xmath62 ) given the information . to discuss the probabilities related to this situation",
    ", we implement observational data _ simultaneously _ with an expectation value .",
    "we start with the usual negative relative entropy for the joint space,@xmath9=-\\sum\\limits_{m}\\int d\\theta ~p(m,\\theta |n)\\log \\frac{% p(m,\\theta |n)}{p_{\\text{old}}(m,\\theta |n)}~.\\]]we also have the following constraints,@xmath63@xmath64@xmath65where @xmath66 @xmath67 and @xmath68 is the observed data . notice the use of the kronecker for the discrete case .",
    "now we maximize the entropy given the constraints with respect to @xmath69 which yields,@xmath70    we need to determine @xmath71 and @xmath72 for our problem . the equation that we will use for the _ likelihood , _",
    "@xmath71 is simply the multinomial distribution,@xmath73prior to receiving the information that the die is _ not _ fair due to the bias , we were completely ignorant of the status of the die . therefore to incorporate this ignorance",
    "we use a _ prior _ that is flat , thus @xmath74 _",
    "constant_. being a constant , the prior can come out of the integral and cancels with the same constant in the numerator .",
    "( also , the particular form of @xmath75 is not important for our current purpose so for the sake of definiteness we can choose it flat for our example . )",
    "now we include our average information .",
    "to do this , we rewrite the moment constraint ( [ ex4(expectaion ) ] ) to reflect the special case by replacing the function @xmath76 with @xmath77 where @xmath78 is a discrete parameter that reflects the label for the outcomes and @xmath53 is the average .",
    "the sum relates the relationship of the sides and @xmath62 is the continuous parameter that we wish to infer something about .",
    "thus the constraint is rewritten the following way.@xmath79where,@xmath80notice that @xmath53 reflects the _ average _ relationship of the sides .",
    "the resulting posterior is the product of the _ likelihood _ and what we have called the _ likelihood modifier _ , @xmath81 or in this case , @xmath82 divided by the normalization of the two,@xmath83where @xmath84 .    to determine @xmath41 we use ( [ f ] ) .",
    "this function can be complicated .",
    "one may need to find a numerical solution for @xmath41 or an advanced search technique such as newton s method .    for simplicity",
    "we reduce the final @xmath85 to @xmath86 dimensions,@xmath87where @xmath88 .",
    "the denominator , @xmath89 , which is the normalization factor , can be a difficult integral .",
    "the general solution for the @xmath51 sided die is a hypergeometric series which is calculated on a @xmath86 simplex,@xmath90where@xmath91and where @xmath92 , @xmath93the terms @xmath94 and @xmath95 , @xmath96 , @xmath41 is the lagrange multiplier and , @xmath78 and @xmath97 comes from @xmath98 is the gamma function , and the terms @xmath99 and @xmath100 the index @xmath101 takes all discrete values from @xmath102 to @xmath86 .",
    "the total number of counts or rolls of the die is @xmath59 with @xmath103 being the amount of counts for each parameter or dimension , thus @xmath104 the summation terms for each level of this nested series are represented by @xmath105 the factory information is codified in @xmath106 , where @xmath41 is the lagrange multiplier and , @xmath78 and @xmath97 comes from ( [ ex4(expectaion2 ) ] ) .",
    "a few technical details are worth mentioning : first , one can have singular points when @xmath107 . in these cases",
    "the sum must be evaluated in the limit as @xmath108 second , since @xmath109 and @xmath110 are positive integers the beta functions involve no singularities .",
    "lastly , the sums converge because @xmath111 .",
    "we will extend the econometric example by applying the above solutions to a specific problem where there are three kinds of balls labeled 1 , 2 and 3 .",
    "so for this problem we have @xmath112 @xmath113 and @xmath114 further , we are given information regarding the average of all the boxes , @xmath115 for our example this average will be , @xmath116 notice that this implies that on the average there are more @xmath117 s in each box .",
    "next we take a sample of one of the boxes where @xmath118 @xmath119 and @xmath120 the numerical solution for this example is,@xmath121where @xmath122 and @xmath123 we show the relationship between @xmath41 and @xmath53 in fig 1 . the purpose of the lagrange multiplier is to enforce the moment constraint , therefore , as @xmath53 goes to the extreme ( @xmath124 ) , @xmath125 this is important to mention because it graphically illustrates that whether the deviation from the sample mean is large or small , the me method holds .",
    "another possible method suggested to use for this problem is the method of types @xcite .",
    "this method essentially uses a form of sanov s theorem , which for this problem would be written as,@xmath126where @xmath127 is `` estimated '' with the frequency of the data sample . thus @xmath128 etc .",
    "this produces the following results:@xmath129taking the means of the me solution yields,@xmath130clearly the numerical solutions are very close , however , there are several flaws with this large deviation method .",
    "the first is that @xmath131 is treated as a frequency . in the asymptotic case",
    "it would be appropriate to use a frequency , unfortunately this is not that case .",
    "the data sample is finite , @xmath132 another flaw is that the method does not allow for fluctuations where as the me method does .",
    "of course in the asymptotic case , fluctuations would be ruled out , but again , this is not the case .",
    "there is an underlying theme here : probabilities are not equivalent to frequencies _ except _ in the asymptotic case .",
    "using the me method we were able to use information in the form of data and moments to update our prior probabilities .",
    "a general example was shown where the solution resembled the traditional form of bayes rule with the standard likelihood being modified by a factor resulting from the moment constraint .",
    "a specific econometric example was then solved in detail to illustrate the application of the method .",
    "this case can be used as a template for real world problems .",
    "numerical results were obtained to illustrate explicitly how the method compares to other methods that are currently employed .",
    "the memethod was shown to be superior in that it did not need to make asymptotic assumptions to function and allows for fluctuations .",
    "it must be emphasized that in the asymptotic limit , the me form is analogous to sanov s theorem .",
    "however , this is only one special case .",
    "the me method is more robust in that it can also be used to solve traditional bayesian problems .",
    "in fact it was shown that if there is no moment constraint , one recovers bayes rule .    therefore",
    ", we would like to emphasize that anything one can do with bayes , one can now do with me .",
    "additionally , in me one now has the ability to apply additional information that bayesian methods could not .",
    "further , any work done with bayesian techniques can be implemented into the me method directly through the joint prior .",
    "finally the me method can now also be used to solve ill - posed problems in econometrics",
    ".      9 e. t. jaynes , phys . rev . *",
    "106 * , 620 and * 108 * , 171 ( 1957 ) ; r. d. rosenkrantz ( ed . ) , _",
    "e. t. jaynes : papers on probability , statistics and statistical physics _",
    "( reidel , dordrecht , 1983 ) ; e. t. jaynes , _ probability theory : the logic of science _ ( cambridge university press , cambridge , 2003 ) .        a. caticha and a. giffin , updating probabilities , _ bayesian inference and maximum entropy methods in science and engineering _ , ed . by ali mohammad - djafari ( ed . ) , aip conf . proc . * 872 * , 31 ( 2006 ) ( http://arxiv.org/abs/physics/0608185 ) ."
  ],
  "abstract_text": [
    "<S> we demonstrate how information in the form of observable data and moment constraints are introduced into the method of maximum relative entropy ( me ) . </S>",
    "<S> a general example of updating with data and moments is shown . </S>",
    "<S> a specific econometric example is solved in detail which can then be used as a template for real world problems . </S>",
    "<S> a numerical example is compared to a large deviation solution which illustrates some of the advantages of the me method . </S>"
  ]
}