{
  "article_text": [
    "in pattern recognition , learning , and data mining one obtains information from information - carrying objects .",
    "this involves an objective definition of the information in a single object , the information to go from one object to another object in a pair of objects , the information to go from one object to any other object in a multiple of objects , and the shared information between objects .    the notion of kolmogorov complexity @xcite is an objective measure for the information in an a _ single _ object , and information distance measures the information between a _",
    "pair _ of objects @xcite .",
    "this leads to the notion of similarity we shall explore below .",
    "objects can be given literally , like the literal four - letter genome of a mouse , or the literal text of _ war and peace _ by tolstoy .",
    "for simplicity we take it that all meaning of the object is represented by the literal object itself .",
    "objects can also be given by name , like `` the four - letter genome of a mouse , '' or `` the text of _ war and peace _ by tolstoy . ''",
    "there are also objects that can not be given literally , but only by name and acquire their meaning from their contexts in background common knowledge in humankind , like `` home '' or `` red . '' in the literal setting , similarity of objects can be established by feature analysis , one type of similarity per feature . in the abstract ``",
    "name '' setting , all similarity must depend on background knowledge and common semantics relations , which is inherently subjective and `` in the mind of the beholder . ''    as an aside , in many applications we are interested in shared information between _ many _ objects instead of just a pair of objects .",
    "for example , in customer reviews of gadgets , in blogs about public happenings , in newspaper articles about the same occurrence , we are interested in the most comprehensive one or the most specialized one .",
    "thus , we want to extend the information distance measure from pairs to multiples .",
    "this approach was introduced in @xcite while most of the theory is developed in @xcite .",
    "all data are created equal but some data are more alike than others .",
    "we have proposed methods expressing this alikeness , using a new similarity metric based on compression .",
    "it is parameter - free in that it does nt use any features or background knowledge about the data , and can without changes be applied to different areas and across area boundaries .",
    "it is universal in that it approximates the parameter expressing similarity of the dominant feature in all pairwise comparisons .",
    "it is robust in the sense that its success appears independent from the type of compressor used ( among equally good compressors ) .",
    "the clustering we use is hierarchical clustering in dendrograms based on a new fast heuristic for the quartet method @xcite .",
    "if we consider @xmath0 objects , then we find @xmath1 pairwise distances .",
    "these distances are between natural data .",
    "we let the data decide for themselves , and construct a hierarchical clustering of the @xmath0 objects concerned . for details",
    "see the cited reference .",
    "the method takes the @xmath2 distance matrix as input , and yields a dendrogram with the @xmath0 objects as leaves ( so the dendrogram contains @xmath0 external nodes or leaves and @xmath3 internal nodes .",
    "we assume @xmath4 .",
    "the method is available as an open - source software tool , @xcite .",
    "we are presented with unknown data and the question is to determine the similarities among them and group like with like together .",
    "commonly , the data are of a certain type : music files , transaction records of atm machines , credit card applications , genomic data . in these data there are hidden relations that we would like to get out in the open .",
    "for example , from genomic data one can extract letter- or block frequencies ( the blocks are over the four - letter alphabet ) ; from music files one can extract various specific numerical features , related to pitch , rhythm , harmony etc .",
    "one can extract such features using for instance fourier transforms  @xcite or wavelet transforms  @xcite , to quantify parameters expressing similarity .",
    "the resulting vectors corresponding to the various files are then classified or clustered using existing classification software , based on various standard statistical pattern recognition classifiers  @xcite , bayesian classifiers  @xcite , hidden markov models  @xcite , ensembles of nearest - neighbor classifiers  @xcite or neural networks  @xcite .",
    "for example , in music one feature would be to look for rhythm in the sense of beats per minute .",
    "one can make a histogram where each histogram bin corresponds to a particular tempo in beats - per - minute and the associated peak shows how frequent and strong that particular periodicity was over the entire piece .",
    "in @xcite we see a gradual change from a few high peaks to many low and spread - out ones going from hip - hip , rock , jazz , to classical .",
    "one can use this similarity type to try to cluster pieces in these categories .",
    "however , such a method requires specific and detailed knowledge of the problem area , since one needs to know what features to look for .",
    "our aim is to capture , in a single similarity metric , _ every effective distance _ : effective versions of hamming distance , euclidean distance , edit distances , alignment distance , lempel - ziv distance , and so on .",
    "this metric should be so general that it works in every domain : music , text , literature , programs , genomes , executables , natural language determination , equally and simultaneously",
    ". it would be able to simultaneously detect _ all _ similarities between pieces that other effective distances can detect seperately .",
    "such a `` universal '' metric was co - developed by us as a normalized version of the `` information metric '' of @xcite .",
    "there it was shown that the information metric minorizes up to a constant all effective distances satisfying a mild density requirement ( excluding for example distances that are 1 for every pair @xmath5 such that @xmath6 ) .",
    "this justifies the notion that the information distance is universal .",
    "we may be interested what happens in terms of properties or features of the pair of objects analyzed , say @xmath7 and @xmath8 .",
    "it can be shown that the information distance captures every property of which the kolmogorov complexity is logarithmic in the length of @xmath9 .",
    "if those lengths go to infinity , then logarithm of those lengths go to infinity too . in this case",
    "the information distance captures every property .",
    "this information distance ( actually a metric ) is normalized so that the resulting distances are in @xmath10 $ ] and can be shown to retain the metric property , @xcite .",
    "( a nonoptimal precursor was given in @xcite . )",
    "the result is the `` normalized information distance '' ( actually a metric ) .",
    "all this is in terms of kolmogorov complexity @xcite .",
    "intuitively , two objects are deemed close if we can significantly `` compress '' one given the information in the other , the intuition being that if two pieces are more similar , then we can more succinctly describe one given the other .",
    "the normalized information distance discovers all effective similarities in the sense that if two objects are close according to some effective similarity , then they are also close according to the normalized information distance .",
    "put differently , the normalized information distance represents similarity according to the dominating shared feature between the two objects being compared . in comparisons of more than two objects , different pairs may have different dominating features .",
    "for every two objects , this normalized information metric distance zooms in on the dominant similarity between those two objects out of a wide class of admissible similarity features .",
    "since the normalized information distance also satisfies the metric ( in)equalities , and takes values in @xmath10 $ ] , it may be called _",
    "`` the '' similarity metric_.      unfortunately , the universality of the normalized information distance comes at the price of noncomputability .",
    "in fact , the normalized information distance is not even semicomputable ( this is weaker than computable ) and there is no semicomputable function at a computable distance of it @xcite .",
    "but since the kolmogorov complexity of a string or file is the length of the ultimate compressed version of that file , we can use real data compression programs to approximate the kolmogorov complexity .",
    "therefore , to apply this ideal precise mathematical theory in real life , we have to replace the use of the noncomputable kolmogorov complexity by an approximation using a standard real - world compressor .",
    "starting from the normalized information distance , if @xmath11 is a compressor and we use @xmath12 to denote the length of the compressed version of a string @xmath7 , then we arrive at the _ normalized compression distance _ : @xmath13 where for convenience we have replaced the pair @xmath14 in the formula by the concatenation @xmath15 , and we ignore logarithmic terms in the numerator and denominator , see @xcite . in @xcite we propose axioms to capture the real - world setting , and show that approximates optimality . actually , the ncd is a family of compression functions parameterized by the given data compressor @xmath11 .",
    "[ example.genome ] ( phylogeny ) one can not find more appropriate data than dna sequences to test our theory .",
    "a dna sequence is a finite string over a 4-letter alphabet @xmath16 .",
    "we used the entire mitochondrial genomes of 20 mammals , each of about 18,000 base pairs , to test a hypothesis about the eutherian orders .",
    "it has been hotly debated in biology which two of the three main placental mammalian groups , primates , ferungulates , and rodents , are more closely related .",
    "one cause of the debate is that in the analysis of the genomics the standard maximum likelihood method , which depends on the multiple alignment of sequences corresponding to an individual protein , gives ( rodents , ( ferungulates , primates ) ) for half of method , which depends on the multiple alignment of sequences corresponding to an individual protein , gives ( rodents , ( ferungulates , primates ) ) for half of the proteins in the mitochondrial genome , and ( ferungulates , ( primates , rodents ) ) for the other half .       in recent years , as a result of more sophisticated methods , together with biological evidence , it is believed that ( rodents , ( ferungulates , primates ) ) reflects the true evolutionary history .",
    "we confirm this from the whole - genome perspective using the ncd distance .",
    "we use the complete mitochondrial genome sequences from the following 20 species : rat ( _ rattus norvegicus _ ) , house mouse ( _ mus musculus _ ) , gray ( or grey ) seal ( _ halichoerus grypus _ ) , harbor seal ( _ phoca vitulina _ ) , cat ( _ felis catus _ ) , white rhino ( _ ceratotherium simum _ ) , horse ( _ equus caballus _ ) , finback whale ( _ balaenoptera physalus _ ) , blue whale ( _ balaenoptera musculus _ ) , cow ( _ bos taurus _ ) , gibbon ( _ hylobates lar _ ) , gorilla ( _ gorilla gorilla _ ) , human ( _ homo sapiens _ ) , chimpanzee ( _ pan troglodytes _ ) , pygmy chimpanzee ( _ pan paniscus _ ) , orangutan ( _ pongo pygmaeus _ ) , sumatran orangutan ( _ pongo pygmaeus abelii _ ) , with opossum ( _ didelphis virginiana _ ) , wallaroo ( _ macropus robustus _ ) , and platypus ( _ ornithorhynchus anatinus _ ) as the outgroup .    for every pair of mitochondrial genome sequences @xmath7 and @xmath8 , evaluate the formula in equation  [ eq.ncd ] using a special - purpose dna sequence compressor dnacompress , or a good general - purpose compressor such as ppmz .",
    "the resulting distances are the entries in an @xmath2 distance matrix .",
    "constructing a phylogeny tree from the distance matrix , using common tree - reconstruction software , gives the tree in figure  [ tree - mammal ] .",
    "this tree confirms the accepted hypothesis of ( rodents , ( primates , ferungulates ) ) , and every single branch of the tree agrees with the current biological classification .",
    "( hierarchical clustering ) the normalized compression distance has been used to fully automatically reconstruct language and phylogenetic trees as above .",
    "it can , and has , also be used for a plethora of new applications of general clustering and classification of natural data in arbitrary domains , for clustering of heterogeneous data , and for anomaly detection across domains . it has further been applied to authorship attribution , stemmatology , music classification , internet knowledge discovery , to analyze network traffic and cluster computer worms and viruses , software metrics and obfuscation , web page authorship , topic and domain identification , hurricane risk assessment , ortholog detection , and clustering fetal heart rate tracings .",
    "we test gross classification of files based on heterogeneous data of markedly different file types : ( i ) four mitochondrial gene sequences , from a black bear , polar bear , fox , and rat obtained from the genbank database on the worldwide web ; ( ii ) four excerpts from the novel _ the zeppelin s passenger _ by e.",
    "phillips oppenheim , obtained from the project gutenberg edition on the worldwide web ; ( iii ) four midi files without further processing , two works by jimi hendrix and two movements from debussy s `` suite bergamasque , '' downloaded from various repositories on the worldwide web ; ( iv ) two linux x86 elf executables ( the _ cp _ and _ rm _ commands ) , copied directly from the redhat 9.0 linux distribution ; and ( v ) two compiled java class files , generated directly .",
    "the program correctly classifies each of the different types of files together with like near like .",
    "the result is reported in figure  [ figfiletypes ] .",
    "this experiment shows the power and universality of the method : no features of any specific domain of application are used .",
    "we believe that there is no other method known that can cluster data that are so heterogeneous this reliably .",
    "researchers from the data - mining community noticed that this methodology is in fact a parameter - free , feature - free , data - mining tool .",
    "they have experimentally tested a closely related metric on a large variety of sequence benchmarks . comparing the compression - based method with 51 major parameter - loaded methods found in the 7 major data - mining conferences ( sigkdd , sigmod , icdm , icde , ssdb , vldb , pkdd , and pakdd ) in 19942004 , on every database of time sequences used , ranging from heartbeat signals to stock market curves , they established clear superiority of the compression - based method for clustering heterogeneous data , for anomaly detection , and competitiveness in clustering domain data @xcite .",
    "in @xcite it is proved that the ncd is a metric .",
    "the compression - based ncd method establishes a similarity metric among objects given as finite binary strings .",
    "it has been applied to objects like genomes , music pieces in midi format , computer programs in ruby or c , pictures in simple bitmap formats , or time sequences such as heart rhythm data , heterogenous data and anomaly detection .",
    "this method is feature - free in the sense that it does nt analyze the files looking for particular features ; rather it analyzes all features simultaneously and determines the similarity between every pair of objects according to the most dominant shared feature .",
    "the crucial point is that the method analyzes the objects themselves .",
    "this precludes comparison of abstract notions or other objects that do nt lend themselves to direct analysis , like emotions , colors , socrates , plato , mike bonanno and albert einstein .",
    "to make computers more intelligent one would like to represent meaning in computer - digestable form . long - term and labor - intensive efforts like the _ cyc _ project @xcite and the _ wordnet _ project @xcite try to establish semantic relations between common objects , or , more precisely , _",
    "names _ for those objects .",
    "the idea is to create a semantic web of such vast proportions that rudimentary intelligence and knowledge about the real world spontaneously emerges .",
    "this comes at the great cost of designing structures capable of manipulating knowledge , and entering high quality contents in these structures by knowledgeable human experts . while the efforts are long - running and large scale ,",
    "the overall information entered is minute compared to what is available on the world - wide - web .",
    "the rise of the world - wide - web has enticed millions of users to type in trillions of characters to create billions of web pages of on average low quality contents .",
    "the sheer mass of the information available about almost every conceivable topic makes it likely that extremes will cancel and the majority or average is meaningful in a low - quality approximate sense .",
    "below , we give a general method to tap the amorphous low - grade knowledge available for free on the world - wide - web , typed in by local users aiming at personal gratification of diverse objectives , and yet globally achieving what is effectively the largest semantic electronic database in the world .",
    "moreover , this database is available for all by using any search engine that can return aggregate page - count estimates like google for a large range of search - queries .",
    "while the previous ncd method that compares the objects themselves using is particularly suited to obtain knowledge about the similarity of objects themselves , irrespective of common beliefs about such similarities , we now develop a method that uses only the name of an object and obtains knowledge about the similarity of objects by tapping available information generated by multitudes of web users .",
    "the new method is useful to extract knowledge from a given corpus of knowledge , in this case the world - wide - web accessed by a search engine returning aggregate page counts , but not to obtain true facts that are not common knowledge in that database .",
    "for example , common viewpoints on the creation myths in different religions may be extracted by the web - based method , but contentious questions of fact concerning the phylogeny of species can be better approached by using the genomes of these species , rather than by opinion .",
    "the approach below was proposed by @xcite .",
    "let the set of singleton _ search terms _ be denoted by @xmath17 . in the sequel we use both singleton search terms and doubleton search terms @xmath18 .",
    "let the set of web pages indexed ( possible of being returned ) by search engine be @xmath19 .",
    "the cardinality of @xmath19 is denoted by @xmath20 .",
    "assume that a priori all web pages are equiprobable , with the probability of being returned being @xmath21 .",
    "a subset of @xmath19 is called an _ event_. every _ search term _",
    "@xmath7 defines a _",
    "singleton event _",
    "@xmath22 of web pages that contain an occurrence of @xmath7 and are returned by the search engine if we do a search for @xmath7 .",
    "let @xmath23 $ ] be the uniform mass probability function .",
    "the probability of such an event @xmath24 is @xmath25 .",
    "similarly , the _ doubleton event _",
    "@xmath26 is the set of web pages returned if we do a search for pages containing both search term @xmath7 and search term @xmath8 .",
    "the probability of this event is @xmath27 .",
    "we can also define the other boolean combinations : @xmath28 and @xmath29 , each such event having a probability equal to its cardinality divided by @xmath30 . if @xmath31 is an event obtained from the basic events @xmath32 , corresponding to basic search terms @xmath33 , by finitely many applications of the boolean operations , then the probability @xmath34 .",
    "these events capture in a particular sense all background knowledge about the search terms concerned available ( to search engine ) on the web .",
    "therefore , it is natural to consider code words for those events as coding this background knowledge .",
    "however , we can not use the probability of the events directly to determine a prefix code such as the shannon - fano code @xcite .",
    "the reason is that the events overlap and hence the summed probability exceeds 1 . by the kraft inequality @xcite",
    "this prevents a corresponding shannon - fano code .",
    "the solution is to normalize : we use the probability of the events to define a probability mass function over the set @xmath35 of search terms , both singleton and doubleton . define @xmath36 counting each singleton set and each doubleton set ( by definition unordered ) once in the summation .",
    "since every web page that is indexed by contains at least one occurrence of a search term , we have @xmath37 .",
    "on the other hand , web pages contain on average not more than a certain constant @xmath38 search terms .",
    "therefore , @xmath39 .",
    "define @xmath40 then , @xmath41 .",
    "note that @xmath42 is not a conventional joint distribution since possibly @xmath43 .",
    "rather , we consider @xmath44 to be a probability mass function over the sample space @xmath45 .",
    "this @xmath44-distribution changes over time , and between different samplings from the distribution . but",
    "let us imagine that @xmath44 holds in the sense of an instantaneous snapshot .",
    "the real situation will be an approximation of this . given the search machinery , these are absolute probabilities which allow us to define the associated shannon - fano code for both the singletons and the doubletons .",
    "the _ web code _",
    "length @xmath46 is defined by @xmath47 in contrast to strings @xmath7 where the complexity @xmath12 represents the length of the compressed version of @xmath7 using compressor @xmath11 , for a search term @xmath7 ( just the name for an object rather than the object itself ) , the code of length @xmath48 represents the shortest expected prefix - code word length of the associated event @xmath24 .",
    "the expectation is taken over the distribution @xmath49 . in this sense we can use the distribution as a compressor for `` meaning '' associated with the search terms . the associated _ normalized web distance _ ( nwd )",
    "is defined just as with the search engine in the role of compressor yielding code lengths @xmath50 for the singleton search terms @xmath5 being compaired and a code length @xmath51 for the doubleton pair @xmath14 , by @xmath52 this @xmath53 uses the background knowledge on the web as viewed by the search engine as conditional information .",
    "many web search engines index more than twenty - five billion pages on the web .",
    "each such page can be viewed as a set of index terms .",
    "a search for a particular index term ( in 2004 ) say `` horse '' , returned a certain number of hits ( web pages where this term occurred ) , say 46,700,000 .",
    "the number of hits for the search term `` rider '' is , say , 12,200,000 .",
    "it is also possible to search for the pages where both `` horse '' and `` rider '' occur .",
    "this gives , say , 2,630,000 hits .",
    "the search engine searched at that time , say , 8,058,044,651 web pages .",
    "the same formula as can be written in terms of frequencies as @xmath54 and if @xmath55 and @xmath56 then @xmath57 .",
    "it is easy to see that    1 .",
    "@xmath58 is undefined for @xmath59 ; 2 .",
    "@xmath60 for @xmath56 and either or both @xmath61 and @xmath62 ; and 3 .",
    "@xmath63 otherwise .",
    "our experimental results suggest that every reasonable ( greater than any @xmath64 ) value can be used for the normalizing factor @xmath65 , and our results seem in general insensitive to this choice . in our software",
    ", this parameter @xmath65 can be adjusted as appropriate , and we often use @xmath30 for @xmath65 . in the @xcite",
    "we analyze the mathematical properties of nwd , and prove the universality of the search engine distribution .",
    "we show that the nwd is not a metric , in contrast to the ncd .",
    "the generic example showing the nonmetricity of semantics ( and therefore the nwd ) is that a man is close to a centaur , and a centaur is close to a horse , but a man is very different from a horse .    with the hit numbers above , using @xmath30 for @xmath65 , we can compute from that @xmath66 the nwd formula itself is _ scale - invariant _ in the sense that if @xmath30 doubles and so do the @xmath67-frequencies then the result stays the same .",
    "( classification ) in cases in which the set of objects can be large , in the millions , clustering can not do us much good",
    ". we may also want to do definite classification , rather than the more fuzzy clustering .",
    "one can use the ncd / nwd distances as an oblivious feature - extraction technique to convert generic objects into finite - dimensional vectors .",
    "( we have used this technique to train a support vector machine ( svm ) based ocr system to classify handwritten digits by extracting 80 distinct , ordered ncd features from each input image in the manner explained below in the context of the nwd experiments . for details about the svm see @xcite .",
    "we achieved a handwritten single decimal digit recognition accuracy of 87% .",
    "the current state of the art for this problem , after half a century of interactive feature - driven classification research , is in the upper ninety percent level .",
    "these experiments were benchmarked on the standard nist special data base 19 . )    for classification using the nwd distance , the setting is , say , a binary classification problem on examples represented by search terms . in this experiment , we require a human expert to provide a list of at least 40 _ training words _ , consisting of at least 20 positive examples and 20 negative examples , to illustrate the contemplated concept class .",
    "the expert also provides , say , six _ anchor words _",
    "@xmath68 , of which half are in some way related to the concept under consideration .",
    "then , we use the anchor words to convert each of the 40 training words @xmath69 to 6-dimensional _ training vectors _",
    "the entry @xmath71 of @xmath72 is defined as @xmath73 ( @xmath74 , @xmath75 ) .",
    "the training vectors are then used to train an svm to learn the concept .",
    "the test words are classified using the same anchors and trained svm model .",
    "the libsvm software was used for all svm experiments  @xcite .    in an experiment to learn prime numbers",
    ", we used the literal search terms below ( digital numbers and alphabetical words ) in the google search engine .",
    "+ _ positive training examples _ : 11 , 13 , 17 , 19 , 2 , 23 , 29 , 3 , 31 , 37 , 41 , 43 , 47 , 5 , 53 , 59 , 61 , 67 , 7 , 71 , 73 .",
    "+ _ negative training examples _ :",
    "10 , 12 , 14 , 15 , 16 , 18 , 20 , 21 , 22 , 24 , 25 , 26 , 27 , 28 , 30 , 32 , 33 , 34 , 4 , 6 , 8 , 9 .",
    "+ _ anchor words _ : composite , number , orange , prime , record .",
    "+ _ unseen test examples _ : the numbers 101 , 103 , 107 , 109 , 79 , 83 , 89 , 97 were correctly classified as primes .",
    "the numbers 36 , 38 , 40 , 42 , 44 , 45 , 46 , 48 , 49 were correctly classified as nonprimes .",
    "the numbers 91 and 110 were false positives , since they were incorrectly classified as primes .",
    "there were no false negatives .",
    "the accuracy on the test set is @xmath76 .",
    "thus , the method automatically learns to distinguish prime numbers from nonprime numbers by example , using a search engine that knows nothing about mathematics .",
    "note that in this example we have to keep the numbers under , say , 200 , since larger numbers do not necessarily occur on the web in the required multitude .",
    "the nwd has been used for clustering , classification , and translation of small samples @xcite .",
    "that reference reports a massive experiment comparing the performance of the nwd ",
    "svm method with the human - expert - entered information in the wordnet database @xcite .",
    "they showed a mean accuracy of agreement of 87.25% of the nwd ",
    "svm method with the wordnet semantic concordance .",
    "by now applications abound . see the many references to the papers @xcite in google scholar .",
    "below we treat some favorite applications we are fimiliar with out of this multitude .",
    "applications of ncd : we and others performed experiments in vastly different application fields to test the quality and universality of the method .",
    "the success of the method as reported below depends strongly on the judicious use of encoding of the objects compared . here",
    "one should use common sense on what a real world compressor can do .",
    "there are situations where our approach fails if applied in a straightforward way .",
    "for example : comparing text files by the same authors in different encodings ( say , unicode and 8-bit version ) is bound to fail . for the ideal similarity metric based on kolmogorov complexity",
    "as defined in @xcite this does not matter at all , but for practical compressors used in the experiments it will be fatal . similarly , in the music experiments we use symbolic midi music file format rather than wave format music files .",
    "the reason is that the strings resulting from straightforward discretizing the wave form files may be too sensitive to how we discretize .",
    "further research may overcome this problem .",
    "the method is implemented and available as public software @xcite .",
    "this approach gives the first completely automatic construction of the phylogeny tree based on whole mitochondrial genomes ( above ) , and a completely automatic construction of a language tree for over 50 euro - asian languages @xcite , detects plagiarism in student programming assignments @xcite , gives phylogeny of chain letters @xcite , and clusters music @xcite .",
    "moreover , the method turns out to be more - or - less robust under change of the underlying compressor - types : statistical ( ppmz ) , lempel - ziv based dictionary ( gzip ) , block based ( bzip2 ) , or special purpose ( gencompress ) .",
    "obviously the window size matters , as well as how good the compressor is . for example , ppmz gives for mtdna of the investigated species diagonal elements ( @xmath77 ) between 0.002 and 0.006 .",
    "the compressor bzip2 does considerably worse , and gzip gives something in between 0.5 and 1 on the diagonal elements .",
    "nonetheless , for texts like books gzip does fine in our experiments ; the window size is sufficient and we do not use the diagonal elements .",
    "but for genomics gzip is no good .    in @xcite",
    "we report evidence of successful application in areas as diverse as genomics , virology , languages , literature , music , handwritten digits , astronomy , and combinations of objects from completely different domains ( see example above ) , using statistical , dictionary , and block sorting compressors . in genomics we presented new evidence for major questions in mammalian evolution , based on whole - mitochondrial genomic",
    "analysis : the eutherian orders ( we gave this example above ) and the marsupionta hypothesis against the theria hypothesis .",
    "apart from the experiments reported in @xcite , the clustering by compression method reported in this paper has recently been used in many different areas all over the world .",
    "for example , how to analyze network traffic and cluster computer worms and virusses  @xcite .",
    "applications of nwd : this method is proposed in @xcite to extract semantic knowledge from the world - wide - web for both supervised and unsupervised learning using a search engine in an unconventional manner .",
    "the approach is novel in its unrestricted problem domain , simplicity of implementation , and manifestly ontological underpinnings .",
    "evidence is given of elementary learning of the semantics of concepts , in contrast to most prior approaches ( outside of knowledge representation research ) that have neither the appearance nor the aim of dealing with ideas , instead using abstract symbols that remain permanently ungrounded throughout the machine learning application .",
    "the world - wide - web is the largest database on earth , and it induces a probability mass function via page counts for search queries .",
    "this distribution allows us to tap the latent semantic knowledge on the web .",
    "while in the ncd compression - based method one deals with the objects themselves , in the nwd method we deal with just names for the objects .    in @xcite , as proof of principle , we demonstrate positive correlations , evidencing an underlying semantic structure , in both numerical symbol notations and number - name words in a variety of natural languages and contexts .",
    "next , we give applications in ( i ) unsupervised hierarchical clustering , demonstrating the ability to distinguish between colors and numbers , and to distinguish between 17th century dutch painters ; ( ii ) supervised concept - learning by example , using support vector machines , demonstrating the ability to understand electrical terms , religious terms , emergency incidents , and by conducting a massive experiment in understanding wordnet categories ; and ( iii ) matching of meaning , in an example of automatic english - spanish translation .",
    "m. li , j.h .",
    "badger , x. chen , s. kwong , p. kearney , and h. zhang , an information - based sequence distance and its application to whole mitochondrial genome phylogeny , _ bioinformatics _ , 17:2(2001 ) , 149154 ."
  ],
  "abstract_text": [
    "<S> first we consider pair - wise distances for literal objects consisting of finite binary files . </S>",
    "<S> these files are taken to contain all of their meaning , like genomes or books . </S>",
    "<S> the distances are based on compression of the objects concerned , normalized , and can be viewed as similarity distances . </S>",
    "<S> second , we consider pair - wise distances between names of objects , like `` red '' or `` christianity . '' in this case the distances are based on searches of the internet . </S>",
    "<S> such a search can be performed by any search engine that returns aggregate page counts . </S>",
    "<S> we can extract a code length from the numbers returned , use the same formula as before , and derive a similarity or relative semantics between names for objects . </S>",
    "<S> the theory is based on kolmogorov complexity . </S>",
    "<S> we test both similarities extensively experimentally . </S>"
  ]
}