{
  "article_text": [
    "when a bayesian evaluates two competing models or theories , @xmath0 and @xmath1 , having observed a vector of observations @xmath2 , bayes theorem determines the posterior ratio of the models probabilities :    @xmath3    the quantity @xmath4 is called a _ bayes factor _ and the quantities @xmath5 and @xmath6 are called the theories _ marginal likelihoods_.    the types of bayesian models considered in this paper have a fixed finite number of parameters , each with their own probability function .",
    "if @xmath7 are parameters for a model @xmath8 , then    @xmath9    unfortunately , this integral is difficult to compute in practice .",
    "the purpose of this paper is to describe one method for estimating it .",
    "evaluating integral ( [ main integral ] ) is sometimes called the problem of computing normalizing constants .",
    "the following formula shows how @xmath10 is a normalizing constant .",
    "@xmath11    thus the marginal likelihood @xmath10 is also the normalizing constant of the posterior parameter distribution @xmath12 assuming we are given the density @xmath13 which is often easy to compute in bayesian models .",
    "furthermore , bayesian statisticians typically produce samples from the posterior parameter distribution @xmath12 even when not concerned with theory choice . in these case , computing the marginal likelihood is equivalent to computing the normalizing constant of a distribution from which samples and the scaled density at these samples are available .",
    "the method described in this paper takes this approach .",
    "given how basic ( [ bayes factor ] ) is , it is perhaps surprising that there is no easy and definitive way of applying it , even for simple models .",
    "furthermore , as the dimensionality and complexity of probability distributions increase , the difficulty of approximation also increases .",
    "the following three techniques for computing bayes factors or marginal likelihoods are important but will not be mentioned further here .    1 .   analytic asymptotic approximations such as laplace s method ,",
    "see for instance kass and raftery ( 1995 ) , 2 .   bridge sampling / path sampling / thermodynamic integration ( gelman and meng , 1998 ) , and 3",
    ".   chib s mcmc approximation ( chib , 1995 ; chib and jeliazkov , 2005 ) .    kass and raftery ( 1995 ) is a popular overview of the earlier literature on bayes factor computation .",
    "all these methods can be very successful in the right circumstances , and can often handle problems too complex for the method described here",
    ". however , the method of this paper may still be useful due to its convenience .",
    "the rest of section [ lit review ] describes three approaches that are relevant to this paper .",
    "importance sampling is a technique for reducing the variance of monte carlo integration .",
    "this section will note some general facts ; see owen and zhou ( 1998 ) for more information .",
    "suppose we are trying to compute the ( possibly multidimensional ) integral @xmath14 of a well - behaved function @xmath15 .",
    "then    @xmath16    so if @xmath17 is a probability density function and @xmath18 are independent samples from it , then    @xmath19 \\approx \\frac{1}{n } \\sum_{i=1}^n",
    "\\frac{f({\\boldsymbol \\theta}_i)}{g({\\boldsymbol \\theta}_i ) } = i_n.\\ ] ]    @xmath20 is an unbiased approximation to @xmath14 and by the central limit theorem will tend to a normal distribution .",
    "it has variance    @xmath21 = \\frac{1}{n } \\int \\left(\\frac{f({\\boldsymbol \\theta})}{g({\\boldsymbol \\theta } ) } - i\\right)^2 g({\\boldsymbol \\theta})\\ , d{\\boldsymbol \\theta}=     \\frac{1}{n } \\int \\frac{(f({\\boldsymbol \\theta } ) - ig({\\boldsymbol \\theta}))^2}{g({\\boldsymbol \\theta})}\\ , d{\\boldsymbol \\theta}\\ ] ]    sometimes @xmath22 is called the _ target _ and @xmath23",
    "is called the _ proposal _ distribution .    assuming that @xmath22 is non - negative , then minimum variance ( of @xmath24 ) is achieved when @xmath25in other words when @xmath23 is just the normalized version of @xmath22 .",
    "this can not be done in practice because normalizing @xmath22 requires knowing the quantity @xmath14 that we wanted to approximate ; however ( [ imp var ] ) is still important because it means that the more similar the proposal is to the target , the better our estimator @xmath20 becomes . in particular , @xmath22 must go to 0 faster than @xmath23 or the estimator will have infinite variance .    to summarize this section :    1 .",
    "importance sampling is a monte carlo integration technique which evaluates the target using samples from a proposal distribution .",
    "the estimator is unbiased , normally distributed , and its variance ( if not 0 or infinity ) decreases as @xmath26 ( using big-@xmath27 notation ) .",
    "3 .   the closer the proposal is to the target , the better the estimator .",
    "the proposal also needs to have longer tails than the target .",
    "a difficulty with importance sampling is that it is often difficult to choose a proposal distribution @xmath23 .",
    "not enough is known about @xmath22 to choose an optimal distribution , and if a bad distribution is chosen the result can have large or even infinite variance .",
    "one approach to the selection of proposal @xmath23 is to use non - parametric techniques to build @xmath23 from samples of @xmath22 .",
    "i call this class of techniques self - importance sampling , or * arrogance sampling * for short , because they attempt to sample @xmath22 from itself without using any external information .",
    "( and also is nt it a bit arrogant to try to evaluate a complex , multidimensional integral using only the values at a few points ? ) the method of this paper falls into this class and particularly deserves the name because the target and proposal ( when they are both non - zero ) have exactly the same values up to a multiplicative constant",
    ".    two papers which apply nonparametric importance sampling to the problem of marginal likelihood computation ( or computation of normalizing constants ) are zhang ( 1996 ) and neddermeyer ( 2009 ) .",
    "although both authors apply their methods to more general situations , here i will use the framework suggested by ( [ norm constant ] ) and assume that we can compute @xmath13 for arbitrary @xmath7 and also that we can sample from the posterior parameter distribution @xmath12 .",
    "the goal is to estimate the normalizing constant , the marginal likelihood @xmath10 .",
    "zhang s approach is to build the proposal @xmath23 using traditional kernel density estimation .",
    "@xmath28 samples are first drawn from @xmath29 and used to construct @xmath23 . then @xmath30 samples are drawn from @xmath23 and used to evaluate @xmath10 as in traditional importance sampling .",
    "this approach is quite intuitive because kernel estimation is a popular way of approximating an unknown function .",
    "zhang proves that the variance of his estimator decreases as @xmath31 where @xmath32 is the dimensionality of @xmath7 , compared to @xmath26 for standard ( parametric ) importance sampling .",
    "there were , however , a few issues with zhang s method :    1 .",
    "a kernel density estimate is equal to 0 at points far from the points the kernel estimator was built on .",
    "this is a problem because importance sampling requires the proposal to have longer tails than the target .",
    "this fact forces zhang to make the restrictive assumption that @xmath33 has compact support .",
    "it is hard to compute the optimal kernel bandwidth .",
    "zhang recommends using a plug - in estimator because the function @xmath13 is available , which is unusual for kernel estimation problems .",
    "still , bandwidth selection appears to require significant additional analysis .",
    "3 .   finally , although the variance may decrease as @xmath31 as @xmath28 increases , the difficulty of computing @xmath17 also increases with @xmath28 , because it requires searching through the @xmath28 basis points to find all the points close to @xmath7 . in multiple dimensions ,",
    "this problem is not trivial and may outweigh the @xmath34 speedup ( in the worst case , practical evaluation of @xmath17 at a single point may be @xmath35 ) .",
    "see zlochin and baram ( 2002 ) for some discussion of these issues .",
    "neddermeyer ( 2009 ) uses a similar approach to zhang and also achieves a variance of @xmath31 .",
    "it improves on zhang s approach in two ways relevant to this paper :    1 .   the support of @xmath33 is not required to be compact .",
    "2 .   instead of using kernel density estimators , linear blend frequency polynomials ( lbfps ) are used instead .",
    "lbfps are basically histograms whose density is interpolated between adjacent bins . as a result , the computation of @xmath17 requires only finding which bin @xmath7 is in , and looking up the histogram value at that and adjacent bins ( @xmath36 bins in total ) .",
    "as we will see in section [ my technique ] , the arrogance sampling described in this paper is similar to the methods of zhang and neddermeyer .",
    "the harmonic mean estimator is a simple and notorious method for calculating marginal likelihoods .",
    "it is a kind of importance sampling , except the proposal @xmath23 is actually the distribution @xmath37 to be normalized and the target @xmath22 is the known distribution @xmath38 . then if @xmath18 are samples from @xmath39 , we apparently have    @xmath40    hence    @xmath41    two advantages of the harmonic mean estimator are that it is simple to compute and only depends on samples from @xmath42 and the likelihood @xmath43 at those samples .",
    "the main drawback of the harmonic mean estimator is that it does nt work  as mentioned earlier the importance sampling proposal distribution needs to have longer tails than the target . in this case",
    "the target @xmath38 typically has longer tails than the proposal @xmath12 and thus ( [ hme ] ) has infinite variance . despite not working ,",
    "the harmonic mean estimator continues to be popular ( neal , 2008 ) .",
    "this paper s arrogance sampling technique is a simple method that applies the nonparametric importance techniques of zhang and neddermeyer in an attempt to develop a method almost as convenient as the harmonic mean estimator .",
    "the only required inputs are samples @xmath18 from @xmath29 and the values @xmath44 .",
    "this is similar to the harmonic mean estimator , but perhaps slightly less convenient because @xmath45 is required instead of @xmath46 .",
    "there are two basic steps :    1 .",
    "take @xmath28 samples from @xmath12 and using modified histogram density estimation , construct probability density function @xmath15 .",
    "2 .   with @xmath30",
    "more samples from @xmath12 , estimate @xmath47 via importance sampling with target @xmath22 and proposal @xmath12 .",
    "these steps are described in more detail below .      of the @xmath48 total samples @xmath18 from @xmath12",
    ", the first @xmath28 will be used to make a histogram .",
    "the optimal choice of @xmath28 will be discussed below , but in practice this seems difficult to determine .",
    "an arbitrary rule of @xmath49 can be used in practice .    with a traditional histogram ,",
    "the only available information is the location of the sampled points . in this case",
    "we also know the ( scaled ) heights @xmath13 at each sampled point .",
    "we can use this extra information to improve the fit .",
    "our `` arrogant '' histogram @xmath22 is constructed the same as a regular histogram , except the bin heights are not determined by the number of points in each bin , but rather by the minimum density over all points in the bin . if a bin contains no sampled points , then @xmath50 for @xmath7 in that bin .",
    "then @xmath22 is normalized so that @xmath51 .    to determine our bin width",
    ", we can simply and somewhat arbitrarily set our bin width @xmath52 so that the histogram is positive for 50% of the sampled points from the distribution @xmath12 .",
    "to approximate @xmath52 , we can use a small number of samples ( say , 40 ) from @xmath12 and set @xmath52 so that @xmath53 for exactly half of these samples .",
    "figure [ histograms ] compares the traditional and new histograms for a one dimensional normal distribution based on 50 samples .",
    "the green rug lines indicate the @xmath54 sampled points which are the same for all .",
    "the arrogant histogram s bin width is chosen as above .",
    "the traditional histogram s optimal bin width was determined by scott s rule to minimize mean squared error . as the figure shows , the modified histogram is much smoother for a given bin width , so a smaller bin width can be used . on the other hand , @xmath22 will either equal 0 or have about twice the original density at each point , while the traditional histogram s density is numerically close to the original density .",
    "the remaining @xmath55 sampled points can be used for importance sampling . using equation ( [ imp approx ] ) with histogram @xmath22 as our target and @xmath12 as the proposal , we have    @xmath56    hence    @xmath57    to underscore the self - important / arrogant nature of this approximation @xmath58 , we can rewrite ( [ arrogance ] ) as    @xmath59    where @xmath60 is the histogram normalizing constant .",
    "this equation shows that all the values in the numerator and the denominator of our importance sampling are from the same distribution @xmath13 .",
    "note that the histogram @xmath22 is the target of the importance sampling and @xmath13 is the proposal .",
    "this is backwards from the usual scheme where the unknown distribution is the target and the known distribution is the proposal .",
    "instead here the unknown distribution is the proposal , as in the harmonic mean estimator ( see robert and wraith ( 2009 ) for another example of this . )    as in section [ imp sampling ] , our approximation of @xmath61 tends to a normal distribution as @xmath62 by the central limit theorem .",
    "this fact can be used to estimate a confidence interval around @xmath10 .",
    "this section will investigate the performance of the method .",
    "first , note that this method is just an implementation of importance sampling , so @xmath63 should converge to @xmath61 with finite variance as long as the proposal density @xmath12 exists and is finite and positive on the compact region where the target histogram density is positive .    to calculate the speed of convergence we will use equation ( [ imp var ] ) where @xmath22 is the histogram , @xmath64 , and @xmath65 because the histogram has been normalized .",
    "unless otherwise noted , we will assume below that @xmath66 is finite , twice differentiable and positive , and that @xmath67 is finite .",
    "one important issue will be how quickly the @xmath32-dimensional histogram s selected bin width @xmath52 goes to 0 as the number of samples @xmath68 .",
    "this section will only offer an intuitive argument . for any @xmath28",
    ", the histogram will enclose about the same probability ( @xmath69 ) and will have about the same average density in a fixed region .",
    "each bin has volume @xmath70 , so if @xmath71 is the number of bins then @xmath72 and @xmath73 .",
    "furthermore , the distribution of the sampled points converges to the actual distribution @xmath17 . if @xmath74 , an unbounded number of sampled points would end up in each bin .",
    "if @xmath75 , then some bins would have no points in them .",
    "neither of these is possible because exactly one sampled point is necessary to establish each bin .",
    "thus @xmath76 and @xmath77 .",
    "before estimating the convergence rate of @xmath58 we will prove something about the conditional variance of importance sampling .",
    "let @xmath78 , @xmath79 be the characteristic function of @xmath80 , and @xmath81 .",
    "define    @xmath82    then @xmath83 is the density of @xmath23 conditional on @xmath84 .",
    "define @xmath85 and @xmath86 to mean the variance and expectation conditional on @xmath53 .",
    "thus    @xmath87    we will assume below that @xmath88 , so that    @xmath89      with @xmath22 , @xmath23 , and @xmath80 as defined above , @xmath22 and @xmath83 have the same domain . assuming errors in estimating @xmath90 and normalization errors are of a lesser order of magnitude , we can treat the histogram heights as being sampled from @xmath83 .",
    "suppose the histogram has @xmath71 bins @xmath91 , each with width @xmath52 and based around the points @xmath92 .",
    "then by equation ( [ imp var ] ) ,    @xmath93    because @xmath77 where @xmath32 is the number of dimensions , and @xmath28 is the number of samples used to make the histogram ,    @xmath94    where @xmath95 . putting this together with ( [ i var ] ) , we get    @xmath96",
    "the variance of @xmath97 given by ( [ final variance ] ) is asymptotically equal to @xmath98 , which is the typical importance sampling rate . in practice however , the asymptotic results can not distinguish useful from impractical estimators . if @xmath99 is small and @xmath100 , then @xmath10 can be approximated in only 1000 samples to about @xmath101 with 95% confidence .",
    "for many theory choice purposes , this is quite sufficient .",
    "thus in typical problem cases the factor of @xmath99 will be very significant .",
    "if @xmath102 , then the convergence rate may in practice be similar to @xmath103 .",
    "compare this to the rate of @xmath104 for the methods proposed by zhang and neddermeyer .",
    "this method also uses simple histograms , instead of a more sophisticated density estimation method ( zhang uses kernel estimation , neddermeyer uses linear blend frequency polynomials ) . although simple histograms converge slower for large @xmath32 as shown above , they are much faster to compute for large @xmath32 .",
    "neddermeyer s lbfp algorithm is quite efficient compared to zhang s , but its running time is @xmath105 .",
    "@xmath32 is a constant for any fixed problem , but if , say , @xmath106 , then the dimensionality constant multiplies the running time by @xmath107 .    by contrast , this paper s method takes only @xmath108 time to construct the initial histogram , and an additional @xmath109 time to do the importance sampling .",
    "the main reason for the difference is that querying a simple histogram can be done in @xmath110 time by computing the bin coordinates and looking up the bin s height in a tree structure .",
    "however , querying a lbfp requires blending all nearby bins and is thus exponential in @xmath32 .",
    "our discussion assumed that @xmath64 was always positive .",
    "if @xmath23 goes to 0 where the histogram is positive , the variance of @xmath63 will be infinite .",
    "however , this paper s method can still be used if @xmath17 is 0 over some well - defined area .",
    "for instance , suppose one dimension @xmath112 of @xmath38 is defined by a gamma distribution , so that @xmath113 if and only if @xmath114 . then we can ensure the variance is not infinite by checking that the histogram is only defined where @xmath115 for some fixed @xmath116 .",
    "the ` marglikarrogance ` package contains a simple mechanism to do this .",
    "the user may specify a range along each dimension of @xmath7 where it is known that @xmath117 .",
    "if the histogram is non - zero outside of this range , the method aborts with an error .    note that the variance of the estimator increases with @xmath118 . in practice",
    "the estimator will work well only when @xmath23 does nt go to 0 too quickly where the histogram is positive . in these cases",
    "the histogram will be defined well away from any region where @xmath119 and infinite variance wo nt be an issue even if @xmath119 somewhere .",
    "cubic histogram bins were used above  their widths were fixed at @xmath52 in each dimension . although the asymptotic results are nt affected by the shape of each bin , for usable convergence rates the bins dimensions need to compatible with the shape of the high probability region of @xmath12 .",
    "unfortunately , it is difficult to determine the best bin shapes .",
    "the ` marglikarrogance ` package contains a simple workaround : by default the distribution is first scaled so that the sampled standard deviation along each dimension is constant .",
    "this is equivalent to setting each bin s width by dimension in proportion to that dimension s standard deviation .",
    "if this simple rule of thumb is insufficient , the user can scale the sampled values of @xmath29 manually ( and make the corresponding adjustment to the estimate @xmath58 ) .",
    "this paper has described an `` arrogance sampling '' technique for computing the marginal likelihood or bayes factor of a bayesian model .",
    "it involves using samples from the model s posterior parameter distribution along with the scaled values of the distribution s density at those points .",
    "these samples are divided into two main groups : @xmath28 samples are used to build a histogram ; @xmath30 are used to importance sample the histogram using the posterior parameter distribution as the proposal .",
    "this method is simple to implement and runs quickly in @xmath120 time .",
    "its asymptotic convergence rate , @xmath97 , is not remarkable , but in practice convergence is fast for many problems .",
    "because the required inputs are similar to those of the harmonic mean estimator , it may be a convenient replacement for it .",
    "1 .   s. chib .",
    "`` marginal likelihood from the gibbs output '' _ journal of the american statistical association_. vol 90 , no 432 .",
    "( 1995 ) 2 .",
    "s. chib and i. jeliazkov .",
    "`` accept - reject metropolis - hastings sampling and marginal likelihood estimation '' _ statistica neerlandica_. vol 59 , no 1 .",
    "( 2005 ) 3 .",
    "a. gelman and x. meng .",
    "`` simulating normalizing constants : from importance sampling to bridge sampling to path sampling '' _ statistical science_. vol 13 , no 2 .",
    "( 1998 ) 4 .",
    "r. kass and a. raftery .",
    "`` bayes factors '' _ journal of the american statistical association_. vol 90 , no 430 .",
    "( 1995 ) 5 .",
    "`` the harmonic mean of the likelihood : worst monte carlo method ever '' . blog post , ` http://radfordneal.wordpress.com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-monte-carlo-method-ever/ ` .",
    "( 2008 ) 6 .",
    "j. neddermeyer .",
    "`` computationally efficient nonparametric importance sampling '' _ journal of the american statistical association_. vol 104 , no 486 .",
    "( 2009 ) arxiv:0805.3591v2 7 .",
    "a. owen and y. zhou .",
    "`` safe and effective importance sampling '' _ journal of the american statistical association_. vol 95 , no 449 .",
    "( 2000 ) 8 .   c. robert and d. wraith .",
    "`` computational methods for bayesian model choice '' arxiv:0907.5123v1 9 .",
    "`` nonparametric importance sampling '' _ journal of the american statistical association_. vol 91 , no 435 .",
    "( 1996 ) 10 .",
    "m. zlochin and y. baram . `` efficient nonparametric importance sampling for bayesian inference '' _ proceedings of the 2002 international joint conference on neural networks _"
  ],
  "abstract_text": [
    "<S> this paper describes a method for estimating the marginal likelihood or bayes factors of bayesian models using non - parametric importance sampling ( `` arrogance sampling '' ) . </S>",
    "<S> this method can also be used to compute the normalizing constant of probability distributions . </S>",
    "<S> because the required inputs are samples from the distribution to be normalized and the scaled density at those samples , this method may be a convenient replacement for the harmonic mean estimator . </S>",
    "<S> the method has been implemented in the open source r package ` marglikarrogance ` .    </S>",
    "<S> marginal likelihood estimation via arrogance sampling   +   + </S>"
  ]
}