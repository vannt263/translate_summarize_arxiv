{
  "article_text": [
    "in spite of the availability of high performance multi - core systems based on traditional architectures , there is recently a renewed interest in floating point _ accelerators _ and _ co - processors _ that can be defined as devices that carry out arithmetic operations concurrently with or in place of the cpu . among the solutions that have received more attention from the high performance computing community",
    "there are the nvidia graphics processing units ( gpu ) , originally developed for video cards and graphics , since they are able to support very demanding computational tasks . as a matter of fact ,",
    "astonishing results have been reported by using them for a number of applications covering , among others , atomistic simulations , fluid - dynamic solvers and option pricing .",
    "simulations of statistical mechanics systems based on montecarlo techniques are another example of applications that may benefit of the gpu computing capabilities . in the present work",
    "we report the results obtained by following different approaches for the implementation of a typical statistical mechanics system : the classic heisenberg spin glass model .",
    "the paper is organized as follows : section [ sec : spin ] contains a short introduction to the features of spin systems that are of interest from the computational viewpoint ; section [ sec : gpu ] summarizes the main features of the gpus used for the experiments ; section [ sec : results ] presents the performances obtained by using a number of possible approaches for the implementation of the 3d heisenberg spin glass model ; section [ sec : conc ] concludes with a summary of the main results and a perspective about possible future activities in this field .",
    "in statistical mechanics `` spin system '' indicates a broad class of models used for the description of a number of physical phenomena .",
    "although apparently quite simple , spin systems are far from being trivial to be studied and most of the times numerical simulations ( very often based on montecarlo methods ) are the only way to understand their behaviour .",
    "a spin system is usually described by its hamiltonian which has the following general form @xmath0 the spins are defined on _ lattice _ which may have one , two , three or even a higher number of dimensions .",
    "the sum in equation [ equ : spinequ ] runs usually on the first neighbors of each spin ( 2 in 1d , 4 in 2d and 6 in 3d ) .",
    "the spin @xmath1 and the coupling @xmath2 may be either discrete or continuous and their values determine the specific model . in the present work we focus on the heisenberg spin glass model where @xmath3 is a 3-component vector such that @xmath4 and @xmath5 is gaussian distributed with average value equal to @xmath6 and variance equal to @xmath7 .",
    "+ in a 3-dimensional system of size @xmath8 , the contribution to the total energy of the spin @xmath9 with coordinates @xmath10 such that @xmath11 is @xmath12 where @xmath13 indicates the scalar product of two @xmath1 vectors . in most montecarlo techniques used for the simulation of the heisenberg spin glass model ( metropolis , _ heat bath _ , _ etc .",
    "_ ) it is necessary to evaluate the expression in equation [ equ : staple ] for each spin .",
    "the main goal of the present work is to present several approaches and to assess what is the most effective scheme to compute this expression on a gpu . as a consequence we are not going to address other issues , like the generation of random numbers , even if we understand their importance for an efficient gpu based simulation of spin systems , because they are already faced in other studies @xcite",
    "actually , other authors already described efficient techniques for the simulation , on gpu , of spin systems ( _ e.g. _ , @xcite for the ising model in 2d and 3d and @xcite for the three - dimensional heisenberg anisotropic model ) . however their analysis appears somehow limited since they present results basically for a single implementation whereas a gpu offers several alternatives for an effective implementation that deserve to be considered and analyzed .",
    "in table [ table : hw ] we report the key aspects of the three gpus we used for our numerical experiments : a tesla c1060 , a tesla c2050 and a gtx 480 .",
    "the c2050 and the gtx 480 are based on the latest architecture ( `` fermi '' ) recently introduced by nvidia .",
    "[ table : hw ]    .main features of the nvidia gpus used for the experiments [ cols=\"<,>,>,>\",options=\"header \" , ]",
    "in table [ tab : hits ] we report the results of a test in which the spins of the version sm4p are updated multiple times ( `` multi hit '' ) before moving to the next two planes .",
    "since the updates after the first one do not require new accesses to the global memory ( all data are already in the shared memory ) this technique allows to measure the overhead introduced by the initial loading of the data as the difference between the total time required by the single hit and the time required by an infinite number of hits .",
    "obviously it is not possible to carry out an infinite number of hits but it is easy to extrapolate the corresponding value by using a simple fit .",
    "the time we found is @xmath14 ns for both the compilation setups used for the tests .",
    "interestingly , at this time , it is more efficient to compile on the fermi architecture with a setup ( _ arch = sm_13 _ ) introduced for the previous generation gpus . on the other hand it is absolutely necessary to take into account the specific features of the new architecture like the different size of the shared memory or the new integer multiplication capability .",
    "for instance , if on the fermi architecture the multiplication between two integers is carried out by using the ` _ _",
    "mul24 ` ( as it was suggested on the previous generation gpus when the result of the multiplication fitted in 24 bits ) there is a penalty of about @xmath15 for our code .",
    "if we consider the time ( @xmath16 ns per spin update ) required by the best `` single - hit '' technique , that is the texture based one , and use the estimate of 60 floating point operations per spin update ( that does not take into account the index arithmetics ) , we obtain a sustained performance very close to @xmath17 gflops .",
    "besides that , the most interesting observation is that , despite the much higher latency of the global memory , the simple versions ( the gm one and the text one ) that use neither the shared memory nor the cache ( where available ) are significantly faster unless a `` multi - hit '' variant of the shared memory version is used .",
    "a possible explanation of this behaviour is that the limited size of the shared memory ( and of the cache ) allows to start fewer threads with respect to the case of a global memory based version where the only limitation is the number of registers .",
    "moreover , although the shared memory version allows to reduce the number of global memory transactions when loading data ( spins and couplings ) , it offers no advantage when the updated spins are stored back in global memory .",
    "the advantage of using texture fetches instead of simple load operations from the global memory is quite limited ( about @xmath15 ) .",
    "however , only very minor changes are required to the code to take this advantage so it is worth it .",
    "it is more puzzling that , apparently the cache does not offer advantages at all .",
    "a possible explanation is that the overhead introduced by calling the computational kernel a number of times equal to twice the linear dimension of the lattice ( instead of calling it just once as we do in the gm or text version ) is as large and possibly larger than the saving in time that the cache may offer .",
    "the requirement of calling the kernel multiple times arises because on each plane it is necessary to update all white spins and then all black spins .",
    "so , for each step in the @xmath18 direction , two invocations are required .",
    "+ as shown in table [ tab : ecc ] , the error checking and correction introduces a sizeable overhead for the methods working in global memory ( gm , text , ca ) . for reasons that we could not determine ,",
    "the methods working in shared memory undergo a much more limited penalty when the ecc is on .",
    "+ finally , it is worth to highlight two facts : _",
    "i ) _ a well tuned gpu implementation can achieve almost an order of magnitude of speedup with respect to a well tuned vector - multicore implementation ( @xmath16 ns per spin update for the text version on the gtx 480 _ vs. _",
    "@xmath19 ns per spin update on a multicore intel _ i7 _ ; _ ii ) _ the performances of the gpus keep on to increase significantly in a pretty short time ( the performances of the new architecture are double and in some cases even more than double with respect to the previous generation cards that are only two years old ) making it a very interesting platform for the numerical simulation of spin systems .",
    "we thank luis antonio fernandez , filippo mantovani , victor martin - mayor , sergio perez , fabio schifano and raffaele tripiccione for useful discussions . +",
    "we thank massimiliano fatica for the _ hint _ about the three planes technique and for running preliminary tests on fermi architecture based gpus .",
    "+ we thank luca leuzzi and edmondo silvestri for the access to their gtx 480 gpu . + special thanks to jos manuel sanz gonzlez for providing us with his program for simulating the heisenberg spin glass ( including anisotropic couplings ) and sharing his preliminary performance results ."
  ],
  "abstract_text": [
    "<S> we describe different implementations of the 3d heisenberg spin glass model for graphics processing units ( gpu ) . </S>",
    "<S> the results show that the _ fast _ shared memory gives better performance with respect to the _ slow _ global memory only if a multi - hit technique is used .    </S>",
    "<S> _ keywords _ : spin systems , gpu , vector processing . </S>"
  ]
}