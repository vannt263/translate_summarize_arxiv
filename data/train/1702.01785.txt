{
  "article_text": [
    "big data platforms have evolved over the last decade to address the unique challenges posed by the ability to collect data at vast scales , and the need to process them rapidly .",
    "these platforms have also leveraged the availability of distributed computing resources , such as commodity clusters and clouds , to allow the application to scale out as data sizes grow .",
    "in particular , platforms like apache hadoop and spark have allowed massive data volumes to be processed with high throughput , and nosql databases like hive and hbase support low latency queries over semi - structured data at large scales .",
    "however , much of the research and innovation in big data platforms has skewed toward the _ volume _ rather than the _ velocity _ dimension  @xcite . on the other hand ,",
    "the growing prevalence of internet of things ( iot ) is contributing to the deployment of physical and virtual sensors to monitor and respond to infrastructure , nature and human activity is leading to a rapid influx of streaming data  @xcite .",
    "these emerging applications complement the existing needs of micro - blogs like twitter that already contend with the need to rapidly process tweet streams for detecting trends or malicious activity  @xcite .",
    "such streaming applications require low - latency processing and analysis of data streams to take decisions that control the physical or digital eco - system they observe .",
    "a _ distributed stream processing system ( dsps ) _ is a big data platform designed for online processing of such data streams  @xcite .",
    "while early stream processing systems date back to applications on wireless sensor networks  @xcite , contemporary dsps s such as apache storm from twitter , flink and spark streaming are designed to execute complex dataflows over tuple streams using commodity clusters  @xcite .",
    "these dataflows are typically designed as directed acyclic graphs ( dags ) , where user tasks are vertices and streams are edges .",
    "they can leverage data parallelism across tuples in the stream using multiple threads of execution per task , in addition to pipelined and task - parallel execution of the dag , and have been shown to process @xmath0 of tuples per second  @xcite .",
    "a dsps executes streaming dataflow applications on distributed resources such as commodity clusters and cloud virtual machines ( vms ) . in order to meet the required performance for these applications ,",
    "the dsps needs to schedule these dataflows efficiently over the resources .",
    "scheduling for a dsps has two parts : _ resource allocation _ and _ resource mapping_. the former determines the appropriate degrees of parallelism per task ( e.g. , threads of execution ) and quanta of computing resources ( e.g. , number and type of vms ) for the given dataflow . here , care has to be taken to avoid both over - allocation , that can have monetary costs when using cloud vms , or under - allocation , that can impact performance .",
    "resource mapping decides the specific assignment of the threads to the vms to ensure that the expected performance behavior and resource utilization is met .    despite their growing use",
    ", resource scheduling for dspss tends to be done in an _ ad hoc _ manner , favoring empirical and reactive approaches , rather than a model - driven and analytical approach .",
    "such empirical approaches may arrive at an approximate resource allocation for the dsps based on a linear extrapolation of the resource needs and performance of dataflow tasks , and hand - tune these to meet the quality of service ( qos )  @xcite .",
    "mapping of tasks to resources may be round - robin or consider data locality and resource capacities  @xcite .",
    "more sophisticated research techniques support dynamic scheduling by monitoring the queue waiting times or tuple latencies to incrementally increase / decrease the degrees of parallelism for individual tasks or the number of vms they run on  @xcite .",
    "while these dynamic techniques have the advantage of working for arbitrary dataflows and stream rates , such schedules can lead to local optima for individual tasks without regard to global efficiency of the dataflow , introduce latency and cost overheads due to constant changes to the mapping , or offer weaker guarantees for the qos .    in this article",
    ", we propose a model - driven approach for scheduling streaming applications that effectively utilizes _ a priori _ knowledge of the applications to provide predictable scheduling behavior .",
    "specifically , we leverage our observation that dataflow tasks have diverse performance behavior as the degree of parallelism increases , and use performance models to offer reliable estimates of the resource allocation required .",
    "further , this intuition also drives resource mapping to mitigate the impact of multi - tenancy of different tasks on the same resource , and helps narrow the estimated and actual dataflow performance and resource utilization .",
    "together , this model - driven scheduling approach gives a predictable application performance and resource utilization behavior for executing a given dsps application at a target input stream rate on distributed resources .",
    "often , importance is given to lower latency of resource usage rather than predictable behavior .",
    "but in stream processing that can be latency sensitive , it may be more important to offer tighter bounds rather than lower bounds .",
    "we limit this article to static scheduling of the dataflow on distributed resources , before the application starts running .",
    "this is complementary to dynamic scheduling algorithms that can react to changes in the stream rates  @xcite , application composition  @xcite and make use of cloud elasticity  @xcite .",
    "however , our work can be extended and applied to a dynamic context as well . rather than incrementally increase or decrease resource allocation and the mapping until the qos stabilizes , a dynamic algorithm can make use of our model to converge to a stable configuration more rapidly .",
    "our work is of particular use for enterprises and service providers who have a large class of infrastructure applications that are run frequently  @xcite , or who reuse a library of common tasks when composing their applications  @xcite , as is common in the scientific workflow community  @xcite .",
    "this amortizes the cost of building task - level performance models .",
    "our approach also resembles scheduling in hpc centers that typically have a captive set of scientific applications that can benefit from such a model - driven approach  @xcite  @xcite .    specifically , we make the following key contributions in this article :    1 .",
    "we highlight the gap between ideal and actual performance of dataflow tasks using performance models , that causes many existing dsps scheduling algorithms to fail and motivates our model - based approach for reliable scheduling .",
    "we propose an allocation and a mapping algorithm that leverage these performance models to schedule dsps dataflows for a fixed input rate , minimizing the distributed resources used and offering predictable performance behavior .",
    "we offer detailed experimental results and analysis evaluating our scheduling algorithm using the apache storm dsps , and compare it against the state - of - the - art scheduling approaches , for micro and application dataflows .",
    "the rest of the article is organized as follows :   [ sec : bg ] introduces the problem _ motivation _ and   [ sec : problem ] formalizes the scheduling _ problem _ ;   [ sec : approach ] offers a high - level intuition of the analytical _ approach _ taken to solving the problem ;   [ sec : bm ] offers evidence on the diversity of task s behavior using _ performance models _ , leveraged in the solution ;   [ sec : allocation ] proposes a novel _ model based allocation ( mba ) algorithm _ using these models , and also describes a linear scaling allocation ( lsa ) used as a contemporary baseline ;   [ sec : mapping ] presents our _ slot - aware mapping ( sam ) algorithm _ that leverages thread - locality in a resource slot , and lists two existing algorithms from literature and practice used as comparison ; we offer detailed experimental _ results and analysis _ of these allocation and mapping algorithms in   [ sec : results ] ; contrast our work against _ related literature _ in ",
    "[ sec : related ] ; and lastly , present our _ conclusions _ in   [ sec : conclusions ] .",
    "we offer an overview of the generic composition and execution model favored by contemporary dspss such as apache storm , apache spark streaming , apache flink and ibm infosphere streams in this section .",
    "we further use this to motivate the specific research challenges and technical problems we address in this article ; a formal definition follows in the subsequent section ,   [ sec : problem ] .",
    "while we use features and limitations of the popular apache storm as a representative dsps here , similar features and short - comings of other dspss are discussed in the related work ,   [ sec : related ] .",
    "streaming applications are composed as a dataflow in a dsps , represented as a _ directed acyclic graph ( dag ) _ , where _ tasks _ form vertices and _ tuple streams _ are the edges connecting the output of one task to the input of its downstream task .",
    "contents of the _ tuples _ ( also called _ events _ or _ messages _ ) are generally opaque to the dsps , except for special fields like ids and keys that may be present for recovery and routing . _ source tasks _ in the dag contain user logic responsible for acquiring and generating the initial input stream to the dag , say by subscribing to a message broker or pulling events over the network from sensors .",
    "for other tasks , their logic is executed once for each input tuple arriving at that task , and may produce zero or more output tuples for each invocation .",
    "these output tuples are passed to downstream tasks in the dag , and so on till the _ sink tasks _ are reached .",
    "these sinks do not emit any output stream but may store the tuples or notify external services .",
    "apache storm uses the terms _ topology _ and _ component _ for a dag and a task , and more specifically _ spout _ and _ bolt _ for source tasks and non - source tasks , respectively .",
    "multiple outgoing edges connecting one task to downstream tasks may indicate different _ routing semantics _ for output tuples on that edge , based on the application definition ",
    "tuples may be _ duplicated _ to all downstream tasks , passed in a _ round - robin _ manner to the tasks , or _ mapped _",
    "based on an output key in the tuple .",
    "likewise , multiple input streams incident on a task typically have their tuples _ interleaved _ into a single logical stream , though semantics such as _ joins _ across tuples from different input streams are possible as well .",
    "further , the _ selectivity _ of an outgoing edge of a task is the ratio between the average number of tuples generated on that output stream for each input tuple to the task .",
    "streaming applications are designed to process tuples with low latency .",
    "the _ end - to - end latency _ for processing an input tuple from the source to the sink task(s ) is typically a measure of the _ quality of service ( qos ) _ expected .",
    "this qos depends on both the _ input stream rate _ at the source task(s ) and the resource allocation to the tasks in the dsps .",
    "a key requirement is that the execution performance of the streaming application remains _ stable _ ,",
    "i.e. , the end - to - end latency is maintained within a narrow range over time and the queue size at each task does not grow .",
    "otherwise , an unstable application can lead to an exponential growth in the latency and the queue size , causing hosts to run out of memory .    the execution model of a dsps",
    "can naturally leverage _ pipelining _ and _ task parallelism _ due to the composition of linear and concurrent tasks in the dag , respectively .",
    "these benefits are bound by the length and the number of tasks in the dag .",
    "in addition , they also actively make use of _ data - parallel _ execution for a single task by assigning multiple threads of execution that can each operate on an independent tuple in the input stream .",
    "this data parallelism is typically limited to stateless tasks , where threads of execution for a task do not share a global variable or state , such as a sum and a count for aggregation ; stateful tasks require more involved distributed coordination  @xcite .",
    "stateless tasks are common in streaming dataflows , allowing the dsps to make use of this _ important dimension of parallelism that is not limited by the dataflow size but rather the stream rate and resource availability . _    in operational dspss such as apache storm , yahoo s4  @xcite , twitter heron  @xcite and ibm infosphere streams  @xcite , the platform expects the application developer to provide the _ number of threads _ or degrees of data parallelism that should be exploited for a single task .",
    "as we show in   [ sec : bm ] , general rules of thumb are inadequate for deciding this and both over- and under - allocation of threads can have a negative effect on performance . this value may also change with the input rate of the stream .",
    "thread allocation is one of the challenges we tackle in this paper .",
    "in addition , the user is responsible for deciding the _ number of compute resources _ to be allocated to the dag .",
    "typically , as with many big data platforms , each host or virtual machine ( vm ) in the dsps cluster exposes multiple _ resource slots _ , and those many _",
    "worker processes _ can be run on the host .",
    "typically , there are as many workers as the number of cpu cores in that host .",
    "each worker process can internally run multiple threads for one or more tasks in the dag .",
    "the user specifies the number of hosts or slots in the dsps cluster to be used for the given dag when it is submitted for execution . for e.g. , in apache storm , the threads for the dataflow can use all available resource slots in the dsps cluster by default . in practice , this again ends up being a trial and error empirical process for the user or the system to decide the resource allocation for the dag , and will change for each dag or the input rate that it needs to support .",
    "ascertaining the cloud vm resource allocation for the given dag and input rate is another problem that we address in this article , and this is equally applicable to commodity hosts in a cluster as well .",
    "the dsps , on receiving a dag for scheduling , is responsible for deploying the dataflow on the cluster and coordinating its execution . as part of this deployment",
    ", it needs to decide the mapping from the threads of the tasks to the slots in the workers .",
    "there has been prior work on making this mapping efficient for stream processing platforms  @xcite  @xcite . for e.g. , the native scheduler of apache storm",
    "uses a round - robin technique for assigning threads from different tasks to all available slots for the dag .",
    "its intuition is to avoid contention for the same resource by threads for the same task , and also balance the workload among the available workers .",
    "more recently , storm has incorporated the r - storm scheduler  @xcite that is both resource and network topology aware , and this offers better efficiency by considering the resource needs for an individual task thread . in infosphere streams  @xcite and our own prior work  @xcite , the mapping decision is dynamic and relies on the current load and previous resource utilization for the tasks .",
    "this placement decision is important , as an optimal resource allocation for a given dag and input rate may still under - perform if the task thread to worker mapping is not effective",
    ". this inefficiency can be due to additional costs for resource contention between threads of a task or different tasks on a vm , excess context switching between threads in a core , movement of tuples between distributed tasks over the network , among other reasons .",
    "this inefficiency is manifest in the form of additional latency for the messages to be processed , or a lower input rate that is supported in a stable manner for the dag with the given resources and mapping . in this article",
    ", we enhance the mapping strategy for the dsps by using a model - driven approach that goes beyond a resource - aware approach , such as used in r - storm .    in summary ,",
    "the aim of this paper is to _ use a model - driven approach to perform predictable and efficient resource scheduling for a given dag and input event rate_. the specific goals are to determine :    * the thread allocation per task , * the vm allocation for the entire dag , and * the resource mapping from a task thread to a resource slot .",
    "the outcome of this schedule will be to improve the efficiency and reduce the contention for vm resources , reduce the number of vm resources , and hence _ monetary costs _",
    ", for executing the dag , and ensure a stable ( and preferably low ) latency for execution .",
    "the _ predictable performance _ of the proposed schedule is also important as it reduces uncertainty and trial and error .",
    "further , when using this scheduling approach for handling dynamism in the workload or resources , say when the input rate or the dag structure changes , this predictability allows us to update the schedule and pay the overhead cost for the rebalancing once , rather than constantly tweak the schedule purely based on monitoring of the execution .",
    "while our article does not directly address dynamism , such as changing input rates , non - deterministic vm performance or modifications to the dag , the approach we propose offers a valuable methodology for supporting it .",
    "likewise , we limit our work here to scheduling a single streaming application on an exclusive cluster , that is common in on - demand cloud environments , rather than multi - tenancy of applications in the same cluster .",
    "our algorithm in fact helps determine the smallest number of vms and their sizes required to meet the application s needs .",
    "let the input _ dag _ be given as @xmath1 , where @xmath2 is the set of @xmath3 _ tasks _ that form the vertices of the dag , and @xmath4 is the set of _ tuple stream _ edges connecting the output of a task @xmath5 to the input of its downstream task @xmath6 .",
    "let @xmath7 be the _ selectivity _ for the edge @xmath8 .",
    "we assume interleave semantics on the input streams and duplicate semantics on the output streams to and from a task , respectively .",
    "further , we are given a set of vms , @xmath9 , with each vm @xmath10 having multiple identical resource slots , @xmath11 .",
    "each slot is homogeneous in resource capacity and corresponds to a single cpu core of a rated speed and a specific quanta of memory allocated to that core .",
    "let @xmath12 be the number of processing slots present in vm @xmath10 .",
    "the vms themselves may be heterogeneous in terms of the number of slots that they have , even as we assume for simplicity that the slots themselves are homogeneous .",
    "this is consistent with the `` container '' or `` slot '' model followed by big data platforms like apache hadoop  @xcite and storm , though it can be extended to allow heterogeneous slots as well .",
    "_ task threads",
    "_ , @xmath13 , are responsible for executing the logic for a task @xmath5 on a tuple arriving on the input stream for the task .",
    "each task has one or more threads allocated to it , and each thread is mapped to a resource slot .",
    "different threads can execute different tuples on the input stream , in a data - parallel manner .",
    "the order of execution does not matter .",
    "each resource slot can run multiple threads from one or more tasks concurrently .",
    "we are given an input stream rate of @xmath14  tuples / sec ( or events / sec ) that should be supported for the dag @xmath15 . our goal is to schedule the tasks of the dag on _ vms with the least number of cumulative resource slots , to support a stable execution of the dag _ for the given input rate .",
    "this problem can be divided into two phases :    1 .",
    "_ resource allocation : _ find the minimum number @xmath16 of task threads required per task @xmath5 , and the cumulative resource slots @xmath17 required to meet the input rate to the dag .",
    "minimizing the slots translates to a minimization of costs for these on - demand vm resources from cloud providers since the pricing for vms are typically proportional to the number of slots they offer . 2 .",
    "_ resource mapping : _ given the set of task threads @xmath18 for all tasks in the dag , and the number of resource slots @xmath17 allocated to the dag , determine the set of vms @xmath19 such that they have an adequate number of slots , @xmath20 .",
    "further , for resource slots @xmath21 present in the vms @xmath22 , find an optimal mapping function @xmath23 for the allocated task threads on to available slots , to match the resources needed to support the required input rate @xmath14 in a stable manner .",
    "there are several qualitative and quantitative measures we use to evaluate the solution to this problem .    1 .",
    "the number of resource slots and vms estimated by the allocation algorithm should be minimized .",
    "2 .   the actual number of resource slots and vms required by the mapping algorithm to successfully place the threads should be minimized .",
    "this is the actual cost paid for acquiring and using the vms .",
    "closeness of this value to the estimate from above indicates a reliable estimate .",
    "the actual stable input rate that is supported for the dag by this allocation and mapping at runtime should be greater than or equal to the required input rate @xmath14 .",
    "a value close to @xmath14 indicates better predictability .",
    "4 .   the expected resource usage",
    "as estimated by the scheduling algorithm and the actual resource usage for each slot should be proximate .",
    "the closer these two values , the better the predictability of the dataflow s performance and the scheduling algorithm s robustness under dynamic conditions .",
    "we propose a model - based approach to solving the two sub - problems of resource allocation and resource mapping , in order to arrive at an efficient and predictable schedule for the dag to meet the required input rate .",
    "the intuition for this is as follows .",
    "the stable input rate that can be supported by a task depends on the the number of concurrent threads for that task that can execute over tuples on the input stream in a data - parallel manner .",
    "the number of threads for a task in turn determines the resources required by the task .",
    "traditional scheduling approaches for dspss assume that both these relationships  between thread count and rate supported , and thread count and resources required  are a linear function .",
    "that is , if we double the number of threads for a task , we can achieve twice the input rate and require twice the computing resources .",
    "however , as we show later in ",
    "[ sec : bm ] and fig .",
    "[ fig : bm ] , this is not true .",
    "instead , we observe that depending on the task , both these relationships may range anywhere from flat line to a bell curve to a linear function with a positive or a negative slope .",
    "the reason for this is understandable .",
    "as the number of threads increase in a single vm or resource slot , there is more contention for those specific resources by threads of a task that can mitigate their performance .",
    "this can also affect the actual resource used by the threads .",
    "for simplicity , we limit our analysis to cpu and memory resources , though it can be extended to disk iops and network bandwidth as well . as a result of this real - word behavior of tasks , scheduling that assumes a linear behavior can under - perform .    in our approach",
    ", we embrace this non - uniformity in the task performance and incorporate the empirical model of the performance into our schedule planning .",
    "first , we use micro - benchmarks on a single resource slot to develop a _ performance model _",
    "function @xmath24 for each task @xmath5 which , given a certain number of threads @xmath25 for the task on a single resource slot , provides the peak input rate @xmath26 supported , and the cpu and memory utilization , @xmath27 and @xmath28 , at that rate .",
    "this is discussed in   [ sec : bm ] .",
    "second , we use these performance models to determine the number of threads @xmath16 for each task @xmath5 in the dag that is required to support a given input rate , and the cumulative number of resource slots @xmath17 for all threads in the dag .",
    "this _ model based allocation ( mba ) _ described in ",
    "[ sec : allocation ] offers an accurate estimate of the resource needs and task performance , for individual tasks and for the entire dag .",
    "we also discuss a commonly used baseline approach , _ linear scaling allocation ( lsa ) _ , in that section .",
    "as mentioned before , it assumes that the performance of a single thread on a resource slot can be linearly extrapolated to multiple threads on that slot , as long as the resource capacity of the slot is not exhausted .",
    "it under - performs , as we show later .",
    "this resource allocation can subsequently be used by different resource mapping algorithms that exist , such as the round - robin algorithm used by default in apache storm  @xcite , which we refer to as the _ default storm mapping ( dsm ) _ , or a resource - aware mapping proposed in r - storm  @xcite and included in the latest apache storm distribution , which we call _ r - storm mapping ( rsm)_. however , these mapping algorithms do not provide adequate co - location of threads onto the same slot to exploit the intuition of the model based allocation . we propose a novel _ slot aware mapping ( sam ) _ algorithm that attempts to map threads from the same task as a group to individual slots , as a form of _ gang scheduling _  @xcite . here",
    ", our goal is to maximize the peak event rate that can be exploited from that slot , minimize the interference between threads from different tasks , and ensure predictable performance .",
    "these allocation strategies are explored in   [ sec : mapping ] .",
    "we provide a high - level visual overview of the schedule planning in fig .",
    "[ fig : flow ] for a given dag @xmath15 with four tasks , ` blue ` , ` orange ` , ` yellow ` and ` green ` , with a required input rate of @xmath14 .",
    "the procedure has three phases . in the _ modeling _ phase ,",
    "we build a performance model @xmath24 for tasks in the dag that do not already have a model available .",
    "this gives a profile of the peak input tuple rates supported by a task with different numbers of threads , and their corresponding cpu and memory utilization , using a single resource slot .",
    "for e.g. , the performance models for the four tasks in the dag are given by @xmath29 and @xmath30 in fig .",
    "[ fig : flow ] .",
    "in the _ allocation _ phase , we use the above performance model to decide the number of threads @xmath16 for each task required to sustain the tuple rate that is incident on it",
    ". the input rate to the task is itself calculated based on the dag s input rate and the selectivity of upstream tasks .",
    "we use the linear scaling allocation ( lsa ) and our model based allocation ( mba ) approaches for this .",
    "while lsa only uses the performance model for a single task thread , our mba uses the full profile that is available .",
    "these algorithms also return the cpu% and memory% for the threads of each task that are summed up to get the total number of resource slots for the dag . fig .",
    "[ fig : flow ] shows the table for each of the four tasks after allocation , with the number of threads @xmath31 and @xmath32 , and their estimated resource usages @xmath27 and @xmath28 that are summed up to calculate the total resource slot needs for the dag @xmath17 .",
    "lastly , the _ mapping _ phase decides the number and types of vms required to meet the resource needs for the dag .",
    "it then maps the set of @xmath33 threads allocated for the dag to the slots @xmath34 in the vms , and the total number of these slots can be greater than the estimated @xmath17 , depending on the algorithm . here",
    ", we use the default storm mapping ( dsm ) , r - storm mapping ( rsm ) and our proposed slot aware mapping ( sam ) algorithms as alternatives .",
    "as shown in the figure , dsm is not resource aware and only uses the information on the number of threads @xmath16 and the number of slots @xmath17 for its round - robin mapping .",
    "rsm and sam use the task dependencies between the dag and the allocation table .",
    "rsm uses the performance of a single thread while sam uses all values in the performance model for its mapping .      as with any approach that relies on prior profiling of tasks , our approach has the short - coming of requiring effort to empirically build the performance model for each task before it can be used .",
    "however , this is mitigated in two respects .",
    "first , as has been seen for scientific workflows , enterprise workloads and even hpc applications  @xcite , many domains have common tasks that are reused in compositional frameworks by users in that domain .",
    "similarly , for dsps applications in domains like social network analytics , iot or even enterprise etl ( extract , transform and load ) , there are common task categories and tasks such as parsing , aggregation , analytics , file i / o and cloud service operations  @xcite . identifying and developing performance models for such common tasks for a given user - base  even if all tasks are not exhaustively profiled  can help leverage the benefits of more efficient and predictable schedules for streaming applications .",
    "second , the effort in profiling a task is small and can be fully automated , as we describe in the next section .",
    "it also does not require access to the eventual dag that will be executed .",
    "this ensures that as long as we can get access to the individual task , some minimal characteristics of the input tuple streams , and vms with single resource slots comparable to slots in the eventual deployment , the time , costs and management overheads for building the performance model are mitigated .",
    "performance modeling for a task builds a profile of the peak input tuple rate supported by the task , and its corresponding cpu and memory usage , using a single resource slot .",
    "it does this by performing a constrained parameter sweep of the number of threads and different inputs rates as a series of micro - benchmark trials .",
    "algorithm  [ alg : bm ] gives the pseudo - code for build such a performance model .",
    "for a given task @xmath35 , we initialize the number of threads @xmath25 and the input rate @xmath26 to @xmath36 , and iteratively increase the number of threads ( lines  [ alg : bm : thread - start][alg : bm : thread - end ] ) , and for each thread count , the input rate ( lines  [ alg : bm : rate - start][alg : bm : rate - end ] ) . the steps at which the thread count and rates are increased , @xmath37 and @xmath38 , can either be fixed , or be a function of the iteration or the prior performance .",
    "this determines the granularity of the model  while a finer granularity of thread and rate increments offers better modeling accuracy , it requires more experiments , and costs time and money for vms .    @xmath39 @xmath40 @xmath41 + [ alg : bm : thread - start ] + [ alg : bm : rate - start ] + @xmath42 runtasktrial(@xmath43 ) [ alg : bm : trial ] * break*[alg : bm : unstable ] @xmath44 [ alg : bm : rate - end ] @xmath45slope(@xmath46 ) @xmath47 [ alg : bm : thread - end ] + @xmath48    for each combination of @xmath25 and @xmath26 , we run a trial of the task ( line  [ alg : bm : trial ] ) that creates a sequential 3-task dag , with one source task that generates input tuples at the rate @xmath26 , the task @xmath35 in the middle with task threads set to @xmath25 , and a sink task to collect statistics .",
    "the threads for task @xmath35 are assigned one independent resource slot on one vm while on a second vm , the source and sink tasks run on one or more separate resource slots so that they are not resource - constrained .",
    "this trial is run for a certain interval of time that goes past the `` warm - up '' period where initial scheduling and caching effects are overcome , and the dag executes in a uniform manner . for e.g. , in our experiments ,",
    "the warm up period was seen to be @xmath49 . during the trial",
    ", a profiler collects statistics on the cpu and memory usage for that resource slot , and the source and sink collect the latency times for the tuples . running the source and",
    "sink tasks on the same vm avoids clock skews . at the end of the trial ,",
    "these statistics are summarized and returned to the algorithm .    in running these experiments ,",
    "two key termination conditions need to be determined for automated execution and generation of the model . for a given number of threads ,",
    "we need to decide when we should stop increasing the input rate given to the task .",
    "this is done by checking the latency times for the tuples processed in the trial . under stable conditions , the latency time for",
    "tuples are tightly bound and fall within a narrow margin beyond the warm - up period . however ,",
    "if the task is resource constrained , then it will be unable to keep up its processing rate with the input rate causing queuing of input tuples .",
    "as the queue size keeps increasing , there will be an exponential growth in the end - to - end latency for successive tuples .    to decide if the task execution is stable , we calculate the slope @xmath50 of the tuple latency values for the trial period past the warm - up and check if it is constant or less than a small positive value , @xmath51 . if @xmath52 , this execution configuration is stable , and if not , it is unstable .",
    "once we reach an input rate that is not stable , we stop the trials for these number of threads for the task , and move to a higher number of threads ( line  [ alg : bm : unstable ] ) . in our experiments , using a tight slope value of @xmath53 was possible , and none of the experiments ran for over @xmath54 .",
    "the second termination condition decides when to stop increasing the number of threads . here",
    ", the expectation is that as the thread count increases there is an improvement , if any , in the peak rate supported until a point at which it either stabilizes or starts dropping .",
    "we maintain the peak rate supported for previous thread counts in the @xmath48 hashmap object .",
    "as before , we take the slope @xmath55 of the rates for the trailing window of thread counts to determine if the slope remains flat at @xmath56 or turns negative .",
    "once the rate drops or remains flat for the window , we do not expect to see an improvement in performance by increasing the thread count , and terminate the experiments . in our experiments , we set @xmath57 .",
    "we identify @xmath58 representative tasks , shown in table  [ tbl : bm ] , to profile and build performance models for .",
    "they also empirically motivate the need for fine - grained control over thread and resource allocation .",
    "these tasks have been chosen to be diverse , and representative of the categories of tasks often used in dsps domains such as social media analytics , iot and etl pipelines  @xcite .    * _ parse xml .",
    "_ it parses an array of in - memory xml strings for every incoming tuple .",
    "parsing is often required for initial tasks in the dag that receive text or binary encoded messages from external sources , and need to translate them to a form used by downstream tasks in the dag .",
    "xml was used here due its popular usage though other formats like json or google protocol buffers are possible as well .",
    "our xml parsing implementation uses the java sax parser that allows serial single - pass parsing even over large messages at a fast rate .",
    "parsing xml is cpu intensive and requires high memory due to numerous string operations ( table  [ tbl : bm ] ) . * _ pi computation .",
    "_ this task numerically evaluates the approximate value of @xmath59 using an infinite series proposed by _ viete _",
    "rather than running it non - deterministically till convergence , we evaluate the series for a fixed number of iterations , which we set to 15 .",
    "this is a cpu intensive floating - point task , and is analogous to tasks that may perform statistical and predictive analytics , or computational modeling . *",
    "_ batch file write .",
    "_ it is an accumulator task that resembles both window operations like aggregation or join , and disk i / o intensive tasks like archiving data .",
    "the implementation buffers a @xmath60 string in - memory for every incoming tuple for a window size of @xmath61 tuples , and then writes the batched strings to a local file on a hdd attached to vm .",
    "the number of disk operations per second is proportional to the input message rate . *",
    "_ azure blob download .",
    "_ streaming applications may download metadata annotations , accumulated time - series data , or trained models from cloud storage services to use in their execution .",
    "microsoft azure blob service stores unstructured data as files in the cloud .",
    "this task downloads a file with a given name from the azure blob storage service using the native azure java sdk . in our implementation ,",
    "a @xmath62 file is downloaded and stored in - memory for each input tuple , making it both memory and network intensive . *",
    "_ azure table query .",
    "_ some streaming applications require to access historic data stored in databases , say , for aggregation and comparison .",
    "microsoft azure table service is a nosql columnar database in the cloud .",
    "our task implementation queries a table containing @xmath63 records , each with @xmath64 columns and @xmath65 record size  @xcite , using the native azure java sdk .",
    "the query looks up a single record by passing a randomly generated record i d corresponding to a unique row key in the table .    as can be seen",
    ", these tasks cover a wide range of operations . these span from text parsing and floating - point operations to both local and cloud - based file and table operations .",
    "there is also diversity in these tasks with respect to the resources they consume as shown in table  [ tbl : bm ] , be they memory , cpu , disk or network , and some rely on external services with their own service level agreement ( sla ) .",
    ".characteristics of representative tasks for which performance modeling is performed [ cols=\"<,<,<,<,<,<\",options=\"header \" , ]     we wrap each of these tasks as a _ bolt _ in the apache storm dsps .",
    "we compose a _ topology _ dag consisting of 1 _ spout _ that generates synthetic tuples with 1 field ( ` message - id ` ) at a given constant rate determined by that trial , 1 bolt with the task that is being modeled , and 1 _ sink _ bolt that accepts the response from the predecessor . for each input tuple ,",
    "the task bolt emits one output tuple after executing its application logic , keeping the selectivity @xmath66 .",
    "apache storm   is deployed on a set of standard _ d type vms _ running ubuntu 14.04 in microsoft azure s infrastructure as a service ( iaas ) cloud , in the southeast asia data - center .",
    "each vm has @xmath67 resource slots , where @xmath68 corresponds to the vm size , @xmath69 .",
    "each slot has a one intel xeon e5 - 2673  v3 core @xmath702.40  ghz processor with hyper - threading disabled  , @xmath71 memory and @xmath72 ssd , for e.g. , the d3 vm has @xmath73  cores , @xmath74  gib ram and @xmath75  gib ssd .",
    "a separate @xmath72 hdd is present for i / o tasks like batch file write .    for the performance modeling",
    ", we deploy the spout and the sink on separate slots of a single d2 size vm , and the task bolt being evaluated on one d1 size vm . the spout and sink have @xmath76 threads each to ensure they are not the bottleneck , while the number of threads for the task bolt and the input rate to this topology is determined by algorithm  [ alg : bm ] .",
    "each trial is run for @xmath54 , to be conservative .",
    "we measure the cpu% and memory% using the linux ` top ` command , and record the peak stable rate supported for each of these task bolts for specific numbers of threads .",
    "the experiments are run multiple times , and the representative measurements are reported .       +    the goal of these experiments is to collect the performance model data used by the scheduling algorithms .",
    "however , we also supplement it with some observations on the task behavior . fig .  [",
    "fig : bm ] shows the performance model plots for the @xmath58 tasks we evaluate on a single resource slot . on the primary y axis ( left ) ,",
    "the plots show the peak stable rate supported ( tuples / sec ) , and the corresponding cpu and memory utilization on the secondary y axis ( right ) , as the number of threads for the task increases along the x axis . the cpu% and memory% are given as a fraction of the utilization above the base load on the machine when no application topology is running , but when the os and the stream processing platform are running .",
    "so a @xmath77 cpu or memory usage in the plots indicate that only the base load is present , and a @xmath78 indicates that the full available resource of the slot is used .",
    "we see from fig .",
    "[ fig : bm : xml ] that the _ parse xml _ task is able to support a peak input rate of about @xmath79 with just 1 thread , and increasing the number of threads actually reduces the input throughput supported , down to about @xmath80 with 7 threads .",
    "the cpu usage even for 1 thread is high at about @xmath81 . here , we surmise that since a single thread is able to utilize the cpu efficiently , increasing the threads causes additional overhead for context - switching and the performance deteriorates linearly .",
    "we also see that the parse xml task uses about @xmath82 of memory due to java s memory allocation for string manipulation , which is higher than for other tasks .",
    "_ pi computation _ is a floating - point heavy task and uses nearly @xmath83 cpu at the peak rate of about @xmath84 using a single thread .",
    "however , unlike xml parse where the peak rate linearly reduces with an increase in threads , we see pi manage to modestly increase the peak rate with 2 threads , to about @xmath85 , with a similar increase in cpu usage .",
    "however , beyond 2 threads , the performance drops and remains flat with the cpu usage being flat as well .",
    "this behavior was consistent across multiple trials , and is likely due to the intel haswell architecture s execution pipeline characteristics  .",
    "the memory usage is minimal at between @xmath86 . _",
    "batch file write _ is an aggregation task that is disk i / o bound .",
    "it supports a high peak rate of about @xmath87 with 1 thread , which translates to writing @xmath88 , each @xmath89 in size .",
    "this peak rate decreases with an increase in the number of threads , but is non - linear .",
    "there is a sharp drop in the peak rate to @xmath90 with 3  threads , but this increases and stabilizes at @xmath91 with more threads .",
    "the initial drop can be attributed to the disk i / o contention , hence the drop in cpu usage as well , but beyond that the thread contention may dominate , causing an increase in cpu usage even as the supported rate is stable the _ azure blob _ and _ azure table _ tasks rely on an external cloud service to support their execution . as such , the throughput of these tasks are dependent on the sla offered for these services by the cloud provider , in addition to the local resource constraints of the slot .",
    "we see the benefit of having multiple threads clearly in these cases . the peak rate supported by both increases gradually until a threshold , beyond which the peak rate flattens and drops .",
    "their cpu and memory utilization follow a similar trend as well .",
    "blob s rate grows from about @xmath92 with 1 thread to @xmath93 with 50 threads , while table s increases from @xmath94 to @xmath95 when scaling from 1 to 60 threads .",
    "this closely correlates with the sla of the blob service which is @xmath96 , and matches with the @xmath97 of @xmath98 each that are cumulatively downloaded  .",
    "both these tasks are also network intensive as they download data from the cloud services .",
    "_ summary .",
    "_ the first three tasks show a flat or decreasing peak rate performance with some deviations , but with differing cpu and memory resource behavior . the last two exhibit a bell - curve in their peak rates as the threads increase .",
    "these highlight the distinct characteristics of specific tasks ( and even specific cpu architectures and cloud services they wrap ) that necessitate such performance models to support scheduling .",
    "simple rules of thumbs assuming static of linear scaling are inadequate , and we see later , can cause performance degradation and resource wastage .",
    "resource allocation determines the number of resource slots @xmath17 to be allocated for a dag @xmath99 for a given input rate @xmath14 , along with the number of threads @xmath100 required for each task @xmath101 . in doing so",
    ", the allocation algorithm needs to be aware of the input rate to each task that will inform it of the resource needs and data parallelism for that task .",
    "we can define this _ input rate _",
    "@xmath102 for a task @xmath6 based on the input rate to the dag , the connectivity of the dag , and the selectivity of each input stream to a task , using a recurrence relation as follows : @xmath103    in other words , if task @xmath6 is a source task without any incoming edges , its input rate is implicitly the rate to the dag , @xmath14 .",
    "otherwise , the input rate to a downstream task is the sum of the tuple rates on the out edges of the tasks @xmath5 immediately preceding it .",
    "this output rate is itself given by the product of those predecessor tasks input rates @xmath104 and their selectivities @xmath7 on the out edge connecting them to task @xmath6 .",
    "this recurrence relationship can be calculated in the topological order of the dag starting from the source task(s ) .",
    "let the procedure @xmath105 evaluate this for a given task @xmath6 .",
    "next , the allocation algorithm will need to determine the threads and resources needed by each task @xmath6 to meet its input rate @xmath102 .",
    "algorithms can use prior knowledge on resource usage estimates for the task , which may be limited to the cpu% and memory% for a single thread of the task , irrespective of the input rate , or approaches like ours that use a more detailed performance model .",
    "say the following functions are available as a result of the performance modeling algorithm , alg .",
    "[ alg : bm ] , or some other means . @xmath106 and @xmath107 respectively return the incremental cpu% and memory% used by task @xmath5 when running on a single resource slot with @xmath108 threads .",
    "further , let @xmath109 provide the peak input rate that is supported by the task @xmath5 on a single slot for @xmath108 number of threads .",
    "lastly , let @xmath110 be the inverse function of @xmath109 such that it gives the smallest number of threads @xmath108 adequate to satisfy the given input rate @xmath26 on a single resource slot",
    ". since the @xmath26 values returned by @xmath109 for integer values of @xmath108 would be at coarse increments , @xmath111 may offer an over - estimate depending on the granularity of @xmath38 and @xmath37 used in alg .",
    "[ alg : bm ] .",
    "next , we describe two allocation algorithms  a baseline which uses simple estimates of resource usage for tasks , and another we propose that leverages the more detailed performance model available for the tasks in the dag .      the linear scaling allocation ( lsa ) approach uses a simplifying assumption that the behavior of a single thread of a task will linearly scale to additional threads of the task . this scaling assumption is made both for the input rate supported by the thread , and the cpu% and memory% for the thread . for e.g. ,",
    "the r - storm  @xcite scheduler assumes this additive behavior of resource needs for a single task thread as more threads are added , though it leaves the selection of the number of threads to the user .",
    "other dsps schedulers make this assumption as well  @xcite  @xcite .",
    "algorithm  [ alg : lsa ] shows the behavior of such a linear allocation strategy .",
    "it first estimates the input rate @xmath104 incident at each task @xmath5 using the @xmath112 procedure discussed before .",
    "it then uses information on the peak rate @xmath113 sustained by a single thread of a task @xmath5 running in one resource slot , and its cpu% and memory% at that rate , @xmath114 and @xmath115 , as a baseline . using this",
    ", it tries to incrementally add more threads until the input rate required , in multiples of the peak rate , is satisfied ( line  [ alg : lsa : greater ] ) .",
    "when the remaining input rate to be satisfied is below the peak rate ( line  [ alg : lsa : smaller ] ) , we linearly scale down the cpu and memory utilization , proportional to the required rate relative to the peak rate .",
    "the result of this lsa algorithm is the thread counts @xmath116 per task @xmath117 .",
    "in addition , the sum of the cpu and memory allocation for all tasks , rounded up to the nearest integer , gives the nominal lower bound on the resource slots @xmath17 required to support this dag at the given rate .",
    "@xmath118 @xmath119 @xmath120 @xmath121 @xmath122    [ alg : lsa : greater ] + @xmath123 @xmath124 @xmath125 @xmath126    [ alg : lsa : smaller ] @xmath123",
    "@xmath127 @xmath128 @xmath129     +      while the lsa approach is simple and appears intuitive , it suffers from two key problems that make it unsuitable for may tasks . _",
    "first , the assumption that the input rate supported will linearly increase with the number of threads is not valid .",
    "_ based on our observations of the performance models from fig .",
    "[ fig : bm ] , we can see that for some tasks like azure blob and table , there is a loose correlation between the number of threads and the input rate supported .",
    "but even this evens out at a certain number of threads .",
    "others like parse xml , pi computation and batch file write see their peak input rate supported remain flat or decrease as the threads increase _ on a single resource slot _ due to contention .",
    "one could expect a linear behavior if the threads run on different slots or vms , but that would increase the slots required ( and the corresponding cost ) .",
    "_ second , the assumption that the resource usage linearly scales with the number of threads , relative to the resources for 1 thread , is incorrect . _",
    "this again follows from the performance models , and in fact , the behavior of cpu and memory usage themselves vary for a task . for e.g. , in fig .",
    "[ fig : bm : pi ] for pi , the cpu usage remains flat while the memory usage increases even as the rate supported decreases with the number of threads . for azure table query in fig .",
    "[ fig : bm : table ] , the cpu and memory increase with the threads but with very different slopes .",
    "our model - based allocation algorithm , shown in algorithm  [ alg : mba ] , avoids such inaccurate assumptions and instead uses the performance models measured for the tasks to drive its thread and resource allocation . here",
    ", the intuition is to select the sweet spot of the number of threads such that the peak rate @xmath130 among all number of threads ( for which the model is available ) is the highest for task @xmath5 ( lines  [ alg : mba : greater : start][alg : mba : greater : end ] ) .",
    "this ensures that we maximize the input rate that we can support from a single resource slot for that task . at the same time ,",
    "when we allocate these many threads to saturate a slot , we also disregard the actual cpu% and memory% and instead treat the whole slot as being allocated ( @xmath78 usage ) .",
    "this is because that particular task can not make use of additional cpu or memory resources available in that slot due to a resource contention , and we do not wish additional threads on this slot to exacerbate this problem .",
    "when the residual input rate to be supported for a task falls below this maximum peak rate ( line  [ alg : mba : smaller ] ) , we instead select smallest number of threads that are adequate to support this rate , and use the corresponding cpu and memory% .",
    "if a single thread is adequate ( line  [ alg : mba:1 ] ) , just as for lsa , we scale down the resources needed proportion to the residual input rate relative to the peak rate using 1 thread .    @xmath118 @xmath131 @xmath132 @xmath120 @xmath121 @xmath122    [ alg : mba : greater : start ] + @xmath133 @xmath134 @xmath135 @xmath136 [ alg : mba : greater : end ] [ alg : mba : smaller ] @xmath137 @xmath138 @xmath139 @xmath140 [ alg : mba:1 ] @xmath141 @xmath142 @xmath129 +    the result of running mba is also a list of thread counts @xmath116 per task in the dag , which will be used by the mapping algorithm .",
    "it also gives the cpu% and memory% per task which , as before , helps estimate the slots for the dag : @xmath143",
    "as we saw , the allocation algorithm returns two pieces of information @xmath116 , the number of threads per task , and @xmath17 , the number of resource slots allocated .",
    "the goal of resource mapping is to assign these task threads , @xmath18 where @xmath144 , to specific resource slots to meet the input rate requirements for the dag .",
    "the first step is to identify and acquire adequate number and sizes of vms that have the estimated number of slots .",
    "this is straight - forward .",
    "most iaas cloud providers offer on - demand vm sizes with slots that are in powers of @xmath76 , and with pricing that is a multiple of the number of slots . for e.g. , the microsoft azure d - series vms in the southeast asia data center have @xmath145 and @xmath73 cores for sizes d1d4 , with @xmath146 ram per core , and costing @xmath147 and @xmath148/hour , respectively  .",
    "amazon aws and google compute engine iaas cloud services have similar vm sizes and pricing as well .",
    "so the total price for a given slot - count @xmath17 does not change based on the mix of vm sizes used , and one can use a simple packing algorithm to select vms @xmath22 with sizes such that @xmath149 , where @xmath12 is the number of slots per vm @xmath10 . at the same time , having more vms means higher network transfer latency , and end - to - end latency for the dag will increase .",
    "hence , minimizing the number of distinct vms is useful as well rather than having many small vms .",
    "one approach that we take to balance pricing with communication latency is to acquire as many vms ` @xmath3 ' with the largest number of slots as possible , say , @xmath150 , which cumulatively have @xmath151 .",
    "then , for the remaining slots , we assign to the smallest possible vm whose number of slots is @xmath152 .",
    "this may include some additional slots than required , but is bound by @xmath153 if slots per vm are in powers of @xmath76 , as is common .",
    "other approaches are possible as well , based on the trade - off between network latency costs and slot pricing .",
    "for the @xmath22 set of vms thus acquired , let @xmath21 be the set of slots in these vms , where @xmath154 and @xmath155 , such that @xmath156 .",
    "the second step , that we emphasize in this article , is to map the threads for each task to one of the slots we have acquired , and determine the mapping function @xmath23 .",
    "next , we discuss two mapping approaches available in literature , that we term dsm and rsm , as comparison and propose our novel mapping sam as the third .",
    "while dsm is not `` resource aware '' , i.e. , it does not consider the resources required by each thread in performing the mapping , rsm and our sam are resource aware , and use the output from the performance models developed earlier .",
    "dsm is the default mapping algorithm used in apache storm , and uses a simple round - robin algorithm to assign the threads to slots .",
    "all threads are assumed to have a similar resource requirement , and all slots are assumed to have homogeneous capacity .",
    "under such an assumption , this nave algorithm will balance the load of the number of threads across all slots .",
    "algorithm  [ alg : dsm ] describes its behavior .",
    "the task threads and resource slots available are provided as a set to the algorithm .",
    "the slots are considered as a list in some random order .",
    "the algorithm iterates through the set of threads in arbitrary order . for each thread",
    ", it picks the next slot in the list and repeats this in a round - robin fashion for each pending thread ( line [ alg : dsm : rr ] ) , wrapping around the slot list if its end is reached .",
    "[ fig : mapping ] illustrates the different mapping algorithms for a sample dag with four tasks , ` lue , range , ellow ` and ` reen ` , and say @xmath157 and @xmath58 threads allocated to them , respectively , by some allocation algorithm .",
    "let the resources estimated for them be @xmath158 slots that are acquired across three vms with @xmath76 slots each",
    ".    given this , the dsm algorithm first gets the list of threads and slots in some random order . for this example , let them be ordered as , @xmath159 , and @xmath160 .",
    "firstly , the five blue threads are distributed across the first @xmath58 slots , @xmath161 sequentially .",
    "then , the distribution of the orange threads resumes with the sixth slot @xmath162 , and wraps around to the first slot to end at @xmath163 .",
    "the three yellow threads map to slots @xmath164 , and lastly , the five green threads wrap around and go to the first @xmath58 slots . as we see ,",
    "dsm distributes the threads evenly across all the acquired slots irrespective of the resources available on them or required by the threads , with only the trailing slots having fewer threads .",
    "this can also help distribute threads of the same task to different slots to avoid them contending for the same type of resources .",
    "however , this is optimistic and , as we have seen from the performance models , the resource usages sharply vary not just across tasks but also based on the number of threads of a task present in a slot .",
    "@xmath165 @xmath166 \\gets \\textsc{settolist}(s)$ ] @xmath167 [ alg : dsm : rr ] @xmath168 @xmath169 $ ] @xmath170 @xmath171 +      the r - storm mapping ( rsm ) algorithm  @xcite was proposed earlier as a resource - aware scheduling approach to address the deficiencies of the default storm scheduler .",
    "it has subsequently been included as an alternative scheduler for apache storm , as of v1.0.1 .",
    "rsm offers three features that improve upon dsm .",
    "first , in contrast to dsm that balances the thread count across all available slots , rsm instead maximizes the resource usage in a slot and thus minimizes the number of slots required for the threads of the dag .",
    "for this , it makes use of the resource usage for _ single threads _ that we collect from the performance model ( @xmath172 ) , and the resource capacities of slots and vms .",
    "a second side - effect of this is that it prevents slots from being over - allocated , which can cause an unstable dag mapping at runtime . for e.g. ,",
    "dsm could place threads in a slot such that their total cpu% or memory% is greater than @xmath78 .",
    "this is avoided by rsm .",
    "lastly , rsm is aware of the network topology of the vms , and it places the threads on slots such that the communication latency between adjacent tasks in the dataflow graph is reduced . at the heart of the rsm algorithm is a _ distance function _ based on the available and required resources , and a network latency measure .",
    "this euclidean distance between a given task thread @xmath18 and a vm @xmath22 is defined as : @xmath173 where @xmath174 and @xmath175 are the incremental cpu% and memory% required by a single thread of the task @xmath5 on one slot , and @xmath176 and @xmath177 are the cpu% and memory% not yet mapped across all slots of vm @xmath10 . a network latency multiplier , from a _ reference vm _ , @xmath178 , to the candidate vm @xmath10 , is also defined using the nwdist function .",
    "this reference vm is the last vm on which a task thread was mapped , and the network latency multiplier is set to @xmath179 if the candidate vm is the reference vm , @xmath180 if the vm is in the same rack as the reference , and @xmath181 if on a different rack .",
    "lastly , the weights @xmath182 and @xmath183 are coefficients to tune this distance function as appropriated . given this , the rsm algorithm , given in alg .",
    "[ alg : rsm ] , works as follows .",
    "it initializes the cpu% and memory% resources available for the candidate vms to @xmath78 of their number of slots , the memory available per slot to @xmath78 , and the number of threads to be mapped per task ( lines  [ alg : rsm : init : start][alg : rsm : init : end ] ) .",
    "the initial reference vm is set to some vm , say @xmath184 .",
    "then , it performs one sweep where one thread of each task , in topological order rooted at the source task(s ) , is mapped to a slot ( lines  [ alg : rsm : map : start][alg : rsm : map : end ] ) .",
    "this mapping in the order of bfs traversal increases the chance that threads of adjacent tasks in the dag are placed on the same vm to reduce network latency .    during the sweep , we first check if the task has any pending threads to map , and if so , we test the vms in the ascending order of their distance function , returned by function getsortedvms , to see if they have adequate resources for that task thread ( lines  [ alg : rsm : dist][alg : rsm : map : loop ] ) .",
    "there are two checks that are performed : one to see if the vm has adequate cpu% available for the thread , and second if any slot in that vm has enough memory% to accommodate that thread .",
    "this differentiated check is because in storm , the memory allocation per slot is tightly bound , while the cpu% available across slots is assumed to be usable by any thread placed in that vm .",
    "if no available slot meets the resource needs of a thread , then rsm fails .",
    "as we show later , this is not uncommon depending on the allocation algorithm . if a valid slot , @xmath185 , is found , the task thread is mapped to this slot , and the thread count and resource availability updated ( lines  [ alg : rsm : post : start]  [ alg : rsm : post : end ] ) .",
    "the reference vm is also set to the current vm having that slot",
    ". then the next threads in the sweep are mapped , and this process repeated till all task threads are mapped .",
    "[ fig : mapping ] shows the rsm algorithm applied to the sample dag .",
    "mapping of the threads to slots is done in bfs ordering of tasks , @xmath186 and @xmath187 . for each thread of the task in this order ,",
    "a slot on the vm with the minimum distance and available resources is chosen .",
    "say in the first sweep , the threads @xmath188 and @xmath189 are mapped to the same slot @xmath190 , and then next thread @xmath191 be mapped to new slot @xmath192 due to resource constraint on @xmath190 for this thread .",
    "the new slot @xmath192 is picked on same vm as it has the least distance among all vms . in the second sweep ,",
    "thread @xmath193 and @xmath194 are mapped to slots @xmath192 and @xmath163 , and likewise for the third sweep . in the fourth sweep , there are no threads for the yellow task pending . also , we see that thread @xmath195 is not mapped to slot @xmath196 due to lack of resources , instead going to @xmath197 .",
    "however , a slot @xmath196 does have resources for a subsequent thread @xmath198 , and the distance to @xmath196 is lesser than @xmath197 .",
    "thus rsm tries to perform a network - aware best fit packing to minimize the number of slots .",
    "@xmath199 [ alg : rsm : init : start ] @xmath200 @xmath201 [ alg : rsm : init : end ] @xmath165 @xmath202 [ alg : rsm : map : start ] -3em @xmath203 \\gets \\textsc{getsortedvms}(v , t_i , \\widehat{v})$ ] [ alg : rsm : dist ] @xmath204 @xmath205 [ alg : rsm : map : loop ] @xmath206 @xmath207 [ alg : rsm : map ] @xmath208 [ alg : rsm : post : start ] @xmath209 [ alg : rsm : post : end ] @xmath210 [ alg : rsm : map : end ] +      while the rsm algorithm is resource aware , it linearly extrapolates the resource usage for multiple threads of a task in a vm or slot based on the behavior of a single thread . as we saw earlier in the ",
    "[ sec : bm ] , this assumption does not hold , and as we show later in ",
    "[ sec : results ] , it causes inefficient mapping , non - deterministic performance and needs over - allocation of resources .",
    "our slot aware mapping ( sam ) addresses these deficiencies by fully utilizing the performance model and the strategy used by our model based allocation .",
    "they key idea is to map a _ bundle of threads _ of the same task exclusively to a single slot such that the stream throughput is maximized for that task on that slot based on its performance model , and the interference from threads of other tasks on that slot is reduced .    in algorithm  [ alg",
    ": sam ] , as for rsm , we initialize the resources for the vms and slots .",
    "further , in addition to the total slots @xmath17 required by the dag , we also have the quantity of cpu% and memory% required by all the threads of each task available as @xmath211 and @xmath212 .",
    "recollect that the mba algorithm returns this information based on the performance model . as for rsm , we iterate through the tasks in topological order ( line  [ alg : sam : topo ] ) .",
    "however , rather than map one thread of each task , we first check if the number of pending threads forms a _ full bundle _ , which we define to be as @xmath130 , the number of threads at the peak rate supported by the task on a single slot ( line  [ alg : sam : full ] ) . if so , we select an empty slot in the last mapped vm , or if none exist , in its neighboring one ( line  [ alg : sam : fullslot ] ) .",
    "we @xmath130 unmapped threads for this task and assign this whole bundle of threads to this exclusive slot , i.e. , @xmath78 of its cpu and memory ( line  [ alg : sam : full : map ] ) .",
    "the resource needs of the task are reduced concomitantly , and this slot is fully mapped .",
    "it is possible that the task has a _ partial bundle _ of unmapped threads , having fewer than @xmath130 ones ( line  [ alg : sam : partial ] ) . in this case",
    ", we find the best - fit slot as the one whose sum of available cpu% and memory% is the smallest , while being adequate for the cpu% and memory% required for this partial bundle ( line  [ alg : sam : bfslot ] ) . we assign this partial bundle of threads to this slot and reduce the resources available for this slot by @xmath211 and @xmath212 . at this point ,",
    "all threads of this task will be assigned ( line  [ alg : sam : partial : done ] ) .",
    "notice that slots co - locate threads from different tasks only for the last partial bundle of each task .",
    "so we have an upper bound on the number of slots with mixed thread types as @xmath213 .",
    "since the performance models offers information on the behavior of the same thread type on a slot , this limits the interference between threads of different types , that is not captured by the model . in practice , as we show in the experiments , most slots have threads of a single task type . as a result , sam has a more predictable resource usage and behavior for the mapped dag .",
    "it is possible that even in sam , the resources allocated may not be adequate for the mapping ( lines  [ alg : sam : full : na ] , [ alg : sam : partial : na ] ) , though the chances of this happening is smaller than for rsm since sam uses a strategy similar to the allocation algorithm , mba .",
    "this is a side - effect of the binning , when resource available in partly used slots are not adequate to fully fit a partial bundle .",
    "also , while we do not explicitly consider network distance unlike in rsm , the mapping of tasks in topological order combined with picking a bundle at a time achieves a degree of network proximity between threads of adjacent tasks in the dag .",
    "@xmath201 @xmath132 @xmath214    @xmath165 [ alg : sam : topo ] [ alg : sam : full ] @xmath215 [ alg : sam : fullslot ] [ alg :",
    "sam : full : na ] @xmath216 \\gets \\ { r_i^k \\ } \\in r ~|~ \\not\\exists \\mathcal{m}(r_i^k),~~|r_i'| = \\widehat{\\tau_i}$ ] @xmath217 \\rightarrow s_j')$ ] [ alg : sam : full : map ] @xmath218 @xmath219 [ alg : sam : full : end ] [ alg : sam : partial ] @xmath220 [ alg : sam : bfslot ] [ alg : sam : partial : na ] @xmath216 \\gets \\ { r_i^k \\ } \\in r ~|~ \\not\\exists \\mathcal{m}(r_i^k)$ ] @xmath217 \\rightarrow s_j')$ ] @xmath221 @xmath222 [ alg : sam : partial : done ] +    fig .",
    "[ fig : mapping ] shows the sam algorithm applied to the sample dag .",
    "say a full bundle of the four tasks , @xmath186 and @xmath187 , have 2 , 3 , 3 and 4 threads , respectively .",
    "we iteratively consider each task in the bfs order of the dag , similar to rsm , and attempt to assign a full bundle from their remaining threads to an exclusive slot . for e.g. , in the first sweep , the full bundles @xmath223 are mapped to the four slots , @xmath224 , respectively , occupying 2 vms .",
    "in the next sweep , we still have a full bundle for the blue task , @xmath225 , that takes an independent slot @xmath197 , but the orange and green tasks have only partial bundle consisting of one thread each .",
    "@xmath198 is mapped to a new slot @xmath162 as there are no partial slots , and @xmath226 is mapped to the same slot as it is the best - fit partial slot .",
    "all threads of the yellow task are already mapped . in the last sweep , the only remaining partial bundle for the blue task , @xmath227",
    "is mapped to the partial slot @xmath162 as the best fit .",
    "we validate our proposed allocation and mapping techniques , mba and sam , on the popular apache storm dsps , open - sourced by twitter . streaming applications in storm , also called _ topologies _ , are composed in java as a dag , and the resource allocation  number of threads per task ( _ parallelism hint _ ) and the resource slots for the topology ( _ workers _ )  is provided by the user as part of the application . here",
    ", we implement our mba algorithm within a script that takes the dag and the performance model for the tasks as input , and returns the number of threads and slots required .",
    "we manually embed this information in the application , though this can be automated in future .",
    "we take a similar approach and implement the lsa algorithm as well , which is used as a baseline .",
    "a storm cluster has multiple hosts or vms , one of which is the master and the rest are compute vms having one or more resource slots . when the application is submitted to the storm cluster , the master vm runs the _ nimbus scheduling service _ responsible for mapping the threads of the application s tasks to worker slots . a _ supervisor service _ on each compute vm receives the mapping from nimbus and assigns threads of the dag for execution .",
    "while it is possible to run multiple topologies concurrently in a cluster , our goal is to run each application on an exclusive on - demand storm cluster with the exact number of required vms and slots , determined based on the allocation algorithm . for e.g. ,",
    "one scenario is to acquire a storm cluster on - demand from azure s hdinsight platform as a service ( paas )  .",
    "nimbus natively implements the default round - robin scheduler ( dsm ) and recently , the scheduling algorithm of r - storm ( rsm ) using the ` defaultscheduler ` and ` resourceawarescheduler ` classes , respectively . we implement our sam algorithm as a custom scheduler in nimbus , `",
    "slotawarescheduler ` .",
    "it implements the ` schedule ` method of the ` ischeduler ` interface which is periodically invoked by the nimbus service with the pending list of threads to be scheduled .",
    "when a thread for the dag first arrives for mapping , the sam scheduler generates and caches a mapping for all the threads in the given dag to slots available in the cluster . the host ids and slot ids available in the cluster",
    "is retrieved using methods in storm s ` cluster ` class .",
    "then , the algorithm groups the threads by their slot i d as storm requires all thread for a slot to be mapped at once .",
    "the actual mapping is enacted by calling the ` assign ` method of the ` cluster ` class that takes the slot i d and the list of threads mapped to it .      the setup for validating and comparing the allocation and mapping algorithms are similar to the one used for performance modeling ,   [ sec : bm : setup ] . in summary ,",
    "apache storm @xmath228 is deployed on microsoft azure d - series vms in the southeast asia data center .",
    "the type and number of vms depend on the experiment , but each slot of this vm series has one core of intel xeon e5 - 2673  v3 cpu @xmath229 , @xmath146 ram and a @xmath230 ssd .",
    "we use three vm sizes in our experiments to keep the costs manageable  `",
    "d3 ` having @xmath231  slots , ` d2 ` with @xmath76  slots , and ` d1 ` with @xmath36  slot .",
    "we perform two sets of experiments . in the first",
    ", we evaluate the resource benefits of our model based allocation ( mba ) combined with our slot aware mapping ( sam ) , in comparison with the baseline linear storm allocation ( lsa ) with the resource - aware r - storm mapping ( rsm ) , for a given input rate (   [ sec : results : usage ] ) . for the allocated number of resource slots ,",
    "we acquire the largest size vms ( ` d3 ` ) first to meet the needs , and finally pick a ` d2 ` or a ` d1 ` vm for the remaining slot(s ) , as discussed in   [ sec : acquire ] .    in the second set of experiments (   [ sec : results : accy ] ) , we verify the predictability of the performance of our mba and sam approaches , relative to the existing techniques . here , we perform five experiments , using a combination of 2 allocation and 3 mapping algorithms .",
    "we measure the highest stable input rate supported by the dags using these algorithms on a fixed number of five ` d3 ` vms , and compare the observed rate and resource usage against the planned rate , and with the rate and usage estimated by our model . in all experiments , a separate ` d3 ` vm is used as the master node on which the nimbus scheduler and other shared services run . for the rsm implementation , we need to explicitly specify the memory available to a slot and to the vm , which we set to @xmath146 and the number of slots times @xmath146 , respectively . for rsm and sam , we set the available cpu% per slot to 100% and for the vm to be the number of vm slots times @xmath78 .",
    "+    in our experiments , we use two types of streaming dataflows  _ micro - dags _ and _ application dags_. the micro - dags capture common dataflow patterns that are found as sub - graphs in larger applications , and are commonly used in literature , including by r - storm  @xcite .",
    "these include _ linear _ , _ diamond _ and _ star _ micro - dags that respectively capture a sequential flow , a fan - out and fan - in , and a hub - and - spoke model ( fig .",
    "[ fig : micro : dag ] ) . while the linear dag has a uniform input and output tuple rate for all tasks , the diamond exploits task parallelism , and the star doubles the input and output rates for the hub task .",
    "all three micro - dags have 5 tasks , in addition to the source and sink tasks , and we randomly assign the five different tasks that were modeled in   [ sec : bm ] to these five dag vertices , as labeled in fig .",
    "[ fig : micro : dag ] .",
    "the figure also shows the input rates to each task based on a sample input rate of @xmath232 to the dag .",
    "all tasks have a selectivity of @xmath66 .",
    "the application dags have a structure based on three real - world streaming applications that analyze traffic patterns from gps sensor streams ( _ traffic _ )",
    "@xcite , compute the bargain index value from real - time stock trading prices ( _ finance _ )  @xcite , and perform data pre - processing and predictive analytics over electricity meter and weather data streams from smart power grids ( _ grid _ )  @xcite . in the absence of access to the actual application logic , we reuse and iteratively",
    "assign the five tasks we have modeled earlier to random vertices in these application dags and use a task selectivity of @xmath66 .",
    "these three applications dags have between @xmath233 logic tasks , and exhibit different composition patterns .",
    "their overall dag selectivity ranges from @xmath234 to @xmath235 .",
    "we also see that the five diverse tasks we have chosen as proxies for these domain tasks are representative of the native tasks , as described in literature . for e.g. , a task of the traffic application does parsing of input streams , similar to our xml parse task , and another archives data for historical analysis , similar to the batch file write task .",
    "the moving average price and bargain index value tasks in the finance dag are floating - point intensive like the pi task .",
    "the grid dag performs parsing and database operations , similar to xml parse and azure table , as well as time - series analytics that tend to be floating - point oriented . as a result , these are reasonable equivalents of real - world applications for evaluating resource scheduling algorithms .    along with the application logic tasks , we have separate source and sink tasks for passing the input stream and logging the output stream for all dags .",
    "the source task generates synthetic tuples with a single opaque field of 10  bytes at a given constant rate .",
    "the sink task logs the output tuples and helps calculate the latency and other empirical statistics .",
    "both these tasks are mapped by the scheduler just like the application tasks . given the light - weight nature of these tasks , we empirically observe that a single thread for each of these tasks is adequate , with a static allocation of @xmath236 cpu and @xmath237 memory for the source and @xmath236 cpu and @xmath238 memory for the sink .",
    "each experiment is run for @xmath239  minutes and over multiple runs , and we report the representative values seen for the entire experiment .",
    "we compare our combination of mba allocation and sam mapping , henceforth called * mba+sam * , against lsa allocation with rsm mapping , referred to as * lsa+rsm*. the metric of success here is the ability to _ minimize the overall resources allocated _ for a stable execution of the dag at a given fixed input rate .",
    "we consider both micro - dags and applications dags .",
    "first the allocation algorithm determines the minimum number of resource slots required and then the mapping algorithm is used to schedule the threads on slots for the dag . there may be cases where the resource - aware mapping algorithm is unable to find a valid schedule for the resource allocation , in which case , we incrementally increase the number of slots by @xmath36 until the mapping is successful .",
    "we report and discuss this as well .",
    "we then execute the dag and check if it is able to support the expected input rate or not .",
    "[ fig : micro : slots ]    the experiments are run for the micro - dags with input rates of @xmath240 and @xmath241 .",
    "this allows us to study the changes in resource allocation and usage as the input rate changes .",
    "these specific rates are chosen to offer diversity while keeping within reasonable vm costs . for e.g. ,",
    "each run costs @xmath242 , and many such runs are required during the course of these experiments .",
    "[ fig : micro : slots ] shows the number of resource slots allocated by the lsa and mba algorithms ( yellow bars , left y axis ) for the three different input rates to the three micro - dags .",
    "further , it shows the additional slots beyond this allocation ( green bars , left y axis ) that is required by the resource - aware rsm and sam mapping algorithms to ensure that threads in a slot are not under - provisioned .",
    "the dags are run with the input rate they the schedule was planned for ( i.e. , @xmath240 or @xmath241 ) , and if they were not stable , we incrementally reduced the rate by @xmath243 until the execution is stable . the stable input rate , less than or equal to the planned schedule , is shown on the right y axis ( blue circle ) .",
    "_ we can observe that lsa allocates more slots than mba in all cases .",
    "_ in fact , the resources allocated by lsa is nearly twice as that by mba requiring , say , 7 , 13 and 28 slots respectively for the linear dag for the rates of @xmath240 and @xmath241 compared to mba that correspondingly allocates only 4 , 7 and 15 slots .",
    "this is observed for the other two micro - dags as well .",
    "the primary reason for this resource over - allocation in lsa is due to a linear extrapolation of resources with the number of threads .",
    "in fact , while mba allocates @xmath244 more threads than lsa for the dags , the resource allocation for these threads by lsa is much higher . for e.g.",
    ", lsa allocates @xmath245 cpu and @xmath246 memory for its @xmath247 threads of the blob download task for the linear dag at @xmath232 while mba allocates only @xmath248 of cpu and @xmath249 of memory for its corresponding @xmath250 blob download threads .",
    "this alone translates to a difference between @xmath251 slots allocated by lsa ( based on memory% ) and @xmath252 slots by mba .",
    "_ despite the higher allocation by lsa , we see that rsm is often unable to complete the mapping without requiring additional slots .",
    "_ this happens for 6 of the 9 combination of dag and rates , for e.g. , requiring @xmath36 more slot for the diamond dag with @xmath253 ( fig .",
    "[ fig : slots : diamond ] , green bar ) and @xmath254 more for the linear dag at @xmath241 ( fig .",
    "[ fig : slots : linear ] ) .",
    "in contrast , our sam mapping uses 1 additional slot , only in the case of linear dag at @xmath253 ( fig .",
    "[ fig : slots : linear ] ) and none other , despite being allocated fewer resources by mba compared to lsa .",
    "both rsm and sam are resource aware , which means they will fail if they are unable to pack threads on to allocated slots such that their expected resource usage by all threads on a slot is within the slot s capacity . rsm more often fails to find a valid bin - packing than sam .",
    "this is because of its distribution of a task s threads across many slots , based on the distance function , which causes resource fragmentation .",
    "we see memory fragmentation to be more common , causing vacant holes in slots that are each inadequate to map a thread but are cumulatively are sufficient .    for e.g. , the linear dag at @xmath241 is assigned @xmath255 slots by lsa . during mapping by rsm ,",
    "the @xmath256 blob download threads dominate the memory and occupy @xmath257 of memory in each of the @xmath255 slots , leaving only @xmath258 memory on each slot .",
    "this is inadequate to fit threads for other tasks like xml parse which requires @xmath259 of memory for one of its thread , though over @xmath260 of fragmented memory is available across slots .",
    "this happens much less frequently in sam due to its preference for packing a slot at a time with a full bundle of threads in a single slot without any fragmentation .",
    "fragmenting can only happen for the last partial thread bundle for each task .",
    "a full bundle also takes less resources according to the performance models than linear extrapolation from a single thread . for e.g. , mba packs @xmath247 threads of the same blob download task from above in a single slot .",
    "_ we see that in several cases , the dags are unable to support the rate that the schedule was planned for . _",
    "this reduction in rate is up to @xmath261 for lsa+rsm and up to @xmath236 for mba+sam .",
    "for e.g. , in the diamond dag in fig .",
    "[ fig : slots : diamond ] , the observed / expected rates in @xmath262 for lsa+rsm is @xmath263 and @xmath264 while it is @xmath264 and @xmath265 for mba+sam .",
    "the reasons vary between the two approaches .    in lsa+rsm , lsa allocates threads assuming a linear scaling of the rate with the threads but this holds only if each thread is running on an exclusive slot . as rsm packs multiple threads to a slot , the rate supported by these threads is often lower than estimated . for e.g. , for the diamond dag in fig .",
    "[ fig : slots : diamond ] , lsa allocates 18  threads for the azure table task for an input rate of @xmath253 based on a single thread supporting @xmath94 .",
    "however , rsm distributes 2  threads each on 4  slots and the remaining 9  threads on 1  slot . as per our performance model for the azure table task , 2  threads on a slot support",
    "@xmath243 and 9  threads support @xmath266 , to give a total of @xmath267 .",
    "this is close to the observed @xmath268 supported by this dag for lsa+rsm .",
    "while sam s model - based mapping of thread bundles mitigates this issue , it does suffer from an imbalance in message routing by storm to slots .",
    "storm s _ shuffle grouping _ from an upstream task sends an equal number of tuples to each downstream thread .",
    "however , the individual threads may not have the same per - capita capacity to process that many tuples on its assigned slot , as seen from the performance models .",
    "this can cause a mismatch between tuples that arrive and those that can be processed on slots . for e.g. , the diamond dag at @xmath232 ( fig .",
    "[ fig : slots : diamond ] ) , mba allocates 160  threads for the azure table task and sam maps two full bundles of 60  threads each to 2 slots , and the remaining 40  threads on 1 partial slot . as per the model",
    ", sam expects the threads in a full slot to support @xmath269 and the partial slot to support @xmath270 .",
    "however , due to storm s shuffle grouping , the full slots receive @xmath271 while the partial slot receives @xmath272 .",
    "this problem does not affect rsm since it distributes threads across many slots achieving a degree of balance across slots .",
    "as future work , it is worth considering a slot - aware _ routing _ in storm as well  .",
    "also figs .",
    "[ fig : micro : slots ] show that as expected , _ the resource requirements increase proportionally with the input rate for both lsa+rsm and mba+sam_. some minor variations exist due to rounding up of partial slots to whole , or marginal differences in fragmentation . for e.g. , lsa+rsm assigns the star dag @xmath273 and @xmath274 for @xmath247 and @xmath232 rates , respectively .    _",
    "lastly , we also observe that all three micro - dags acquire about the same number of slots , for a given rate , using lsa+rsm , _ e.g. , using about 7 slots for @xmath253 for all three micro - dags .",
    "these three dags have the same 5 tasks though their composition pattern is different .",
    "however , for lsa , the memory% of the blob download task threads dominates the resource requirements for each dag , and the input rate to this task is the same as the dag input rate in all cases . as a result , for a given rate , the threads and resource needs for this task is the same for all three dags at 25 threads taking @xmath275 memory for @xmath253 , while the memory% for the entire linear dag is marginally higher at @xmath276 for this rate and its total cpu% need is only @xmath277 .",
    "hence , the resource needs for all other tasks , which are more cpu heavy , falls within the available 7  slots that is dominated by this blob task , i.e. , @xmath278 . in case of mba+sam",
    ", there is diversity both in cpu and memory utilization , and the number of threads for each task for the different dags .",
    "so the resource requirements are not overwhelmed by a single task . for e.g. , the same blob task at @xmath253",
    "requires only @xmath279 memory according to mba while the cpu% required for the entire linear dag is @xmath280 , which becomes the deciding factor for slot allocation .",
    "we perform similar experiments to analyze the resource benefits for more complex applications dags , limited to input rates of @xmath247 and @xmath232 that require as much as 65  slots for lsa+rsm costing @xmath281 per run .",
    "[ fig : app : slots ] plot the results .",
    "several of our observations here mimic the behavior of the scheduling approaches for the micro - dags , but at a larger scale .",
    "we also see more diversity across the dags , with finance taking @xmath282 fewer resources than grid for the same data rates .    as before for micro - dags , we see that mba+sam consistently uses @xmath283 fewer slots than lsa+rsm for all the application dags and rates .",
    "this is seen both for the allocation and in the incremental slots acquired by the mapping during packing .",
    "in fact , rsm acquires additional slots for all application dags allocated by lsa while mba needs this only for grid dag at @xmath253 .",
    "the resource benefit for mba+sam relative to lsa+rsm is the least for the finance dag in fig .",
    "[ fig : slots : finance ] , using @xmath254 fewer slots for @xmath253 rate and @xmath58 fewer for @xmath232 rate .",
    "this is because total cpu% tends to dominate for mba+sam , and this is higher for finance compared to the other two due to a higher input rate that arrives at the compute - intensive pi task . on the other hand",
    ", lsa+rsm consumes over twice the slots for traffic and grid dags due having one less pi task and one more blob task , which is memory intensive .",
    "this is due to random mapping of our candidate tasks to application tasks . as we saw earlier for the micro - dag",
    ", this memory intensive task tends to be sub - optimal when bin - packing by rsm , and that causes a the resource needs to grow for the traffic and grid dags .",
    "that said , while the dags are complex for these application workloads , the fraction of additional slots required by mapping relative to allocation does not grow much higher .",
    "in fact , the additional slots required by rsm is small in absolute numbers , at @xmath284 slots , as the bin - packing efficiency improves with more slots and threads .",
    "this shows that the punitive impact of rsm s additional slot requirements is mitigated for larger application dags .    as for the micro - dags ,",
    "several of the application dags are also unable to support the planned input rate .",
    "the impact worsens for lsa+rsm with its stable observed rate up to @xmath285 below expected , while this impact is much smaller for mba+sam with only up to @xmath286 lower rate observed despite the complex dag structure .",
    "the reasoning for sam is the same as for the micro - dags , where the shuffle grouping unformly distributes the output tuple rate across all threads . for rsm ,",
    "an additional factor comes into play for these larger dags . in practice",
    ", this algorithm allows threads in a slot to access all cores in a vm while restraining their memory use to only that slot s limit .",
    "this means threads in a single slot of a ` d3 ` vm can consume up to @xmath287 cpu , as long as their memory% is @xmath288 .",
    "this causes more cpu bound threads like pi and xmlparse to be mapped to a single slot , consuming @xmath289 of a vm s cpu in the grid dag for @xmath253 .",
    "however , each slot has just a single worker thread responsible for tuple buffering and routing between threads and across workers .",
    "having many cpu intensive task threads on the same slot stresses this worker thread and cause a slowdown , as seen for the grid dag which has an observe / expected tuple rates of @xmath290  .",
    "this consistently happens across all vms where threads with high cpu% are over - allocated to a single slot . in mba",
    ", the mapping of full bundles to a slot rather than over - allocating cpu% means that we have a better estimate of the collective behaviour of threads on each worker slot and these side - effects are avoided .    as before , we see that the resource requirements increase proportionally as the rate doubles from @xmath253 to @xmath232 in most cases . however , unlike the micro - dags where all the dataflows for a given input rate consumed about the same number of slots using lsa+rsm , this is not the case for the application dags . here , the number of tasks of each type vary and their complex compositions cause much higher diversity in input rates to these tasks . for e.g. , the same table task in traffic , finance and smart grid dags in figs  [ fig : app : dag ] have input rates of @xmath291 and @xmath292 .",
    "in fact , this complexity means that the resource usage does not just proportionally increase with the number of tasks either .",
    "this argues the need for non - trivial resource- and dag - aware scheduling strategies for streaming applications , such as rsm , mba and sam .      in the previous experiments",
    ", we showed that our mba+sam scheduling approach offered _ lower resource costs _ than the lsa+rsm scheduler while meeting the planned input rate more often . in these experiments , we show that our model based scheduling approach offers _ predictable resource utilization _ , and consequently _ reliable performance _ that can be generalized to other dags and data rates .",
    "further , we also show that it is possible to independently use our performance - model technique to accurately predict the resource usage and supported rates for other scheduling algorithms as well .    rather than determine the allocation for a given application and rate , we instead design these experiments with a fixed number of vms and slots  five ` d3 ` vms with @xmath64 total slots , for the three micro - dags .",
    "we then examine the highest input rate that our performance model estimates will be supported by the given schedule , and what is actually supported on enacting the schedules .",
    "the _ planned input rate _ is the peak rate for which the dag s resource requirements can be fulfilled with the fixed number of five d3 vms , according to the allocation+mapping algorithm pair that we consider . for this",
    ", we independently run the allocation _ and _ mapping algorithm plans outside storm , adding incremental input rates of @xmath266 until the resources required is just within or equal to @xmath64 slots according to the respective algorithm pairs .",
    "subsequently , for the threads and their slot mappings determined by the scheduling algorithm , we use our performance models to find the _ predicted rate _ supported by that dag .",
    "we also use our model to _ predict the cpu% and memory% _ for the slots as well , and report the cumulative value for each of the @xmath58 vms . the actual input rate for the dag",
    "is obtained empirically by increasing the rate in steps of @xmath266 as long as the execution remains stable .",
    "the actual cpu and memory utilization corresponding to the peak rate supported is reported for each vm as well . besides comparing the predicted and actual input rates",
    ", we also compare the predicted and actual vm resource usage in the analysis since there is a causal effect of the latter on the former .",
    "we further show that our model based allocation algorithm can be used independently with other mapping algorithms , besides sam . to this end",
    ", we evaluate and compare the baseline combination of lsa allocation with dsm and rsm mappings available in storm ( lsa+dsm and lsa+rsm ) , against our mba allocation with dsm , rsm and sam mapping algorithms ( mba+dsm , mba+rsm and mba+sam ) .      fig .",
    "[ fig : plot : rate : planned ] shows a scatter plot comparing the _ actual rate _ ( x axis ) and the _ planned rate _ by the scheduling algorithm ( y axis ) for the linear , diamond and star micro - dags , while fig .",
    "[ fig : plot : rate ] does the same for the _ actual rate _ ( x axis ) and our _ model predicted rate _ ( y axis ) .",
    "we see that our performance model is able to _ accurately predict the input rate for these dags with a high correlation coefficient of @xmath293 ranging from @xmath294_. this is actually significantly better than the planned rate by the schedulers for the three dags whose @xmath293 values fall between @xmath295 .",
    "thus , our performance model is able to accurately predict the input rates for the schedules from all 5 algorithm pairs , better than even the scheduling algorithms themselves .",
    "there are also algorithm - specific behavior of the prediction models , which we analyze .",
    "the input rate predictions are more accurate for mba+sam ( fig .",
    "[ fig : plot : rate ] , blue ` + ' ) , falling close to the 1:1 line in all cases , since it uses the model both for allocation and for mapping .",
    "however , it is not @xmath78 accurate due to storm s shuffle grouping that routes a different rate to downstream threads than expected .",
    "we also see our model underestimate the supported rate for lsa in fig .",
    "[ fig : plot : rate ] by a small fraction .",
    "this happens due to the granularity of the model .",
    "with lsa , several slots have 3 table threads mapped to them .",
    "as we do not have exact performance models with 3 threads , we interpolate between the available thread values which estimates the rate supported at @xmath296 while the observed rate is closer to @xmath297 . in such cases ,",
    "the predictions can be made more accurate if the performance modeling is done at a finer granularity of thread counts ( @xmath37 ) in algo .",
    "[ alg : bm ] .",
    "the algorithms also show distinctions in the actual rates that they support the same quanta of resources for a dag . when using mba with dsm , the actual rate is often much smaller than the planned rate and , to a lesser extent , than the predicted rate as well . for e.g. , the linear dag s planned rate is @xmath298 , predicted is @xmath241 and actual is @xmath299 . since dsm does a round - robin mapping of threads without regard to the model , it is unable to leverage the full potential of the allocation . in the case of the linear dag ,",
    "the allocation estimates the planned performance for , say , the blob download task with @xmath300 based on it being distributed in bundles of @xmath247 threads each on @xmath301 slots but dsm assigns them uniformly with @xmath302 .",
    "hence , using mba with dsm is not advisable , compared to rsm or sam mapping approaches .",
    "however , compared to lsa , mba offers a higher predicted and actual input rate irrespective of the mapping , offering improvements of @xmath303 .",
    "we observe from the plots that the cluster of points for mba ( in blue ) is consistently at a higher rate than the lsa cluster ( in red ) despite both being allocated the same fixed number of resources .",
    "as discussed before , lsa allocates fewer data - parallel threads than mba due to its linear - scaling assumption , and they are unable to fully exploit the available resources .",
    "this is consistent with the lower cpu% and memory% for lsa observed in figs .",
    "[ fig : plot : cpu ] and [ fig : plot : mem ] . for e.g. , the azure table task in the linear dag",
    "is assigned only @xmath304 threads by lsa , with a planned rate of @xmath305 whereas mba assigns it @xmath306 threads with a planned rate of @xmath298 .",
    "interestingly , when using mba , rsm is able to offer a higher actual input rate compared to sam in two of the three dags , linear and star ( fig .",
    "[ fig : plot : rate ] ) , even as its planned rate is lower than sam . for e.g.",
    ", we see that rsm s distance measure is able find the sweet spot for distributing the @xmath300 of blob download for the linear dag across 15 slots with @xmath307 threads each and 3 slots with under 10 threads each , to offer a predicted rate of @xmath308 and actual rate of @xmath309 .",
    "sam on the other hand favors predictable performance within exclusive slots and bundles @xmath310 each on 9 slots and the rest in a partial slot to give a predicted and actual rate close to @xmath298 .",
    "this highlights the trade - off that sam makes in favor of a predictable model - driven performance , while sacrificing some of performance benefits relative to rsm",
    ".      figs .",
    "[ fig : plot : cpu ] and  [ fig : plot : mem ] show the actual ( x axis ) and predicted ( y axis ) cpu% and memory% values for the three dags .",
    "each scatter plot has a data point for each of the 5 vms and for every scheduling algorithm pair .",
    "it is immediately clear from figs .",
    "[ fig : plot : cpu ] that our performance model is able to _ accurately predict the cpu% for each vm for these dags with a high correlation coefficient @xmath311 .",
    "_ this consistently holds for all three dags , scheduling algorithms , and for cpu utilization that ranges from @xmath312 .",
    "while for the linear dag , the cpu utilization accuracy is high , there are a few cases in the diamond and star dags where our predictions deviate from the observed for higher values of cpu% .",
    "the under - prediction of cpu for diamond dag with mba+sam is because the vms with pi thread bundles receive a slightly higher input rate than expected due to storm s shuffle grouping that impacts @xmath73 of the @xmath58 slots , and pi s cpu model is sensitive to even small changes in the rate . for e.g. , in fig .",
    "[ fig : cpu : diamond ] , a vm with a predicted cpu use of @xmath313 for a predicted input rate of @xmath85 ends up having an actual cpu usage of @xmath314 as it actually gets @xmath315 .",
    "this happens for the star dag with pi and blob threads we well . as mentioned before , enhancing storm s shuffle grouping to be sensitive to resource allocation for downstream slots will address this skew while improving the performance as well .",
    "at the same time , just from a modeling perspective , it is also possible to capture the round - robin routing of storm s shuffle grouping in the model to improve the predictability .    for star dag in fig .",
    "[ fig : cpu : star ] , there is one vm whose predicted cpu% is more than the actual for both mba+rsm and mba+sam .",
    "we find that both these vms have @xmath76 threads of the parse task , each on a separate slot , that are meant to support a required input rate of @xmath316 .",
    "however , a single thread of parse supports @xmath79 .",
    "since these two threads receive less than the peak rate of input , our model proportionally scales down the expected resource usage and estimates it at @xmath317 cpu usage .",
    "however , the actual turns out to be @xmath318 , causing an overestimate .",
    "as we mentioned , there is a balance between the costs for building fine - grained models and the accuracy of the models , and this prediction error is an outcome of this trade - off that causes us to interpolate .    for the diamond dag in fig .",
    "[ fig : cpu : diamond ] , we see two vms with expected cpu% of @xmath319 for mba+rsm but the observed values that are much lesser .",
    "these correspond to two pi threads that the mba algorithm expects the pair to be placed on the same slot with @xmath320 combined usage while rsm actually maps them onto different vms with @xmath236 fewer usage by each for 1 thread .    the prediction of memory utilization shown in figs .",
    "[ fig : plot : mem ] , while not as good as the cpu% is still valuable at @xmath321 .",
    "unlike the cpu usage that spans the entire spectrum from @xmath322 for each vm , the memory usage has a compact range with a median value of @xmath323 .",
    "this indicates that the dags are more compute - bound than memory - bound . due to this low memory% , even small variations in predictions",
    "has a large penalty on the correlation coefficient .",
    "we do see a few outlying clusters in these memory scatter plots . in the linear and star dags",
    ", we see that mba+dsm over - predicts the memory usage .",
    "this is because the round - robin mapping of dsm assigns single threads of xml parse to different slots , each of which receive fewer than their peak supported input rate . as a result , our model proportionally scales down the resources but ends up with an over - estimate . on the other hand ,",
    "we also see cases where we marginally under - predict the memory usage for these same dags for mba+sam . here , the shuffle grouping that sends a higher rate than expected to some slots with full thread bundles , and consequently a lower to other downstream threads , causing the resource usage to be lower than expected .",
    "we also see broader resource usage trends for specific scheduling approaches that can impact their input rate performance .",
    "_ we see that plans that use lsa consistently under - utilize resources . _",
    "the cpu% used is particularly bad for lsa , with the 5 vms for lsa - based plans using an average of just @xmath324 cpu each while the mba - based schedules use an average of @xmath325 per vm .",
    "this reaffirms our earlier insight that the allocation of the number of data - parallel threads by lsa is inadequate to utilize the resources available in the given vms . among dsm and rsm",
    ", we do see that rsm clearly has a better cpu% when using lsa though the memory% between dsm and rsm is relatively similar .",
    "the latter is because rsm ends up distributing memory intensive threads across multiple slots due to constraints on a slot , which has a pattern similar to dsm s round - robin approach .",
    "this shows the benefits of rsm over dsm , as is also seen in the input rates supported .",
    "however , _",
    "rsm has a more variable cpu% and memory% utilization across vms irrespective of the allocation .",
    "_ for e.g. in fig .",
    "[ fig : cpu : linear ] , the linear dag has cpu% that ranges from @xmath326 for lsa+rsm and from @xmath327 for mba+rsm .",
    "this is because rsm tries to pack all slots in a vm as long as the cumulative cpu% for the _ vm _ and the memory% _ per slot _ is not exhausted .",
    "this causes the cpu% of initially mapped vms to grow quickly due to the best - fit distance measure , while the remaining vms are packed with more memory - heavy tasks .",
    "this causes the skew .",
    "the dsm mapping uses a round - robin distribution of threads to slots and hence has is more tightly grouped . while sam uses a best - fit packing similar to rsm , this is limited to the partial thread bundles , and hence its resource skew across vms",
    "is mitigated .",
    "reducing the latency is not an explicit goal for our scheduling algorithms , though ensuring a stable latency is .",
    "however , some applications may require a low absolute latency value that is a factor in the schedule generator .",
    "so we also report the average latency distribution observed for the different scheduling algorithm pairs for the three micro - dags executed on a static set of 5 vms .",
    "the _ average latency _ of the dag is the average of time difference between each message consumed at the source tasks and all its causally dependent messages that are generated at the sink tasks .",
    "the latency of a message depends on the input rate and resources allocated to the task .",
    "it includes the network , queuing and processing time of tuple .",
    "the average latency is relevant only for a dag with a stable latency and resource configuration .",
    "we have used separate spout and sink tasks for logging each input and output tuple with a timestamp , and use this to plot the distribution of the average latency for a dag .",
    "[ fig : latency : dag ] shows a violin plot for the average latency for the three micro - dags executed on 5 vms using both lsa and mba based allocation with dsm , rsm , mba mappings , at stable rate .",
    "these results are for the same experiment setup as ",
    "[ sec : results : accy ] .",
    "we can make a few observations based on these plots .",
    "we see that the diamond micro - dag has a consistently lower latency , followed by the star dag and then the linear dag .",
    "as is evident from the dataflow structure , this is proportional to the number of tasks on the critical path of the dag , from the source to the sink .",
    "this is @xmath73 for diamond , @xmath58 for star and @xmath328 for linear .",
    "the median observed latency values typically increase in the order : mba+dsm @xmath329 \\{lsa+dsm , mba+rsm } @xmath329 lsa+rsm @xmath329 mba+sam .",
    "however , this has to be tempered by the input rates that these schedules support for the same dag and resource slots .",
    "while mba+dsm has a low latency , it supports the lowest rate among the three scheduling pairs that use mba , though all support a higher rate than the lsa - based algorithm pairs .",
    "so this is suitable for low latency and average throughput .",
    "mba+rsm has the next best latency given that rsm is network - aware and hence , able to lower the network transfer latency .",
    "this is positive given that it is also able to support a high input rate .",
    "the lsa+rsm schedule have the second worst latency and also the worst input rate , seen earlier .",
    "so this algorithm pair is not a good selection .",
    "separately , we also report that the mba schedules have a long tail distribution of latency values , indicating that the threads are running at peak utilization that is sensitive to small changes .",
    "the popularity of storm as a dsps has led to several publications on streaming dag scheduling that is customized for storm . as discussed before",
    ", storm natively supports the default round - robin scheduler and the r - storm resource - aware scheduler . both of these only participate in the mapping phase and not in thread or resource allocation .",
    "the round - robin scheduler @xcitedoes not take into account the actual vm resources required for a thread instead distributes them uniformly on all available vms .    in r - storm  @xcite",
    ", the user is expected to provide the cpu% , memory% and network bandwidth for each task thread under a stable input message rate , along with the number of threads for each task .",
    "it uses its resource - aware distance function to pack threads to vms with the goal of achieving a high resource utilization and minimum network latency costs . as we have shown earlier",
    ", this linear extrapolation is not effective in many cases .",
    "further , r - storm does not consider the input rates to the dag in its model .",
    "this means the resource utilization provided by the user is not scaled based on the actual rate that is received at a task thread .",
    "our techniques use both a performance model and interpolation over it to support non - linear behavior and diverse input rates that make it amenable to efficient scheduling even under dynamic conditions .",
    "however , r - storm is well suited for collections of dataflows that execute on large , shared strom clusters with hundred s vms that can be distributed across many racks in the data center . here",
    ", the network latency between vms vary greatly depending on their placement , and this can impact applications that have a high inter - task communication .",
    "our algorithms do not actively consider the network latency other than scheduling the threads in bfs order , like r - storm , to increase the possibility of collocating neighboring task threads in the dag on the same slot .",
    "consequently , our latency values suffer even as we offer predictable performance .",
    "our target is streaming applications launched on a paas storm cluster with tens of vms that have network proximity , and for this scenario , the absence of network consideration is acceptable",
    ". that said , including network distance is a relatively simple extension to our model .",
    "others have considered _ dynamic scheduling _ for apache storm as well , where the scheduler adapts to changes in input rates at runtime .",
    "latency is often the objective function they try to minimize while also limiting the resource usage .",
    "@xcite proposes an offline and an online scheduler which aim at minimizing the inter - node traffic by packing threads in decreasing order of communication cost into the same vm , while taking cpu capacity as constraint based on the resource usage at runtime .",
    "the goal here is on the mapping phase as well with the number of threads and slots for the tasks assumed to be known _",
    "a priori_. the offline algorithm just examines the topological structure and does not take message input rate or resource usage into consideration for scheduling .",
    "it just place the threads of adjacent tasks on same slot and then slots are assigned to worker nodes in round robin fashion .",
    "the online algorithm monitors the communication patterns and resource usage at run time to decide the mapping .",
    "it tries to reduce the inter - node and inter - slot traffic among the threads .",
    "the online algorithm have two phases , in the first phase threads are partitioned among the workers assigned to dag , minimizing the traffic among threads of distinct workers and balancing the cpu on each worker . in the second phase",
    "these workers are assigned to available slots in the cluster , minimizing the inter - node traffic .",
    "both these algorithms uses tuning parameters that controls the balancing of threads assigned per worker .",
    "these algorithms also have the effect of reducing intra - vm communication traffic besides inter - vm messaging .",
    "_ t - storm _",
    "@xcite also takes a similar mapping approach , but uses the summation of incoming and outgoing traffic rates for the tasks in descending order to decide the packing of threads to a vm .",
    "it further seeks to limit the messaging within a vm by just running one worker on each vm that is responsible for all threads on the vm .",
    "the algorithm monitors the load at run time and assigns the thread to available slot with minimum increamental traffic load .",
    "the number of threads for each task is user defined and their distribution among worker nodes is controlled by some parameter ( consolidation factor ) , which is obtained emperically .",
    "also , algorithm does not gurantee that communicating threads will be mapped to the same node as ordering is done based on total traffic and not on traffic between threads .",
    "both these schedulers  @xcite use only cpu% as the packing criteria at runtime , and this can cause memory overflow for memory intensive tasks or when the message queue size grows .",
    "their online algorithms also require active monitoring of the various tasks and slots at runtime , which can be invasive and entail management overheads . at the same time , despite their claim of adapting to runtime conditions , neither scheduler actually acquires new vm resources or relinquishes them , and instead just redistributes the existing threads on the captive set of vms for load balancing and reducing the active slots .",
    "thus , the input - rate capacity of the dataflow is bounded or the acquired captive resources can be under - utilized .",
    "further , the use of a single worker per vm in t - storm can be a bottleneck for queuing and routing when the total input and output rate of threads on that vm are high . while we do not directly address dynamic scheduling",
    ", our algorithms can potentially be rerun when the input rate changes to rebalance the schedule , without the need for fine - grained monitoring .",
    "this can include acquiring and releasing vms as well since we offer both allocation and mapping algorithms .",
    "we consider both memory and cpu usage in our model - based approach .",
    "a predictable schedule behavior is a stated goal for us rather than reducing absolute latency through reduced communication .",
    "p - scheduler _",
    "@xcite uses the ratio of total cpu usage from all threads to the vm s cpu threshold to find the number of vms required for the dag at runtime .",
    "the goal here is to dynamically determine the number of vms required at runtime based on cpu usage and then map the threads to these vms such that the traffic among vms is minimized .",
    "the mapping hierarchically partitions the dag , with edge - weights representing tuple transfer rate .",
    "it first maps threads to vms and then re - partitions threads within the vm to slots .",
    "this reduces the communication costs but the partitioning can cause unbalanced cpu usage , and the memory usage is not considered at all . while the algorithm does vm allocation , it does not consider thread allocation that can cause vms to be under - utilized .",
    "it also requires a centralized global monitoring of the data rates between threads and cpu% to perform the partitioning and allocation.as mentioned before , our scheduling offers both vm and thread allocation in addition to mapping , consider input rate , cpu% and memory% for the decisions , and our model does not rely on active runtime monitoring .",
    "there have been few works on resource and thread allocation for storm .",
    "the _ drs _",
    "@xcite is one such scheduler that models the dag as open queuing network to find the expected latency of a task for a given number of threads .",
    "its goal is to limit the expected task latency to within a user - defined threshold at runtime , while minimizing the total number of threads required and hence the resources .",
    "it monitors the tuple arrival and service rate at every task to find the expected latency using _ erlang formula _  @xcite .",
    "the approach first finds the expected number of threads required for the task so that latency bound is met .",
    "this is done by increasing a thread for the task which gives maximum latency improvement obtained by erlang formula discussed in paper , but it requires that an upper bound on total number of threads to be set by user . also paper assumes that only a fixed number of threads can run on a vm , independent of thread type .",
    "the number of vms are identified using total number of threads and number of threads that can run on a vm , already fixed by user .",
    "mapping is done by default scheduler only .",
    "drs uses an analytical approach like us , but based on queuing theory rather than empirical models .",
    "they apply it for runtime application but do not consider mapping of the threads to vm slots .",
    "we consider both allocation and mapping , but do not apply them to a dynamic scenario yet .",
    "neither approaches require intensive runtime monitoring of resources and rates . like us",
    ", they consider cpu resources and threads for allocation and not network , but unlike us , they do not consider memory% .",
    "their experiments show a mismatch between expected and observed performance from failing to include network delays in their model while our experiments do not highlight network communication to be a key contributor , partly because of the modest rates supported on the dags and resources we consider .",
    "they also bound the total number of cpu slots and the number of threads per vm , which we relax .",
    "while our scheduling algorithms were designed and evaluated in the context of storm , it is generalizable to other dsps as well .",
    "there has been a body of work on scheduling for dsps , both static and adaptive scheduling on cloud vms , besides those related to storm .",
    "borealis  @xcite an extnesion to aurora  @xcite provides parallel processing of streams .",
    "it uses local and neighbor load information for balancing load across the cluster by moving operators .",
    "they also differ from cloud based dsps as they assume that only fixed amount resources are available beforehand .",
    "some extensions to borealis like  @xcite , does not use intra operator level parallelism and considers only dynamic mapping of tasks for load balancing .",
    "telegraphcq  @xcite uses adaptive routing using special operators like eddy and juggle to optimize query plans .",
    "these special operators decides how to route data to different operators , reorders input tuples based on their content .",
    "it also dynamically decides the optimal stream partitioning for parallel processing .",
    "these systems allocate queries to seperate nodes for scaling with the number of queries and are not designed to run on cloud .",
    "cola  @xcite for system s , scalable distributed stream processing system aims at finding best operator fusion ( multiple operators within same process ) possible for reducing inter process stream traffic.the approach first uses list scheduling ( longest processing time ) to get operators schedule then it checks for vm capacity(only cpu ) if schedule is unfeasible , uses graph partitioning to split processing element to other vms.thus cola also does not take memory intensive operators in to account .",
    "infosphere streams  @xcite uses component - based programming model .",
    "it helps in composing and reconfiguring individual components to create different applications .",
    "the scheduler component  @xcite finds the best partitioning of data - flow graph and distributes it across a set of physical nodes .",
    "it uses the computational profiles of the operators , the loads on the nodes for making its scheduling decisions .",
    "apache s4  @xcite follows the actor model and allocation is decided manually based on distinct keys in the input stream .",
    "the messages are distributed across the vms based on hash function on all keyed attribute in input messages .",
    "s4 schedules parallel instances of operators but does not manage their parallelism and state . since it does not support dynamic resource management thus unable to handle varying load .",
    "ibm system s  @xcite run jobs in the form of data - flow graphs .",
    "it supports intra - query parallelism but management is manual .",
    "it also supports dynamic application composition and stream discovery , where multiple applications can directly interact .",
    "this support for sharing of streams across applications is done by annotating the messages with already declared types in global type system .",
    "this enables sharing of applications written by different developers through streams .",
    "esc  @xcite which process streaming data as key - value pairs .",
    "hash functions are used to balance load by dynamically mapping the keys to the machines and function itself can be updated at run time .",
    "hash function can also use the cpu , memory load based on the vm statistics for message distribution .",
    "dynamic updation of the dag based on the custom rules from user is also supported for load balance .",
    "a processing element in a dag can have multiple operators and can be created at run time as per need .",
    "there can be many workers for a processing element . since it dynamically adjusts the required computational resources based on the current load of the system it is good fit for use cases with varying load , with deployment on cloud .",
    "@xcite have used variant called dynamic dataflows that adapts to changing performance of cloud resources by using alternate processing elements.the logic uses variable sized bin packing for allocating processing element over the vms on cloud .",
    "dynamic rates are managed by allocating resources for alternate processing elements thus making tradeoff between cost and qos on cloud .    in  @xcite",
    "have proposed elastic auto - parallelization for balancing the dynamic load in case of spl applications .",
    "the scaling is based on a control algorithm that monitors the congestion and throughput at runtime to adjust data parallelism .",
    "each ope rator maintains a local state for every stream partition .",
    "an incremental migration protocol is proposed for maintaining states while scaling , minimizing the amount of state transfer between hosts .",
    "streamcloud  @xcite modifies the parallelism level by splitting queries into sub queries minimizing the distribution overhead of parallel processing , each of which have utmost one stateful operator that stores its state in a _ tuple - bucket _ , where the key for a state is a hash on a tuple . at the boundary between sub - queries ,",
    "tuples are hashed and routed to specific stateful task instances that hold tuple - bucket with their hash key .",
    "this ensures consistent stateful operations with data - parallelism .",
    "it uses special operators called load balancers placed over outgoing edge of each instance of subcluster , lb does bucket instance mapping to map buckets with instances of downstream clusters .",
    "chronosstream  @xcite hash - partitions computation states into collection of fine - grained slices and then distributes them to muliple nodes for scaling .",
    "each slice is a computationally independent unit associated with a subset of input streams and and can be transparently relocated without affecting the consistency .",
    "the elasticity is achieved by migrating the workload to new nodes using a lightweight transactional migration protocol based on states .",
    "elasticstream  @xcite considers a hybrid model for processing streams as it is impossible to process them on local stream computing environment due to finite resources .",
    "the goal is to minimize the charges for using the cloud while satisfying the sla , as a trade - off between the application s latency and charges uisng linear programming .",
    "the approach dynamically adjusts the resources required with dynamic rates in place of over - provisioning with fixed amount of resources .",
    "the implementation done on system s is able to assign or remove computational resources dynamically .",
    "twitter heron  @xcite does user defined thread allocation and mapping by aurora scheduler . in the paper  @xcite proposed an analytical model for resource allocation and dynamic mapping to meet latency requirement while maximizing throughput , for processing real time streams on hadoop .",
    "stela  @xcite uses effective throughput percentage ( etp ) as the metric to decide the task to be scaled when user requests scaling in / out with given number of machines .",
    "the number of threads required for the tasks and their mapping to slots is not being discussed in the paper .",
    "our model - based approach is similar to scheduling strategies employed in parallel job and thread scheduling for hpc applications .",
    "the performance modeling frameworks  @xcite for large hpc systems predicts application performance from a function of system profiles ( e.g. , memory performance , communication performance ) .",
    "these profiles can be analysed to improve the application performance by understanding the tuning parameters .",
    "also  @xcite proposes methods to reduce the time required for performance modelling , like combining the results of several compilation , execution , performance data analysis cycles into a application signature , so that these steps need not to be repeated each time a new performance question is asked .",
    "warwick performance prediction ( warpp )  @xcite simulator is used to construct application performance models for complex parallel scientific codes executing on thousands of processing cores .",
    "it utilises coarse - grained compute and network models to enable the accurate assessment of parallel application behaviour at large scale .",
    "the simulator exposes six types of discrete events ranging from compute to i / o read , write to generate events representing the behaviour of a parallel application .",
    "@xcite models the aplication performance for future architectures with several millions or billions of cores .",
    "it considers algebraic multigrid ( amg ) , a popular and highly efficient iterative solver to discuss the model - based predictions .",
    "it uses local computation and communication models as baseline for predicting the performance and its scalability on future machines .",
    "the paper  @xcite proposes simple analytical model to predict the execution time of massively parallel programs on gpu architecture .",
    "this is done by estimating the number of parallel memory requests by considering the number of running threads and memory bandwidth . the aplication execution time in gpu",
    "is dominated by the latency of memory instructions .",
    "thus by finding the number of memory requests that can be executed concurrently ( memory warp parallelism ) the effective costs of memory requests is estimated .",
    "@xcite proposes planning systems and compares them to queuing systems for resource managament in hpc .",
    "features like advance resource reservation , request diffusing can not be achieved using queuing because it considers only present resource usgae .",
    "planning systems like ccs , maui scheduler does resource planning for present and future by assigning start time to all requests and using run time estimates for each job .",
    "recent works like  @xcite uses statistical approach to predict application execution time using emperical analysis of execution time for small input sizes .",
    "the paper uses a collection of well known kernel benchmarks for modelling the execution time of each phase of an application .",
    "the approach collects profiles obtained by few short application runs to match phases to kernels and uses it for predicting the execution times accurately .",
    "our model - based mapping of a bundle of threads also has some similarities with _ co - scheduling _",
    "@xcite or _ gang scheduling _",
    "@xcite of threads in concurrent and parallel systems . in the former , a set of co - dependent processes that are part of a working set are scheduled simultaneously by the operating system ( os ) on multi - processors to avoid process thrashing . in gang scheduling ,",
    "a set of interacting threads are scheduled to concurrently execute on different processors and coordinate through busy - waits .",
    "the intution is to assign the threads to dedicated processors so that all dependent threads progress together without blocking .",
    "our allocation and mapping based on performance models tries to identify and leverage the benefits of co - scheduling coarse - grained thread bundles from the same task onto one cpu , with deterministic performance given by the models , and by separating out thread bundles from different tasks onto different slots to execute concurrently without resource contention .",
    "we also encounter issues seen in gang scheduling that may cause processor over or under - allocation if sizes of the gangs do not match the number processors , which is similar to the partial thread bundles mapped to the same slot in our case , causing interference but reusing partial slots  @xcite . at the same time",
    ", we perform the thread to slot mapping once in the streaming dataflow environment , and do not have to remap unless the input rate changes .",
    "hence , we do not deal with recurrent or fine - grained scheduling issues such as constantly having to schedule threads since the number of threads are much more than the cpu cores , paired gang scheduling for threads with interleaved blocking i / o and compute  @xcite , or admission control due to inadequate resources  @xcite .",
    "based on these results , we see that lsa+rsm consistently allocates more resources than mba+sam , often twice as many slots due to its linear extrapolation of rate and resources .",
    "however , it still misses the planned input rate supported by @xmath330 in several cases due to unbalanced mapping by rsm where the rate does not scale as it expects .",
    "we see a @xmath331 drop for mba+sam due to the shuffle grouping that uniformly routes tuples to threads .",
    "also , rsm often requires additional resources than ones allocated by lsa due to fragmentation during bin - packing , though this tends to be marginal .",
    "sam has less fragmentation due to packing full bundles to exclusive slots .",
    "these hold for all dags , both micro and application , small and large .",
    "the model - based prediction of input rates is much more accurate than the planned prediction , correlating with the actual rate with and @xmath332 .",
    "the few outliers we see are due to the model expecting a different routing compared to storm s shuffle grouping , and due to the interpolation of rates based on the granularity of the performance models .",
    "mba is consistently is better than lsa in the input rate supported for the same quanta of resources , through mba+dsm shows the least improvement .",
    "mba+rsm is often better than mba+sam in actual rate though mba+sam gives a predictable observed rate .",
    "our performance model is able to predict the resource utilization for individual vms with high accuracy , having @xmath293 value @xmath333 for cpu% and @xmath334 for mem% , independent of the allocation and mapping technique used .",
    "the few prediction errors we see are due to threads receiving fewer than the peak rate for processing , where our model proportionally scales down the estimated resource usage relative to a single - thread usage at the peak rate .",
    "the low memory% also causes the error to be sensitive to even small skews in the prediction , giving a lower correlation coefficient value .",
    "mba consistently has a higher resource utilization than lsa , that is also reflected in the better input rate performance .",
    "while the resource usage across vms for schedules based on mba are close together , rsm shows a greater variation of its cpu% and memory% across vms .",
    "the current slot aware mapping does not consider load aware shuffle groping , we can leverage it to have more accuracy for predicting supported input rate and resource requirement . also dynamic resource allocation and mapping for the given distribution of input rate or monitored input rate at run time",
    "is part of our future work ."
  ],
  "abstract_text": [
    "<S> distributed stream processing frameworks are being commonly used with the evolution of internet of things(iot ) . </S>",
    "<S> these frameworks are designed to adapt to the dynamic input message rate by scaling in / out.apache storm , originally developed by twitter is a widely used stream processing engine while others includes flink  @xcite spark streaming  @xcite . for running the streaming applications successfully there is need to know the optimal resource requirement , as over - estimation of resources adds extra cost.so we need some strategy to come up with the optimal resource requirement for a given streaming application . in this article </S>",
    "<S> , we propose a model - driven approach for scheduling streaming applications that effectively utilizes _ a priori _ knowledge of the applications to provide predictable scheduling behavior . </S>",
    "<S> specifically , we use application performance models to offer reliable estimates of the resource allocation required . </S>",
    "<S> further , this intuition also drives resource mapping , and helps narrow the estimated and actual dataflow performance and resource utilization . </S>",
    "<S> together , this model - driven scheduling approach gives a predictable application performance and resource utilization behavior for executing a given dsps application at a target input stream rate on distributed resources . </S>"
  ]
}