{
  "article_text": [
    ", there has been substantial interest in the theory of recovering sparse approximations of signals that satisfy linear measurements .",
    "compressed sensing research ( see , for example @xcite ) has developed conditions for measurement matrices under which ( approximately ) sparse signals can be recovered by solving a linear programming relaxation of the original np - hard combinatorial problem .",
    "this linear programming relaxation is usually known as `` basis pursuit . ''    in particular , in one of the first papers in this area , _ cf .",
    "_  @xcite , cands and tao presented a setup they called `` decoding by linear programming , '' henceforth called compressed sensing linear programming decoding ( * cs - lpd * ) , where the sparse signal corresponds to real - valued noise that is added to a real - valued signal that is to be recovered in a hypothetical communication problem .    at about the same time , in an independent line of research , feldman , wainwright , and karger considered the problem of decoding a binary linear code that is used for data communication over a binary - input memoryless channel , a problem that is also np - hard in general . in",
    "@xcite , they formulated this channel coding problem as an integer linear program , along with presenting a linear programming relaxation for it , henceforth called channel coding linear programming decoding ( * cc - lpd * ) .",
    "several theoretical results were subsequently proven about the efficiency of * cc - lpd * , in particular for low - density parity - check ( ldpc ) codes ( see , _",
    "e.g. _ , @xcite ) .    as we will see in the subsequent sections , * cs - lpd *  and * cc - lpd *  ( and the setups they are derived from ) look like similar linear programming relaxations , however , a priori it is rather unclear if there is a connection beyond this initial superficial similarity .",
    "the main technical difference is that * cs - lpd*is a relaxation of the objective function of a problem that is naturally over the reals while * cc - lpd *  involves a polytope relaxation of a problem defined over a finite field .",
    "indeed , cands and tao in their original paper asked the question  ( * ? ? ? * section vi.a ) : _ ``  in summary , there does not seem to be any explicit known connection with this line of work [ @xcite ] but it would perhaps be of future interest to explore if there is one . '' _    in this paper",
    "we present such a connection between * cs - lpd *  and * cc - lpd*. the general form of our results is that if a given binary parity - check matrix is `` good '' for * cc - lpd *  then the same matrix ( considered over the reals ) is a `` good '' measurement matrix for * cs - lpd*. the notion of a `` good '' parity - check matrix depends on which channel we use ( and a corresponding channel - dependent quantity called pseudo - weight ) .",
    "* based on results for the binary symmetric channel ( bsc ) , we show that if a parity - check matrix can correct any @xmath0 bit - flipping errors under * cc - lpd * , then the same matrix taken as a measurement matrix over the reals can be used to recover all @xmath0-sparse error signals under * cs - lpd*. * based on results for binary - input output - symmetric channels with bounded log - likelihood ratios , we can extend the previous result to show that performance guarantees for * cc - lpd *  for such channels can be translated into robust sparse - recovery guarantees in the @xmath1 sense ( see , _ e.g. _ , @xcite ) for * cs - lpd*. * performance guarantees for * cc - lpd *  for the binary - input additive white gaussian noise channel ( awgnc ) can be translated into robust sparse - recovery guarantees in the @xmath2 sense for * cs - lpd*. * max - fractional weight performance guarantees for * cc - lpd *  can be translated into robust sparse - recovery guarantees in the @xmath3 sense for * cs - lpd*. * performance guarantees for * cc - lpd *  for the binary erasure channel ( bec ) can be translated into performance guarantees for the compressed sensing setup where the support of the error signal is known and the decoder tries to recover the sparse signal ( _ i.e. _ , tries to solve the linear equations ) by back - substitution only .",
    "all our results are also valid in a stronger , point - wise sense . for example , for the bsc , if a parity - check matrix can recover a _ given set _ of @xmath0 bit flips under * cc - lpd * , the same matrix will recover any sparse signal supported on those @xmath0 coordinates under * cs - lpd*. in general , `` good '' performance of * cc - lpd *  on a given error support set will yield `` good '' * cs - lpd *  recovery for sparse signals supported by the same set .",
    "it should be noted that all our results are only one - way : we do not prove that a `` good '' zero - one measurement matrix will always be a `` good '' parity - check matrix for a binary code .",
    "this remains an interesting open problem .",
    "besides these main results we also present reformulations of * cc - lpd *  and * cs - lpd *  in terms of so - called graph covers : these reformulations will help in seeing further similarities and differences between these two linear programming relaxations .",
    "moreover , based on an operator that we will call the zero - infinity operator , we will define an optimization problem called @xmath4 , along with a relaxation of it called @xmath5 .",
    "let * cs - opt *  be the np - hard combinatorial problem mentioned at the beginning of the introduction whose relaxation is * cs - lpd*. first , we will show that @xmath5 is equivalent to @xmath6 .",
    "secondly , we will argue that the solution of * cs - lpd *  is `` closer '' to the solution of @xmath4 than the solution of * cs - lpd *  is to the solution of * cs - opt*. this is interesting because @xmath4 is , like * cs - opt * , in general an intractable optimization problem , and so @xmath4 is at least as justifiably as * cs - opt *  a difficult optimization problem whose solution is approximated by * cs - lpd*.    the organization of this paper is as follows . in section  [ sec : notation:1 ]",
    "we set up the notation that will be used .",
    "then , in sections  [ sec : cs : lpd:1 ] and  [ sec : cc : lpd:1 ] we review the compressed sensing and channel coding problems , along with their respective linear programming relaxations .",
    "section  [ sec : bridge:1 ] is the heart of this paper : it establishes the lemma that will bridge * cs - lpd *  and * cc - lpd *  for zero - one matrices . technically speaking",
    ", this lemma shows that non - zero vectors in the real nullspace of a measurement matrix ( _ i.e. _ , vectors that are problematic for * cs - lpd * ) can be mapped to non - zero vectors in the fundamental cone defined by that same matrix ( _ i.e. _ , to vectors that are problematic for * cc - lpd * ) .",
    "afterwards , in section  [ sec : translation:1 ] we use the previously developed machinery to establish the main results of this paper , namely the translation of performance guarantees from channel coding to compressed sensing . by relying on prior channel coding results  @xcite and the above - mentioned lemma , we present novel results on sparse compressed sensing matrices .",
    "perhaps the most interesting corollary involves the sparse deterministic matrices constructed in gallager s thesis  ( * ? ? ?",
    "* appendix  c ) .",
    "in particular , by combining our translation results with a recent breakthrough by arora _",
    "et al . _",
    "@xcite we show that high - girth deterministic matrices can be used for compressed sensing to recover sparse signals . to the best of our knowledge ,",
    "this is the first deterministic construction of measurement matrices with an order - optimal number of rows .",
    "subsequently , section  [ sec : reformulations:1 ] tightens the connection between * cc - lpd *  and * cs - lpd *  with the help of graph covers , and section  [ sec : minimizing : zero : infty : norm:1 ] presents the above - mentioned results involving the zero - infinity operator . finally , some conclusions are presented in section  [ sec : conclusions:1 ] .",
    "the appendices contain the longer proofs .",
    "moreover , appendix  [ sec : extensions : bridge : lemma:1 ] presents three generalizations of the bridge lemma ( _ cf .",
    "_  lemma  [ lemma : equation : nullspace : to : fc:1 ] in section  [ sec : bridge:1 ] ) to certain types of integer and complex valued matrices .",
    "let @xmath7 , @xmath8 , @xmath9 , @xmath10 , @xmath11 , @xmath12 , @xmath13 , and @xmath14 be the ring of integers , the set of non - negative integers , the set of positive integers , the field of real numbers , the set of non - negative real numbers , the set of positive real numbers , the field of complex numbers , and the finite field of size @xmath15 , respectively . unless noted otherwise , expressions , equalities , and inequalities will be over the field @xmath10 . the absolute value of a real number @xmath16 will be denoted by @xmath17 .    the size of a set @xmath18 will be denoted by @xmath19 . for any @xmath20 , we define the set @xmath21 { \\triangleq}\\ { 1 , \\ldots , m \\}$ ] .",
    "all vectors will be _ column _ vectors .",
    "if @xmath22 is some vector with integer entries , then @xmath23 will denote an equally long vector whose entries are reduced modulo @xmath15 .",
    "if @xmath18 is a subset of the set of coordinate indices of a vector @xmath24 then @xmath25 is the vector with @xmath19 entries that contains only the coordinates of @xmath24 whose coordinate index appears in @xmath18 .",
    "moreover , if @xmath24 is a real vector then we define @xmath26 to be the real vector @xmath27 with the same number of components as @xmath24 and with entries @xmath28 for all @xmath29 .",
    "finally , the inner product @xmath30 of two equally long vectors @xmath24 and @xmath31 is written @xmath32 .",
    "we define @xmath33 to be the support set of some vector @xmath24 .",
    "moreover , we let @xmath34 and @xmath35 be the set of vectors in @xmath36 and @xmath37 , respectively , which have at most @xmath0 non - zero components . we refer to vectors in these sets as @xmath0-sparse vectors .    for any real vector @xmath24",
    ", we define @xmath38 to be the @xmath39 norm of @xmath24 , _",
    "i.e. _ , the number of non - zero components of @xmath24 . note that @xmath40 , where @xmath41 is the hamming weight of @xmath24 .",
    "furthermore , @xmath42 , @xmath43 , and @xmath44 , and @xmath45 norms of @xmath24 .    for a matrix @xmath46 over @xmath10 with @xmath47 columns",
    "we denote its @xmath10-nullspace by @xmath48 and for a matrix @xmath46 over @xmath14 with @xmath47 columns we denote its @xmath14-nullspace by @xmath49 .",
    "let @xmath50 be some matrix .",
    "we denote the set of row and column indices of @xmath51 by @xmath52 and @xmath53 , respectively . we will also use the sets @xmath54 , @xmath55 , and @xmath56 , @xmath57 .",
    "moreover , for any set @xmath58 , we will denote its complement with respect to @xmath53 by @xmath59 , _",
    "i.e. _ , @xmath60 . in the following , when no confusion can arise , we will sometimes omit the argument @xmath51 in the preceding expressions .",
    "finally , for any @xmath61 and any vector @xmath62 , we define the @xmath63-fold lifting of @xmath24 to be the vector @xmath64 with components given by @xmath65 \\times [ m].\\end{aligned}\\ ] ] ( one can think of @xmath66 as the kronecker product of the vector @xmath24 with the all - one vector with @xmath63 components . )",
    "moreover , for any vector @xmath67 or @xmath68 we define the projection of @xmath69 to the space @xmath70 to be the vector @xmath71 with components given by @xmath72 }             \\tilde a_{(i , m ) } ,",
    "\\quad i \\in [ n].\\end{aligned}\\ ] ] ( in the case where @xmath69 is over @xmath14 , the summation is over @xmath13 and we use the standard embedding of @xmath73 into @xmath13 . )",
    "let @xmath74 be a real matrix of size @xmath75 , called the _ measurement matrix _ , and let @xmath76 be a real - valued vector containing @xmath77 measurements",
    ". in its simplest form , the compressed sensing problem consists of finding the sparsest real vector @xmath78 with @xmath47 components that satisfies @xmath79 , namely     +   +    assuming that there exists a sparse signal @xmath80 that satisfies the measurement @xmath81 , * cs - opt *  yields , for suitable matrices @xmath74 , an estimate @xmath82 that equals @xmath80 .",
    "this problem can also be interpreted  @xcite as part of the decoding problem that appears in a coded data communicating setup where the channel input alphabet is @xmath83 , the channel output alphabet is @xmath84 , and the information symbols are encoded with the help of a real - valued code @xmath85 of block length @xmath47 and dimension @xmath86 as follows .",
    "* the code is @xmath87 .",
    "because of this , the measurement matrix @xmath74 is sometimes also called an _",
    "annihilator matrix_. * a matrix @xmath88 for which @xmath89 is called a _ generator matrix _ for the code @xmath85 . with the help of such a matrix , information vectors @xmath90",
    "are encoded into codewords @xmath91 according to @xmath92 .",
    "* let @xmath93 be the _ received vector_. we can write @xmath94 for a suitably defined vector @xmath95 , which will be called the _",
    "error vector_. we initially assume that the channel is such that @xmath80 is _ sparse _ , _ i.e. _ , that the number of non - zero entries is bounded by some positive integer @xmath0 .",
    "this will be generalized later to channels where the vector @xmath80 is _ approximately sparse _ , _",
    "i.e. _ , where the number of large entries is bounded by some positive integer @xmath0 . *",
    "the receiver first computes the syndrome vector @xmath76 according to @xmath96 .",
    "note that @xmath97 in a second step , the receiver solves * cs - opt *  to obtain an estimate @xmath82 for @xmath80 , which can be used to obtain the codeword estimate @xmath98 , which in turn can be used to obtain the information word estimate @xmath99 .    because the complexity of solving * cs - opt *  is usually exponential in the relevant parameters",
    ", one can try to formulate and solve a related optimization problem with the aim that the related optimization problem yields very often the same solution as * cs - opt * , or at least very often a very good approximation to the solution given by * cs - opt*. in the context of * cs - opt * , a popular approach is to formulate and solve the following related optimization problem ( which , with the suitable introduction of auxiliary variables , can be turned into a linear program ) :     +   +    this relaxation is also known as _",
    "basis pursuit_.      a central question of compressed sensing theory is under what conditions the solution given by * cs - lpd *  equals ( or is very close to ) the solution given by * cs - opt*.    clearly , if @xmath100 and the matrix @xmath74 has rank @xmath47 , there is only one feasible @xmath78 and the two problems have the same solution .    in this paper",
    "we typically focus on the linear sparsity regime , _",
    "i.e. _ , @xmath101 and @xmath102 , but our techniques are more generally applicable",
    ". the question is for which measurement matrices ( hopefully with a small number of measurements @xmath77 ) the lp relaxation is tight , _",
    "i.e. _ , the estimate given by @xmath6 equals the estimate given by @xmath103",
    ".    celebrated compressed sensing results  ( _ e.g._@xcite ) established that `` good '' measurement matrices exist . here",
    ", by `` good '' measurement matrices we mean measurement matrices that have only @xmath104 rows and can recover all ( or almost all ) @xmath0-sparse signals under * cs - lpd*. note that for the linear sparsity regime , @xmath105 , the optimal scaling requires to construct matrices with a number of measurements that scales linearly in the signal dimension @xmath47 .",
    "one _ sufficient _ way to certify that a given measurement matrix is `` good '' is the well - known restricted isometry property ( rip ) , indicating that the matrix does not distort the @xmath106-norm of any @xmath0-sparse vector by too much .",
    "if this is the case , the lp relaxation will be tight for all @xmath0-sparse vectors @xmath80 and further the recovery will be robust to approximate sparsity  @xcite . as is well known , however , the rip is not a complete characterization of the lp relaxation of `` good '' measurement matrices ( see , _",
    "e.g. _ , @xcite ) . in this paper",
    "we use the nullspace characterization instead ( see , _ e.g. _ , @xcite ) , that gives a necessary and sufficient condition for a matrix to be `` good . ''",
    "[ def : nullspace : property : sets:1 ]    let @xmath107 and let @xmath108 .",
    "we say that @xmath74 has the nullspace property @xmath109 , and write @xmath110 , if @xmath111 we say that @xmath74 has the strict nullspace property @xmath112 , and write @xmath113 , if @xmath114    \\end{aligned}\\ ] ]    [ def : nullspace : property : k:1 ]    let @xmath115 and let @xmath108 .",
    "we say that @xmath74 has the nullspace property @xmath116 , and write @xmath117 , if @xmath118 we say that @xmath74 has the strict nullspace property @xmath119 , and write @xmath120 , if @xmath121    \\end{aligned}\\ ] ]    note that in the above two definitions , @xmath122 is usually chosen to be greater than or equal to @xmath123 .    as was shown independently by several authors ( see @xcite and references therein ) the nullspace condition in definition  [ def : nullspace : property : k:1 ]",
    "is a necessary and sufficient condition for a measurement matrix to be `` good '' for @xmath0-sparse signals , _",
    "i.e. _ , that the estimate given by * cs - lpd *  equals the estimate given by * cs - opt *  for these matrices . in particular , the nullspace characterization of `` good '' measurement matrices will be one of the keys to linking * cs - lpd *  with * cc - lpd*. observe that the requirement is that vectors in the nullspace of @xmath74 have their @xmath124 mass spread in substantially more than @xmath0 coordinates .",
    "( in fact , for @xmath125 , at least @xmath126 coordinates must be non - zero ) .",
    "the following theorem is adapted from  ( * ? ? ?",
    "* proposition  2 ) .",
    "[ theorem : sparse : error:1 ]    let @xmath74 be a measurement matrix . further , assume that @xmath127 and that @xmath80 has at most @xmath0 nonzero elements , _",
    "i.e. _ , @xmath128 .",
    "then the estimate @xmath82 produced by * cs - lpd *  will equal the estimate @xmath82 produced by * cs - opt *  if @xmath129 .",
    "_ remark : _ actually , as discussed in  @xcite , the condition @xmath130 is also necessary , but we will not use this here .    the next performance metric ( see , _",
    "e.g. _ , @xcite ) for cs involves recovering approximations to signals that are not exactly @xmath0-sparse .    an @xmath131 approximation guarantee for",
    "* cs - lpd *  means that * cs - lpd *  outputs an estimate @xmath82 that is within a factor @xmath132 from the best @xmath0-sparse approximation for @xmath80 , _",
    "i.e. _ , @xmath133 where the left - hand side is measured in the @xmath134-norm and the right - hand side is measured in the @xmath135-norm .",
    "note that the minimizer of the right - hand side of   ( for any norm ) is the vector @xmath136 that has the @xmath0 largest ( in magnitude ) coordinates of @xmath80 , also called the best @xmath0-term approximation of @xmath80  @xcite .",
    "therefore the right - hand side of   equals @xmath137 where @xmath138 is the support set of the @xmath0 largest ( in magnitude ) components of @xmath80 .",
    "also note that if @xmath80 is @xmath0-sparse then the above condition suggests that @xmath139 since the right hand - side of   vanishes , therefore it is a strictly stronger statement than recovery of sparse signals .",
    "( of course , such a stronger approximation guarantee for @xmath82 is usually only obtained under stronger assumptions on the measurement matrix . )",
    "the nullspace condition is a necessary and sufficient condition on a measurement matrix to obtain @xmath140 approximation guarantees .",
    "this is stated and proven in the next theorem which is adapted from  ( * ? ? ?",
    "* theorem  1 ) .",
    "( actually , we omit the necessity part in the next theorem since it will not be needed in this paper . )",
    "[ theorem : l1:l1:approximation : guarantee:1 ]    let @xmath74 be a measurement matrix , and let @xmath141 be a real constant . further , assume that @xmath127 .",
    "then for any set @xmath107 with @xmath142 the solution @xmath82 produced by * cs - lpd *  will satisfy @xmath143 if @xmath117 .",
    "see appendix  [ sec : proof : theorem : l1:l1:approximation : guarantee:1 ] .",
    "we consider coded data transmission over a memoryless channel with input alphabet @xmath144 , output alphabet @xmath145 , and channel law @xmath146 .",
    "the coding scheme will be based on a binary linear code @xmath147 of block length @xmath47 and dimension @xmath148 , @xmath149 . in the following",
    ", we will identify @xmath150 with @xmath14 .",
    "* let @xmath151 be a _ generator matrix _ for @xmath147 .",
    "consequently , @xmath152 has rank @xmath148 over @xmath14 , and information vectors @xmath153 are encoded into codewords @xmath154 according to @xmath155 , _",
    "i.e. _ , @xmath156 .",
    "* let @xmath157 be a _ parity - check matrix _ for @xmath147 .",
    "consequently , @xmath158 has rank @xmath159 over @xmath14 , and any @xmath154 satisfies @xmath160 if and only if @xmath161 , _",
    "i.e. _ , @xmath162 .",
    "* in the following we will mainly consider the three following channels ( see , for example , @xcite ) : the binary - input additive white gaussian noise channel ( awgnc , parameterized by its signal - to - noise ratio ) , the binary symmetric channel ( bsc , parameterized by its cross - over probability ) , and the binary erasure channel ( bec , parameterized by its erasure probability ) .",
    "* let @xmath163 be the _ received vector _ and define for each @xmath164 the _ log - likelihood ratio _ @xmath165 .",
    "is binary then @xmath145 can be identified with @xmath14 and we can write @xmath166 for a suitably defined vector @xmath167 , which will be called the error vector . moreover , we can define the _ syndrome vector _ @xmath168 .",
    "note that @xmath169 however , in the following , with the exception of section  [ sec : reformulations:1 ] , we will only use the log - likelihood ratio vector @xmath170 , and not the binary syndrome vector @xmath76 .",
    "( see definition  [ def : syndrome : for : general : binary : input : channels:1 ] for a way to define a syndrome vector also for non - binary channel output alphabets @xmath145 . ) ]    upon observing @xmath171 , the _ ( blockwise ) maximum - likelihood decoding _",
    "( mld ) rule decides for @xmath172 where @xmath173 .",
    "formally :     +   +    it is clear that instead of @xmath174 we can also maximize @xmath175 . noting that @xmath176 for @xmath177 , @xmath178",
    "can then be rewritten to read     +   +    because the cost function is linear , and a linear function attains its minimum at the extremal points of a convex set , this is essentially equivalent to     +   +    ( here , @xmath179 denotes the convex hull of @xmath147 after it has been embedded in @xmath36 .",
    "note that we wrote `` essentially equivalent '' because if more than one codeword in @xmath147 is optimal for @xmath178 then all points in the convex hull of these codewords are optimal for @xmath180 . )",
    "although * cc - mld2 *  is a linear program , it usually can not be solved efficiently because its description complexity is typically exponential in the block length of the code .",
    "vertices ) , and tree codes ( _ i.e. _ , codes whose tanner graph is a tree ) .",
    "( for more on this topic , see for example  @xcite .",
    ") however , these classes of codes are not good enough for achieving performance close to channel capacity even under ml decoding ( see , for example , @xcite . ) ]    however , one might try to solve a relaxation of * cc - mld2*. namely , as proposed by feldman , wainwright , and karger  @xcite , we can try to solve the optimization problem     +   +    where the relaxed set @xmath181 is given in the next definition .    for every @xmath182 ,",
    "let @xmath183 be the @xmath184-th row of @xmath158 and let @xmath185 then , the _ fundamental polytope _ @xmath186 of @xmath158 is defined to be the set @xmath187 vectors in @xmath188 will be called _ pseudo - codewords_.    in order to motivate this choice of relaxation , note that the code @xmath147 can be written as @xmath189 and so @xmath190 it can be verified  @xcite that this relaxation possesses the important property that all the vertices of @xmath179 are also vertices of @xmath188 .",
    "let us emphasize that different parity - check matrices for the same code usually lead to different fundamental polytopes and therefore to different * cc - lpd*s .    similarly to the compressed sensing setup",
    ", we want to understand when we can guarantee that the codeword estimate given by @xmath191 equals the codeword estimate given by @xmath192 .",
    "clearly , the performance of * cc - mld *  is a natural upper bound on the performance of * cc - lpd * , and a way to assess * cc - lpd *  is to study the gap to * cc - mld * , _",
    "e.g. _ , by comparing the here - discussed performance guarantees for * cc - lpd *  with known performance guarantees for * cc - mld*.    when characterizing the * cc - lpd *  performance of binary linear codes over binary - input output - symmetric memoryless channels we can , without loss of generality , assume that the all - zero codeword was transmitted  @xcite",
    ". with this , the success probability of * cc - lpd *  is the probability that the all - zero codeword yields the lowest cost function value when compared to all non - zero vectors in the fundamental polytope .",
    "because the cost function is linear , this is equivalent to the statement that the success probability of * cc - lpd *  equals the probability that the all - zero codeword yields the lowest cost function value compared to all non - zero vectors in the conic hull of the fundamental polytope .",
    "this conic hull is called the _ fundamental cone _ @xmath193 and it can be written as @xmath194 the fundamental cone can be characterized by the inequalities listed in the following lemma  @xcite . (",
    "similar inequalities can be given for the fundamental polytope but we will not list them here since they are not needed in this paper . )    [ lemma : fundamental : cone:1 ]    the fundamental cone @xmath193 of @xmath158 is the set of all vectors @xmath195 that satisfy @xmath196 -0.35 cm @xmath197    note that in the following , not only vectors in the fundamental polytope , but also vectors in the fundamental cone will be called pseudo - codewords .",
    "moreover , if @xmath74 is a _",
    "zero - one measurement matrix _ , _",
    "i.e. _ , a measurement matrix where all entries are in @xmath73 , then we will consider @xmath74 to represent also the parity - check matrix of some linear code over @xmath14 .",
    "consequently , its fundamental polytope will be denoted by @xmath198 and its fundamental cone by @xmath199 .",
    "the following lemma gives a sufficient condition on @xmath158 for * cc - lpd *  to succeed over a bsc .",
    "[ lemma : bsc : strict : balancedness:1 ]    let @xmath158 be a parity - check matrix of a code @xmath147 and let @xmath200 be the set of coordinate indices that are flipped by a bsc with non - zero cross - over probability . if @xmath158 is such that @xmath201 for all @xmath202 , then the * cc - lpd *  decision equals the codeword that was sent .",
    "_ remark : _ the above condition is also necessary ; however , we will not use this fact in the following .    see appendix  [ sec :",
    "proof : lemma : bsc : strict : balancedness:1 ] .",
    "note that the inequality in   is _ identical _ to the inequality that appears in the definition of the strict nullspace property for @xmath203  ( ! ) .",
    "this observation makes one wonder if there is a deeper connection between * cs - lpd *  and * cc - lpd *  beyond this apparent one , in particular for measurement matrices that contain only zeros and ones . of course , in order to formalize a connection we first need to understand how points in the nullspace of a zero - one measurement matrix @xmath74 can be associated with points in the fundamental polytope of the parity - check matrix @xmath74 ( now seen as a parity - check matrix for a code over @xmath14 ) .",
    "such a mapping will be exhibited in the upcoming section  [ sec : bridge:1 ] . before turning to that section , though",
    ", we need to discuss pseudo - weights , which are a popular way of measuring the importance of the different pseudo - codewords in the fundamental cone and which will be used for establishing performance guarantees for * cc - lpd*.      note that the fundamental polytope and cone are functions only of the parity - check matrix of the code and _ not _ of the channel .",
    "the influence of the channel is reflected in the pseudo - weight of the pseudo - codewords , so it is only natural that every channel has its own pseudo - weight definition .",
    "therefore , every communication channel model comes with the right measure of `` distance '' that determines how often a ( fractional ) vertex is incorrectly chosen in * cc - lpd*.    [ def : pseudo : weights:1 ]    let @xmath204 be a nonzero vector in @xmath205 with @xmath206 @xmath207 .    * the awgnc pseudo - weight of @xmath204",
    "is defined to be @xmath208 * in order to define the bsc pseudo - weight @xmath209 , we let @xmath210 be the vector with the same components as @xmath204 but in non - increasing order , _",
    "i.e. _ , @xmath210 is a `` sorted version '' of @xmath204 .",
    "now let @xmath211 with this , the bsc pseudo - weight @xmath209 of @xmath204 is defined to be @xmath212 . *",
    "the bec pseudo - weight of @xmath204 is defined to be @xmath213 * the max - fractional weight of @xmath204 is defined to be @xmath214    for @xmath215 we define all of the above pseudo - weights and the max - fractional weight to be zero .    for a parity - check matrix @xmath158",
    ", the minimum awgnc pseudo - weight is defined to be @xmath216 the minimum bsc pseudo - weight @xmath217 , the minimum bec pseudo - weight @xmath218 , and the minimum max - fractional weight @xmath219 of @xmath158 are defined analogously .",
    "note that although @xmath219 yields weaker performance guarantees than the other quantities  @xcite , it has the advantage of being efficiently computable  @xcite .",
    "there are other possible definitions of a bsc pseudo - weight .",
    "for example , the bsc pseudo - weight of @xmath204 can also be taken to be @xmath220 where @xmath210 is defined as in definition  [ def : pseudo : weights:1 ] and where @xmath221 is the smallest integer such that @xmath222 .",
    "this definition of the bsc pseudo - weight was for example used in  @xcite .",
    "( note that in  @xcite the quantity @xmath223 was introduced as `` bsc effective weight . '' )    of course , the values @xmath209 and @xmath223 are tightly connected .",
    "namely , if @xmath223 is an even integer then @xmath224 , and if @xmath223 is an odd integer then @xmath225 .",
    "the following lemma establishes a connection between bsc pseudo - weights and the condition that appears in lemma  [ lemma : bsc : strict : balancedness:1 ] .",
    "[ lemma : meaning : of : bsc : pseudo : weight:1 ]    let @xmath158 be a parity - check matrix of a code @xmath147 and let @xmath204 be an arbitrary non - zero pseudo - codeword of @xmath158 , _",
    "i.e. _ , @xmath202 .",
    "then , for all sets @xmath200 with @xmath226 it holds that @xmath227    see appendix  [ sec : proof : lemma : meaning : of : bsc : pseudo : weight:1 ] .",
    "we are now ready to establish the promised bridge between * cs - lpd *  and * cc - lpd*to be used in section  [ sec : translation:1 ] to translate performance guarantees from one setup to the other .",
    "our main tool is a simple lemma that was already established in  @xcite , but for a different purpose .",
    "we remind the reader that we have extended the use of the absolute value operator @xmath228 from scalars to vectors .",
    "so , if @xmath229 is a real ( complex ) vector then we define @xmath230 to be the real ( complex ) vector @xmath231 with the same number of components as @xmath24 and with entries @xmath232 for all @xmath29 .",
    "[ lemma : equation : nullspace : to : fc:1 ]    let @xmath74 be a zero - one measurement matrix",
    ". then @xmath233    _ remark : _ note that @xmath234 .",
    "let @xmath235 .",
    "in order to show that such a vector @xmath204 is indeed in the fundamental cone of @xmath74 , we need to verify   and  . the way @xmath204 is defined , it is clear that it satisfies  .",
    "therefore , let us focus on the proof that @xmath204 satisfies  .",
    "namely , from @xmath236 it follows that for all @xmath237 , @xmath238 , _",
    "i.e. _ , for all @xmath237 , @xmath239",
    ". this implies @xmath240 for all @xmath237 and all @xmath241 , showing that @xmath204 indeed satisfies  .",
    "this lemma gives a one - way result : with every point in the @xmath10-nullspace of the measurement matrix @xmath74 we can associate a point in the fundamental cone of @xmath74 , but not necessarily vice - versa .",
    "therefore , a problematic point for the @xmath10-nullspace of @xmath74 will translate to a problematic point in the fundamental cone of @xmath74 and hence to bad performance of * cc - lpd*. similarly , a `` good '' parity - check matrix @xmath74 must have no low pseudo - weight points in the fundamental cone , which means that there are no problematic points in the @xmath10-nullspace of @xmath74 .",
    "therefore , `` positive '' results for channel coding will translate into `` positive '' results for compressed sensing , and `` negative '' results for compressed sensing will translate into `` negative '' results for channel coding .",
    "further , lemma  [ lemma : equation : nullspace : to : fc:1 ] preserves the support of a given point @xmath242 .",
    "this means that if there are no low pseudo - weight points in the fundamental cone of @xmath74 with a given support , there are no problematic points in the @xmath10-nullspace of @xmath74 with the same support , which allows point - wise versions of all our results in section  [ sec : translation:1 ] .",
    "note that lemma  [ lemma : equation : nullspace : to : fc:1 ] assumes that @xmath74 is a zero - one measurement matrix , _",
    "i.e. _ , that it contains only zeros and ones .",
    "as we show in appendix  [ sec : extensions : bridge : lemma:1 ] , there are suitable extensions of this lemma that put less restrictions on the measurement matrix .",
    "however , apart from remark  [ remark : dense : measurement : matrix:1 ] , we will not use these extensions in the following .",
    "( we leave it as an exercise to extend the results in the upcoming sections to this more general class of measurement matrices . )",
    "in this section we use the above - established bridge between * cs - lpd *  and * cc - lpd *  to translate `` positive '' results about * cc - lpd *  to `` positive '' results about * cs - lpd*. whereas sections  [ sec : role : bsc:1 ] to  [ sec : role : bec:1 ] focus on the translation of abstract performance bounds , section  [ sec : translating : performance : guarantees:1 ] presents the translation of numerical performance bounds .",
    "finally , in section  [ sec : dense : measurement : matrices:1 ] , we briefly discuss some limitations of our approach when dense measurement matrices are considered .",
    "[ lemma : from : bsc : to : cs : lpd:1 ]    let @xmath243 be a cs measurement matrix and let @xmath0 be a non - negative integer . then @xmath244    fix some @xmath245 . by lemma  [ lemma :",
    "equation : nullspace : to : fc:1 ] we know that @xmath246 is a pseudo - codeword of @xmath74 , and by the assumption @xmath247 we know that @xmath248 .",
    "then , using lemma  [ lemma : meaning : of : bsc : pseudo : weight:1 ] , we conclude that for all sets @xmath249 with @xmath142 , we must have @xmath250 . because @xmath242 was arbitrary , the claim @xmath251 clearly follows .",
    "this result , along with theorem  [ theorem : sparse : error:1 ] can be used to establish sparse signal recovery guarantees for a compressed sensing matrix @xmath74 .",
    "note that compressed sensing theory distinguishes between the so - called * strong bounds * and the so - called * weak bounds*. the former bounds correspond to a worst - case setup and guarantee the recovery of all @xmath0-sparse signals , whereas the latter bounds correspond to an average - case setup and guarantee the recovery of a signal on a randomly selected support with high probability regardless of the values of the non - zero entries .",
    "note that a further notion of a weak bound can be defined if we randomize over the non - zero entries also , but this is not considered in this paper .",
    "similarly , for channel coding over the bsc , there is a distinction between being able to recover from @xmath0 worst - case bit - flipping errors and being able to recover from randomly positioned bit - flipping errors .",
    "in particular , recent results on the performance analysis of * cc - lpd *  have shown that parity - check matrices constructed from expander graphs can correct a constant fraction ( of the block length @xmath47 ) of worst - case errors ( _ cf .",
    "_  @xcite ) and random errors ( _ cf .",
    "_  @xcite ) .",
    "these worst - case error performance guarantees implicitly show that the minimum bsc pseudo - weight of a binary linear code defined by a tanner graph with sufficient expansion ( expansion strictly larger than @xmath252 ) must grow linearly in @xmath47 .",
    "( a conclusion in a similar direction can be drawn for the random error setup . ) now , with the help of lemma  [ lemma : from : bsc : to : cs : lpd:1 ] , we can obtain new performance guarantees for * cs - lpd*.    let us mention that in  @xcite , expansion arguments were used to directly obtain similar types of performance guarantees for compressed sensing ; in section  [ sec : translating : performance : guarantees:1 ] we compare these results to the guarantees we can obtain through our translation techniques .",
    "in contrast to the present subsection , which deals with the recovery of ( exactly ) sparse signals , the next three subsections ( sections  [ sec : beyond : bsc:1 ] , [ sec : role : awgnc:1 ] , and  [ sec : role : maxfrac : weight:1 ] ) deal with the recovery of approximately sparse signals .",
    "note that the type of guarantees presented in these subsections are known as * instance optimality * guarantees  @xcite .      in lemma  [ lemma : from : bsc :",
    "to : cs : lpd:1 ] we established a connection between , on the one hand , performance guarantees for the bsc under * cc - lpd * , and , on the other hand , the strict nullspace property @xmath253 for @xmath203 .",
    "it is worthwhile to mention that one can also establish a connection between performance guarantees for a certain class of binary - input channels under * cs - lpd *  and the strict nullspace property @xmath253 for @xmath141 . without going into details ,",
    "this connection is established with the help of results from  @xcite , that generalize results from  @xcite , and which deal with a class of binary - input memoryless channels where all output symbols are such that the magnitude of the corresponding log - likelihood ratio is bounded by some constant @xmath254 .",
    "is gained not by quantization , but rather by restricting the llrs to have finite support . ''",
    "should read `` this suggests that the asymptotic advantage over [  ] is gained not by quantization , but rather by restricting the llrs to have bounded support . ''",
    "] this observation , along with theorem  [ theorem : l1:l1:approximation : guarantee:1 ] , can be used to establish instance optimality @xmath1 guarantees for a compressed sensing matrix @xmath74 .",
    "let us point out that in some recent follow - up work  @xcite this has been accomplished .",
    "[ theorem : weight : l2:l1:and : approximation : guarantees:1 ]    let @xmath243 be a measurement matrix and let @xmath76 and @xmath80 be such that @xmath127 .",
    "let @xmath107 with @xmath255 , and let @xmath256 be an arbitrary positive real number with @xmath257 .",
    "then the estimate @xmath82 produced by * cs - lpd *  will satisfy @xmath258 if @xmath259 holds for all @xmath260 .",
    "( in particular , this latter condition is satisfied for a measurement matrix @xmath74 with @xmath261 . )",
    "see appendix  [ sec : proof : theorem : weight : l2:l1:and : approximation : guarantees:1 ] .",
    "[ theorem : weight : linfty : l1:and : approximation : guarantees:1 ]    let @xmath243 be a measurement matrix and let @xmath76 and @xmath80 be such that @xmath127 .",
    "let @xmath107 with @xmath255 , and let @xmath256 be an arbitrary positive real number with @xmath263 .",
    "then the estimate @xmath82 produced by * cs - lpd *  will satisfy @xmath264 if @xmath265 holds for all @xmath260 .",
    "( in particular , this latter condition is satisfied for a measurement matrix @xmath74 with @xmath266 . )",
    "see appendix  [ sec : proof : theorem : weight : linfty : l1:and : approximation : guarantees:1 ] .      for the binary erasure channel , * cc - lpd *  is identical to the peeling decoder ( see , _",
    "e.g. _ , ( * ? ? ?",
    "* chapter  3.19 ) ) that solves a system of linear equations by only using back - substitution .",
    "we can define an analogous compressed sensing problem by assuming that the _ support _ of the sparse signal @xmath80 is known to the decoder , and that the recovering of the values is performed only by back - substitution .",
    "this simple procedure is related to iterative algorithms that recover sparse approximations more efficiently than by solving an optimization problem ( see , _",
    "e.g. _ , @xcite and references therein ) .    for this special case ,",
    "it is clear that * cc - lpd *  for the bec and the described compressed sensing decoder have identical performance since back - substitution behaves exactly the same way over any field , be it the field of real numbers or any finite field .",
    "( note that whereas the result of * cc - lpd *  for the bec equals the result of the back - substitution - based decoder for the bec , the same is not true for compressed sensing , _ i.e. _ , * cs - lpd *  with given support of the sparse signal can be strictly better than the back - substitution - based decoder with given support of the sparse signal . )      in this section we use the bridge lemma , lemma  [ lemma : equation : nullspace : to : fc:1 ] , along with previous positive performance results for * cc - lpd * , to establish performance results for the * cs - lpd *  / basis pursuit setup .",
    "in particular , three positive threshold results for * cc - lpd *  of low - density parity - check ( ldpc ) codes are used to obtain three results that are , to the best of our knowledge , novel for compressed sensing :    * * corollary  [ cor : translation1 ] * ( which relies on work by feldman , malkin , servedio , stein , and wainwright  @xcite ) is very similar to  @xcite , although our proof is obtained through the connection to channel coding .",
    "we obtain a strong bound with similar expansion requirements . *",
    "* corollary  [ cor : daskalakis : etal:1 ] * ( which relies on work by daskalakis , dimakis , karp , and wainwright  @xcite ) is a result that yields better constants ( _ i.e. _ , larger recoverable signals ) but only with high probability over supports ( _ i.e. _ , it is a so - called weak bound ) .",
    "* * corollary  [ cor : arora : etal:1 ] * ( which relies on work by arora , daskalakis , and steurer  @xcite ) is , in our opinion the most important contribution .",
    "we show the first deterministic construction of compressed sensing measurement matrices with an order - optimal number of measurements .",
    "further we show that a property that is easy to check in polynomial time ( _ i.e. _ , girth ) , can be used to certify measurement matrices .",
    "further , in the follow - up paper  @xcite it is shown that similar techniques can be used to construct the first optimal measurement matrices with @xmath1 sparse approximation properties .    at the end of the section we also use lemma  [ lemma :",
    "equation : nullspace : to : fc:1:complex : case ] ( _ cf . _",
    "appendix  [ sec : extensions : bridge : lemma:1 ] ) with @xmath267 to study dense measurement matrices with entries in @xmath268 .",
    "before we can state our first translation result , we need to introduce some notation .",
    "let @xmath269 be a bipartite graph where the nodes in the two node classes are called left - nodes and right - nodes , respectively .",
    "if @xmath18 is some subset of left - nodes , we let @xmath270 be the subset of the right - nodes that are adjacent to @xmath18 . then",
    ", given parameters @xmath271 , @xmath272 , @xmath273 , we say that @xmath269 is a @xmath274-expander if all left - nodes of @xmath269 have degree @xmath275 and if for all left - node subsets @xmath18 with @xmath276 it holds that @xmath277 .",
    "expander graphs have been studied extensively in past work on channel coding ( see , _",
    "e.g. _ , @xcite ) and compressed sensing ( see , _",
    "e.g. _ , @xcite ) .",
    "it is well known that randomly constructed left - regular bipartite graphs are expanders with high probability ( see , _ e.g. _ , @xcite ) .    in the following ,",
    "similar to the way a tanner graph is associated with a parity - check matrix  @xcite , we will associate a tanner graph with a measurement matrix .",
    "note that the variable and constraint nodes of a tanner graph will be called left - nodes and right - nodes , respectively .    with this",
    ", we are ready to present the first translation result , which is a so - called strong bound ( _ cf . _  the discussion in section  [ sec : role : bsc:1 ] ) .",
    "it is based on a theorem from  @xcite .",
    "[ cor : translation1 ]    let @xmath278 and @xmath272 .",
    "let @xmath279 be a measurement matrix such that the tanner graph of @xmath74 is a @xmath274-expander with sufficient expansion , more precisely , with @xmath280 ( along with the technical condition @xmath281 ) .",
    "then * cs - lpd * based on the measurement matrix @xmath74 can recover all @xmath0-sparse vectors , _",
    "i.e. _ , all vectors whose support size is at most @xmath0 , for @xmath282    this result is easily obtained by combining lemma  [ lemma : equation : nullspace : to : fc:1 ] with  ( * ? ? ?",
    "* theorem  1 ) .",
    "interestingly , for @xmath283 the recoverable sparsity @xmath0 matches exactly the performance of the fast compressed sensing algorithm in  @xcite and the performance of the simple bit - flipping channel decoder of sipser an spielman  @xcite , however , our result holds for the * cs - lpd *  / basis pursuit setup .",
    "moreover , using results about expander graphs from  @xcite , the above corollary implies , for example , that , for @xmath284 and @xmath285 , sparse expander - based zero - one measurement matrices will recover all @xmath286 sparse vectors for @xmath287 . to the best of our knowledge ,",
    "the only previously known result for sparse measurement matrices under basis pursuit is the work of berinde _ et al . _",
    "as shown by the authors of that paper , the adjacency matrices of expander graphs ( for expansion @xmath288 ) will recover all @xmath0-sparse signals .",
    "further , these authors also state results giving @xmath140 instance optimality sparse approximation guarantees .",
    "their proof is directly done for the compressed sensing problem and is therefore fundamentally different from our approach which uses the connection to channel coding .",
    "the result of corollary  [ cor : translation1 ] implies a strong bound for all @xmath0-sparse signals under basis pursuit and zero - one measurement matrices based on expander graphs . since we only require expansion @xmath289 , however , we can obtain slightly better constants than  @xcite .",
    "even though we present the result of recovering exactly @xmath0-sparse signals , the results of  @xcite can be used to establish @xmath140 sparse recovery for the same constants .",
    "we note that in the linear sparsity regime @xmath290 , the scaling of @xmath291 is order optimal and also the obtained constants are the best known for strong bounds of basis pursuit . still , these theoretical bounds are quite far from the observed experimental performance .",
    "also note that the work by zhang and pfister  @xcite and by lu _",
    "et al . _",
    "@xcite use density evolution arguments to determine the precise threshold constant for sparse measurement matrices , but these are for message - passing decoding algorithms which are often not robust to noise and approximate sparsity .",
    "in contrast to corollary  [ cor : translation1 ] that presented a strong bound , the following corollary presents a so - called weak bound ( _ cf . _  the discussion in section  [ sec : role : bsc:1 ] ) , but with a better threshold .",
    "[ cor : daskalakis : etal:1 ]    let @xmath278 .",
    "consider a random measurement matrix @xmath292 formed by placing @xmath275 random ones in each column , and zeros elsewhere .",
    "this measurement matrix succeeds in recovering a randomly supported @xmath286 sparse vector with probability @xmath293 if @xmath294 is below some threshold value @xmath295 .",
    "the result is obtained by combining lemma  [ lemma : equation : nullspace : to : fc:1 ] with  ( * ? ? ?",
    "* theorem  1 ) .",
    "the latter paper also contains a way to compute the achievable threshold values @xmath295 .    using results about expander graphs from  @xcite , the above corollary implies , for example ,",
    "that for @xmath296 and @xmath297 , a random measurement matrix will recover with high probability a @xmath286 sparse vector with random support if @xmath298 .",
    "this is , of course , a much higher threshold compared to the one presented above , but it only holds with high probability over the vector support ( therefore it is a so - called weak bound ) . to the best of our knowledge ,",
    "this is the first weak bound obtained for random sparse measurement matrices under basis pursuit .",
    "the best thresholds known for lp decoding were recently obtained by arora , daskalakis , and steurer  @xcite but require matrices that are both left and right regular and also have logarithmically growing girth .",
    "a random bipartite matrix will not have logarithmically growing girth but there are explicit deterministic constructions that achieve this ( for example the construction presented in gallager s thesis  ( * ? ? ?",
    "* appendix  c ) ) .",
    "[ cor : arora : etal:1 ]    let @xmath299 .",
    "consider a measurement matrix @xmath300 whose tanner graph is a @xmath301-regular bipartite graph with @xmath302 girth .",
    "this measurement matrix succeeds in recovering a randomly supported @xmath286 sparse vector with probability @xmath293 if @xmath294 is below some threshold function @xmath303 .",
    "the result is obtained by combining lemma  [ lemma : equation : nullspace : to : fc:1 ] with  ( * ? ? ?",
    "* theorem 1 ) .",
    "the latter paper also contains a way to compute the achievable threshold values @xmath304 .    using results from  @xcite , the above corollary yields for @xmath305 and a @xmath306-regular tanner graph with logarithmic girth ( obtained from gallager s construction ) the fact that sparse vectors with sparsity @xmath307 are recoverable with high probability for @xmath308 .",
    "therefore , zero - one measurement matrices based on gallager s deterministic ldpc construction form sparse measurement matrices with an order - optimal number of measurements ( and the best known constants ) for the * cs - lpd *  / basis pursuit setup .    * a note on deterministic constructions : *",
    "we say that a method to construct a measurement matrix is deterministic if it can be created deterministically in polynomial time , or it has a property that can be verified in polynomial time .",
    "unfortunately , all known bipartite expansion - based constructions are non - deterministic because even though random constructions will have the required expansion with high probability , there is , to the best of our knowledge , no known efficient way to check expansion above @xmath309 . similarly , there are no known ways to verify the nullspace property or the restricted isometry property of a given candidate measurement matrix in polynomial time",
    ".    there are several deterministic constructions of sparse measurement matrices  @xcite which , however , would require a slightly sub - optimal number of measurements ( _ i.e. _ , @xmath77 growing super - linearly as a function of @xmath47 for @xmath286 ) .",
    "the benefit of such constructions is that reconstruction can be performed via algorithms that are more efficient than generic convex optimization . to the best of our knowledge",
    ", there are no previously known constructions of deterministic measurement matrices with an optimal number of rows  @xcite .",
    "the best known constructions rely on explicit expander constructions  @xcite , but have slightly sub - optimal parameters  @xcite .",
    "our construction of corollary  [ cor : arora : etal:1 ] seems to be the first optimal deterministic construction .",
    "one important technical innovation that arises from the machinery we develop is that _ girth _ can be used to certify good measurement matrices . since",
    "checking and constructing high - girth graphs is much easier than constructing graphs with high expansion , we can obtain very good deterministic measurement matrices .",
    "for example , we can use gallager s construction of ldpc matrices with logarithmic girth to obtain sparse zero - one measurement matrices with an order - optimal number of measurements under basis pursuit .",
    "the transition from expansion - based arguments to girth - based arguments was achieved for the channel coding problem in  @xcite , then simplified and brought to a new analytical level by arora _",
    "et al . _  in  @xcite , and afterwards generalized in  @xcite .",
    "our connection results extend the applicability of these results to compressed sensing .",
    "we note that corollary  [ cor : arora : etal:1 ] yields a weak bound , _",
    "i.e. _ , the recovery of almost all @xmath0-sparse signals and therefore does not guarantee recovering all @xmath0-sparse signals as the capalbo _",
    "et al . _  @xcite construction ( in conjunction with corollary  [ cor : translation1 ] )",
    "would ensure .",
    "on the other hand , girth - based constructions have constants that are orders of magnitude higher than the ones obtained by random expanders .",
    "since the construction of  @xcite gives constants that are worse than the ones for random expanders , it seems that girth - based measurement matrices have significantly higher provable thresholds of recovery .",
    "finally , we note that following  @xcite , logarithmic girth @xmath310 will yield a probability of failure decaying exponentially in the matrix size @xmath47",
    ". however , even the much smaller girth requirement @xmath311 is sufficient to make the probability of error decay as an inverse polynomial of @xmath47 .    a final remark : chandar  @xcite showed that zero - one measurement matrices can not have an optimal number of measurements if they must satisfy the restricted isometry property for the @xmath106 norm .",
    "note that this does not contradict our work , since , as mentioned earlier on , rip is just a sufficient condition for signal recovery .",
    "we conclude this section with some considerations about dense measurement matrices , highlighting our current understanding that the translation of positive performance guarantees from * cc - lpd *  to * cs - lpd *  displays the following behavior : the denser a measurement matrix is , the weaker the translated performance guarantees are .",
    "[ remark : dense : measurement : matrix:1 ]    consider a randomly generated @xmath75 measurement matrix @xmath74 where every entry is generated i.i.d .  according to the distribution @xmath312 this matrix , after multiplying it by the scalar @xmath313 , has the restricted isometry property ( rip ) with high probability .",
    "( see  @xcite , which proves this property based on results in  @xcite , which in turn proves that this family of matrices has a non - zero threshold . ) on the other hand , one can show that the family of parity - check matrices where every entry is generated i.i.d .  according to the distribution @xmath314 does _",
    "not _ have a non - zero threshold under * cc - lpd *  for the bsc  @xcite .",
    "therefore , we conclude that the connection between * cs - lpd *  and * cc - lpd *  given by lemma  [ lemma : equation : nullspace : to : fc:1:complex : case ] ( an extension of lemma  [ lemma : equation : nullspace : to : fc:1 ] that is discussed in appendix  [ sec : extensions : bridge : lemma:1 ] ) is not tight for dense matrices , in the sense that the performance of * cs - lpd *  for dense measurement matrices can be much better than predicted by the translation of performance results for * cc - lpd *  of the corresponding parity - check matrix .",
    "the aim of this section is to tighten the already close formal relationship between * cc - lpd *  and * cs - lpd *  with the help of ( topological ) graph covers  @xcite .",
    "we will see that the so - called ( blockwise ) graph - cover decoder  @xcite ( see also  @xcite ) , which is equivalent to * cc - lpd *  and which can be used to explain the close relationship between * cc - lpd *  and message - passing iterative decoding algorithms like the min - sum algorithm , can be translated to the * cs - lpd *  setup .    for an introduction to graph covers in general , and the graph - cover decoder in particular , see  @xcite .",
    "figures  [ fig : graph : cover : samples:1 ] and  [ fig : simple : code:1 ] ( taken from  @xcite ) show the main idea behind graph covers .",
    "namely , figure  [ fig : graph : cover : samples:1 ] shows possible graph covers of some ( general ) graph and figure  [ fig : simple : code:1 ] shows possible graph covers of some tanner graph .    note that in this section the compressed sensing setup will be over the complex numbers .",
    "also , the entries of the size-@xmath75 measurement matrix @xmath74 will be allowed to take on any value in @xmath13 , _ i.e. _ , the entries of @xmath74 are not restricted to have absolute value equal to zero or one . moreover , as in section  [ sec : cc : lpd:1 ] , the channel coding problem assumes an arbitrary binary - input output - symmetric memoryless channel , of which the binary - input additive white gaussian noise ( awgn ) channel and the binary symmetric channel ( bsc ) are prominent examples . as before",
    ", @xmath315 will be the sent vector , @xmath316 will be the received vector , and @xmath317 will contain the log - likelihood ratios @xmath318 , @xmath319 .",
    "the rest of this section is organized as follows . in sections  [ sec : reformulations : of : ccmld:1 ] and  [ sec : reformulations : of : cclpd:1 ] we show a variety of reformulations of * cc - mld *  and * cc - lpd * , respectively . in particular , the latter subsection shows reformulations of * cs - lpd *  in terms of graph covers .",
    "switching to compressed sensing , in section  [ sec : reformulations : of : csopt:1 ] we discuss reformulations of * cs - opt *  that allow to see the close relationship of * cc - mld*and * cs - opt*. afterwards , in section  [ sec : reformulations : of : cslpd:1 ] , we present reformulations of * cs - lpd *  which highlight the close connections , and also the differences , between * cc - lpd *  and * cs - lpd*.     +      this subsection discusses several reformulations of * cc - mld * , first for general binary - input output - symmetric memoryless channels , then for the bsc .",
    "we start by repeating two reformulations of * cc - mld *  from section  [ sec : cc : lpd:1 ] .     +   +    towards yet another reformulation of * cc - mld *  that we would like to present in this subsection , it is useful to introduce the hard - decision vector @xmath320 , along with the syndrome vector @xmath76 induced by @xmath320 .",
    "[ def : syndrome : for : general : binary : input : channels:1 ]    let @xmath321 be the hard - decision vector based on the log - likelihood ratio vector @xmath170 , namely let @xmath322 ( if @xmath323 , we set @xmath324 or @xmath325 according to some deterministic or random rule . )",
    "moreover , let @xmath326 be the syndrome induced by @xmath320 .    clearly , if the channel under consideration is a bsc with cross - over probability smaller than @xmath327 then @xmath328 .    with this , we have for any binary - input output - symmetric memoryless channel the following reformulation of * cc - mld *  in terms of @xmath329 .",
    "+   +    clearly , once the error vector estimate @xmath78 is found , the codeword estimate @xmath330 is obtained with the help of the expression @xmath331 .",
    "note that for the special case of a binary - input awgnc , this reformulation can be found , for example , in  @xcite or  ( * ? ? ?",
    "* chapter  10 ) .",
    "[ theorem : mld : reformulation:1 ]    * cc - mld3 * is a reformulation of * cc - mld1*.    see appendix  [ sec : proof : theorem : mld : reformulation:1 ] .    for a bsc we can specialize the above reformulations .",
    "namely , for a bsc with cross - over probability @xmath332 , @xmath333 , we have @xmath334 , @xmath335 , where @xmath336 .",
    "then , with a slight abuse of notation by employing @xmath337 also for vectors over @xmath14 , we obtain the following reformulation .",
    "+   +    moreover , with a slight abuse of notation by employing @xmath338 also for vectors over @xmath14 , @xmath339 can be written as follows .",
    "+   +      we start by repeating the definition of * cc - lpd *  from section  [ sec : cc : lpd:1 ] .",
    "+   +    the aim of this subsection is to discuss various reformulations of * cc - lpd *  in terms of graph covers . in particular , the following reformulation of * cc - lpd*was presented in  @xcite and was called ( blockwise ) graph - cover decoding .",
    "+   +    here the minimization is over all @xmath340 and over all parity - check matrices @xmath341 induced by all possible @xmath63-covers of the tanner graph of @xmath158 .",
    "is obtained by the standard procedure to construct a graph cover  @xcite , and not by the procedure in definition  [ def : measurement : matrix : cover:1 ] ( _ cf .",
    "_  appendix  [ sec : extensions : bridge : lemma:1 ] ) . ]    using the same line of reasoning as in section  [ sec : reformulations : of : ccmld:1 ] , * cc - lpd *  can be rewritten as follows .",
    "+   +    again , the minimization is over all @xmath340 and over all parity - check matrices @xmath341 induced by all possible @xmath63-covers of the tanner graph of @xmath158 .    for the bsc with cross - over probability @xmath332 , @xmath342",
    ", we get , with a slight abuse of notation as in section  [ sec : reformulations : of : ccmld:1 ] , the following specialized results .",
    "+   +     +   +      we start by repeating the definition of * cs - opt *  from section  [ sec : cs : lpd:1 ] .",
    "+   +    clearly , this is formally very similar to * cc - mld5 *  ( bsc ) .    in order to show the tight formal relationship of * cs - opt *  with * cc - mld *  for general binary - input output - symmetric memoryless channels , in particular with respect to the reformulation * cc - mld3 * , we rewrite * cs - opt *  as follows .",
    "+   +      we now come to the main part of this section , namely the reformulation of * cs - lpd *  in terms of graph covers .",
    "we start by repeating the definition of * cs - lpd *  from section  [ sec : cs : lpd:1 ] .",
    "+   +    as shown in the upcoming theorem  [ theorem : cslpd : reformulation:2:vs:1 ] , * cs - lpd *  can be rewritten as follows .",
    "+   +    here the minimization is over all @xmath340 and over all measurement matrices @xmath343 induced by all possible @xmath63-covers of the tanner graph of @xmath74 .",
    "[ theorem : cslpd : reformulation:2:vs:1 ]    * cs - lpd1 * is a reformulation of * cs - lpd*.    see appendix  [ sec : proof : theorem : cslpd : reformulation:2:vs:1 ] .",
    "clearly , * cs - lpd1 * is formally very close to * cc - lpd3 * ( bsc ) , thereby showing that graph covers can be used to exhibit yet another tight formal relationship between * cs - lpd *  and * cc - lpd*.    nevertheless , these graph - cover based reformulations also highlight differences between the relaxation used in the context of channel coding and the relaxation used in the context of compressed sensing .    * when relaxing * cc - mld *  to obtain * cc - lpd * , the cost function remains the same ( call this property @xmath344 ) but the domain is relaxed ( call this property @xmath345 ) . in the graph - cover reformulations of * cc - lpd",
    "* , property @xmath344 is reflected by the fact that the cost function is a straightforward generalization of the cost function for * cc - mld*. property @xmath345 is reflected by the fact that in general there are feasible vectors in graph covers that can not be explained as liftings of ( convex combinations of ) feasible vectors in the base graph and that , for suitable @xmath170-vectors , have strictly lower cost function values than any feasible vector in the base graph . * when relaxing * cs - opt *  to obtain * cs - lpd * , the cost function",
    "is changed ( call this property @xmath346 ) , but the domain remains the same ( call this property @xmath347 ) .",
    "in the graph - cover reformulations of * cs - lpd * , property @xmath346 is reflected by the fact that the cost function is _ not _ a straightforward generalization of the cost function of * cs - opt*. property @xmath347 is reflected by the fact that feasible vectors in graph covers are such that they _ do not _ yield cost function values that are smaller than the cost function value of the best feasible vector in the base graph .",
    "for any real vector @xmath24 we define the zero - infinity operator to be @xmath348 _ i.e. _ , the product of the zero norm @xmath349 of @xmath24 and of the infinity norm @xmath350 of @xmath24 .",
    "note that for any @xmath351 and any real vector @xmath24 it holds that @xmath352 .",
    "based on this operator , in the present section we introduce @xmath4 , and we show , with the help of graph covers , that * cs - lpd *  can not only be seen as a relaxation of * cs - opt *  but also as a relaxation of @xmath4 .",
    "we do this by proposing a relaxation of @xmath4 , called @xmath5 , and by then showing that @xmath5 is equivalent to * cs - lpd*.    moreover , we argue that the solution of * cs - lpd *  is `` closer '' to the solution of @xmath4 than the solution of * cs - lpd *  is to the solution of * cs - opt*. note that similar to * cs - opt * , the problem @xmath4 is in general an intractable optimization problem .",
    "one motivation for looking for different problems whose relaxations equals * cs - lpd *  is to better understand the `` strengths '' and `` weaknesses '' of * cs - lpd*. in particular , if * cs - lpd *  is the relaxation of two different problems ( like * cs - opt *  and @xmath4 ) , but these two problems yield different solutions , then the solution of the relaxed problem will disagree with the solution of at least one of the two problems .",
    "this section is structured as follows .",
    "we start by defining @xmath4 in section  [ sec : def : csoptzeroinf:1 ] .",
    "then , in section  [ sec : geometrical : aspects : of : csoptzeroinf:1 ] , we discuss some geometrical aspects of @xmath4 , in particular with respect to the geometry behind * cs - opt *  and * cs - lpd*. finally , in section  [ sec : relaxation : of : csoptzeroinf:1 ] , we introduce @xmath5 and show its equivalence to * cs - lpd*.      the optimization problem @xmath4 is defined as follows .     +   +    whereas the cost function of * cs - opt * , _ i.e. _ , @xmath353 , measures the sparsity of @xmath78 but not the magnitude of the elements of @xmath78 , the cost function of @xmath4 , _",
    "i.e. _ , @xmath354 , represents a trade - off between measuring the sparsity of @xmath78 and measuring the largest magnitude of the components of @xmath78 . clearly , in the same way that there are many good reasons to look for the vector @xmath78 that minimizes the zero - norm ( among all @xmath78 that satisfy @xmath79 ) , there are also many good reasons to look for the vector @xmath78 that minimizes the zero - infinity operator ( among all @xmath78 that satisfy @xmath355 ) . in particular , the latter is attractive when we are looking for a sparse vector @xmath78 that does not have an imbalance in magnitudes between the largest component and the set of most important components .    with a slight abuse of notation",
    ", we can apply the zero - infinity operator @xmath356 also to vectors over @xmath14 and obtain the following reformulation of * cc - mld *  ( bsc ) .",
    "( note that for any vector @xmath24 over @xmath14 it holds that @xmath357 . )",
    "+   +    this clearly shows that there is a close formal relationship not only between * cc - mld *  ( bsc ) and * cs - opt * , but also between * cc - mld *  ( bsc ) and @xmath4 .",
    "we want to discuss some geometrical aspects of * cs - opt * , @xmath4 , and * cs - lpd*. namely , as is well known , * cs - opt *  can be formulated as finding the smallest @xmath39-norm ball of radius @xmath358 ( _ cf .",
    "_  figure  [ fig : unit : balls:1 ]  ( left ) ) that intersects the set @xmath359 , and in the same spirit , * cs - lpd *  can be formulated as finding the smallest @xmath124-norm ball of radius @xmath358 ( _ cf .",
    "_  figure  [ fig : unit : balls:1 ]  ( right ) ) that intersects with the set @xmath359 . clearly , the fact that * cs - opt *  and * cs - lpd *  can yield different solutions stems from the fact that these balls have different shapes .",
    "of course , the success of * cs - lpd*is a consequence of the fact that , nevertheless , under suitable conditions , the solution given by the @xmath124-norm ball is ( nearly ) the same as the solution given by the @xmath39-norm ball .    in the same vein",
    ", @xmath4 can be formulated as finding the smallest zero - infinity - operator ball of radius @xmath358 ( _ cf . _  figure  [ fig : unit : balls:1 ]  ( middle ) ) that intersects the set @xmath359 . as it can be seen from figure  [",
    "fig : unit : balls:1 ] , the zero - infinity - operator unit ball is closer in shape to the @xmath124-norm unit ball than the @xmath39-norm unit ball is to the @xmath124-norm unit ball .",
    "therefore , we expect that the solution given by * cs - lpd *  is `` closer '' to the solution given by @xmath4 than the solution of * cs - lpd *  is to the solution given by * cs - opt*. in that sense , @xmath4 is at least as justifiably as * cs - opt *  a difficult optimization problem whose solution is approximated by * cs - lpd*.      in this subsection we introduce @xmath5 as a relaxation of @xmath4 ; the main result will be that @xmath5 equals * cs - lpd*. our results will be formulated in terms of graph covers , we therefore use the graph - cover related notation that was introduced in section  [ sec : reformulations:1 ] , along with the mapping @xmath360 that was defined in section  [ sec : notation:1 ] .    in order to motivate the formulation of @xmath5 , we first present a reformulation of  ( bsc ) .",
    "namely , * cc - lpd3 *  ( bsc ) or * cc - lpd4 *  ( bsc ) from section  [ sec : reformulations : of : cclpd:1 ] can be rewritten as follows .     +   +    then , because for any vector @xmath361 it holds that @xmath362 if and only if @xmath363 , * cc - lpd5 * ( bsc ) can also be written as follows .",
    "+   +    the transition that leads from * cc - mld *  to its relaxation * cc - lpd6 *  ( bsc ) inspires a relaxation of @xmath4 as follows .",
    "+   +    here the minimization is over all @xmath340 and over all measurement matrices @xmath343 induced by all possible @xmath63-covers of the tanner graph of @xmath74 .",
    "note that , in contrast to * cc - lpd6 *  ( bsc ) , in general the optimal solution @xmath364 of @xmath5 does _ not _ satisfy @xmath363 .    towards establishing the equivalence of @xmath5 and * cs - lpd",
    "* , the following simple lemma will prove to be useful .",
    "[ lemma : onenorm : zeroinfoperator : relationship:1 ]    for any real vector @xmath24 it holds that @xmath365 with equality if and only if all non - zero components of @xmath24 have the same absolute value .",
    "the proof of this lemma is straightforward .    [",
    "theorem : cslpd : and : csoptzeroinfty : connection:1 ]    let @xmath74 be a measurement matrix over the reals with entries equal to zero , one , and minus one . for syndrome vectors",
    "@xmath76 that have only rational components , * cs - lpd *  and @xmath5 are equivalent in the sense that there is an optimal @xmath78 in * cs - lpd *  and an optimal @xmath366 in @xmath5 such that @xmath367",
    ".    see appendix  [ sec : proof : theorem : cslpd : and : csoptzeroinfty : connection:1 ] .",
    "in this paper we have established a mathematical connection between channel coding and compressed sensing lp relaxations . the key observation , in its simplest version , was that points in the nullspace of a zero - one matrix ( considered over the reals ) can be mapped to points in the fundamental cone of the same matrix ( considered as the parity - check matrix of a code over @xmath14 ) .",
    "this allowed us to show , among other results , that parity - check matrices of `` good '' channel codes can be used as provably `` good '' measurement matrices under basis pursuit .",
    "let us comment on a variety of topics .",
    "* in addition to * cs - lpd * , a number of combinatorial algorithms ( _ e.g. _  @xcite ) have been proposed for compressed sensing problems , with the benefit of faster decoding complexity and comparable performance to * cs - lpd*. it would be interesting to investigate if the connection of sparse recovery problems to channel coding extends in a similar manner for these decoders .",
    "one example of such a clear connection is the bit - flipping algorithm of sipser and spielman  @xcite and the corresponding algorithm for compressed sensing by xu and hassibi  @xcite .",
    "channel - coding - inspired message - passing decoders for compressed sensing problems were also recently discussed in  @xcite . *",
    "an interesting research direction is to use optimized ldpc matrices ( see , _",
    "@xcite ) to create measurement matrices .",
    "there is a large body of channel coding work that could be transferable to the measurement matrix design problem .",
    "+ in this context , an important theoretical question is related to being able to certify in polynomial time that a given measurement matrix has `` good '' performance . to the best of our knowledge ,",
    "our results form the first known case where girth , an efficiently checkable property , can be used as a certificate of goodness of a measurement matrix .",
    "it is possible that girth can be used to establish a success witness for * cs - lpd *  directly , and this would be an interesting direction for future research .",
    "* one important research direction in compressed sensing involves dealing with noisy measurements .",
    "this problem can still be addressed with @xmath124 minimization ( see , _",
    "e.g. _ , @xcite ) and also with less complex signal reconstruction algorithms ( see , _",
    "e.g. _ , @xcite )",
    ". it would be very interesting to investigate if our nullspace connections can be extended to a coding theory result equivalent to noisy compressed sensing . * beyond channel coding problems ,",
    "the lp relaxation of  @xcite is a special case of a relaxation of the marginal polytope for general graphical models .",
    "one very interesting research direction is to explore if the connection we have established between * cs - lpd *  and * cc - lpd *  is also just a special case of a more general theory . * we have also discussed various reformulations of the optimization problems under investigation .",
    "this leads to a strengthening of the ties between some of the optimization problems .",
    "moreover , we have introduced the zero - infinity operator optimization problem @xmath4 , an optimization problem with the property that the solution of * cs - lpd *  can be considered to be at least as good an approximation of the solution of @xmath4 as the solution of * cs - lpd *  is an approximation of the solution of * cs - opt*. we leave it as an open question if the results and observations of section  [ sec : minimizing : zero : infty : norm:1 ] can be generalized for more general matrices or specific families of signals ( like non - negative sparse signals as in  @xcite ) .",
    "we would like to thank babak hassibi and waheed bajwa for stimulating discussions with respect to the topic of this paper .",
    "moreover , we greatly appreciate the reviewers comments that lead to an improved presentation of the results .",
    "suppose that @xmath74 has the claimed nullspace property .",
    "since @xmath81 and @xmath368 , it easily follows that @xmath369 is in the nullspace of @xmath74 .",
    "so , @xmath370 where step  @xmath371 follows from the fact that the solution of * cs - lpd*satisfies @xmath372 , where step  @xmath373 follows from applying the triangle inequality property of the @xmath124-norm twice , and where step  @xmath374 follows from @xmath375 here , step  @xmath376 is a consequence of @xmath377 where step  @xmath378 follows from applying twice the fact that @xmath379 and the assumption that @xmath380 . subtracting the term @xmath381 on both sides of  , and solving for @xmath382 yields the promised result .",
    "without loss of generality , we can assume that the all - zero codeword was transmitted .",
    "let @xmath383 be the log - likelihood ratio associated with a received @xmath384 , and let @xmath385 be the log - likelihood ratio associated with a received @xmath123 .",
    "therefore , @xmath386 if @xmath387 and @xmath388 if @xmath389 .",
    "then it follows from the assumptions in the lemma statement that for any @xmath202 it holds that @xmath390 where step  @xmath371 follows from the fact that @xmath391 for all @xmath164 , and where step  @xmath373 follows from  .",
    "therefore , under * cc - lpd *  the all - zero codeword has the lowest cost function value when compared to all non - zero pseudo - codewords in the fundamental cone , and therefore also compared to all non - zero pseudo - codewords in the fundamental polytope .",
    "* case 1 : * let @xmath392 . the proof is by contradiction : assume that @xmath393 .",
    "this statement is clearly equivalent to the statement that @xmath394 , which is equivalent to the statement that @xmath395 . in terms of the notation in definition  [ def : pseudo : weights:1 ]",
    ", this means that @xmath396 where at step  @xmath371 we have used the fact that @xmath397 is a ( strictly ) non - decreasing function and where at step  @xmath373 we have used the fact that the slope of @xmath397 ( over the domain where @xmath397 is defined ) is at least @xmath398 . the obtained inequality , however , is a contradiction to the assumption that @xmath399 .",
    "* case 2 : * let @xmath400 .",
    "the proof is by contradiction : assume that @xmath393 .",
    "then , using the definition of @xmath210 based on @xmath204 ( _ cf . _",
    "section  [ sec : pseudo : weight : definitions:1 ] ) , we obtain @xmath401 if @xmath223 is an even integer , then the above line of inequalities shows that @xmath402 , which is a contradiction to the assumption that @xmath403 . if @xmath223 is an odd integer , then the above line of inequalities shows that @xmath404 , which again is a contradiction to the assumption that @xmath403 .",
    "the aim of this appendix is to extend lemma  [ lemma : equation : nullspace : to : fc:1 ] ( _ cf . _",
    "section  [ sec : bridge:1 ] ) to measurement matrices beyond zero - one matrices . in that vein",
    "we will present three generalizations in lemmas  [ lemma : equation : nullspace : to : fc:1:complex : case ] , [ lemma : bridge : for : lifted : matrices:1 ] , and  [ lemma : equation : nullspace : to : fc:1:multiple : complex : vector : case ] .",
    "note that the setup in this appendix will be slightly more general than the compressed sensing setup in section  [ sec : cs : lpd:1 ] ( and in most of the rest of this paper ) .",
    "in particular , we allow matrices and vectors to be over @xmath13 , and not just over  @xmath10 .    we will need some additional notation .",
    "namely , similarly to the way that we have extended the absolute value operator @xmath228 from scalars to vectors at the beginning of section  [ sec : bridge:1 ] , we will now extend its use from scalars to matrices .    moreover , we let @xmath405 be an arbitrary norm for the complex numbers . as such ,",
    "@xmath405 satisfies for any @xmath406 the triangle inequality @xmath407 and the equality @xmath408 . in the same way the absolute value operator @xmath409 was extended from scalars to vectors and matrices ,",
    "we extend the norm operator @xmath405 from scalars to vectors and matrices .",
    "we let @xmath410 be an arbitrary vector norm for complex vectors that reduces to @xmath405 for vectors with one component .",
    "as such , @xmath410 satisfies for any @xmath351 and any complex vectors @xmath24 and @xmath31 with the same number of components the triangle inequality @xmath411 and the equality @xmath412 .    we are now ready to discuss our first extension of lemma  [ lemma : equation : nullspace : to : fc:1 ] , which generalizes the setup of that lemma from real measurement matrices where every entry is equal to either zero or one to complex measurement matrices where the absolute value of every entry is equal to either zero or one .",
    "note that the upcoming lemma also generalizes the mapping that is applied to the vectors in the nullspace of the measurement matrix .",
    "[ lemma : equation : nullspace : to : fc:1:complex : case ]    let @xmath413 be a measurement matrix over @xmath13 such that @xmath414 for all @xmath415 , and let @xmath405 be an arbitrary norm on @xmath13",
    ". then @xmath416    _ remark : _ note that @xmath417 .",
    "let @xmath418 .",
    "in order to show that such a vector @xmath204 is indeed in the fundamental cone of @xmath419 , we need to verify   and  . the way @xmath204 is defined , it is clear that it satisfies  .",
    "therefore , let us focus on the proof that @xmath204 satisfies  .",
    "namely , from @xmath420 it follows that for all @xmath237 , @xmath238 . for all @xmath237 and",
    "all @xmath241 this implies that @xmath421 showing that @xmath204 indeed satisfies  .",
    "the measurement matrix @xmath422 satisfies @xmath423 and so lemma  [ lemma : equation : nullspace : to : fc:1:complex : case ] is applicable .",
    "an example of a vector in @xmath424 is @xmath425 choosing @xmath426 , we obtain @xmath427    the second extension of lemma  [ lemma : equation : nullspace : to : fc:1 ] generalizes that lemma to hold also for complex measurement matrices where the absolute value of every entry is an integer . in order to present this lemma ,",
    "we need the following definition , which is subsequently illustrated by example  [ example : hcs : cover:1 ] .",
    "[ def : measurement : matrix : cover:1 ]    let @xmath428 be a measurement matrix over @xmath13 such that @xmath429 for all @xmath415 , and let @xmath340 be such that @xmath430 .",
    "we define an @xmath63-fold cover @xmath343 of @xmath74 as follows : for @xmath431 , if the scalar @xmath432 is non - zero then it is replaced by a matrix , namely @xmath433 times the sum of @xmath434 arbitrary @xmath435 permutation matrices with non - overlapping support . however , if @xmath436 then the scalar @xmath432 is replaced by an all - zero matrix of size @xmath435 .",
    "note that all entries of the matrix @xmath343 in definition  [ def : measurement : matrix : cover:1 ] have absolute value equal to either zero or one .",
    "[ example : hcs : cover:1 ]    let @xmath437 clearly @xmath438 and so , choosing @xmath439 and @xmath440 we obtain a matrix described by the procedure of definition  [ def : measurement : matrix : cover:1 ] .",
    "[ lemma : bridge : for : lifted : matrices:1 ]    let @xmath413 be a measurement matrix over @xmath13 such that @xmath429 for all @xmath415 .",
    "let @xmath340 be such that @xmath430 , and let @xmath343 be a matrix obtained by the procedure in definition  [ def : measurement : matrix : cover:1 ] .",
    "moreover , let @xmath405 be an arbitrary norm on @xmath13 .",
    "then @xmath441 additionally , with respect to the first implication sign we have the following converse : for any @xmath442 we have @xmath443    let @xmath444 .",
    "note that by the construction in definition  [ def : measurement : matrix : cover:1 ] , it holds that @xmath445 }        \\tilde h_{(j , m'),(i , m ) }        & = h_{j , i }        \\ \\ \\text{for any $ ( j , i , m ) \\in { { \\mathcal{j}}}\\!\\times\\ ! { { \\mathcal{i}}}\\!\\times\\ ! [ m]$ } , \\\\      \\sum_{m \\in { \\mathcal{[}}m ] }        \\tilde h_{(j , m'),(i , m ) }        & = h_{j , i }        \\ \\ \\text{for any $ ( j , m',i ) \\in { { \\mathcal{j}}}\\!\\times\\ ! [ m ] \\!\\times\\ ! { { \\mathcal{i}}}$}.    \\end{aligned}\\ ] ] let @xmath446 .",
    "then , for every @xmath447 $ ] we have @xmath448 } \\!\\!\\",
    "!        \\tilde h_{(j , m'),(i , m ) } \\nu^{\\uparrow m}_{(i , m ) }         =   \\sum_{(i , m ) \\in { { \\mathcal{i}}}\\times { \\mathcal{[}}m ] } \\!\\!\\ !",
    "\\tilde h_{(j , m'),(i , m ) }             \\nu_i \\\\        & \\quad\\quad         =   \\sum_{i \\in { { \\mathcal{i } } } }             \\nu_i             \\sum_{m \\in { \\mathcal{[}}m ] }             \\tilde h_{(j , m'),(i , m ) }         =   \\sum_{i \\in { { \\mathcal{i } } } }             \\nu_i             h_{j , i }         = 0 ,    \\end{aligned}\\ ] ] where the last equality follows from the assumption that @xmath420 .",
    "therefore @xmath449 .",
    "because @xmath450 for all @xmath451 \\times { { \\mathcal{i}}}\\times [ m]$ ] , we can then apply lemma  [ lemma : equation : nullspace : to : fc:1:complex : case ] to conclude that @xmath452 .",
    "now , in order to prove the last part of the lemma , assume that @xmath453 and define @xmath454 .",
    "then for every @xmath237 we have @xmath455 }               \\tilde \\nu_{(i , m ) } \\\\        & = \\frac{1}{m }           \\sum_{i \\in { { \\mathcal{i } } } }           \\sum_{m \\in [ m ] }             h_{j , i }             \\cdot               \\tilde \\nu_{(i , m ) } \\\\        & = \\frac{1}{m }           \\sum_{i \\in { { \\mathcal{i } } } }           \\sum_{m \\in [ m ] }           \\sum_{m ' \\in [ m ] }             \\tilde h_{(j , m'),(i , m ) }             \\cdot               \\tilde \\nu_{(i , m ) } \\\\        & = \\frac{1}{m }           \\sum_{m ' \\in [ m ] }           \\left (             \\sum_{i \\in { { \\mathcal{i } } } }             \\sum_{m \\in [ m ] }               \\tilde h_{(j , m'),(i , m ) }               \\cdot                 \\tilde \\nu_{(i , m ) }           \\right ) \\\\        & = 0 ,    \\end{aligned}\\ ] ] where the last equality follows from the assumption that @xmath453 , _",
    "i.e. _ , for every @xmath447 $ ] the expression in parentheses equals zero .",
    "therefore , @xmath456 .",
    "consider the measurement matrix @xmath74 of example  [ example : hcs : cover:1 ] .",
    "a possible vector in @xmath424 is given by @xmath457 applying lemma  [ lemma : bridge : for : lifted : matrices:1 ] with @xmath439 and @xmath426 , we obtain @xmath458 where @xmath459 , and where @xmath343 can be chosen as in example  [ example : hcs : cover:1 ] .    our third extension of lemma  [ lemma : equation : nullspace : to : fc:1 ] generalizes the mapping that is applied to the vectors in the nullspace of the measurement matrix .",
    "[ lemma : equation : nullspace : to : fc:1:multiple : complex : vector : case ]    let @xmath413 be a measurement matrix over @xmath13 such that @xmath414 for all @xmath415 .",
    "let @xmath460 , let @xmath410 be an arbitrary norm for complex vectors , and let @xmath461}$ ] be a collection of vectors with @xmath47 components .",
    "then @xmath462 where @xmath195 is defined such that for all @xmath463 , @xmath464    the proof is very similar to the proof of lemma  [ lemma : equation : nullspace : to : fc:1:complex : case ] .",
    "namely , in order to show that @xmath204 is indeed in the fundamental cone of @xmath419 , we need to verify   and  . the way @xmath204 is defined , it is clear that it satisfies  .",
    "therefore , let us focus on the proof that @xmath204 satisfies  .",
    "namely , from @xmath465 , @xmath466 $ ] , it follows that @xmath467 , @xmath237 , @xmath468 $ ] . for all @xmath237 and",
    "all @xmath241 this implies that @xmath469 showing that @xmath204 indeed satisfies  .",
    "[ cor : lemma : equation : nullspace : to : fc:1:multiple : complex : vector : case ]    consider the setup of lemma  [ lemma : equation : nullspace : to : fc:1:multiple : complex : vector : case ] .",
    "let @xmath460 , and select @xmath470 arbitrary scalars @xmath471 , @xmath466 $ ] , and @xmath470 arbitrary vectors @xmath472 , @xmath466 $ ] .    * for @xmath473 we have @xmath474 }          \\alpha^{(\\ell ) } \\ ,          { \\lvert { \\boldsymbol{\\nu}}^{(\\ell ) } \\rvert }          & \\in { { \\mathcal{k}}}\\big ( { \\lvert { { { { \\bm{h}}}}_{\\mathrm{cs}}}\\rvert } \\big ) .",
    "\\end{aligned}\\ ] ] * for @xmath475 we have @xmath476 }            ( \\alpha^{(\\ell)})^2 \\ ,            { \\lvert { \\boldsymbol{\\nu}}^{(\\ell ) } \\rvert}^2        }          & \\in { { \\mathcal{k}}}\\big ( { \\lvert { { { { \\bm{h}}}}_{\\mathrm{cs}}}\\rvert } \\big ) ,      \\end{aligned}\\ ] ] where the square root and the square of a vector are understood component - wise",
    ".    these are straightforward consequences of applying lemma  [ lemma : equation : nullspace : to : fc:1:multiple : complex : vector : case ] to @xmath477}$ ] .    because @xmath478 is a convex cone , the first statement in corollary  [ cor : lemma : equation : nullspace : to : fc:1:multiple : complex : vector : case ] can also be proven by combining @xmath479 , @xmath466 $ ] , with the fact that any conic combination of vectors in @xmath478 is a vector in @xmath478 . in that respect , the second statement of corollary  [ cor : lemma : equation : nullspace : to :",
    "fc:1:multiple : complex : vector : case ] is noteworthy in the sense that although @xmath470 vectors in @xmath480 are combined in a `` non - conic '' way , we nevertheless obtain a vector in @xmath478 .",
    "( of course , for the latter to work it is important that these @xmath470 vectors are not arbitrary vectors in @xmath478 but that they are derived from vectors in the @xmath13-nullspace of @xmath74 . )",
    "we conclude this appendix with two remarks .",
    "first , it is clear that lemma  [ lemma : equation : nullspace : to : fc:1:multiple : complex : vector : case ] can be extended in the same way as lemma  [ lemma : bridge : for : lifted : matrices:1 ] extends lemma  [ lemma : equation : nullspace : to : fc:1:complex : case ] .",
    "second , although most of section  [ sec : translation:1 ] is devoted to using lemma  [ lemma : equation : nullspace : to : fc:1 ] for translating `` positive results '' about * cc - lpd *  to `` positive results '' about * cs - lpd *  , it is clear that lemmas  [ lemma : equation : nullspace : to : fc:1:complex : case ] , [ lemma : bridge : for : lifted : matrices:1 ] , and  [ lemma : equation : nullspace : to : fc:1:multiple : complex : vector : case ] can equally well be the basis for translating results from * cc - lpd *  to * cs - lpd*.",
    "by definition , @xmath80 is the original signal . since @xmath481 and @xmath368",
    ", it easily follows that @xmath369 is in the nullspace of @xmath74 .",
    "so , @xmath482 where step  @xmath371 follows from the fact that the solution of * cs - lpd*satisfies @xmath372 and where step  @xmath373 follows from applying the triangle inequality property of the @xmath124-norm twice . moreover , step  @xmath374 follows from @xmath483 where step  @xmath376 follows from the assumption that @xmath484 holds for all @xmath485 , _",
    "i.e. _ , that @xmath486 holds for all @xmath487 , where step  @xmath378 follows from the inequality @xmath488 that holds for any real vector @xmath24 with @xmath0 components , and where step  @xmath489 follows from the inequality @xmath490 that holds for any real vector @xmath24 whose set of coordinate indices includes @xmath491 . subtracting the term @xmath381 on both sides of   , and solving for @xmath492",
    ", we obtain the claim .",
    "by definition , @xmath80 is the original signal . since @xmath81 and @xmath368",
    ", it easily follows that @xmath369 is in the nullspace of @xmath74 .",
    "so , @xmath493 where step  @xmath371 follows from the same line of reasoning as in going from   to  , and where step  @xmath373 follows from @xmath494 where step  @xmath374 follows from the assumption that @xmath495 holds for all @xmath485 , _",
    "i.e. _ , @xmath496 holds for all @xmath487 , where step @xmath376 follows from the inequality @xmath497 that holds for any real vector @xmath24 with @xmath0 components , and where step  @xmath378 follows the inequality @xmath498 that holds for any real vector @xmath24 whose set of coordinate indices includes @xmath491 .",
    "subtracting the term @xmath381 on both sides of   , and solving for @xmath499 we obtain the claim .",
    "in a first step , we discuss the reformulation of the cost function .",
    "namely , for arbitrary @xmath500 , let @xmath501 , _",
    "i.e. _ , @xmath502 for all @xmath503 .",
    "then @xmath504 where at step @xmath371 we used the fact that for @xmath505 , the result of @xmath506 can be written over the reals as @xmath507 , and at step @xmath373 we used the fact that for all @xmath335 , @xmath508 . notice that the first sum in the last line of   is only a function of @xmath509 , hence minimizing @xmath510 over @xmath330 is equivalent to minimizing @xmath511 over @xmath78 .    in a second step ,",
    "we discuss the reformulation of the constraint .",
    "namely , for arbitrary @xmath500 , and corresponding @xmath512 , we have @xmath513 .",
    "because for @xmath514 the measurement matrix @xmath343 equals the measurement matrix @xmath74 , it is clear that any feasible vector of * cs - lpd *  yields a feasible vector of * cs - lpd1*.    therefore , let us show that for @xmath515 no feasible vector of * cs - lpd1 * yields a smaller cost function value than the cost function value of the best feasible vector in the base tanner graph . to that end , we demonstrate that for any @xmath340 , any @xmath63-cover based @xmath343 , and any @xmath366 with @xmath516 , the cost function value of @xmath366 is never smaller than the cost function value of the feasible vector in the base tanner graph given by the projection @xmath517 .",
    "indeed , the cost function value of @xmath517 is @xmath518}\\tilde e'_{i , m }            \\right\\rvert }       { \\leqslant}\\sum_{i \\in { { \\mathcal{i } } } }            \\frac{1}{m }            \\sum_{m \\in [ m ] }              { \\lvert \\tilde e'_{i , m } \\rvert } \\\\      & = \\frac{1}{m }         \\sum_{i \\in { { \\mathcal{i } } } }           \\sum_{m \\in [ m ] }             { \\lvert \\tilde e'_{i , m } \\rvert }       = \\frac{1}{m }         \\cdot         { \\lvert { { { { \\bm{\\tilde e } } } } } ' \\rvert_1},\\end{aligned}\\ ] ] _ i.e. _ , it is never larger than the cost function value of @xmath366 .",
    "moreover , since @xmath516 implies that @xmath519 , we have proven the claim that @xmath520 is a feasible vector in the base tanner graph .",
    "the proof has two parts .",
    "first we show that the minimal cost function value of @xmath5 is never smaller than the minimal cost function value of * cs - lpd*. second , we show that for any vector that minimizes the cost function of @xmath6 there is a graph cover and a configuration therein whose zero - infinity operator equals the minimal cost function value of * cs - lpd*.    we prove the first part .",
    "let @xmath78 minimize @xmath521 over all @xmath78 such that @xmath79 .",
    "for any @xmath340 , any @xmath343 whose tanner graph is an @xmath63-cover of the tanner graph of @xmath74 , and any @xmath522 with @xmath523 and @xmath362 , it holds that @xmath524 where step @xmath371 follows from lemma  [ lemma : onenorm : zeroinfoperator : relationship:1 ] , where step @xmath373 uses the same line of reasoning as the proof of theorem  [ theorem : cslpd : reformulation:2:vs:1 ] , and where step @xmath374 follows from the easily verified fact that @xmath525 , along with the definition of @xmath78 . because @xmath522 was arbitrary ( subject to @xmath523 and @xmath526 ) , this observation concludes the first part of the proof .",
    "we now prove the second part .",
    "again , let @xmath78 minimize @xmath521 over all @xmath78 such that @xmath79 .",
    "once * cs - lpd *  is rewritten as a linear program ( with the help of suitable auxiliary variables ) , we see that the coefficients that appear in this linear program are all rationals . using cramr s rule for determinants , it follows that the set of feasible points of this linear program is a polyhedral set whose vertices are all vectors with rational entries .",
    "therefore , if @xmath78 is unique then @xmath78 is a vector with rational entries .",
    "if @xmath78 is not unique then there is at least one vector @xmath78 with rational entries that minimizes the cost function of * cs - lpd*. let @xmath78 be such a vector .    before continuing ,",
    "let us simplify the notation slightly .",
    "namely , we rearrange the constraint @xmath79 in @xmath6 so that it reads @xmath527 and then we replace   by @xmath528 this is done by redefining @xmath74 to stand for @xmath529 , and redefining @xmath78 to stand for @xmath530 .",
    "note that the redefined @xmath74 contains zeros , ones , or minus ones .",
    "similarly , we rearrange the constraint @xmath531 in @xmath5 so that it reads @xmath532 and then we replace   by @xmath533 this is done by redefining @xmath343 to stand for @xmath534 , and redefining @xmath366 to stand for @xmath535 .",
    "note that the redefined @xmath343 contains only zeros , ones , or minus ones , and that the tanner graph representing the redefined @xmath343 is a valid @xmath63-fold cover of the tanner graph representing the redefined @xmath74 .",
    "we will now exhibit a suitable @xmath63-fold cover and a configuration @xmath366 therein such that @xmath536 and such that for some @xmath537 the vector @xmath366 will satisfy @xmath538 .",
    "\\label{eq : csrelzeroinf : graph : cover : assignment:1}\\end{aligned}\\ ] ] then for such a vector the following holds @xmath539 where step @xmath371 follows from the fact that the equality condition in lemma  [ lemma : onenorm : zeroinfoperator : relationship:1 ] is satisfied , step @xmath373 follows from the fact that for every @xmath335 , all @xmath540 , \\ , \\tilde e'_{(i , m ) } \\neq 0}$ ] have the same sign , and step @xmath374 follows from @xmath536 .    towards constructing such a graph cover and a vector @xmath366 , we make the following observations .",
    "namely , fix some @xmath541 and some @xmath542 , @xmath543 $ ] , and consider the hyperplane @xmath544 } h_i a_i = 0         \\right\\}.\\end{aligned}\\ ] ] let @xmath545 be a vector with all its coordinates satisfying @xmath546 , @xmath543 $ ] .",
    "let @xmath547 be the set @xmath548 & \\text { if } & a^{*}_i > 0 \\\\              a_i = 0         & \\text { if } & a^{*}_i = 0 \\\\              a_i \\in [ -1,0 ] & \\text { if } & a^{*}_i < 0           \\end{array }         \\right\\},\\end{aligned}\\ ] ] which is a box around @xmath549 whose vertices have only integer coordinates .",
    "consider now the set @xmath550 , and let @xmath551 be the set of vertices of @xmath552 .",
    "the set @xmath552 is a polytope and , interestingly , it can be verified that the set of vertices of @xmath552 is a subset of the set of vertices of @xmath547 , _",
    "i.e. _ , all the points in @xmath551 have integer coordinates . because @xmath553 , this vector can be written as a convex combination of the vertices of @xmath552 , _",
    "i.e. _ , there are non - negative real numbers @xmath554 with @xmath555 such that @xmath556 .",
    "note that for all @xmath543 $ ] the following holds : if @xmath557 then @xmath558 for all @xmath559 , if @xmath560 then @xmath561 for all @xmath562 , and if @xmath563 then @xmath564 for all @xmath559 .",
    "we now define @xmath565 and apply the above observations to our setup , in particular to the vector @xmath566 , whose coordinates are rational numbers lying between @xmath567 and @xmath568 inclusive .",
    "namely , for every @xmath237 , we have @xmath569 with @xmath570 , @xmath571 , and so there is a set @xmath572 and non - negative rational numbers @xmath573 with @xmath574 , such that @xmath575 holds , where @xmath576 is the vector @xmath78 restricted to the coordinates indexed by the set @xmath577 .",
    "note that the set @xmath572 is such that for all @xmath241 the following holds : if @xmath578 then @xmath579 for all @xmath580 , if @xmath581 then @xmath582 for all @xmath583 , and if @xmath584 then @xmath564 for all @xmath585 .",
    "we are now ready to construct the promised @xmath63-fold cover of the base tanner graph and the valid configuration @xmath366 .",
    "we choose @xmath591 ( clearly , @xmath340 ) , and so the constructed @xmath366 will need to have the properties shown in   with @xmath592 . without going into the details , the @xmath63-fold cover with valid configuration @xmath366 can be obtained with the help of the above @xmath593 values by using a construction that is very similar to the explicit graph cover construction in  ( * ? ? ?",
    "* appendix  a.1 ) .",
    "for example , for every @xmath335 with @xmath578 we set @xmath594 of the values in @xmath595}$ ] equal to @xmath596 , and we set @xmath597 of the values in @xmath598}$ ] equal to @xmath384 , etc .. similarly , for every @xmath237 and @xmath590 we set the local configuration of @xmath599 out of the @xmath63 copies of the @xmath184-th check node equal to @xmath600 . finally , the edges between the variable and the constraint nodes of the @xmath63-fold cover of the base tanner graph are suitably defined .",
    "( note that the definition of the matrix in   implies that the edge connections in the part of the graph cover corresponding to the right - hand side of the matrix have already been pre - selected .",
    "however , this is not a problem because the variable nodes associated with this part of the matrix have degree one and because the above - mentioned constraint node assignments can always be chosen suitably . )",
    "a.  g. dimakis and p.  o. vontobel , `` lp decoding meets lp decoding : a connection between channel coding and compressed sensing , '' in _ proc .",
    "47th allerton conf .  on communications , control , and computing _ ,",
    "allerton house , monticello , il , usa , sep .",
    "30oct .  2 2009 .",
    "a.  g. dimakis , r.  smarandache , and p.  o. vontobel , `` channel coding lp decoding and compressed sensing lp decoding : further connections , '' in _ proc .",
    "2010 intern .",
    "zurich seminar on communications _ , zurich , switzerland , mar .",
    "35 2010 .",
    "j.  feldman , `` decoding error - correcting codes via linear programming , '' ph.d .",
    "dissertation , dept .  of electrical engineering and computer science ,",
    "massachusetts institute of technology , cambridge , ma , 2003 .",
    "j.  feldman , t.  malkin , r.  a. servedio , c.  stein , and m.  j. wainwright , `` lp decoding corrects a constant fraction of errors , '' in _ proc .",
    "theory _ , chicago , il , usa , june 27july 2 2004 , p.  68 .",
    "r.  berinde , a.  gilbert , p.  indyk , h.  karloff , and m.  strauss , `` combining geometry and combinatorics : a unified approach to sparse signal recovery , '' in _ proc .",
    "46th allerton conf .  on communications , control , and computing _ ,",
    "allerton house , monticello , il , usa , sept .",
    "2326 2008 .",
    "w.  xu and b.  hassibi , `` compressed sensing over the grassmann manifold : a unified analytical framework , '' in _ proc .",
    "46th allerton conf .  on communications , control , and computing _",
    ", allerton house , monticello , il , usa , sept .",
    "2326 2008 .",
    "m.  stojnic , w.  xu , and b.  hassibi , `` compressed sensing ",
    "probabilistic analysis of a null - space characterization , '' in _ proc .",
    "ieee intern .",
    "acoustics , speech and signal processing _",
    ", las vegas , nv , usa , mar .",
    "31apr .  4 2008 , pp . 33773380",
    ".                      g.  d. forney , jr .",
    ", r.  koetter , f.  r. kschischang , and a.  reznik , `` on the effective weights of pseudocodewords for codes defined on graphs with cycles , '' in _ codes , systems , and graphical models ( minneapolis , mn , 1999 ) _ , ser .",
    "i m a vol .",
    "b.  marcus and j.  rosenthal , eds.1em plus 0.5em minus 0.4emspringer verlag , new york , inc .",
    ", 2001 , vol .",
    "101112 .",
    "a.  khajehnejad , a.  s. tehrani , a.  g. dimakis , and b.  hassibi , `` explicit matrices for sparse approximation , '' in _ proc .",
    "ieee int .",
    "inf . theory _ , st .  petersburg , russia , jul .  31aug .  5 2011 , pp .",
    "469473 .",
    "f.  zhang and h.  d. pfister , `` on the iterative decoding of high rate ldpc codes with applications in compressed sensing , '' _ submitted , available online under _ ` http://arxiv.org/abs/0903.2232`_ _ , mar .",
    "y.  lu , a.  montanari , and b.  prabhakar , `` counter braids : asymptotic optimality of the message passing decoding algorithm , '' in _ proc .",
    "46th allerton conf .  on communications , control , and computing _ ,",
    "allerton house , monticello , il , usa , sept .",
    "2326 2008 .",
    "o. vontobel , `` a factor - graph - based random walk , and its relevance for lp decoding analysis and bethe entropy characterization , '' in _ proc .",
    "information theory and applications workshop _ , uc san diego , la jolla , ca , usa , jan .",
    "31feb .  5 2010 .",
    "m.  capalbo , o.  reingold , s.  vadhan , and a.  wigderson , `` randomness conductors and constant - degree lossless expanders , '' in _ proc .",
    "34th annual acm symposium on theory of computing _",
    ", montral , canada , may  1921 2002 .",
    "v.  guruswami , c.  umans , and s.  p. vadhan , `` unbalanced expanders and randomness extractors from parvaresh  vardy codes , '' in _ proc .",
    "ieee conf .  on computational complexity _ , san diego , ca , usa , jun .  1216 2007 , pp .",
    "96108 .",
    "r.  koetter and p.  o. vontobel , `` on the block error probability of lp decoding of ldpc codes , '' in _ proc .",
    "inaugural workshop of the center for information theory and applications _ , uc san diego , la jolla , ca , usa , feb .",
    "610 2006 .",
    "o. vontobel , `` counting in graph covers : a combinatorial characterization of the bethe entropy function , '' _ submitted to ieee trans .",
    "theory , available online under _ ` http://arxiv.org/abs/1012.0065`_ _",
    ", nov . 2010 .",
    "v.  guruswami , j.  lee , and a.  wigderson , `` euclidean sections with sublinear randomness and error - correction over the reals , '' in _ proc .",
    "12th intern .",
    "workshop on randomization and computation _ , cambridge , ma , usa , aug .  2527 2008 .      r.  calderbank , s.  howard , and s.  jafarpour , `` construction of a large class of deterministic sensing matrices that satisfy a statistical isometry property , '' _ ieee j.  sel",
    ".  topics in sig .",
    "_ , vol .  4 , no .  2 ,",
    "pp . 358374 , apr ."
  ],
  "abstract_text": [
    "<S> we present a mathematical connection between channel coding and compressed sensing . </S>",
    "<S> in particular , we link , on the one hand , _ channel coding linear programming decoding ( cc - lpd ) _ , which is a well - known relaxation of maximum - likelihood channel decoding for binary linear codes , and , on the other hand , _ compressed sensing linear programming decoding ( cs - lpd ) _ , also known as basis pursuit , which is a widely used linear programming relaxation for the problem of finding the sparsest solution of an under - determined system of linear equations . more specifically </S>",
    "<S> , we establish a tight connection between cs - lpd based on a zero - one measurement matrix over the reals and cc - lpd of the binary linear channel code that is obtained by viewing this measurement matrix as a binary parity - check matrix . </S>",
    "<S> this connection allows the translation of performance guarantees from one setup to the other . </S>",
    "<S> the main message of this paper is that parity - check matrices of `` good '' channel codes can be used as provably `` good '' measurement matrices under basis pursuit . in particular , we provide the first deterministic construction of compressed sensing measurement matrices with an order - optimal number of rows using high - girth low - density parity - check ( ldpc ) codes constructed by gallager .    </S>",
    "<S> approximation guarantee , basis pursuit , channel coding , compressed sensing , graph cover , linear programming decoding , pseudo - codeword , pseudo - weight , sparse approximation , zero - infinity operator . </S>"
  ]
}