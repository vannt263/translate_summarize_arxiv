{
  "article_text": [
    "compressive sensing ( cs )  @xcite is proposed as a novel technique in the field of signal processing . based on the sparsity of signals in some typical domains ,",
    "this method takes global measurements instead of samples in signal acquisition .",
    "the theory of cs confirms that the measurements required for recovery are far fewer than conventional signal acquisition technique .    with the advantages of sampling below nyquist rate and little loss in reconstruction quality",
    ", cs can be widely applied in the regions such as source coding  @xcite , medical imaging  @xcite , pattern recognition  @xcite , and wireless communication  @xcite .",
    "suppose that an @xmath2-dimensional vector @xmath3 is a sparse signal with sparsity @xmath4 , which means that only @xmath4 entries of @xmath5 are nonzero among all @xmath2 elements .",
    "an @xmath6 measurement matrix @xmath7 with @xmath8 is applied to take global measurements of @xmath9 .",
    "consequently an @xmath10 vector @xmath11 is obtained and the information of @xmath2-dimensional unknown signal is reduced to the @xmath12-dimensional measurement vector . exploiting the sparse property of @xmath5 , the original signal can be reconstructed through @xmath13 and @xmath7 .",
    "the procedure of cs mainly includes two stages : signal measurement and signal reconstruction .",
    "the key issues are the design of measurement matrix and the algorithm of sparse signal reconstruction , respectively .    on the signal reconstruction of cs ,",
    "a key problem is to derive the sparse solution , i.e. , the solution to the under - determined linear equation which has the minimal @xmath14 norm , @xmath15    however , ( [ l0 ] ) is a non - deterministic polynomial ( np ) hard problem .",
    "it is demonstrated that under certain conditions  @xcite , ( [ l0 ] ) has the same solution as the relaxed problem @xmath16 ( [ l1 ] ) is a convex problem and can be solved through convex optimization .    in non - ideal scenarios ,",
    "the measurement vector @xmath13 is inaccurate with noise perturbation and ( [ yax ] ) never satisfies exactly .",
    "consequently , ( [ l1 ] ) is modified to @xmath17 where @xmath18 is a positive number representing the energy of noise .",
    "many algorithms have been proposed to recover the sparse signal from @xmath19 and @xmath20 .",
    "these algorithms can be classified into several main categories , including greedy pursuit , optimization algorithms , iterative thresholding algorithms and other algorithms .",
    "the greedy pursuit algorithms always choose the locally optimal approximation to the sparse solution iteratively in each step .",
    "the computation complexity is low but more measurements are needed for reconstruction .",
    "typical algorithms include matching pursuit ( mp )  @xcite , orthogonal matching pursuit ( omp )  @xcite , stage - wise omp ( stomp )  @xcite , regularized omp ( romp )  @xcite , compressive sampling mp ( cosamp )  @xcite , subspace pursuit ( sp )  @xcite , and iterative hard thresholding ( iht )  @xcite .",
    "optimization algorithms solve convex or non - convex problems and can be further divided into convex optimization and non - convex optimization .",
    "convex optimization methods have the properties of fewer measurements demanded , higher computation complexity , and more theoretical support in mathematics .",
    "convex optimization algorithms include primal - dual interior method for convex objectives ( pdco )  @xcite , least square qr ( lsqr )  @xcite , large - scale @xmath1-regularized least squares ( @xmath1-@xmath21 )  @xcite , least angle regression ( lars )  @xcite , gradient projection for sparse reconstruction ( gpsr )  @xcite , sparse reconstruction by separable approximation ( sparsa )  @xcite , spectral projected - gradient @xmath1 ( spgl1 )  @xcite , nesterov algorithm ( nesta )  @xcite and constrained split augmented lagrangian shrinkage algorithm ( c - salsa )  @xcite .",
    "non - convex optimization methods solve the problem of optimization by minimizing @xmath22 norm with @xmath23 , which is not convex .",
    "this category of algorithms demands fewer measurements than convex optimization methods .",
    "however , the non - convex property may lead to converging towards the local extremum which is not the desired solution . moreover , these methods have higher computation complexity .",
    "typical non - convex optimization methods are focal underdetermined system solver ( focuss )  @xcite , iteratively reweighted least square ( irls )  @xcite and @xmath14 analysis - based sparsity ( l0abs )  @xcite .",
    "a new kind of method , zero - point attracting projection ( zap ) , has been recently proposed to solve ( [ l0 ] ) or ( [ l1 ] )  @xcite .",
    "the projection of the zero - point attracting term is utilized to update the iterative solution in the solution space .",
    "compared with the other algorithms , zap has advantages of faster convergence rate , fewer measurements demanded , and a better performance against noise .",
    "however , zap is proposed with heuristic and experimental methodology and lacks a strict proof of convergence  @xcite .",
    "though abundant computer simulations verify its performance , it is still essential to prove its convergence , provide the specific working condition , and analyze performances theoretically including the reconstruction precision , the convergence rate and the noise resistance .",
    "this paper aims to provide a comprehensive analysis for zap . specifically , it studies @xmath1-zap , which uses the gradient of @xmath1 norm as the zero - point attracting term .",
    "@xmath14-zap is non - convex and its convergence will be addressed in future work .",
    "the main contribution of this work is to prove the convergence of @xmath1-zap in non - noisy scenario .",
    "our idea is summarized as follows .",
    "firstly , the distance between the iterative solution of @xmath1-zap and the original sparse signal is defined to evaluate the convergence .",
    "then we prove that such distance will decrease in each iteration , as long as it is larger than a constant proportional to the step - size .",
    "therefore , it is proved that @xmath1-zap is convergent to the original sparse signal under non - noisy case , which provides a theoretical foundation for the algorithm .",
    "lemma 1 is the crucial contribution of this work , which reveals the relationship between @xmath1 norm and @xmath24 norm in the solution space .",
    "another contribution is about the signal reconstruction with measurement noise .",
    "it is demonstrated that @xmath1-zap can approach the original sparse signal to some extent under inaccurate measurements . in the noisy case , the recovery precision is linear with not only the step - size but also the energy of noise .",
    "other contributions include the discussions on some related topics .",
    "the convergence rate is estimated as an upper bound of iteration number .",
    "the constraint of initial value and its influence on convergence are provided .",
    "the convergence of @xmath1-zap for @xmath0-compressible signal is also discussed .",
    "experiment results are provided to verify the analysis .    at the time of revising this paper , we are noticed of a similar algorithm called projected subgradient method  @xcite , which leads to some related researches  @xcite . though obtained from different frameworks",
    ", @xmath1-zap shares the same recursion with the other .",
    "however , the two algorithms are not exactly the same .",
    "the attracting term of zap is not restricted to the subgradient of a objective function , and can be used to solve either a convex problem or a non - convex one , while only the subgradient of a convex function is allowed in the mentioned method .",
    "furthermore , the available analysis of the projected subgradient method studies the convergence of the objective function , while this work focuses on the properties of the iterative sequence , as derived from the significant lemma 1 .",
    "the theoretical analysis in this work may contribute to promoting the projected subgradient method .",
    "the remainder of this paper is organized as follows . in section",
    "ii , some preliminary knowledge is introduced to prepare for the main theorems . the main contribution in non - noisy scenario",
    "is presented as theorem 4 in section iii , which proves the convergence of @xmath1-zap .",
    "some related topics about theorem 4 are also discussed in section iii .",
    "section iv shows another main theorem in noisy scenario , and some discussions are also brought out .",
    "experiment results are shown in section v. the whole paper is concluded in section vi .",
    "in this subsection , restricted isometry property ( rip ) and coherence are introduced and then some theorems on ( [ l1 ] ) and ( [ l1_eps ] ) are presented , which will be helpful to the following content .    @xcite",
    "suppose @xmath25 is the @xmath26 submatrix by extracting the columns of @xmath6 matrix @xmath7 corresponding to the indices in set @xmath27 .",
    "the rip constant @xmath28 is defined as the smallest nonnegative quantity such that @xmath29 holds for all subsets @xmath30 with @xmath31 and vectors @xmath32 .",
    "@xcite if the rip constant of matrix @xmath7 satisfies the condition @xmath33 where @xmath4 is the sparsity of @xmath5 , then the solution of ( [ l1 ] ) is unique and identical to the original signal .",
    "@xcite if the rip constant of matrix @xmath7 satisfies the condition @xmath34 then the solution @xmath35 of ( [ l1_eps ] ) obeys @xmath36 where @xmath37 is the original signal of sparsity @xmath4 and @xmath38 is a positive constant related to @xmath4 .",
    "rip determines the property of the measurement matrix .",
    "recent results on rip can be found in  @xcite .",
    "@xcite the coherence of an @xmath6 matrix @xmath7 is defined as @xmath39 where @xmath40 is the @xmath41th column of @xmath7 and @xmath42 .",
    "@xcite if the sparsity @xmath4 of @xmath5 and the coherence of matrix @xmath7 satisfy the condition @xmath43 then the solution of ( [ l1_eps ] ) is unique .",
    "theorem 1 provides the sufficient condition on exact recovery of the original signal without any perturbation .",
    "it is also a loose sufficient condition of the unique solution of ( [ l1 ] ) .",
    "theorem 2 indicates that under the condition ( [ eq47 ] ) , the solution of ( [ l1_eps ] ) is not too far from the original signal , with a deviation proportional to the energy of measurement noise .",
    "theorem 3 provides a sufficient condition of the uniqueness of the solution of ( [ l1_eps ] ) .      in zap algorithm ,",
    "the zero - point attracting term is used to update the iterative solution and then the updated iterative solution is projected to the solution space .",
    "the procedures of zap can be summarized as follows .",
    "input : @xmath44 .",
    "initialization : @xmath45 and @xmath46 .",
    "iteration :    while stop condition is not satisfied    \\1 .",
    "zero - point attraction : @xmath47    \\2 .",
    "projection : @xmath48    \\3 .",
    "update the index : @xmath49    end while    in the initialization and ( [ eq22 ] ) , @xmath50 denotes the pseudo - inverse of @xmath7 . in ( [ eq11 ] ) , @xmath51 is the zero - point attracting term , where @xmath52 is a function representing the sparse penalty of vector @xmath5 .",
    "positive parameter @xmath53 denotes the step - size in the step of zero - point attraction .",
    "zap was firstly proposed in @xcite with a specification of @xmath14-norm constraint , termed @xmath14-zap , in which the approximate @xmath14 norm is utilized as the function f(x ) .",
    "@xmath14-zap belongs to the non - convex optimization methods and has an outstanding performance beyond conventional algorithms . in @xcite ,",
    "the penalty function is @xmath54 and its gradient is approximated as @xmath55^{\\rm t}\\ ] ] and @xmath56 the piecewise and non - convex zero - point attracting term further increases the difficulty to theoretically analyze the convergence of @xmath14-zap .    as another variation of zap ,",
    "@xmath1-zap is analyzed in this work .",
    "the function @xmath52 is the @xmath1 norm of @xmath5 in the zero - point attracting term .",
    "since it is non - differentiable , the gradient of @xmath52 can be replaced by its sub - gradient .",
    "considering that the gradient of @xmath52 is @xmath57 when none of the components of @xmath5 are zero , ( [ eq11 ] ) can be specified as @xmath58 where the gradient is replaced by one of the sub - gradients @xmath57 .",
    "the sign function @xmath57 has the same size with @xmath5 and each entry of @xmath57 is the scalar sign function of the corresponding entry of @xmath5 .",
    "experiments show that though its performance is better than conventional algorithms , @xmath1-zap behaves not as good as @xmath14 norm constraint variation .",
    "however , as a convex optimization method , @xmath1-zap has advantages beyond non - convex methods , as mentioned in introduction .",
    "@xmath1-zap is considered in this paper as the first attempt to analyze zap in theory .    the steps ( [ eq62 ] ) and ( [ eq22 ] ) of @xmath1-zap can be combined into the following recursion @xmath59 with the projection matrix @xmath60    notice that following ( [ eq5 ] ) , ( [ eq4 ] ) and the initialization , the sequence has the property @xmath61 which means all iterative solutions fall in the solution space .",
    "numerical simulations demonstrate that the sparse solution of under - determined linear equation can be calculated by @xmath1-zap .",
    "in fact , the sequence @xmath62 calculated through ( [ eq5 ] ) is not strictly convergent .",
    "@xmath62 will fall into the neighborhood of @xmath63 after finite iterations , with radius proportional to step - size @xmath53 . with the increasing of iterations , @xmath64 approaches",
    "@xmath63 step by step at first . however , it vibrates in the neighborhood of @xmath63 when @xmath64 is close enough to @xmath63",
    ". if the step - size @xmath53 decreases , the radius of neighborhood also decreases .",
    "consequently , one can get the approximation to the sparse solution at any precision by choosing appropriate step - size .    in this work the convergence of @xmath1-zap",
    "is proved .",
    "the main results are the following theorems in section iii and iv , corresponding to non - noisy scenario and noisy scenario , respectively .",
    "the main contribution is included in this section .",
    "a lemma is proposed in subsection a for preparing the main theorem in subsection b. then the condition of exact signal recovery by @xmath1-zap is given in subsection c. several constants and variables in the proof of convergence are discussed in subsections d and e. in subsection f , an estimation on the convergence rate is given .",
    "the initial value of @xmath1-zap is discussed in subsection g.      [ lemma2 ] suppose that @xmath3 satisfies @xmath65 , with given @xmath66 and @xmath67 .",
    "@xmath63 is the unique solution of ( [ l1 ] ) .",
    "if @xmath68 is bounded by a positive constant @xmath69 , then there exists a uniform positive constant @xmath70 depending on @xmath71 and @xmath69 , such that @xmath72 holds for arbitrary @xmath5 satisfying @xmath65 .",
    "the outline of the proof is presented here while the details are included in appendix a.    by defining @xmath73 equation ( [ eqinlemma2 ] ) is equivalent to the following inequality @xmath74 define the index set @xmath75 , then there exists a positive constant @xmath76 such that @xmath77 , when @xmath5 satisfies @xmath78 the above proposition means that @xmath5 and @xmath63 share the same sign for the entries indexed by @xmath79 .",
    "define sets @xmath80 and @xmath81 as @xmath82 consequently , for the separate cases of @xmath83 and @xmath84 , it is proved that @xmath85 has a positive lower bound , respectively . combining the two cases , lemma [ lemma2 ]",
    "is proved .",
    "[ theorem4 ] suppose that @xmath63 is the unique solution of ( [ l1 ] ) . @xmath86 and @xmath64 satisfy the recursion ( [ eq5 ] ) and @xmath64 is energy constrained by @xmath87 , where @xmath69 is a positive constant .",
    "then the iteration obeys @xmath88 when @xmath89 where @xmath90 are two constants with a parameter @xmath91 , and @xmath92 denotes the lower bound specified in lemma  1 .",
    "for a given under - determined constraint ( [ yax ] ) and the unique sparsest solution of ( [ l1 ] ) , theorem [ theorem4 ] demonstrates the convergence property and provides the convergence conditions of @xmath1-zap . as long as the iterative result @xmath64 is far away from the sparse solution @xmath63 , the new result @xmath86 in next iteration affirmatively becomes closer than its predecessor .",
    "furthermore , the decrease in @xmath24 distance is a constant @xmath93 , which means @xmath64 will definitely get into the @xmath94-neighborhood of @xmath63 in finite iterations . according to the definition of @xmath95",
    ", @xmath64 can approach the sparse solution @xmath63 to any extent if the step - size @xmath53 is chosen small enough .",
    "therefore , @xmath1-zap is convergent , i.e. , the iterative result can get close to the sparse solution at any precision . here",
    "@xmath96 is a tradeoff parameter which balances the estimated precision and convergence rate .",
    "the proof of theorem [ theorem4 ] goes in appendix b.      using theorem 4 and conditions added , the convergence of @xmath1-zap can be deduced , as the following corollary .    under the condition ( [ eq23 ] )",
    ", @xmath1-zap can recover the original signal at any precision if the step - size @xmath53 can be chosen small enough .",
    "firstly , it will be demonstrated that the condition of energy constraint in theorem 4 can always be satisfied .",
    "in fact , @xmath69 can be chosen greater than @xmath97 .",
    "if the energy constraint @xmath98 holds for index @xmath99 , the conditions of theorem 4 are satisfied and then @xmath100 holds naturally according to ( [ eq9 ] ) .",
    "consequently , it is readily accepted that the condition of energy constraint is satisfied for each index @xmath99 , with the utilization of theorem 4 in each step .    combining the explanation after theorem 4 ,",
    "it is clear that the @xmath1-zap is convergent to the solution of ( [ l1 ] ) at any precision as long as the step - size is chosen small enough .    according to theorem 1 ,",
    "it is known that under the condition of ( [ eq23 ] ) , the solution of ( [ l1 ] ) is unique and identical to the original sparse signal",
    ". then corollary 1 is proved .    according to theorem 4 and corollary 1",
    ", the sequence will surely get into the @xmath94-neighborhood of @xmath63 .",
    "in fact , because of several inequalities used in the proof , @xmath101 is merely a theoretical radius with conservative estimation .",
    "the actual convergence may get into a even smaller neighborhood .",
    "the details will be discussed in subsection f.      involved in ( [ eq25 ] ) of theorem 4 , constant @xmath70 is essential to the convergence of @xmath1-zap .",
    "in fact , the key contribution of this work is to indicate the existence of this constant .",
    "however , one can merely obtain the existence of @xmath70 from the proof of lemma 1 , other than its exact value . because @xmath63 in the definition of ( [ eq6 ] ) is unknown ,",
    "it is difficult to give the exact value or formula of @xmath70 , even though it is actually determined by @xmath7 , @xmath13 , and @xmath69 .",
    "whereas , an upper bound is given with some information about @xmath70 , which leads to theorem 5 .",
    "according to ( [ eq25 ] ) , constant @xmath95 is inversely proportional to @xmath70 . with a small @xmath70 , the radius of convergent neighborhood is large and the convergence precision is worse .",
    "the maximum of @xmath103 is also involved in the definition of @xmath95 .",
    "according to the range of sign function , i.e. @xmath104 , there are @xmath105 choices of vector @xmath106 altogether . similar to @xmath70 , the extremum of @xmath103",
    "is determined by @xmath20 .",
    "the relationship between @xmath70 and extremum of @xmath103 is presented in theorem 5 .",
    "if @xmath70 is defined by ( [ eq6 ] ) , one has the following inequality @xmath107    the proof of theorem 5 is postponed to appendix c.    according to the theorem , the minimum of @xmath103 restricts the value of @xmath70 , as leads to worse precision of @xmath1-zap .",
    "hence , the measurement matrix @xmath7 should be chosen with relatively large @xmath108 to improve the performance of the mentioned algorithm .",
    "the mathematical meaning of @xmath109 is the projection of @xmath57 to the solution space of @xmath110 . for a particular instance , if there exists a sign vector , to whom the solution space is almost orthogonal , then the minimum of @xmath103 is rather small and the precision of convergence is bad .",
    "an additional explanation is that the solution space can not be strictly orthogonal to any sign vector , or else it will lead to a contradiction with the condition of ( [ eq23 ] ) , i.e. , the uniqueness of @xmath63 .",
    "a parameter @xmath96 is involved in theorem 4 .",
    "we will discuss the choice of @xmath96 and some related problems .",
    "first of all , it needs to be stressed that @xmath96 is just a parameter for the bound sequence in theoretical analysis , other than a parameter for actual iterations .    according to the proof in appendix b , as long as @xmath96 is chosen satisfying the conditions of @xmath111 and @xmath112 theorem 4 holds and the distance between @xmath64 and @xmath63 decreases in the next iteration .",
    "however , considering the expression of ( [ eq26 ] ) , the decrease of @xmath113 by each iteration is different for various @xmath96 .",
    "there are two strategies to choose the parameter @xmath96 , a constant or a variable one .    when @xmath96 is chosen as a constant , theorem 4 indicates that as long as the distance between @xmath114 and @xmath63 is larger than @xmath101 , the next iteration leads to a decrease at least a constant step of @xmath93 .    when the parameter @xmath96 is variable , the decrease step of @xmath115 is also variable .",
    "the expressions show that @xmath95 and @xmath116 increase as the increase of @xmath96 .",
    "notice that @xmath96 must obey @xmath117 where the right inequality is necessary to satisfy ( [ eq99 ] ) , which ensures the convergence of the sequence . during the very beginning of recursions",
    ", @xmath64 is far from @xmath63 .",
    "consequently , @xmath96 satisfying ( [ eq54 ] ) can be larger , and lead to a faster convergence . however , as @xmath64 gets closer to @xmath63 by iterations , @xmath96 satisfying ( [ eq54 ] ) is definitely just a little larger than one .",
    "to be emphasized , the actual convergence of iterations can not speed up by choosing the parameter @xmath96 .",
    "the value of @xmath96 only impacts the sequence of @xmath118 which is a sequence bounding the actual sequence in the proof of convergence .",
    "theorem 4 tells little about the convergence rate .",
    "considering several inequalities utilized in the proof , the actual convergence is faster than that of the sequence in ( [ eq55 ] ) .",
    "it means that a lower bound of the convergence rate can be derived in theory .",
    "corresponding to the variable selection of @xmath96 , a sequence @xmath119 is put forward with properties @xmath120 where @xmath121 combining ( [ eq46 ] ) and ( [ eq45 ] ) , the iteration of @xmath122 obeys @xmath123 the distance between @xmath122 and @xmath63 with variable @xmath96 decreases the most for each step .",
    "therefore , @xmath119 has a faster convergence rate compared with sequences satisfying ( [ eq55 ] ) with other choices of @xmath96 .",
    "however , as a theoretical result , it still converges more slowly than the actual sequence .",
    "derived from lemma 2 , which gives a rough estimation , theorem 6 provides a much better lower bound of the convergence rate .",
    "supposing @xmath62 is the iterative sequence by @xmath1-zap , it will take at most @xmath124 steps for @xmath62 to get into the @xmath125-neighborhood from the @xmath126-neighborhood of @xmath63 , where @xmath127 and @xmath128 must obey @xmath129    supposing @xmath62 is the iterative sequence by @xmath1-zap , it will get into the @xmath130-neighborhood of @xmath63 within at most @xmath131 steps . here",
    "@xmath132 , and @xmath70 have the same definitions with those in theorem 4 , and @xmath133 must obey @xmath134    the proofs of lemma 2 and theorem 6 are postponed to appendix d and e , respectively .      in @xmath1-zap ,",
    "the initial value is the least square solution of the under - determined equation , @xmath135 from theorem 4 and corollary 1 , one knows that if the initial value obeys @xmath136 , the iterative sequence @xmath62 is convergent .",
    "therefore , the restriction to the initial value is to be in the solution space , other than to be the least square solution .",
    "however , it is still a convenient way to initialize using the least square solution .",
    "the convergence of @xmath1-zap in noisy scenario is analyzed in this section . the main theorem in noisy scenario",
    "is given in subsection a. in subsection b , the problem of signal recovery from inaccurate measurements is discussed .",
    "subsection c shows different choices of initial value and the impact on the quality of reconstruction .",
    "the reconstruction of @xmath0-compressible signal by @xmath1-zap is discussed in subsection d.      considering the perturbation on measurement vector @xmath13 , theorem 7 is presented to analyze the convergence of @xmath1-zap . similar to lemma 1 , lemma 3 is proposed at first corresponding to the noisy case .",
    "suppose that @xmath3 satisfies @xmath137 , with given @xmath66 and @xmath67 .",
    "@xmath35 is the unique solution of ( [ l1_eps ] ) .",
    "@xmath138 is bounded by a positive number @xmath69 .",
    "then there exists a positive number @xmath70 depending on @xmath7 , @xmath13 , @xmath69 , and @xmath18 , such that @xmath139    with the definition of ( [ eq1 ] ) , ( [ eqinlemma4 ] ) is equivalent to the following inequality @xmath140    following the proof of lemma 1 , it can be readily proved that lemma 3 is correct .",
    "notice that here @xmath141 is not in the null - space of @xmath20 , but a unit vector satisfying @xmath142 .",
    "the remaining procedures are similar .",
    "the details of the proof are omitted for short .",
    "supposing that @xmath35 is the unique solution of ( [ l1_eps ] ) , sequence @xmath62 satisfies the iterative formula ( [ eq5 ] ) with conditions @xmath143 and @xmath144 where @xmath69 is a positive constant .",
    "then the iteration obeys @xmath145 when @xmath146 where @xmath147 , @xmath95 and @xmath116 are defined by ( [ eq25 ] ) and ( [ eq26 ] ) , respectively . here",
    "@xmath111 is a parameter , @xmath70 is the positive lower bound in lemma 3 , and @xmath148 is the largest eigenvalue of matrix @xmath149 .",
    "the proof of theorem 7 goes in appendix f.    theorem 7 indicates that under measurement perturbation with energy less than @xmath18 , the iterative sequence @xmath62 will get into the @xmath150-neighborhood of @xmath35 . for the fixed original signal and measurement matrix , the precision of @xmath114 approaching @xmath35 depends on both the step - size and the noise energy bound .",
    "it means that @xmath64 can not get close to the solution @xmath35 at any precision by choosing small step - size , because the noise energy also controls a deviation component , @xmath151 .",
    "corollary 2 indicates the property of signal reconstruction with inaccurate measurements .",
    "suppose the original signal is @xmath152 , and the conditions of ( [ eq47 ] ) and ( [ eq49 ] ) are satisfied .",
    "there exist real numbers @xmath153 , @xmath154 such that @xmath1-zap can be convergent to a @xmath155-neighborhood of @xmath37 , i.e. , @xmath1-zap can approach the original signal to some extent under inaccurate measurements .",
    "referring to the proof of corollary 1 , it can be readily accepted that the condition ( [ eq53 ] ) is always satisfied for any index @xmath99 .",
    "it is known from theorem 3 that ( [ l1_eps ] ) has a unique solution under the condition ( [ eq49 ] ) .",
    "consequently , according to theorem 7 , the sequence @xmath62 finally gets into the neighborhood of @xmath35 with the radius @xmath156 .",
    "theorem 2 shows that under the condition of ( [ eq47 ] ) , the solution of ( [ l1_eps ] ) is not far from the original signal @xmath37 , with the inequality",
    "@xmath157 combining theorem 7 , ( [ eq61 ] ) , and the triangle inequality , one sees that the sequence gets into the neighborhood of @xmath37 with the radius @xmath158 .",
    "denote @xmath159 and the conclusion of corollary 2 is drawn .      among the assumptions of theorem 7 ,",
    "a condition of ( [ eq52 ] ) is assumed to be satisfied .",
    "considering the recursion ( [ eq5 ] ) , one readily sees that @xmath160 under the simple condition of @xmath161 where @xmath162 is not necessarily the least square solution of @xmath110 , it will suffice to get ( [ eq52 ] ) , which satisfies the condition of theorem 7 .",
    "if the initial value satisfies ( [ eq19 ] ) , by defining @xmath163 , one has @xmath164 inequality ( [ eq34 ] ) provides the upper bound of @xmath165 and it is used to prove theorem 7 .",
    "if the iterations begin with the least square solution of the perturbed measurement @xmath13 , it obeys @xmath166 and according to ( [ eq321 ] ) one has @xmath167 which means that ( [ eq34 ] ) can be modified to @xmath168 hence , the parameter @xmath18 can be reduced to a half throughout the proof of theorem 7 .",
    "therefore , if the initial value is chosen as the least square solution , the neighborhood of convergence will be smaller , i.e. , a better estimation can be reached .",
    "the original signal is not always absolutely sparse .",
    "the reconstruction of compressible signal is discussed here .",
    "signal @xmath5 is @xmath0-compressible with magnitude @xmath169 if the components of @xmath9 decay as @xmath170 where @xmath171 is the @xmath41th largest absolute value among the components of @xmath5 , and @xmath0 is a number between @xmath172 and @xmath173 .",
    "supposing that @xmath174 is a best @xmath4-sparse approximation to @xmath5 , the following inequalities hold @xcite , @xmath175 where @xmath176 and @xmath177 .    for a @xmath0-compressible signal @xmath5",
    ", one has @xmath178 by proposition 3.5 in @xcite , the norm of @xmath179 can be estimated as @xmath180 combining ( [ eq101 ] ) , ( [ eq102 ] ) and ( [ eq103 ] ) , one has @xmath181 according to theorem 7 and corollary 2 , the reconstruction property of @xmath0-compressible signal by @xmath1-zap can be deduced as follows .",
    "supposing @xmath3 is @xmath0-compressible signal and the conditions of ( [ eq47 ] ) and ( [ eq49 ] ) are satisfied , then the @xmath1-zap sequence can approach @xmath5 with a deviation @xmath182 where @xmath95 and @xmath183 are the same with those in corollary 2 , and @xmath18 is the energy bound of observation noise .    the non - noisy scenario for compressible signal",
    "can be naturally obtained by setting @xmath18 to zero in corollary 3 .",
    "several experiments are conducted in this section .",
    "the performance of @xmath14-zap and @xmath1-zap are shown in subsection a , compared with several other algorithms for sparse recovery .",
    "the deviations of actual @xmath1-zap sequence and bound sequences in the proof are illustrated in subsection b. in subsection c , experiment results demonstrate the impacts of the step - size and the noise level on the signal reconstruction via @xmath1-zap .",
    "the performances of @xmath1-zap and @xmath14-zap are simulated , compared with other sparse recovery algorithms .    in the experiments ,",
    "the @xmath6 matrix @xmath7 is generated with the entries independent and following a normal distribution with mean zero and variance @xmath184 .",
    "the support set of original signal @xmath63 is chosen randomly following uniform distribution .",
    "the nonzero entries follow a normal distribution with mean zero .",
    "finally the energy of the original signal is normalized .    for parameters @xmath185 , @xmath186 ,",
    "the probability of exact reconstruction for various number of measurements is shown as fig .",
    "if the reconstruction snr is higher than a threshold of @xmath187db , the trial is regarded as exact reconstruction .",
    "the number of @xmath12 varies from @xmath188 to @xmath189 and each point in the experiment is repeated 200 times .",
    "the step - size of @xmath1-zap is @xmath190 .",
    "the parameters of other algorithms are selected as recommended by respective authors .",
    "it can be seen that for any fixed @xmath12 from @xmath191 to @xmath192 , @xmath14-zap and @xmath1-zap have higher probability of reconstruction than other algorithms , which means zap algorithms demand fewer measurements in signal reconstructions .",
    "the experiment also indicates that the performance of @xmath14-zap is better than @xmath1-zap , as discussed in section ii .",
    ".,width=384 ]    for parameters @xmath185 , @xmath193 , fig .",
    "[ fig2 ] illustrates the probability of exact reconstruction for various sparsity @xmath4 from @xmath194 to @xmath195 .",
    "all the algorithms are repeated 200 times for each value .",
    "the parameters of algorithms are the same as those in the previous experiment .",
    "@xmath14-zap has the highest probability for fixed sparsity @xmath4 and @xmath1-zap is the second beyond other conventional algorithms .",
    "the experiment indicates that zap algorithms can recover less sparse signals compared with other algorithms .",
    ".,width=384 ]    the snr performance is illustrated in fig .",
    "[ fig3 ] with the measurement snr varying from @xmath196db to @xmath197db and 200 times repeated for each value .",
    "the noise is zero - mean white gaussian and added to the observed vector @xmath13 .",
    "the parameters are selected as @xmath185 , @xmath193 and @xmath198 .",
    "the parameters of algorithms have the same choice with previous experiments .",
    "the reconstruction snr and measurement snr are the signal - to - noise ratios of reconstructed signal @xmath199 and measurement signal @xmath13 , respectively .",
    "@xmath14-zap outperforms other algorithms , while @xmath1-zap is almost the same as others .",
    "the experiment indicates that @xmath14-zap has a better performance against noise and @xmath1-zap does not have visible defects compared with other algorithms .",
    ".,width=384 ]    the experiments above demonstrate that @xmath1-zap has a better performance compared with conventional algorithms .",
    "@xmath1-zap demands fewer measurements and can recover signals with higher sparsity , with similar property against noise .",
    "the performance of @xmath14-zap is better than @xmath1-zap .      according to theorem 4 ,",
    "the deviation from the actual iterative sequence to the sparse solution is bounded by the sequence satisfying ( [ eq55 ] ) . in theorem 4 ,",
    "a sequence with parameter @xmath96 is utilized to bound the actual sequence and proved to be convergent . as discussed in iii - e and f , the sequence defined in ( [ eq46 ] ) and ( [ eq45 ] ) with",
    "adaptive @xmath96 approaches the sparse solution faster than any sequence with constant @xmath96 .",
    "the reconstruction snr curves of the actual sequence and three bound sequences with different choices of @xmath96 are demonstrated in fig .",
    "[ fig4 ] . as can be seen in the figure , the bound sequence with adaptive @xmath96 is the best estimation among different choices . for a constant @xmath96 ,",
    "the larger one leads to faster convergence and less precision .    , where @xmath200 , @xmath185 , @xmath186 , @xmath201.,width=384 ]     throughout the iteration for adaptive @xmath96.,width=384 ]    for adaptive @xmath96 , as illustrated in fig .",
    "[ fig4 ] , the reconstruction snr reaches steady - state after about @xmath202 iterations .",
    "however , referring to fig .",
    "[ fig5 ] , the value of @xmath96 keeps decreasing until over @xmath203 iterations , though it impacts little to the convergence behavior .",
    "in fact , adaptive @xmath96 will decrease towards @xmath173 throughout the iteration and never stop .",
    "nevertheless , the precision of simulation platform limits its variation after it is below @xmath204 .",
    "the deviations of the actual iterative sequence and a bound sequence are both proportional to the step - size , with the difference in the scale factor . though the bound is not very strict , it does well in the proof of the convergence of @xmath1-zap .",
    "as proved in theorem 4 , in non - noisy scenario , @xmath1-zap can reconstruct the original signal at arbitrary precision by choosing the step - size small enough .",
    "theorem 7 demonstrates that in noisy scenario the reconstruction snr is determined by both the step - size and noise level .",
    "experiment results shown in fig .",
    "[ fig6 ] verify the analysis .",
    "each combination of step - size and measurement snr is simulated 100 times .",
    "experiment results indicate that in non - noisy scenario , the reconstruction snr increases as the decreasing of step - size . in noisy scenario ,",
    "the reconstruction snr can not increase arbitrarily due to the impact of noise . for small step - size",
    ", the reconstruction snr is mainly determined by noise level .",
    "the reconstruction snr is higher when the measurement snr is higher . for large step - size",
    ", the step - size mainly controls the reconstruction snr and the reconstruction snr increase as the decreasing of step - size .    ,",
    "@xmath185 , @xmath205.,width=384 ]    the figure also offers a way to choose the step - size under noise .",
    "it is not necessary to choose the step - size too small because it benefits little under the impact of noise . for an estimated reconstruction snr ,",
    "the best choice of step - size is the value just entered the flat region .",
    "this paper provides @xmath1-zap a comprehensive theoretical analysis .",
    "firstly , the mentioned algorithm is proved to be convergent to a neighborhood of the sparse solution with the radius proportional to the step - size of iteration .",
    "therefore , it is non - biased and can approach the sparse solution to any extent and reconstruct the original signal exactly . secondly , when the measurements are inaccurate with noise perturbation",
    ", @xmath1-zap can also approach the sparse solution and the precision is linearly reduced by the disturbance power . in addition , some related topics about the initial value and the convergence rate are also discussed .",
    "the convergence property of @xmath0-compressible signal by @xmath1-zap is also discussed .",
    "finally , experiments are conducted to verify the theoretical analysis on the convergence process and illustrate the impacts of parameters on the reconstruction results .",
    "it is to be proved that @xmath85 defined in ( [ eq1 ] ) has a positive lower bound respectively for @xmath83 and @xmath206 .    for @xmath83 ,",
    "the function @xmath85 is continuous for @xmath5 and the domain is a bounded closed set . as a basic theorem in calculus , the value of a continuous function can reach the infinum if the domain is a bounded closed set . as a consequence , there exists an @xmath207 , such that @xmath208 . by the uniqueness of @xmath63 and the definition of @xmath85 ,",
    "@xmath209 is positive in @xmath80 .",
    "then @xmath210 is positive and this leads to the conclusion that the infimum of @xmath85 is positive in @xmath80 .    on the other hand",
    ", it will be proved that @xmath85 has a positive lower bound for @xmath211 .",
    "any vector in the solution space of @xmath212 can be represented by @xmath213 where @xmath214 denote the distance and direction , respectively .",
    "considering the definition of @xmath76 , one has @xmath215 combining ( [ eq3 ] ) with ( [ eq7 ] ) , one gets @xmath216 as a consequence , for @xmath217 , the objective function can be simplified as @xmath218    index set @xmath219 is the support set of @xmath220 .",
    "@xmath221 denotes the complement of @xmath222 . for @xmath223 ,",
    "considering the definition of @xmath76 , @xmath224 for @xmath225 , considering @xmath226 and the definition of @xmath227 in ( [ defineu ] ) , @xmath228 consequently , @xmath85 can be rewritten as a function of @xmath229 , @xmath230 where @xmath231 and @xmath232    it can be seen that @xmath233 is continuous for @xmath229 and the domain of @xmath234 is @xmath235 . since the domain of @xmath233 is the intersection of two closed sets and the first set",
    "is bounded , it is a bounded closed set and @xmath233 can reach the infimum .",
    "then @xmath85 has the minimum . by the uniqueness of @xmath236 ,",
    "@xmath85 is positive , consequently @xmath237 .    to sum up",
    ", the lower bound of @xmath85 is positive for @xmath238 which completes the proof of lemma  1 .",
    "by denoting @xmath239 as the iterative deviation and subtracting the unique solution @xmath63 from both sides of ( [ eq5 ] ) , one has @xmath240    according to ( [ eq57 ] ) , @xmath241 considering @xmath242 @xmath243 and using lemma 1 , one can shrink the second item of ( [ eq10 ] ) to @xmath244    using ( [ eq12 ] ) and ( [ eq10 ] ) , one has @xmath245 consequently , for any @xmath246 , if @xmath247 one has @xmath248 theorem 4 is proved .",
    "noticing that @xmath227 is in the kernel of @xmath20 and @xmath249 is a symmetric projection matrix to the solution space , with ( [ eq4 ] ) and ( [ defineu ] ) , one has @xmath250 because @xmath227 is a unit vector , it can be further derived that @xmath251 consider the definition of @xmath70 in ( [ eqinlemma2 ] ) and ( [ eq35 ] ) , @xmath252 where @xmath80 and @xmath81 are defined in ( [ eq15 ] ) . combining ( [ usignzoom ] ) and ( [ eq13 ] ) , consequently , the left inequality of ( [ eq24 ] ) is proved",
    ".    now let s turn to the right inequality of ( [ eq24 ] ) . because of the property of projection matrix , @xmath253 , the eigenvalue of @xmath254 is either @xmath172 or @xmath173 . for all @xmath5 ,",
    "one has @xmath255 where @xmath256 denotes the eigenvalue set of @xmath254 .",
    "the arbitrariness of @xmath5 leads to @xmath257 therefore , theorem 5 is proved .",
    "for @xmath128 satisfying ( [ eq2 ] ) , there exists @xmath258 such that @xmath259 considering the recursion of sequence @xmath119 in ( [ eq21 ] ) , it is expected to prove that @xmath260 when @xmath261 using ( [ eq14 ] ) , the difference between the left side and the right side of ( [ eq8 ] ) is @xmath262-\\frac{2\\gamma t}{\\mu'}\\|{\\bf x}_n'-{\\bf x}^*\\|_2 \\le -\\gamma^2 t^2\\left(1-\\frac{1}{\\mu'}\\right)^2<0.\\end{aligned}\\ ] ] as a consequence , ( [ eq8 ] ) holds and it leads to @xmath263 according to ( [ eq41 ] ) , the quantity of decrease by each step is at least @xmath264",
    ".    considering that @xmath62 has a faster convergence rate than that of @xmath119 , and the trip of @xmath62 is from @xmath265-ball to @xmath266-ball , consequently the iteration number is at most @xmath267",
    "according to lemma  2 , the iteration number needed from @xmath268-neighborhood to @xmath269-neighborhood is at most @xmath270 where @xmath271 and @xmath272 is larger than @xmath173 .",
    "assume that @xmath273 obeys @xmath274 where @xmath275 is a positive integer . utilizing ( [ eqnton1 ] ) , the total iteration number from @xmath69-neighborhood to @xmath130-neighborhood is at most @xmath276 which is less than @xmath277 thus theorem 6 is proved .",
    "the relation between ( [ in1 ] ) and ( [ in2 ] ) comes from the following plain algebra , @xmath278\\\\ < & \\frac{k_0}{t}\\left[m+\\frac{1}{\\mu_0 - 1}+\\frac{1}{\\mu_0}(\\ln{(m-1)}+1)\\right]\\\\ = & \\frac{k_0}{t}\\left[m+\\frac{1}{\\mu_0}\\ln{(m-1)}+\\left(\\frac{1}{\\mu_0 - 1}+\\frac{1}{\\mu_0}\\right)\\right]\\\\ < & \\frac{m_0}{t\\gamma}+\\frac{k_0}{t}\\ln{\\left(\\frac{m_0}{k_0\\gamma}\\right)+\\frac{k_0}{t}\\frac{\\mu_0}{\\mu_0 - 1 } } = \\text{(\\ref{in2})}.\\end{aligned}\\ ] ]",
    "similar to ( [ eq10 ] ) , by defining @xmath279 and @xmath280 , the deviation iterates by @xmath281 from lemma 3 and referring to ( [ eq12 ] ) , one has @xmath282 next the third item of ( [ eq36 ] ) will be studied . by the property of symmetric matrices , @xmath283 where @xmath284 and @xmath285 denote its eigenvalues .",
    "notice that @xmath286 , therefore @xmath287 is at most one , and at least @xmath288 of the eigenvalues are zeros .",
    "consequently , one has @xmath289 where the last step can be derived by @xmath290 it can be easily seen that @xmath291 is positive , if @xmath292 is an invertible matrix .    because @xmath293 is a scalar , combining ( [ eq39 ] ) and ( [ eq40 ] )",
    ", one has @xmath294    for @xmath295 , if @xmath296 using ( [ eq37 ] ) , ( [ eq42 ] ) and ( [ eq43 ] ) , we have @xmath297    combining ( [ eq36 ] ) and ( [ eq44 ] ) , it can be concluded that under the condition of ( [ eq43 ] ) , @xmath298 then theorem 7 is proved .",
    "the authors appreciate jian jin and three anonymous reviewers for their helpful comments to improve the quality of this paper .",
    "yuantao gu wishes to thank professor dirk lorenz for his notification of the projected subgradient method .",
    "g.  valenzise , g.  prandi , m.  tagliasacchi , and a.  sarti , `` identification of sparse audio tampering using distributed source coding and compressive sensing techniques , '' _ journal on image and video processing _ ,",
    "vol .  2009 ,",
    "jan .  2009 .",
    "j.  wright , y.  ma , j.  mairal , g.  sapiro , t.  s.  huang , and s.  yan , `` sparse representation for computer vision and pattern recognition , '' _ proceedings of the ieee _ ,",
    "98 , no .  6 , pp .",
    "1031 - 1044 , june  2010 .",
    "y.  c.  pati , r.  rezaiifar , and p.  s.  krishnaprasad , `` orthogonal matching pursuit : recursive function approximation with applications to wavelet decomposition , '' _ proc .",
    "27th annu .",
    "asilomar conf .",
    "signals , syst .",
    ", comput . _ , pacific grove , ca , nov .",
    "1993 , vol .  1 ,",
    "40 - 44 .",
    "d.  needell and r.  vershynin , `` uniform uncertainty principle and signal recovery via regularized orthogonal matching pursuit , '' _ foundations of computational mathematics _ , vol .  9 , no .  3 , pp .  317 - 334 , 2009 .    d.  needell and r.  vershynin , `` signal recovery from incomplete and inaccurate measurements via regularized orthogonal matching pursuit , '' _ ieee j. sel .",
    "topics signal process . _ ,",
    "vol .  4 , no .  2 , pp .  310 - 316 , apr .  2010 .",
    "s.  kim , k.  koh , m.  lustig , s.  boyd , and d.  gorinvesky , `` an interior - point method for large - scale @xmath1-regularized least squares , '' _ ieee j. sel .",
    "topics signal process .",
    "_ , vol .  1 , no .  4 , pp .  606 - 617 , dec .  2007 .",
    "m.  a.  t.  figueiredo , r.  d.  nowak , and s.  j.  wright , `` gradient projection for sparse reconstruction : application to compressed sensing and other inverse problems , '' _ ieee j. sel .",
    "topics signal process . _ ,",
    "vol .  1 , no .  4 , pp .",
    "586 - 597 , dec .",
    "2007 .",
    "m.  v.  afonso , j.  m.  bioucas - dias , m.  a.  t.  figueiredo , `` a fast algorithm for the constrained formulation of compressive image reconstruction and other linear inverse problems , '' _ icassp 2010 _ , pp .",
    "4034 - 4037 , mar .  2010 .",
    "i.  f.  gorodnitsky , j.  george , and b.  d.  rao , `` neuromagnetic source imaging with focuss : a recursive weighted minimum norm algorithm , '' _ electrocephalography and clinical neurophysiology _ , pp .",
    "231 - 251 , 1995 .",
    "j.  jin , y.  gu , and s.  mei , `` a stochastic gradient approach on compressive sensing signal reconstruction based on adaptive filtering framework , '' _ ieee j. sel .",
    "topics signal process . _ , vol .  4 , no .  2 , apr .  2010 .",
    "d.  a.  lorenz , m.  e.  pfetsch , a.  m.  tillmann , `` infeasible - point subgradient algorithm and computational solver comparison for @xmath1-minimization , '' submitted , july 2011 .",
    "optimization online e - print i d 2011 - 07 - 3100 , http://www.optimization-online.org/db_html/2011/07/3100.html              e.  j.  candes , j.  romberg , and t.  tao , `` stable signal recovery from incomplete and inaccurate measurements , '' _ communications on pure and applied mathematics _",
    "59 , no .  8 , pp .  1207 - 1223 , aug .",
    "z.  ben - haim , y.  c.  eldar , and m.  elad , `` coherence - based performance guarantees for estimating a sparse vector under random noise , '' _ ieee trans . signal process .",
    "_ , vol .  58 , no .  10 , pp .  5030 - 5043 , oct .  2010"
  ],
  "abstract_text": [
    "<S> a recursive algorithm named zero - point attracting projection ( zap ) is proposed recently for sparse signal reconstruction . compared with the reference algorithms , </S>",
    "<S> zap demonstrates rather good performance in recovery precision and robustness . </S>",
    "<S> however , any theoretical analysis about the mentioned algorithm , even a proof on its convergence , is not available . in this work , a strict proof on the convergence of zap </S>",
    "<S> is provided and the condition of convergence is put forward . based on the theoretical analysis , it is further proved that zap is non - biased and can approach the sparse solution to any extent , with the proper choice of step - size . </S>",
    "<S> furthermore , the case of inaccurate measurements in noisy scenario is also discussed . </S>",
    "<S> it is proved that disturbance power linearly reduces the recovery precision , which is predictable but not preventable . </S>",
    "<S> the reconstruction deviation of @xmath0-compressible signal is also provided . </S>",
    "<S> finally , numerical simulations are performed to verify the theoretical analysis .    </S>",
    "<S> * keywords : * compressive sensing ( cs ) , zero - point attracting projection ( zap ) , sparse signal reconstruction , @xmath1 norm , convex optimization , convergence analysis , perturbation analysis , @xmath0-compressible signal . </S>"
  ]
}