{
  "article_text": [
    "an @xmath2-step self - avoiding walk ( saw ) on the @xmath3-dimensional cubic lattice is a mapping @xmath4 with @xmath5 for each @xmath6 ( @xmath7 denotes the euclidean norm of @xmath8 ) , with @xmath9 at the origin , and with @xmath10 for all @xmath11 .",
    "it is of fundamental interest in the theory of critical phenomena as the @xmath12 limit of the @xmath13-vector model , and is the simplest model which captures the universal behavior of polymers in a good solvent .",
    "the number of self - avoiding walks of length @xmath2 on @xmath14 , which we denote @xmath15 , is believed to be given by @xmath16 the exponents @xmath0 and @xmath17 are universal , i.e. they are dependent only on the dimensionality of the lattice , while the growth constant @xmath18 and amplitude @xmath19 are not .",
    "the exponent @xmath20  @xcite , and next - to - leading correction terms with exponents @xmath21 are folded into the @xmath22 expression . for bipartite lattices",
    "there is an additional `` anti - ferromagnetic '' term which has a factor of @xmath23 .",
    "it is important to take this into account when studying series from exact enumeration  @xcite , but it is negligible for the values of @xmath2 that are accessible to the monte carlo computer experiments considered here and so we neglect it .    in two dimensions the critical exponent @xmath0 is known exactly , predicted to be @xmath24 over thirty years ago via coulomb gas arguments by nienhuis  @xcite .",
    "this has been verified to extremely high precision via enumerations using the finite lattice method  @xcite ; the most recent estimate confirms the exact result to more than five decimal places , @xmath25  @xcite .    in three dimensions",
    "the finite lattice method is not as powerful , and the best estimates for @xmath0 come from other enumeration techniques  @xcite and monte carlo simulation  @xcite .    in this work",
    ", we will calculate the critical exponent @xmath0 and amplitude @xmath19 for saws on @xmath14 to a high degree of accuracy via a monte carlo computer experiment .",
    "we will use an efficient implementation of the pivot algorithm  @xcite which makes it feasible to rapidly sample self - avoiding walks of millions of steps .",
    "our simulation framework is similar to an earlier calculation of the growth constant @xmath18  @xcite ; here we go into more depth and explicitly study the behaviour of the autocorrelation function of the markov chain .",
    "we introduce our method to calculate @xmath0 in sec .",
    "[ sec : method ] , which includes a calculation of the autocorrelation function of the markov chain for different choices of sampling scheme .",
    "we then present our results and analysis in sec .",
    "[ sec : results ] .",
    "finally , we compare our estimates for @xmath0 and @xmath19 to values from the literature , discuss the potential for scale - free moves as a paradigm for modeling polymers , and give a brief conclusion in sec .",
    "[ sec : discussion ] .",
    "the pivot algorithm is the most efficient method known for sampling self - avoiding walks  @xcite , and recent improvements  @xcite have made it even more effective , especially in the large @xmath2 limit .",
    "these improvements are highly beneficial as they allow one to obtain accurate data for large @xmath2 , which reduces systematic errors due to corrections - to - scaling .",
    "the method described here is very similar to that of a recent paper  @xcite , but as we wish to emphasize different aspects of the method we will keep the description self - contained , even though this will result in a degree of repetition .",
    "the principal difficulty in applying the pivot algorithm to the estimation of @xmath0 is that it samples walks in the fixed - length ensemble , whereas @xmath0 is intrinsically associated with the growth in the number of walks as a function of length .",
    "caracciolo et al .",
    "@xcite overcame this difficulty by inventing the join - and - cut algorithm which samples pairs of self - avoiding walks of fixed total length .",
    "another approach is the berretti - sokal algorithm  @xcite which naturally samples walks of different lengths .",
    "we wish to use the pivot algorithm to sample self - avoiding walks , and so we must find an observable that allows us to estimate @xmath0 from the fixed - length ensemble .    to do this we sample the same observable as a previous paper  @xcite : the probability that two self - avoiding walks of length @xmath2 can be concatenated to form a self - avoiding walk of length @xmath26 .",
    "we note that the use of pairs of walks to estimate @xmath0 was suggested by madras and sokal  @xcite , and that the join - and - cut algorithm  @xcite is also similar .",
    "we define an observable @xmath27 on pairs of walks @xmath28 and @xmath29 via @xmath30 the concatenation operation is illustrated in fig .",
    "[ fig : concatenation ] ; it is not the standard operation because an additional bond is inserted between @xmath28 and @xmath29 .",
    "we adopt the convention that the sites of the walk which are incident to the concatenating bond are labeled 0 , and increase in number going out to the free ends .",
    "at ( 0,-0.5 ) 0 ; at ( 3.0,-3.5 ) 10 ; + ( 3,-3 ) circle ( 5pt ) ;    at ( 0,-0.5 ) 0 ; at ( 3,-1.5 ) 10 ; + ( 3,-1 ) circle ( 5pt ) ;    at ( 0,-0.5 ) 0 ; at ( 1,-3.5 ) 10 ; + ( 1,-3 ) circle ( 5pt ) ;    at ( 0,-0.5 ) 0 ; at ( -3,0.5 ) 10 ; + ( -3,1 ) circle ( 5pt ) ;    we define @xmath31 as the expectation of @xmath27 on the set of all pairs of self - avoiding walks of @xmath2 steps : @xmath32 where @xmath33 is the coordination number of the lattice ( @xmath34 for the simple cubic lattice ) .",
    "now we define @xmath35 , and use the asymptotic form of @xmath15 from ( [ eq : asymptotic ] ) to obtain : @xmath36      the pivot algorithm is a markov chain monte carlo algorithm which samples walks of fixed length @xmath2 . the elementary move is a _ pivot _",
    ", where a lattice symmetry operation ( rotation or reflection ) is applied to part of a walk , and it generates a correlated sequence of self - avoiding walks as follows :    1 .   select a pivot site of the current saw according to some prescription ( usually uniformly at random , here we will use a non - uniform distribution ) ; 2 .   randomly choose a lattice symmetry ( rotation or reflection ) which is not the identity ; 3 .   apply this symmetry to one of the two sub - walks created by splitting the walk at the pivot site ; 4 .",
    "if the resulting walk is self - avoiding : _",
    "accept _ the pivot and update the configuration ; 5 .   if the resulting walk is not self - avoiding : _ reject _ the pivot and keep the old configuration ; 6 .",
    "repeat .",
    "the pivot algorithm is ergodic , and satisfies the detailed balance condition which ensures that self - avoiding walks are sampled uniformly at random  @xcite .",
    "successful pivot moves make large changes to global observables which measure the size of a walk , and madras and sokal  @xcite argued that in fact the integrated autocorrelation time , @xmath37 , for such observables was of the same order as the mean time for a successful pivot to occur . for the simple cubic lattice the probability of a pivot move being successful",
    "is @xmath38 with @xmath39 , which leads to @xmath40 for global observables .",
    "madras and sokal  @xcite gave a hash table implementation of the pivot algorithm which resulted in mean cpu time of @xmath41 per pivot attempt ( alternatively , cpu time @xmath42 per successful pivot ) .",
    "this has since been improved by kennedy  @xcite to roughly mean cpu time of @xmath43 per pivot attempt , and further still by the present author  @xcite to @xmath44 .",
    "this makes the pivot algorithm extremely efficient for sampling global observables for self - avoiding walks , but it is not obvious how efficient it is for sampling our observable @xmath27 .      we now proceed to calculate the autocorrelation function for the markov chain sampling of the observable @xmath27 for different choices of pivot site distribution .    as @xmath27",
    "is either 0 or 1 , we can write down a closed form expression for its variance in terms of its expectation : @xmath45 then , following @xcite , we define the autocorrelation function for the time series measurement of our observable @xmath27 as @xmath46 the integrated autocorrelation time for @xmath27 , @xmath47 , is given in terms of @xmath48 as @xmath49 which then enters the expression for the standard deviation of the estimate of the expectation of @xmath27 after @xmath50 markov chain time steps : @xmath51 @xmath47 may be thought of as the number of markov chain time steps to reach an effectively new configuration with respect to @xmath27 .",
    "it is clear from fig .",
    "[ fig : concatenation ] that the shape of each of the walks close to the joint is crucially important with respect to the probability of intersection , whereas the shape of the walks at their far ends will have almost negligible effect on the intersection probability .",
    "in fact , if on the square lattice either walk is like that of fig .  [",
    "fig : trapped ] , then an intersection _ must _ occur regardless of the shapes of the remainders of the walks .",
    "in @xcite we argued that sampling pivot sites uniformly at random would lead to configurations like that in fig .",
    "[ fig : trapped ] being frozen for @xmath42 markov chain time steps , and this in turn would lead to @xmath52 .",
    "    u    in  @xcite we argued that sampling pivot sites uniformly at all length scales with respect to the distance to the concatenated ends would dramatically reduce the integrated autocorrelation time , and conjectured that in this case @xmath53 .",
    "we will further test this assumption that scale - free moves drastically reduce the integrated autocorrelation time by directly calculating the autocorrelation function , and also by estimating the integrated autocorrelation time .",
    "we calculated the autocorrelation function for three separate choices of pivot site distribution . in each case",
    "we initialized the system as follows :",
    "1 .   use the pseudo_dimerize procedure of @xcite to generate two initial @xmath2-step saw configurations .",
    ".   initialize markov chain by performing at least @xmath54 successful pivots on each saw .",
    "pivot sites are sampled uniformly at random .",
    "the stopping criterion must be based on the number of attempted pivots so as not to introduce bias .",
    "our sampling procedure for the uniform pivot site distribution case was :    1 .",
    "select one of the two walks uniformly at random .",
    "2 .   select a pivot site on this walk by selecting a pivot site uniformly at random in the interval @xmath55 $ ] .",
    "attempt pivot move , applied to the free end of the walk ; update walk if result is self - avoiding .",
    "calculate @xmath56 , and update our estimate of @xmath57 .",
    "repeat .",
    "the procedure with log uniform sampling was :    1 .",
    "select one of the two walks uniformly at random .",
    "2 .   select a pivot site on this walk by generating a pseudorandom number @xmath8 between 0 and @xmath58 , and let pivot site @xmath59 , so that @xmath60 $ ] .",
    "attempt pivot move , applied to the free end of the walk ; update walk if result is self - avoiding .",
    "calculate @xmath56 , and update our estimate of @xmath57 .",
    "repeat .",
    "finally , the procedure with log uniform sampling plus global rotations ( which we denote log+ ) was :    1 .   select one of the two walks uniformly at random .",
    "2 .   randomly pivot each of the walks around the innermost sites , i.e. those with label 0 .",
    "( these pivot moves are always successful . )",
    "3 .   select a pivot site on this walk by generating a pseudorandom number @xmath8 between 0 and @xmath61 , and let pivot site @xmath62 , so that @xmath63 $ ] .",
    "attempt pivot move , applied to the free end of the walk ; update walk if result is self - avoiding .",
    "calculate @xmath56 , and update our estimate of @xmath57 .",
    "repeat .",
    "we refer to the log and log+ sampling distributions as `` scale - free '' because pivot sites are sampled uniformly at all possible length scales with respect to the distance to the concatenation sites .",
    "we calculated the autocorrelation function @xmath48 for the uniform , log , and log+ procedures , for walks of length @xmath64 ( 1000 sites ) and @xmath65 ( @xmath66 sites ) , by running simulations of the markov chains , and collecting information about correlations at 40 different time intervals between 1 and 1048576 .",
    "we make log - log plots of @xmath48 against @xmath67 for @xmath64 in fig .",
    "[ fig : shortautocorrelation ] and for @xmath65 in fig .",
    "[ fig : longautocorrelation ] , so that we can see the behaviour over many time scales simultaneously . for regimes where @xmath48 is decaying as a power law @xmath68 with @xmath69",
    ", we expect that the plot will be linear with slope @xmath70 , whereas when @xmath48 is decaying exponentially we expect to see the plot sharply decreasing , as @xmath71 which implies that @xmath72 will grow exponentially rapidly towards negative infinity as a function of @xmath73 .    in figs  [ fig : shortautocorrelation ] and [ fig :",
    "longautocorrelation ] it can be seen that when pivot sites are selected uniformly the autocorrelation function decays slowly until @xmath67 is of the same order as @xmath2 ( i.e. to within a constant factor ) , and then decays exponentially .",
    "it is possible that @xmath74 is the only important timescale for this markov chain . for the log sampling procedure ,",
    "we see rapid decay which appears to be approaching a straight line , and so is consistent with a power law .",
    "decay in the autocorrelation function is dramatically faster than for uniform sampling , as expected .",
    "finally , for the log+ sampling scheme we see a dramatic drop for the first markov chain time step , due to the use of global rotations which causes initially rapid decorrelation , and thereafter it decays in a similar manner to the log sampling scheme .",
    "in fact , for large @xmath67 we expect that @xmath48 will be the same for log and log+ , as for long times it becomes increasingly likely that global rotations have occurred for each walk under the log sampling procedure .",
    "thus log and log+ will behave similarly in terms of asymptotic performance , but the steep initial drop in the autocorrelation function makes it clear that log+ will better by a not - insignificant constant factor for lengths which are accessible to computer experiments .     for uniform and logarithmic choices of pivot site distribution for @xmath64 .",
    "[ fig : shortautocorrelation],scaledwidth=40.0% ]     for uniform and logarithmic choices of pivot site distribution for @xmath65 .",
    "[ fig : longautocorrelation],scaledwidth=40.0% ]      to extract information about @xmath0 from ( [ eq : bn ] ) we must estimate @xmath57 in the large @xmath2 limit in order to reduce the influence of corrections - to - scaling .",
    "we sample pairs of self - avoiding walks using the pivot algorithm and we invest computational resources approximately uniformly on a wide range of length scales , from @xmath75 to @xmath76 .",
    "the situation is quite different for the calculation of @xmath18 in  @xcite , for which a near - optimal design for the computer experiment required almost all computational effort to be expended on sampling short walks .",
    "the log+ procedure was very similar to the method used for the main computer experiment .",
    "however , the main computer experiment was slightly sub - optimal in two ways : ( a ) it was possible for the log uniform sampling to select the sites labeled 0 , and ( b ) one of the two global pivot moves allowed for the identity symmetry .",
    "each of these differences result in slightly worse performance , and for future numerical experiments the log+ procedure will be used ( unless a procedure that is better still can be devised ) .",
    "the computer experiment was run for 200 thousand cpu hours on dell poweredge fc630 machines with intel xeon e5 - 2680 cpus ( these were run in hyperthreaded mode which gave a modest performance boost ; 400 thousand cpu thread hours were used ) . in total",
    "there were @xmath77 batches of @xmath78 attempted pivots , and thus there were a grand total of @xmath79 attempted pivots across all walk sizes .",
    "we report our results for @xmath57 in table  [ tab : data ] of appendix  [ sec : data ] .    in fig .",
    "[ fig : tint ] we plot estimates for @xmath80 which we obtain indirectly from ( [ eq : stdev ] ) , inferring it from batch estimates of the error in table  [ tab : data ] .",
    "the accuracy of this technique relies on the assumption that the batch error estimate is accurate , which in turn relies upon the degree of correlation between successive batches being negligible . for the batch sizes of @xmath78 used in this computer experiment",
    "this condition is undoubtedly satisfied . in the plot of @xmath80",
    "we see , remarkably , that over the range of @xmath2 plotted @xmath81 is growing less quickly than @xmath82 !",
    "this is significantly smaller than the @xmath83 behaviour postulated in our earlier paper  @xcite .",
    "it may be that fig .",
    "[ fig : tint ] does not capture the asymptotic regime , perhaps due to the steep initial decline in @xmath48 which is apparent for the log+ procedure in figs  [ fig : shortautocorrelation ] and [ fig : longautocorrelation ] .",
    "however , it is possible , perhaps even plausible , that @xmath84 , and it certainly seems highly probable that @xmath85 .    .",
    "these data are from the full monte carlo computer experiment and are calculated via the batch method .",
    "[ fig : tint],scaledwidth=50.0% ]    we now proceed to analyze our data for @xmath57 to extract estimates for the critical exponent @xmath0 and amplitude @xmath19 via ( [ eq : asymptotic ] ) .",
    "we utilize an improved observable , similarly to  @xcite , and more recently  @xcite .",
    "the idea is to combine our estimates for @xmath57 with estimates from another observable , so as to create a new improved observable for which the amplitude of the leading correction - to - scaling term is negligible . for this purpose",
    "we use the estimates of the ratio of the mean - squared end - to - end distance and the mean - squared radius of gyration , @xmath86 , from table iv of appendix b of  @xcite .",
    "note that @xmath2 in that table refers to the number of sites , whereas here our @xmath2 refers to the number of steps , which is one fewer , and so these lengths are in one - to - one correspondence .",
    "the expected asymptotic form of this ratio is @xmath87 note that asymptotically this ratio of observables is a pure number , namely the universal amplitude ratio @xmath88 .",
    "we now form the observable @xmath89 , which involves an arbitrary constant @xmath90 which we will choose a value for shortly . from ( [ eq : bn ] ) and ( [ eq : rerg ] ) we determine the asymptotic form of our new observable : @xmath91^\\kappa \\\\     & = \\frac{2^{\\gamma-1}\\mu}{a }   \\left(\\frac{{d_{\\mathrm{e}}}}{{d_{\\mathrm{g}}}}\\right)^\\kappa n^{1-\\gamma } \\left(1 +      \\frac{b - d\\kappa}{n^{\\delta_1 } } + o\\left(\\frac{1}{n}\\right ) \\right ) \\\\     & = k n^{1-\\gamma } \\left(1 +      \\frac{b - d\\kappa}{n^{\\delta_1 } } + o\\left(\\frac{1}{n}\\right ) \\right ) ,      \\label{eq : improved}\\end{aligned}\\ ] ] taking @xmath92 . thus it becomes apparent that if we choose @xmath90 judiciously so that @xmath93 , then our observable will have negligible leading - order correction - to - scaling . in this case",
    "we say that the new observable is `` improved '' with respect to the original observable @xmath57 .",
    "our analysis was completed as follows .",
    "we fixed @xmath90 at an arbitrary value ( initially 0 ) , and calculated estimates of the new observable @xmath94 , with confidence intervals , from our data for @xmath57 in table  [ tab : data ] of appendix  [ sec : data ] , and the data for @xmath95 in table iv of appendix b of  @xcite .",
    "we then performed weighted non - linear fits of these data using the statistical programming language r , where our statistical model was a single power law of the form @xmath96 .",
    "we truncated our data by only fitting values with @xmath97 , varying @xmath98 to get a sequence of estimates for which we expect the systematic error due to unfitted corrections - to - scaling to decrease . to determine a near - optimal choice of @xmath90",
    ", we varied @xmath90 and calculated the reduced @xmath99 for these fits , eventually settling on a value of @xmath100 for which the reduced @xmath99 was approximately one for all @xmath101 .",
    "these fits with @xmath100 gave a sequence of estimates for @xmath102 ( which we converted to estimates of @xmath0 ) and @xmath103 from ( [ eq : improved ] ) .",
    "we plot these estimates in figs  [ fig : gamma ] and [ fig : amplitude ] respectively , against @xmath104 as this is the expected order of magnitude of the systematic error .",
    "this choice of variable for the @xmath8-axis should result in linear convergence as the asymptotic regime is reached ; we extrapolate the fits from the right to where they intersect the @xmath105-axis which corresponds to the @xmath106 limit .",
    ", with the weighted least squares linear fit of the last six values shown .",
    "our best estimate @xmath107 is shown in bold on the @xmath105-axis .",
    "[ fig : amplitude],scaledwidth=100.0% ]    , with the weighted least squares linear fit of the last six values shown .",
    "our best estimate @xmath107 is shown in bold on the @xmath105-axis .",
    "[ fig : amplitude],scaledwidth=100.0% ]    we have extrapolated these sequences of estimates to obtain @xmath108 and @xmath107 . using our estimates for @xmath103 and @xmath0 , together with estimates of @xmath109  @xcite and @xmath110  @xcite , we obtain @xmath111 . the dominant contribution to",
    "the error of this estimate comes from @xmath103 .",
    "we note that analysis of results from a previous computer experiment with poorer statistics gave @xmath112 , where the method of analysis used the non - improved observable @xmath57 .",
    "this is consistent with but much less precise than the final estimate reported here .",
    "this unpublished value was used in the estimation of critical exponents @xmath113 , for self - avoiding walks tethered to a surface , and @xmath114 , for bridges  @xcite .",
    "we compare our estimates for @xmath0 and @xmath19 with previous estimates in table  [ tab : parameters ] , and see that the new estimates significantly improve on the state of the art .",
    "we make the observation that estimates for @xmath0 have trended downwards over time , both for the series and monte carlo estimates , which is perhaps symptomatic of the fact that the systematic influence corrections - to - scaling have diminished as data for larger @xmath2 has become available .",
    "the most recent series estimates have @xmath115 , while this paper provides monte carlo data up to @xmath116 .",
    ".comparison of parameter estimates . [ cols= \" < ,",
    "< , < , < \" , ]     besides the estimates for @xmath0 and @xmath19 , our other main results are the striking evidence in figs  [ fig : shortautocorrelation ] and [ fig : longautocorrelation ] of the efficiency gain of scale - free sampling versus uniform sampling , and evidence from fig .  [ fig : tint ] which suggests that @xmath117 for the log+ markov chain algorithm",
    ".    the scale - free move framework described here could be applied equally as well to other global monte carlo moves besides the pivot move , in particular to cut - and - paste moves  @xcite .",
    "we expect scale - free moves to also be useful when simulating polymers which satisfy a geometric restriction , as has already proved to be the case for self - avoiding walks tethered to a hard surface  @xcite .",
    "equally , it could be useful for the sampling of branched polymers where the distances to internal joints introduce additional internal length scales .",
    "one major advantage of the scale - free approach is that it is not necessary to decide which length scale is important .",
    "suppose , for the sake of argument , that for a given system optimal efficiency is attained by performing moves at one particular length scale . since the scale - free framework performs moves at all length scales , including the important one , the penalty of using the scale - free algorithm is at most @xmath118 in the integrated autocorrelation time , and @xmath119 in the error .",
    "support from the australian research council under the future fellowship scheme ( project number ft130100972 ) and discovery scheme ( project number dp140101110 ) is gratefully acknowledged .",
    "rrrr & & &         + 1023 & 1.4507968(16 ) & 65535 & 0.7536518(22 ) + 1447 & 1.3734488(17 ) & 92671 & 0.7137264(22 ) + 2047 & 1.3002643(17 ) & 131071 & 0.6759013(22 ) + 2895 & 1.2310935(18 ) & 185343 & 0.6401084(22 ) + 4095 & 1.1656136(19 ) & 262143 & 0.6061940(23 ) + 5791 & 1.1037063(19 ) & 524287 & 0.5436837(23 ) + 8191 & 1.0450800(20 ) & 1048575 & 0.4876280(23 ) + 11583 & 0.9896313(20 ) & 2097151 & 0.4373552(23 ) + 16383 & 0.9371139(20 ) & 4194303 & 0.3922662(23 ) + 23167 & 0.8874326(21 ) & 8388607 & 0.3518267(23 ) + 32767 & 0.8403684(21 ) & 16777215 & 0.3155514(23 ) + 46335 & 0.7958358(22 ) & 33554431 & 0.2830274(22 ) +"
  ],
  "abstract_text": [
    "<S> we implement a scale - free version of the pivot algorithm and use it to sample pairs of three - dimensional self - avoiding walks , for the purpose of efficiently calculating an observable that corresponds to the probability that pairs of self - avoiding walks remain self - avoiding when they are concatenated . </S>",
    "<S> we study the properties of this markov chain , and then use it to find the critical exponent @xmath0 for self - avoiding walks to unprecedented accuracy . </S>",
    "<S> our final estimate for @xmath0 is @xmath1 .    </S>",
    "<S> * keywords * self - avoiding walk ; critical exponent ; monte carlo ; pivot algorithm    = 1 </S>"
  ]
}