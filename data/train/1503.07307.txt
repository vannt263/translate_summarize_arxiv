{
  "article_text": [
    "integrated nested laplace approximations ( inla ) were introduced by  @xcite as a tool to do approximate bayesian inference in latent gaussian models ( lgms ) .",
    "the class of lgms covers a large part of models used today , and the inla approach has been shown to be very accurate and extremely fast in most cases .",
    "software is provided through the r - inla package , see http://www.r-inla.org .",
    "an important subclass of lgms is the rich family of generalized linear mixed models ( glmms ) with gaussian priors on fixed and random effects  @xcite .",
    "the use of inla for bayesian inference for glmms was investigated by  @xcite , who reanalyzed all of the examples from  @xcite .",
    "@xcite found that inla works very well in most cases , but one of their examples shows some inaccuracy for binary data with few or no replications . in this paper , we introduce a new correction term for inla , significantly improving accuracy while adding negligibly to the overall computational cost .        to set the scene , we consider a minimal simulated example illustrating the problem ( postponing more thorough empirical evaluations until section  [ sec : examples ] ) .",
    "consider the following model : for @xmath0 , let @xmath1 , @xmath2 , and @xmath3 where @xmath4 , iid .",
    "let the precision @xmath5 have a @xmath6 prior , while the prior for @xmath7 is @xmath8 .",
    "we simulated data from this model , setting @xmath9 , @xmath10 and @xmath11 .",
    "figure  [ figure1 ] shows the resulting posterior distributions for the intercept @xmath7 and for the log precision , @xmath12 , where the histograms show results from long mcmc runs using jags  @xcite , the black curves show posteriors from inla without any correction , and the red curves show results using the new correction defined in section  [ sec : method ] .",
    "while some of our later examples show more dramatic differences between inla and long mcmc runs , these results exemplify quite well our general experience with using inla for `` difficult '' binary response glmms : variances of both random and fixed effects tends to be underestimated , while the means of the fixed effects are reasonably well estimated .",
    "one part of the problem is that the usual assumptions ensuring asymptotic validity of the laplace approximation do not hold here ( for details on asymptotic results , see the discussion in section 4 of @xcite ) .",
    "the independence of the random effects make the effective number of parameters  @xcite on the order of the number of data points . in more complex models",
    ", there is often some amount of smoothing or replication that alleviates the problem , but it may still occur . except in the case of spline smoothing models  @xcite , there is a lack of strong asymptotic results for random effects models with a large effective number of parameters . in the simulation from model  ,",
    "the data provide little information about the parameters , with the shape of the likelihood function adding to the problem .",
    "figure  [ figure2 ] illustrates the general problem . here , the top panel shows the log - likelihood of a single bernoulli observation @xmath13 as a function of the linear predictor @xmath14 , i.e.  @xmath15 where @xmath16 . the bottom panel shows the corresponding derivative .",
    "we see that the log - likelihood gets very flat ( and the derivative near zero ) for high values of @xmath14 , so inference will get difficult .    bayesian and frequentist estimation for glmms with binary outcomes has been given some attention in the recent literature  @xcite , but a computationally efficient bayesian solution appropriate for the inla approach has been lacking .",
    "an alternative to our new approach would be to consider higher - order laplace approximations  @xcite , other modifications to the laplace approximation  @xcite , or expectation propagation - type solutions  @xcite , but we view them as too costly to be applicable for general use in inla .",
    "the motivation for using inla is speed , so we see it as a design requirement for any correction that it should add minimally to the running time of the algorithm .",
    "we proceed as follows . in section  [",
    "sec : method ] , we present a derivation of our new correction method .",
    "section  [ sec : examples ] presents empirical studies , both on real and simulated data , showing that the method works well in practice .",
    "finally section  [ sec : conclusion ] gives a brief discussion and some concluding remarks .",
    "consider a latent gaussian model @xcite , with hyperparameters @xmath17 , latent field @xmath18 and observed data @xmath19 ( for @xmath20 ) , where the joint distribution may be written as @xmath21 where @xmath22 is a multivariate gaussian density .",
    "we want to approximate the posterior marginals @xmath23 and @xmath24 . the laplace approximation of @xmath25 is @xmath26 where @xmath27 is a gaussian approximation found by matching the mode and",
    "the curvature at the mode of @xmath28 , and @xmath29 is the mean of the gaussian approximation .",
    "given @xmath25 and some approximation @xmath30 ( see below ) , the posterior marginals of interest are calculated using numerical integration : @xmath31    in the current implementation of inla , the @xmath32 used in   are approximated using skew normal densities @xmath33  @xcite , based on a second laplace approximation ; see section 3.2.3 of  @xcite for details . notice",
    "that , in equation   we use a gaussian approximation @xmath27 , with marginals @xmath34 .",
    "thus , both @xmath35 and @xmath27 are approximations to the marginals @xmath36 , but the @xmath37 are more accurate since they are based on a second laplace approximation . in",
    "we need to approximate the full joint distribution @xmath38 .",
    "our basic idea is to use the improved approximations @xmath39 in order to construct a better approximation to the joint distribution @xmath38 .",
    "we aim for an approximation of @xmath38 that retains the dependence structure of the gaussian approximation @xmath40 , while having the improved marginals @xmath39 .",
    "this can be achieved by using a gaussian copula .",
    "before we describe the copula construction , we need to define some notation . first",
    ", for @xmath41 , let @xmath42 and @xmath43 denote the mean and variance of each marginal @xmath44 , and let @xmath45 be the cumulative distribution function corresponding to @xmath46 .",
    "second , let @xmath47 and assume that @xmath48 is the distribution of @xmath49 . as usual , @xmath50 denotes the cumulative standard gaussian distribution function .",
    "furthermore , let @xmath51 and @xmath52 denote the marginal means and variances of the gaussian approximation @xmath27 , let @xmath53 be the precision matrix of @xmath54 , and let @xmath55 where @xmath56 , and define @xmath57 .",
    "note that we have @xmath58 from the definition of @xmath44 ( the construction of the skew normal changes the mean and adds skewness , but keeps the variance unchanged ; again , see section 3.2.3 of  @xcite for detailed explanations ) , so from here on we denote both simply by @xmath52 .",
    "we will now show how to construct a joint distribution having marginals @xmath45 and the dependence structure from @xmath59 , using a gaussian copula ( see e.g.  @xcite for a general introduction ) .",
    "first , note that @xmath60 $ ] by the probability integral transform ( pit ) .",
    "let @xmath61 . applying the inverse of the pit",
    "then yields that @xmath62 , from which it follows that @xmath63 is distributed as @xmath47 , which is the marginal distribution we want .",
    "since we have only done marginal transformations , the dependence structure of the original @xmath64 is still intact .",
    "thus , to construct the new approximation to the joint distribution @xmath65 , we define the transformed value @xmath66 as follows : @xmath67 + \\tilde\\mu _",
    "i(\\theta ) .",
    "\\label{eq : copula}\\ ] ]    we may simplify the construction above by replacing the @xmath48 in   by @xmath50 .",
    "this means that we do not correct for skewness , but we take advantage of the improved mean @xmath42 from @xmath68 . we denote this as the `` mean only '' correction .",
    "( we will later discuss the possibility of retaining @xmath48 as a skew normal ; this we denote as the `` mean and skewness '' correction . ) in the simple `` mean only '' case , the transformation reduces to a shift in mean : @xmath69 the jacobian is equal to one , and the transformed joint density function is a multivariate normal with mean @xmath70 and precision matrix @xmath71 , i.e. @xmath72 in the laplace approximation defined in equation  , both the numerator and the denominator should be evaluated in the point @xmath73 , where @xmath74 is the mean of the gaussian approximation @xmath40 .",
    "thus , the density functions above should be evaluated in @xmath75 . from equation  ,",
    "the original ( uncorrected ) log posterior is @xmath76 evaluated at @xmath77 , where @xmath78 comparing equations  , , and , we see that the copula approximation can be implemented by adding the term @xmath79 to the already calculated log posterior evaluated at @xmath29 , where @xmath80 the addition of the term @xmath79 does not add significantly to the computational cost of inla  this simple operation is essentially free .    for the inla implementation of the copula correction",
    ", we have found that it is sufficient to only include fixed effects ( including any random effects of length one ) in the calculation of @xmath79 .",
    "the effect of the correction is strongest and most consistent for the fixed effects , while the ( often very numerous ) random effects contribute very small individual effects to the correction , mainly adding extraneous noise to the estimation . for these reasons ,",
    "including only fixed effects gives better numerical stability and also seems to provide a more accurate approximation , while reducing computational costs .",
    "conceptually , including only the fixed effects involves finding @xmath81 , and then again finding @xmath82 ( where @xmath83 is the index set of the fixed effects ) , which might seem computationally costly .",
    "however , it can be done cheaply by using the linear combination feature described in section 4.4 of  @xcite : if @xmath84 is the number of fixed effects , only the ( parallel ) solution of a @xmath84-dimensional linear system is needed .",
    "additionally , to guard against over - correction , we perform a soft thresholding on @xmath79 , as follows : first we define a sigmoidal function @xmath85 : @xmath86 which is increasing , has derivative equal to one at the point @xmath87 , and where @xmath88 as @xmath89 .",
    "then we replace @xmath79 by @xmath90 , where @xmath91 for @xmath92 and with the `` correction factor '' parameter @xmath93 determining the degree of shrinkage ( more shrinkage for smaller values of @xmath94 ) .",
    "since the function @xmath85 is approximately linear with unit slope around zero , @xmath90 will be close to @xmath79 for small and moderate values of @xmath79 , while larger values will be increasingly shrunk toward zero .",
    "note that since @xmath95 for all @xmath96 , @xmath97 .",
    "the value of @xmath94 does not have a large impact on the results unless a too small value is chosen .",
    "its main purpose is as a safeguard to avoid too large corrections in very difficult cases . in our experience",
    "@xmath98 gives too strong shrinkage , while for example @xmath99 corresponds to no shrinkage , so it seems clear that @xmath94 should be somewhere in between these extremes .",
    "we have found that @xmath100 is a good choice , letting the correction do its job while guarding against too large changes , and we have used this value for all of the examples .",
    "results appear to be very robust to the exact value chosen for @xmath94 .",
    "note also that since the correction effect @xmath101 is scaled with the number @xmath84 of fixed effects , it is less surprising that a single value for @xmath94 could work well in a wide variety of circumstances .",
    "as mentioned , we have also investigated a more general case of the copula construction , where we retain @xmath45 as a skew normal distribution , i.e.  the cdf of @xmath44 .",
    "this results in a more complicated correction term @xmath102 , derived in appendix  [ app ] .",
    "we have not found any appreciable differences in the accuracy compared to the simpler case without skewness , so we have concluded that the non - skew version is preferable due to its simplicity .",
    "we will show both the skew and the non - skew correction for the toenail data discussed in section  [ sec : toenail ] , but otherwise we show only results from the simpler non - skew version .",
    "we have tried both corrections on many ( both real and simulated ) data sets , and never seen a significant difference in the results .",
    "as mentioned in the introduction , @xcite studied the use of inla for binary valued glmms , and they showed that the approximations were inaccurate in some cases .",
    "we have redone the simulation experiment described on pages 1014 of the supplementary material of  @xcite , both for inla without any correction , and inla with the `` mean only '' correction described in section  [ sec : method ] .    in the original simulation study by @xcite , @xmath103 are iid @xmath104 , with @xmath105 clusters , @xmath106 observations per cluster , and @xmath107 . given @xmath108 for @xmath109 and @xmath110 otherwise , and sampling times @xmath111 , the following two models were considered : @xmath112 which corresponds to models ( 0.7 ) and ( 0.8 ) on page 11 of the supplementary material of  @xcite .",
    "we first consider model  .",
    "we only show the results for @xmath113 ( i.e. binary data ) , as this is the most difficult case with the largest errors in the approximation .",
    "the correction also works well for @xmath114 , but this case is easier to deal with for inla .",
    "this is seen empirically , and is also as expected based on considering the asymptotic properties of the laplace approximation : for @xmath114 there is more `` borrowing of strength''/replication , so the original approximation should work better .",
    "we use the same settings as @xcite : @xmath115 where @xmath116 , the prior @xmath117 for @xmath118 and @xmath119 priors for the @xmath120 .",
    "the true values of the fixed effects are @xmath121 .",
    "we made 1,000 simulated data sets , running inla both with and without the new correction , as well as very long mcmc chains using jags ( each of the 1,000 datasets were run with 1,000,000 mcmc samples after a burn - in of 100,000 , using every 100th sample ) .",
    "@|llrrrrrrrr|@ + & & & & & & & & & + & true values & 1.000 & 1.000 & 0.000 & -2.500 & 1.000 & -1.000 & -0.500 & + & uncorrected inla & 0.705 & 0.722 & 1.133 & -2.494 & 0.998 & -1.052 & -0.486 & + & corrected inla & 0.952 & 0.850 & 0.775 & -2.562 & 1.024 & -1.080 & -0.504 & + & mcmc & 0.946 & 0.849 & 0.773 & -2.537 & 1.017 & -1.081 & -0.482 & +   +   + & & & & & & & & & + & uncorrected inla & -0.382 & -0.403 & 0.390 & 0.120 & -0.127 & 0.052 & -0.016 & + & corrected inla & -0.003 & 0.000 & 0.002 & -0.073 & 0.046 & -0.002 & -0.101 & +   +   + & & & & & & & & & + & uncorrected inla & 0.585 & 0.812 & 1.174 & 0.822 & 0.834 & 0.882 & 0.889 & + & corrected inla & 0.933 & 0.956 & 0.998 & 0.904 & 0.871 & 0.943 & 0.908 & +   +   + & & & & & & & & & + & uncorrected inla & 90.3% & 90.0% & 90.8% & 92.6% & 92.7% & 93.5% & 93.5% & + & corrected inla & 94.2% & 93.9% & 94.4% & 93.5% & 93.1% & 94.3% & 93.7% & +    results from the simulation study are shown in table  [ table1 ] . note that the aim here is to be as close as possible to the mcmc results , not the true values .",
    "the upper part of the table shows averages of the posterior means over the 1,000 simulations .",
    "we see that inla gets much closer to the mcmc results for all parameters except @xmath122 , which is in any case reasonably close to the mcmc value .",
    "the improvement is particularly large for the variance parameter .",
    "this is also seen in the second panel , which shows @xmath123 for each parameter @xmath124 ( averaged over the 1,000 simulations ) , i.e.  the difference in inla and mcmc estimates scaled by the mcmc standard deviation . here ,",
    "the random effects variance and the fixed effects except @xmath122 are also more accurately estimated .",
    "the third lower panel shows the ratios @xmath125 ; here the correction improves the estimation of the variance for all parameters . for @xmath126 , @xmath127 and @xmath128",
    "we get very close to a ratio of one , and there are also major improvements for the fixed effects variances . finally , the bottom panel shows average coverage of 95% ( i.e. , @xmath129 ) credible intervals from inla over the mcmc samples for each simulated data set . clearly , coverage is improved considerably by the correction .",
    "table  [ table2 ] reports summary statistics for the computation times in seconds over the 1,000 data sets .",
    "note that in this case computational times are abnormally high due to somewhat extreme parameter settings  inla will usually be much faster .",
    "however , ratios of computing times for the corrected vs the uncorrected versions should stay approximately the same .",
    "appendix  [ appsim ] contains additional simulation studies : results from simulations for model   for different values of the covariance matrix of @xmath130 are shown in appendix  [ sec : model - with - two ] .",
    "furthermore , in appendix  [ sec : model - with - very ] we consider the effect of having extremely few observations per cluster , while we in appendix  [ sec : simul - with - missp ] study a misspecified model , simulating from model   while estimating model  .",
    "the correction appears to work well for all the different cases considered in appendix  [ appsim ] .",
    ".summary statistics for computation times in seconds for each data set [ cols=\"<,^,^,^,^,^,^\",options=\"header \" , ]          we start by discussing the toenail data , which is a classical data set with a binary response and repeated measures  @xcite .",
    "the data are from a clinical trial comparing two competing treatments for toenail infection ( dermatophyte onychomycosis ) .",
    "the 294 patients were randomized to receive either itraconazole or terbinafine , and the outcome ( either `` not severe '' infection , coded as @xmath131 , or `` severe '' infection , coded as @xmath132 ) was recorded at seven follow - up visits .",
    "not all patients attended all the follow - up visits , and the patients did not always appear at the scheduled time .",
    "the exact time of the visits ( in months since baseline ) was recorded . for individual @xmath133 ,",
    "visit @xmath134 , with outcome @xmath135 , treatment @xmath136 and time @xmath137 our model is then @xmath138 \\text{logit } \\",
    "p_{ij } & = & \\alpha_0 + \\alpha_{\\text{trt } } \\text{trt}_i + \\alpha_{\\text{time } } \\text{time}_{ij } + \\alpha_{tt } \\text{trt}_i \\text { time}_{ij } + b_i\\\\[-1pt ] b_i & \\sim & n(0 , \\sigma^2).\\end{aligned}\\]]notice that this is the same model as model  , except that the time variable here varies over individuals .",
    "normal priors with mean zero and variance @xmath139 were used for @xmath140 , @xmath141 , @xmath142 , and @xmath143 .",
    "inla underestimates @xmath144 quite severely .",
    "the top panel of figure  [ figure3 ] shows the different estimates of the posterior distribution of the log precision , @xmath145 .",
    "the histogram shows the results from a long mcmc run using jags , the black curve shows the posterior from inla without the correction , the red curve shows the simple ( non - skew ) version of the inla correction , while the green curve shows the inla correction accounting for skewness ( as discussed in appendix [ app ] ) .",
    "the bottom panel of figure  [ figure3 ] shows the additive correction to the log posterior density , as a function of the hyperparameter ( log precision ) .",
    "we see that there is very little difference between the two corrections .    for the toenail data , the estimated random effects standard deviation is approximately @xmath146 , which is very high .",
    "to investigate how the copula correction works as @xmath147 increases , we studied simulated data sets from the model above , where we set @xmath147 to different values , and where the @xmath148 parameters were fixed to the values from a long mcmc run using the real toenail data ( i.e. , we simulate only the outcome , keeping the covariates unchanged ) .",
    "results are shown in figure  [ figure4 ] for different values of @xmath147 ranging from @xmath149 to @xmath150 .",
    "we clearly get very accurate corrected posteriors for @xmath151 . for @xmath152",
    ", we gradually get a tendency of under - correction .",
    "( the value of @xmath147 is shown above each histogram ) .",
    "the histograms are from long mcmc runs , uncorrected inla are shown as black curves , while the red curves shows inla with the correction . since the goal here is to study the difference between mcmc and inla , we omit axes ",
    "the relevant scale is given by the mcmc variances , which are evident from the histograms . ]",
    "we shall now study the case where the data are poisson distributed .",
    "we consider a simple simulated ( extreme ) example in order to investigate how well the correction works in the poisson case .",
    "for @xmath153 we generated iid @xmath154 where @xmath155 with @xmath4 .",
    "we chose @xmath156 and @xmath10 , a @xmath157 prior for the precision @xmath158 , and a @xmath8 prior for @xmath7 .",
    "figure  [ figure5 ] shows the results for different values of the intercept @xmath7 .",
    "each histogram is based on ten parallell mcmc runs using jags , each with 200,000 iterations after a burn - in of 100,000 . here ,",
    "reduction of @xmath7 implies that estimation is more difficult , since negative @xmath7 with large absolute value will imply that the counts @xmath159 are very low , with many zeroes , and the data are uninformative .",
    "we see that uncorrected inla tends to get less accurate as @xmath7 moves towards more extreme values , while the correction seems to work well .",
    "( the value of @xmath7 is shown above each histogram ) .",
    "the histograms are from long mcmc runs , uncorrected inla posteriors are shown as black curves , while the red curves shows inla posteriors with the correction . ]      until now , we have considered fairly simple latent structures , where the random effects have been iid ( multivariate ) normally distributed .",
    "the reader may perhaps wonder if the generality implied by having `` latent gaussian models '' in the title is really justified ",
    "what if latent structures are more complicated , with for example temporal or spatial structure ?",
    "in fact , the complexity of the latent field is not particularly relevant for the accuracy problems we study here .",
    "this can be seen by considering the basic formulation of the latent gaussian model together with the main building blocks of the inla machinery : essentially , the latent structure is contained within the gaussian part , for which the computations are exact ( and fast , since the precision matrix of the gaussian part will usually be sparse ) . in a sense , the lgm approach separates the estimation in an `` easy '' ( gaussian ) part and a `` difficult '' part .",
    "it is perhaps somewhat counterintuitive at first sight that the dynamic / time - series / spatial model constitutes the `` easy '' part ! in this paper we have in fact considered the `` difficult '' part , aiming to choose examples at the boundary of what we considered to be feasible .",
    "thus , we argue that our general title is indeed justified .    we illustrate this with a simple simulated example where the latent structure is auto - regressive of order one ( ar1 ) , using a similar setup as in the `` minimal '' example presented in the introduction . for @xmath0 ,",
    "let @xmath1 , @xmath2 , and @xmath160 where the @xmath161 are now given an ar1 model , as follows : @xmath162 , @xmath163 ( where @xmath164 ) for @xmath165 , where @xmath166 is the marginal precision .",
    "define @xmath167 and @xmath168 which is the parameterization used internally in inla .",
    "we use a gamma@xmath169 prior for @xmath170 , and @xmath8 priors for both @xmath7 and @xmath171 .",
    "data was simulated from model  , using @xmath172 , @xmath173 and @xmath174 as the true values . as in the example in the introduction , we made long mcmc runs and compared the results to inla both with and without the correction .",
    "the results are shown in figure  [ figure6 ] .",
    "again , it is clear that the overall accuracy of inla is improved using the correction .",
    ", the middle panel shows results for @xmath175 , and the bottom panel show results for @xmath176 , the `` internal @xmath177 '' of inla .",
    "the histograms show posterior distributions from a long mcmc run ( ten chains of one million iterations each ) , the black curves show the posterior from inla , while the red curve shows the posteriors using our new correction to inla . ]",
    "the binary ( and , more generally , binomial ) glmms discussed in sections  [ sec : simulation - study ] and  [ sec : toenail ] are are important in many applications , particularly for biomedical data .",
    "poisson glmms are also of great interest , and among the difficult cases here are point processes such as log - gaussian cox processes  @xcite , where data are typically extremely sparse : essentially there are ones at the observed points , and zeroes everywhere else .",
    "our example in section  [ sec : poisson ] shows a stylized , extreme case of this type .",
    "studying the correction for the full log - gaussian cox process case could be a topic for future work .",
    "even though the point process case may be difficult , there will often be some degree of smoothing and/or replication making inference easier , so real data sets should be less extreme than the simulated example in section  [ sec : poisson ] .    from the results in this paper",
    ", it appears that the copula correction is robust and works well .",
    "there is no general theory guaranteeing that the method will always work under all circumstances , but we feel that the intuition underlying the method is quite strong .",
    "since inla for lgms is quite accurate in most cases , the correction is not needed in general , only for problematic cases such as those discussed in this paper . using the copula correction method",
    ", we can stretch the limits of applicability of inla , while maintaining its computational speed .",
    "let @xmath48 denote the `` standardized '' skew normal cdf corresponding to@xmath44 .",
    "we start by finding the jacobian of the transformation defined in equation  .",
    "note first that , immediately from   @xmath178 letting @xmath179 and @xmath180 denote the density functions corresponding to @xmath48 and @xmath50 , differentiating with respect to @xmath181 then gives @xmath182 so @xmath183\\right)}\\ ] ] and the jacobian of the transformation is @xmath184 since @xmath185 for @xmath186 and @xmath187 for all @xmath133 .",
    "note that @xmath188 where @xmath189 .",
    "collecting the different terms and again substituting @xmath190,\\ ] ] the transformed joint log density @xmath191 is therefore @xmath192 \\phi^{-1 } \\left[\\tilde f_j \\left(\\frac{\\tilde x_j-\\tilde\\mu_j(\\theta ) } { \\sigma_j(\\theta)}\\right)\\right]\\\\ + \\sum_{i=1}^n \\log\\tilde f_i\\left(\\frac{\\tilde x_i-\\tilde\\mu_i(\\theta ) } { \\sigma_i(\\theta)}\\right ) - \\sum_{i=1}^n \\log\\phi\\left(\\phi^{-1 } \\left[\\tilde f_i \\left(\\frac { \\tilde x_i-\\tilde\\mu_i(\\theta)}{\\sigma_i(\\theta)}\\right)\\right]\\right ) + \\text{constant}\\end{gathered}\\ ] ]    the original ( uncorrected ) log posterior is @xmath193 evaluated at @xmath77 where @xmath194 .",
    "therefore , the version of the copula correction accounting for skewness amounts to adding a term @xmath195 to the original log joint posterior , where @xmath196\\ ! \\phi^{-1 }",
    "\\!\\left[\\tilde f_j \\left(\\frac{\\mu_j(\\theta)-\\tilde\\mu_j(\\theta ) } { \\sigma_j(\\theta)}\\right)\\right]\\\\ + \\sum_{i=1}^n \\log\\tilde f_i\\left(\\frac{\\mu_i(\\theta)-\\tilde\\mu _ i(\\theta)}{\\sigma_i(\\theta)}\\right ) - \\sum_{i=1}^n \\log\\phi\\left(\\phi^{-1 } \\left[\\tilde f_i \\left(\\frac{\\mu _ i(\\theta)-\\tilde\\mu_i(\\theta)}{\\sigma_i(\\theta)}\\right)\\right]\\right).\\end{gathered}\\ ] ]    calculations of @xmath48 and @xmath179 were done using the functions ` psn ` and ` dsn ` in the r package ` sn `  @xcite .",
    "here we study model ( 0.8 ) from page 11 of the supplementary material of  @xcite , where the observations @xmath103 are iid binomial@xmath197 , with @xmath198 clusters , @xmath199 observations per cluster , @xmath108 for @xmath109 and @xmath110 otherwise , and sampling times @xmath200 .",
    "the model is @xmath201 where the @xmath130 are iid bivariate normally distributed with mean @xmath202 .",
    "following  @xcite , the prior for the precision matrix of @xmath130 is a wishart distribution with three degrees of freedom and diagonal scale parameter with diagonal elements @xmath203 and @xmath204 .",
    "the fixed effects are given @xmath119 priors .",
    "as in  @xcite , we shall consider the case when @xmath205 and @xmath206 are uncorrelated , but we shall here also consider the correlated case with correlation @xmath172 and @xmath207 , respectively .",
    "additionally , we consider two different settings of the marginal variances of @xmath205 and @xmath206 :    1 .",
    "var@xmath208 , var@xmath209 ( as in  @xcite ) , 2 .",
    "var@xmath210 , var@xmath211 .    for each of the two settings of the marginal variances above , we ran the simulation experiment for the three settings of @xmath177 ( @xmath131 , @xmath212 and @xmath213 correlation ) , giving six simulation settings in total . for each simulation setting , we made 200 simulated data sets , and ran two mcmc chains of 200,000 iterations each ( after discarding the first 100,000 iterations ) for each simulated data set .",
    "we have yet to specify the number of trials @xmath214 in the binomial distribution .",
    "it turns out that this model is nearly unidentifiable for @xmath215 , with very slow mcmc convergence and with numerical instability when running inla ( both with and without the correction ) .",
    "therefore , we will here consider @xmath216 , and show the results for @xmath217 .",
    "results ( not shown ) are similar also for @xmath218 .",
    "as expected , the estimation becomes more accurate as @xmath214 grows , and for large @xmath214 ( say , @xmath219 ) there is no need for the inla correction anymore .",
    "results are shown in tables  [ table3][table8 ] below , where we use the parameterization @xmath220 used internally by inla , where @xmath221 , @xmath222 and @xmath223 ( note that @xmath224 , @xmath171 and @xmath225 are defined on the whole real line ) .",
    "it seems like the correction is working quite well , giving an overall improvement .",
    "the coverage probabilities are improved in all cases expect for the @xmath226 with @xmath227 , so we see an improvement for @xmath228 of the @xmath229 combinations of parameters and simulation settings .",
    "the variance ratio var(inla)/var(mcmc ) is also improved for nearly all the cases , while the other performance measures show an overall ( though not uniform ) improvement .",
    "the method does not seem to deteriorate for higher values of the marginal variances and correlation .",
    "@|llcccccccc|@ + & & & & & & & & & + & true values & 0.693 & 1.386 & 0.000 & -2.500 & 1.000 & -1.000 & -0.500 & + & uncorrected inla & 1.527 & 2.130 & 1.566 & -2.664 & 1.021 & -0.692 & -0.428 & + & corrected inla & 1.380 & 2.016 & 1.313 & -2.703 & 1.038 & -0.704 & -0.429 & + & mcmc & 1.449 & 2.022 & 1.707 & -2.694 & 1.032 & -0.707 & -0.421 & +   +   + & & & & & & & & & + & uncorrected inla & 0.113 & 0.167 & -0.124 & 0.125 & -0.086 & 0.042 & -0.038 & + & corrected inla & -0.086 & -0.028 & -0.329 & -0.028 & 0.045 & 0.007 & -0.047 & +   +   + & & & & & & & & & + & uncorrected inla & 0.882 & 0.986 & 0.927 & 0.905 & 0.907 & 0.937 & 0.948 & + & corrected inla & 0.943 & 0.974 & 0.996 & 0.994 & 0.982 & 0.977 & 0.987 & +   +   + & & & & & & & & & + & uncorrected inla & 92.4% & 93.3% & 92.8% & 93.7% & 93.7% & 94.2% & 94.3% & + & corrected inla & 92.9% & 93.7% & 90.0% & 93.7% & 93.8% & 94.6% & 94.7% & +    @|llcccccccc|@ + & & & & & & & & & + & true values & 0.693 & 1.386 & 1.099 & -2.500 & 1.000 & -1.000 & -0.500 & + & uncorrected inla & 1.444 & 1.866 & 2.051 & -2.623 & 1.139 & -0.721 & -0.590 & + & corrected inla & 1.366 & 1.786 & 1.900 & -2.642 & 1.148 & -0.730 & -0.594 & + & mcmc & 1.363 & 1.790 & 2.176 & -2.649 & 1.148 & -0.731 & -0.582 & +   +   + & & & & & & & & & + & uncorrected inla & 0.126 & 0.134 & -0.118 & 0.111 & -0.073 & 0.027 & -0.041 & + & corrected inla & 0.013 & -0.031 & -0.252 & 0.035 & -0.005 & 0.002 & -0.066 & +   +   + & & & & & & & & & + & uncorrected inla & 0.891 & 0.998 & 0.930 & 0.904 & 0.912 & 0.934 & 0.953 & + & corrected inla & 0.948 & 1.001 & 1.043 & 0.942 & 0.953 & 0.952 & 0.979 & +   +   + & & & & & & & & & + & uncorrected inla & 92.8% & 94.1% & 93.1% & 93.8% & 93.9% & 94.2% & 94.4% & + & corrected inla & 93.7% & 94.8% & 92.8% & 93.9% & 94.1% & 94.4% & 94.6% & +    @|llcccccccc|@ + & & & & & & & & & + & true values & 0.693 & 1.386 & 2.944 & -2.500 & 1.000 & -1.000 & -0.500 & + & uncorrected inla & 0.700 & 1.530 & 3.133 & -2.543 & 1.027 & -0.947 & -0.460 & + & corrected inla & 0.662 & 1.471 & 3.062 & -2.553 & 1.031 & -0.954 & -0.465 & + & mcmc & 0.605 & 1.466 & 3.224 & -2.569 & 1.037 & -0.959 & -0.448 & +   +   + & & & & & & & & & + & uncorrected inla & 0.195 & 0.130 & -0.107 & 0.110 & -0.080 & 0.034 & -0.059 & + & corrected inla & 0.113 & -0.007 & -0.183 & 0.066 & -0.049 & 0.013 & -0.087 & +   +   + & & & & & & & & & + & uncorrected inla & 0.958 & 1.023 & 0.943 & 0.905 & 0.921 & 0.920 & 0.950 & + & corrected inla & 0.989 & 1.050 & 1.087 & 0.925 & 0.951 & 0.932 & 0.973 & +   +   + & & & & & & & & & + & uncorrected inla & 93.4% & 94.7% & 93.7% & 93.9% & 94.1% & 94.0% & 94.3% & + & corrected inla & 94.4% & 95.4% & 94.6% & 94.1% & 94.4% & 94.2% & 94.5% & +    @|llcccccccc|@ + & & & & & & & & & + & true values & -1.099 & 0.693 & 0.000 & -2.500 & 1.000 & -1.000 & -0.500 & + & uncorrected inla & -0.749 & 0.962 & -0.243 & -2.274 & 0.956 & -0.661 & -0.584 & + & corrected inla & -0.809 & 0.850 & -0.294 & -2.318 & 0.978 & -0.672 & -0.592 & + & mcmc & -0.844 & 0.867 & -0.250 & -2.306 & 0.971 & -0.652 & -0.593 & +   +   + & & & & & & & & & + & uncorrected inla & 0.335 & 0.317 & 0.017 & 0.101 & -0.106 & -0.021 & 0.049 & + & corrected inla & 0.126 & -0.058 & -0.106 & -0.039 & 0.047 & -0.050 & 0.003 & +   +   + & & & & & & & & & + & uncorrected inla & 0.953 & 0.986 & 0.984 & 0.902 & 0.908 & 0.886 & 0.903 &",
    "+ & corrected inla & 0.951 & 0.969 & 0.950 & 0.950 & 0.978 & 0.933 & 0.976 & +   +   + & & & & & & & & & + & uncorrected inla & 92.9% & 93.2% & 94.8% & 93.9% & 93.9% & 93.6% & 93.8% & + & corrected inla & 94.2% & 94.7% & 94.3% & 94.3% & 94.6% & 94.2% & 94.7% & +    @|llcccccccc|@ + & & & & & & & & & + & true values & -1.099 & 0.693 & 1.099 & -2.500 & 1.000 & -1.000 & -0.500 & + & uncorrected inla & -0.714 & 1.289 & 1.778 & -2.151 & 1.039 & -1.013 & -0.559 & + & corrected inla & -0.816 & 1.106 & 1.348 & -2.229 & 1.080 & -1.041 & -0.570 & + & mcmc & -0.816 & 1.172 & 1.643 & -2.166 & 1.048 & -1.003 & -0.557 & +   +   + & & & & & & & & & + & uncorrected inla & 0.336 & 0.273 & 0.141 & 0.045 & -0.061 & -0.022 & -0.010 & + & corrected inla & 0.002 & -0.170 & -0.306 & -0.203 & 0.226 & -0.091 & -0.068 & +   +   + & & & & & & & & & + & uncorrected inla & 0.954 & 1.030 & 0.977 & 0.892 & 0.905 & 0.901 & 0.934 & + & corrected inla & 0.934 & 0.964 & 0.761 & 0.984 & 1.031 & 0.976 & 1.041 & +   +   + & & & & & & & & & + & uncorrected inla &",
    "92.8% & 93.5% & 93.7% & 93.5% & 93.6% & 93.7% & 94.2% & + & corrected inla & 93.3% & 94.2% & 90.5% & 93.7% & 94.2% & 94.6% & 95.3% & +    @|llcccccccc|@ + & & & & & & & & & + & true values & -1.099 & 0.693 & 2.944 & -2.500 & 1.000 & -1.000 & -0.500 & + & uncorrected inla & -1.087 & 1.025 & 4.483 & -2.672 & 1.015 & -0.876 & -0.590 & + & corrected inla & -1.160 & 0.940 & 4.466 & -2.699 & 1.018 & -0.902 & -0.606 & + & mcmc & -1.169 & 0.941 & 4.537 & -2.703 & 1.039 & -0.845 & -0.572 & +   +   + & & & & & & & & & + & uncorrected inla & 0.288 & 0.192 & -0.063 & 0.083 & -0.150 & -0.061 & -0.079 & + & corrected inla & 0.032 & -0.008 & -0.075 & 0.011 & -0.130 & -0.111 & -0.149 & +   +   + & & & & & & & & & + & uncorrected inla & 0.938 & 0.961 & 0.883 & 0.883 & 0.897 & 0.887 & 0.932 & + & corrected inla & 0.978 & 1.019 & 1.060 & 0.933 & 0.943 & 0.937 & 0.986 & +   +   + & & & & & & & & & + & uncorrected inla & 92.1% & 92.6% & 91.4% & 93.5% & 93.5% & 93.5% & 93.9% & + & corrected inla & 93.5% & 93.9% & 93.3% & 94.1% & 94.1% & 94.0% & 94.3% & +      in the simulation study described in section  [ sec : simulation - study ] we followed  @xcite and used @xmath230 observations per cluster .",
    "as suggested by a reviewer , we here consider the effect of having an even smaller value of @xmath231 .",
    "we only show the results for the most extreme possible case , which is @xmath232 . using the close to non - informative prior settings of model   in section  [ sec : simulation - study ] ( @xmath119 priors for the @xmath120 and a gamma@xmath233 prior for @xmath5 ) , the case with @xmath230 is already quite difficult .",
    "using the settings described in section  [ sec : simulation - study ] , the simulated data are relatively low - informative , making stable and reliable inference non - trivial .    in order to study the even more extreme case of @xmath232 , more informative priors",
    "are needed , otherwise both mcmc and inla will fail . for non - informative ( or very weakly informative ) priors",
    "the model is just too close to being singular .",
    "therefore , to study the case of @xmath232 , we use the following priors : @xmath8 for the @xmath120 , and also a @xmath8 prior for the log precision , @xmath234 .",
    "we used sampling times @xmath235 , and 200 simulated data sets .",
    "one million mcmc samples ( after a burn - in of 100,000 ) were used for each data set .",
    "the results are shown in table  [ table9 ] .",
    "the correction seems to work well , giving improved estimates in nearly all cases .",
    "note in particular that the 95% coverage is uniformly improved , and that all the coverage values are between 93.7% and 96.2% when using the correction .",
    "@|llrrrrrrrr|@ + & & & & & & & & & + & true values & 1.000 & 1.000 & 0.000 & -2.500 & 1.000 &",
    "-1.000 & -0.500 & + & uncorrected inla & 0.667 & 0.762 & 0.689 & -1.990 & 0.837 & -1.178 & -0.423 & + & corrected inla & 1.104 & 0.933 & 0.358 & -2.032 & 0.860 & -1.219 & -0.442 & + & mcmc & 0.956 & 0.899 & 0.392 & -2.114 & 0.891 & -1.230 & -0.427 & +   +   + & & & & & & & & & + & uncorrected inla & -0.337 & -0.362 & 0.355 & 0.250 & -0.295 & 0.079 & 0.020 & + & corrected inla & 0.149 & 0.085 & -0.041 & 0.163 & -0.168 & 0.013 & -0.059 & +   +   + & & & & & & & & & + & uncorrected inla & 0.393 & 0.592 & 0.841 & 0.876 & 0.823 & 0.902 & 0.873 &",
    "+ & corrected inla & 2.030 & 1.360 & 1.127 & 0.920 & 0.896 & 0.933 & 0.907 & +   +   + & & & & & & & & & + & uncorrected inla & 89.2% & 89.2% & 89.5% & 93.5% & 92.6% & 93.7% & 93.3% & + & corrected inla & 96.2% & 95.9% & 96.0% & 94.2% & 93.8% & 94.1% & 93.7% & +      as suggested by a reviewer , we here study the effect of the case of estimation from a misspecified model : we simulate data from the model   and estimate using model   ( with prior settings as in section  [ sec : simulation - study ] ) . as before ,",
    "we use extremely long mcmc chains ( one million iterations after discarding 100,000 iterations ) as the `` gold standard '' .",
    "we simulated 200 data sets from each of the six configurations described in appendix  [ sec : model - with - two ] .",
    "the results are shown in tables  [ table10][table15 ] .",
    "again , the correction improves the results in nearly all cases , so it does not seem like using a misspecified model presents any particular problems for the inla correction",
    ".    @|llrrrrrrrr|@ + & & & & & & & & & + & uncorrected inla & 0.424 & 0.536 & 1.778 & -2.482 & 1.068 & -0.833 & -0.561 & + & corrected inla & 0.591 & 0.640 & 1.440 & -2.541 & 1.093 & -0.847 & -0.579 & + & mcmc & 0.623 & 0.660 & 1.374 & -2.533 & 1.091 & -0.855 & -0.562 & +   +   + & & & & & & & & & + & uncorrected inla & -0.378 & -0.394 & 0.361 & 0.150 & -0.147 & 0.042 & 0.007 & + & corrected inla & -0.069 & -0.067 & 0.058 & -0.019 & 0.011 & 0.012 & -0.079 & +   +   + & & & & & & & & & + & uncorrected inla & 0.492 & 0.714 & 1.044 & 0.834 & 0.845 & 0.900 & 0.898 & + & corrected inla & 0.888 & 0.923 & 1.002 & 0.905 & 0.881 & 0.948 & 0.916 & +   +   + & & & & & & & & & + & uncorrected inla & 87.6% & 87.3% & 88.4% & 92.8% & 92.9% & 93.7% & 93.6% & + & corrected inla & 93.9% & 93.4% & 93.9% & 93.6% & 93.3% & 94.4% & 93.8% & +    @|llrrrrrrrr|@ + & & & & & & & & & + & uncorrected inla & 0.983 & 0.890 & 0.585 & -2.667 & 0.973 & -1.060 & -0.220 & + & corrected inla & 1.255 & 1.018 & 0.282 & -2.737 & 0.996 & -1.088 & -0.230 & + & mcmc & 1.362 & 1.066 & 0.172 & -2.746 & 1.004 & -1.111 & -0.214 & +   +   + & & & & & & & & & + & uncorrected inla & -0.498 & -0.541 & 0.544 & 0.202 & -0.202 & 0.084 & -0.024 & + & corrected inla & -0.147 & -0.150 & 0.140 & 0.017 & -0.050 & 0.038 & -0.069 & +   +   + & & & & & & & & & + & uncorrected inla & 0.592 & 0.854 & 1.318 & 0.780 & 0.812 & 0.850 & 0.869 & + & corrected inla & 0.851 & 0.943 & 1.059 & 0.851 & 0.842 & 0.905 & 0.888 & +   +   + & & & & & & & & & + & uncorrected inla & 89.3% & 89.1% & 89.6% & 91.8% & 92.2% & 93.0% & 93.3% & + & corrected inla & 94.0% & 93.8% & 94.1% & 93.1% & 92.9% & 93.8% & 93.6% & +    @|llrrrrrrrr|@ + & & & & & & & & & + & uncorrected inla & 1.071 & 0.937 & 0.451 & -2.621 & 1.097 & -1.243 & -0.478 & + & corrected inla & 1.447 & 1.104 & 0.073 & -2.713 & 1.133 & -1.287 & -0.501 & + & mcmc & 1.471 & 1.118 & 0.037 & -2.698 & 1.132 & -1.295 & -0.478 & +   +   + & & & & & & & & & + & uncorrected inla & -0.496 & -0.538 & 0.542 & 0.192 & -0.208 & 0.081 & 0.004 & + & corrected inla & -0.039 & -0.042 & 0.041 & -0.046 & 0.015 & 0.008 & -0.099 & +   +   + & & & & & & & & & + & uncorrected inla & 0.596 & 0.854 & 1.308 & 0.775 & 0.783 & 0.842 & 0.855 & + & corrected inla & 0.959 & 0.989 & 1.027 & 0.871 & 0.830 & 0.918 & 0.881 & +   +   + & & & & & & & & & + & uncorrected inla & 89.5% & 89.4% & 89.8% & 91.7% & 91.6% & 92.9% & 93.0% & + & corrected inla & 95.0% & 94.8% & 95.0% & 93.2% & 92.6% & 94.0% & 93.4% & +    @|llrrrrrrrr|@ + & & & & & & & & & + & uncorrected inla & 2.644 & 1.587 & -0.873 & -2.461 & 0.960 & -0.626 & -0.486 & + & corrected inla & 2.938 & 1.671 & -0.976 & -2.508 & 0.977 & -0.637 & -0.497 & + & mcmc & 3.161 & 1.736 & -1.053 & -2.546 & 0.998 & -0.623 & -0.505 & +   +   + & & & & & & & & & + & uncorrected inla & -0.471 & -0.500 & 0.523 & 0.215 & -0.271 & -0.005 & 0.103 & + & corrected inla & -0.211 & -0.219 & 0.225 & 0.096 & -0.153 & -0.026 & 0.043 & +   +   + & & & & & & & & & + & uncorrected inla & 0.683 & 0.828 & 1.021 & 0.788 & 0.786 & 0.826 & 0.838 & + & corrected inla & 0.848 & 0.921 & 1.018 & 0.843 & 0.811 & 0.880 & 0.858 & +   +   + & & & & & & & & & + & uncorrected inla & 91.1% & 91.1% & 91.4% & 91.8% & 91.5% & 92.6% & 92.6% & + & corrected inla & 94.2% & 94.2% & 94.4% & 93.0% & 92.4% & 93.4% & 93.1% & +    @|llrrrrrrrr|@ + & & & & & & & & & + & uncorrected inla & 3.246 & 1.749 & -1.056 & -3.025 & 0.936 & -0.642 & -0.182 & + & corrected inla & 3.821 & 1.888 & -1.203 & -3.117 & 0.957 & -0.676 & -0.186 & + & mcmc & 3.990 & 1.938 & -1.261 & -3.144 & 0.978 & -0.663 & -0.183 & +   +   + & & & & & & & & & + & uncorrected inla & -0.506 & -0.541 & 0.570 & 0.251 & -0.277 & 0.032 & 0.009 & + & corrected inla & -0.144 & -0.154 & 0.161 & 0.063 & -0.139 & -0.017 & -0.012 & +   +   + & & & & & & & & & + & uncorrected inla & 0.643 & 0.802 & 1.020 & 0.761 & 0.793 & 0.806 & 0.830 & + & corrected inla & 0.922 & 0.973 & 1.050 & 0.846 & 0.826 & 0.885 & 0.860 & +   +   + & & & & & & & & & + & uncorrected inla & 90.4% & 90.4% & 90.7% & 91.2% & 91.6% & 92.3% & 92.6% & + & corrected inla & 94.8% & 94.8% & 95.0% & 93.0% & 92.7% & 93.5% & 93.1% & +    @|llrrrrrrrr|@ + & & & & & & & & & + & uncorrected inla & 3.622 & 1.854 & -1.180 & -3.170 & 1.160 &",
    "-0.564 & -0.213 & + & corrected inla & 4.118 & 1.973 & -1.302 & -3.253 & 1.185 & -0.582 & -0.220 & + & mcmc & 4.468 & 2.058 & -1.389 & -3.312 & 1.218 & -0.576 & -0.222 & +   +   + & & & & & & & & & + & uncorrected inla & -0.526 & -0.564 & 0.595 & 0.276 & -0.326 & 0.017 & 0.037 & + & corrected inla & -0.229 & -0.240 & 0.248 & 0.117 & -0.186 & -0.008 & 0.007 & +   +   + & & & & & & & & & + & uncorrected inla & 0.641 & 0.802 & 1.021 & 0.751 & 0.755 & 0.797 & 0.804 & + & corrected inla & 0.844 & 0.925 & 1.033 & 0.817 & 0.788 & 0.860 & 0.832 & +   +   + & & & & & & & & & + & uncorrected inla & 90.1% & 90.1% & 90.4% & 90.9% & 90.6% & 92.1% & 92.1% & + & corrected inla & 94.3% & 94.2% & 94.5% & 92.6% & 91.9% & 93.1% & 92.7% & +",
    "we thank youyi fong for providing r code relating to the simulation study described in section  [ sec : simulation - study ] .",
    "we are also very grateful to leonard held and rafael sauter for providing us with a copy of their unpublished paper along with r code relevant for the analysis of toenail data described in section  [ sec : toenail ] .",
    "we thank janine illian , geir - arne fuglstad , dan simpson and two anonymous reviewers for helpful comments that have led to an improved presentation .",
    "this research was supported by the norwegian research council ."
  ],
  "abstract_text": [
    "<S> we introduce a new copula - based correction for generalized linear mixed models ( glmms ) within the integrated nested laplace approximation ( inla ) approach for approximate bayesian inference for latent gaussian models . while inla is usually very accurate , some ( rather extreme ) cases of glmms with e.g.  binomial or poisson data have been seen to be problematic . </S>",
    "<S> inaccuracies can occur when there is a very low degree of smoothing or `` borrowing strength '' within the model , and we have therefore developed a correction aiming to push the boundaries of the applicability of inla . our new correction has been implemented as part of the r - inla package , and adds only negligible computational cost . </S>",
    "<S> empirical evaluations on both real and simulated data indicate that the method works well .    * * </S>"
  ]
}