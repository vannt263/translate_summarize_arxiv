{
  "article_text": [
    "consider a sequence of probability measures @xmath2 on a common measurable space @xmath3 ; we assume that the probabilities have common dominating finite - measure @xmath4 and write the densities w.r.t .  @xmath4 as @xmath5 .",
    "in particular , for some known @xmath6 , we let @xmath7 where the normalizing constant @xmath8 may be unknown .",
    "the context of interest is when the sequence of densities is associated to an ` accuracy ' parameter @xmath9 , with @xmath10 as @xmath11 with @xmath12 .",
    "this set - up is relevant to the context of discretised numerical approximations of continuum fields , as we will explain below .",
    "the objective is to compute : @xmath13 : = \\int_e g(u)\\eta_\\infty(u)du\\ ] ] for potentially many measurable @xmath14integrable functions @xmath15 . in practice",
    "one can not treat @xmath16 and must consider these distributions with @xmath17 .",
    "problems involving numerical approximations of continuum fields are discretized before being solved numerically .",
    "finer - resolution solutions are more expensive to compute than coarser ones . such discretizations naturally give rise to hierarchies of resolutions via the use of nested meshes .",
    "successive solution at refined meshes can be utilized to mitigate the number of necessary solves for the finest resolutions . for the solution of linear systems ,",
    "the coarsened systems are solved as pre - conditioners within the framework of iterative linear solvers in order to reduce the condition number , and hence the number of necessary iterations at the finer resolution .",
    "this is the principle of multi - grid methods . for monte carlo methods , as in the context above",
    ", a telescoping sum of associated differences at successive refinement levels can be utilized .",
    "this is so that the bias of the resulting multilevel estimator is determined by the finest level but the variance of the estimators of the differences decays .",
    "the reduction in the variance at finer levels implies that the number of samples required to reach a given error tolerance is also reduced with increasing resolution .",
    "this procedure is then optimized to balance the extra per - sample cost at the finer levels .",
    "overall one can obtain a method with smaller computational effort to reach a pre - determined error than applying a standard monte carlo method immediately at the finest resolution @xcite .",
    "multi level monte carlo ( mlmc ) @xcite ( see also @xcite ) methods are such that one typically sets an error threshold for a target expectation , and then sets out to attain an estimator with the prescribed error utilizing an optimal allocation of monte carlo resources . within the context of @xcite ,",
    "the continuum problem is a stochastic differential equation ( sde ) or pde with random coefficients , and the target quantity is an expectation of a functional , say @xmath18 , of the parameter of interest @xmath19 , over an ideal measure @xmath20 that avoids discretisation .",
    "the levels are a hierarchy of refined approximations of the function - space , specified in terms of a small resolution parameter say @xmath9 , for @xmath21 , thus giving rise to a corresponding sequence of approximate laws @xmath22 .",
    "the method uses the telescopic sum @xmath23 = { \\mathbb{e}}_{\\eta_0}[g(u ) ] + \\sum_{l=1}^l   \\{{\\mathbb{e}}_{\\eta_l}[g(u)]-{\\mathbb{e}}_{\\eta_{l-1}}[g(u)]\\}\\ ] ] and proceeds by coupling the consecutive probability distributions @xmath24 , @xmath25 .",
    "thus , the expectations are estimated via the standard unbiased monte carlo averages @xmath26 where @xmath27 are i.i.d",
    ". samples , with marginal laws @xmath24 , @xmath22 , respectively , carefully constructed on a joint probability space .",
    "this is repeated independently for @xmath28 .",
    "the overall multilevel estimator will be @xmath29 under the convention that @xmath30 .",
    "a simple error analysis gives that the mean squared error ( mse ) is @xmath31 \\}^2 =   \\underbrace{{\\mathbb{e}}\\ { \\hat{y}_{l,{\\rm multi}}- { \\mathbb{e}}_{\\eta_{l } } [ { g}(u)]\\}^2}_{\\rm variance }    + \\underbrace{\\{{\\mathbb{e}}_{\\eta_l } [ { g}(u ) ] - { \\mathbb{e}}_{\\eta_{\\infty } } [ g(u)]\\}^2}_{\\rm bias}\\   .",
    "\\label{eq : mse}\\ ] ] one can now optimally allocate @xmath32 to minimize the variance term @xmath33 for fixed computational cost @xmath34 , where @xmath35 is the variance of @xmath36 $ ] and @xmath37 the computational cost for its realisation . using lagrange multipliers for the above constrained optimisation , we get the optimal allocation of resources @xmath38 . in more detail ,",
    "the typical chronology is that one targets an mse , say @xmath39 , then ( i ) given a characterisation of the bias as an order of @xmath9 , one determines @xmath40 , @xmath41 , for some integer @xmath42 , and chooses a horizon @xmath43 such that the bias is @xmath44 and ( ii ) given a characterisation of @xmath35 , @xmath37 as some orders of @xmath9 , one optimizes the required samples @xmath45 needed to give variance @xmath44 .",
    "thus , a specification of the bias , variance and computational costs as functions of @xmath9 is needed .    as a prototypical example of the above setting @xcite ,",
    "consider the case @xmath46 with @xmath47 being the terminal position of the solution @xmath48 of a sde and @xmath22 is the distribution of @xmath47 under the consideration of a numerical approximation with time - step @xmath49 .",
    "the laws @xmath24 , @xmath22 can be coupled via use of the same driving brownian path .",
    "invoking the relevant error analysis for sde models , one can obtain ( for @xmath20 , @xmath50 , and defined on the common probability space ) :    * weak error @xmath51| = { \\mathcal{o}}(h_l^\\alpha)$ ] , providing the bias @xmath52 , * strong error , @xmath53 , giving the variance @xmath54 , * computational cost for a realisation of @xmath55 , @xmath56 ,    for some constants @xmath57 related to the details of the discretisation method .",
    "the standard euler marayuma method for solution of sde gives the orders @xmath58 .",
    "assuming a general context , given such rates for bias , @xmath35 and @xmath37 , one proceeds as follows .",
    "recall that @xmath59 , for some integer @xmath42 .",
    "then , targeting an error tolerance of @xmath60 and letting @xmath61 , one has @xmath62 , as in @xcite . using the optimal allocation @xmath38",
    ", one finds that @xmath63 .",
    "taking under consideration a target error of size @xmath64 , one sets @xmath65 , with @xmath66 chosen to control the total error for increasing  @xmath43 .",
    "thus , for the resulted estimator in ( [ eq : multi])-([eq : mse ] ) , we have : @xmath67 to have a variance of @xmath39 , one sets @xmath68 , so @xmath66 may or may not depend on @xmath60 depending on whether this sum converges or not ( recalling that @xmath69 ) . in the case of euler - marayuma , for example , @xmath70 , @xmath71 , and the cost is @xmath72 , versus @xmath73 using a single level with mesh - size @xmath74 . if @xmath75 , corresponding for instance to the milstein method , then the cost is @xmath76 .",
    "the latter is the cost of obtaining the given level of error for a scalar random variable , and is therefore optimal .",
    "the worst scenario is when @xmath77 . in this case",
    "it is sufficient to set @xmath78 to make the variance @xmath39 , and then the number of samples on the finest level is given by @xmath79 whereas the total algorithmic cost is @xmath80 , where @xmath81 . in this case",
    ", one can choose the largest value for the bias , @xmath82 , so that @xmath83 and the total cost , @xmath84 , is dominated by this single sample .",
    "see @xcite for more details .",
    "it is important to note that the realizations @xmath85 , @xmath86 for a given increment must be coupled to obtain decaying variances @xmath35 . in the case of an sde driven by brownian motion one",
    "can simply simulate the driving noise on level @xmath87 and then upscale it to level @xmath88 by summing elements of the finer path @xcite . for the case of a pde",
    "forward model relying on uncertain input the scenario is quite similar @xcite .",
    "for example , in the case that the input is of fixed dimension and the levels arise due to discretization of the forward map alone within a finite element context , one would use the same realization of the input on two separate meshes for a pairwise - coupled realization . note",
    "that in the more general context of pde , it is natural to decompose @xmath89 , where @xmath90 is the spatio - temporal dimension of the underlying continuum . in particular ,",
    "the number of degrees of freedom of a @xmath91dimensional field approximated on a mesh of diameter @xmath9 is given by @xmath92 .",
    "then , the forward solve associated to the evaluation of @xmath93 may range from linear ( @xmath94 ) to cubic ( @xmath95 ) in the number of degrees of freedom .",
    "for example , the solution of an sde or a sparse matrix - vector multiplication give @xmath94 , a dense matrix - vector multiplication would give @xmath96 , and direct linear solve by gaussian elimination would give @xmath95 .",
    "the present work will focus on the case of an inverse problem with fixed - dimensional input . indeed",
    "the difficulty arises here because we only know how to _ evaluate _ ( up - to a constant ) the target density at any given level , and can not directly obtain independent samples from it .",
    "there exist many approaches to solving such problem , for example one can review the recent works @xcite which use markov chain monte carlo ( mcmc ) methods in the multilevel framework . in this article a more natural and powerful formulation is considered , related with the use of sequential monte carlo approaches .",
    "sequential monte carlo ( smc ) methods are amongst the most widely used computational techniques in statistics , engineering , physics , finance and many other disciplines . in particular smc samplers @xcite",
    "are designed to approximate a sequence @xmath97 of probability distributions on a common space , whose densities are only known up - to a normalising constant .",
    "the method uses @xmath98 samples ( or particles ) that are generated in parallel , and are propagated with importance sampling ( often ) via mcmc and resampling methods .",
    "several convergence results , as @xmath99 grows , have been proved ( see e.g.  @xcite ) .",
    "smc samplers have also recently been proven to be stable in certain high - dimensional contexts @xcite .",
    "current state of the art for the analysis of smc algorithms include the work of @xcite . in this work ,",
    "the method of smc samplers is perfectly designed to approximate the sequence of distributions , but as we will see , implementing the standard telescoping identity of mlmc requires some ingenuity .",
    "in addition , in order to consider the benefit of using smc , one must analyze the variance of the estimate ; in such scenarios this is not a trivial extension of the convergence analysis previously mentioned . in particular",
    ", one must very precisely consider the auto - covariance of the smc approximations and consider the rate of decrease of this quantity as the time - lag between smc approximations increases .",
    "such a precise analysis does not appear to exist in the literature .",
    "we note that our work , whilst presented in the context of pdes , is not restricted to such scenarios and , indeed can be applied in almost any other similar context ( that is , a sequence of distributions on a common space , with increasing computational costs associated to the evaluation of the densities which in some sense converge to a given density ) ; however , the potential benefit of doing so , may not be obvious in general .",
    "this article is structured as follows . in section [ sec : set_up ] the ml identity and smc algorithm are given . in section [ sec : complex ] our main complexity result is given under assumptions and their implications are discussed . in section [ sec : ip ] we give a context where the assumptions of our theoretical results can be verified . in section [ sec : numerics ] our approach is numerically demonstrated on a bayesian inverse problem . section [ sec : complex ] and the appendix provide the proofs of our main theorem .",
    "let @xmath3 be a measurable space .",
    "the notation @xmath100 denotes the class of bounded and measurable real - valued functions .",
    "the supremum norm is written as @xmath101 and @xmath102 is the set of probability measures on @xmath3 .",
    "we will consider non - negative operators @xmath103 such that for each @xmath104 the mapping @xmath105 is a finite non - negative measure on @xmath106 and for each @xmath107 the function @xmath108 is measurable ; the kernel @xmath109 is markovian if @xmath110 is a probability measure for every @xmath104 . for a finite measure @xmath111 on @xmath3 , and a real - valued , measurable @xmath112",
    ", we define the operations : @xmath113 we also write @xmath114 .",
    "in addition @xmath115 , @xmath116 , denotes the @xmath117norm , where the expectation is w.r.t .",
    "the law of the appropriate simulated algorithm .      as described in section [ sec : intro ] , the context of interest is when a sequence of densities @xmath118 , as in ( [ eq : target ] ) , are associated to an ` accuracy ' parameter @xmath9 , with @xmath10 as @xmath11 , such that @xmath119 . in practice one can not treat @xmath120 and so must consider these distributions with @xmath17 .",
    "the laws with large @xmath9 are easy to sample from with low computational cost , but are very different from @xmath121 , whereas , those distributions with small @xmath9 are hard to sample with relatively high computational cost , but are closer to @xmath121 .",
    "thus , we choose a maximum level @xmath122 and we will estimate @xmath123 : = \\int_e g(u)\\eta_l(u)du\\ .\\ ] ] by the standard telescoping identity used in mlmc , one has @xmath124 & =   \\mathbb{e}_{\\eta_0}[g(u ) ] + \\sum_{l=1}^{l}\\big\\ { \\mathbb{e}_{\\eta_l}[g(u ) ] - \\mathbb{e}_{\\eta_{l-1}}[g(u)]\\big\\ } \\nonumber \\nonumber \\\\   & = \\mathbb{e}_{\\eta_0}[g(u ) ] + \\sum_{l=1}^{l}\\mathbb{e}_{\\eta_{l-1}}\\big [ \\big(\\frac{\\gamma_l(u)z_{l-1}}{\\gamma_{l-1}(u)z_l } - 1\\big)g(u)\\big]\\ .",
    "\\label{eq : ml_approx}\\end{aligned}\\ ] ]    suppose now that one applies an smc sampler @xcite to obtain a collection of samples ( particles ) that sequentially approximate @xmath125 .",
    "we consider the case when one initializes the population of particles by sampling i.i.d",
    ".  from @xmath126 , then at every step resamples and applies a mcmc kernel to mutate the particles .",
    "we denote by @xmath127 , with @xmath128 , the samples after mutation ; one resamples @xmath129 according to the weights @xmath130 , for indices @xmath131",
    ". we will denote by @xmath132 the sequence of mcmc kernels used at stages @xmath133 , such that @xmath134 . for @xmath135 , @xmath136 ,",
    "we have the following estimator of @xmath137 $ ] : @xmath138 we define @xmath139 the joint probability distribution for the smc algorithm is @xmath140 if one considers one more step in the above procedure , that would deliver samples @xmath141 , a standard smc sampler estimate of the quantity of interest in ( [ eq : ml_approx ] ) is @xmath142 ; the earlier samples are discarded . within a multilevel context , a consistent smc estimate of is @xmath143 and this will be proven to be superior than the standard one , under assumptions .",
    "there are two important structural differences within the mlsmc context , compared to the standard ml implementation of @xcite , sketched in section [ sec : intro ] :    * the @xmath144 terms in ( [ eq : smc_est ] ) are _ not _ unbiased estimates of the differences @xmath145 - \\mathbb{e}_{\\eta_{l-1}}[g(u)]$ ] , so the relevant mse error decomposition here is : @xmath146 \\}^2\\big ] \\le 2\\,\\mathbb{e}\\big[\\{\\widehat{y}-\\mathbb{e}_{\\eta_l}[g(u)]\\}^2\\big ] +   2\\,\\ { \\mathbb{e}_{\\eta_l}[g(u ) ] - \\mathbb{e}_{\\eta_\\infty}[g(u ) ]   \\}^2\\ .\\ ] ] * the same @xmath144 estimates are _ not _ independent .",
    "hence a substantially more complex error analysis will be required to characterise @xmath147\\}^2]$ ] . in section [ sec : complex ] , we will obtain an expression for this discrepancy , which will be more involved than the standard @xmath148 , but will still allow for a relevant constrained optimisation to determine the optimal allocation of particle sizes @xmath149 along the levels .",
    "given an appropriate classification of both terms on the r.h.s .",
    "of ( [ eq : dec ] ) as an order of the tolerance for a bayesian inverse problem ( to be described in section [ sec : ip ] ) , one can specify a level @xmath43 , and optimal monte - carlo sample sizes @xmath149 so that the mse of @xmath150 is @xmath39 at a reduced computational cost .",
    "we will now obtain an analytical result that controls the error term @xmath147\\}^2]$ ] in expression ( [ eq : dec ] ) .",
    "this is of general significance for the development of mlsmc in various contexts .",
    "then , we will look in detail at an inverse problem context ( developed in section [ sec : ip ] ) and fully investigate the mlsmc method .",
    "for any @xmath151 and @xmath152 we write : @xmath153 we introduce the following assumptions , which will be verifiable in some contexts .",
    "they are rather strong , but could be relaxed at condsiderable increase in the complexity of the arguments , which will ultimately provide the same information .",
    "in addition , the assumptions are standard in the literature of smc methods ; see @xcite .",
    "[ hyp : a ] there exist @xmath154 such that @xmath155    [ hyp : b ] there exists a @xmath156 such that for any @xmath157 , @xmath158 , @xmath159 : @xmath160    [ theo : main_error ] assume ( a[hyp : a]-[hyp : b ] ) .",
    "there exist @xmath161 and @xmath162 such that for any @xmath163 , with @xmath164 , @xmath165\\}^2\\big ]   \\leq   c\\,\\bigg(\\frac{1}{n_0 } + & \\sum_{l=1}^{l}\\frac{\\|\\tfrac{z_{l-1}}{z_{l}}g_{l-1}-1\\|_{\\infty}^2}{n_{l-1 } } \\\\ & +   \\sum_{1\\le l < q\\le l}\\|\\tfrac{z_{l-1}}{z_{l}}g_{l-1}-1\\|_{\\infty } \\|\\tfrac{z_{q-1}}{z_{q}}g_{q-1}-1\\|_{\\infty } \\big\\{\\tfrac{\\kappa^{q - l}}{n_{l-1 } } + \\tfrac{1}{n_{l-1}^{1/2}n_{q-1 } }   \\big\\}\\bigg)\\ .\\end{aligned}\\ ] ]      the following notations are adopted ; this will substantially simplify subsequent expressions : @xmath166   y_{l-1 } & = \\frac{\\eta_{l-1}(gg_{l-1})}{\\eta_{l-1}(g_{l-1 } ) } - \\eta_{l-1}(g ) \\,\\,\\,\\,\\big(\\ ,   \\equiv \\eta_{l}(g ) - \\eta_{l-1}(g)\\ , \\big)\\ , \\label{eq : analytical } \\\\[0.3 cm ] \\nonumber \\overline{\\varphi_l}(u ) & = \\big(\\tfrac{z_{l-1}}{z_l}g_{l-1}(u)-1\\big ) \\",
    ", \\\\[0.3 cm ] \\nonumber \\widetilde{\\varphi_l}(u ) & =   g(u ) \\overline{\\varphi_l}(u )   \\ , \\\\[0.3 cm ] \\label{eq : ay } a_n(\\varphi , n ) &   = \\eta_n^n(\\varphi g_n)/\\eta_n^n(g_n ) \\ , \\quad \\varphi\\in\\mathcal{b}_b(e)\\ , \\quad   0\\leq n\\leq l-1   \\ , \\\\[0.2 cm ] \\label{eq : aybar } \\overline{a}_n(\\varphi , n ) &   =   a_n(\\varphi , n ) - \\frac{\\eta_n(\\varphi g_n)}{\\eta_n(g_n)}\\ .\\end{aligned}\\ ] ] throughout this section , @xmath167 is a constant whose value may change , but does not depend on any time parameters of the feynman - kac formula , nor @xmath149 .",
    "the proof of theorem [ theo : main_error ] follows from several technical lemmas which are now given and supported by further results in the appendix ; the proof of the theorem is at the end of this subsection .",
    "it is useful to observe that @xmath168 , @xmath169 and @xmath170 with probability 1 as the conditional @xmath171-norm of functional @xmath172 over a discrete distribution .",
    "we will make repeated use of the following identity which follows from these observations upon adding and subtracting @xmath173 : @xmath174    [ lem : tech_lem ] assume ( a[hyp : a]-[hyp : b ] ) .",
    "there exists a @xmath161 such that for any @xmath157 : @xmath175    from ( [ eq : basic ] ) and the @xmath176-inequality we obtain : @xmath177 by ( * ? ? ?",
    "* theorem 7.4.4 ) we have that both @xmath178-norms are upper bounded by @xmath179 .",
    "this completes the proof .    by the @xmath176-inequality and standard properties of i.i.d .",
    "random variables one has : @xmath165\\}^2\\big ]   = \\mathbb{e}\\big[\\big\\{\\sum_{l=1}^{n}(y_{l-1}^{n_{l-1 } } - y_{l-1})\\big\\}^2\\big ]   \\le \\frac{c}{n_0 } + 2\\,\\mathbb{e}\\big[\\big\\{\\sum_{l=2}^{n}(y_{l-1}^{n_{l-1 } } - y_{l-1})\\big\\}^2\\big]\\ .\\end{aligned}\\ ] ] we have that : @xmath180 = \\mathbb{e}\\big [ \\sum_{l=2}^{n}(y_{l-1}^{n_{l-1 } } - y_{l-1})^2\\big ] + 2\\sum_{2\\le l < q\\le",
    "l }   \\mathbb{e}\\big[(y_{l-1}^{n_{l-1 } } - y_{l-1 } ) ( y_{q-1}^{n_{q-1 } } - y_{q-1})\\big]\\ ] ] lemma [ lem : tech_lem ] gives that : @xmath181\\le   c\\sum_{l=2}^{l}\\frac{\\|\\frac{z_{l-1}}{z_{l}}g_{l-1}-1\\|_{\\infty}^2}{n_{l-1}}\\ ] ] thus it remains to treat the cross - interaction terms . using the decomposition in ( [ eq : basic ] ) , we obtain @xmath182 =    \\\\ & = \\sum_{2\\le l < q\\le",
    "l }   \\mathbb{e}\\,\\big[a_{l-1}(g , n)a_{q-1}(g , n)\\{\\eta_{l-1}^{n_{l-1}}-\\eta_{l-1}\\}(\\overline{\\varphi_l})\\{\\eta_{q-1}^{n_{q-1}}-\\eta_{q-1}\\}(\\overline{\\varphi_q})\\,\\big]\\\\ & \\hspace{1.5cm}+\\sum_{2\\le l < q\\le l } \\mathbb{e}\\,\\big[\\,a_{l-1}(g , n)\\{\\eta_{l-1}^{n_{l-1}}-\\eta_{l-1}\\}(\\overline{\\varphi_l})\\{\\eta_{q-1}^{n_{q-1}}-\\eta_{q-1}\\}(\\widetilde{\\varphi_q})\\,\\big]\\\\ & \\hspace{1.5cm}+\\sum_{2\\le l < q\\le l } \\mathbb{e}\\,\\big[\\,a_{q-1}(g , n)\\{\\eta_{l-1}^{n_{l-1}}-\\eta_{l-1}\\}(\\widetilde{\\varphi_l})\\{\\eta_{q-1}^{n_{q-1}}-\\eta_{q-1}\\}(\\overline{\\varphi_q})\\,\\big]\\\\ & \\hspace{1.5cm}+\\sum_{2\\le l < q\\le",
    "l } \\mathbb{e}\\,\\big[\\,\\{\\eta_{l-1}^{n_{l-1}}-\\eta_{l-1}\\}(\\widetilde{\\varphi_l})\\{\\eta_{q-1}^{n_{q-1}}-\\eta_{q-1}\\}(\\widetilde{\\varphi_q})\\,\\big]\\ .\\end{aligned}\\ ] ] we will now apply proposition [ prop : prop_corr_bd3 ] to the relevant terms in the sum , to yield the upper - bound : @xmath183 from here one can conclude the proof of theorem [ theo : main_error ] .",
    "this section considers the specification of parameters for the mlsmc algorithm after consideration of theorem [ theo : main_error ] .",
    "recall that in the simpler sde setting of @xcite one must work with the strong error estimate @xmath53 and the deduced variance @xmath184 = { \\mathcal{o}}(h_l^{\\beta})$ ] . from theorem [ theo : main_error",
    "] , a similar role within mlsmc is taken by : @xmath185 we assume that in the given context one can obtain that @xmath186 for some appropriate rate constant @xmath187 .",
    "recall that we have @xmath40 , for some integer @xmath42 and we assume a bias of @xmath188 .",
    "thus , targeting an error tolerance of @xmath60 , we have @xmath189 , so that @xmath62 .",
    "now , to optimally allocate @xmath190 , one proceeds along the lines outlined in the introduction under consideration of theorem [ theo : main_error ] . notice that @xmath191 and @xmath192 is smaller than @xmath35 ( in terms of the obtained upper bounds ) , so the upper bound in theorem [ theo : main_error ] can be bounded by : @xmath193 we also assume a computational cost proportional to @xmath194 , for some rate @xmath195 , with the resampling cost considered to to be negligible for practical purposes compared to the cost of the calculating the importance weights ( as it is the case for the inverse problems we focus upon later ) . as with standard mlmc in @xcite , we need to find @xmath196 that optimize ( [ eq : up ] ) given a fixed computational cost @xmath194 .",
    "such a constrained optimization with the complicated error bound in ( [ eq : up ] ) , results in the need to solve a quartic equation as a function of @xmath35 and @xmath37 .",
    "instead , one can _ assume _ that the second term on the r.h.s .",
    "of ( [ eq : up ] ) is negligible , solve the constrained optimization ignoring that term , and then check that the effect of that term for the given choice of @xmath197 is smaller than @xmath39 .",
    "following this approach gives a constrained optimisation problem identical to the simple case of @xcite , with solution @xmath198 .",
    "one works as in section [ sec : intro ] , and selects : @xmath199 then returning to ( [ eq : up ] ) one can check that indeed the extra summand is smaller than @xmath39 for the above choice of @xmath149 .",
    "notice that : ( i ) @xmath200 , and the sum @xmath201 is dominated by @xmath202 ; ( ii)we have @xmath203 .",
    "therefore , @xmath204 thus , when @xmath205 , the overall mean squared error is still @xmath39 . in the inverse problem context of section [ sec : ip ] , we will establish that @xmath206 , @xmath207 .",
    "also , in many cases ( depending on the chosen pde solver ) we have @xmath208 .",
    "a context will now be introduced in which the results are of interest and the assumptions can be satisfied .",
    "we begin with another round of notations .",
    "introduce the gelfand triple @xmath209 , where the domain @xmath210 will be understood .",
    "furthermore , denote by @xmath211 the inner product and norm on @xmath212 , with superscripts to denote the corresponding inner product and norm on the hilbert spaces @xmath213 and @xmath214 .",
    "denote the finite dimensional euclidean inner product and norms as @xmath215 , with the latter also representing size of a set and absolute value , and denote weighted norms by adding a subscript as @xmath216 , with corresponding norms @xmath217 or @xmath218 for euclidean and @xmath212 spaces , respectively ( for symmetric , positive definite @xmath219 with @xmath220 being the unique symmetric square root ) . in the following , the generic constant @xmath167 will be used for the right - hand side of inequalities as necessary , its precise value actually changing between usage .",
    "let @xmath221 with @xmath222 convex . for @xmath223 , consider the following pde on @xmath210 : @xmath224 where : @xmath225 define @xmath226 , with @xmath227 $ ] i.i.d .",
    "this determines the prior distribution for @xmath228 .",
    "assume that @xmath229 for all @xmath230 and that @xmath231 .",
    "in particular , assume @xmath232 decay it is important that they decay with a suitable rate in order to ensure @xmath228 lives almost surely in an appropriate sequence - space , or equivalently @xmath233 lives in the appropriate function - space .",
    "however , here we down - weight higher frequencies as necessary only to induce certain smoothness properties , while actually for a given value of @xmath104 the resulting permeability @xmath234 for all @xmath235 . ] with @xmath230 .",
    "the state space is @xmath236 $ ] .",
    "it is important that the following property holds : @xmath237 so that the operator on the left - hand side of is uniformly elliptic .",
    "let @xmath238 denote the weak solution of for parameter value @xmath228 .",
    "define the following the vector - valued function @xmath239^\\top\\ , \\ ] ] where @xmath240 are elements of the dual space @xmath214 for @xmath241 .",
    "it is assumed that the data take the form @xmath242 where @xmath243 denotes the gaussian random variable with mean @xmath244 and covariance @xmath245 , and @xmath246 denotes independence .",
    "the unnormalized density then is given by : @xmath247 }   \\ ; \\quad \\phi({\\mathcal{g } } ) = \\tfrac{1}{2}\\ , | { \\mathcal{g}}- y|^2_\\gamma \\ .\\ ] ]    consider the triangulated domains @xmath248 approximating @xmath210 , where @xmath87 indexes the number of nodes @xmath249 , so that we have @xmath250 , with sufficiently regular triangles .",
    "consider a finite element discretization on @xmath251 consisting of @xmath252 functions @xmath253 .",
    "in particular , continuous piecewise linear hat functions will be considered here , the explicit form of which will be given in section [ ssec : numset ] .",
    "denote the corresponding space of functions of the form @xmath254 by @xmath255 , and notice that @xmath256 . by making the further assumption 7 of @xcite that the weak solution @xmath238 of -([eq : bv ] ) for parameter value @xmath228 is in the space @xmath257 , one obtains a well - defined finite element approximation @xmath258 of @xmath238 .",
    "thus , the sequence of distributions of interest in this context is : @xmath259}\\ , \\quad l=0,1,\\ldots , l\\ .\\ ] ]      notice one can take the inner product of with the solution @xmath260 , and perform integration by parts on the right - hand side , in order to obtain @xmath261 .",
    "therefore @xmath262 so the following bound holds in @xmath213 , uniformly over @xmath228 : @xmath263 notice that : @xmath264 so the following uniform bound also holds : @xmath265 the uniform bound on @xmath266 provides the lipschitz bound @xmath267 obtained as follows : @xmath268 setting @xmath269 gives the boundedness of @xmath270 .    considering some sequence @xmath9 indicating the maximum diameter of an individual element at level @xmath87 , with @xmath271 ( e.g. @xmath272 ) , the following asymptotic bound holds for continuous piecewise linear hat functions @xcite @xmath273 furthermore , proposition 29 of @xcite provides a uniform bound based on the following decomposition of : @xmath274 thus",
    ", we have @xmath275 where the first line holds by equivalence of norms , the second holds since @xmath276 , by the triangle inequality and cauchy - schwarz inequality , and the last line holds by and the fact @xmath277 for some @xmath278 . the constant @xmath167 depends on @xmath279 and @xmath278 .",
    "note that @xmath280 by .",
    "note that the bound in together with provides a uniform bound over @xmath87 for @xmath281 , defined by @xmath282 , following the same argument as , which means that the lipschitz bound in holds here over different @xmath87 as well .",
    "now , the following holds by , , , and the triangle inequality @xmath283 hence , from ( [ eq : gunifu ] ) @xmath284 where @xmath167 is independent of the realization of @xmath228 .",
    "[ pr : v ] for @xmath285 one has the following estimates , uniformly in @xmath228 : @xmath286    in combination with , equation ( [ eq : glincrement ] ) gives the stated result .",
    "[ pr : bias ] let @xmath287 .",
    "then @xmath288 - \\mathbb{e}_{\\eta_\\infty}[g(u)]| \\le c h_l\\   .\\ ] ]    it follows from the same reasoning as in proposition [ pr : v ] , upon observing that @xmath123 - \\mathbb{e}_{\\eta_\\infty}[g(u ) ] = \\mathbb{e}_{\\eta_\\infty}\\left[g(u)\\left ( \\frac{d \\eta_l}{d\\eta_\\infty } - 1\\right)\\right]\\   .\\ ] ]      assumption ( a[hyp : a ] ) is satisfied by letting @xmath289 : = _ l 1 _",
    "l  ; : = _ l 1 _",
    "l  .@xmath289 notice that the asymptotic bounds of proposition [ pr : v ] imply that @xmath290 is increasing with @xmath87 while @xmath291 are decreasing with @xmath87 .",
    "therefore , these will actually be minimum and maximum over a sufficiently large set of low indices .",
    "assumption ( a[hyp : b ] ) can be shown to hold in this context , if a gibbs sampler is constructed .",
    "let @xmath292 be the uniform measure on @xmath293 $ ] and consider a probability measure @xmath294 on @xmath295 $ ] with density w.r.t .",
    "the measure @xmath296 : @xmath297 where it is assumed that @xmath298 , @xmath299 $ ] .",
    "this is the setting above , for all @xmath87 , following from equations and .",
    "let @xmath300 be given and consider a partition of @xmath301 $ ] into @xmath230 disjoint subsets @xmath302 .",
    "for example @xmath303 and @xmath304 and @xmath305 are the sets of ( positive ) odd and even numbers up to @xmath109 , respectively .",
    "one can consider the gibbs sampler to generate from @xmath294 , with kernel : @xmath306 with @xmath307^{|\\{a_j\\}| } } \\pi(u_{a_1:a_{j}}',u_{a_{j+1}:a_{k } } ) \\bigotimes_{i\\in(a_j ) } \\theta(du_i')}.\\ ] ] one can , for example , perform rejection sampling on @xmath294 using the prior as a proposal ( and accepting with probability @xmath308 ) and we would still have a theoretical acceptance probability of @xmath309 sampling from the full conditionals will have a higher - acceptance probability and thus the gibbs sampler is not an unreasonable algorithm .    for any @xmath310 @xmath311    consider @xmath312^{|a_j| } }",
    "\\pi(u_{a_1:a_j}',\\tilde{u}_{a_{j+1}:a_k } ) \\bigotimes_{i\\in ( a_j ) } \\theta(du_i ' ) } { \\int_{[-1,1]^{|a_j| } } \\pi(u_{a_1:a_j}',u_{a_{j+1}:a_k})\\bigotimes_{i\\in ( a_j ) } \\theta(du_i')}\\\\ &   \\leq & \\exp\\{2\\phi^*\\}.\\end{aligned}\\ ] ] thus , since @xmath313 and @xmath314 and the final element in each product is identical , it follows that @xmath311 as was to be proved .",
    "in this section a 1d version of the elliptic pde problem in is considered .",
    "let @xmath315 $ ] and consider @xmath316 .",
    "for the prior specification of @xmath228 , set @xmath317 , @xmath318 , @xmath319 , @xmath320 , @xmath321 and @xmath322",
    ". the forward problem at resolution level @xmath87 is solved using a finite element method with piecewise linear shape functions on a uniform mesh of width @xmath323 , for some starting @xmath324 ( so that there are at least two grid - blocks in the finest , @xmath325 , case ) .",
    "thus , on the @xmath326 level the finite - element basis functions are @xmath327 defined as ( for @xmath328 ) @xcite : @xmath329 & \\quad if \\quad x\\in[x_i - h_l , x_i ] , \\quad\\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad \\\\ ( 1/h_l)[x_i+h_l -x ] & \\quad if \\quad x\\in [ x_i , x_i+h_l ] .",
    "\\quad\\quad\\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad\\end{split}\\ ] ] the functional of interest @xmath330 is taken as the solution of the forward problem at the midpoint of the domain , that is @xmath331 .",
    "the observation operator is @xmath332^\\top$ ] , and the observational noise covariance is taken to be @xmath333 .    to solve the pde ,",
    "the ansatz @xmath334 is plugged into , and projected onto each basis element : @xmath335 resulting in the following linear system : @xmath336 where we introduce the matrix @xmath337 with entries @xmath338 , and vectors @xmath339 with entries @xmath340 and @xmath341 , respectively . omitting the index @xmath87 , the matrix is sparse and tridiagonal with @xmath342 and zero otherwise .",
    "the elements @xmath343 are computed analogously .",
    "the system can therefore be solved with cost @xmath344 , corresponding to a computational cost rate of @xmath94 .",
    "to get some understanding about the numerics and validate the theory , a number of results and figures will be generated .",
    "first , the pde solution is obtained for a reference value of @xmath228 on a very fine mesh .",
    "this reference value of @xmath345 is used to numerically obtain the rate @xmath346 in upper bounds of the form @xmath347 for the quantities in , hence also in , over increasing @xmath87 .",
    "then , @xmath149 are optimally allocated using this @xmath346 and the @xmath348 above using the formulae from section [ ssec : multilevel_component ] . following the error analysis in section [ sec :",
    "verify ] , once @xmath346 has been decided , we have @xmath207 . then , observing the cost / error trend for a range of errors @xmath60 , we expect to observe the appropriate scaling between computational cost and mean squared error ( e.g. mse @xmath349 cost@xmath350 for mlsmc ) .      the following setting is simulated .",
    "the sequence of step - sizes is given by @xmath351 , @xmath352 .",
    "the data @xmath353 is simulated with a given @xmath354 $ ] ( i=1,2 ) and @xmath355 . the observation variance and other algorithmic elements",
    "are as stated above .",
    "we will contrast the accuracy of two algorithms .",
    "the first is ( i ) mlsmc as detailed above ; the second is ( ii ) plain smc : the same sequence of distributions as mlsmc , but using equal number of particles for a given @xmath43 , and averaging only the samples at the last level . for both mlsmc and smc algorithms , random walk mcmc kernels",
    "were used ( iterated 10 times ) with scale parameters falling deterministically ( the ratio of standard deviation used for target @xmath22 versus the one for target @xmath356 is set to @xmath357 ) .      to numerically estimate the rate @xmath346 , the quantity @xmath358 is computed over increasing levels @xmath87",
    ". figure  [ fig : rate1 ] shows these computed values plotted against @xmath9 on base-2 logarithmic scales .",
    "a fit of a linear model gives rate @xmath359 , and a similar experiment gives @xmath360 .",
    "this is consistent with the rate @xmath206 and @xmath207 expected from the theoretical error analysis in section [ sec : verify ] ( and agrees also with other literature @xcite ) .",
    "an expensive preliminary mlsmc is executed to get some first results over the algorithmic variabilty . in this execution",
    "the number of particles are set with the recursion @xmath361 and @xmath362 .",
    "the simulations are repeated 100 times .",
    "the estimated variance of @xmath363 , as a proxy of @xmath35 , is plotted in figure  [ fig : var ] against @xmath9 on the same scales as before .",
    "the estimate of the rate now is @xmath364 . in this case",
    "the numerical estimate is much stronger than the theoretical rate used here .",
    "in fact , under suitable regularity conditions one may theoretically obtain the rate @xmath365 with a stronger @xmath366 bound on @xmath367 , which follows from an aubin - nitsche duality argument @xcite .",
    "however , even this stronger estimate is still beat by the empirical estimate . nonetheless , the objective of the present work is to illustrate the theory and not to really optimize the implementation .",
    "in fact , similar results as presented below are obtained using either rate , presumably owing to the fact that @xmath368 , which is already the optimal relationship of @xmath346 and @xmath369 and hence already provides the optimal asymptotic behavior of mse@xmath349cost@xmath350 . in case",
    "an optimal @xmath346 induces a change in the relationship between @xmath346 and @xmath369 , one may expect a change in asymptotic behavior of mse vs. cost , which justifies such empirical rate estimation .",
    ", with @xmath228 equal to the true value used to generate the data , for various choices of @xmath9 . ]",
    "given the choices of @xmath370 and @xmath346 as above , the performance of the mlsmc algorithm is benchmarked by simulating samplers with different maximum levels @xmath43 .",
    "the value of @xmath371 was first estimated with the smc algorithm targeting @xmath372 ( @xmath373 ) , with @xmath374 .",
    "this sampler was realized 100 times and the average of the estimator is take as the ground truth .",
    "the standard deviation is much smaller than the smallest bias of subsequent simulations . when updating @xmath375 , the new bias is approximately a factor @xmath376 smaller than the previous one",
    ". therefore the two sources of error in can be roughly balanced by setting @xmath377 , for @xmath41 , and @xmath378 .",
    "to check the effectiveness of the mcmc steps employed for dispersing the particles within the smc methods , we show in figure  [ fig : acc ] the average ( over the number of particles ) acceptance probability for each of the @xmath43 iterations when the mcmc was executed ( here @xmath379 ) .",
    "the plot indicates reasonable performance of this particular aspect of the sequential algorithm .",
    "the error - vs - cost plots for smc and mlsmc are shown in figure  [ fig : mlsmc ] .",
    "note that the bullets in the graph correspond to different choices of @xmath43 ( ranging from @xmath380 to @xmath381 ) . then , as mentioned earlier , for a given @xmath43 , the single level smc uses a fixed number of particles over the sequence of targets over @xmath41 , and this number is tuned to have approximately the same computational cost as mlsmc with the same @xmath43 .",
    "the mse data points are each estimated with @xmath382 realizations of the given sampler .",
    "the fitted linear model of @xmath383 against @xmath384 has a gradient of @xmath385 and @xmath386 for smc and mlsmc respectively .",
    "this verifies numerically the expected asymptotic behavior mse@xmath349cost@xmath350 for mlsmc , determined from the theory .",
    "furthermore , the first rate indicates that the single level smc performs similarly to the single level vanilla mc with asymptotic behavior mse@xmath349cost@xmath387 .",
    "the results clearly establish the potential improvements of mlsmc versus a standard smc sampler .",
    "it is remarked that the mlsmc algorithm can be improved in many directions and this is subject to future work .",
    "aj , kl & yz were supported by an acrf tier 2 grant : r-155 - 000 - 143 - 112 .",
    "aj is affiliated with the risk management institute and the center for quantitative finance at nus .",
    "rt , kl & aj were additionally supported by king abdullah university of science and technology ( kaust ) .",
    "ab was supported by the leverhulme trust prize .",
    "introduce the following notations . for @xmath388 , @xmath389 and @xmath390 @xmath391 where @xmath392 . define the operator @xmath393 and denote @xmath394 ( @xmath395",
    ", @xmath396 is the identity operator ) .",
    "also set @xmath397 @xmath398 is the identity operator , and define the following @xmath399(\\varphi ) \\ ,   \\nonumber\\\\ r_{p+1}^{n_p}(d_{p , n}(\\varphi ) ) & = & \\frac{\\eta_p^{n_p}(d_{p , n}(\\varphi))}{\\eta_p^{n_p}(g_p)}[\\eta_p(g_p)-\\eta_p^{n_p}(g_p ) ] \\ ,",
    "\\label{eq : r_def}\\end{aligned}\\ ] ] with the convention that @xmath400 .",
    "working similarly to the derivation of ( * ? ? ?",
    "* eq .  ( 6.2 ) ) , but now with varying number of particles , we have that for any @xmath401 @xmath402(\\varphi ) & = [ \\eta_n^{n_n } - \\phi_{n}(\\eta_{n-1}^{n_{n-1}})](\\varphi ) +   [ \\phi_{n } ( \\eta_{n-1}^{n_{n-1 } } ) -   \\eta_n](\\varphi ) \\\\[0.2 cm ]   & = \\frac{v_{n}^{n_n}(\\varphi)}{\\sqrt{n_{n } } } + \\frac{\\eta_{n-1}^{n_{n-1}}(g_{n-1}m_{n}(\\varphi))}{\\eta_{n-1}^{n_{n-1}}(g_{n-1 } ) } - \\frac{\\eta_{n-1}(g_{n-1}m_{n}(\\varphi))}{\\eta_{n-1}(g_{n-1})}\\\\   & = \\frac{v_{n}^{n_n}(\\varphi)}{\\sqrt{n_n } } + r_n^{n_{n-1}}(d_{n-1,n}(\\varphi ) ) + [ \\eta_{n-1}^{n_{n-1 } } -\\eta_{n-1}](d_{n-1,n}(\\varphi))\\end{aligned}\\ ] ] where notice that @xmath403 .",
    "thus , working iteratively we have that @xmath404(\\varphi ) = \\sum_{p=0}^n \\frac{v_p^{n_p}(d_{p , n}(\\varphi))}{\\sqrt{n_p } } +   \\sum_{p=0}^{n-1}r_{p+1}^{n_p}(d_{p , n}(\\varphi ) ) .",
    "\\label{eq : delmoral_decomp}\\ ] ] throughout this section @xmath167 is a constant whose value may change , but does not depend on any time parameters of the feynman - kac formula , nor @xmath405 .          for",
    "this follows from standard calculations in the analysis of feynman - kac formulae ; see e.g. the proof of proposition 2 in @xcite . for .this follows from ( * ? ? ?",
    "* lemma 7.3.3 ) and . for . recall and note",
    "that @xmath417 ; then on application of cauchy - schwarz and assumption ( a[hyp : a ] ) one has that @xmath418 the result follows from ( * ? ? ?",
    "* theorem 7.4.4 ) and",
    ". follows from cauchy - schwarz and .",
    "follows from cauchy - schwarz , and . follows from cauchy - schwarz and .",
    "* @xmath424\\,\\big|    \\leq \\frac{c\\|\\varphi_n\\|_{\\infty}\\|\\varphi_q\\|_{\\infty } \\kappa^{n - q}}{n_q}$ ] . * @xmath425\\,\\big|   \\leq \\frac{c\\|\\varphi_n\\|_{\\infty}\\|\\varphi_q\\|_{\\infty } \\kappa^{n - q}}{n_q^{3/2 } } $ ] . *",
    "@xmath426\\ , \\big| \\leq \\frac{c\\|\\varphi_n\\|_{\\infty}\\|\\varphi_q\\|_{\\infty}}{\\sqrt{n_q}n_n } $ ] . *",
    "@xmath427\\,\\big|   \\leq \\frac{c\\|\\varphi_n\\|_{\\infty}\\|\\varphi_q\\|_{\\infty}}{n_n n_q } $ ] .",
    "i)we start by noting that @xmath431=0 $ ] for any @xmath432 , so that : @xmath433 = \\sum_{0\\le p , s\\le q } \\mathbb{e}\\,\\big[\\,\\tfrac{1}{\\sqrt{n_p n_s}}\\,a_q(g , n_q)v_p^{n_p}(d_{p , n}(\\varphi_n))v_s^{n_s}(d_{s , q}(\\varphi_q))\\,\\big]\\ .\\ ] ] as @xmath434 , one can use lemma [ lem : tech_lem_imp ] , to obtain the upper bound @xmath435",
    "\\ii ) again using @xmath436=0 $ ] for @xmath432 , we have @xmath437 = \\sum_{0\\le p , s\\le q } \\mathbb{e}\\,\\big[\\,\\tfrac{1}{\\sqrt{n_p}}\\,a_q(g , n_q)v_p^{n_p}(d_{p , n}(\\varphi_n))r_{s+1}^{n_s}(d_{s",
    ", q}(\\varphi_q))\\,\\big]\\ .\\ ] ] by @xmath434 and lemma [ lem : tech_lem_imp ] we have the upper - bound @xmath438    \\iii ) by @xmath434 and lemma [ lem : tech_lem_imp ] , we have the upper bound @xmath439\\,\\big|",
    "\\le    \\sum_{p=0}^{n-1 } \\sum_{s=0}^q\\mathbb{e}\\,\\big|\\,\\tfrac{1}{\\sqrt{n_s}}a_q(g , n_q)r_{p+1}^n(d_{p , n}(\\varphi_n))v_s^n(d_{s , q}(\\varphi_q))\\,\\big|\\\\ & \\leq   c\\|\\varphi_n\\|_{\\infty}\\|\\varphi_q\\|_{\\infty }   \\sum_{p=0}^{n-1}\\sum_{s=0}^{q } \\tfrac { \\kappa^{n - p+q - s } } { \\sqrt{n_s}n_p } \\leq \\tfrac{c\\|\\varphi_n\\|_{\\infty}\\|\\varphi_q\\|_{\\infty}}{\\sqrt{n_q}n_n}\\ .\\end{aligned}\\ ] ]    \\iv ) working as in ( iii ) , by @xmath434 and lemma [ lem : tech_lem_imp ] , we have @xmath440\\,\\big| \\le   \\sum_{p=0}^{n-1}\\sum_{s=0}^{q-1}\\mathbb{e}\\,\\big|\\,a_q(g , n_q)r_{p+1}^n(d_{p , n}(\\varphi_n))r_{s+1}^n(d_{s , q}(\\varphi_q))\\,\\big| \\\\ & \\leq    c\\|\\varphi_n\\|_{\\infty}\\|\\varphi_q\\|_{\\infty }   \\sum_{p=0}^{n-1}\\sum_{s=0}^{q } \\tfrac{\\kappa^{n - p+q - s}}{n_p n_s } \\leq \\tfrac{c\\|\\varphi_n\\|_{\\infty}\\|\\varphi_q\\|_{\\infty}}{n_n n_q}\\ .\\end{aligned}\\ ] ]        [ prop : prop_corr_bd3 ] assume ( a[hyp : a]-[hyp : b ] ) .",
    "there exist a @xmath161 , @xmath406 such that for any @xmath422 and @xmath443 , @xmath444 : @xmath445\\,\\big| \\leq c \\|\\varphi_n\\|_{\\infty}\\|\\varphi_q\\|_{\\infty } \\big(\\tfrac{\\kappa^{n - q}}{n_q } + \\tfrac{1}{n_q^{1/2}n_n}\\big ) \\ ] ]    from the definition of @xmath446 , @xmath447 we have that @xmath448 = \\\\    & \\delta_{1,q , n}(f , g,\\varphi_{q},\\varphi_{n } , n_q , n_n ) +    \\tfrac{\\eta_n(g g_n)}{\\eta_n(g_n)}\\,\\delta_{2,q , n}(f,\\varphi_{q},\\varphi_{n } , n_q , n_n)\\end{aligned}\\ ] ] where we have defined @xmath449\\ , \\\\",
    "\\delta_{2,q , n}(f,\\varphi_{q},\\varphi_{n } , n_q , n_n ) & = \\mathbb{e}\\,\\big[\\,a_q(f , n_q)(\\eta_n^{n_n}-\\eta_n)(\\varphi_n)(\\eta_q^{n_q}-\\eta_q)(\\varphi_q)\\,\\big]\\ .",
    "\\end{aligned}\\ ] ]    by lemma [ lem : tech_lem_res1.5 ] and the fact that @xmath450 , we have that @xmath451 thus we concentrate on @xmath452 .",
    "we have via : @xmath453\\ .\\end{aligned}\\ ] ] we will deal with each of the 4 terms on the r.h.s .  separately .",
    "we start with @xmath454 $ ] and work as follows , @xmath455\\,\\big| \\\\ & = \\big|\\,\\sum_{p=0}^{n } \\sum_{s=0}^q \\mathbb{e}\\,\\big[\\,\\tfrac{1}{\\sqrt{n_p n_s}}\\overline{a}_n(g , n_n)a_{q}(f , n_q)v_p^{n_p}(d_{p , n}(\\varphi_n))v_s^{n_s } ( d_{s , q}(\\varphi_q))\\,\\big]\\,\\big|   \\\\ & \\le    \\sum_{p=0}^{n } \\sum_{s=0}^{q-1}\\tfrac{1}{\\sqrt{n_p n_s}}\\,\\|\\overline{a}_n(g , n)\\|_3\\ , \\|v_p^{n_p}(d_{p , n}(\\varphi_n))\\|_{3}\\ , \\|v_s^{n_s}(d_{s",
    ", q}(\\varphi_q))\\|_3\\ \\\\ & \\le    c\\|\\varphi_n\\|_{\\infty}\\|\\varphi_q\\|_{\\infty } \\tfrac{1}{\\sqrt{n_n } } \\sum_{p=0}^{n } \\sum_{s=0}^q \\tfrac{\\kappa^{n - p+q - s}}{\\sqrt{n_p n_s}}\\\\   & \\leq \\tfrac{c\\|\\varphi_n\\|_{\\infty}\\|\\varphi_q\\|_{\\infty}}{n_n\\sqrt{n_q}}\\ .\\end{aligned}\\ ] ] where for the third line we have used @xmath456 and two applications of hlder s inequality ; for the forth line we have used lemma [ lem : tech_lem_imp]([lem : tech_lem2 ] ) and lemma [ lem : tech_lem_res2 ] .    using very similar calculations",
    "one can obtain the upper bounds , @xmath457\\,\\big| & \\le &   \\tfrac{c\\|\\varphi_n\\|_{\\infty}\\|\\varphi_q\\|_{\\infty}}{n_n n_q}\\ , \\\\",
    "\\big|\\,\\mathbb{e}\\,[\\,\\overline{a}_n(g , n_n)a_q(f , n_q)\\mathcal{r}_n(\\varphi_n , n)\\mathcal{v}_q(\\varphi_q , n)\\,]\\,\\big| & \\le & \\tfrac{c\\|\\varphi_n\\|_{\\infty}\\|\\varphi_q\\|_{\\infty}}{n_n^{3/2}n_q^{1/2}}\\ , \\\\ \\big|\\,\\mathbb{e}\\,[\\,\\overline{a}_n(g , n)a_q(f , n)\\mathcal{r}_n(\\varphi_n)\\mathcal{r}_q(\\varphi_q)\\,]\\,\\big| & \\le & \\tfrac{c\\|\\varphi_n\\|_{\\infty}\\|\\varphi_q\\|_{\\infty}}{n_n^{3/2}n_q}.\\end{aligned}\\ ] ] the proof is now complete .                                  , c. , scheichl , r. & teckentrup , a. l.  ( 2013 ) . a hierarchical multilevel markov chain monte carlo algorithm with applications to uncertainty quantification in subsurface flow",
    "arxiv preprint arxiv:1303.7343 ."
  ],
  "abstract_text": [
    "<S> in this article we consider the approximation of expectations w.r.t .  </S>",
    "<S> probability distributions associated to the solution of partial differential equations ( pdes ) ; this scenario appears routinely in bayesian inverse problems . in practice , </S>",
    "<S> one often has to solve the associated pde numerically , using , for instance finite element methods and leading to a discretisation bias , with the step - size level @xmath0 . </S>",
    "<S> in addition , the expectation can not be computed analytically and one often resorts to monte carlo methods . in the context of this problem </S>",
    "<S> , it is known that the introduction of the multilevel monte carlo ( mlmc ) method can reduce the amount of computational effort to estimate expectations , for a given level of error . </S>",
    "<S> this is achieved via a telescoping identity associated to a monte carlo approximation of a sequence of probability distributions with discretisation levels @xmath1 . in many practical problems of interest </S>",
    "<S> , one can not achieve an i.i.d .  </S>",
    "<S> sampling of the associated sequence of probability distributions . </S>",
    "<S> a sequential monte carlo ( smc ) version of the mlmc method is introduced to deal with this problem . </S>",
    "<S> it is shown that under appropriate assumptions , the attractive property of a reduction of the amount of computational effort to estimate expectations , for a given level of error , can be maintained within the smc context . </S>",
    "<S> the approach is numerically illustrated on a bayesian inverse problem .    </S>",
    "<S> multilevel monte carlo , sequential monte carlo , bayesian inverse problems .    </S>",
    "<S> _ ams subject classification _ </S>",
    "<S> : 65c30 , 65y20 . </S>"
  ]
}