{
  "article_text": [
    "sparse vectors are widely used tools in fields related to high dimensional data analytics such as machine learning , compressed sensing and statistics .",
    "this makes estimation of sparse vectors an important field of research . in a compressive sensing setting",
    ", the problem is to closely approximate a @xmath0dimensional signal by an @xmath1sparse vector without losing much information . for regression , this is usually done by observing the inner product of the signal with a design matrix .",
    "it is a well known fact that if the design matrix satisfies the restricted isometry property ( rip ) then estimation can be done efficiently with a sample complexity of @xmath2 .",
    "many algorithms such as cosamp  @xcite , subspace pursuit ( sp )  @xcite and iterative hard thresholding ( iht )  @xcite provide high probability performance guarantees .",
    "baraniuk and others  @xcite came up with a model based sparse recovery framework . under this framework",
    ", the sufficient number of samples for correct recovery is logarithmic with respect to the cardinality of the sparsity model .",
    "a major issue with the model based framework is that it does not provide any recovery algorithm on its own .",
    "in fact , it is some times very hard to come up with an efficient recovery algorithm . addressing this issue , hegde and others",
    "@xcite came up with a weighted graph model for graph structured sparsity and provided a nearly linear time recovery algorithm .",
    "they also analyzed the sufficient number of samples for efficient recovery . in this paper",
    ", we will provide the necessary condition on the sample complexity for sparse recovery on a weighted graph model .",
    "we will also note that our information theoretic lower bound can be applied not only to linear regression but also to other linear prediction tasks such as classification .",
    "the paper is organized as follows .",
    "we describe our setup in section  [ sec : model ] .",
    "then we briefly describe the weighted graph model in section  [ sec : wgm ] .",
    "we state our results in section  [ sec : results ] . in section",
    "[ sec : specific example ] , we apply our technique to some specific examples . at",
    "last , we provide some concluding remarks in section  [ sec : conclusion ] .",
    "in this section , we introduce the observation model for linear prediction and later specify how to use it for specific problems such as linear regression and classification . formally , the problem is to estimate an @xmath1sparse vector @xmath3 from noisy observations of the form , @xmath4 where @xmath5 is the observed output , @xmath6 is the design matrix , @xmath7 is a noise vector and @xmath8 is a fixed function .",
    "our task is to recover @xmath9 from the observations @xmath10 .",
    "linear regression is a special case of the above by choosing @xmath11 .",
    "then we simply have , @xmath12 prior work analyzes the sample complexity of sparse recovery for the linear regression setup .",
    "in particular , if the design matrix @xmath13 satisfies the restricted isometry property ( rip ) then algorithms such as cosamp  @xcite , subspace pursuit ( sp )  @xcite and iterative hard thresholding ( iht )  @xcite can recover @xmath3 quite efficiently and in a stable way with a sample complexity of @xmath14 .",
    "furthermore , it is known that gaussian random matrices ( or sub - gaussian in general ) satisfy rip  @xcite . if we choose our design matrix to be a gaussian matrix and we have a good sparsity model that incorporates extra information on the sparsity structure then we can reduce the sample complexity to @xmath15 where @xmath16 is number of possible supports in the sparsity model , i.e. , the cardinality of the sparsity model  @xcite . in the same line of work , hegde and others",
    "@xcite proposed a weighted graph based sparsity model to efficiently learn @xmath3 .",
    "we can model binary classification problems by choosing @xmath17 or in other words , we can have , @xmath18 similar to the linear regression setup , there is also prior work  @xcite , @xcite , @xcite , on analyzing the sample complexity of sparse recovery for binary classification problem ( also known as 1-bit compressed sensing ) .",
    "since arguments for establishing information theoretic lower bounds are not algorithm specific , we can extend our basic argument to the both settings mentioned above .",
    "for comparison , we will use the results by hegde and others  @xcite in a linear regression setup .",
    "in this section , we introduce the weighted graph model ( wgm ) and formally state the sample complexity results from  @xcite . the weighted graph model is defined on an underlying graph @xmath19 whose vertices are on the coefficients of the unknown @xmath1sparse vector @xmath9 i.e. @xmath20 = \\{1 , 2 , \\dots , d\\}$ ] . moreover ,",
    "the graph is weighted and thus we introduce a weight function @xmath21 . borrowing some notations from  @xcite , for a forest @xmath22 we denote @xmath23 as @xmath24 .",
    "@xmath25 denotes the weight budget , @xmath26 denotes the sparsity ( number of non - zero coefficients ) of @xmath3 and @xmath27 denotes the number of connected components in @xmath28 .",
    "the weight - degree @xmath29 of a node @xmath30 is the largest number of adjacent nodes connected by edges with the same weight , i.e. , @xmath31 we define the weight - degree of @xmath32 , @xmath33 to be the maximum weight - degree of any @xmath30 .",
    "next , we define the weighted graph model on coefficients of @xmath3 as follows :    [ def : wgm ] the @xmath34 is the set of supports defined as @xmath35 \\ | \\ |s| = s \\ and \\ \\exists\\ f \\subseteq g\\ with\\ v_f = s , \\right . \\\\      \\left .",
    "\\gamma(f ) = g,\\ w(f ) \\leq b \\right\\rbrace\\ ,      \\end{aligned}\\ ] ]    where @xmath36 is number of connected components in a forest @xmath28 .",
    "authors in  @xcite provide the following sample complexity result for linear regression under their model :    let @xmath9 be in the @xmath34",
    ". then @xmath37 i.i.d .",
    "gaussian observations suffice to estimate @xmath3 .",
    "more precisely , let @xmath38 be an arbitrary noise vector from equation and @xmath13 be an i.i.d .",
    ". then we can efficiently find an estimate @xmath39 such that @xmath40 where @xmath41 is a constant indepenedent of all variables above .",
    "notice that in the noiseless case @xmath42 , we recover the exact @xmath3 .",
    "we will prove that information - theoretically , the bound on the sample complexity is tight and thus the algorithm of  @xcite is statistically optimal .",
    "in this section , we will state our results for both the noiseless and the noisy case .",
    "we establish an information theoretic lower bound on linear prediction problem defined on wgm .",
    "we use fano s inequality  @xcite to prove our result by carefully constructing an ensemble , i.e. , a wgm . any algorithm which infers @xmath3 from this particular wgm",
    "would require a minimum number of samples .",
    "it follows that in the case of linear regression , the upper bound on the sample complexity by hegde and others  @xcite is indeed tight .      here",
    ", we provide a necessary condition on the sample complexity for exact recovery in the noiseless case .",
    "more formally ,    [ thm : mainresult ] there exists a particular @xmath34 , and a particular set of weights for the entries in the support of @xmath3 such that if we draw a @xmath9 uniformly at random and we have a data set @xmath43 of @xmath44 i.i.d .",
    "observations as defined in equation   with @xmath45 then @xmath46 irrespective of the procedure we use to infer @xmath39 on @xmath47 from @xmath43 .",
    "we use fano s inequality  @xcite on a carefully chosen restricted ensemble to prove our theorem .",
    "a detailed proof can be found in appendix .",
    "a similar result can be stated for the noisy case . however , in this case recovery is not exact but is sufficiently close in @xmath48-norm with respect to noise in the signal .",
    "another thing to note is that in  @xcite inferred @xmath39 can come from a slightly bigger wgm model but here we actually infer @xmath39 from the same wgm .",
    "[ thm : noisymainresult ] there exists a particular @xmath34 , and a particular set of weights for the entries in the support of @xmath3 such that if we draw a @xmath9 uniformly at random and we have a data set @xmath43 of @xmath49 i.i.d .",
    "observations as defined in equation   with @xmath50 then @xmath51 for @xmath52 irrespective of the procedure we use to infer @xmath39 on @xmath47 from @xmath43 .",
    "note that when @xmath53 and @xmath54 then @xmath55 is roughly @xmath56 .",
    "we will prove this result in three steps .",
    "first , we will carefully construct an underlying graph @xmath32 for the wgm .",
    "second , we will bound mutual information between @xmath3 and @xmath43 by bounding the kullback - leibler ( kl ) divergence .",
    "third , we will bound the size of properly defined restricted ensemble to complete our proof .",
    "[ [ constructing - an - underlying - graph - g - for - the - wgm ] ] constructing an underlying graph @xmath32 for the wgm + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we construct an underlying graph for the wgm using the following steps :    * divide @xmath57 nodes equally into @xmath27 groups with each group having @xmath58 nodes .",
    "* for each group @xmath59 , we denote a node by @xmath60 where @xmath59 is the group index and @xmath61 is the node index .",
    "each group @xmath59 , contains nodes from @xmath62 to @xmath63 .",
    "* we allow for circular indexing , i.e. , a node @xmath60 where @xmath64 is same as node @xmath65 . * for each @xmath66 ,",
    "node @xmath60 has an edge with nodes @xmath67 to @xmath68 with weight @xmath69 . * cross edges between nodes in two different groups",
    "are allowed as long as they have edge weights greater than @xmath70 and they do not affect @xmath33 .",
    "figure  [ figgraphconst ] shows an example of a graph constructed using the above steps .",
    "furthermore , parameters for our @xmath71 satisfy the following requirements :    1 .",
    "@xmath72 , 2 .",
    "@xmath73 , 3 .",
    "@xmath54  .",
    "these are quite mild requirements ( see appendix ) on the parameters and are easy to fulfill .",
    "figure  [ figgraphex ] shows one graph which follows our construction and also fulfills r1 , r2 and r3 .",
    "we define our restricted ensemble @xmath74 on @xmath32 as : @xmath75 for some @xmath76 and @xmath77 is as in definition  [ def : wgm ] .",
    "our true @xmath3 is picked uniformly at random from the above restricted ensemble .",
    "we will prove that on this restricted ensemble , our theorem  [ thm : noisymainresult ] holds .",
    "we will make use of following lemmas for our proof :    [ lemma : ballcovering ] given the restricted ensemble @xmath74 , @xmath78    we are dealing with high dimensional cases , hence moving forward we will assume that @xmath79 .",
    "we state another lemma :    [ lemma : ebound ] for some @xmath76 , @xmath80    from lemma  [ lemma : ballcovering ] , lemma  [ lemma : ebound ] and using the fact that @xmath81 and @xmath82 , the corollary below follows :    [ cor : concen bound ] @xmath83    [ [ bound - on - the - mutual - information ] ] bound on the mutual information + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we will assume that the elements of design matrix @xmath13 have been chosen at random and independently from @xmath84 .",
    "the linear prediction problem from section  [ sec : model ] can be described by the following markov s chain : @xmath85 lets say @xmath43 contains @xmath86 i.i.d .",
    "observations of @xmath10 and @xmath87 contains @xmath86 i.i.d .",
    "observations of @xmath88 . then using the data processing inequality",
    "@xcite we can say that , @xmath89 hence , for our purpose it suffices to have an upper bound on @xmath90 .",
    "now we can bound the mutual information by the following  @xcite : @xmath91 where @xmath92 is the kullback - leibler divergence .",
    "note that @xmath87 consists of @xmath86 i.i.d .",
    "observations of @xmath88 .",
    "hence , @xmath93 furthermore , from equation   and noting that the elements of @xmath13 come independently from @xmath94 , @xmath95 we can bound the kullback - leibler divergence between @xmath96 and @xmath97 as follows : @xmath98 the first inequality holds because @xmath99 , the second inequality holds by taking the largest value of numerators and the smallest value of denominators .",
    "the other inequalities follow from simple algebraic manipulation . substituting @xmath100 in equation",
    "we get , @xmath101    [ [ bound - on - mathcalf ] ] bound on @xmath102 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    now we will count elements in @xmath74 to complete our proof .",
    "we present the following counting argument to establish a lower bound on all the possible supports for our restricted ensemble :    1 .",
    "we choose one node from each of the @xmath27 groups in underlying graph @xmath32 to be root of a connected component .",
    "each group has @xmath58 possible candidates for the root and hence we can choose them in @xmath103 possible ways .",
    "since we are interested only in establishing a lower bound on @xmath74 , we will only consider the cases where each connected component has @xmath104 nodes .",
    "moreover , given a root node @xmath60 in group @xmath59 , we will choose the remaining @xmath105 nodes connected with the root only from the nodes @xmath106 to nodes @xmath107 ( using circular indices if needed ) .",
    "construction of the graph @xmath32 allows us to do this . at least till the last @xmath108 nodes",
    ", we always include node @xmath60 and we never include @xmath109 in our selection .",
    "furthermore , r1 guarantees that we have enough nodes to avoid any possible repetitions due to circular indices for the last @xmath110 nodes and r2 ensures that we have enough nodes to form a connected component .",
    "this guarantees that all the supports are unique .",
    "hence , given a root node @xmath60 we have @xmath111 choices which across all the groups comes out to be @xmath112 .",
    "each entry in the support of @xmath113 can take two values which can either be @xmath114 or @xmath115 .",
    "it should be noted that any support chosen using the above steps satisfies constraint on weight budget , i.e. , @xmath116 as the maximum edge weight in any connected component will always be less than or equal to @xmath70 . combining all the above steps together we get : @xmath117 using fano s inequality  @xcite and results from equation   and equation  ,",
    "it is easy to prove the following lemma ,    [ lemma : fano ] if @xmath118 then @xmath119 .    by using bayes theorem and combining corollary  [ cor : concen bound ] and lemma  [ lemma : fano ] , @xmath120 the last inequality   holds when @xmath86 is @xmath121 .",
    "we also know that @xmath122 and if we choose @xmath123 , then we can write inequality   as , @xmath124",
    "here , we will provide counting arguments for some of the well - known sparsity structures , such as tree sparsity and block sparsity models .",
    "it should be noted that barring the count of possible supports in the specific model our technique can be used to prove lower bounds of the sample complexity for other sparsity structures .",
    "the tree - sparsity model  @xcite ,  @xcite is used in many applications such as wavelet decomposition of piecewise smooth signals and images . in this model",
    ", we assume that the coefficients of the @xmath1sparse signal form a @xmath125ary tree and the support of the sparse signal form a rooted and connected sub - tree on @xmath26 nodes in this @xmath125ary tree . the arrangement is such that if a node is part of this subtree then its parent is also included in it . here",
    ", we will discuss the case of a binary tree which can be generalized to a @xmath125ary tree .",
    "in particular , the following proposition provides a lower bound on the number of possible supports of an @xmath1sparse signal following a binary tree - structured sparsity model .",
    "[ prop : tree sparse ] in a binary tree - structured sparsity model @xmath74 where @xmath126 , @xmath127 for some @xmath128 .    from the above and following the same proof technique as before , it is easy to prove the following corollary for noisy case ( a similar result holds for noiseless case as well ) .",
    "[ cor : tree sparse ] in a binary tree - structured sparsity model , if @xmath129 then @xmath130 .",
    "essentially , corollary  [ cor : tree sparse ] proves that the @xmath131 sample complexity achieved in  @xcite is optimal for the tree - sparsity model .      in the block sparsity model ,",
    "@xcite , an @xmath1sparse signal , @xmath132 , can be represented as a matrix with @xmath133 rows and @xmath134 columns .",
    "the support of @xmath113 comes from @xmath135 columns of this matrix such that @xmath136 .",
    "more precisely ,    @xmath137\\in \\mathbb{r}^{j \\times n } \\ such\\ that\\ \\right . \\\\",
    "\\beta_n = 0\\ for\\ n \\notin l,\\ l \\subseteq \\{1,\\dots , n\\},\\ |l|=k \\right\\rbrace \\ .",
    "\\end{aligned}\\ ] ]    the above can be modeled as a graph model . in particular",
    ", we can construct a graph @xmath32 over all the elements in @xmath113 by treating nodes in the column of the matrix as connected nodes ( see fig .",
    "[ figblock ] ) and then our problem is to choose @xmath135 connected components from @xmath134 .",
    "it is easy to see that the number of possible supports in this model , @xmath74 , would be , @xmath138 .",
    "correspondingly the necessary number of samples for efficient signal recovery comes out to be @xmath139 .",
    "an upper bound of @xmath140 was derived in  @xcite which matches our lower bound .",
    "we proved that the necessary number of samples required to efficiently recover a sparse vector in the weighted graph model is of the same order as the sufficient number of samples provided by hegde and others  @xcite .",
    "moreover , our results not only pertain to linear regression but also apply to linear prediction problems in general .",
    "1    r.  g. baraniuk , v.  cevher , m.  f. duarte , and c.  hegde , `` model - based compressive sensing , '' _ ieee transactions on information theory _ ,",
    "56 , no .  4 , pp .",
    "19822001 , 2010 .    c.  hegde , p.  indyk , and l.  schmidt , `` a nearly - linear time framework for graph - structured sparsity , '' in _ proceedings of the 32nd international conference on machine learning ( icml-15 ) _ , pp .",
    "928937 , 2015 .",
    "thomas blumensath and mike  e davies .",
    "iterative hard thresholding for compressed sensing . , 27(3):265274 , 2009 .",
    "wei dai and olgica milenkovic .",
    "subspace pursuit for compressive sensing : closing the gap between performance and complexity .",
    "technical report , dtic document , 2008 .",
    "deanna needell and joel  a tropp .",
    "cosamp : iterative signal recovery from incomplete and inaccurate samples .",
    ", 26(3):301321 , 2009 .",
    "richard baraniuk , mark davenport , ronald devore , and michael wakin . a simple proof of the restricted isometry property for random matrices . , 28(3):253263 , 2008 .",
    "chinmay hegde , piotr indyk , and ludwig schmidt .",
    "a fast approximation algorithm for tree - sparse recovery . in _ 2014 ieee international symposium on information theory _ , pages 18421846 .",
    "ieee , 2014 .    b.  yu .",
    "ssouad , ano , and e am . in torgersen",
    "e. pollard  d. and yang g. , editors , _ festschrift for lucien",
    "le cam : research papers in probability and statistics _ , pages 423435 .",
    "springer new york , 1997 .",
    "ankit gupta , robert  d nowak , and benjamin recht .",
    "sample complexity for 1-bit compressed sensing and sparse classification . in _ isit",
    "_ , pages 15531557 , 2010 .",
    "sivakant gopi , praneeth netrapalli , prateek jain , and aditya  v nori .",
    "one - bit compressed sensing : provable support and vector recovery . in _",
    "icml ( 3 ) _ , pages 154162 , 2013 .",
    "t.  cover and j.  thomas . .",
    "john wiley & sons , 2nd edition , 2006 .",
    "albert ai , alex lapanowski , yaniv plan , and roman vershynin .",
    "one - bit compressed sensing with non - gaussian measurements .",
    ", 441:222239 , 2014 .",
    "first note that when @xmath141 then its obvious that lemma  [ lemma : ballcovering ] holds . here",
    ", we will prove that two arbitrarily chosen @xmath142 and @xmath143 such that @xmath144 where @xmath145 then @xmath146 .",
    "@xmath74 is as defined in equation  .",
    "[ [ beta_1-and - beta_2-have - the - same - support ] ] @xmath142 and @xmath143 have the same support + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    since we assume that @xmath145 , thus they must differ in at least one position on their support .",
    "lets say that one such position is @xmath61 .",
    "then , @xmath147    [ [ beta_1-and - beta_2-have - different - supports ] ] @xmath142 and @xmath143 have different supports + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    when @xmath142 and @xmath143 have different supports then we can always find @xmath61 and @xmath59 such that @xmath148 and @xmath149 where @xmath150 and @xmath151 are supports of @xmath142 and @xmath143 respectively . then , @xmath152 since this is true for any two arbitrarily chosen @xmath142 and @xmath143 , hence it holds for @xmath3 and @xmath39 as well .",
    "this proves the lemma .",
    "@xmath153}{\\exp\\big(\\frac{\\lambda}{2 } \\frac{n}{1-\\epsilon}\\big)}\\\\      & = \\exp\\big(\\frac{-\\lambda}{2 } \\frac{n}{1 -\\epsilon}\\big ) \\big(\\frac{1}{1-\\lambda}\\big)^{\\frac{n}{2 } } \\ .",
    "\\end{aligned}\\ ] ]    the first equality holds for any @xmath154 , we take @xmath155 .",
    "the second inequality comes from markov s inequality .",
    "the last equality follows since @xmath156 .",
    "now , by taking @xmath157 , @xmath158 the last inequality holds because for @xmath76 , @xmath159 .",
    "this proves our lemma , @xmath160",
    "using fano s inequality  @xcite , we can say that , @xmath161 the first inequality follows from equation   and the second inequality follows from the upper bound on the mutual information established in equation  . now , we want @xmath162 , then it follows that @xmath86 must be , @xmath163 this proves the lemma .",
    "[ [ constructing - an - underlying - graph - g ] ] constructing an underlying graph @xmath32 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we assume that our underlying graph @xmath32 fulfills all the properties mentioned while proving theorem  [ thm : noisymainresult ] . on this underlying graph @xmath32 ,",
    "we define our restricted ensemble @xmath74 as : @xmath164 where @xmath77 is as in definition  [ def : wgm ] .",
    "[ [ bound - on - the - mutual - information-1 ] ] bound on the mutual information + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we will assume that the elements of design matrix @xmath13 have been chosen at random and independently from @xmath165 .",
    "as in the proof of theorem  [ thm : noisymainresult ] , we can describe noiseless linear prediction problem as the following markov s chain : @xmath166 lets say @xmath43 contains @xmath86 i.i.d . observations of @xmath10 and @xmath87 contains @xmath86 i.i.d",
    ". observations of @xmath88 . then using the data processing inequality  @xcite",
    ", we can say that , @xmath167 hence , for our purpose it suffices to have an upper bound on @xmath90 . now using results from  @xcite , @xmath168 where @xmath92 is the kullback - leibler divergence .",
    "note that @xmath87 consists of @xmath86 i.i.d .",
    "observations of @xmath88 .",
    "hence , @xmath169 furthermore from equation   and noting that the elements of @xmath13 come independently from @xmath170 , @xmath171 we can bound @xmath100 by , @xmath172 substituting @xmath100 in equation   we get , @xmath173    [ [ bound - on - mathcalf-1 ] ] bound on @xmath102 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    using a similar counting logic used in theorem  [ thm : noisymainresult ] , we can get : latexmath:[\\[\\begin{aligned } \\label{eq : bound on f noiseless }    we prove the theorem by substituting the mutual information from equation   and @xmath102 from equation   in the fano s inequality  @xcite .",
    "we know that the connected subtree @xmath175 on the support of @xmath113 is rooted . hence , it will have a left subtree ( possibly with zero nodes ) say @xmath176 and a right subtree ( again possibly with zero nodes ) say @xmath177 hanging from its root . counting from @xmath176 first",
    "lets say that the root is @xmath178 element of the support in a particular arrangement .",
    "then , @xmath179 is the number of elements in @xmath74 where the root is the @xmath178 element of the support .",
    "since we are only interested in the lower bound on @xmath180 , we will only count the cases where each node in @xmath176 or @xmath177 is at a different height .",
    "we can always do it since @xmath126 .",
    "this way , it is easy to see that , @xmath181 using equation  , @xmath182 discussion on the requirements for the underlying graph @xmath32 -----------------------------------------------------------------------      [ prop : mild requirements ] given any value of @xmath183 and @xmath184 , there are infinitely many choices for @xmath33 and @xmath57 that satisfy r1 and r2 and hence , there are infinitely many @xmath185-wgm which follow our construction .",
    "r3 is readily satisfied if each edge has at least unit edge weight and we are not forced to choose isolated nodes in support .",
    "most of the graph - structured sparsity models fulfill this requirement .",
    "r2 gives us a lower bound on the choice of @xmath33 , @xmath186 similarly , given a value of @xmath33 , r1 just provides a lower bound on choice of @xmath57 , @xmath187 clearly , there is an infinite number of combinations for @xmath33 and @xmath57 ."
  ],
  "abstract_text": [
    "<S> we analyze the necessary number of samples for sparse vector recovery in a noisy linear prediction setup . </S>",
    "<S> this model includes problems such as linear regression and classification . </S>",
    "<S> we focus on structured graph models . </S>",
    "<S> in particular , we prove that sufficient number of samples for the weighted graph model proposed by hegde and others  @xcite is also necessary . </S>",
    "<S> we use the fano s inequality  @xcite on well constructed ensembles as our main tool in establishing information theoretic lower bounds .    </S>",
    "<S> [ [ keywords ] ] keywords : + + + + + + + + +    compressive sensing , linear prediction , classification , fano s inequality , mutual information , kullback leibler divergence . </S>"
  ]
}