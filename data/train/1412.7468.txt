{
  "article_text": [
    "with rapid advances of modern technology , high - throughput data sets of unprecedented size , such as genetic and proteomic data , fmri and functional data , and panel data in economics and finance , are frequently encountered in many contemporary applications . in these applications , the dimensionality @xmath1 can be comparable to or even much larger than the sample size @xmath2 .",
    "a key assumption that often makes large - scale inference feasible is the sparsity of signals , meaning that only a small fraction of covariates contribute to the response when @xmath1 is large compared to @xmath2",
    ". high - dimensional modeling with dimensionality reduction and feature selection plays an important role in these problems .",
    "a sparse modeling procedure typically produces a sequence of candidate models , each involving a possibly different subset of covariates .",
    "an important question is how to compare different models in high dimensions when models are possibly misspecified .",
    "the problem of model selection has a long history with numerous contributions by many researchers . among others ,",
    "well - known model selection criteria are the aic ( akaike , 1973 and 1974 ) and bic ( schwarz , 1978 ) , where the former is based on the kullback - leibler ( kl ) divergence principle of model selection and the latter is originated from the bayesian principle . a great deal of work",
    "has been devoted to understanding and extending these methods .",
    "see , for example , bozdogan ( 1987 ) , foster and george ( 1994 ) , konishi and kitagawa ( 1996 ) , ing ( 2007 ) , chen and chen ( 2008 ) , chen and chan ( 2011 ) , ing and lai ( 2011 ) , liu and yang ( 2011 ) , and chang et al .",
    "( 2014 ) in different model settings .",
    "the connections between the aic and cross - validation have been investigated in stone ( 1977 ) , hall ( 1990 ) , and peng et al .",
    "( 2013 ) in various contexts .",
    "model selection criteria such as aic and bic are frequently used for tuning parameter selection in regularization methods .",
    "for instance , mode selection in the context of penalized likelihood methods has been studied in fan and li ( 2001 ) , wang et al .",
    "( 2007 ) , wang et al . ( 2009 ) , zhang et al . ( 2010 ) , and fan and tang ( 2013 ) .",
    "in particular , fan and tang ( 2013 ) showed that classical information criteria such as aic and bic can be inconsistent for model selection when the dimensionality @xmath1 grows very fast relative to sample size @xmath2 .",
    "most existing work on model selection usually makes an implicit assumption that the model under study is correctly specified or of fixed dimensions .",
    "for example , white ( 1982 ) laid out a general theory of maximum likelihood estimation in misspecified models for the case of fixed dimensionality and independent and identically distributed ( i.i.d . ) observations .",
    "recently , lv and liu ( 2014 ) investigated the problem of model selection with model misspecification and derived asymptotic expansions of both kl divergence and bayesian principles in misspecified generalized linear models , leading to the generalized aic and generalized bic , for the case of fixed dimensionality .",
    "a specific form of prior probabilities motivated by the kl divergence principle leads to the generalized bic with prior probability ( @xmath0-l in lv and liu ( 2014 ) for the high - dimensional counterpart . ] ) .",
    "yet model misspecification and high dimensionality are both common in real applications .",
    "thus a natural and important question is how to characterize the impact of model misspecification on model selection in high dimensions .",
    "we intend to provide some answer to this question in this paper .",
    "our analysis enables us to suggest the generalized bic with prior probability ( @xmath0 ) that involves a logarithmic factor of the dimensionality in penalizing model complexity .    to gain some insights into the challenges of the aforementioned problem ,",
    "let us consider a motivating example .",
    "assume that the response @xmath3 depends on the covariate vector @xmath4 through the functional form @xmath5 where @xmath6 and the remaining setting is as specified in section [ sec::simu - multiple - index ] .",
    "consider sample size @xmath7 and vary dimensionality @xmath1 from 200 to 3200 . without prior knowledge about the true model structure ,",
    "we take the linear regression model @xmath8 as the working model , with the same notation therein , and apply some information criteria to hopefully recover the oracle working model consisting of the first five covariates .",
    "when @xmath9 , the traditional aic and bic , which ignore model misspecification , tend to select a model with size larger than five .",
    "as expected , @xmath0-l works reasonably well by selecting the oracle working model half of the time .",
    "however , when @xmath1 is increased to @xmath10 , these methods fail to select such a model with significant probability and the prediction performance of the selected models deteriorates .",
    "this motivates us to study the problem of model selection in high - dimensional misspecified models .",
    "in contrast , our newly suggested @xmath0 can recover the oracle working model with significant probability in this challenging scenario .",
    "the main contributions of our paper are threefold .",
    "first , we establish a systematic theory of model selection with model misspecification in high dimensions .",
    "the asymptotic expansions for different model selection principles involve delicate and challenging technical analysis .",
    "second , our work provides rigorous theoretical justification of the covariance contrast matrix estimator that incorporates the effect of model misspecification and is crucial for practical implementation .",
    "such an estimator is shown to be consistent in the general setting of high - dimensional misspecified models .",
    "third , we suggest the use of a new prior in the expansion for @xmath0 involving the @xmath11 term .",
    "this criterion has connections to the model selection criteria in chen and chen ( 2008 ) and fan and tang ( 2013 ) with the @xmath11 factor for the case of correctly specified models .",
    "the rest of the paper is organized as follows .",
    "section [ sec::definition ] introduces the setup for model misspecification .",
    "we present some key asymptotic properties of the quasi - maximum likelihood estimator and provide asymptotic expansions of kl divergence and bayesian model selection principles in high dimensions in section [ sec::results ] .",
    "section [ sec::numerical ] demonstrates the performance of different model selection criteria in high - dimensional misspecified models through several simulation and real data examples .",
    "we provide some discussions of our results and possible extensions in section [ sec::discussions ] .",
    "the proofs of some main results are relegated to the appendix .",
    "additional technical proofs and numerical results are provided in the supplementary material .",
    "assume that conditional on the covariates , the @xmath2-dimensional random response vector @xmath12 has a true unknown distribution @xmath13 with density function @xmath14 where @xmath15 .",
    "model ( [ 001 ] ) entails that all components of @xmath16 are independent but not necessarily identically distributed .",
    "consider a set of @xmath17 covariates out of all @xmath1 available covariates , where @xmath1 can be much larger than @xmath2 .",
    "denote by @xmath18 the corresponding @xmath19 deterministic design matrix . to simplify the technical presentation , we focus on the case of deterministic design . in practice",
    ", one chooses a family of working models to fit the data .",
    "model misspecification generally occurs when the family of distributions is misspecified or some true covariates are missed .",
    "since the true model @xmath13 is unknown , we choose a family of generalized linear models ( glms ) @xmath20 with a canonical link as our working models , each of which has density function @xmath21 d\\mu(z_i),\\ ] ] where @xmath22 , @xmath23 with @xmath24 , @xmath25 is a smooth convex function , @xmath26 is the lebesgue measure , and @xmath27 is some fixed measure on @xmath28 .",
    "assume that @xmath29 is continuous and bounded away from @xmath30 , @xmath18 is of full column rank @xmath17 , and @xmath31 are bounded .",
    "clearly @xmath32 is a family of distributions in the regular exponential family and may not contain @xmath33 s .    to ease the presentation ,",
    "define two vector - valued functions @xmath34 and @xmath35 , and a matrix - valued function @xmath36 . for any @xmath2-dimensional random vector @xmath37 with distribution @xmath38 given by ( [ 003 ] )",
    ", it holds that @xmath39 and @xmath40 . the density function ( [ 003 ] )",
    "can be rewritten as @xmath41 \\prod_{i = 1}^n \\frac{d\\mu}{d\\mu_0}(z_i),\\ ] ] where @xmath42 denotes the radon - nikodym derivative .",
    "given the observations @xmath43 and @xmath18 , this gives the quasi - log - likelihood function @xmath44 the quasi - maximum likelihood estimator ( qmle ) of the @xmath17-dimensional parameter vector @xmath45 is defined as @xmath46 which is the solution to the score equation @xmath47 = { \\mbox{\\bf 0}}$ ] .",
    "this equation becomes the normal equation @xmath48 in the linear regression model .",
    "the kl divergence ( kullback and leibler , 1951 ) of the model @xmath49 from the true model @xmath13 can be written as @xmath50 .",
    "the best working model that is closest to the true model under the kl divergence has parameter vector @xmath51 , which solves the equation @xmath52 = { \\mbox{\\bf 0}}.\\end{aligned}\\ ] ] we introduce two matrices that play a key role in model selection with model misspecification .",
    "define @xmath53 = { \\mathrm{cov}}\\left({\\mbox{\\bf x}}\\t { \\mbox{\\bf y}}\\right ) = { \\mbox{\\bf x}}\\t { \\mathrm{cov}}({\\mbox{\\bf y } } ) { \\mbox{\\bf x}}= { \\mbox{\\bf b}}_n\\ ] ] with @xmath54 by the independence assumption , @xmath55 and @xmath56 . observe that @xmath57 and @xmath58 are the covariance matrices of @xmath59 under the best misspecified glm @xmath60 and the true model @xmath13 , respectively .",
    "we now present the asymptotic expansions of both kl divergence and bayesian model selection principles in high - dimensional misspecified glms .",
    "we list a few technical conditions required to prove the asymptotic properties of qmle with diverging dimensionality .",
    "denote by @xmath61 the vector @xmath62-norm and the matrix operator norm .",
    "[ cond1 ] there exists some constant @xmath63 such that for each @xmath64 , @xmath65 for any @xmath66 , where @xmath67 .",
    "[ condition : min_ev_b_n and min_ev_v ] there exist positive constants @xmath68 , @xmath69 , and @xmath70 such that for sufficiently large @xmath2 , @xmath71 and @xmath72 , where @xmath73 , @xmath74 , and @xmath75 . moreover , @xmath76 .",
    "[ condition : yi_deviation and sub - gaussian ] assume @xmath77 and @xmath78 .",
    "[ condition : widetilde_v_n ] assume @xmath79 where @xmath80 and @xmath81 @xmath82 with @xmath83 a @xmath84 matrix with @xmath85th row the corresponding row of @xmath86 for each @xmath87 .",
    "moreover , @xmath88 is a polynomial order of @xmath2 .",
    "conditions [ cond1 ] and [ condition : min_ev_b_n and min_ev_v ] are some basic assumptions for establishing the consistency of the qmle @xmath89 in theorem [ thm5 ] . in particular , condition [ cond1 ] assumes that the standardized response has sub - gaussian distribution which facilitates the derivation of the deviation probability bound .",
    "conditions [ condition : min_ev_b_n and min_ev_v][condition : widetilde_v_n ] are similar to those in lv and liu ( 2014 ) , except for some major differences due to the high - dimensional setting .",
    "in particular , condition [ condition : min_ev_b_n and min_ev_v ] allows the minimum eigenvalue of @xmath90 to converge to zero at a certain rate as @xmath2 increases in a neighborhood @xmath91 of @xmath92 .",
    "such a neighborhood is wider compared to that for the case of fixed dimensionality .",
    "the dimensionality @xmath17 of the qmle is allowed to diverge with @xmath2 .",
    "conditions [ condition : yi_deviation and sub - gaussian ] and [ condition : widetilde_v_n ] are imposed to establish the asymptotic normality of @xmath89 .    _ ( consistency of qmle)_. [ thm5 ] under conditions [ cond1][condition : min_ev_b_n and min_ev_v ] , the qmle @xmath93 satisfies @xmath94 and further @xmath95 with probability @xmath96 for some large positive constant @xmath97 .    _",
    "( asymptotic normality)_. [ thm6 ] under conditions [ cond1][condition : widetilde_v_n ] , the qmle @xmath93 satisfies @xmath98 where @xmath99 and @xmath100 is any @xmath101 matrix such that @xmath102 .",
    "theorems [ thm5 ] and [ thm6 ] establish the consistency and asymptotic normality of the qmle in high - dimensional misspecified glm .",
    "these results provide the theoretical foundation for the technical analyses in sections [ sec2.5][sec2.3 ] .",
    "the asymptotic theory of the qmle reduces to that of the maximum likelihood estimator ( mle ) when the model is correctly specified .",
    "our results extend those in lv and liu ( 2014 ) for the case of fixed dimensionality .",
    "we next introduce a few additional conditions for deriving the asymptotic expansions of the two model selection principles .    [",
    "condition : region ] there exists some constant @xmath103 with @xmath104 such that @xmath105 and for sufficiently large @xmath2 , @xmath106 , where constant @xmath97 is given in theorem [ thm5 ] .",
    "[ condition : prior density and rho_n ] assume that @xmath107 satisfies @xmath108 with @xmath109 some constants , and @xmath110 .",
    "[ condition : lipschitz ] assume that @xmath111 , @xmath112 , and @xmath113 \\circ [ { \\mbox{\\boldmath $ \\mu$}}({\\mbox{\\bf x}}{\\mbox{\\boldmath $ \\beta$ } } ) - { \\mbox{\\boldmath $ \\mu$}}({\\mbox{\\bf x}}{\\mbox{\\boldmath $ \\beta$}}_{n,0 } ) ] \\}{\\mbox{\\bf x}}$ ] are lipschitz ( in operator norm ) with constant @xmath114 in @xmath91 , and @xmath115 with constant @xmath116 , where @xmath117 represents the hadamard ( componentwise ) product and @xmath118 denotes the entrywise matrix @xmath119-norm .",
    "[ condition : bias_variance ] assume @xmath120 ^ 2/{\\mathrm{var}}(y_i)\\}^2 = o(n^{\\alpha_3})$ ] with some constant @xmath121 .    the first part of condition [ condition : region ] holds naturally for linear and logistic regression models , and is introduced to accommodate the case of poisson regression .",
    "the second part of condition [ condition : region ] is a mild assumption ensuring that the restricted qmle coincides with its unrestricted version with significant probability , which is key to the asymptotic expansion of the kl divergence principle in high dimensions in theorem [ thm3 ] .",
    "it is worth mentioning that the set @xmath122 grows with @xmath2 , while the neighborhood @xmath91 is asymptotically shrinking .    condition [ condition : prior density and rho_n ] is similar to the one in lv and liu ( 2014 ) , except that we need to specify the rate at which @xmath123 converges to zero .",
    "condition [ condition : lipschitz ] requires the lipschitz property for those matrix - valued functions .",
    "the bound on the entry - wise matrix @xmath124-norm of the design matrix is mild .",
    "condition [ condition : bias_variance ] is a sensible assumption bounding the effect of the model bias . in particular , conditions",
    "[ condition : lipschitz ] and [ condition : bias_variance ] are introduced only for proving the consistency of the covariance contrast matrix in the general setting in theorem [ thm4 ] .      given a sequence of subsets @xmath125 of the full model @xmath126",
    ", we can construct a sequence of qmle s @xmath127 by fitting the glm ( [ 003 ] ) .",
    "a natural question is how to compare those fitted models .",
    "the qmles @xmath127 become the mles when the model is correctly specified .",
    "akaike s principle of model selection is choosing the model @xmath128 that minimizes the kl divergence @xmath129 of the fitted model @xmath130 from the true model @xmath13 , that is , @xmath131 where @xmath132 with @xmath133 and @xmath134 an independent copy of @xmath16 .",
    "thus @xmath135 which shows that akaike s principle of model selection is equivalent to choosing the model @xmath128 that maximizes the expected log - likelihood with the expectation taken with respect to an independent copy of @xmath16 . using the asymptotic theory of mle , akaike ( 1973 ) showed that for the case of i.i.d .",
    "observations , @xmath136 can be asymptotically expanded as @xmath137 for simplicity , we drop the last term in ( [ 002 ] ) which does not depend on @xmath45 , and redefine the quasi - log - likelihood as @xmath138 hereafter .",
    "[ thm3 ] under conditions [ cond1][condition : region ] , we have with probability tending to one , @xmath139 where @xmath140 .",
    "theorem [ thm3 ] generalizes the corresponding result in lv and liu ( 2014 ) to high dimensions . however , we would like to point out that our new technical analysis differs substantially from theirs due to the challenges of diverging dimensionality .",
    "the asymptotic expansion in theorem [ thm3 ] enables us to introduce the generalized aic ( gaic ) as follows .",
    "[ def::gaic ] we define @xmath141 of model @xmath142 as @xmath143 where @xmath144 is a consistent estimator of @xmath145 specified in section [ sec::est - h - n ] .",
    "when the model is correctly specified , it holds that @xmath146 , under which gaic reduces to aic asymptotically .",
    "we demonstrate in the simulation studies that gaic can improve over the original aic substantially in the presence of model misspecification .      from the asymptotic expansions for the gaic , gbic , and @xmath0 ( the latter two to be introduced in section [ sec2.3 ] )",
    ", a common term is the covariance contrast matrix @xmath145 , which characterizes the impact of model misspecification .",
    "therefore , providing an accurate estimator for such a matrix @xmath145 is of vital importance in the application of these information criteria .    consider the plug - in estimator @xmath147 with @xmath148 and @xmath149 defined as follows .",
    "since the qmle @xmath93 provides a consistent estimator of @xmath150 in the best misspecified glm @xmath151 , a natural estimate of matrix @xmath57 is given by @xmath152 when the model is correctly specified , the following simple estimator @xmath153 \\circ \\left[{\\mbox{\\bf y}}- { \\mbox{\\boldmath $ \\mu$}}({\\mbox{\\bf x}}{\\widehat{\\mbox{\\boldmath $ \\beta$}}}_n)\\right]\\right\\ } { \\mbox{\\bf x}}\\ ] ] gives an asymptotically unbiased estimator of @xmath58 .",
    "( consistency of estimator ) [ thm4 ] assume that conditions [ cond1][condition : yi_deviation and sub - gaussian ] and [ condition : lipschitz][condition : bias_variance ] hold , the eigenvalues of @xmath154 and @xmath155 are bounded away from 0 and @xmath156 , and @xmath157 . then the plug - in estimator @xmath144 satisfies @xmath158 and @xmath159 .",
    "theorem [ thm4 ] improves the result in lv and liu ( 2014 ) in two important aspects .",
    "first , the consistency of the covariance contrast matrix estimator was previously justified in lv and liu ( 2014 ) for the case of correctly specified model .",
    "our new result shows that the simple plug - in estimator @xmath144 still enjoys consistency in the general setting of model misspecification .",
    "second , the result in theorem [ thm4 ] holds for the case of diverging dimensionality .",
    "these theoretical guarantees are crucial to the practical implementation of those information criteria .",
    "our numerical studies reveal that such an estimate works well in a variety of model misspecification settings .      given a set of competing models @xmath160",
    ", a popular bayesian model selection procedure is to first put nonzero prior probability @xmath161 on each model @xmath162 , and then choose a prior distribution @xmath163 for the parameter vector in the corresponding model .",
    "assume that the density function of @xmath163 is bounded in @xmath164 with @xmath165 and locally bounded away from zero throughout the domain .",
    "the bayesian principle of model selection is to choose the most probable model _ a posteriori _ , that is , choose model @xmath128 such that @xmath166 where the log - marginal - likelihood is @xmath167 d\\mu_{\\mathfrak{m}_m}({\\mbox{\\boldmath $ \\beta$}})\\ ] ] with the log - likelihood @xmath168 as in ( [ 002 ] ) and the integral over @xmath169 .    to ease the presentation , for any @xmath24",
    "we define a quantity @xmath170 which is the deviation of the quasi - log - likelihood from its maximum .",
    "then from ( [ 083 ] ) and ( [ eq : ellnstar ] ) , we have @xmath171 + \\log \\alpha_{\\mathfrak{m}_m},\\ ] ] where @xmath172 $ ] .",
    "[ thm1 ] under conditions [ cond1][condition : yi_deviation and sub - gaussian ] and [ condition : prior density and rho_n ] , we have with probability tending to one , @xmath173 where @xmath140 and @xmath174 $ ] .",
    "the asymptotic expansion of the bayes factor in theorem [ thm1 ] leads us to introduce the generalized bic ( gbic ) as follows .",
    "we define gbic of model @xmath142 as @xmath175 where @xmath144 is a consistent estimator of @xmath145 .",
    "it is clear from that gbic contains an extra term compared to bic that replaces the factor @xmath176 with @xmath177 in penalizing model complexity in ( [ 062 ] ) .",
    "this additional term reflects the effect of model misspecification .",
    "when the model is correctly specified , gbic reduces to bic asymptotically .",
    "the choice of the prior probabilities @xmath178 is important in high dimensions .",
    "lv and liu ( 2014 ) suggested prior probability @xmath179 for each candidate model @xmath180 , where the quantity @xmath181 is defined as @xmath182\\ ] ] and the subscript @xmath183 indicates a particular candidate model .",
    "the motivation is that the further the qmle @xmath184 is away from the best misspecified glm @xmath185 , the lower prior we assign to that model . in the high - dimensional setting when @xmath1 can be much larger than @xmath2 , it is sensible to take into account the complexity of the space of all possible sparse models with the same size as @xmath162 .",
    "this observation motivates us to consider a new prior of the form @xmath186 with @xmath187 .",
    "such a complexity factor has been exploited in the extended bic ( ebic ) in chen and chen ( 2008 ) , who showed that using the term @xmath188 with some constant @xmath189 , the ebic can be model selection consistent for @xmath190 with some positive constant @xmath191 satisfying @xmath192 .    under the assumption of @xmath193 ,",
    "an application of stirling s formula shows that up to an additive constant , it holds that @xmath194 .",
    "thus for the prior defined in , we have an additional term @xmath195 in the asymptotic expansion for gbic .",
    "when @xmath1 is of order @xmath196 with some constant @xmath197 , this new term is of the same order as @xmath198 .",
    "when @xmath11 is of order @xmath196 with some constant @xmath199 , the @xmath11 term dominates that involving @xmath177 .",
    "fan and tang ( 2013 ) proposed a similar term @xmath200 term to ameliorate the bic for the case of correctly specified models with non - polynomially growing dimensionality @xmath1 .",
    "the following theorem provides the asymptotic expansion of the bayes factor with the particular choice of prior in ( [ prior ] ) .",
    "[ thm2 ] assume that conditions [ cond1][condition : prior density and rho_n ] hold , @xmath201 with @xmath202 some normalization constant , and @xmath193 .",
    "then we have with probability tending to one , @xmath203 where @xmath140 , @xmath204 , and @xmath174 $ ] .    similarly to the gbic , we now define a new information criterion , the generalized bic with prior probability ( @xmath0 ) , based on theorem [ thm2 ] .",
    "we define @xmath205 of model @xmath142 as @xmath206 where @xmath144 is a consistent estimator of @xmath145 .    in correctly specified models ,",
    "the term @xmath207 is asymptotically close to @xmath208 when @xmath144 is a consistent estimator of @xmath209 .",
    "thus compared to bic with factor @xmath177 , the @xmath0 contains a larger factor @xmath11 when @xmath1 grows non - polynomially with @xmath2 .",
    "this leads to a heavier penalty on model complexity similarly as in fan and tang ( 2013 ) .",
    "as pointed out in lv and liu ( 2014 ) , the right hand side of ( [ def::gbic_p ] ) can be viewed as a sum of three terms : the goodness of fit , model complexity , and model misspecification .",
    "an important distinction with the low - dimensional counterpart of @xmath0 is that our new criterion explicitly takes into account the dimensionality of the whole feature space .",
    "the asymptotic expansions of both kl divergence and bayesian principles in section [ sec::results ] have enabled us to introduce the gaic , gbic , and @xmath0 for model selection in high dimensions with model misspecification .",
    "we now investigate their performance in comparison to the information criteria aic , bic , and @xmath0-l in high - dimensional misspecified models via simulation examples as well as two real data sets . for each simulation study , we set the number of repetitions to be 100 and examined the scenarios when the dimensionality grows ( @xmath9 , 400 , 1600 , and 3200 ) .        the first model we consider is the following high - dimensional linear regression model with interaction and weak effects @xmath210 where @xmath211 is an @xmath212 design matrix , @xmath213 is an interaction term which is the product of the first two covariates , the rows of @xmath18 are sampled as i.i.d .",
    "copies from @xmath214 , and the error vector @xmath215 .",
    "we set @xmath216 , @xmath217 , and @xmath218 .",
    "although the data was generated from model ( [ eq::simu - linear - interaction ] ) , we fit the linear regression model without interaction , which is a typical example of model misspecfication . in view of ( [ eq::simu - linear - interaction ] ) , the true model involves only the first ten covariates in a nonlinear form .",
    "since the other covariates are independent of those ten covariates , the oracle working model is @xmath219 as argued in lv and liu ( 2014 ) . due to the high dimensionality , it is computationally prohibitive to implement the best subset selection",
    ". therefore , we first applied the regularization method sica ( lv and fan , 2009 ) to build a sequence of sparse models and then selected the final model using a model selection criterion . in practice , one can apply any preferred variable selection procedure to obtain a sequence of candidate models .",
    "in addition to comparing the models selected by different information criteria , we also considered the estimate based on the oracle working model @xmath220 as a benchmark and used both measures of prediction and variable selection .",
    "denote by @xmath221 the selected model .",
    "we split the oracle working model into the set of strong effects @xmath222 and that of weak effects @xmath223 .",
    "it is interesting to observe that all criteria tend to miss the entire set of weak effects @xmath224 due to their very low signal strength .",
    "therefore , we focused on comparing the model selection performance in recovering the set of strong effects @xmath225 .",
    "we report the strong effect consistent selection probability ( the portion of simulations where @xmath226 ) , the strong effect inclusion probability ( the portion of simulations where @xmath227 ) , and the prediction error @xmath228 with @xmath229 an estimate and @xmath230 an independent observation . to evaluate the prediction performance of different criteria , we calculated the average prediction error on an independent test sample of size 10,000 .",
    "the results for prediction error and model selection performance are summarized in table [ tb::simu - linear - inter - weak ] . to save space ,",
    "the number of false positives @xmath231 and the numbers of false negatives for strong effects @xmath232 and weak effects @xmath233 , respectively , are reported in table [ tb::simu - linear - inter - weak - fp - fn ] in the supplementary material .    it is clear that as the dimensionality @xmath1 increases , the consistent selection probability tends to decrease and the prediction error tends to increase for all information criteria . generally speaking , gaic",
    "improved over aic , and gbic , @xmath0-l , and @xmath0 performed better than bic in terms of both prediction and variable selection .",
    "in particular , the model selected by our new information criterion @xmath0 delivered the best performance with the smallest prediction error and highest strong effect consistent selection probability across all settings .",
    "meanwhile it is also interesting to see what results different model selection criteria lead to when the model is correctly specified . to this end , we regenerate the solution path based on the linear regression model with the interaction @xmath234 added .",
    "the same performance measures are calculated for this scenario with the results reported in tables [ tb::simu - linear - inter - weak - correct ] and [ tb::simu - linear - inter - weak - fp - fn - correct ] , where the latter table is included in the supplementary material .",
    "a comparison of these results with those in tables [ tb::simu - linear - inter - weak ] and [ tb::simu - linear - inter - weak - fp - fn ] gives several interesting observations .",
    "first , all model selection criteria have a better performance when the model is correctly specified in terms of both model selection and prediction .",
    "second , it is worth noting that while all model selection criteria except aic work reasonably well for the correctly specified model , all but the newly proposed @xmath0 have a very low consistent selection probability under both model misspecification and high dimensionality .",
    "third , it is interesting to see that @xmath0 outperforms the existing methods even under the correctly specified model in terms of consistent selection probability .",
    "c|ccccccc     + @xmath1 & aic & bic & gaic & gbic & @xmath0-l & @xmath0&oracle + 200 & 0(99 ) & 29(99 ) & 21(99 ) & 32(99 ) & 67(98 ) & 73(98)&100(100 ) + 400 & 0(100 ) & 9(100 ) & 8(100 ) & 19(100 ) & 54(100 ) & 76(100)&100(100 ) + 1600 & 0(100 ) & 0(100 ) & 9(100 ) & 0(100 ) & 27(100 ) & 66(100)&100(100 ) + 3200 & 0(100 ) & 0(100 ) & 4(100 ) & 0(100 ) & 16(100 ) & 64(100 ) & 100(100 ) +   + 200 & 164(35 ) & 130(13 ) & 130(10 ) & 128(12 ) & 125(8 ) & 125(8 ) & 121(7 ) + 400 & 162(29 ) & 154(38 ) & 129(13 ) & 131(22 ) & 125(9 ) & 122(10 ) & 120(7 ) + 1600 & 168(31 ) & 172(28 ) & 134(13 ) & 170(28 ) & 129(14 ) & 125(10 ) & 121(7 ) +",
    "3200 & 159(22 ) & 169(23 ) & 135(14 ) & 167(23 ) & 134(15 ) & 125(13 ) & 120(8 ) +    c|ccccccc     + @xmath1 & aic & bic & gaic & gbic & @xmath0-l & @xmath0&oracle + 200 & 2(100 ) & 82(100 ) & 81(99 ) & 82(100 ) & 87(100 ) & 91(100 ) & 100(100 ) + 400 & 8(100 ) & 76(100 ) & 76(100 ) & 84(100 ) & 90(100 ) & 94(100)&100(100 ) + 1600 & 39(95 ) & 74(99 ) & 65(89 ) & 79(99 ) & 88(100 ) & 96(100)&100(100 ) + 3200 & 64(94 ) & 84(98 ) & 72(88 ) & 84(98 ) & 94(100 ) & 95(100 ) & 100(100 ) +   + 200 & 13.6(1.9 ) & 11.2(1.0 ) & 11.2(1.0 ) & 11.2(1.0 ) & 11.6(1.3 ) & 11.7(1.2 ) & 7.0(0.4 ) + 400 & 12.1(1.4 ) & 11.5(1.3 ) & 11.5(1.2 ) & 11.5(1.2 ) & 11.7(1.3 ) & 11.8(1.0 ) & 6.9(0.4 ) + 1600 & 12.4(8.3 ) & 12.0(7.9 ) & 11.9(9.8 ) & 12.0(8.0 ) & 12.2(7.7 ) & 12.4(7.3 ) & 7.0(0.4 ) + 3200 & 21.2(10.2 ) & 20.7(9.4 ) & 21.8(11.0 ) & 20.7(9.4 ) & 20.4(8.8 ) & 20.3(8.5 ) & 7.0(0.3 ) +",
    "we next consider another model misspecification setting that involves the multiple index model @xmath235 where the response depends on the covariates only through the first five ones but with nonlinear functions and @xmath6 . here",
    "the design matrix @xmath211 was generated as in section [ sec::simu - linear - interaction - weak ] .",
    "we set the true parameter vector @xmath236 , @xmath217 , and @xmath218 .",
    "note that the oracle working model is @xmath237 for this example .",
    "although the data was generated from model ( [ simu::multiple - index ] ) , we fit the linear regression model .",
    "the results are summarized in tables [ tb::simu - multiple - index ] and [ tb::simu - multiple - index - fp - fn ] ( the latter available in supplementary material ) .",
    "the consistent selection probability and inclusion probability are now calculated based on @xmath238 .",
    "in general , the conclusions are similar to those in example [ sec::simu - linear - interaction - weak ] .",
    "an interesting observation is the comparison between @xmath0-l and @xmath0 in terms of model selection .",
    "while @xmath0-l is comparable to @xmath0 when the dimension is not large ( @xmath9 ) , the difference between these two methods increases as the dimensionality increases . in the case when @xmath239 , @xmath0 has 77% success probability of consistent selection , while all the other criteria have at most 5% success probability .",
    "this confirms the necessity of including the @xmath11 factor in the model selection criterion to take into account the high dimensionality , which is in line with the conclusion in fan and tang ( 2013 ) for the case of correctly specified models .",
    "c|ccccccc + @xmath1 & aic & bic & gaic & gbic & @xmath0-l & @xmath0&oracle + 200 & 2(100 ) & 4(100 ) & 2(100 ) & 6(100 ) & 51(100 ) & 65(100 ) & 100(100 ) + 400 & 1(100 ) & 1(100 ) & 2(100 ) & 1(100 ) & 28(100 ) & 67(100 ) & 100(100 ) + 1600 & 0(100 ) & 0(100 ) & 3(100 ) & 0(100 ) & 5(100 ) & 63(100 ) & 100(100 ) + 3200 & 0(100 ) & 0(100 ) & 5(100 ) & 0(100 ) & 5(100 ) & 77(100 ) & 100(100 ) +   + 200 & 26(3 ) & 26(3 ) & 26(3 ) & 26(3 ) & 23(3 ) & 23(2 ) & 22(1 ) + 400 & 28(3 ) & 28(3 ) & 27(3 ) & 28(3 ) & 25(4 ) & 23(2 ) &",
    "22(1 ) + 1600 & 31(3 ) & 31(3 ) & 30(4 ) & 31(3 ) & 30(4 ) & 23(4 ) & 22(1 ) + 3200 & 31(4 ) & 31(4 ) & 30(3 ) & 31(4 ) & 30(3 ) & 23(2 ) & 22(1 ) +      our last simulation example is high - dimensional logistic regression with interaction .",
    "we simulated 100 data sets from the logistic regression model with interaction and an @xmath2-dimensional parameter vector @xmath240 where @xmath211 is an @xmath212 design matrix , @xmath213 and @xmath241 are two interaction terms , and the rest is the same as in ( [ eq::simu - linear - interaction ] ) .",
    "for each data set , the @xmath2-dimensional response vector @xmath43 was sampled from the bernoulli distribution with success probability vector @xmath242\\t$ ] with @xmath243 given in ( [ 096 ] ) . as in section [ sec::simu - linear - interaction - weak ] , we consider the case where all covariates are independent of each other .",
    "we chose @xmath244 and set sample size @xmath245 .",
    "although the data was generated from the logistic regression model with parameter vector ( [ 096 ] ) , we fit the logistic regression model without the two interaction terms .",
    "this provides another example of misspecified models . as argued in section",
    "[ sec::simu - linear - interaction - weak ] , the oracle working model is @xmath246 which corresponds to the logistic regression model with the first five covariates .    since the goal in logistic regression is usually classification",
    ", we replace the prediction error with the classification error rate .",
    "tables [ tb::simu - logi - inter ] and [ tb::simu - logi - inter - fp - fn ] ( the latter available in supplementary material ) show similar phenomenon as in sections [ sec::simu - linear - interaction - weak ] and [ sec::simu - multiple - index ] .",
    "again @xmath0 outperformed all other model selection criteria with greater advantage for the high - dimensional case ( e.g. , @xmath239 ) .",
    "c|ccccccc + @xmath1 & aic & bic & gaic & gbic & @xmath0-l & @xmath0&oracle + 200 & 0(99 ) & 32(94 ) & 1(99 ) & 39(94 ) & 49(91 ) & 49(91)&100(100 ) + 400 & 0(99 ) & 19(97 ) & 0(99 ) & 36(93 ) & 50(92 ) & 55(92 ) & 100(100 ) + 1600 & 0(96 ) & 0(96 ) & 0(94 ) & 21(90 ) & 35(88 ) & 47(81)&100(100 ) + 3200 & 0(95 ) & 0(95 ) & 0(96 ) & 10(90 ) & 21(86 ) & 41(72)&100(100 ) +   + 200 & 22(3 ) & 15(2 ) & 16(2 ) & 15(1 ) & 14(1 ) & 14(1 ) & 14(1 ) + 400 & 21(3 ) & 16(5 ) & 17(2 ) & 15(1 ) & 15(1 ) & 15(1 ) & 13(1 ) + 1600 & 21(2 ) & 21(2 ) & 18(1 ) & 16(3 ) & 15(1 ) & 16(2 ) & 14(1 ) + 3200 & 22(2 ) & 21(2 ) & 19(2 ) & 18(3 ) & 15(2 ) & 15(2 ) & 13(1 ) +",
    "we finally consider two gene expression data sets : prostate ( singh et al . , 2002 ) and neuroblastoma ( oberthuer et al . , 2006 ) .",
    "the prostate data set contains @xmath247 genes with @xmath248 samples including 59 positives and 77 negatives .",
    "the neuroblastoma ( nb ) data set , available from the microarray quality control phase - ii ( maqc - ii ) project ( maqc consortium , 2010 ) , consists of gene expression profiles for @xmath249 genes from 239 patients ( 49 positives and 190 negatives ) of the german neuroblastoma trials nb90-nb2004 with the 3-year event - free survival ( 3-year efs ) information available .",
    "see those references for more detailed description of the data sets .",
    "we fit the logistic regression model with sica implemented with ica algorithm ( fan and lv , 2011 ) . before applying the regularization method",
    ", we exploited the sure independence screening approach to reduce the dimensionality .",
    "the random permutation idea ( fan et al . , 2011 ) was applied to determine the threshold for marginal screening .",
    "after the screening step , the numbers of retained variables are 430 ( prostate ) and 2778 ( neuroblastoma ) , respectively .",
    "we then chose the final model using those six model selection criteria .",
    "moreover , we randomly split the data into training ( 80% ) and testing ( 20% ) sets for 100 times , and reported the median test classification error rate along with the median model size in table [ tb::real ] . from table",
    "[ tb::real ] , for the prostate data set the best criterion appears to be @xmath0-l , which has the smallest test classification error rate . for the neuroblastoma data set , if we only look at the median test classification error rate , @xmath0-l again",
    "has the best performance with a small model size .",
    "it is worth noting that @xmath0 leads to the most parsimonious model , with median model size 3 , at the expense of slightly increasing the test classification error rate . from the results of real examples ,",
    "it is evident that by taking into account the effect of model misspecification , the performance of the original model selection criteria can be improved in general .",
    "this is important since the true model structure is generally unavailable to us in real applications .",
    "our results suggest that the term involving model misspecification in the asymptotic expansions is usually nonnegligible for model selection .",
    "c|cccccc + data set & aic & bic & gaic & gbic & @xmath0-l & @xmath0 + prostate & 19(9 ) & 15(6 ) & 15(9 ) & 15(9 ) & 13(9 ) & 15(10 ) + nb & 18(5 ) & 18(5 ) & 18(3 ) & 18(3 ) & 18(5 ) & 19(5 ) +   + prostate & 15.0(3.7 ) &",
    "8.5(4.5 ) & 3.0(1.5 ) & 6.0(3.7 ) & 6.0(3.7 ) & 5.0(3.0 ) + nb & 27.0(3.0 ) & 26.0(2.2 ) & 8.5(3.7 ) & 6.0(3.7 ) & 5.0(3.0 ) & 3.0(2.2 ) +",
    "despite the rich literature on model selection , the general case of model misspecification in high dimensions is less well studied .",
    "our work has investigated the problem of model selection in high - dimensional misspecified models and characterized the impact of model misspecification .",
    "the newly suggested information criterion @xmath0 involving a logarithmic factor of the dimensionality in penalizing model complexity has been shown to perform well in high - dimensional settings .",
    "moreover , we have established the consistency of the covariance contrast matrix estimator that captures the effect of model misspecification in the general setting .",
    "the @xmath11 term in @xmath0 is adaptive to high dimensions . in the setting of correctly specified models ,",
    "fan and tang ( 2013 ) showed that such a term is necessary for the model selection consistency of information criteria when the dimensionality diverges fast with the sample size .",
    "it would be interesting to study the optimality of those different information criteria under model misspecification .",
    "it would also be interesting to investigate model selection principles in more general high - dimensional misspecified models such as the additive models and survival models .",
    "these problems are beyond the scope of the current paper and are interesting topics for future research .",
    "this appendix presents the proofs of theorems [ thm5 ] and [ thm3][thm4 ] . to save space , the proofs of all other theorems and technical lemmas",
    "are included in the supplementary material . for notational simplicity , throughout the proofs we may specify the orders of different quantities without stating the exact constants , and use the notation @xmath43 for observed response and @xmath16 for random response interchangeably when it is convenient .",
    "throughout this proof @xmath61 denotes the euclidean norm of a given vector .",
    "the main idea of the proof is to obtain a probabilistic lower bound for the event @xmath250 to accomplish that we first consider an event that is a subset of this event and calculate the probabilistic lower bound for the smaller event .",
    "recall that @xmath29 is continuous and bounded away from @xmath30 , and @xmath18 is of full column rank @xmath17 .",
    "recall the definition @xmath74 , and let @xmath251 denote the boundary of this neighborhood . since @xmath91 is compact ,",
    "@xmath252 a continuous strictly concave function , whenever the event @xmath253 occurs , @xmath93 will be in @xmath91 .",
    "the strict concavity of the log - likelihood function follows from the positive definiteness of @xmath254 , which is the negative of the hessian of the log - likelihood .",
    "this property entails that on the event @xmath255 , the global maximizer @xmath93 must belong to the interior of the neighborhood @xmath91 .",
    "hereafter we condition on the event @xmath255 defined in .",
    "the technical arguments that follow herein , in order to prove that @xmath255 holds with significant probability , require delicate analyses due to growing dimensionality @xmath17 .",
    "applying taylor s expansion to the log - likelihood function @xmath252 around @xmath150 , we obtain @xmath256 where @xmath257 is on the line segment joining @xmath45 and @xmath150 and @xmath258 $ ] . by letting @xmath259 ,",
    "the above taylor s expansion can be rewritten as @xmath260 where @xmath261 .    from the definition of @xmath262",
    ", @xmath263 is equivalent to @xmath264 , and @xmath263 implies @xmath265 since @xmath91 is convex .",
    "also it is clear that @xmath266 from condition [ condition : min_ev_b_n and min_ev_v ] , for @xmath2 sufficiently large , @xmath71 where @xmath267 . using this condition and since @xmath265",
    ", it holds that @xmath268 hence by combining  and taking a supremum on the boundary @xmath251 in we derive @xmath269 .",
    "\\nonumber\\end{aligned}\\ ] ] by ( [ eq : normal : equation ] ) , we have @xmath270=0 $ ] .",
    "hence , @xmath258 = { \\mbox{\\bf x}}^t({\\mbox{\\bf y}}-e{\\mbox{\\bf y}})$ ] .",
    "denote by @xmath271 .",
    "notice that @xmath272 and @xmath273 .",
    "clearly the left hand side of is negative with probability given by @xmath274 from the expression of @xmath275 , we have @xmath276 [ { \\mathrm{cov}}({\\mbox{\\bf y}})^{1/2}{\\mbox{\\bf x}}{\\mbox{\\bf b}}_n^{-1}{\\mbox{\\bf x}}\\t{\\mathrm{cov}}({\\mbox{\\bf y}})^{1/2}]\\\\ & \\cdot [ { \\mathrm{cov}}({\\mbox{\\bf y}})^{-1/2}({\\mbox{\\bf y}}-e{\\mbox{\\bf y}})],\\end{aligned}\\ ] ] where @xmath277 denotes product",
    ". denote by @xmath278 and @xmath279 .",
    "it is easy to check that @xmath280 .",
    "therefore , @xmath281 is a projection matrix with rank @xmath282 .",
    "in addition , we have @xmath283 and @xmath284 .    we now decompose @xmath285 into two terms , the summations of the diagonal entries and the off - diagonal entries , respectively , @xmath286 where @xmath287 denotes the @xmath288-entry of @xmath281 .",
    "next we obtain probabilistic bounds for each of the two terms .    from the sub - gaussian tail condition for @xmath289 in condition [ cond1 ] , there exists some positive constant @xmath290 such that for any @xmath66 , @xmath291 for all @xmath64 .",
    "thus for any @xmath66 , it holds that @xmath292 on the event @xmath293 , we can bound the first term of as @xmath294    denote by @xmath295 a diagonal matrix with diagonal entries @xmath296 . as a result , we observe that @xmath297 .",
    "it is easy to see that @xmath298 = 0 $ ] .",
    "we will use a version of the hanson - wright inequality ( see , e.g. , theorem 1.1 of rudelson and vershynin , 2013 ) to obtain the concentration bound of the quadratic form @xmath299 .",
    "but we first start with some notation and preparation .",
    "let @xmath300 denote the sub - gaussian norm of a sub - gaussian random variable @xmath301 defined as @xmath302 . from condition [ cond1 ] ,",
    "that is , the condition on sub - gaussian tails , we derive @xmath303 where the last line follows directly from the definition of the gamma function .",
    "taking the @xmath183-th root , we have @xmath304 rewriting after bounding @xmath305 by 1 , we obtain @xmath306 since @xmath307 .",
    "therefore , it holds that @xmath308 for all @xmath309 , where @xmath310 .",
    "we now need bounds on the operator and frobenius norms of @xmath311 .",
    "denote @xmath312 and @xmath313 as the matrix operator and frobenius norms , respectively .",
    "note that @xmath314 and @xmath315 .",
    "thus using the fact @xmath316 , we obtain @xmath317 . since @xmath318 , we further obtain @xmath319 .",
    "thereby , a direct application of the hanson - wright inequality yields @xmath320 for any @xmath321 , where @xmath322 and @xmath323 are some positive constants . to ensure @xmath321",
    ", we choose @xmath73 for some constant @xmath69 and @xmath324 .",
    "therefore , the probability bound holds for large enough @xmath2 .    combining and , with probability at least @xmath325",
    ", we have @xmath326 in view of our choice of @xmath327 , it holds that @xmath328 and thus @xmath329 where @xmath330 \\wedge ( c_1 ^ 2c_6c_0/8)$ ] . note that @xmath331 since @xmath69 .",
    "this leads to @xmath332 the positive constant @xmath97 can be large if @xmath333 in @xmath334 is chosen to be large . from condition [ condition : min_ev_b_n and min_ev_v ] ,",
    "@xmath335 at a faster rate than @xmath336 .",
    "then we have the consistency @xmath337 .",
    "define @xmath338 , where @xmath89 stands for the qmle .",
    "note that @xmath339 does depend on @xmath2 , but for simplicity of notation we will omit the subscript @xmath2 in sequel . to establish this theorem",
    "we require a possibly dimension dependent bound on the quantity @xmath340 .",
    "the need for bounding the specified quantity , particularly with growing dimensionality , can be intuitively understood by trying to put some restriction on the parameter space .",
    "this is analogous to the case of penalized likelihood .",
    "recall the neighborhood @xmath341 , where @xmath103 is some positive constant satisfying @xmath342 .",
    "one way of bounding the quantity @xmath340 is to restrict the qmle @xmath93 on the set @xmath343 ) .",
    "as mentioned in theorem [ thm5 ] , the constant @xmath97 can be large if @xmath333 is chosen to be large , which ensures that @xmath103 is positive . from condition [ condition : region ] ,",
    "@xmath344 for all sufficiently large @xmath2 to ensure that conditional on @xmath339 , the restricted mle coincides with its unrestricted version .",
    "however , this condition is very mild in the sense that the constant @xmath103 can be chosen as large as desired to make @xmath345 large enough , whereas the neighborhood @xmath91 is asymptotically shrinking .",
    "hereafter in this proof @xmath93 will be referred to as the restricted mle , unless specified otherwise .",
    "recall that @xmath346 , where @xmath347 is an independent copy of @xmath43 . in the glm setup",
    ", we have @xmath348 and @xmath349 .    * part 1 : expansion of @xmath350 .",
    "* we approach the proof by splitting @xmath350 in the region @xmath339 and its complement , that is , @xmath3511_{\\mathcal{e}^c}\\ } , \\ ] ] where the second equality follows from the definition of @xmath352 .",
    "we aim to show that the second term on the right hand side of is @xmath353 .",
    "performing componentwise taylor s expansion of @xmath354 around @xmath355 and evaluating at @xmath356 , we obtain @xmath357 , where @xmath358 with @xmath359 and @xmath360 lying in the line segment joining @xmath93 and @xmath355 . recall that @xmath361 is the constrained mle here , @xmath31 is bounded uniformly in @xmath309 and @xmath2 , and @xmath105 uniformly in its argument .",
    "the condition on @xmath362 can be much weakened in many cases including linear and logistic regression models .",
    "this condition also accommodates poisson regression where @xmath363 for @xmath364 since @xmath365 .",
    "then it follows that @xmath366 for sufficiently large @xmath2 .",
    "the last inequality follows from the fact that @xmath367 and we recall that @xmath368 . to verify the orders",
    ", we note that the four bounds @xmath369 , @xmath370 , @xmath371 , and @xmath372 + @xmath373 . on the event @xmath339 , we first expand @xmath374 around @xmath92 . by the definition of @xmath92 , @xmath374 attains its maximum at @xmath92 . by taylor s expansion of @xmath352 around @xmath92 and evaluating at @xmath93",
    ", we derive @xmath375 { \\mbox{\\bf v}}_n - \\frac{s_n}{2 } ,   \\nonumber\\end{aligned}\\ ] ] where @xmath376 , @xmath377 , @xmath378 ( { \\widehat{\\mbox{\\boldmath $ \\beta$}}}_n - { \\mbox{\\boldmath $ \\beta$}}_{n,0})$ ] , @xmath379 , and @xmath380 is on the line segment joining @xmath92 and @xmath93",
    ". then it follows that @xmath381^t [ { \\mbox{\\bf v}}_n({\\mbox{\\boldmath $ \\beta$}}^ * ) - { \\mbox{\\bf v}}_n ] [ { \\mbox{\\bf b}}_n^{1/2 } ( { \\widehat{\\mbox{\\boldmath $ \\beta$}}}_n - { \\mbox{\\boldmath $ \\beta$}}_{n,0})]\\right| 1_\\mathcal{e } \\nonumber \\\\ & \\leq \\   \\|{\\mbox{\\bf v}}_n({\\mbox{\\boldmath $ \\beta$}}^ * ) - { \\mbox{\\bf v}}_n \\|_2 \\delta_n^2 d 1_\\mathcal{e } , \\nonumber \\ ] ] where @xmath382 and @xmath383 .",
    "note that on the event @xmath339 , by the convexity of the neighborhood @xmath91 we have @xmath384 . from condition [ condition : widetilde_v_n ] ,",
    "therefore we deduce that @xmath386 is of order @xmath387 , which follows from ( [ order_an ] ) in the proof of theorem [ thm6 ] . from ( [ decom_an ] ) in the proof of theorem [ thm6 ] , we have the decomposition @xmath388 with @xmath389 and @xmath390 \\left[{\\mbox{\\bf b}}_n^{1/2 } ( { \\widehat{\\mbox{\\boldmath $ \\beta$}}}_n - { \\mbox{\\boldmath $ \\beta$}}_{n , 0})\\right].\\ ] ] for simplicity of notation , denote by @xmath391 .",
    "recall that @xmath392 . with some calculations we obtain @xmath393",
    "note that @xmath394 . from theorem [ thm5",
    "] , we have @xmath395 as @xmath396 .",
    "let @xmath397 ensuring that this quantity is bounded away from zero .",
    "we will apply vitali s convergence theorem to show that @xmath398 . to establish uniform integrability we use the following lemma ,",
    "the proof of which has been provided in appendix [ secb ] in supplementary material .",
    "[ ui ] for some constant @xmath399 , @xmath400 .",
    "this leads to @xmath398 .",
    "hence we have @xmath401    it remains to show that @xmath402 = o(\\mu_n).\\ ] ] note that on the event @xmath339 , we have @xmath403 in view of the assumption @xmath404 , it holds that @xmath405 . for the cross term @xmath406 , applying the cauchy - schwarz inequality yields @xmath407 , \\nonumber\\end{aligned}\\ ] ] which entails that @xmath408 . note that @xmath409 is of order @xmath353 by similar calculations as in ( [ orderonethm5 ] ) . thus combining ( [ split ] )  ( [ eq : thm 5 part 1 last ] ) yields @xmath410 .    *",
    "part 2 : expansion of @xmath411 . *",
    "similarly we expand @xmath252 around @xmath93 and evaluate at @xmath92 . from condition [ condition : region ] , @xmath344 for sufficiently large @xmath2 , we see that @xmath412 . on the event @xmath339 ,",
    "since @xmath413 attains its maximum at the restricted mle @xmath93 , we have @xmath414 { \\mbox{\\bf v}}_n \\nonumber - \\frac{s_n}{2}.\\label{ell_expansion}\\end{aligned}\\ ] ] then similarly as in part 1 , we can obtain @xmath415 if we can show that @xmath416 and @xmath417 are both of order @xmath353 , then we obtain the desired asymptotic expansion @xmath418    to see why @xmath416 is of order @xmath353 , we derive @xmath419 similarly as in ( [ orderonethm5 ] ) and using @xmath420 \\leq e[|{\\mbox{\\bf y}}^t{\\mbox{\\bf x}}{\\mbox{\\boldmath $ \\beta$}}_{n,0}|^2]^{1/2 } p(\\mathcal{e}^c)^{1/2}$ ] . similarly we can also show that @xmath421 is of order o(1 ) . the only difference in the above derivation",
    "is to bound @xmath422 instead of @xmath423 , which holds from the definition of the restricted qmle .",
    "this concludes the proof .      in view of the expansions of gaic , gbic , and @xmath0 , we need to show that @xmath159 and @xmath158 . to establish this",
    "we show that @xmath424 , where the @xmath425 denotes the convergence in probability of the matrix operator norm .",
    "let @xmath426 be a @xmath84 square matrix .",
    "denote by @xmath427 the normalized trace and @xmath428 the spectral radius .",
    "then we have @xmath429 where @xmath312 denotes the matrix operator norm .",
    "the equality of the spectral radius and the operator norm follows from the symmetry of the matrix @xmath430 .",
    "similarly define the normalized log determinant , that is , @xmath431 for any arbitrary matrix @xmath426 .",
    "denote @xmath432 as the eigenvalues arranged in the increasing order .",
    "then we have latexmath:[\\ ] ] then we have @xmath600 where @xmath601 .    using the chi - square tail bound , that is , for any positive @xmath602",
    "it is known that @xmath603 and after minor modification it holds that @xmath604 . with this observation , define @xmath605 and the proof concludes .",
    "in tables [ tb::simu - linear - inter - weak - fp - fn][tb::simu - logi - inter - fp - fn ] , we report additional variable selection results for the three simulation examples in section [ sec::simulation ] .",
    ".example [ sec::simu - linear - interaction - weak ] .",
    "median false positives with median false negatives ( strong / weak effects ) in parentheses when the model is misspecified .",
    "[ tb::simu - linear - inter - weak - fp - fn ] [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]"
  ],
  "abstract_text": [
    "<S> model selection is indispensable to high - dimensional sparse modeling in selecting the best set of covariates among a sequence of candidate models . </S>",
    "<S> most existing work assumes implicitly that the model is correctly specified or of fixed dimensions . </S>",
    "<S> yet model misspecification and high dimensionality are common in real applications . in this paper , we investigate two classical kullback - leibler divergence and bayesian principles of model selection in the setting of high - dimensional misspecified models . </S>",
    "<S> asymptotic expansions of these principles reveal that the effect of model misspecification is crucial and should be taken into account , leading to the generalized aic and generalized bic in high dimensions . with a natural choice of prior probabilities , we suggest the generalized bic with prior probability which involves a logarithmic factor of the dimensionality in penalizing model complexity . </S>",
    "<S> we further establish the consistency of the covariance contrast matrix estimator in a general setting . </S>",
    "<S> our results and new method are supported by numerical studies .    0.1 in * key words : * model misspecification ; high dimensionality ; model selection ; kullback - leibler divergence principle ; bayesian principle ; aic ; bic ; gaic ; gbic ; @xmath0 . </S>"
  ]
}