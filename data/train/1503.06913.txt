{
  "article_text": [
    "careful subjective elicitation of prior distributions for variable selection , although ideal , quickly becomes intractable as the number of variables increases , motivating the need for objective prior distributions that are automatic and with good frequentist properties for default usage @xcite . in the context of bayesian variable selection for linear models , zellner s @xmath0-prior and , in particular ,",
    "mixtures of @xmath0-priors have witnessed widespread use due to computational tractability , consistency , invariance , and other desiderata @xcite that leads to the preference of these priors over many other conventional prior distributions .",
    "@xcite proposed the @xmath0-prior as a simple partially informative reference distribution in gaussian regression models @xmath3 , @xmath4 , where formulation of informative prior distributions for regression coefficients @xmath5 has been and remains a challenging problem . through the use of imaginary samples taken at the same observed design matrix @xmath6",
    ", he obtained a conjugate gaussian prior distribution @xmath7 , with an informative mean @xmath8 , but a covariance matrix that was a scaled version of the covariance matrix of the maximum likelihood estimator ] , @xmath9 .",
    "this greatly simplified elicitation to two quantities : the prior mean @xmath8 of the regression coefficients , for which practitioners often had prior beliefs , and the scalar @xmath0 which controlled both the shrinkage towards the prior mean and the dispersion of the posterior covariance through the shrinkage factor @xmath10 .    in bayesian variable selection ( bvs ) and",
    "bayesian model averaging ( bma ) problems for gaussian regression models with @xmath11 predictors , every subset model , indexed by @xmath12 , may be expressed as @xmath13 , where @xmath14 is a column vector of ones of length @xmath15 , @xmath16 is the intercept , @xmath17 is the model specific design matrix with @xmath18 columns of full rank , and @xmath19 is the vector of length @xmath18 of the non - zero regression coefficients in model @xmath20 . the most common formulation of zellner s @xmath0-prior , as in @xcite , uses the independent jeffreys prior for @xmath16 and @xmath21 @xmath22 where @xmath23 is the orthogonal projection on the space spanned by the column vector @xmath24 .",
    "while it is often assumed that the columns of the design matrix @xmath17 must be centered so that the fisher information matrix is block diagonal ( due to @xmath25 ) to justify the use of the improper reference priors on the common intercept and variance , @xcite argue that measurement invariance , which leads to and , combined with predictive matching , for which bayes factors under minimal sample sizes do not favor @xmath20 or @xmath26 , lead to the form of the @xmath0-prior above , providing an alternative justification for centering the design matrix .",
    "it is well known that the choice of @xmath0 affects both shrinkage in estimation / prediction , bvs , and bma , with various approaches being put forward to determine a @xmath0 with desirable properties .",
    "independent of @xcite , @xcite arrived at @xmath0-priors in linear and logistic regression by considering shrinkage of maximum likelihood estimators ( mles ) to improve prediction and estimation , as in jame - stein estimators , proposing empirical bayes estimates of the shrinkage factor to improve frequentist properties of the estimators .",
    "related to @xcite , @xcite considered risk and expected loss in selecting @xmath0 , @xcite derived global empirical bayes estimators , while @xcite derived model specific local empirical bayes estimates of @xmath0 from an information theory perspective .",
    "@xcite studied consistency of bma under @xmath0-priors in linear models , recommending @xmath27 , which lead to bayes factors that behave like bic when @xmath28 or the risk inflation criterion @xcite when @xmath29 .",
    "mixtures of @xmath0-priors , obtained by specifying a prior distribution on the hyper parameter @xmath0 in , including the hyper-@xmath0 and related hyper-@xmath2 priors @xcite , the beta - prime prior @xcite , the robust prior @xcite , and the intrinsic prior @xcite , among others , are widely used in model selection and model averaging problems , due to their attractive theoretical properties in contrast to @xmath0-priors with fixed @xmath0 @xcite .",
    "mixtures of @xmath0-priors not only inherit desirable measurement invariance property from the @xmath0-prior but under a range of hyper parameters also resolve the information paradox @xcite and bartlett s paradox @xcite that occur with a fixed @xmath0 , meanwhile leading to asymptotic consistency for model selection and estimation @xcite . furthermore , by yielding exact or analytic expressions for marginal likelihoods in tractable forms , these mixtures of @xmath0-priors enjoy most of the computational efficiency of the original @xmath0-prior , permitting efficient computational algorithms for stochastic search of the posterior distribution over the model space @xcite .    for generalized linear models ( glms ) , many variants of @xmath0-priors have been proposed in the literature , including @xcite , with current methods favoring adaptive estimates of @xmath0 via mixtures of @xmath0-priors or empirical bayes estimates of @xmath0 . while these priors have a number of desirable properties , no consensus on an objective prior has emerged for glms .",
    "the seminal paper of @xcite takes an alternative approach and explores whether a consensus of criteria or desiderata that any objective prior should satisfy can instead be used to identify an objective prior , leading to their recommendation of the `` robust '' prior in gaussian variable selection problems . in this article",
    ", we view @xmath0-priors in glms through this lens seeing if the desiderata can essentially determine an objective prior in glms for practical use .",
    "the remainder of the article is arranged as follows . in section [ section : g - prior_glm ] , we begin by reviewing @xmath0-priors in glms and corresponding ( approximate ) bayes factors , and the closely related bayes factors based on test statistics @xcite . as tractable expressions are generally unavailable in glms , we focus attention on using an integrated laplace approximation and show that @xmath0-priors based on observed information lead to distributions that are closed under sampling ( conditionally conjugate ) . to unify results with linear models and @xmath0-priors in glms , in section [ section : mix_g ]",
    "we introduce the truncated compound confluent hypergeometric distribution @xcite , a flexible generalized beta distribution , which encompasses current mixtures of @xmath0-priors as special cases .",
    "this leads to a new family of `` compound hypergeometric information criteria '' or chic . in section [ section : criteria ] we review the desiderata for model selection priors of @xcite and use them to establish theoretical properties of the chic family , which provides general recommendations for hyper parameters . in section [ sec : examples ] , we study the bvs and bma performance of the chic @xmath0-prior with various hyper parameters , using simulation studies and the gusto - i data @xcite . finally in section",
    "[ section : conclusion ] , we summarize recommendation and discuss directions for future research .",
    "to begin we define notations and assumptions for the generalized linear models ( glms ) under consideration .",
    "glms arise from distributions within the exponential family @xcite , with density @xmath30 where @xmath31 and @xmath32 are specific functions that determine the distribution .",
    "the mean and variance for each observation @xmath33 can be written as @xmath34 and @xmath35 , respectively , where @xmath36 and @xmath37 are the first and second derivatives of @xmath38 . in , @xmath39 are independent but not identically distributed , as their corresponding canonical parameters @xmath40 are linked with the predictors via @xmath41 , where @xmath42 is the @xmath43-th entry of the linear predictor @xmath44 under model @xmath20 , providing the `` linear model '' . under this parameterization",
    "the canonical link corresponds to the identity function for @xmath45 .    to begin",
    ", we will assume that the scale parameter @xmath46 with fixed @xmath47 and @xmath48 , a known weight that may vary with the observation .",
    "this includes popular glms such as binary and binomial regression , poisson regression , and heteroscedastic normal linear model with known variances . later in section [ section : mix_g ] , we will relax the assumption of known @xmath47 to illustrate the connections between the prior distributions developed here and existing mixtures of @xmath0-priors in normal linear models with unknown precision @xmath49 , and extend results to consider glms with over - dispersion .    unless specified otherwise , we assume that the design matrix @xmath6 under the full model has full column rank @xmath11 and the column space @xmath50 does not contain @xmath14 . furthermore , we assume that the true model , @xmath51 , is included in the @xmath52 models under consideration . under @xmath51 , true values of the intercept and regression coefficients",
    "are denoted by @xmath53 . for a model @xmath20 ,",
    "if @xmath17 contains all columns of @xmath54 ( including the case that @xmath55 ) , we say @xmath56 , otherwise , @xmath57 .",
    "the mles @xmath58 are assumed to exist and are unique . under standard regularity conditions provided in the supplementary materials appendix [",
    "section : assumptions ] , mles are consistent and asymptotically normal . in section",
    "[ sec : no_mle ] we will relax the conditions to consider non - full rank design matrices and data separation problems in binary regressions .    in bayesian variable selection or bayesian model averaging , posterior probabilities of models are critical components for posterior inference , which in the context of @xmath0-priors , may be expressed as @xmath59 where @xmath60 is the prior probability of model @xmath20 , and @xmath61 is the marginal likelihood of model @xmath20 . in normal linear regression , @xmath0-priors yield closed form marginal likelihoods , which permits quick posterior probability computation and efficient model search , by avoiding the time - consuming procedure to sample @xmath16 and @xmath62 .",
    "when the likelihood is non - gaussian , normal priors no longer have conjugacy , however laplace approximations to the likelihood @xcite combined with normal priors such as @xmath0-priors may be used to achieve computational efficiency such as in integrated nested laplace approximations @xcite .      there have been several variants of @xmath0-priors suggested for glms , starting with @xcite who proposed a normal prior centered at zero , with a covariance based on a scaled version of the inverse expected fisher information evaluated at the mle of @xmath16 and @xmath63 . under a large sample normal approximation for the distributions of the mles , this leads to conjugate updating and closed form expressions for bayes factors . unlike gaussian models , however , both the observed information @xmath64 , which is the negative hessian matrix of the log likelihood , and the expected fisher information @xmath65 $ ] , depend on the parameters @xmath16 and @xmath5 , leading to alternative @xmath0-priors based on whether the expected information @xcite or observed information @xcite is adopted ; they are equal under canonical links when evaluated at the same values .",
    "as these information matrices depend on @xmath19 , the asymptotic covariance is typically evaluated at either @xmath66 or at the model specific mle .",
    "for expected information , @xmath67 , with @xmath68 a diagonal matrix whose @xmath43-th diagonal entry under model @xmath20 is @xmath69 $ ] , for @xmath70 . when @xmath66 , all @xmath71 under all models , and @xmath68 is equal to @xmath72 where @xmath73 $ ] is the unit information under the null model .",
    "the resulting @xmath0-priors have precision matrices that are multiples of @xmath74 as in the gaussian case .",
    "similar in spirit to zellner s derivation of the @xmath0-prior , priors based on imaginary data have been developed in the context of glms by @xcite among others . in general , these do not lead to normal prior distributions and typically require mcmc methods to sample both parameters and models for bvs and bma .",
    "the @xmath0-prior introduced by @xcite and later modified by @xcite adopts a large sample approximation to justify a normal density : @xmath75 where imaginary samples are generated from the null model @xmath26 and the constant @xmath76 is inverse of the unit information given above evaluated at the mle of @xmath16 under @xmath26 . for the normal linear regression",
    ", @xmath77 recovers the usual @xmath0-prior .    under large sample approximations to the likelihood , the @xmath0-prior in permits conjugate updating , however , unlike the gaussian case , evaluating the resulting bayes factors that contain ratios of information matrix determinants among others can increase computational complexity , and thus negates some of the advantages that made the @xmath0-prior so popular in linear models .",
    "classic asymptotic theory suggests that @xmath78 measures the large sample precision of @xmath19 , while @xmath64 is recommended as a more accurate measurement of the same quantity @xcite .",
    "when the true model @xmath79 , evaluating information matrices at the mle @xmath80 @xcite may better capture the large sample covariance structures of @xmath19 .",
    "this suggests that for glms , priors `` centered '' at the null model may lead to @xmath0-priors that do not adequately capture the geometry under model @xmath20 , potentially leading to prior - likelihood conflict and slower rates of convergence . on the other hand ,",
    "using large sample approximations to imaginary data generated from @xmath20 leads to a prior distribution for @xmath19 that is not centered at zero , and therefore will not satisfy the predictive matching criterion of @xcite .",
    "next , we propose a @xmath0-prior that incorporates the local geometry at the mle with the objective of providing a prior that satisfies the model selection desiderata , permits analytic expressions that lead to both computationally efficient algorithms under large sample approximations to likelihoods , as well as deeper understanding of their theoretical properties .",
    "the invariance and predictive matching criteria in @xcite lead to adoption of for location families .",
    "although the poisson and bernoulli families are not location families , it is desirable that the prior / posterior distribution for @xmath81 is invariant under any location changes in the design matrix @xmath17 . in the following proposition ,",
    "we will use the uniform prior in as a starting point for deriving the ( approximate ) integrated likelihood for @xmath19 and subsequent prior distribution for @xmath19 .",
    "[ proposition : marlik_beta ] for any model @xmath20 , with a uniform prior @xmath82 , the marginal likelihood of @xmath19 under model @xmath20 is proportional to @xmath83 where the observed information of @xmath84 , @xmath16 , and @xmath19 at the mles @xmath85 are @xmath86 respectively , and @xmath87 is the perpendicular projection onto @xmath14 under the information @xmath88 inner product , where @xmath89 for @xmath90 and a positive definite @xmath91 .    the proof of proposition [ proposition : marlik_beta ] is given in the supplementary material appendix [ proofproposition : marlik_beta ] .",
    "the approximate marginal likelihood in is proportional to a normal kernel of @xmath19 with a precision ( inverse covariance matrix ) that is equal to the marginal observed information @xmath92 and is a function of the `` centered '' predictors , @xmath93\\mathbf{x}_{\\mm}\\ ] ] where the column means for centering are weighted average @xmath94 , with the weights proportional to @xmath95 in .",
    "for non - gaussian glms , @xmath95 s are not equal , and hence this centering step is different from the conventional procedure that uses the column - wise arithmetic average .",
    "this leads to the following proposal for a @xmath0-prior under all models @xmath20 @xmath96 the advantage of is two - fold : geometric interpretability through local orthogonality , which will be illustrated next , and computational efficiency in bayes factor approximation ( see section [ sec : marlik ] ) .",
    "note that we may reparameterize the model @xmath97 where ( with apologies for abuse of notation ) @xmath16 is the intercept in the centered parameterization . under this centered parameterization and with @xmath82 ,",
    "the observed information at the mles is block diagonal , and leads to the same marginal likelihood as in .    in hypothesis testing , where parameter @xmath5 is tested against a null value @xmath98 with a nuisance parameter @xmath16",
    ", @xcite argues that when the fisher information is block diagonal for all values of @xmath5 and @xmath16 , improper uniform priors on @xmath16 can be justified .",
    "this global orthogonality , however , rarely holds outside of normal models @xcite . under a local alternative hypothesis where the true value of @xmath5 is in an @xmath99 neighborhood of @xmath98 , @xcite",
    "show that bayes factors are not sensitive to prior choices on the nuisance parameter , under a weaker condition of null orthogonality , where @xmath100 is block diagonal for all @xmath16 under the null hypothesis .",
    "in particular , under null orthogonality , the logarithm of the bayes factor under the unit information prior for @xmath5 can be approximated by bic with an error of @xmath101 @xcite . for glms",
    ", the @xmath0-prior implies null orthogonality under the centered reparameterization from @xmath17 to @xmath102 .    for variable selection ,",
    "if the true value @xmath103 does not lie in a neighborhood of the null value , @xcite point out that the bayes factor will likely be decisive and for practical purposes the accuracy of bic does not matter .",
    "however , for estimation , local orthogonality at the mle , as in the @xmath0-prior in , better captures the large sample geometry of the likelihood parameters @xmath104 than null orthogonality , and as we will see , greatly simplifies posterior derivations and theoretical calculations . under the null model ,",
    "local orthogonality implies null orthogonality asymptotically .",
    "note that local or null orthogonalization is not required for @xmath16 to have a uniform prior as in @xcite , but instead the uniform prior leads to the use of the centered @xmath6 that is locally orthogonal to the column of ones under the information inner product and invariant under any location changes for the columns of @xmath6 .",
    "for ease of exposition , however , we will adopt the centered parameterization in for the remainder of the article .      under the @xmath0-prior on @xmath19 and a uniform prior on @xmath16 for the centered parameterization ,",
    "asymptotic limiting distribution theory @xcite under a laplace approximation yields the approximate posterior distributions conditional on @xmath20 as @xmath105 which depend on @xmath106 through functions of mles . due to local orthogonality ,",
    "the posterior distributions of @xmath19 and @xmath16 are independent .",
    "thus for large @xmath15 , the marginal posterior distribution of @xmath16 is proper , although its prior distribution is improper .",
    "the conditional posterior mean of @xmath62 is shrunk from the mle @xmath107 towards the prior mean @xmath108 by the ratio @xmath109 , which is usually referred to as the shrinkage factor for @xmath0-priors in normal linear regression @xcite . under a different variant of the @xmath0-prior for glms ,",
    "the same shrinkage factor @xmath10 is obtained by @xcite , by assuming that @xmath110 equals the block diagonal matrix @xmath111 , which approximates the expected information when @xmath19 is in a neighborhood of zero . as discussed in @xcite , for normal linear regression and glms ,",
    "shrinking predicted values toward the center of responses , or equivalently , shrinking regression coefficients towards zero , may alleviate over - fitting , and thus yield optimal prediction performance . later in section [",
    "subsection : gusto - i ] , the gusto - i data logistic regression example shows that the methods in favor of smaller values of @xmath0 , i.e. , smaller shrinkage factors , tend to be more accurate in out - of - sample prediction .      in glms ,",
    "normal priors such as and yield closed form marginal likelihoods under laplace approximations which are precise to @xmath112 . under an integrated laplace approximation @xcite",
    "with the uniform prior on @xmath16 and @xmath0-prior in for any model @xmath20 , the approximate marginal likelihood for @xmath20 and @xmath0 in has a closed form expression @xmath113 where @xmath18 is the column rank of @xmath114 , and @xmath115 is the wald statistic ( under observed information ) .",
    "for the null model @xmath26 where @xmath116 , @xmath117 so that still holds .",
    "the approximate marginal likelihood is a function of mles , which is fast to compute using existing algorithms such as the iterative weighted least square @xcite .    to compare a pair of models @xmath118 and @xmath119 , the bayes factor @xcite , defined as @xmath120 ,",
    "is commonly used in bayesian model selection , assuming the two models are equally likely _ a priori_. if @xmath121 is greater ( less ) than one , then @xmath118 ( @xmath119 ) is favored . when @xmath52 models are considered simultaneously , under the uniform prior @xmath122 , comparing their posterior probabilities",
    "is equivalent to comparing their bayes factors where each model is compared to a common baseline model , such as the null model @xcite . with",
    "the availability of closed form approximate marginal likelihoods , the @xmath0-prior yields closed form bayes factors @xmath123^{\\frac{1}{2 } }              ( 1 + g)^{-\\frac{p_\\mm}{2 } } \\exp\\left\\ { - \\frac{q_\\mm}{2(1+g ) } \\right\\},\\ ] ] where @xmath124 is the change in deviance or two times the likelihood ratio test statistic for comparing model @xmath20 to @xmath26 .",
    "for simplicity , @xmath125 will be referred as the deviance statistic for the rest of this article .",
    "the bayes factors under the @xmath0-prior provides an adjustment to the likelihood ratio test with a penalty that depends on @xmath0 and the wald statistic .",
    "the expression for the bayes factor in is closely related to the test - based bayes factors ( tbf ) of @xcite which is derived from the asymptotic distribution of @xmath125 under the @xmath20 and @xmath26 . for glms ,",
    "the tbf of @xcite is expressed as @xmath126 where @xmath127 denotes the density of a gamma distribution of mean @xmath128 , evaluated at @xmath129 . under the null or a local alternative where @xmath19 is in a neighborhood of @xmath99 of the null , the wald statistic @xmath130 and deviance statistic @xmath125 are asymptotically equivalent . in this case , replacing @xmath130 by @xmath125 in leads to the expression for the tbf .",
    "when the distance between @xmath19 and the null does not vanish with @xmath15 , we find that the tbf exhibits a small but systematic bias , but leads to little difference in inference for large @xmath28 , where they are close to bic . in section [ sec",
    ": examples ] , using simulation and real examples , we find that with @xmath28 , tbf and dbf have almost identical performance in model selection , estimation , and prediction . more discussions and an empirical example with tbf",
    "are available in the supplementary material appendix [ section : tbf ] .      before turning to the choice of @xmath0 and other properties ,",
    "we briefly investigate the use of @xmath0-priors when mles of @xmath131 or @xmath19 do not exist .",
    "two different cases are considered : data separation in binary regression , and non - full rank design matrices for glms in general .",
    "for binary regression models with a finite sample size , data separation problems may cause serious issues @xcite . for @xmath17 of full rank ,",
    "the data exhibit separation if there exists a scalar @xmath132 and a non - null vector @xmath133 such that @xmath134 in particular , there is complete separation if in strict inequalities hold for all observations . in the absence of complete separation , there is quasi - complete separation if holds with at least one quasi - separated sample for which the equality holds . by @xcite , in the presence of quasi - complete separation , there exists a non - empty set of observations @xmath135 that can only be quasi - separated by all @xmath136 pairs that satisfy . for the design matrix @xmath137 formed by these observations , its rank @xmath138 , because @xmath17 is full rank and columns of @xmath139 are linearly dependent .",
    "if there is complete or quasi - complete separation , then mles @xmath58 do not exist , i.e. , they tend to @xmath140 infinity @xcite and mles of probabilities are on the boundary of the parameter space in binary regression .",
    "the following proposition summarizes results for bayes factors , for the two most commonly used binary models , logistic and probit regressions .",
    "[ proposition : separation_g - prior ] for both logistic and probit regression models , under model @xmath20 ,    1 .",
    "if there is complete separation , then the observed information @xmath88 in has diagonal elements that are all zero , the @xmath0-prior in is not proper and the laplace approximation is no longer valid for approximating the bayes factor .",
    "if there is quasi - complete separation , the rank of the precision matrix of is @xmath141 , i.e. , the @xmath0-prior has a singular precision matrix unless @xmath142 , and the bayes factor formula is bounded .",
    "the proof is available in supplementary material appendix [ proofproposition : separation_g - prior ] . under complete separation",
    ", the @xmath0-prior in violates the `` basic criterion '' in section [ section : basic ] .",
    "while the @xmath0-prior , which depends on the covariance structure under the null , is well defined in the presence of data separation and leads to bounded bayes factors as the expected information under @xmath20 is not used in the laplace approximation , its posterior estimates of probabilities inherit the instability of the mles .",
    "design matrices that are not full rank also lead to identifiability problems with mles of @xmath131 and @xmath19 for all glms .",
    "consider a model @xmath20 where @xmath143 , and a full rank design matrix @xmath144 that contains @xmath145 columns and spans the same column spaces as @xmath17 , i.e. , @xmath146 .",
    "although the mle of the coefficients @xmath80 are not all unique , mles of the linear predictors @xmath147 are unique ; in fact , @xmath148 and @xmath149 is unique and positive definite .",
    "the precision matrix of the @xmath0-prior , @xmath150 is well - defined , however , since @xmath151 , its inverse does not exist due to singularity .",
    "note that the null - based @xmath0-prior suffers from a similar singularity problem .",
    "we may extend the definition of @xmath0 priors to include singular covariance matrices by adopting generalized inverses in defining the @xmath0-prior .",
    "because of the invariance of orthogonal projections to choices of generalized inverse and uniqueness of the mle of @xmath81 , we have the following proposition regarding the bayes factors in models that are not full rank .    [",
    "proposition : non_full_rank_bf ] suppose @xmath143 , then @xmath152^{\\frac{1}{2 } }             ( 1 + g)^{-\\frac{r_\\mm}{2 } } \\exp\\left\\ { - \\frac{q_\\mm}{2(1+g ) } \\right\\}.\\ ] ] if @xmath153 is a full rank model whose column space @xmath154 , then @xmath155 , @xmath156 , and @xmath157 .",
    "the proof is available in supplementary material appendix [ proofproposition : non_full_rank_bf ] .",
    "here the two models @xmath20 and @xmath153 have the same bayes factor if their design matrices span the same column space .",
    "this form of invariance is not possible with other conventional independent prior distributions , such as generalized ridge regression or independent scale mixtures of normals .",
    "while posterior means of coefficients under bma will not be well defined , predictive quantities under model selection or model averaging will be stable , however , care must be taken in assigning prior probabilities over equivalent models .",
    "problems with fixed values of @xmath0 prompted @xcite to study data - dependent or adaptive values for @xmath0 .",
    "this includes the unit information prior where @xmath28 @xcite , and local and global empirical bayes ( eb ) estimates of @xmath0 @xcite .    for the local eb",
    ", each model @xmath20 has its own optimal value of @xmath0 that maximizes its marginal likelihood : @xmath158 and the local eb estimator of the marginal likelihood is obtained by simply plugging in the estimator : @xmath159 .",
    "for example , under the @xmath0-prior , @xcite derive @xmath160 which has a similar format to @xmath161 , its counterpart for the test - based marginal likelihood under the @xmath0-prior , derived by @xcite .",
    "the global eb involves only a single estimator of @xmath0 , based on the marginal likelihood averaged over all models @xmath162 .",
    "the global eb estimator may be obtained via an em algorithm when all models may be enumerated @xcite , but is more difficult to compute for larger problems @xcite . for the remainder of the article , we will restrict attention to the local eb approach .",
    "the eb estimates of @xmath0 do not lead to consistent model selection under the null model @xcite although provide consistent estimation .",
    "mixtures of @xmath0-priors provide an alternative that propagate uncertainty in @xmath0 with other desirable properties .",
    "@xcite highlight some of the problems with using a fixed value of @xmath0 for model selection or bma and recommended mixtures of @xmath0-priors that lead to closed form expressions or tractable approximations . in order to consider the model selection criteria of @xcite",
    ", we propose an extremely flexible mixture of @xmath0-priors family that can encompass the majority of the existing mixtures of @xmath0-priors as special cases .",
    "furthermore , utilizing laplace approximations to obtain , it yields marginal likelihoods and ( data - based ) bayes factors in closed form , for both glms , and extensions such as normal linear regressions with unknown variances and over - dispersed glms .",
    "this tractability permits establishing properties such as consistency .",
    "the parameter @xmath0 enters into the posterior distribution for @xmath19 and the marginal likelihood through the shrinkage factor @xmath10 or the complementary shrinkage factor @xmath163 .",
    "since the approximate marginal likelihood depends on @xmath0 in the format of @xmath164 , @xmath165 , a conjugate prior for @xmath164 ( given @xmath47 ) should contain the kernel of a truncated gamma density with the support @xmath166 $ ] .",
    "beta distributions are also natural prior choice for @xmath164 , such as the hyper-@xmath0 prior of @xcite",
    ". other mixtures of @xmath0-priors such as the robust prior @xcite and the intrinsic prior @xcite truncate the support of @xmath0 away from zero , so the resulting @xmath164 has an upper bound strictly smaller than one .    to incorporate the above choices in one unified family",
    ", we adopt a generalized beta distribution introduced by @xcite called the compound confluent hypergeometric distribution , whose density function contains both gamma and beta kernels , and allows truncation on the support through a straightforward extension .",
    "we say that @xmath164 has a truncated compound confluent hypergeometric distribution if @xmath167 with density expressed as @xmath168^r}\\   \\mathbf{1}_{\\{0 < u < \\frac{1}{v }   \\}}\\ ] ] where parameters @xmath169 . here ,",
    "@xmath170 $ ] is the confluent hypergeometric function of two variables or humbert series @xcite , and @xmath171 is the pochammer coefficient or shifted factorial : @xmath172 if @xmath173 and @xmath174 for @xmath175 .",
    "note that the parameter @xmath176 controls the support of @xmath164 .",
    "when @xmath177 , the support is @xmath178 $ ] .",
    "when @xmath179 , the upper bound of the support is strictly less than one , which may accommodate priors with truncated @xmath0 .",
    "this leads to conjugate updating of @xmath164 as follows :    [ proposition : tcch_marlik ] let @xmath163 have the prior distribution @xmath180 where @xmath181 and @xmath182 , then for glms with a fixed dispersion @xmath47 , integrating the marginal likelihood in with respect to the prior on @xmath164 yields the marginal likelihood for @xmath20 which is proportional to @xmath183 where @xmath184 is the rank of @xmath114 , and @xmath185 is given in .",
    "the posterior distribution of @xmath164 under model @xmath20 is also a tcch distribution @xmath186 allowing conjugate updating under integrated laplace approximations .",
    "the bayes factor for comparing @xmath20 to @xmath26 is @xmath187^{\\frac{1}{2 } } v^{-\\frac{p_\\mm}{2 } } \\exp\\left\\{\\frac{z_\\mm}{2 }    - \\frac{q_\\mm}{2v}\\right\\ }   \\frac{b\\left(\\frac{a + p_\\mm}{2 } , \\frac{b}{2 } \\right )    \\phi_{1}\\left ( \\frac{b}{2 } , r , \\frac{a + b + p_\\mm}{2 } , \\frac{s+q_\\mm}{2v } , 1-\\kappa \\right )    }    { b\\left(\\frac{a}{2 } , \\frac{b}{2 } \\right ) \\phi_{1}\\left ( \\frac{b}{2 } , r , \\frac{a+b}{2 } , \\frac{s}{2v } , 1-\\kappa \\right)}\\ ] ] and depends on the data through the deviance @xmath125 and the wald statistic @xmath130 .",
    "we refer to the model selection criterion based on the bayes factor above as the `` confluent hypergeometric information criterion '' or chic as it involves the confluent hypergeometric function in two variables and the @xmath0-prior is derived using the information matrix ; the hierarchical prior formed by , and will be denoted as the chic @xmath0-prior .    in the conjugate updating scheme , the parameter @xmath188 and",
    "@xmath189 are updated by the model rank @xmath18 and the wald statistic @xmath130 , respectively , while none of the remaining four parameters are updated by the data .",
    "the parameters @xmath190 and @xmath191 play a role similar to the shape parameters in beta distributions , where small @xmath188 or large @xmath192 tends to put more prior weight on small values of @xmath164 , or equivalently , large values of @xmath0 .",
    "we will show later that @xmath188 also controls the tail behavior of the marginal prior on @xmath19 .",
    "the parameter @xmath176 controls the support , while parameters @xmath193 , and @xmath194 `` squeeze '' the prior density to left or right @xcite .",
    "in particular , large @xmath189 skews the prior distribution of @xmath164 towards the left side and in turn favoring large @xmath0 .",
    "table [ tb : tcchg_parameters ] lists special cases of the chic @xmath0-prior and corresponding hyper parameters that have appeared in the literature .",
    "the last column indicates whether the model selection consistency holds for all models which will be presented in section [ sec : selection_consistency ] .",
    "we provide more details about these special cases in the next sections .",
    ".special cases of the chic @xmath0-prior with hyper parameters and whether the prior distributions lead to consistency for model selection under all models .",
    "if no , the models where consistency fails are indicated . [",
    "cols=\"<,^,^,^,^,^,^,<\",options=\"header \" , ]"
  ],
  "abstract_text": [
    "<S> mixtures of zellner s @xmath0-priors have been studied extensively in linear models and have been shown to have numerous desirable properties for bayesian variable selection and model averaging . </S>",
    "<S> several extensions of @xmath0-priors to generalized linear models ( glms ) have been proposed in the literature ; however , the choice of prior distribution of @xmath0 and resulting properties for inference have received considerably less attention . in this paper , we unify mixtures of @xmath0-priors in glms by assigning the truncated compound confluent hypergeometric ( tcch ) distribution to @xmath1 , which encompasses as special cases several mixtures of @xmath0-priors in the literature , such as the hyper-@xmath0 , beta - prime , truncated gamma , incomplete inverse - gamma , benchmark , robust , hyper-@xmath2 , and intrinsic priors . through an integrated laplace approximation , </S>",
    "<S> the posterior distribution of @xmath1 is in turn a tcch distribution , and approximate marginal likelihoods are thus available analytically , leading to `` compound hypergeometric information criteria '' for model selection . </S>",
    "<S> we discuss the local geometric properties of the @xmath0-prior in glms and show how the desiderata for model selection proposed by bayarri et al , such as asymptotic model selection consistency , intrinsic consistency , and measurement invariance may be used to justify the prior and specific choices of the hyper parameters . </S>",
    "<S> we illustrate inference using these priors and contrast them to other approaches via simulation and real data examples .    _ </S>",
    "<S> keywords : _ bayesian model selection , bayesian model averaging , variable selection , linear regression , hyper-@xmath0 priors </S>"
  ]
}