{
  "article_text": [
    "understanding information in classical and quantum physics has helped us shed light on the fundamental nature of these theories . indeed , it has even been suggested that quantum theory could be more naturally formulated in terms of its information - theoretic properties  @xcite . yet",
    ", we have barely scratched the surface of understanding the role of information in the natural world . to gain a deeper understanding of information in physical systems , and to help explain _ why",
    "_ nature is quantum , it is sometimes instructive to take a step back and view quantum mechanics in a much broader context of possible physical theories .",
    "many examples are known that indicate that if our world were only slightly different , our ability to perform information processing tasks could change dramatically  @xcite .    however , before we can hope to really investigate general theories from the perspective of information processing , we first need to find a way to quantify information . in a quantum and classical world , this can be done using the von neumann and shannon entropy respectively , which capture our notions of information and uncertainty in an intuitive way .",
    "these quantities have countless practical applications , and have played an important role in understanding the power of such theories with respect to information processing .    here , we propose a measure of information that applies to _ any _ physical theory   which admits the minimal notions of finite physical _ systems _ , their _ states _ , and the probabilistic outcomes of _ measurements _ performed on them .",
    "many such theories have been suggested , each of which shares some aspects with quantum theory , yet have important differences .",
    "for example , we might consider quantum mechanics itself with a limited set of allowed measurements , quantum mechanics in a real hilbert space , generalized probabilistic theories  @xcite , general @xmath0-algebraic theories @xcite , box world  @xcite ( a theory admitting all non - signalling correlations @xcite , previously called generalized non - signalling theory @xcite ) , classical theories with an epistemic restriction  @xcite or theories derived by relaxing uncertainty relations  @xcite .",
    "we propose an entropic measure of information @xmath1 that can be used in any such theory in section  [ sec : entropydef ] .",
    "we will show that our measure reduces to the von neumann and shannon entropy in the quantum and classical setting respectively .",
    "in addition , we show that it shares many of their appealing intuitive properties .",
    "for example , we show that the quantity is always positive and bounded for the finite systems we consider .",
    "this provides us with a notion that each system has some maximum amount of information that it can contain .",
    "furthermore , we might expect that mixing increases entropy .",
    "i.e. that the entropy of a probabilistic mixture of states can not be less than the average entropy of its components .",
    "this is indeed the case for our entropic quantity .",
    "another property that is desirable of a useful measure of information is that it should take on a similar value for states which are close , in the sense that there exists no way to tell them apart very well .",
    "this is the case for the von neumann and shannon entropy , and also for our general entropic quantity , given one extra minor assumption .",
    "finally , when considering two different systems @xmath2 and @xmath3 , one may consider how the entropy of the joint system @xmath4 relates to the entropy of the individual systems .",
    "it is intuitive that our uncertainty about the entire system @xmath4 should not exceed the sum of our uncertainties about @xmath2 and @xmath3 individually .",
    "this property is known as subadditivity and is obeyed by our measure of entropy given one additional reasonable assumption on the physical theory .",
    "our entropic quantity thus behaves in very intuitive ways .",
    "yet , we will see that there exist physical theories for which it is not strongly subadditive , unlike in quantum mechanics .",
    "of course , there are multiple ways to quantify information and we discuss our choice by examining some alternatives and possible extensions such as notions of accessible information , relative entropy as well as rnyi entropic quantities in sections  [ sec : renyi ] and  [ sec : decomp ] .      clearly , it is also desirable to capture our uncertainty about some system @xmath2 _ conditioned _ on the fact that we have access to another system @xmath3 .",
    "this is captured by the _ conditional _",
    "entropy , for which we provide two definitions in section  [ sec : conditionaldef ] which are both interesting and useful in their own right .",
    "based on such definitions we also define notions of mutual information which allow us to quantify the amount of information that two systems hold about each other .",
    "our first definition of conditional entropy is analogous to the quantum setting , and indeed reduces to the conditional von neumann entropy in a quantum world .",
    "this is an appealing feature , and opens the possibility of interesting operational interpretations of this quantity as in a quantum setting  @xcite .",
    "yet , we will see that there exists a theory ( called box world ) for which not only the subadditivity of the conditional entropy is violated , but also where conditioning _ increases _ entropy .",
    "intuitively , we would not expect to grow more uncertain when given additional information , which we could always choose to ignore .",
    "we will hence also introduce a second definition of conditional entropy , which does not reduce to the von neumann entropy in the quantum world .",
    "however , it has the advantage that in _ any _ theory conditioning _ reduces _ our uncertainty , as we would intuitively expect when taking an operational viewpoint .",
    "nevertheless , even our second definition of the conditional entropy violates subadditivity .",
    "naturally , one might ask whether the fact that both our definitions of the conditional entropy violate subadditivity is simply a shortcoming of our definitions . in section  [ sec : general ] we therefore examine what properties any ` reasonable ' measure of conditional entropy can have in principle . by reasonable here",
    "we mean that if given access to a system @xmath3 we have no uncertainty about some classical information @xmath2 , then the quantity is 0 , and otherwise it is positive ( or even non - zero ) .",
    "we show that under this simple assumption there exists _ no _ measure of conditional entropy in box world that is subadditive or obeys a chain rule .",
    "to give some intuition about how our entropies can be used outside of quantum theory , we examine a very simple example in box world in section  [ sec : boxentropy ] , which illustrates all the peculiar properties our entropies can have .",
    "this is based on a task in which alice must produce an encoding of a string @xmath5 , such that bob can retrieve any bit of his choosing with some probability  @xcite ( known as a random access encoding ) .",
    "it is known that superstrong random access codes exist in box world  @xcite , leading to a violation of the quantum bound for such encodings  @xcite .",
    "a similar game was used in  @xcite to argue that one of the defining characteristics that sets the quantum world apart from other possibilities ( and particularly box world ) is that communication of @xmath6 classical bits causes information gain of at most @xmath6 bits , a principle called ` information causality ' . in section  [ sec : infocausality ] , we examine this statement using our entropic quantity .",
    "we notice that it is the failure of subadditivity of conditional entropy in box world that leads to a violation of the inequality quantifying information causality given in  @xcite .",
    "we conclude our examples by discussing the definition of ` information causality ' more generally .      in the classical , as well as",
    "the quantum setting , the shannon and von neumann entropies have appealing operational interpretations as they capture our ability to compress information . in section  [ sec : codingtheorem ] , we show that the quantity @xmath7 has a similar interpretation for some physical theories . when defining entropy we have chosen to restrict ourselves to a minimal set of assumptions , only assuming that a theory would have some notion of states and measurements . to consider compressing a state or indeed decoding it again , however , we need to know a little more about our theory . in particular , we first have to define a notion of ` size ' for any compression procedure to make sense .",
    "second , we need to consider what kind of encoding and decoding operations we are allowed to perform .",
    "given these ideas , and several additional assumptions on our physical theory , we prove a simple coding theorem .      in section  [ sec : assumptions ] , we introduce a framework for describing states , measurements and transformations in general physical theories , followed in section  [ sec : examples ] by some examples . in section  [ sec : entropy ]",
    "we then define our entropic measures of information that can be applied in any theory .",
    "examples of of how these entropies can be applied in box world can be found in section  [ sec : boxentropy ] . in section  [ sec : general ] we examine what properties we can hope to expect from a conditional entropy in box world .",
    "section  [ sec : infocausality ] investigates the notion of information causality in our framework and finally we show a coding theorem for many theories in section  [ sec : codingtheorem ] .",
    "we conclude with many open questions in section  [ sec : openquestions ] .",
    "we now present a simple framework , based on minimal operational notions ( such as systems , states , measurements and probabilities ) , that encompasses both classical and quantum physics , as well as more novel possibilities ( such as ` box world ' )  @xcite .",
    "our approach is similar to that in  @xcite , however it is slightly more general as it does not assume that all measurements that are mathematically well - defined are physically implementable , or that joint systems can be characterised by local measurements .",
    "firstly , we will assume that there is a notion of discrete physical _",
    "systems_. with each system @xmath2 we associate a set of allowed _ states _",
    "@xmath8 , which may differ for each system .",
    "we furthermore assume that we can prepare arbitrary mixtures of states ( for example by tossing a biased coin , and preparing a state dependent on the outcome ) , and therefore take @xmath8 to be a convex set , with @xmath9 denoting the state that is the mixture of @xmath10 with probability @xmath11 and @xmath12 with probability @xmath13 .",
    "to characterize when two states are the same , or close to each other , we first need to introduce the notion of measurements .",
    "secondly , we thus assume that on each system @xmath2 , we can perform a certain set of allowed measurements @xmath14 .",
    "if the system @xmath2 is clear from context , we will omit the subscripts and simply write @xmath15 and @xmath16 .    with each measurement",
    "@xmath17 we associate a set of outcomes @xmath18 , which for simplicity of exposition we take to be finite .",
    "when a particular measurement is performed on a system , the probability of each outcome should be determined by its state .",
    "we therefore associate each possible outcome @xmath19 with a functional @xmath20 $ ] , such that @xmath21 is the probability of obtaining outcome @xmath22 given state @xmath23 .",
    "we refer to such a functional as an _",
    "effect_. to ensure that measurement behaves according to our intuition when applied to mixed states , we require that @xmath24 .",
    "this means that each effect can be taken to be _",
    "linear_. in order for the probabilities of all measurement outcomes to sum to one , we also require that @xmath25 where @xmath26 is the _ unit effect _ , which has the property that @xmath27 for all @xmath28 . we can thus characterize a measurement @xmath17 as a set of outcome / effect pairs   @xmath29 we write @xmath30 for the probability distribution over outcomes when @xmath31 is performed on a state @xmath23 . note that in this general framework , not all measurements that are mathematically well - defined need be part of a particular physical theory .",
    "one measurement can be equivalent to , or strictly more informative than , another .",
    "consider two measurements @xmath31 ( with outcomes @xmath32 and effects @xmath33 ) and @xmath34 ( with outcomes @xmath35 and effects @xmath36 ) , for which there exists a map @xmath37 such that @xmath38 if @xmath39 is one - to - one it corresponds to a _ re - labelling _ of the outcomes . otherwise , we say that @xmath34 is a _ coarse - graining _ of @xmath31 ( or alternatively that @xmath31 is a _ refinement _ of @xmath34 ) . because we can always re - label the outcomes of an experiment according to any map @xmath39 , we assume that @xmath40 is closed under re - labelling and coarse - graining .",
    "this implies that @xmath40 always contains the trivial measurement @xmath41 ( with one outcome corresponding to effect @xmath26 ) .",
    "a refinement / coarse - graining is trivial if @xmath42 in this case , the measurement of @xmath31 is equivalent to performing @xmath34 and obtaining @xmath43 , then outputting a randomly selected @xmath22 satisfying @xmath44 ( where the distribution depends on the proportionality constant in  ) . hence the two measurements are equally informative about the state . in contrast ,",
    "when @xmath31 is a non - trivial refinement of @xmath34 it offers strictly more information about the state , and in this case we write @xmath45 .",
    "a subset of measurements of particular importance are the _ fine - grained _",
    "measurements @xmath46 , which have no non - trivial refinements , and are therefore optimal for gathering information about the state .",
    "formally , @xmath47 we will also call an effect @xmath48 fine - grained if it is part of a fine - grained measurement . we assume that @xmath49 is non - empty ( i.e. that there exists at least one finite outcome fine - grained measurement ) . in quantum and classical theory",
    "this restricts us to the finite - dimensional case .      as well as preparing states and performing measurements",
    ", it may be possible to perform transformations on a system .",
    "as in the case of effects , in order to behave reasonably when applied to mixed states , a transformation must correspond to a linear map @xmath50 taking allowed states to allowed states ( although the input and output systems may be of a different type ) . for each type of system , there will be some set of allowed transformations @xmath51 .",
    "we assume that the identity transformation @xmath52 is allowed , and that the composition of two allowed transformations is allowed ( as long as the system output by the first transformation is of the same type as the input to the second ) .",
    "furthermore , it must be the case that any allowed transformation followed by an allowed measurement is an allowed measurement .",
    "we can also combine the notion of transformation with that of measurement in a natural way to represent non - destructive measurements  @xcite . to incorporate non - destructive measurements ,",
    "define the sub - normalised states @xmath53 .",
    "a measurement can then be described by assigning a subnormalised transformation @xmath54 to each outcome @xmath22 .",
    "result @xmath22 occurs with probability @xmath55 and the post measurement state is @xmath56 .",
    "however , we will not need such constructions in the main part of this paper .",
    "having introduced measurements , we can now define what it means for two states to be equal .",
    "given that we are taking an operational viewpoint , we adopt the intuitive notion that two states @xmath57 are equal , if and only if there exists no measurement that distinguishes them .",
    "that is , @xmath58    we can also define a natural measure of distance for states @xmath59 that directly relates to the probability that we can distinguish these states using measurements available in our theory , in analogy to the quantum setting  @xcite .",
    "suppose we are given either @xmath60 or @xmath61 with equal probability , and perform a measurement @xmath17 to distinguish the two cases .",
    "note that the above implies that any theory that admits at least two possible states has at least one measurement @xmath17 with two possible outcomes .",
    "furthermore any such theory must have a measurement @xmath17 with exactly two outcomes since any theory admits arbitrary coarse - grainings of measurements .",
    "we will base our decision on the maximum likelihood rule , that is , when we obtain outcome @xmath22 , we will conclude we received state @xmath60 if @xmath62 and @xmath61 otherwise .",
    "the probability of distinguishing the two states using measurement @xmath17 is then given by @xmath63 where @xmath64 and @xmath65 .",
    "we now define the distance as @xmath66    by the above , we see that this measure of distance has an appealing operational interpretation because it directly captures our ability to distinguish the two states @xmath60 and @xmath61 using any available measurement ( see appendix [ app : distance ] , lemma  [ lem : distancemeasure ] for details ) . in the quantum setting",
    ", it thus directly reduces to the well - known trace distance .",
    "suppose that we have two systems @xmath2 and @xmath3 , each of which may admit different sets of states and measurements .",
    "we allow that two individual systems can be combined into a _ composite _",
    "system @xmath4 , which we can treat as a new type of system having its own set of allowed states , measurements , and transformations just as in the single - system case .",
    "however , these sets must bear some relation to those of the component subsystems .",
    "with respect to states , we would like it to be possible to independently prepare any state @xmath67 of system a and @xmath68 of system b. this corresponds to a _ product state _ of the composite system , which we denote by @xmath69 .",
    "note that at this point we have not proved that @xmath70 corresponds to a tensor product in the usual sense , but we would nevertheless expect that it is distributive for mixtures and associative .",
    "we make use of the standard terminology that states are _ separable _ if they can be written as a mixture of product states , and _ entangled _ otherwise . to avoid excessive subscripts",
    "when dealing with multiple systems , we will usually refer to the state of systems @xmath4 and @xmath3 directly by these letters , rather than the more cumbersome @xmath71 and @xmath72 ( e.g. @xmath73 etc . ) .",
    "similarly , we would expect to be able to perform a measurement @xmath74 and @xmath75 , giving a product measurement which we denote by @xmath76 ( with outcome set @xmath77 and effects @xmath78 ) . by considering coarse - graining and tri - partite systems",
    ", we would again expect @xmath70 to be distributive and associative . when applying a product measurement to a product state",
    "we furthermore require that @xmath79    when considering multiple systems , we can consider what happens if we only measure some of these systems .",
    "note that this means that we perform a measurement consisting of a unit effect on some of these systems .",
    "this only makes sense if marginal states are well defined and we hence assume that even when a bipartite state is entangled each part is an allowed marginal state .",
    "we can thus have @xmath80 furthermore , in the case in which b performs a measurement on his subsystem and obtains result @xmath22 ( corresponding to an effect @xmath33 ) we would expect a s subsystem to ` collapse ' to an allowed state @xmath81 .",
    "we will denote such a state as @xmath82    finally , a crucial constraint on multi - partite systems is the existence of product transformations @xmath83 . in a variant of quantum theory in which all positive ( rather than completely positive ) trace - preserving maps are allowed transformations",
    ", this would prevent the existence of entangled states .",
    "in this section we show how quantum theory and classical probability theory fit into the framework defined above , and also describe the theory known as ` box world ' @xcite , which admits all non - signalling correlations @xcite , and was one of the main motivations for this work .      in classical probability theory , a state @xmath23 corresponds to a probability distribution @xmath84 over a finite set of elements .",
    "the effects correspond to linear functionals of the form @xmath85 for any @xmath86 $ ] .",
    "note that the unit effect corresponds to @xmath87 .",
    "normalisation of measurements therefore requires @xmath88 transformations correspond to stochastic maps .      in quantum theory ,",
    "the convex set of states are the density operators @xmath89 ( trace-1 positive operators ) , and effects correspond to linear functionals of the form @xmath90 where @xmath91 is a positive operator . all measurements satisfying the normalisation constraint @xmath92 are allowed , and the fine - grained measurements are those for which all @xmath91 are rank 1 operators .",
    "the allowed transformations represent completely positive trace - preserving maps  .",
    "note that unlike other approaches  @xcite our framework also encompasses real hilbert space quantum mechanics .",
    "furthermore , because we do not assume that all well - defined operations are physically realizable , it can be used to study quantum or classical theory with a restricted set of states , measurements and transformations ( for an interesting example in the classical case consider spekkens toy model  @xcite ) .",
    "the entropies we would assign in such cases would differ from the standard von neumann entropy , and may be interesting to study .      in box world",
    ", the state of a single system @xmath93 corresponds to a conditional probability distribution @xmath94 where @xmath95 and @xmath96 are elements of a finite set of ` inputs ' and ` outputs ' respectively .",
    "the intuition is that there is a special set of measurements on each system represented by @xmath95 ( referred to as _ fiducial _",
    "measurements ) , and that any probability distribution for these measurements corresponds to an allowed state .",
    "we represent a system @xmath93 with @xmath97 possible inputs @xmath95 and @xmath6 possible outputs @xmath96 by        in the special case in which there is only one possible input , the conditional probability distribution reduces to the standard unconditional probability distribution @xmath98 , and we omit the input line to the box in the diagram . thus box",
    "world contains classical probability theory as a special case , and we will use such _ classical boxes _ to represent classical information in our treatment of information - theoretic protocols in box world .",
    "a multi - partite state in box world corresponds to a joint conditional probability distribution @xmath99 with a separate input and output for each system . aside from the usual constraints of normalisation and positivity",
    ", the allowed states must also satisfy the non - signalling conditions : that the marginal probability distribution obtained by summing over @xmath100 , @xmath101 is independent of @xmath102 for all @xmath97 .",
    "this means that the other parties can not learn anything about a distant party s measurement choice from their own measurement results .",
    "a bipartite state of particular interest is the pr - box state  @xcite , for which all inputs and outputs are binary , and the probability distribution is @xmath103 where @xmath104 denotes addition modulo 2 .",
    "this state is ` more entangled ' than any quantum state , yielding correlations that achieve the maximum possible value of 4 for the clauser - horne - shimony - holt ( chsh ) expression  @xcite , compared to @xmath105 for quantum theory ( tsirelson s bound @xcite ) , and @xmath106 for classical probability theory .",
    "we represent entanglement between systems in box world by a zigzag line between them , and classical correlations ( i.e. separable but non - product states ) by a dotted line .        in box world , we allow all mathematically well - defined measurements and transformations to be physically implemented .",
    "writing @xmath107 and @xmath108 , all effects take the form @xmath109 where @xmath110 can be taken to be positive  @xcite . the effect @xmath111 corresponding to performing joint fiducial measurements",
    "@xmath112 and obtaining results @xmath113 is represented by @xmath114 . because of the positivity of @xmath115 , any effect can be expressed as a weighted sum of such fiducial measurement effects . it follows that a measurement is fine - grained if and only if each of its effects is proportional to some @xmath116 , and that products of fine - grained measurements are themselves fine - grained .",
    "the shannon entropy @xmath117 and von neumann entropy @xmath118 are extremely useful tools for analyzing information processing in a classical or quantum world . here",
    ", we would like to define an analogous entropy for general probabilistic theories which reduces to @xmath119 and @xmath120 for classical probability theory and quantum theory respectively .",
    "we would also like our new entropy to retain as many of the mathematical properties of the shannon and von neumann entropy as possible . not only",
    "will this help our new entropy conform to our intuitive notions , but it will make it easier to prove general results using these quantities , and transfer known results to the general case . note that although we can use any base for the logarithm in the definition of the shannon and von neumann entropies ( as long as we are consistent ) , in what follows we will use base 2 ( i.e. @xmath121 ) throughout .",
    "we now give a concrete definition of entropy for any physical theory , which satisfies the above desiderata .",
    "other definitions are certainly possible , and we will consider one alternative ( based on mixed state decomposition ) in section [ sec : decomp ] .",
    "however , the following definition has many appealing properties .    given any state @xmath28 , we define its entropy @xmath122 by @xmath123 where the infimum is taken over all fine - grained measurements @xmath124 on the state space @xmath16 and @xmath125 is the shannon entropy of the probability distribution @xmath126 over possible outcomes of @xmath17 .",
    "this has an intuitive operational meaning as the minimal output uncertainty of any fine - grained measurement on the system .",
    "note that for information - gathering purposes , the best measurements are always fine - grained , and without restricting to this subset the unit measurement would always be optimal ( giving zero outcome uncertainty ) .",
    "furthermore note that trivial refinements of @xmath31 always generate a higher output entropy , so it is sufficient to only consider measurements in the infimum that have no parallel effects .    in appendix [ app : entropy ]",
    ", we prove that @xmath1 retains several important properties of the shannon and von neumann entropy .",
    "in particular , we show :    1 .",
    "( _ reduction _ )",
    "@xmath1 reduces to the shannon entropy for classical probability theory , and the von neumann entropy for quantum theory .",
    "( _ positivity and boundedness _ )",
    "suppose that the minimal number of outcomes for a fine - grained measurement in @xmath127 is @xmath128 .",
    "then for all states @xmath28 , @xmath129 3 .",
    "( _ concavity _ ) for any @xmath130 and any mixed state @xmath131 : @xmath132",
    "( _ limited subadditivity _ ) consider a theory with the additional property that fine - grained measurements remain fine - grained for composite systems .",
    "i.e. @xmath133 this is true in quantum theory , classical theory , and box world .",
    "when holds , then for any bipartite state @xmath134 and reduced states @xmath135 and @xmath136 @xmath137 5 .",
    "( _ limited continuity _ ) . consider a system for which all allowed measurements have at most @xmath138 outcomes , or for which restricting the allowed measurements to have at most @xmath138 outcomes does not change the entropy of any state . this is true in quantum theory , with @xmath139 , and also in box world and classical theory .",
    "then we can prove an analogue of the fannes inequality  @xcite , which says that the entropy of two states which are close does not differ by too much .",
    "in particular , given @xmath140 satisfying @xmath141 , @xmath142    we will also see in section  [ sec : codingtheorem ] that @xmath1 has an appealing operational interpretation as a measure of compressibility for some theories .    however , one property of the von neumann entropy that does not carry over to @xmath1 is strong subadditivity  .",
    "in particular , we will see in section [ sec : boxentropy ] there exists a tripartite state in box world such that @xmath143        based on the entropy @xmath1 , we can also define a notion of conditional entropy . in analogy to the von neumann entropy  @xcite , we define the conditional entropy of a general bipartite state @xmath134 with reduced states @xmath135 and @xmath144 by @xmath145 this has the nice property that for quantum or classical systems it reduces to the conditional von neumann and shannon entropies respectively . in some theories ( including quantum theory but not classical probability theory ) , @xmath146 can be negative , which is strange , but opens the way for an appealing operational interpretation as in the quantum setting  @xcite .",
    "however , unlike in quantum theory , we will see that @xmath147 has the counterintuitive property that it can _ decrease _ when ` forgetting ' information in some probabilistic theories .",
    "in particular , the violation of strong subadditivity for @xmath1 in box world implies that it is possible to obtain @xmath148 , and that @xmath147 is not subadditive .",
    "these properties will motivate us to consider an alternative definition of the conditional entropy below .",
    "however , we will show that no ` reasonable ' entropy in box world can have all the appealing properties of the conditional von neumann entropy .    in analogy to the quantum case",
    ", we can also define the _ mutual information _ via @xmath149",
    "this quantity will be positive whenever subadditivity holds , and reduces to the usual mutual information in the quantum and classical case .",
    "similarly , we may define a notion of _ accessible information _ analogous to the quantum setting as @xmath150 where @xmath151 is the classical mutual information .",
    "given the problems observed with the previous definition in some theories , we now define a second form of conditional entropy based on @xmath1 , which sometimes captures our intuitive notions about information in a nicer way .",
    "for any bipartite state @xmath152 with reduced states @xmath153 and @xmath144 we define @xmath154 where the infimum is taken over all measurements on @xmath3 , and @xmath155 is the reduced state of the first system conditioned on obtaining measurement outcome @xmath156 when performing @xmath157 on the second system .",
    "this definition has the appealing property that conditioning on more systems always reduces the entropy , that is , @xmath158 ( see appendix [ app : condentropy ] , lemma  [ lem : conditioningreducesentropy ] ) , and it reduces to the conditional shannon entropy in the classical case .",
    "note , however , that @xmath159 does not reduce to the conditional von neumann entropy in the quantum setting , as it is always positive .",
    "furthermore , we will see in section  [ sec : general ] that it is not subadditive , and does not obey the usual chain rule .",
    "( even though a limited form of chain rule holds in box world as we show in the appendix section  [ sec : boxchain ] )",
    ". nevertheless @xmath159 seems quite a natural entropic quantity , and its corresponding quantum version has found an interesting application in the study of quantum correlations  @xcite .",
    "we can also define a corresponding information quantity via @xmath160 which is always positive .",
    "however , unlike @xmath161 , this definition is not symmetric and hence it can not really be considered ` mutual information ' .",
    "instead , @xmath162 captures the amount of information that @xmath3 holds about @xmath2 .      for cryptographic purposes , such as in the setting of device independent security for quantum key distribution , it is useful to define the following rnyi entropic variants of @xmath1 .",
    "more precisely , we define @xmath163 where @xmath164 is the rnyi entropy of order @xmath165 .",
    "note that @xmath166 ( taking the limit of @xmath167 ) .",
    "these quantities can also be useful in order to bound the value of @xmath7 itself as for any state @xmath28 and @xmath168 we have @xmath169 .    to define a notion of relative entropy , we adopt a purely operational viewpoint .",
    "suppose we are given @xmath170 copies of a state @xmath61 or a state @xmath171 , and let @xmath172 classically , as well as quantumly , the relative entropy captures our ability to distinguish @xmath173 from @xmath174 for large @xmath170 .",
    "note that to distinguish the two cases , it is sufficient to coarse grain any measurement to a two outcome measurement @xmath175 , where without loss of generality we associate the outcome ` 1 ' with the state @xmath173 and ` 2 ' with @xmath174",
    ". then @xmath176 denotes the probability that we conclude that the state was @xmath174 , when really we were given @xmath173 .",
    "similarly , @xmath177 denotes the probability that we falsely conclude that the state was @xmath174 . in what is called asymmetric hypothesis testing",
    ", we wish to minimize the error @xmath176 while simultaneously demanding that @xmath177 is bounded from above by a parameter @xmath178 . here",
    "we fix @xmath179 .",
    "we therefore want to determine @xmath180 in a quantum setting , it has been shown that the quantum relative entropy is directly related to this quantity via the quantum stein s lemma  @xcite , which states that we have @xmath181 this is a deep result giving a clear operational interpretation to the relative entropy , telling us that in the large @xmath170 limit the probability of making the error @xmath182 decreases exponentially with @xmath183 . furthermore ,",
    "as it is expressed in operational terms , we can simply adopt ( [ eq : rel_ent ] ) as our definition of relative entropy in any theory for which the limit is well defined .",
    "thus we recover the usual value in the quantum ( and classical ) case , and in all other theories we still capture the same operational interpretation .",
    "note also that our choice of @xmath179 was quite arbitrary , and one may consider a family of relative entropies , one for each choice of @xmath178 .",
    "in quantum theory , these are all equivalent  @xcite , but they may yield different values in other theories .",
    "although the entropy @xmath1 has several appealing properties , and seems quite intuitive , it is nevertheless interesting to consider alternative notions of entropy for general theories .",
    "one seemingly natural alternative is the decomposition entropy , which measures the mixedness of a state .",
    "there is a special subset of states @xmath184 which can not be obtained by mixing other states : @xmath185 @xmath186 form the extreme points of @xmath187 and are referred to as _ pure states _ ( with the remaining states being _ mixed _ ) .",
    "suppose that any state in @xmath187 can be decomposed into a finite sum of pure states .",
    "then we can define the entropy of a state by the minimal shannon entropy of its decompositions into pure states .",
    "define a decomposition @xmath188 of a state @xmath28 as a probability distribution over the set of pure states that is non - zero for only a finite set of states @xmath189 with probabilities @xmath190 $ ] such that @xmath191 . then define the decomposition entropy as @xmath192    like our previous entropy definition , we show in appendix [ app : decomposition ] that @xmath193 reduces to the shannon and von neumann entropy in classical probability theory and quantum theory respectively .",
    "however , it has a number of unappealing properties when compared with @xmath1 . in particular",
    "it is neither concave nor subadditive , as revealed by explicit counterexamples from box world given in appendix [ app : decomposition ] .    after studying simple examples in box world",
    ", it seems that @xmath193 is a less intuitive and helpful measure of uncertainty than @xmath1 . for this reason , although @xmath193 may play an important role in discussions of entanglement or purity in many generalized theories , and may also lead to interesting operational interpretations , we do not discuss it further here .",
    "we now investigate how our entropic quantity @xmath7 behaves in box world with a simple , yet illustrative , example .",
    "to first gain some intuition on how @xmath1 behaves in such a setting , consider a trivial classical system @xmath93 which admits only one possible measurement and outputs @xmath194 possible values @xmath195 each which probability @xmath196 .",
    "clearly , since the system admits only one possible measurement @xmath17 , we have @xmath197 consider now a pr - box ( a bipartite system in the state ( [ eq : pr ] ) )        where alice holds system @xmath198 ( with binary input @xmath199 and output @xmath200 ) and bob holds system @xmath201 ( with binary input @xmath202 and output @xmath203 ) .",
    "note that the fine - grained measurements on the entire system correspond to a sequence of fiducial measurements on the two subsystems ( where the choice of input to the second subsystem may depend on the output of the first)@xcite , and the outcome is the output of both measurements .",
    "the minimal entropy for the joint system can be obtained by inputting ` 0 ' into both boxes , giving outputs ` 00 ' or ` 11 ' each with probability @xmath196 ( in fact , any other fine - grained measurement is equally good ) , and the marginal states yield a random output bit for any input .",
    "hence we have that @xmath204 we now consider a scenario for which it is known that pr - boxes yield an advantage over the quantum setting in terms of information processing .",
    "the basis of our example is a simple non - local game in which alice is given a random ` parity ' bit @xmath5 , and has to output two bits @xmath205 and @xmath206 satisfying @xmath207 ( where @xmath104 denotes addition modulo 2 ) . then , without receiving any communication from alice , bob is given a random target bit @xmath208 and has to successfully output @xmath209  @xcite .",
    "this game is equivalent to the chsh - game  @xcite .",
    "we begin with alice having the parity bit ( which we model by a classical box in the state @xmath93 described above ) , and alice and bob sharing a pr - box in the state @xmath210 .",
    "now alice performs the following procedure , which corresponds to an allowed transformation in box world .",
    "she measures the parity bit @xmath93 to obtain @xmath211 , then uses this as the input to her part of the pr - box , setting @xmath212 and obtaining outcome @xmath200 .",
    "finally , she prepares two new classical bits @xmath213 and @xmath214 ( represented by classical boxes @xmath215 ) .",
    "note that because of the correlations inherent in the pr box , the output of bob s system will now be described by @xmath216 .",
    "hence the state of @xmath217 after this procedure is the classically correlated state :        @xmath218    given any target bit @xmath208 , bob can win the game by setting @xmath219 and outputting the result @xmath220 .",
    "we can think of bob s system as a perfect random access encoding of the two - bit string @xmath221 @xcite .",
    "consider the entropies of the state @xmath217 .",
    "all of the individual systems yield a random output bit , giving @xmath222 and @xmath205 and @xmath206 are independent random bits , so @xmath223 also note that we have @xmath224 since for any input @xmath202 , the output @xmath203 will be perfectly correlated with one of the other bits ( giving only 2 independent random output bits ) . finally , because we can make @xmath203 perfectly correlated with either of the remaining bits we have @xmath225 where the optimal measurements are @xmath226 and @xmath227 respectively .",
    "these entropy values all seem very intuitive ( note in contrast that for the decomposition entropy @xmath228 .",
    "however , they violate several natural properties of the shannon and von neumann entropies .    _",
    "( ) @xmath229 strong subadditivity_.@xmath229    first of all , it is easy to see from the above that @xmath230 which violates strong subadditivity .",
    "we now turn to the two possible forms of conditional entropy that we defined , where our simple example clearly illustrates their differences .",
    "first of all , we consider the standard form of conditional entropy , which reduces to the von neumann entropy in the quantum settings . by the above , we can immediately see that it has the following interesting properties .    _",
    "( ) @xmath229 subadditivity of the conditional entropy_.@xmath229    using we deduce that @xmath231 which seems intuitive , as we can perfectly predict the output of either @xmath232 or @xmath233 ( but not both ) using @xmath201",
    ". however , this yields a violation of subadditivity for the conditional entropy , as @xmath234 this may seem rather bizarre at first glance , however , we will see in section  [ sec : general ] that no ` reasonable ' measure of conditional entropy in box world is subadditive , unlike the von neumann entropy .",
    "it is also interesting to consider the corresponding mutual information quantities , which are @xmath235 again , these seem intuitive , as we can extract one bit of information about either @xmath232 or @xmath233 or the pair @xmath236 from @xmath201",
    ".    it may be tempting to conclude that the point at which @xmath237 becomes subadditive ( or equivalently , where @xmath238 becomes strongly subadditive ) is exactly when the pr - box is weakened to obey tsirelson s bound .",
    "note that our trivial example only shows that pr - boxes which are more than @xmath239 correct do not obey subadditivity .",
    "however , note that constraining non - local boxes to obey tsirelson s bound alone is insufficient to reduce box world to quantum theory ( e.g. each quantum system admits a continuum of fine - grained measurements whereas any box admits only a finite set ) .    _ ( ) @xmath229 conditioning can increase entropy_.@xmath229    our small example also emphasizes another curious property of the conditional entropy . by definition ,",
    "@xmath240 but this is strange , because we can perfectly determine the output of @xmath232 given @xmath201 .",
    "furthermore , since @xmath241 , we then clearly have @xmath242 which means that ` forgetting information ' , namely discarding @xmath233 , can _ decrease _ uncertainty .",
    "again , it may seem that this is a consequence of not choosing the ` correct ' definition of entropy .      reevaluating the conditional entropies of the previous section using this new definition we find that @xmath243 as before , hence this new measure still violates subadditivity .",
    "however we now have @xmath244 as we would intuitively expect .",
    "this means that conditioning on @xmath233 no longer increases the entropy .",
    "however , it generates a violation of the chain rule @xmath245 on balance though , this measure of conditional entropy seems more reasonable than the original one in this example .",
    "we now show that _ any _ ` reasonable ' measure of the conditional entropy in box world will necessarily defy our intuition about information in several ways .    intuitively , the goal of any entropic quantity is to capture the degree of uncertainty we have about a system , possibly given access to some additional information . we assign a label @xmath2 to the system of interest and use @xmath3 to denote any additional systems or information available to us . for simplicity ,",
    "let us suppose that @xmath2 corresponds to some classical information ( i.e. it is a state of a classical box ) .",
    "let @xmath246 denote some entropic quantity that quantifies our uncertainty about @xmath2 given @xmath3 .",
    "if we were able to determine @xmath2 with certainty given access to @xmath3 ( i.e. to determine the precise output of the classical box @xmath2 ) , we would intuitively say that there is no uncertainty and the quantity @xmath246 should vanish .",
    "conversely , if we can not determine @xmath2 given @xmath3 , but will necessarily have some residual uncertainty , then the quantity @xmath246 should be positive .",
    "motivated by this intuition in quantifying uncertainty we demand the following two properties to hold for any ` reasonable ' measure of uncertainty when @xmath2 is classical .    1 .   if the output of @xmath2 can be obtained from @xmath3 with certainty , @xmath247 .",
    "if the output of @xmath2 can not be obtained from @xmath3 with certainty , then @xmath248 .    in the classical and quantum world , all commonly used entropic quantities satisfy these conditions ( given that @xmath2 is classical ) . in both such worlds , there also exist entropic quantities that are subadditive and obey a chain rule , for example the conditional shannon and von neumann entropies . in box world",
    ", @xmath249 is ` reasonable ' according to this definition , while @xmath146 is ` unreasonable ' .",
    "curiously , it turns out that in box world there can not be _ any _ reasonable measure of conditional entropy that obeys conditions \\{1 } and \\{2 } , but at the same time is subadditive or obeys a chain rule .    _",
    "( ) @xmath229 subadditivity of the conditional entropy_.@xmath229    consider the state of the two classical bits @xmath250 and bob s binary input / output box @xmath251 described by ( [ eq : bwstate ] ) in the previous section .",
    "we now show that in this case _ no _ reasonable measure of entropy that obeys properties \\{1 } and \\{2 } is subadditive .",
    "first of all , note that bob can determine one of the bits perfectly , given access to @xmath201 .",
    "therefore from condition \\{1 } , we obtain that @xmath252 however , since bob can not determine the parity of the two bits , he certainly can not learn both bits perfectly and hence from condition \\{2 } we have @xmath253 in order for subadditivity to hold , we would need that @xmath254 which using   and   leads to a contradiction .",
    "note that subadditivity could still hold , if the quantity @xmath255 were negative .    _",
    "( ) @xmath229 chain rule for the conditional entropy_.@xmath229    we now show that a chain rule is impossible in box world for any entropic quantity that satisfies \\{1 } and \\{2}. in fact , for the purposes of this proof it is sufficient to replace \\{2 } by the weaker assumption    1 .   if the output of @xmath2 can not be obtained from @xmath3 with certainty , then @xmath256 .",
    "note that for the state described by ( [ eq : bwstate ] ) , condition \\{1 } gives us @xmath257 because @xmath205 can be obtained perfectly from @xmath251 or @xmath258 .",
    "a chain rule for the conditional entropy would mean that @xmath259 using eq .",
    ", together with eqs .   and   again gives us a contradiction .",
    "note that @xmath159 obeys conditions \\{1 } and \\{2 } , and hence does not admit a chain rule in box world .",
    "as @xmath147 satisfies a chain rule , it follows from the above that it must be ` unreasonable ' .",
    "indeed , this can be seen from the fact that @xmath260 despite the fact that we can perfectly determine the output of @xmath232 given @xmath201 and @xmath233 , violating condition \\{1}. it is easy to see that if we were to drop the conditions that make an entropy ` reasonable ' but simply assume that it is not subadditive , but we do enforce a chain rule , then conditioning can increase entropy .",
    "we now use our entropic quantities to investigate the game given in  @xcite .",
    "this task relates to ` information causality ' , which is expressed as the principle that ` communication of @xmath97 classical bits causes information gain of at most @xmath97 bits ' . in  @xcite it",
    "is reported that this principle can be violated in box world using the following simple game ( where we take @xmath261 ) : alice is given two random classical bits @xmath262 and @xmath263 and bob is given a single random bit @xmath208 .",
    "alice is allowed to send a single bit message @xmath6 to bob , after which he must output a bit @xmath264 .",
    "the couple succeed in the task if @xmath265 .",
    "this task is clearly very similar to the non - local game considered in section [ sec : boxentropy ] .",
    "indeed , any solution to the previous problem can also be used to solve this one .",
    "alice takes the parity bit as @xmath266 , then generates @xmath205 and @xmath267 as before .",
    "she sends the message @xmath268 to bob . using the previous protocol",
    ", bob generates @xmath209 , and then outputs @xmath269 .    in the context of this game ,",
    "` information causality ' is interpreted as meaning that @xmath270 where @xmath271 is the classical conditional mutual information .",
    "this inequality is obeyed in quantum theory .",
    "however , given the above argument it is clear that it can be violated in box world , as alice and bob can achieve @xmath272 .",
    "let us examine why   fails in terms of our general entropies .",
    "we consider the state just after bob has received the message from alice , when she holds classical bits @xmath273 and @xmath274 , and bob holds the classical message @xmath39 and his part of the pr - box @xmath201 .",
    "this state is described by @xmath275 we can compute entropies explicitly in this case as in section [ sec : boxentropy ] , and will obtain similar results .",
    "however , @xcite also contains a proof of in quantum theory based on the quantum mutual information .",
    "it is interesting to attempt to follow this proof using our general mutual information @xmath276 ( or @xmath277 ) to see where it fails .",
    "the quantum proof relies on the chain rule for quantum mutual information ( which @xmath276 satisfies by definition ) , where @xmath278 .",
    "] , positivity of the mutual information ( which is true for @xmath276 in box world due to the subadditivity of @xmath1 ) , and non - signalling ( which is one of the defining features of box world ) . however , the crucial step is a use of the data processing inequality to deduce that @xmath279 although it is very natural that ` forgetting ' @xmath274 can only decrease the mutual information , this inequality is violated in box world .",
    "indeed , for the state we find @xmath280 this is again a consequence of the _ violation of strong subadditivity _ for @xmath1 , which forms the key ingredient in why   can be violated in box world .",
    "although the violation of @xmath281 in box world , and its validity in quantum theory , is a very interesting result , it is interesting to consider whether this really implies that communicating @xmath97 bits has caused an information gain of more than @xmath97 bits . from the state it is easy to check that @xmath282 hence under both these measures the total information about the composite system @xmath283 has only increased by one bit due to the one bit classical message .",
    "we show in section  [ sec : boxchain ] in the appendix that in box world we indeed have that given some arbitrary system @xmath201 held by bob , the mutual information about a classical string @xmath2 can never increase by more than the length of a classical message @xmath39 that is transmitted .",
    "furthermore , bob can extract only one of the two bits , either @xmath273 _ or _",
    "@xmath274 , with the help of the message as is indeed noted in  @xcite .",
    "it is therefore arguable that the information gain of bob is only one bit .",
    "perhaps ` information causality ' should be restated in a clearer way , that more directly represents the form of .",
    "e.g. the principle that an @xmath6 bit classical communication allows us to learn any one out of at most @xmath6 unknown bits .",
    "we now show that for some theories , the entropic quantity @xmath7 has an appealing operational interpretation in capturing our ability to compress information . here",
    ", we will only show this for theories obeying further restrictions , and it is an interesting open question how generally this interpretation applies .      before we can talk about compression",
    ", we first need to clarify our notions of the size of a system .",
    "intuitively , the size of a system should limit the amount of uncertainty we can have about it .",
    "furthermore , to compress , we will clearly need to shrink the original state space .",
    "it is therefore helpful to define a notion of size for any subset of allowed states @xmath284 .",
    "we refer to the size of a set of states @xmath285 as its _ dimension _",
    "@xmath128 , which we define by @xmath286 this corresponds to eliminating all measurement outcomes that can not occur for any state in @xmath285 , and then counting the minimal number of remaining outcomes for any fine - grained measurement .",
    "it follows that @xmath287 for all @xmath288 . in quantum theory @xmath128",
    "corresponds precisely to the dimension of a hilbert space .",
    "a natural way to select a subset of states is to consider all states that yield a given measurement outcome with certainty .",
    "we refer to an effect @xmath289 such that @xmath290 is an allowed measurement , and that occurs with certainty for some state , as a _ full _ effect ( i.e. @xmath289 is full if there exists @xmath291 such that @xmath292 ) . for any full effect @xmath289",
    ", we can therefore define a non - empty subset of states @xmath293 .",
    "we refer to such a subset as the _ subspace _ of @xmath16 given by @xmath289 .",
    "note that subspaces are always convex , and the subspace corresponding to an effect @xmath289 which is both full _ and _ fine - grained obeys @xmath294 .",
    "so far , we were never concerned about what happens to a state after a measurement . in our compression protocol , however , we will need to use an abstract notion of post - measurement states as described in section  [ sec : trans ] . in particular , we will consider _ pseudo - projective measurements _ , which we define to be measurements that fullfill two conditions .    1 .   _ ( repeatability ) _ [ ass : repeatability ] a pseudo - projective measurement is repeatable , such that if the same measurement is applied again the same result is obtained .",
    "this requires that the output state @xmath295 after obtaining a result @xmath22 lies in the subspace given by @xmath33 ( i.e. , @xmath296 ) .",
    "consequently , all effects in a pseudo - projective measurement must be full effects .",
    "( weak disturbance ) _ if a particular outcome @xmath22 of a pseudo - projective measurement occurs with probability @xmath297 for a state @xmath23 , then the post measurement state @xmath295 after this result is obtained satisfies @xmath298 , where @xmath299 and @xmath300 $ ] are constants depending on the particular theory .",
    "for example , for projective measurements in quantum theory @xmath301 and @xmath179 .",
    "any projective measurement in quantum theory fulfills these conditions , but these conditions alone do not define projective measurements , hence the slightly different name .",
    "in quantum theory , the weak disturbance property can be understood as an instance of the gentle measurement lemma  @xcite .",
    "furthermore , in order to prove our simple coding theorem , we will need to make some additional assumptions on the states and the measurements that achieve the minimal output entropy @xmath7 in our theory .",
    "in particular , we assume that for all states , the minimal output entropy can be attained by a pseudo - projective measurement .",
    "that is , we assume that for all @xmath28 there exists some pseudo - projective measurement @xmath302 such that @xmath303 .",
    "we further assume that for all such measurements , @xmath304 is fine - grained and pseudo - projective , and that course grainings of @xmath305 can also be made pseudo - projective .",
    "lastly , we assume that the dimension of @xmath306 is @xmath307 . these assumptions are all true in the classical and quantum case ( where @xmath31 is projective ) .",
    "we will see in appendix  [ sec : codingtheoremproof ] , that this is all we will need to show the following simple coding theorem following the steps taken by shannon  @xcite and schumacher  @xcite ( see for example  ) .",
    "we consider a source that emits a state @xmath308 with probability @xmath309 , chosen independently at random in each time step .",
    "when considering @xmath310 time steps , we hence obtain a sequence of states @xmath311 with @xmath312 , where each sequence occurs with probability @xmath313 .",
    "a compression scheme consists of an encoding and decoding procedure .",
    "the encoding procedure maps each possible @xmath314 into a state @xmath315 .",
    "in turn the decoding procedure maps the states @xmath316 back to states @xmath317 on the original state space . in analogy with the quantum case",
    ", we say that the _ compression scheme has rate _ @xmath318 , if the dimension of the smaller space obeys @xmath319 .",
    "note that in order for a compression scheme to be useful , it must have @xmath320 ( and hence @xmath321 ) .",
    "a compression scheme is called _ reliable _ , if we can recover the original state ( almost ) perfectly , in the sense that the average distance between the original and the reconstructed state can be made arbitrarily small for sufficiently large @xmath310 .",
    "i.e. for any @xmath322 and all sufficiently large @xmath310 , @xmath323    note that the output of the source can be described as a mixed state @xmath324 in each time step , and a product state @xmath325 over the course of @xmath310 time steps .",
    "we then obtain the following theorem ( see appendix section  [ sec : codingapp ] ) in terms of the entropy of the source @xmath326 .",
    "note that in order to establish that @xmath7 truly characterizes our ability to compress information , we would also like to have a converse stating that for @xmath329 there exists no reliable compression scheme .",
    "in quantum theory , it is not hard to prove the converse of the above theorem since it admits a strong duality between states and measurements , which may also hold for other theories . here , however , we explicitly tried to avoid introducing any such strong assumptions .",
    "at the core of our little coding theorem lies an observation about @xmath178-typical sequences analogous to the classical and quantum setting .",
    "define the set of @xmath178-typical outcomes when measuring @xmath450 on the state @xmath451 as @xmath452 when @xmath310 and @xmath178 are clear from context , we will also use the effects @xmath453 since we assumed that any theory contains arbitrary coarse - grainings of measurements , we can consider the measurement @xmath454 which by assumption we can make pseudo - projective . we refer to the subspaces given by @xmath455 and @xmath456 as the typical and atypical subspaces respectively .",
    "if we observe outcome t for the measurement @xmath457 , we conclude that a state lies in the typical subspace associated with the set @xmath458 .",
    "otherwise , we conclude that the states lies in the atypical subspace .",
    "note that by assumption we have that @xmath450 is a fine - grained measurement .",
    "for all states in the typical subspace , only outcomes in the typical set @xmath458 will occur .",
    "hence we have that the dimension of the typical subspace satisfies @xmath459 , then for any @xmath460 and sufficiently large @xmath310 , @xmath461 @xmath462      given the statement about typical sequences , we can now complete the proof of theorem  [ thm : codingtheorem ] : recall that the source emits a sequence of states @xmath314 with probability @xmath465 . to compress the state we perform a pseudo - projective measurement of @xmath457 given by  .",
    "if we obtain outcome ` t ' ( corresponding to the typical subspace ) we output the post - measurement state @xmath466 $ ] , which must lie in the typical subspace as the measurement is repeatable . otherwise , we prepare an arbitrary fixed state in the typical subspace which we will call @xmath467 .",
    "the resulting state is thus a mixed state in the typical subspace of the form @xmath468 + h_a(\\tilde{s}_{\\vec{k } } ) s_{\\rm fail}\\ .\\end{aligned}\\ ] ] note that condition ( ii ) of the theorem tells us that the dimension of the typical subspace is at most @xmath469 . for any @xmath470",
    ", we can therefore find an @xmath178 such that we achieve a compression of rate @xmath318 .",
    "to decompress , we will do nothing and simply output @xmath471 and so all that remains is to show that @xmath316 is in fact close to the original state @xmath314 .",
    "suppose for simplicity that the maximum is attained when computing the distance , and let @xmath17 denote the optimal measurement .",
    "that is @xmath472 we then have @xmath473),{\\textbf{e}}(\\tilde{s}_{\\vec{k } } ) ) + \\nonumber \\\\   & \\qquad h_a(\\tilde{s}_{\\vec{k } } ) { \\mathcal{c}}({\\textbf{e}}(s_{\\rm fail}),{\\textbf{e}}(\\tilde{s}_{\\vec{k } } ) ) \\nonumber \\\\ & \\leq   h_t(\\tilde{s}_{\\vec{k } } ) { \\mathcal{d}}(t[\\tilde{s}_{\\vec{k}}],\\tilde{s}_{\\vec{k } } ) + \\nonumber \\\\   & \\qquad h_a(\\tilde{s}_{\\vec{k } } ) { \\mathcal{d}}(s_{\\rm fail},\\tilde{s}_{\\vec{k } } )   \\nonumber \\\\ & \\leq c h_a(\\tilde{s}_{\\vec{k}})^{{\\varepsilon } } + h_a(\\tilde{s}_{\\vec{k}})\\ , \\nonumber \\\\ & \\leq ( c+1 ) h_a(\\tilde{s}_{\\vec{k}})^{{\\varepsilon } } \\nonumber\\end{aligned}\\ ] ] where the first inequality follows from the properties of the classical trace distance and the linearity of effects , the second from the definition of distance , and the third from the weak disturbance property of a pseudo - projective measurement , where @xmath474 and @xmath300 $ ] are constants given by a particular theory .",
    "we then note that @xmath475 the inequality in the last line follows from the typical subspace theorem .",
    "as @xmath383 can be chosen to be arbitrarily small , this concludes our proof .",
    "we introduced entropic measures to quantify information in any physical theory that admits minimal notions of systems , states and measurements .",
    "even though these measures necessarily have some limitations , we nevertheless showed that they also exhibit many intuitive properties , and for some theories have an appealing operational interpretation , quantifying our ability to compress states .",
    "most of the problems we encountered with the conditional entropy seem to arise due to a violation of strong subadditivity .",
    "it is an interesting question whether quantum and classical theories are the only ones in which @xmath1 is strongly subadditive , or whether this is true for other theories .",
    "indeed , it would be an exciting question to turn things around and start by demanding that our entropic measures _ do _ satisfies these properties , and determine how this restricts the set of possible theories .",
    "in @xmath159 we defined a natural entropic quantity which differs from the conditional von neumann entropy in quantum theory , and has been used in  @xcite to study quantum correlations .",
    "it would be interesting to study whether this quantity can shed any further light on quantum phenomena , or if an alternative conditional entropy can be defined that behaves like @xmath159 in box world , but still reduces to the conditional shannon entropy in quantum theory .    whereas we have proved some intuitive properties of our quantities , it is interesting to see whether other properties of the von neumann or shannon entropy carry over to this setting .",
    "in particular , it would be interesting to prove bounds on the mutual and accessible information analogous to holevo s theorem when none of the systems are classical .",
    "another interesting question is whether one can find a closed form expression for the relative entropy in general theories . in quantum theory",
    ", we can define the mutual information ( and indeed the entropy itself ) in terms of the relative entropy , is the same as the relative entropy between @xmath330 and @xmath331 , and the entropy of @xmath332 is ( minus ) the relative entropy between @xmath332 and the identity operator .",
    "] hence such an approach may also yield an alternative definition of other entropic quantities for general theories .",
    "the non - local game used in our example above was discovered in collaboration with andrew doherty , whom we thank for the kind permission to use it here .",
    "the authors also thank sergio boixo , matthew elliot and jonathan oppenheim for interesting discussions , and matt leifer and ronald de wolf for comments on an earlier draft .",
    "sw is supported by nsf grants phy-04056720 and phy-0803371 .",
    "ajs is supported by a royal society urf , and in part by the eu qap project ( ct-015848 ) .",
    "part of this work was done while ajs was visiting caltech ( pasadena , usa ) .",
    "s.  popescu and d.  rohrlich . causality and nonlocality as axioms for quantum mechanics . in _ proceedings of the symposium of causality and locality in modern physics and astronomy : open questions and possible solutions _ , 1997 .",
    "consider states @xmath334 . clearly , @xmath335 using the property of the classical statistical distance , where equality holds iff @xmath336 by definition of the state space @xmath16 . it remains to show that @xmath337 obeys a triangle inequality .",
    "let @xmath338 be the optimal measurement to distinguish states @xmath339 and @xmath156 .",
    "we then have @xmath340 where the second inequality follows from the fact that the classical statistical distance @xmath341 itself obeys the triangle inequality .",
    "we now show that the entropic quantity   reduces to the von neumann and shannon entropy in the classical and quantum settings respectively . for the relation to the von neumann entropy",
    ", we will need the following little lemma .",
    "our goal will be to show that for any fine - grained measurement @xmath31 with @xmath347 the shannon entropy of the distribution @xmath348 is always at least as large as the distribution obtained by measuring in the eigenbasis of @xmath332 , that is , @xmath349 with @xmath350 .",
    "let @xmath351 and note that @xmath352 .",
    "first of all , note that we can always extend a distribution @xmath353 over @xmath128 elements to a distribution @xmath354 over @xmath170 elements by letting @xmath355 for all @xmath356 and @xmath357 for all @xmath358 .",
    "clearly , @xmath359 with @xmath360",
    ".    second , note that @xmath361 from which we immediately obtain together with   that @xmath362 consider the @xmath363 matrix @xmath39 determined by the entries @xmath364 which allows us to write @xmath365 . note that since @xmath366 and @xmath367",
    ", @xmath39 is a doubly stochastic matrix . using birkhoff s theorem ( see e.g . ,  ) , we may thus write @xmath39 as a convex combination of permutation matrices , that is , @xmath368 where @xmath369 is a probability distribution over the group of permutations @xmath370 . using the concavity of the shannon entropy we obtain @xmath371 as we can always measure @xmath332 in its eigenbasis it follows that @xmath372 and it is easy to see that @xmath373 .",
    "_ boundedness : _ the existence of a measurement @xmath124 with @xmath128 outcomes , combined with the fact that the shannon entropy is maximized for a uniform probability distribution , ensure that @xmath374 which gives boundedness .    _ concavity : _ to see that @xmath1 is concave , suppose first that the infimum in the definition   of @xmath375 is achieved , such that @xmath376 for some @xmath377 .",
    "as effects are linear maps , @xmath378 .",
    "hence , by the concavity of the shannon entropy @xmath379 which concludes our claim . on the other hand ,",
    "if the infimum is not achievable then for all sufficiently small @xmath380 we can find an @xmath377 such that @xmath381 .",
    "using the same argument as before , we find @xmath382 as this holds for all sufficiently small @xmath383 the result follows .",
    "_ limited subadditivity : _ given an additional reasonable assumption , we can prove that @xmath1 is subadditive , we first assume that there exist @xmath384 and @xmath385 such that @xmath386 and @xmath387 . by assumption",
    ", @xmath388 is a fine - grained measurement on the joint system @xmath389 .",
    "thus by the subadditivity of the shannon entropy @xmath390 as claimed .",
    "now suppose that the infimum for one or both of @xmath391 or @xmath392 is not achieved .",
    "then for all sufficiently small @xmath380 we can find @xmath384 and @xmath385 such that @xmath393 as this holds for all sufficiently small @xmath380 the result follows .",
    "note that if @xmath2 and @xmath3 are in a product state , and the theory only allows product measurements on @xmath4 then equality holds in  .",
    "however given we allow an arbitrary set of joint measurements , equality does not hold when @xmath2 and @xmath3 are in a product state for any possible probabilistic theories ( consider the case in which @xmath394 , but there exists a fine - grained measurement on @xmath4 with only 2 outcomes ) .",
    "_ limited continuity : _ here we prove an analogue of the fannes inequality  @xcite , given an additional reasonable assumption that we can restrict to measurements with at most @xmath138 outcomes without changing the entropy of a system .",
    "suppose without loss of generality that @xmath395 .",
    "initially , we also suppose that the infimum in the definition of @xmath396 is achieved for some @xmath397 , such that @xmath398 .",
    "we can then bound @xmath399 where the first inequality follows from the fact that @xmath400 , the second from fannes inequality  @xcite applied to the classical case , and the final inequality by noting that @xmath401 if the infimum is not achieved , then for all sufficiently small @xmath380 there nevertheless exists @xmath397 such that @xmath402 .",
    "following the same procedure as before , we find @xmath403 from which the result follows .",
    "the first inequality follows by choosing the unit measurement in the infimum over @xmath407 in the definition of @xmath249 , and noting that @xmath408 .",
    "the second inequality comes from restricting to measurements of the form @xmath409 in the infimum over @xmath410 in the definition of @xmath411 .",
    "we now prove a very restricted form of chain rule in box world .",
    "this will allow us to show that for our notions of entropy the mutual information about any classical information given an arbitrary state in box world can never increase by more than @xmath412 bits when transmitting @xmath412 bits of information .",
    "to show our simple chain rule , we will use the fact that in box world , we have that when considering a composite of a classical system @xmath39 and an arbitrary system @xmath3 , the only allowed measurements on the composite system @xmath413 take the form of first performing the only allowed measurement on @xmath39 , followed by a choice of measurement on @xmath3 that may depend on the outcome of the measurement on @xmath39 . since classical systems in box world admit exactly one measurement ( possibly followed by some classical post - processing ) , we simply write @xmath414 to denote the resulting entropy .      for simplicity , we only examine the case where the infimum is attained in @xmath404 , the other case can again be obtained by taking the appropriate limit .",
    "since the only measurements on @xmath413 are as described above , we clearly have @xmath417 where the first equality follows from the definition of @xmath404 and the fact that @xmath39 is classical , the second from the definition of the conditional shannon entropy , the third from the chain rule for the conditional shannon entropy , and the final inequality from the definition of @xmath404 , the fact that @xmath418 for classical systems and the fact that conditioning reduces entropy for the shannon entropy .",
    "we now see that in consistency with the no - signalling principle , the transmition of an @xmath412 bit message @xmath39 causes the mutual information about a classical system @xmath341 given access to some aribtrary box information @xmath3 to increase by at most @xmath412 bits .",
    "note that for our alternate definition of conditional entropy and mutual information we have @xmath419 first , note that we can write @xmath420 by definition .",
    "we hence have @xmath421",
    "suppose @xmath423 , where @xmath84 are some set of probabilities and @xmath424 are density operators",
    ". then @xmath425 with equality if and only if the states @xmath424 have support on orthogonal subspaces .",
    "note that when @xmath424 are pure states , @xmath426 .",
    "hence for any pure state decomposition @xmath427 , this implies @xmath428 furthermore , denoting an eigendecomposition of @xmath332 by @xmath429 , it is easy to see that @xmath430 .",
    "hence it follows that @xmath431        first consider a single box with binary input / output . for clarity",
    ", we will represent its state by giving its probability distribution @xmath432 in vector form : @xmath433 now consider the two states @xmath434 these can both be optimally decomposed into two equally weighted pure states , e.g. @xmath435 hence they satisfy @xmath436 .",
    "however now consider the mixed state , @xmath437 which has @xmath438 . hence in this case",
    "we violate concavity @xmath439    to obtain a violation of subadditivity we consider a bipartite state in which each system has a binary input / output , represented in the form of a matrix @xmath440 choose the following allowed state @xmath441 it is known that in this case there are exactly 24 pure states for the bipartite binary input / output case ( 16 product states and 8 entangled states )  @xcite , which we denote by @xmath442 . by demanding that @xmath443 be a positive matrix for each pure state we find that any decomposition must satisfy @xmath444 .",
    "hence @xmath445 .",
    "in fact we can construct an explicit decomposition in terms of an entangled state and three product states ( all equally weighted ) , giving @xmath446 .",
    "the marginal states on the other hand satisfy @xmath447 hence we obtain @xmath448 in violation of subadditivity ."
  ],
  "abstract_text": [
    "<S> information plays an important role in our understanding of the physical world . </S>",
    "<S> we hence propose an entropic measure of information for _ any _ physical theory that admits systems , states and measurements . in the quantum and classical world </S>",
    "<S> , our measure reduces to the von neumann and shannon entropy respectively . </S>",
    "<S> it can even be used in a quantum or classical setting where we are only allowed to perform a limited set of operations . in a world that admits superstrong correlations in the form of non - local boxes , our measure can be used to analyze protocols such as superstrong random access encodings and the violation of ` information causality ' . </S>",
    "<S> however , we also show that in such a world _ no _ entropic measure can exhibit all properties we commonly accept in a quantum setting . </S>",
    "<S> for example , there exists _ no _ ` reasonable ' measure of conditional entropy that is subadditive . </S>",
    "<S> finally , we prove a coding theorem for some theories that is analogous to the quantum and classical setting , providing us with an appealing operational interpretation . </S>"
  ]
}