{
  "article_text": [
    "analysis of the large scale structure of the universe allows us to probe the universe throughout its expansion history and provides the most robust route to measuring the late - time evolution of the universe . over the last decade , large sky - area galaxy surveys such as the sloan digital sky survey ( sdss ; @xcite ) , 2df galaxy redshift survey ( 2dfgrs ; @xcite ) , 6df galaxy redshift survey ( 6dfgrs , @xcite ) and",
    "wigglez survey @xcite have allowed us to probe this large scale structure and have provided us with a wealth of cosmological information . in particular measurements of the baryon acoustic oscillation scale ( bao ; @xcite ) provide us with a standard ruler , allowing us to measure the accelerated expansion of the universe , whilst redshift space distortions ( rsd ; @xcite ) provide a direct probe of the growth of structure and the fidelity of general relativity .",
    "these probes have become more and more accurate in recent years , with @xcite providing a 1% measurement of the distance scale to @xmath3 , the most precise from a galaxy survey to date .",
    "however , these measurements , and their errors , require intimate knowledge of the statistical and systematic distributions from which they are drawn .",
    "this need will only be exacerbated as future surveys , such as the large sky synoptic telescope ( lsst ; @xcite ) and euclid @xcite , strive for even greater precision .    under the assumption of gaussian - distributed errors , the statistical errors inherent in large scale clustering measurements",
    "are encapsulated by the covariance matrix .",
    "although this can be calculated analytically in the linear regime @xcite , the non - linear galaxy covariance matrix is a complex function of non - linear shot - noise , galaxy evolution and the unknown relationship between the galaxies and the underlying dark matter . in any real application",
    "this is further compounded by the effect of rsd .",
    "as such , a much more common solution is to use a set of detailed galaxy simulations , otherwise known as mock catalogues ( mocks ) , to either fully estimate the covariance matrix or as the basis for an empirically motivated analytic fit @xcite .",
    "ideally these simulations would take the form of fully realised n - body simulations , with accurate small scale clustering , covering the whole volume of the galaxy survey .",
    "however , for current surveys , recent studies @xcite show that we require 1000 mocks to obtain an accurate numerical estimate of the covariance matrix with sub - dominant errors compared to the statistical errors themselves .",
    "higher precision measurements in the future may require many more .",
    "instead there have been many studies looking at fast methods of producing simulations that enable us to produce mocks hundreds of times faster than an tree - pm n - body simulation , at the cost of reduced small scale clustering accuracy .",
    "past measurements of the large scale structure have used lognormal models to generate realizations of the galaxy overdensity field and estimate the covariance matrix @xcite . however",
    "this approach does not accurately capture the non - gaussian behaviour of gravitational collapse .",
    "recently , @xcite and @xcite have used an implementation of the more accurate pthalos method @xcite to generate mock catalogues for the data releases 9 and 10 of the baryon oscillation spectroscopic survey ( boss ; @xcite + @xcite ) .",
    "in addition to this there is a wealth of alternative methods such as pinocchio @xcite , quick particle mesh simulations ( qpm ; @xcite ) , augmented lagrangian perturbation theory ( alpt ; @xcite ) , effective zeldovich approximation mocks ( ezmocks ; @xcite ) and the comoving lagrangian acceleration method ( cola ; @xcite , @xcite ) which can all be used to produce mock catalogues comparable in accuracy to , if not better than , 2lpt and with similar speed . in this paper",
    "we present a stand - alone parallel implementation of the latter of these , with emphasis on maximising speed , memory conservation and ease of use .",
    "this code , which we dub l - picola , combines a range of features that will be of increasing interest for the next generation of galaxy surveys , including the ability to produce lightcone simulations , replicate the simulation at runtime and include primordial non - gaussianity based on a generic input bispectrum , as per @xcite . on top of this the cola method itself",
    "is able to reproduce the dark matter field with much greater accuracy on small , non - linear scales than the pthalos method , at only a moderate increase in computational cost .    as such",
    ", we expect our implementation to be suitable for both current and future surveys , being able to both capture non - linear evolution with a precision necessary to reach the required covariance matrix accuracy for these surveys and scalable up to very large numbers of particles and volumes .",
    "in fact , we have already used this code to measure the bao and rsd signals from a subset of luminous red galaxies drawn from the sloan digital sky survey data release 7 main galaxy sample @xcite .",
    "additionally , manera et al . , ( in prep . ) describe an application of l - picola to the dark energy survey ( des ; @xcite ) , making use of the fast lightcone algorithm that we will discuss in this paper .",
    "it should be noted that in previous studies using this code , we named the code picola .",
    "however , very recently , @xcite present an extension to the cola method that allows one to decouple the short and long range gravitational forces _ spatially _ in addition to temporally .",
    "this allows the user to calculate the non - linear displacements for only a small subsection of the full simulation volume and still recover reasonable accuracy across the whole simulation . in this work",
    "they also present a code pycola , a shared - memory python implementation of the extended cola method . to avoid confusion in the names of these codes , and highlight the additional features we have implemented since the first application of our code",
    ", we have renamed it l - picola .",
    "this paper is structured as follows : in section 2 we provide a brief description of the theory behind the 2lpt and cola methods .",
    "section 3 introduces l - picola , with section 4 detailing the steps we have taken to parallelise the cola method for a distributed - memory machine .",
    "sections 5 and 6 detail the additional features we have included in l - picola , beyond the standard snapshot simulations . in particular , in section 6",
    "we validate the need for lightcone simulations and perform a rigorous test of our implementation . in sections 7",
    "we compare the accuracy of l - picola to 2lpt and a full n - body simulation . in this section",
    "we also test the effect on the clustering accuracy of several of the free parameters that can be used to speed up the convergence of the cola method . in section 8",
    "we compare the speed of l - picola with the 2lpt and n - body simulations , and look at the scaling of different segments of l - picola itself . finally in section 9",
    "we conclude and discuss further improvements that can be made to the code . included in the appendix",
    "are details of the memory footprint of l - picola .",
    "unless otherwise stated , we assume a fiducial cosmology given by @xmath4 , @xmath5 , @xmath6 , @xmath7 , and @xmath8 . also , unless otherwise stated , all simulations presented use a number of mesh cells equal to the number of particles , the cola method with modified cola timestepping , @xmath9 and 10 linearly spaced timesteps .",
    "these l - picola - specific parameters are stated here for completeness but are explained within this paper .",
    "in the following section we describe the cola method for evolving a system of dark matter particles under gravity , as first introduced by @xcite .",
    "we begin with a summary of the theoretical underpinnings of the algorithm , including a brief overview of second order lagrangian perturbation theory ( 2lpt ) , before moving onto the algorithmic implementation .",
    "as described in @xcite ( see also @xcite and @xcite ) , cold dark matter particles evolving over cosmological time in an expanding universe follow the equation of motion ( eom ) @xmath10 where @xmath11 is the gravitational potential , @xmath12 is the conformal hubble parameter and @xmath13 is the scale factor .",
    "@xmath14 is the displacement vector of the particle and relates the particle s eulerian position @xmath15 to its initial , lagrangian position , @xmath16 , via @xmath17 by taking the divergence of the equation of motion and using the poisson equation , we find @xmath18 here @xmath19 is the matter density at @xmath20 , whilst @xmath21 is the local overdensity .",
    "lagrangian perturbation theory seeks to solve this equation by perturbatively expanding the displacement vector , @xmath22 if we then apply the continuity equation , @xmath23 , which states that a mass element @xmath24 centred at @xmath16 at time zero becomes a mass element @xmath25 , centred at @xmath26 , at time @xmath27 , we find that , to first order @xmath28 this is the well known zeldovich appoximation ( za ; @xcite ) . here",
    "@xmath29 is the linear growth factor , @xmath30 is the linear overdensity field and we have rewritten the divergence as a function of @xmath16 by using the fact that they are related via the jacobian of the transformation from @xmath26 to @xmath16 , i.e. , @xmath31 . solving to second order introduces corrections to the first order displacement of the form @xmath32 where , for brevity , we have defined @xmath33 .",
    "@xcite provide a good approximation for @xmath34 , the second order growth factor , for a flat universe with non - zero cosmological constant @xmath35 for further computational ease , we can define two langragian potentials , @xmath36 , such that eq .",
    "( [ lagrangian ] ) becomes @xmath37 and the lagrangian potentials are obtained by solving the corresponding pair of poisson equations derived from eq .",
    "( [ za ] ) and eq .",
    "( [ 2lpt ] ) respectively , @xmath38 @xmath39      the cola method @xcite provides a much more accurate solution to eq.([eom ] ) than 2lpt , at only a moderate ( @xmath40 ) reduction in speed .",
    "it does this by utilising the first and second - order lagrangian displacements , which provide an exact solution at large , quasi - linear scales , and solving for the resultant , non - linear component .",
    "by switching to a frame of reference comoving with the particles in lagrangian space , we can split the dark matter equation of state as follows , @xmath41 + t[{d}_{1}]\\boldsymbol{\\psi}_{1 } + t[{d}_{2}]\\boldsymbol{\\psi}_{2 } + \\nabla \\phi = 0 , \\label{eq : cola}\\ ] ] where , @xmath42 = \\frac{d^{2}x}{d\\tau^{2 } } + \\mathcal{h}\\frac{dx}{d\\tau}.\\ ] ] @xmath43 is the remaining displacement when we subtract the quasi - linear 2lpt displacements from the full , non - linear displacement each particle should actually feel .    the reason this method is so useful is because we only need to calculate the lagrangian displacements once , at redshift @xmath0 , and scale them by the appropriate derivatives of the growth factor .",
    "in fact , as we will see in later sections , it is common practice in many n - body simulations to use 2lpt to generate the initial positions of the particles at a suitably high redshift , where the results are exact .    in l - picola ,",
    "( [ eq : cola ] ) is solved as a whole ( as opposed to evaluating @xmath44 individually ) by discretising the operator ` t ' using the kick - drift - kick algorithm @xcite , such that at each iteration the velocity and position of each particle is updated based on the gravitational potential @xmath11 _ and _ the stored 2lpt displacements .",
    "the well - known particle - mesh algorithm , with forward ( fft ) and inverse ( ifft ) fourier transforms , is used to evaluate the gradient of @xmath11 using the particle density . i.e , @xmath45\\right]\\ ] ] the following sections detail the kick - drift - kick method and the particle - mesh algorithm used in l - picola and how these are easily modified to solve eq .",
    "( [ eq : cola ] ) as opposed to the standard dark matter equation of motion .",
    ".  [ eq : cola ] is discretised using the kick - drift - kick / leapfrog + method @xcite .",
    "the modified , cola dark matter eom is solved iteratively , and at each iteration the particle velocities and positions are updated based on the gravitational potential felt by each particle .",
    "particle velocities are calculated from the displacements and updated to the nearest half - integer timestep .",
    "the particle positions are then updated to the nearest integer timestep using the previous velocity . in this way",
    "the particle velocities and positions are never ( except at the beginning and end ) calculated for the same point in time but rather ` leapfrog ' over each other with the next iteration of the velocity dependent on the position from the previous iteration and so on .    in the standard , non - cola method",
    ", the dark matter eom can be solved via .",
    "@xmath47 encapsulates the time interval and appropriate numerical factors required to convert the displacement to a velocity and the velocity to a position .",
    "@xcite evaluate these as @xmath48    the equations for updating the particle positions and velocities can be modified to solve the cola eom in the following way @xmath49\\delta a_{1 } , \\label{eq : vcola } \\\\ & \\boldsymbol{r}_{i+1 } = \\boldsymbol{r}_{i } + \\boldsymbol{v}_{i+1/2}\\delta a_{2 } + \\delta d_{1}\\boldsymbol{\\psi}_{1 } + \\delta d_{2}\\boldsymbol{\\psi}_{2 } \\label{eq : rcola}\\end{aligned}\\ ] ] here @xmath50 denotes the change in the first and second order growth factors over the timestep .",
    "the modified kick - drift - kick equations are derived under the condition that , for eq .",
    "( [ eq : cola ] ) to be valid , the displacements felt by each particle must be computed in the 2lpt reference frame .",
    "in other words , the acceleration each particle feels due to the the gravitational potential must be modified , and the 2lpt contribution to the acceleration removed .",
    "the new gravitational potential is then , by design , @xmath51 $ ] .",
    "the exact procedure used to calculate @xmath52 is not important and as such any code that updates the particle velocities and positions iteratively based on the gravitational potential , i.e. , a tree - pm code , can be modified in the above way to include the cola mechanism .    an important point of note in enforcing the change of reference frame is that particle velocities at the beginning of the simulation , after creation of the 2lpt initial conditions but before iterating , must be identically 0 . at this point",
    "the velocity a particle has is exactly equal to the velocity of the reference frame we are moving too .",
    "however when the particles are output at the end of the simulation we want the particle velocities in _ eulerian _ coordinates .",
    "this means that the initial particle velocities must be removed and stored at the beginning of the timestepping and then added back on at the end of the simulation .    when implementing the modified cola timestepping , the time intervals , @xmath47 , for each timestep do not get explicitly changed and as such can remain the same as the those presented in @xcite .",
    "however , @xcite present a second , cola specific , formulation which gives faster convergence , hence allowing us to recover our evolved dark matter field to greater accuracy in fewer time steps . in their method , @xmath53 where they find the best results using a value @xmath54 .",
    "as the choice of @xmath47 is somewhat arbitrary l - picola retains both methods as options .",
    "this choice ( and @xmath55 ) should be treated formally as an extra degree of freedom in the code .",
    "in fact we find that the value of @xmath55 used in the code can affect the final shape of the power spectrum recovered from cola due to the way different growing modes are emphasised by different values .",
    "this is pointed out in @xcite and means that for a given set of simulation parameters one would ideally experiment to find the type of timestepping that recovers the required clustering in the fewest timesteps possible .",
    "this is demonstrated further in section [ sec : accuracy ] .    for the timestepping method presented here and adopted in l - picola ,",
    "the only remaining piece of the puzzle is the calculation of @xmath56 = -t[{d}_{1}]\\boldsymbol{\\psi}_{1 } - t[{d}_{2}]\\boldsymbol{\\psi}_{2 } - \\nabla \\phi$ ] . as the za and 2lpt displacements",
    "have been stored we only need a method of evaluating @xmath57 . in l - picola",
    "this is done using the particle - mesh algorithm , though could be done using a method such as the tree - pm algorithm .",
    "the evaluation of @xmath58 $ ] and @xmath59 $ ] can be performed numerically for a given cosmological model very easily , although a suitable approximation for @xmath60 must be adopted .",
    "for flat cosmologies one could use eq ( [ eq:2lptgrowth ] ) , however in l - picola we adopt the expression of @xcite which is also accurate for non - flat cosmologies .      here",
    "we provide a brief overview of the particle - mesh ( pm ) algorithm ( see @xcite for a good review of this method ) .",
    "our implementation is based on the publicly available pmcode @xcite , and as such we refer the reader to the associated documentation for full details on the set of equations we solve to get the displacement .",
    "in the pm method we place a mesh over our dark matter particles and solve for the gravitational forces at each mesh point .",
    "we then interpolate to find the force at the position of each particle and use this to calculate the gravitational potential each particle feels .",
    "this gravitational potential is then related to the additional velocity , and resultant displacement , for each particle as per the this is performed iteratively over a series of small timesteps . for @xmath61 mesh points and @xmath62 particles , this means that at each iteration we only need to perform @xmath61 force calculations , which is much faster than a direct summation of the contribution to the gravitational force from each individual particle ( at least for all practical applications , where @xmath63 ) .    at each iteration",
    "we perform the following steps to calculate the displacement :    1 .",
    "use the cloud - in - cell linear interpolation method to assign the particles to the mesh , thereby calculating the mass density , @xmath64 , at each mesh point .",
    "2 .   use a fast fourier transform ( fft ) to fourier transform the density and solve the comoving poisson equation in fourier space .",
    "@xmath65 3 .",
    "use the gravitational potential and an inverse - fft to generate the force in each direction in real - space . here",
    "we also deconvolve the cloud - in - cell window function .",
    "@xmath66 4 .",
    "calculate the acceleration each particle receives in each direction , again using the cloud - in - cell interpolation method to interpolate from the mesh points .",
    "as suggested by the name , l - picola is a parallel implementation of the cola method described in the previous section .",
    "we have designed the code to be ` stand alone ' , in the sense that we can generate a dark matter realisation based solely on a small number of user defined parameters .",
    "this includes preparing the initial linear dark matter power spectrum , generating an initial particle distribution with k - space distribution that matches this power spectrum , and evolving the dark matter field over a series on user specified timesteps until some final redshift is reached . at any point in the simulation the particle position and velocities",
    "can be output , allowing us to capture the dark matter field across a variety of epochs in a single simulation .    in order to make l - picola as useful as possible we have also implemented several options that modify how l - picola",
    "is actually built at compile time . on top of allowing variations in output format and memory / speed balancing",
    "we also allow the user to create ( and then evolve ) initial particle distributions containing primordial non - gaussianity .",
    "another significant improvement , and one that will be extremely important for future large scale structure surveys , is the option to generate a lightcone simulation , which contains variable clustering as a function of distance from the observer , as opposed to a snapshot simulation at one fixed redshift .",
    "although lightcone simulations can be reconstructed from a series of snapshots @xcite , l - picola can produce lightcone simulations ` on - the - fly ' in a short enough time to be suitable for generating significant numbers of mock galaxy catalogues .",
    "these additions will be detailed and tested in later sections .",
    "[ picolachart ] shows a simple step - by - step overview of how l - picola works .",
    "the different coloured boxes highlight areas where the structure of the code actually changes depending on how it is compiled .",
    "the blue box shows where the different types of non - gaussianity can be included .",
    "the red boxes show where significant algorithmic changes occur in the code if lightcone simulations are requested .",
    "these will we detailed in the following sections , along with an explanation of how we parallelise the cola method .",
    "l - picola is publicly available under the gnu general public license at https://cullanhowlett.github.io/l-picola .",
    "in this section we will detail the steps we have taken to parallelise the cola method .",
    "all parallelisation in the code uses the message passing interface ( mpi ) library .",
    "see @xcite for a comprehensive guide to the usage and syntax of mpi . in the following subsections we provide an overview to the parallelisation and",
    "detail the three main parallel algorithms in the code : parallel cloud - in - cell interpolation , parallel fft s and moving particles between processors .",
    "parallelisation of l - picola has been performed with the goal that each processor can run a small section of the simulation whilst needing minimal knowledge of the state of the simulation as a whole .",
    "we have separated both the mesh and particles across processors in one direction . in this way",
    "each processor gets a planar portion of the mesh , and the particles associated with that portion .",
    "we have tried to balance the load on each processor as much as possible whilst adhering to the fact that each processor must have an integer number of mesh cells in the direction over which we have split the full mesh .",
    "this process is enabled by use of the publicly available fftw - mpi libraries , which also serve to perform the fast fourier transforms when the mesh is split over different processors . in a simulation utilising @xmath67 processors and consisting of a cubic mesh of size @xmath68 , each processor gets @xmath69 slices of the mesh where each slice consists of @xmath70 cells .",
    "the extra @xmath71 cells in each slice are required as buffer memory for the fftw routines .",
    "depending on the ratio of @xmath72 to @xmath67 this may give too many slices in total , so then we work backwards , removing slices until the total number of slices is equal to @xmath72 .",
    "the number of particles each processor has is related to the number of mesh cells on that processor as each processor only requires knowledge of any particles that interact with its portion of the mesh .",
    "hence , as the particles are originally spaced equally across the mesh cells , each processor initially holds @xmath73 particles , multiplied by the number of slices it has .",
    "as each processor only contains particles which belong to the mesh cells it has , and our interpolation assigns particles to the mesh by rounding down to the nearest mesh cell , the density assignment step proceeds as per the standard cloud - in - cell interpolation method , except near the ` left - hand ' edge of the processor . here",
    "the density depends on particles on the preceding processor .",
    "figure [ cloudincell ] shows a 2-d graphical representation of this problem .     which are then transferred and added to the left most slice on processor @xmath74.,width=317 ]    in order to compensate for this we assign an extra mesh slice to the ` right - hand ' edge of each processor .",
    "this slice represents the leading slice on the neighbouring processor and by assigning the particles to these where appropriate and then transferring and adding the ` slices ' to the appropriate processors , each portion of the mesh now contains an estimate of the density which matches the estimate as if all the mesh were contained on a single processor .    it should also be noted that a reverse of this process must also be done after calculating the forces at each mesh point , as the displacement of a particle near the edge of a processor is reliant on the force at the edge of the neighbouring processor .      to take the fourier transform of our mesh once it is split over many processors we use the parallel fftw - mpi routines available alongside the aforementioned fftw libraries .",
    "this is intimately linked to the way in which the particles and mesh are actually split over processors and routines are provided in this distribution that enable us to perform this split in the first place .",
    "the fftw routines use a series of collective mpi communications to transpose the mesh and perform a multi - dimensional real - to - complex discrete fourier transformation of the density , assigning the correct part of the transformed mesh to each processor . in terms of implementing this , all that is required is for us to partition the particles and mesh in a way that is compatible with the fftw routines , create a fftw plan for the arrays we wish to transform and perform the fourier transformation once we have calculated the required quantity at each mesh point .",
    "the fftw libraries perform all mpi communications and operations internally .",
    "one final modification to the particle - mesh algorithm is to compensate for the fact that , over the course of the simulation , particles may move outside the physical area contained on each processor .",
    "their position may now correspond to a portion of the mesh that the processor in question does not have .",
    "as such , after each timestep we check to see which particles have moved outside the processor boundaries and move them to the correct processor .",
    "this is made particularly important as the cola method converges in very few timesteps , meaning the particles can move large distances in the space of a single timestep .    in the case where we have a high particle density or small physical volume assigned to each processor",
    ", a single particle can jump across several neighbouring processors in a single timestep .",
    "so , when moving the particles , we iterate over the maximum number processors any single particle has jumped across .",
    "however the number of particles that need to be moved is unknown _ a priori _ and so to be conservative and make sure that we do not overload the buffer memory set aside for the transfer , not all the particles that are moving are transferred simultaneously ( i.e. via a collective mpi - alltoall command ) .",
    "rather , all the particles that have moved from processor",
    "n to n@xmath751 are moved first then all the particles that have moved from processor n to n@xmath752 are transferred .",
    "although this requires iterating over the particles on processor n multiple times , in the majority of cases there are no particles moving to any processors beyond n@xmath751 and so only one iteration is required .",
    "as the simulation progresses the particles will not remain homogeneously spread over the processes , so we assign additional buffer memory to each processor to hold any extra particles it acquires .",
    "this is utilised during the moving of the particles and all particles a processor receives are stored in this buffer .",
    "however , in order to make sure this buffer is not filled too quickly we also use the fact that each processor is likely to lose some particles . when a particle is identified as having left a particular processor the particle is moved into temporary memory and the gap is filled with a particle from the end of the processors main particle memory . in this way we collect all remaining particles together before moving the new particles across , ensuring a contiguous , compact particle structure .",
    "this is shown in figure [ moveparticles ] .",
    "in order to allow l - picola to run a simulation from scratch we have integrated an initial conditions generator into the code .",
    "this means that we can simply store the first and second order lagrangian displacements for each particle as they are calculated rather than assume some initial positions for the particles and reconstruct them .",
    "we use the latest version of the parallelised 2lptic code @xcite to generate the initial conditions , with some modifications to allow a more seamless combination of the two codes , especially in terms of parallelisation . for compatibility with l - picola",
    "we have removed the warm dark matter and non - flat cosmology options from the 2lpt initial conditions generator , though these are improvements that could easily be added in the future .",
    "the particles are initially placed uniformly , in a grid pattern throughout the simulation volume , so rather than creating the particles at this stage , we also conserve memory by only generating the 2lpt displacements at these points and creating the particles themselves just before timestepping begins .    because of this addition",
    ", l - picola can be used very effectively to create the initial conditions for other n - body simulations , as well as evolving the dark matter field itself .",
    "in fact in a single run we can output both the initial conditions and the evolved field at any number of redshifts between the redshift of the initial conditions and the final redshift , which allows easy comparison between picola and other n - body codes .",
    "a final point is that because the 2lpt section is based on the latest version of the 2lptic code , we are also able to generate , and then evolve , initial conditions with local , equilateral , orthogonal or generic primordial non - gaussianity .",
    "local , equilateral and orthogonal non - gaussianity can be added simply by specifying the appropriate option before compilation and providing a value for @xmath76 .",
    "we can also create primordial non - gaussianity for any generic bispectrum configuration using a user - defined input kernel , following the formalism in the appendix of @xcite .",
    "the final large modification we have made to the code , and one which will be very useful for future large scale structure surveys , is the ability to generate lightcone simulations in a single run , as opposed to running a large number of snapshots and piecing them together afterwards .",
    "snapshot simulations , generated at some effective redshift , have been widely used in the past to calculate the covariance matrix and perform systematic tests on data @xcite . however , as future surveys begin to cover larger and larger cosmological volumes with high completeness across all redshift ranges it is no longer good enough to produce a suite of simulations at one redshift .",
    "lightcone simulations mimic the observed clustering as a function of redshift and so introduce a redshift dependence into the covariance matrix . on top of this ,",
    "once a full redshift range has been simulated we can apply identical cuts to the mock galaxy catalogues and the data .",
    "as such , in the case when we make measurements at multiple effective redshifts with a single sample , we may need less simulations in total , especially if multiple runs would be required to produce the snapshots at multiple redshifts .",
    "figure [ lightcone_pk ] demonstrates the effect of simulating a lightcone using the power spectrum .",
    "we populate a @xmath77 box with @xmath78 particles , place the observer at ( 0,0,0 ) , and using a flat , @xmath79 cosmology ( all other parameters match our fiducial cosmology ) , simulate an eighth of the full - sky out to a maximum redshift of 0.75 .",
    "the power spectrum is then calculated using the method of @xcite for three redshift slices between 0.0 , 0.25 , 0.5 and 0.75 , using a random , unclustered catalogue to capture the window function .",
    "as expected we see a significant evolution of the clustering as a function of redshift that would not be captured in a single snapshot simulation .",
    "the overall clustering amplitude increases as we go to lower redshifts with additional non - linear evolution on small scales at later times .    to further compare the clustering of this lightcone simulation with the expected clustering , we overlay the power spectrum from a snapshot simulation at the effective redshift of each lightcone slice .",
    "we define the effective redshift of each slice , bounded by the redshifts @xmath80 and @xmath81 , using the formulation of @xcite where @xmath82 for our simulations the number density , @xmath83 , is constant and the weighting factors , @xmath84 and @xmath85 @xcite cancel .",
    "this in turn reduces the effective redshift to @xmath86 where @xmath87 is the comoving distance , @xmath88 is the speed of light and @xmath89 is the hubble parameter .",
    "we can see good agreement on all scales between the snapshot and lightcone power spectra for each of the redshift slices .",
    "the window function causes noise on the largest scales , especially for the lowest volume slice , however the redshift - dependent amplitude is captured very well within a single lightcone simulation .        in the following subsections",
    "we will provide a detailed description of how lightcone simulations are produced in l - picola , test the accuracy of our implementation and also looking at how we can replicate the simulations volume to fill the full lightcone during run - time .      in order to simulate the past lightcone",
    ", we require the properties of each particle in the simulation at the moment when it leaves a lightcone shrinking towards the observer .",
    "as has been done in several studies @xcite , we can interpolate these particle properties using a set of snapshot simulations , however this requires significant post - processing and more storage space than generating a lightcone simulation at run - time . as such , in order to provide a useful tool for future cosmology surveys , we have implemented the latter into l - picola .",
    "this is done as follows : the user specifies an initial redshift , at which we begin the simulation , and an origin , the point at which the observer sits .",
    "each of the output redshifts is then used to set up the timesteps we will use in the simulation , with the first output denoting the point at which we start the lightcone and the final output corresponding to the final redshift of the simulation .",
    "any additional redshifts in between these two can be used to set up variable timestep sizes . if we imagine the lightcone as shrinking towards the origin as the simulation progresses , then for every timestep between these two redshifts we output only those particles that have left the lightcone .",
    "this is shown pictorally in figure [ lightcone ] .",
    "slice of a l - picola dark matter field simulated on the past lightcone with an observer situated at the ( 0,0,0 ) .",
    "as the lightcone shrinks with each timestep ( the lightcone radius is denoted by the black lines ) we only output the particles that have left the lightcone that timestep , with their position interpolated to the exact point at which they left .",
    "this means that the particles shown in the diagram were output in stages with the particles between lines t0 and t1 output first . between output stages",
    "the particles evolve as normal , resulting in clustering that is dependent on the distance from the observer.,width=317 ]    mathematically , it is simple to identify whether the particle should be output between timesteps @xmath90 and @xmath74 by looking for particles which satisfy both @xmath91 and @xmath92 where @xmath93 is the comoving distance between the particle and the lightcone origin at scale factor @xmath94 and @xmath95 is the comoving radius of the lightcone at this time .",
    "however , we really wish to output a given particle at the exact moment it satisfies the equation , @xmath96 from the cola method we have @xmath97 where @xmath98 is the position of the lightcone origin and the ` @xmath99 ' terms are dependent on the value of @xmath100 .",
    "the comoving lightcone radius at @xmath100 is simply the comoving distance @xmath101 equating these should allow us to solve for @xmath100 .",
    "once this is known we can calculate the properties of each particle we wish to output .",
    "however , this equation can not be solved analytically and so requires us to numerically solve it for each individual particle that we wish to output .",
    "this would be prohibitively time - consuming and instead we approximate the solution by linearly interpolating both the lightcone radius and the particle position between the times @xmath102 and @xmath103 . substituting the linear interpolation into eq.([eq : lightcone ] ) and rearranging we find @xmath104 this is trivial to calculate as we already need to know @xmath93 and @xmath105 in order to update the particle during timestepping anyway , and @xmath95 and @xmath106",
    "are needed to identify which particles have left the lightcone in the first place .",
    "in fact the whole procedure can be performed with minimal extra runtime , as we simply modify the ` drift ' part of the code .",
    "the only extra computations are to check the particle s new position against the lightcone and interpolate if necessary .",
    "once we know the exact time the particle left the lightcone we can update the particle s position , using eq.(16 ) , to the position it had when it left the lightcone and output the particle .    in l - picola lightcone simulations we do not interpolate the velocity , using instead the velocity at time @xmath107 .",
    "we make this choice as it mimics the inherent assumption of the kick - drift - kick method , that the velocity is constant between @xmath102 and @xmath103 . to properly interpolate the velocity in the same way as the particle position would require us to evaluate the velocity at times @xmath102 and @xmath103 which in turn would require us to measure the particle density at half timestep intervals .",
    "one could also imagine assuming that the non - linear velocity is constant and interpolating the za and 2lpt velocities ( which must be added back on before outputting to move back to the correct reference frame ) .",
    "however , we find that the assumption of constant velocity between @xmath102 and @xmath103 is a reasonable one .    to test the numerical interpolation against the analytic expectations , and provide a graphical representation of the particle positions and velocities output during lightcone simulations , we compare particles output during the final timestep of a lightcone simulation to the same particles output from snapshot simulations evaluated at the beginning and end of that timestep ( the corresponding redshifts are @xmath108 and @xmath109 in this case ) .",
    "the particles are matched based on a unique identification number which is assigned when the particle are created and as such is consistent between the three simulations .    for both the particle positions and velocities we look at the difference between the lightcone properties and the properties of the @xmath108 snapshot , normalised by the same difference between the @xmath109 and @xmath108 snapshots .",
    "we plot this in figure [ picola_interp_vel ] as a function of the distance from the observer ( also normalised , using the comoving distance to @xmath109 ) . if we were to interpolate the particle positions after runtime using the two snapshots we would expect the particles to lie exactly on the diagonal in figure [ picola_interp_vel ] .",
    "we find that the particle positions interpolated _ during _ the simulation also lie close to the diagonal , which validates the accuracy of our numerical interpolation .",
    "the small scatter in both of these plots is due to floating - point errors and the normalisation in the particle positions .",
    "particles that do not move much between the two snapshots will have a normalisation close to zero , which in turn makes our choice of plotting statistic non - optimal .",
    "the particle velocities show no trend as a function of distance to the observer or when they were output . in this case",
    "the velocities in each direction are all situated close to the mid point between the two simulations .",
    "this validates the kick - drift - kick assumption , that the velocities evolve approximately linearly between two timesteps , such that the velocity at time @xmath107 is half way between that at time @xmath102 and @xmath103 , although there is some scatter and offset due to the true non - linear nature of the velocity .      on top of comparing the numerical interpolation during runtime to the analytic interpolation between two snapshots",
    ", we also check the assumption that we can use linear interpolation between two timesteps _ at all_. as mentioned previously , the exact time the particle leaves the lightcone , which we ll call @xmath110 , is given by numerically solving eq.([eq : lightcone ] ) , but solving this for each particle is extremely time consuming and so we linearly interpolate instead .",
    "to test this we find the exact solution for a subset of the particles in the @xmath111 simulation and compare this to the approximate solution , @xmath112 .",
    "this is shown in figure [ picola_interp_a ] .",
    "we find that the linear interpolation slightly overestimates the value of @xmath100 , with a common trend across all timesteps , however this effect is less than 0.5% across all times for this simulation .",
    "the two solutions agree almost perfectly close to the timestep boundaries , denoted by the dashed vertical lines .",
    "this is because the particle positions and lightcone radius are known exactly at these points .",
    "further away from the timestep boundaries inaccuracies are introduced as the assumption that the particle position and lightcone radius are linear functions of the scale factor is less accurate .",
    "and the value recovered using eq.([eq : lightconeinterp ] ) , as a function of the true scale factor .",
    "the dashed lines show the scale factor at which we evaluate the timesteps of the simulations , and hence know the exact positions of the particles and the lightcone radius.,width=317 ]     and those recovered using eq.([eq : lightconeinterp ] ) , as a function of the distance from the observer , normalised by the maximum lightcone radius of the simulation .",
    "we plot the magnitude of the difference vector between the two methods .",
    "we see good agreement , with a maximum difference of @xmath113 , across all scales.,width=317 ]    we can further quantify the reliability of the linear interpolation by looking at the positions of the particles output in both these simulations .",
    "this is shown in figure [ picola_interp_pos2 ] , where we plot the difference in particle position ( we take the magnitude of the difference vector ) as a function of the distance between the particle and the observer , normalised to the maximum lightcone radius for the simulation .",
    "we can see that the linear interpolation is indeed very accurate , and even at large radii , where the comoving distance between timesteps is largest , the particle positions are equivalent to within @xmath114 .",
    "this is well below the mesh scale of this simulation , and is subdominant compared to the errors caused by the finite mesh size and the large timesteps .      on top of the lightcone interpolation",
    "we have accounted for the fact that lightcones built from snapshot simulations often replicate the simulation output to reach the desired redshift .",
    "l - picola has the ability to replicate the box as many times as required in each direction during runtime .",
    "this is done by simply modifying the position of each particle as if it was in a simulation box centred at some other location . in this way we can build up a large cosmological volume whilst still retaining a reasonable mass and force resolution .",
    "however it is important to note that this can have undesired effects on the power spectrum and covariance matrix calculated from the _ full _ replicated simulations volume , which will be detailed subsequently .",
    "figure [ lightcone_replicate ] highlights the replication process .",
    "here we run a similar lightcone to that used in figure [ lightcone ] , however the actual simulation contains 64 times less particles in a volume 64 times smaller and is replicated 64 times . in this way",
    "we can cover the full volume and mass range required but the cpu and memory requirements are much smaller . to help identify the replication we have used the friends - of - friends algorithm @xcite to group the particles into halos and plotted the centre - of - mass position of each halo .",
    "this results in obvious points where the same halo is reproduced after more particles have accreted onto that halo , and the halo has evolved in time .",
    "but using 64 times less particles and in a volume 64 times smaller .",
    "we then replicate the box 64 times at runtime as shown by the dashed lines ( we only show 1 replicate in the z direction ) .",
    "to aid visualisation",
    "we also over plot the halos recovered from this simulation using a friends - of - friends algorithm.,width=317 ]      the downside of the replication procedure is that in repeating the same structures we are not be sampling as many independent modes as would be expected from an unreplicated simulation of the same volume .",
    "rather we are just sampling the same modes multiple times .",
    "this affects both the power spectrum and the covariance matrix . to test the effects of replication we use a set of 500 lightcone simulations , containing @xmath78 particles in a box of edge length @xmath115",
    "we then compare this to another set of 500 simulations with @xmath116 particle in a @xmath117 box , which is then replicated 8 times .",
    "we calculate the power spectra for both using the method of @xcite , in bins of @xmath118 , estimating the expected overdensity from the total number of simulation particles and the box volume .",
    "this works for the lightcone simulations as the maximum lightcone radius is larger than the diagonal length of the cubic box , such that the simulation still fills volume .",
    "particle lightcone simulations in a @xmath115 box and @xmath116 particle lightcone simulations in a @xmath119 box .",
    "the lines correspond to the average power spectra from 500 independent realisations and the errors are those on a single realisation calculated from the diagonal of the covariance matrix constructed from the 500 realisations .",
    "the blue line represents the average power spectrum when we replicate the @xmath116 particle simulation 8 times so that it has the same volume and number of particles as the larger simulation , and as expected is virtually indistinguishable from the large , unreplicated simulation .",
    "the amplitude of power spectra match the order of the legend.,width=317 ]    the average power spectra are shown in figure [ replicate_pk ] , where the errors come from the diagonal elements of the covariance matrix and are those for a single realisation .",
    "as the simulations are periodic in nature we expect the power spectra for the two box sizes to be almost identical except for the fact that the larger simulation volume has a greater effective redshift and hence a power spectrum with lower amplitude and less non - linear evolution .",
    "we see that this holds true for our lightcone simulations , and that the difference in the replicated and unreplicated @xmath78 simulations is , at least on linear scales , equal to the difference in the linear growth factor between the effective redshifts of the two sets of simulations .",
    "however , in order to produce the replicated power spectrum , it is necessary to correct for the replication procedure .",
    "when we replicate a simulation , we are changing the fundamental mode of the simulation but without adding any additional information , either in the number of independent modes we sample , or on scales beyond the box size of the unreplicated simulation .",
    "this in turn creates ringing on the order of the unreplicated box size .",
    "this can demonstrated using a simple toy model .    in figure [ replicate_pk_toy ]",
    "we show a small @xmath120 overdensity field before and after taking the discrete fourier transform .",
    "then , if we replicate the @xmath120 overdensity field 4 times and take the discrete fourier transform , we assign the fourier components to a grid 4 times larger than for the unreplicated field as the fundamental mode of the simulation should be twice as small .",
    "however we have not added any information beyond that contained in the original @xmath120 grid and as such every other component of the fourier transformed replicated field is zero , creating ringing within the power spectrum .",
    "this also highlights the correction we perform to remove this affect .",
    "after fourier transforming the replicated overdensity field we simply remove the zero components and place the remaining non - zero components in the same size grid as that used for the unreplicated box , correcting for the differences in normalisation between the two fields .",
    "we then compute the power spectrum using this smaller grid .",
    "this removes the ringing on the order of the box size and returns the power spectrum as seen in figure [ replicate_pk ] .",
    "it is important to note that this procedure still lacks the k - space resolution one would naively expect due to the fact our simulation box is larger .",
    "neither our replication method nor our correction for ringing adds in modes larger than the unreplicated box size ( there are , however , methods that do do this , see e.g. @xcite )    this is an important correction and one that should be used whenever a simulation is replicated .",
    "it is important to note however that we believe such a correction to only be necessary when looking at a portion of a replicated simulation with volume equal to or greater than the unreplicated simulation . for most practical applications",
    ", the unreplicated simulation would be much larger than that used here , and the lightcone simulations themselves would undergo significant post - processing , such as the application of a survey window function and cutting into redshift slices . in this case the volume of each redshift slice will most likely be less than the original unreplicated simulation volume and so no correction will be necessary .",
    "utilising our 500 realisations for both sets of simulations we also look at the effect of replication on the covariance matrix .",
    "this is shown in figure [ replicate_cov ] .",
    "assuming gaussian covariance , .i.e . , @xcite , we would expect the covariance to scale as the inverse of the simulation volume .",
    "our two sets of unreplicated simulations show this behaviour , with the larger volume simulation having a covariance 8 times smaller than the smaller simulation , at least on linear scales .",
    "but , as with the power spectrum , artificially increasing the simulation volume by replication does not add in any extra unique modes and so does not increase the variance .",
    "this in turn means that the covariance matrix of the replicated simulation does not display the expected volume dependence .",
    "particle lightcone simulations in a @xmath115 box and @xmath116 particle lightcone simulations in a @xmath119 box .",
    "the errors are derived from bootstrap resampling with replacement over the 500 realisations .",
    "the dashed line shows the covariance of the replicated simulations after dividing by the difference in volume between the two sets of unreplicated simulations .",
    "the amplitude of the covariance matches the order of the legend.,width=317 ]    knowing the expected volume dependence , however , we can correct for this effect .",
    "this correction is shown in [ replicate_cov ] as the dashed blue line .",
    "the corrected , replicated covariance agrees very well with the unreplicated covariance , however there is some residual differences on small scales .",
    "we hypothesise that this arises due to the absence of modes larger than the unreplicated box size , which would otherwise couple with the small scale modes within the simulation and increase the small scale covariance .",
    "this coupling is referred to as the super - sample covariance by @xcite and @xcite , who also explore corrections for this effect that could be applied to replicated simulations .",
    "however as , like the power spectrum , most applications of l - picola will involve some manipulation of the final simulation output , we would not expect to see this incorrect volume dependence unless the comoving volume of the region we were analysing was close to the unreplicated simulation size .",
    "on the other hand , with this in mind , we still recommend that for any usage of l - picola involving replication of the simulation region , the effects on the power spectrum and covariance matrix are throughly tested .",
    "this could be done using a procedure similar to that shown here , comparing replicated and unreplicated simulations after applying any survey geometry and data analysis effects .",
    "obviously replication will only be necessary if maintaining both the full volume and number density is unfeasible , however as these effects arise due to the simulation volume rather than the particle number density one would be able to test this without simulating the full number of particles in the unreplicated volume .      in l - picola ,",
    "lightcone simulations are performed in such a way as to add no additional memory requirements to the run , however the amount of time to drift the particles will increase proportionally to the number of replicates . in order to speed this up we identify , each timestep , which replicates are necessary to loop over .",
    "any replicates that have all 8 vertices inside the lightcone at the end of the timestep will not have particles leaving the lightcone and so can be ignored for the current iteration .",
    "furthermore , for replicates not fully inside the lightcone , we calculate the shortest distance between the replicate and the origin by first calculating the distance to each face of the replicate then the shortest distance to each line segment on that face .",
    "if the shortest distance to the origin is larger than the lightcone radius then the replicate has completely exited the lightcone and will no longer be required for the duration of the simulation .",
    "overall , this means that even if the simulation box is replicated @xmath121 times in each direction we will only need to look at a small fraction of the replicates ( @xmath122 in each direction unless the simulation box is so small that the lightcone radius changes by more than the boxsize in a single timestep ) .",
    "in this section we compare the accuracy of l - picola to a full n - body simulation using the tree - pm code gadget-2 @xcite and to the results returned using only 2lpt , which has been used to generate mock catalogues for the boss survey @xcite . in all cases",
    "we use a simulation containing @xmath123 particles in a box of edge length @xmath124 , starting at an initial redshift of 9.0 and evolving the dark matter particles to a final redshift of 0.0 .",
    "we use our fiducial cosmology and a linear power spectrum calculated at redshift 0.0 from camb @xcite . in all cases , unless this choice itself is being tested , we set @xmath125 . the memory requirements for this simulation are given in appendix a , where this simulation is used as an example .",
    "we first look at how well l - picola recovers the two - point clustering of the dark matter field compared to the n - body simulation , which we treat as our fully correct solution . in all cases",
    "we estimate the power spectrum within our cubic simulations using the method of @xcite . in figure [ picola_accuracy1 ]",
    "we show the ratio of the power spectra recovered from the approximate simulations and from the gadget-2 run .",
    "we plot the results recovered using 2lpt and l - picola runs with 10 timesteps and 50 timesteps , and for a set of runs with the cola modification turned off and the same numbers of timesteps .",
    "the act of turning the cola method off reduces l - picola to a standard particle - mesh code .",
    "we also plot the cross correlation , @xmath126 , between the approximate dark matter field , @xmath127 , and the full non - linear field from our n - body run , @xmath128 , defined as @xmath129 we neglect the contribution from shot - noise here as for dark matter particles and our simulation specifications this will be very sub - dominant on all scales we consider .",
    "we see that the cola method creates a much better approximation of the full non - linear dark matter field than 2lpt and the particle - mesh algorithms alone for a small number of timesteps .",
    "the agreement between the cola and n - body fields is remarkable , with the power spectra agreeing to within @xmath1 up to @xmath130 , which covers all the scales currently used for bao and rsd measurements .",
    "an @xmath131 agreement remains even up to scales of @xmath132 .",
    "this level of conformity is mirrored in the cross correlation , which for the cola run remains above 98% for all scales plotted .",
    "further to this , where the cross - correlation is 1 , we would not expect this to deviate between realisations .",
    "it is non - stochastic . as such",
    "we would expect that where the cross - correlation is 1 , the covariance of the l - picola and gadget-2 simulations would be identical ( at the level of noise caused by using a finite number of realizations ) .",
    "figure [ picola_accuracy1 ] indicates that the real - space covariance matrix recovered from l - picola is exact on all scales of interest to bao and rsd measurements . even where the cross - correlation between the l - picola and gadget-2 simulations deviates from 1",
    ", it still remains very high , such that the covariance matrix recovered from l - picola would match extremely well that from a full ensemble of n - body realisations even up to @xmath132    in the same number of timesteps the particle - mesh algorithm can not match the accuracy of cola on any scales . even on large scales there is a discrepancy between the pm and gadget runs , as there are not enough timesteps for the pm algorithm to fully recover the linear growth factor .",
    "this validates the reasoning behind the cola method as the 2lpt solution provides the solution on linear scales but performs much worse than the pm algorithm on smaller scales .",
    "the time taken for a single timestep under both the cola and pm methods is identical and as such the cola method gives much better results for a fixed computational time .",
    "interestingly , however , the cola and the standard pm algorithm converge if a suitable number of timesteps is used ( 50 in this case ) . when this many timesteps are used",
    "the pm code can accurately recover the linear growth factor and the non - linear clustering is greatly improved . using a larger number of timesteps for the cola run only affects the non - linear scales as the linear and quasi - linear scales are already fully captured .",
    "using larger and larger numbers of timesteps has a diminishing effect on both algorithms , as the small scale accuracy becomes bounded by the lack of force resolution below the mesh scale . as the cola method is already quite accurate for a few timesteps increasing the number of timesteps for a fixed mesh size does not add as much accuracy as for the pm method alone . incorporating the cola mechanism into a tree - pm code would negate this effect and we expect that increasing the number of timesteps used would then continue to increase the small scale accuracy beyond that achieved using the pm algorithm only .",
    "figure [ picola_accuracyrsd ] compares the real and redshift - space cross correlation for the cola and pm runs using 10 timesteps .",
    "the additional displacement each particle receives due to redshift space distortions , @xmath133 , is evaluated using @xmath134 where @xmath135 is the line of sight velocity of each particle for an observer situated in the centre of the simulation box .    in all cases",
    "we see that the accuracy of the simulation in redshift - space is worse than in real space .",
    "the @xmath136 cross correlation continues only up to @xmath137 .",
    "however , this is to be expected as , in addition to slightly under - predicting the spatial clustering of the dark matter particles , the approximate methods do not recover the full non - linear evolution of the particle s velocities as a function of time .",
    "the agreement in redshift space between the cola method and the gadget-2 run is still very good on all scales of interest to bao and rsd measurements and the cola method still outperforms the pm algorithm .",
    "similarly we would expect the redshift - space covariance matrix to remain extremely accurate on these scales of interest .",
    "we also look at the accuracy with with l - picola recovers the three - point clustering of the dark matter field .",
    "in particular we use the reduced bispectrum , @xmath138 where @xmath139 is the bispectrum for our periodic , cubic simulation .    in order to explore the agreement between gadget-2 and l - picola across a wide range of bispectrum configurations we plot the reduced bispectrum ratio for l - picola and gadget-2 as a function of the ratios @xmath140 and @xmath141 for a variety of different values of @xmath142 .",
    "this is shown in figure [ reduced_bispectrum2 ] .",
    "for clarity in this figure and to avoid double plotting the same configurations we enforce the conditions @xmath143 and @xmath144 .        from figure [ reduced_bispectrum2 ] we find that l - picola is able to reproduce the reduced bispectrum to within 6% for",
    "_ any _ bispectrum configuration up to @xmath145 .",
    "we can also identify the configurations that l - picola reproduces with greatest and least accuracy .",
    "regardless of the scale we find that the bispectrum in the squeezed , elongated or folded limit is reproduced extremely well , to within @xmath1 on all scales . this is because these configurations contain large contributions from triangles with one or two large scale modes , which we expect l - picola to reproduce exactly .",
    "the least accurate regime is the equilateral configuration , with accuracy decreasing as we go to smaller scales ( larger @xmath142 ) .",
    "this is because these triangles contain the biggest contribution from small scale modes in the simulation , which are not reproduced quite as accurately in l - picola .",
    "it should be noted that the convergence time of cola depends intimately on the choice of timestepping and mesh size used and the accuracy after a given number of timesteps can vary based on the exact choices made .",
    "the representative run in figure [ picola_accuracy1 ] uses the modified cola timestepping and the value of @xmath9 suggested by @xcite and a number of mesh cells equal to the number of particles .",
    "figure [ picola_accuracymesh ] shows how the accuracy of cola is reduced when lower force resolutions ( less mesh cells ) are used .",
    "we look at the case where the number of mesh points is equal to 1 , 1/2 and 1/4 times the number of particles .",
    "we do not consider a number of mesh cells larger than the number of particles as , from the nyquist - shannon sampling theorem , we do not expect any improvement in the clustering at early times , when the particle distribution is approximately grid based .",
    "furthermore @xcite and @xcite advocate that there is little justification in using a force resolution higher than the mean particle separation due to the inevitable differences in clustering between different simulations caused by using a finite number of particles . for most practical applications of l - picola",
    "it also becomes computationally infeasible to use a number of mesh cells much larger than the number of particles , due to the large increase in computational time for the fourier transforms .",
    "as expected we find a reduction in the non - linear clustering accuracy as each mesh cell becomes larger , corresponding to a larger force smoothing .",
    "the large scales are still well recovered for all mesh sizes tested .",
    "using smaller mesh sizes results in faster simulations and so for a given application of l - picola a balance between mesh size and speed should be carefully considered based on the accuracy required and at which scales .        in figure [ picola_accuracy2 ]",
    "we look at the effect of using timesteps linearly and logarithmically spaced in @xmath13 and also the effect of using the modified timestepping ( with @xmath9 still ) compared to the standard @xcite method .     and using the modified method of @xcite in place of the standard @xcite timestepping .",
    "we also compare the cola runs to standard pm runs using linearly and logarithmically spaced timesteps . in all cases we run the simulation for 10 timesteps .",
    "the order of the legend matches the amplitude of the lines at large @xmath146.,width=317 ]    in all cases we see that the cola method still outperforms the standard particle - mesh algorithm , although to differing degrees",
    ". in the case of identical timestepping choices between the cola and pm runs we see that the large scale and quasi - linear power is recovered much better .",
    "one point of interest is that using linearly spaced timesteps in the pm method reduces the accuracy on large scales below that of the logarithmically - spaced pm run , but greatly improves the non - linear accuracy , beyond even that of cola with logarithmic steps .",
    "this is because using timesteps logarithmically spaced in @xmath13 means the code takes more timesteps at higher redshift , where the evolution of the dark matter field is more linear .",
    "this means that the pm algorithm recovers the linear growth factor more accurately . using linear timesteps results in more ` time ' spend at low redshifts , where the evolution is non - linear and so the non - linear growth",
    "is captured more accurately , at the expense of the large scale clustering .",
    "as the cola method gets the large scale clustering correct very quickly , using linear timesteps to increase the non - linear accuracy is much more beneficial .",
    "indeed , we find even more improvement using the modified timestepping method , eq.([eq : modifiedt ] ) , which emphasises the non - linear modes and corroborates the claims of @xcite",
    ".    it should be noted , however , that using the modified timestepping value puts additional emphasis on different growing modes , based on the value of @xmath55 , which can change the shape of the power spectrum .",
    "this is shown in figure [ picola_accuracy3 ] where we plot the power spectrum ratio between the n - body and l - picola runs for different values of @xmath55 , exciting different combinations of decaying and growing modes , which are dominant at different cosmological times .",
    "we indeed see that different values produce slightly different power spectra .",
    "however the cross - correlations for these runs are all very similar , indicating that the difference is non - stochastic and can not vary from realisation to realization .    as such ,",
    "though the ` correct ' choice depends on the exact scales and statistics we wish to reproduce with our mock realisations , this is not very important .",
    "the results can be calibrated afterwards simply by comparing two different simulations with different values of @xmath55 .",
    "we find that for our case a value @xmath9 shows reasonable behaviour on all scales .    throughout this section",
    "we have shown that the dark matter clustering recovered by l - picola is extremely accurate on all scales of interest to bao and rsd measurements .",
    "it is important to note however that when producing mock catalogues it is a representative galaxy field that is needed . in order to produce these l - picola",
    "can be combined with other codes for identifying halos and populating the dark matter field with galaxies . using the friends - of - friends algorithm @xcite and halo occupation distribution model @xcite ,",
    "@xcite generated mock catalogues from l - picola fields . in this case no modification of the friends - of - friends linking length or the hod model was needed .",
    "other methods such as those presented by @xcite , @xcite or @xcite could also be used .",
    "we have shown that the cola method itself outperforms both the 2lpt and particle - mesh algorithms in terms of the accuracy with which it reproduces the ` true ' clustering recovered from a tree - pm n - body simulations . in this section",
    "we highlight the transformation of the cola method into a viable code for use with current and next generation large scale structure surveys by demonstrating the speed of l - picola and showing how long it takes to produce a dark matter realisation compared to 2lpt and gadget-2 .",
    "we run a series of simulations with differing numbers of particles , box sizes and numbers of processors and look at the time taken in both the strong and weak scaling regimes .",
    "strong scaling is defined as the change in the runtime of the code for different numbers of processors for a fixed simulation size , whereas weak scaling is the change in runtime for a fixed simulation size _ per processor_. for the strong scaling test",
    "we use the same simulation specifications as for our accuracy tests , with numbers of processors equal to \\{8 , 16 , 32 , 64 , 128 , 256}. the simulations we use for the weak scaling are similar to those used for the strong scaling with additional details listed in table [ tab : speedweak ] . in all cases",
    "we fix the number of mesh cells to the number of particles .",
    ".the specifications of the l - picola , gadget-2 and 2lpt runs used in our weak scaling tests . in all cases",
    "we fix the number of mesh cells to the number of particles .",
    "all other simulation parameters are as used for the strong scaling runs and accuracy tests of section [ sec : accuracy ] .",
    "[ cols=\"^,^,^ \" , ]     all the run times are shown in figure [ picola_scaling ] , for both the strong and weak scaling . in both cases",
    "we have plotted the cpu time in such a way that perfect scaling will result in a constant horizontal line ( total cpu time summed across all processors for strong scaling and cpu time per processor for weak scaling ) .",
    "the top panel of this figure shows the full cpu time taken for each run .",
    "we find that our l - picola runs generally take about 3 times longer to run that a simple 2lpt realisation , however this is a relatively small cost compared to the difference in the accuracy of the methods .",
    "+     +     +    in terms of the actual scaling we find that although l - picola does not scale perfectly in either the strong or weak regimes , the increase in runtime with number of processors is still reasonable .",
    "we use a simple least squares fitting method to fit a linear trend to the cpu time as a function of the number of processors .",
    "we find gradients of 0.41 and 0.30 , compared to the ideal value of 0 , for l - picola in the strong and weak scaling regimes respectively .",
    "we find that this trend can be extrapolated well beyond our fitting range .",
    ", i.e. , for a @xmath147 particle simulation in a ( @xmath148 box run on 1024 processors we find a cpu time per processor of 348 seconds , which matches very well our predicted value of 345 seconds .",
    "there exists some scatter in the runtimes for our simulations .",
    "this generally stems from the fourier transforms involved , the efficiency of which depends on the way the mesh in partitioned across the processors . in the case where we have a number of mesh cells ( in the x - direction ) that is not a multiple of the number of processors ( .i.e , the @xmath149 , strong - scaling run ) the time taken for the calculation of the 2lpt displacements and the interparticle forces during timestepping is increased .",
    "we investigate this further in the second and third rows of figure [ picola_scaling ] .",
    "here we show the time taken for different contributions to the full run and to each timestep therein .",
    "this highlights the fact that the scatter occurs mainly during the 2lpt and force calculation parts of l - picola as expected if it is due to the fourier transform efficiency .",
    "additionally this also suggests that the non - optimal strong and weak scaling does not stem from any particular part of the code , but rather due to the extra mpi communications needed when we use larger numbers of processors .",
    "looking at the contributions from the 2lpt , timestepping and output stages , we see that there is some evolution with processor number in the 2lpt stage from the fact that the fourier transforms require extra communications between different processors to transform the full mesh .",
    "we also see an increasing contribution from the output stage of the code as we go to larger numbers of processors .",
    "this is because of an option in the code to limit the number of processors outputting at once , stopping all processors outputting simultaneously .",
    "we set this to 32 processors and as expected we see an increase in the time taken to output the data once we run simulations with more than 32 processors due to the need for some processors to wait before they can output .",
    "looking at the contributions to an individual timestep we find that the drift , kick and displacement parts of the code are reasonably constant when the number of particles per processor remains constant .",
    "these consist mainly of loops over each particle and so this is too be expected . the density calculation and",
    "force calculation steps contain the fourier transforms required for each timestep and as such are the biggest contributions to the time taken for a timestep . looking at the strong scaling case we see an increase in both of these as a function of the number of processors , which indicates they are dominated by the mpi communications as expected .    for the weak scaling",
    "we also see a large jump in the cpu time for both of these after 16 processors .",
    "this is another indication that the mpi communications are the cause of the scaling trends we see , as the architecture of the high performance computer we use is such that 16 processors are located on a single node and intra - node communication is much faster than inter - node .",
    "once we start to require inter - node communication to compute the fourier transforms the cpu time increases .",
    "finally we see that the move particles section of the code does not contribute much to the total time for each timestep , except where the number of inter - processor communications becomes large .",
    "this is due to the effort taken to produce a fast algorithm to pass the particles , whereas a simpler algorithm would result in a larger amount of time and memory needed to identify and store the particles that need transferring .",
    "in this paper we have introduced and tested a new code l - picola , which , due to its fast nature , can be used to generate large numbers of accurate dark matter matter simulations .",
    "the code is available under the gnu general public license at https:/cullanhowlett.github.io / l - picola .",
    "the main points of the paper are summarised as follows :    * l - picola is a memory conservative , planar parallelisation of the 2lpt and cola algorithms .",
    "this is enabled by parallel algorithms for cloud - in - cell interpolation , fast fourier transforms , and fast movement of particles between processors after each timestep .",
    "* we have included additional features in l - picola such as the fast creation of initial conditions for other simulation codes , with optional primordial non - gaussianity , and the ability to produce lightcone simulation , with optional replication of the simulation volume at run - time .",
    "these will be of particular use to future large scale structure surveys .",
    "* we have quantified the accuracy of the method l - picola uses to produce lightcone simulations , verifying that it s accuracy is not unduly affected by the approximations made to ensure a fast algorithm .",
    "* we have investigated the effect of replicating the simulation volume on both the power spectrum and covariance matrix using a set of 500 individual lightcone realisations .",
    "we find that , due to the fact the replication procedure modifies the simulation volume without adding additional information , the power spectrum can suffer from ringing on the scale of the unreplicated box size and that the covariance matrix demonstrates the volume dependence of the unreplicated box size as opposed to the replicated volume .",
    "we show simple corrections for both of these effects and hypothesise that this is only a problem when analysing regions of the simulation larger than the unreplicated box size .",
    "* we have compared the accuracy of l - picola to the approximate 2lpt and pm methods and to a fully non - linear tree - pm gadget-2 simulation .",
    "we find that l - picola performs much better than the 2lpt and pm algorithms , and that the power spectra from l - picola agree with that from our gadget-2 simulations to within @xmath1 on all scales of interest to bao and rsd measurements and to within @xmath150 up to @xmath132 .",
    "the reduced bispectrum from l - picola also shows remarkable agreement with our gadget-2 simulation , to within 6% for all configurations up to @xmath145 .",
    "we do however find that this agreement has some dependence on the exact type of timestepping used in the code .",
    "* we have compared the speed of l - picola to the 2lpt and gadget-2 simulations .",
    "we find that the remarkable accuracy of l - picola comes at only a small cost to speed compared to 2lpt .",
    "l - picola exhibits reasonable scaling properties in the strong and weak scaling regimes , even up to large numbers of processors .",
    "we find that these trends are dominated by the need for extra inter - processor communication when using large numbers of processors .",
    "still , there are several improvements that could be made to l - picola in the future . in terms of parallelisation , splitting the mesh into blocks rather than slices could improve both the speed and scalability of the code to large numbers of processors , however the need for additional mpi communication during the fast fourier transforms means that the level of improvement is indeterminate at this time",
    ". furthermore one could imagine hybridising the code , using open - mp and mpi such that communication between ` local ' processors does not rely on slower mpi communication .    in terms of the physics behind l - picola it would be simple to add in support for warm dark matter .",
    "another obvious addition to the code would be to implement the spatial extension of the cola method , presented by @xcite .",
    "such an improvement would allow us to simulate a large cosmological volume whilst only spending computational time evaluating the non - linear displacements for a small portion of that volume .",
    "lightcone simulations within l - picola in particular would greatly benefit from this as we would be able to simulate a small pencil - beam region of the full lightcone and scale this up to the required simulation volume .",
    "also , as this extended cola method still requires us to calculate the 2lpt displacements for all the particles within the full volume , implementing this into our distributed - memory code would allow us to simulate much larger cosmological volumes and higher particle densities than the current shared - memory implementation .",
    "additional small scale accuracy could be achieved by a suitable scaling of the mesh during the simulation , such as using a finer mesh at late times when the particles become more clustered .",
    "this would be particularly easy to implement as , in the optimal memory case , the mesh is deallocated and reallocated each time step anyway . using an adaptive mesh for high density portions of the simulation , or the tree - pm algorithm instead of the pm algorithm ,",
    "could also be implemented though these methods would come at a cost to speed .",
    "furthermore , as l - picola is so fast , we find that for current applications , the total cpu time taken to produce a mock galaxy catalogue is dominated by outputting and post - processing of ( mainly reading in ) the dark matter field , especially the creation of dark matter halos .",
    "this is exacerbated even more for lightcone simulations with replication as we are effectively outputting the simulation multiple times , resulting in large increases to the amount of time taken to output and process the data .",
    "this could be vastly improved by adding in a halo finder into l - picola , either by identifying shell - crossing as it occurs during the simulation , or via the fof algorithm .",
    "this would mean that the amount of time taken to output the data , and read it in for post - processing could be reduced drastically .",
    "we would like to thank angela burden , gary burton , james briggs and john pennycook for their help and insightful comments throughout the development and testing of l - picola .",
    "we make special acknowledgement to the facilities and staff of the uk sciama high performance computing cluster supported by the icg , sepnet and the university of portsmouth . code development and testing ,",
    "mock catalogue generation , power spectrum estimation , and other analysis all made use of the computing and storage offered therein .",
    "this research would not have been possible without this support .",
    "ch is grateful for funding from the united kingdom science & technology facilities council ( uk stfc ) grant + st / k502248/1 .",
    "wjp acknowledges support from the uk stfc through the consolidated grant st / k0090x/1 , and from the european research council through grants 202686 ( mdepugs ) and 614030 ( _ darksurvey _ ) .",
    "this work used the dirac data analytic system at the university of cambridge , operated by the university of cambridge high performance computing service and the cosma data centric system at durham university , operated by the institute for computational cosmology , both on behalf of the stfc dirac hpc facility ( www.dirac.ac.uk ) .",
    "this equipment was funded by bis national e - infrastructure capital grants + st / k001590/1 and st / k00042x/1 , stfc capital grants + st / h008861/1 and st / h00887x/1 , and stfc dirac operations grants st / k00333x/1 and st / k00087x/1 .",
    "dirac is part of the national e - infrastructure .",
    "this work was also undertaken on the cosmos shared memory system at damtp , university of cambridge operated on behalf of the stfc dirac hpc facility .",
    "this equipment is funded by bis national e - infrastructure capital grant + st / j005673/1 and stfc grants st / h008586/1 , st / k00333x/1 .",
    "this research has made use of nasa s astrophysics data system bibliographic services .",
    "99    ahn , c.  p. , alexandroff , r. , allende prieto , c. , et al .",
    "2012 , apjs , 203 , 21    ahn , c.  p. , alexandroff , r. , allende prieto , c. , et al .",
    "2014 , apjs , 211 , 17    anderson , l. , aubourg ,  . , bailey , s. , et al .",
    "2014 , mnras , 441 , 24    angulo , r.  e. , baugh , c.  m. , frenk , c.  s. , & lacey , c.  g.  2014 , mnras , 442 , 3256    berlind , a.  a. , & weinberg , d.  h.  2002 , apj , 575 , 587    bouchet , f.  r. , colombi , s. , hivon , e. , & juszkiewicz , r.  1995 , a&a , 296 , 575    chuang , c .- h . ,",
    "kitaura , f .- s . , prada , f. , zhao , c. , & yepes , g.  2015 , mnras , 446 , 2621    cole , s.  1997 , mnras , 286 , 38    cole , s. , percival , w.  j. , peacock , j.  a. , et al .  2005 ,",
    "mnras , 362 , 505    coles , p. , & jones , b.  1991 , mnras , 248 , 1    colless , m. , dalton , g. , maddox , s. , et al .",
    "2001 , mnras , 328 , 1039    colless , m. , peterson , b.  a. , jackson , c. , et al .",
    "2003 , arxiv : astro - ph/0306581    davis , m. , efstathiou , g. , frenk , c.  s. , & white , s.  d.  m.  1985 , apj , 292 , 371    dawson , k.  s. , schlegel , d.  j. , ahn , c.  p. , et al .  2013 ,",
    "aj , 145 , 10    the dark energy survey collaboration 2005 , arxiv : astro - ph/0510346    de la torre , s. , & peacock , j.  a.  2013 , mnras , 435 , 743    dodelson , s. , & schneider , m.  d.  2013 , phys . rev .",
    "d , 88 , 063537    drinkwater , m.  j. , jurek , r.  j. , blake , c. , et al .",
    "2010 , mnras , 401 , 1429    eisenstein , d.  j. , zehavi , i. , hogg , d.  w. , et al .",
    "2005 , apj , 633 , 560    eisenstein , d.  j. , weinberg , d.  h. , agol , e. , et al .",
    "2011 , aj , 142 , 72    feldman , h.  a. , kaiser , n. , & peacock , j.  a.  1994 , apj , 426 , 23    fosalba , p. , crocce , m. , gaztanaga , e. , & castander , f.  j.  2013 , arxiv:1312.1707    hockney , r. w. , eastwood , j. w. , _ computer simulation using particles _ , 1988 , adam hilger    howlett , c. , lewis , a. , hall , a. , & challinor , a.  2012 , j. cosmo .",
    "astroparticle phys . , 4",
    ", 027    howlett , c. , ross , a.  j. , samushia , l. , percival , w.  j. , & manera , m.  2015 , mnras , 449 , 848    ivezic , z. , tyson , j.  a. , abel , b. , et al .",
    "2008 , arxiv:0805.2366    jones , d.  h. , saunders , w. , colless , m. , et al .",
    "2004 , mnras , 355 , 747    jones , d.  h. , read , m.  a. , saunders , w. , et al .",
    "2009 , mnras , 399 , 683    kaiser n. , 1987 , mnras , 227 , 1    kitaura , f .- s . , & he , s.  2013 , mnras , 435 , l78    kitaura , f .- s . ,",
    "yepes , g. , & prada , f.  2014 , mnras , 439 , l21    klypin , a. , & holtzman , j.  1997 , arxiv : astro - ph/9712217    laureijs , r. , amiaux , j. , arduini , s. , et al .",
    "2011 , arxiv:1110.3193    lewis , a. , challinor , a. , & lasenby , a.  2000 , apj , 538 , 473    li , y. , hu , w. , & takada , m.  2014 , phys .",
    "d , 89 , 083519    manera , m. , scoccimarro , r. , percival , w.  j. , et al .",
    "2013 , mnras , 428 , 1036    manera , m. , samushia , l. , tojeiro , r. , et al .",
    "2015 , mnras , 447 , 437    matsubara , t.  1995 , progress of theoretical physics , 94 , 1151    merson , a.  i. , baugh , c.  m. , helly , j.  c. , et al .",
    "2013 , mnras , 429 , 556    monaco , p. , theuns , t. , taffoni , g. , et al .",
    "2002 , apj , 564 , 8    monaco , p. , sefusatti , e. , borgani , s. , et al .",
    "2013 , mnras , 433 , 2389    moutarde , f. , alimi , j .- m . , bouchet , f.  r. , pellat , r. , & ramani , a.  1991 , apj , 382 , 377    pacheco , p. s. , _ parallel programming with mpi _ , 1997 , morgan kaufmann .",
    "peebles , p.  j.  e. , melott , a.  l. , holmes , m.  r. , & jiang , l.  r.  1989 , apj , 345 , 108    percival , w.  j. , ross , a.  j. , snchez , a.  g. , et al .",
    "2014 , mnras , 439 , 2531    quinn , t. , katz , n. , stadel , j. , & lake , g.  1997 , arxiv : astro - ph/9710043    ross , a.  j. , samushia , l. , howlett , c. , et al .",
    "2015 , mnras , 449 , 835    scoccimarro , r.  1998 , mnras , 299 , 1097    scoccimarro , r. , & sheth , r.  k.  2002 , mnras , 329 , 629    scoccimarro , r. , hui , l. , manera , m. , & chan , k.  c.  2012 , phys .",
    "d , 85 , 083002    seo h .-",
    "j . , eisenstein d.j . , 2003 , apj , 598 , 720    splinter , r.  j. , melott , a.  l. , shandarin , s.  f. , & suto , y.  1998 , apj , 497 , 38    springel , v.  2005 , mnras , 364 , 1105    takada , m. , & hu , w.  2013 , phys .",
    "d , 87 , 123504    tassev , s. , zaldarriaga , m. , & eisenstein , d.  j.  2013 , j. cosmo .",
    "astroparticle phys . , 6 , 36    tassev , s. , eisenstein , d.  j. , wandelt , b.  d. , & zaldarriaga , m.  2015 , arxiv:1502.07751",
    "taylor , a. , joachimi , b. , & kitching , t.  2013 , mnras , 432 , 1928    tegmark , m.  1997 , physical review letters , 79 , 3806    tormen , g. , & bertschinger , e.  1996 , apj , 472 , 14    white , m. , tinker , j.  l. , & mcbride , c.  k.  2014 , mnras , 437 , 2594    xu , x. , padmanabhan , n. , eisenstein , d.  j. , mehta , k.  t. , & cuesta , a.  j.  2012 , mnras , 427 , 2146    york , d.  g. , adelman , j. , anderson , j.  e. , jr .",
    ", et al .  2000 ,",
    "aj , 120 , 1579    zeldovich , y.  b.  1970 , a&a , 5 , 84",
    "considerable effort has been made to reduce the memory footprint of l - picola as much as possible , including the introduction of a compilation option to conserve as much memory as possible .",
    "when this option is used the memory consumption for a l - picola run is reduced significantly and the mean memory per processor can be calculated reasonably simply .    here",
    "we detail the calculation of the memory needed for an l - picola simulation under these optimum conditions . with the optimal memory setting we use floating point precision for the particles and double precision for the mesh .",
    "the information for each particle consists of x , y and z coordinates , velocities in those same directions , and the za and 2lpt displacements in those directions , resulting in @xmath151 per particle .",
    "the main contributions to the memory arise from the particles and the mesh and the key parameters are the number of mesh cells , @xmath72 , number of particles , @xmath67 and the amount of buffer memory allocated to each processor to account for the non - uniformity of the particle distribution over processors at late times , @xmath152 .",
    "the code can be split into six distinct sections : the calculation of the initial 2lpt potentials ; the calculation of the initial 2lpt displacements ; the initialisation of the particles ; the moving of particles across processors each timestep ; the evaluation of the interparticle mesh - based force each timestep ; and the calculation of the particle displacements for each timestep .",
    "the corresponding memory requirements are : @xmath153 @xmath154 @xmath155 @xmath156 @xmath157 @xmath158 the maximum memory required for an l - picola simulation is the largest of these 6 contributions .",
    "a utility for calculating the memory requirements , even when using suboptimal ( in terms of memory ) compilation options is provided with the public release of the code .    .",
    "the solid lines show the contributions from different sections of the code for varying numbers of processors , whilst the intersection of the dashed line with the solid lines gives the minimum number of processors required to run the simulation assuming there is 4 gb of memory available per processor.,width=317 ]    as an example figure [ picola_mem ] shows the memory requirements as a function of number of processors for the l - picola simulations used in section [ sec : accuracy ] .",
    "we can see that if we have 4 gb of memory available per processor , this simulation can be run using only 25 processors if the optimal compilation options are used ( 32 were used for the simulations in this paper ) ."
  ],
  "abstract_text": [
    "<S> robust measurements based on current large - scale structure surveys require precise knowledge of statistical and systematic errors </S>",
    "<S> . this can be obtained from large numbers of realistic mock galaxy catalogues that mimic the observed distribution of galaxies within the survey volume . to this end </S>",
    "<S> we present a fast , distributed - memory , planar - parallel code , l - picola , which can be used to generate and evolve a set of initial conditions into a dark matter field much faster than a full non - linear n - body simulation . additionally , l - picola has the ability to include primordial non - gaussianity in the simulation and simulate the past lightcone at run - time , with optional replication of the simulation volume . through comparisons </S>",
    "<S> to fully non - linear n - body simulations we find that our code can reproduce the @xmath0 power spectrum and reduced bispectrum of dark matter to within @xmath1 and @xmath2 respectively on all scales of interest to measurements of baryon acoustic oscillations and redshift space distortions , but 3 orders of magnitude faster . </S>",
    "<S> the accuracy , speed and scalability of this code , alongside the additional features we have implemented , make it extremely useful for both current and next generation large - scale structure surveys . </S>",
    "<S> l - picola is publicly available at https://cullanhowlett.github.io/l-picola </S>",
    "<S> .    large - scale structure of universe - methods : numerical </S>"
  ]
}