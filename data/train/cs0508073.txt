{
  "article_text": [
    "today , data mining and machine learning is typically treated in a _ problem - specific _ way : people propose algorithms to solve a particular problem ( such as learning to classify points in a vector space ) , they prove properties and performance guarantees of their algorithms ( e.g.  for support vector machines ) , and they evaluate the algorithms on toy or real data , with the ( potential ) aim to use them afterwards in real - world applications . in contrast , it seems that _ universal learning _ , i.e.  a _ single _ algorithm which is applied for _ all _ ( or at least ",
    "many \" ) problems , is neither feasible in terms of computational costs nor competitive in ( practical ) performance . nevertheless , understanding universal learning is important : on the one hand , its practical success would lead a way to artificial intelligence . on the other hand ,",
    "_ principles _ and ideas from universal learning can be of immediate use , and of course machine learning research aims at exploring and establishing more and more general concepts and algorithms .    because of its practical restrictions , most of the understanding of universal learning so far is theoretical .",
    "some approaches which have been suggested in the past are ( adaptive ) levin search @xcite , optimal ordered problem solver @xcite and reinforcement learning with split trees @xcite among others . for a thorough discussion",
    "see e.g.@xcite . in this paper , we concentrate on two approaches with very strong theoretical guarantees in the limit : the ai@xmath1  agent based on bayesian learning @xcite and @xmath2 based on prediction with expert advice @xcite .",
    "both models work in the setup of a _ sequential decision problem _ : an _ agent _ interacts with an _ environment _ in discrete time @xmath3 . at each time step , the agent does some _ action _ and receives a _",
    "feedback _ from the environment .",
    "the feedback consists of a _",
    "loss _ ( or _ reward _ ) plus maybe more information .",
    "( it is usually just a matter of convenience if losses or rewards are considered , as one can be transformed into the other by reverting the sign .",
    "accordingly , in this paper we switch between both , always preferring the more convenient one . ) in addition to this _ instantaneous _ loss ( or reward ) , we will also consider the _ cumulative _ loss which is the sum of the instantaneous losses from @xmath4 up to the current time step , and the _ average per round _ loss which is the cumulative loss divided by the total number of time steps so far .",
    "most learning theory known so far concentrates on _ passive _ problems , where our actions have an influence on the instantaneous loss , but _ not on the future behavior of the environment_. all regression , classification , ( standard ) time - series prediction tasks , common bayesian learning and prediction with expert advice , and many others fall in this category .",
    "in contrast , here we deal with _ active _ problems .",
    "the environment may be _ reactive _ , i.e.  react to our actions , which is the standard situation considered in reinforcement learning .",
    "these cases are harder in theory , and it is often impossible to obtain relevant performance bounds in general .",
    "both approaches we consider and compare are based on finite or countably infinite _",
    "base classes_. in the bayesian decision approach , the base class consists of _ hypotheses _ or _ models _ for the environment .",
    "a model is a complete description of the ( possibly probabilistic ) behavior of the environment . in order to prove guarantees",
    ", it is usually assumed that the true environment is contained in the model class .",
    "experts algorithms in contrast work with a class of decision - makers or _",
    "experts_. performance guarantees are proven without any assumptions _ in the worst case _ , but only relative to the best expert in the class . in both approaches ,",
    "the model class is endowed with a _",
    "prior_. if the model class is finite and contains @xmath5 elements , it is common to choose the uniform prior @xmath6 . for _ universal _",
    "learning it turns out that universal base classes for both approaches can be constructed from the set of all programs on some fixed universal ( prefix ) turing machine .",
    "then each program naturally corresponds to an element in the base class , and a prior weight is defined by @xmath7 ( provided that the input tape of the turing machine is binary ) .",
    "the prior is a ( sub-)probability distribution on the class , i.e.  @xmath8 .",
    "* contents .",
    "* the aim of this paper is to better understand the _ actual learning dynamics _ and properties of the two universal approaches , which are both  universally optimal \" in a sense specified later . clearly , the universal base class is computationally very expensive or infeasible to use .",
    "so we will restrict on simpler base classes which are  universal \" in a much weaker sense : we will employ complete _ markov _ base classes where each element sees only the previous time step . although these classes are not truly universal , they are general enough ( and not tailored towards our applications ) , such that we expect the outcome to be a good indication for the dynamics of true universal learning .",
    "the problems we study in this paper are _ @xmath0 matrix games_. ( due to lack of space , we will not go into the deep literature on learning equilibria in matrix games , as our primary interest the universal learning dynamics . )",
    "matrix games are simple enough such that a  universal \" algorithm with our restricted base class can learn something , yet they provide interesting and nontrivial cases for reactive environments , where really active learning is necessary . moreover , in this way we can set up a direct competition between the two universal learners .",
    "the paper is structured as follows : in the next two sections , we present both universal learning approaches together with their theoretical guarantees .",
    "section [ secsim ] contains the simulations , followed by a discussion in section [ secdc ] .",
    "* passive problems . * every inductive inference problem can be brought into the following form : given a string @xmath9 , guess its continuation @xmath10 . here and in the following we assume that the symbols @xmath10 are in a finite alphabet @xmath11 , for concreteness the reader may think of @xmath12 .",
    "if strings are sampled from a probability distribution @xmath13 $ ] , then predicting according to @xmath14 , the probability conditioned on the history , is optimal . if @xmath15 is unknown , predictions may be based on an approximation of @xmath15 .",
    "this is what happens in _ bayesian sequential prediction _ : let the _ model class _ @xmath16 be a finite or countable set of distributions on strings @xmath17 which are additionally conditionalized to the past actions @xmath18 .",
    "the actions are necessary for dealing with sequential decision problems as introduced above .",
    "we agree on the convention that the learner issues action @xmath19 _ before _ seeing @xmath10 .",
    "let @xmath20 be a prior on @xmath21 satisfying @xmath22 .",
    "then the _ bayes mixture _ is the weighted average ( x_1:t|y_1:t ) : = _ iw_i_i(x_1:t|y_1:t ) .",
    "one can show that the @xmath1-predictions rapidly converge to the @xmath15-predictions almost surely , if we assume that @xmath21 contains the true distribution : @xmath23 .",
    "this is not a serious constraint if we include _ all _ computable probability distributions in @xmath21 .",
    "this universal model class corresponds to all programs on a fixed universal turing machine ( cf . the introduction and @xcite ) .    in a passive prediction problem ,",
    "the behavior of the environments @xmath24 do not depend on our actions @xmath25 . here",
    "we may interpret our action @xmath19 as the prediction of @xmath10 .",
    "assume that @xmath26 $ ] is a function defining our instantaneous loss .",
    "then _ the average per round regret of @xmath1 tends to 0 at least at rate @xmath27 _ , precisely [ eq : lb ] l^_1:tl^_1:t+2 .",
    "here , @xmath28 is the cumulative @xmath15-expected loss of the @xmath1-predictions .",
    "the @xmath1-prediction ( and likewise the @xmath15-prediction ) is chosen _ bayes optimal _ for the given loss function : @xmath29 .",
    "the difference @xmath30 is termed _ regret_.    * active problems .",
    "* if the environment is _ reactive _ , i.e.  depends on our action , then it is easy to construct examples where the greedy bayes optimal loss minimization is not optimal . instead , the _ far - sighted ai@xmath1-agent _ chooses the action [ ydotxi ] y_t^,d= _ y_t_x_t ... _ y_t+d_x_t+d _ t : t+d(x_1:t+d|y_1:t+d ) . where @xmath31 and @xmath32 is the depth of the _ expectimin - tree _ the agent computes by means of ( [ ydotxi ] ) .",
    "we refer to @xmath33 as the ( current ) _",
    "if we knew the final time @xmath34 in advance and had enough computational resources , we could choose @xmath35 according to the _ fixed horizon _",
    "taking @xmath32 fixed and small ( e.g. @xmath36 ) is computationally feasible , this is the _ moving horizon _ variant .",
    "however , this can cause consistency problems : a sequence of actions which is started some time step @xmath3 may not seem favorable any more in the next time step @xmath37 ( since the horizon shifts ) , and thus is disrupted .",
    "we therefore also use an _ almost consistent horizon variant _ which takes @xmath36 in the first step , then @xmath38 , and so on down to @xmath39 , after which we start again with @xmath36 .",
    "( actually , we do not go down to @xmath40 , since then the agent would be greedy , which can for instance disrupt consecutive runs of cooperation in the prisoner s dilemma , see below . )",
    "a theoretically very appealing alternative is to consider the future _ discounted _ loss and infinite depth , which is a solution of the bellman equations .",
    "this can be found in @xcite , together with more discussion and the proof of the following _ optimality theorem _ for ai@xmath1 .",
    "[ thselfopt ] if there exists a self - optimizing policy @xmath41 in the sense that its expected average loss @xmath42 converges for @xmath43 to the optimal average @xmath44 for all environments @xmath45 , then this also holds for the universal policy @xmath1 , i.e. : l_1:t^ l_1:t^ l_1:t^ l_1:t^    * matrix games * ( as defined in section [ secsim ] ) are straightforward in our setup .",
    "we just have to consider that the opponent , i.e.  the environment , does not know our action @xmath19 when deciding its reaction @xmath10 : @xmath46 .",
    "ai@xmath1  for @xmath0 matrix games can then be implemented recursively as shown in figure [ fig : aixi ] , if we additionally assume that the environments are _ markov players _ with two internal states , corresponding to the reaction @xmath10 they are playing . since in step @xmath3",
    ", we do nt know @xmath10 yet , ai@xmath1  must evaluate both @xmath47 and @xmath48 and compute a weighted mixture for both possible actions @xmath49 . as long as we do not yet know the loss matrix @xmath26 $ ] completely ( which we assume to be deterministic )",
    ", we additionally compute an expectation over all assignments of losses which are consistent with the history . to this aim",
    ", we pre - define a finite set @xmath50 which contains all possible losses . in the simulations below , we use @xmath51 , where the actual losses are always in @xmath52 and the large negative value of @xmath53",
    "encourages the agent to explore as long as he does nt know the losses completely .",
    "this is for obtaining interesting results with moderate tree depth : otherwise , when the loss observed by ai@xmath1  is relatively low , ai@xmath1  would explore only with a large depth .",
    "this phenomenon is explained in detail in section [ secsim ] .",
    "* markov decision processes * ( mdp ) have probably been most intensively studied from all possible environments . in an mdp ,",
    "the environmental behavior depends only on the last action and observation , precisely @xmath54 in case of a matrix game . for a @xmath0 game",
    ", a markov player is modelled by a @xmath55 transition matrix .",
    "it turns out that the ( uncountable ) class of all transition matrices with a uniform prior admits a closed - form solution : ( x_t|x_<t , y_<t)=n_x_t-1x_t^y_t-1 + 1 n_x_t-10^y_t-1+n_x_t-11^y_t-1 + 2 , where @xmath56 counts how often in the history the state @xmath57 transformed to state @xmath10 under action @xmath58 .",
    "this is just laplace law of succession ( * ? ? ?",
    "* prob.2.11&5.14 ) .",
    "( observe that @xmath1 is not markov but depends on the full history . )",
    "note that the @xmath1 posterior estimate changes along the expectimin tree ( [ ydotxi ] ) . disregarding this important fact",
    "as is done in temporal difference learning and variants would result in greedy policies who have to rescue exploration by ad - hoc methods ( like @xmath59-greedy ) .",
    "one can show that there exist self - optimizing policies @xmath60 for the class of ergodic mdps @xcite .",
    "although the class of transition matrices contains non - ergodic environments , a variant of theorem [ thselfopt ] applies , and hence the bayes optimal policy @xmath61 is self - optimizing for ergodic markov players ( which we will exclusively meet ) .",
    "the intuitive reason is that the class is compact and the non - ergodic environments have measure zero .",
    "instead of predicting or acting optimally with respect to a model class , we may construct an agent from a class of _ base agents_. we show how this can be accomplished for fully active problems .",
    "the resulting algorithm will radically differ from the ai@xmath1  agent .",
    "* _ prediction _ with expert advice * has been very popular in the last two decades .",
    "the base predictors are called experts .",
    "our goal is to design a master algorithm which in each time step @xmath3 selects one expert @xmath62 and follows its advice ( i.e.  predicts as the expert does ) .",
    "thereby , we want to keep the master s _ regret _ @xmath63 small , where @xmath64 is the cumulative loss of the best expert in hindsight at time @xmath34 .",
    "usually , @xmath65 not known in advance .",
    "the state - of - the - art experts algorithms achieve this : loss bounds similar to ( [ eq : lb ] ) can be proven , with @xmath66 replaced by @xmath64 and @xmath67 replaced by the prior weight of the best expert in hindsight , @xmath68 .",
    "these bounds hold _ in the worst case _ ,",
    "i.e.  without any assumption on the data generating process .",
    "in particular , the environment which provides the feedback may be an adaptive adversary . since these bounds imply bounds in expectation in the bayesian setting ( with slightly larger constants than ( [ eq : lb ] ) ) , expert advice is in a sense the stronger prediction strategy .    in order to protect against adaptive adversaries , we need to randomize . in this work ,",
    "we build on the _ follow the perturbed leader _ @xmath69 algorithm introduced by @xcite .",
    "( for space constraints , we wo nt discuss the more popular alternative of weighted sampling at all . )",
    "we do nt even need to be told the true outcome after the master s decision .",
    "all we need for the analysis is learning the _ losses _ of all experts , which are bounded wlog .  in @xmath70 $ ]",
    "( this is an important restriction which applies to all standard experts algorithms ) . in this way ,",
    "the master s actual decision is based on the _ past cumulative loss _ of the experts .",
    "a key concept is that we must prevent the master from learning too fast ( or to slowly ) .",
    "this is achieved by introducing a _ learning rate _ @xmath71 , which decreases to zero at an appropriate rate with growing @xmath3 .",
    "most of the literature assumes experts classes of finite size @xmath5 with uniform prior @xmath6 , in particular when the learning rate @xmath71 is non - stationary . for @xmath69 ,",
    "the case of arbitrary non - uniform prior and countable expert classes has been treated in @xcite .",
    "* active problems . * in the passive _ full observation game _ discussed so far ( i.e.  we learn all losses ) , the notion of regret is not problematic even against an adaptive adversary .",
    "however , the situation changes if the reaction of the environment depends on our past actions .",
    "consider the simple case of two experts , one always suggesting action 0 and the other one action 1 . the environment is reactive and  unfair \" : each expert incurs no loss as long as we stay with its initial action ( e.g. the action sequences 00000 and 111 have no loss ) .",
    "but as soon as we perform a different action ( e.g. 001 ) , in all subsequent rounds both experts incur loss 1 .",
    "each sensible strategy will soon explore both actions , and compared to the pure experts , we incur large loss .",
    "consequently , we need to consider a different notion of regret : our performance is compared to what an expert could achieve when he is actually put in our situation . in this example , after the action sequence 001 , we perform badly , but so do all experts .    another problem with reactive environments is that we do not necessarily get valid feedback for all experts in each round . in the previous example , if we chose 0 as the first action and learned that expert 0 had no loss at time @xmath72 , it is not legitimate to make any assumption on the loss of expert 1 at @xmath72 . even if the environment tells us that the _ pure _ expert 1 had no loss , we are interested in the loss of expert 1 put in our situation , i.e.  after the first action 0 . but this loss we do not know .",
    "precisely , we know only the loss of an expert with the _ correct action history _ after the last time step in the past , where we ( maybe coincidentally ) acted as he suggested . instead of trying to track the action history ( which is possibly expensive ) , we therefore use _ only the feedback from the currently selected expert _",
    "@xmath62 and discard all other information .",
    "this is commonly referred to as _",
    "bandit setup_. fortunately , this issue can be successfully addressed by forcing exploration , i.e.  sampling according to the prior , with a certain probability @xmath73 @xcite .",
    "this _ exploration rate _",
    "@xmath73 is decreased to zero appropriately with growing @xmath3 .",
    "thus , in each time step we decide to either _ follow _ the perturbed leader or _",
    "explore_. accordingly , we call our algorithm @xmath2 ( follow or explore ) .",
    "bounds for the bandit setup are typically similar to ( [ eq : lb ] ) , but with @xmath74 replaced by ( something larger than ) @xmath75 .",
    "hence they are exponentially larger in @xmath68 , and one can show that this is sharp in general .    * increasing horizon . *",
    "if the environment is reactive , it is not sufficient to consider only the short - term performance of the selected expert @xmath62 .",
    "this was first recognized by @xcite , who considered the repeated game of  prisoner s dilemma \" and the ",
    "tit for tat \" opponent as a motivating example ( see section [ secsim ] for details )",
    ". in this case , a good long term strategy is cooperating ( because of the particular opponent ) .",
    "however defecting is dominant , i.e.  the instantaneous loss of defecting is _ always _ smaller than that of cooperating .",
    "so in order to notice that an expert ( for instance the always cooperating one ) performs well , we have to evaluate it at least over two time steps .",
    "in general , if we evaluate a chosen expert over an _ increasing number of time steps _ , we hope that we perform well in arbitrary reactive environments .",
    "this means that the master works at a _ different time scale _ @xmath76 : in its @xmath76th time step , it gives the control to the selected expert for @xmath77 time steps ( in the original time scale @xmath3 ) . as a consequence , the instantaneous losses",
    "which the master observes are no longer uniformly bounded in @xmath70 $ ] , but in @xmath78 $ ] .",
    "fortunately , it turns out that the analysis remains valid if @xmath79 grows unboundedly but slowly enough .",
    "only the convergence rate of the average master s loss to the optimum is affected : we will obtain a final rate of @xmath80 .",
    "the resulting algorithm @xmath2 ( for a finite expert class ) is specified in figure [ fig : foe ] together with its subroutine @xmath69 .",
    "we may have instantaneous and cumulative losses in both time scales , this is always clear from the notation ( e.g.  @xmath81 vs. @xmath82 ) .",
    "not surprisingly , most of @xmath2 works in the master time scale .",
    "note that @xmath2 makes use of its observation _ only if he decided to explore _ ,",
    "i.e.  if @xmath83 .",
    "this seems an unnecessary waste of information .",
    "this is motivated from the analysis , since @xmath2 needs an _ unbiased loss estimate _ @xmath84 ( with respect to @xmath2 s randomization ) .",
    "we just chose the simplest way to guarantee this .",
    "for the simulations , we concentrate on the following _ faster learning variant _ : approximate the probability @xmath85 of the selected expert @xmath62 ( jointly for exploration and exploitation ) by a monte - carlo simulation",
    ". then always learn a ( close to ) unbiased estimate @xmath86 .",
    "the analysis of @xmath2 works in the same way for this modification , however not resulting in better bounds . on the other hand , we will see that modified @xmath2 learns faster .    in case of a non - uniform prior and possibly infinitely many experts ,",
    "the exploration must be according to the prior weights .",
    "this causes another problem : @xmath2 s loss estimates @xmath84 need to be bounded , which forbids exploring experts with very small prior weights .",
    "hence we define for each expert @xmath62 , an _ entering time _",
    "@xmath87 ( at the master time scale ) .",
    "then @xmath2 ( including its subroutine @xmath69 ) is modified such that it uses only _ active _ experts from @xmath88 .",
    "this guarantees additionally that we have only a finite active set in each step , and the algorithm remains computationally feasible .",
    "[ th : foe ] assume @xmath2 acts in an online decision problem with bounded instantaneous losses @xmath89 $ ] .",
    "let the exploration rate be @xmath90 and the learning rate be @xmath91 .",
    "in case of uniform prior , choose @xmath92 . in case of",
    "arbitrary prior let @xmath93 and @xmath94 .",
    "then in case of uniform prior , for all experts @xmath62 and all @xmath65 we have e^_1:t & & _ 1:t^i+o ( n^2 t^-1/10 ) , + ^_1:t & & _",
    "1:t^i+ o(n^2 t^-1/10 ) 1-t^-2 .",
    "consequently , @xmath95 a.s . for non - uniform",
    "prior , corresponding assertions hold with @xmath96-terms replaced by @xmath97 .",
    "the proof of this main theorem on the performance of @xmath2 can be found in @xcite .",
    "similar bounds hold for larger @xmath98 .",
    "these bounds are improvable @xcite , and it is possible to prove any regret bound @xmath99 , at the cost of increasing @xmath100 where @xmath101 . in the simulations , we used @xmath102 for faster learning . for playing @xmath0 matrix games , we will use the class of all 16 deterministic four - state markov experts .",
    "that is , each expert consists of a lookup table with all the actions for each of the 4 possible combination of moves in the last round . in the first round , the expert plays uniformly random .",
    "( compare the standard results on learning matrix games with expert advice by @xcite . )",
    "[ sec : pd ] as already indicated , it is our goal to explore and compare the performance of the two universal learning approaches presented so far , in particular for problems which are not solved by passive or greedy learners .",
    "to this aim , repeated @xmath0 matrix games are well suited :    * they are simple , such that ( close to ) universal learning is computationally feasible even with brute - force implementation ; * they provide situations where optimal long - term behavior significantly differs from greedy behavior ( e.g.  prisoner s dilemma ) ; * moreover , we can observe how universal learners can exploit potentially weak adversaries ; * and finally , we can test the two universal learners against each other .",
    "we begin by describing the experimental setup and the universal learners .",
    "after that , we will discuss five @xmath0 matrix games , presenting experimental results and highlighting their interesting aspects .",
    "* setup . * a @xmath0 matrix game consists of two matrices @xmath103 , the first one containing rewards for the row player , the second one rewards for the column player .",
    "( it does not cause any problem that for convenience , we have developed the theory in terms of _ losses _ rather than rewards : one may be transformed into the other by simply inverting the sign .",
    "so for the discussion of the results , we will keep the rewards , as this is more standard in game theory . ) a _ single game _",
    "proceeds in the following way : the first player chooses a row action @xmath104 and simultaneously the second a column action @xmath105 , both players without knowing the opponent s move .",
    "then reward @xmath106 is payed to player @xmath107 ( @xmath108 ) , and @xmath62 and @xmath109 are revealed to both players .",
    "a repeated game consists of @xmath34 single games .",
    "we chose @xmath110 , if at least one opponent is @xmath2 ( which has slow learning dynamics , as we will see ) , and @xmath111 for the fast learning ai@xmath1  ( unless it is plotted in the same graph as @xmath2 ) .",
    "if at least one randomized player participates , the run is repeated 10 times , and usually the average is shown .",
    "we will consider only _ symmetric _ games , where one player , when put in the position of the other player ( i.e.when exchanging @xmath112 and @xmath113 ) , has a symmetric strategy ( maybe after exchanging the actions ) .",
    "we will meet precisely three types of symmetry : in the  matching pennies \" , @xmath114 after inverting the action of the row player , and in the  battle of sexes \" game , @xmath114 after inverting both players actions , and in all other games we @xmath115 ( transpose ) . in these latter games",
    ", we will call the action 0  defect \" and 1  cooperate \" .",
    "all games we consider have rewards in @xmath52 .",
    "the ai@xmath1  and @xmath2 agents are used as specified in the previous sections , with the classes of all two - state markov environments and all deterministic four - state markov experts , respectively . for ai@xmath1 , we will concentrate the presentation on the almost consistent horizon variant , since it performs always better than the moving horizon variant .",
    "for @xmath2 , we will concentrate on the faster learning variant .",
    "0 & 1,1 & 4,0 + 1 & 0,4 & 2,3 +     0 & 2,2 & 3,0 + 1 & 0,3 & 4,4 +     0 & 0,0 & 4,1 + 1 & 1,4 & 2,2 +     +     0 & 2,4 & 0,0 + 1 & 0,0 & 4,2 +     0 & 4,0 & 0,4 + 1 & 0,4 & 4,0 +    * prisoner s dilemma .",
    "* this dilemma is classical .",
    "the reward matrices are @xmath116 and @xmath117 , with the following interpretation : the two players are accused of a crime they have committed together .",
    "they are being interrogated separately .",
    "each player can either cooperate with the other player ( do nt tell the cops anything ) , or he defects ( tells the cops everything but blame the colleague ) .",
    "the punishments are according to the players joint decision : if none of them gives evidence , both get a minor sentence .",
    "if one gives evidence and the other one keeps quiet , the traitor gets free , while the other gets a huge sentence . if both give evidence , then they both get a significant sentence .",
    "( there is also an easier variant  deadlock \" which we not discuss . )",
    "it is clear that giving evidence , i.e.  defecting , is an instantaneously _ dominant _ action : regardless of what the opponent does , the immediate reward is always larger for defecting .",
    "however , if both players would agree to cooperate , this is the  social optimum \" and guarantees the better long - term reward in the repeated game .",
    "a well - known instance for this case is playing against the ",
    "tit for tat \" strategy strategy which cooperates in the first move and subsequently performs the action we did in the previous move .",
    "similar but harder to learn are  two tit for tat \" and  three tit for tat \" , which defect in the first move and cooperate only if we cooperated two respectively three times in a row . note that although  two tit for tat \" and  three tit for tat \" are not in ai@xmath1 s model class , probabilistic versions of the strategies are : if the probability of  adversary defected , i cooperate , then the adversary will cooperate in the next round \" is chosen correctly ( namely @xmath118 for 2-tit for tat and @xmath119 for 3-tit for tat ) , then the expected number of rounds i have to cooperate until the adversary will do so is 2 respectively 3 .",
    "figure [ fig : pdaixi ] shows that in most cases , ai@xmath1learns very quickly the best actions .",
    "( this is the consistent horizon variant , the moving horizon variant will be discussed with the next game , stag hunt . ) if the opponent is memoryless as for example the uniform random player , ai@xmath1  constantly defects after short time . against tit for tat and two",
    "tit for tat , ai@xmath1  cooperates after short time .",
    "the figure shows the average per round rate of cooperation , which after a few exploratory moves converges to the optimal action as @xmath120 .",
    "however , ai@xmath1  does not learn to cooperate against three tit for tat .",
    "the reason is the general problem that in order to increase exploration , ai@xmath1  needs exponential depth of the expectimin tree .",
    "assume that a certain action sequence of length @xmath5 is favorable against the true environment , which has however not too high a current weight .",
    "in this instance , cooperating three times in a row is favorable against ( the probabilistic version of ) 3-tit for tat . in order to recognize that this is worth exploring ,",
    "ai@xmath1  has to build a branch of depth @xmath121 in the expectimin tree , which has ( because of the relatively low prior weight ) very small probability @xmath122 however .",
    "then it needs an exponentially large subtree below this branch to accumulate enough ( virtual ) reward in order to encourage exploration .",
    "one more problem arises when ai@xmath1  plays against another ai@xmath1 . here",
    ", the perfectly symmetric setting results in both playing the same actions in each move , hence they are not correctly learning .",
    "we might try to remedy this by varying the tree depth of the second ai@xmath1  ( denoted ai@xmath12 in the figure ) , however it turns out that in this case , both ai@xmath1 s do not learn at all to cooperate ( see ( * ? ? ? *",
    "sec.8.5.2 ) for a possible reason ) .",
    "we now turn to the performance of @xmath2 ( the faster learning variant ) as evaluated in figure [ fig : pdfoe ] .",
    "as expected , @xmath2 learns much slower than ai@xmath1  ( note the different time scale ) . on the other hand ,",
    "its exploration is strong enough to learn 3-tit for tat ( and even harder instances ) .",
    "when playing against another instance of @xmath2 , we notice however that they usually do not succeed to overcome the dominance of mutual defection .",
    "also when @xmath2 competes with ai@xmath1 , they tend to learn mutual defection rather than cooperation .",
    "( sometimes , they learn cooperation in one or two of the possible states of the mdp . )",
    "* stag hunt .",
    "* this game is also known as  assurance \" .",
    "the reward matrices are @xmath123 and @xmath117 .",
    "two players are hunting together .",
    "if they cooperate , they will catch the stag .",
    "however , one player might not trust the other , in which case he chases rabbits on his own instead . in this case",
    ", the other one wo nt get anything if he tries to cooperate . if both defect , then they are in conflict , and each player gets less rabbits . although the optimum for both players is to cooperate , they need to trust each other sufficiently .",
    "if one player plays uniformly random , it is better for the other to go for the rabbits . also , defecting has the lower variance .",
    "maybe it is surprising to observe that ai@xmath1  ( with a depth of 8) does not learn to cooperate against 2-tit for tat ( figure [ fig : shaixi ] ) .",
    "the reason is that defecting has a relatively good payoff , and therefore exploration is not encouraged as discussed in the previous subsection . if the depth of the tree is increased to 9 , ai@xmath1  learns cooperation against 2-tit for tat ( but not against 3-tit for tat ) .",
    "we also see that the moving horizon variant of ai@xmath1  has even more problems with exploration : it does not learn cooperating against 2-tit for tat , even with depth 9 .",
    "the explanation is that even if ai@xmath1  decides to explore in one time step , in the next step this exploration might not be correctly continued , as the tree is now explored to a different level .",
    "this observation can also be made for the prisoners dilemma .",
    "in fact , the consistent horizon variant performs _ always _ better than moving horizon .",
    "as before , @xmath2 learns much slower but explores more robustly ( figure [ fig : shfoe ] ) , neither 2- nor 3-tit for tat are a problem .",
    "unlike in the prisoner s dilemma , if ai@xmath1and @xmath2 are competing , they learn mutual cooperation in almost half of the cases , an average over such lucky instances is given in the figure .",
    "the same is valid for @xmath2 against @xmath2 , while ai@xmath1  against ai@xmath1  has the same symmetry problem as already observed in the prisoner s dilemma .",
    "the original slower learning variant of @xmath2 reaches the same average level of performance only after @xmath124 time steps instead of @xmath125 steps , and moreover with a variance twice as high .",
    "* chicken . *",
    "the reward matrices @xmath126 and @xmath117 of the  chicken \" game ( also known as  hawk and dove \" ) can be interpreted as follows : two coauthors write a paper , but each tries to spend as little effort as possible .",
    "if one succeeds to let the other do the whole work , he has a high reward .",
    "on the other hand , if no one does anything , there will be no paper and thus no reward .",
    "finally , if both decide to cooperate , both get some reward . here , in the repeated game , it is socially optimal to take turns cooperating and defecting .",
    "still the best situation for one player is if he emerges as the ",
    "dominant defector \" , defecting in most or all of the games , while the other one cooperates .",
    "if the opponent steadily alternates between cooperating and defecting , then ai@xmath1  quickly learns to adapt .",
    "this can be observed in figure [ fig : chickenaixi ] , where the performance is given in terms of average per round reward instead of cooperation rate .",
    "however , ai@xmath1  is not obstinate enough to perform well against a  stubborn \" adversary that would cooperate only after his opponent has defected for three successive time steps . here ,",
    "ai@xmath1  learns to cooperate , leaving his opponent the favorable role as the dominant defector .",
    "( however , ai@xmath1  learns to dominate the less stubborn adversary which cooperates after two defecting actions . )",
    "when two ai@xmath1s play against each other , they again have the symmetry problem .",
    "interestingly , if we break symmetry by giving the second ai@xmath1  a depth of 9 , he will turn out the dominant defector ( not shown in the graph ) .",
    "@xmath2 behaves differently in this game ( figure [ fig : chickenfoe ] ) .",
    "while he learns to deal with the steadily alternating adversary and emerges as the dominant defector against the stubborn one , he would give precedence to ai@xmath1  in most cases .",
    "this is not hard to explain , since @xmath2 in the beginning plays essentially random .",
    "thus ai@xmath1  learns quickly to defect , and for @xmath2 remains nothing but learning to cooperate .",
    "however , this does not always happen : in the minority of the cases , @xmath2 defects enough such that ai@xmath1decides to cooperate , and @xmath2 will be the dominant defector .",
    "( hence the average shown in the graph is less clear in favor of ai@xmath1 . )    * battle of sexes . * in this game , a married couple wants to spend the evening together , but they did nt settle if they would go to the theater ( her preference ) or the pub ( his preference ) . however , if they fail to meet , both have a boring evening ( and no reward at all ) .",
    "the reward matrices are @xmath127 and @xmath128 .",
    "coordination is clearly important in the repeated game .",
    "like in  chicken \" , taking turns is a social optimum , while it is best for one player if his choice becomes dominant .    in figure",
    "[ fig : bos ] , our universal learners show similar performance like in the chicken game . both learn to deal with an alternating partner .",
    "@xmath2 also learns to dominate over a stubborn adversary which plays his less favorite action only after the opponent insists three times on that .",
    "ai@xmath1  is dominated by this stubborn player .",
    "however , ai@xmath1  always dominates @xmath2 . finally , in contrast to the chicken game , ai@xmath1  against ai@xmath1  does not have the symmetry problem , but they both learn to alternate .    [",
    "fig : mp ]    * matching pennies . *",
    "each player conceals in his palm a coin with either heads or tails up .",
    "they are revealed simultaneously . if they match ( both heads or both tails )",
    ", the first player wins , otherwise the second .",
    "this is the only zero - sum game of the games we consider , where @xmath129 and @xmath130 .",
    "thus , there is a minimax strategy for both players , which is actually uniform random play . on the other hand ,",
    "deterministic repeated play is potentially exploitable by the adversary .",
    "figure [ fig : mp ] shows the results for this last game we present .",
    "both ai@xmath1  and @xmath2 learn to exploit a predictable adversary , namely the player alternating between 0 and 1 .",
    "the other games are balanced in the long run , only in the beginning ai@xmath1  succeeds to exploit @xmath2 a little . if two ai@xmath1s compete , it is important to break symmetry , then both learn to alternate ( this situation is shown in the graph ) .",
    "if symmetry is not broken , the row player ( who tries to match ) always wins .",
    "altogether , universal learners perform well in repeated matrix games . they usually learn to prefer the optimal long - term action to greedy behavior ( prisoner s dilemma and stag hunt ) . if possible they are able to exploit a predictable adversary ( matching pennies ) . and they learn good strategies when it is necessary to foresee the opponent s action ( chicken and battle of sexes ) . of the two approaches we presented and compared , ai@xmath1  learns much faster than @xmath2 , but @xmath2 explores more thoroughly . of course , there is a trade - off between exploration and fast learning .",
    "interestingly , it may depend on the adversary ( and thus on the environment ) if fast learning or exploration is the better _ long - term _ strategy : in chicken and battle of sexes , ai@xmath1  profits against @xmath2 by learning fast and dictating its preferred action , but looses against the stubborn opponent because of not exploring enough .        d.  pucci de  farias and n.  megiddo . how to combine expert ( and novice )",
    "advice when actions impact the environment ?",
    "in sebastian thrun , lawrence saul , and bernhard schlkopf , editors , _ advances in neural information processing systems 16_. mit press , cambridge , ma , 2004 .",
    "m.  hutter .",
    "self - optimizing and pareto - optimal policies in general environments based on bayes - mixtures . in _ proc .",
    "15th annual conference on computational learning theory ( colt 2002 ) _ , lecture notes in artificial intelligence , pages 364379 , sydney , australia , july 2002 . springer ."
  ],
  "abstract_text": [
    "<S> we study and compare the learning dynamics of two universal learning algorithms , one based on bayesian learning and the other on prediction with expert advice . both approaches have strong asymptotic performance guarantees . </S>",
    "<S> when confronted with the task of finding good long - term strategies in repeated @xmath0 matrix games , they behave quite differently . </S>"
  ]
}