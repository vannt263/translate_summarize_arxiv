{
  "article_text": [
    "let @xmath0 be a @xmath1 random vector and @xmath2 be a random variable .",
    "then , @xmath0 and @xmath3 are said to follow a _ normal - gamma distribution _",
    "( ng distribution ) , if their joint probability density function is given by    @xmath4    where @xmath5 denotes a multivariate normal density with mean @xmath6 and covariance @xmath7 and @xmath8 denotes a gamma density with shape @xmath9 and rate @xmath10 . in full , the density function is given by ( koch , 2007 , p.  55 )    @xmath11 \\cdot \\frac{{b}^{a}}{\\gamma(a ) } \\ , y^{a-1 } \\ , \\exp[-b y ] \\ ; .\\ ] ]    the _ kullback - leibler divergence _",
    "( kl divergence ) is a non - symmetric distance measure for two probability distributions @xmath12 and @xmath13 and is defined as    @xmath14 = \\sum_{i \\in \\omega } p(i ) \\ , \\ln \\frac{p(i)}{q(i ) } \\ ; .\\ ] ]    for continuous probability distribtions @xmath12 and @xmath13 with probability density functions @xmath15 and @xmath16 on the same domain @xmath17 , it is given by ( bishop , 2006 , p.  55 )    @xmath18 = \\int_{x } p(x ) \\",
    ", \\ln \\frac{p(x)}{q(x ) } \\ , \\mathrm{d}x \\ ; .\\ ] ]    the kl divergence becomes important in information theory and statistical inference .",
    "here , we derive the kl divergence for two ng distributions with vector - valued @xmath0 and real - positive @xmath3 and provide two examples of its application .",
    "first , consider two multivariate normal distributions over the @xmath1 vector @xmath0 specified by    @xmath19    according to equation ( [ eq : cont - kl ] ) , the kl divergence of @xmath12 from @xmath13 is defined as    @xmath20 = \\int_{\\mathbb{r}^k } \\mathrm{n}(x ; \\mu_1 , \\sigma_1 ) \\ , \\ln \\frac{\\mathrm{n}(x ; \\mu_1 , \\sigma_1)}{\\mathrm{n}(x ; \\mu_2 , \\sigma_2 ) } \\ , \\mathrm{d}x \\ ; .\\ ] ]    using the multivariate normal density function    @xmath21 \\ ; , \\ ] ]    it evaluates to ( duchi , 2014 )    @xmath22 = \\frac{1}{2 } \\left [ ( \\mu_2 - \\mu_1)^t \\sigma_2^{-1 } ( \\mu_2 - \\mu_1 ) + \\mathrm{tr}(\\sigma_2^{-1 } \\sigma_1 ) - \\ln \\frac{|\\sigma_1|}{|\\sigma_2| } - k \\right ] \\ ; .\\ ] ]      next , consider two univariate gamma distributions over the real - positive @xmath3 specified by    @xmath23    according to equation ( [ eq : cont - kl ] ) , the kl divergence of @xmath12 from @xmath13 is defined as    @xmath20 = \\int_{0}^{\\infty } \\mathrm{gam}(y ; a_1 , b_1 ) \\ , \\ln \\frac{\\mathrm{gam}(y ; a_1 , b_1)}{\\mathrm{gam}(y ; a_2 , b_2 ) } \\ , \\mathrm{d}y \\ ; .\\ ] ]    using the univariate gamma density function    @xmath24 \\quad \\text{for } \\quad y > 0 \\ ; , \\ ] ]    it evaluates to ( penny , 2001 )    @xmath25 = a_2 \\ , \\ln \\frac{b_1}{b_2 } - \\ln \\frac{\\gamma(a_1)}{\\gamma(a_2 ) } + ( a_1 - a_2 ) \\ ,",
    "\\psi(a_1 ) - ( b_1 - b_2 ) \\ , \\frac{a_1}{b_1}\\ ] ]    where @xmath26 is the digamma function .      now , consider two normal - gamma distributions over @xmath0 and @xmath3 specified by    @xmath27    according to equation ( [ eq : cont - kl ] ) , the kl divergence of @xmath12 from @xmath13 is defined as    @xmath28 = \\int_{0}^{\\infty } \\int_{\\mathbb{r}^k } p(x , y ) \\",
    ", \\ln \\frac{p(x , y)}{q(x , y ) } \\ , \\mathrm{d}x \\ , \\mathrm{d}y \\ ; .\\ ] ]    using the law of conditional probability , it can be evaluated as follows :    @xmath29 & = \\int_{0}^{\\infty } \\int_{\\mathbb{r}^k } p(x|y ) p(y ) \\ , \\ln \\frac{p(x|y ) p(y)}{q(x|y ) q(y ) } \\ , \\mathrm{d}x \\ , \\mathrm{d}y \\\\ & = \\int_{0}^{\\infty } p(y ) \\int_{\\mathbb{r}^k } p(x|y ) \\ , \\ln \\frac{p(x|y)}{q(x|y ) } \\ , \\mathrm{d}x \\ , \\mathrm{d}y \\\\ & + \\int_{0}^{\\infty } p(y ) \\ , \\ln \\frac{p(y)}{q(y ) } \\int_{\\mathbb{r}^k } p(x|y ) \\ , \\mathrm{d}x \\ , \\mathrm{d}y \\\\ & = \\left\\langle \\mathrm{kl}[p(x|y)||q(x|y ) ] \\right\\rangle_{p(y ) } + \\mathrm{kl}[p(y)||q(y ) ] \\end{split}\\ ] ]    in other words , the kl divergence for two normal - gamma distributions over @xmath0 and @xmath3 is equal to the sum of a multivariate normal kl divergence regarding @xmath0 conditional on @xmath3 , expected over @xmath3 , and a univariate gamma kl divergence regarding @xmath3 .",
    "together with equation ( [ eq : mvn - kl ] ) , the first term becomes    @xmath30 \\right\\rangle_{p(y ) } \\\\ & = \\left\\langle \\frac{1}{2 } \\left [ ( \\mu_2 - \\mu_1)^t ( y \\lambda_2 ) ( \\mu_2 - \\mu_1 ) + \\mathrm{tr}\\left ( ( y \\lambda_2 ) ( y \\lambda_1)^{-1 } \\right ) - \\ln \\frac{|(y \\lambda_1)^{-1}|}{|(y \\lambda_2)^{-1}| } - k \\right ] \\right\\rangle_{p(y ) } \\\\ & = \\left\\langle \\frac{y}{2 } ( \\mu_2 - \\mu_1)^t \\lambda_2 ( \\mu_2 - \\mu_1 ) + \\frac{1}{2 } \\ , \\mathrm{tr}(\\lambda_2 \\lambda_1^{-1 } ) - \\frac{1}{2 } \\ln \\frac{|\\lambda_2|}{|\\lambda_1| } - \\frac{k}{2 } \\right\\rangle_{p(y ) } \\ ; .",
    "\\end{split}\\ ] ]    using the relation @xmath31 , we have    @xmath32 \\right\\rangle_{p(y ) } = \\frac{1}{2 } \\frac{a_1}{b_1 } ( \\mu_2 - \\mu_1)^t \\lambda_2 ( \\mu_2 - \\mu_1 ) + \\frac{1}{2 } \\",
    ", \\mathrm{tr}(\\lambda_2 \\lambda_1^{-1 } ) - \\frac{1}{2 } \\ln \\frac{|\\lambda_2|}{|\\lambda_1| } - \\frac{k}{2 } \\ ; . \\end{split}\\ ] ]    thus , from ( [ eq : exp - mvn - kl ] ) and ( [ eq : gam - kl ] ) , the kl divergence in ( [ eq : ng - kl1 ] ) becomes    @xmath33 & = \\frac{1}{2 } \\frac{a_1}{b_1 } \\left [ ( \\mu_2 - \\mu_1)^t \\lambda_2 ( \\mu_2 - \\mu_1 ) \\right ] + \\frac{1}{2 } \\ , \\mathrm{tr}(\\lambda_2 \\lambda_1^{-1 } ) - \\frac{1}{2 } \\ln \\frac{|\\lambda_2|}{|\\lambda_1| } - \\frac{k}{2 } \\\\ & + a_2 \\ , \\ln \\frac{b_1}{b_2 } - \\ln \\frac{\\gamma(a_1)}{\\gamma(a_2 ) } + ( a_1 - a_2 ) \\ ,",
    "\\psi(a_1 ) - ( b_1 - b_2 ) \\ , \\frac{a_1}{b_1 } \\ ; . \\end{split}\\ ] ]      consider bayesian inference on data @xmath3 using model @xmath34 with parameters @xmath35 . in this case ,",
    "bayes theorem is a statement about the posterior density :    @xmath36    the denominator @xmath37 acts as a normalization constant on the posterior density @xmath38 and according to the law of marginal probability is given by    @xmath39    this is the probability of the data given only the model , regardless of any particular parameter values .",
    "it is also called `` marginal likelihood '' or `` model evidence '' and can act as a model quality criterion in bayesian inference , because parameters are integrated out of the likelihood .    for computational reasons ,",
    "only the logarithmized or log model evidence ( lme ) @xmath40 is of interest in most cases . by rearranging equation ( [ eq : bt ] )",
    ", the model evidence can be represented as    @xmath41    logarithmizing both sides of the equation and taking the expectation with respect to the posterior density over model parameters @xmath35 gives the lme    @xmath42    using this reformulation , the lme as a model quality measure can be naturally decomposed into an accuracy term , the posterior expected likelihood , and a complexity term , the kl divergence between the posterior and the prior distribution :    @xmath43 \\end{split}\\ ] ]    intuitively , the accuracy acts increasing and the complexity acts decreasing on the log model evidence .",
    "this reflects the capability of the lme to select models that achieve the best balance between accuracy and complexity , i.e.  models that explain the observations sufficiently well ( high accuracy ) without employing too many principles ( low complexity ) .",
    "the fact that the complexity term is a kl divergence between posterior and prior means that models with prior assumptions that are close to the posterior evidence receive a low complexity penalty , because one is not surprised very much when accepting such a model which renders the bayesian complexity a measure of surprise .      consider multiple linear regression using the univariate general linear model ( glm )    @xmath44    where @xmath3 is an @xmath45 vector of measured data , @xmath17 is an @xmath46 matrix called the design matrix , @xmath47 is a @xmath48 vector of weight parameters called regression coefficients and @xmath49 is an @xmath45 vector of errors or noise .",
    "these residuals are assumed to follow a multivariate normal distribution whose covariance matrix is the product of a variance factor @xmath50 and an @xmath51 correlation matrix @xmath52 .",
    "usually , @xmath17 and @xmath52 are known while @xmath47 and @xmath53 are unknown parameters to be inferred via model estimation .    for mathematical convenience ,",
    "we rewrite @xmath54 and @xmath55 so that equation ( [ eq : glm ] ) implies the following likelihood function :    @xmath56    the conjugate prior relative to this likelihood function is a normal - gamma distribution on the model parameters @xmath47 and @xmath53 ( koch , 2007 , ch .",
    "2.6.3 ) :    @xmath57    due to the conjugacy of ( [ eq : glm - ng - prior ] ) to ( [ eq : glm - lf ] ) , the posterior is also a normal - gamma distribution    @xmath58    where the posterior parameters in ( [ eq : glm - ng - post ] ) are given by ( koch , 2007 , ch .",
    "4.3.2 )    @xmath59    from ( [ eq : lme2 ] ) , the complexity for the model defined by ( [ eq : glm - lf ] ) and ( [ eq : glm - ng - prior ] ) is given by    @xmath60 \\ ; .\\ ] ]    in other words , the complexity penalty for a general linear model with normal - gamma priors ( glm - ng ) is identical to a kl divergence between two ng distributions and using ( [ eq : ng - kl2 ] ) can be written in terms of the prior and posterior parameters as    @xmath61 + \\frac{1}{2 } \\",
    ", \\mathrm{tr}(\\lambda_0 \\lambda_n^{-1 } ) - \\frac{1}{2 } \\ln \\frac{|\\lambda_0|}{|\\lambda_n| } - \\frac{p}{2 } \\\\ & + a_0 \\ , \\ln \\frac{b_n}{b_0 } - \\ln \\frac{\\gamma(a_n)}{\\gamma(a_0 ) } + ( a_n - a_0 ) \\ , \\psi(a_n ) - ( b_n - b_0 ) \\",
    ", \\frac{a_n}{b_n } \\ ; . \\end{split}\\ ] ]",
    "consider a linear model with polynomial basis functions ( bishop , 2006 , p.  5 ) given by    @xmath62    essentially , this model assumes that @xmath3 is an additive mixture of polynomial terms @xmath63 weighted with the coefficients @xmath64 with @xmath65 where the natural number @xmath66 is called the model order .",
    "this means that @xmath67 corresponds to a constant value ( plus noise @xmath49 ) ; @xmath68 corresponds to a linear function ; @xmath69 corresponds to a quadratic pattern ; @xmath70 corresponds to a 3rd degree polynomial etc .",
    "given that @xmath0 is an @xmath45 vector of real numbers between @xmath71 and @xmath72 , this model can be rewritten as a glm given in equation ( [ eq : glm ] ) with    @xmath73    based on this reformulation , we simulate polynomial data .",
    "we perform @xmath74 simulations with @xmath75 data points in each simulation .",
    "we generate simulated data based on a true model order @xmath76 and analyze these data using a set of models ranging from @xmath77 to @xmath78 .",
    "the predictor @xmath0 is equally spaced between @xmath71 and @xmath72 and design matrices @xmath79 are created according to equation ( [ eq : pbf2 ] ) .    in each simulation , six regression coefficients @xmath80 are drawn independently from the standard normal distribution @xmath81 .",
    "then , gaussian noise @xmath49 is sampled from the multivariate normal distribution @xmath82 with a residual variance of @xmath83 .",
    "finally , simulated data is generated as as @xmath84 .",
    "then , for each @xmath85 , bayesian model estimation is performed using the design matrix @xmath79 , a correlation matrix @xmath86 and the prior distributions ( [ eq : glm - ng - prior ] ) with the prior parameters @xmath87 , @xmath88 invoking a standard multivariate normal distribution and @xmath89 , @xmath90 invoking a relatively flat gamma prior .",
    "posterior parameters are calculated using equation ( [ eq : glm - ng - post - par ] ) and give rise to the model complexity via ( [ eq : glm - ng - com2 ] ) as well as model accuracy and the log model evidence via ( [ eq : lme2 ] ) .",
    "average lmes , accuracies and complexities are shown in figure  1 .",
    "one can see that the true model order is correctly identified by the maximal log model evidence .",
    "this is achieved by an increasing complexity penalty which outweighs the saturating accuracy gain for models with @xmath91 .",
    "this demonstrates that the kl divergence for the ng distribution can be used to select polynomial basis functions when basis sets can not be separated based on model accuracy alone .",
    "[ fig : figure_pbf ]     * figure 1 . * bayesian model selection for polynomial basis functions .",
    "all displays have model order on the x - axis and average model quality measures ( across simulations ) on the y - axis .",
    "intuitively , the model accuracy ( middle panel ) increases with model order , but saturates at around @xmath92 with no major increase after @xmath93 .",
    "moreover , the model complexity ( lower panel )  which is the kl divergence between prior and posterior distribution  also grows with model order , but switches to a linear increase at around @xmath93 reaching a value of @xmath94 at @xmath95 .",
    "together , this has the consequence that the log model evidence ( upper panel ) is maximal for @xmath93 ( black cross ) where exact values are : @xmath96 .      in neuroimaging , especially functional magnetic resonance imaging ( fmri ) , glms as given by equation ( [ eq : glm ] )",
    "are applied to time series of neural data @xmath3 ( friston et al . , 1995 ) .",
    "the design matrix @xmath17 is specified by the temporal occurrence of experimental conditions and the covariance structure @xmath52 is estimated from residual auto - correlations .",
    "model estimation and statistical inference are performed `` voxel - wise '' , i.e.  separately for each measurement location in the brain , usually referred to as the `` mass - univariate glm '' .    here",
    ", we analyze data from a study on orientation pop - out processing ( bogler et al . , 2013 ) .",
    "during the experimental paradigm , the screen showed a @xmath97 array of homogeneous bars oriented either 0 , 45 , 90 or 135 relative to the vertical axis .",
    "this background stimulation changed every second and was interrupted by trials in which one target bar on the left and one target bar on the right were independently rotated either 0 , 30 , 60 or 90 relative to the rest of the stimulus display .",
    "those trials of orientation contrast ( oc ) lasted 4 seconds and were alternated with inter - trial intervals of 7 , 10 or 13 seconds .",
    "each combination of oc on the left side and oc on the right side was presented three times resulting in 48 trials in each of the 5 sessions lasting 672 seconds .",
    "after fmri data preprocessing ( slice - timing , realignment , normalization , smoothing ) , two different models of hemodynamic activation were applied to the fmri data . the first model ( glm i ) considers the experiment a factorial design with two factors ( left vs. right oc ) having four levels ( 0 , 30 , 60 , 90 ) .",
    "this results in @xmath98 possible combinations or experimental conditions modelled by @xmath99 onset regressors convolved with the canonical hemodynamic response function ( hrf ) . the second model ( glm ii )",
    "puts all trials from all conditions into one hrf - convolved regressor and encodes orientation contrast using a parametric modulator ( pm ) that is given as @xmath100 with @xmath101 , resulting in @xmath102 , such that the parametric modulator is proportional to orientation contrast .",
    "there was one pm for each factor of the design , i.e.  one pm for left oc and one pm for right",
    "note that both models encode the same information and that every signal that can be identified using glm ii can also be characterized using glm i , but not vice versa , because the first model allows for a greater flexibility of activation patterns across experimental conditions than the second .",
    "for these two models , we performed bayesian model estimation . to overcome the challenge of having to specify prior distributions on the model parameters , we apply cross - validation across fmri sessions .",
    "this gives rise to a cross - validated log model evidence ( cvlme ) as well as cross - validated accuracies and complexities for each model in each subject .",
    "we then performed a paired t - test to find voxels where glm ii has a significantly higher cvlme than glm i. due to the specific assumptions in glm ii and the higher flexibility of glm i , we assumed that these differences might be primarily based on a complexity advantage of glm ii over glm i.    we focus on visual area 4 ( v4 ) that is known to be sensitive to orientation contrast . within left v4 , specified by a mask from a separate localizer paradigm ( bogler et al . ,",
    "2013 ) , we identified the peak voxel ( @xmath103 = [ -15 , -73 , -5]$ ]  mm ) defined by the maximal t - value ( @xmath104 ) and extracted log model evidence as well as model accuracy and model complexity from this voxel for each subject .",
    "differences in lme , accuracy and complexity are shown in figure  2 .",
    "again , the model complexity enables a model selection that would not be possible based on the model accuracy alone .",
    "[ fig : figure_nms ]     * figure 2 . * bayesian model selection for orientation pop - out processing .",
    "all displays have subject on the x - axis and difference in model qualities ( @xmath105 = glm i , @xmath106 = glm ii ) on the y - axis .",
    "interestingly , there is a slight disadvantage for glm ii regarding only the model accuracy ( middle panel ) , its mean difference across subjects being smaller than zero .",
    "however , model complexity ( lower panel )  measured as the kl divergence between prior and posterior distribution  is consistently higher for glm i. together , this has the consequence that the log model evidence ( upper panel ) most often favors glm ii .",
    "average values are : @xmath107 .",
    "we have derived the kullback - leibler divergence of two normal - gamma distributions using earlier results on the kl divergence for multivariate normal and univariate gamma distributions . moreover , we have shown that the kl divergence for the ng distribution occurs as the complexity term in the univariate general linear model when using conjugate priors .",
    "analysis of simulated and empirical data demonstrates that the complexity penalty has the desired theoretical features , namely to quantify the relative informational content of two generative models and to detect model differences that can not be detected by just relying on model accuracy , e.g.  given by the maximum log - likelihood ( as in information criteria like aic or bic ) or the posterior log - likelihood ( as in the bayesian log model evidence ) .",
    "9          friston kj , holmes ap , worsley kj , poline jp , frith cd , frackowiak rsj ( 1995 ) : `` statistical parametric maps in functional imaging : a general linear approach '' .",
    "_ human brain mapping _ , vol .  2 , iss .  4 , pp .  189 - 210"
  ],
  "abstract_text": [
    "<S> we derive the kullback - leibler divergence for the normal - gamma distribution and show that it is identical to the bayesian complexity penalty for the univariate general linear model with conjugate priors . based on this finding </S>",
    "<S> , we provide two applications of the kl divergence , one in simulated and one in empirical data .    </S>",
    "<S> kullback - leibler divergence + for the normal - gamma distribution + joram soch^1,3,^ & carsten allefeld^1,2^    ^1^ bernstein center for computational neuroscience , berlin , germany + ^2^ berlin center for advanced neuroimaging , berlin , germany + ^3^ department of psychology , humboldt - universitt zu berlin , germany +  corresponding author : joram.soch@bccn-berlin.de . </S>"
  ]
}