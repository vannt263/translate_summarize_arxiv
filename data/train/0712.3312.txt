{
  "article_text": [
    "complex networks have been studied extensively in recent years in the fields of mathematics , physics , computer science , biology , sociology , etc @xcite .",
    "various networks are used to model and analyze real world objects and their interactions with each other .",
    "for example , in sociology , airports and airflights that connect them can be represented by a network @xcite ; in biology , yeast reactions is also modeled by network @xcite ; etc . the mathematical terminology for a network",
    "is conveniently described in the language of graph theory .",
    "a common encoding of graphs uses an adjacency matrix , or an edge list , when the adjacency matrix is sparse @xcite .",
    "however , even for a large network the edge list contains a large information storage . in the case",
    "that some important network is transfered frequently between computers , it will save time and cost if there is a scheme to efficiently encode , and therefore compress the network first . fundamentally we find it a relevant issue to ask how much information is necessary to present a given network , and how symmetry can be exploited to this end .    in this paper",
    "we will demonstrate one way to reduce the information storage of a network by using the idea that habitually graphs have many nodes that share many common neighbors .",
    "so instead of recording all the links we could rather just store some of them and the difference between neighbors .",
    "the ideal compression ratio using this scheme will be @xmath0 where @xmath1 is the average degree of the network , compared to the standard compression using yale sparse matrix format @xcite which gives @xmath2 . in practice",
    "this ratio is not attainable but the real compression ratio is still better than using ysmf as shown by our results .",
    "a graph @xmath3 is a set of vertices ( or nodes ) @xmath4 together with edges ( or links ) @xmath5 which are the connected pairs .",
    "graphs are often used to model networks .",
    "it is sometimes convenient to call the vertices that connect to a vertex @xmath6 in a graph to be the neighbors of @xmath6",
    ". we will only consider undirected and unweighted graph in this paper .",
    "a drawing as in figure [ fignetwork ] allows us to directly visualize the graph ( i.e. the nodes and the connections between them ) , but a truism that anyone who works with real world graphs from real data knows is that commonly those graphs are so large that even a drawing will not give any insight . visualizing structure in graphs of such sizes ( @xmath7 to @xmath8 ) begs for some computer assistance .",
    "an _ adjacency matrix _ is a common , although inefficient data representation of a graph .",
    "the adjacency matrix @xmath9 of a graph @xmath3 is a @xmath10 square matrix where @xmath11 is the number of vertices of the graph and the entries @xmath12 of @xmath9 are defined by : @xmath13    for example , the adjacency matrix @xmath9 for the graph in figure [ fignetwork ] is @xmath14.\\ ] ]    however , in the case that the number of edges in a graph are so few that the corresponding adjacency matrix is sparse , the _ edge list _ will be used instead .",
    "the edge list is a list of all the pairs of nodes that form edges in a graph .",
    "it is essentially the same as the edge set @xmath15 for a graph @xmath3 . using edge list @xmath16 to represent the same graph as above we will have : @xmath17    note here that in the edge list we actually record the label of nodes for each edge in the graph , so for undirected graph",
    ", we can exchange the order for each pair of nodes .",
    "we will only consider sparse simple graphs , whose adjacency matrices will thus be binary sparse matrices , and the standard information storage for such graphs or matrices will be the information units that are needed for the corresponding edge list ( or two dimensional arrays ) .",
    "we now sharpen the definition for the _ unit of information _ in our context . from the perspective of information theory , a message which contains",
    "@xmath11 different symbols will require @xmath18 bits for each symbol , without any further coding scheme .",
    "the edge list representation is one example of a text file which contains @xmath11 different symbols ( often represented by natural numbers from @xmath19 to @xmath11 ) for a graph containing @xmath11 vertices .",
    "note that the unit of information depends only on the number of symbols that appear in the message , i.e. the number of vertices in a graph , so for any given graph this will be a fixed number .",
    "thus , when we restrict the disscussion to any particular graph , it is convenient to assume that each pair of labels in the edge list requires one information unit without making explicit what is the size of that unit .",
    "for example , the above graph requires @xmath20 information units . in this paper",
    "we will focus on how to represent the same graph using fewer information units than its original representation .",
    "as a motivating example , let us consider the following graph and its edge list .",
    "note that here the neighbors of node @xmath19 are almost the same as those of node @xmath21 .",
    "the edge list @xmath16 for this graph will be : @xmath22    this requires @xmath23 information units for the edge list .",
    "however , if we look back to the graph , we note that in this graph there are many common neighbors between node @xmath19 and node @xmath21 , so there is a great deal of information redundancy . considering the subgraphs ,",
    "the neighbors of node @xmath19 are almost the same as the neighbors of node @xmath21 , except that node @xmath24 links to @xmath19 , but not @xmath21 , while node @xmath25 links to @xmath21 , but not @xmath19 .",
    "taking the redundancy into account , we generate a new way to describe the same graph , exploiting the graphs . in the graph of figure [ figp ]",
    ", we see that the subgraph including vertices @xmath26 is very similar to the subgraph including vertices @xmath27 , see figure [ subgraphs ] .",
    "we exploit this redundancy in our coding .",
    "we store the subgraph which only consists of node 1 , and all its neighbors .",
    "then , we add just two more parameters , @xmath28 and @xmath29 that allows us to reconstruct the original graph . here the ordered pair @xmath30 tells us that in order to reconstruct the original graph we need to first copy node @xmath19 to node @xmath21 . by copy , we mean the addition of a new node into the exsiting graph with label @xmath21 , and then linking all the neighbors of node @xmath19 to the new node @xmath21 .    .",
    "copy from node @xmath19 to node @xmath21.,width=405,height=183 ]    the set @xmath31 tells us that we should then delete the link that connects the new node 2 and 3 and add a new link between 2 and 10 .",
    ".,width=480,height=192 ]    after all these operations we see that we successfully reconstruct the graph with fewer information units , in this case , nearly half as many as the original edge list .",
    "so instead of equation ( [ exedges ] ) , we may use the edge list of the subgraph @xmath32 as well as two sets @xmath33 to represent the same graph .     and",
    "@xmath34.,width=403,height=211 ]    the above example suggests that by exploiting symmetry of the graph , we might be able to reduce the information storage for certain graphs by using a small subgraph as well as @xmath35 and @xmath34 as defined above .",
    "however , there remains the question of how to choose the pair of vertices so that we actually reduce the information , and which is the best possible pair ?",
    "it is important to answer these questions since most of the graphs are so large that we never will be able to see the symmetry just by inspection as we did for the above toy example .    in the following",
    "we answer the first question , and partly the second , by using a greedy algorithm . in section 3",
    "we will define information redundancy for a binary sparse matrix and show that it reveals the neighbor similarity between vertices in a graph which is represented by its corresponding adjacency matrix . then in section 4 we will give a detailed description of our algorithm which allows us to implement our main idea .",
    "then in section 5 we will show some examples of these applications followed by discussion in section 6 .",
    "the graphs we seek to compress are typically represented by large sparse adjacency matrices .",
    "an edge - list is a specific data structure for representing such matrices , to reduce information storage .",
    "we will consider the edge - list form to be the standard way of storing sparse matrices , which requires @xmath36 units of information for a graph with @xmath36 edges .",
    "there are approaches of compressing sparse matrices , among which the most general is the yale sparse matrix format @xcite , which does not make any assumption on the structure of the matrix and only requires @xmath37 units of information .",
    "there are other approaches , such as @xcite which emphasize not only the storage but also the cost for data access time .",
    "we will focus on the data storage , so the yale format will be considered as a basic benchmark approach for compression of a sparse matrix , to which we will compare our results .",
    "the yale format yields the compression ratio : @xmath38 where @xmath39 is the average degree of the graph .",
    "we will show our approach of compressing the sparse matrices by first illustrating how the redundancy of a binary sparse matrix will be defined regarding to our specific operation on the matrix .",
    "generally , the adjacency matrix is a binary sparse matrix , @xmath40 where @xmath41 equals @xmath42 or @xmath19 indicating the connectivity between node @xmath6 and @xmath43 . for a simple graph consisting of @xmath36 edges this matrix has @xmath44 nonzero entries , but since it is symmetric only half of them are necessary to represent the graph , which yields @xmath36 units of information for the edge - list .",
    "now , if two nodes @xmath6 and @xmath43 in the graph share a lot of similar neighbors , in the adjacency matrix row @xmath6 and row @xmath43 will have a lot of common column entries , and likewise for column @xmath6 and column @xmath43 ( due to the symmetry of the matrix ) .",
    "suppose that we apply the operation to the graph , mentioned in the last section , by choosing @xmath45 and the corresponding @xmath34 , we will not need row @xmath43 and column @xmath43 in the matrix , to represent the graph .",
    "the number of nonzero entries in row @xmath43 and column @xmath43 is @xmath46 where @xmath47 is the degree of node @xmath43 in the graph . by doing that , the number of nonzero entries in the new adjacency matrix becomes @xmath48 , which requires @xmath49 units of information .",
    "however , the extra information we have to record is encoded in @xmath35 and @xmath34 .",
    "@xmath35 always has two entries , which requires @xmath19 unit of information , and the units of information for @xmath34 depend on the number of different neighbors between node @xmath6 and node @xmath43 .",
    "if @xmath6 and @xmath43 have @xmath50 different neighbors , the size of @xmath34 will be latexmath:[\\[\\label{sizebeta }    @xmath34 will thus be @xmath52 . taking both the reduction of the matrix and the extra information into account ,",
    "the actual information it requires after the operation is @xmath53 this is true for @xmath6 different from @xmath43 .",
    "we could extend the operation to allow @xmath54 meaning a self - match , then we will put all the neighbors of @xmath6 into the corresponding set @xmath34 , and then delete these links associated with @xmath6 . then by a similar argument we find that after this operation we need @xmath55 units of information using the new format .    note that here we need to clarify exactly the meaning of different neighbors since in the case that @xmath6 and @xmath43 are connected @xmath6 is a neighbor of @xmath43 but @xmath43 is not , and likewise for @xmath43 .",
    "however , this extra information can be simply encoded in @xmath35 by making the following rule : @xmath45 means when we reconstruct we do not connect @xmath6 and @xmath43 and @xmath56 means we connect @xmath6 and @xmath43 when we reconstruct",
    ". then we can write @xmath57 .    from the above discussion",
    "we see that if we define @xmath58 then by choosing @xmath45 , @xmath59 measures exactly the amount of information it reduces .",
    "we call @xmath59 the information redundancy between node @xmath6 and @xmath43 . note here that in general this redundancy is not symmetric in @xmath6 and @xmath43 , since for any pair of nodes @xmath50 is symmetric but the degree of these two nodes can be different , and deleting the node with higher degree will always reduce more units of information compared to deleting the lower degree node .",
    "we form the redundancy matrix @xmath60 by setting the entry in row @xmath6 and columnn @xmath43 to be @xmath59 .",
    "we perform the shrinking operation for the pair with maximum @xmath59 , thus saving the maximum amount of information .",
    "for example , again using the graph from section @xmath21 , the adjacency matrix is :    @xmath61,\\ ] ]    and the corresponding redundancy matrix is : @xmath62,\\ ] ]    the maximum entry in @xmath60 is @xmath63 , indicating that either choice of @xmath30 or @xmath64 will give the maximum information reduction , and the corresponding @xmath34 can be obtained by recording the column entries in row @xmath19 and row @xmath21 according to our rule .    in the above discussion",
    "we only consider a one step shrinking operation on the graph and find out the direct relationship between the maximum information reduction and the redanduncy matrix .",
    "but we know that after deleting one node the resulting graph is still sparse and so could be compressed further by our scheme .",
    "the question is then how to successively choose @xmath35 and @xmath34 to obtain the best overall compression .",
    "let @xmath66 denote the operation at step @xmath67 , @xmath68 ( here the sign for @xmath69 would not affect our analysis so by convience we just write @xmath69 ) .",
    "in order to analyze the multi - step effect , we first consider how the adjacency matrix @xmath70 is affected by the orbit @xmath71 .",
    "let @xmath72 be the original adjacency matrix .",
    "let @xmath73 be the corresponding adjacency matrix after applying @xmath74 and the entries in it be @xmath75 .",
    "on deleting node @xmath69 we actually set row and column @xmath69 to be zero in @xmath76 and all the other entries are unchanged , to obtain the new matrix @xmath73 , i.e.    @xmath77    so by induction we see that    @xmath78    then we analyze how the redundancy matrix @xmath60 changes .",
    "use @xmath79 to represent the redundancy matrix , @xmath80 the degree of node @xmath6 , and @xmath81 the number of different neighbors of node @xmath6 and @xmath43 , associated with the graph of @xmath73 .",
    "since our goal is to achieve compression , once a node is deleted in the graph it is useless for future operations .",
    "so we will set @xmath82 if @xmath6 or @xmath43 has been deleted before , i.e. @xmath83    now for those @xmath6 and @xmath43 that have not been deleted , i.e. @xmath84 , by equation [ reddef ] we see that @xmath85 for @xmath86 and @xmath87 .",
    "since @xmath73 is obtained by deleting row and columnn @xmath69 in @xmath76 , the degree of each node changes according to : @xmath88 and @xmath50 changes according to @xmath89    thus , we conclude that for @xmath86 @xmath90\\nonumber\\\\             & = & k_{t-1}(j)-1-\\frac{1}{2}\\delta_{t-1}(i , j)-a_{t-1}(j , j_{t})+\\frac{1}{2}|a_{t-1}(i , j_{t})-a_{t-1}(j , j_{t})|\\nonumber\\\\             & = & r_{t-1}(i , j)+[\\frac{1}{2}|a_{t-1}(i , j_{t})-a_{t-1}(j , j_{t})|-a_{t-1}(j , j_{t})]\\end{aligned}\\ ] ] and for @xmath91 @xmath92    by induction , we obtain that for @xmath86 : @xmath93\\end{aligned}\\ ] ] and for @xmath91 : @xmath94.\\end{aligned}\\ ] ]    by use oft the fact that @xmath84 , by equation [ updatea ] , we can simplify the above two expressions to yield , @xmath95\\mbox { } \\mbox { } \\mbox { } \\mbox { } \\mbox { } \\mbox { if } i \\neq j\\nonumber\\\\ r_{t}(i , i ) & = & r_{0}(i , i)+\\sum_{\\tau=1}^{t}[-\\frac{1}{2}a_{0}(i , j_{\\tau})].\\end{aligned}\\ ] ]    note that if we choose a pair @xmath96 at step @xmath67 , the information we save is measured by @xmath97 .",
    "thus , for any orbit @xmath98 satisfying @xmath99 for @xmath100 ( we call such an orbit a _ natural orbit _ ) , the total information reduction ( or information saving ) will be : @xmath101\\end{aligned}\\ ] ] where @xmath102 is defined by : @xmath103\\mbox { } \\mbox { } \\mbox { } \\mbox { } \\mbox { } \\mbox { if } i \\neq j\\nonumber\\\\ c(i , i , t ) & = & \\sum_{\\tau=1}^{t}[-\\frac{1}{2}a_{0}(i , j_{\\tau})].\\end{aligned}\\ ] ]    so the compression problem can be stated as : @xmath104    one more thing to mention is that the length of the orbit , @xmath105 , is also a variable , which could not be larger than @xmath11 since there are only @xmath11 nodes in the graph and it is meaningless to delete an ` empty ' node which does not even exist .",
    "from the previous section we see that for a given adjacency matrix , the final compression ratio depends on the orbit @xmath106 we choose , and the compression problem becomes an optimization problem . however , to find the maximum of @xmath107 and the corresponding best orbit is not trivial .",
    "one reason is that the number of natural orbits is of order @xmath108 , which makes it impractical to test and try for all possible orbits .",
    "another reason which is crucial here is that for any given orbit of length @xmath105 , evaluating @xmath107 costs @xmath109 operations , making it hard to find an appropriate scheme to search for the true maximum or even the approximate maximum .",
    "instead , we use a greedy algorithm to find an orbit which gives a reasonable compression ratio , and which is easy to apply .",
    "the idea of the greedy algorithm is that at each iteration step we choose the pair of nodes @xmath110 and @xmath69 which maximizes @xmath111 over all possible pairs , and we stop if the maximum value is non - positive . also we need to record @xmath35 and @xmath34 according to the graph .",
    "here we summarize the greedy algorithm as pseudocode :    given the adjacency matrix @xmath70 of a graph ( @xmath11 nodes and @xmath36 edges ) .",
    "begin :    set @xmath72 ;    calculate @xmath112 for all @xmath113 .",
    "this forms the redundancy matrix @xmath114 .",
    "set t=1 .",
    "1 .   @xmath97 .",
    "+ @xmath115 +   +   +   + 2 .",
    "@xmath116 @xmath74 + ; + ; + .",
    "3 .   set @xmath117 and go to step 1 .    the compressed version of the matrix will consist of : the final matrix @xmath118 , the orbit @xmath119 and the vectors @xmath120 , which will allow us to reconstruct @xmath121 and any intermediate matrix @xmath73 during the compression process .",
    "in this section we will show some examples of our compression scheme on several networks .",
    "we begin with the lattice graph , which is expected to be readily compressible due to the high degree of overlapping between neighbors of nodes . as a secondary example",
    ", we add some random alterations , and apply our method to the corresponding watts - strogatz network . finally we show some results for real - world networks .",
    "one of the most symmetric graphs is the lattice graph , a one - dimensional chain where each site is connected to @xmath122 nearest neighbors to its right and left . in this case",
    "@xmath123 represents the degree of each vertex in the lattice graph .",
    "the total number of nodes is @xmath124 , the corresponding adjacency matrix is sparse .",
    "we implement our algorithm for lattice graph with different @xmath1 .",
    "the results are shown in figure [ latticeresults ] . here",
    "we take @xmath125 .",
    "@xmath21 to @xmath126 .",
    "the compression limit is indicated by the bottom curve given by @xmath127 , and we find that for @xmath1 large the compression ratio is close to the empirical formula : @xmath128 ( upper curve ) . for comparison , we plot the result using ysmf ( broken line ) : @xmath129 . for @xmath130",
    ", our algorithm always achieves a better result than the ysmf and the advantage increases with increasing @xmath1.,width=384,height=288 ]      it is not surprising that the lattice graphs are easy to compress since these graphs are highly symmetric and nodes have lots of overlaps in their neighbors .",
    "however , in the case that we do nt have such perfect symmetry , we still hope to achieve compression . here",
    "we apply our algorithm to the ws graphs .",
    "the ws graph comes from the famous watts - strogatz model for real - world networks by showing the so called small - world phenomenon .",
    "the ws graph is generated from a lattice graph by the usual rewiring of each edge with some given probability @xmath131 from the uniform distribution .",
    "we apply our algorithm to ws graphs with different @xmath131 to explore how @xmath131 affects the compression behavior .",
    "results are shown in figure [ wsresults ] .     and @xmath132 .",
    "the stars show the compression results by our algorithm .",
    "the lower line is the compression ratio for the lattice @xmath125 and @xmath132 and the upper line is the ratio from the ysmf . we see that as @xmath131 increases there is less and less overlapping between neighbors in the network and the compression ratio increases . for @xmath133 ,",
    "we obtain worse result than ysmf.,width=384,height=288 ]      in the following we show the compression results for some real world graphs : a c.elegans metabolic network @xcite ( figure [ metabolic ] ) , a yeast network constructed from yeast reactions @xcite , an email network @xcite , and an airline network of flight connections @xcite . in the table 1",
    "we summarize the compression results for these real world graphs .     during each step ( left ) , and information redundancy @xmath134 each step ( right).,title=\"fig:\",width=268,height=201 ]   during each step ( left ) , and information redundancy @xmath134 each step ( right).,title=\"fig:\",width=268,height=201 ]    .compression results for some networks . [ cols=\"<,<,<,<,<,<\",options=\"header \" , ]",
    "from the previous section we see that our algorithm works for various kinds of graphs and gives a reasonable result .",
    "the ideal limit of our method for a graph with @xmath11 nodes , @xmath36 edges and average degree @xmath39 , which is relative large , is @xmath135 .",
    "this is obtained when each @xmath116 during the compression process is empty , meaning that most of the nodes share common neighbors , in which case we only need to record all the @xmath74 , requiring @xmath136 units of information and yields @xmath137    notice that trees do not compress , since for trees @xmath138 , so on average the overlap in neighbors will be even smaller ( likely to be @xmath42 ) , and a possible way to achieve compression is by self - matching for large degree nodes , for example , the hubs in a star graph . for comparison",
    ", the ysmf always gives the compression ratio @xmath139 which does not compress trees , and has a lower bound @xmath140 , while our method in principle approaches @xmath42 as @xmath141 .",
    "actually the compression ratio using ysmf can be achieved by choosing a special orbit in our approach which only contains self - matches @xmath35 , i.e. @xmath142 in this case the neighbors of each node will be put into corresponding @xmath34 sets and since any @xmath143 contains the same pair of numbers @xmath144 we can just use one @xmath6 to represent the pair , resulting in a total @xmath145 information units .",
    "so our approach can be considered as a generalization of the ysmf .    however , as we observed in our compression results , the compression ratio given by [ idealcpr ] is in general not attainable since it is only achieved for the ideal case that nearly every node in the graph shares the same neighbors , and yet the graph needs to be sparse ! however , for lattices we observe that the actual compression ratio achieved by our algorithm is about @xmath146 , which is of the same order as the ideal compression ratio . for ws graphs ,",
    "when the noise @xmath131 is small , our algorithm achieves better compression ratio than ysmf , and the compression ratio is nearly linearly dependent on @xmath131 for @xmath147 . for @xmath148",
    "the graph resembles erdos - renyi random graphs @xcite , there is no symmetry between nodes to be used and thus our approach does not give good result , as compared to the ysmf .    for real world graphs ,",
    "the results by our algorithm are better than using ysmf , but not as good as we observed for lattice graphs .",
    "this suggests that in real world graphs nodes , in general , share certain amount of common neighbors even when the total number of links is small .",
    "this kind of overlap in neighbors is certainly not as common as we see in lattice graphs since real world graphs in general have more complicated structures .",
    "e.m.b have been supported for this work by the army research office grant 51950-ma .",
    "e.m.b . has been further supported by the national science foundation under dms-0708083 and dms-0404778 , and d.b.a is supported by the national science foundation under phy-0555312 .",
    "we thank joseph d. skufca and james p. bagrow for discussion .",
    "s. c. eisenstat and h. c. elman and m. h. schultz and a. h. sherman , _ the ( new ) yale sparse matrix package _ , in elliptic problem solvers ii , g. birkho and a. schoenstadt , editors , academic press , new york , 1984 , pp ."
  ],
  "abstract_text": [
    "<S> in this paper we raise the question of how to compress sparse graphs . by introducing the idea of redundancy , we find a way to measure the overlap of neighbors between nodes in networks . </S>",
    "<S> we exploit symmetry and information by making use of the overlap in neighbors and analyzing how information is reduced by shrinking the network and using the specific data structure we created , we generalize the problem of compression as an optimization problem on the possible choices of orbits . to find a reasonably good solution to this problem we use a greedy algorithm to determine the orbit of symmetry identifications , to achieve compression . </S>",
    "<S> some example implementations of our algorithm are illustrated and analyzed . </S>"
  ]
}