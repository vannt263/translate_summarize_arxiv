{
  "article_text": [
    "in this paper , we develop a new framework for * compressed sensing * ( sparse signal recovery )  @xcite .",
    "we focus on nonnegative sparse signals , i.e. , @xmath13 and @xmath14 .",
    "note that real - world signals are often nonnegative .",
    "we consider the scenario in which neither the magnitudes nor the locations of the nonzero entries of @xmath15 are unknown ( e.g. , data streams ) .",
    "the task of compressed sensing is to recover the locations and magnitudes of the nonzero entries .",
    "our framework differs from mainstream work in that we use maximally - skewed @xmath0-stable distributions for generating our design matrix , while classical compressed sensing algorithms typically adopt gaussian or gaussian - like distributions ( e.g. , distributions with finite variances ) .",
    "the use of skewed stable random projections was originally developed in  @xcite , named * compressed counting ( cc ) * , in the context of data stream computations .",
    "note that in this paper we focus on dense design matrix and leave the potential use of `` very sparse stable random projections ''  @xcite for sparse recovery as future work , which will connect this line of work with the well - known `` sparse matrix '' algorithm  @xcite .",
    "+ in compressed sensing , the standard procedure first collects @xmath16 non - adaptive linear measurements @xmath17 and then reconstructs the signal @xmath15 from the measurements , @xmath18 , and the design matrix , @xmath19 . in this context , the design matrix is indeed `` designed '' in that one can manually generate the entries to facilitate signal recovery .",
    "in fact , the design matrix can be integrated in the sensing hardware ( e.g. , cameras , scanners , or other sensors ) . in classical settings , entries of the design matrix , @xmath19 ,",
    "are typically sampled from gaussian or gaussian - like distributions .",
    "the recovery algorithms are often based on linear programming ( _ basis pursuit _ )  @xcite or greedy pursuit algorithms such as _ orthogonal matching pursuit _  @xcite . in general",
    ", lp is computationally expensive .",
    "omp might be faster although it still requires scanning the coordinates @xmath20 times .",
    "+ it would be desirable to develop a new framework for sparse recovery which is much faster than linear programming decoding ( and other algorithms ) without requiring more measurements",
    ". it would be also desirable if the method is robust against measurement noises and is applicable to * data streams*. in this paper , our method meets these requirements by sampling @xmath19 from maximally - skewed @xmath0-stable distributions  @xcite .      in our proposal , we sample entries of the design matrix @xmath19 from an @xmath0-stable maximally - skewed distribution , denoted by @xmath21 , where the first `` 1 '' denotes maximal skewness and the second `` 1 '' denotes unit scale . if a random variable @xmath22 , then its characteristic function is @xmath23 suppose @xmath24 i.i.d .",
    "for any constants @xmath25 , we have @xmath26 . more generally , @xmath27 if @xmath28 i.i.d .",
    "there is a standard procedure to sample from @xmath21  @xcite .",
    "we first generate an exponential random variable with mean 1 , @xmath29 , and a uniform random variable @xmath30 , and then compute @xmath31^{\\frac{1}{\\alpha } } } \\left[\\frac{\\sin\\left ( u - \\alpha u\\right)}{w } \\right]^{\\frac{1-\\alpha}{\\alpha } } \\sim s(\\alpha,1,1)\\end{aligned}\\ ] ] in practice , we can replace the stable distribution with a heavy - tailed distribution in the domain of attractions  @xcite , for example , @xmath32^{1/\\alpha}}$ ] .",
    "again , we leave it as future work to use a sparsified design matrix .      the use of maximally - skewed stable random projections for nonnegative ( dynamic ) data stream computations was proposed in a line of work called _ compressed counting ( cc ) _  @xcite .",
    "prior to cc , it was popular to use _ symmetric stable random projections _",
    "@xcite in data stream computations .    in the standard",
    "_ turnstile _ data stream model  @xcite , at time @xmath33 , an arriving stream element @xmath34 updates one entry of the data vector in a linear fashion : @xmath35 .",
    "the dynamic nature of data streams makes computing the summary statistics , e.g. , @xmath36 , and recovering the nonzero entries more challenging , especially if the streams arrive at high - speed ( e.g. , network traffic ) .",
    "linear projections are naturally capable of handling data streams . to see this ,",
    "suppose we denote the linear measurements as @xmath37 when a new stream element @xmath34 arrives , we only need to update the measurement as @xmath38 the entries @xmath39 are re - generated as needed by using pseudo - random numbers  @xcite , i.e. , no need to materialize the entire design matrix .",
    "this is the standard practice in data stream computations .    here , we should mention that this streaming model is actually very general .",
    "for example , the process of histogram - building can be viewed as a typical example of turnstile data streams . in machine learning , databases ,",
    "computer vision , and nlp ( natural language processing ) applications , histogram - based features are popular . in network applications , monitoring",
    "traffic histograms is an important mechanism for ( e.g. , ) anomaly detections  @xcite . detecting ( recovering ) heavy components ( e.g. , so called `` elephant detection '' ) using compressed sensing is an active research topic in networks ; see ( e.g. , )  @xcite . + for the rest of paper",
    ", we will drop the superscript @xmath40 in @xmath41 and @xmath42 , while readers should keep in mind that our results are naturally applicable to data streams .      for recovering a nonnegative signal @xmath43 , @xmath44 to @xmath45",
    ", we collect linear measurements @xmath46 , @xmath47 to @xmath16 , where @xmath48 i.i.d . in this paper",
    ", we focus on @xmath49 $ ] and leave the study for @xmath50 in future work . at the decoding stage",
    ", we estimate the signal coordinate - wise : @xmath51 the number of measurements @xmath16 is chosen so that @xmath52 ( e.g. , @xmath53 ) . +",
    "* main result * :  when @xmath3 $ ] , it suffices to use @xmath4 measurements so that , with probability @xmath5 , all coordinates will be recovered within @xmath6 additive precision , in one scan of the coordinates .",
    "the constant @xmath7 when @xmath8 and @xmath9 when @xmath10 .",
    "in particular , when @xmath8 , the required number of measurements is essentially @xmath11 , where @xmath12 is the number of nonzero coordinates of the signal .",
    "+ in the literature , it is known that the sample complexity of compressed sensing using gaussian design ( i.e. , @xmath54 ) is essentially about @xmath55  @xcite .",
    "this means our work already achieves smaller complexity with explicit constant , by requiring only one linear scan of the coordinates .",
    "very encouragingly , it is perhaps not surprising that our method as presented in this paper is merely a tip of the iceberg and we expect a variety of followup works can be developed along this line .",
    "for example , it appears possible to further improve the algorithm by introducing iterations .",
    "it is also possible to sparsify the design matrix to significantly speed up the processing ( matrix - vector multiplication ) and recovery .",
    "our proposed algorithm utilizes only the ratio statistics @xmath56 for recovery , while the observed data include more information , i.e. , @xmath57 for @xmath58 , and @xmath59 .",
    "thus , we first need to provide an explanation why we restrict ourselves to the ratio statistics . for convenience ,",
    "we define @xmath60 and denote the probability density function of @xmath61 by @xmath62 . by a conditional probability argument",
    ", the joint density of @xmath63 can be shown to be @xmath64 .",
    "the mle procedure amounts to finding @xmath65 to maximize the joint likelihood @xmath66 interestingly , the following lemma shows that @xmath67 approaches infinity at the poles @xmath68 .",
    "[ lem_mle ] the likelihood in ( [ eqn_lik ] ) approaches infinity , i.e. , @xmath69 , if @xmath70 , for any @xmath71 , @xmath72 .",
    "+ * proof * :  see appendix  [ app_lem_mle].@xmath73 +    the result in lemma  [ lem_mle ] suggests us to use only the ratio statistics @xmath56 to recover @xmath74 . by the property of stable distributions , @xmath75 where @xmath76 and @xmath77 i.i.d",
    "this motivates us to study the probability distribution of two independent stable random variables : @xmath78 . for convenience ,",
    "we define @xmath79    [ lem_f ]    for any @xmath80 , @xmath81 , i.i.d . , @xmath82 where @xmath83^{\\alpha/(1-\\alpha)}\\left[\\frac{\\sin u_1}{\\sin u_2 } \\right]^{\\frac{1}{1-\\alpha } } \\frac{\\sin\\left ( u_2 - \\alpha u_2\\right)}{\\sin\\left ( u_1 - \\alpha u_1\\right)}\\end{aligned}\\ ] ] in particular , closed - forms expressions are available when @xmath84 or @xmath85 : @xmath86 moreover , for any @xmath87 $ ] , @xmath88 , we have @xmath89    * proof : *  see appendix  [ app_lem_f ] .",
    "figure  [ fig_f ] plots @xmath90 for selected @xmath0 values .",
    "@xmath73 +     for @xmath91 $ ] and @xmath92 and @xmath93 ( from bottom to top).,width=336 ]    lemma  [ lem_f ] has proved that , when @xmath84 , @xmath90 is of order @xmath33 , and when @xmath10 , @xmath90 is of order @xmath94 .",
    "lemma  [ lem_f_order ] provide a more general result that @xmath95 .",
    "[ lem_f_order ] for @xmath96 and @xmath97 , @xmath98    * proof : *  see appendix  [ app_lem_f_order ] . @xmath73    * remarks for lemma  [ lem_f_order ] * :    * the result restricts @xmath99 . here",
    "@xmath100 is monotonically decreasing in @xmath0 and @xmath101 for @xmath102 $ ] .",
    "later we will show that our method only requires very small @xmath33 . *",
    "the constant @xmath103 can be numerically evaluated as shown in figure  [ fig_calpha ] .",
    "* when @xmath84 , we have @xmath104 . hence @xmath105 . * when @xmath85 , we have @xmath106 . hence @xmath107 .     as in lemma  [ lem_f_order ] .",
    "numerically , it varies between 1 and @xmath108.,width=336 ]    to conclude this section , the next lemma shows that the maximum likelihood estimator using the ratio statistics is actually the `` minimum estimator '' .",
    "[ lem_ratio_mle ] use the ratio statistics , @xmath56 , @xmath47 to @xmath16 .",
    "when @xmath3 $ ] , the maximum likelihood estimator ( mle ) of @xmath74 is the sample minimum @xmath109 * proof : *  see appendix  [ app_lem_ratio_mle ] .",
    "@xmath73 +    lemma  [ lem_ratio_mle ] largely explains our proposed algorithm . in the next section ,",
    "we analyze the error probability of @xmath110 and its sample complexity bound .",
    "the following lemma concerns the tail probability of the estimator @xmath110 . because @xmath110 always over - estimates @xmath74 , we only need to provide a one - sided error probability bound .    [ lem_err ]    @xmath111^m\\\\\\label{eqn_err_bound } \\leq & \\left[\\frac{1}{1+\\left(\\epsilon/\\theta_i\\right)^{\\alpha/(1-\\alpha ) } }   \\right]^m\\end{aligned}\\ ] ]    for @xmath97 and @xmath112 , @xmath113^m\\end{aligned}\\ ] ] in particular , when @xmath85 , @xmath114^m\\end{aligned}\\ ] ] * proof : *  recall @xmath115 and @xmath116 .",
    "we have @xmath117^m= \\left[1-f_\\alpha\\left(\\left(\\epsilon/\\theta_i\\right)^{\\alpha/(1-\\alpha)}\\right)\\right]^m%\\\\\\notag % \\leq&\\left[1-\\frac{1}{1 + 1/\\left(\\epsilon/\\theta_i\\right)^{\\alpha/(1-\\alpha ) } }   \\right]^m % = \\left[\\frac{1}{1+\\left(\\epsilon/\\theta_i\\right)^{\\alpha/(1-\\alpha ) } }   \\right]^m\\end{aligned}\\ ] ] the rest of the proof follows from lemma  [ lem_f ] and lemma  [ lem_f_order].@xmath73    * remark for lemma  [ lem_err ] : *  the probability bound ( [ eqn_err_bound ] ) is convenient to use .",
    "however , it is conservative in that it does not give the right order unless @xmath0 is small ( i.e. , when @xmath118 ) . in comparison , ( [ eqn_err_order ] ) provides the exact order , which will be useful for analyzing the precise sample complexity of our proposed algorithm . as shown in lemma  [ lem_f_order ] , @xmath119 holds for relatively small @xmath99 . in our case ,",
    "@xmath120 , i.e. , the result requires @xmath121 , or @xmath122 . when @xmath8 , this means we need @xmath123 , which is virtually always true . for larger @xmath0 , the relation @xmath124 should hold for any reasonable settings .",
    "+    [ thm_complexity ] to ensure @xmath52 , it suffices to choose @xmath16 by @xmath125}\\end{aligned}\\ ] ] where @xmath126 is defined in lemma  [ lem_f ] . if @xmath127 , then it suffices to use @xmath128}\\end{aligned}\\ ] ] which is sharp when @xmath8 . in general , for @xmath3 $ ] and @xmath129 , the ( sharp ) bound can be written as , @xmath130 where the constant @xmath103 is the same constant in lemma  [ lem_f_order ] .    when @xmath10 and @xmath127 , a precise bound exists : @xmath131    * proof : *  the result ( [ eqn_mf ] ) follows from lemma  [ lem_err ] , ( [ eqn_m0 ] ) from lemma  [ lem_f ] , ( [ eqn_mc ] ) from lemma  [ lem_f_order ] .",
    "we provide more details for the proof of the more precise bound ( [ eqn_m05 ] ) .",
    "when @xmath10 , @xmath132}\\end{aligned}\\ ] ] which can be simplified to be @xmath133 , using the fact that @xmath134 $ ] .",
    "to see this inequality , we can check @xmath135 it suffices to show @xmath136 which is true because the equality holds when @xmath137 or @xmath138 , and @xmath139 this completes the proof.@xmath73    * remarks for theorem  [ thm_complexity ] : *  the convenient bound ( [ eqn_m0 ] ) is only sharp for @xmath8 .",
    "for example , when @xmath10 , @xmath140 , but the true order should be in terms of @xmath141 instead of @xmath6 . the other bound ( [ eqn_mc ] ) provides the precise order , where the constant @xmath103 is the same as in lemma  [ lem_f_order ] . the fact that the complexity is proportional to @xmath142 is important and presents a substantial improvement over the previous @xmath143 result in count - min sketch  @xcite .",
    "for example , if we let @xmath8 , then @xmath144 .",
    "in other words , the complexity for exact @xmath20-sparse recovery is essentially @xmath145 and the constant is basically 1 .",
    "we will comment more on the choice of @xmath0 later in the paper .",
    "+ to conclude this section , we provide the analysis for the .",
    "the minimum estimator @xmath110 is biased and it always over - estimates @xmath74 .",
    "the following lemma evaluates the bias precisely .",
    "[ lem_bias ] @xmath146^mdt\\end{aligned}\\ ] ] in particular , when @xmath10 , @xmath147 where @xmath148 is the beta function and @xmath149 is the bernoulli number satisfying @xmath150 e.g. , @xmath151 , @xmath152 , @xmath153 , @xmath154 , @xmath155 , @xmath156 , @xmath157 , ... + * proof : *  see appendix  [ app_lem_bias ] .",
    "figure  [ fig_dm05 ] plots the @xmath158 for @xmath10 .",
    "@xmath73 +     for the bias analysis in lemma  [ lem_bias].,width=336 ]",
    "our proposed algorithm for sparse recovery is simple and requires merely one scan of the coordinates .",
    "our theoretical analysis provides the sharp sample complexity bound with the constant ( i.e. , @xmath103 ) specified ( e.g. , figure  [ fig_calpha ] ) .",
    "it is nevertheless still interesting to include an experimental study .",
    "all experiments presented in this study were conducted in matlab on a workstation with 256 gb memory .",
    "we did not make special effort to optimize our code for efficiency .",
    "we compare our proposed method with two popular l1 decoding packages : _",
    "l1magic _",
    "@xcite and _ spgl1",
    "_  @xcite[multiblock footnote omitted ] , on simulated data .",
    "although it is certainly not our intension to compare the two l1 decoding solvers , we decide to present the results of both . while it is known that spgl1 can often be faster than l1magic , we observe that in some cases spgl1 could not achieve the desired accuracy . on the other hand",
    ", spgl1 better uses memory and can handle larger problems than l1magic .",
    "+ in each simulation , we randomly select @xmath20 out @xmath45 coordinates and set their values ( @xmath74 ) to be 1 .",
    "the other @xmath159 coordinates are set to be 0 . to simulate the design matrix @xmath160 ,",
    "we generate two random matrices : @xmath161 and @xmath162 , @xmath44 to @xmath45 , @xmath163 to @xmath16 , where @xmath164 and @xmath165 , i.i.d .",
    "then we apply the formula ( [ eqn_cms ] ) to generate @xmath166 , for @xmath167 to 0.5 , spaced at 0.01 .",
    "we also use the same @xmath168 and @xmath169 to generate standard gaussian ( @xmath170 ) variables for the design matrix used by l1magic and spgl1 , based on the interesting fact :  @xmath171 .",
    "+ in this experimental setting , since @xmath172 , the sample complexity of our algorithm is essentially @xmath173 , where @xmath174 and @xmath175 . in our simulations , we choose @xmath16 by two options : ( i ) @xmath176 ; ( ii ) @xmath177 , where @xmath178 . + we compare our method with l1magic and spgl1 in terms of the decoding times and the recovery errors .",
    "the ( normalized ) recovery error is defined as @xmath179      figure  [ fig_l1n1000000k10b1 ] presents the recovery errors ( left panel ) and ratios of the decoding times ( right panel ) , for @xmath180 , @xmath181 , and @xmath11 ( where @xmath182 ) .",
    "the results confirm that our proposed method is computationally very efficient and is capable of producing accurate recovery for @xmath183 .",
    "+    , @xmath181 , and @xmath11 ( where @xmath182 ) . for each @xmath0 ( from 0.04 to 0.5 spaced at 0.01 ) , we conduct simulations 100 times and report the median results . in the * left panel * , our proposed method ( solid curve ) produces very accurate recovery results for @xmath183 . for larger @xmath0 values",
    ", however , the errors become large .",
    "this is expected because when @xmath10 , the required number of samples should be @xmath184 instead of @xmath145 . in this case ,",
    "l1magic also produces accuracy recovery results .",
    "note that for all methods , we report the top-@xmath20 entries of the recovered signal as the estimated nonzero entries . in the * right panel * ,",
    "we plot the ratios of the decoding times .",
    "basically , spgl1 package uses about 580 times more time than our proposed method ( which requires only one scan ) , and l1magic package needs about 290 times more time than our method . ,",
    "title=\"fig:\",width=288 ] , @xmath181 , and @xmath11 ( where @xmath182 ) . for each @xmath0 ( from 0.04 to 0.5 spaced at 0.01 ) , we conduct simulations 100 times and report the median results . in the * left panel * , our proposed method ( solid curve ) produces very accurate recovery results for @xmath183 . for larger @xmath0 values ,",
    "however , the errors become large .",
    "this is expected because when @xmath10 , the required number of samples should be @xmath184 instead of @xmath145 . in this case ,",
    "l1magic also produces accuracy recovery results .",
    "note that for all methods , we report the top-@xmath20 entries of the recovered signal as the estimated nonzero entries . in the * right panel * , we plot the ratios of the decoding times .",
    "basically , spgl1 package uses about 580 times more time than our proposed method ( which requires only one scan ) , and l1magic package needs about 290 times more time than our method . ,",
    "title=\"fig:\",width=288 ]    figure  [ fig_spgl1n10000000k10b1 ] presents the results for a larger problem , with @xmath185 and @xmath181 . because we can not run l1magic in this case , we only present the comparisons with spgl1 . again",
    ", our method is computationally very efficient and produces accurate recovery for about @xmath183 .    , @xmath181 , and @xmath11 ( where @xmath182 ) .",
    "see the caption of figure  [ fig_l1n1000000k10b1 ] for more details . in this larger problem",
    ", we can not run l1magic as the program simply halts without making progress.,title=\"fig:\",width=288 ] , @xmath181 , and @xmath11 ( where @xmath182 ) .",
    "see the caption of figure  [ fig_l1n1000000k10b1 ] for more details . in this larger problem , we can not run l1magic as the program simply halts without making progress.,title=\"fig:\",width=288 ]    for @xmath0 close to 0.5 , we need to increase the number of measurements , as shown in the theoretical analysis",
    ".      to study the behavior as @xmath0 approaches 0.5 , we increase the number of measurements to @xmath187 .",
    "figure  [ fig_l1n1000000k10b1.6 ] and figure  [ fig_spgl1n10000000k10b1.6 ] present the experimental results for @xmath180 and @xmath185 , respectively .",
    "interestingly , when @xmath10 , our algorithm still produces accurate recovery results ( with the normalized errors around 0.007 ) , although the results at smaller @xmath0 values are even more accurate . in the next subsection ( section  [ subsec_bias ] ) , we will experimentally show that the recovery accuracy can be further improved by a bias - correction procedure as derived lemma  [ lem_bias ] .",
    ", @xmath181 , and @xmath187 ( where @xmath182 ) .",
    "again , for each @xmath0 , we conduct simulations 100 times and report the median results . in the * left panel * , our proposed method ( solid curve ) produces accurate recovery results , although the errors increase with increasing @xmath0 ( the maximum error is around 0.007 ) . in the * right panel",
    "* , we can see that in this case , our method is only , respectively , 27 times and 39 times faster than spgl1 and l1magic . we should mention that we did not make special effort to optimize our matlab code for efficiency .",
    ", title=\"fig:\",width=288 ] , @xmath181 , and @xmath187 ( where @xmath182 ) .",
    "again , for each @xmath0 , we conduct simulations 100 times and report the median results . in the * left panel * , our proposed method ( solid curve ) produces accurate recovery results , although the errors increase with increasing @xmath0 ( the maximum error is around 0.007 ) . in the * right panel * , we can see that in this case , our method is only , respectively , 27 times and 39 times faster than spgl1 and l1magic .",
    "we should mention that we did not make special effort to optimize our matlab code for efficiency .",
    ", title=\"fig:\",width=288 ]    , @xmath181 , and @xmath11 ( where @xmath182).,title=\"fig:\",width=288 ] , @xmath181 , and @xmath11 ( where @xmath182).,title=\"fig:\",width=288 ]      as analyzed in lemma  [ lem_bias ] , the minimum estimator @xmath110 is slightly biased : @xmath188 , where the constant @xmath158 can be pre - computed and tabulated for each @xmath16 and @xmath0 ( e.g. , figure  [ fig_dm05 ] for @xmath158 with @xmath10 ) .",
    "we also need to estimate @xmath189 , for which we resort the estimator in the prior work on compressed counting  @xcite .",
    "for example , for @xmath10 , the bias - corrected estimator is @xmath190 ^ 2d_{m,0.5}\\end{aligned}\\ ] ]    as verified in figure  [ fig_ccbiask10s1b1.6 ] , the bias - corrected estimator ( [ eqn_minc ] ) does improve the original minimum estimator .    .",
    "in this experiment , we choose @xmath181 , @xmath187 , and @xmath191 . in each simulation",
    ", we use the original minimum estimator @xmath110 together with the bias - corrected estimator @xmath192 as in ( [ eqn_minc ] ) .",
    "we can see that the bias - correction step does improve the accuracy , as the dashed error curve ( @xmath192 ) is lower than the solid error curve ( @xmath110 ) .",
    ", width=336 ]      figure  [ fig_noise ] presents an experimental study to illustrate that our proposed algorithm is robust against usual measurement noise model : @xmath193 where the noise @xmath194 can , for example , come from transmission channel after collecting the measurements .",
    "it is actually very intuitive to understand why our proposed algorithm can be robust against measurement noises .",
    "using the ratio statistics , we have @xmath195 .",
    "because @xmath196 is very heavy - tailed , the noise in terms of @xmath197 , has essentially no impact . in this paper",
    ", we only provide the intuitive explanation and leave a formal analysis in future work .",
    "while our proposed algorithm for sparse recovery based on compressed counting ( maximally - skewed @xmath0-stable random projections ) is simple and fast , it is clear that the work presented in this paper is merely a tip of the iceberg .",
    "we expect many interesting related research problems will arise .",
    "+ one important issue is the choice of @xmath0 . in this paper ,",
    "our analysis focuses on @xmath3 $ ] and our theoretical results show that smaller @xmath0 values lead to better performance .",
    "the natural question is : why can we simply use a very small @xmath0 ?",
    "there are numerical issues which prevents us from using a too small @xmath0 .    for convenience ,",
    "consider the approximate mechanism for generating @xmath21 by using @xmath198 , where @xmath199 ( based on the theory of domain of attractions and generalized central limit theorem ) . if @xmath200 , then we have to compute @xmath201 , which may potentially create numerical problems . in our matlab simulations , we use @xmath202 $ ] and we do not notice obvious numerical problems even with @xmath200 as shown in section  [ sec_exp ] .",
    "however , if a device ( e.g. , camera or other hand - held device ) has more limited precision and memory , then we expect that we must use a larger @xmath0 .",
    "fortunately , our experiments in section  [ sec_exp ] show that the performance is not too sensitive to @xmath0 .",
    "for example , in our experimental setting , the recovery accuracies are very good for @xmath183 even when we choose the sample size @xmath16 based on @xmath8 . + among many potential future research problems , we list a few examples as follows :    * when the signal can have both positive and negative components , we need to use symmetric stable random projections . *",
    "the sample complexity of our algorithm is @xmath203 . for small @xmath0 ,",
    "the value of @xmath142 is close to 1 even for small @xmath6 , for example @xmath204 .",
    "if a device allows the use of very small @xmath0 , then we expect some iteration scheme might be able to substantially reduce the required number of measurements . * in this paper , we focus on dense design matrix . in kdd07 , the work on `` very sparse random projections ''  @xcite showed that one can significantly sparsify the design matrix without hurting the performance in estimating summary statistics .",
    "we expect that it is also possible to use sparsified design matrix in our framework for sparse recovery .",
    "however , since recovering summary statistics is in general an easier task than recovering all the coordinates , we expect there will be nontrivial analysis for ( e.g. , ) deciding the level of sparsity without hurting the recovery results . *",
    "another interesting issue is the coding of the measurements @xmath18 , which is a practical issue because storing and transmitting the measurements can be costly .",
    "recently , there is work  @xcite for coding gaussian random projections in the context of search and learning .",
    "we expect some ideas in  @xcite might be borrowed for sparse recovery .",
    "we develop a new compressed sensing algorithm using _ compressed counting ( cc ) _ which is based on _ maximally - skewed @xmath0-stable random projections_. our method produces accurate recovery of nonnegative sparse signals and our procedure is computationally very efficient .",
    "the cost is just one linear scan of the coordinates .",
    "our theoretical analysis provides the sharp complexity bound .",
    "while our preliminary results are encouraging , we expect many promising future research problems can be pursued in this line of work .",
    "for @xmath205 , the sampling approach in ( [ eqn_cms ] ) provides a method to compute its cdf @xmath206^{\\frac{1}{\\alpha } } } \\left[\\frac{\\sin\\left ( u - \\alpha u\\right)}{w } \\right]^{\\frac{1-\\alpha}{\\alpha}}\\leq s\\right)\\\\\\notag = & \\mathbf{pr}\\left(\\frac{\\left[\\sin\\left(\\alpha u\\right)\\right]^{\\alpha/(1-\\alpha)}}{\\left[\\sin u \\cos\\left(\\alpha\\pi/2",
    "\\right ) \\right]^{\\frac{1}{1-\\alpha } } } \\left[\\frac{\\sin\\left ( u - \\alpha u\\right)}{w } \\right]\\leq s^{\\alpha/(1-\\alpha)}\\right)\\\\\\notag = & \\frac{1}{\\pi}\\int_0^\\pi \\exp\\left\\{-\\frac{\\left[\\sin\\left(\\alpha u\\right)\\right]^{\\alpha/(1-\\alpha)}}{\\left[\\sin u \\cos\\left(\\alpha\\pi/2 \\right ) \\right]^{\\frac{1}{1-\\alpha } } } \\left[\\frac{\\sin\\left ( u - \\alpha u\\right)}{s^{\\alpha/(1-\\alpha ) } } \\right ] \\right\\}du\\\\\\notag = & \\frac{1}{\\pi}\\int_0^\\pi \\exp\\left\\{-q_\\alpha(u ) s^{-\\alpha/(1-\\alpha)}\\right\\}du\\end{aligned}\\ ] ] and the pdf @xmath207 hence , @xmath208 therefore , the likelihood @xmath209 if @xmath210 , provided @xmath211 .",
    "note that here we can choose @xmath189 and @xmath74 to maximize the likelihood .",
    "since @xmath77 , i.i.d . , we know that @xmath212^{\\frac{1}{\\alpha } } } \\left[\\frac{\\sin\\left ( u_1 - \\alpha u_1\\right)}{w_1 } \\right]^{\\frac{1-\\alpha}{\\alpha}},\\\\\\notag & s_2 = \\frac{\\sin\\left(\\alpha u_2\\right)}{\\left[\\sin u_2 \\cos\\left(\\alpha\\pi/2 \\right ) \\right]^{\\frac{1}{\\alpha } } } \\left[\\frac{\\sin\\left ( u_2 - \\alpha u\\right)}{w_2 } \\right]^{\\frac{1-\\alpha}{\\alpha}}\\end{aligned}\\ ] ] where @xmath213 , @xmath214 , @xmath215 are independent .",
    "thus , we can write @xmath216^{\\alpha/(1-\\alpha)}\\left[\\frac{\\sin u_1}{\\sin u_2 } \\right]^{\\frac{1}{1-\\alpha } } \\frac{\\sin\\left ( u_2 - \\alpha u_2\\right)}{\\sin\\left ( u_1 - \\alpha u_1\\right)}\\end{aligned}\\ ] ]    using properties of exponential distributions , for any @xmath80 , @xmath217    when @xmath84 , @xmath218 point - wise . by dominated convergence , @xmath219 .",
    "when @xmath85 , @xmath220 can be simplified to be @xmath221\\left[\\frac{\\sin u_1}{\\sin u_2 } \\right]^{2 } \\frac{\\sin\\left ( u_2/2 \\right)}{\\sin\\left ( u_1/2\\right ) } = \\frac{\\cos^2\\left(u_1/2\\right)}{\\cos^2\\left ( u_2/2\\right)}\\end{aligned}\\ ] ] which can be used to obtain the closed - form expression for @xmath222 : @xmath223 to show @xmath224 for any @xmath225 $ ] , we first note that the equality holds when @xmath226 and @xmath227 . to see the latter case , we write @xmath228 , where @xmath229 and @xmath230 are i.i.d . when @xmath227 , @xmath231 by symmetry .",
    "it remains to show @xmath90 is monotonically increasing in @xmath0 for fixed @xmath225 $ ] . for convenience ,",
    "we define @xmath232 and @xmath233 , where @xmath234^{\\alpha/(1-\\alpha)}\\left[\\sin u\\right]^{\\frac{-1}{1-\\alpha } } \\sin\\left ( u - \\alpha u\\right)\\end{aligned}\\ ] ] @xmath235 we can check that both @xmath232 and @xmath233 are monotonically increasing in @xmath236 $ ] .",
    "@xmath237 we consider three terms ( in curly brackets ) separately and show they are all @xmath238 when @xmath239 $ ] .    for the first term , @xmath240 where the last inequality holds because the derivative ( w.r.t .",
    "@xmath241 ) is @xmath242 . for the second term",
    ", it suffices to show @xmath243 for the third term , it suffices to show @xmath244 thus , we have proved the monotonicity of @xmath233 in @xmath245 $ ] , when @xmath239 $ ] .    to prove the monotonicity of @xmath232 in @xmath241",
    ", it suffices to check if its logarithm is monotonic , i.e. @xmath246 for which it suffices to show @xmath247    at this point , we have proved that both @xmath232 and @xmath233 are monotonically increasing in @xmath248 $ ] at least for @xmath239 $ ] .",
    "@xmath249    by symmetry @xmath250 thus , to show @xmath251 , it suffices to show @xmath252 which holds because @xmath253 and @xmath254 as both @xmath233 and @xmath232 are monotonically increasing functions of @xmath255 $ ] .",
    "this completes the proof .",
    "the goal is to show that @xmath95 . by our definition , @xmath256",
    "where @xmath257^{\\alpha/(1-\\alpha)}\\left[\\frac{1}{\\sin u } \\right]^{\\frac{1}{1-\\alpha } } { \\sin\\left ( u - \\alpha u\\right)}\\end{aligned}\\ ] ]    we can write the integral as @xmath258 where @xmath259^{\\alpha/(1-\\alpha)}\\left[\\frac{1}{\\sin(\\pi- u ) } \\right]^{\\frac{1}{1-\\alpha } } { \\sin\\left ( \\pi - u - \\alpha ( \\pi - u)\\right)}\\\\\\notag = & \\left[{\\sin\\left(\\alpha ( \\pi- u)\\right)}\\right]^{\\alpha/(1-\\alpha)}\\left[\\frac{1}{\\sin u } \\right]^{\\frac{1}{1-\\alpha } } { \\sin\\left ( u + \\alpha ( \\pi - u)\\right)}\\end{aligned}\\ ] ]    first , using the fact that @xmath260 , we obtain @xmath261^{\\alpha/(1-\\alpha)}\\left[\\frac{1}{\\sin u } \\right]^{\\frac{1}{1-\\alpha } } { ( 1-\\alpha)\\sin\\left ( u\\right ) } = \\alpha^{\\alpha/(1-\\alpha)}(1-\\alpha)\\end{aligned}\\ ] ] we have proved in the proof of lemma  [ lem_f ] that @xmath232 is a monotonically increasing function of @xmath245 $ ] . since @xmath262^{\\alpha/(1-\\alpha ) } { \\cos\\left ( \\alpha \\pi/2\\right)}$ ] , we have @xmath263^{\\alpha/(1-\\alpha ) } { \\cos\\left ( \\alpha \\pi/2\\right)}\\leq 1,\\hspace{0.2 in } u\\in [ 0,\\ \\pi/2]\\end{aligned}\\ ] ] in other words , we can view @xmath232 as a constant ( i.e. , @xmath264 ) when @xmath265 $ ] .    on the other hand , note that @xmath266 as @xmath267 .",
    "moreover , when @xmath265 $ ] , we have @xmath268 and @xmath269 .",
    "thus , @xmath270 dominates @xmath232 .",
    "therefore , the order of @xmath90 is determined by one term : @xmath271    since @xmath272 we have , for @xmath273 $ ] , @xmath274    consider @xmath99 . because @xmath275 for @xmath276 , we have @xmath277 uniformly for @xmath278 . when @xmath279 ( i.e. , @xmath280 ) , we also have @xmath281 for the other term with @xmath282 $ ] , we have @xmath283 combining the results , we obtain @xmath284    this completes the proof .",
    "define @xmath285 and @xmath286 . to find the mle of @xmath74 , we need to maximize @xmath287 . using the result in lemma  [ lem_f ] , for @xmath77",
    ", we have @xmath288 @xmath289 @xmath290 where @xmath220 is defined in lemma  [ lem_f ] and @xmath291 @xmath292 if @xmath293 .",
    "this means , @xmath294 when @xmath295 and @xmath296 is nondecreasing in @xmath297 if @xmath293 .",
    "therefore , given @xmath16 observations , @xmath298 , the mle is the sample minimum .",
    "this completes the proof .",
    "@xmath299^mdt\\\\\\notag = & x_i + \\theta_i\\int_{0}^\\infty \\left[1-f_\\alpha\\left(\\left(t\\right)^{\\alpha/(1-\\alpha)}\\right)\\right]^mdt\\\\\\notag = & x_i+\\theta_id_{m,\\alpha}\\end{aligned}\\ ] ]    we have proved in lemma  [ lem_f ] that @xmath300 thus , @xmath301^mdt\\\\\\notag \\leq & \\int_{0}^\\infty \\left[\\frac{1}{1+\\left(t\\right)^{\\alpha/(1-\\alpha)}}\\right]^mdt\\\\\\notag = & \\frac{1-\\alpha}{\\alpha}\\int_{0}^1 t^m\\left(1/t-1\\right)^{(1-\\alpha)/\\alpha-1}\\frac{1}{t^2}dt\\\\\\notag = & \\frac{1-\\alpha}{\\alpha}\\int_{0}^1 t^{m-(1-\\alpha)/\\alpha-1}\\left(1-t\\right)^{(1-\\alpha)/\\alpha-1}dt\\\\\\notag = & \\frac{1-\\alpha}{\\alpha}beta\\left(m-(1-\\alpha)/\\alpha,\\ ( 1-\\alpha)/\\alpha\\right)\\end{aligned}\\ ] ]    when @xmath85 , then @xmath302 , and @xmath303^mdt = \\int_{0}^\\infty \\left[1-\\frac{2}{\\pi}\\tan^{-1}t\\right]^mdt\\\\\\notag = & \\int_0^{\\pi/2}\\left[1-\\frac{2u}{\\pi}\\right]^m d \\tan^2{u }   = \\int_0^{\\pi/2}\\left[1-\\frac{2u}{\\pi}\\right]^m d \\frac{1}{\\cos^2u } \\\\\\notag   = & \\int_1^{0}u^m d \\frac{1}{\\sin^2\\left(u\\pi/2\\right ) }   = m\\int_0 ^ 1 \\frac{u^{m-1}}{\\sin^2\\left(u\\pi/2\\right ) } du -1\\\\\\notag   = & m\\left(\\frac{2}{\\pi}\\right)^{m}\\int_0^{\\pi/2 } \\frac{u^{m-1}}{\\sin^2u}du -1\\end{aligned}\\ ] ]    from the integral table @xcite , we have @xmath304 therefore , to facilitate numerical calculations , we resort to ( let @xmath305 ) @xmath306 where @xmath149 is the bernoulli number satisfying @xmath150 and @xmath151 , @xmath152 , @xmath153 , @xmath154 , @xmath155 , @xmath156 , @xmath157 , ..."
  ],
  "abstract_text": [
    "<S> compressed sensing ( sparse signal recovery ) has been a popular and important research topic in recent years . by observing that natural signals </S>",
    "<S> are often nonnegative , we propose a new framework for nonnegative signal recovery using _ compressed counting ( cc)_. cc is a technique built on _ maximally - skewed @xmath0-stable random projections _ originally developed for data stream computations . </S>",
    "<S> our recovery procedure is computationally very efficient in that it requires only one linear scan of the coordinates . </S>",
    "<S> + in our settings , the signal @xmath1 is assumed to be nonnegative , i.e. , @xmath2 . </S>",
    "<S> our analysis demonstrates that , when @xmath3 $ ] , it suffices to use @xmath4 measurements so that , with probability @xmath5 , all coordinates will be recovered within @xmath6 additive precision , in one scan of the coordinates . </S>",
    "<S> the constant @xmath7 when @xmath8 and @xmath9 when @xmath10 . </S>",
    "<S> in particular , when @xmath8 , the required number of measurements is essentially @xmath11 , where @xmath12 is the number of nonzero coordinates of the signal . </S>"
  ]
}