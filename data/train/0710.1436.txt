{
  "article_text": [
    "distributed computing was stimulated by human endeavours in the `` big science '' domain and , at the same time , by hopes of industry to commercialize developments in networking technologies .",
    "some original ideas of organizing computing for science , as e.g. pioneering seti@home @xcite , initiated by d. gedye for search for signals of extraterrestial civilizations , eventually evolved into coordinated networks supporting research projects of unprecedented scale , as e.g. the large hadron collider ( lhc ) computing grid ( lcg ) in particle physics @xcite .",
    "nowadays , the worlwide lhc computing grid ( wlcg ) is running on the infrastructure provided by the european initiative enabling grids for e - science ( egee ) @xcite , encompassing european national and regional grids , coupled to the american open science grid ( osg ) and collaborating grid centres in the asia - pacific region ( cf .",
    "[ figure0 ] ) .",
    "this infrastructure is shared with a number of smaller projects .",
    "overall wlcg computing architecture is based on the hierarchical multi - tier model developed by monarc collaboration @xcite , as given in fig .",
    "[ figure1 ] .",
    "the top tier-0 is responsible for storage of raw data coming from the experiment data acquisition system , its first off - line processing and distribution of data over tier-1 s .",
    "all data are copied to tier-1 centres in order to speedup access during processing and ensure storage redundancy .",
    "data reprocessing and higher - level reconstructions of real and simulated data are foreseen to be performed at tier-1 and tier-2 levels .",
    "data and physics analyses are normally relegated to tier-2 and -3 centres , closer to end - users .",
    "tier-2s are powerful enough not only to support local needs but also to complement higher tiers with computing power and storage for more specialized purposes .",
    "tier-2s are not required to provide with massive tape storage .",
    "based on the middleware glite-3.0 @xcite , wlcg builds environment with the physical resource layer hidden behind core grid services ( cf .",
    "[ figure2 ] ) .",
    "essential services , e.g. file and metadata catalogues , replica location , application resource catalogues or workflow engines , are either available directly for users or support other , complex services , e.g. grid monitoring is used by resource brokers for process management .",
    "intelligent scheduling and resource brokering are now combined in a complex workload management service .",
    "this system comprises a set of grid middleware components responsible for the distribution and management of tasks across the grid .",
    "information , monitoring and logging are available through the relational grid monitoring architecture ( r - gma ) service , being an implementation of the gma standard .",
    "[ figure7 ] presents counting of numbers of ces and numbers of jobs monitored during one year , as provided by the r - gma monitoring service .",
    "users interact with r - gma through application programming interfaces available for high - level programming languages .",
    "crucial for grid management is the distributed grid accounting system ( dgas ) .",
    "its purpose is to implement resource usage metering , accounting and account balancing in a fully distributed grid environment .",
    "the last function , i.e. account balancing , is still not in use , because the grid has not yet come into commercial phase .",
    "large amounts of data are handled with dcache system @xcite .",
    "terabytes of data are distributed over many disc storage nodes but the name space is uniquely represented within a single file system tree .",
    "the system has shown to significantly improve the efficiency of connected tape storage systems , through caching , optimizing buffers and scheduled staging techniques .",
    "furthermore , it optimizes the throughput to and from data clients as well as smoothing the load of the connected disc storage nodes by dynamically replicating datasets .",
    "according to recent trend towards service oriented architecture ( soa ) , grid components are reengineered as web - services and published on the net .",
    "dedicated portals are used for designing complex workflows within soa .",
    "users are organized in virtual organizations ( vos ) and managed within vos through the virtual organization membership service ( voms ) .",
    "each site is designed in a way typical for globus - operated @xcite grids ( cf .",
    "[ figure4 ] ) .",
    "it contains the computing element ( ce ) , normally consisting of the front - end machine playing gatekeeping role and a set of worker nodes ( wns ) . the storage element ( se )",
    "is a main data container in a site and usually consists of disc matrices managed by dedicated machine .",
    "the user interfaces ( uis ) , enabling user access to the infrastructure , may be either located on the spot or installed remotely .",
    "the grids of egee and wlcg are built on top of the gant network @xcite  a collaboration of 26 national and research networks in europe , led by the dante company .",
    "gant project aims to deliver a quality - of - service gigabit speed backbone network for research in europe .",
    "the gant connectivity scheme is given in fig .",
    "[ figure3 ] .",
    "gant has 12  gbps connectivity to north america and 2.5  gbps to japan and to trans - eurasia information network ( trein2 ) , thus ensuring collaboration of egee with osg , japaneese national research grid initiative ( naregi ) and asian - pacific grids .",
    "the entry point to polish national regional network has bandwidth of 10 gbps with 4470 b maximum transition unit on the switch .",
    "four lhc experiments : alice @xcite , atlas @xcite , cms @xcite and lhcb @xcite , represent the largest consumers of resources on the grid . raw data ( raw ) coming from real experiment s daq or monte carlo simulation are recorded and processed off - line by reconstruction programs giving event summary data ( esd ) , programs extracting physical variables and providing with analysis object data ( aod ) , and to further data reduction , selection and filtering , leading to event tags ( tag ) .",
    "in addition , derived streams of filtered data at the esd and aod levels and specialized data for detector alignment and calibration are recorded and analysed .",
    "grid is also used for the quasi on - line processing of data used for on - line calibration and filtering of data in the framework of the interactive european grid ( ieg ) project @xcite .",
    "summary of data - flow parameters and requirements for off - line resources is given in tab .",
    "[ table1 ] .",
    ".data flow and resource requirements for lhc experiments [ cols=\"^,^,^,^,^\",options=\"header \" , ]     tab .",
    "[ table3 ] shows resourse sharing over virtual organizations in polish tier-2 clusters . besides already mentioned vos , one finds vos related to the baltic grid project ( baltgrid ) , biology ( biomed ) , chemistry ( compchem ) , internal egee development vo ( dteam ) , the eu - china grid initiative ( euchina ) , central european federation vo ( voce ) and a couple of minor vos . in this report",
    "we do not distinguish between egee vos , official global vos , official local vos and others , although these distinctions are important from managerial viewpoint .",
    "inspection of the table reveals differences in local policies of resource allocation to vos .",
    "there is no yet official regulation for these policies and resource allocations are usually negotiated between user communities and site managements . in order to ensure optimal cpu usage , fair share system is normally implemented in queues , unless given vo uses privately funded machines . in future , the quality - of - service system allowing reservations and hiring is foreseen for both computational resources and communication bandwidth .",
    "these issues are related to future commercialization of the grid .",
    "european grid is supposed to provide a permanent and reliable infrastructure for research and science .",
    "the hardware is run by staff of participating institutions and is under local responsibility .",
    "both central and local services are run by dedicated groups of people and are shared between partners , depending on their competence , size and needs of regional scientific groups .",
    "the case of large lhc collaborations , consisting even of thousands of researchers , somewhat violates this scheme .",
    "while the lowest - order tier-3 nodes are traditionally situated in scientific institutes , tier-2 s and tier-1 s are often run by large regional or national computing centres , capable of fulfilling operational requirements .",
    "daily operations are monitored by grid operational centre ( goc ) located at uk @xcite .",
    "the goc is responsible for coordinating the overall operation of the grid .",
    "it acts as a central point of operational information such as configuration information and contact details .",
    "the goc has responsibility for monitoring the operation of the grid infrastructure as a whole , devising and managing mechanisms and procedures which encourage optimal operation of the grid , and working with local support groups to assist them in providing the best possible service while their equipment is connected to the grid .",
    "basic functionality of services is regularly tested and monitored using site functional tests ( sfts ) .",
    "the sft uses a small test job that runs at each site and determines the availability of the main grid functions .",
    "similarly , the grid status monitor retrieves information published by each site about its status .",
    "their use and subsequent triggering of follow - up action is supervised by the core infrastructure centre ( cic ) on duty staff raising operational tickets against sites to resolve observed problems .",
    "two - level ticketing system is incorporated .",
    "ticket flow diagram is displayed in fig .",
    "[ figure9 ] .    global grid user support ( ggus )",
    "portal run by fzk is a principal entry point for all sorts of tickets @xcite .",
    "daily ticket operations , including initial recognition of the type of the problem , opening and assigning ticket to supporters , directing it to local support units , care about timely solving and contact with users , is a duty of ticket processing management group ( tpm ) .",
    "the tpm works in a shift system , 5 days a week , 8 hours a day .",
    "sft failure tickets are send to ggus and redirected automatically to local support units and from there to the front - line supporters at sites .",
    "another type tickets are issued by end - users who may encounter many kinds of problems with a system or with applications .",
    "user tickets , depending on the type of the problem , are either solved by dedicated group of supporters asked by tpm from ggus , or redirected to local support units and solved there .",
    "there is at least one local support unit in federation . for polish tier-2",
    ", ticketing tool @xcite is running using _",
    "one - or - zero _ portal software @xcite .",
    "bulk of applications in experimental particle physics and other sciences needs a high - throughput batch processing of large amounts of data . for a number of applications ,",
    "however , often interaction with intermediate results or fast response to a well defined computational problem is desired .",
    "this sort of applications , called ( quasi- ) interactive , draws an attention of grid community since almost beginning . in poland , involvement in deploying such applications on the grid and building appropriate tools and infrastructure for them , dates back to the crossgrid project @xcite and is nowadays continued in the framework of ieg project @xcite .",
    "the ieg resources are split into two separated infrastructures :    * the production infrastructure aimed at providing computing and storage resources for the end - users running scientific applications , * the development infrastructure being fully independent of the production and aimed at supporting the project software development , the test of new middleware and its rollout process .",
    "as such this infrastructure does not provide a service as stable and reliable as the production .",
    "development sites may also be occasionally reconfigured with specific setups to evaluate or validate software components .",
    "currently , the production infrastructure provides with 300 cpu cores and 8 tb disc storage , located in 8 computing centres in europe , with a considerable contribution of three polish computing centres .",
    "the ieg supports the following interactive applications :    * ultra sound computer tomography . * medical applications on brain images * flood forecasting application .",
    "this application was first deployed on the crossgrid testbed .",
    "* visualisation of baltic wave model .",
    "* evolution of pollution clouds in the atmosphere .",
    "this application was first deployed on the crossgrid testbed . * atlas online monitoring and calibration system .",
    "this application is related to atlas experiment at lhc but it does not run on the wlcg infrastructure . *",
    "analysis of maps of cosmic microwave background .",
    "* visualization of plasma in fusion reactors .",
    "this application runs also in less interactive mode on the egee infrastructure .",
    "being a global - scale initiative with large investment and social impact , computing grid needs associated actions attracting and training users , and explaining the newest grid technology to wider public .",
    "training is provided by organizing courses , normally given by staff members of academic partner institutes of large grid projects ( cf .",
    "e.g. refs @xcite ) , and using dedicated training infrastructure .",
    "courses are attended by students of universities at the engineering , m.sc . and ph.d",
    ". levels , practicing scientists of informatics , natural sciences and engineering , developers and managerial staff from commercial companies .",
    "dissemination of grid technology is assured by its active promotion in communities of the actual and potential users by using press news and specialized publications , participating in conferences of possibly wide spectrum of subjects , and through media @xcite .",
    "the on - demand tv , being nowadays a distributor of knowledge , may shortly become grid s customer exploiting its computing power for own programme casting and production .",
    "the wlcg is an example of well developed grid infrastructure for science . to large extent , however , it was designed for specific needs of experimental particle physics where high - throughput , massive .",
    "asynchronous data - intensive processing of segmented data is needed .",
    "parallel processing and using massage passing interface software is rather rare .",
    "occasional usage of such gateway services like application resource catalogs or workflow engines is not a common practice .",
    "workflow management is quite often done semi - manually and resource brokerage is still far from being adaptive and autonomous .",
    "prospective line of development guides toward soa where applications are distributed over the network and are accessible from everywhere as services .",
    "data are going to be virtualized ( dcache ) and workflows are dynamically designed and redesigned according to needs ( cf .",
    "e.g. pgrade portal @xcite ) .    after fulfilling lhc commitments",
    ", polish tier-2 should evolve towards new computational paradigm where complex reserach scenarios are executed in response to external events ( e.g. rapid weather change ) in closed loops with instruments and humans ( interactivity ) . on - demand allocation of computing resources should ensure solving identified important problems .    from commercial perspective , other aspects of grid should be underlined . for pure research , robustness and security for applications is not really critical unless facilities are being built .",
    "scientific groups gladly relegate operations to commercial entities . but",
    "this practice often results undesirably for science because business itself is only interested in research in case of visible income .",
    "interesting game between two aspects of distributed , large - scale computing : the economic and the research , is in front of us",
    ".    10 d.p .",
    "anderson , j. cobb , e. korpela , m. lebofski and d. werthimer , `` seti@home - an experiment in public - resource computing '' , _ communications of the acm _ , vol .",
    "56 - 61 , november 2002 , + ( http://setiathome.ssl.berkeley.edu/ ) m. lamanna , `` the lhc computing grid project at cern '' , _ nuclear instruments and methods in physcics research a _ , vol .",
    "534 , pp . 1 - 6 , november 2004 , + ( http://lcg.web.cern.ch/lcg/ ) egee is the project funded by the european commission in the 6th framework programme under contract infso - ri-031688 + ( http://www.eu-egee.org/ ) http://cern.ch/monarc/ e. laure et al . ,",
    "`` programming the grid using glite '' , egee - pub-2006 - 029 , submitted to _ computational methods in science and technology _ b. sotomayor and l. childers , _",
    "globus toolkit 4 : programming java services _ , morgan kaufmann ( december 16 , 2005 ) , + ( http://www.globus.org/ ) http://www.geant.net m. de riese et al .",
    ", `` dcache , the book '' , + ( http://www.dcache.org/ , http://www.dcache.org/ manuals / book/ ) alice : b. allessandro et al . ,",
    "`` alice : physics performance report , volume ii '' , _ journal of physics g _",
    "32 , pp . 1295 - 2040 , september 2006 , + ( http://aliceinfo.cern.ch/ ) atlas : a. di ciaccio et al .",
    ", `` the atlas experiment at cern : two years before the start of the data taking '' , 9th icatpp conference on astroparticle , particle , space physics , detectors and medical physics applications , villa erba , como , italy , 17 - 21 oct 2005 , + ( http://atlas.web.cern.ch/atlas/index.html ) cms : k. hoepfner et al . , `` physics with cms - potential and challenges '' , _ particles and nuclei international conference ( panic 05 ) _ , santa fe , new mexico , 24 - 28 october 2005 , american institute of physics conference proceedings , vol 842 , pp . 1094 - 1096 , july 2006 , + ( http://cms.cern.ch/ )",
    "lhcb : t. nakada et al . , `` status of the lhcb experiment '' , conference on physics at lhc , vienna , austria , 13 - 17 july 2004 , _ czech jounal of physics b _ vol .",
    "405 - 418 , july 2005 , + ( http://lhcb.web.cern.ch/lhcb/ ) ieg is the project funded by the european commission in the 6th framework programme under contract fp6 - 2005-infrastructures-7 , + ( http://dissemination.interactive - grid.eu/. http://dissemination.interactive-grid.eu/applications/hep ) compass : e.s .",
    "ageev et al . ,",
    "_ nuclear physics b _ , vol .",
    "31 - 70 , january 2007 , + ( http://wwwcompass.cern.ch/ ) h1 and zeus : g. brandt et al . , `` exotic physics at hera '' , 33rd international conference on high energy physics ( ichep 06 ) , moscow , russia , july / august 2006 , + ( http://www-zeus.desy.de/ ) http://goc.grid.sinica.edu.tw/gstat//index.html http://www.polgrid.pl/modules.php?name=technicaldocumentation http://goc.grid-support.ac.uk/gridsite/gocmain/?page=2 https://gus.fzk.de/pages/home.php",
    "http://www.polgrid.pl/ academic scientific tv distributed on the internet , ( http://atvn.icm.edu.pl/ ) http://www.lpds.sztaki.hu/pgrade/"
  ],
  "abstract_text": [
    "<S> structure , functionality , parameters and organization of the computing grid in poland is described , mainly from the perspective of high - energy particle physics community , currently its largest consumer and developer . </S>",
    "<S> it represents distributed tier-2 in the worldwide grid infrastructure . </S>",
    "<S> it also provides services and resources for data - intensive applications in other sciences .     * </S>",
    "<S>  computer networks , distributed computing . </S>"
  ]
}