{
  "article_text": [
    "large datasets used in training modern machine learning models , such as deep neural networks , are often affected by label noise .",
    "the problem is pervasive for a simple reason : manual expert - labelling of each instance at a large scale is not feasible , and so researchers often resort to cheap but imperfect surrogates .",
    "two such popular surrogates are crowdsourcing using non - expert labellers , and ( especially for images ) the use of a search engine to query instances by a keyword , where it is assumed that the keyword is a valid label for what is collected from the query @xcite .",
    "both approaches offer the possibility to scale the acquisition of training labels , but invariably result in the introduction of noise in the labels , which may adversely affect model training .",
    "our goal is to effectively train neural networks with modern architectures under label noise .",
    "we do so by marrying two different lines of recent research .",
    "the first strand is work on _ ad - hoc deep architectures _ tailored to the problem , primarily developed in computer vision  @xcite . while some such approaches have shown good experimental performance on specific domains , they lack of a solid theoretical framework , and often need large amount of clean labels to obtain acceptable results  in particular , for pre - training or validating hyper - parameters  @xcite .",
    "the second strand is recent machine learning research on _ theoretically grounded means of combatting label noise_. roughly , two avenues have been explored , focussing on the high- and low - capacity model regimes respectively , with a sharp distinction : high - capacity models are known to be robust to essentially any level of such noise , given sufficiently many samples @xcite , while low - capacity models are known to be defeated by even mild uniform label noise @xcite .",
    "the apparently pessimistic results for the low - capacity regime can be mitigated via the design of _ corrected losses _ that are _ robust _ to such noise @xcite ; however , these methods crucially require the noise rates to be known _ a priori_. interestingly , recent",
    "work has provided practical algorithms for estimating noise rates @xcite ; remarkably , this is achievable with _ absolutely no knowledge of ground truth labels_. to our knowledge , there has been no prior work on combining these noise estimators with the loss correction techniques , nor any application of either idea to modern deep architectures .    this work aims to unify those research streams by the introduction of two alternative procedures for loss correction .",
    "they both amount to simple linear algebra provided that we know a stochastic matrix @xmath0 summarising the probability of one class being flipped into another under noise . the first procedure , a multi - class extension of @xcite applied to neural networks , is called `` _ _ backward _ _ '' correction as it multiplies the loss by @xmath1 .",
    "the second , inspired by @xcite , is named `` _ _ forward _ _ '' correction as it multiplies the network predictions by @xmath0 .",
    "we prove that both procedures enjoy formal guarantees of robustness with regard to the clean data distribution .",
    "since we only operate on the loss function , the approach is architecture independent and not tied to a particular application domain , other than viable for any chosen loss function . in real applications practitioners may be able to obtain a good estimate of @xmath0 by polishing a subset of the available training set @xcite  something undoubtedly useful and often necessary for tuning hyper - parameters and testing the model anyway .",
    "nevertheless , we take a further step extending the noise estimator of @xcite to the multi - class setting .",
    "incidentally , we also prove that , when the network only non - linearity is relu , the hessian of the loss is independent from noise .",
    "an extensive empirical analysis shows that a range of high - capacity networks are not inherently noise robust , arguably because of the lack of a sufficient amount of training data to reflect the theory ; this fact is in line with common experience and it is backed by the momentum of current research interest on the topic .",
    "we apply our loss corrections on problems of image recognition on mnist , cifar-10 , cifar-100 and sentiment analysis on imdb ; we simulate corruption by artificially injecting noise on the training labels . in order to show that no architectural choice is the secret ingredient of our robustification recipe , we experiment with a variety of network modules currently in fashion : convolutions and pooling @xcite , dropout @xcite , batch normalization @xcite , word embedding and residual units @xcite .",
    "additional tests on lstm @xcite confirms that the procedures can be seamless applied to recurrent neural networks as well .",
    "comparisons with non - corrected losses and several methods from the literature confirm robustness of our two procedures , with the forward correction dominating the backward .",
    "unsurprisingly , the noise estimation is the bottleneck in obtaining near - perfect robustness , yet in most experiments our approach is often the best compared to prior work .",
    "the rest of this paper is organized as follows :  [ sec : rel ] connects our work with relevant background literature ,  [ sec : setting ] fixes notation and some key concepts ,  [ sec : noise ] presents noise correction and estimation in the context of neural networks ,  [ sec : exp ] provides empirical results , and ",
    "[ sec : disc ] closes the paper by discussing our findings , drawbacks and potential extensions to the noise model and its estimator .",
    "our work leverages recent research in a number of different areas , summarised below .",
    "_ noise robustness_. learning with noisy labels has been widely investigated in the literature @xcite . from the theoretical standpoint label noise has been studied in two different regimes , with vastly different conclusions . in the case of low - capacity ( typically linear ) models , even mild symmetric , _",
    "i.e. _ class - independent ( versus asymmetric , _ i.e. _ class - dependent ) , label noise can produce solutions that are akin to random guessing @xcite . on the other hand ,",
    "the bayes - optimal classifier remains unchanged under symmetric @xcite and even instance dependent label noise @xcite implying that high - capacity models will be robust to essentially any level of such noise , given sufficiently many samples .",
    "a caveat with the latter is that the introduction of label noise adversely affects the number of samples needed for learning ( * ? ? ?",
    "* chapter 3 ) .",
    "_ surrogate losses_. suppose one wishes to minimise a loss @xmath2 on clean data .",
    "when the level of noise is known _ a priori _ , @xcite provided the general form of a _ noise corrected _ loss @xmath3 such that minimisation of @xmath3 on noisy data is _ equivalent _ to minimisation of @xmath4 on clean data . in the idealized case of symmetric label noise , for",
    "certain @xmath2 one in fact does not need to know the noise rate : @xcite gives a sufficient condition for which @xmath2 is always robust , and several examples of such robust non - convex losses , while @xcite shows that the ( convex ) linear or _ unhinged _ loss @xmath5 is its own noise - corrected loss .",
    "another example of non - convex loss with robustness properties is given in @xcite .",
    "_ noise rate estimation_. recent work has provided methods to estimate label flip probabilities directly from noisy samples .",
    "typically , it is required that the generating distribution is such that for each class , there exists some `` perfect '' instance , _",
    "i.e. _ one that is classified with probability equal to one .",
    "proposed estimators involve either the use of kernel mean embedding @xcite , or post - processing the output of a standard class - probability estimator such as logistic regression using order statistics on the range of scores @xcite or the slope of the induced roc curve @xcite .",
    "a common limitation is the focus on the case of binary labels , with the exception of @xcite .    _ deep learning with",
    "noisy labels_. several works in deep learning have attempted to deal with noisy labels of late , especially in computer vision .",
    "this is often achieved formulating noise - aware models .",
    "@xcite builds a noise model for binary classification of aerial image patches , which can handle omission and wrong location of training labels ; due to the form of the resulting objective , some parameters are learned via expectation maximization ( em ) .",
    "@xcite constructs a more sophisticated mix of symmetric , asymmetric and instance - dependent noise ; two convolutional neural networks are learnt by em as model for both the classifier and the noise type .",
    "@xcite augments the objective similarly to entropy regularization .",
    "in practice , it is often the case that a small set of clean labels is needed in order either to pre - train or fine - tune the model @xcite .",
    "the work of @xcite deserves a particular mention .",
    "the method extends the architecture by adding a linear layer on top of the network .",
    "once learnt , this layer plays the role of our matrix @xmath0 .",
    "the insight is that , at training time , the effect of the noise will be captured by the linear layer , emulating the corruption of the model predictions ; at test time , the linear layer must be removed to obtain clean predictions .",
    "although , learning the augmented architecture appears problematic ; heuristics such as trace regularization and a fixed updating schedule for the linear layer are necessary .",
    "we sidestep those issues by decoupling the two phases : we first estimate @xmath0 and then learn with loss correction .    we are not aware of any other attempt at either applying the noise - corrected loss approach of @xcite to neural networks , nor on combining those losses with the above noise rate estimators .",
    "our work sits precisely in this intersection .",
    "note that even though in principle loss correction should not be necessary for high - capacity models like neural networks , owing to aforementioned theoretical results , in practice such correction may offset the sub - optimality of these models arising from training on finite samples : specifically , we expect that directly optimising the ( corrected ) objective we care about will be beneficial in the finite - sample case .",
    "we begin by fixing notation .",
    "we let @xmath6 { \\stackrel{.}{=}}\\{1,\\dots , c\\}$ ] for any @xmath7 positive integer .",
    "column vectors are written in bold ( _ e.g. _  @xmath8 ) and matrices in capitals ( _ e.g. _  @xmath9 ) .",
    "coordinates of a vector are denoted by a subscript ( _ e.g. _  @xmath10 ) , while rows and columns of a matrix are denoted _ e.g. _  @xmath11 and @xmath12 respectively .",
    "we denote the all - ones vector by @xmath13 , with dimensionality clear from context , and @xmath14^c$ ] the @xmath7-dimensional simplex .    in supervised @xmath7-class classification ,",
    "one has feature space @xmath15 and label space @xmath16 \\}$ ] , where @xmath17 denotes the @xmath18th standard canonical vector in @xmath19 by , _",
    "i.e. _  @xmath20 .",
    "one observes examples @xmath21 drawn from an unknown distribution @xmath22 over @xmath23 .",
    "note that each @xmath24 only has one non - zero value at the coordinate corresponding to the underlying label .",
    "an @xmath25-layer neural network comprises a transformation @xmath26 , where @xmath27 is the composition of a number of intermediate transformations , the layers , defined by : @xmath28 ) \\ , & \\bm{h}^{(i ) } ( \\bm{z } ) & & = \\bm{\\sigma}(w^{(i)}\\bm{z } + \\bm{b}^{(i)})\\:\\ : , \\\\      & \\bm{h}^{(n ) } ( \\bm{z } ) & & = w^{(i)}\\bm{z } + \\bm{b}^{(i)}\\:\\:.\\end{aligned}\\ ] ] where @xmath29 and @xmath30 are parameters to be estimated , the original feature dimensionality , and @xmath31 , the label dimensionality . ] , and @xmath32 is any activation function that acts _ coordinate - wise _ , such as the relu @xmath33 .",
    "observe that the final layer applies a _ linear _ projection , unlike all preceding layers . to simplify notation",
    ", we will write : @xmath34 ) \\ , \\bm{x}^{(i ) } { \\stackrel{.}{=}}\\bm{h}^{(i ) } ( \\bm{x}^{(i - 1 ) } ) , \\ ] ] with the base case @xmath35 , so that _ e.g. _",
    "@xmath36 is exactly the representation in the first layer .",
    "the coordinates of @xmath37 represent the relative weights that the model assigns to each class @xmath38 to be predicted .",
    "the predicted label is thus simply @xmath39 } \\bm{h}_i(\\bm{x})$ ] . in the training phase , the output of the final layer",
    "is contrasted with the true label @xmath24 via two steps .",
    "first , @xmath40 passes through the _ softmax _ function @xmath41 .",
    "the softmax output may be interpreted as the vector of class - wise probabilities living in the simplex @xmath42 , and hence we may denote it by @xmath43 .",
    "next , we measure the discrepancy between label @xmath44 and network s output by a loss function @xmath45^{c } \\to { \\mathbb{r}}$ ] , given for example by the _ cross - entropy _ : @xmath46 with some abuse of notation , we also define the loss in a vectorial form computed on every possible label : @xmath47 in the following , we will deal with a generic loss function @xmath2 for which all formal results hold under very mild conditions . at times",
    "we provide examples for the cross - entropy and , for simplicity , one could think of cross - entropy every time @xmath2 is mentioned .",
    "we now consider the problem of learning with label noise . we assume the _ asymmetric _ , _ i.e. _ class - conditional noise setting @xcite , where each label @xmath24 in the training set is flipped to @xmath48 with probability @xmath49 ;",
    "the feature vectors are assumed untouched .",
    "thus , we effectively observe samples from a distribution @xmath50 .",
    "denote by @xmath51^{c \\times c}$ ] the noise transition matrix specifying the probability of one label being flipped to another , so that @xmath52 .",
    "this matrix is row - stochastic and not necessarily symmetric across the classes .",
    "the class - conditional noise setting is a simplification of real - world noise , which may also be _ feature dependent _ ; relatively little is known about learning under such noise , with few exceptions @xcite .",
    "we aim to modify any loss @xmath2 in order to make it robust to label noise .",
    "in fact , this is possible when @xmath0 is known . under this assumption ( that we will relax later on ) we introduce two alternative corrections inspired respectively by @xcite and @xcite .",
    "we can build an _ unbiased estimator _ of the loss function , such that _ under expected label noise _",
    "the corrected loss equals the original one computed on clean data .",
    "this property is stated in the next theorem , a multi - class generalization of ( * ? ? ?",
    "* theorem 1 ) .",
    "the theorem is also a particular instance of the more abstract ( * ? ? ?",
    "* theorem 3.2 ) .",
    "[ th : unbias ] suppose that the noise transition matrix @xmath0 is non - singular",
    ". given a loss @xmath53 , define the _ backward _ corrected loss as : @xmath54 then , the loss correction is unbiased , _ i.e. _ , its expectation under label noise is exactly the loss : @xmath55 and therefore the minimizers are the same : @xmath56    * proof*. we have : @xmath57 @xmath58 the corrected loss is effectively a linear combination of the loss values for each observable label , which coefficients are due to the probability that @xmath1 attributes to each possible true label @xmath24 , given the observed one @xmath59 .",
    "intuitively , we are `` going one step back '' in the noise process described by the markov chain @xmath0 .",
    "the corrected loss is differentiable  although not always non - negative  and can be minimized with any off - the - shelf algorithm for back - propagation . a note on invertibility : although in practice @xmath0 would be invertible almost surely , its condition number may be problematic .",
    "a simple solution is to mix @xmath0 with the identity matrix before inversion ; this can otherwise be interpreted as taking a more optimistic view on the noise process by combining it with the identity , _",
    "i.e. _ no noise .",
    "alternatively , we can operate a correction acting on the model predictions .",
    "following @xcite , we start by observing that a neural network learnt with standard cross - entropy ( no loss correction ) would result in a predictor for noisy labels @xmath60 .",
    "we can make explicit the dependency on @xmath0 .",
    "for instance , with cross - entropy we have : @xmath61 or in matrix form : @xmath62    this loss compares the noisy label @xmath63 to averaged noisy prediction corrupted by @xmath0 .",
    "we call this procedure `` forward '' correction . in order to analyse its behaviour",
    ", we first need to recall definition and properties of a broad family of losses named _ proper composite _",
    "* section 4 ) .",
    "many losses are said to be _ composite _ , in the sense that they can be expressed by the aid of an _ link function _",
    "@xmath64 , invertible , as @xmath65 in the case of cross - entropy , the softmax is the _ inverse _ link function . when composite losses are also _ proper _",
    "@xcite , their minimizer assumes the particular shape of the link function applied to the class probability : @xmath66 an intriguing robustness property holds for forward correction of proper composite losses .",
    "[ th : forward ] suppose that the noise transition matrix @xmath0 is non - singular .",
    "given a proper composite loss @xmath67 , define the _ forward _ loss correction as : @xmath68 then , the minimizer of the corrected loss under the noisy distribution is the same as the minimizer of the original loss under the clean distribution : @xmath69    * proof*. first notice that @xmath70 where we denote @xmath71 , or equivalently @xmath72 by rule of inverse of composition .",
    "@xmath73 is invertible by composition of invertible functions , its domain is @xmath74 $ ] as of @xmath75 and its codomain is @xmath76 because of composition of @xmath1 with @xmath75 .",
    "the last loss in equation [ proof : forward ] is therefore proper composite with link @xmath73 . finally ,",
    "from equation [ eq : proper ] , the loss minimizer over the noisy distribution is @xmath77 @xmath58    the result expresses a weaker property with respect to unbiasedness of theorem [ th : unbias ] .",
    "robustness applies to the minimizer only : the model learnt by forward correction is the minimizer over the _",
    "clean _ distribution .",
    "yet , theorem [ th : forward ] guarantees noise independence without explicitly inverting the noise process , but it does it `` behind the scenes '' by the `` denoisying '' link function @xmath73 .",
    "this turns out to be an important factor in practice , as shown in section [ sec : exp ] experimentally and discussed in section [ sec : disc ] .",
    "a clear limitation of the above procedures is that they require knowing @xmath0 . in most applications",
    ", the matrix @xmath0 will be unknown and to be estimated .",
    "we present here an extension of the recent noise estimator of @xcite to the multi - class settings .",
    "the estimator is derived under two assumptions .",
    "[ th : noise - estimate ] assume @xmath78 is such that :    1 .   there exist `` perfect examples '' of each of class @xmath79 $ ] , in the sense that @xmath80 2 .   given sufficiently many corrupted samples , @xmath81 is rich enough to model @xmath60 accurately .",
    "it follows that : @xmath82,~~t_{i j } = p({\\tilde{\\bm{y}}}=\\bm{e}^j | \\bar{\\bm{x}}^i)\\:\\:.\\end{aligned}\\ ] ]    * proof*. for any @xmath79 $ ] and any @xmath83 , we have that : @xmath84 } p({\\tilde{\\bm{y}}}= \\bm{e}^j | \\bm{y } = \\bm{e}^k)~p(\\bm{y}=\\bm{e}^k | \\bm{x } ) = \\sum_{k \\in [ c ] } t_{kj}~p(\\bm{y}=\\bm{e}^k | \\bm{x})\\:\\:.\\end{aligned}\\ ] ] when @xmath85 , @xmath86 for @xmath87 , which proves the result .",
    "@xmath58    * input * : @xmath88 , the noisy training set ; @xmath2 ; if @xmath0 is unknown : train a network @xmath89 on @xmath88 with loss @xmath2 obtain an unlabelled sample @xmath90 estimate @xmath91 by equations ( [ eq : est1])-([eq : est2 ] ) on @xmath90 train the network @xmath89 on @xmath88 with the corrected loss @xmath92 or @xmath93 ; * output * : @xmath40    rather surprisingly , theorem [ th : noise - estimate ] tells us that we can estimate each component of matrix @xmath0 _ just based on noisy class probability estimates _ , that is , the output of the softmax of a network trained with noisy labels .",
    "in particular , let @xmath90 be any set of features vectors",
    ". this can be the @xmath94 itself , but not necessarily : we do not require this sample to have _ any _ label at all and therefore whenever more unlabelled samples are easy to obtain from the same distributions , those could be used in place of @xmath94 .",
    "we can approximate @xmath0 with two steps : @xmath95    in practice , assumption ( 1 ) of theorem [ th : noise - estimate ] might hold true when our @xmath90 is large enough .",
    "assumption ( 2 ) of theorem [ th : noise - estimate ] is more difficult to justify : we are requiring that the network can perfectly model the probability of _ the noisy labels_. although , we observe in the experiments that we can often recover @xmath0 close to the ground truth and that estimation errors have only a mild but not catastrophic effect on the quality of the corrected loss .",
    "algorithm [ algo:1 ] summarises the end - to - end approach .",
    "if we know @xmath0 , for example by cleaning manually a subset of training data , we can train with @xmath92 or @xmath93 .",
    "otherwise , we first have to train the network with @xmath2 on noisy data , and obtain from it estimates of @xmath60 for each class via the output of the softmax .",
    "after training @xmath91 is computable in @xmath96 .",
    "finally , we re - train with the corrected loss , while potentially utilizing the first network to help initializing the second one .      [ cols=\"<,<,<,<\",options=\"header \" , ]     we now compare with other methods .",
    "datasets , architectures and artificial noise are the same as above . additionally , we explore the case of symmetric label noise : @xmath97 is the probability of label flip that is spread uniformly among all the other classes .",
    "we select other methods that prescribe changes in the loss function , similarly to what we do : unhinged @xcite , sigmoid @xcite , savage @xcite and soft and hard bootstrapping @xcite ; hyper - parameters of the last two methods are set in accordance with their paper .",
    "unhinged loss is unbounded and can not be used alone : in the original work @xmath98 regularization is applied to address the problem , when training non - parametric kernel models .",
    "we tried to regularize every layer with little success : learning either does not converge ( too small regularization ) or converge to very poor solutions ( too high ) . on preliminary experiments sigmoid loss ran into the opposite issue , namely premature saturation :",
    "the loss reaches a plateau too quickly , a well - known problem with sigmoidal activation functions @xcite . to make those losses usable for comparison , we stack a layer of batch normalization right before the loss function . essentially , we normalize the network output to @xmath74 $ ] and therefore operate in a bounded and non - saturated area of the loss ; notice that this is never required for shallow models such as linear or kernel classifiers .",
    "table [ table : big ] presents the empirical analysis .",
    "we list the key findings .",
    "( a ) in the absence of artificial noise ( first column for each dataset ) , all losses reach similar accuracies with a spread of @xmath99 points ; exceptions are some instances of unhinged and sigmoid , and savage on cifar100 that makes learning impossible .",
    "additionally , with imdb there are cases ( @xmath100 in table [ table : big ] ) of loss correction with noise estimation that perform slightly better than assuming no noise : clearly , the estimator is able to recover the _",
    "natural _ noise in the sentiment reviews .",
    "( b ) with low asymmetric noise ( second column ) results differ between simple architecture / tasks ( datasets on the left ) and deep networks / more difficult problems ( right ) : in the former case , the two corrections behave similarly and are not statistically far from the competitors ; in the latter case , forward correction with known @xmath0 is unbeaten , with no clear winner among the remaining ones .",
    "( c ) with asymmetric noise ( last two columns ) the two loss corrections with known @xmath0 are overall the best performing , confirming the practical implications of their theoretical guarantees ; forward is usually the best .",
    "( d ) if we exclude cifar100 , the noise estimation accounts for average accuracy drops between @xmath101 ( imbd with lstm model ) and @xmath102 points ( mnist ) ; nevertheless , their performance is better than any other methods in many occasions . in the experiment on cifar100",
    "we obtain essentially perfect noise robustness with the ideal forward correction .",
    "noise estimation appears poorly performing in the very last column , yet it guarantees again better accuracy over competing methods .",
    "we discuss this issue and potential solutions in section [ sec : disc ] .      as a final experiment we consider the dataset introduced by * ? ? ?",
    "the data consist of 1 m images of clothing with noisy labels , with additional @xmath103 of clean images respectively used in training , validation and testing .",
    "the goal is to classify test images in one out of 14 categories , _",
    "e.g. _ t - shirt , suit , vest , underwear .",
    "we learn with a 50-layer residual network pre - trained on imagenet .",
    "table [ table : last ] collects the results .",
    "model & loss & training data & notes & accuracy + alexnet & cross - entropy & @xmath104 clean & - & @xmath105 + @xcite & cross - entropy & @xmath104 clean + @xmath106 noisy & upsampling , imagenet pre - training & @xmath107 + @xcite & cross - entropy & @xmath104 clean + @xmath106 noisy & upsampling , imagenet pre - training & @xmath108 + 50-resnet & cross - entropy & @xmath104 clean & - & @xmath109 + 50-resnet & cross - entropy & @xmath106 noisy & - & @xmath110 + 50-resnet & backward & @xmath106 noisy & - & @xmath110 + 50-resnet & backward , @xmath91 & @xmath106 noisy & - & @xmath110 + 50-resnet & forward & @xmath106 noisy & - & @xmath110 + 50-resnet & forward , @xmath91 & @xmath106 noisy & - & @xmath110 +",
    "it is known that the architecture depth can severely impair the convergence of the optimisation algorithms based on back - propagation .",
    "the use of multiple non - linear layers easily lead to several issues such as saturated non - linear units , vanishing / exploding gradients , and proliferation of saddle points  @xcite .",
    "we conjecture that the shape of corrected loss can actually aggravate the issues caused by non - linear layers .",
    "let the gradient in the standard case with noise be @xmath111 for any parameter @xmath112 .",
    "instead , the gradient with noise correction is @xmath113    although statistically relevant , being an unbiased estimate of the loss , from the optimization standpoint the gradient may be smoothed out by the linear combination and miss to give a clear descending direction . in some sense ,",
    "the minimization path is taken with caution : the loss informs the back - propagation algorithm not to trust too much the observed labels , and instead to average the descent on the expected ( unknown ) true label , given the observed one .",
    "we have proposed a framework for training neural networks with noisy labels with a solid theoretical grounding and extensive empirical validation .",
    "our approach boils down to two loss corrections based on modelling the noise by a row - stochastic matrix @xmath0 .",
    "the advantage of loss correction is evident : test set accuracy is consistently only few percents away from training cross - entropy _ on clean data _ , while corruption does often worsen performance of vanilla cross - entropy by @xmath114 points or more .    of our two correction procedures ,",
    "forward usually wins over the backward correction .",
    "the explicit inversion of the matrix  that sometimes has high condition number  may be the root of the problem ; indeed , backward correction results in linear combination of loss values for each possible label , and their coefficients may be dissimilar by orders of magnitude , which intuitively makes optimization difficult .",
    "in contrast , forward correction projects model predictions into another distribution in the probability simplex .",
    "the quality of noise estimation is evidently the key factor for successfully obtaining robustness .",
    "estimation works fairly well in most experiments with a median drop of only @xmath115 points of accuracy with respect to the same method using the true @xmath0 .",
    "the only exception is the very last column of tests on cifar100 , where estimation destroys most of the gain from correcting the loss .",
    "we believe that the combination of high noise and small number of images per class ( 500 ) is detrimental to the proposed estimator .",
    "this is confirmed by the sensitivity of the hyper - parameter @xmath116 .",
    "in fact , the same configuration of the other experiments led to @xmath91 with no resemblance with the ground truth and consequently staled the learning process to very poor solutions .",
    "future work shall focus on improving the estimation phase by incorporating knowledge of the structure of the noise , for example assuming @xmath0 with low rank .",
    "improvements on this direction may also enlarge the applicability of our approach to massively multi - class with thousands or tens of thousands of classes .",
    "furthermore , we have focussed on the widely studied model of asymmetric class - dependent noise .",
    "a straightforward extension is the outlier label noise considered in @xcite ; the treatment to this problem is simple and could be included .",
    "it remains an open question whether more realistic _",
    "instance_-dependent noise may modelled within our approach @xcite .",
    "ultimately , the methods shall be tested on widely - used naturally - corrupted datasets , _ e.g _",
    "satellite aerial images @xcite and images of product scraped from e - commerce websites @xcite .",
    "we also anticipate the application of our approach as a seamless tool for training models with noisy , publicly - available data from the web , in the spirit of @xcite ."
  ],
  "abstract_text": [
    "<S> we present a theoretically grounded approach to train deep neural networks , including recurrent networks , subject to class - dependent label noise . </S>",
    "<S> our method only performs a correction on the loss function , and is agnostic to both the application domain and network architecture . </S>",
    "<S> we propose two procedures for loss correction : they simply amount to at most a matrix inversion and multiplication , provided that we know the probability of each class being corrupted into another . </S>",
    "<S> we further show how one can _ estimate _ these probabilities , adapting a recent technique for noise estimation to the multi - class setting , and thus providing an end - to - end framework . </S>",
    "<S> extensive experiments on mnist , imdb , cifar-10 , cifar-100 employing a diversity of architectures  stacking dense , convolutional , pooling , dropout , batch normalization , word embedding , lstm and residual layers  demonstrate the noise robustness of our proposals . </S>",
    "<S> incidentally , we also prove that , when relu is the only non - linearity , the loss curvature is immune to class - dependent label noise . </S>"
  ]
}