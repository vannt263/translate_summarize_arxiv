{
  "article_text": [
    "l.  e. y .- s . acknowledges support by the fapesp of brazil under project 2012/21871 - 7 and 2014/16363 - 8 .",
    "d.  v. and a.  b. acknowledge support by the ministry of education , science , and technological development of the republic of serbia under projects oi1611005 , on171017 and iii43007 .",
    "m. acknowledges support by the science and engineering research board , department of science and technology , government of india under project no .",
    "s.  k.  a. acknowledges support by the cnpq of brazil under project 303280/2014 - 0 , and by the fapesp of brazil under project 2012/00451 - 0 .",
    "s. gautam , e - print arxiv:1601.06020 ( 2016 ) ; + i. vasi and a. bala , e - print arxiv:1602.03538 ( 2016 ) ; + t. khellil , a. bala , and a. pelster , accepted for publication in new j. phys .",
    "( 2016 ) , e - print arxiv:1510.04985 ; + t. khellil and a. pelster , accepted for publication in j. stat .",
    "mech .- theory exp .",
    "( 2016 ) , e - print arxiv:1511.08882 ; + j. akram and a. pelster , phys .",
    "a * 93 * ( 2016 ) 023606 ; + j. akram and a. pelster , phys .",
    "a * 93 * ( 2016 ) 033610 ; + t. mithun , k. porsezian , and b. dey , phys .",
    "a * 93 * ( 2016 ) 013620 ; + l. salasnich and s.  k. adhikari , acta phys .",
    "a * 128 * ( 2015 ) 979 ; + y.  h. wang , a. kumar , f. jendrzejewski , r.  m. wilson , m. edwards , s. eckel , g.  k. campbell , and c.  w. clark , new j. phys . * 17 * ( 2015 ) 125012 ; + j.  b. sudharsan , r. radha , h. fabrelli , a. gammal , and b.  a. malomed , phys .",
    "a * 92 * ( 2015 ) 053601 ; + v.  s. bagnato , d.  j. frantzeskakis , p.  g. kevrekidis , b.  a. malomed , and d. mihalache , rom .",
    "phys . * 67 * ( 2015 ) 5 ; + k .- t .",
    "xi , j. li , and d .-",
    "shi , phys .",
    "b * 459 * ( 2015 ) 6 ; + h.  l.  c. couto and w.  b. cardoso , j. phys .",
    "b : at . mol .",
    "* 48 * ( 2015 ) 025301 ; + e. chiquillo , j. phys . a : math",
    "* 48 * ( 2015 ) 475001 ; + s. sabari , c.  p. jisha , k. porsezian , and v.  a. brazhnyi , phys .",
    "e * 92 * ( 2015 ) 032905 ; + w. wen , t.  k. shui , y.  f. shan , and c.  p. zhu , j. phys .",
    "b : at . mol .",
    "* 48 * ( 2015 ) 175301 ; + p.",
    "das and p.  k. panigrahi , laser phys .",
    "* 25 * ( 2015 ) 125501 ; + y.  s. wang , s.  t. ji , y.  e. luo , and z.  y. li , j. korean .",
    "* 67 * ( 2015 ) l1504 ; + s.  k. adhikari , j. phys .",
    "b : at . mol .",
    "* 48 * ( 2015 ) 165303 ; + f.  i. moxley iii , t. byrnes , b. ma , y. yan , and w. dai , j. comput .",
    "* 282 * ( 2015 ) 303 ; + s.  k. adhikari , phys .",
    "e * 92 * ( 2015 ) 042926 ; + r.  r. sakhel , a.  r. sakhel , and h.  b. ghassib , physica b * 478 * ( 2015 ) 68 ; + s. gautam and s.  k. adhikari , phys .",
    "a * 92 * ( 2015 ) 023616 ; + d. novoa , d. tommasini , and j.  a. nvoa - lpez , phys .",
    "e * 91 * ( 2015 ) 012904 ; + s. gautam and s.  k. adhikari , laser phys .",
    "* 12 * ( 2015 ) 045501 ; + k .- t .",
    "xi , j. li , and d .-",
    "shi , physica b * 459 * ( 2015 ) 6 ; + s. gautam and s.  k. adhikari , phys .",
    "rev . a * 91 * ( 2015 ) 013624 ; + a. i. nicolin , m.  c. raportaru , and a. bala , rom .",
    "* 67 * ( 2015 ) 143 ; + s. gautam and s.  k. adhikari , phys .",
    "rev . a * 91 * ( 2015 ) 063617 ; + e.  j.  m. madarassy and v.  t. toth , phys .",
    "d * 91 * ( 2015 ) 044041 ; + x. antoine and r. duboscq , comput .",
    "* 185 * ( 2014 ) 2969 ; + s.  k. adhikari and l.  e. young - s . , j. phys .",
    "b : at . mol .",
    "opt . phys .",
    "* 47 * ( 2014 ) 015302 ; + k. manikandan , p. muruganandam , m. senthilvelan , and m. lakshmanan , phys .",
    "e * 90 * ( 2014 ) 062905 ; + s.  k. adhikari , phys .",
    "a * 90 * ( 2014 ) 055601 ; + a. bala , r. paun , a. i. nicolin , s. balasubramanian , and r. ramaswamy , phys . rev .",
    "a * 89 * ( 2014 ) 023609 ; + s.  k. adhikari , phys . rev . a * 89 * ( 2014 ) 013630 ; + j. luo , commun .",
    "nonlinear sci .",
    "* 19 * ( 2014 ) 3591 ; + s.",
    "k. adhikari , phys .",
    "rev . a * 89 * ( 2014 ) 043609",
    "; + k .- t .",
    "xi , j. li , and d .-",
    "shi , physica b * 436 * ( 2014 ) 149 ; + m.  c. raportaru , j. jovanovski , b. jakimovski , d. jakimovski , and a. mishev , rom . j. phys . * 59 * ( 2014 ) 677 ; + s. gautam and s.  k. adhikari , phys .",
    "a * 90 * ( 2014 ) 043619 ; + a.  i. nicolin , a. bala , j. b. sudharsan , and r. radha , rom .",
    "* 59 * ( 2014 ) 204 ; + k. sakkaravarthi , t. kanna , m. vijayajayanthi , and m. lakshmanan , phys .",
    "e * 90 * ( 2014 ) 052912 ; + s.  k. adhikari , j. phys .",
    "b : at . mol .",
    "* 47 * ( 2014 ) 225304 ; + r.  k. kumar and p. muruganandam , numerical studies on vortices in rotating dipolar bose - einstein condensates , proceedings of the 22nd international laser physics workshop , j. phys .",
    "* 497 * ( 2014 ) 012036 ; + a.  i. nicolin and i. rata , density waves in dipolar bose - einstein condensates by means of symbolic computations , high - performance computing infrastructure for south east europe s research communities : results of the hp - see user forum 2012 , in springer series : modeling and optimization in science and technologies * 2 * ( 2014 ) 15 ; + s.  k. adhikari , phys .",
    "rev . a * 89 * ( 2014 ) 043615 ; + r.  k. kumar and p. muruganandam , eur .",
    "j. d * 68 * ( 2014 ) 289 ; + i. vidanovi , n.  j. van druten , and m. haque , new j. phys .",
    "* 15 * ( 2013 ) 035008 ; + s. balasubramanian , r. ramaswamy , and a.  i. nicolin , rom .",
    "* 65 * ( 2013 ) 820 ; + l.  e. young - s . and s.  k. adhikari , phys . rev .",
    "a * 87 * ( 2013 ) 013618 ; + h. al - jibbouri , i. vidanovi , a. bala , and a. pelster , j. phys .",
    "b : at . mol . opt",
    ". phys . * 46 * ( 2013 ) 065303 ; + x. antoine , w. bao , and c. besse , comput .",
    "* 184 * ( 2013 ) 2621 ; + b. nikoli , a. bala , and a. pelster , phys .",
    "a * 88 * ( 2013 ) 013624 ; + h. al - jibbouri and a. pelster , phys .",
    "a * 88 * ( 2013 ) 033621 ; + s.  k. adhikari , phys .",
    "a * 88 * ( 2013 ) 043603 ; + j.  b. sudharsan , r. radha , and p. muruganandam , j. phys .",
    "b : at . mol .",
    "* 46 * ( 2013 ) 155302 ; + r.  r. sakhel , a.  r. sakhel , and h.  b. ghassib , j. low temp",
    "* 173 * ( 2013 ) 177 ; + e.  j.  m. madarassy and v.  t. toth , comput .",
    "* 184 * ( 2013 ) 1339 ; + r.  k. kumar , p. muruganandam , and b.  a. malomed , j. phys .",
    "* 46 * ( 2013 ) 175302 ; + w. bao , q. tang , and z. xu , j. comput",
    "* 235 * ( 2013 ) 423 ; + a.  i. nicolin , proc .",
    "ser . a - math",
    "* 14 * ( 2013 ) 35 ; + r.  m. caplan , comput .",
    "* 184 * ( 2013 ) 1250 ; + s.  k. adhikari , j. phys .",
    "b : at . mol .",
    "* 46 * ( 2013 ) 115301 ; +  .",
    "marojevi , e. gkl , and c. lmmerzahl , comput .",
    "phys . commun .",
    "* 184 * ( 2013 ) 1920 ; + l.  e. young - s . and",
    "s.  k. adhikari , phys .",
    "a * 86 * ( 2012 ) 063611 ; + s.  k. adhikari , j. phys .",
    "b : at . mol .",
    "opt . phys .",
    "* 45 * ( 2012 ) 235303 ."
  ],
  "abstract_text": [
    "<S> we present new version of previously published fortran and c programs for solving the gross - pitaevskii equation for a bose - einstein condensate with contact interaction in one , two and three spatial dimensions in imaginary and real time , yielding both stationary and non - stationary solutions . to reduce the execution time on multicore processors , new versions of parallelized programs </S>",
    "<S> are developed using open multi - processing ( openmp ) interface . </S>",
    "<S> the input in the previous versions of programs was the mathematical quantity nonlinearity for dimensionless form of gross - pitaevskii equation , whereas in the present programs the inputs are quantities of experimental interest , such as , number of atoms , scattering length , oscillator length for the trap , etc . </S>",
    "<S> new output files for some integrated one- and two - dimensional densities of experimental interest are given . </S>",
    "<S> we also present speedup test results for the new programs .    </S>",
    "<S> bose - einstein condensate ; gross - pitaevskii equation ; split - step crank - nicolson scheme ; real- and imaginary - time propagation ; c program ; fortran program ; openmp ; partial differential equation    02.60.lj ; 02.60.jh ; 02.60.cb ; 03.75.-b    * new version program summary *    _ program title : _ bec - gp - omp package , consisting of : ( i ) imag1d , ( ii ) imag2d , ( iii ) imag3d , ( iv ) imagaxi , ( v ) imagcir , ( vi ) imagsph , ( vii ) real1d , ( viii ) real2d , ( ix ) real3d , ( x ) realaxi , ( xi ) realcir , ( xii ) realsph . </S>",
    "<S> + _ catalogue identifier : _ </S>",
    "<S> aedu_v4_0 . </S>",
    "<S> + _ program summary url : _ http://cpc.cs.qub.ac.uk/summaries/aedu_v4_0.html + _ program obtainable from : _ cpc program library , queen s university of belfast , n. ireland . </S>",
    "<S> + _ licensing provisions : _ apache license 2.0 + _ no . of lines in distributed program , including test data , etc . : _ 130308 . </S>",
    "<S> + _ no . of bytes in distributed program , including test data , etc . : _ 929062 . </S>",
    "<S> + _ distribution format : _ </S>",
    "<S> tar.gz . </S>",
    "<S> + _ programming language : _ openmp c ; openmp fortran . </S>",
    "<S> + _ computer : _ any multi - core personal computer or workstation . </S>",
    "<S> + _ operating system : _ linux and windows . </S>",
    "<S> + _ ram : _ 1 gb . + _ number of processors used : _ all available cpu cores on the executing computer . </S>",
    "<S> + _ classification : _ 2.9 , 4.3 , 4.12 . </S>",
    "<S> + _ catalogue identifier of previous version : _ </S>",
    "<S> aedu_v1_0 , aedu_v2_0 . </S>",
    "<S> + _ journal reference of previous version : _ comput . phys . </S>",
    "<S> commun . </S>",
    "<S> * 180 * ( 2009 ) 1888 ; _ ibid . _ </S>",
    "<S> * 183 * ( 2012 ) 2021 . </S>",
    "<S> + _ does the new version supersede the previous version ? </S>",
    "<S> : _ no . </S>",
    "<S> it does supersedes versions aedu_v1_0 and aedu_v2_0 , but not aedu_v3_0 , which is mpi - parallelized version . </S>",
    "<S> +   + _ nature of problem : _ the present openmp fortran and c programs solve the time - dependent nonlinear partial differential gross - pitaevskii ( gp ) equation for a bose - einstein condensate in one ( 1d ) , two ( 2d ) , and three ( 3d ) spatial dimensions in a harmonic trap with six different symmetries : axial- and radial - symmetry in 3d , circular - symmetry in 2d , and fully anisotropic in 2d and 3d .     </S>",
    "<S> + _ solution method : _ the time - dependent gp equation is solved by the split - step crank - nicolson method by discretizing in space and time . the discretized equation is then solved by propagation , in either imaginary or real time , over small time steps . </S>",
    "<S> the method yields the solution of stationary and/or non - stationary problems .     </S>",
    "<S> + _ reasons for the new version : _ previously published fortran and c programs @xcite for solving the gp equation are recently enjoying frequent usage @xcite and application to a more complex scenario of dipolar atoms @xcite . </S>",
    "<S> they are also further extended to make use of general purpose graphics processing units ( gpgpu ) with nvidia cuda @xcite , as well as computer clusters using message passing interface ( mpi ) @xcite . however , a vast majority of users use single - computer programs , with which the solution of a realistic dynamical 1d problem , not to mention the more complicated 2d and 3d problems , could be time consuming . now practically all computers have multicore processors , ranging from 2 up to 18 and more cpu cores . </S>",
    "<S> some computers include motherboards with more than one physical cpu , further increasing the possible number of available cpu cores on a single computer to several tens . </S>",
    "<S> the present programs are parallelized using openmp over all the cpu cores and can significantly reduce the execution time . </S>",
    "<S> furthermore , in the old version of the programs @xcite the inputs were based on the mathematical quantity nonlinearity for the dimensionless form of the gp equation . </S>",
    "<S> the inputs for the present versions of programs are given in terms of phenomenological variables of experimental interest , as in refs .  </S>",
    "<S> @xcite , i.e. , number of atoms , scattering length , harmonic oscillator length of the confining trap , etc . also , the output files are given names which make identification of their contents easier , as in refs .  </S>",
    "<S> @xcite . </S>",
    "<S> in addition , new output files for integrated densities of experimental interest are provided , and all programs were thoroughly revised to eliminate redundancies .     </S>",
    "<S> + _ summary of revisions : _ previous fortran @xcite and c @xcite programs for the solution of time - dependent gp equation in 1d , 2d , and 3d with different trap symmetries have been modified to achieve two goals . </S>",
    "<S> first , they are parallelized using openmp interface to reduce the execution time in multicore processors . </S>",
    "<S> previous c programs @xcite had openmp - parallelized versions of 2d and 3d programs , together with the serial versions , while here all programs are openmp - parallelized . secondly , the programs now have input and output files with quantities of phenomenological interest </S>",
    "<S> . there are six trap symmetries and both in c and in fortran there are twelve programs , six for imaginary - time propagation and six for real - time propagation , totaling to 24 programs . in 3d , </S>",
    "<S> we consider full radial symmetry , axial symmetry and full anisotropy . in 2d , we consider circular symmetry and full anisotropy . </S>",
    "<S> the structure of all programs is similar .    for the fortran programs the input data ( number of atoms , scattering length , harmonic oscillator trap length , trap anisotropy , etc . </S>",
    "<S> ) are conveniently placed at the beginning of each program . for the c programs </S>",
    "<S> the input data are placed in separate input files , examples of which can be found in a directory named input . </S>",
    "<S> the examples of output files for both fortran and c programs are placed in the corresponding directories called output . </S>",
    "<S> the programs then calculate the dimensionless nonlinearities actually used in the calculation . </S>",
    "<S> the provided programs use physical input parameters that give identical nonlinearity values as the previously published programs @xcite , so that the output files of the old and new programs can be directly compared . </S>",
    "<S> the output files are conveniently named so that their contents can be easily identified , following refs .  @xcite . for example , file named @xmath0code@xmath1-out.txt , where @xmath0code@xmath1 is a name of the individual program , is the general output file containing input data , time and space steps , nonlinearity , energy and chemical potential , and was named fort.7 in the old fortran version . </S>",
    "<S> the file @xmath0code@xmath1-den.txt is the output file with the condensate density , which had the names fort.3 and fort.4 in the old fortran version for imaginary- and real - time propagation , respectively . </S>",
    "<S> other density outputs , such as the initial density , are commented out to have a simpler set of output files . </S>",
    "<S> the users can re - introduce those by taking out the comment symbols , if needed .    </S>",
    "<S> also , some new output files are introduced in this version of programs . </S>",
    "<S> the files @xmath0code@xmath1-rms.txt are the output files with values of root - mean - square ( rms ) sizes in the multi - variable cases . </S>",
    "<S> there are new files with integrated densities , such as imag2d - den1d_x.txt , where the first part ( imag2d ) denotes that the density was calculated with the 2d program imag2d , and the second part ( den1d_x ) stands for the 1d density in the @xmath2-direction , obtained after integrating out the 2d density @xmath3 in the @xmath4 plane over @xmath5-coordinate , @xmath6 similarly , imag3d - den1d_x.txt and real3d - den1d_x.txt represent 1d densities from a 3d calculation obtained after integrating out the 3d density @xmath7 over @xmath5- and @xmath8-coordinate . </S>",
    "<S> the files imag3d - den2d_xy.txt and real3d - den2d_xy.txt are the integrated 2d densities in the @xmath4 plane from a 3d calculation obtained after integrating out the 3d density over the @xmath8-coordinate , and similarly for other output files . </S>",
    "<S> again , caclulation and saving of these integrated densities is commented out in the programs , and can be activated by the user , if needed .    </S>",
    "<S> .wall - clock execution times ( in seconds ) for runs with 1 and 20 cpu cores with different programs using the intel fortran ifort ( f-1 and f-20 , respectively ) and intel c icc ( c-1 and c-20 , respectively ) compilers , and obtained speedups ( speedup - f = f-1/f-20 , speedup - c = c-1/c-20 ) . [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]      cores was calculated as the ratio between wall - clock execution times with one and @xmath9 cores . </S>",
    "<S> ( b ) wall - clock time of the same runs as a function of the number of cores.,title=\"fig:\",width=302 ]   cores was calculated as the ratio between wall - clock execution times with one and @xmath9 cores . </S>",
    "<S> ( b ) wall - clock time of the same runs as a function of the number of cores.,title=\"fig:\",width=302 ]    in real - time propagation programs there are additional results for the dynamics saved in files , such as real2d-dyna.txt , where the first column denotes time , the second , third and fourth columns display rms sizes for the @xmath2- , @xmath5- , and @xmath10-coordinate , respectively . </S>",
    "<S> the dynamics is generated by multiplying the nonlinearity with a pre - defined factor during the nrun iterations , and starting with the wave function calculated during the npas iterations . </S>",
    "<S> such files were named fort.8 in the old fortran versions of programs . </S>",
    "<S> there are similar files in the 3d real - time programs as well .    </S>",
    "<S> often it is needed to get a precise stationary state solution by imaginary - time propagation and then use it in the study of dynamics using real - time propagation . for that purpose , </S>",
    "<S> if the integer number nstp is set to zero in real - time propagation , the density obtained in the imaginary - time simulation is used as initial wave function for real - time propagation , as in refs .  </S>",
    "<S> @xcite . </S>",
    "<S> in addition , at the end of output files @xmath0code@xmath1-out.txt , we have introduced two new outputs , wall - clock execution time and cpu time for each run .    </S>",
    "<S> we tested our programs on a workstation with two 10-core intel xeon e5 - 2650 v3 cpus , and present results for all programs compiled with the intel compiler . in table  </S>",
    "<S> [ tab1 ] we show different wall - clock execution times for runs on 1 and 20 cpu cores for fortran and c. the corresponding columns  speedup - f \" and  speedup - c \" give the ratio of wall - clock execution times of runs on 1 and 20 cpu cores , and denotes the actual measured speedup . </S>",
    "<S> for the programs with effectively one spatial variable , the fortran programs turn out to be quicker for small number of cores , whereas for larger number of cores and for the programs with three spatial variables the c programs are faster . </S>",
    "<S> we also studied the speedup of the programs as a function of the number of available cores . </S>",
    "<S> the results for the imag3d fortran and c programs are illustrated in figs .  </S>",
    "<S> [ fig1](a ) and [ fig1](b ) , where we plot the speedup and actual wall - clock time of the imag3d c and fortran programs as a function of number of cores in a workstation with two intel xeon e5 - 2650 v3 cpus , with a total of 20 cores . </S>",
    "<S> the plot in fig .  </S>",
    "<S> [ fig1](a ) shows that the c program parallelizes more efficiently than the fortran program . </S>",
    "<S> however , as the wall - clock time in fortran for a single core is less than that in c , the wall - clock times in both cases are comparable , viz .  fig .  </S>",
    "<S> [ fig1](b ) . </S>",
    "<S> a saturation of the speedup with the increase of the number of cores is expected in all cases . </S>",
    "<S> however , the saturation is attained quicker in fortran than in c programs , and therefore the use of c programs could be recommended for larger number of cpu cores . for a small number of cores </S>",
    "<S> the fortran programs should be preferable . in fig .  </S>",
    "<S> [ fig1](a ) the saturation of the speedup of the fortran program is achieved for approximately 10 cores , when the wall - clock time of the c program crosses that of the fortran program .     + _ additional comments : _ </S>",
    "<S> + this package consists of 24 programs , see program title above . for the particular purpose of each program </S>",
    "<S> , please see descriptions below .     </S>",
    "<S> + _ running time : _ + example inputs provided with the programs take less than 30 minutes in a workstation with two intel xeon processor e5 - 2650 v3 , 2 qpi links , 10 cores ( 25  mb cache , 2.3  ghz ) .     </S>",
    "<S> + program summary ( i ) , ( v ) , ( vi ) , ( vii ) , ( xi ) , ( xii ) + _ program title : _ </S>",
    "<S> imag1d , imagcir , imagsph , real1d , realcir , realsph . </S>",
    "<S> + _ title of electronic files in c : _ ( imag1d.c and imag1d.h ) , ( imagcir.c and imagcir.h ) , ( imagsph.c and imagsph.h ) , ( real1d.c and real1d.h ) , ( realcir.c and realcir.h ) , ( realsph.c and realsph.h ) . </S>",
    "<S> + _ title of electronic files in fortran 90 : _ imag1d.f90 , imagcir.f90 , imagsph.f90 , real1d.f90 , realcir.f90 , realsph.f90 . </S>",
    "<S> + _ maximum ram memory : _ 1  gb for the supplied programs . + _ </S>",
    "<S> programming language used : _ openmp c and fortran 90 . </S>",
    "<S> + _ typical running time : _ minutes on a modern four - core pc . </S>",
    "<S> + _ nature of physical problem : _ these programs are designed to solve the time - dependent nonlinear partial differential gp equation in one spatial variable . + _ method of solution : _ the time - dependent gp equation is solved by the split - step crank - nicolson method by discretizing in space and time . the discretized equation is then solved by propagation in imaginary time over small time steps . </S>",
    "<S> the method yields the solution of stationary problems . </S>",
    "<S> +   + program summary ( ii ) , ( iv ) , ( viii ) , ( x ) +   + _ program title : _ </S>",
    "<S> imag2d , imagaxi , real2d , realaxi . </S>",
    "<S> + _ title of electronic files in c : _ ( imag2d.c and imag2d.h ) , ( imagaxi.c and imagaxi.h ) , ( real2d.c and real2d.h ) , ( realaxi.c and realaxi.h ) . </S>",
    "<S> + _ title of electronic files in fortran 90 : _ imag2d.f90 , imagaxi.f90 , real2d.f90 , realaxi.f90 . </S>",
    "<S> + _ maximum ram memory : _ 1  gb for the supplied programs . + _ programming language used : _ openmp c and fortran 90 . </S>",
    "<S> + _ typical running time : _ hour on a modern four - core pc . </S>",
    "<S> + _ nature of physical problem : _ these programs are designed to solve the time - dependent nonlinear partial differential gp equation in two spatial variables </S>",
    "<S> . + _ method of solution : _ the time - dependent gp equation is solved by the split - step crank - nicolson method by discretizing in space and time . the discretized equation is then solved by propagation in imaginary time over small time steps . </S>",
    "<S> the method yields the solution of stationary problems . </S>",
    "<S> + program summary ( iii ) , ( ix ) +   + _ program title : _ </S>",
    "<S> imag3d , real3d . </S>",
    "<S> + _ title of electronic files in c : _ ( imag3d.c and imag3d.h ) , ( real3d.c and real3d.h ) . </S>",
    "<S> + _ title of electronic files in fortran 90 : _ imag3d.f90 , real3d.f90 . </S>",
    "<S> + _ maximum ram memory : _ 1  gb for the supplied programs . + _ </S>",
    "<S> programming language used : _ openmp c and fortran 90 . </S>",
    "<S> + _ typical running time : _ few hours on a modern four - core pc . </S>",
    "<S> + _ nature of physical problem : _ these programs are designed to solve the time - dependent nonlinear partial differential gp equation in three spatial variables </S>",
    "<S> . + _ method of solution : _ the time - dependent gp equation is solved by the split - step crank - nicolson method by discretizing in space and time . the discretized equation is then solved by propagation in imaginary time over small time steps . </S>",
    "<S> the method yields the solution of stationary problems . </S>"
  ]
}