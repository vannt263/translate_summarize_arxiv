{
  "article_text": [
    "many algorithms , such as spectral clustering , kernel principal component analysis or more generally kernel - based methods , are based on estimating eigenvalues and eigenvectors of integral operators defined by a kernel function , from a given random sample . to set the context from a statistical point of view ,",
    "let @xmath0 be an unknown probability distribution on a compact space @xmath1 and let @xmath2 be a kernel on @xmath1 .",
    "the goal is to estimate the integral operator @xmath3 from an i.i.d .",
    "random sample drawn according to @xmath4 + a first study on the relationship between the spectral properties of a kernel matrix and the corresponding integral operator can be found in  @xcite for the case of a symmetric square integrable kernel @xmath5 they prove that the ordered spectrum of the kernel matrix @xmath6 converges to the ordered spectrum of the kernel integral operator @xmath7 .",
    "connections between this empirical matrix and its continuous counterpart have been subject of much research , for example in the framework of kernel - pca ( e.g.  @xcite ,  @xcite ,  @xcite ) and spectral clustering ( e.g.  @xcite ) . in  @xcite , the authors study the connection between the spectral properties of the empirical kernel matrix @xmath8 and those of the corresponding integral operator @xmath7 by introducing two extension operators on the ( same ) reproducing kernel hilbert space defined by @xmath2 , that have the same spectrum ( and related eigenfunctions ) as @xmath9 and @xmath7 respectively .",
    "in such a way they overcome the difficulty of dealing with objects ( @xmath10 and @xmath7 ) operating in different spaces . +",
    "the integral operator @xmath7 is related to the gram operator @xmath11 where @xmath12 denotes the reproducing kernel hilbert space defined by the kernel @xmath2 and @xmath13 the corresponding feature map .",
    "+ the main objective of this paper is to estimate gram operators on ( infinite - dimensional ) hilbert spaces .",
    "some bounds on the deviation of the empirical gram operator from the true gram operator in separable hilbert spaces can be found in  @xcite in the case of gaussian random vectors .",
    "+ let us introduce some notation .",
    "we denote by @xmath14 a separable hilbert space and by @xmath15 a ( possibly unknown ) probability distribution on @xmath14 . remark that with the above notation @xmath16 our goal is to estimate the gram operator @xmath17 defined as @xmath18 from an i.i.d . sample drawn according to @xmath19 our approach consists in first considering the finite - dimensional setting where @xmath20 is a random vector in @xmath21 and then in generalizing the results to the infinite - dimensional case of separable hilbert spaces . to be able to go from finite to infinite dimension",
    "we will establish dimension - free inequalities . to be more precise , we consider the related problem of estimating the quadratic form @xmath22 which rewrites explicitly as @xmath23 in the finite - dimensional setting we construct an estimator of the quadratic form @xmath24 and we provide non - asymptotic dimension - free bounds for the approximation error that hold under weak moment assumptions .",
    "observe that in the finite - dimensional case the quadratic form @xmath24 can be seen as the quadratic form associated to the gram matrix @xmath25 observe also that the study of the gram matrix is of interest in the case of a non - centered criterion and it coincides , in the case of centered data ( i.e. @xmath26=0 $ ] ) , with the study of the covariance matrix @xmath27 ) ( x -   \\mathbb e[x])^{\\top}\\right].\\ ] ] many theoretical results have been proposed on the estimation of covariance matrices , e.g.  @xcite ,  @xcite ,  @xcite .",
    "these results follow from the study of random matrix theory and use as an estimator of @xmath28 the matrix obtained by replacing the unknown probability distribution @xmath29 with the sample distribution @xmath30 in  @xcite the non - commutative khintchine inequality is used to obtain bounds on the sample covariance matrix of a bounded random vector .",
    "non - asymptotic results are obtained in  @xcite as a consequence of the analysis of random matrices with independent rows , while in  @xcite the author uses an extension of the bernstein inequality to matrices .",
    "however , such an empirical estimator becomes less efficient when the data have a long tail distribution . in",
    "@xcite the author presents a different estimator based on the geometrical median which is more robust that the classical empirical one .",
    "+ we first present a way to construct a robust estimator of the gram matrix @xmath28 in finite dimension and then we extend the results to the infinite - dimensional case .",
    "the paper is organized as follows .",
    "section [ sec1 ] deals with the finite - dimensional case .",
    "we provide a new robust estimator of the gram matrix @xmath28 and we use a pac - bayesian approach to obtain non - asymptotic dimension - independent bounds of its approximation error . in section  [ sec2 ]",
    "we extend the results to the infinite - dimensional case , taking advantage of the fact that they are independent of the dimension of the ambient space . in section  [ sec_emp ]",
    "we propose some empirical results to show the performance of our estimator . in appendix  [ emp_g ]",
    "we compare from a theoretical point of view the behavior of our robust estimator to the one of the classical empirical estimator .",
    "finally in appendix  [ sec3 ] we extend the results to estimate the expectation of a symmetric random matrix and we consider the problem of estimating the covariance matrix in the case when the expectation is unknown .",
    "let @xmath31 be an unknown probability distribution on @xmath21 and let @xmath32 be a random vector of law @xmath19 we denote by @xmath33 the expectation with respect to @xmath29 .",
    "our goal is to estimate the quadratic form @xmath34 , \\quad \\theta \\in \\mathbb r^d,\\ ] ] ( that computes the energy in the direction @xmath35 ) from an i.i.d . sample @xmath36 drawn according to @xmath19 observe that @xmath24 can be seen as the quadratic form associated to the gram matrix @xmath37.\\ ] ] indeed to recover the gram matrix @xmath28 from the above quadratic form it is sufficient to use the polarization identity @xmath38\\\\ & =   \\frac{1}{4 } \\left [ n(e_i+e_j ) - n(e_i - e_j ) \\right]\\end{aligned}\\ ] ] where @xmath39 is the canonical basis of @xmath40    a classical empirical estimator of the quadratic form @xmath24 is @xmath41 obtained by replacing the unknown probability distribution @xmath29 with the sample distribution .",
    "however , as shown in  @xcite , if the distribution of @xmath42 has a heavy tail for some values of @xmath35 , the quality of the approximation provided by the classical empirical estimator can be improved , using some @xmath43-estimator with a suitable influence function and a scale parameter depending on the sample size .",
    "thus , for any @xmath44 and any @xmath45 , we consider @xmath46 where the function @xmath47 , defined as @xmath48    is symmetric non - decreasing and bounded and satisfies @xmath49    introduce @xmath50 and observe that , since the function @xmath51 is continuous , @xmath52 as soon as @xmath53 moreover , since the function @xmath54 is close to the identity in a neighborhood of the origin , @xmath55 and therefore it is natural to consider as an estimator of @xmath24 a quantity related to @xmath56 , for a suitable value of @xmath57 we consider the family of ( robust ) estimators @xmath58 and we observe that , since @xmath59 is homogeneous of degree @xmath60 in @xmath35 , @xmath61    in the following we will use a pac - bayesian approach linked to gaussian perturbations of the parameter @xmath35 to first construct a confidence region for @xmath24 and then define and study a robust estimator by choosing a suitable value @xmath62 for the parameter @xmath63 . + given @xmath44 , we consider the family of gaussian perturbation @xmath64 of mean the true value @xmath35 and covariance matrix @xmath65 where @xmath66 is a free parameter .",
    "+ let @xmath67 be a finite set of possible values of the couple of parameters @xmath68 and @xmath69 its cardinality .",
    "let us introduce @xmath70^{1/4 } \\quad \\text{and } \\quad \\kappa   = \\sup_{\\substack{\\theta \\in \\mathbb{r}^d \\\\ \\mathbb e [ \\langle \\theta , x \\rangle^2 ] > 0 } } \\frac{\\displaystyle\\mathbb e \\bigl [ \\langle \\theta , x \\rangle^4 \\bigr]}{\\mathbb e \\bigl [ \\langle \\theta , x \\rangle^2 \\bigr]^2}\\end{aligned}\\ ] ] assuming that these two quantities are finite .   and",
    "@xmath71 by their upper bounds . ] for any @xmath72 and @xmath73 , we put @xmath74 where @xmath75    [ prop0 ] with probability at least @xmath76 , for any @xmath77 , any @xmath72 , @xmath78 where @xmath79 and @xmath80 are non - increasing functions defined as @xmath81\\\\ \\phi_{\\theta,+}(t ) & =   t \\ , \\left ( 1 + \\frac{\\gamma + \\delta \\lambda \\lvert \\theta \\rvert^2 / t}{1 - \\mu - \\gamma - 2 \\delta \\lambda \\lvert\\theta \\rvert^2 / t } \\right)^{-1 }    \\mathds{1 } \\bigg [ \\xi+ \\mu+\\gamma + 2 \\delta \\lambda \\lvert \\theta\\rvert^2 / t < 1 \\bigg].\\end{aligned}\\ ] ]    for the proof we refer to section  [ proof_prop0 ] .",
    "+ observe that since those functions depend on @xmath35 only through @xmath82 , if @xmath35 is such that @xmath83 it is natural to omit the dependence of @xmath35 and write @xmath84 and @xmath85 . in the following",
    "we will omit the dependence of @xmath35 of the functions defined in proposition  [ prop0 ] , so that we write @xmath84 and @xmath85 instead of @xmath79 and @xmath80 .",
    "[ prop1 ] let @xmath86 be any energy level .",
    "we consider the set @xmath87 and the bound @xmath88 with probability at least @xmath89 , for any @xmath77 , any @xmath72 , @xmath90.\\ ] ]    we observe that , for any @xmath91 , if @xmath92 then @xmath93 since , by definition @xmath94 and similarly if @xmath95 , then @xmath96 thus , according to the definition of @xmath97 in equation  , we get @xmath98 and moreover @xmath99 which concludes the proof .    from now on we fix the finite set @xmath100 of all possible values of the couple @xmath68 as @xmath101 where @xmath102 , with @xmath103 , and @xmath104}.\\end{aligned}\\ ] ] we put @xmath105\\ ] ] and we define our robust estimator as @xmath106    [ prop2 ] let us fix a threshold @xmath107 and set the value of the parameter @xmath108 to @xmath109 .",
    "introduce @xmath110 where @xmath111",
    "$ ] denotes the trace of the gram matrix , and @xmath112 \\zeta _ * ( \\max \\ { t , \\sigma \\ } ) \\leq \\sqrt{n } \\\\",
    "+ \\infty & \\text { otherwise . } \\end{cases}\\ ] ] with probability at least @xmath89 , for any @xmath77 , @xmath113.\\ ] ]    for the proof we refer to section  [ proof_prop2 ] .",
    "remark that equation of the proof provides a bound for any choice of the parameter @xmath114 and that we reported here only the numerical value of the bound when @xmath115 for the sake of simplicity .",
    "+ assuming any reasonable bound on the sample size we can bound the logarithmic factor @xmath116 hidden in @xmath117 with a relatively small constant . in particular , if we choose @xmath118 , we get @xmath119 .",
    "observe that the bound @xmath120 does not depend explicitly on the dimension @xmath121 of the ambient space .",
    "more specifically , the dimension has been replaced by the entropy term @xmath122 moreover , we do not need to know the exact values of @xmath71 and @xmath123 to compute the estimator and evaluate the bound , it is sufficient to know upper bounds instead .",
    "indeed , if we use those upper bounds to define our estimator , the above result is still true with @xmath71 and @xmath123 replaced by their upper bounds .",
    "+ we also observe that in order to have a meaningful ( finite ) bound we can choose the threshold @xmath124 such that @xmath125 so that @xmath126 for any @xmath127 , assuming that @xmath128 .",
    "more precisely , using the inequality @xmath129 , we see that equation holds when @xmath130 with this choice the threshold @xmath124 decays to zero at speed @xmath131 as the sample size grows to infinity .",
    "remark that the estimator @xmath132 is not necessarily a quadratic form .",
    "we conclude this section by introducing and studying a quadratic estimator of @xmath133 + we observe that proposition  [ prop0 ] provides a confidence region for @xmath24 .",
    "define @xmath134 where we recall that @xmath135 and @xmath100 is defined in equation  . according to proposition  [ prop0 ] , with probability at least @xmath76 , for any @xmath44 , @xmath136 from a theoretical point of view we can consider as an estimator of @xmath137 any quadratic form belonging to the confidence interval @xmath138 $ ] for any @xmath35 .",
    "such a quadratic form exists with probability at least @xmath89 according to equation .",
    "however , from an algorithmic point of view , we would like to impose these constraints only for a finite number of directions @xmath35 .",
    "in particular , in the following we are going to study the properties of a symmetric matrix @xmath139 that satisfies @xmath140 and @xmath141 where @xmath142 is any finite @xmath143-net of the unit sphere @xmath144 , meaning that @xmath145 the matrix @xmath139 can be computed using a convex optimization algorithm as described in  @xcite or  @xcite .",
    "+ from now on let @xmath1460,s_4 ^ 2]$ ] be a threshold such that @xmath147 .",
    "next proposition provides the analogous for the quadratic form @xmath148 of the dimension - free bound presented in proposition  [ prop2 ] for @xmath149 .",
    "[ prop3 ] with the same notation as in proposition  [ prop2 ] , with probability at least @xmath150 for any @xmath151 , @xmath152    since for any @xmath153 there is @xmath154 such that @xmath155 we have @xmath156 let us put @xmath157 we observe that , with probability at least @xmath150 @xmath158 where @xmath85 and @xmath84 are defined in proposition  [ prop0 ] and depend on @xmath35 only through @xmath82 .",
    "indeed , in the event of probability at least @xmath76 described in equation  , @xmath159 since equation   also holds for @xmath137 , and in the same way we get @xmath160 which proves equation  . according to corollary  [ lem1a ] in section  [ appx ] we conclude the proof .    according to equation  , for any @xmath153 @xmath161 where @xmath162 is such that @xmath163",
    "there is still some improvement to bring , since at this stage we are not sure that @xmath139 is non - negative .",
    "decomposing @xmath139 into its positive and negative parts and writing @xmath164 , we can consider as an estimator of @xmath28 the positive semi - definite symmetric matrix @xmath165 as shown in the following proposition .",
    "[ prop4 ] with probability at least @xmath89 , for any @xmath166 , @xmath167 where @xmath120 is defined in proposition [ prop2 ] .",
    "let us put as before @xmath157 for any @xmath166 , there is @xmath154 such that @xmath168 , so that , according to equation  , @xmath169 then we deduce that @xmath170 therefore , for any @xmath166 , @xmath171 combining the above equation with proposition  [ prop3 ] we conclude the proof .",
    "in this section we extend the results obtained in the previous section to the infinite - dimensional setting .",
    "+ let @xmath14 be a separable hilbert space and let @xmath172 be an unknown probability distribution on @xmath173 we consider the gram operator @xmath17 defined by @xmath174 and we assume @xmath175 where @xmath176 denotes a random vector of law @xmath19 in analogy to the previous section we denote by @xmath137 the quadratic form associated with the gram operator @xmath177 so that @xmath178 we consider @xmath179 an increasing sequence of subspaces of @xmath14 of finite dimension such that @xmath180 and we endow each space @xmath181 with the probability @xmath182 which arises from the disintegration of @xmath29 , meaning that , if @xmath183 is the orthogonal projector on @xmath181 , @xmath184 .",
    "we denote by @xmath185 the quadratic form in @xmath181 associated with the probability distribution @xmath182 and we observe that , for any @xmath186 , we have @xmath187 in the following we consider i.i.d .",
    "samples of size @xmath188 in @xmath14 drawn according to @xmath19 according to proposition [ prop0 ] , the event @xmath189 is such that @xmath190 . since @xmath191 , by the continuity of measure , @xmath192 this means that with probability at least @xmath89 , for any @xmath193 and any @xmath72 , @xmath194 consequently , since @xmath195 , for any @xmath196 , the following result holds .",
    "[ prop_hspace ] with probability at least @xmath89 , for any @xmath196 , @xmath197 where @xmath198    if we do not want to go to the limit , we can use the explicit bound @xmath199 this bound depends on @xmath200 .",
    "we will see in the following another bound that goes uniformly to zero for any @xmath201 when @xmath2 tends to infinity . in the same way , proceeding as already done in the previous section we state the analogous of proposition  [ prop2 ] .",
    "[ prop1.40fr ] let @xmath202 be known constants and put @xmath203 where @xmath204 .",
    "define @xmath205 and consider , according to equation  , the estimator @xmath206 for any @xmath196 , define @xmath207 by choosing any accumulation point of the sequence @xmath208 .",
    "define the bound @xmath209 where @xmath146 0 , s_4 ^ 2]$ ] is some energy level such that @xmath210 \\zeta _ * ( \\sigma ) \\leq \\sqrt{n}.\\ ] ] with probability at least @xmath89 , for any @xmath186 , @xmath211.\\ ] ]    this is a consequence of the fact that @xmath212 and of the continuity of @xmath120 .",
    "as already discuss at the end of proposition  [ prop2 ] , any reasonable bound on the sample size @xmath188 allows bounding the logarithmic factor @xmath117 by a relatively small constant .",
    "in particular , putting @xmath118 , we get @xmath119 .    in the following",
    "we construct an estimator of the gram operator @xmath177 .",
    "let @xmath213 be an i.i.d .",
    "sample drawn according to @xmath19 define @xmath214 and @xmath215 let @xmath216 be a @xmath143-net of @xmath217 ( where @xmath218 denotes the unit sphere in @xmath14 ) and remark that @xmath216 is finite because @xmath219 .",
    "we compute a linear operator @xmath220 such that @xmath221 and @xmath222 observe that @xmath223 plays the same role as the symmetric matrix @xmath224 in the finite - dimensional setting .",
    "we consider as an estimator of @xmath225 the operator @xmath226 where @xmath227 is the orthogonal projector on @xmath228 .",
    "let us decompose @xmath229 in its positive and negative parts and write @xmath230    [ propq ] for any threshold @xmath86 such that @xmath107 and @xmath231 , with probability at least @xmath89 , for any @xmath201 and for any @xmath2 , @xmath232    \\bigl\\lvert \\max \\bigl\\ { \\langle \\theta , \\mathcal{q}_+ \\theta \\rangle_{\\mathcal h } , \\sigma \\bigr\\ } & - \\max \\bigl\\ { \\langle \\pi_k \\theta , \\mathcal{g } \\pi_k \\theta \\rangle_{\\mathcal h } , \\sigma \\bigr\\ } \\bigr\\rvert \\\\ & \\leq 2 \\max \\bigl\\ { \\langle \\theta , \\mathcal{q}_+\\theta \\rangle_{\\mathcal h } , \\sigma \\bigr\\ } b _ * \\bigl (   \\min \\ { \\langle \\theta , \\mathcal{q}_+ \\theta \\rangle_{\\mathcal h } , s_4 ^ 2 \\ } \\bigr )   + 7 \\delta \\sqrt { \\mathbf{tr}(\\mathcal{g}^2)}.\\end{aligned}\\ ] ]    for the proof",
    "we refer to section  [ proof_propq ] .",
    "let us consider @xmath233 an orthonormal basis of eigenvectors of @xmath225 such that the corresponding sequence of eigenvalues @xmath234 is non - increasing .",
    "[ prop1.41fm ] consider some threshold @xmath86 such that @xmath107 and @xmath231 .",
    "with probability at least @xmath89 , for any @xmath201 and for any @xmath2 , @xmath235 where @xmath236    it is enough to observe that @xmath237 and , for any @xmath201 , we have @xmath238 indeed , @xmath239 so that @xmath240 and in the same way @xmath241    remark that we can use this result to bound @xmath242 , using the inequality @xmath243    let us mention to conclude this section another way to extend the results from finite dimension to separable hilbert spaces .",
    "this second method consists in defining directly a gaussian perturbation of the parameter in the hilbert space . indeed , taking a basis",
    ", we can identify any separable hilbert space @xmath244 with the sequence space @xmath245 .",
    "we can take as our prior parameter distribution @xmath246 the law on @xmath247 of a sequence of independent one - dimensional gaussian random variables with mean @xmath248 and variance @xmath249 .",
    "for any @xmath250 , we can then define the posterior @xmath251 as @xmath252 .",
    "one can prove that @xmath253 moreover , when @xmath254 , @xmath255 exists @xmath251-almost surely and is a gaussian process indexed by @xmath256 , with mean @xmath257   = \\langle   \\theta , x \\rangle_{\\mathcal{h } } , \\qquad x \\in \\mathcal{h},\\ ] ] and covariance @xmath258 - \\mathbb{e } \\bigl [ w(x ) \\bigr ] \\mathbb{e }   \\bigl [ w(y ) \\bigr ] =    \\frac{1}{\\beta } \\langle x , y \\rangle_{\\mathcal{h } } ,   \\qquad x , y \\in \\mathcal{h}.\\ ] ] using these properties , we can directly generalize to @xmath259 the pac - bayesian bounds stated in @xmath260 in the previous section .",
    "nevertheless , we thought it was interesting to study more explicitly how the infinite dimension case could be approximated by a finite dimension estimator , as done in this section .",
    "we present some empirical results which show the performance of our robust estimator .",
    "we use here a simplified construction that does not lead exactly to the estimator @xmath139 , for which we have proved theoretical results in the previous section , but should nonetheless exhibit the same kind of behavior . +",
    "we use an iterative scheme based on the polarization formula to estimate the coefficients of the gram matrix in an orthonormal basis of eigenvectors and then update this basis iteratively to a basis of eigenvectors of the current estimate .",
    "let @xmath36 be a sample drawn according to the probability distribution @xmath29 and let @xmath261 let @xmath262 and define @xmath263 as the solution of @xmath264=0.\\ ] ] in practice we compute @xmath265 using the newton algorithm .",
    "we observe that , when @xmath266 and @xmath63 is suitably chosen , @xmath263 is an approximation of the estimator @xmath149 of the quadratic form @xmath24 .",
    "+ define @xmath267 as the solution obtained when the parameter @xmath63 is set to @xmath268}\\ ] ] where @xmath269 , @xmath270 and @xmath271 . + according to @xcite , this value of the scale parameter @xmath63 should be close to optimal for the estimation of a single expectation from an empirical sample distribution .",
    "let @xmath272 be the eigenvalues of the empirical gram matrix @xmath273 , that will be our starting point , and let @xmath274 be a corresponding orthonormal basis of eigenvectors .",
    "we decompose the empirical gram matrix as @xmath275 where @xmath276 is the orthogonal matrix whose columns are the eigenvectors of @xmath273 and @xmath277 is the diagonal matrix @xmath278 .",
    "we observe that , by the polarization formula , @xmath279 , \\quad i , j = 1 , \\dots , d,\\ ] ] where @xmath280 , with @xmath281 , is approximated by @xmath282 taking notation , for any @xmath283 matrix @xmath284 , we define @xmath285 as the @xmath286 matrix of entries @xmath287.\\ ] ] let @xmath288 be the matrix whose @xmath289-th row is the vector @xmath290 , so that @xmath291 we update the gram matrix estimate to @xmath292 then we iterate the update scheme , decomposing @xmath293 as @xmath294 where @xmath295 and @xmath296 is a diagonal matrix and computing @xmath297 the inductive update step is more generally the following .",
    "assuming we have constructed @xmath298 , we decompose it as @xmath299 where @xmath300 and @xmath301 is a diagonal matrix and we define the new updated estimator of @xmath28 as @xmath302 we stop this iterative estimation scheme when @xmath303 falls under a given threshold . in the following numerical experiment we more simply performed four updates .",
    "we take as our robust estimator of @xmath28 the last update @xmath298 .",
    "we now present an example of the performance of this estimator , for some i.i.d .",
    "sample of size @xmath304 in @xmath305 drawn according to the gaussian mixture distribution @xmath306 where @xmath307 and @xmath308[0pt]{\\large 0 } \\\\    &   &   & \\ddots & \\\\    &   &   \\hspace{10pt}\\raisebox{8pt}[0pt][0pt]{\\large 0 } & & 0.01   \\end{array } \\right].\\ ] ] the gram matrix of @xmath309 is equal to @xmath310[0pt]{\\large 0 } \\\\    &   &   & \\ddots & \\\\    &   &   \\hspace{10pt}\\raisebox{8pt}[0pt][0pt]{\\large 0 } & & 0.8095   \\end{array } \\right].\\ ] ]    this example illustrates a favorable situation where the performance of the robust estimator is particularly striking when compared to the empirical gram matrix . as it can be seen on the expression of the sample distribution as well as on the configuration plots below ,",
    "this is a situation of intermittent high variance : the sample is a mixture of a rare high variance signal and a frequent low variance more structured signal .",
    "+ we tested the algorithm on 500 different samples drawn according to the gaussian mixture distribution defined above .",
    "random sample configurations are presented in figure  [ fig1 ] .",
    "+ figure [ fig2 ] shows that the robust estimator @xmath139 significantly improves the error in terms of square of the frobenius norm when compared to the empirical estimator @xmath311 .",
    "the red solid line represents the empirical quantile function of the errors of the robust estimator , whereas the blue dotted line represents the quantiles of @xmath312 . +",
    "this quantile function is obtained by sorting the 500 empirical errors in increasing order . + the mean of the square distances @xmath313 on 500 trials is @xmath314 , where the indicated mean estimator and confidence interval is the non - asymptotic confidence interval given by proposition 2.4 of @xcite at confidence level @xmath315 . in the case of the empirical estimator ,",
    "the mean is @xmath316 .",
    "the empirical standard deviations are respectively @xmath317 and @xmath318 .",
    "so we see that in this case the robust estimator reliably decreases the error by a factor larger than @xmath317 and also produces errors with a much smaller standard deviation from sample to sample .",
    "+ in appendix  [ emp_g ] we show from a theoretical point of view that the two estimators @xmath139 and @xmath311 behave in a similar way in light tail situations .",
    ", the blue dotted line represents the distances @xmath319 .",
    ", height=188 ]",
    "in this section we propose the proofs of the results presented in the previous sections .",
    "more precisely , section  [ proof_prop0 ] refers to proposition  [ prop0 ] ( on page ) , section  [ proof_prop2 ] refers to proposition  [ prop2 ] ( on page ) and section  [ proof_propq ] to proposition  [ propq ] ( on page ) .",
    "finally section  [ appx ] is a ( needed ) technical section .",
    "the proof of proposition  [ prop0 ] requires a sequence of preliminary results . +",
    "our approach relies on perturbing the parameter @xmath35 with the gaussian perturbation @xmath320 where @xmath66 is a free parameter .",
    "we have @xmath321    let @xmath322 be a random variable drawn according to @xmath323 therefore , @xmath324 is one - dimensional gaussian random variable with mean @xmath325 and variance @xmath326 .",
    "consequently @xmath327 ^ 2 + \\mathbf{var}[\\langle w , x\\rangle ] = \\langle \\theta , x \\rangle^2 + \\frac{\\lvert x \\rvert^2}{\\beta}.\\ ] ]    accordingly we get @xmath328 .",
    "\\end{aligned}\\ ] ] in order to pull the expectation with respect to @xmath251 out of the influence function @xmath54 , with a minimal loss of accuracy , we introduce the function @xmath329 where @xmath330 $ ] is such that @xmath331 and @xmath332 is defined by the condition @xmath333 using the explicit expression of the first and second derivative of @xmath54 , we get @xmath334 and @xmath335 + \\frac{1 + 2 \\sqrt{2}}{2 } .$ ]    for any @xmath336 , @xmath337    we first prove that @xmath338 .",
    "the inequality is trivial for @xmath339 since @xmath340 for @xmath341,$ ] performing a taylor expansion at @xmath342 we obtain that @xmath343 since @xmath344 for @xmath345 .",
    "finally we observe that , for any @xmath346 , @xmath347 let us now show that @xmath348 for @xmath349 , we have already seen that the inequality is satisfied since @xmath340 moreover we observe that the function @xmath350 is such that @xmath351 and also @xmath352 performing a taylor expansion at @xmath342 we get @xmath353 since for any @xmath354 $ ] , @xmath355 we deduce that @xmath356 in particular , @xmath357 .",
    "recalling that @xmath358 is an increasing function while @xmath359 is constant on the interval @xmath360 , we conclude the proof .",
    "next lemma allows us to pull the expectation with respect to @xmath251 out of the function @xmath359 .",
    "[ l1 ] let @xmath361 be a measurable space .",
    "for any @xmath362 and any @xmath363 , @xmath364 where by definition @xmath365 moreover , @xmath366    to prove equation   we observe that performing a taylor expansion of the function @xmath359 at @xmath367 @xmath368   \\geq \\chi(z ) + \\bigl ( h(\\theta ) - z \\bigr ) \\chi'(z )   + \\inf \\chi '' \\   \\frac { \\bigl ( h(\\theta ) - z \\bigr)^2}{2},\\ ] ] so that , recalling that @xmath369 , we get @xmath370 \\ ,",
    "\\mathrm{d } \\rho ( \\theta ) & \\geq \\chi \\bigl ( \\int h \\ , \\mathrm{d } \\rho\\bigr ) - \\frac{1}{8 } \\int \\left ( h(\\theta ) - \\int h(\\theta )   \\mathrm{d } \\rho \\right)^2 \\mathrm{d } \\rho(\\theta)\\\\ & = \\chi \\bigl ( \\int h \\ , \\mathrm{d } \\rho\\bigr ) - \\frac{1}{8 } \\mathbf{var } \\bigl ( h \\ , \\mathrm{d } \\rho \\bigr).\\end{aligned}\\ ] ] combining equation   with the fact that @xmath338 , for any @xmath336 , we obtain that @xmath371 moreover , since @xmath372 we conclude the proof .",
    "applying this result to our problem we obtain @xmath373 \\bigr\\},\\end{gathered}\\ ] ] where , putting @xmath374 , @xmath375 and denoting by @xmath376 a centered gaussian random variable , @xmath377 & = \\mathbf{var } \\bigl [ ( m + w)^2 \\bigr ] \\\\ & = 4 m^2 \\sigma^2 + 2 \\sigma^4   = \\frac{4 \\langle \\theta , x \\rangle^2 \\lvert x \\rvert^2 } { \\beta } + \\frac{2 \\lvert x \\rvert^4}{\\beta^2}.\\end{aligned}\\ ] ]    let us remark that , for any @xmath378 and @xmath379 , @xmath380 since @xmath381 .",
    "therefore , taking the expectation with respect to @xmath284 of this inequality and remarking that @xmath284 and @xmath382 have the same probability distribution we get @xmath383 .\\ ] ] thus in our context we put @xmath384 , @xmath385 and @xmath386 and we obtain @xmath387    [ lem1.5 ] for any positive constants @xmath388 and any @xmath389 , @xmath390    for any positive real constants @xmath391 , @xmath392 since the function @xmath393 is non - decreasing for @xmath394 it follows that @xmath395.\\ ] ]    applying this inequality to @xmath396 $ ] and reminding that @xmath397 , we conclude the proof .    as a consequence ,",
    "choosing @xmath398 @xmath399 and @xmath400 , we get @xmath401 \\ , \\mathrm{d }   \\pi_{\\theta}(\\theta ' ) , \\end{gathered}\\ ] ] where @xmath402 + we observe that the above inequality allows to compare @xmath403 with the expectation with respect to the gaussian perturbation @xmath251 . in terms of the empirical criterion @xmath404 we have proved that @xmath405 \\ , \\mathrm{d } \\pi_{\\theta}(\\theta ' ) .",
    "\\end{gathered}\\ ] ] we are now ready to use the following general purpose pac - bayesian inequality .",
    "[ pac]let @xmath406 be a prior probability distribution on @xmath260 and let @xmath407 $ ] be a measurable function with @xmath408 .",
    "with probability at least @xmath409 for any posterior distribution @xmath410 , @xmath411   \\ , \\mathrm{d } \\rho(\\theta ' ) + \\frac{\\mathcal{k}(\\rho , \\nu ) + \\log \\bigl (   \\epsilon^{-1 } \\bigr)}{n},\\ ] ] where @xmath412 is the kullback divergence of @xmath413 with respect to @xmath414 . by convention , a non measurable event is said to happen with probability at least @xmath415 when it includes a measurable event of probability non - smaller than @xmath415 .    for the proof we refer to @xcite or  @xcite .    in our context",
    ", we consider as prior distribution @xmath416 and the family of posterior distributions consisting in the gaussian perturbations @xmath417 so that the kullback divergence is given by @xmath418 we observe that since the result holds for any choice of the posterior , it allows us to obtain uniform results in @xmath419 more precisely , we apply the above pac - bayesian inequality to @xmath420,\\ ] ] where @xmath421 . using the fact that @xmath422 , we get that , with probability at least @xmath409 for any @xmath423 @xmath424 \\mathrm d \\pi_{\\theta } ( \\theta ' ) \\\\ & \\hspace{45 mm } +   \\frac{\\beta \\|\\theta\\|^2}{2n } + \\frac { \\log(\\epsilon^{-1})}{n}\\\\ & = \\mathbb e \\biggl [ \\langle   \\theta , x \\rangle^2 - \\lambda+ \\frac{1}{2 } \\biggl ( \\bigl ( \\langle \\theta , x \\rangle^2 - \\lambda \\bigr)^2 + \\frac{4 \\langle \\theta , x \\rangle^2 \\lvert x \\rvert^2}{\\beta } + \\frac{2 \\lvert x \\rvert^4}{\\beta^2 } \\biggr ) \\\\   & \\hskip45 mm   + \\frac{c \\lvert x \\rvert^2}{\\beta } \\biggl ( \\langle \\theta , x \\rangle^2 + \\frac{3 \\lvert x \\rvert^2}{2 \\beta } \\biggr ) \\ , \\biggr ] + \\frac{\\beta",
    "\\lvert \\theta \\rvert^2}{2 n } + \\frac{\\log(\\epsilon^{-1})}{n}.\\end{aligned}\\ ] ] to obtain the last line we have used the fact that @xmath425 & = \\frac{4 \\langle \\theta , x \\rangle^2 \\lvert x \\rvert^2}{\\beta }   + \\frac{2 \\lvert x \\rvert^4}{\\beta}. \\end{aligned}\\ ] ]    let us recall the definition of @xmath426 and @xmath71 introduced in equation  .",
    "we have defined @xmath427^{1/4 }   \\quad \\text{and } \\quad \\kappa = \\sup_{\\substack{\\theta \\in \\mathbb{r}^d \\\\ \\mathbb e [ \\langle \\theta , x \\rangle^2 ] > 0 } } \\frac{\\displaystyle\\mathbb e \\bigl",
    "[ \\langle \\theta , x \\rangle^4 \\bigr]}{\\mathbb e \\bigl [ \\langle \\theta , x \\rangle^2 \\bigr]^2}.\\ ] ] using the cauchy - schwarz inequality , since @xmath428 & \\leq \\kappa n(\\theta)^2\\\\ \\mathbb e[\\langle \\theta , x\\rangle^2 \\|x\\|^2 ] & \\leq \\kappa^{1/2 } s_4 ^ 2 n(\\theta),\\end{aligned}\\ ] ] we get that , with probability at least @xmath409 for any @xmath423 @xmath429 ^ 2 + \\biggl [ 1 + ( \\kappa -1 ) \\lambda + \\frac{(2+c ) \\kappa^{1/2 } s_4 ^ 2}{\\beta }   \\ , \\biggr ] \\bigl [ n(\\theta ) -",
    "\\lambda \\bigr ] \\\\",
    "+ \\frac{(\\kappa-1 ) \\lambda^2}{2 } +   \\frac{(2+c ) \\kappa^{1/2 } s_4 ^ 2 \\lambda}{\\beta } +   \\frac{(2 + 3c ) s_4 ^ 4}{2 \\beta^2 } + \\frac{\\beta \\lvert \\theta \\rvert^2}{2n }   + \\frac{\\log(\\epsilon^{-1})}{n}. \\end{gathered}\\ ] ]    according to the ( compact ) notation introduced in equation  , the above inequality rewrites as @xmath430 similarly , observing that @xmath431 we obtain a lower bound for the empirical criterion @xmath432 namely , with probability at least @xmath415 , for any @xmath77 , any @xmath72 , @xmath433 we now combine the two bounds above to get the confidence region for @xmath24 defined in proposition  [ prop0 ] .",
    "assume that both equation   and equation   hold for any @xmath44 , an event that happens with probability at least @xmath434 + let us introduce @xmath435 and @xmath436 \\ , z   - \\gamma - \\tau(\\theta ) , \\qquad z \\in \\mathbb{r}.\\ ] ] we observe that @xmath437 , and consequently @xmath438 for any @xmath439 .",
    "we consider the case when @xmath440 , meaning that @xmath441 in this case , the second degree polynomial @xmath442 has two distinct real roots , @xmath443 and @xmath444 , where @xmath445 ^ 2 - 4 \\xi \\bigl [ \\gamma + \\tau(\\theta ) \\bigr]}}{2 \\xi } , \\qquad \\sigma\\in \\{1,- 1\\}.\\ ] ] since equation can also be written as @xmath446 $ ] , we get @xmath447 ^ 2 } { 1 - \\mu - \\gamma - 2 \\tau(\\theta ) } + \\bigl [ 1 - \\mu - \\tau(\\theta ) \\bigr ] \\frac{\\gamma + \\tau(\\theta)}{1 - \\mu - \\gamma - 2 \\tau(\\theta ) } - \\gamma - \\tau(\\theta ) = 0 , \\end{gathered}\\ ] ] which implies @xmath448 therefore , since according to equation , for any @xmath449 $ ] , @xmath450 it holds that @xmath451 \\cap \\bigl ] z_{-1 } , z_{+1 } \\bigr [ =   \\emptyset.\\ ] ] observing that @xmath452 , it follows that @xmath453 this proves that , for any @xmath77 satisfying equation , @xmath454 which rewrites as @xmath455 \\leq \\frac{\\lambda}{\\widehat{\\alpha}(\\theta)^2}.\\ ] ] moreover , this inequality is trivially true when condition is not satisfied , because its left - hand side is equal to zero and its right - hand side is non - negative . + proving the second part of the proposition requires a new argument and not a mere update of signs in the proof of the first part .",
    "although it may seem at first sight that we are just aiming at a reverse inequality , the situation is more subtle than that .",
    "let us first remark that in the case when @xmath456 is not satisfied , the bound @xmath457 is trivially satisfied because the left - hand size is equal to zero . in the case when equation is true , it holds that @xmath458 , so that @xmath459 = 0 $ ] , and therefore , according to equation , @xmath460 where @xmath461 + since condition can also be written as @xmath462 , it implies that the second order polynomial @xmath463 has two roots and that @xmath464 is on the right of its largest root , which is larger than @xmath60 . on the other hand , we observe that , under condition , putting @xmath465 , we get    @xmath466}{1 + \\mu - \\gamma + \\widehat{\\tau}(\\theta ) } + \\gamma + \\widehat{\\tau}(\\theta )   = 0.\\ ] ]    therefore , when condition is satisfied , @xmath467 which rewrites as equation  .",
    "we first observe that , according to proposition  [ prop1 ] , with probability at least @xmath89 ,    @xmath468   = \\inf_{(\\lambda , \\beta ) \\in \\lambda } b_{\\lambda , \\beta } \\left [    \\|\\theta\\|^{-2}\\widetilde n_{\\lambda}(\\theta ) \\right]\\end{aligned}\\ ] ]    since , by definition , @xmath469 are the values which minimize @xmath470.$ ] according to equation  , since @xmath471 and @xmath97 is a decreasing function , we get @xmath472 } \\biggr).\\ ] ] with the same notation as in proposition  [ prop1 ] , we introduce the subset @xmath473 of @xmath474 defined as @xmath475 \\xi + \\mu + \\gamma + 4 \\delta \\lambda / \\max \\ { t , \\sigma \\ } & < 1 , \\\\ \\mu + \\gamma + 2 \\delta \\lambda / \\max \\ { t , \\sigma \\ } & \\leq 1/2 , \\\\ \\text { \\rm and }    2 \\gamma + \\delta \\lambda / \\max \\ { t ,   \\sigma \\ }   & \\leq 1/2 ~~ \\bigr\\ } \\end{aligned}\\ ] ] and the function @xmath476    [ lem1.14 ] for any @xmath72 and any @xmath477 ,",
    "@xmath478    we first observe that when @xmath479 then @xmath480 and hence the two inequalities are trivial .",
    "we now assume @xmath481 and we put @xmath482 we prove the second inequality first . since @xmath483 , we have @xmath484    in order to prove the first inequality , we first check that @xmath485 we start observing that , since @xmath486 , \\sigma \\right\\ } \\geq \\max \\ { t , \\sigma \\ } / [ 1 + b_{\\lambda , \\beta}(t ) ] , \\ ] ] then @xmath487   \\frac{\\delta \\lambda   } { \\max \\ { t , \\sigma \\ } } \\\\ & = \\xi + \\mu + \\gamma + 2 \\tau + 2 \\tau b_{\\lambda , \\beta}(t).\\end{aligned}\\ ] ]    moreover , as @xmath488 , we get @xmath489 so that @xmath490 , \\sigma \\ }   \\leq \\xi + \\mu + \\gamma + 4 \\tau < 1,\\ ] ] which proves that , indeed , @xmath491 therefore    @xmath492 } \\\\ & = \\frac{\\left(\\gamma + \\tau \\right ) \\left ( 1 +   \\tau / ( 1 - \\mu - \\gamma - 2 \\tau ) \\right)}{1 - \\mu - \\gamma - 2 \\tau - 2 \\tau b_{\\lambda , \\beta}(t ) } , \\end{aligned}\\ ] ]    where in the last line we have used the definition of @xmath493 observing that @xmath494    we obtain @xmath495    considering that @xmath496    since when @xmath481 , it is true that @xmath497 and @xmath498 we conclude that @xmath499    applying the above lemma to our problem we get that , with probability at least @xmath89 , for any @xmath77 , @xmath500.\\ ] ]    let us recall the definition of the finite set @xmath100 given in  .",
    "let @xmath103 and @xmath501 we define @xmath502 where @xmath503}. \\end{aligned}\\ ] ]    we introduce the explicit bound @xmath504 and @xmath505 \\zeta ( \\max \\ { t , \\sigma \\ } ) \\leq \\sqrt{n } \\\\   + \\infty & \\text { otherwise . } \\end{cases}\\ ] ]    for any @xmath477 , we have @xmath506    we recall that the function @xmath507 is non - increasing so that @xmath508 moreover , since @xmath509 it is sufficient to prove the result for @xmath510 $ ] . + as equation is trivial when @xmath511 we may assume that @xmath512 , so that @xmath513 in particular , by considering only the second term in the definition of @xmath514 we obtain that @xmath515 which implies @xmath516 therefore , since @xmath517,\\ ] ] there exists @xmath518 for which @xmath519 we recall that by equation   @xmath520 and we observe that @xmath521 defined as @xmath522 are the desired values which optimize @xmath523 we also remark that , by equation  , @xmath524    thus , evaluating @xmath525 in @xmath526 we obtain that    @xmath527 \\frac{\\lambda _ * } { \\lambda_{\\widehat{\\jmath } } } \\\\",
    "\\shoveright { + \\sqrt{\\frac{(2+c ) \\kappa^{1/2 } s_4 ^ 2}{2 n \\max \\",
    "{ t , \\sigma \\ } } } \\left (   \\frac{\\beta_*}{\\beta_{\\widehat{\\jmath } } } + \\frac{\\beta_{\\widehat{\\jmath}}}{\\beta_*}\\right ) } \\\\",
    "\\leq \\sqrt{\\frac{2(\\kappa-1)}{n } \\left[\\frac{(2 + 3c ) s_4 ^ 2}{4 ( 2 + c ) k^{1/2 } \\max \\ { t , \\sigma \\ } } + \\log(k / \\epsilon ) \\right ] } \\cosh \\biggl [ \\log\\left ( \\frac{\\lambda_{\\widehat{\\jmath}}}{\\lambda _ * } \\right ) \\biggr ]   \\\\   + \\sqrt{\\frac{2 ( 2+c ) \\kappa^{1/2 } s_4 ^ 2}{n \\max \\ { t , \\sigma\\ } } } \\cosh   \\biggl [ \\log\\left ( \\frac{\\beta_{\\widehat{\\jmath}}}{\\beta_*}\\right ) \\biggr].\\end{gathered}\\ ] ]    by equation   we get @xmath528 } \\cosh \\left ( \\frac{a}{4 } \\right )",
    "\\\\   + \\sqrt{\\frac{2 ( 2+c ) \\kappa^{1/2 } s_4 ^ 2}{n \\max \\ { t , \\sigma \\ } } } \\cosh \\left ( \\frac{a}{2}\\right )   .\\end{gathered}\\ ] ] we also observe that @xmath529 \\leq 4n^{-1/2 } \\zeta(t),\\ ] ] since by definition @xmath530 in the same way , observing that @xmath531 we obtain @xmath532 \\gamma_{\\widehat{\\jmath } } + 4 \\delta_{\\widehat{\\jmath } } \\lambda_{\\widehat{\\jmath } } / \\max \\ { t , \\sigma\\ } \\\\ & \\leq   \\bigl[6 + ( \\kappa-1)^{-1 } \\bigr]n^{-1/2 }   \\zeta ( \\max \\ { t , \\sigma\\})\\end{aligned}\\ ] ]    and similarly , @xmath533 & \\leq 6n^{-1/2 }   \\zeta ( \\max \\ { t , \\sigma\\ } ) ,",
    "\\\\   2 \\bigl [   2\\gamma_{\\widehat{\\jmath } } + \\delta_{\\widehat{\\jmath } } \\lambda_{\\widehat{\\jmath } } / \\max \\ { t , \\sigma \\ } \\bigr ] & \\leq 4 n^{-1/2 } \\zeta ( \\max \\ { t , \\sigma\\}).\\end{aligned}\\ ] ] this implies that , whenever @xmath512 , then @xmath534 we have then proved that @xmath535    applying the above lemma to equation   and observing that , for any @xmath77 , @xmath536",
    "\\leq s_4 ^ 2,\\ ] ] we obtain that , with probability at least @xmath89 , for any @xmath77 , @xmath537.\\ ] ]    since by the cauchy - schwarz inequality @xmath538 we get @xmath539 choosing @xmath540 and computing explicitly the constants we conclude the proof .",
    "we observe that it is sufficient to prove that with probability at least @xmath76 @xmath541 where @xmath542 and @xmath543 indeed , if equation   holds true , according to corollary  [ lem1a ] , @xmath544 which is the analogous , in the infinite - dimensional setting , of proposition  [ prop3 ] .",
    "thus , following the proof of proposition  [ prop4 ] we obtain the desired bounds .",
    "+ let us now prove equation  .",
    "observe that , for any @xmath545 , @xmath546 where @xmath547 is the closest point in @xmath142 to @xmath548 .",
    "since @xmath549 , with probability at least @xmath415 , for any @xmath72 , @xmath550.\\ ] ] let us now remark that for any @xmath551 $ ] , we have @xmath552 , so that @xmath553 . therefore @xmath554",
    "\\bigr\\ } + \\eta \\\\",
    "\\leq   \\lvert \\pi_{v_k } \\theta \\rvert_{\\mathcal h}^2 \\",
    "\\phi_+^{-1 } \\circ \\phi_-^{-1 }    \\bigl\\ { n   \\bigl [ \\lvert \\pi_{v_k } \\theta \\rvert_{\\mathcal h}^{-1 }   \\bigl (   \\lvert \\pi_{v_k } \\theta   \\rvert_{\\mathcal h } \\xi + \\bigl ( \\pi_k - \\pi_{v_k } \\bigr ) \\theta \\bigr ) \\bigr ]    \\bigr\\ } + \\eta \\\\ \\leq   \\phi_+^{-1 } \\circ \\phi_-^{-1 } \\bigl\\ {   n \\bigl [ \\lvert \\pi_{v_k } \\theta \\rvert_{\\mathcal h } \\xi + \\bigl (   \\pi_k - \\pi_{v_k } \\bigr ) \\theta \\bigr ) \\bigr ] \\bigr\\ } + \\eta \\\\ \\leq \\phi_+^{-1 } \\circ \\phi_-^{-1 } \\bigl ( n \\bigl ( \\pi_k \\theta \\bigr )   + \\eta \\bigr ) + \\eta . \\end{gathered}\\ ] ] indeed , @xmath555 and this is a difference of two vectors belonging to the unit ball . in the same way @xmath556 \\bigr\\ } - \\eta \\\\ \\quad \\",
    "\\geq   \\lvert \\pi_{v_k } \\theta \\rvert_{\\mathcal h}^2 \\",
    "\\phi_- \\circ \\phi_+ \\bigl\\ {   n \\bigl [ \\lvert \\pi_k \\theta \\rvert_{\\mathcal h}^{-1 }   \\bigl ( \\lvert \\pi_{v_k } \\theta \\rvert_{\\mathcal h }",
    "\\xi   + \\bigl ( \\pi_k - \\pi_{v_k } \\bigr ) \\theta \\bigr ) \\bigr ] \\bigr\\ } - \\eta \\\\",
    "\\geq \\phi_- \\circ \\phi_+ \\bigl ( n \\bigl ( \\pi_k \\theta \\bigr )   - \\eta \\bigr ) - \\eta\\end{gathered}\\ ] ] which proves equation  .      in all this section we use the same notation as in section  [ sec1 ] .",
    "let @xmath1460,s_4 ^ 2]$ ] be such that @xmath557 where @xmath558 is defined in proposition [ prop2 ] .",
    "[ lem0a ] the function @xmath559 where @xmath120 is defined in proposition [ prop2 ] , is non - decreasing for any @xmath560    if @xmath561 , then @xmath562 , so that @xmath563 is obviously non - decreasing .",
    "otherwise , @xmath564 so that @xmath565 therefore the function @xmath566 is of the form @xmath567 where @xmath568 , @xmath569 @xmath570 , and the constants @xmath571 , and @xmath572 are positive .",
    "let @xmath573 and observe that @xmath574 and that @xmath575 .",
    "therefore @xmath576 for any @xmath577 , and @xmath578 showing that @xmath566 is non - decreasing .",
    "[ lem2a ] for any @xmath579 such that , for any @xmath72 , @xmath580 and any threshold @xmath86 such that @xmath581 and @xmath107 , we have @xmath582    by symmetry of @xmath108 and @xmath583 , equation is a consequence of equation . + * step 1 .",
    "* we will prove that @xmath584 where @xmath585 is defined in equation . + * case 1 . *",
    "assume that @xmath586 and remark that , since @xmath85 is non - decreasing and @xmath94 , @xmath587 according to equation , where @xmath97 is defined in equation . therefore in this case , @xmath588 but when @xmath589 , @xmath590 because @xmath591 is a non - increasing function of @xmath592 , thus equation implies that @xmath593 since @xmath594 , equation holds true .",
    "+ * case 2 .",
    "* assume now that we are not in * case 1 * , implying that @xmath595 in this case @xmath596\\end{gathered}\\ ] ] according to equation  .",
    "moreover , continuing the above chain of inequalities @xmath597",
    "\\\\ = \\phi_+ \\bigl ( \\max \\ { b- \\eta , \\sigma \\ } \\bigr )   \\bigl [ 1 - b_{\\lambda , \\beta}(a + \\eta ) \\bigr ]   \\\\",
    "\\quad \\geq \\max \\ { b - \\eta , \\sigma \\ }    \\frac{1 - b_{\\lambda , \\beta}(a + \\eta ) } { 1 +   b_{\\lambda , \\beta}(\\max\\ { b- \\eta , \\sigma \\ } ) }   \\\\",
    "\\geq \\max \\ { b-\\eta , \\sigma \\ } \\frac { 1 - b_{\\lambda , \\beta}(a + \\eta ) } { 1 + b_{\\lambda , \\beta}(a+\\eta)}. \\end{gathered}\\ ] ] therefore @xmath598 according to lemma [ lem1.14 ] .",
    "this concludes the proof of * step 1*. + * step 2 * taking the infimum in @xmath72 in equation , according to equation  , we obtain that @xmath599 we can then use the fact that @xmath600 is non - decreasing ( proved in lemma [ lem0a ] ) to deduce that @xmath601 since there is nothing to prove when already @xmath602 .",
    "remark that @xmath603 and that in the same way @xmath604 .",
    "this proves that @xmath605 by symmetry , we can then exchange @xmath108 and @xmath583 to prove the same bounds for @xmath606 , and therefore also for the absolute value of this quantity , which ends the proof of the lemma .    as a consequence the following result holds .",
    "[ lem1a ] for any @xmath579 such that , for any @xmath72 , @xmath580 and any threshold @xmath86 such that @xmath581 and @xmath107 , we have @xmath607    this is a consequence of the previous lemma , of the fact that @xmath608 , and of the fact that @xmath609 .",
    "the main goal of section  [ sec1 ] is to estimate the gram matrix @xmath610 , where @xmath32 is a random vector of unknown law @xmath611 , from an i.i.d .",
    "sample @xmath612 drawn according to @xmath19 we have constructed a robust estimator of the gram matrix and in section  [ sec_emp ] we have shown empirically its performance in the case of a gaussian mixture distribution . in this section we show from a theoretical point of view that the classical empirical estimator @xmath613 behaves similarly to our robust estimator in light tail situations , while it may perform worse otherwise .",
    "+ as already done in section  [ sec1 ] , we consider the quadratic form @xmath614 and we denote by @xmath615 the quadratic form associated to the empirical gram matrix @xmath311 .",
    "according to the notation introduced in section  [ sec1 ] , let @xmath103 and let @xmath616 where @xmath617 and @xmath204 .",
    "let us put @xmath618 and let us introduce @xmath619 where @xmath620 is defined in equation   as @xmath621 at the end of the section we mention some assumptions under which it is possible to give a non - random bound for @xmath622 .",
    "the following proposition , compared with the result obtained for the robust estimator @xmath623 , presented in proposition  [ prop2 ] , shows that the different behavior of the two estimators @xmath624 and @xmath625 can appear only for heavy tail data distributions .",
    "[ prop2.8eg ] consider any threshold @xmath86 such that @xmath626 .",
    "with probability at least @xmath89 , for any @xmath166 , @xmath627_+   \\bigl [ 1 - b _ * \\bigl ( n(\\theta ) \\bigr ) \\bigr]_+}.\\ ] ] where @xmath120 is defined in proposition  [ prop2 ] .    before proving the above proposition",
    "we observe that , also in this case , the bound does not depend explicitly on the dimension @xmath121 of the ambient space and thus the result can be extended to any infinite - dimensional hilbert space .",
    "let @xmath67 be the finite set defined in equation  .",
    "we use as a tool the family of estimators @xmath628 introduced in equation  , where @xmath629 is defined in equation  .",
    "let us put @xmath630 we divide the proof into 4 steps .",
    "+ * step 1 . *",
    "the first step consists in linking the empirical estimator @xmath625 with @xmath631 + we claim that , with probability at least @xmath409 for any @xmath166 , any @xmath632 such that @xmath633 , @xmath634_+^{-1},\\ ] ] with the convention that @xmath635 moreover , with probability at least @xmath409 for any @xmath166 , any @xmath72 , such that @xmath636 , @xmath637 we first observe that , according to the definition of @xmath629 , for any threshold @xmath86 , @xmath638   \\leq   r \\bigl ( \\lambda^{1/2 }   \\widetilde{n}_{\\lambda}(\\theta)^{-1/2 } \\theta \\bigr )   = r_{\\lambda } \\bigl ( \\widehat{\\alpha}(\\theta ) \\ , \\theta \\bigr )   \\leq 0,\\ ] ] where we have used the fact that the function @xmath54 , introduce in equation  , is non - decreasing .",
    "moreover @xmath639 as soon as @xmath640 and this holds true , according to proposition  [ prop0 ] , with probability at least @xmath415 , for any @xmath166 and any @xmath72 such that @xmath633 . indeed , by proposition  [ prop0 ] , with probability at least @xmath415 , for any @xmath166 , any @xmath632 @xmath641 defining @xmath642 we get @xmath643.\\end{gathered}\\ ] ] in the same way , with probability at least @xmath415 , for any @xmath166 , any @xmath72 such that @xmath633 , we obtain @xmath644.\\ ] ] we remark that the derivative of @xmath645 is @xmath646\\\\ \\displaystyle \\frac{\\frac{z^2}{2}}{1+z+\\frac{z^2}{2 } } & \\text{if } \\ z \\in [ -1,0]\\\\   \\displaystyle\\frac{\\frac{z^2}{2}}{1-z+\\frac{z^2}{2 } }   & \\text{if } \\ z \\in [ 0,1 ] , \\end{cases}\\ ] ] showing that @xmath647 , and therefore that @xmath645 is a non - decreasing function satisfying @xmath648 applying equation   to equation   we obtain @xmath649 where we have used the fact that @xmath650 since , by the cauchy - schwarz inequality , @xmath651 we get @xmath652 which proves the first inequality .",
    "similarly , since @xmath645 in non - decreasing , we obtain that , with probability at least @xmath415 , for any @xmath166 , any @xmath72 such that @xmath653 , @xmath654 where the last inequality follows from equation . +",
    "* this is an intermediate step .",
    "we claim that , with probability at least @xmath89 , for any @xmath166 , any @xmath72 , any @xmath655 , @xmath656_+^{-1 } \\\\   \\max \\bigl\\ { \\bar{n}(\\theta ) , \\sigma \\bigr\\ } & \\leq \\phi_-^{-1 } \\bigl ( \\max \\ { n(\\theta ) , \\sigma \\ } \\bigr)\\biggl [ 1 - \\tau_{\\lambda } \\bigl (   \\bar{n}(\\theta ) \\bigl [ 1 - \\tau_{\\lambda}(\\sigma ) \\bigr]_+ \\bigr ) \\biggr]_+^{-1 } \\\\ \\bar{n}(\\theta ) & \\geq \\biggl ( 1 - \\frac{\\lambda^2}{3 } \\biggr)_+ \\phi_+\\bigl ( n(\\theta ) \\bigr ) \\end{aligned}\\ ] ] where @xmath85 and @xmath84 are defined in proposition  [ prop0 ] .",
    "+ we consider the threshold @xmath657 where we have used the fact that , by definition , @xmath658 for any @xmath659 we assume that we are in the intersection of the two events of proposition [ prop0 ] , which holds true with probability at least @xmath660 so that @xmath661 according to * step 1 * , choosing as a threshold @xmath662 we get @xmath663_+^{-1},\\end{aligned}\\ ] ] ( where @xmath664 is still defined with respect to @xmath124 ) , so that , according to equation  , @xmath665_+^{-1}.\\ ] ] as a consequence , recalling the definition of @xmath666 we have @xmath667_+^{-1}.\\ ] ] thus , observing that @xmath668_+^{-1},\\ ] ] we obtain the first inequality . to prove the second inequality , we use equation once to see that @xmath669_+\\geq \\bar{n}(\\theta ) \\bigl [ 1 - \\tau_{\\lambda } ( \\sigma ) \\bigr]_+,\\ ] ] and we use it again to get @xmath670_+^{-1}\\\\ & \\leq \\phi_-^{-1 } \\bigl ( \\max \\ { n(\\theta ) , \\sigma \\ } \\bigr ) \\bigl [     1 - \\tau_{\\lambda } \\bigl ( \\bar{n}(\\theta ) [ 1 - \\tau_{\\lambda}(\\sigma ) ] _ + \\bigr ) \\bigr]_+^{-1}.\\end{aligned}\\ ] ] to complete the proof of the second inequality , it is enough to remark that @xmath671_+ \\bigr ) \\bigr]_+^{-1}.\\ ] ] to prove the last inequality , it is sufficient to remark that @xmath672 by proposition [ prop0 ] and hence , when @xmath673 @xmath674 on the other hand , when @xmath675 this inequality is also obviously satisfied . + * step 3 . * we now prove that , with probability at least @xmath89 , for any @xmath166 , any @xmath72 , any @xmath655 , @xmath676_+ \\bigl [ 1 - b_{\\lambda , \\beta } \\bigl ( n(\\theta ) \\bigr ) \\bigr]_+ } , \\\\   1 - \\frac{\\max \\ { \\bar{n}(\\theta ) ,   \\sigma \\ } } { \\max \\ { n(\\theta ) , \\sigma \\ } } & \\leq b_{\\lambda , \\beta } \\bigl(n(\\theta ) \\bigr ) + \\frac{\\lambda^2}{3},\\end{aligned}\\ ] ] where @xmath97 is defined in equation   and @xmath585 in equation  .",
    "+ we observe that , according to * step 2 * , @xmath677_+ ^{-1 } \\\\ & \\leq \\frac{\\max \\ { n(\\theta ) , \\sigma   \\ } } { \\bigl [ 1 - \\tau_{\\lambda } \\bigl ( n(\\theta ) \\bigr ) \\bigr]_+ \\bigl [ 1 -b_{\\lambda , \\beta } \\bigl ( n(\\theta )",
    "\\bigr ) \\bigr]_+ } , \\end{aligned}\\ ] ] which implies @xmath678_+ } + \\frac { \\tau_{\\lambda } \\bigl ( n(\\theta ) \\bigr)}{\\bigl [ 1 - \\tau_{\\lambda } \\bigl ( n(\\theta ) \\bigr ) \\bigr]_+ \\bigl [ 1 -b_{\\lambda , \\beta } \\bigl ( n(\\theta )",
    "\\bigr ) \\bigr]_+ } .\\ ] ] applying lemma  [ lem1.14 ] we obtain the first inequality .",
    "+ to prove the second inequality we observe that , using again * step 2 * , @xmath679^{-1},\\end{aligned}\\ ] ] where we have used the fact that @xmath680 as shown in equation  .",
    "thus we conclude that @xmath681 + * step 4 . * from * step 3 * we deduce that @xmath682_+ \\bigl [ 1 -b_{\\lambda , \\beta } \\bigl ( n(\\theta )",
    "\\bigr ) \\bigr]_+ } .\\ ] ] to conclude the proof it is sufficient to apply * step 3 * to @xmath683 defined in equation  . indeed , by equation  , for any @xmath684 @xmath685 and , by equation",
    ", we have @xmath686      we conclude this section by mentioning assumptions under which it is possible to give a non - random bound for @xmath687 defined in equation .",
    "let us assume that , for some exponent @xmath688 and some positive constants @xmath689 and @xmath690 , @xmath691 \\leq 1.\\ ] ] in this case , with probability at least @xmath415 , @xmath692 where we recall that @xmath693 . $ ] + to give a point of comparison , in the centered gaussian case where @xmath694 is a gaussian vector , we have , for any @xmath695 @xmath696 \\biggr ] = 1,\\ ] ] where @xmath697 are the eigenvalues of @xmath698 therefore , with probability at least @xmath415 , @xmath699 moreover we observe that @xmath700    in order to replace equation   with some polynomial assumptions we need to replace @xmath622 by @xmath701 indeed , by the bienaym chebyshev inequality , we get that , with probability at least @xmath415 , @xmath702 + \\biggl ( \\frac { \\mathbb e \\bigl [ \\lvert x \\rvert^{12 } \\bigr]}{n \\epsilon } \\biggr)^{1/2 } \\biggr)^{1/6 } \\leq \\bigl ( 1 + ( n \\epsilon)^{-1/2 } \\bigr)^{1/6 } \\ ,   \\mathbb e   \\bigl [ \\lvert x \\rvert^{12 } \\bigr]^{1/12}\\ ] ] and hence , with probability at least @xmath703 , @xmath704^{1/12}.\\ ] ]    also in the case where we consider this new quantity @xmath705 we obtain a bound of the form of proposition  [ prop2.8eg ] .",
    "more precisely , the following proposition holds true .",
    "let @xmath706 and let us put @xmath707 with probability at least @xmath89 , for any @xmath166 , @xmath708_+}.\\ ] ]    we observe that another way to take advantage of equation is to write    @xmath709 thus , putting @xmath710 we get that , for any @xmath166 , @xmath711 the same reasoning used to prove * step 2 * of proposition  [ prop2.8eg ] shows that , with probability at least @xmath415 , for any @xmath166 , any @xmath72 , any @xmath655 , @xmath712   .\\ ] ] as a consequence , with probability at least @xmath89 , for any @xmath166 , any @xmath72 , @xmath713_+}.\\ ] ]",
    "in this section we come back to the finite - dimensional framework and we consider the problem of estimating the expectation of a symmetric random matrix .",
    "we will use these results to estimate the covariance matrix in the case of unknown expectation .",
    "let @xmath714 be a symmetric random matrix of size @xmath715 as already observed for the gram matrix , the expectation of @xmath716 can be recovered via the polarization identity from the quadratic form @xmath717 , \\qquad \\theta \\in \\mathbb r^d,\\ ] ] where the expectation is taken with respect to the unknown probability distribution of @xmath716 on the space of symmetric matrices of size @xmath715 observe that , if we decompose @xmath716 in its positive and negative parts @xmath718 the quadratic form @xmath719 rewrites as @xmath720 - \\mathbb e \\left [ \\theta^{\\top}a_-\\theta\\right ] = n_{a_+}(\\theta ) - n_{a_-}(\\theta ) . \\end{aligned}\\ ] ] thus in the following we will consider the case of a symmetric positive semi - definite random matrix of size @xmath715 + from now on let @xmath714 be a symmetric positive semi - definite random matrix of size @xmath121 and let @xmath29 be a probability distribution on the space of symmetric positive semi - definite random matrices of size @xmath715 our goal is to estimate @xmath721 , \\quad \\theta \\in \\mathbb r^d,\\ ] ] from an i.i.d .",
    "sample @xmath722 of symmetric positive semi - definite matrices drawn according to @xmath19 we observe that the quadratic form @xmath24 rewrites as @xmath723\\ ] ] where @xmath724 denotes the square root of @xmath725 + the construction of the ( robust ) estimator @xmath149 follows the one already done in the case of the gram matrix with the necessary adjustments . for any @xmath45 and for any @xmath423",
    "we consider the empirical criterion @xmath726 where the influence function @xmath54 is defined as in equation  , and we perturb the parameter @xmath35 with the gaussian perturbation @xmath727 of mean @xmath35 and covariance matrix @xmath728 where @xmath66 is a free real parameter .",
    "we consider the family of estimators @xmath729 where @xmath730 .",
    "let us put @xmath731^{1/4 } \\quad \\text{and } \\quad   \\kappa = \\sup_{\\substack{\\theta\\in \\mathbb r^d\\\\ \\mathbb e [ \\lvert a^{1/2 } \\theta \\rvert^2 ] > 0 } } \\frac{\\displaystyle\\mathbb e   \\bigl [ \\lvert a^{1/2 } \\theta \\rvert^4 \\bigr ] } { \\displaystyle\\mathbb e   \\bigl [ \\lvert a^{1/2 } \\theta \\rvert^2 \\bigr]^{2}}.\\ ] ] the finite set @xmath67 of possible values of the couple of parameters @xmath68 is defined as @xmath502 where @xmath732 with @xmath114 , and @xmath733}.\\end{aligned}\\ ] ] we recall that @xmath734 is defined in equation   as @xmath735 .",
    "+ according to equation  , let @xmath736 and put @xmath737 $ ] .",
    "define the estimator @xmath624 as @xmath738    [ props1 ] let @xmath7390,s_4 ^ 2]$ ] be some energy level . with probability at least @xmath89 , for any @xmath77 , @xmath537,\\ ] ] where @xmath120 is defined as @xmath112 \\zeta _ * ( \\max \\ { t , \\sigma \\ } ) \\leq \\sqrt{n } \\\\",
    "+ \\infty & \\text { otherwise } \\end{cases}\\ ] ] and @xmath740 } { \\kappa^{1/2 } \\mathbb e   \\bigl [ \\lvert a \\rvert_{\\infty}^2\\bigr]^{1/2 } t }   + \\log(k ) + \\log(\\epsilon^{-1 } )   \\biggr ) } + \\sqrt { \\frac{98.5 \\kappa^{1/2 } \\mathbb e \\bigl [ \\lvert a \\rvert_{\\infty}^2 \\bigr]^{1/2 } } { t}}.\\ ] ]      to prove proposition  [ props1 ] we use many results already proved in the case of the gram matrix ( with the necessary adjustments ) . +",
    "we first observe that , denoting by @xmath743 a gaussian random vector with mean @xmath744 and covariance matrix @xmath745 we have @xmath746   & = \\sum_{i=1}^d \\mathbb e \\bigl ( \\langle w , e_i\\rangle^2 \\bigr)\\\\ & =   \\| a^{1/2 } \\theta\\|^2+\\frac{\\mathbf{tr}(a)}{\\beta}\\end{aligned}\\ ] ] where @xmath39 is the canonical basis of @xmath21 ( and @xmath747 is a one - dimensional gaussian random variable with mean @xmath748 and variance @xmath749 ) .",
    "therefore the empirical criterion rewrites as @xmath750.\\ ] ] we now use lemma  [ l1 ] to pull the expectation outside the influence function @xmath54 .",
    "we decompose @xmath716 into @xmath751 , where @xmath752 and @xmath753 and we observe that @xmath754 has the same distribution as @xmath755 , where @xmath756 so that @xmath757   = \\mathbf{var } \\biggl ( \\sum_{i=1}^d \\bigl ( ( u^{\\top } \\theta)_i   + w_i \\bigr)^2 \\lambda_i \\biggr )",
    "\\\\ = \\sum_{i=1}^d \\lambda_i^2   \\mathbf{var } \\bigl [ \\bigl ( ( u^{\\top } \\theta)_i + w_i \\bigr)^2 \\bigr ]   = \\sum_{i=1}^d \\biggl (   \\frac{2}{\\beta^2 } + \\frac{4}{\\beta } ( u^{\\top } \\theta))_i^2 \\biggr )   \\lambda_i^2 \\\\ = \\frac{2}{\\beta^2 } \\mathbf{tr}(a^2 ) + \\frac{4}{\\beta } \\lvert a \\theta   \\rvert^2.\\end{gathered}\\ ] ]    as a consequence we get @xmath758\\end{gathered}\\ ] ] where the function @xmath359 is defined in equation  .",
    "we then apply equation   with @xmath759 @xmath760 @xmath761 and @xmath762 to obtain @xmath763\\end{gathered}\\ ] ] and we conclude , by lemma  [ lem1.5 ] , that @xmath764 \\ , \\mathrm{d } \\pi_{\\theta}(\\theta'),\\end{gathered}\\ ] ] where @xmath765",
    "we then apply the pac - bayesian inequality ( proposition [ pac ] ) to @xmath766\\ ] ] where @xmath767 and we choose as posterior distributions the family of gaussian perturbations @xmath768 we obtain that , with probability at least @xmath769 , for any @xmath44 , @xmath770 \\mathrm d \\pi_{\\theta } ( \\theta ' ) \\\\",
    "\\shoveright{+   \\frac{\\beta \\|\\theta\\|^2}{2n }   + \\frac { \\log(\\epsilon^{-1})}{n } }   \\\\",
    "=   \\mathbb{e } \\biggl [ \\lvert a^{1/2 } \\theta \\rvert^2 - \\lambda +   \\frac{1}{2 } \\bigl ( \\lvert a^{1/2 } \\theta \\rvert^2 - \\lambda \\bigr)^2   + \\frac{(c+2)\\lvert a \\theta \\rvert^2}{\\beta } +   \\frac{(2 + 3c ) \\mathbf{tr}(a^2)}{2 \\beta^2 } \\biggr ] \\\\",
    "+   \\frac{\\beta \\lvert \\theta \\rvert^2}{2 n } + \\frac{\\log(\\epsilon^{-1})}{n}.   \\end{gathered}\\ ] ] using the cauchy - schwarz inequality remark that @xmath771 \\leq \\mathbb e   \\bigl [ \\lvert a \\rvert_{\\infty } \\lvert a^{1/2 } \\theta \\rvert^2 \\big ]   \\leq \\mathbb e   \\bigl [ \\lvert a \\rvert_{\\infty}^2 \\bigr]^{1/2 }   \\mathbb e",
    "\\bigl [ \\lvert a^{1/2 } \\theta \\rvert^4 \\bigr]^{1/2 } \\leq \\mathbb e   \\bigl [ \\lvert a \\rvert_{\\infty}^2 \\bigr]^{1/2 }   \\kappa^{1/2 } n(\\theta)\\ ] ] where @xmath71 is defined in equation  .",
    "thus @xmath429 ^ 2 + \\biggl [ 1 + ( \\kappa -1 ) \\lambda + \\frac{(2+c ) \\kappa^{1/2 }   \\mathbb e   \\bigl [ \\lvert a \\rvert_{\\infty}^2 \\bigr]^{1/2 }   } { \\beta }   \\ , \\biggr ] \\bigl [ n(\\theta ) -",
    "\\lambda \\bigr ] \\\\",
    "+ \\frac{(\\kappa-1 ) \\lambda^2}{2 } +   \\frac{(2+c ) \\kappa^{1/2 }   \\mathbb e   \\bigl [ \\lvert a \\rvert_{\\infty}^2 \\bigr]^{1/2 } \\lambda}{\\beta } +   \\frac{(2 + 3c ) \\mathbb e   \\bigl [ \\mathbf{tr}(a^2 ) \\bigr]}{2 \\beta^2 } + \\frac{\\beta \\lvert \\theta \\rvert^2}{2n }   + \\frac{\\log(\\epsilon^{-1})}{n}. \\end{gathered}\\ ] ] this is the analogous of equation  .",
    "thus , proceeding as already done in the case of the gram matrix we conclude the proof .",
    "remark that to obtain the above result we have used the fact that @xmath771 \\leq \\mathbb e   \\bigl [ \\lvert a \\rvert_{\\infty}^2 \\bigr]^{1/2 } \\kappa^{1/2 } n(\\theta).\\ ] ] however , if we use any upper bound of the form @xmath771 \\leq f(\\mathbb e[a ] ) n(\\theta)\\ ] ] propositon [ props1 ] holds replacing @xmath772^{1/2 } \\kappa^{1/2}$ ] with @xmath773)$ ] in the definition of @xmath558 .",
    "similarly we can replace @xmath774 $ ] by an upper bound . + we observe in particular that @xmath775^{1/2}}{\\kappa^{1/2 } }   \\leq \\frac { \\mathbb e   \\bigl [ \\mathbf{tr}(a^2 ) \\bigr]}{\\kappa^{1/2 } \\mathbb e   \\bigl [ \\lvert a \\rvert_{\\infty}^2 \\bigr]^{1/2 } } \\leq   \\mathbb e   \\bigl [ \\mathbf{tr}(a ) \\bigr ] = \\mathbf{tr } \\bigl [ \\mathbb{e }   ( a ) \\bigr].\\ ] ] indeed , to see the first inequality it is sufficient to observe that @xmath776",
    ". moreover we have that @xmath777 \\leq \\mathbb e   \\bigl [ \\lvert a \\rvert_{\\infty }   \\mathbf{tr}(a ) \\bigr ] \\leq \\mathbb e   \\bigl [ \\lvert a \\rvert_{\\infty}^2 \\bigr]^{1/2 }   \\mathbb e   \\bigl [ \\mathbf{tr}(a)^2 \\bigr]^{1/2},\\ ] ] and , denoting by @xmath778 an orthonormal basis of @xmath779 @xmath780 =   \\sum _ { \\substack { 1 \\leq i \\leq d , \\\\   1 \\leq j \\leq d } } \\mathbb e   \\bigl [ \\lvert a^{1/2 } e_i \\rvert^2 \\lvert a^{1/2 } e_j \\rvert^2 \\bigr ]   \\\\ \\leq   \\sum _ { \\substack { 1 \\leq i \\leq d , \\\\   1 \\leq j \\leq d } } \\mathbb e   \\bigl [ \\lvert a^{1/2 } e_i   \\rvert^4 \\bigr]^{1/2 }   \\mathbb e   \\bigl [ \\lvert a^{1/2 } e_j \\rvert^4 \\bigr]^{1/2 }    \\\\",
    "\\leq \\kappa \\sum _ { \\substack { 1 \\leq i \\leq d , \\\\   1 \\leq j \\leq d } } \\mathbb e   \\bigl [ \\lvert a^{1/2 } e_i \\rvert^2 \\bigr ]   \\mathbb e   \\bigl [ \\lvert a^{1/2 } e_j \\rvert^2 \\bigr ]   = \\kappa \\mathbb e   \\bigl [ \\mathbf{tr}(a ) \\bigr]^2 . \\end{gathered}\\ ] ] this implies that we can bound @xmath558 in proposition  [ props1 ] by @xmath781 } { t }   + \\log(k ) + \\log(\\epsilon^{-1 } ) \\biggr ) } + \\sqrt { \\frac{98.5\\ \\kappa \\ \\mathbb e [ \\mathbf{tr}(a ) ] } { t}}.\\ ] ]    we conclude this section observing that , since the entropy terms are dominated by @xmath782 $ ] , the result can be generalized to the case where @xmath716 is a random symmetric positive semi - definite operator in a infinite - dimensional hilbert space with the only additional assumption that @xmath782 < + \\infty.$ ]      let @xmath32 be a random vector distributed according to the unknown probability measure @xmath783 the covariance matrix of @xmath20 is defined by @xmath784 ) ( x- \\mathbb e[x])^{\\top } \\right]\\ ] ] and our goal is to estimate , uniformly in @xmath785 the quadratic form @xmath786   \\rangle^2 \\right ] , \\quad \\theta \\in \\mathbb r^d,\\ ] ] from an i.i.d .",
    "sample @xmath787 drawn according to @xmath19 we can not use the results we have proved for the gram matrix , since the quadratic form @xmath137 depends on the unknown quantity @xmath26 $ ] .",
    "however we can find a workaround , using the results of the previous section about symmetric random matrices .",
    "indeed , we do not need to estimate @xmath26 $ ] in order to estimate @xmath137 but it is sufficient to observe that the quadratic form @xmath137 can be written as @xmath788\\ ] ] where @xmath789 is an independent copy of @xmath790 more generally , given @xmath791 we may consider @xmath792 independent copies @xmath793 of @xmath20 and the random matrix @xmath794 so that @xmath795 =   \\mathbb e   \\bigl [ \\ , \\theta^{\\top } \\!\\ ! a \\",
    ", \\theta \\ , \\bigr].\\ ] ] we will discuss later how to choose @xmath792 .",
    "in the following we use a robust block estimate which consists in dividing the sample @xmath796 in blocks of size @xmath792 and then in considering the original sample as a `` new '' sample of @xmath797 symmetric matrices @xmath798 ( of independent copies of @xmath716 ) defined as @xmath799 that thus correspond to the empirical covariance estimates on each block .",
    "we can use the results of the previous section to define a robust estimator of @xmath24 .",
    "+ let us introduce @xmath800 } {   \\mathbb e   \\bigl [ \\lvert a^{1/2 } \\theta \\rvert^2 \\bigr]^2 } , \\\\   \\text{and } \\kappa & = \\sup_{\\substack { \\theta \\in \\mathbb{r}^d\\\\\\mathbb e   \\bigl [ \\langle \\theta , x - \\mathbb e ( x ) \\rangle^2 \\bigr]>0 } } \\frac{\\mathbb e   \\bigl [   \\langle \\theta , x - \\mathbb e ( x ) \\rangle^4 \\bigr]}{\\mathbb e   \\bigl [ \\langle \\theta , x - \\mathbb e ( x ) \\rangle^2 \\bigr]^2}. \\end{aligned}\\ ] ]      replacing @xmath20 with @xmath803 $ ] we may assume during the proof that @xmath804 = 0 $ ] .",
    "it holds that @xmath805 = \\mathbb e   \\biggl [ \\biggl ( \\frac{1}{q(q-1 ) } \\sum_{1 \\leq j",
    "< k \\leq q } \\langle \\theta ,   x^{(j ) } - x^{(k ) } \\rangle^2 \\biggr)^2 \\biggr ]    \\\\ = \\frac{1}{q^2(q-1)^2 }   \\sum_{\\substack { 1 \\leq j < k \\leq q\\\\ 1 \\leq s < t \\leq q } }    \\mathbb e   \\bigl [   \\langle \\theta , x^{(j ) } - x^{(k ) } \\rangle^2   \\langle \\theta , x^{(s ) } - x^{(t ) } \\rangle^2   \\bigr]\\end{gathered}\\ ] ] recalling the definition of covariance , we have @xmath805 = \\frac{1}{q^2(q-1)^2 }    \\biggl\\ { \\sum_{\\substack { 1 \\leq j < k \\leq q\\\\ 1 \\leq s < t \\leq q } }      \\mathbb e   \\bigl [   \\langle \\theta , x^{(j ) } - x^{(k ) } \\rangle^2 \\bigr ] \\mathbb e   \\bigl [ \\langle \\theta , x^{(s ) } - x^{(t ) } \\rangle^2   \\bigr ] \\\\   +   \\sum_{1 \\leq j",
    "< k \\leq q }   \\mathbb e   \\bigl [ \\langle \\theta , x^{(j ) } - x^{(k ) }   \\rangle^4 \\bigr ] - \\mathbb e   \\bigl [ \\langle \\theta , x^{(j ) } - x^{(k ) } \\rangle^2   \\bigr]^2   \\\\ +   \\sum_{\\substack { 1 \\leq j < k \\leq q\\\\ 1 \\leq s < t",
    "\\leq q\\\\ \\lvert \\ { j , k \\ }",
    "\\cap \\{s , t \\ } \\rvert = 1 } }   \\biggl ( \\mathbb e   \\bigl [   \\langle \\theta , x^{(j ) } - x^{(k ) } \\rangle^2   \\langle \\theta , x^{(s ) } - x^{(t ) } \\rangle^2   \\bigr ]   \\\\ -   \\mathbb e   \\bigl [   \\langle \\theta , x^{(j ) } - x^{(k ) } \\rangle^2 \\bigr ]   \\mathbb e   \\bigl [ \\langle \\theta , x^{(s ) } - x^{(t ) } \\rangle^2   \\bigr ] \\biggr ) \\biggr\\ }   \\\\",
    "=   \\frac{1}{4 } \\mathbb e   \\bigl [ \\langle \\theta , x^{(2 ) } - x^{(1 ) } \\rangle^2   \\bigr]^2 + \\frac{1}{2q(q-1 ) }   \\mathbb e   \\bigl [ \\langle \\theta , x^{(2 ) } - x^{(1 ) }   \\rangle^4 \\bigr ] - \\mathbb e   \\bigl [ \\langle \\theta , x^{(2 ) } - x^{(1 ) } \\rangle^2   \\bigr]^2   \\\\",
    "+ \\frac{q-2}{q(q-1 ) } \\biggl ( \\mathbb e   \\bigl [   \\langle \\theta , x^{(1 ) } - x^{(2 ) } \\rangle^2   \\langle \\theta , x^{(1 ) } - x^{(3 ) } \\rangle^2   \\bigr ] -   \\mathbb e   \\bigl [ \\langle \\theta , x^{(1 ) } - x^{(2 ) } \\rangle^2 \\bigr]^2 \\biggr).\\end{gathered}\\ ] ] define @xmath806 and observe that @xmath807 ^ 2 & = 4 n(\\theta)^2 , \\\\   \\mathbb e   \\bigl [ ( w_1 - w_2)^4 \\bigr ] &    = \\mathbb e   \\bigl [ w_1 ^ 4 \\bigr ]   + 6 \\mathbb e   \\bigl [ w_1 ^ 2 \\bigr ] \\mathbb e   \\bigl [   w_2 ^ 2 \\bigr ] + \\mathbb e   \\bigl [ w_2 ^ 4 \\bigr ] \\\\   & = 2 \\mathbb e   \\bigl [ w_1 ^ 4 \\bigr ]   + 6 \\mathbb e   \\bigl [ w_1 ^ 2 \\bigr]^2   \\leq ( 2 \\kappa + 6 )",
    "n(\\theta)^2 , \\\\",
    "\\mathbb e   \\bigl[(w_1 - w_2)^2 ( w_1 - w_3)^2 \\bigr ] & =   \\mathbb e   \\bigl [ w_1 ^ 4 \\bigr ] + 3 \\mathbb e   \\bigl [ w_2 ^ 2]^2   \\leq ( \\kappa + 3 ) n(\\theta)^2.\\end{aligned}\\ ] ] therefore @xmath808 \\leq   \\biggl ( 1 + \\frac{(q-2)(\\kappa-1)}{q(q-1 ) } + \\frac{(\\kappa + 1)}{q(q-1 ) }    \\biggr ) n(\\theta)^2 = \\biggl ( 1 + \\frac{\\tau_q(\\kappa)}{q } \\biggr )   n(\\theta)^2,\\ ] ] and hence @xmath809 , since @xmath810 = n(\\theta)$ ] .",
    "the result follows from proposition  [ props1 ] , using the definition of @xmath558 given in equation  , where we replace @xmath71 by @xmath816 and @xmath188 by @xmath797 . according to lemma  [ lem137 ]",
    "we conclude the proof .    here",
    "we have used the upper bound for the entropy factor defined in terms of @xmath817=   \\mathbf{tr } ( \\sigma)$ ] , as mentioned in the remarks following proposition [ props1 ] .",
    "we can improve somehow the constants by evaluating more carefully @xmath818 $ ] and @xmath819 $ ] .",
    "[ lem2 ] it holds that @xmath820 & \\leq \\biggl ( 1 - \\frac{q-2}{q(q-1 ) } \\biggr ) \\lvert \\sigma \\rvert_{\\infty } n(\\theta ) +   \\frac{1}{q } \\biggl ( \\kappa + \\frac{1}{q-1 } \\biggr ) \\mathbf{tr}(\\sigma ) n(\\theta ) \\\\",
    "\\mathbb e   \\bigl [ \\mathbf{tr } \\bigl ( a^2 \\bigr ) \\bigr ] & \\leq \\biggl ( 1 - \\frac{q-2}{q(q-1 ) } \\biggr ) \\mathbf{tr } \\bigl ( \\sigma^2 \\bigr )   +   \\frac{1}{q } \\biggl ( \\kappa + \\frac{1}{q-1 } \\biggr ) \\mathbf{tr}(\\sigma)^2.\\end{aligned}\\ ] ]    replacing @xmath20 with @xmath803 $ ] we may assume that @xmath804 = 0 $ ] . recall that @xmath821 \\leq \\kappa \\mathbb e [ \\| x \\|^2 ] ^2 = \\kappa \\mathbf{tr}(\\sigma)^2\\ ] ] and @xmath822 = \\mathbf{tr}(\\sigma^2).$ ] we observe that @xmath771 =   \\mathbb e   \\biggl [   \\frac{1}{q^2(q-1)^2 } \\sum_{\\substack{1 \\leq j < k \\leq q \\\\ 1 \\leq s < t \\leq q } }   \\langle \\theta , x^{(j ) } - x^{(k ) } \\rangle \\langle x^{(j ) } - x^{(k ) } ,   x^{(s ) } - x^{(t ) } \\rangle \\langle x^{(s ) } - x^{(t ) } , \\theta \\rangle     \\biggr]\\ ] ] and @xmath823   \\\\ & = 4 \\mathbb e   \\bigl [ \\langle \\theta , x^{(1 ) } \\rangle \\langle x^{(1 ) } , x^{(2 ) }   \\rangle \\langle x^{(2 ) } , \\theta \\rangle \\bigr ] = 4 \\lvert \\sigma \\theta \\rvert^2 \\leq 4 \\lvert \\sigma \\rvert_{\\infty } n(\\theta ) , \\\\",
    "\\mathbb e   \\bigl [ \\langle \\theta , x^{(1 ) } - x^{(2 ) } \\rangle & \\langle x^{(1 ) } - x^{(2 ) } ,   x^{(1 ) } - x^{(3 ) } \\rangle \\langle x^{(1 ) } - x^{(3 ) } , \\theta \\rangle   \\bigr ]   \\\\ & = \\mathbb e   \\bigl [ \\langle \\theta , x^{(1 ) } \\rangle^2 \\lvert x^{(1 ) } \\rvert^2   \\bigr ] + 3   \\mathbb e   \\bigl [ \\langle \\theta , x^{(1 ) } \\rangle \\langle x^{(1 ) } , x^{(2 ) }   \\rangle \\langle x^{(2 ) } , \\theta \\rangle \\bigr ] \\\\   & \\leq \\kappa \\mathbf{tr}(\\sigma ) n(\\theta ) + 3 \\lvert \\sigma \\rvert_{\\infty } n(\\theta ) ,",
    "\\\\   \\mathbb e   \\bigl [ \\langle \\theta , x^{(1 ) } - x^{(2 ) } \\rangle & \\langle x^{(1 ) } - x^{(2 ) } ,    x^{(1 ) } - x^{(2 ) } \\rangle \\langle x^{(1 ) } - x^{(2 ) } , \\theta \\rangle   \\bigr ]   \\\\ & = 2 \\mathbb e   \\bigl [ \\langle \\theta , x^{(1 ) } \\rangle^2 \\lvert x^{(1 ) } \\rvert^2   \\bigr ] + 2 \\mathbb e   \\bigl [ \\langle \\theta , x^{(1 ) } \\rangle^2 \\bigr ] \\mathbb e   \\bigl [ \\lvert x^{(1 ) } \\rvert^2 \\bigr ] \\\\ & \\quad + 4   \\mathbb e   \\bigl [ \\langle \\theta , x^{(1 ) } \\rangle \\langle x^{(1 ) } , x^{(2 ) }   \\rangle \\langle x^{(2 ) } , \\theta \\rangle \\bigr ]   \\\\ & \\leq 2(\\kappa+1 ) \\mathbf{tr}(\\sigma ) n(\\theta ) + 4 \\lvert \\sigma \\rvert_{\\infty } n(\\theta),\\end{aligned}\\ ] ] which proves the first inequality . in the same way ,",
    "@xmath824 = \\mathbb e   \\biggl [   \\frac{1}{q^2(q-1)^2 } \\sum_{\\substack{1 \\leq j < k \\leq q \\\\ 1 \\leq s < t",
    "\\leq q } }   \\langle x^{(j ) } - x^{(k ) } , x^{(s ) } - x^{(t ) } \\rangle^2   \\biggr],\\ ] ] and @xmath825   & = 4 \\mathbb e   \\bigl [ \\langle x^{(1 ) } , x^{(2 ) } \\rangle^2 \\bigr ]   \\\\ & = 4 \\mathbf{tr } \\bigl ( \\sigma^2 \\bigr ) , \\\\   \\mathbb e   \\bigl [ \\langle x^{(1 ) } - x^{(2 ) } , x^{(1 ) } - x^{(3 ) } \\rangle^2 \\bigr ]   & = \\mathbb e   \\bigl [ \\lvert x^{(1 ) } \\rvert^4 \\bigr ] + 3   \\mathbb e   \\bigl [ \\langle x^{(1 ) } , x^{(2 ) } \\rangle^2 \\bigr ]   \\\\ & \\leq   \\kappa \\mathbf{tr}(\\sigma)^2 + 3 \\mathbf{tr } \\bigl ( \\sigma^2 \\bigr ) , \\\\ \\mathbb e   \\bigl [ \\langle x^{(1 ) } - x^{(2 ) } , x^{(1 ) } - x^{(2 ) } \\rangle^2 \\bigr ]   & = 2 \\mathbb e   \\bigl [ \\lvert x^{(1 ) } \\rvert^4 \\bigr ]   + 2 \\mathbb e   \\bigl [ \\lvert x^{(1 ) } \\rvert^2   \\bigr]^2 + 4 \\mathbb e   \\bigl [ \\langle x^{(1 ) } , x^{(2 ) } \\rangle^2 \\bigr ] \\\\   & \\leq 2 ( \\kappa + 1 ) \\mathbf{tr}(\\sigma)^2 + 4 \\mathbf{tr } \\bigl ( \\sigma^2 \\bigr).,\\end{aligned}\\ ] ] which concludes the proof .    using these tighter bounds , we can improve @xmath826 to @xmath827}{\\bigl [ \\bigl ( 1 -   \\frac{q-2}{q(q-1 ) } \\bigr ) \\lvert \\sigma \\rvert_{\\infty }   + \\frac{1}{q } \\bigl ( \\kappa + \\frac{1}{q-1 } \\bigr ) \\mathbf{tr}(\\sigma ) \\bigr ]   t }   \\\\   \\shoveright { + \\log(k ) + \\log(\\epsilon^{-1 } ) \\biggr ) \\biggr]^{1/2 } }   \\\\   + \\sqrt { \\frac{98.5 \\bigl [ q \\bigl ( 1 -   \\frac{q-2}{q(q-1 ) } \\bigr ) \\lvert \\sigma \\rvert_{\\infty }   + \\bigl ( \\kappa + \\frac{1}{q-1 } \\bigr ) \\mathbf{tr}(\\sigma ) \\bigr ] } { t}}.\\end{gathered}\\ ] ] therefore , in the case when @xmath828 we have @xmath829 \\leq \\frac{1}{q}\\biggl ( \\kappa+1 + \\frac{2}{q(q-1 ) }",
    "\\biggr )   \\mathbf{tr}(\\sigma ) n(\\theta)\\ ] ] and hence , recalling that @xmath830 , we can take @xmath831 if we compare the above result with the bound obtained in proposition [ prop2 ] for the gram matrix estimator , we see that the first appearance of @xmath71 in the definition of @xmath826 has been replaced with @xmath832 and that the second appearance of @xmath71 has been replaced with @xmath833 thus , when @xmath834 , that is not a very strong hypothesis , we can take at least @xmath835 , and obtain an improved bound for the estimation of @xmath836 that is not much larger than the bound for the estimation of the centered gram matrix , that requires the knowledge of @xmath837 , since the difference between the two bounds is just a matter of replacing @xmath71 with @xmath838 .                    shawe - taylor , j. , williams , c. and cristianini , c. and kandola , j. ( 2005 ) _ on the eigenspectrum of the gram matrix and its relationship to the operator eigenspectrum_. eds . ) : alt 2002 , lnai 2533 , pages 2340 , springer - verlag            zwald , l. , bousquet , o. and blanchard , g. ( 2004 ) _ statistical properties of kernel principal component analysis_. in learning theory , volume 3120 of lecture notes in comput .",
    ", pages 594608 .",
    "springer , berlin"
  ],
  "abstract_text": [
    "<S> in this paper we investigate the question of estimating the gram operator by a robust estimator from an i.i.d . sample in a separable hilbert space and we present uniform bounds that hold under weak moment assumptions . </S>",
    "<S> the approach consists in first obtaining non - asymptotic dimension - free bounds in finite - dimensional spaces using some pac - bayesian inequalities related to gaussian perturbations of the parameter and then in generalizing the results in a separable hilbert space . </S>",
    "<S> we show both from a theoretical point of view and with the help of some simulations that such a robust estimator improves the behavior of the classical empirical one in the case of heavy tail data distributions . </S>"
  ]
}