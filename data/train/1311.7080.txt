{
  "article_text": [
    "traditional machine learning methods usually assume that there are sufficient training samples to train the classifier .",
    "however , in many real - world applications , the number of labeled samples are always limited , making the learned classifier not robust enough .",
    "recently , cross - domain learning has been proposed to solve this problem @xcite , by borrowing labeled samples from a so called  source domain \" for the learning problem of the  target domain \" in hand .",
    "the samples from these two domains have different distributions but are related , and share the same class label and feature space .",
    "two types of domain transfer learning methods have been studied : * classifier transfer * method which learns a classifier for the target domain by the target domain samples with help of the source domain samples , while * cross domain data representation * tries to map all the samples from both source and target domains to a data representation space with a common distribution across domains , which could be used to train a single domain classifier for the target domain @xcite . in this paper , we focus on the cross domain representation problem .",
    "some works have been done in this field by various data representation methods .",
    "for example , blitzer et al .",
    "@xcite proposed the structural correspondence learning ( scl ) algorithm to induce correspondence among features from the source and target domains , daume iii @xcite proposed the feature replication ( fr ) method to augment features for cross - domain learning .",
    "pan et al .",
    "@xcite proposed transfer component analysis ( tca ) which learns transfer components across domains via maximum mean discrepancy ( mmd ) @xcite , and extended it to semi - supervised tca ( sstca ) @xcite .",
    "recently , sparse coding has attracted many attention as an effective data representation method , which represent a data sample as the sparse linear combination of some codewords in a codebook @xcite .",
    "most of the sparse coding algorithms are unsupervised , due to the small number of labeled samples .",
    "some semi - supervised sparse coding methods are proposed to utilize the labeled samples and significant performance improvement has been reported @xcite . in this case",
    ", it would be very interesting to investigate the use of cross - domain representation to provide more available labeled samples from the source domain .",
    "to our knowledge , no work has been done using the sparse coding method to solve the cross - domain problem to fill in this gap , in this paper , we propose a novel cross - domain sparse coding method to combine the advantages of both sparse coding and cross - domain learning . to this end , we will try to learn a common codebook for the sparse coding of the samples from both the source and target domains . to utilize the class labels ,",
    "a semi - supervised regularization will also be introduced to the sparse codes .",
    "moreover , to reduce the mismatch between the distributions of the sparse codes of the source and target samples , we adapt the mmd rule to sparse codes .",
    "the remaining of this paper is organized as follows : in section [ sec : crodomsc ] , we will introduce the formulations of the proposed cross - domain sparse coding ( crodomsc ) , and its implementations .",
    "section [ sec : exp ] reports experimental results , and section [ sec : concl ] concludes the paper .",
    "in this section , we will introduce the proposed crodomsc method .",
    "we denote the training dataset with @xmath0 samples as @xmath1 , where @xmath0 is the number of data samples , @xmath2 is the feature vector of the @xmath3-th sample , and @xmath4 is the feature dimensionality . it is also organized as a matrix @xmath5\\in \\mathbb{r}^{d\\times n}$ ] .",
    "the training set is composed of the source domain set @xmath6 and target domain set @xmath7 , i.e. , @xmath8 . we also denote @xmath9 and @xmath10 as the number of samples in source and target domain set separatively . all the samples from the source domain set @xmath6",
    "are labeled , while only a few samples from the target domain @xmath7 are labeled . for each labeled sample @xmath2",
    ", we denote its class label as @xmath11 , where @xmath12 is the class label space . to construct the objective function , we consider the following three problems :    sparse coding problem : :    given a sample @xmath13 and a    codebook matrix    @xmath14\\in \\mathbb{r}^{d\\times k}$ ] , where    the @xmath15-th column is the @xmath15-th codeword and    @xmath16 is the number of codewords in the codebook , sparse    coding tries to reconstruct @xmath17 by the linear    reconstruction of the codewords in the codebook as    @xmath18 ,    where    @xmath19^\\top \\in \\mathbb{r}^{k}$ ]    is the reconstruction coefficient vector for    @xmath2 , which should be as sparse as possible ,    thus @xmath20 is also called sparse code",
    ". the    problem of sparse coding can be formulated as follows :    @xmath21 where    @xmath22\\in \\mathbb{r}^{k \\times n}$ ]    is the sparse code matrix , with its @xmath3-th collum the    sparse code of @xmath3-th sample .",
    "semi - supervised sparse coding regularization : :    in the sparse code space , the intra - class variance should be minimized    while the inter - class variance should be maximized for all the samples    labeled , from both target and source domains .",
    "we first define the    semi - supervised regularization matrix as    @xmath23\\in \\{+1,-1,0\\ } ^{n\\times n}$ ] , where    @xmath24 we define the degree of    @xmath2 as    @xmath25 ,    @xmath26 , and @xmath27 as the    is the laplacian matrix",
    ". then we formulate the semi - supervised    regularization problem as the following problem :    @xmath28=tr(v l v^\\top )    \\end{aligned}\\ ] ] in this way , the @xmath29 norm distance    between sparse codes of intra - class pair ( @xmath30 ) will    be minimized , while inter - class pair ( @xmath31 )    maximized . reducing mismatch of sparse code distribution",
    ": :    to reduce the mismatch of the distributions of the source domain and    target domain in the sparse code space , we adopt the mmd @xcite as a    criterion , which is based on the minimization of the distance between    the means of codes from two domains .",
    "the problem of reducing the    mismatch of the sparse code distribution between source and target    domains could be formatted as follows ,    @xmath32=tr(v \\pi v^\\top )    \\end{aligned}\\ ] ] where    @xmath33^\\top \\in \\mathbb{r}^{n}$ ]    with @xmath34 the domain indicator of @xmath3-th    sample defined as @xmath35 and    @xmath36 .    by summarizing the formulations in ( [ equ : sc ] ) , ( [ equ : semisupervised ] ) and ( [ equ : mismatch ] ) , the crodomsc problem is modeled as the following optimization problem : @xmath37 + \\gamma tr[v \\pi v^\\top ] + \\alpha \\sum_{i:{{\\textbf{x}}}_i\\in \\mathcal{d } } \\|{{\\textbf{v}}}_i\\|_1 \\\\ & = \\|x - uv\\|^2_2 + tr[v e v^\\top ] + \\alpha \\sum_{i:{{\\textbf{x}}}_i\\in \\mathcal{d } }",
    "\\|{{\\textbf{v}}}_i\\|_1 \\\\ s.t.&\\|{{\\textbf{u}}}_k\\|\\leq c,~k=1,\\cdots , k \\end{aligned}\\ ] ] where @xmath38 .      since direct optimization of ( [ equ",
    ": objective ] ) is difficult , an iterative , two - step strategy is used to optimize the codebook @xmath39 and sparse codes @xmath40 alternately while fixing the other one .      by fixing the codebook @xmath39",
    ", the optimization problem ( [ equ : objective ] ) is reduced to @xmath41 + \\alpha \\sum_{i:{{\\textbf{x}}}_i\\in \\mathcal{d } } \\|{{\\textbf{v}}}_i\\|_1 \\end{aligned}\\ ] ]    since the reconstruction error term can be rewritten as @xmath42 , and the sparse code regularization term could be rewritten as @xmath43 $ ]   @xmath44 , ( [ equ : object_v ] ) could be rewritten as : @xmath45 when updating @xmath20 for any @xmath46 , the other codes @xmath47 for @xmath48 are fixed .",
    "thus , we get the following optimization problem : @xmath49 with @xmath50 .",
    "the objective function in ( [ equ : scv ] ) could be optimized efficiently by the modified feature - sign search algorithm proposed in @xcite .      by fixing the sparse codes @xmath40 and removing irrelevant terms , the optimization problem ( [ equ : objective ] ) is reduced to @xmath51 the problem is a least square problem with quadratic constraints , and it can be solved in the same way as @xcite .",
    "the proposed * cross domain sparse coding * algorithm , named as * crodomsc * , is summarized in algorithm [ alg : crodomss ] .",
    "we have applied the original sparse coding methods to the samples from both the source and target domains for initialization .",
    "* input * : training sample set from both source and target sets @xmath8 ;    initialize the codebooks @xmath52 and sparse codes @xmath53 for samples in @xmath54 by using single domain sparse coding .    update the sparse code @xmath55 for @xmath2 by fixing @xmath56 and other sparse codes @xmath57 for @xmath48 by solving ( [ equ : scv ] ) .    update the codebook @xmath58 by fixing the sparse code matrix @xmath59 by solving ( [ eqe : object_u ] ) .",
    "* output * : @xmath60 and @xmath61 .",
    "when a test sample from target domain comes , we simply solve problem ( [ equ : scv ] ) to obtain its sparse code .",
    "in the experiments , we experimentally evaluate the proposed cross domain data representation method , crodomsc .      in the first experiment , we considered the problem of cross domain image classification of the photographs and the oil paintings , which are treated as two different domains .",
    "we collected an image database of both photographs and oil paintings .",
    "the database contains totally 2,000 images of 20 semantical classes .",
    "there are 100 images in each class , and 50 of them are photographs , and the remaining 50 ones are oil paintings .",
    "we extracted and concatenated the color , texture , shape and bag - of - words histogram features as visual feature vector from each image .    to conduct the experiment , we use photograph domain and oil painting domain as source domain and target domain in turns .",
    "for each target domain , we randomly split it into training subset ( 600 images ) and test subset ( 400 images ) , while 200 images from the training subset are randomly selected as label samples and all the source domain samples are labeled .",
    "the random splits are repeated for 10 times .",
    "we first perform the crodomsc to the training set and use the sparse codes learned to train a semi - supervised svm classifier @xcite .",
    "then the test samples will also be represented as sparse code and classified using the learned svm .",
    "we compare our crodomsc against several cross - domain data representation methods : sstca @xcite , tca @xcite , fr @xcite and scl @xcite .",
    "the boxplots of the classification accuracies of the 10 splits using photograph and oil painting as target domains are reported in figure [ fig : figpho ] . from figure",
    "[ fig : figpho ] we can see that the proposed crodomsc outperforms the other four competing methods for both photograph and oil painting domains .",
    "it s also interesting to notice that the classification of the fr and sci methods are poor , at around 0.7 .",
    "sstca and tca seems better than fr and sc but are still not competitive to crodomsc .      in the second experiment , we will evaluate the proposed cross - domain data representation method for the multiple user based spam email detection .",
    "a email dataset with 15 inboxes from 15 different users is used in this experiment @xcite .",
    "there are 400 email samples in each inbox , and half of them are spam and the other half non - spam . due to the significant differences of the email source among different users , the email set of different users could be treated as different domains .    to conduct the experiment , we randomly select two users inboxes as source and target domains . the target domain will further be split into test set ( 100 emails ) and training set ( 300 emails , 100 of which labeled , and 200 unlabeled ) .",
    "the source domain emails are all labeled .",
    "the word occurrence frequency histogram is extracted from each email as original feature vector .",
    "the crodomsc algorithm was performed to learn the sparse code of both source and target domain samples , which were used to train the semi - supervised classifier .",
    "the target domain test samples were also represented as sparse codes , which were classified using the learned classifier .",
    "this selection will be repeated for 40 times to reduce the bias of each selection .",
    "figure [ fig : figspam ] shows the boxplots of classification accuracies on the spam detection task . as we can observed from the figure , the proposed crodomsc always outperforms its competitors .",
    "this is another solid evidence of the effectiveness of the sparse coding method for the cross - domain representation problem .",
    "moreover , sstca , which is also a semi - supervised cross - domain representation method , seems to outperform other methods in some cases .",
    "however , the differences of its performances and other ones are not significant .",
    "in this paper , we introduce the first sparse coding algorithm for cross - domain data representation problem .",
    "the sparse code distribution differences between source and target domains are reduced by regularizing sparse codes with mmd criterion . moreover ,",
    "the class labels of both source and target domain samples are utilized to encourage the discriminative ability .",
    "the developed cross - domain sparse coding algorithm is tested on two cross - domain learning tasks and the effectiveness was shown .",
    "this work was supported by the national key laboratory for novel software technology , nanjing university ( grant no .",
    "kfkt2012b17 ) .",
    "j.  blitzer , r.  mcdonald , and f.  pereira .",
    "domain adaptation with structural correspondence learning . in _",
    "2006 conference on empirical methods in natural language processing , proceedings of the conference _ , pages 120  128 , 2006 ."
  ],
  "abstract_text": [
    "<S> sparse coding has shown its power as an effective data representation method . </S>",
    "<S> however , up to now , all the sparse coding approaches are limited within the single domain learning problem . in this paper , we extend the sparse coding to cross domain learning problem , which tries to learn from a source domain to a target domain with significant different distribution . </S>",
    "<S> we impose the maximum mean discrepancy ( mmd ) criterion to reduce the cross - domain distribution difference of sparse codes , and also regularize the sparse codes by the class labels of the samples from both domains to increase the discriminative ability . </S>",
    "<S> the encouraging experiment results of the proposed cross - domain sparse coding algorithm on two challenging tasks  image classification of photograph and oil painting domains , and multiple user spam detection  show the advantage of the proposed method over other cross - domain data representation methods . </S>"
  ]
}