{
  "article_text": [
    "in the bayesian quickest change detection problem proposed by shiryaev @xcite , there is a sequence of random variables , @xmath0 , whose distribution changes at a random time @xmath1 .",
    "it is assumed that before @xmath1 , @xmath0 are independent and identically distributed ( i.i.d . ) with density @xmath2 , and after @xmath1 they are i.i.d .  with density @xmath3 .",
    "the distribution of @xmath1 is assumed to be known and modeled as a geometric random variable with parameter @xmath4 .",
    "the objective is to find a stopping time @xmath5 , at which time the change is declared , such that the average detection delay is minimized subject to a constraint on the probability of false alarm .    in this paper",
    "we extend shiryaev s formulation by explicitly accounting for the cost of the observations used in the detection process .",
    "we capture the observation penalty ( cost ) through the average number of observations used before the change point @xmath1 , and allow for a dynamic control policy that determines whether or not a given observation is taken .",
    "the objective is to choose the observation control policy along with the stopping time @xmath5 , so that the average detection delay is minimized subject to constraints on the probability of false alarm and the observation cost .",
    "the motivation for this model comes from the consideration of the following engineering applications .    in many monitoring applications , for example infrastructure monitoring , environment monitoring , or habitat monitoring , especially of endangered species ,",
    "surveillance is only possible through the use of inexpensive battery operated sensor nodes .",
    "this could be due to the high cost of employing a wired sensor network or a human observer , or the infeasibility of having a human intervention .",
    "for example in habitat monitoring of certain sea - birds as reported in @xcite , the very reason the birds chose the habitat was because of the absence of humans and predators around it .",
    "in these applications the sensors are typically deployed for long durations , possibility over months , and due the constraint on energy , the most effective way to save energy at the sensors is to switch the sensor between on and off states .",
    "an energy - efficient quickest change detection algorithm can be employed here that can operate over months and trigger other more sophisticated and costly sensors , which are possibly power hungry , or more generally , trigger a larger part of the sensor network @xcite .",
    "this change could be a fault in the structures in infrastructure monitoring @xcite , the arrival of the species to the habitat @xcite , etc .    in industrial quality control ,",
    "statistical control charts are designed that can detect a sustained deviation of the industrial process from normal behavior @xcite .",
    "often there is a cost associated with acquiring the statistics for the control charts and it is of interest to consider designing _",
    "economic - statistical _ control chart schemes @xcite .",
    "one approach to economic - statistical control chart design has been to use algorithms from the change detection literature , such as shewhart , ewma and cusum , as control charts , and optimize over the choice of sample size , sampling interval and control limits @xcite .",
    "another approach has been to find optimal sampling rates in the problem of detection of a change in the drift of a sequence of brownian motions with global false alarm constraint @xcite .",
    "thus , these approaches are essentially non - bayesian .",
    "it has been demonstrated , mostly through numerical results , that bayesian control charts , which choose the parameters of the detection algorithms based on the posterior probability that the process is out of control , perform better than the traditional control charts based on shewhart , ewma or cusum ; see @xcite , and the references therein .",
    "the problem of dynamic sampling for detecting a change in the drift of a standard brownian motion is considered for an exponentially distributed change point in @xcite . for practical applications ,",
    "it is of interest to consider the economic design of bayesian control charts in discrete time .",
    "the design of a bayesian economic - statistical control chart is considered for a shift in the mean vector of a multivariate gaussian model in @xcite .",
    "but , the problem is modeled as an optimal stopping problem that minimizes the long term average cost , and hence , there is no control on the number of observations used at each time step .",
    "the process control problem is fundamentally a quickest change detection problem , and it is therefore appropriate that economic - statistical schemes for process control are developed in this framework . in most of the above mentioned or similar applications , changes are rare and quick detection is often required .",
    "so , ideally we would like to take as few observations as possible before change to reduce the observation cost , and skip as few as possible after change to minimize delay , while maintaining an acceptable probability of false alarm .",
    "there have been other formulations of the bayesian quickest change detection problem that are relevant to sensor networks : see @xcite-@xcite . the change detection problem studied here",
    "was earlier considered in a similar set - up for sensor networks in @xcite . but owing to the complexity of the problem , the structure of the optimal policy was studied only numerically , and for the same reason , no analytical expressions were developed for the performance .",
    "the goal of this paper is to develop a deeper understanding of the trade - off between delay , false alarm probability , and the cost of observation or information , and to identify a control policy for data - efficient quickest change detection that has some optimality property and is easy to design .",
    "we extend the shiryaev s formulation by also accounting for the cost of observations used before the change point , and obtain an _ a posteriori _ probability based two - threshold algorithm that is asymptotically optimal .",
    "specifically , we show that the probability of false alarm and the average detection delay of the two - threshold algorithm approaches that of the shiryaev algorithm , for a fixed observation cost constraint , as the probability of false alarm goes to zero . even for moderate values of the false alarm probability",
    ", we will show using simulations that the two - threshold algorithm provides good performance .",
    "we also provide an asymptotic analysis of the two - threshold algorithm , i.e. , we obtain expressions for the delay , probability of false alarm and the average number of observations used before and after change , using which the thresholds can be set to meet the constraints on probability of false alarm and observation cost .",
    "the layout of the paper is as follows . in the following section",
    ", we set up the data - efficient quickest change detection problem with on - off observation control and introduce the two - threshold algorithm .",
    "in section  [ sec : analysis ] , we provide an asymptotic analysis of the two - threshold algorithm . in section  [ sec : approx_and_numerical ] , we provide approximations using which the analytical expressions in section  [ sec : analysis ] can be computed , and validate the approximations by comparing them with the corresponding values obtained via simulations . in section",
    "[ sec : optimalityoftwothreshold ] , we prove the asymptotic optimality of the two - threshold algorithm , provide its false alarm - delay - observation cost trade - off curves and also compare its performance with the naive approach of fractional sampling , where observations are skipped randomly .",
    "as in the model for the classical bayesian quickest change detection problem described in section [ sec : intro ] , we have a sequence of random variables @xmath0 , which are i.i.d .  with density @xmath2 before the random change point @xmath1 , and i.i.d .  with density @xmath3 after @xmath1",
    "the change point @xmath1 is modeled as geometric with parameter @xmath4 , i.e. , for @xmath6 , @xmath7 where @xmath8 is the indicator function , and @xmath9 represents the probability of the change having happened before the observations are taken .",
    "typically @xmath9 is set to 0 .    in order to minimize the average number of observations used before @xmath1 , at each time instant , a decision is made on whether to use the observation in the next time step , based on all the available information .",
    "let @xmath10 , with @xmath11 if it is been decided to take the observation at time @xmath12 , i.e. @xmath13 is available for decision making , and @xmath14 otherwise .",
    "thus , @xmath15 is an on - off ( binary ) control input based on the information available up to time @xmath16 , i.e. , @xmath17 with @xmath18 denoting the control law and @xmath19 defined as : @xmath20.\\ ] ] here , @xmath21 represents @xmath22 if @xmath23 , otherwise @xmath22 is absent from the information vector @xmath24 .",
    "the choice of @xmath25 is based on the prior @xmath9 .    as in the classical change detection problem",
    ", the end goal is to choose a stopping time on the observation sequence at which time the change is declared . denoting the stopping time by @xmath5 , we can define the average detection delay ( add ) as @xmath26.\\ ] ] further , we can define the probability of false alarm ( pfa ) as @xmath27 the new performance metric for our problem is the average number of observations ( ano ) used before @xmath1 in detecting the change : @xmath28.\\ ] ]    let @xmath29 represent a policy for cost - efficient quickest change detection .",
    "we wish to solve the following optimization problem : @xmath30 where @xmath31 and @xmath32 are given constraints . towards solving ,",
    "we consider a lagrangian relaxation of this problem which can be approached using dynamic programming : @xmath33 where @xmath34 and @xmath35 are lagrange multipliers .",
    "it is easy to see that if @xmath34 and @xmath35 can be found such that the solution to achieves the pfa and ano constraints with equality , then the solution to is also the solution to .",
    "the problem in can be converted to an appropriate markov control problem using steps similar to those followed in @xcite .",
    "let @xmath36 denote the state of the system at time @xmath12 .",
    "after the stopping time @xmath5 it is assumed that the system enters a terminal state @xmath37 and stays there . for @xmath38 , we have @xmath39 for @xmath40 , and @xmath41 otherwise",
    ". then we can write @xmath42\\ ] ] and @xmath43 $ ] .",
    "furthermore , let @xmath44 denote the stopping decision variable at time @xmath12 , i.e. , @xmath45 if @xmath38 and @xmath46 otherwise . then the optimization problem in can be written as a minimization of an additive cost over time : @xmath47\\ ] ] with @xmath48.\\ ] ] using standard arguments @xcite it can be seen that this optimization problem can be solved using infinite horizon dynamic programming with sufficient statistic ( belief state ) given by : @xmath49 using bayes rule , @xmath50 can be shown to satisfy the recursion @xmath51 where @xmath52 and @xmath53 with @xmath54 being the likelihood ratio , and @xmath55 .",
    "note that the structure of recursion for @xmath50 is independent of time @xmath12 .",
    "the optimal policy for the problem given in can be obtained from the solution to the bellman equation : @xmath56,\\ ] ] where @xmath57 with @xmath58 and @xmath59.\\ ] ] it can be shown by an induction argument ( see , e.g. , @xcite ) that @xmath60 , @xmath61 and @xmath62 are all non - negative concave functions on the interval @xmath63 $ ] , and that @xmath64 .",
    "also , by jensen s inequality @xmath65 ) = b_{0 } ( p ) , \\quad p \\in [ 0,1].\\ ] ] let @xmath66 then , from the above properties of @xmath60 , @xmath61 and @xmath62 , it is easy to show that the optimal policy @xmath67 for the problem given in has the following structure : @xmath68    since , @xmath69 , the algorithm in reduces to the classical shiryaev algorithm when @xmath70 @xcite .",
    "the optimal stopping rule @xmath71 is similar to the one of the shiryaev problem .",
    "but , the observation control is not explicit and one has to evaluate the differential cost function @xmath72 at @xmath50 at each time step to choose @xmath73 .    in fig .",
    "[ fig : bellmantwothresholdcurvesb0b1 ] we plot the differential cost function @xmath74 and @xmath75 as a function of @xmath76 .",
    "we note that , although @xmath77 and @xmath78 are concave in @xmath76 , their difference @xmath79 is not .",
    "thus , the line @xmath75 can intersect @xmath79 at more than two points .",
    "however , in fig .",
    "[ fig : bellmantwothresholdcurvesb0b1 ] we see that there are exactly two points of intersection , one at @xmath80 an another at @xmath81 . in fig .",
    "[ fig : bellmantwothresholdcurvesaj ] we plot the functions @xmath82 and @xmath83 as a function of @xmath76 .",
    "this figure shows that the stopping threshold is @xmath84 .",
    "thus , from fig  [ fig : bellmantwothresholdcurvesb0b1 ] and [ fig : bellmantwothresholdcurvesaj ] we see that the optimal policy has two thresholds . for most of the system parameters",
    "we have tried , the cost functions behave in this way , and hence for these values , the following two - threshold policy is optimal .",
    "[ algo : twothreshold ] start with @xmath85 and use the following control , with @xmath86 , for @xmath87 : @xmath88 the probability @xmath50 is updated using and .",
    "extensive numerical studies of the bellman equation also shows that there exists choices of @xmath4 , @xmath2 , @xmath3 , @xmath34 and @xmath35 for which is not optimal . in fig .",
    "[ fig : bellmanthreethresholdcurves ] we plot one such case . note from fig .",
    "[ fig : bellmanthreethresholdcurvesb0b1 ] that again there are two points of intersection of the plotted curves , one at @xmath89 and another at @xmath90 . but fig .",
    "[ fig : bellmanthreethresholdcurvesaj ] shows that @xmath91 .",
    "thus , the optimal policy has three thresholds .",
    "but , note that the value of @xmath92 is quite large and hence impractical .",
    "also , simulations with these choices of thresholds show that the ano is approximately zero .",
    "in all the cases we have found , for which the two - threshold policy is not optimal , the value of @xmath4 is large and ano is almost zero .    from a practical point of view , even if a two - threshold policy or algorithm is not optimal , one would like to use the algorithm for the following reasons . first , as the asymptotic analysis given in section [ sec : analysis ] will reveal , if the pfa constraint is moderate to small and the ano constraint is not very severe , then the thresholds @xmath93 and @xmath94 in @xmath95 can be set independently : the threshold @xmath93 can be set only based on the constraint @xmath31 , and the threshold @xmath94 can be set based on the constraint @xmath32 alone .",
    "second , apart from being simple , the two - threshold algorithm is asymptotically optimal as the pfa @xmath96 . finally , @xmath97 has good trade - off curves , i.e. , the ano of @xmath95 can be reduced by up to 70% , by keeping the add of the @xmath95 within 10% of the add of the shiryaev algorithm .",
    "it is interesting to note that a two - threshold algorithm similar to that in was shown to be exactly optimal in @xcite for a different but related problem of quality control where inspection costs are considered or when the tests are destructive .",
    "in this section we derive asymptotic approximations for add , pfa and ano for the two - threshold algorithm @xmath95 . to that end",
    ", we first convert the recursion for @xmath50 ( see and ) to a form that is amenable to asymptotic analysis .",
    "define , @xmath98 for @xmath87 .",
    "this new variable @xmath99 has a one - to - one mapping with @xmath50 .",
    "by defining @xmath100 we can write the recursions ( [ eq : recursionskip ] ) and ( [ eq : recursiontake ] ) in terms of @xmath99 .    for @xmath101 , @xmath102 and @xmath103 with @xmath104 here",
    "we have used the fact that @xmath105 if @xmath106 , and @xmath107 otherwise ( see ( [ eq : optimalalgo ] ) ) .",
    "the crossing of thresholds @xmath93 and @xmath94 by @xmath50 is equivalent to the crossing of thresholds @xmath108 and @xmath109 by @xmath99 . thus the stopping time for @xmath95 ( equivalently @xmath110 with some abuse of notation ) is @xmath111    in this section we study the asymptotic behavior of @xmath110 in terms of @xmath99 , under various limits of @xmath112 and @xmath4 .",
    "specifically , we provide two asymptotic expressions for add , one for fixed thresholds @xmath112 , as @xmath113 , and another for fixed @xmath109 and @xmath4 , as @xmath114 .",
    "we also provide , as @xmath114 and @xmath115 , an asymptotic expression for pfa for fixed @xmath109 .",
    "finally , we also provide asymptotic estimates of the average number of observations used before ( ano ) and after the change point @xmath1 .",
    "note that the limit of @xmath114 corresponds to pfa @xmath96 .",
    "[ fig : zkevolution ] shows a typical evolution of @xmath110 , i.e. , of @xmath99 using ( [ eq : zkrecursiontake ] ) and ( [ eq : zkrecursionskip ] ) , starting at time 0 .",
    "note that for @xmath116 , recursion ( [ eq : zkrecursiontake ] ) is employed , while outside that interval , recursion ( [ eq : zkrecursionskip ] ) , which only uses the prior @xmath4 , is employed . as a result",
    "@xmath99 increases monotonically outside @xmath117 .     for @xmath118 , @xmath119 , and @xmath120 , with thresholds @xmath121 , and @xmath122 , corresponding to the @xmath50 thresholds @xmath123 and @xmath124 , respectively .",
    "also @xmath125.,width=340,height=226 ]    from fig .",
    "[ fig : zkevolution ] again , each time @xmath99 crosses @xmath109 from below , it can either increase to @xmath108 ( point @xmath5 ) , or it can go below @xmath109 and approach @xmath109 monotonically from below , at which time it faces a similar set of alternatives .",
    "thus the passage to threshold @xmath108 possibly involves multiple cycles of the evolution of @xmath99 below @xmath109 .",
    "we will show in section  [ sec : delay ] that after the change point @xmath1 , following a finite number of cycles below @xmath109 , @xmath99 grows up to cross @xmath108 , and the time spent on the cycles below @xmath109 is insignificant as compared to @xmath126 , as @xmath114 .",
    "in fact we show that , asymptotically , the time to reach @xmath108 is equal to the time taken by the classical shiryaev algorithm to reach @xmath108 .",
    "( note that for the classical shiryaev algorithm the evolution of @xmath99 would be based on ) .",
    "when @xmath99 crosses @xmath108 from below , it does so with an overshoot .",
    "overshoots play a significant role in the performance of many sequential algorithms ( see @xcite , @xcite ) and they are central to the performance of @xmath110 as well . in section [ sec :",
    "pfaanalysis ] , we show that pfa depends on the threshold @xmath108 and the overshoot @xmath127 as @xmath128 , but is _ not _ a function of the threshold @xmath109 .",
    "the number of observations taken during the detection process is the total time spent by @xmath99 between @xmath109 and @xmath108 . as @xmath114 ,",
    "@xmath99 crosses @xmath108 only after change point @xmath1 , with high probability .",
    "the total number of observations taken can thus be divided in to two parts : the part taken before @xmath1 ( ano ) , which is the fraction of time @xmath99 is above @xmath109 ( and hence depends only on @xmath109 ) , and the part taken after @xmath1 . in section [ sec : energy ] we show that , asymptotically , the average number of observations taken after @xmath1 is approximately equal to the delay itself . in section [ sec : approx_and_numerical ]",
    "we provide approximations using which the asymptotic expressions can be computed and provide numerical results to demonstrate that under various scenarios , for limiting as well as moderate values of @xmath108 , @xmath109 , and @xmath4 , our asymptotic expressions for add , pfa and ano provide good approximations . in section [ sec : optimalityoftwothreshold ] we use the asymptotic expressions for add and pfa to show asymptotic optimality of @xmath110 .",
    "we begin our analysis by first obtaining the asymptotic overshoot distribution for @xmath127 using nonlinear renewal theory @xcite . as mentioned above",
    ", this will be critical to the pfa analysis .",
    "for convenience of reference , in table [ tab : glosaary ] , we provide a glossary of important terms used in this paper .",
    "[ htbp ]    in what follows , we use @xmath129 and @xmath130 to denote , respectively , the expectation and probability measure when change happens at time @xmath131 .",
    "we use @xmath132 and @xmath133 to denote , respectively , the expectation and probability measure when the entire sequence @xmath0 is i.i.d . with density @xmath2 . note that , @xmath134 as @xmath135 is used to denote that @xmath136 in the specified limit .      in this section",
    "we characterize the overshoot distribution of @xmath99 as it crosses @xmath108 as @xmath128 . in analyzing the trajectory of @xmath99",
    ", it useful to allow for arbitrary starting point @xmath137 ( shifting the time axis ) .",
    "we first combine the recursions in ( [ eq : zkrecursiontake ] ) and ( [ eq : zkrecursionskip ] ) to get : @xmath138 by defining @xmath139 and expanding the above recursion , we can write an expression for @xmath140 : @xmath141 here @xmath142 is used to represent all terms other than the first in the equation above : @xmath143 as defined in @xcite , @xmath142 is a _ slowly changing _ sequence if @xmath144{n\\to\\infty } 0,\\ ] ] and for every @xmath145 , there exists @xmath146 and @xmath147 such that for all @xmath148 @xmath149 if indeed @xmath150 is a slowly changing sequence , then the distribution of @xmath151 , as @xmath128 , is equal to the asymptotic distribution of the overshoot when the random walk @xmath152 crosses a large positive boundary .",
    "we have the following result .",
    "[ thm : asympovershootdist ] let @xmath153 be the asymptotic distribution of the overshoot when the random walk @xmath152 crosses a large positive boundary under @xmath154 .",
    "then for fixed @xmath4 and @xmath109 , under @xmath154 , we have the following :    1 .",
    "@xmath150 is a slowly changing sequence .",
    "@xmath153 is the distribution of @xmath155 as @xmath128 , i.e. , @xmath156 = r(x).\\ ] ]    when @xmath157 , @xmath99 evolves as in the classical shiryaev algorithm statistic , and it is easy to see that in this case : @xmath158 \\\\ & = & \\log\\left[e^{z_0 } + \\sum_{k=0}^{n-1 } \\rho ( 1-\\rho)^k \\prod_{i=1}^k \\frac{f_0(x_i)}{f_1(x_i)}\\right ] .",
    "\\nonumber\\\\\\end{aligned}\\ ] ] it was shown in @xcite that this @xmath150 sequence ( for @xmath159 ) , with @xmath160 , is a slowly changing sequence .",
    "it is easy to show that @xmath150 is a slowly changing sequence even if @xmath137 is a random variable .",
    "also , if @xmath161 is the last time @xmath99 crosses @xmath109 from below , then note that , after @xmath161 , the last term @xmath162 in vanishes , and @xmath142 in behaves like the @xmath142 for @xmath157 .",
    "we prove the theorem using these observations . the detailed proof is given in the appendix to this section .",
    "we first obtain an expression for pfa as a function of the overshoot when @xmath99 crosses @xmath108 .",
    "[ lem : pfaeq ] for fixed @xmath4 and @xmath109 , @xmath164 = e^{-a } \\mathrm{e}[e^{-(z_{\\tau } - a)}|   \\tau \\geq \\gamma](1+o(1 ) ) \\ \\ \\ \\ \\ \\",
    "\\mbox { as } a \\ \\to \\infty.\\end{aligned}\\ ] ]    see the appendix for the proof .",
    "from lemma [ lem : pfaeq ] , it is evident that pfa depends on the overshoot when @xmath99 crosses @xmath108 as @xmath128 .",
    "since the overshoot has an asymptotic distribution ( theorem [ thm : asympovershootdist ] ) that depends only on densities @xmath2 , @xmath3 and prior @xmath4 , and is independent of @xmath109 , it is natural to expect that as @xmath128 , pfa is completely characterized by the asymptotic distribution @xmath153 and is not a function of the threshold @xmath109 .",
    "this is indeed true and is established in the following theorem .",
    "[ thm : pfa ] for a fixed @xmath109 and @xmath4 , @xmath165    the proof is provided in the appendix .      the pfa for @xmath110 have the following bound : @xmath166 \\leq 1-a = \\frac{1}{1+e^a } \\leq e^{-a}.\\ ] ] using this upper bound we can show that the add of @xmath110 is given by : @xmath167 \\nonumber\\\\ & = & \\mathrm{e}[\\tau-\\gamma| \\tau\\geq \\gamma](1 + o(1 ) )",
    "\\   \\mbox { as } a \\to \\infty.\\end{aligned}\\ ] ] in the following we provide two different expressions for @xmath168 $ ] . the first one is obtained by keeping @xmath109 fixed and taking @xmath113 .",
    "this expression will be used to get accurate delay estimates for @xmath110 in section [ sec : approx_and_numerical ] next , we will provide another asymptotic expression for @xmath168 $ ] for a fixed @xmath109 , @xmath4 and as @xmath128 .",
    "we show that in this limit , @xmath168 $ ] converges to the shiryaev delay .",
    "this fact will be used to prove the asymptotic optimality of @xmath110 in section [ sec : optimalityoftwothreshold ] .",
    "it was discussed in reference to fig .",
    "[ fig : zkevolution ] that each time @xmath99 crosses",
    "@xmath109 from below , it faces two alternatives , to cross @xmath108 without ever coming back to @xmath109 or to go below @xmath109 and cross it again from below .",
    "it was mentioned that the passage to the threshold @xmath108 is through multiple such cycles .",
    "motivated by this we define the following stopping times @xmath169 and @xmath170 : @xmath171 and @xmath172 let @xmath173 be the constant time taken by @xmath99 to move from @xmath174 to @xmath175 using the recursion ( [ eq : zkrecursionskip ] ) , i.e. @xmath176 then , we can write @xmath170 as a function of @xmath169 using : @xmath177 the significance of these stopping times is as follows .",
    "if we start the process at @xmath125 and _ reset @xmath99 to @xmath109 each time it crosses @xmath109 from below _ , then the time taken by @xmath99 to move from @xmath109 to @xmath108 is the sum of a finite but random number of random variables with distribution of @xmath170 , say @xmath178 . for @xmath179 , @xmath180 , and @xmath181 .",
    "thus the time to reach @xmath108 in this case is @xmath182 $ ] .",
    "let @xmath183.\\ ] ]    the behavior of the delay path depends on @xmath184 , the value of @xmath99 at the change point @xmath1 , and how @xmath99 evolves after that point .",
    "we use @xmath185 to indicate that @xmath99 approaches @xmath109 from below for some @xmath186 , i.e. @xmath187 . and use @xmath188 to represent the event that @xmath99 crossed @xmath108 without ever coming back to @xmath109 , i.e. , @xmath189 .",
    "we define the following three disjoint events : @xmath190 thus , under the event @xmath191 , the process @xmath99 starts below @xmath109 at @xmath1 , and reaches @xmath108 after multiple up - crossings of the threshold @xmath109 . under the event @xmath192 , the process @xmath99 starts above @xmath109 at @xmath1 , and crosses @xmath109 before @xmath108 . it then has multiple up - crossings of @xmath109 , similar to the case of event @xmath191 . under event @xmath193 , the process @xmath99 starts above @xmath109 at @xmath1 , and reaches @xmath108 without ever coming below @xmath109",
    ".    also define , @xmath194 and let @xmath195 be defined with @xmath174 similar to ( [ eq : extendedtest ] ) .",
    "thus , @xmath169 and @xmath196 have the same distribution .",
    "similarly , @xmath170 and @xmath197 are identically distributed .",
    "the following theorem gives an asymptotic expression for the conditional delay .",
    "[ thm : addexact ] for a fixed values of the thresholds @xmath112 , the conditional delay is given by @xmath198 \\hspace{0.2 cm }    = & & \\hspace{-0.5cm}\\bigg [ \\mathrm{add}^s \\ \\ \\mathrm{p}(\\mathcal{a } \\cup \\mathcal{b}| \\tau \\geq \\gamma ) \\bigg.\\nonumber \\\\    & + & \\mathrm{e}[\\lambda(z_\\gamma ) | \\mathcal{c } , \\tau \\geq \\gamma ] \\ \\ \\mathrm{p}(\\mathcal{c}|",
    "\\tau \\geq \\gamma ) \\nonumber \\\\    & + & \\mathrm{e}[t(z_\\gamma , b)| \\mathcal{a } , \\tau \\geq \\gamma ] \\ \\",
    "\\mathrm{p}(\\mathcal{a}| \\tau \\geq \\gamma ) \\nonumber \\\\    & + & \\bigg.\\mathrm{e}[\\lambda(z_\\gamma ) | \\mathcal{b } , \\tau \\geq \\gamma ] \\ \\",
    "\\mathrm{p}(\\mathcal{b}| \\tau \\geq \\gamma ) \\bigg ] \\big ( 1 + o(1)\\big ) \\mbox { as } \\rho \\to 0.\\end{aligned}\\ ] ]    the proof is provided in the appendix .",
    "in section [ sec : approx_and_numerical ] we will provide approximations for various terms in to get an accurate estimate of add .",
    "in lemma [ eq : addsenuequi ] we provide expressions for @xmath199 .",
    "let @xmath200 represent the shiryaev recursion , i.e. , updating @xmath99 using only ( [ eq : zkrecursiontake ] ) .",
    "define @xmath201 thus , @xmath202 is the time for the shiryaev algorithm to reach @xmath175 starting at @xmath203 . also , define the stopping times : @xmath204 and @xmath205 note that , @xmath206 is the stopping time for the classical shiryaev algorithm @xcite and @xmath207 is its modified form which starts at @xmath109 .",
    "we have the following asymptotic expression .",
    "[ eq : addsenuequi ] for a fixed @xmath109 and @xmath4 , @xmath199 , the average time for @xmath99 to cross @xmath108 starting at @xmath109 , under @xmath208 , with @xmath99 reset to @xmath109 each time it crosses @xmath109 from below , is given by @xmath209 + \\mathrm{e}_1[t(z_\\lambda , b ) | \\{z_\\lambda < b\\ } ] \\mathrm{p}_1(z_\\lambda < b)}{\\mathrm{p}_1(z_\\lambda > a)},\\ ] ] and is asymptotically equal to the time taken by the shiryaev algorithm to move from @xmath109 to @xmath108 , i.e. , @xmath210 ( 1 + o(1 ) ) \\mbox { as } a \\to \\infty.\\ ] ]    we have @xmath211 \\\\ & \\overset{({\\romannumeral 1})}{= } & \\mathrm{e}_1[n ] \\mathrm{e}_1[\\lambda ]   \\nonumber\\\\         & \\overset{({\\romannumeral 2})}{= } & \\frac{\\mathrm{e}_1[\\lambda]}{\\mathrm{p}_1(z_\\lambda > a)}\\nonumber\\\\        & = &   \\frac{\\mathrm{e}_1[\\lambda ] + \\mathrm{e}_1[t(z_\\lambda , b ) | \\{z_\\lambda < b\\ } ] \\mathrm{p}_1(z_\\lambda",
    "< b)}{\\mathrm{p}_1(z_\\lambda > a)}.\\end{aligned}\\ ] ] in the above equation , equality @xmath212 follows from wald s lemma @xcite , and equality @xmath213 follows because @xmath214 . to obtain ,",
    "the main idea of the proof is to find stopping times which upper and lower bound the shiryaev time on average and have delay equal to @xmath215}{\\mathrm{p}_1(z_\\lambda > a)}$ ] as @xmath128 .",
    "the details are provided in the appendix .",
    "note that theorem [ thm : addexact ] takes @xmath113 .",
    "we now provide another expression for @xmath168 $ ] , for a fixed @xmath109 and @xmath4 as @xmath128 , which will be used to prove the asymptotic optimality of @xmath110 in section  [ sec : optimalityoftwothreshold ] .",
    "[ lem : add_1 ] for a fixed @xmath109 and @xmath4 , we have as @xmath114 @xmath216 \\leq \\mathrm{add}^s\\left(1 + o(1 ) \\right),\\ ] ] and hence , we have @xmath217 = \\left [ \\frac{a}{d(f_1 , f_0 ) + |\\log(1-\\rho)|}\\right]\\big(1 + o(1)\\big ) \\mbox { as } a\\to \\infty,\\ ] ] where , @xmath218 is the k - l divergence between @xmath2 and @xmath3 .    to get ,",
    "we show that @xmath199 is the dominant term in an upper bound to @xmath168 $ ] as @xmath114 .",
    "the steps followed are very similar to those used to obtain .",
    "the proof is given in the appendix .",
    "to obtain , from lemma [ eq : addsenuequi ] and we have , @xmath219 \\leq \\mathrm{e}_1 [ \\nu_b ] ( 1 + o(1 ) ) \\mbox { as } a \\to \\infty.\\ ] ] to evaluate @xmath220 $ ] , following steps similar to those in section [ sec : overshoot ] , it is easy to show that evolution of @xmath99 from @xmath109 to @xmath108 , with @xmath125 , is according to the random walk @xmath221 and a slowly changing term .",
    "thus , according to lemma 9.1.3 , pg 191 of @xcite , @xmath222 = \\left [ \\frac{a}{d(f_1 , f_0 ) + |\\log(1-\\rho)|}\\right]\\big(1 + o(1)\\big ) \\mbox { as } a\\to \\infty,\\ ] ] and @xmath219 \\leq \\left [ \\frac{a}{d(f_1 , f_0 ) + |\\log(1-\\rho)|}\\right]\\big(1 + o(1)\\big ) \\mbox { as } a\\to \\infty.\\ ] ]    to complete the proof of theorem [ lem : add_1 ] , we now show that @xmath168 $ ] is asymptotically lower bounded by @xmath223 $ ] . from theorem 1 in @xcite , @xmath224 \\geq \\frac{a}{d(f_1 , f_0 ) + |\\log(1-\\rho)|}(1 + o(1 ) )",
    "\\mbox { as } a\\to \\infty.\\ ] ] also , from theorem [ thm : pfa ] , @xmath225 = { \\mathrm{p}}[\\nu_0 < \\gamma](1 + o(1 ) ) \\mbox { as } a\\to \\infty.\\ ] ] thus , we have @xmath219 \\geq \\mathrm{e}[\\nu_0 - \\gamma| \\nu_0 \\geq \\gamma ] ( 1 + o(1 ) ) \\mbox { as } a\\to \\infty.\\ ] ] this is true because shiryaev algorithm is optimal for problem with @xmath226 .",
    "this completes the proof .",
    "first note that , @xmath228\\\\&= & \\mathrm{e}\\left[\\sum_{k=1}^{\\gamma-1 } s_k \\bigg| \\tau \\geq \\gamma \\right]\\mathrm{p}(\\tau \\geq \\gamma ) + \\mathrm{e}\\left[\\sum_{k=1}^{\\tau } s_k \\bigg| \\tau <",
    "\\gamma\\right]\\mathrm{p}(\\tau < \\gamma)\\\\ & = & \\mathrm{e}\\left[\\sum_{k=1}^{\\gamma-1 } s_k \\bigg| \\tau \\geq \\gamma \\right]\\left(1 + o(1)\\right )   \\ \\ \\ \\mbox { as } \\ \\ \\ a \\to \\infty .",
    "\\end{aligned}\\ ] ] the last equality follows because @xmath229 on @xmath230 , and @xmath231 as @xmath128 .    following , we define @xmath232 the theorem below an gives asymptotic expression for @xmath227 .",
    "[ lem : energy ] for fixed @xmath109 , we have as @xmath114 , and as @xmath113 , @xmath233}{\\mathrm{p}_\\infty[\\gamma \\leq \\hat{\\lambda } + t(z_{\\hat{\\lambda}},b ) ] } \\frac{1}{1+e^b}(1 + o(1)),\\end{aligned}\\ ] ] where , @xmath234 is as defined in .",
    "let @xmath235 be the first time @xmath99 crossed @xmath109 from below , i.e. , @xmath236 .",
    "using the fact that observations are used only after @xmath235 , we can write the following : @xmath237 \\nonumber\\\\ & = & \\hspace{-0.3 cm } \\mathrm{e}\\left[\\sum_{k = t(b)}^{\\gamma-1 } s_k \\bigg| \\gamma > t(b ) , \\tau \\geq \\gamma \\right ] \\mathrm{p}(\\gamma > t(b)| \\tau \\geq",
    "\\gamma ) .\\end{aligned}\\ ] ] we now compute each of the two terms in .",
    "for the first term in , we have the following lemma .    [",
    "lem : anobeforegamma ] for a fixed @xmath109 , as @xmath238 , @xmath115 , @xmath239 = \\frac{\\ \\mathrm{e}_\\infty[\\hat{\\lambda}]}{\\mathrm{p}_\\infty[\\gamma \\leq \\hat{\\lambda } + t(z_{\\hat{\\lambda}},b)]}(1+o(1)).\\ ] ]    note that @xmath240 = \\mathrm{e}\\left[\\sum_{k = t(b)}^{\\gamma-1 } s_k \\bigg| \\gamma >",
    "t(b ) , a=\\infty \\right].\\ ] ] to compute the right hand side of the above equation , note that conditioned on @xmath241 , @xmath242 is approximately the number of observations used when the process @xmath99 starts at @xmath125 , goes through multiple cycles below @xmath109 , with each cycle length having distribution of @xmath234 , and the sequence of cycles is interrupted by occurrence of change .",
    "see the appendix for the detailed proof .",
    "for the second term in , we show that @xmath243 is equal to @xmath244 in the limit and is independent of @xmath245 .",
    "[ lem : anoprob ] @xmath246    the proof is provided in the appendix .    the lemmas [ lem : anobeforegamma ] and [ lem : anoprob ] taken together completes the proof of theorem [ lem : energy ] .",
    "define , @xmath247.\\ ] ] thus , @xmath248 is the average number of observations used after the change point @xmath1 . in some applications it might be of interest to have an estimate of @xmath248 as well .",
    "the following theorem shows that @xmath248 is approximately equal to the delay itself .",
    "[ lem : ano1 ] for fixed @xmath109 and @xmath4 , we have @xmath249 ( 1 + o(1 ) ) , \\ \\",
    "\\mbox { as } a \\to \\infty.\\end{aligned}\\ ] ]    the number of observations used after @xmath1 can be written as the difference between the time for @xmath99 to reach @xmath108 and the time spend by it below @xmath109 . for this",
    "we define the variable @xmath250.\\ ] ] thus @xmath251-t_b + 1.\\ ] ] we know from theorem [ lem : add_1 ] that @xmath252 \\approx \\mathrm{e}_1[\\nu_b]$ ] . as @xmath128 , @xmath253 converges , and therefore @xmath254 $ ] for large @xmath108 as well .",
    "the detailed proof is given in the appendix .",
    "in sections [ sec : pfaanalysis]-[sec : energy ] , we have obtained asymptotic expressions for add , pfa , and ano as a function of the system parameters : the thresholds @xmath108 , @xmath109 , the densities @xmath2 and @xmath3 , and the prior @xmath4 .",
    "we now provide approximations for some of the analytical expressions obtained in these sections , and also provide numerical results to validate the analysis .",
    "the observations are assumed to be gaussian with @xmath255 , and @xmath256 , @xmath257 , for the simulations and analysis . in the simulations ,",
    "the pfa values are computed using the expression @xmath258 $ ] .",
    "this guarantees a faster convergence for small values of pfa .      by theorem [ thm : pfa ]",
    ", we have the following approximation for pfa : @xmath259 we note that @xmath260 and @xmath261 can be computed numerically , at least for gaussian observations @xcite .",
    "in this section we provide numerical results to show the accuracy of the above expression for pfa .    in table [ tab : pfaonly ] we compare the analytical approximation with the pfa obtained using simulations of @xmath110 for various choices of @xmath4 , thresholds @xmath112 , and post change mean @xmath262 . from the table we see that the analytical approximation is quite good .    [ htbp ]    in table [ tab : pfa ] , we show that pfa is not a function of @xmath109 for large values of @xmath108 .",
    "we fix @xmath263 , and increase @xmath109 from -2.2 to 0.85 .",
    "we notice that pfa is unchanged in simulations when @xmath109 is changed this way .",
    "this is also captured by the analysis and it is quite accurate .",
    "[ htbp ]      we recall the expressions for @xmath227 from theorem [ lem : energy ] and for @xmath248 from theorem [ lem : ano1 ] : @xmath264}{\\mathrm{p}_\\infty[\\gamma \\leq \\hat{\\lambda } + t(z_{\\hat{\\lambda}},b ) ] } \\frac{1}{1+e^b } \\\\",
    "\\mathrm{ano}_1 & = & \\mathrm{e}_1[\\nu_b].\\end{aligned}\\ ] ]    we first simplify the expression for @xmath227 .",
    "note that @xmath265 & = & 1 - \\mathrm{p}_\\infty[\\gamma >   \\hat{\\lambda } + t(z_{\\hat{\\lambda}},b ) ] \\\\ & = &   1- \\mathrm{e}_\\infty[(1-\\rho)^{\\hat{\\lambda } + t(z_{\\hat{\\lambda}},b)}].\\end{aligned}\\ ] ] thus , using binomial approximation we get @xmath266 \\approx \\rho\\left(\\mathrm{e}_\\infty[\\hat{\\lambda } ] + \\mathrm{e}_\\infty[t(z_{\\hat{\\lambda}},b)]\\right).\\ ] ] thus , we have @xmath267}{\\mathrm{e}_\\infty[\\hat{\\lambda } ] + \\mathrm{e}_\\infty[t(z_{\\hat{\\lambda}},b ) ] } \\frac{1}{1+e^b}.\\end{aligned}\\ ] ] we now provide approximation to compute @xmath268 $ ] and @xmath269 $ ] in . invoking wald s lemma @xcite , we write @xmath268 $ ] as , @xmath270 = \\frac{\\mathrm{e}_\\infty[z_{\\hat{\\lambda } } ] - \\mathrm{e}_\\infty[\\eta_{\\hat{\\lambda}}]}{-d(f_1 , f_0 ) + |\\log(1-\\rho)|}.\\ ] ] we have developed the following approximation for @xmath271 $ ] : @xmath272 \\approx \\frac{\\bar{r } + \\log(1+\\rho e^{-b})}{d(f_1 , f_0 ) - |\\log(1-\\rho)|}.\\ ] ] here , @xmath273 is an approximation to @xmath274 $ ] by ignoring all the random terms after @xmath109 is factored out of it .",
    "this extra @xmath109 will cancel with the @xmath109 in @xmath275 = b + \\mathrm{e}_\\infty[z_{\\hat{\\lambda}}-b]$ ] .",
    "we approximate @xmath276 $ ] by @xmath261 , the mean overshoot of the random walk @xmath277 , with mean @xmath278 , when it crosses a large boundary ( see ( [ eq : zneqsnetan ] ) ) .    for the term",
    "@xmath279 $ ] , we have the following lemma .",
    "[ lem : txyexprn_add ] for fixed values of @xmath203 and @xmath175 , we have @xmath280    the proof is provided in the appendix .",
    "we use to get the following approximation : @xmath281 \\approx \\int_0^{\\infty } \\frac{\\log(1+e^b ) - \\log(1+e^{b - x})}{|\\log(1-\\rho)| } dr(x).\\ ] ] thus , we approximate the distribution of @xmath282 by @xmath153 . based on the second order approximation for @xmath283 $ ] developed in @xcite , we have obtained the following approximation for @xmath223 $ ] : @xmath284 = \\frac{a - \\mathrm{e}[\\eta(b ) ] + \\bar{r}}{d(f_1 , f_0 ) + |\\log(1-\\rho)| } + o(1 ) \\mbox { as } a\\to \\infty,\\ ] ] where , @xmath285 is the a.s .",
    "limit of the slowly changing sequence @xmath142 with @xmath125 under @xmath3 , ( see ) and @xmath286 with @xmath153 as in theorem  [ thm : asympovershootdist ] .    in table [ tab : ano0ano1 ] we demonstrate the accuracy of approximations for @xmath227 and @xmath248 , for various values of @xmath4 , thresholds @xmath112 , and post change mean @xmath262 .",
    "the table shows that the approximations are quite accurate for the parameters chosen .",
    "[ htbp ]      theorem [ lem : add_1 ] gave a first order approximation for @xmath168 $ ] : @xmath219 \\approx \\left [ \\frac{a}{d(f_1 , f_0 ) + |\\log(1-\\rho)|}\\right].\\ ] ] note that , from @xcite , this is also the first order approximation for the add of the shiryaev algorithm , and gives a good estimate of the delay when pfa is small .",
    "for the shiryaev delay , a second order approximation was developed in @xcite ( also see ):",
    "@xmath288 = \\left [ \\frac{a-{\\mathrm{e}}[\\eta(-\\infty ) ] + \\bar{r}}{d(f_1 , f_0 ) + |\\log(1-\\rho)|}\\right ] + o(1 ) \\mbox { as } a \\to \\infty.\\ ] ] so , instead of using @xmath289 , we propose to use the following : @xmath290 \\approx \\left [ \\frac{a-{\\mathrm{e}}[\\eta(-\\infty ) ] + \\bar{r}}{d(f_1 , f_0 ) + |\\log(1-\\rho)|}\\right].\\ ] ]    for the shiryaev algorithm , provides a very good estimate of the delay even for moderate values of pfa . in case of @xmath110 ,",
    "the accuracy of depends on the choice of @xmath109 and hence on the constraint @xmath32 , as having @xmath291 increases the delay .",
    "before we demonstrate this through numerical and simulation results we introduce the following concept : @xmath292.\\ ] ] for example , if @xmath293 , and for some choice of system parameters @xmath294 , then @xmath295 .",
    "thus , the concept of @xmath296 captures the reduction in the average number of observations used before change by employing @xmath110 .    in table",
    "[ tab : addpfa ] we provide various numerical examples where is a good approximation for @xmath168 $ ] .",
    "since , is a good approximation for the shiryaev delay as well , it follows that , for these parameter values , the delay of @xmath110 is approximately equal to the shiryaev delay .",
    "it might be intuitive that if we are aiming for large @xmath296 values of say 90% , then the delay will be close to the shiryaev delay .",
    "but values in table [ tab : addpfa ] shows that it is possible to achieve considerably smaller values of @xmath296 without significantly affecting the delay .",
    "[ htbp ]    however , if the @xmath296 value is small , then this means that the value of @xmath109 is large , and further that the delay is large . in this case , it might happen that is a good approximation only for values of pfa which are very small .",
    "this is demonstrated in table [ tab : addanopercdemo ] .",
    "it is clear from the table that , for the parameter values considered , estimating the delay with less than 10% error is only possible at pfa values of the order of @xmath297 .",
    "[ htbp ]    this motivates the need for a more accurate estimate of the delay .",
    "this is provided below .    from theorem [ thm : addexact ] , recall that we had the following three events : @xmath190 as a first step towards the approximations , we ignore the event @xmath192 : @xmath298 .",
    "that is , we assume that if @xmath299 , then @xmath99 climbs to @xmath108 .",
    "define , @xmath300 then , @xmath301 \\approx",
    "p_b \\ \\mathrm{e}[\\lambda(z_\\gamma ) | \\mathcal{c } , \\tau \\geq \\gamma ] + ( 1-p_b ) ( \\mathrm{e}[t(z_\\gamma , b)| \\mathcal{a } , \\tau \\geq \\gamma ] + \\mathrm{add}^s).\\ ] ] from lemma [ eq : addsenuequi ] , it is easy to show the following : @xmath302 + \\left(\\mathrm{e}_1[\\lambda| \\{z_\\lambda < b\\ } ] + \\mathrm{e}_1[t(z_\\lambda , b ) | \\{z_\\lambda < b\\ } ] \\right ) \\frac{\\mathrm{p}_1(z_\\lambda < b)}{1-\\mathrm{p}_1(z_\\lambda < b)}.\\end{aligned}\\ ] ] we now use the following approximations : @xmath303   \\ & \\approx & \\ \\mathrm{e}[\\lambda(z_\\gamma ) | \\mathcal{c } , \\tau \\geq \\gamma ] \\ \\approx \\",
    "\\frac{a-{\\mathrm{e}}[\\eta(-\\infty ) ] + \\bar{r}}{d(f_1 , f_0 ) + |\\log(1-\\rho)|},\\\\ \\mathrm{e}_1[\\lambda| \\{z_\\lambda < b\\ } ] & \\approx & \\frac{\\bar{r } + \\log(1+\\rho e^{-b})}{d(f_1 , f_0 ) - |\\log(1-\\rho)|},\\\\ \\mathrm{e}_1[t(z_\\lambda , b ) | \\{z_\\lambda",
    "< b\\ } ] & \\approx & t(b-\\bar{r } , b ) \\approx \\frac{\\log(1+e^b ) - \\log(1+e^{b-\\bar{r}})}{|\\log(1-\\rho)|}.\\end{aligned}\\ ] ] to compute , we also need approximations for @xmath304 , @xmath305 and @xmath306 $ ] . those are provided below .",
    "setting @xmath307 we have , by wald s likelihood identity , proposition 2.24 , pg 13 , @xcite , @xmath308.\\ ] ] under @xmath133 , @xmath169 a.s . ends in @xmath109 , and with high probability it takes very small values .",
    "hence , this expressions can be computed using monte carlo simulations .",
    "further , @xmath309}{\\mathrm{e}_\\infty[\\hat{\\lambda } ] + \\mathrm{e}_\\infty[t(z_{\\hat{\\lambda}},b)]}.\\end{aligned}\\ ] ] we already have the approximations for @xmath271 $ ] and @xmath269 $ ] from section [ sec : approxanoano1 ] .",
    "the approximation for @xmath306 $ ] can be obtained as follows ( all expectations conditioned on @xmath310 ) : @xmath311 & = & ( 1-p_b)\\mathrm{e}[t(z_\\gamma , b)|\\{z_\\gamma < b\\}]\\\\ & = &   \\mathrm{e}[t(z_\\gamma , b)|\\{z_\\gamma < b\\ } \\cap",
    "\\{\\gamma > t(-\\infty , b)\\ } ] \\mathrm{p}(\\{\\gamma > t(-\\infty , b)\\ } \\cap \\{z_\\gamma < b\\ } ) \\\\ & & + \\mathrm{e}[t(z_\\gamma , b)|\\{z_\\gamma < b\\ } \\cap\\{\\gamma \\leq t(-\\infty , b)\\}]\\mathrm{p}(\\{\\gamma \\leq t(-\\infty , b)\\ } \\cap \\{z_\\gamma < b\\}).\\end{aligned}\\ ] ] this can be computed using @xmath312}{\\mathrm{e}_\\infty[\\hat{\\lambda } ] + \\mathrm{e}_\\infty[t(z_{\\hat{\\lambda}},b)]},\\ ] ] and @xmath313 to compute conditional expectation of @xmath314 , we need to subtract from @xmath315 , the mean of @xmath1 conditioned on @xmath316 .",
    "specifically , @xmath317 = t(b-\\bar{r},b ) - \\frac{1}{\\mathrm{p}(\\gamma \\leq t(b-\\bar{r},b ) ) } \\sum_{k=1}^{t(b-\\bar{r},b ) } k ( 1-\\rho)^{k-1 } \\rho,\\ ] ] and , @xmath318 = t(-\\infty , b ) - \\frac{1}{\\mathrm{p}(\\gamma \\leq t(-\\infty , b ) ) } \\sum_{k=1}^{t(-\\infty , b ) } k ( 1-\\rho)^{k-1 } \\rho.\\ ] ] thus we have obtained approximations for all the terms for the new approximation for @xmath319 $ ] in ( [ eq : newadd_1 ] ) .    in table [ tab : addanopercdemo_newadd ] , we now reproduce table [ tab : addanopercdemo ] with a new column containing delay estimates computed using the new add ( for @xmath319 $ ] ) approximation .",
    "the values shows that all estimates are nearly within 10% of the actual value .    in table",
    "[ tab : newaddapprox ] , we show the accuracy of the new add approximation , for various values of the system parameters , by comparing it with simulations and also with .",
    "we also set pfa around @xmath320 .",
    "the table clearly demonstrates that the new add approximation predicts the add with less than 10% error .",
    "[ htbp ]    [ htbp ]",
    "in theorem [ lem : add_1 ] we saw that for a fixed @xmath109 and @xmath4 , @xmath219 = \\left [ \\frac{a}{d(f_1 , f_0 ) + |\\log(1-\\rho)|}\\right]\\big(1 + o(1)\\big ) \\mbox { as } a\\to \\infty.\\ ] ] we recall that from @xcite , this is also the asymptotic delay of the shiryaev algorithm .",
    "moreover , from theorem [ thm : pfa ] , the pfa for @xmath110 is @xmath321 again from @xcite , this is the pfa for the shiryaev algorithm .",
    "we thus have the following asymptotic optimality result for @xmath110 .",
    "[ thm : gammaabopt ] with @xmath322 define @xmath323 then for a fixed @xmath32 and @xmath4 , @xmath324(1+o(1 ) ) \\mbox { as } \\alpha\\to 0.\\ ] ] here , for each @xmath325 , @xmath326 is the smallest @xmath109 such that @xmath327 as @xmath128 .",
    "fix @xmath109 such that @xmath328 as @xmath128",
    ". it may happen that the constraint @xmath32 is not met with equality .",
    "then we choose the smallest @xmath109 which satisfies the constraint @xmath32 as @xmath128 .",
    "this choice of threshold @xmath109 is unique for a given @xmath32 because ano is not a function of threshold @xmath108 as @xmath128 .    as @xmath128 , the pfa and add both approach the shiryaev pfa and shiryaev delay , respectively .",
    "thus , as @xmath114 , @xmath110 is optimal over the class of all control policies @xmath329 that satisfy the constraints @xmath31 and @xmath32 .",
    "theorem [ thm : gammaabopt ] shows that for small values of pfa , @xmath110 is approximately optimal , i.e. , it is not possible to outperform @xmath110 by a significant margin .",
    "but for moderate values of pfa , it is not clear if their exists algorithms which can significantly outperform @xmath110 .",
    "our aim is to partially address this issue in this section .    in fig .",
    "[ fig : tradeoffgauss ] we plot the ano - add trade - off for the two - threshold algorithm .",
    "specifically , we compare the two - threshold algorithm with the classical shiryaev algorithm and study how much ano can be reduced without significantly loosing in terms of add . for fig .",
    "[ fig : tradeoffgauss ] we pick four values of @xmath330 . for a fixed @xmath4 ,",
    "we fix @xmath157 and select threshold @xmath108 such that the @xmath331 .",
    "we then increase the threshold @xmath109 to have ano% values of @xmath332 .",
    "we note that it was possible to reduce the ano to 15% of @xmath333 $ ] by increasing the threshold @xmath109 this way , without affecting the probability of false alarm .",
    "[ fig : tradeoffgauss ] shows that we can reduce ano by up to 25% while getting approximately the same add performance as that of the shiryaev algorithm .",
    "moreover , if we allow for a 10% increase in add compared to that of the shiryaev algorithm , then we can reduce ano by up to 70% ( see plot for ano% = 30% ) .    , @xmath334 , and @xmath335.,width=415,height=264 ]    such a behavior",
    "was also observed in table [ tab : addpfa ] , where we saw that the delay for @xmath110 is approximately equal to the shiryaev delay for moderate to large ano% values .",
    "thus , for moderate pfa values , when the ano% is moderate to large , @xmath110 is approximately optimal .      in this section",
    "we compare the performance of @xmath110 with the naive approach of fractional sampling , in which an ano% of @xmath336% is achieved by employing shiryaev algorithm and using a sample with probability @xmath336 . also , in fractional sampling , when a sample is skipped , the posterior probability @xmath50 is updated using .",
    "figure [ fig : comparewithfracsample50perc ] compares the two schemes for ano% of 50% .",
    "we also plot the performance of the shiryaev algorithm for the same values of pfa and @xmath4 .",
    "the figure shows that @xmath110 helps in reducing the observation cost by a significant margin as compared to the fractional sampling scheme . from our approximations , we know that for large @xmath108 @xmath337 when the k - l distance @xmath218 dominates the sum @xmath338 , then we would expect that any scheme that ignores the past observations for observation control will perform poorly as compared to the one that relies on the state of the system to decide whether or not to take a sample in the next time slot . this is verified by the figure : as @xmath113",
    ", we see a significant difference in performances of @xmath110 and the fractional sampling scheme .",
    "the figure also shows that as @xmath4 becomes large , and begins to dominate the sum @xmath338 , the add performance of the fractional sampling scheme approach that of the two - threshold algorithm @xmath110 .    , @xmath339 , and @xmath340.,width=415,height=264 ]",
    "we posed a data - efficient version of the classical bayesian quickest change detection problem , where we control the number of observations taken before the change occurs .",
    "we obtained a two - threshold bayesian algorithm that is asymptotically optimal , has good trade - off curves and is easy to design .",
    "we derived analytical approximations for the add , pfa and ano performance of the two - threshold algorithm using which we can design the algorithm by choosing the thresholds . in particular , we showed that , when the constraint on the pfa is moderate to small and that on the ano is not very small , the two - thresholds can be set independent of each other .",
    "we also provided extensive numerical and simulation results that validate our analysis .",
    "our results indicate that our two - threshold algorithm can significantly save on the number of observations taken before the change , while maintaining the delay relatively unchanged . a comparison with the naive approach of fractional sampling shows that the two - threshold algorithm is indeed very efficient in using observations to detect the change .",
    "our two - threshold algorithm has many engineering applications in settings where an abrupt change has to be detected in a process under observation , but there is a cost associated with acquiring the data needed to make accurate decisions .",
    "an important problem for future research is to see if two - threshold policies are optimal in non - bayesian ( e.g. , minimax ) settings , where we do not have a prior on @xmath1 . in particular , it is of interest to understand how to update the algorithm metric in a non - bayesian setting when we skip an observation . from an application point of view",
    ", one can design a two - threshold algorithm based on the shiryaev - roberts or cusum approaches @xcite , and use the undershoot of the metric when it goes below the threshold ` @xmath109 ' , to design the off times .",
    "furthermore , if we are able to find useful lower bounds on delay for given false alarm and ano constraints , we may be able to use these to prove asymptotic optimality of such heuristic algorithms , as is done for the standard quickest change detection problem @xcite , @xcite . also , such lower bounds can possibly help in obtaining insights for cases where the observations are not i.i.d .",
    "@xcite , @xcite .",
    "other interesting problems in this area include the design of data - efficient optimal algorithms for robust change detection and nonparametric change detection .",
    "we first show that @xmath142 with @xmath157 , and @xmath137 a random variable , is a slowly changing sequence .",
    "let @xmath137 takes value @xmath245 , then @xmath341 \\xrightarrow[n\\to \\infty]{\\mathrm{p}_1- a.s . }",
    "\\log\\left[e^{z_0 } + \\sum_{k=0}^{\\infty } \\rho ( 1-\\rho)^k \\prod_{i=1}^k \\frac{f_0(x_i)}{f_1(x_i)}\\right].\\ ] ] define @xmath342.\\ ] ]    note that @xmath343 as a function of @xmath137 is well defined and finite under @xmath154 .",
    "this is because by jensen s inequality , for @xmath344 , @xmath345 & \\leq & \\log \\left [ e^{z_0 } + \\sum_{k=0}^{\\infty } \\rho ( 1-\\rho)^k \\mathrm{e}_1 \\left(\\prod_{i=1}^k \\frac{f_0(x_i)}{f_1(x_i)}\\right)\\right]\\\\ & = & \\log \\left [ e^{z_0 } + \\sum_{k=0}^{\\infty } \\rho ( 1-\\rho)^k\\right ] = \\log \\left ( e^{z_0 } + 1 \\right).\\end{aligned}\\ ] ] thus @xmath346{\\mathrm{p}_1- a.s . } \\eta(z_0 ) = \\log\\left(e^{z_0 } + \\rho\\right ) + \\sum_{k=1}^\\infty \\log\\left(1 + e^{-z_k}\\rho\\right).\\ ] ] this implies @xmath347 converges a.s . for i.i.d .",
    "@xmath348 and @xmath157 .",
    "this series will also converge with probability 1 if we condition on a set with positive probability .",
    "let change happen at @xmath349 .",
    "we set @xmath350 and assume that @xmath348 , @xmath351 have density @xmath3 , which would happen after @xmath1 .",
    "we first show that starting with the above @xmath137 , the sequence @xmath142 generated in is slowly changing . to verify the first condition , from note that , @xmath352.\\ ] ] since , @xmath353 a.s .",
    ", @xmath354 , also , @xmath355 a.s .",
    "thus both the sequences @xmath356 and @xmath357 are cesaro summable and have cesaro sum of zero .",
    "thus the term inside the square bracket above , when divided by @xmath358 , goes to zero a.s . and hence also in probability .",
    "thus the first condition is verified .    to verify the second condition ,",
    "we first obtain a bound on @xmath359 .",
    "@xmath360 thus , @xmath361 here , for convenience of computation , we use @xmath362 and @xmath363 to represent the first and second partial sums respectively .",
    "now , @xmath364 and we bound the probability @xmath365 as follows .    on the event that @xmath366 , @xmath363 is identically zero , thus for @xmath358 large enough , @xmath367 this is because @xmath368 behaves like a partial sum of a series of type in ( [ eq : appenetan ] ) . since the series in ( [ eq : appenetan ] ) converges if random variables are generated i.i.d .",
    "@xmath3 , it will also converge if conditioned on the event @xmath369 .",
    "thus , the partial sum @xmath368 converges to 0 almost surely , and hence converges to 0 in probability , i.e. , @xmath370 .",
    "select , @xmath371 such that @xmath372 , @xmath373 .",
    "define @xmath374 with @xmath375 if no such @xmath12 exists . on the event @xmath376 , which is the compliment of @xmath369",
    ", @xmath161 is a.s .",
    "then , by noting that @xmath377 for @xmath378 , we get for @xmath358 large enough , @xmath379 since , @xmath161 is almost surely finite , @xmath380 as @xmath381 .",
    "thus we can select @xmath382 such that @xmath383 , @xmath384 .",
    "for the second term , note that conditioned on @xmath378 , @xmath368 behaves like a partial sum of a series of type in ( [ eq : appenetan ] ) , with @xmath137 replaced by @xmath385 . since the series in ( [ eq : appenetan ] ) converges if random variables are generated i.i.d .",
    "@xmath3 beyond @xmath161 , it will also converge if conditioned on the event @xmath386 .",
    "thus , the partial sum @xmath368 converges to 0 almost surely , and hence converges to 0 in probability , i.e. , @xmath387 .",
    "select , @xmath388 such that @xmath389 , @xmath390 .",
    "then @xmath391 , is the desired @xmath146 and pick any @xmath147 .",
    "then for @xmath392 , @xmath393    since the sequence @xmath142 is slowly changing , according to @xcite , the asymptotic distribution of the overshoot when @xmath99 crosses a large boundary under @xmath3 is @xmath153 .",
    "thus we have the following result , @xmath394 = r(x),\\ ] ] where @xmath395 is the probability measure with change happening at @xmath396 .",
    "now , @xmath397",
    "= \\sum_{l=1}^\\infty \\mathrm{p}_l\\left[z_{\\tau}- a \\leq x |",
    "\\tau \\geq l \\right ] \\mathrm{p}(\\gamma = l| \\tau \\geq \\gamma),\\ ] ] and @xmath398 \\mathrm{p}(\\gamma = l| \\tau \\geq \\gamma ) = r(x ) \\mathrm{p}(\\gamma = l ) \\leq 1.\\ ] ] hence we have the desired result by dominated convergence theorem .",
    "since , @xmath399 imply @xmath400 , we have , @xmath401 the required result is obtained by obtaining upper and lower bounds on pfa as follows .",
    "@xmath402 = \\mathrm{e}\\left[\\frac{1}{1+e^{z_\\tau}}\\right ] \\leq \\mathrm{e}\\left[e^{-z_\\tau}\\right].\\end{aligned}\\ ] ] also , @xmath402 = \\mathrm{e}\\left[\\frac{1}{1+e^{z_\\tau}}\\right ] & = & \\mathrm{e}\\left[\\frac{1}{e^{z_\\tau } } \\frac{1}{1+e^{-z_\\tau}}\\right ] \\\\ & \\geq & \\mathrm{e}\\left[\\frac{1}{e^{z_\\tau } } \\frac{1}{1+e^{-a}}\\right ] = \\mathrm{e}\\left[e^{-z_\\tau}\\right](1+o(1 ) ) \\mbox { as } a \\to \\infty.\\end{aligned}\\ ] ] thus , @xmath403(1+o(1 ) ) = e^{-a } \\mathrm{e}[e^{-(z_{\\tau } - a)}](1+o(1 ) ) \\mbox { as } a \\ \\to \\infty.\\ ] ] now note that , @xmath404 = \\mathrm{e}[e^{-(z_{\\tau } - a)}| \\tau \\geq \\gamma](1-\\mathrm{p}(\\tau < \\gamma ) ) + \\mathrm{e}[e^{-(z_{\\tau } - a)}|   \\tau < \\gamma ] \\mathrm{p}(\\tau < \\gamma).\\ ] ] since , @xmath405 \\leq 1-a \\leq e^{-a}$ ] , we can write , @xmath406(1+o(1 ) ) \\ \\ \\ \\ \\ \\   \\mbox { as } a \\ \\to \\infty.\\ ] ] this proves the lemma .",
    "each time @xmath99 crosses @xmath109 from below , is satisfies @xmath407 define , @xmath408 .",
    ". also , each time @xmath99 crosses @xmath109 from below , the average time for @xmath99 to reach @xmath108 can be decreased by setting @xmath410 and increased by setting @xmath411 .",
    "let , @xmath412 ( @xmath413 ) be one plus the number of times @xmath99 goes below @xmath109 before it crosses @xmath108 , when it is reset to @xmath109 ( @xmath414 ) , each time it crosses @xmath109 from below .",
    "now recall the three disjoints events : @xmath190 we can write , @xmath415 & = & \\mathrm{e}[\\tau-\\gamma ; \\mathcal{a } | \\tau \\geq \\gamma ] + \\mathrm{e}[\\tau-\\gamma ;   \\mathcal{b}| \\tau \\geq \\gamma ] + \\mathrm{e}[\\tau-\\gamma ; \\mathcal{c}| \\tau",
    "\\geq \\gamma].\\end{aligned}\\ ] ] now consider each of the three terms on the right hand side of the above equation .    under the event @xmath191 , the process @xmath99 starts below @xmath109 and reaches @xmath108 after multiple up - crossings of the threshold @xmath109 .",
    "then , @xmath416 \\leq \\mathrm{e}[t(z_\\gamma , b)| \\mathcal{a } , \\tau \\geq \\gamma ] \\ \\",
    "\\mathrm{p}(\\mathcal{a}| \\tau \\geq \\gamma )   + \\mathrm{e}_1\\left [ \\sum_{k=1}^{n } \\lambda_k(b)\\right ] \\mathrm{p}(\\mathcal{a}|",
    "\\tau \\geq \\gamma).\\ ] ] this upper bound was obtained by resetting @xmath99 to @xmath109 each time it crosses @xmath109 from below .",
    "similarly , we can get a lower bound by setting @xmath410 each time @xmath99 crosses @xmath109 from below .",
    "thus , @xmath417 \\geq \\mathrm{e}[t(z_\\gamma , b)| \\mathcal{a } , \\tau \\geq \\gamma ] \\ \\",
    "\\mathrm{p}(\\mathcal{a}| \\tau \\geq \\gamma )   + \\mathrm{e}_1\\left [ \\sum_{k=1}^{n_1 } \\lambda_k(b_1)\\right ] \\mathrm{p}(\\mathcal{a}| \\tau \\geq \\gamma).\\ ] ]    now by wald s lemma @xcite , @xmath418 & = & \\mathrm{e}_1[n_1 ] \\mathrm{e}_1[\\lambda(b_1)]\\\\                                                            & \\xrightarrow[\\rho\\to 0 ] { } & \\mathrm{e}_1[n ] \\mathrm{e}_1[\\lambda(b ) ]                                                             \\hspace{0.2cm}= \\hspace{0.2cm}\\mathrm{e}_1\\left [ \\sum_{k=1}^{n } \\lambda_k(b)\\right ] = \\mathrm{add}^s .                                                            \\end{aligned}\\ ] ] thus , @xmath419   = & \\bigg [ \\mathrm{e}[t(z_\\gamma , b)| \\mathcal{a } , \\tau \\geq \\gamma ] \\ \\",
    "\\mathrm{p}(\\mathcal{a}| \\tau \\geq \\gamma ) \\bigg . \\\\   & + \\bigg . \\mathrm{add}^s   \\ \\",
    "\\mathrm{p}(\\mathcal{a}| \\tau \\geq \\gamma ) \\bigg ] \\big(1+o(1)\\big ) \\mbox { as } \\rho\\to 0 .   \\end{split}\\ ] ] under the event @xmath192 , the process @xmath99 starts above @xmath109 and crosses @xmath109 before @xmath108",
    ". it then has multiple up - crossings of @xmath109 , similar to the case of event @xmath191 . arguing in a similar manner",
    ", we get @xmath420    = & \\bigg [ \\mathrm{e}[\\lambda(z_\\gamma ) | \\mathcal{b } , \\tau \\geq \\gamma ]   \\ \\ \\mathrm{p}(\\mathcal{b}| \\tau \\geq \\gamma ) \\bigg . \\\\   & + \\bigg . \\mathrm{add}^s   \\ \\",
    "\\mathrm{p}(\\mathcal{b}| \\tau \\geq \\gamma ) \\bigg ] \\big(1+o(1)\\big ) \\mbox { as } \\rho\\to 0 .",
    "\\end{split}\\ ] ] similarly , considering the event @xmath193 , we get @xmath421 = \\bigg [ \\mathrm{e}[\\lambda(z_\\gamma ) | \\mathcal{c } , \\tau \\geq \\gamma ] \\",
    "\\ \\mathrm{p}(\\mathcal{c}| \\tau \\geq \\gamma ) \\bigg ] \\big(1+o(1)\\big ) \\mbox { as } \\rho\\to 0.\\ ] ]      based on @xmath200 , we define two new recursions , one in which the evolution of @xmath99 is truncated at @xmath109 , @xmath422 and , another in which the overshoot is ignored each time the shiryaev recursion crosses @xmath109 from below , @xmath423 based on these two recursions we define two new stopping times : @xmath424 these two stopping times stochastically upper and lower bound the shiryaev stopping time @xmath207 defined in , i.e. , @xmath425 \\hspace{-0.2cm}&\\leq &   \\hspace{-0.2cm}\\mathrm{e}_1[\\nu_b ] \\leq \\mathrm{e}_1[\\hat{\\nu}_b].\\end{aligned}\\ ] ] recall from that @xmath426 using wald s lemma @xcite , we can get the following expressions : @xmath427 = \\frac{\\mathrm{e}_1[\\lambda]}{\\mathrm{p}_1(z_\\lambda > a ) } , \\hspace{2 cm } \\mathrm{e}_1[\\hat{\\nu}_b ] = \\frac{\\mathrm{e}_1[\\lambda ] + \\mathrm{e}_1[\\nu(z_\\lambda , b ) ; \\{z_\\lambda < b\\ } ] } { \\mathrm{p}_1(z_\\lambda > a)}.\\ ] ] multiplying and dividing @xmath199 by @xmath428 $ ] we get @xmath429 + \\mathrm{e}_1[t(z_\\lambda , b ) ; \\{z_\\lambda < b\\}]}{\\mathrm{e}_1[\\lambda ] } \\frac{\\mathrm{e}_1[\\lambda]}{\\mathrm{p}_1(z_\\lambda > a ) } \\\\ & = & \\mathrm{e}_1[\\tilde{\\nu}_b ] \\frac{\\mathrm{e}_1[\\lambda ] + \\mathrm{e}_1[t(z_\\lambda , b ) ; \\{z_\\lambda < b\\}]}{\\mathrm{e}_1[\\lambda]}\\\\ & = & \\mathrm{e}_1[\\tilde{\\nu}_b ] ( 1 + o(1))\\ \\ \\ \\ \\mbox { as } a\\to \\infty.\\end{aligned}\\ ] ] the last equality follows because @xmath430\\to \\infty$ ] as @xmath128 , while @xmath431 $ ] is not a function of @xmath108 .",
    "similarly , multiplying and dividing @xmath199 by @xmath428 + \\mathrm{e}_1[\\nu(z_\\lambda , b ) ; \\{z_\\lambda < b\\}]$ ] we get @xmath432 \\left(1 + o(1 ) \\right ) \\ \\ \\ \\ \\mbox { as } \\ \\",
    "a\\to \\infty.\\end{aligned}\\ ] ] using these two expressions for @xmath199 and the relationship",
    "that @xmath433 \\leq \\mathrm{e}_1[\\nu_b ] \\leq \\mathrm{e}_1[\\hat{\\nu}_b]$ ] , we have , @xmath434(1 + o(1 ) ) \\mbox { as } a \\to \\infty.\\ ] ]    consider the upper bound : @xmath417 \\leq \\mathrm{e}[t(z_\\gamma , b)| \\mathcal{a } , \\tau \\geq \\gamma ] \\ \\",
    "\\mathrm{p}(\\mathcal{a}| \\tau \\geq \\gamma )   + \\mathrm{add}^s \\",
    "\\ \\mathrm{p}(\\mathcal{a}| \\tau \\geq \\gamma).\\ ] ] similarly , the upper bounds corresponding to the other two events @xmath192 and @xmath193 are : @xmath435 \\leq \\mathrm{e}[\\lambda(z_\\gamma ) | \\mathcal{b } , \\tau \\geq \\gamma ] \\",
    "\\mathrm{p}(\\mathcal{b}| \\tau \\geq \\gamma ) + \\mathrm{add}^s \\ \\",
    "\\mathrm{p}(\\mathcal{b}| \\tau \\geq \\gamma)\\end{aligned}\\ ] ] and , @xmath436 & = & \\mathrm{e}[\\lambda(z_\\gamma ) | \\mathcal{c } , \\tau \\geq \\gamma ] \\ \\",
    "\\mathrm{p}(\\mathcal{c}| \\tau \\geq \\gamma ) \\\\ & \\leq & \\mathrm{e}_1[\\lambda(b ) | z_{\\lambda(b ) } > a ] \\ \\",
    "\\mathrm{p}(\\mathcal{c}| \\tau \\geq \\gamma ) \\\\ & \\leq & \\mathrm{add}^s \\",
    "\\ \\mathrm{p}(\\mathcal{c}| \\tau \\geq \\gamma).\\end{aligned}\\ ] ] substituting in we get , @xmath437 & = & \\mathrm{e}[\\tau-\\gamma ; \\mathcal{a } | \\tau \\geq \\gamma ] + \\mathrm{e}[\\tau-\\gamma ;   \\mathcal{b}| \\tau \\geq \\gamma ] + \\mathrm{e}[\\tau-\\gamma ; \\mathcal{c}| \\tau \\geq \\gamma ] .",
    "\\nonumber \\\\ & \\leq &   \\mathrm{add}^s   + \\mathrm{e}[t(z_\\gamma , b)| \\mathcal{a } , \\tau \\geq \\gamma ] + \\mathrm{e}[\\lambda(z_\\gamma ) | \\mathcal{b } , \\tau \\geq \\gamma].\\end{aligned}\\ ] ]    in equation ( [ eq : taucminusgammma ] ) , we observe that except for @xmath199 , other terms are not a function of threshold @xmath108 .",
    "thus we have @xmath219 \\leq \\mathrm{add}^s\\left(1 + o(1 ) \\right ) \\ \\",
    "\\mbox { as } a \\ \\to \\infty.\\ ] ]    first note that by definition , @xmath438 . also , from ( [ eq : zkrecursionskip ] )",
    "@xmath439 thus @xmath440 equivalently @xmath441 further , the recursion ( [ eq : zkrecursionskip ] ) can be written in terms of @xmath442 for @xmath443 : @xmath444 using this we can write an expression for @xmath445 : @xmath446 using the bounds for @xmath447 obtained above , we get @xmath448 this gives us bounds for @xmath173 : @xmath449 by keeping @xmath450 fixed and taking @xmath451 we get .",
    "each time @xmath99 crosses @xmath109 from below , is satisfies : @xmath407 define , @xmath408 .",
    "then @xmath409 . also ,",
    "each time @xmath99 crosses @xmath109 from below , the average number of observations used before @xmath1 can be increased by setting @xmath410 and decreased by setting @xmath411 .",
    "this is because of the geometric nature of change .",
    "let @xmath452 when it crosses @xmath109 from below , and suppose we reset @xmath99 to @xmath414 .",
    "then , the number of observations used before change , on an average , would be the number of observations used before @xmath99 reaches @xmath203 from @xmath414 , plus the number of observations used there onwards as if the process started at @xmath203 .",
    "similar reasoning can be given to explain why the average number of observations used decreases , if we reset @xmath99 to @xmath109 , each time it crosses @xmath109 from below .",
    "define the following stopping time : @xmath453 thus , @xmath454 is the time for @xmath99 , to start at @xmath174 with @xmath307 , and stop the first time , either @xmath99 approaches @xmath109 from below , or when change happens .",
    "also , let @xmath455 be such that @xmath456 is the number of observations used before @xmath99 was stopped by @xmath454 , i.e. , fraction of @xmath454 when @xmath457 . if @xmath458 and @xmath459 be sequences with distribution of @xmath460 and @xmath461 respectively and if @xmath462 is the number of times @xmath99 crosses @xmath109 from below and is set to @xmath203 at each such instant , then , @xmath463 \\ \\",
    "\\delta^b ] = \\mathrm{e}_\\infty\\left [ \\sum_{k=1}^{l^{b } } { \\tilde{\\lambda}}_k^{b } \\delta_k^{b } \\right ] & \\leq & \\mathrm{e}\\left[\\sum_{k = t(b)}^{\\gamma-1 } s_k \\bigg| \\gamma > t(b ) , a=\\infty \\right ] \\\\ & \\leq&\\mathrm{e}_\\infty\\left [ \\sum_{k=1}^{l^{b_1 } } { \\tilde{\\lambda}}_k^{b_1 } \\delta_k^{b_1 } \\right ] = \\mathrm{e}_\\infty[l^{b_1 } ] \\",
    "\\ \\mathrm{e}_\\infty[{\\tilde{\\lambda}}^{b_1 } \\delta^{b_1}].\\end{aligned}\\ ] ] here the equalities follows from wald s lemma @xcite .    in the above",
    ", @xmath464 is @xmath465)$ ] , and hence @xmath466 = \\frac{1}{\\mathrm{p}_\\infty[\\gamma \\leq { \\tilde{\\lambda}}^{b_1}]}$ ] .",
    "also note that @xmath467}{\\mathrm{p}_\\infty[\\gamma \\leq { \\tilde{\\lambda}}^b ] } \\to 1 \\mbox { as }   \\rho \\to 0.\\ ] ] further , for @xmath468 or @xmath469 , define @xmath470 based on as @xmath471 it is clear that @xmath472 .",
    "thus we have , for both @xmath468 and @xmath469 , @xmath473 & = & \\mathrm{e}_\\infty[{\\tilde{\\lambda}}^x \\delta^x|\\gamma \\leq { \\tilde{\\lambda}}^x \\delta^x]\\mathrm{p}_\\infty[\\gamma \\leq { \\tilde{\\lambda}}^x\\delta^x ] + \\mathrm{e}_\\infty[{\\tilde{\\lambda}}^x \\delta^x| \\gamma > {",
    "\\tilde{\\lambda}}^x \\delta^x]\\mathrm{p}_\\infty[\\gamma > { \\tilde{\\lambda}}^x\\delta^x]\\\\                        & \\to & \\mathrm{e}_\\infty[\\hat{\\lambda}(x ) ] \\mbox { as }   \\rho \\to 0.\\end{aligned}\\ ] ] here , the result follows because as @xmath115 , @xmath456 converges a.s . to a finite limit and @xmath474 \\to 0 $ ] .",
    "also for the same reason , @xmath475 \\to 1 $ ] as @xmath113 .",
    "moreover , since @xmath476 as @xmath113 , we have as @xmath113 @xmath477 \\to \\mathrm{e}_\\infty[\\hat{\\lambda}(b ) ] = \\mathrm{e}_\\infty[\\hat{\\lambda}].\\ ] ] thus , @xmath478=\\frac{\\mathrm{e}_\\infty[\\hat{\\lambda}]}{\\mathrm{p}_\\infty[\\gamma \\leq { \\tilde{\\lambda}}^b]}(1+o(1 ) ) \\ \\ \\ \\mbox { as }   \\rho \\to 0.\\ ] ]    since @xmath479 as @xmath114 , @xmath480 from in lemma [ lem : txyexprn_add ] , with @xmath481 and @xmath482 , we have @xmath483 from this , it is easy to show that @xmath484 by substituting this in the expression for @xmath243 we get the desired result .",
    "using theorem [ lem : add_1 ] we write @xmath248 as @xmath485\\left(1-\\frac{t_b-1}{\\mathrm{e}\\left[\\tau-\\gamma| \\tau \\geq \\gamma\\right]}\\right)\\\\                 & = & { \\mathrm{e}}_1[\\nu_b]\\left(1-\\frac{t_b-1}{\\mathrm{e}\\left[\\tau-\\gamma| \\tau \\geq \\gamma\\right]}\\right ) ( 1 + o(1 ) ) \\ \\ \\ \\mbox { as } \\ \\ a \\to \\infty.\\end{aligned}\\ ] ]",
    "we now obtain an upper bound on @xmath486}$ ] which goes to zero as @xmath114 .",
    "recall that @xmath191 and @xmath192 are the events under which excursions below @xmath109 are possible .",
    "the passage to @xmath108 is through multiple cycles below @xmath109 , and the time spend below @xmath109 in each cycle can be bounded by @xmath487 .",
    "define @xmath488 and @xmath489 as one plus the number of cycles below @xmath109 , under events @xmath191 and @xmath192 respectively .",
    "then , @xmath490 +    \\mathrm{p}_1(\\mathcal{b } ) t(-\\infty , b)\\mathrm{e}[n_{\\mathcal{b}}].\\ ] ] the averages @xmath491 $ ] and @xmath492 $ ] can be written as a series of probabilities , where each term correspond to the event that @xmath99 goes below @xmath109 , and not above @xmath108 , each time it crosses @xmath109 from below .",
    "each of these probabilities can be maximized by setting @xmath99 to @xmath109 , each time it crosses @xmath109 from below .",
    "hence , @xmath491\\leq \\mathrm{e}[n]$ ] and @xmath492\\leq \\mathrm{e}[n]$ ] .",
    "this gives a bound on @xmath493 .",
    "@xmath494.\\ ] ] by using we get as @xmath114 , @xmath495 } \\ \\leq \\",
    "\\frac{t(-\\infty , b)\\mathrm{e}[n]}{\\mathrm{e}_1[\\nu_b ] } ( 1 + o(1 ) ) \\ \\leq \\",
    "\\frac{t(-\\infty , b)\\mathrm{e}[n]}{\\mathrm{e}_1[\\tilde{\\nu}_b ] } ( 1 + o(1 ) ) .",
    "\\ ] ] from we know that @xmath433=\\mathrm{e}_1[\\lambda]\\mathrm{e}[n]$ ] .",
    "thus the upper bound on @xmath486}$ ] goes to 0 as @xmath114 .",
    "this proves the theorem .",
    "a. n. shiryaev , `` on optimal methods in quickest detection problems , '' theory probab .",
    ", vol . 8 , pp .",
    "22 - 46 , 1963 . z.",
    "g. stoumbos , m. r. reynolds jr . , t. p. ryan and w. h. woodall , `` the state of statistical process control as we proceed into the 21st century , '' journal of the american statistical association , vol .",
    "992 - 998 , 2000 .",
    "g. tagaras , `` a survey of recent developments in the design of adaptive control charts , '' journal of quality technology .",
    "212 - 231 . 1998 .",
    "z. g. stoumbos and m. r. reynolds jr . , `` economic statistical design of adaptive control schemes for monitoring the mean and variance : an application to analyzers , '' nonlinear analysis : real world applications , volume 6 , issue 5 , pages 817 - 844 , 2005 .",
    "d. assaf , m. pollak , y. ritov , and b. yakir , `` detecting a change of a normal mean by dynamic sampling with a probability bound on a false alarm , '' ann .",
    "statist . 21:1155 - 1165 , 1993",
    ". b. yakir , `` dynamic sampling policy for detecting a change in distribution , with a probability bound on false alarm , '' ann .",
    "statist . ; 24:2199 - 2214 , 1996",
    ". v. makis , `` multivariate bayesian control chart , '' oper",
    "56 , 2 , pp .",
    "487 - 496 , 2008 .",
    "d. assaf , `` a dynamic sampling approach for detecting a change in distribution , '' ann .",
    "16:236 - 253 , 1988 .",
    "a. mainwaring , d. culler , j. polastre , r. szewczyk , and j. anderson , `` wireless sensor networks for habitat monitoring , '' in proceedings of the 1st acm international workshop on wireless sensor networks and applications ( wsna 02 ) , acm , new york , ny , usa , 88 - 97 , 2002 .",
    "j. a. rice , k. mechitov , s. sim , t. nagayama , s. jang , r. kim , b. f. spencer , g. agha and y. fujino , `` flexible smart sensor framework for autonomous structural health monitoring , '' smart structures and systems , vol . 6 , no .",
    "423 - 438 , 2010 .",
    "v. v. veeravalli , `` decentralized quickest change detection , '' ieee transactions on information theory , vol .",
    "1657 - 1665 , 2001",
    ". y. mei , `` information bounds and quickest change detection in decentralized decision systems , '' ieee transactions on information theory , vol .",
    "51 , pp . 26692681 , 2005 .",
    "a. g. tartakovsky and v. v. veeravalli , `` asymptotically optimal quickest change detection in distributed sensor systems , '' sequential analysis , 27(4 ) : 441 - 475 , 2008 .",
    "l. zacharias and r. sundaresan,``decentralized sequential change detection using physical layer fusion , '' proc .",
    "isit , france , 2007 . t. banerjee , v. sharma , v. kavitha and a. jayaprakasam,``generalized analysis of a distributed energy efficient algorithm for change detection , '' wireless communication , ieee transactions on , vol .",
    "91101 , 2011 . k. premkumar and a. kumar , `` optimal sleep / wake scheduling for quickest intrusion detection using sensor networks , '' ieee infocom , arizona , usa , 2008 . m. a. girshick and h. rubin , `` a bayes approach to a quality control model '' , the annals of mathematical statistics , vol .",
    "114 - 125 , 1952 , d. siegmund , `` sequential analysis : tests and confidence intervals '' , springer series in statistics , 1985 . m. woodroofe , `` nonlinear renewal theory in sequential analysis , '' philadelphia : siam , 1982 .",
    "a. g. tartakovsky and v.v .",
    "veeravalli , `` general asymptotic bayesian theory of quickest change detection , '' siam theory of probability and its applications , vol 49 , no .",
    "458 - 497 , 2005 .",
    "d. p. bertsekas ,  dynamic programming and optimal control ,  vol .",
    "i and ii ( 3rd ed . ) .",
    "athena scientific .",
    "a. g. tartakovsky and g. v. moustakides , `` state - of - the - art in bayesian changepoint detection , '' sequential analysis , vol .",
    "125 - 145 , 2010 . special issue `` celebration of the 75th anniversary of albert shiryaev '' , guest editor a.g . tartakovsky ."
  ],
  "abstract_text": [
    "<S> in this paper we extend the shiryaev s quickest change detection formulation by also accounting for the cost of observations used before the change point . </S>",
    "<S> the observation cost is captured through the average number of observations used in the detection process before the change occurs . </S>",
    "<S> the objective is to select an on - off observation control policy , that decides whether or not to take a given observation , along with the stopping time at which the change is declared , so as to minimize the average detection delay , subject to constraints on both the probability of false alarm and the observation cost . by considering a lagrangian relaxation of the constraint problem , and using dynamic programming arguments </S>",
    "<S> , we obtain an _ a posteriori _ probability based two - threshold algorithm that is a generalized version of the classical shiryaev algorithm . </S>",
    "<S> we provide an asymptotic analysis of the two - threshold algorithm and show that the algorithm is asymptotically optimal , i.e. , the performance of the two - threshold algorithm approaches that of the shiryaev algorithm , for a fixed observation cost , as the probability of false alarm goes to zero . </S>",
    "<S> we also show , using simulations , that the two - threshold algorithm has good observation cost - delay trade - off curves , and provides significant reduction in observation cost as compared to the naive approach of fractional sampling , where samples are skipped randomly . </S>",
    "<S> our analysis reveals that , for practical choices of constraints , the two thresholds can be set independent of each other : one based on the constraint of false alarm and another based on the observation cost constraint alone . </S>"
  ]
}