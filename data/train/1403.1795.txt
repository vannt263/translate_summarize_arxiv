{
  "article_text": [
    "in recent years there has been an increasing activity in the study of optimal switching problems , associated reflected backward stochastic differential equations and systems of variational inequalities , due to the potential use of these types of models / problems to address the problem of valuing investment opportunities , in an uncertain world , when the investor / producer is allowed to switch between different investments / portfolios or production modes . to briefly outline the traditional setting of multi - modes optimal switching problems",
    ", we consider a production facility which can be run in @xmath0 ( @xmath1 ) different production modes and assume that the running pay - offs in the different modes , as well as the cost of switching between modes , depend on an @xmath2-dimensional diffusion process @xmath3 which is a solution to the system of stochastic differential equations @xmath4 where @xmath5 $ ] and @xmath6 is an @xmath7-dimensional brownian motion , @xmath8 , defined on a filtered probability space @xmath9 . in the case of electricity and energy production the process @xmath3 can , for instance , be the electricity price , functionals of the electricity price , or other factors , like the national product or other indices measuring the state of the local and global business cycle , which in turn influence the price . given @xmath3 as in , let the payoff rate in production mode @xmath10 , at time @xmath11 , be @xmath12 and let @xmath13 be the continuous switching cost for switching from mode @xmath10 to mode @xmath14 at time @xmath11 .",
    "a management strategy is a combination of a non - decreasing sequence of @xmath15-adapted stopping times @xmath16 , where , at time @xmath17 , the manager decides to switch the production from its current mode to another one , and a sequence of @xmath15-adapted indicators @xmath18 , taking values in @xmath19 , indicating the mode to which the production is switched . at @xmath17",
    "the production is switched from mode @xmath20 ( current mode ) to @xmath21 .",
    "when the production is run under a strategy @xmath22 , over a finite horizon @xmath23 $ ] , the total expected profit is defined as@xmath24,\\end{aligned}\\ ] ] where @xmath25 is the to @xmath26 associated index process .",
    "the traditional multi - modes optimal switching problem now consists of finding an optimal management strategy @xmath27 such that @xmath28 let from now on @xmath29 denote the filtration generated by the process @xmath30 up to time @xmath11 , i.e. , @xmath31 .",
    "we let @xmath32 $ ] denote the set of all ( admissible ) strategies @xmath22 such that @xmath33 for @xmath34 , and such that the stopping times @xmath16 and the indicators @xmath18 are adapted to the filtration @xmath35 . furthermore ,",
    "given @xmath36 $ ] , @xmath37 , we let @xmath38 , be the subset of strategies such that @xmath39 and @xmath40 a.s . we let @xmath41.\\end{aligned}\\ ] ] then @xmath42 \\to \\r$ ] represents the value function associated with the optimal switching problem on time interval @xmath43 $ ] , and @xmath44 is the optimal expected profit if , at time @xmath45 , the production is in mode @xmath10 and the underlying process is at @xmath46 . under sufficient assumptions",
    "it can be proved that the vector @xmath47 satisfies a system of variational inequalities , e.g. , see @xcite . using another perspective ,",
    "the solution to the optimization problem can be phrased in the language of reflected backward stochastic differential equations .",
    "for these connections , and the application of multi - mode optimal switching problems to economics and mathematics , see @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite and the references therein .",
    "more on reflected backward stochastic differential equations in the context of optimal switching problems can be found in @xcite , @xcite , @xcite , @xcite and @xcite .      in this paper",
    "we formulate and consider a multi - mode optimal switching problem under _ partial _ or _ incomplete _ information .",
    "while assuming that the running pay - offs in the different modes of production , as well as the cost of switching between modes , depend on @xmath48 , with @xmath48 as in , we also assume that the manager of the production facility can only observe an auxiliary , and @xmath30-dependent process , @xmath49 , based on which the manager can only retrieve partial information of the @xmath2-dimensional stochastic process @xmath30 .",
    "more precisely , we assume that the manager can only observe an @xmath50-dimensional diffusion process @xmath51 which solves the system of stochastic differential equations @xmath52 here @xmath53 $ ] and @xmath54 is an @xmath55-dimensional brownian motion , @xmath56 , defined on @xmath57 and independent of @xmath6 .",
    "@xmath58 is assumed to be a continuous and bounded function . from here on we let @xmath59 , denote the filtration generated by the observation process @xmath49 up to time @xmath11 .",
    "note that in our set - up we have @xmath60 , and hence knowledge of the process @xmath49 only gives partial information about the process @xmath30 .",
    "we emphasize that although the value of the fully observable process @xmath49 is known with certainty at time @xmath45 , the value of the process @xmath30 is not .",
    "the observed process @xmath49 acts as a source of information for the underlying process @xmath30 . by construction , in the formulation of an optimal switching problem under partial information we must restrict our strategies , and decisions at time @xmath45 , to only depend on the information accumulated from @xmath49 up to time @xmath45 .",
    "hence , an optimal switching problem under partial information must differ from the standard optimal switching problem in the sense that in the case of partial information , the value of the running payoff functions @xmath61 , and the switching costs @xmath62 , are not known with certainty at time @xmath45 , even though we know @xmath63 .",
    "hence , in this context the production must be run under incomplete information .",
    "our formulation of an optimal switching problem under partial information is based on ideas and techniques from stochastic filtering theory . generally speaking ,",
    "stochastic filtering theory deals with the estimation of an evolving system ( `` the signal '' @xmath30 ) by using observations which only contain partial information about the system ( `` the observation '' @xmath49 ) .",
    "the solution to the stochastic filtering problem is the conditional distribution of the signal process @xmath64 given the @xmath65-algebra @xmath66 , and in the context of stochastic filtering theory the goal is to compute the conditional expectations @xmath67 $ ] , for suitably chosen test functions @xmath68 . in the following the conditional distribution of @xmath64 given @xmath66 ,",
    "is denoted by @xmath69 , i.e. , @xmath70 : = \\int_{\\mathbb{r}^{n_1}}\\phi(x)\\pi_t(dx ) = : \\pi_t(\\phi).\\ ] ] note that the measure - valued ( random ) process @xmath69 introduced in can be viewed as a stochastic process taking values in an infinite dimensional space of probability measures over the state space of the signal .",
    "concerning stochastic filtering we refer to @xcite and @xcite for a survey of the topic .",
    "based on the above we define , when the production is run using an @xmath71-adapted strategy @xmath22 , over a finite horizon @xmath23 $ ] , the total expected profit up to time @xmath72 as @xmath73ds -\\sum_{0 \\leq \\tau_k \\leq t } \\e \\bigl[c_{\\xi_{k-1},\\xi_{k } } ( x_{\\tau_k},y_{\\tau_k},\\tau_k)|\\f^{y}_{\\tau_k}\\bigr]\\biggr ] , \\ ] ] where the to @xmath26 associated index process @xmath25 is defined in the bulk of the paper .",
    "again we are interested in finding an optimal management strategy @xmath27 which maximizes @xmath74 .",
    "let @xmath75 $ ] be defined in analogy with @xmath32 $ ] but with @xmath76 replaced by @xmath71 , and let , for given @xmath36 $ ] , @xmath37 , @xmath77 , be the subset of strategies such that @xmath39 and @xmath40 a.s . given @xmath78 $ ] , and a measure of finite mass @xmath79 , we let @xmath80.\\end{aligned}\\ ] ] then @xmath81 \\to \\r$ ] represents the value function associated with the optimal switching problem under partial information formulated above , on the time interval @xmath43 $ ] , and @xmath82 is the optimal expected profit if , at time @xmath45 , the production is in mode @xmath10 , @xmath83 and the distribution of @xmath84 is given by @xmath79 , @xmath85 . note that for a test function @xmath68 , @xmath67 $ ] is an @xmath86-adapted random variable and hence , the problem in can be seen as a full information problem with underlying process @xmath49 .",
    "in fact , it is this connection to an optimal switching problem with perfect information that underlies our formulation of the optimal switching problem under partial information .",
    "furthermore , note that if @xmath30 is an @xmath87-adapted process , then reduces to , i.e. , to the standard optimal switching problem under complete information .",
    "the object of study in this paper is the value function @xmath82 introduced in and we emphasize and iterate the probabilistic interpretation of the underlying problem in . in the manager",
    "wishes to maximize @xmath74 by selecting an optimal @xmath88 .",
    "however , the manager only has access to the observed process @xmath49 .",
    "the state @xmath30 is not revealed and can only be partially inferred through its impact on the drift of @xmath49 .",
    "thus , @xmath88 must be based on the information contained solely in @xmath49 , i.e. , @xmath88 must be @xmath71-adapted . hence",
    ", the optimal switching problem under partial information considered here gives a model for the decision making of a manager who is not fully aware of the economical environment he is acting in . as pointed out in @xcite , one interesting feature here is the interaction between learning and optimization .",
    "namely , the observation process @xmath49 plays a dual role as a source of information about the system state @xmath30 , and as a reward ingredient .",
    "consequently , the manager has to consider the trade - off between further monitoring @xmath49 in order to obtain a more accurate inference of @xmath30 , vis - a - vis making the decision to switch to other modes of production in case the state of the world is unfavorable .",
    "the contribution of this paper is fourfold .",
    "firstly , we are not aware of any papers dealing with optimal switching problems under partial information and we therefore think that our paper represent a substantial contribution to the literature devoted to optimal switching problems and to stochastic optimization problems under partial information . secondly , we propose a theoretically sound and entirely simulation - based approach to calculate the value function @xmath89 in when @xmath30 and @xmath49 satisfy the kalman - bucy setting of linear stochastic filtering .",
    "in particular , we propose a probabilistic numerical algorithm to approximate @xmath89 in based on dynamic programming and regression monte carlo methods .",
    "thirdly , we carry out a rigorous error analysis and prove the convergence of our scheme .",
    "fourthly , we illustrate some of the features of partial information in a computational example . it is known that in the linear kalman - bucy setting it is possible to solve the stochastic filtering problem analytically and describe the a posteriori probability distribution @xmath90 explicitly .",
    "although much of the analysis in this paper also holds in the non - linear case , we focus on the , already quite involved , linear setting . in general , numerical schemes for",
    "optimal switching problems , already under perfect information , seem to be a less developed area of research and we are only aware of the papers @xcite and @xcite where numerical schemes are defined and analyzed rigorously .",
    "our research is influenced by @xcite but our setting is different since we consider an optimal switching problem assuming only partial information .",
    "the paper is organized as follows .",
    "section 2 is of preliminary nature and we here state the assumptions on the systems in , , the payoff rate in production mode @xmath10 , @xmath91 , and the switching costs @xmath92 , assumptions used throughout the paper .",
    "section 3 is devoted to the general description of the stochastic filtering problem and the linear kalman - bucy filter . in section 4",
    "we prove that the value function @xmath93 in satisfies the dynamic programming principle .",
    "this is the result on which the numerical scheme , outlined in the subsequent sections , rests .",
    "section 5 gives , step by step , the details of the proposed numerical approximation scheme . in section [ sec : convanalysis ] we perform a rigorous mathematical convergence analysis of the proposed numerical approximation scheme and the main result , theorem [ thm : convergence ] , is stated and proved .",
    "we emphasize that by proving theorem [ thm : convergence ] we are able to establish a rigorous error control for the proposed numerical approximation scheme .",
    "section 7 contains a numerical illustration of our algorithm and the final section , section 8 , is devoted to a summary and conclusions .",
    "we first state the assumptions on the systems , , the payoff rate in production mode @xmath10 , @xmath91 , and the switching costs , @xmath92 , which will be used in this paper .",
    "we let @xmath94 denote the ( finite ) set of available states and we let @xmath95 for @xmath96 .",
    "as stated , the profit made ( per unit time ) in state @xmath10 is given by the function @xmath91 .",
    "the cost of switching from state @xmath10 to state @xmath14 is given by the function @xmath92 .",
    "focusing on the problem in , and in particular on the value function in , we need to give a precise definition of the strategy process @xmath97 and the notation @xmath98 .",
    "indeed , in our context a strategy @xmath26 , over a finite horizon @xmath23 $ ] , corresponds to a sequence @xmath99 , where @xmath16 is a sequence of @xmath100-adapted stopping times and @xmath18 is a sequence of measurable random variables taking values in @xmath101 and such that @xmath21 is @xmath102-adapted . given @xmath22 we let @xmath103 where @xmath104 is the indicator function for a measurable set @xmath105 , be the associated index process .",
    "in particular , to each strategy @xmath22 there is an associated index process @xmath25 and this is the process used in the definition of @xmath98 .",
    "we denote by @xmath106 the space of all real - valued functions @xmath107 such that @xmath108 and all its partial derivatives up to order @xmath109 are continuous and bounded on @xmath110 .",
    "given @xmath111 we let @xmath112 similarly , we denote by @xmath113)$ ] the space of all real - valued functions @xmath114",
    "\\to \\r$ ] such that @xmath108 , @xmath115 , @xmath116 , @xmath117 , and @xmath118 are continuous and bounded on @xmath119 $ ] . with a slight abuse of notation",
    "we will often write @xmath120 instead of @xmath121 .",
    "we denote by @xmath122 the space of of all positive measures on @xmath123 with finite mass . considering the systems in , , we assume that @xmath124 \\to \\r^{n_1}$ ] , @xmath125 \\to \\mathcal m_{n_1,m_1}$ ] and @xmath126 \\to \\r^{n_2}$ ] are continuous and bounded functions . here",
    "@xmath127 is the set of all @xmath128-dimensional real - valued matrices .",
    "furthermore , concerning the regularity of these functions we assume that @xmath129).\\end{aligned}\\ ] ] clearly implies that @xmath130 , @xmath131 , for all @xmath132 , and whenever @xmath133 $ ] . here",
    "@xmath134 is the standard euclidean norm of @xmath135 .",
    "given and , we see , using the standard existence theory for stochastic differential equations , that there exist unique strong solutions @xmath84 and @xmath63 to the systems in and . concerning regularity of the payoff functions @xmath136\\to \\r$ ] and",
    "the switching costs @xmath137\\to \\r$ ] , we assume that @xmath138)=c^{2,1}_b(\\r^{n_1 } \\times \\r^{n_2}\\times[0,t]).\\end{aligned}\\ ] ] for future reference we note , in particular , that @xmath139 for some constants @xmath140 , @xmath141 whenever @xmath142 $ ] . note that implies that @xmath143 and @xmath144 , @xmath145 , are , for @xmath45 fixed , lipschitz continuous w.r.t .",
    "@xmath46 , uniformly in @xmath146 , and vice versa . concerning the switching costs we also impose the following structural assumptions on the functions @xmath147 , @xmath148$},\\notag\\\\   ( iii)&&\\mbox{$c_{i_1,i_2}(x , y , t)+c_{i_2,i_3}(x , y , t)\\geq c_{i_1,i_3}(x , y , t)$ for all $ ( x , y , t)\\in\\r^{n_1 } \\times \\r^{n_2}\\times[0,t]$,}\\notag\\\\   & & \\mbox{and for any sequence of indices $ i_1 $ , $ i_2 $ , $ i_3 $ , $ i_j\\in\\{1,\\dots , d\\}$ for $ j\\in\\{1,2,3\\}$.}\\end{aligned}\\ ] ] note that @xmath149 states that it is always less expensive to switch directly from state @xmath10 to state @xmath109 compared to passing through an intermediate state @xmath14 .",
    "we emphasize that we are able to carry out most of the analysis in the paper assuming only . however , there is one instance where we currently need to impose stronger structural restrictions on the functions @xmath147 to pull the argument through . indeed",
    ", our final argument relies heavily on the lipschitz continuity of certain value functions , established in lemma [ lemma : lipschitzm ] and lemma [ lemma : lipschitzy ] below . currently , to prove these lemmas we need the extra assumption that @xmath150$}.\\end{aligned}\\ ] ] in particular , we need the switching cost to depend only on @xmath45 and the sole reason is that we need to be able to estimate terms of the type @xmath151 appearing in the proof of lemma [ lemma : lipschitzm ] ( lemma [ lemma : lipschitzy ] ) . while we strongly believe that these lemmas remain true without",
    ", we also believe that the proofs in the more general setting require more refined techniques beyond the dynamic programming principle , and that we have to resort to the connection to systems of variational inequalities with interconnected obstacles and reflected backward stochastic differential equations .",
    "as outlined in the introduction , the general goal of the filtering problem is to find the conditional distribution of the signal @xmath30 given the observation @xmath49 .",
    "in particular , given @xmath152 , @xmath153,\\ ] ] and the aim is to find the ( random ) measure @xmath69 .",
    "note that @xmath154 can be viewed as a stochastic process taking values in the infinite dimensional space of probability measures over the state space of the signal .",
    "let @xmath155 where @xmath156 is the transpose of @xmath65 , and let @xmath157 be the following partial differential operator @xmath158 using this notation and the assumptions stated in section [ sec : prel ] , one can show , e.g. , see @xcite , that the stochastic process @xmath159 satisfies @xmath160[dy_t^k-\\pi_t(h_k)dt],\\ ] ] for any @xmath152 . recall that @xmath161 is the function appearing in .",
    "the non - linear stochastic pde in is called the kushner - stratonovich equation .",
    "furthermore , it can also be shown , under certain conditions , that the kushner - stratonovich equation has , up to indistinguishability , a pathwise unique solution , e.g. , see @xcite . from here",
    "on in we will , to simplify the notation , write @xmath162 : = \\mathbb e \\left [ \\cdot \\ , \\vline \\ ,    \\pi_t= \\gamma , y_t = y    \\right ] . \\ ] ]      it is known that in some particular cases the filtering problem outlined above can be solved explicitly and hence the a posteriori distribution @xmath154 is known . in particular , if we assume that the signal @xmath30 and the observation @xmath49 solve linear sdes , then the solution to the filtering problem can be given explicitly . to be even more specific ,",
    "assume that the signal @xmath30 and the observation @xmath49 are given by the systems in , , with @xmath163 respectively , where @xmath164 \\to \\r^{n_1 \\times n_1}$ ] , @xmath165 \\to \\r^{n_1 \\times m_1}$ ] and @xmath166 \\to \\r^{n_2 \\times n_1}$ ] are measurable and locally bounded time - dependent functions .",
    "furthermore , assume that @xmath167 , where @xmath168 denotes the @xmath2-dimensional multivariate normal distribution defined by the vector of means @xmath169 and by the covariance matrix @xmath170 , is independent of the underlying brownian motions @xmath171 and @xmath172 .",
    "let @xmath173 $ ] and @xmath174 $ ] denote the conditional mean and the covariance matrix of @xmath84 , respectively .",
    "the following results concerning the filter @xmath154 and the processes @xmath175 and @xmath176 , can be found in , e.g. , @xcite or chapter 6 in @xcite .",
    "[ thm : multivariatenormal ] assume and that @xmath177 for some @xmath178 $ ]",
    ". then the conditional distribution @xmath154 of @xmath84 , conditional on @xmath71 , is a multivariate normal distribution , @xmath179 .",
    "[ thm : filter ] assume and that @xmath177 for some @xmath178 $ ] .",
    "then the conditional covariance matrix @xmath176 satisfies the deterministic matrix equation @xmath180$},\\end{aligned}\\ ] ] with initial condition @xmath181 , x_{t_0 } - \\e [ x_{t_0 } ] \\rangle { \\right}]$ ] , and the conditional mean @xmath175 satisfies the stochastic differential equation @xmath182$},\\end{aligned}\\ ] ] with initial condition @xmath183 $ ] .    for a positive semi - definite matrix @xmath184 ,",
    "let @xmath185 denote the unique positive semi - definite matrix @xmath186 such that @xmath187 , where , as above , @xmath188 denotes the transpose of @xmath186 . recalling that the density defining @xmath189 in @xmath190 , at @xmath191 , equals @xmath192 we see that the following result follows immediately from theorem [ thm : multivariatenormal ] and theorem [ thm : filter ] .",
    "[ cordir ] the distribution @xmath154 is fully characterized by @xmath175 and @xmath176 and @xmath193 for any @xmath194 .",
    "note that the covariance matrix @xmath176 is deterministic and depends only on the known quantities @xmath195 , @xmath196 , @xmath197 , see , and the distribution @xmath198 of the starting point of @xmath30 .",
    "hence , once the initial distribution @xmath199 is given , the covariance matrix @xmath176 can be determined for all @xmath200 $ ] .",
    "furthermore , in the kalman - bucy setting , the measure @xmath154 is gaussian and hence fully characterized by its mean @xmath175 and its covariance matrix @xmath176 . as a consequence , the value function to the partial information optimal switching problem , @xmath201 , can in this setting be seen as a function @xmath202 \\to \\r$ ] .",
    "we will , when @xmath90 is a gaussian measure with mean @xmath175 and covariance matrix @xmath176 , write @xmath203 : = \\mathbb e \\left [ \\cdot \\ , \\vline \\ ,",
    "m_t = m , \\theta_t=\\theta , y_t = y   \\right]\\ \\text{or}\\ \\mathbb e^{m , y , t}\\left   [ \\cdot \\right   ] : = \\mathbb e \\left [ \\cdot \\ , \\vline \\ ,    m_t= m , y_t = y\\right].\\ ] ]    [ remark : timeshift ] consider a fixed @xmath178 $ ] , and let @xmath204 , @xmath205 , @xmath206 , whenever @xmath207 $ ] .",
    "let @xmath208 and @xmath209 .",
    "let @xmath210 , with initial distribution determined by @xmath175 and @xmath176 , and @xmath211 be given as above for @xmath212 $ ] .",
    "furthermore , given @xmath213 and @xmath214 , let @xmath215 and @xmath216 be the unique solutions to the systems in and , with @xmath217 , @xmath65 , @xmath58 , defined as in but with @xmath218 replaced by @xmath219 and with initial data @xmath220 and @xmath221 .",
    "in addition , let @xmath222 be defined as in and , with @xmath223 and @xmath224 . finally , consider the value function @xmath225 and let @xmath226 be the value function of the optimal switching problem on @xmath227 $ ] , with @xmath228 replaced by @xmath229 . then @xmath230$}.\\ ] ]",
    "in particular , @xmath231 and we see that there is no loss of generality to assume that initial observations are made at @xmath232 .",
    "[ rmk : theta ] as the covariance matrix @xmath233 solves the deterministic riccati equation in , it is completely determined by the parameters of the model and the covariance matrix of @xmath84 at time @xmath232 .",
    "hence , once the initial condition @xmath170 is given , @xmath176 can be solved deterministically for all @xmath234 $ ] , and consequently viewed as a known parameter .",
    "therefore , we omit the dependence of @xmath233 in the value function @xmath235 and instead , with a slight abuse of notation , simply write @xmath236 .    [ remthetanud ] although is a deterministic ordinary differential equation , it may not be possible to solve it analytically .",
    "therefore , in a general numerical treatment of the problem outlined above one has to use numerical methods to find the covariance matrix @xmath237 .",
    "the error stemming from the numerical method used to solve will have influence on the total error , defined as the absolute value of the difference between the true value function @xmath236 and its numerical approximation derived in this paper .",
    "however , as @xmath238 is deterministic , it can be solved off - line and to arbitrary accuracy without effecting the computational efficiency of the main numerical scheme presented in this paper .",
    "therefore , we will throughout this paper consider @xmath238 as exactly known and ignore any error caused by the numerical algorithm used for solving .",
    "as mentioned in the introduction , the problem in can interpreted as a full information optimal switching problem with underlying process @xmath49 .",
    "we here expand on this interpretation in the context of kalman - bucy filters .",
    "let , using the notation in remark [ rmk : theta ] , @xmath239 whenever @xmath240 , and let @xmath241 be defined through @xmath242.\\end{aligned}\\ ] ] furthermore , let @xmath243 and @xmath244 be defined as @xmath245,\\notag\\\\ \\bar c_{\\xi_{k-1},\\xi_k}(m , y , t)&=&\\frac{1}{(2\\pi)^{n_1/2}}\\biggl [ \\int_{\\r^{n_1}}c_{\\xi_{k-1},\\xi_k}^z(m , y ,",
    "t)\\exp(- \\dfrac{|z|^2}{2 } ) d z\\biggr].\\end{aligned}\\ ] ] then , for @xmath240 fixed , @xmath241 is a solution to an optimal switching problem with perfect information . using the above notation",
    ", we see that @xmath246\\end{aligned}\\ ] ] and that the upper bound @xmath247 \\notag\\end{aligned}\\ ] ] holds .",
    "moreover , based on we see that also @xmath248 is a solution to an optimal switching problem with perfect information , with payoff rate in production mode @xmath10 , at time @xmath11 , defined by @xmath249 , and with switching cost , for switching from mode @xmath10 to mode @xmath14 at time @xmath11 , defined by @xmath250 .",
    "in this section we prove that the value function @xmath93 associated to our problem satisfies the dynamic programming principle ( dpp ) .",
    "this is the result on which the numerical scheme outlined in the next section rests .",
    "it should be noted that the dynamic programming principle holds for general systems as in and , systems which are not necessarily linear .",
    "[ thm : dpp ] let @xmath251 $ ] and let @xmath82 be defined as in . then @xmath252   \\end{aligned}\\ ] ] for all @xmath253-adapted stopping times @xmath254 , @xmath255 .",
    "let @xmath256 and @xmath257 be the unique solutions to the systems in , , with initial conditions @xmath83 and @xmath258 , respectively . note that these processes , as well as @xmath30 , are markov processes .",
    "hence , using the strong markov property of @xmath49 and @xmath90 we have that @xmath259 for any @xmath253-adapted stopping time @xmath260 $ ] and for all @xmath11 such that @xmath261 .",
    "let @xmath262\\ ] ] for @xmath263 .",
    "then , @xmath264 next , using and the law of iterated conditional expectations we see that @xmath265 \\notag \\\\ & \\leq \\e^{\\gamma , y , t } \\biggl [ \\int _ t ^\\tau \\pi_s { \\left } ( f_{\\mu_s}(\\cdot , y_s , s ) { \\right } ) ds - \\sum _ { t \\leq \\tau_n \\leq \\tau } \\pi_{\\tau_n } { \\left } ( c_{\\xi_{n-1},\\xi_n } ( \\cdot , y_{\\tau_n},\\tau_n ) { \\right})\\notag\\\\   & \\qquad\\qquad +   v_{\\mu_\\tau } ( \\pi_\\tau^{\\gamma , t},y_\\tau^{y , t } , \\tau )   \\biggr ] \\notag\\end{aligned}\\ ] ] for any @xmath253-adapted stopping time @xmath266 $ ] and any strategy @xmath267 . in particular , since @xmath26 is arbitrary in this deduction we see that @xmath268,\\end{aligned}\\ ] ] for any @xmath253-adapted stopping time @xmath266 $ ] . to complete the proof it remains to prove the opposite inequality , i.e. , to prove that @xmath269.\\end{aligned}\\ ] ] consider @xmath270 and let @xmath267 and @xmath266 $ ] , be a fixed strategy and a fixed @xmath253-adapted stopping time , respectively .",
    "recall that all stochastic processes are defined on the probability space @xmath57 . by the definition of @xmath93 there exists , for any @xmath271 and for any @xmath272 , @xmath273 , such that @xmath274 given @xmath26 , @xmath254 , @xmath275 , we define , for all @xmath272 , @xmath276 $ } \\\\",
    "\\mu^{\\epsilon}_s(\\omega ) \\hspace{1 cm } \\mbox { for $ s \\in [ \\tau(\\omega ) , t]$}. \\end{cases}\\ ] ] then @xmath277 and , again using the law of iterated conditional expectations , we obtain that @xmath278 .",
    "\\notag\\end{aligned}\\ ] ] finally , using and the above display we deduce that @xmath279 -{\\epsilon}.\\end{aligned}\\ ] ] since @xmath26 , @xmath254 and @xmath280 are arbitrary in the above argument we see that implies . combining and theorem",
    "[ thm : dpp ] follows .",
    "we note that the proof of the dpp here outlined follows the usual lines and perhaps a few additional statements concerning measurability issues could have been included .",
    "however , we here omit further details and refer to the vast literature on dynamic programming for exhaustive proofs of similar statements .",
    "[ discdpp ] in this section the dynamic programming principle is proven with the assumption that @xmath281 and @xmath282 are continuous in time . however , the approximation scheme introduced in the following section is based on ( euler ) discretized versions of these processes .",
    "we here just note that the proof above can be adjusted to also yield the dynamic programming principle in the context of the discretized processes .",
    "in this section we introduce a simulation based numerical scheme for determining @xmath82 as in and give a step by step presentation of the approximations defining the scheme . recall that @xmath82 is the optimal expected payoff , starting from state @xmath10 at time @xmath45 , with initial conditions @xmath283 and @xmath83 .",
    "we will from now on assume the dynamics of @xmath30 and @xmath49 are given by and , respectively , with assumption in effect .",
    "consequently , the results of section [ sec : kb ] are applicable . based on remark [ rmk : theta ] we in the following write @xmath284",
    "likewise , we will write @xmath285 $ ] instead of @xmath286 $ ] .",
    "furthermore , we can and will , w.l.o.g . , assume that the initial distribution @xmath79 is given at time @xmath232 and hence that the value function @xmath287 , at time @xmath45 , is a function of the conditional mean @xmath175 ( and the deterministic @xmath176 ) , based on the observation @xmath288 , see remark [ remark : timeshift ] . in other words , we assume that the distribution of @xmath30 is given at time @xmath232 and the task of the controller is to run the facility using updated beliefs of the conditional mean of the signal , conditional upon the information carried by the observation @xmath49 .    for the convenience of readers ,",
    "we in this section list the steps of the proposed numerical scheme and the associated notation . by theorem [ thm : multivariatenormal ]",
    "the a posteriori distribution @xmath90 is a gaussian measure , and in the following we denote by @xmath289 the gaussian measure with mean @xmath175 and covariance matrix @xmath176 .",
    "we emphasize that the outcome of the numerical scheme to be outlined , is an approximation of @xmath290 based on we emphasize that we consider the systems in and with initial data at @xmath232 , assuming the additional structure in . in particular , when considering the systems in and , for the calculation in and with data at @xmath232 , the initial conditions boil down , all in all , to the initial condition @xmath291 , at @xmath232 , for @xmath292 .    given @xmath293 fixed , and a large positive integer , @xmath294 , we let @xmath295 , @xmath296 . we let @xmath297 denote the naturally defined partition of the interval @xmath23 $ ] based on @xmath298 , i.e. , @xmath299 . throughout the paper",
    "any discretization of time will be identical to @xmath297 .",
    "the following steps constitute our numerical approximation scheme in the context of but starting at @xmath300 .    1 .   step 1  bermudan approximation .",
    "we restrict the manager to be allowed to switch only at the time points @xmath301 .",
    "this results in a bermudan approximation , @xmath302 , of @xmath303 .",
    "2 .   step 2  time discretization and euler discretization of @xmath175 and @xmath63 . @xmath175 and @xmath63 are replaced by corresponding discrete versions , also starting at @xmath304 at @xmath305 , based on the euler scheme and the partition @xmath297 .",
    "this results in an approximation , @xmath306 , of @xmath302 .",
    "step 3  space localization .",
    "the processes @xmath175 and @xmath63 are replaced by versions which are constrained to a bounded convex set @xmath307 .",
    "this gives an approximation @xmath308 , of @xmath309 .",
    "4 .   step 4  representation of conditional expectation using true regression . to calculate @xmath308 we use a regression type technique , replacing the future values by a ( true ) regression .",
    "this results in an approximation , @xmath310 , of @xmath308 .",
    "step 5  replacing the true regression by a sample mean . to calculate @xmath310 we replace the coefficients in the true regression by their corresponding sample means .",
    "this results in an approximation , @xmath311 , of @xmath310 .",
    "the final value produced by the algorithm is @xmath312 and this is an approximation of the true value @xmath303 .",
    "in the remaining part of this section we will discuss step  1  step  5 in more detail .",
    "the rigorous error analysis is postponed to section [ sec : convanalysis ] .",
    "let @xmath313 be the set of strategies @xmath314 such that @xmath315 $ ] for all @xmath316 .",
    "based on @xmath313 we let @xmath317 \\end{split}\\ ] ] and we refer to @xmath318 as the value function of the bermudan version of our optimal switching problem under partial information .",
    "the difference @xmath319 is quantified in proposition [ lemma : bermudan ] . + for future reference we here also introduce what we call the _",
    "bermudan strategy_.    [ def : approximatingstrategy ] let @xmath320 and let @xmath297 be given .",
    "let @xmath321 be the strategy in @xmath313 defined by @xmath322 then , @xmath323 is the _ bermudan strategy _ associated to @xmath26 and @xmath324 .",
    "given the continuous time @xmath45 we let @xmath326 . in this step",
    "we replace the continuous processes @xmath175 and @xmath63 with their corresponding discrete euler approximations . to be specific ,",
    "we first calculate ( pathwise ) the euler approximation of the signal @xmath30 , denoted by @xmath327 and given by the dynamics @xmath328 based on @xmath327 we then introduce the discrete processes @xmath329 and @xmath330 , euler approximations of @xmath49 and @xmath325 , respectively , and given by @xmath331 and @xmath332 based on this we have @xmath333 for any @xmath194 . recall that we consider @xmath238 as completely known , see remark [ remthetanud ] , and hence @xmath238 is not subject to discretization . based on the above processes we let @xmath334 \\notag\\\\ = & \\sup _ { \\mu \\in \\a^{y,\\pi}_{t_k , i } } \\e^ { m_{t_k},y_{t_k},t_k } \\biggl [ \\delta \\sum_{\\ell = k}^n   \\pi_{t_\\ell}^{\\bar m_{t_{\\ell } } } { \\left } ( f_{\\mu_{t_\\ell}}(\\cdot , \\bar y_{t_\\ell},t_\\ell ) { \\right } ) \\notag \\\\   & \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad- \\sum _ { t_k \\leq \\tau_n \\leq t } \\pi_{\\tau_{n}}^ { \\bar m_{\\tau_{n } } } { \\left}(c_{\\xi_{n-1},\\xi_{{n}}}(\\cdot,\\bar y_{\\tau_{n } } , \\tau_{n } ) { \\right})\\biggr].\\end{aligned}\\ ] ] @xmath335 is the value function based on the discretized time @xmath336 and the euler approximations @xmath337 and @xmath338 .",
    "the difference between @xmath339 and @xmath306 is quantified in proposition [ lemma : discretization ] .",
    "+ in the following we will , in an attempt to slightly ease the notation , omit the bar indicating euler discretization of @xmath325 and @xmath49 and simply write @xmath340 instead of @xmath341 when we believe there is no risk of confusion . in particular , we let the very notation @xmath335 also symbolize that @xmath325 and @xmath49 are discretized as above .      a localization in space will be necessary when estimating the errors induced by step @xmath342 and step @xmath343 below and we here describe this localization . in particular , to be able to work in bounded space time domains we consider , for a fixed parameter @xmath344 , ( time dependent ) domains @xmath345}\\subset \\r^{n_1 } \\times \\r^{n_2}$ ] such that @xmath346 is a convex domain for all @xmath36 $ ] .",
    "we assume that there exists a constant @xmath347 , depending only on @xmath45 and @xmath280 , such that @xmath348}$ ] , where @xmath349}$ ] is the hypercube in @xmath350 with sides of length @xmath347 .",
    "let @xmath351 be the projection of a generic stochastic process @xmath352 onto the domain @xmath353 , i.e. , @xmath354 if @xmath355 , and @xmath351 is the , by convexity of @xmath346 , naturally defined unique projection of @xmath352 onto @xmath353 , along the normal direction , otherwise . following @xcite",
    ", we assume that @xmath353 can be chosen such that @xmath356 \\leq { \\epsilon}\\mbox { for all $ t \\in [ 0,t]$}.\\ ] ] the space @xmath345}$ ] can be seen as the domain in which the @xmath357-dimensional process @xmath358 lives . roughly speaking ,",
    "condition states that most of the time the @xmath357-dimensional process @xmath359 will be found inside this domain .",
    "as mentioned , this localization in space will be necessary when estimating the errors induced by step @xmath342 and step @xmath343 .",
    "in particular , based on we will in these steps be able to reuse results on the full information optimal switching problem developed in @xcite .",
    "we therefore refer to @xcite for more details on the assumption in and a constructive example .",
    "the construction above stresses generalities but , although not necessary , we will in the follwoing assume , to be consistent and to minimize notation , that @xmath360 $ , for some $ d^{\\epsilon}\\subset \\r^{n_1 } \\times \\r^{n_2}$ , and that}\\notag\\\\ & & \\mbox{$d^{\\epsilon}= q^m_{[-c_m^{\\epsilon},c_m^{\\epsilon}]}\\times q^y_{[-c_y^{\\epsilon},c_y^{\\epsilon}]}$ for some constants $ c_m^{\\epsilon}$ and $ c_y^{\\epsilon}$.}\\end{aligned}\\ ] ] in other words , @xmath307 is assumed to be a time - independent cartesian product of hypercubes .",
    "the result of this step is that @xmath337 and @xmath338 are replaced by their corresponding projected versions , @xmath361 and @xmath362 , respectively .",
    "we let @xmath363 denote the associated value function when @xmath337 and @xmath338 are replaced by @xmath361 and @xmath362 .",
    "in particular , when writing @xmath363 , @xmath280 also indicates that the underlying dynamics is that of @xmath361 and @xmath362 . the error introduced by considering @xmath361 and @xmath362 , i.e.",
    ", the difference between @xmath309 and @xmath308 , is quantified in proposition [ lemma : projected ] .",
    "note that for the discretized bermudan version of the optimal switching problem , the value function can , for any @xmath364 , be simplified to read @xmath365   , \\end{aligned}\\ ] ] where on the right hand side , consequently , @xmath366 , @xmath367 . as a result ,",
    "the dpp for in the discretized bermudan setting , see remark [ discdpp ] , reduces to @xmath368 , \\notag   \\\\ & \\qquad\\max _ { j \\in \\q^{-i } }   { \\left}\\ { v_j^{\\check\\pi,{\\epsilon}}(m_{t_k},y_{t_k},t_k ) - \\pi^{m_{t_{k}}}_{t_k}(c_{i , j}(\\cdot , y_{t_k},t_k ) ) { \\right}\\ }    \\bigg \\}. \\notag\\end{aligned}\\ ] ] taking @xmath369 and @xmath149 into account this can be further simplified to @xmath370\\notag\\\\ & -   \\pi^{m_{t_{k}}}_{t_k}(c_{i , j}(\\cdot , y_{t_k},t_k ) ) \\bigg \\}.\\end{aligned}\\ ] ] based on the recursive scheme based on the dpp becomes @xmath371\\notag\\\\ & \\qquad\\qquad-   \\pi^{m_{t_{k}}}_{t_k}(c_{i , j}(\\cdot , y_{t_k},t_k ) )   \\bigg \\}.\\end{aligned}\\ ] ] an important feature of the scheme in is that @xmath372 depends explicitly on the value functions at time @xmath373 , @xmath374 .",
    "these functions are unknown at time @xmath300 .",
    "furthermore , the optimal strategy at time @xmath300 also depends directly on this future value .",
    "indeed , at time @xmath300 it is optimal to switch from state @xmath10 to @xmath14 if the difference between the expected future value retrieved from being in mode @xmath14 and the switching cost @xmath92 , is greater than the expected profit made from staying in state @xmath10 .",
    "more precisely , at time @xmath300",
    "it is optimal to switch from state @xmath10 to @xmath14 if @xmath375\\notag\\\\ & \\qquad\\quad-\\pi^{m_{t_{k}}}_{t_k}(c_{i , j}(\\cdot , y_{t_k},t_k))\\biggr\\ } \\notag \\\\ & > \\delta\\pi^{m_{t_{k}}}_{t_k}(f_{i}(\\cdot , y_{t_k},t_k ) ) + \\e^{m_{t_k},y_{t_k},t_k } { \\left } [   v_i^{\\check \\pi}(m_{t_{k+1}},y_{t_{k+1}},t_{k+1 } ) { \\right}].\\end{aligned}\\ ] ] if this inequality holds with @xmath376 replaced by @xmath377 it is optimal to stay in state @xmath10 . to construct an @xmath71-adapted strategy",
    "it is hence necessary to estimate the future expected value at time @xmath373 , based on the information available at time @xmath300 , i.e. , to estimate @xmath378.\\end{aligned}\\ ] ] since we , by assumption and through the space localization in step 3 , consider continuous pay - off functions and switching costs on bounded domains , as well as a finite horizon problem , there exist lower and upper bounds for .",
    "however , a sound way of finding an approximation of the conditional expectation in is needed , and this approximation is the focus of this step of the numerical scheme proposed . to perform an approximation of the conditional expectation in ,",
    "we make use of an empirical least square regression model based on simulation . in particular , we consider a test function @xmath379 and the function @xmath380.\\ ] ] we will use a least square regression onto a set of @xmath186 preselected basis functions , @xmath381 , to create an estimator @xmath382 of @xmath383 . to elaborate on this ,",
    "assume that we are given @xmath186 basis functions @xmath384 .",
    "given the test function @xmath379 , we define @xmath385 as @xmath386,\\ ] ] and set @xmath387 recall that in our numerical scheme , @xmath388 and @xmath389 . based on this we define a new set of functions @xmath390 through the recursive scheme @xmath391 in particular , we obtain a new approximation @xmath392 of the true value function , defined through the recursive scheme in .",
    "the error @xmath393 is analyzed in proposition [ prop : regression ] .    in the above construction , the set of basis functions used is arbitrary and several options are possible .",
    "furthermore , it is possible to choose a different set of basis functions for each time @xmath300 without adding difficulties beyond additional notation .",
    "also , as pointed out in @xcite , one can use stochastic partitions of the domain @xmath394 to enable the use of adaptive partitioning methods , possibly increasing the convergence speed of the numerical scheme .      given a test function @xmath379 , in numerical calculations the true regression parameters @xmath395 will not be known , and they have to be replaced by a sample mean @xmath396 based on simulations . recall that in the context of we in the end want to approximate @xmath397 for @xmath398 given and fixed .",
    "in particular , this means that in the original model for the process @xmath359 we consider the initial condition @xmath304 , at @xmath232 .",
    "this also implies that the approximations of @xmath359 introduced above , @xmath399 and @xmath400 , will also start at @xmath304 at @xmath232 . to outline the estimation of the true regression parameters we let",
    "@xmath401 , @xmath402 , denote @xmath403 simulated trajectories of the observed process @xmath63 , starting at @xmath146 at @xmath232 , and we use these to calculate the corresponding values of @xmath404 , @xmath405 as outlined in section [ sec : kb ] and subsection [ subsec : eulerdisc ] , with initial condition @xmath406 for all @xmath407 .",
    "based on the paths @xmath401 and @xmath404 , @xmath405 , we now compute the empirical vector @xmath408 , estimating @xmath409 as @xmath410 given @xmath379 , we in this way fix @xmath411 for @xmath412 . in the following",
    "we indicate that the estimation of @xmath411 is based on @xmath403 sample paths by writing @xmath413 instead of @xmath411 .",
    "next , using @xmath414 we set @xmath415 whenever @xmath416 .",
    "in particular , while the coefficients @xmath417 are estimated based on a finite set of sample paths @xmath401 , @xmath404 , @xmath405 , we use these coefficients in to construct an estimator for the conditional expectation @xmath418\\ ] ] for all @xmath416 . based on @xmath419",
    "we let @xmath420 be defined through the recursive scheme @xmath421 then @xmath422 is an approximation of @xmath310 and , since @xmath423 , @xmath424 is an approximation of @xmath425 .",
    "hence , the final value produced by the algorithm , @xmath426 , is an approximation of the true value @xmath427 .    following @xcite , we will in this paper use indicator functions on hypercubes as basis functions , @xmath428 , for our regression and subsequent error analysis .",
    "these hypercubes are defined in relation to the space ( time ) localization domain @xmath429}$ ] . here",
    "we briefly outline the idea of using such a basis for regression but we also refer to @xcite . recall that @xmath307 is assumed to have the structure specified in .",
    "we let @xmath430 be a partition of the bounded domain @xmath307 into @xmath186 hypercubes , i.e. , we split @xmath307 in to @xmath186 open hypercubes @xmath431 such that @xmath432 and @xmath433 if @xmath434 . furthermore , to achieve notational simplicity , we in the following also assume that each hypercube has side length @xmath435 in each dimension . using @xmath430",
    "we define basis functions to be used in the regression as @xmath436 for @xmath437 , and @xmath438 . by definition @xmath439",
    "if @xmath440 and @xmath441 otherwise . note that we use the same symbol @xmath431 to denote both the @xmath442-th hypercube and its corresponding basis function / indicator function . since",
    "conditional expectation is mean - square error - minimizing , this choice of basis functions reduces the vectors and to @xmath443\\notag\\\\ & =    \\frac{\\e { \\left}[\\hat v_i^{\\check\\pi,{\\epsilon } } ( m_{t_{k+1}},y_{t_{k+1}},t_{k+1 } ) \\i_{\\{(m_{t_k},y_{t_k } ) \\in b_r \\}}{\\right}]}{\\p((m_{t_k},y_{t_k } ) \\in b_r)},\\end{aligned}\\ ] ] and @xmath444}{\\frac{1}{m } \\sum_{\\ell=1}^m \\i_{\\{(m^\\ell_{t_k},y^\\ell_{t_k } ) \\in b_r \\ } } } \\notag\\end{aligned}\\ ] ] with the convention that @xmath445 if , at time @xmath300 , no path @xmath446 lies inside the hypercube @xmath431 .",
    "hence , in our numerical scheme , if @xmath447 the expected future value @xmath448\\ ( i \\in \\q)\\ ] ] will be approximated by @xmath449 .",
    "the error @xmath450 is estimated in proposition [ lemma : truetosample ] .",
    "in this section we establish the convergence of the numerical scheme outlined in the previous section by proving theorem [ thm : convergence ] stated below . however , we first recall degrees of freedom , at our disposal , in the numerical scheme proposed .    * @xmath295  the time discretization parameter , * @xmath280  the error tolerance when choosing the bounded convex domains @xmath353 for the projection , * @xmath435  the edge size of the hypercubes used in the regression , * @xmath403  the number of simulated trajectories of @xmath451 and @xmath452 , used in .",
    "recall that @xmath57 is the underlying probability space and in the following @xmath453 with norm @xmath454 .",
    "we prove the following convergence theorem .",
    "[ thm : convergence ] assume that @xmath455 and that all assumptions and conditions stated and used in the previous sections are fulfilled .",
    "then there exist a constant @xmath456 , independent of @xmath457 , @xmath458 , @xmath435 and @xmath403 , and a constant @xmath459 , independent of @xmath457 , @xmath435 and @xmath403 , such that @xmath460 where @xmath461}\\min_{b_r\\subset d^{\\epsilon}}\\mathbb p[(m_{t},y_t)\\in b_r ] \\ ] ] is a strictly positive quantity .",
    "in particular , if @xmath462 , @xmath463 , @xmath464 and @xmath465 such that @xmath466 then @xmath467    we first note , using the notation introduced in the previous section , that @xmath468 where @xmath469 in the subsequent subsections we prove that the errors @xmath470 can be controlled and we ending the section with a summary proving theorem [ thm : convergence ] .      before deducing the relevant error estimates ,",
    "i.e. , quantifying @xmath471 to @xmath472 , we here state and prove some auxiliary results , lemma [ prop : finitenumberofswitches ]  lemma [ lemma : lipschitzy ] , which will be used in the subsequent proofs .",
    "[ prop : finitenumberofswitches ] assume , , , and let @xmath27 be such that @xmath473 let @xmath474",
    ". then @xmath475 where @xmath476 is the positive constant appearing in @xmath477 . in particular ,",
    "the number of switches in an optimal strategy @xmath88 is finite .",
    "let @xmath478 denote the trivial strategy , i.e. , no switches . then , using , and",
    ", we see that @xmath479 now , let @xmath480 be a strategy with an unbounded number of switches .",
    "then , by @xmath369 and @xmath477 , it follows that @xmath481 .",
    "hence , the optimal strategy @xmath88 must consist of a finite number of switches .",
    "finally , let @xmath482 be an arbitrary strategy with a finite number of switches and assume that @xmath483 . then , using @xmath369 and @xmath477 we see that @xmath484 in particular , @xmath485 .",
    "[ prop : bmlpnorm ] let @xmath486 be a @xmath0-dimensional brownian motion defined on a filtered probability space @xmath487 .",
    "then , for any @xmath488 there exists a finite constant @xmath489 such that @xmath490   \\leq c_p { \\left } ( \\delta \\log(\\frac { 2t}{\\delta}){\\right})^{p/2}.\\ ] ]    this is theorem 1 in @xcite .",
    "[ lemma : lipschitzm ] assume , , and .",
    "then , there exists a constant @xmath491 , independent of @xmath457 , such that @xmath492 .",
    "we claim that there exist positive constants @xmath184 and @xmath493 , independent of @xmath457 , such that the following holds .",
    "there exists a sequence of constants @xmath494 such that @xmath495 and such that @xmath496 to prove this we proceed by ( backward ) induction on @xmath109 and we let @xmath497 if holds with a constant @xmath498 whenever @xmath499 , and if @xmath498 is related to the bounds @xmath500 as stated in . we want to prove that @xmath497 whenever @xmath501 .",
    "since @xmath502 for all @xmath325 , we immediately see that @xmath503 . assuming that @xmath504 for some @xmath505 $ ] we next prove that @xmath497 by constructing @xmath498 . to do this we first note , simply by the ( discrete ) dpp , that @xmath506\\notag\\\\    & \\qquad\\qquad- \\pi_{t_k}^{m_1 } { \\left } ( c_{i , j}(\\cdot , y_{t_k},t_k ) { \\right } )   \\notag \\\\ & +   ( 1 - 1)\\biggl [ [ \\delta \\pi_{t_k}^{m_2 } { \\left}(f_j(\\cdot , y_{t_k},t_k ) { \\right } ) + \\e^{m_2,y_{t_k},t_k } { \\left } [ v_j^{\\check \\pi,{\\epsilon}}(m_{t_{k+1}},y_{t_{k+1}},t_{k+1 } )",
    "{ \\right}]\\notag\\\\ & \\qquad\\qquad - \\pi_{t_k}^{m_2 } { \\left } ( c_{i , j}(\\cdot , y_{t_k},t_k ) { \\right } ) \\biggr ]   \\bigg \\}. \\notag\\end{aligned}\\ ] ] furthermore , by elementary manipulations the above can be rewritten as @xmath507\\notag\\\\   & \\qquad\\qquad - \\pi_{t_k}^{m_2 } { \\left } ( c_{i , j}(\\cdot , y_{t_k},t_k ) { \\right } ) \\notag \\\\ & \\qquad\\qquad+ \\delta { \\left}(\\pi_{t_k}^{m_1 } { \\left}(f_j(\\cdot , y_{t_k},t_k ) { \\right } ) - \\pi_{t_k}^{m_2 } { \\left}(f_j(\\cdot , y_{t_k},t_k ) { \\right } ) { \\right})\\notag \\\\ & \\qquad\\qquad+ \\e^ { m_1,y_{t_k},t_k } { \\left}[v_j^{\\check \\pi,{\\epsilon}}(m_{t_{k+1}},y_{t_{k+1}},t_{k+1 } )   { \\right}]\\notag\\\\   & \\qquad\\qquad-\\e^{m_2,y_{t_k},t_k } { \\left } [ v_j^{\\check \\pi,{\\epsilon}}(m_{t_{k+1}},y_{t_{k+1}},t_{k+1 } )   { \\right } ]   \\notag \\\\ & \\qquad\\qquad- { \\left } ( \\pi_{t_k}^{m_1 } { \\left } ( c_{i , j}(\\cdot , y_{t_k},t_k ) { \\right } )   - \\pi_{t_k}^{m_2 } { \\left } ( c_{i , j}(\\cdot , y_{t_k},t_k ) { \\right } ) { \\right } )   \\bigg \\}. \\notag\\end{aligned}\\ ] ] in particular , @xmath508 where @xmath509\\notag\\\\   & \\qquad\\qquad-\\e^{m_2,y_{t_k},t_k } { \\left } [ v_j^{\\check \\pi,{\\epsilon}}(m_{t_{k+1}},y_{t_{k+1}},t_{k+1 } )   { \\right } ] \\biggr|\\biggr\\ } , \\notag \\\\",
    "a_3 = &   \\max_{j \\in \\q } \\bigg \\{\\biggl |\\pi_{t_k}^{m_2 } { \\left } ( c_{i , j}(\\cdot , y_{t_k},t_k ) { \\right } ) - \\pi_{t_k}^{m_1 } { \\left } ( c_{i , j}(\\cdot , y_{t_k},t_k ) { \\right})\\biggr|\\biggr\\}. \\notag\\end{aligned}\\ ] ] we now need to bound the terms @xmath510 and @xmath151 with @xmath511 .",
    "we first treat the term @xmath512 using the lipschitz property of @xmath513 .",
    "indeed , recalling corollary [ cordir ] we first have @xmath514 hence , by the lipschitz property of @xmath513 we see that @xmath515 where @xmath516 is simply the lipschitz constant of @xmath513 . in particular , @xmath517 to treat the term @xmath518 we first note that @xmath519-\\e^{m_2,y_{t_k},t_k } { \\left } [ v_j^{\\check \\pi,{\\epsilon}}(m_{t_{k+1}},y_{t_{k+1}},t_{k+1 } )   { \\right } ] \\biggr|^2\\notag\\\\ & & \\leq \\biggl\\|v_j^{\\check \\pi,{\\epsilon}}(m_{t_{k+1}}^ { m_1,t_k},y_{t_{k+1}}^{y_{t_k},t_k},t_{k+1})-v_j^{\\check \\pi,{\\epsilon}}(m_{t_{k+1}}^{m_2,t_k},y_{t_{k+1}}^{y_{t_k},t_k},t_{k+1 } ) \\biggr\\|_{l^2}^2\\end{aligned}\\ ] ] where @xmath520 and @xmath521 indicate that at @xmath300 the processes @xmath325 and @xmath49 are starting at @xmath522 and @xmath523 , respectively .",
    "now , using the induction hypothesis @xmath504 , i.e. , the lipschitz property of @xmath524 and @xmath525 , we can conclude that @xmath526 next , using , the equation for @xmath175 and elementary estimates , e.g. , see the proof of lemma @xmath527 in @xcite , we can conclude that @xmath528 for some constant @xmath493 independent of @xmath457 , @xmath7 and @xmath55 . finally , using the assumption in",
    "we see that @xmath529 .",
    "putting the estimates together we can conclude that @xmath530 where we have chosen @xmath531 in the definition of @xmath498 .",
    "in particular , if @xmath504 for some @xmath532 , then @xmath497 and hence the lemma follows by induction .",
    "[ lemma : lipschitzy ] assume , and .",
    "then , there exists a constant @xmath533 , independent of @xmath457 , such that @xmath534 .",
    "the proof of the lemma is analogous to the proof of lemma [ lemma : lipschitzm ] and we here omit further details .",
    "we emphasize that in this paper the structural assumption is only used in the proof of lemma [ lemma : lipschitzm ] and lemma [ lemma : lipschitzy ] .",
    "these lemmas are then only used in the proof of proposition [ prop : regression ] stated below . in particular ,",
    "if lemma [ lemma : lipschitzm ] and lemma [ lemma : lipschitzy ] can be proved without assuming all of the remaining arguments go through unchanged .",
    "[ lemma : bermudan]there exist positive constants @xmath535 and @xmath536 , independent of @xmath457 , such that @xmath537 whenever @xmath538 .",
    "recall that @xmath539 , \\notag\\end{aligned}\\ ] ] and @xmath540 .",
    "\\end{aligned}\\ ] ] using lemma [ prop : finitenumberofswitches ] we may assume that @xmath541 .",
    "given @xmath320 we consider the strategy @xmath321 , where @xmath542 then @xmath543 and it is the bermudan strategy associated to @xmath544 @xmath543 , see definition [ def : approximatingstrategy ] .",
    "using this and we see that @xmath545 . \\notag\\end{aligned}\\ ] ] furthermore , since @xmath546 we also have that @xmath547 . putting these estimates together",
    "we can conclude that @xmath548 where @xmath549 ,   \\notag \\\\",
    "e_{12}^{m_{t_k},y_{t_k},t_k}(\\mu ) & = \\e^{m_{t_k},y_{t_k},t_k } \\biggl [ \\sum_{t_k \\leq \\tau_n \\leq t}\\biggl |\\pi^{m_{\\tau_{n}}}_{\\tau_{n } } { \\left}(c_{\\xi_{n-1},\\xi_{n}}(\\cdot , y_{\\tau_{n}},\\tau_{n } ) { \\right})\\notag\\\\   & \\qquad\\qquad\\qquad\\qquad\\qquad-\\pi^{m_{\\tilde \\t_{n}}}_{\\tilde \\tau_{n } } { \\left}(c_{\\tilde \\xi_{n-1},\\tilde \\xi_{n}}(\\cdot , y_{\\tilde \\tau_{n}},\\tilde",
    "\\tau_{n } ) { \\right } ) \\biggr | \\biggr]\\notag.\\end{aligned}\\ ] ] since , by assumption , @xmath91 is bounded we immediately see that @xmath550 \\right).\\ ] ] furthermore , using lemma [ prop : finitenumberofswitches ] we see that @xmath551 \\leq \\delta",
    "\\e ^{m_{t_k},y_{t_k},t_k } [ n(\\mu )   ] \\leq \\biggl [ \\frac { 2t\\sup_i \\|f_i\\|_\\infty}{\\nu}\\biggr ] \\delta.\\ ] ] putting these estimates together we can conclude that @xmath552   \\delta\\ ] ] and this gives the appropriate bound on @xmath553 . to estimate @xmath554",
    "we first note that @xmath555 where @xmath556,\\notag \\\\ e_{122}^{m_{t_k},y_{t_k},t_k}(\\mu)&=\\e ^{m_{t_k},y_{t_k},t_k } \\biggl [ \\sum_{t_k \\leq \\tau_n \\leq t}\\biggl |\\pi^{m_{\\tau_{n}}}_{\\tau_{n}}{\\left}(c_{\\xi_{n-1},\\xi_{n}}(\\cdot , y_{\\tilde \\tau_{n}},\\tilde \\tau_{n}){\\right})\\notag\\\\   & \\qquad\\qquad\\qquad\\qquad\\qquad-\\pi^{m_{\\tilde \\tau_{n}}}_{\\tilde \\tau_{n}}{\\left}(c_{\\tilde \\xi_{n-1},\\tilde \\xi_{n}}(\\cdot , y_{\\tilde \\tau_{n}},\\tilde \\tau_{n}){\\right } ) \\biggr| \\biggr ] \\notag.\\end{aligned}\\ ] ] by using that @xmath557 by construction , @xmath477 , and that @xmath558 is @xmath559-adapted , we see that there exists a constant @xmath560 such that @xmath561\\notag \\\\ & \\leq c \\e ^{m_{t_k},y_{t_k},t_k } { \\left } [   n(\\mu)\\left ( \\delta +    \\sup \\limits_{\\substack{0\\leq s , u \\leq t \\\\ |s - u|\\leq \\delta } } |y_u - y_s|     \\right ) { \\right}].\\end{aligned}\\ ] ] we now recall that @xmath562 where @xmath563 is a standard @xmath55-dimensional brownian motion .",
    "hence , latexmath:[\\ ] ] hence , using this , and the induction hypothesis @xmath504 , we can conclude that @xmath640 by symmetry the same inequality holds for @xmath641 and thus @xmath642 with @xmath620 defined as @xmath643 in particular , and hold for @xmath109 and we have proved that if @xmath504 for some @xmath644 , then also @xmath497 .",
    "hence @xmath497 for all @xmath501 by induction . based on and we complete the proof of the proposition by observing that , for any @xmath644 , @xmath645      [ lemma : truetosample ] there exist a constant @xmath646 , independent of @xmath457 , @xmath435 , @xmath280 and @xmath403 , and a constant @xmath647 , independent of @xmath457 , @xmath435 , and @xmath403 , such that @xmath648 whenever @xmath538 . here",
    "d^{\\epsilon}}\\mathbb p\\biggl [ ( m_{t},y_t)\\in b_r\\biggr ] \\ ] ] is a strictly positive quantity .",
    "@xmath649 is strictly positive quantity due to the fact that the domain @xmath307 is bounded . to prove the proposition we claim that there exist a positive constant @xmath184 , independent of @xmath457 , @xmath435 , @xmath280 and @xmath403 , and a constant @xmath650 , independent of @xmath457 , @xmath435 , and @xmath403 , such that the following holds .",
    "there exists a sequence of constants @xmath651 such that @xmath652 whenever @xmath499 and such that @xmath653 we prove , by ( backward ) induction on @xmath109 and we say that @xmath497 if holds with a constant @xmath654 whenever @xmath538 , and if @xmath654 is related to the bounds @xmath655 ,  , @xmath656 , as stated in .",
    "we want to prove that @xmath497 whenever @xmath501 .",
    "again , we immediately see that @xmath503 .",
    "assuming that @xmath504 for some @xmath657 we next prove that @xmath497 by constructing @xmath654 .",
    "note that @xmath184 is a degree of freedom appearing in the following argument and the important thing is , in particular , that @xmath184 does not depend on @xmath109 . to start the proof , given @xmath658 we in the following let @xmath659 and @xmath660 be such that ( see and ) @xmath661 note that @xmath659 and @xmath660 are not neccessarily equal , at time @xmath300 , for @xmath662 .",
    "hence , by proceeding as in the proof of proposition [ prop : regression ] , we see that @xmath663 in particular , by symmetry we can use this inequality to conclude that @xmath664 for all @xmath658 , where @xmath665 in particular , @xmath666 and , hence , we need to find proper bounds for @xmath667 and @xmath668 .",
    "firstly , to bound @xmath667 we can simply apply lemma 3.6 of @xcite to conclude that @xmath669 where @xmath670 is independent of @xmath457 , @xmath435 , @xmath280 and @xmath403 , while @xmath671 is independent of @xmath457 , @xmath435 , and @xmath403 .",
    "furthermore , @xmath670 and @xmath671 are independent of @xmath109 .",
    "secondly , to bound @xmath668 we first note , by definition , that @xmath672 where @xmath673 hence , using the induction hypothesis @xmath504 we see that @xmath674 we now let @xmath675 , @xmath676 and define @xmath677 then , using , and we conclude that @xmath678 whenever @xmath538 .",
    "in particular , and hold for @xmath109 and we have proved that if @xmath504 for some @xmath644 , then also @xmath497 .",
    "hence @xmath497 for all @xmath501 by induction . based on and we complete the proof of proposition [ lemma : truetosample ] by observing that , for any @xmath644 , @xmath679    as can be seen from propositions [ lemma : bermudan ]  [ prop : regression ] ,",
    "errors @xmath471 to @xmath612 are obtained in the pathwise sense , because these approximations are constructed for each path of @xmath359 .",
    "however , recalling subsection [ sec.sample_mean_approx ] , at time @xmath300 , the @xmath680-adapted @xmath626 , defined in , is approximated by @xmath681 as in .",
    "the corresponding regression coefficient @xmath682 is a monte carlo type approximation of @xmath683 constructed by using @xmath403 independent trajectories @xmath684 of the process @xmath685 .",
    "therefore , instead of in the pathwise sense , @xmath472 is controlled in the associated @xmath686-norm , at each @xmath300 .",
    "in addition , since the sample mean approximations are done independently of each other , and backwards in time , the error at time @xmath300 is controlled by the sum of errors at later time steps .",
    "this is the statement of proposition [ lemma : truetosample ] .      combining the error bounds obtained in propositions [ lemma : bermudan ]  [ lemma : truetosample ] with @xmath687 proves theorem [ thm : convergence ] .",
    "in this section we present a simplistic numerical example showing some of the features stemming from our set up with only partial information . in particular , we consider our optimal switching problem under partial information with two modes , @xmath688 , and we use the numerical scheme proposed and detailed in the previous sections to estimate @xmath689 . to simplify the presentation ,",
    "we will only focus on the value function starting in mode @xmath690 , i.e. , we will here only estimate @xmath691 .",
    "we will in this section slightly abuse notation and let @xmath692 denote both the true value function @xmath692 as well as its approximation @xmath693",
    ". the interpretation should be clear from the context .",
    "we consider constants parameters and , if nothing else is specified , the parameters laid forth in table [ tab : parameters ] are the ones used in the simulations .",
    ".parameter values [ cols=\"<,<,>,<,<,>\",options=\"header \" , ]      we now turn to study the dependence of the value function @xmath692 on the amount of available information , the latter being determined through the function @xmath196 defining @xmath58 , see .",
    "larger @xmath694 means more information while smaller @xmath694 means less information .",
    "as shown by figures [ fig : mu0runs ] , [ fig : surfaceplots ] and [ fig : crosssections ] ( a ) , the value function increases as the amount of available information increases .",
    "the reason for this behaviour is intuitively clear . when the amount of information increases , it becomes easier for the controller to make the correct decision concerning when to open / close the facility , increasing the possible output of the facility . for comparison , consider the case with no switching costs , i.e. , @xmath695 .",
    "it is intuitively clear that the optimal strategy is then to open the facility as soon as the underlying @xmath84 is positive and to close as soon as it becomes negative .",
    "if the controller has full information , he knows exactly when the signal switches sign and can thus implement this strategy .",
    "however , when the observation of the signal @xmath84 is noisy , the possibility of making correct decisions decreases and , consequently , the value @xmath692 decreases . note that the case @xmath696 yields @xmath697 , since @xmath696 implies @xmath698=e[x_t \\",
    ", \\vline \\ , \\f^y_0]=m_0=0 , \\forall t \\in [ 0,t].\\ ] ] the limiting cases @xmath699 ( full information ) and @xmath696 ( no information )",
    "give upper and lower bounds , respectively , for the value function @xmath692 .",
    "it can also be seen from figure [ fig : crosssections ]  ( a ) that , for fixed volatility @xmath700 of the underlying process , the volatility being determined through the function @xmath197 in , the value function is concave as a function of @xmath694 . when @xmath696 , there is no information about the underlying signal in the observation . in other words ,",
    "we only observe noise . as mentioned above",
    ", the more information about the underlying we know with certainty , the more valuable it is for the decision making .",
    "this phenomenon is even more significant when the proportion of the information contained in @xmath49 is relatively small compared with the noise ( i.e. when @xmath694 is small ) .",
    "this is because when @xmath694 is small , increasing @xmath694 increases the ratio between information and noise ( in the observation ) much faster than when @xmath694 i large . in other words , for small @xmath694",
    "the value function should increase more rapidly .",
    "as @xmath694 becomes larger , the percentage of information in @xmath49 becomes larger , and our problem starts to resemble that of complete information ( @xmath699 ) . in this case , increasing @xmath694 by a small amount may not have a noticeable effect in the observation , i.e. , the ratio between the underlying and the noise in the observation becomes stable when @xmath694 is large .",
    "this means that the observation @xmath49 will not change as significantly as before when @xmath694 increases , leading to the flattening of the value function @xmath692 seen in figure [ fig : crosssections ]  ( a ) .     for @xmath701 . ]     as a function @xmath700 and @xmath694 . ]",
    "( a  ) @xmath702     as a function @xmath700 and @xmath694 . ]",
    "( b )  @xmath703     ( a ) ( solid ) and [ fig : surfaceplots ] ( b ) ( dashed ) . ]    ( a )  @xmath704 ,     ( a ) ( solid ) and [ fig : surfaceplots ] ( b ) ( dashed ) . ]",
    "( b )  @xmath705      as for the amount of available information , the value function @xmath692 is monotonically increasing with respect to the volatility of the underlying signal .",
    "this behaviour comes from the fact that the possible gain increases when the chance of the signal being high increases , while the risk of making big losses is eliminated by the possibility for the controller to turn the facility off ( it is here relevant to compare to the monotonicity of the value function of an american option ) . in total",
    ", this makes the value function increase .",
    "similarly to the above , @xmath706 yields @xmath707 , since @xmath706 implies @xmath708 $ ] .",
    "this is also confirmed by the figures .",
    "figure [ fig : crosssections ] ( b ) shows the intersections of figure [ fig : surfaceplots ] ( a ) and ( b ) at @xmath705 and it indicates that @xmath692 is a convex function of @xmath700 for @xmath694 fixed . in this case , the volatility of the underlying @xmath30 varies , but the proportion of the information of @xmath30 contained in the observation @xmath49 stays unchanged .",
    "@xmath706 means that the underlying @xmath30 is deterministic , in this case the observation @xmath49 is useless because @xmath30 can be known deterministically from its initial value and its deterministic dynamics .",
    "when we increase @xmath700 , the underlying process @xmath30 becomes random , which means that we are not able to determine the position of @xmath30 merely from its initial data . in this case",
    "the observation @xmath49 begins to play a role as it contains information about the now unknown signal @xmath30 .",
    "the value function @xmath692 should become larger as @xmath700 increases , as the volatility of @xmath30 can make it grow significantly , and this growth can be exploited by the manager thanks to the information obtained through the process @xmath49 .",
    "the impact of observing @xmath49 is not dramatically significant for small values of @xmath700 as for small @xmath700 , the unknown @xmath30 is only modestly volatile and can be well estimated by considering only the deterministic part of its dynamics .",
    "however , when @xmath700 continues growing , making @xmath30 more and more volatile , less and less of the signal @xmath30 can be known based only on its initial values .",
    "the observation @xmath49 hence becomes increasingly important as it provides insight on how to optimally manage the facility in a now highly volatile environment .",
    "this explains the convexity of the curves in figure [ fig : crosssections ]  ( b ) .      in the discussion above",
    ", the drift @xmath625 of the underlying signal process is assumed to be @xmath709 .",
    "we conclude this numerical example by briefly discussing the impact of the drift of the underlying signal , i.e. , what happens when the drift @xmath710 . in the case of constant coefficient functions exemplified here",
    ", we will assume that @xmath703 .",
    "obviously , a positive drift of the signal should have a positive impact on the value function @xmath692 since @xmath711 is monotonically increasing in @xmath46 .",
    "this behavior is also observed in figure [ fig : surfaceplots ] ( b ) and figure [ fig : crosssections ] .",
    "in this paper we introduced and studied an optimal switching problem under partial information . in particular , we examined , in detail , the case of a kalman - bucy system and we constructed a numerical scheme for approximating the value function .",
    "we proved the convergence of our numerical approximation to the true value function using stochastic filtering theory and previous results concerning full information optimal switching problems . through a numerical example we showed the influence of information on the value function , and after comparison with a deterministic pde method",
    "we could conclude that our numerical scheme gives a reliable estimate of the value function for computationally manageable parameter values .",
    "it should be noted that , although we obtain the complete convergence result for the kalman - bucy setting , parts of the analysis does not rely on the linear structure . to the authors knowledge ,",
    "the difficulties of extending the complete result to the non - linear setting are twofold .",
    "firstly , we rely heavily on the gaussian structure and explicit expression of the measure @xmath90 to obtain the lipschitz continuity of the value function , and the lipschitz property is crucial in the subsequent convergence proof . for the general non - linear setting , however , we do not explicitly know the distribution @xmath90 and are not able to ensure lipschitzness .",
    "secondly , apart from the temporal parameter @xmath45 , in the kalman - bucy setting the value function depends only on three parameters , @xmath325 , @xmath238 and @xmath49 .",
    "however , in the non - linear setting , we may have to use the particle method to approximate @xmath90 and subsequently construct the value function based on this approximating measure . in this case",
    "the number of parameters that the value function will depend on will be at least equal to the number of particles in the approximating measure .",
    "hence , the numbers of parameters may not be fixed and can be arbitrarily large in this non - linear setting .",
    "the two problems discussed above are challenges when generalizing our analysis to the non - linear setting and naturally lead to interesting directions for future research . to be specific , in future studies we intend to extend our study to the non - linear setting and , in this generalized setting , develop a computationally tractable numerical scheme for estimating the value function .",
    "lundstrm , k. nystrm , m. olofsson , _ systems of variational inequalities in the context of optimal switching problems and operators of kolmogorov type _ , to appear in annali di mathematica pura ed applicata ."
  ],
  "abstract_text": [
    "<S> in this paper we formulate and study an optimal switching problem under partial information . in our model the agent / manager / investor attempts to maximize the expected reward by switching between different states / investments . however , he is not fully aware of his environment and only an observation process , which contains partial information about the environment / underlying , is accessible . </S>",
    "<S> it is based on the partial information carried by this observation process that all decisions must be made . </S>",
    "<S> we propose a probabilistic numerical algorithm based on dynamic programming , regression monte carlo methods , and stochastic filtering theory to compute the value function . in this paper , the approximation of the value function and the corresponding convergence result are obtained when the underlying and observation processes satisfy the linear kalman - bucy setting . a numerical example </S>",
    "<S> is included to show some specific features of partial information .    </S>",
    "<S> 2000 _ mathematics subject classification : 60c05 , 60f25 , 60g35 , 60g40 , 60h35 , 62j02 . _    _ keywords and phrases : optimal switching problem , partial information , diffusion , regression , monte - carlo , euler scheme , stochastic filtering , kalman - bucy filter . _ </S>"
  ]
}