{
  "article_text": [
    "let @xmath8 be a multi - set of elements from a totally ordered universe and let @xmath5 be an integer in the range @xmath9 .",
    "the _ selection problem _ is to find a @xmath5th smallest element of @xmath8 , that is , an element @xmath10 that is @xmath5th in some non - decreasing total ordering of  @xmath8 .",
    "selection is a fundamental problem in computer science and a key building block of many algorithms .",
    "selection is trivial when @xmath8 is sorted , but when @xmath8 is not given in sorted order it becomes more challenging .",
    "a classical divide - and - conquer algorithm  @xcite solves the selection problem for unsorted inputs in @xmath11 time .",
    "often , the input is naturally organized as a two - dimensional matrix  @xmath3 with @xmath12 rows and @xmath13 columns . using the classical algorithm one can perform selection in @xmath3 in @xmath14 time , which is optimal in the worst case . when the rows and columns of the matrix are sorted , however , one can do much better .",
    "frederickson and johnson  @xcite gave an algorithm for this case  we will call it the _ fj - algorithm _ from now on  that runs in @xmath15 time ; here we assume without loss of generality that @xmath16 . note that when @xmath17 the running time is simply @xmath18 .    in some applications the matrix  @xmath3",
    "is defined succinctly by the cartesian product of two given vectors @xmath0 $ ] and @xmath19 $ ] .",
    "we are interested in the case where @xmath20 , that is , @xmath21[i ] = x[i ] + y[j]\\ ] ] where @xmath22 and @xmath23 are sorted .",
    "( the symbol ` + ' can mean any monotone binary operator . )",
    "since @xmath22 and @xmath23 are sorted , the rows and columns of @xmath3 are sorted .",
    "hence , one can perform selection in @xmath3 in @xmath24 time by fj - algorithm .",
    "selection in such sorted @xmath25 matrices is used as a subroutine in several other algorithms  see  @xcite for some examples .",
    "the fj - algorithm is efficient in terms of cpu computation time .",
    "unfortunately , it is not efficient when it comes to io behavior , because it accesses elements of the input arrays @xmath22 and @xmath23 non - sequentially , in a pattern that does not exhibit locality of reference .",
    "this is the goal of our paper : to develop a variant of the algorithm that has better io behavior .    the _ input - output complexity _ , or _",
    "io - complexity _ , of an algorithm is usually analyzed in the external - memory model introduced by aggarwal and vitter  @xcite . in this model",
    "the memory consists of two levels : a fast memory and a slow memory .",
    "the fast memory can store up to @xmath26 words and the slow memory has unlimited storage capacity .",
    "data is stored in the slow memory in blocks of size  @xmath7 . to be able to do computations on data in the slow memory , that data first has to be brought into the fast memory ; data which is evicted from fast memory ( to make room for other data ) needs to be written back to the slow memory .",
    "data is transferred between fast and slow memory in blocks .",
    "the io - complexity of an algorithm is the number of block transfers it performs .",
    "the two levels in this abstract model can stand for any two consecutive levels in a multi - level memory hierarchy : the slow memory could be the disk and the fast memory the main memory , the slow memory could be the main memory and the fast memory the l3 cache , and so on .",
    "the values of @xmath26 and @xmath7 are different at different levels ; the higher up in the memory hierarchy , the larger the memory size @xmath26 and block size @xmath7 .",
    "our main result is a variant of the fj - algorithm for sorted @xmath25 matrices whose io - complexity is @xmath27 . here",
    ", @xmath28 is the number of ios performed when scanning  @xmath29 consecutive items ; @xmath30 .",
    "our algorithm is _ cache - oblivious _",
    "@xcite , which means it is oblivious of the parameters  @xmath26 and  @xmath7 . in other words ,",
    "the parameters @xmath26 and @xmath7 are only used in the analysis of the algorithm ; they are not used in the algorithm itself .",
    "the beauty of cache - oblivious algorithms is that , since they do not depend on the values @xmath26 and @xmath7 , they are io - efficient for all values of  @xmath26 and  @xmath7 and , hence , io - efficient at all levels of a multi - level memory hierarchy .",
    "first , we give a rough outline of the fj - algorithm  @xcite . a detailed description is given in figure  [ fig : algfj ] .",
    "let @xmath0 $ ] and @xmath1 $ ] be two input arrays of real numbers , given in sorted order :  @xmath31 { \\leqslant}x[1 ] { \\leqslant}\\cdots { \\leqslant}x[n-1]$ ] and @xmath32 { \\leqslant}y[1 ] { \\leqslant}\\cdots { \\leqslant}y[m-1]$ ] .",
    "let @xmath33[0 .. { n-1}]$ ] be the matrix @xmath25 , that is , the matrix defined by @xmath4[i]=x[i]+y[j]$ ] .",
    "we assume that @xmath34 and that @xmath13 is a power of  2 ; this can easily be ensured by implicitly padding the arrays @xmath22 and @xmath23 suitably .    following frederickson and johnson , we call a submatrix of @xmath3 a _",
    "cell_. the algorithm maintains a set @xmath35 of _ active cells _ , such that the desired element will be present in one of the active cells .",
    "initially , the entire matrix @xmath3 is the sole active cell .",
    "the algorithm proceeds in @xmath36 iterations .",
    "let @xmath37 denote the set of active cells at the beginning of the @xmath38th iteration , where @xmath39,@xmath40,@xmath41,@xmath36 .",
    "the @xmath38th iteration begins by splitting each cell of @xmath37 into four smaller cells by bisecting each dimension .",
    "let @xmath42 denote the list of cells obtained by splitting each cell of @xmath37 into four .",
    "the algorithm next discards certain cells from @xmath42 which do not contain the desired element , thus obtaining the set @xmath43 to be used in the next iteration .",
    "cells are discarded based on their minimum and maximum elements . a cell @xmath44",
    "for which @xmath45 is larger than a certain number of other minima can safely be discarded because all elements of @xmath46 will be larger than the desired element .",
    "similarly , a cell @xmath47 for which @xmath48 is smaller than a certain number of other maxima can be discarded because all elements of @xmath46 will be smaller than the desired element .",
    "the exact condition for discarding cells is given in step  ( [ stepb ] ) of the algorithm in figure  [ fig : algfj ] .",
    "the cells in @xmath37 have size @xmath49 and the cells in @xmath42 have size @xmath50 .",
    "hence , after iteration @xmath51 , the cells in @xmath37 are singletons ( that is , @xmath52 cells ) .",
    "the classical selection algorithm is then used to find the desired element among these singletons .    ' '' ''",
    "fj - algorithm@xmath53 :    1 .",
    "[ step1 ] initialize @xmath54 such that its only cell is the entire matrix @xmath55 .",
    "[ step2 ] * for * @xmath56 * to * @xmath36 * do * 1 .",
    "[ stepa ] split each @xmath57 into four subcells to obtain the set  @xmath42 .",
    "let @xmath58 .",
    "[ stepb ] let @xmath59 . + * if * @xmath60 + * then * + use a standard selection algorithm to select a @xmath61th element @xmath62 in the multiset @xmath63 . discard @xmath64 cells from @xmath42 , retaining every cell @xmath46 with @xmath65 and no cell with @xmath66 .",
    "[ stepc ] let @xmath67 . + * if * @xmath68 + * then * + use a standard selection algorithm to select an @xmath69th element @xmath70 in the multiset @xmath71 . discard @xmath69 cells from @xmath42 , retaining every cell @xmath46 with @xmath72 and no cell with @xmath73 .",
    "[ stepd ] let @xmath74 and let @xmath75 .",
    "[ step3 ] select the @xmath5th element from the cells in @xmath37 using a standard selection algorithm .    ' '' ''    the following theorem stating the performance of the fj - algorithm is a special case of the general theorem proved by frederickson and johnson  @xcite .",
    "@xcite given two sorted arrays @xmath22 and @xmath23 , each of size  @xmath13 , the fj - algorithm correctly computes an element of rank @xmath5 in the matrix @xmath76 in @xmath18 time .",
    "let @xmath77 and @xmath78 denote the subsets of @xmath22 and @xmath23 respectively that are in the @xmath38th iteration , i.e. , the elements of @xmath22 and @xmath23 that are needed to compute @xmath79 .",
    "we know that @xmath80 .",
    "the io - cost of extracting @xmath77 and @xmath78 from arrays @xmath22 and @xmath23 respectively is @xmath81 .",
    "then , the cells in @xmath79 can be computed from the cartesian product @xmath82 .",
    "assuming that the cache can hold at least two blocks of data , i.e. , @xmath83 , the cartesian product can be computed in @xmath84 ios .",
    "the total io - cost of the fj - algorithm is , therefore , @xmath85 the above sum is proportional to the sum of the following three terms : +    [ cols=\"^,^,^ \" , ]     sum  ( a ) is @xmath86 .",
    "sum  ( b ) is @xmath87 .",
    "sum  ( c ) is @xmath88 .",
    "however , the resulting complexity is no better than that of explicitly computing all @xmath89 elements of the matrix  @xmath3 and writing them to an array  @xmath90 and then selecting the @xmath5th element from  @xmath90 in @xmath91 ios using the standard selection algorithm ( see lemma  [ lemma : standard - selection ] ) .",
    "next , we show how to make the algorithm of the previous section io - efficient .",
    "henceforth , we will refer to the slow memory in our two - level hierarchy as the _ disk _ and to the fast memory as the _",
    "cache_. we assume that the array  @xmath22 is laid out in order in  @xmath13 consecutive memory locations on disk . similarly , the array  @xmath23 is laid out in order in  @xmath13 consecutive memory locations on disk .",
    "the fj - algorithm needs an efficient selection algorithm in steps  ( [ stepb ] ) ,  ( [ stepc ] ) , and  ( [ step3 ] ) .",
    "fortunately , the standard selection algorithm has good io - behavior .",
    "[ le : standard - selection ] the standard selection algorithm  @xcite selects an element of a given rank  @xmath5 from an array of  @xmath29 elements in @xmath92 time and using @xmath93 ios .",
    "[ lemma : standard - selection ]    even though selection is the main subroutine used by the matrix selection algorithm , lemma  [ lemma : standard - selection ] does not imply that the fj - algorithm is io - efficient .",
    "the main problem is that maintaining the list of active cells can dominate the io - cost of a nave implementation of the fj - algorithm , leading to @xmath18 io - complexity rather than @xmath86 . to make the algorithm io - efficient",
    ", we need to take a detailed look at the manipulation of active cells .",
    "the fj - algorithm needs a data structure to store the sets @xmath37 and @xmath79 of active cells .",
    "one could use linked lists , but traversing a linked list is not io - efficient because adjacent list elements could be stored in different blocks , requiring as many as one io - operation per list element .",
    "instead , we use arrays , which can store any list @xmath94 compactly on disk in @xmath95 blocks .",
    "we represent a cell @xmath96[i_1 .. {i_2 - 1}]$ ] by the @xmath97-tuple @xmath98 , x[i_2 - 1 ] , y[j_1 ] , y[j_2 - 1 ] } , \\ ] ] and we identify a cell with its corresponding @xmath97-tuple .",
    "the active cells are stored in lexicographic order of their corresponding @xmath97-tuples . from the @xmath97-tuple representing cell",
    "@xmath46 we can compute @xmath99+y[j_1]$ ] and @xmath100+y[j_2 - 1]$ ] in @xmath101 time and no additional ios .",
    "hence , steps  ( [ stepb ] ) ,  ( [ stepc ] ) , and  ( [ step3 ] ) of the fj - algorithm can all be done in @xmath102 ios .",
    "the problem lies in step  ( [ stepa ] ) , where we compute @xmath79 from @xmath37 by splitting each cell into four subcells .",
    "suppose we have to split the cell @xmath103 , x[i_2 ] , y[j_1 ] , y[j_2]}$ ] .",
    "let @xmath104 and let @xmath105 .",
    "the four subcells we must generate are as follows : +   + most components of the subcells can be computed from the components of @xmath46 , except that @xmath106 $ ] and @xmath107 $ ] need to be fetched from the array  @xmath22 , and @xmath108 $ ] and @xmath109 $ ] need to be fetched from the array  @xmath23 . if we are not careful , fetching these values will cost us an io each time and the whole algorithm will not be io - efficient .",
    "next we describe how to overcome this problem .",
    "let us examine in what order the algorithm accesses the array @xmath22 ; the array @xmath23 will be discussed later .",
    "first consider the elements from @xmath22 needed for the fifth component of the cells , which stores the minimum @xmath22-value in the cell . in the initialization step ,",
    "the entire matrix @xmath3 is the only active cell ; its minimum @xmath22-value is @xmath31 $ ] . in the first iteration ( @xmath39 ) we split @xmath3 into four subcells .",
    "the minimum @xmath22-values in those subcells are either @xmath31 $ ] ( for the north - west and south - west subcells ) or @xmath110 $ ] ( for the north - east and south - east subcells ) . since @xmath31 $ ] is conveniently stored in the original cell , we only need to access @xmath110 $ ] . in the second iteration",
    ", we need to access @xmath111 $ ] and @xmath112 $ ] . in general , in the @xmath38th iteration ( @xmath113 ) , each active cell in @xmath37 has dimension @xmath114 , and the elements that need to be accessed to obtain their minimum x - values are @xmath115 $ ] for @xmath116 .",
    "( in fact , we do not necessarily need all these elements , since not all cells have to be active . )    to enable io - efficient access to these elements in @xmath22 , we construct an array @xmath117 $ ] that stores , for any @xmath38 with @xmath113 , the elements needed in the @xmath38th iteration consecutively .",
    "thus we define array @xmath118 so that it has the following property :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ for all @xmath38 in the range @xmath119 , for all @xmath120 in the range @xmath121 , we have @xmath122 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    note that , together with @xmath31 $ ] , the elements in @xmath118 are exactly the elements in @xmath22 at even - numbered positions .    similarly , the elements that need to be accessed to obtain the maximum x - values in the @xmath38-th iteration , namely @xmath123 $ ] for @xmath116 , are stored in an array @xmath124 . thus array @xmath125 $ ] stores the odd - numbered elements in @xmath22 ( except @xmath126 $ ] ) , as follows :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ for all @xmath38 in the range @xmath119 , for all @xmath120 in the range @xmath121 , we have @xmath127 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    next we show how to compute the array @xmath118 efficiently ; @xmath124 can be computed similarly .",
    "given an integer @xmath120 , the _ bit - reversal _ of @xmath120 is the integer @xmath128 such that the binary string representing @xmath128 is the reverse of the binary string representing  @xmath120 . the _ bit - reversal permutation _",
    "@xmath129 of an array @xmath90 is the permutation that maps that @xmath130 $ ] to @xmath131 $ ] .",
    "the bit - reversal permutation can be computed recursively as follows : copy all elements in even - numbered positions in @xmath90 in order to the first half of the array @xmath129 , and copy all elements in odd - numbered positions of @xmath90 in order to the second half of @xmath129 ; recurse on both halves .",
    "now suppose we only recurse on the first half of the array @xmath129 ; the elements in the second half are kept in the same relative order as in the input array  @xmath90 .",
    "we call the resulting permutation the _ partial bit reversal_. as we will show below , the partial bit reversal of array @xmath22 is closely related to the array @xmath118 that we want to compute . the recursive algorithm pbr given in fig .",
    "[ fig : algpbr]a non - recursive version would also be possible  computes a partial bit reversal @xmath129 of a given array @xmath132 $ ] . in the initial call , @xmath129 is a copy of @xmath90 , and @xmath133 .",
    "( recall that we assumed @xmath13 is a power of  2 . )    ' '' ''    algorithm pbr@xmath134 :    1 .   *",
    "if * @xmath135 2 .   [ pbr2 ] * then * + _ comment : @xmath136 $ ] and @xmath137 $ ] are auxiliary arrays .",
    "_ 1 .   [ pbr2a ] * for * @xmath138 * to * @xmath139 * do * + @xmath120 is even * then * @xmath140 : = z'[i]$ ] * else * @xmath141:= z'[i]$ ] 2",
    ".   [ pbr2b ] * for * @xmath138 * to * @xmath142 * do * @xmath143 : = \\even[i]$ ] + * for * @xmath144 * to * @xmath139 * do * @xmath143 : = \\odd[i - s/2]$ ] 3 .",
    "[ pbr2c ] pbr@xmath145    ' '' ''    the following lemma , which gives the running time and io complexity of pbr , follows easily from the fact that steps  [ pbr2a ]  and  [ pbr2b ] of pbr are just linear scans of arrays @xmath129 , @xmath146 , and @xmath147 , so these steps run in @xmath92 time and @xmath93 ios .",
    "pbr@xmath148 runs in @xmath18 time and uses @xmath86 ios .",
    "[ lemma : permutecost ]    the next lemma shows the correspondence between the partial bit reversal of our input array @xmath22 and the array @xmath149 we want to compute .",
    "it implies that @xmath118 can be obtained by computing the partial bit reversal @xmath150 of @xmath22 and then taking the elements from @xmath151 $ ] in order .",
    "[ le : reorder ] let @xmath129 be the partial bit reversal of an array @xmath132 $ ] , where @xmath13 is a power of 2 , as computed by pbr . then for all @xmath38 in the range @xmath119 , for all @xmath120 in the range @xmath121 , we have @xmath152 [ lemma : permutecorrectly ]    let @xmath153 be an even index , and let @xmath154 and @xmath155 be such that @xmath156 .",
    "now consider what happens to element @xmath157 $ ] .",
    "initially @xmath129 is a copy of @xmath90 , so @xmath157 $ ] is stored in @xmath158 $ ] .",
    "then , in the first call to pbr  that is , the call with @xmath133it will be moved to @xmath159 $ ] by steps  ( [ pbr2a ] )  and  [ pbr2b ] . in the recursive call with @xmath160",
    "it will be moved to @xmath161 $ ] ( if @xmath162 ) .",
    "this process continues @xmath5 times , until the recursive call is made with @xmath163 . at this point",
    "@xmath157 $ ] is stored in @xmath164 $ ] , and step  ( [ pbr2a ] ) moves the element to @xmath165 $ ] .",
    "after that it will not be moved anymore by the algorithm .",
    "now set @xmath166 and take @xmath38 such that @xmath167 .",
    "then @xmath168 and we can conclude that @xmath169 $ ] ends up in @xmath170 $ ] , as required .    in what follows , we use @xmath171 to denote the position of @xmath172 $ ] in the array @xmath149 , for @xmath153 and @xmath173 even .",
    "thus , according to equation  ( [ def : permutation1 ] ) , we have @xmath174 .",
    "similarly , @xmath175 denotes the position of @xmath172 $ ] in the array @xmath176 , for @xmath177 and @xmath173 odd ; thus @xmath178 .",
    "the arrays @xmath118 and @xmath124 give us the @xmath179$]-values in the order they are needed by the cell - partitioning step of the fj - algorithm .",
    "however , to partition a cell we also need to fetch new @xmath180 $ ] values . for this we would like to use the same approach :",
    "compute in a preprocessing step two arrays @xmath181 $ ] and @xmath182 $ ] , which contain the @xmath180$]-values in the order needed by the algorithm . with the @xmath179$]-values this approach was possible , because the cells in @xmath37 are kept in lexicographical order , with the @xmath183-value being dominant .",
    "hence , we knew exactly not only which @xmath179$]-values were needed in the @xmath38-th iteration ( namely @xmath184 $ ] for @xmath116 ) , but also in which order ( namely according to increasing index ) .",
    "but for the @xmath180$]-values we only know which values we need in the @xmath38-th iteration ; we do not know in which order we need them , because the @xmath183-coordinate is dominant in the order of the cells in @xmath37 .",
    "next we will show that the approach works nevertheless .",
    "thus we compute arrays @xmath185 and @xmath186 in exactly the same way as the arrays @xmath118 and @xmath124 were computed .",
    "then we partition the cells with the algorithm shown in figure  [ fig : partition ] .    ' '' ''",
    "partition@xmath187 :    1 .",
    "let @xmath188 and @xmath189 be two arrays of twice the size as @xmath37 .",
    "[ halveonx-2 ] * for * @xmath138 * to * @xmath190 * do * + + let @xmath191,x[i_2 - 1],y[j_1],y[j_2 - 1]}$ ] be the cell in @xmath192 $ ] .",
    "+ let @xmath104 and let @xmath105 .",
    "1 .   fetch @xmath106 $ ] from @xmath193 $ ] and @xmath107 $ ] from @xmath194 $ ] [ partition.fetchx ] 2 .",
    "fetch @xmath108 $ ] from @xmath195 $ ] and @xmath109 $ ] from @xmath196 $ ] [ partition.fetchy ] 3 .",
    "@xmath197 \\leftarrow \\paren{i_1,j_1,i_m , j_m , x[i_1],x[i_m-1],y[j_1],y[{j_m}-1]}$ ] 4 .",
    "@xmath198 \\leftarrow \\paren{i_m , j_1,i_2,j_m , x[i_m],x[i_2 - 1],y[j_1],y[j_m-1]}$ ] 5 .",
    "@xmath199 \\leftarrow \\paren{i_1,j_m , i_m , j_2,x[i_1],x[i_m-1],y[j_m],y[j_2 - 1]}$ ] 6 .",
    "@xmath200 \\leftarrow \\paren{i_m , j_m , i_2,j_2,x[i_m],x[i_2 - 1],y[j_m],y[j_2 - 1]}$ ] 3 .",
    "_ comment : now @xmath188 and @xmath189 together contain the new subcells , and both arrays are sorted lexicographically .",
    "[ partition.merge ] merge @xmath188 and @xmath189 into an array @xmath42 that is sorted lexicographically .",
    "* return * @xmath42 .    ' '' ''    before we can prove that this algorithm is indeed io - efficient , we need to deal with one subtlety : we need to be more specific about the exact implementation of step  ( [ stepb ] ) of the fj - algorithm in case the @xmath61th element , @xmath62 , is not unique .",
    "more precisely , we need to specify which of the cells @xmath46 with @xmath201 are discarded and which are kept .",
    "similarly , we must specify which of the cells @xmath46 with @xmath202 are discarded and which are kept in step  ( [ stepc ] ) .",
    "we do this as follows .",
    "recall that we maintain @xmath42 in lexicographic order .",
    "now we can implement step  ( [ stepb ] ) by removing from @xmath42 exactly those cells whose ranks are greater than @xmath61 according to this lexicographical order .",
    "this implies that if we remove a certain cell @xmath46 , we will also remove all cells to the south - east of @xmath46 ( including the ones to the south of @xmath46 , and the ones to the east of @xmath46 ) .",
    "we use a similar strategy to guarantee that when we remove a cell in step  ( [ stepc ] ) , we also remove all cells to its north - west . with this implementation ,",
    "the active cells have the following properties  see also figure  [ fig : active - cells ] .    1 .",
    "all active cells with the same column index are consecutive .",
    "2 .   the active cell with the largest row index in a given column ",
    "note that row indices increase when going downwards in figure  [ fig : active - cells]cannot have row index smaller than the any active cell in the column to its right .",
    "in other words , if we consider the lowest active cells in each column and we consider the columns from left to right , then the the row indices of these highest active cells are non - increasing .",
    "these properties are essential to get good io - complexity of partition .",
    "[ fig : active - cells ] [ fig : staircases ]    algorithm  partition produces a lexicographically sorted array @xmath42 of all subcells resulting from partitioning every cell in @xmath37 into four . partition runs in @xmath203 time and performs @xmath204 ios .",
    "[ le : partition ]    the correctness of the algorithm directly follows from the fact that , by definition of @xmath205 and @xmath206 , the correct values are fetched in steps  ( [ partition.fetchx ] )  and  ( [ partition.fetchy ] ) .    to bound the running time",
    ", we note that @xmath207 and @xmath208 can be evaluated in @xmath101 time . indeed , when we evaluate e.g.  @xmath171 for some @xmath173 , we know the value of @xmath38 such that @xmath209this @xmath38 is a parameter of  partition .",
    "given @xmath38 , we have @xmath210 .",
    "it follows that the running time is @xmath18 .    as for the number of ios , all accesses to @xmath37 , as well as step  ( [ partition.merge ] ) ,",
    "take @xmath204 ios in total .",
    "hence , it remains to argue about the accesses to @xmath118 , @xmath124 , @xmath185 , and @xmath186 .",
    "we first consider the accesses to @xmath118 . as argued earlier , the cells in @xmath37 have size @xmath114 , which means we need to fetch from @xmath118 ( a subset of ) the elements @xmath211 $ ] for @xmath116 . by the definition of @xmath118see equation  ( [ def :",
    "permutation1])these elements are consecutive in @xmath118 .",
    "moreover , these elements are accessed from left to right in @xmath118 , because the cells in @xmath37 are sorted in increasing order of their first coordinate .",
    "hence , all these accesses to @xmath118 take @xmath212 ios in total .",
    "symmetric reasoning gives the same bound on the number of accesses to @xmath124 .",
    "now consider the accesses to @xmath185 ; symmetric reasoning bounds the accesses to @xmath186 .",
    "consider figure  [ fig : active - cells ] .",
    "the active cells will be visited by the algorithm in lexicographic order , as indicated in the figure .",
    "this means that the algorithm may go back and forth in @xmath185 .",
    "moreover , when going back , the algorithm may _ jump _ from accessing some element @xmath213 $ ] to accessing another element @xmath214 $ ] where @xmath215 ; we call @xmath216 the _ length _ of the jump .",
    "jumps are significant because each jump may incur a cost of one io operation .",
    "( jumps are also possible when accessing @xmath118 or @xmath124 .",
    "since in @xmath118 and @xmath124 we only jump forward , this does not increase the number of ios there . )",
    "note that the elements needed within a single column of active cells , are stored in the correct order in @xmath185 .",
    "( here the term `` column '' refers to a column in the matrix of whose cells are submatrices of size @xmath217 . ) when we step from the lowest active cell in one column to highest active cell in the next column , however , we may jump in @xmath185 .",
    "now suppose that instead of jumping from one location to the next , we visit all intermediate locations as well . hence , after visiting @xmath213 $ ] , the new traversal always proceeds to either @xmath218 $ ] or @xmath219 $ ] .",
    "we call such a traversal _ well - behaved_. clearly the number of ios needed by the new traversal of @xmath185 is not more than the number of traversals needed by the original traversal .",
    "the original traversal visited @xmath220 ( not necessarily distinct ) locations in @xmath185 .",
    "we claim that the length of the new , well - behaved traversal is @xmath221 . to show this",
    ", we must bound the total length of all backward jumps . consider a backward jump from the lowest active cell in some column  @xmath46 to the highest active cell in the next column  @xmath222 .",
    "this jump crosses a number of rows . by properties  ( i )  and",
    "( ii ) of the active cells , for each row that is crossed , at least one of the following three condition holds : @xmath46 contains an active cell in this row , @xmath222 contains an active cell in this row , or the row will not be visited again later .",
    "this is easily seen to imply that the total length of all jumps is @xmath221 , as claimed .",
    "it remains to observe that , assuming @xmath223that is , assuming at least two blocks fit in the cache  any well - behaved traversal of length @xmath94 needs @xmath224 ios .",
    "indeed , suppose we need to read a new block when we step from @xmath225 $ ] to @xmath226 $ ] .",
    "then we read the block starting at @xmath226 $ ] and can keep the block ending at @xmath225 $ ] in cache .",
    "hence , at least @xmath227 more forward steps or at least @xmath7 backward steps are needed before another block needs to be read .",
    "we conclude that the number of ios performed in accessing @xmath185 ( and , similarly , @xmath186 ) is @xmath228 , which finishes the proof for the number of ios .",
    "there exists a cache - oblivious implementation of the matrix selection algorithm of frederickson and johnson for sorted @xmath25 matrices using @xmath86 ios and @xmath18 time , where @xmath13 is the maximum of the lengths of @xmath22 and @xmath23 .    by lemma  [ lemma : permutecost ]",
    ", the computation of the arrays @xmath118 , @xmath124 , @xmath185 , and @xmath186 takes @xmath86 ios and @xmath18 time .",
    "now consider the main algorithm .",
    "frederickson and johnson  @xcite proved that @xmath229 , the number of active cells in the beginning of the @xmath38th iteration , is @xmath230 . by lemmas  [ le : standard - selection ] , [ le :",
    "reorder ] , and  [ le : partition ] , this implies that the total io - cost is bounded by @xmath231 since the subroutine partition runs in @xmath203 , the running time of the main algorithm is unchanged from the original fj - algorithm , which runs in  @xmath18 time .",
    "in this paper , we gave an io - efficient cache - oblivious version of the classical matrix selection algorithm of frederickson and johnson for selecting a rank-@xmath5 element in an @xmath232 matrix given succinctly in the form @xmath233 .",
    "if the matrix @xmath3 is not square  that is , if its dimensions were @xmath234 where @xmath235then a different approach seems to be required to make the matrix selection algorithm io - efficient .",
    "one would like to obtain an io - cost of @xmath236 however , we already spend @xmath6 ios in permuting the input arrays as a pre - processing step , which dominates the io - cost of the subsequent algorithm .",
    "it seems difficult to avoid the high io - cost of permuting both input arrays so that they can be accessed io - efficiently .",
    "a completely new algorithm may be necessary to achieve io - optimal matrix selection in sorted @xmath25 matrices that are not square .",
    "a.  golzman , k.  kedem , and g.  spitalnik . on some geometric selection and optimization problems via sorted matrices . in _ proc .",
    "4th workshop on algorithms and data structures _ , lncs 955 , pages 2637 , 1995 ."
  ],
  "abstract_text": [
    "<S> let @xmath0 $ ] and @xmath1 $ ] be two sorted arrays , and define the @xmath2 matrix  @xmath3 by @xmath4[i ] = x[i ] + y[j]$ ] . </S>",
    "<S> frederickson and johnson  @xcite gave an efficient algorithm for selecting the @xmath5th smallest element from  @xmath3 . </S>",
    "<S> we show how to make this algorithm io - efficient . </S>",
    "<S> our cache - oblivious algorithm performs @xmath6 ios , where @xmath7 is the block size of memory transfers . </S>"
  ]
}