{
  "article_text": [
    "the spectral distribution of light reflected off a surface is a function of an intrinsic material property of the surface ",
    "its reflectance  and also of the spectral distribution of the light illuminating the surface .",
    "consequently , the observed color of the same surface under different illuminants in different images will be different . to be able to reliably use color computationally for identifying materials and objects , researchers are interested in deriving an encoding of color from an observed image that is invariant to changing illumination .",
    "this task is known as color constancy , and requires resolving the ambiguity between illuminant and surface colors in an observed image .",
    "since both of these quantities are unknown , much of color constancy research is focused on identifying models and statistical properties of natural scenes that are informative for color constancy .",
    "while pschophysical experiments have demonstrated that the human visual system is remarkably successful at achieving color constancy  @xcite , it remains a challenging task computationally .",
    "early color constancy algorithms were based on relatively simple models for pixel colors .",
    "for example , the gray world method  @xcite simply assumed that the average true intensities of different color channels across all pixels in an image would be equal , while the white - patch retinex method  @xcite assumed that the true color of the brightest pixels in an image is white .",
    "most modern color constancy methods , however , are based on more complex reasoning with higher - order image features . many methods  @xcite use models for image derivatives instead of individual pixels .",
    "others are based on recognizing and matching image segments to those in a training set to recover true color  @xcite .",
    "a recent method proposes the use of a multi - layer convolutional neural network ( cnn ) to regress from image patches to illuminant color .",
    "there are also many `` combination - based '' color constancy algorithms , that combine illuminant estimates from a number of simpler `` unitary '' algorithms  @xcite , sometimes using image features to give higher weight to the outputs of some subset of methods .    in this paper , we demonstrate that by appropriately modeling and reasoning with the statistics of individual pixel colors , one can computationally recover illuminant color with high accuracy .",
    "we consider individual pixels in isolation , where the color constancy task reduces to discriminating between the possible choices of true color for the pixel that are feasible given the observed color and a candidate set of illuminants .",
    "central to our method is a function that gives us the relative likelihoods of these true colors , and therefore a distribution over the corresponding candidate illuminants .",
    "our global estimate for the scene illuminant is then computed by simply aggregating these distributions across all pixels in the image .",
    "we formulate the likelihood function as one that measures the conditional likelihood of true pixel chromaticity given observed luminance , in part to be agnostic to the scalar ( i.e. , color channel - independent ) ambiguity in observed color intensities .",
    "moreover , rather than committing to a parametric form , we quantize the space of possible chromaticity and luminance values , and define the function over this discrete domain .",
    "we begin by setting the conditional likelihoods purely empirically , based simply on the histograms of true color values over all pixels in all images across a training set .",
    "even with this purely empirical approach , our estimation algorithm yields estimates with higher accuracy than current state - of - the - art methods .",
    "then , we investigate learning the per - pixel belief function by optimizing an objective based on the accuracy of the final global illuminant estimate . we carry out this optimization using stochastic gradient descent , and using a sub - sampling approach ( similar to `` dropout ''  @xcite ) to improve generalization beyond the training set .",
    "this further improves estimation accuracy , without adding to the computational cost of inference .",
    "assuming lambertian reflection , the spectral distribution of light reflected by a material is a product of the distribution of the incident light and the material s reflectance function .",
    "the color intensity vector @xmath0 recorded by a tri - chromatic sensor at each pixel @xmath1 is then given by @xmath2 where @xmath3 is the reflectance at @xmath1 , @xmath4 is the spectral distribution of the incident illumination , @xmath5 is a geometry - dependent shading factor , and @xmath6 denotes the spectral sensitivities of the color sensors .",
    "color constancy is typically framed as the task of computing from @xmath7 the corresponding color intensities @xmath8 that would have been observed under some canonical illuminant @xmath9 ( typically chosen to be @xmath10 ) .",
    "we will refer to @xmath11 as the `` true color '' at @xmath1 .",
    "since involves a projection of the full incident light spectrum on to the three filters @xmath12 , it is not generally possible to recover @xmath11 from @xmath7 even with knowledge of the illuminant @xmath4 .",
    "however , a commonly adopted approximation ( shown to be reasonable under certain assumptions  @xcite ) is to relate the true and observed colors @xmath11 and @xmath7 by a simple per - channel adaptation : @xmath13 where @xmath14 refers to the element - wise hadamard product , and @xmath15 depends on the illuminant @xmath4 ( for @xmath9 , @xmath16^t$ ] ) . with some abuse of terminology",
    ", we will refer to @xmath17 as the illuminant in the remainder of the paper .",
    "moreover , we will focus on the single - illuminant case in this paper , and assume @xmath18 in an image . our goal during inference",
    "will be to estimate this global illuminant @xmath19 from the observed image @xmath7 .",
    "the true color image @xmath11 can then simply be recovered as @xmath20 , where @xmath21 denotes the element - wise inverse of @xmath19 .",
    "note that color constancy algorithms seek to resolve the ambiguity between @xmath19 and @xmath11 in only up to a channel - independent scalar factor .",
    "this is because scalar ambiguities show up in @xmath19 between @xmath22 and @xmath9 due to light attenuation , between @xmath11 and @xmath23 due to the shading factor @xmath5 , and in the observed image @xmath7 itself due to varying exposure settings .",
    "therefore , the performance metric typically used is the angular error @xmath24 between the true and estimated illuminant vectors @xmath19 and @xmath25 .    [ [ database ] ] database + + + + + + + +    for training and evaluation , we use the database of 568 natural indoor and outdoor images captured under various illuminants by gehler et al .",
    "we use the version from shi and funt  @xcite that contains linear images ( without gamma correction ) generated from the raw camera data .",
    "the database contains images captured with two different cameras ( 86 images with a canon 1d , and 482 with a canon 5d ) .",
    "each image contains a color checker chart placed in the image , with its position manually labeled .",
    "the colors of the gray squares in the chart are taken to be the value of the true illuminant @xmath26 for each image , which can then be used to correct the image to get true colors at each pixel ( of course , only up to scale ) .",
    "the chart is masked out during evaluation .",
    "we use k - fold cross - validation over this dataset in our experiments .",
    "each fold contains images from both cameras corresponding to one of k roughly - equal partitions of each camera s image set ( ordered by file name / order of capture ) .",
    "estimates for images in each fold are based on training only with data from the remaining folds .",
    "we report results with three- and ten - fold cross - validation .",
    "these correspond to average training set sizes of 379 and 511 images respectively .",
    "a color vector @xmath27 can be characterized in terms of ( 1 ) its _ luminance _",
    "@xmath28 , or absolute brightness across color channels ; and ( 2 ) its _ chromaticity _ , which is a measure of the relative ratios between intensities in different channels .",
    "while there are different ways of encoding chromaticity , we will do so in terms of the unit vector @xmath29 in the direction of @xmath30 .",
    "note that since intensities can not be negative , @xmath31 is restricted to lie on the non - negative eighth of the unit sphere @xmath32 .",
    "remember from sec .",
    "[ sec : prelim ] that our goal is to resolve the ambiguity between the true colors @xmath11 and the illuminant @xmath19 only up to scale .",
    "in other words , we need only estimate the illuminant chromaticity @xmath33 and true chromaticities @xmath34 from the observed image @xmath7 , which we can relate from as @xmath35    a key property of natural illuminant chromaticities is that they are known to take a fairly restricted set of values , close to a one - dimensional locus predicted by planck s radiation law  @xcite . to be able to exploit this ,",
    "we denote @xmath36 as the set of possible values for illuminant chromaticity @xmath33 , and construct it from a training set .",
    "specifically , we quantize .",
    "see supplementary material for details . ]",
    "the chromaticity vectors @xmath37 of the illuminants in the training set , and let @xmath38 be the set of unique chromaticity values . additionally , we define a `` prior '' @xmath39 over this candidate set , based on the number @xmath40 of training illuminants that were quantized to @xmath41",
    ".    given the observed color @xmath7 at a single pixel @xmath1 , the ambiguity in @xmath33 across the illuminant set @xmath38 translates to a corresponding ambiguity in the true chromaticity @xmath34 over the set @xmath42 .",
    "figure  [ fig : hdist](a ) illustrates this ambiguity for a few different observed colors @xmath43 .",
    "we note that while there is significant angular deviation within the set of possible true chromaticity values for any observed color , values in each set lie close to a one dimensional locus in chromaticity space .",
    "this suggests that the illuminants in our training set are indeed a good fit to planck s law .    , see legend ) consistent with the pixel s observed chromaticity ( color of the points ) and different candidate illuminants @xmath41 .",
    "( b ) distributions over different values for true chromaticity of a pixel conditioned on its observed luminance , computed as empirical histograms over the training set .",
    "values @xmath44 are normalized per - image by the median luminance value over all pixels .",
    "( c ) corresponding distributions learned with end - to - end training to maximize accuracy of overall illuminant estimation . ]",
    "the goal of our work is to investigate the extent to which we can resolve the above ambiguity in true chromaticity on a per - pixel basis , without having to reason about the pixel s spatial neighborhood or semantic context .",
    "our approach is based on computing a likelihood distribution over the possible values of @xmath34 , given the observed luminance @xmath45 .",
    "but as mentioned in sec .",
    "[ sec : prelim ] , there is considerable ambiguity in the scale of observed color intensities .",
    "we address this partially by applying a simple per - image global normalization to the observed luminance to define @xmath46 .",
    "this very roughly compensates for variations across images due to exposure settings , illuminant brightness , etc .",
    "however , note that since the normalization is global , it does not compensate for variations due to shading .",
    "the central component of our inference method is a function @xmath47 $ ] that encodes the belief that a pixel with normalized observed luminance @xmath44 has true chromaticity @xmath31 .",
    "this function is defined over a discrete domain by quantizing both chromaticity and luminance values : we clip luminance values @xmath44 to four ( i.e. , four times the median luminance of the image ) and quantize them into twenty equal sized bins ; and for chromaticity @xmath31 , we use a much finger quantization with @xmath48 equal - sized bins in @xmath49 ( see supplementary material for details ) . in this section",
    ", we adopt a purely empirical approach and define @xmath47 $ ] as @xmath47 = \\log \\left(n_{\\hat{\\mathbf{x}},y } / \\sum_{\\hat{\\mathbf{x}}'}n_{\\hat{\\mathbf{x}}',y}\\right),$ ] where @xmath50 is the number of pixels across all pixels in a set of images in a training set that have true chromaticity @xmath31 and observed luminance @xmath44 .",
    "we visualize these empirical versions of @xmath47 $ ] for a subset of the luminance quantization levels in fig .",
    "[ fig : hdist](b ) .",
    "we find that in general , desaturated chromaticities with similar intensity values in all color channels are most common .",
    "this is consistent with findings of statistical analysis of natural spectra  @xcite , which shows the `` dc '' component ( flat across wavelength ) to be the one with most variance .",
    "we also note that the concentration of the likelihood mass in these chromaticities increasing for higher values of luminance @xmath44 .",
    "this phenomenon is also predicted by traditional intuitions in color science : materials are brightest when they reflect most of the incident light , which typically occurs when they have a flat reflectance function with all values of @xmath51 close to one .",
    "indeed , this is what forms the basis of the white - patch retinex method  @xcite .",
    "amongst saturated colors , we find that hues which combine green with either red or blue occur more frequently than primary colors , with pure green and combinations of red and blue being the least common .",
    "this is consistent with findings that reflectance functions are usually smooth ( pca on pixel spectra in @xcite revealed a fourier - like basis ) . both saturated green and red - blue combinations would require the reflectance to have either a sharp peak or crest , respectively , in the middle of the visible spectrum .",
    "we now describe a method that exploits the belief function @xmath47 $ ] for illuminant estimation . given the observed color @xmath7 at a pixel @xmath1 , we can obtain a distribution @xmath52\\}_i$ ] over the set of possible true chromaticity values @xmath42 , which can also be interpreted as a distribution over the corresponding illuminants @xmath41 .",
    "we then simply aggregate these distributions across all pixels @xmath1 in the image , and define the global probability of @xmath41 being the scene illuminant @xmath19 as @xmath53 , where @xmath54 + \\beta b_i,\\ ] ] @xmath55 is the total number of pixels in the image , and @xmath56 and @xmath57 are scalar parameters .",
    "the final illuminant chromaticity estimate @xmath25 is then computed as @xmath58 \\approx \\underset{\\mathbf{m } ' , \\|\\mathbf{m}'\\|_2 = 1}{\\arg \\max } \\mathbb{e}[\\mathbf{m}^t\\mathbf{m } ' ] = \\frac{\\sum_i p_i \\mathbf{\\hat{m}}_i}{\\|\\sum_i p_i \\mathbf{\\hat{m}}_i\\|_2}.\\ ] ] note that also incorporates the prior @xmath59 over illuminants .",
    "we set the parameters @xmath56 and @xmath57 using a grid search , to values that minimize mean illuminant estimation error over the training set .",
    "the primary computational cost of inference is in computing the values of @xmath60 .",
    "we pre - compute values of @xmath61 using over the discrete domain of quantized chromaticity values for @xmath31 and the candidate illuminant set @xmath38 for @xmath33 .",
    "therefore , computing each @xmath62 essentially only requires the addition of @xmath55 numbers from a look - up table .",
    "we need to do this for all @xmath63 illuminants , where summations for different illuminants can be carried out in parallel .",
    "our implementation takes roughly 0.3 seconds for a 9 mega - pixel image , on a modern intel 3.3ghz cpu with 6 cores , and is available at http://www.ttic.edu / chakrabarti / chromcc/.    this empirical version of our approach bears some similarity to the bayesian method of @xcite that is based on priors for illuminants , and for the likelihood of different true reflectance values being present in a scene .",
    "however , the key difference is our modeling of true chromaticity _ conditioned _ on luminance that explicitly makes estimation agnostic to the absolute scale of intensity values .",
    "we also reason with all pixels , rather than the set of unique colors in the image .    [ [ experimental - results . ] ] experimental results .",
    "+ + + + + + + + + + + + + + + + + + + + +    table [ tab : etab ] compares the performance of illuminant estimation with our method ( see rows labeled `` empirical '' ) to the current state - of - the - art , using different quantiles of angular error across the gehler - shi database  @xcite .",
    "results for other methods are from the survey by li et al .  @xcite .",
    "( see the supplementary material for comparisons to some other recent methods ) .",
    "we show results with both three- and ten - fold cross - validation .",
    "we find that our errors with three - fold cross - validation have lower mean , median , and tri - mean values than those of the best performing state - of - the - art method from @xcite , which combines illuminant estimates from twelve different `` unitary '' color - constancy method ( many of which are also listed in table  [ tab : etab ] ) using support - vector regression .",
    "the improvement in error is larger with respect to the other combination methods  @xcite , as well as those based the statistics of image derivatives  @xcite .",
    "moreover , since our method has more parameters than most previous algorithms ( @xmath47 $ ] has @xmath64k entries ) , it is likely to benefit from more training data .",
    "we find this to indeed be the case , and observe a considerable decrease in error quantiles when we switch to ten - fold cross - validation .    figure .",
    "[ fig : efig ] shows estimation results with our method for a few sample images .",
    "for each image , we show the input image ( indicating the ground truth color chart being masked out ) and the output image with colors corrected by the global illuminant estimate . to visualize the quality of contributions from individual pixels",
    ", we also show a map of angular errors for illuminant estimates from individual pixels .",
    "these estimates are based on values of @xmath62 computed by restricting the summation in to individual pixels .",
    "we find that even these pixel - wise estimates are fairly accurate for a lot of pixels , even when it s true color is saturated ( see cart in first row ) . also ,",
    "to evaluate the weight of these per - pixel distributions to the global @xmath62 , we show a map of their variance on a per - pixel basis . as expected from fig .",
    "[ fig : hdist](b ) , we note higher variances in relatively brighter pixels .",
    "the image in the last row represents one of the poorest estimates across the entire dataset ( higher than @xmath65ile ) .",
    "note that much of the image is in shadow , and contain only a few distinct ( and likely atypical ) materials .",
    "+ [ tab : etab ]     90%-ile +    bayesian  @xcite & 6.74@xmath66&5.14@xmath66&5.54@xmath66&2.42@xmath66&9.47@xmath66&14.71@xmath66 + gamut mapping  @xcite & 6.00@xmath66&3.98@xmath66&4.52@xmath66&1.71@xmath66&8.42@xmath66&14.74@xmath66 + deriv .",
    "gamut mapping  @xcite & 5.96@xmath66&3.83@xmath66&4.32@xmath66&1.68@xmath66&7.95@xmath66&14.72@xmath66 + gray world  @xcite & 4.77@xmath66&3.63@xmath66&3.92@xmath66&1.81@xmath66&6.63@xmath66&10.59@xmath66 + gray edge@xmath67  @xcite & 4.19@xmath66&3.28@xmath66&3.54@xmath66&1.87@xmath66&5.72@xmath66&8.60@xmath66 + sv - regression  @xcite & 4.14@xmath66&3.23@xmath66&3.35@xmath66&1.68@xmath66&5.27@xmath66&8.87@xmath66 + spatio - spectral  @xcite & 3.99@xmath66&3.24@xmath66&3.45@xmath66&2.38@xmath66&4.97@xmath66&7.50@xmath66 + scene geom .",
    "@xcite & 4.56@xmath66&3.15@xmath66&3.46@xmath66&1.41@xmath66&6.12@xmath66&10.39@xmath66 + nearest-30% comb .",
    "@xcite & 4.26@xmath66&2.95@xmath66&3.19@xmath66&1.49@xmath66&5.39@xmath66&9.67@xmath66 + classifier - based comb .",
    "@xcite & 3.83@xmath66&2.75@xmath66&2.93@xmath66&1.34@xmath66&4.89@xmath66&8.19@xmath66 + neural comb .",
    "( elm )  @xcite & 3.43@xmath66&2.37@xmath66&2.62@xmath66&1.21@xmath66&4.53@xmath66&6.97@xmath66 + svr - based comb .",
    "@xcite & 2.98@xmath66&1.97@xmath66&2.35@xmath66&1.13@xmath66&4.33@xmath66&6.37@xmath66 +   +   + ( 3-fold)empirical & 2.89@xmath66&1.89@xmath66&2.15@xmath66&1.15@xmath66&3.68@xmath66&6.24@xmath66 + end - to - end trained&2.56@xmath66&1.67@xmath66&1.89@xmath66&0.91@xmath66&3.30@xmath66&5.56@xmath66 + ( 10-fold )  empirical&2.55@xmath66&1.58@xmath66&1.83@xmath66&0.85@xmath66&3.30@xmath66&5.74@xmath66 + end - to - end trained&2.20@xmath66&1.37@xmath66&1.53@xmath66&0.69@xmath66&2.68@xmath66&4.89@xmath66 +    ccccc &  &  global estimate &  per - pixel estimate error&belief variance +   +   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ]  &  &   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ]  &   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ]  &   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ] +  input+mask & &  error = 0.56@xmath66 +   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ]  &  &   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ]  &   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ]  &   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ] +  ground truth & &  error = 0.24@xmath66 +   +   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ]  &  &   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ]  &   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ]  &   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ] +  input+mask & &  error = 4.32@xmath66 +   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ]  &  &   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ]  &   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ]  &   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ] +  ground truth & &  error = 3.15@xmath66 +   +   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ]  &  &   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ]  &   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ]  &   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ] +  input+mask & &  error = 16.22@xmath66 +   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ]  &  &   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ]  &   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ]  &   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ] +  ground truth & &  error = 10.31@xmath66 +   +     +   based on distributions from only a single pixel ) .",
    "we also show a map of the variance var(@xmath68 ) of these beliefs , to gauge the weight of their contributions to the global illuminant estimate.,title=\"fig : \" ]",
    "while the empirical approach in the previous section would be optimal if pixel chromaticities in a typical image were infact i.i.d .",
    ", that is clearly not the case .",
    "therefore , in this section we propose an alternate approach method to setting the beliefs in @xmath47 $ ] , that optimizes for the accuracy of the final global illuminant estimate .",
    "however , unlike previous color constancy methods that explicitly model statistical co - dependencies between pixels  for example , by modeling spatial derivatives  @xcite , or learning functions on whole - image histograms  @xcite  we retain the overall parametric `` form '' by which we compute the illuminant in .",
    "therefore , even though @xmath47 $ ] itself is learned through knowledge of co - occurence of chromaticities in natural images , estimation of the illuminant during inference is still achieved through a simple aggregation of per - pixel distributions .",
    "specifically , we set the entries of @xmath47 $ ] to minimize a cost function @xmath69 over a set of training images : @xmath70 where @xmath71 is the true illuminant chromaticity of the @xmath72 training image , and @xmath73 is computed from the observed colors @xmath74 using .",
    "we augment the training data available to us by `` re - lighting '' each image with different illuminants from the training set .",
    "we use the original image set and six re - lit copies for training , and use a seventh copy for validation .",
    "we use stochastic gradient descent to minimize .",
    "we initialize @xmath75 to empirical values as described in the previous section ( for convenience , we multiply the empirical values by @xmath56 , and then set @xmath76 for computing @xmath62 ) , and then consider individual images from the training set at each iteration .",
    "we make multiple passes through the training set , and at each iteration , we randomly sub - sample the pixels from each training image .",
    "specifically , we only retain @xmath77 of the total pixels in the image by randomly sub - sampling @xmath78 patches at a time .",
    "this approach , which can be interpreted as being similar to `` dropout ''  @xcite , prevents over - fitting and improves generalization .",
    "derivatives of the cost function @xmath79 with respect to the current values of beliefs @xmath47 $ ] are given by @xmath80 } & = \\frac{1}{n}\\sum_i \\left(\\sum_n \\delta\\left(g(\\mathbf{v}^t(n),\\hat{\\mathbf{m}}_i ) = \\hat{\\mathbf{x}}\\right)\\delta\\left(y^t(n ) = y\\right)\\right ) \\times \\frac{\\partial c^t}{\\partial l^t_i},\\\\ \\mbox{where~~}\\frac{\\partial c^t}{\\partial l^t_i } & = p_i^t\\left(\\cos^{-1}(\\hat{\\mathbf{m}}_i^t\\hat{\\mathbf{m}}^t ) - c^t\\right).\\end{aligned}\\ ] ] we use momentum to update the values of @xmath47 $ ] at each iteration based on these derivative as @xmath81 = l[\\hat{\\mathbf{x}},y ] - l^\\nabla[\\hat{\\mathbf{x}},y],\\quad l^\\nabla[\\hat{\\mathbf{x}},y ] = r\\frac{\\partial c^t}{\\partial l[\\hat{\\mathbf{x}},y ] } + \\mu l^\\nabla_*[\\hat{\\mathbf{x}},y],\\ ] ] where @xmath82 $ ] is the previous update value , @xmath83 is the learning rate , and @xmath84 is the momentum factor . in our experiments ,",
    "we set @xmath85 , run stochastic gradient descent for 20 epochs with @xmath86 , and another 10 epochs with @xmath87 .",
    "we retain the values of @xmath75 from each epoch , and our final output is the version that yields the lowest mean illuminant estimation error on the validation set .",
    "we show the belief values learned in this manner in fig .",
    "[ fig : hdist](c ) .",
    "notice that although they retain the overall biases towards desaturated colors and combined green - red and green - blue hues , they are less `` smooth '' than their empirical counterparts in fig .  [",
    "fig : hdist](b)in many instances , there are sharp changes in the values @xmath47 $ ] for small changes in chromaticity . while harder to interpret , we hypothesize that these variations result from shifting beliefs of specific @xmath88 pairs to their neighbors , when they correspond to incorrect choices within the ambiguous set of specific observed colors .    [ [ experimental - results.-1 ] ] experimental results .",
    "+ + + + + + + + + + + + + + + + + + + + +    we also report errors when using these end - to - end trained versions of the belief function @xmath75 in table  [ tab : etab ] , and find that they lead to an appreciable reduction in error in comparison to their empirical counterparts .",
    "indeed , the errors with end - to - end training using three - fold cross - validation begin to approach those of the empirical version with ten - fold cross - validation , which has access to much more training data . also note that the most significant improvements ( for both three- and ten - fold cross - validation ) are in `` outlier '' performance , i.e. , in the @xmath89 and @xmath90-ile error values .",
    "color constancy methods perform worst on images that are dominated by a small number of materials with ambiguous chromaticity , and our results indicate that end - to - end training increases the reliability of our estimation method in these cases .",
    "we also include results for the end - to - end case for the example images in figure .",
    "[ fig : efig ] . for all three images , there is an improvement in the global estimation error .",
    "more interestingly , we see that the per - pixel error and variance maps now have more high - frequency variation , since @xmath75 now reacts more sharply to slight chromaticity changes from pixel to pixel .",
    "moreover , we see that a larger fraction of pixels generate fairly accurate estimates by themselves ( blue shirt in row 2 ) .",
    "there is also a higher disparity in belief variance , including within regions that visually look homogeneous in the input , indicating that the global estimate is now more heavily influenced by a smaller fraction of pixels .",
    "in this paper , we introduced a new color constancy method that is based on a conditional likelihood function for the true chromaticity of a pixel , given its luminance .",
    "we proposed two approaches to learning this function .",
    "the first was based purely on empirical pixel statistics , while the second was based on maximizing accuracy of the final illuminant estimate .",
    "both versions were found to outperform state - of - the - art color constancy methods , including those that employed more complex features and semantic reasoning . while we assumed a single global illuminant in this paper , the underlying per - pixel reasoning",
    "can likely be extended to the multiple - illuminant case , especially since , as we saw in fig .",
    "[ fig : efig ] , our method was often able to extract reasonable illuminant estimates from individual pixels .",
    "another useful direction for future research is to investigate the benefits of using likelihood functions that are conditioned on _ lightness_estimated using an intrinsic image decomposition method  instead of normalized luminance .",
    "this would factor out the spatially - varying scalar ambiguity caused by shading , which could lead to more informative distributions .              we adopt a standard approach to uniformly quantize unit vectors .",
    "consider a chromaticity unit vector @xmath91^t$ ] .",
    "we parametrize this vector in terms of @xmath92 , and @xmath93 .",
    "since the elements of @xmath31 are constrained to be positive , it follows that @xmath94 $ ] and @xmath95 $ ] .",
    "we uniformly quantize @xmath96 and @xmath97 in their respective domains , using @xmath98 bins for each when quantizing illuminant chromaticities to construct @xmath38 , and @xmath99 bins for true pixel chromaticities to define @xmath47 $ ] .",
    "[ [ b - black - level - offset - issues - with - results - of - exemplar - based - and - deep - cnn - methods ] ] b : black - level offset issues with results of exemplar - based  @xcite and deep - cnn methods  @xcite ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~    in this section , we compare our method to two recent semantic reasoning - based methods ",
    "exemplar - based  @xcite and deep - cnn  @xcite .",
    "while the illuminant estimates computed by these methods on the gehler - shi database were made available by their authors , unfortunately , they are based on training and testing with older incorrect versions of the ground - truth and intensity data for images from one of the cameras .",
    "specifically , the intensities in the image files from the canon 5d camera in the gehler - shi dataset includes a `` black - level '' offset of 129 , that needs to be subtracted from the intensities of all pixels in all color channels .",
    "this offset affects both the observed image data , as well as the ground truth computed from the color checker chart . while this was eventually clarified by the authors of @xcite ( and the ground - truth made available by them",
    "reflects this correction ) , there remain older versions of the ground truth without this correction , and the estimated illuminants for @xcite and @xcite made available are with respect to these old versions .",
    "we attempt to compare the performance of our method with that of @xcite in two ways .",
    "first , we generate corrected estimates @xmath100 from the illuminant chromaticities @xmath25 estimated by these methods , by `` subtracting '' the effect of the black level offset .",
    "this is done based on the true `` un - normalized '' ground truth illuminants @xmath26 ( i.e. , the color of the gray squares in the color chart ) as : @xmath101^t.\\ ] ] essentially , we rendered the color of the gray square by multiplying the luminance of non - corrected ground - truth with the estimated illuminant @xmath25 , and then subtracted the black level offset .",
    "we show the errors of these corrected illuminant estimates with respect to the actual ground truth in table [ tab : etab2a ] , and also copy the results of our method from table [ tab : etab ] .",
    "next , we do the reverse .",
    "we `` add '' the offset back to the illuminant estimates from our method : @xmath102^t,\\ ] ] and compare these to the non - corrected ground truth with respect to which the results of @xcite and @xcite are reported .",
    "table [ tab : etab2 ] provides this comparison .",
    "we find that the proposed method outperforms @xcite and @xcite in both comparisons .",
    "however , the quantiles reported in both table [ tab : etab2a ] and table [ tab : etab2 ] for @xcite should be interpreted only as a conservative estimate of their performance .",
    "while the corrections in , allowed us to report errors for these methods and ours with respect to a common ground truth , it does _ not _ correct for the fact that estimates for @xcite were computed on offset data . in particular",
    ", the presence of this offset means that the linear relationship between observed colors and the scene illuminant no longer holds .",
    "90%-ile + exemplar - based  @xcite & 3.66@xmath66&2.91@xmath66&3.04@xmath66&1.52@xmath66&4.81@xmath66&7.22@xmath66 + deep - cnn  @xcite & 3.45@xmath66&2.45@xmath66&2.65@xmath66&1.40@xmath66&4.29@xmath66&7.23@xmath66 +   +   + ( 3-fold)empirical & 2.89@xmath66&1.89@xmath66&2.15@xmath66&1.15@xmath66&3.68@xmath66&6.24@xmath66 + end - to - end trained&2.56@xmath66&1.67@xmath66&1.89@xmath66&0.91@xmath66&3.30@xmath66&5.56@xmath66 + ( 10-fold )  empirical&2.55@xmath66&1.58@xmath66&1.83@xmath66&0.85@xmath66&3.30@xmath66&5.74@xmath66 + end - to - end trained&2.20@xmath66&1.37@xmath66&1.53@xmath66&0.69@xmath66&2.68@xmath66&4.89@xmath66 +        exemplar - based  @xcite & 2.89@xmath66&2.27@xmath66&2.42@xmath66&1.29@xmath66&3.84@xmath66&5.68@xmath66 + deep - cnn  @xcite & 2.63@xmath66&1.98@xmath66&2.13@xmath66&1.16@xmath66&3.41@xmath66&5.46@xmath66 +   +   + ( 3-fold)empirical & 2.46@xmath66&1.55@xmath66&1.75@xmath66&0.89@xmath66&3.04@xmath66&5.05@xmath66 + end - to - end trained&2.21@xmath66&1.39@xmath66&1.54@xmath66&0.71@xmath66&2.65@xmath66&4.79@xmath66 + ( 10-fold )  empirical&2.18@xmath66&1.23@xmath66&1.46@xmath66&0.65@xmath66&2.73@xmath66&4.63@xmath66 + end - to - end trained&1.91@xmath66&1.11@xmath66&1.24@xmath66&0.54@xmath66&2.21@xmath66&4.08@xmath66 +"
  ],
  "abstract_text": [
    "<S> color constancy is the recovery of true surface color from observed color , and requires estimating the chromaticity of scene illumination to correct for the bias it induces . in this paper , we show that the per - pixel color statistics of natural scenes  without any spatial or semantic context </S>",
    "<S>  can by themselves be a powerful cue for color constancy . </S>",
    "<S> specifically , we describe an illuminant estimation method that is built around a `` classifier '' for identifying the true chromaticity of a pixel given its luminance ( absolute brightness across color channels ) . during inference , </S>",
    "<S> each pixel s observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants , and applying the classifier over these values yields a distribution over the corresponding illuminants . </S>",
    "<S> a global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels . </S>",
    "<S> we begin by simply defining the luminance - to - chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images . </S>",
    "<S> these histograms reflect a preference for hues corresponding to smooth reflectance functions , and for achromatic colors in brighter pixels . despite its simplicity </S>",
    "<S> , the resulting estimation algorithm outperforms current state - of - the - art color constancy methods . </S>",
    "<S> next , we propose a method to learn the luminance - to - chromaticity classifier `` end - to - end '' . using stochastic gradient descent , we set chromaticity - luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set . </S>",
    "<S> this leads to further improvements in accuracy , most significantly in the tail of the error distribution .    </S>",
    "<S> = 1 </S>"
  ]
}