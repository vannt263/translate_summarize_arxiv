{
  "article_text": [
    "in artificial neural networks , the issue of learning from examples has been one of the most attractive problems @xcite .",
    "traditionally emphasis has been put on the off - line ( or batch ) learning . in the off - line learning scenario ,",
    "the student sees a set of examples ( called a training set ) repeatly until the equilibrium is reached .",
    "this learning scenario can be analyzed in the framework of equilibrium statistical mechanics based on the energy cost function which means student s total error for a training set or on other types of cost functions @xcite .",
    "however , recently , several important features of learning from examples were derived from the paradigm of on - line learning . in the on - line learning scenario ,",
    "the student sees each example only once and throws it out , and he never sees it again . in other words , at each learning stage , the student receives a randomly drawn example and is not able to memorize it .",
    "the most recent example is used for modifying the student weight vector only by a small amount .",
    "the on - line learning has an advantage over the off - line counterpart that it explicitly carries information about the current stage of achievement of the student as a function of the training time ( which is proportional to the number of examples ) . during these several years",
    ", many interesting results have been reported in relation to the on - line learning . among them",
    ", the generalization ability of multilayer networks is one of the central problems @xcite .",
    "multilayer neural networks are much more powerful machines for information representation than the simple perceptron .",
    "recently , the properties of neural networks with a non - monotonic transfer function have also been investigated by several authors @xcite .",
    "a perceptron with a non - monotonic transfer function has the same input - output relations as a multilayer neural network called the parity machine .",
    "this parity machine has one hidden layer composed of three hidden units ( the @xmath3 parity machine ) .",
    "the output of each unit is represented as @xmath4 , @xmath5 and @xmath6 , where @xmath7 . here",
    "@xmath8 is the @xmath9-dimensional synaptic connection vector and @xmath10 denotes the input signal .",
    "then the final output of this machine is given as the product @xmath11 .",
    "we regard this final output of the @xmath3 parity machine as the output of a perceptron with non - monotonic transfer function .",
    "recently , engel and reimers @xcite investigated the generalization ability of this non - monotonic perceptron following the off - line learning scenario .",
    "their results are summarized as follows ; for @xmath12 , there exists a poor generalization phase with a large generalization error .",
    "as the number of presented patterns increases , a good generalization phase appears after a first order phase transition at some @xmath13 .",
    "no studies have been made about the present system following the on - line learning scenario . in this paper",
    "we study the on - line learning process and the generalization ability of this non - monotonic perceptron by various learning algorithms .",
    "this paper is organized as follows . in the next section",
    "we introduce our model system and derive the dynamical equations with respect to two order parameters for a general learning algorithm .",
    "one is the overlap between the teacher and student weight vectors and the other is the length of the student weight vector . in sec .",
    "iii , we investigate the dynamics of on - line learning in the non - monotonic perceptron for the conventional perceptron learning and hebbian leaning algorithms . we also investigate the asymptotic form of the differential equations in both small and large @xmath13 limits and get the asymptotic behavior of the generalization error . in sec .",
    "iv we investigate the adatron learning algorithm and modify the conventional adatron algorithm . in this modification procedure",
    ", we improve the weight function of the adatron learning so as to adopt it according to the range of @xmath1 . in sec .",
    "v , we optimize the learning rate and the general weight function appearing in the on - line dynamics . as the weight function contains the variables unknown for the student , we average over these variables over distribution function unknown using the bayes formula .",
    "section vi contains concluding remarks .",
    "we investigate the generalization ability of the non - monotonic perceptrons for various learning algorithms .",
    "the student and teacher perceptron are characterized by their weight vectors , namely @xmath14 and @xmath15 with @xmath16 , respectively . for a binary input signal @xmath17 , the output",
    "is calculated by the non - monotonic transfer function as follows : + @xmath18   \\label{teacher}\\end{aligned}\\ ] ] for the teacher and + @xmath19 \\label{student}\\end{aligned}\\ ] ] for the student , where we define the local fields of the teacher and student as @xmath20 and @xmath21 , respectively .",
    "the on - line learning dynamics is defined by the following general rule for the change of the student vector under presentation of the @xmath22th example ; + @xmath23 well - known examples are the perceptron learning , @xmath24 , the hebbian learning , @xmath25 , and the adatron learning , @xmath26 .",
    "we rewrite the update rule , eq .",
    "( [ general ] ) , of @xmath27 as a set of differential equations introducing the dynamical order parameter describing the overlap between the teacher and student weight vectors @xmath28 ) with @xmath29 and by squaring both sides of the same equation , we obtain the dynamical equations in the limit of large @xmath22 and @xmath9 keeping @xmath30 finite as + @xmath31 and + @xmath32 here @xmath33 denotes the average over the randomness of inputs @xmath34 with @xmath35 as we are interested in the typical behavior under our training algorithm , we have averaged both sides of eqs .",
    "( [ dldagen ] ) and ( [ drdagen ] ) over all possible instances of examples .",
    "the gaussian distribution ( [ dist1 ] ) has been derived from the central limit theorem .",
    "the generalization error , which is the probability of disagreement between the teacher and the trained student , is represented as @xmath36 .",
    "after simple calculations , we obtain the generalization error as @xmath37 where we have set @xmath38 with @xmath39 .",
    "we would like to emphasize that the generalization error obtained above ( [ e2 ] ) is independent of the specific learning algorithm . in fig .",
    "1 , we plot @xmath40 for several values of @xmath1 .",
    "this figure tells us that the student can acquire a perfect generalization ability if he is trained so that @xmath41 converges to @xmath42 for all values of @xmath1 .",
    "we have confirmed also analytically that @xmath43 is a monotonically decreasing function of @xmath41 for any value of @xmath1 .",
    "we first investigate the performance of the on - line hebbian learning @xmath25 . we get the differential equations for @xmath44 and @xmath41 as follows + @xmath45/l   \\label{dldahebb1 } \\\\",
    "% % \\mbox{}\\frac{d r}{d \\alpha } & = & \\left [ -\\frac{r}{2}\\frac{2}{\\sqrt{2\\pi } } ( 1 - 2{\\delta})(1-r^{2})l\\right]/l^{2}.   \\label{drdahebb1}\\end{aligned}\\ ] ] to determine whether or not @xmath41 increases with @xmath13 according to @xmath1 , we approximate the differential equation for @xmath41 around @xmath46 as @xmath47 therefore we use @xmath48 for @xmath49 and @xmath50 for @xmath51 . when @xmath52 , we obtain @xmath53 and @xmath54 on the other hand , for @xmath51 we obtain @xmath55 and @xmath56 we see that the hebbian learning algorithms lead to the state @xmath57 for @xmath51 .",
    "we next investigate the on - line perceptron learning @xmath24 by solving the next differential equations numerically ; @xmath58/l \\label{dldaper } \\\\ % % % % % % \\mbox{}\\frac{d r}{d \\alpha } & = & [ -\\frac{1}{2}e(r)r+(f(r)r - g(r))l ] /l^{2 } \\label{drdaper}\\end{aligned}\\ ] ] where @xmath59 and @xmath60 . using the distribution ( [ dist1 ] )",
    "we can rewrite these functions as @xmath61 and @xmath62 where @xmath63 . in fig .",
    "2 we plot the change of @xmath41 and @xmath44 as learning proceeds under various initial conditions for the case of @xmath64 .",
    "we see that the student can reach the perfect generalization state @xmath65 for any initial condition .",
    "the @xmath41-@xmath44 flow in the opposite limit @xmath66 is shown in fig .",
    "apparently , for this case the student reaches the state with the weight vector opposite to the teacher , @xmath57 , after an infinite number of patterns are presented . from eqs .",
    "( [ teacher ] ) and ( [ student ] ) , we should notice that the case of @xmath66 is essentially different from the case of a simple perceptron .",
    "since the two limiting cases , @xmath64 and @xmath66 , follow different types of behavior , it is necessary to check what happens in the intermediate region .",
    "for this purpose , we first investigate the asymptotic behavior of the solution of eqs .",
    "( [ dldaper ] ) and ( [ drdaper ] ) near @xmath67 for large @xmath68 . using the notation @xmath48 , @xmath69 ,",
    "the asymptotic forms of @xmath43 , @xmath70 and @xmath71 are found to be @xmath72 substituting these expressions into the differential equations ( [ dldaper ] ) and ( [ drdaper ] ) , we obtain @xmath73^{2/3 } \\,{\\alpha}^{-2/3 } \\label{asmeps1 } \\\\ % % % % % % % % \\mbox{}l & = &   \\frac{1}{2\\sqrt{\\pi}}\\left ( \\frac{1 + 2{\\delta}}{1 - 2{\\delta } } \\right ) \\left [ \\frac{3\\sqrt{2}(1 - 2{\\delta})^{2}}{(1 + 2{\\delta } ) } \\right]^{1/3 } \\,{\\alpha}^{1/3}. \\label{asml1}\\end{aligned}\\ ] ] therefore , the generalization error is obtained from eq .",
    "( [ asme1 ] ) as @xmath74^{1/3}\\,{\\alpha}^{-1/3}.   \\label{asmge1}\\ ] ] the asymptotic form of @xmath44 , eq .",
    "( [ asml1 ] ) , shows that @xmath75 should satisfy @xmath76 or @xmath77 .",
    "the assumption of @xmath48 with @xmath69 thus fails if @xmath78 .",
    "this fact can be verified from eq .",
    "( [ drdaper ] ) expanded around @xmath46 as @xmath79 for @xmath51 , @xmath41 decreases with @xmath13 .",
    "therefore , we use the relation @xmath80 , instead of @xmath48 for @xmath51 .",
    "we then find the asymptotic form of the generalization error as @xmath81 \\frac{1}{\\sqrt{2\\pi{\\alpha } } } \\label{asmge2}\\ ] ] and @xmath44 goes to infinity as @xmath82 these two results , eqs .",
    "( [ asmge1 ] ) and ( [ asmge2 ] ) , confirm the difference in the asymptotic behaviors between the two cases of @xmath66 and @xmath64 .",
    "we have found that the hebbian and the conventional perceptron learning algorithms lead to the state @xmath57 for @xmath83 .",
    "this anti - learning effect may be understood as follows . if the student perceptron has learned only one example by the hebb rule",
    ", @xmath84 then the output of the student for the same example is @xmath85 this relation indicates the anti - learning effect for the @xmath66 case .",
    "similar analysis holds for the perceptron learning .      in this section",
    ", we introduce a multiplicative factor @xmath86 in front of the perceptron learning function , @xmath87 , and investigate how the generalization ability depends on the parameter @xmath88 . in particular , we are interested in whether or not an optimal value of @xmath88 exists .",
    "the learning dynamics is therefore @xmath89 the case of @xmath90 corresponds to the conventional perceptron learning algorithm . on the other hand ,",
    "the case of @xmath91 and @xmath92 corresponds to the conventional adatron learning . using the above learning dynamics",
    ", we obtain the differential equations with respect to @xmath44 and @xmath41 as @xmath93 \\label{dlda3 } \\\\ % % % % % % % % \\mbox{}\\frac{d r}{d \\alpha } & = & \\frac{1}{l^{2 } } \\left [ -\\frac{r}{2}e_{g}(r)+ ( f_{g}(r)r - g_{g}(r))l \\right ] ,   \\label{drda3}\\end{aligned}\\ ] ] where @xmath94 , @xmath95 and @xmath96 are represented as @xmath97 and @xmath98 let us first investigate the behavior of the @xmath41-@xmath44 flow near @xmath46 . when @xmath41 is very small , the right - hand side of eq .",
    "( [ drda3 ] ) is found to be a @xmath88-dependent constant : @xmath99 where @xmath100 is the gamma function . as the right hand side of eq .",
    "( [ asm ] ) is positive for any @xmath88 as long as @xmath1 satisfies @xmath52 , @xmath41 increases around @xmath46 only for this range of @xmath1 .",
    "thus the generalized perceptron learning algorithm succeeds in reaching the desired state @xmath65 , not the opposite one @xmath57 , only for @xmath52 , similarly to the conventional perceptron learning .",
    "therefore , in this section we restrict our analysis to the case of @xmath52 and investigate how the learning curve changes according to the value of @xmath88 . using the notation @xmath48 ( @xmath69 )",
    ", we obtain the asymptotic forms of @xmath101 , @xmath102 and @xmath103 as follows .",
    "@xmath104 where @xmath105 , @xmath106 , @xmath107 and @xmath108 .",
    "we first investigate the case of @xmath109 ( finite @xmath1 ) , namely , @xmath110 .",
    "the differential equations ( [ dlda3 ] ) and ( [ drda3 ] ) are rewritten in terms of @xmath111 and @xmath112 as @xmath113 + { \\delta}^{2 } \\left [ c_{3}{\\varepsilon}^{1+\\frac{\\gamma}{2 } } -c_{4}{\\varepsilon } \\right ] \\label{asmdlda2 } \\\\ % % % % \\frac{d \\varepsilon}{d \\alpha } & = &   \\frac{{\\delta}^{2}}{2 } \\left [ c_{1}{\\varepsilon}^{\\gamma+\\frac{1}{2 } } + c_{2}{\\varepsilon}^{\\frac{1}{2 } } \\right ] -{\\delta } \\left [ \\left ( \\frac{2+\\gamma}{1+\\gamma } \\right ) c_{3}{\\varepsilon}^{1+\\frac{\\gamma}{2 } } -2c_{4}{\\varepsilon } \\right ] .",
    "\\label{asmdrda2}\\end{aligned}\\ ] ] as @xmath90 corresponds to the perceptron learning , we now assume @xmath114 . when @xmath115 , the terms containing @xmath116 and @xmath117 can be neglected in the leading order .",
    "dividing eq .",
    "( [ asmdlda2 ] ) by eq .",
    "( [ asmdrda2 ] ) , we obtain @xmath118 } { \\left [ c_{2}{\\delta}{\\varepsilon}^{1/2}/2 + 2c_{4}{\\varepsilon } \\right ] } .",
    "\\label{ddde}\\ ] ] if we assume @xmath119 or @xmath120 , eq . ( [ ddde ] ) is solved as @xmath121 , which is in contradiction to the assumption @xmath122 .",
    "thus , we set @xmath123 and determine @xmath124 and @xmath125 . substituting ( [ sol1 ] ) into ( [ ddde ] ) , we find @xmath126 ( @xmath127)and @xmath128 . the negative value of @xmath112 is not acceptable and we conclude that @xmath41 does not approach @xmath42 when @xmath115 .",
    "next we investigate the case of @xmath129 . using the same technique as in the case of @xmath130",
    ", we obtain @xmath131^{\\frac{2}{3 } } { \\alpha}^{-\\frac{2}{3 } } , \\\\ % % % % { \\delta } & = & \\frac{2c_{3}}{c_{1 } } \\left ( \\frac{\\gamma+2}{\\gamma+1 } \\right ) { \\varepsilon}^{\\frac{1}{2}(1-\\gamma ) } -\\frac{4c_{3}}{c_{1}(1-{\\gamma}^{2 } ) } { \\varepsilon}^{\\frac{1}{2}(3-\\gamma)}\\end{aligned}\\ ] ] and @xmath132^{\\frac{1}{3 } } { \\alpha}^{-\\frac{1}{3 } } \\nonumber \\\\ \\mbox { } & { \\equiv } & \\frac{\\sqrt{2}}{\\pi}(1 + 2{\\delta } ) f(\\gamma){\\alpha}^{-\\frac{1}{3}}.\\end{aligned}\\ ] ] we notice that @xmath88 should satisfy @xmath133 , because the prefactor of the leading term of @xmath134 , namely , @xmath135 , must be positive .",
    "as the prefactor of the generalization error increases monotonically from @xmath136 to @xmath90 , we obtain a smaller generalization error for @xmath88 closer to @xmath137 .",
    "next we investigate the case of @xmath92 , namely @xmath138 .",
    "we first assume @xmath139 in the limit of @xmath140 . in this solution",
    ", @xmath141 should be satisfied asymptotically .",
    "then , from eq .",
    "( [ asmdlda2 ] ) , the two terms @xmath142 and @xmath143 should be equal to each other , namely , @xmath144 , which leads to @xmath91 . the learning dynamics ( [ genpercep ] ) with @xmath92 and @xmath91 is nothing but the adatron learning which has already been investigated in detail @xcite .",
    "the result for the generalization error is @xmath145 if we choose @xmath146 as @xmath147 , and @xmath148 if we optimize @xmath146 to minimize the generalization error .",
    "we next assume @xmath149 as @xmath140 .",
    "it is straightforward to see that @xmath111 has the same asymptotic form as in the case of @xmath109 and @xmath129 .",
    "thus we have @xmath150 where @xmath151 is defined as @xmath152^{\\frac{1}{3 } } \\label{f2}\\ ] ] and @xmath88 can take any value within @xmath133 . from the above analysis , we conclude that the student can get the generalization ability @xmath2 if and only if @xmath92 and @xmath153 ( adatron ) . for other cases",
    "the generalization error behaves as @xmath0 , the same functional form as in the case of the conventional perceptron learning , as long as the student can obtain a vanishing residual error .",
    "therefore the learning curve has universality in the sense that it does not depend on the detailed value of the parameter @xmath88 .",
    "in this subsection , we investigate the generalization performance of the conventional adatron learning @xmath154 @xcite .",
    "the differential equations for @xmath44 and @xmath41 are given as follows : @xmath155 where @xmath156 and @xmath157 .",
    "after simple calculations , we obtain @xmath158 \\label{eadat}\\end{aligned}\\ ] ] and @xmath159 \\nonumber \\\\ \\mbox { } & + &   \\frac{2(1-r^{2})^{\\frac{3}{2}}}{\\pi } \\nonumber \\\\ \\mbox { } & { \\times } & { \\biggr [ } { \\delta}{\\exp}\\left[-\\frac{a^{2}r^{2}}{2(1-r^{2})}\\right ] -{\\delta}{\\exp}\\left[-\\frac{a^{2}(1+r)^{2}}{2(1-r^{2})}\\right ] -{\\delta}{\\exp}\\left[-\\frac{a^{2}(1-r)^{2}}{2(1-r^{2})}\\right ] \\nonumber \\\\ \\mbox { } & + & { \\exp}\\left[-\\frac{a^{2}}{2(1-r^{2})}\\right]-\\frac{1}{2 }   { \\biggr ] } \\label{gadat}\\end{aligned}\\ ] ] at first , we check the behavior of @xmath41 around @xmath46 . evaluating the differential equation ( [ drdaadat ] ) around @xmath46 ,",
    "we obtain @xmath160 from this result we find that for any value of @xmath1 , the flow of @xmath41 increases around @xmath46 . in fig .",
    "4 , we display the flows in the @xmath41-@xmath44 plane for several values of @xmath1 by numerical integration of eq .",
    "( [ drdaadat ] ) .",
    "this figure indicates that the overlap @xmath41 increases monotonically , but @xmath41 does not reach the state @xmath65 if @xmath1 is finite .",
    "this means that the differential equation ( [ drdaadat ] ) with respect to @xmath41 has a non - trivial fixed point @xmath161(@xmath162 ) if @xmath163 , which is the solution of the non - linear equation @xmath164 .",
    "therefore , we conclude that for @xmath64 and @xmath66 , we obtain the generalization error as @xmath165 , but the generalization error converges to a finite value exponentially for finite @xmath1 . in fig . 5 , we plot the corresponding generalization error .      in the previous subsection",
    ", we found that the on - line adatron learning fails to obtain the zero residual error for finite @xmath1 . in this subsection , we modify the adatron learning as @xmath166 with @xmath167 and see if the generalization ability of our non - monotonic system is improved .",
    "the motivation for the above choice comes from the optimization of the learning algorithm to be mentioned in the next section .",
    "details of derivation of eq .",
    "( [ adat ] ) are found in appendix a. then the differential equation with respect to @xmath41 is obtained as follows .",
    "@xmath168 where @xmath169 , @xmath170 and @xmath171 . to see the asymptotic behavior of the generalization error",
    ", we evaluate the leading - order contribution as @xmath41 approaches @xmath42 , @xmath48 , as @xmath172 substituting these expressions into the differential equation ( [ drdamod ] ) , we obtain @xmath173 and the generalization error as @xmath174 we should notice that the above result is independent of @xmath1 and the generalization ability of the student is improved by this modification for all finite @xmath1 .",
    "in the present subsection , we improve the conventional perceptron learning by introducing a time - dependent learning late @xcite .",
    "we consider the next on - line dynamics ; @xmath175 using the same technique as in the previous section , we can derive the differential equations with respect to @xmath44 and @xmath41 as follows .",
    "@xmath176 \\label{optdlda}\\\\ % % % % \\frac{d r}{d \\alpha } & = &   \\frac{1}{l^{2 } } \\left[-\\frac{r}{2}e(r)g(\\alpha)^{2}+g(\\alpha)(f(r)r - g(r))l \\right ] \\nonumber \\\\ \\mbox { } & { \\equiv } & l(g(\\alpha ) ) .",
    "\\label{optdrda}\\end{aligned}\\ ] ] the optimal learning rate @xmath177 is determined so as to maximize @xmath178 to accelerate the increase of @xmath41 .",
    "we then find @xmath179l}{re(r)}. \\label{optrate}\\ ] ] substituting this expression into the above differential equations , we obtain @xmath180[f(r)r+g(r)]l}{2r^{2}e(r ) } \\label{optdlda2}\\\\ % % % % % % % % \\frac{d r}{d l } & = & \\frac{[f(r)r - g(r)]^{2 } } { 2re(r)}. \\label{optdrda2}\\end{aligned}\\ ] ] we can obtain the asymptotic form of @xmath181 , @xmath44 and @xmath182 with the same technique of analysis as in the previous section ; @xmath183^{2 } { \\alpha}^{-2 } , \\\\",
    "% % % % l & = & { \\exp } \\left [ -16 \\left ( \\frac{1 + 2\\delta}{(1 - 2\\delta)^{2 } } \\right)^{4 } { \\alpha}^{-4 } \\right],\\end{aligned}\\ ] ] and @xmath184 { \\alpha}^{-1}.   \\label{gemod2}\\ ] ] therefore , the generalization ability has been improved from @xmath0 for @xmath185 to @xmath2 .",
    "the optimal learning rate @xmath177 behaves asymptotically as @xmath186 the factor @xmath187 of @xmath188 appearing in eq .",
    "( [ optrate ] ) is calculated by substituting @xmath70 and @xmath71 in eqs .",
    "( [ frper ] ) and ( [ grper ] ) as @xmath189 .",
    "thus , at @xmath190 , the optimal learning rate vanishes .",
    "therefore our formulation does not work at @xmath191 .",
    "as the optimal learning rate @xmath188 changes the sign at @xmath191 , from the arguments in section iii , we can see why the optimal learning rate can eliminate the anti - learning . in relation to this phenomenon at @xmath192 ,",
    "van den broeck @xcite recently investigated the same reversed - wedge perceptron which learns in the unsupervised mode from the distribution @xmath193 \\label{nondist}\\ ] ] with @xmath194 .",
    "for small @xmath13 , he found @xmath195 for the optimal on - line learning , where @xmath196 denotes the average over the distribution ( [ nondist ] ) .",
    "then he showed that at @xmath192 , the distribution ( [ nondist ] ) leads to @xmath197 and consequently @xmath198 . from this result , he concluded that as long as @xmath197 holds , any kind of on - line learning necessarily fails and the corresponding learning curve has a plateau .",
    "it seems that a similar mechanism may lead to a failure of the optimal learning at @xmath192 in our model .      in this subsection",
    "we try another optimization procedure by kinouchi and caticha @xcite .",
    "we choose the optimal weight function @xmath199 by differentiating the right hand side of eq .",
    "( [ drdagen ] ) with the aim to accelerate the increase of @xmath41 @xmath200 it is important to remember that @xmath201 contains some unknown information for the student , namely , the local field of the teacher @xmath202 .",
    "therefore , we should average @xmath201 over a suitable distribution to erase @xmath202 from @xmath201 . for this purpose",
    ", we transform the variables @xmath203 and @xmath202 to @xmath203 and @xmath204 @xmath205 then , the connected gaussian distribution @xmath206 is rewritten as @xmath207 we then obtain @xmath208 where @xmath209 stands for the averaging over the variable @xmath202 . substituting this into the differential equation ( [ drdagen ] ) ,",
    "we find @xmath210 let us now calculate @xmath211",
    ". for this purpose , we use the distribution @xmath212 .",
    "this quantity means the posterior probability of @xmath204 when @xmath213 and @xmath203 are given , where we have set @xmath214 .",
    "this conditional probability is rewritten by the bayes formula @xmath215 from which we can calculate @xmath211 as @xmath216 here @xmath217 is given as @xmath218 from the distribution @xmath219 .",
    "then , the denominator of eq .",
    "( [ bayes ] ) is calculated as @xmath220 where @xmath221 means the posterior probability of @xmath213 when the local field of the student @xmath203 is given . as we treat the binary output teacher , we obtain from eq .",
    "( [ omega ] ) @xmath222 in figs . 6 ( @xmath223 ) and 7 , ( @xmath224 ) , we plot @xmath225 for the cases of @xmath226 and @xmath227 . from these figures , we find that for any @xmath1 @xmath225 seems to reach @xmath228 as @xmath41 goes to @xmath229 .",
    "using the same technique , we can calculate @xmath230 and obtain @xmath231 substituting this into the right hand side of @xmath232 , eq . ( [ drdabayes2 ] ) , we obtain @xmath233 where @xmath234 stands for the averaging over the distribution @xmath235 . performing this average",
    ", we finally obtain @xmath236 where @xmath237^{2 } \\nonumber \\\\ \\mbox { } & { \\times } &   \\left [ \\frac{1}{h(-a_{1})-h(a_{2})+h(a_{3 } ) } + \\frac{1}{h(a_{1})+h(a_{2})-h(a_{3 } ) } \\right]\\end{aligned}\\ ] ] and @xmath238 , @xmath239 , @xmath240 .",
    "we plot the generalization error by numerically solving eqs .",
    "( [ dldaper]),([drdaper ] ) , ( [ optdlda2]),([optdrda2 ] ) , and ( [ drdabayes ] ) for the cases of @xmath64 in fig . 8 and @xmath241 in fig .",
    "9 . from these figures",
    ", we see that for the both cases of @xmath64 and @xmath242 , the generalization error calculated by the bayes formula converges more quickly to zero than by the optimal learning rate @xmath177 .",
    "recently , simmonetti and caticha @xcite introduced the on - line learning algorithm for the non - overlapping parity machine with general number of nodes @xmath243 . in their method , the weight vector of the student in each hidden unit is trained by the method in ref .",
    "in order to average over the internal fields of teacher in the differential equation with respect to the specific hidden unit @xmath244 of the student , they need the conditional probability which depends not only on the internal field of the unit @xmath244 but also on the internal field of the other units ( @xmath245 ) . this fact shows that their optimal algorithm is non - local . in our problem",
    ", the input - output relation of the machine can be mapped to those of a single layer reversed - wedge perceptron .",
    "therefore , it is not necessary for us to use the information about all units and our optimizing procedure leads to a local algorithm . in order to investigate the performance of the bayes optimization",
    ", we have calculated the asymptotic form of the generalization error from eq .",
    "( [ drdabayes ] ) and the result is @xmath246 for @xmath247 , where @xmath248 the generalization error is then given by eq .",
    "( [ asme1 ] ) as @xmath249 this asymptotic form of the generalization error agrees with the result of kinouchi and caticha @xcite .",
    "we notice that this form is independent of the width of the reversed wedge @xmath1 .",
    "we next mention the physical meaning of @xmath250 appearing in the differential equation ( [ drdabayes ] ) . as the rate of increase @xmath232 is proportional to @xmath250 , this quantity",
    "is regarded as the distribution of the gain which determines the increase of @xmath41 . therefore , @xmath250 yields important information about the strategy to make queries .",
    "a query means to restrict the input signal to the student , @xmath203 , to some subspace .",
    "kinzel and ruj@xmath251n suggested that if the student learns by the hebbian learning algorithm from restricted inputs , namely , inputs lying on the subspace @xmath252 , the prefactor of the generalization error becomes a half @xcite . in the present formulation ( [ drdabayes ] ) ,",
    "a query - making can be incorporated by inserting appropriate delta functions in the integrand .",
    "the learning process is clearly accelerated by choosing the peak position of @xmath250 as the location of these delta functions . in fig .",
    "10 we plot the distribution @xmath250 for @xmath253 ( top ) and @xmath254 ( bottom ) . from these figures",
    ", we learn that for large @xmath1 ( @xmath255 ) , the most effective example lies on the decision boundary ( @xmath252 ) at the initial training stage ( small @xmath41 ) .",
    "however , as the student learns , two different peaks appear symmetrically and in the final stage of training , the distribution has three peaks around @xmath252 and @xmath256 .",
    "on the other hand , for small @xmath1 ( @xmath257 ) , the most effective examples lie at the tails ( @xmath258 ) for the initial stage . in the final stage ,",
    "the distribution has two peaks around @xmath256 .",
    "therefore it is desirable to change the location of queries adaptively .",
    "we have investigated the generalization abilities of a non - monotonic perceptron , which may also be regarded as a multilayer neural network , a parity machine , in the on - line mode .",
    "we first showed that the conventional perceptron and hebbian learning algorithms lead to the perfect learning @xmath65 only when @xmath259 .",
    "the same algorithms yield the opposite state @xmath57 in the other case @xmath51 .",
    "these algorithms have originally been designed having the simple perceptron ( @xmath64 ) in mind , and thus are natural to give the opposite result for the reversed - output system ( @xmath260 ) .",
    "in contrast , the conventional adatron learning algorithm failed to obtain the zero residual error for all finite values of @xmath1 . for the unlearnable situation ( where the structures of the teacher and student are different ) , inoue and",
    "nishimori reported that the adatron learning converges to the largest residual error among the three algorithms @xcite .",
    "it is interesting that the adatron learning algorithm is not useful even for the learnable situation . in order to overcome this difficulty",
    ", we introduced several modified versions of the conventional learning rules .",
    "we first introduced the time - dependent learning rate into the on - line perceptron learning and optimize it . as a result",
    ", the generalization error converges to zero in proportion to @xmath2 except at @xmath192 where the learning rate becomes identically zero .",
    "we next improved the conventional adatron learning by modifying the weight function so that it changes according to the value of the internal potential @xmath203 of the student . by this modification ,",
    "the generalization ability of the student dramatically improved and the generalization error converges to zero with an @xmath1-independent form , @xmath261 .",
    "we also investigated a different type of optimization : we first optimized the weight function @xmath199 appearing in the on - line dynamics , not the rate @xmath262 .",
    "then , as the function @xmath263 contains the unknown variable @xmath202 , we averaged it over the distribution of @xmath202 using the well - known technique of the bayes statistics .",
    "this optimization procedure also provided other useful information for the student , namely , the distribution of most effective examples .",
    "kinzel and ruj@xmath251n @xcite reported that for the situation in which a simple perceptron learns from a simple perceptron ( the @xmath64 case ) , the hebbian learning with selected examples ( @xmath252 ) leads to faster convergence of the generalization error than the conventional hebbian learning .",
    "however , we have found that for finite values of @xmath1 , the most effective examples lie not only on the boundary @xmath252 but also on @xmath256 .",
    "furthermore , we could learn that for small values of @xmath1 and at the initial stage of learning ( @xmath41 small ) , the most effective examples lie on the tails ( @xmath258 ) .",
    "as the learning proceeds , the most effective examples change the locations to @xmath256 .",
    "this information is useful for effective query constructions adaptively at each stage of learning .",
    "we thank professor shun - ichi amari for useful discussions .",
    "one of the authors ( j.i . ) was a partially supported by the junior research associate program of riken .",
    "he also thanks professor c. van den broeck for useful discussion .",
    "was partially supported by a program `` research for the future ( rftf ) '' of japan society for the promotion of science .",
    "the authors thank the referee for a very careful reading and a number of constructive comments .",
    "in this appendix , we explain how we introduced the modified weight function @xmath264 appearing in the adatron learning algorithm in sec .",
    "iv b. from eqs .",
    "( [ fstar ] ) and ( [ averz ] ) in sec .",
    "v , the weight function using the bayes formula is written as @xmath265 as this expression contains the unknown parameter @xmath41 to the student , we try to find the suitable learning weight function which agrees with the asymptotic form of @xmath266 in the limit of @xmath267 @xcite . for this purpose , we investigate the asymptotic form of @xmath268 as follows .",
    "we consider the cases of @xmath269 and @xmath270 separately .",
    "+   + ( i)@xmath271 + using the relation @xmath272 , we find @xmath273 . \\label{asmomega1}\\end{aligned}\\ ] ] the asymptotic form of @xmath268 depends on the range of @xmath203 . for @xmath274 , the asymptotic form of @xmath268",
    "is @xmath275 therefore , @xmath276 .",
    "similarly , we find @xmath277 ( @xmath278 and @xmath279 ) , @xmath280 ( @xmath281 ) and @xmath282 ( @xmath283 ) .",
    "+ ( ii)@xmath270 + using the relation @xmath48 , we find for @xmath274 @xmath284 therefore , the weight function @xmath285 is @xmath286 asymptotically .",
    "similarly , we find @xmath277 ( @xmath287 and @xmath288 ) , @xmath280 ( @xmath289 ) and @xmath290 ( @xmath279 ) . from the results of ( i ) and",
    "( ii ) , we find the modified adatron learning algorithm as @xmath291 where @xmath292    s. amari , _ ieee trans . _ * ec-16 * 299 ( 1967 ) . j. a. hertz , a. krogh and r. g. palmer , _ introduction to the theory of neural computation _ ,",
    "redwood city : addison - wesley ( 1991 ) .",
    "t. h. l. watkin , a. rau and m. biehl , rev .",
    "phys . * 65 * 499 ( 1993 ) .",
    "m. opper and w. kinzel , in _ physics of neural networks iii _ , eds .",
    "e. domany , j. l. van hemmen and k. schulten , berlin : springer ( 1995 ) . m. griniasti and h. gutfreund , j. phys .",
    "a : math . gen . * 24 * , 715 ( 1991 ) .",
    "r. meir and j. f. fontanari , phys .",
    "a * 45 * , 8874 ( 1992 ) .",
    "o. kinouchi and n. caticha , phys .",
    "e * 54 * , r54 ( 1996 ) .",
    "y. kabashima , j. phys . a : math . gen . * 27 * , 1917 ( 1994 ) .",
    "h. sompolinsky , n. barkai and h. s. seung , _ neural networks : the statistical mechanics perspective _ , eds . j. h. oh , c. kwon and s. cho , world scientific ( 1995 ) .",
    "d. saad and s. a. solla , phys .",
    "e * 52 * 4225 ( 1995 ) .",
    "m. morita , s. yoshizawa and k. nakano , trans .",
    "j73-d - ii * , 242 ( 1993 ) ( in japanese ) h. nishimori and i. opris , neural networks * 6 * , 1061 ( 1993 ) .",
    "j. inoue , j. phys .",
    "a : math . gen . * 29 * , 4815 ( 1996 ) .",
    "m. morita , neural networks * 9 * , 1477 ( 1996 ) .",
    "g. boffetta , r. monasson and r. zecchina , j. phys .",
    "a : math . gen . * 26 * , l507 ( 1993 ) .",
    "r. monasson and d. okane , europhys . lett . * 27 * , 85 ( 1994 ) .",
    "a. engel and l. reimers , europhys .",
    "lett * 28 * , 531 ( 1994 ) .",
    "m. biehl and p. riegler , europhys .",
    "* 28 * , 525 ( 1994 ) .",
    "j. inoue and h. nishimori , phys .",
    "e * 55 * 4544 ( 1997 ) .",
    "j. inoue , h. nishimori and y. kabashima , j. phys .",
    "a : math . gen . * 30 * 3795 ( 1997 ) . c. van den broeck , _ proc . of theoretical aspects of neural computation 97 _ ( tanc-97 ) , to be published , springer - verlag . c. van den broeck and p. reimann , phys .",
    "lett . * 76 * , 2188 ( 1996 ) .",
    "o. kinouchi and n. caticha , j. phys .",
    "a : math . gen .",
    "* 26 * , 6243 ( 1992 ) .",
    "r. simonetti and n. caticha , j. phys .",
    "a : math . gen . * 29 * , 4859 ( 1996 ) .",
    "w. kinzel and p. rujn , europhys .",
    "* 13 * , 473 ( 1990 ) ."
  ],
  "abstract_text": [
    "<S> we investigate the generalization ability of a perceptron with non - monotonic transfer function of a reversed - wedge type in on - line mode . </S>",
    "<S> this network is identical to a parity machine , a multilayer network . </S>",
    "<S> we consider several learning algorithms . by the perceptron algorithm </S>",
    "<S> the generalization error is shown to decrease by the @xmath0-law similarly to the case of a simple perceptron in a restricted range of the parameter @xmath1 characterizing the non - monotonic transfer function . </S>",
    "<S> for other values of @xmath1 , the perceptron algorithm leads to the state where the weight vector of the student is just opposite to that of the teacher . </S>",
    "<S> the hebbian learning algorithm has a similar property ; it works only in a limited range of the parameter </S>",
    "<S> . the conventional adatron algorithm does not give a vanishing generalization error for any values of @xmath1 . </S>",
    "<S> we thus introduce a modified adatron algorithm which yields a good performance for all values of @xmath1 . </S>",
    "<S> we also investigate the effects of optimization of the learning rate as well as of the learning algorithm . </S>",
    "<S> both methods give excellent learning curves proportional to @xmath2 . </S>",
    "<S> the latter optimization is related to the bayes statistics and is shown to yield useful hints to extract maximum amount of information necessary to accelerate learning processes .    </S>",
    "<S> pacs numbers : 87.10.+e </S>"
  ]
}