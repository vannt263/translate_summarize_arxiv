{
  "article_text": [
    "we study the capacity of the discrete - time , peak - and - average - power - limited , gaussian channel when its output is quantized using a dithered , infinite - level , uniform quantizer of step size @xmath0 and analyze its behavior in the low- and high - precision limit , where @xmath0 tends to infinity and zero , respectively .",
    "the problem of quantization arises in communication systems where the receiver uses digital signal processing techniques , so the analog received signal must be sampled and then quantized using an analog - to - digital converter ( adc ) .",
    "if the received signal is sampled at nyquist rate or above , and if an adc with high precision is employed , then the effects of sampling and quantization are negligible",
    ". however , high - precision adcs may not be practical when the bandwidth of the system is large and the sampling - rate is high @xcite . in such scenarios , low - resolution adcs must be used .    to better understand what communication rates can be achieved with",
    "low - resolution adcs and nyquist sampling , various works have studied the discrete - time gaussian channel when its output is quantized using a 1-bit quantizer . at low signal - to - noise ratio ( snr ) , where communication at low spectral efficiencies takes place , it is known that a symmetric threshold quantizer if its input is above a threshold , and it produces @xmath1 if its not . a _ symmetric _",
    "threshold quantizer is a threshold quantizer whose threshold is zero . ]",
    "reduces capacity by a factor of @xmath2 , corresponding to a 2 db power loss @xcite , @xcite .",
    "hence the rule of thumb that  hard decisions cause a 2 db power loss . \" it was recently demonstrated that this power loss can be avoided by using asymmetric threshold quantizers and asymmetric signal constellations @xcite .",
    "however , this result requires _ flash - signaling _ input distributions ( * ? ? ?",
    "3 ) ( see ( * ? ? ?",
    "2 ) for a definition ) . since such inputs are known to have a poor spectral efficiency ( * ? ?",
    "16 ) , it follows that for small yet positive spectral efficiencies , the potential power gain is significantly smaller than 2 db .",
    "for example , at spectral efficiencies of 0.001 bits / s / hz , allowing for asymmetric quantizers with corresponding asymmetric signal constellations provides a power gain of merely 0.1 db ( * ? ? ?",
    "v ) .    in the following ,",
    "we refer the gaussian channel with ( @xmath3-bit ) output quantization as the _",
    "( @xmath3-bit ) quantized gaussian channel _ and to the gaussian channel without output quantization simply as the _",
    "gaussian channel_. for the gaussian channel , binary antipodal inputs outperform flash - signaling inputs in terms of spectral efficiency ( * ? ? ?",
    "however , for such inputs quantizing the channel output with a 1-bit quantizers incurs again a 2 db power loss at low snr , since in this case a symmetric threshold quantizer becomes asymptotically optimal as the snr tends to zero ( * ? ? ?",
    "* prop .  2 ) .",
    "recalling that the discrete - time gaussian channel arises from the continuous - time , bandlimited , additive white gaussian noise ( awgn ) channel by sampling the output at nyquist rate , it can be shown that , for binary antipodal signaling and a symmetric threshold quantizer , the 2 db power loss can be reduced by sampling the channel output above the nyquist rate .",
    "for instance , it was demonstrated that , at low snr , sampling the output at twice the nyquist rate improves the power loss from 2 db for nyquist sampling to less than 1.28 db ( * ? ? ? * th .  1 ) ,",
    "* th .  1 ) .",
    "further results on the capacity of the 1-bit quantized gaussian channel and super - nyquist sampling include @xcite@xcite . specifically , zhang @xcite studies the generalized mutual information of this channels for a gaussian codebook ensemble and the nearest - neighbor decoding rule and demonstrates _",
    "inter alia _ that , as the sampling rate tends to infinity , the power loss is not larger than 0.98 db .",
    "shamai @xcite considers the noiseless case and demonstrates that the capacity is unbounded in the sampling rate .",
    "however , it is unknown whether for a symmetric threshold quantizer the power loss can be fully avoided by letting the sampling rate tend to infinity .    going beyond 1-bit quantizers",
    ", it was shown that , at low snr , a uniform 3-bit quantizer and binary antipodal signaling achieves about 95% of the capacity of the gaussian channel , corresponding to a power loss of merely 0.223 db ( * ? ? ?",
    "* eq .  ( 3.4.21 ) ) .",
    "the capacity of the @xmath3-bit quantized gaussian channel was studied , e.g. , in @xcite .",
    "the numerical results obtained in @xcite suggest that , at 0 db snr , a 2-bit quantizer achieves still 95% of the capacity of the gaussian channel , while at 20 db snr , a 3-bit quantizer achieves still 85% of the capacity of gaussian channel .",
    "however , to the best of our knowledge , there exists no closed - form expression for the capacity of the @xmath3-bit quantized gaussian channel , except for the binary case where the channel output is quantized using a symmetric threshold quantizer ( * ? ? ?",
    "* th .  2 ) .    a ubiquitous quantizer is the _ uniform quantizer _ , whose levels are equispaced , say @xmath0 apart , either with an infinite or a finite number of levels .",
    "we refer to @xcite for a comprehensive survey of quantization theory . for finite - level uniform quantizers",
    ", the outermost cells will be semi - infinite and the input space corresponding to these cells is referred to as the _ overload region _ @xcite .",
    "while infinite - level uniform quantizers need an infinite number of bits to describe their output and seem therefore impractical , they have the advantage of eliminating the overload region and resulting overload distortion ( * ? ? ?",
    "for this reason , infinite - level uniform quantizers are typically preferred in theoretical analyses , in the hope that the tail of the source to be quantized decays sufficiently fast so the overload distortion be negligible . by shannon s source coding theorem @xcite , irrespective of the number of levels",
    ", the output of a uniform quantizer can be described by a variable - length code whose expected length is roughly the entropy of the quantizer output .",
    "consequently , the rate of a quantizer is often measured by the entropy of its output .    the step size @xmath0 of the uniform quantizer determines its precision : the smaller @xmath0 , the higher the precision .",
    "the high - precision limit ( where @xmath4 ) was studied by gish and pierce @xcite , who showed that the difference between the entropy of the output of an infinite - level uniform quantizer and the rate distortion function converges to @xmath5 as the permitted distortion ( and hence also @xmath0 ) vanishes .",
    "as for the low - precision limit ( where @xmath6 ) , it was shown that for exponential , laplacian , and gaussian sources the entropy of the quantizer output approaches zero with the same slope as the rate - distortion function as the allowed distortion tends to the source variance , whereas for uniform sources the slope of the entropy of the quantized output becomes infinite , in contrast to the rate - distortion function which has a finite slope @xcite@xcite . to prove their result for gaussian sources @xcite , marco and neuhoff showed that , in the low - precision limit , the entropy of the quantizer output is determined by the probabilities corresponding to the innermost cells , which is in agreement with the intuition that if the tail of the source decays sufficiently fast , then the overload distortion can be neglected ( * ? ? ?",
    "* lemma 3 ) .",
    "a common strategy to further simplify the theoretical analysis of uniform quantizers is _",
    "dithering_. ( we refer again to ( * ? ? ?",
    "v - e ) for a survey of this topic . ) in a dithered quantizer , instead of quantizing an input signal directly , one quantizes the sum of the signal and a random process ( called a _ dither _ ) that is independent of the signal .",
    "this allows one to describe the quantization noise by additive uniform noise that is independent of the input signal . specifically , if the dither is uniformly distributed over @xmath7 $ ] , then the conditional entropy of the quantizer output given the dither is equal to the mutual information between the quantizer input and the sum of the input and independent , uniformly distributed noise ( * ? ? ?",
    "dithered quantization was studied in numerous works .",
    "of particular interest to us is the work by zamir and feder @xcite , which studied the rate - distortion behavior when a bandlimited stationary source is first sampled at nyquist rate or faster , then it undergoes dithered uniform quantization , and finally it is entropy - encoded .",
    "generalizations of dithered quantization can be found , e.g. , in @xcite , @xcite .",
    "observe that analyses of the capacity of the quantized gaussian channel are motivated by the need for low - resolution quantizers and therefore typically consider quantizers with a small number of levels .",
    "however , the analysis of such quantizers becomes intractable as quantizer resolution and/or sampling rate increase .",
    "in contrast , theoretical work on quantization often considers infinite - level uniform quantizers , since they allow for a simplified analysis . in this paper",
    ", we bring together these two lines of research by studying the capacity of the gaussian channel when its output is quantized using a dithered , infinite - level , uniform quantizer of step size @xmath0 .",
    "( we shall refer to this channel as the _ dither - quantized gaussian channel_. ) since a dithered quantizer can be described as an additive noise channel with uniform noise , the dither - quantized gaussian channel is equivalent to an additive noise channel where the noise is the sum of a gaussian and a uniform random variable .",
    "this simplifies the analysis of its capacity . while beyond the scope of this paper",
    ", we hope that , in the long term , studying the capacity of the dither - quantized gaussian channel will help us better understand the tradeoff in channel capacity between sampling rate and quantization resolution of the continuous - time , bandlimited , awgn channel .",
    "the rest of this paper is organized as follows .",
    "section  [ sec : channel ] introduces the channel model and defines the capacity as well as the low - snr asymptotic capacity .",
    "section  [ sec : capacity ] presents the results ( as well as the proofs thereof ) that concern channel capacity .",
    "section  [ sec : lowsnr ] presents the results ( as well as the proofs thereof ) that concern the low - snr asymptotic capacity .",
    "section  [ sec : conclusion ] concludes the paper with a summary and a discussion of our results .",
    "[ fig : system ]    we consider the discrete - time communication system depicted in figure  1 .",
    "a message @xmath8 , which is uniformly distributed over the set @xmath9 , is mapped by an encoder to the length-@xmath10 real sequence @xmath11 of channel inputs .",
    "( here , @xmath12 denotes the set of real numbers . )",
    "the channel corrupts this sequence by adding gaussian noise to produce the unquantized output sequence @xmath13 where @xmath14 is a sequence of independent and identically distributed ( i.i.d . ) gaussian random variables of mean zero and variance @xmath15 .",
    "( here , @xmath16 denotes the set of integers . )",
    "the unquantized sequence is then quantized using a dithered , infinite - level , uniform quantizer of step size @xmath0 .",
    "specifically , the quantizer is a function @xmath17 that produces @xmath18 if @xmath19 , i.e. , @xmath20 where , for every @xmath21 , @xmath22 denotes the largest integer not larger than @xmath23 . , @xmath24 ,",
    "since this choice minimizes the expected squared error .",
    "for ease of exposition , we use the slightly simpler definition . in any case",
    ", the actual reproduction values do not affect the achievable information rates . ] the quantizer output @xmath25 is given by @xmath26 where @xmath27 is a sequence of i.i.d .",
    "random variables that are uniformly distributed over the interval @xmath7 $ ] , referred to as _",
    "dither_. we assume that channel input , additive gaussian noise , and dither are independent .",
    "the decoder observes the quantizer output @xmath28 as well as the dither @xmath29 and guesses which message was transmitted .",
    "we impose both an average - power and a peak - power constraint on the channel inputs : for every realization of @xmath8 , the sequence @xmath30 must satisfy @xmath31 the capacity of the dither - quantized gaussian channel  under the power constraints @xmath32 and @xmath33 on the channel inputs is given by ( * ? ? ?",
    "7.3 ) @xmath34 where the supremum is over all distributions of @xmath35 satisfying @xmath36 and @xmath37 with probability one . here and throughput the paper , we omit the time indices where they are immaterial .",
    "when the peak - power constraint is relaxed ( @xmath38 ) , we shall denote the capacity by @xmath39 . in an analogous manner , we shall denote the capacity of the gaussian channel under the power constraints @xmath32 and @xmath40 by @xmath41 , i.e. , @xmath42 where the supremum is over all distributions of @xmath35 satisfying @xmath36 and @xmath37 with probability one .",
    "we shall omit the second argument when the peak - power constraint is relaxed , i.e. , @xmath43 . by the data processing inequality ( * ? ? ?",
    "2.8.1 ) , @xmath44 while it is well - known that the input distribution achieving @xmath41 is discrete @xcite , to the best of our knowledge , there exists no closed - form expression for @xmath41 .",
    "nevertheless , by relaxing the peak - power constraint , we obtain for every @xmath32 and @xmath40 @xcite @xmath45 here and throughout this paper , @xmath46 denotes the natural logarithm function .",
    "( consequently , all rates are in nats per channel use . ) in section  [ sub : main_capacity ] , we demonstrate that the inequality in becomes tight as @xmath4 and that @xmath47 tends to zero as @xmath6 .",
    "since a dithered quantizer can be described as an additive noise channel with uniform noise @xmath48 , the dither - quantized gaussian channel is equivalent to an additive noise channel with noise @xmath49 . indeed , following the proof of theorem  1 in @xcite",
    ", we show in appendix  [ app : dither_add_noise ] that the mutual information on the right - hand side ( rhs ) of is equal to @xmath50 where the probability density function ( pdf ) of the additive noise @xmath51 is the convolution of the gaussian and the uniform pdf : @xmath52.\\ ] ] here @xmath53 denotes the _ gaussian probability integral _ ( @xmath54-function )",
    "* eq .  ( 1.3 ) ) .",
    "in addition to capacity , we also study the slope of the capacity - vs - power curve at zero when either the peak - power constraint is relaxed ( @xmath38 ) or when the peak - to - average - power ratio @xmath55 is finite and held fixed , i.e. , @xmath56 and @xmath57 we shall refer to the slope of the capacity - vs - power curve at zero as the _ low - snr asymptotic capacity_.    relaxing the peak - power constraint allows for a simple expression for @xmath58 ( * ? ? ?",
    "3 ) : @xmath59 where @xmath60 denotes relative entropy and @xmath61 denotes the conditional distribution of @xmath62 given @xmath63 .",
    "unfortunately , @xmath58 may characterize @xmath39 only at impractically small input powers @xmath32 .",
    "indeed , if the supremum on the rhs of is approached only as @xmath64 ( as is the case for the 1-bit quantized gaussian channel ( * ? ? ?",
    "3 ) ) , then the input distribution that achieves the first derivative of @xmath39 at zero ( i.e. , @xmath58 ) must be flash signaling , which implies that the second derivative of @xmath39 at zero is @xmath65 @xcite . consequently , in such cases , @xmath58 describes the behavior of @xmath39 poorly , unless @xmath32 is very small .    to address this problem ,",
    "we consider also the case where the peak - to - average - power ratio @xmath66 is finite and held fixed , thereby precluding the use of flash signaling input distributions . in this case",
    ", it was demonstrated that if the channel law satisfies a number of technical conditions , then the low - snr asymptotic capacity is given by @xcite , @xcite @xmath67 where @xmath68 denotes the fisher information @xmath69 ^ 2}{f_{z_{\\delta}}(y - x ) } \\d y.\\ ] ]    by and , and by noting that relaxing the peak - power constraint does not reduce capacity , it follows that @xmath70 in section  [ sub : main_lowsnr ] , we demonstrate that the right - most inequality holds with equality irrespective of @xmath0 , while the left - most inequality holds with equality if , and only if , @xmath0 vanishes .",
    "in this section , we study the capacity for arbitrary input powers @xmath32 in the high- and low - resolution limit , i.e. , when @xmath4 and @xmath6 , respectively .",
    "we show that in the former case , the capacity @xmath47 converges to that of the gaussian channel , and in the latter case , it converges to zero .",
    "[ thm : cap_highres ] consider the dither - quantized gaussian channel described in section  [ sec : channel ] .",
    "then , for any distribution on @xmath35 satisfying @xmath36 , @xmath71    recall that @xmath49 . to prove theorem  [ thm : cap_highres ]",
    ", it thus suffices to show that    rcl _ 0 h(x+n+u _ ) & = & h(x+n ) [ eq : thm_highres1 ] + _ 0 h(n+u _ ) & = & h(n).[eq : thm_highres2 ]    since @xmath72 is gaussian and @xmath35 and @xmath72 are independent , the differential entropies on the rhs of and are both finite .",
    "furthermore , @xmath73 , @xmath74 and , by the theorem s assumption , @xmath75 . the above identities and follow therefore directly by specializing the proof of theorem  1 in @xcite to the distortion measures @xmath76 .    equation   holds for any input distribution satisfying the average - power constraint @xmath32 , including the capacity - achieving input - distribution .",
    "consequently , theorem  [ thm : cap_highres ] implies that the inequality in becomes tight as @xmath4 .",
    "[ cor : highres ] consider the dither - quantized gaussian channel described in section  [ sec : channel ] .",
    "then , for every @xmath32 and @xmath40 , @xmath77    in view of , it suffices to show that @xmath78 where @xmath79 denotes the _ limit inferior_. to this end , we use that , by theorem  [ thm : cap_highres ] , we have for any distribution of @xmath35 satisfying @xmath36 and @xmath37 with probability @xmath80 the lower bound , and hence corollary  [ cor : highres ] , follows by maximizing the rhs of over all distributions of @xmath35 satisfying the power constraints @xmath32 and @xmath40 .",
    "theorem  [ thm : cap_highres ] and corollary  [ cor : highres ] demonstrate that , in the high - resolution limit , the dithered quantizer incurs no loss in capacity . as we show next ,",
    "this is in stark contrast to the low - resolution limit .",
    "[ thm : cap_lowres ] consider the dither - quantized gaussian channel described in section  [ sec : channel ] .",
    "then , for every @xmath32 and @xmath40 , @xmath81    see section  [ sec : cap_lowres ] .",
    "let the signal - to - noise - and - quantization - noise - ratio ( snqnr ) of the dither - quantized gaussian channel be defined as @xmath82 theorem  [ thm : cap_lowres ] is perhaps not very surprising since the snqnr tends to zero as @xmath0 tends to infinity , so it may seem plausible that also the capacity vanishes in the low - resolution limit .",
    "however , note that the additive noise @xmath51 is non - gaussian , so it is _ prima facie _ unclear whether there is any relation between capacity and snqnr .",
    "the weak performance of the dithered , infinite - level , uniform quantizer at low quantizer resolutions is due to the dither .",
    "indeed , when the output of the gaussian channel is quantized using a symmetric threshold quantizer , the capacity is given by ( * ? ? ?",
    "2 ) @xmath83 the rhs of is strictly positive , so this implies that @xmath84 .",
    "moreover , since the concatenation of an infinite - level , uniform quantizer and a symmetric threshold quantizer results again in a threshold quantizer , it follows that the _ undithered _ uniform quantizer achieves a capacity that is at least as large as the capacity achieved by the 1-bit quantizer .",
    "consequently , adding dither is highly detrimental in the low - resolution regime .",
    "as we shall see , the same is also true for the low - snr asymptotic capacity , unless the peak - to - average - power ratio is unbounded .",
    "we first note that @xmath85 has the same distribution as @xmath48 .",
    "recalling that @xmath49 , it thus follows that @xmath86 we then prove theorem  [ thm : cap_lowres ] by showing that @xmath87 where the supremum is over all distributions of @xmath35 satisfying @xmath36 and @xmath37 with probability one .",
    "to prove , we will follow the steps that were carried out in ( * ? ? ?",
    "ii ) to derive an upper bound on the capacity of the peak - and - average - power - limited complex gaussian channel .",
    "specifically , we use the upper bound on the mutual information ( * ? ? ?",
    "5.1 ) @xmath88 where @xmath53 denotes the input distribution ; @xmath89 denotes the conditional distribution of the channel output , conditioned on @xmath63 ; and @xmath90 denotes some arbitrary distribution on the output alphabet .",
    "every choice of @xmath90 yields an upper bound on @xmath91 , and the inequality in holds with equality if @xmath90 is the actual distribution of @xmath92 induced by @xmath53 and @xmath93 . here",
    ", we choose @xmath90 such that its pdf is @xmath94 for some @xmath95 and @xmath96 , where @xmath97 is a normalizing constant , i.e. , @xmath98 \\label{eq : thm_cap_lowres_2.2}\\ ] ] and @xmath99 denotes the inverse tangent function .",
    "combining with , and using that conditioning does not increase entropy , we obtain upon substituting @xmath100    lcl i(x;(x+n)+u_1 ) & = & - h((x+n)+u_1|x ) - + & & -h((x+n)+u_1| x , n ) - + & = & - [ eq : thm_cap_lowres_2.5 ]    where the last step follows because @xmath101 is independent of @xmath102 , so ( * ? ?",
    "9.6.3 ) and the expression for the differential entropy of a uniform random variable give @xmath103 we next evaluate    lcl + & = & + ( |y| > ) + [ eq : thm_cap_lowres_3 ]    where @xmath104 denotes the indicator function .",
    "when @xmath105 , then is equal to @xmath106 and  give @xmath107\\right ) .",
    "\\label{eq : thm_cap_lowres_3.7}\\ ] ] in the following , we consider the case where @xmath108 . by the triangle inequality ,",
    "the absolute value of @xmath109 is upper - bounded by @xmath110 .",
    "furthermore , @xmath111 .",
    "consequently , @xmath112 where the right - most inequality follows by chebyshev s inequality ( * ? ? ?",
    "* ( 4.10.7 ) , p.  192 ) and because , for every @xmath35 satisfying @xmath113 , we have @xmath114 . for ease of exposition",
    ", we define @xmath115 .",
    "since @xmath116 , @xmath96 , applying to thus gives    lcl - & & + ^2 ( ) + .",
    "[ eq : thm_cap_lowres_5 ]    to upper - bounded the last term on the rhs of , we use jensen s inequality to obtain @xmath117\\right).\\ ] ] by bayes law , we have @xmath118 = \\frac{\\e{y^2\\i{|y|>\\alpha}}}{\\prob(|y|>\\alpha ) } \\leq \\frac{\\eps^2(\\mathsf{p}+\\sigma^2)+\\frac{1}{12}}{\\prob(|y|>\\alpha)}\\ ] ] where we used in the right - most inequality that @xmath119 and that , for every @xmath35 satisfying @xmath36 , the second moment of @xmath92 is upper - bounded by @xmath120 . combining with",
    "then gives    lcl & & ( |y|>)(1 + ) + & = & ( |y|>)((|y|>)+ ) + & & - ( |y|>)(|y| > ) + & & ^2()(1 + ) + _ 0<^2 ( ) || [ eq : thm_cap_lowres_8 ]    where the last step follows by maximizing @xmath121 over all @xmath122 satisfying and because , by , @xmath123 .    combining and with ,",
    "we obtain for @xmath108 that    lcl + & & + ^2()+ _ 0<^2 ( ) || .",
    "[ eq : thm_cap_lowres_9 ]    since the rhs of is not smaller than the rhs of , it follows that    lcl + & & + ^2()+ _ 0<^2 ( ) || [ eq:40 ]    where the supremum on the left - hand side ( lhs ) of is over all distribution of @xmath35 satisfying @xmath36 and @xmath37 with probability one .",
    "since the function @xmath124 is continuous for @xmath125 and vanishes as @xmath126 , it follows that @xmath127\\right ) \\label{eq : thm_cap_lowres_10}\\ ] ] where @xmath128 denotes the _ limit superior _ and where we have substituted @xmath97 by the rhs of .",
    "the claim , and hence theorem  [ thm : cap_lowres ] , follows from by letting first @xmath129 and then @xmath130 .",
    "in this section , we discuss capacity at low input powers @xmath32 .",
    "we show that , when the peak - power constraint is relaxed , the low - snr asymptotic capacity is equal to that of the gaussian channel irrespective of @xmath0 .",
    "we further derive an expression for the low - snr asymptotic capacity for finite peak - to - average - power ratios and evaluate it in the low- and high - resolution limit .",
    "we demonstrate that , in this case , the low - snr asymptotic capacity converges to that of the unquantized channel when @xmath0 tends to zero , and it tends to zero when @xmath0 tends to infinity .",
    "[ thm : cue_thm1 ] consider the dither - quantized gaussian channel described in section  [ sec : channel ] .",
    "then , irrespective of @xmath131 , @xmath132    see section  [ sec : cue_thm1 ] .    theorem  [ thm : cue_thm1 ] is reminiscent of theorem  2 in @xcite , which states that the low - snr asymptotic capacity of the 1-bit quantized gaussian channel equals @xmath133 , provided that we allow for flash - signaling input distributions .",
    "as noted before , the concatenation of a uniform and a 1-bit quantizer results again in a 1-bit quantizer , so theorem  [ thm : cue_thm1 ] may perhaps not be very surprising .",
    "however , in general it is unclear how a _ dithered _ uniform quantizer compares to a 1-bit quantizer , since the dither potentially reduces capacity .",
    "in fact , as we shall see next , for finite a peak - to - average - power ratio and as @xmath0 becomes large , the dither significantly reduces the low - snr asymptotic capacity .",
    "[ thm : cue_thm2 ] consider the dither - quantized gaussian channel described in section  [ sec : channel ] .",
    "then , irrespective of @xmath66 , @xmath134 ^ 2}{q\\left(\\frac{y-\\delta/2}{\\sigma}\\right)-q\\left(\\frac{y+\\delta/2}{\\sigma}\\right ) } \\d y.\\ ] ]    see section  [ sec : cue_thm2 ] .    observe that for a finite peak - to - average - power ratio , the low - snr asymptotic capacity depends on @xmath0 .",
    "we next study the behavior of @xmath135 as @xmath4 and @xmath6 .",
    "[ cor ] consider the dither - quantized gaussian channel described in section  [ sec : channel ] .",
    "then ,    rrcl & _ 0 _ ^()(0 ) & = & + & _ _",
    "^()(0 ) & = & 0 .    see section  [ sub : proof_cor ] .",
    "corollary  [ cor ] demonstrates that , for finite peak - to - average - power ratios , the low - snr asymptotic capacity of the dither - quantized gaussian channel approaches that of the gaussian channel in the high - resolution limit and it vanishes in the low - resolution limit .",
    "the latter result is in stark contrast to proposition  2 in @xcite ( see also @xcite,@xcite ) , which demonstrates that for a 1-bit quantizer and @xmath136 , the low - snr asymptotic capacity equals @xmath137 .",
    "thus , for finite peak - to - average - power ratios , a low - resolution dithered quantizer performs significantly worse than a 1-bit quantizer .      we shall show that @xmath138 theorem  [ thm : cue_thm1 ] follows then from , , and .",
    "let @xmath139 for some arbitrary @xmath140 . by the data processing inequality for relative entropy ( * ?",
    "2.9 )    lcl d(p_x+z_|x = xp_x+z_|x=0 ) & & d(p_v|x = xp_v|x=0 ) [ eq : thm1_2 ]    where @xmath141 denotes the conditional distribution of @xmath142 given @xmath63 .",
    "intuitively , @xmath142 can be viewed as the output of a threshold quantizer with threshold @xmath143 and input @xmath62 .",
    "introducing @xmath142 thus allows us to analyze the rhs of following similar steps as the ones reported in ( * ? ? ?",
    "viii - a ) . indeed , as in (",
    "* eq .  ( 134 ) ) , we can express the relative entropy as    lcl d(p_v|x = xp_v|x=0 ) & = & + & & + p_v|x(1|x ) - h_(p_v|x(1|x ) ) [ eq : thm1_3 ]    where @xmath46 denotes the natural logarithm function ; @xmath144 denotes the binary entropy function ( * ? ? ?",
    "* eq .  ( 2.5 ) ) ; and @xmath145 , which can be written as @xmath146 using that @xmath147 , @xmath24 and @xmath148 , @xmath149 , can be further lower - bounded as    lcl d(p_v|x = xp_v|x=0 ) & & p_v|x(1|x ) - 2.[eq : thm1_4 ]    we next choose @xmath150 and lower - bound the supremum in by letting @xmath151 tend to infinity .",
    "together with and , this yields    lcl _ x0 & & _ _ 0 .",
    "[ eq : thm1_5 ]    by and the monotonicity of the @xmath54-function , we obtain @xmath152 moreover , by and the following bounds on the @xmath54-function ( * ? ? ?",
    "19.4.2 ) @xmath153 we have for sufficiently large @xmath151 @xmath154 applying and to yields    lcl + & & _ _ 0 + & = & q(- ) .",
    "[ eq : thm1_last ]    the final result , and hence theorem  [ thm : cue_thm1 ] , follows from by letting @xmath155 tend to infinity .      in order for to hold , for every @xmath131 , the channel law must satisfy six conditions ( * ? ? ?",
    "ii ) :    1 .   the channel can be described by a pdf @xmath156 .",
    "the pdf @xmath157 is bounded for all @xmath158 ( for some @xmath159 ) and @xmath160 .",
    "3 .   the partial derivative @xmath161 exists for all @xmath158 and @xmath160 .",
    "the fisher information exists and is finite for all @xmath158 . 5 .",
    "the function @xmath162 is uniformly continuous in the mean square with respect to @xmath158 .",
    "6 .   for any @xmath163 , @xmath164 ^ 2}{f_{y|x}(y|x ) } \\d y \\d x = 0\\ ] ] where @xmath165    note that , for the channel model described in section  [ sec : channel ] , we have @xmath166 .",
    "thus , conditions  a and b follow directly by inspecting . furthermore , using that @xmath167 , we obtain from that    lcl f_y|x(y|x ) & = & [ eq : thm2_1 ]    which proves condition  c. this also demonstrates that the fisher information exists and is given by @xmath168 ^ 2}{q\\left(\\frac{y - x-\\delta/2}{\\sigma}\\right)-q\\left(\\frac{y - x+\\delta/2}{\\sigma}\\right ) } \\d y.\\ ] ] to prove condition  d , it thus remains to show that the fisher information is finite for all @xmath158 .",
    "this , as well as conditions  e and f , require slightly more involved proofs , which are presented in appendix  [ app : thm2 ] . having proven conditions  a - f , theorem  [ thm : cue_thm2 ] follows directly by combining with .        to prove part  i ) , we note that , by , we have @xmath169 .",
    "it thus remains to show that @xmath170 to this end , we use fatou s lemma ( * ? ? ?",
    "* ( 1.6.8 ) , p.  50 ) to lower - bound as    lcl _ 0 _",
    "^()(0 ) & & _ -^ _ 0 y.    we next apply lhpital s rule twice to compute the limit inside the integral . indeed , we have    lcl + & = & 2    and    lcl + & = & q()-q ( ) +    which both tend to zero as @xmath4 .",
    "we further have    lcl ^2 & = & 2 ^ 2 +   + [ eq : cor_hr_1 ]    and    lcl & = & + .[eq : cor_hr_2 ]    noting that , as @xmath4 , and tend to @xmath171 and @xmath172 , respectively , lhpital s rule gives @xmath173 ^ 2}{q\\left(\\frac{y-\\delta/2}{\\sigma}\\right)-q\\left(\\frac{y+\\delta/2}{\\sigma}\\right ) } = \\frac{2\\frac{y^2}{\\sigma^4 } e^{-\\frac{y^2}{\\sigma^2}}}{\\sqrt{\\frac{2}{\\pi\\sigma^2}}e^{-\\frac{y^2}{2\\sigma^2 } } } = \\sqrt{2\\pi\\sigma^2}\\frac{y^2}{\\sigma^4}e^{-\\frac{y^2}{2\\sigma^2}}. \\label{eq : cor_hr_3}\\ ] ] integrating from @xmath65 to @xmath174 yields @xmath175 ^ 2}{q\\left(\\frac{y-\\delta/2}{\\sigma}\\right)-q\\left(\\frac{y+\\delta/2}{\\sigma}\\right ) } \\d y = \\frac{1}{2\\sigma^2}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\int_{-\\infty}^{\\infty } \\frac{y^2}{\\sigma^2 } e^{-\\frac{y^2}{2\\sigma^2 } } \\d y = \\frac{1}{2\\sigma^2}\\ ] ] where the last step follows by identifying the terms after @xmath133 as the variance of a zero - mean , variance-@xmath15 , gaussian random variable divided by @xmath15 , which is equal to one .",
    "this proves , which in turn proves part  i ) of corollary  [ cor ] .      to prove part",
    "ii ) , it suffices to show that the integral on the rhs of is bounded in @xmath0 . to this end",
    ", we first note that the integrand in is symmetric in @xmath176 , so @xmath177 ^ 2}{q\\left(\\frac{y-\\delta/2}{\\sigma}\\right)-q\\left(\\frac{y+\\delta/2}{\\sigma}\\right ) } \\d y = \\frac{1}{4\\pi\\sigma^2}\\int\\limits_{-\\infty}^{\\infty } \\frac{\\left[e^{-\\frac{(|y|-\\delta/2)^2}{2\\sigma^2}}-e^{-\\frac{(|y|+\\delta/2)^2}{2\\sigma^2}}\\right]^2}{q\\left(\\frac{|y|-\\delta/2}{\\sigma}\\right)-q\\left(\\frac{|y|+\\delta/2}{\\sigma}\\right ) } \\d y.\\label{eq : cor_lr_0}\\ ] ] we next divide the integration region into the two regions @xmath178 for a sufficiently large @xmath179 and analyze the corresponding integrals separately .    for @xmath180 , we use the monotonicity of the @xmath54-function to lower - bound @xmath181 where , for @xmath182 , the rhs of is strictly positive and tends to @xmath183 as @xmath6 .",
    "together with the identity @xmath184 , this yields    lcl + & & _ _ 1 e^- + e^- y + & & [ eq : cor_lr_y1 ]    where the last inequality follows by enhancing the integration region from @xmath185 to @xmath12 .",
    "we next consider the case where @xmath186 . by",
    ", we have for @xmath187    lcl + & & ( 1-)e^- - e^- + & & e^- + & & e^- , y_2 [ eq : cor_lr_2 ]    where @xmath188 which , for sufficiently large @xmath189 , is strictly positive and tends to @xmath190 as @xmath6 . the last inequality in",
    "follows because @xmath191 , @xmath192 and because the function @xmath193 is monotonically decreasing in @xmath194 .",
    "we further note that , for @xmath187 , @xmath195 by and ,    lcl + & & _ _ 2 e^-y + & = & e^- .",
    "[ eq : cor_lr_y2 ]    combining and with , we obtain @xmath177 ^ 2}{q\\left(\\frac{y-\\delta/2}{\\sigma}\\right)-q\\left(\\frac{y+\\delta/2}{\\sigma}\\right ) } \\d y \\leq \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\left(\\frac{\\sqrt{2}}{q\\bigl(\\vartheta/\\sigma\\bigr)-q\\bigl(\\delta/(2\\sigma)\\bigr ) } + \\frac{e^{-\\frac{\\vartheta^2}{2\\sigma^2}}}{\\mu_{\\vartheta}(\\delta)}\\right ) .",
    "\\label{eq : cor_lr_final}\\ ] ] part  ii ) of corollary  [ cor ] follows then by noting that , for sufficiently large @xmath189 , the rhs of is bounded in @xmath0 .",
    "we have studied both the capacity and the low - snr asymptotic capacity of the peak - and - average - power - limited gaussian channel when its output is quantized using a dithered , infinite - level , uniform quantizer of step size @xmath0 .",
    "we have demonstrated that the capacity of the dither - quantized channel converges to the capacity of the unquantized channel in the high - resolution limit ( @xmath4 ) , and it converges to zero in the low - resolution limit ( @xmath6 ) .",
    "we have further demonstrated that , when the peak - power constraint is absent , the low - snr asymptotic capacity of the dither - quantized channel is equal to that of the unquantized channel irrespective of @xmath0 .",
    "in contrast , for finite peak - to - average - power ratios , the low - snr asymptotic capacity of the dither - quantized channel depends critically on @xmath0 : as we show , it converges to the low - snr asymptotic capacity of the unquantized channel in the high - resolution limit , but it vanishes in the low - resolution limit .    while dithered , infinite - level , uniform quantizers seem impractical due to the infinite number of bits required to describe their outputs , studying their behavior may help us better understand the behavior of quantizers with a small number of levels , provided that both type of quantizers have similar behaviors .",
    "our results suggest that , with respect to channel capacity , this is the case in the high - resolution limit , but it is not the case in the low - resolution limit .",
    "for example , the capacity of the 1-bit quantized gaussian channel ( with a symmetric threshold quantizer ) is given by , which differs not only from the capacity of the dither - quantized gaussian channel in the low - resolution limit for a given @xmath32 ( which is zero ) , but it also has a distinct asymptotic behavior as @xmath32 tends to zero .",
    "since the concatenation of an infinite - level , uniform quantizer and a 1-bit quantizer results again in a 1-bit quantizer , we conclude that the inferior performance at low quantizer resolutions of the dithered , infinite - level , uniform quantizer is due to the dither . in other words , in the low - resolution regime , adding dither is highly detrimental . nevertheless , sampling the output of the dithered , infinite - level , uniform quantizer above nyquist rate , as studied in @xcite , may perhaps improve the performance in this regime , since such an approach reduces the quantization noise without increasing the quantizer resolution .",
    "we shall prove by showing that    lcl h(y_|u _ ) & = & h(x+z _ ) - [ eq : app_h|u ] + h(y _",
    "|u_,x ) & = & h(z _ ) - .",
    "[ eq : app_h|u , x ]    the proof of and is almost identical to the proof of theorem  1 in @xcite . for the sake of completeness , we repeat it here .",
    "first note that , since @xmath35 and @xmath72 are independent and @xmath72 is gaussian , it follows by ( * ? ?",
    "* th .  4.10 ) that the distribution of the random variable @xmath196 is absolutely continuous with respect to the lebesgue measure , so its pdf , which we shall denote by @xmath197 , is defined .",
    "furthermore , the pdf of @xmath198 relates to @xmath197 via ( * ? ? ?",
    "* th .  4.10 ) @xmath199 where @xmath200 denotes the pdf of @xmath48 , i.e. , @xmath201 , @xmath202 .",
    "( recall that @xmath48 is uniformly distributed over @xmath7 $ ] and @xmath49 . ) likewise , the conditional probability of @xmath203 given @xmath204 is equal to @xmath205 which together with yields @xmath206 we next use and fubini s theorem ( * ? ? ?",
    "* ( 2.6.6 ) , p.  108 ) to express the conditions entropy of @xmath92 given @xmath48 as    lcl h(y_|u _ ) & = & - _ -/2^/2 _",
    "i=-^ p_y|u_(i |u ) p_y|u_(i |u ) u + & = & - _ i=-^ _",
    "-/2^/2 p_y|u_(i |u ) p_y|u_(i |u ) u + & = & - - _",
    "i=-^_-/2^/2 f_x+z_(i+/2-u ) f_x+z_(i+/2-u)u .",
    "[ eq : app1_h|u ]    by the change of variable @xmath207 , it then follows that    lcl h(y_|u _ ) & = & - - _",
    "i^(i+1 ) f_x+z _ ( ) f_x+z _ ( ) + & = & -- h(x+z _ ) .",
    "[ eq : app1_h|u ]    this proves .",
    "the second identity follows along similar lines .",
    "indeed , we have @xmath208 where @xmath209 denotes the pdf of @xmath72 .",
    "furthermore , the conditional probability of @xmath203 given @xmath210 is @xmath211 which together with yields @xmath212 analog to and , we obtain from that for every @xmath24    lcl h(y_|u_,x = x ) & = & -- _",
    "i=-^ _ i - x^(i+1)-x f_z_()f_z _ ( ) + & = & - -h(z _ ) .    averaging over @xmath35 , this yields .",
    "in this appendix , we prove the conditions stated in section  [ sec : cue_thm2 ] that require more involved proofs . specifically , section  [ sub : cond_d ] demonstrates that the fisher information is finite for all @xmath213 , which together with proves condition  d. section  [ sub : cond_e ] proves condition  e and section  [ sub : cond_f ] proves condition  f.    throughout this appendix , we shall use the following notation .",
    "we denote the partial derivative of @xmath157 with respect to @xmath214 by @xmath215 .",
    "we further omit the subscript of @xmath157 to keep notation compact .",
    "finally , we define the sets @xmath216 for some arbitrary @xmath189 .",
    "the fisher information @xmath217 is given by , namely , @xmath218",
    "^ 2}{q\\left(\\frac{y - x-\\delta/2}{\\sigma}\\right)-q\\left(\\frac{y - x+\\delta/2}{\\sigma}\\right ) } \\d y. \\label{eq : appd_0}\\ ] ] to prove that @xmath68 is finite for all @xmath213 and @xmath131 , we divide the integration region into @xmath185 and @xmath219 , for some sufficiently large @xmath179 , and show that the corresponding integrals are finite for all @xmath213 .    since the @xmath54-function is continuous and @xmath185 is a closed and bounded interval , it follows from the extreme value theorem that for every @xmath180 and @xmath220 @xmath221 for some @xmath222 $ ] . together with , this yields for every @xmath180 and @xmath213    lcl f(y|x ) _ .",
    "[ eq : appd_1 ]    by the strict monotonicity of @xmath53 , it further follows that @xmath223 .",
    "we thus have    lcl + & & _ _ 1 ^2 y + & & [ eq : appd_y1 ]    where the second inequality follows because @xmath224    we next consider the case where @xmath186 . to this end , we first note that the pdf @xmath225 is symmetric in @xmath226 , so it can be written as @xmath227.\\ ] ] using , this can be lower - bounded as    lcl f_z_(z ) & & ( 1- ) e^- - e^- + & = & e^- .",
    "[ eq : appd_2 ]    note that the term inside the square brackets on the rhs of tends to one as @xmath228 .",
    "since by the triangle inequality @xmath229 for @xmath186 and @xmath213 , it follows that for any @xmath230 there exists a sufficiently large @xmath189 such that @xmath231 applying to , and using that the integrand is symmetric in @xmath176 , we obtain    lcl + & & _ _ 2 ( |y - x|-/2)e^^2 y. + & & _ _ 2 ( |y - x|-/2)e^- y [ eq : appd_4 ]    where the last step follows because , for sufficiently large @xmath189 , we have @xmath232 , which implies that @xmath233 let @xmath234 and @xmath235 . by a change of variables , can be further upper - bounded by    lcl + & = & _ _ 2 ( |z|-/2)e^- z + & & _ |z|>-(|z|-/2)e^- z + & = & e^- [ eq : appd_y2 ]    where the inequality follows because , by the triangle inequality , @xmath236 and because for @xmath237 and a sufficiently large @xmath189 , the term @xmath238 is nonnegative .    combining and , we obtain for every @xmath213 and a sufficiently large @xmath189 @xmath239 thus , the fisher information @xmath68 is finite for all @xmath213 .      by the chain rule , it follows that @xmath240 to prove condition  e , we need to show that for every @xmath131 ( * ? ? ?",
    "* eq .  ( 2.3 ) ) @xmath241 ^ 2 \\d y \\to 0\\ ] ] as @xmath242 and @xmath243 . since , for all @xmath160 , @xmath244 and @xmath245 are both bounded and continuous functions of @xmath214 and @xmath246 ( for some arbitrary @xmath159 ) , it follows that @xmath247 ^ 2 = 0 , \\quad y\\in\\reals.\\ ] ] to prove , it thus suffices to show that there exists an integrable function @xmath248 that upper - bounds @xmath249 ^ 2 \\leq g(y ) , \\quad y\\in\\reals\\ ] ] for all @xmath250 and @xmath251.the claim , and hence condition  e , follows then by the dominated convergence theorem ( * ? ? ?",
    "* ( 1.6.9 ) , p.  50 ) .    to prove , we follow the approach carried out in section  [ sub : cond_d ] and divide the integration region into @xmath185 and @xmath219 ( for some sufficiently large @xmath179 ) and evaluate the corresponding integrals separately . for @xmath180 , we use the identity @xmath184 , , and to upper - bound    lcl + & & + + & & ( ^2+^2 ) + & & [ eq : appe_y1 ]    which is integrable over the bounded set @xmath185 .",
    "we next consider the case where @xmath186 .",
    "we first note that for any @xmath230 there exists a sufficiently large @xmath189 such that holds . using this result together with the identity @xmath184 and",
    ", we obtain for sufficiently larger @xmath189    lcl + & & + + & & ( |y - x_1|-/2)e^^2 + & & + ( |y - x_2|-/2)e^^2 + & & ( ( |y - x_1|-/2)e^- + ( |y - x_2|-/2)e^- ) . [",
    "eq : appe_107 ]    since , by the triangle inequality , @xmath252 , it follows that for all @xmath253 and @xmath254 @xmath255 ^ 2 \\leq \\frac{1}{2\\pi\\sigma^2}\\frac{\\sqrt{2\\pi}}{\\mu_{\\delta}\\sigma } ( |y|+\\eps-\\delta/2)e^{-\\frac{(|y|-\\eps-\\delta/2)^2}{2\\sigma^2}}. \\label{eq : appe_y2}\\ ] ] note that the rhs of is integrable over @xmath186 .    combining and",
    ", it follows that the integrable function @xmath256 satisfies for all @xmath250 and @xmath251 .",
    "this demonstrates that condition  e is satisfied .",
    "we upper - bound the left - hand side of by deriving an upper bound on @xmath257 ^ 2}{f(y|x ) } \\d y\\ ] ] that holds for sufficiently small @xmath258 and that is independent of @xmath24 and @xmath163 . to this end",
    ", we divide the integration region into @xmath259 and @xmath260 and evaluate each integral separately .",
    "we then show that the resulting upper bound vanishes as @xmath189 tends to infinity , thereby proving condition  f.    we begin by showing that for any @xmath163 and @xmath179 there exists a sufficiently small @xmath261 such that for all @xmath262 @xmath263 where @xmath264 denotes the empty set .",
    "consequently , @xmath265 ^ 2}{f(y|x ) } \\d y=0 , \\quad \\eps<\\eps_0.\\ ] ] to this end , we approximate @xmath266 for every @xmath180 by a taylor series around @xmath267 : @xmath268 for some @xmath269 , where we use the lagrange form of the remainder .",
    "consequently , @xmath270 by , , and , it follows that @xmath271 for some @xmath223 , which implies that @xmath272 using the inequality @xmath273 it follows from , , and the monotonicity of @xmath274f that , for @xmath275 ,    lcl || & = & |(1+x)| + & & + & & .",
    "[ eq : appf_7 ]    the rhs of vanishes as @xmath276 , so for any fixed @xmath163 and @xmath179 , there exists an @xmath261 such that @xmath277 together with the definition of @xmath278 in , this proves .",
    "we continue by evaluating the integral for @xmath186 . to this end",
    ", we use that the integrand is nonnegative and that @xmath279 to upper - bound    lcl _ _ , _ 2 y & & _ _ 2 y. [ eq : appf_8 ]    repeating the steps  in section  [ sub : cond_e ] ,",
    "the rhs of can be further upper - bounded by @xmath280 ^ 2}{f(y|x ) } \\d y \\leq \\sqrt{\\frac{2}{\\pi\\sigma^2 } } \\frac{1}{\\mu_{\\delta}(\\vartheta ) } e^{-\\frac{(\\vartheta-\\eps-\\delta/2)^2}{2\\sigma^2 } } \\label{eq : appf_y2}\\ ] ] where @xmath281 is a positive function that tends to @xmath282 as @xmath283 .    combining with and",
    ", we obtain that for every @xmath163 and sufficiently large @xmath179 , there exists an @xmath261 such that @xmath257 ^ 2}{f(y|x ) } \\d y \\leq \\sqrt{\\frac{2}{\\pi\\sigma^2 } } \\frac{1}{\\mu_{\\delta}(\\vartheta ) } e^{-\\frac{(\\vartheta-\\eps-\\delta/2)^2}{2\\sigma^2 } } , \\quad \\text{for all $ \\eps<\\eps_0$.}\\ ] ] upon integrating over @xmath284 , dividing by @xmath285 , and taking the limit as @xmath276 , this yields for every @xmath163 and sufficiently large @xmath179 @xmath286 ^ 2}{f(y|x ) } \\d y\\d x \\leq \\sqrt{\\frac{2}{\\pi\\sigma^2 } } \\frac{2}{\\mu_{\\delta}(\\vartheta ) } e^{-\\frac{(\\vartheta-\\delta/2)^2}{2\\sigma^2}}.\\ ] ] condition  f follows then by letting @xmath189 tend to infinity .",
    "stimulating discussions with ram zamir are gratefully acknowledged .",
    "j.  singh , o.  dabeer , and u.  madhow , `` on the limits of communication with low - precision analog - to - digital conversion at the receiver , '' _ ieee trans .",
    "_ , vol .",
    "57 , no .  12 , pp .",
    "36293639 , dec . 2009 .",
    "v.  v. prelov and e.  c. van  der meulen , `` an asymptotic expression for the information and capacity of a multidimensional channel with weak input signals , '' _ ieee trans .",
    "inf . theory _ ,",
    "39 , no .  5 , pp . 17281735 , sept .",
    "1993 .",
    "a.  lapidoth and s.  m. moser , `` capacity bounds via duality with applications to multiple - antenna systems on flat fading channels , '' _ ieee trans .",
    "inf . theory _ ,",
    "49 , no .",
    "10 , pp . 24262467 , oct ."
  ],
  "abstract_text": [
    "<S> this paper studies the capacity of the peak - and - average - power - limited gaussian channel when its output is quantized using a dithered , infinite - level , uniform quantizer of step size @xmath0 . </S>",
    "<S> it is shown that the capacity of this channel tends to that of the unquantized gaussian channel when @xmath0 tends to zero , and it tends to zero when @xmath0 tends to infinity . in the low signal - to - noise ratio ( snr ) regime , it is shown </S>",
    "<S> that , when the peak - power constraint is absent , the low - snr asymptotic capacity is equal to that of the unquantized channel irrespective of @xmath0 . </S>",
    "<S> furthermore , an expression for the low - snr asymptotic capacity for finite peak - to - average - power ratios is given and evaluated in the low- and high - resolution limit . </S>",
    "<S> it is demonstrated that , in this case , the low - snr asymptotic capacity converges to that of the unquantized channel when @xmath0 tends to zero , and it tends to zero when @xmath0 tends to infinity . comparing these results with achievability results for ( undithered ) 1-bit quantization , it is observed that the dither reduces capacity in the low - precision limit , and it reduces the low - snr asymptotic capacity unless the peak - to - average - power ratio is unbounded .    </S>",
    "<S> [ multiblock footnote omitted ] </S>"
  ]
}