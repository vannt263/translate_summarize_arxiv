{
  "article_text": [
    "approximate bayesian computation ( abc ) represents an elaborate statistical approach to model - based inference in a bayesian setting in which model likelihoods are difficult to calculate ( due to the complexity of the models considered ) . since its introduction in population genetics @xcite",
    ", the method has found an ever increasing range of applications covering diverse types of complex models in various scientific fields ( see , e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "the principle of abc is to conduct bayesian inference on a dataset through comparisons with numerous simulated datasets .",
    "however , it suffers from two major difficulties .",
    "first , to ensure reliability of the method , the number of simulations is large ; hence , it proves difficult to apply abc for large datasets ( e.g. , in population genomics where tens to hundred thousand markers are commonly genotyped ) .",
    "second , calibration has always been a critical step in abc implementation @xcite .",
    "more specifically , the major feature in this calibration process involves selecting a vector of summary statistics that quantifies the difference between the observed data and the simulated data .",
    "the construction of this vector is therefore paramount and examples abound about poor performances of abc model choice algorithms related with specific choices of those statistics @xcite , even though there also are instances of successful implementations .",
    "we advocate a drastic modification in the way abc model selection is conducted : we propose both to step away from selecting the most probable model from estimated posterior probabilities , and to reconsider the very problem of constructing efficient summary statistics .",
    "first , given an arbitrary pool of available statistics , we now completely bypass selecting among those .",
    "this new perspective directly proceeds from machine learning methodology .",
    "second , we postpone the approximation of model posterior probabilities to a second stage , as we deem the standard numerical abc approximations of such probabilities fundamentally untrustworthy . we instead advocate selecting the posterior most probable model by constructing a ( machine learning ) classifier from simulations from the prior predictive distribution ( or other distributions in more advanced versions of abc ) , known as the abc _",
    "reference table_. the statistical technique of random forests ( rf ) @xcite represents a trustworthy machine learning tool well adapted to complex settings as is typical for abc treatments .",
    "once the classifier is constructed and applied to the actual data , an approximation of the posterior probability of the resulting model can be produced through a secondary random forest that regresses the selection error over the available summary statistics .",
    "we show here how rf improves upon existing classification methods in significantly reducing both the classification error and the computational expense . after presenting theoretical arguments",
    ", we illustrate the power of the abc - rf methodology by analyzing controlled experiments as well as genuine population genetics datasets .",
    "bayesian model choice @xcite compares the fit of @xmath1 models to an observed dataset @xmath2 .",
    "it relies on a hierarchical modelling , setting first prior probabilities @xmath3 on model indices @xmath4 and then prior distributions @xmath5 on the parameter @xmath6 of each model , characterized by a likelihood function @xmath7 .",
    "inferences and decisions are based on the posterior probabilities of each model @xmath8 .",
    "while we can not cover in much details the principles of approximate bayesian computation ( abc ) , let us recall here that abc was introduced in @xcite and @xcite for solving intractable likelihood issues in population genetics .",
    "the reader is referred to , e.g. , @xcite , @xcite , @xcite , @xcite and @xcite for thorough reviews on this approximation method .",
    "the fundamental principle at work in abc is that the value of the intractable likelihood function @xmath9 at the observed data @xmath2 and for a current parameter @xmath6 can be evaluated by the proximity between @xmath2 and pseudo - data @xmath10 simulated from @xmath11 . in discrete settings ,",
    "the indicator @xmath12 is an unbiased estimator of @xmath9 @xcite . for realistic settings ,",
    "the equality constraint is replaced with a tolerance region @xmath13 , where @xmath14 is a measure of divergence between the two vectors and @xmath15 is a tolerance value .",
    "the implementation of this principle is straightforward : the abc algorithm produces a large number of pairs @xmath16 from the prior predictive , a collection called the _ reference table _ , and extracts from the table the pairs @xmath16 for which @xmath17 .    to approximate posterior probabilities of competing models , abc methods @xcite",
    "compare observed data with a massive collection of pseudo - data , generated from the prior predictive distribution in the most standard versions of abc ; the comparison proceeds via a normalized euclidean distance on a vector of statistics @xmath18 computed for both observed and simulated data .",
    "standard abc estimates posterior probabilities @xmath8 at stage ( b ) of algorithm  [ algo : general ] below as the frequencies of those models within the @xmath0 nearest - to-@xmath2 simulations , proximity being defined by the distance between @xmath19 and the simulated @xmath18 s .    selecting a model means choosing the model with the highest frequency in the sample of size @xmath0 produced by abc , such frequencies being approximations to posterior probabilities of models . we stress that this solution means resorting to a @xmath0-nearest neighbor ( @xmath0-nn ) estimate of those probabilities , for a set of simulations drawn at stage ( a ) , whose records constitute the so - called _ reference table _ , see @xcite or @xcite .",
    "* generate a reference table including @xmath20 simulations @xmath21 from @xmath22 * learn from this set to infer about @xmath23 at @xmath24    selecting a set of summary statistics @xmath25 that are informative for model choice is an important issue .",
    "the abc approximation to the posterior probabilities @xmath8 will eventually produce a right ordering of the fit of competing models to the observed data and thus select the right model for a specific class of statistics on large datasets @xcite .",
    "this most recent theoretical abc model choice results indeed show that some statistics produce nonsensical decisions and that there exist sufficient conditions for statistics to produce consistent model prediction , albeit at the cost of an information loss due to summaries that may be substantial .",
    "the toy example comparing ma(1 ) and ma(2 ) models in appendix and figure [ fig : trueppvssummaries ] clearly exhibits this potential loss in using only the first two autocorrelations as summary statistics .",
    "@xcite developed an interesting methodology to select the summary statistics , but with the requirement to aggregate estimation and model pseudo - sufficient statistics for every model under comparison .",
    "that induces a deeply inefficient dimension inflation and can be very time consuming .",
    "-axis ) and on only the first two autocorrelations ( @xmath26-axis ) .",
    "[ fig : trueppvssummaries ] , scaledwidth=50.0% ]    it may seem tempting to collect the largest possible number of summary statistics to capture more information from the data .",
    "this brings @xmath27 closer to @xmath8 but increases the dimension of @xmath18 .",
    "abc algorithms , like @xmath0-nn  and other local methods suffer from the curse of dimensionality ( see e.g.  section 2.5 in @xcite ) so that the estimate of @xmath27 based on the simulations is poor when the dimension of @xmath18 is too large . selecting summary statistics correctly and sparsely is therefore paramount , as shown by the literature in the recent years .",
    "( see @xcite surveying abc parameter estimation . ) for abc model choice , two main projection techniques have been considered so far .",
    "first , @xcite show that the bayes factor itself is an acceptable summary ( of dimension one ) when comparing two models , but its practical evaluation via a pilot abc simulation induces a poor approximation of model evidences @xcite .",
    "the recourse to a regression layer like linear discriminant analysis ( lda , @xcite ) is discussed below and in appendix section a. other projection techniques have been proposed in the context of parameter estimation : see , e.g. , @xcite .    given the fundamental difficulty in producing reliable tools for model choice based on summary statistics @xcite , we now propose to switch to a different approach based on an adapted classification method .",
    "we recall in the next section the most important features of the random forest ( rf ) algorithm .",
    "the classification and regression trees ( cart ) algorithm at the core of the rf scheme produces a binary tree that sets allocation rules for entries as labels of the internal nodes and classification or predictions of @xmath28 as values of the tips ( terminal nodes ) . at a given internal node",
    ", the binary rule compares a selected covariate @xmath29 with a bound @xmath30 , with a left - hand branch rising from that vertex defined by @xmath31 . predicting the value of @xmath28 given the covariate @xmath32 implies following a path from the tree root that is driven by applying these binary rules .",
    "the outcome of the prediction is the value found at the final leaf reached at the end of the path : majority rule for classification and average for regression . to find the best split and the best variable at each node of the tree ,",
    "we minimize a criterium : for classification , the gini index and , for regression , the @xmath33-loss . in the randomized version of the cart algorithm ( see algorithm a1 in the appendix ) , only a random subset of covariates of size @xmath34 is considered at each node of the tree .",
    "the rf algorithm @xcite consists in bagging ( which stands for bootstrap aggregating ) randomized cart .",
    "it produces @xmath35 randomized cart trained on samples or sub - samples of size @xmath36 produced by bootstrapping the original training database .",
    "each tree provides a classification or a regression rule that returns a class or a prediction .",
    "then , for classification we use the majority vote across all trees in the forest , and , for regression , the response values are averaged .",
    "three tuning parameters need be calibrated : the number @xmath35 of trees in the forest , the number @xmath34 of covariates that are sampled at a given node of the randomized cart , and the size @xmath36 of the bootstrap sub - sample .",
    "this point will be discussed in section [ sec : prac ] .    for classification ,",
    "a very useful indicator is the _ out - of - bag _ error ( * ? ? ?",
    "* chapter 15 ) . without any recourse to a test set",
    ", it gives you some idea on how good is your rf classifier .",
    "for each element of the training set , we can define the out - of - bag classifier : the aggregation of votes over the trees not constructed using this element .",
    "the out - of - bag error is the error rate of the out - of - bag classifier on the training set .",
    "the out - of - bag error estimate is as accurate as using a test set of the same size as the training set .      the above - mentioned difficulties in abc model choice drives us to a paradigm shift in the practice of model choice , namely to rely on a classification algorithm for model selection , rather than a poorly estimated vector of @xmath27 probabilities .",
    "as shown in the example described in section 3.1 , the standard abc approximations to posterior probabilities can significantly differ from the true @xmath8 .",
    "indeed , our version of stage ( b ) in algorithm  [ algo : general ] relies on a rf classifier whose goal is to predict the suited model @xmath37 at each possible value @xmath38 of the summary statistics @xmath18 .",
    "the random forest is trained on the simulations produced by stage ( a ) of algorithm  [ algo : general ] , which constitute the reference table .",
    "once the model is selected as @xmath39 , we opt to approximate @xmath40 by another random forest , obtained from regressing the probability of error on the ( same ) covariates , as explained below .    a practical way to evaluate the performance of an abc model choice algorithm and to check whether both a given set of summary statistics and a given classifier is to check whether it provides a better answer than others .",
    "the aim is to come near the so - called _ bayesian classifier _ , which , for the observed @xmath41 , selects the model having the largest posterior probability @xmath42 .",
    "it is well known that the bayesian classifier ( which can not be derived ) minimizes the 01 integrated loss or error @xcite . in the abc framework , we call the integrated loss ( or risk ) the _ prior error rate _ , since it provides an indication of the global quality of a given classifier @xmath43 on the entire space weighted by the prior . this rate is the expected value of the misclassification error over the hierarchical prior @xmath44 it can be evaluated from simulations @xmath45 drawn as in stage ( a ) of algorithm  [ algo : general ] , independently of the reference table @xcite , or with the out - of - bag error in rf that , as explained above , requires no further simulation .",
    "both classifiers and sets of summary statistics can be compared via this error scale : the pair that minimizes the prior error rate achieves the best approximation of the ideal bayesian classifier . in that sense it stands closest to the decision we would take were we able to compute the true @xmath42 .",
    "we seek a classifier in stage ( b ) of algorithm  [ algo : general ] that can handle an arbitrary number of statistics and extract the maximal information from the reference table obtained at stage ( a ) .",
    "as introduced above , random forest ( rf ) classifiers @xcite are perfectly suited for that purpose .",
    "the way we build both a rf classifier given a collection of statistical models and an associated rf regression function for predicting the allocation error is to start from a simulated abc _",
    "reference table _ made of a set of simulation records made of model indices and summary statistics for the associated simulated data .",
    "this table then serves as training database for a rf that forecasts model index based on the summary statistics .",
    "the resulting algorithm , presented in algorithm [ algo : abc - rf ] and called abc - rf , is implemented in the r package abcrf associated with this paper .",
    "generate a reference table including @xmath20 simulation @xmath21 from @xmath22    construct @xmath35 randomized cart which predict @xmath23 using @xmath18 * draw * a bootstrap ( sub-)sample of size @xmath36 from the reference table * grow * a randomized cart @xmath46 ( algorithm a1 in the appendix )    determine the predicted indexes for @xmath19 and the trees @xmath47    affect @xmath19 according to a majority vote among the predicted indexes    the justification for choosing rf to conduct an abc model selection is that , both formally @xcite and experimentally ( * ? ? ?",
    "* chapter 5 ) , rf classification was shown to be mostly insensitive both to strong correlations between predictors ( here the summary statistics ) and to the presence of noisy variables , even in relatively large numbers , a characteristic that @xmath0-nn  classifiers miss .",
    "this type of robustness justifies adopting an rf strategy to learn from an abc reference table for bayesian model selection . within an arbitrary ( and arbitrarily large ) collection of summary statistics ,",
    "some may exhibit strong correlations and others may be uninformative about the model index , with no terminal consequences on the rf performances . for model selection ,",
    "rf thus competes with both local classifiers commonly implemented within abc : it provides a more non - parametric modelling than local logistic regression @xcite , which is implemented in the diyabc software @xcite which is extremely costly  see , e.g. , @xcite which reduces the dimension using linear discriminant projection before resorting to local logistic regression .",
    "this software also includes a standard @xmath0-nn  selection procedure which suffers from the curse of dimensionality and thus forces selection among statistics .",
    "the outcome of rf computation applied to a given target dataset is a classification vote for each model which represents the number of times a model is selected in a forest of n trees .",
    "the model with the highest classification vote corresponds to the model best suited to the target dataset .",
    "it is worth stressing here that there is no direct connection between the frequencies of the model allocations of the data among the tree classifiers ( i.e. the classification vote ) and the posterior probabilities of the competing models .",
    "machine learning classifiers hence miss a distinct advantage of posterior probabilities , namely that the latter evaluate a confidence degree in the selected ( map ) model .",
    "an alternative to those probabilities is the prior error rate . aside from its use to select the best classifier and set of summary statistics , this indicator remains , however , poorly relevant since the only point of importance in the data space is the observed dataset @xmath19 .",
    "a first step addressing this issue is to obtain error rates conditional on the data as in @xcite .",
    "however , the statistical methodology considered therein suffers from the curse of dimensionality and we here consider a different approach to precisely estimate this error .",
    "we recall @xcite that the posterior probability of a model is the natural bayesian uncertainty quantification since it is the complement of the posterior error associated with the loss @xmath48 . while the proposal of @xcite for estimating the conditional error rate induced a classifier given @xmath49 @xmath50 involves non - parametric kernel regression , we suggest to rely instead on a rf regression to undertake this estimation .",
    "the curse of dimensionality is then felt much less acutely , given that random forests can accommodate large dimensional summary statistics .",
    "furthermore , the inclusion of many summary statistics does not induce a reduced efficiency in the rf predictors , while practically compensating for insufficiency .    before describing in more details the implementation of this concept",
    ", we stress that the perspective of @xcite leads to effectively estimate the posterior probability that the true model is the map , thus providing us with a non - parametric estimation of this quantity , an alternative to the classical abc solutions we found we could not trust .",
    "indeed , the posterior expectation satisfies    @xmath51 & = \\sum_{i=1}^k \\mathbb{e}[\\mathbb{i}(\\hat{m}(s({\\mathbf{x}}^0))\\ne m = i)|s({\\mathbf{x}}^0)]\\\\ & = \\sum_{i=1}^k \\mathbb{p}[m = i)|s({\\mathbf{x}}^0)]\\times\\mathbb{i}(\\hat{m}(s({\\mathbf{x}}^0))\\ne i)\\\\ & = \\mathbb{p}[m\\ne \\hat{m}(s({\\mathbf{x}}^0))|s({\\mathbf{x}}^0)]\\\\ & = 1-\\mathbb{p}[m = \\hat{m}(s({\\mathbf{x}}^0))|s({\\mathbf{x}}^0)]\\,.\\end{aligned}\\ ] ]    it therefore provides the complement of the posterior probability that the true model is the selected model .    to produce our estimate of the posterior probability",
    "@xmath52 $ ] , we proceed as follows :    1 .",
    "we compute the value of @xmath53 for the trained random forest @xmath43 and for all terms in the abc reference table ; to avoid overfitting , we use the out - of - bag classifiers ; 2 .",
    "we train a rf regression estimating the variate @xmath53 as a function of the same set of summary statistics , based on the same reference table .",
    "this second rf can be represented as a function @xmath54 that constitutes a machine learning estimate of @xmath55 $ ] ; 3 .",
    "we apply this rf function to the actual observations summarized as @xmath19 and return @xmath56 as our estimate of @xmath57 $ ] .",
    "this corresponds to the representation of algorithm [ algo : posterior ] which is implemented in the r package abcrf associated with this paper .",
    "* use the rf produce by algorithm [ algo : abc - rf ] to compute the out - of - bag classifiers of all terms in the reference table and deduce the associated binary model prediction error * use the reference table to build a rf regression function @xmath54 regressing the model prediction error on the summary statistics * return the value of @xmath56 as the rf regression estimate of @xmath57 $ ]",
    "to illustrate the power of the abc - rf methodology , we now report several controlled experiments as well as two genuine population genetic examples .",
    "the appendix details controlled experiments on a toy problem , comparing ma(1 ) and ma(2 ) time - series models , and two controlled synthetic examples from population genetics , based on single nucleotide polymorphism ( snp ) and microsatellite data .",
    "the toy example is particularly revealing with regard to the discrepancy between the posterior probability of a model and the version conditioning on the summary statistics @xmath19 .",
    "figure [ fig : trueppvssummaries ] shows how far from the diagonal are realizations of the pairs @xmath58 , even though the autocorrelation statistic is quite informative @xcite .",
    "note in particular the vertical accumulation of points near @xmath59 .",
    "table [ tab : mama ] in the appendix demonstrates the further gap in predictive power for the full bayes solution with a true error rate of 12%  versus the best solution ( rf ) based on the summaries barely achieving a 16%  error rate .    for both controlled genetics experiments in the appendix ,",
    "the computation of the true posterior probabilities of the three models is impossible .",
    "the predictive performances of the competing classifiers can nonetheless be compared on a test sample .",
    "results , summarized in tables [ tab : snp ] and [ tab : microsat ] in the appendix , legitimize the use of rf , as this method achieves the most efficient classification in all genetic experiments . note that that the prior error rate of any classifier is always bounded from below by the error rate associated with the ( ideal ) bayesian classifier .",
    "therefore , a mere gain of a few percents may well constitute an important improvement when the prior error rate is low . as an aside",
    ", we also stress that , since the prior error rate is an expectation over the entire sampling space , the reported gain may occult much better performances over some areas of this space .",
    "figure [ fig : mama.post ] in the appendix displays differences between the true posterior probability of the model selected by algorithm [ algo : abc - rf ] and its approximation with algorithm [ algo : posterior ] .      the original challenge was to conduct inference about the introduction pathway of the invasive harlequin ladybird _",
    "( harmonia axyridis _ ) for the first recorded outbreak of this species in eastern north america .",
    "the dataset , first analyzed in @xcite and @xcite via abc , includes samples from three natural and two biocontrol populations genotyped at 18 microsatellite markers .",
    "the model selection requires the formalization and comparison of 10 complex competing scenarios corresponding to various possible routes of introduction ( see appendix for details and analysis 1 in @xcite ) .",
    "we now compare our results from the abc - rf algorithm with other classification methods for three sizes of the reference table and with the original solutions by @xcite and @xcite .",
    "we included all summary statistics computed by the diyabc software for microsatellite markers @xcite , namely 130 statistics , complemented by the nine lda axes as additional summary statistics ( see appendix section g ) .    in this example",
    ", discriminating among models based on the observation of summary statistics is difficult .",
    "the overlapping groups of figure [ fig : cox_lda ] in the appendix reflect that difficulty , the source of which is the relatively low information carried by the 18 autosomal microsatellite loci considered here .",
    "prior error rates of learning methods on the whole reference table are given in table [ tab : asian ] .",
    "as expected in such a high dimension settings ( * ? ? ?",
    "* section 2.5 ) , @xmath0-nn  classifiers behind the standard abc methods are all defeated by rf for the three sizes of the reference table , even when @xmath0-nn  is trained on the much smaller set of covariates composed of the nine lda axes .",
    "the classifier and set of summary statistics showing the lowest prior error rate is rf trained on the 130 summaries and the nine lda axes .",
    "figure [ fig : cox_viss ] in the appendix shows that rfs are able to automatically determine the ( most ) relevant statistics for model comparison , including in particular some crude estimates of admixture rate defined in @xcite , some of them not selected by the experts in @xcite .",
    "we stress here that the level of information of the summary statistics displayed in figure [ fig : cox_viss ] in the appendix is relevant for model choice but not for parameter estimation issues . in other words ,",
    "the set of best summaries found with abc - rf should not be considered as an optimal set for further parameter estimations under a given model with standard abc techniques @xcite .",
    ".*harlequin ladybird data * : estimated prior error rates for various classification methods and sizes of the reference table.[tab : asian ] [ cols=\"^ , > , > , > \" , ]",
    "we here illustrate the potential of our abc - rf algorithm for the statistical processing of massive single nucleotide polymorphism ( snp ) datasets , whose production is on the increase within the field of population genetics . to this aim",
    ", we analyzed a snp dataset obtained from individuals originating from four human populations ( 30 unrelated individuals per population ) using the freely accessible public 1000 genome databases ( i.e. , the vcf format files including variant calls available at ` http://www.1000genomes.org/data ` ) .",
    "the goal of the 1000 genomes project is to find most genetic variants that have frequencies of at least 1% in the studied populations by sequencing many individuals lightly ( i.e. , at a @xmath60 coverage ) .",
    "a major interest of using snp data from the 1000 genomes project @xcite is that such data does not suffer from any ascertainment bias ( i.e. , the deviations from expected theoretical results due to the snp discovery process in which a small number of individuals from selected populations are used as discovery panel ) , which is a prerequisite when using the diyabc simulator of snp data @xcite .",
    "the four human populations included the yoruba population ( nigeria ) as representative of africa ( encoded yri in the 1000 genome database ) , the han chinese population ( china ) as representative of the east asia ( encoded chb ) , the british population ( england and scotland ) as representative of europe ( encoded gbr ) , and the population composed of americans of african ancestry in sw usa ( encoded asw ) .",
    "the snp loci were selected from the 22 autosomal chromosomes using the following criteria : ( i ) all @xmath61 analyzed individuals have a genotype characterized by a quality score @xmath62 ( on a phred scale ) , ( ii ) polymorphism is present in at least one of the @xmath61 individuals in order to fit the snp simulation algorithm of diyabc @xcite , ( iii ) the minimum distance between two consecutive snps is 1 kb in order to minimize linkage disequilibrium between snps , and ( iv ) snp loci showing significant deviation from hardy - weinberg equilibrium at a 1% threshold @xcite in at least one of the four populations have been removed ( 35 snp loci involved ) . after applying the above criteria",
    ", we obtained a dataset including 51,250 snp loci scattered over the 22 autosomes ( with a median distance between two consecutive snps equal to 7 kb ) among which 50,000 were randomly chosen for applying the proposed abc - rf methods .    in this application , we compared six scenarios ( i.e. , models ) of evolution of the four human populations genotyped at the above mentioned 50,000 snps .",
    "the six scenarios differ from each other by one ancient and one recent historical event : _",
    "( i ) _ a single out - of - africa colonization event giving an ancestral out - of - africa population which secondarily splits into one european and one east asia population lineage , versus two independent out - of - africa colonization events , one giving the european lineage and the other one giving the east asia lineage .",
    "the possibility of a second ancient ( i.e. , @xmath63 years ) out - of - africa colonization event through the arabian peninsula toward southern asia has been suggested by archaeological studies , e.g. @xcite ; _ ( ii ) _ the possibility ( or not ) of a recent genetic admixture of americans of african ancestry in sw usa between their african ancestors and individuals of european or east asia origins .",
    "the six different scenarios as well as the prior distributions of the time event and effective population size parameters used to simulate snp datasets using diyabc are detailed in figure [ fig : outofaf ] .",
    "we stress here that our intention is not to bring new insights into human population history , which has been and is still studied in greater details in a number of studies using genetic data , but to illustrate the potential of the proposed abc - rf methods for the statistical processing of large size snp datasets in the context of complex evolutionary histories .",
    "rf computations to discriminate among the six scenarios of figure [ fig : outofaf ] and evaluate error rates were processed on 10,000 , 20,000 , and 50,000 simulated datasets .",
    "we used all summary statistics offered by the diyabc software for snp markers @xcite ( see section g below ) , namely 130 summary statistics in this setting plus the five lda axes as additional summary statistics .",
    "for all illustrations based on genetic data , we used the program diyabc v2.0 @xcite to generate the abc reference tables including a set of simulation records made of model indices , parameter values and summary statistics for the associated simulated data .",
    "diyabc v2.0 is a multithreaded program which runs on three operating systems : gnu / linux , microsoft windows and apple mac os x. computational procedures are written in c++ and the graphical user interface is based on pyqt , a python binding of the qt framework .",
    "the program is freely available to academic users with a detailed notice document , example projects , and code sources ( linux ) from : ` http://www1.montpellier.inra.fr/cbgp/diyabc ` .",
    "the reference table generated this way then served as training database for the random forest constructions .",
    "for a given reference table , computations were performed using the r package ` randomforest ` @xcite .",
    "we have implemented all the proposed methodologies in the r package abcrf available on the cran .",
    "single population statistics + ` hp0_i ` : proportion of monomorphic loci for population i + ` hm1_i ` : mean gene diversity across polymorphic loci @xcite + ` hv1_i ` : variance of gene diversity across polymorphic loci + ` hmo_i ` : mean gene diversity across all loci    two population statistics + ` fp0_i&j ` : proportion of loci with null fst distance between the two samples for populations i and j @xcite + ` fm1_i&j ` : mean across loci of non null fst distances + ` fv1_i&j ` : variance across loci of non null fst distances + ` fmo_i&j ` : mean across loci of fst distances + ` np0_i&j ` : proportion of 1 loci with null nei s distance @xcite ` nm1_i&j ` : mean across loci of non null nei s distances + ` nv1_i&j ` : variance across loci of non null nei s distances + ` nmo_i&j ` : mean across loci of nei s distances    three population statistics + ` ap0_i_j&k ` : proportion of loci with null admixture estimate when pop .",
    "i comes from an admixture between j and k + ` am1_i_j&k ` : mean across loci of non null admixture estimate + ` av1_i_j&k ` : variance across loci of non null admixture estimated + ` amo_i_j&k ` : mean across all locus admixture estimates      single population statistics + ` nal_i ` : mean number of alleles across loci for population i + ` het_i ` : mean gene diversity across loci @xcite + ` var_i ` : mean allele size variance across loci + ` mgw_i ` : mean m index across loci @xcite    two population statistics + ` n2p_i&j ` : mean number of alleles across loci for populations i and j + ` h2p_i&j ` : mean gene diversity across loci + ` v2p_i&j ` : mean allele size variance across loci + ` fst_i&j ` : fst @xcite + ` lik_i&j ` : mean index of classification @xcite + ` das_i&j ` : shared allele distance @xcite + ` dm2_i&j ` : @xmath64 distance @xcite"
  ],
  "abstract_text": [
    "<S> approximate bayesian computation ( abc ) methods provide an elaborate approach to bayesian inference on complex models , including model choice . </S>",
    "<S> both theoretical arguments and simulation experiments indicate , however , that model posterior probabilities may be poorly evaluated by standard abc techniques .    </S>",
    "<S> we propose a novel approach based on a machine learning tool named random forests to conduct selection among the highly complex models covered by abc algorithms . </S>",
    "<S> we thus modify the way bayesian model selection is both understood and operated , in that we rephrase the inferential goal as a classification problem , first predicting the model that best fits the data with random forests and postponing the approximation of the posterior probability of the predicted map for a second stage also relying on random forests . </S>",
    "<S> compared with earlier implementations of abc model choice , the abc random forest approach offers several potential improvements : _ ( i ) _ it often has a larger discriminative power among the competing models , _ ( ii ) _ it is more robust against the number and choice of statistics summarizing the data , _ ( iii ) _ the computing effort is drastically reduced ( with a gain in computation efficiency of at least fifty ) , and _ </S>",
    "<S> ( iv ) _ it includes an approximation of the posterior probability of the selected model . </S>",
    "<S> the call to random forests will undoubtedly extend the range of size of datasets and complexity of models that abc can handle . </S>",
    "<S> we illustrate the power of this novel methodology by analyzing controlled experiments as well as genuine population genetics datasets .    </S>",
    "<S> the proposed methodologies are implemented in the r package abcrf available on the cran .    </S>",
    "<S> * keywords : * approximate bayesian computation , model selection , summary statistics , @xmath0-nearest neighbors , likelihood - free methods , random forests </S>"
  ]
}