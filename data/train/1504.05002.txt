{
  "article_text": [
    "consider the minimization problem @xmath0 where @xmath1 is a compact polyhedral set , @xmath2 and @xmath3 is strongly convex and continuously differentiable over @xmath4 .",
    "note that for a general matrix @xmath5 , the function @xmath6 is not necessarily strongly convex .",
    "when the problem at hand is large - scale , first order methods , which have relatively low computational cost per iteration , are usually utilized .",
    "these methods include , for example , the class of projected ( proximal ) gradient methods .",
    "a drawback of these methods is that under general convexity assumptions , they posses only a sublinear rate of convergence @xcite , while linear rate of convergence can be established only under additional conditions such as strong convexity of the objective function @xcite .",
    "luo and tseng @xcite showed that the strong convexity assumption can be relaxed and replaced by an assumption on the existence of a local error bound , and under this assumption , certain classes algorithms , which they referred to as  feasible descent methods \" , converge in an asymptotic linear time .",
    "the model ( [ eq : problem ] ) with assumptions on strong convexity of @xmath7 , compactness and polyhedrality of @xmath8 was shown in @xcite to satisfy the error bound . in  @xcite wang and lin extended the work @xcite and showed that there exists a _ global _ error bound for problem ( [ eq : problem ] ) with the additional assumption of compactness of @xmath8 ; and derived the exact linear rate for this case .",
    "we note that the family of  feasible descent methods \" include the block alternating minimization algorithm ( under the assumption of block strong convexity ) , as well as gradient projection methods , and therefore are usually at least as complex as evaluating the orthogonal projection operator onto the feasible set @xmath8 at each iteration .",
    "an alternative to algorithms which are based on projection ( or proximal ) operators are _ linear - oracle_-based algorithms such as the conditional gradient ( cg ) method .",
    "the cg algorithm was presented by frank and wolfe in 1956 @xcite , for minimizing a convex function over a compact polyhedral set . at each iteration",
    ", the algorithm requires a solution to the problem of minimizing a linear objective function over the feasible set .",
    "it is assumed that this solution is obtained by a call to a linear - oracle , i.e. , a black box which , given a linear function , returns an optimal solution of this linear function over the feasible set ( see an exact definition in section  [ sec : cg ] ) . in some instances , and specifically for certain types of polyhedral sets , obtaining such a linear - oracle can be done more efficiently than computing the orthogonal projection onto the feasible set ( see examples in @xcite ) , and therefore the cg algorithm has an advantage over projection - based algorithms . the original paper of frank and wolfe also contained a proof of an @xmath9 rate of convergence of the function values to the optimal value .",
    "levitin and polyak showed in @xcite that this @xmath9 rate can also be extended to the case where the feasible set is a general compact convex set .",
    "cannon and culum proved in @xcite that this rate is in fact _",
    "tight_. however , if in addition to strong convexity of the objective function , the optimal solution is in the interior of the feasible set , then linear rate of convergence of the cg method can be established @xcite .",
    "epelman and freund @xcite , as well as beck and teboulle @xcite showed a linear rate of convergence of the conditional gradient with a special stepsize choice in the context of finding a point in the intersection of an affine space and a closed and convex set under a slater - type assumption .",
    "another setting in which linear rate of convergence can be derived is when the feasible set is uniformly ( strongly ) convex and the norm of the gradient of the objective function is bounded away from zero @xcite . +",
    "another approach for deriving a linear rate of convergence is to modify the algorithm .",
    "for example , hazan and garber used _",
    "local _ linear - oracles in @xcite in order to show linear rate of convergence of a  localized \" version of the conditional gradient method . a different modification , which is viable when the feasible set is a compact polyhedral , is to use a variation of the conditional gradient method that incorporates away steps . this version of the conditional gradient method , which we refer to as _ away steps conditional gradient _ ( ascg ) , was initially suggested by wolfe in @xcite and then studied by guelat and marcotte @xcite , where a linear rate of convergence was established under the assumption that the objective function is strongly convex , as well as an assumption on the location of the optimal solution . in @xcite jaggi and lacoste - julien",
    "were able to extend this result for the more general model ( [ eq : problem ] ) for the case where @xmath10 , without restrictions on the location of the solution .",
    "we note that the ascg requires that the linear - oracle will produce an optimal solution of the associated problem which is an extreme point .",
    "we will call such an oracle a _",
    "vertex linear - oracle _ ( see the discussion in section [ sec : vertexlinearoracles ] )",
    ".    * contribution .",
    "* in this work , our starting point and main motivation are the results of jaggi and lacoste - julien @xcite .",
    "our contribution is threefold :    * we extend the results given in @xcite and show that the ascg algorithm converges linearly for the general case of problem  , that is , for any value of @xmath5 and @xmath11 . + the additional linear term @xmath12 enables us to consider much more general models .",
    "for example , consider the @xmath13-regularized least squares problem @xmath14 where @xmath15 is a compact polyhedral , @xmath16 and @xmath17 .",
    "since @xmath18 is compact , we can find a constant @xmath19 for which @xmath20 for any @xmath21 .",
    "we can now rewrite the model as @xmath22 } { \\left\\|{{{\\bf b}}{{\\bf x}}-{{\\bf c}}}\\right\\|}^2+\\lambda y,\\ ] ] which obviously fits the general model ( [ eq : problem ] ) * the analysis in @xcite assumes the existence of a _ vertex _ linear - oracle on the set @xmath23 , rather than an oracle for the set @xmath8 .",
    "this fact is not significant for the  pure \" cg algorithm , since it only requires a linear - oracle and not a _ vertex _ linear - oracle .",
    "this means that for the cg algorithm , a linear - oracle on @xmath23 can be easily obtained by applying @xmath5 on the output of the linear - oracle on @xmath8 . on the other hand",
    ", this argument fails for the ascg algorithm that specifically requires the oracle to return an extreme point of the feasible set , and finding such a vertex linear - oracle on @xmath23 might be a complex task , see section [ sec : vertexlinearoracles ] for more details .",
    "our analysis only requires a vertex linear - oracle on the original set @xmath8 .",
    "* we present an analysis based on simple duality arguments , which are completely different than the geometric arguments in @xcite .",
    "consequently , we obtain a computable constant for the rate of convergence , which is explicitly expressed as a function of the problem s parameters and the geometry of the feasible set .",
    "this constant , which we call  the vertex - facet distance constant \" , replaces the so - called _ pyramidal width _",
    "constant from @xcite , which reflects the geometry of the feasible set and is obtained as the optimal value of a very complex mixed integer saddle point optimization problem whose exact value is unknown even for simple polyhedral sets .    * paper layout .",
    "* the paper is organized as follows .",
    "section  [ sec : preliminaries ] presents some preliminary results and definitions needed for the analysis . in particular , it provides a brief introduction to the classical cg algorithm and linear oracles .",
    "section  [ sec : awaystepconditionalgradient ] presents the ascg algorithm and the convergence analysis , and is divided into four subsections . in section  [ sec : vertexlinearoracles ] the concept of vertex linear - oracle , needed for the implementation of ascg , is presented , and the difficulties of obtaining a vertex linear - oracle on a linear transformation of the feasible set are discussed . in section  [ sec : ascgmethod ] we present the ascg method with different possible stepsize choices . in section  [ sec : rateconvergenceanalysis ] , we provide the rate of convergence analysis of the ascg for problem  , and present the new _ vertex - facet distance _",
    "constant used in the analysis .",
    "finally , in section  [ sec : findingomegaforpolyhedrons ] , we demonstrate how to compute this new constant for a few examples of simple polyhedral sets .",
    "* notations . *",
    "we denote the cardinality of set @xmath24 by @xmath25 .",
    "the difference , union and intersection of two given sets @xmath24 and @xmath26 are denoted by @xmath27 , @xmath28 and @xmath29 respectively .",
    "subscript indices represent elements of a vector , while superscript indices represent iterates of the vector , i.e. , @xmath30 is the @xmath31th element of vector @xmath32 , @xmath33 is a vector at iteration @xmath34 , and @xmath35 is the @xmath31th element of @xmath33 .",
    "the vector @xmath36 is the @xmath31th vector of the standard basis of @xmath37 , @xmath38 is the all - zeros vector , and @xmath39 is the vector of all ones . given two vectors @xmath40 , their dot product is denoted by @xmath41 .",
    "given a matrix @xmath42 and vector @xmath43 , @xmath44 denotes the spectral norm of @xmath45 , and @xmath46 denotes the @xmath47 norm of @xmath32 , unless stated otherwise .",
    "@xmath48 , @xmath49 and @xmath50 represent the transpose , rank and image of @xmath45 respectively .",
    "we denote the @xmath31th row of a given matrix @xmath45 by @xmath51 , and given a set @xmath52 , @xmath53 is the submatrix of @xmath45 such that @xmath54 for any @xmath55 . if @xmath45 is a symmetric matrix , then @xmath56 is its minimal eigenvalue .",
    "if a matrix @xmath45 is also invertible , we denote its inverse by @xmath57 . given matrices @xmath58 and @xmath59 , the matrix @xmath60\\in { \\mathbb{r}}^{n\\times { ( m+k)}}$ ] is their horizontal concatenation .",
    "given a point @xmath32 and a closed convex set @xmath8 , the distance between @xmath32 and @xmath8 is denoted by @xmath61 . the standard unit simplex in @xmath37 is denoted by @xmath62 and its relative interior by @xmath63 .",
    "given a set @xmath64 , its convex hull is denoted by @xmath65 . given a convex set @xmath66 , the set of all its extreme points is denoted by @xmath67 .",
    "we start by presenting two technical lemmas .",
    "the first lemma is the well known _",
    "descent lemma _ which is fundamental in convergence rate analysis of first order methods .",
    "the second lemma is _",
    "hoffman s lemma _ which is used in various error bound analyses over polyhedral sets .",
    "[ lemma : descentlemma ] let @xmath68 be a continuously differentiable function with lipschitz continuous gradient with constant @xmath69 .",
    "then for any @xmath70 we have @xmath71    [ lemma : hoffman ] let @xmath8 be a polyhedron defined by @xmath72 , for some @xmath42 and @xmath73 , and let @xmath74 where @xmath75 and @xmath76 .",
    "assume that @xmath77 .",
    "then , there exists a constant @xmath78 , depending only on @xmath45 and @xmath79 , such that any @xmath80 satisfies @xmath81    a complete and simple proof of this lemma is given in ( * ? ? ?",
    "299 - 301 ) . defining @xmath82 as the set of all matrices constructed by taking linearly independent rows from the matrix @xmath83^t$ ]",
    ", we can write @xmath78 as @xmath84 we will refer to @xmath78 as the _ hoffman constant _ associated with matrix @xmath83^t$ ] .      throughout the article we make the following assumption regarding problem .",
    "[ ass : fx = gex ]    1 .",
    "[ ass : f lipschitz ] @xmath6 is continuously differentiable and has a lipschitz continuous gradient with constant @xmath69 .",
    "[ ass : g strongly convex ] @xmath7 is strongly convex with parameter @xmath85 .",
    "[ ass : xpolyhedron ] @xmath8 is a nonempty compact polyhedral set given by @xmath86 for some @xmath42 , @xmath87 .    we denote the optimal solution set of problem by @xmath88 .",
    "the diameter of the compact set @xmath8 is denoted by @xmath89 , and the diameter of the set @xmath23 ( the diameter of the image of @xmath8 under the linear mapping associated with matrix @xmath5 ) by @xmath90 .",
    "the two diameters satisfy the following relation : @xmath91 we define @xmath92 to be the maximal norm of the gradient of @xmath7 over @xmath23 .",
    "problem   possesses some properties , which we present in the following lemmas .",
    "[ lemma : optimalissubspace ] let @xmath88 be the optimal set of problem .",
    "then , there exists a constant vector @xmath93 and a scalar @xmath94 such that any optimal solution @xmath95 satisfies @xmath96 and @xmath97 .",
    "although the proof of the lemma in the given reference is for polyhedral sets , the extension for any convex set is trivial .",
    "[ lemma : ofupperbound ] let @xmath98 be the optimal value of problem  . then , for any @xmath80 @xmath99 where @xmath100 .",
    "let @xmath101 be some optimal solution of problem  , so that @xmath102 .",
    "then for any @xmath80 , it follows from the convexity of @xmath6 that @xmath103 where the last two inequalities are due to the cauchy - schwartz inequality and the definition of @xmath104,@xmath89 and @xmath90 .",
    "the following lemma provides an _ error bound _ ,",
    "i.e. , a bound on the distance of any feasible solution to the optimal set",
    ". this error bound will later be used as an alternative to a strong convexity assumption on @xmath6 , which is usually needed in order to prove a linear rate of convergence .",
    "this is a different bound than the one given in @xcite , since it relies heavily on the compactness of the set @xmath8 , thus enabling to circumvent the use of the so - called gradient mapping .",
    "[ lemma : errbound ] for any @xmath80 , @xmath105 where @xmath106 , and @xmath78 is the hoffman constant associated with matrix @xmath107^t$ ] .",
    "lemma  [ lemma : optimalissubspace ] implies that the optimal solution set @xmath88 can be defined as @xmath108 where @xmath109 for some @xmath110 and @xmath111 . for any @xmath80 ,",
    "applying lemma  [ lemma : hoffman ] with @xmath112^t$ ] , we have that @xmath113 where @xmath78 is the hoffman constant associated with matrix @xmath107^t$ ] .",
    "now , let @xmath80 and @xmath114 . utilizing the @xmath85-strong convexity of @xmath7",
    ", it follows that @xmath115 by the first order optimality conditions for problem  , we have ( recalling that @xmath80 and @xmath95 ) @xmath116 therefore , @xmath117 \\frac{\\sigma_g}{2}{\\left\\|{{{\\bf e}}{{\\bf x}}-{{\\bf t}}^*}\\right\\|}^2&\\leq { \\left\\langle{\\nabla f({{\\bf x}}^*)},{{{\\bf x}}-{{\\bf x}}^*}\\right\\rangle}+\\frac{\\sigma_g}{2}{\\left\\|{{{\\bf e}}{{\\bf x}}-{{\\bf e}}{{\\bf x}}^*}\\right\\|}^2\\\\ & = { \\left\\langle{\\nabla g({{\\bf e}}{{\\bf x}}^*)},{{{\\bf e}}{{\\bf x}}-{{\\bf e}}{{\\bf x}}^*}\\right\\rangle}+{\\left\\langle{{{\\bf b}}},{{{\\bf x}}-{{\\bf x}}^*}\\right\\rangle}+\\frac{\\sigma_g}{2}{\\left\\|{{{\\bf e}}{{\\bf x}}-{{\\bf e}}{{\\bf x}}^*}\\right\\|}^2 \\end{aligned}\\ ] ] now , using we can continue to obtain @xmath118    we are left with the task of upper bounding @xmath119 . by the definitions of @xmath94 and @xmath6 we have that @xmath120 { \\left\\langle{{{\\bf b}}},{{{\\bf x}}}\\right\\rangle}-s^*&={\\left\\langle{{{\\bf b}}},{{{\\bf x}}-{{\\bf x}}^*}\\right\\rangle}\\\\ & = { \\left\\langle{\\nabla f({{\\bf x}}^*)},{{{\\bf x}}-{{\\bf x}}^*}\\right\\rangle}-{\\left\\langle{\\nabla g({{\\bf e}}{{\\bf x}}^*)},{{{\\bf e}}{{\\bf x}}-{{\\bf e}}{{\\bf x}}^*}\\right\\rangle}\\\\ & = { \\left\\langle{\\nabla f({{\\bf x}}^*)},{{{\\bf x}}-{{\\bf x}}^*}\\right\\rangle}- { \\left\\langle{\\nabla g({{\\bf t}}^*)},{{{\\bf",
    "e}}{{\\bf x}}-{{\\bf t}}^*}\\right\\rangle}. \\end{aligned}\\ ] ] therefore , using , as well as the cauchy - schwartz inequality , we can conclude the following : @xmath121 on the other hand , exploiting , the convexity of @xmath6 and the cauchy - schwartz inequality , we also have that @xmath122 { \\left\\langle{{{\\bf b}}},{{{\\bf x}}}\\right\\rangle}-s^*&= { \\left\\langle{\\nabla f({{\\bf x}}^*)},{{{\\bf x}}-{{\\bf x}}^*}\\right\\rangle}- { \\left\\langle{\\nabla g({{\\bf t}}^*)},{{{\\bf e}}{{\\bf x}}-{{\\bf t}}^*}\\right\\rangle}\\\\ & \\leq f({{\\bf x}})-f^*-{\\left\\langle{\\nabla g({{\\bf t}}^*)},{{{\\bf e}}{{\\bf x}}-{{\\bf t}}^*}\\right\\rangle}\\\\ & \\leq f({{\\bf x}})-f^*+{\\left\\|{\\nabla g({{\\bf t}}^*)}\\right\\|}{\\left\\|{{{\\bf e}}{{\\bf x}}-{{\\bf t}}^*}\\right\\|}. \\end{aligned}\\ ] ] combining , , and the fact that @xmath123 , we obtain that @xmath124    moreover , the definitions of @xmath104 and @xmath90 imply @xmath125 , @xmath126 , and since @xmath80 , it follows from lemma  [ lemma : ofupperbound ] that @xmath127 .",
    "utilizing these bounds , as well as to bound results in @xmath128 ( { \\left\\langle{{{\\bf b}}},{{{\\bf x}}}\\right\\rangle}-s^*)^2&\\leq \\left(f({{\\bf x}})-f^*+g{\\left\\|{{{\\bf e}}{{\\bf x}}-{{\\bf t}}^*}\\right\\|}\\right)^2\\\\ & = ( f({{\\bf x}})-f^*)^2 + 2g{\\left\\|{{{\\bf e}}{{\\bf x}}-{{\\bf t}}^*}\\right\\|}(f({{\\bf x}})-f^*)+g^2{\\left\\|{{{\\bf e}}{{\\bf x}}-{{\\bf t}}^*}\\right\\|}^2\\\\ & \\leq ( f({{\\bf x}})-f^*)c+2gd_{{\\bf e}}(f({{\\bf x}})-f^*)+g^2\\frac{2}{\\sigma_g}(f({{\\bf x}})-f^*)\\\\ & = ( f({{\\bf x}})-f^*)\\left(c+2gd_{{\\bf e}}+\\frac{2g^2}{\\sigma_g}\\right)\\\\ & = ( f({{\\bf x}})-f^*)\\left({\\left\\|{{{\\bf b}}}\\right\\|}d+3gd_{{\\bf e}}+\\frac{2g^2}{\\sigma_g}\\right ) .",
    "\\end{aligned}\\ ] ] plugging and back into , we obtain the desired result : @xmath129      in order to present the cg algorithm , we first define the concept of linear oracles .    given a set @xmath8 , an operator @xmath130 is called a * linear oracle * for @xmath8 , if for each @xmath131 it returns a vector @xmath132 such that @xmath133 for any @xmath80 , i.e. , @xmath134 is a minimizer of the linear function @xmath135 over @xmath8 .",
    "linear oracles are black - box type functions , where the actual algorithm used in order to obtain the minimizer is unknown . for many feasible sets , such as @xmath136 balls and specific polyhedral sets ,",
    "the oracle can be represented by a closed form solution or can be computed by an efficient method .",
    "the cg algorithm and its variants are linear - oracle based algorithms .",
    "the original cg algorithm , presented in @xcite  also known as the frank - wolfe algorithm  is as follows .",
    "[ alg : cg ] + input : a linear oracle @xmath137 + initialize : @xmath138 for @xmath139    1 .",
    "compute @xmath140 .",
    "2 .   choose a stepsize @xmath141 .",
    "3 .   update @xmath142 .",
    "the algorithm is guaranteed to have an @xmath143 rate of convergence for stepsize determined according to exact line search @xcite , adaptive stepsize @xcite and predetermined stepsize @xcite .",
    "this upper bound on the rate of convergence is tight @xcite and therefore variants , such as the ascg were developed .",
    "the ascg algorithm was proposed by frank - wolfe in @xcite .",
    "a linear convergence rate was proven for problems consisting of minimizing strongly convex objective functions over polyhedral feasible sets in @xcite under some restrictions on the location of the optimal solution , and in @xcite without such restrictions .",
    "jaggi and lacoste - julien @xcite showed that the latter result is also applicable for the specific case of problem   where @xmath10 ( or more generally @xmath144 ) , provided that an appropriate linear - oracle is available for the set @xmath23 . in this section",
    ", we extend this result for the general case of problem  , i.e. , for any @xmath5 and @xmath11 .",
    "furthermore , we explore the potential issues with obtaining a linear - oracle for the set @xmath23 , and suggest an alternative analysis , which only assumes existence of an appropriate linear - oracle on the original set @xmath8 . moreover , our analysis differs from the one presented in @xcite by the fact that it is based on duality rather than geometric arguments .",
    "this approach enables to derive a computable constant for the rate of convergence , which is explicitly expressed as a function of the problem s parameters and the geometry of the feasible set .",
    "we separate the discussion of the ascg into four sections . in section  [ sec : vertexlinearoracles ] we define the concept of _ vertex linear oracles _ , which is needed for the ascg method , and the issues of obtaining such an oracle for linear transformations of simple sets .",
    "section  [ sec : ascgmethod ] contains a full description of the ascg method itself , including the concept of vertex representation , and representation reduction . in section  [ sec : rateconvergenceanalysis ] we present the rate of convergence analysis of the ascg for problem  , as well as introduce the new computable convergence constant @xmath145 .",
    "finally , in section  [ sec : findingomegaforpolyhedrons ] we demonstrate how to compute @xmath145 for three types of simple sets .",
    "the ascg algorithm requires a linear oracle which is a _ vertex linear oracle _ , a concept that we now define explicitly .",
    "given a polyhedral set @xmath8 with vertex set @xmath146 , a linear oracle @xmath147 is called a * vertex linear oracle * for @xmath8 , if for each @xmath131 it returns a vertex @xmath148 such that @xmath133 for any @xmath80 .",
    "notice that , according to the fundamental theorem of linear programming ( * ? ? ?",
    "* theorem 2.7 ) , the problem of optimizing any linear objective function over the compact set @xmath8 always has an optimal solution which is a vertex .",
    "therefore , the vertex linear oracle @xmath149 is well defined .",
    "we also note that in this paper the term ",
    "vertex \" is synonymous with the term  extreme point \"    in @xcite , jaggi and lacoste - julien proved that the ascg algorithm is affine invariant .",
    "this means that given the problem @xmath150 where @xmath7 is a strongly convex function and @xmath5 is some matrix , applying the ascg algorithm on the equivalent problem @xmath151 where @xmath152 , yields a linear rate of convergence , which depends only on the strong convexity parameter of @xmath7 and the geometry of the set @xmath153 ( regardless of what @xmath5 generated it ) .",
    "however , assuming that @xmath5 is not of a full column rank , i.e. , @xmath6 is not strongly convex , retrieving an optimal solution @xmath154 from the optimal solution @xmath155 requires solving a linear feasibility problem .",
    "this feasibility problem is equivalent to solving the following constrained least squares problem : @xmath156 which , for a general @xmath5 , may be more computationally expensive than simply applying the linear oracle on set @xmath8 . moreover , in order to apply the algorithm to problem  , a vertex linear oracle must be available for the set @xmath152 .",
    "assuming there exists a vertex linear oracle @xmath149 for @xmath8 , constructing such an oracle @xmath157 for @xmath23 may incur an additional computational cost per iteration",
    ". a naive approach to construct a general linear oracle @xmath158 , given @xmath149 , is by the formula @xmath159 however , the output @xmath160 of this linear oracle is not guaranteed to be a vertex of @xmath23 , and therefore , in order to obtain a vertex linear oracle @xmath161 , a vertex @xmath134 of @xmath23 with the same objective function value as @xmath162 must still be found . as an example , take @xmath8 to be the unit box in three dimensions , @xmath163 ^ 3\\subseteq { \\mathbb{r}}^3 $ ] , and let @xmath5 be given by @xmath164 we denote the vertex set @xmath146 of the set @xmath8 by the letters a - h as follows : @xmath165 and the linear mappings of these vertices by the matrix @xmath5 by a-h : @xmath166 the vertex set of @xmath23 is @xmath167 .    [ fig : x and ex ]     and @xmath23 ]    the sets @xmath8 and @xmath23 are presented in figure  [ fig : x and ex ] .",
    "notice that finding a vertex linear oracle for @xmath8 is trivial , while finding one for @xmath23 is not .",
    "in particular , a vertex linear oracle for @xmath8 may be given by any operator @xmath169 satisfying @xmath170 given the vector @xmath171 , we want to find @xmath172 using the naive approach , described in , we obtain a vertex of @xmath8 by applying the vertex linear oracle @xmath149 described in with parameter @xmath173 , which may return either one of the vertices b , c , g or h. if vertex c is returned , then its mapping c does not yield a vertex in @xmath23 . therefore , the oracle @xmath157 must now search for a vertex with the same objective function value , or alternatively , discover that c lies on the face defined by b and h , and consequently return one of these vertices .",
    "obviously , this is true for any @xmath174 such that @xmath175 returns one of the vertices c , d , e or g. this 3d example illustrates that , even for a simple @xmath8 , understanding the geometry of the set @xmath23 , let alone constructing a vertex linear oracle over it , is not trivial and becomes more complicated as the dimension of the problem increases .",
    "we aim to show that given a vertex linear oracle for @xmath8 , the ascg algorithm converges in a linear rate for problem  . since in our analysis",
    "we do not assume the existence of a vertex linear oracle for @xmath23 , but rather a vertex linear oracle for @xmath8 , the computational cost per iteration is independent of the matrix @xmath5 , and depends only on the geometry of @xmath8 .",
    "we will now present the ascg algorithm . in the following",
    "we denote the vertex set of @xmath8 as @xmath176 . moreover , as part of the ascg algorithm , at each iteration @xmath34 the iterate @xmath33 is represented as a convex combination of points in @xmath146 .",
    "specifically , @xmath33 is assumed to have the representation @xmath177 where @xmath178 .",
    "let @xmath179 , then @xmath180 and @xmath181 provide a compact representation of @xmath33 , and @xmath33 lies in the relative interior of the set @xmath182 . throughout the algorithm we update @xmath180 and @xmath183 via the vertex representation updating ( vru ) scheme .",
    "the ascg method has two types of updates : a _ forward step _ ,",
    "used in the classical cg algorithm , where a vertex is added to the representation , and an _ away step _ , unique to this algorithm , in which the coefficient of one of the vertices used in the representation is reduced or even nullified .",
    "specifically , the away step uses the direction @xmath184 where @xmath185 and step size @xmath186 so that @xmath187 and so @xmath188 . moreover ,",
    "if @xmath189 , then @xmath190 is nullified , and consequently , the vertex @xmath191 is removed from the representation .",
    "this vertex removal is referred to as a _ drop step_.    the full description of the ascg algorithm and the vru scheme is given as follows .",
    "[ alg : ascg ] + input : a vertex linear oracle @xmath192 + initialize : @xmath193 where @xmath194 , @xmath195 for any @xmath196 and @xmath197 for @xmath139    1 .",
    "compute @xmath198 .",
    "2 .   compute @xmath199 .",
    "3 .   if @xmath200 , then set @xmath201 and @xmath202 . + otherwise , set @xmath203 and @xmath204 4 .",
    "choose a stepsize @xmath141 .",
    "update @xmath205 .",
    "employ the vru procedure with input @xmath206 and obtain an updated representation @xmath207 .",
    "the stepsize in the ascg algorithm can be chosen according to one of the following stepsize selection rules , where @xmath208 and @xmath209 are as defined in the algorithm .",
    "@xmath210    [",
    "remark : monotone ] it is simple to show that under the above two choice of stepsize strategies , the sequence of function values @xmath211 is nonincreasing .    since the convergence rate analyses for both of these stepsize options is similar , we chose to conduct a unified analysis for both cases .",
    "following is exact definition of the vru procedure .",
    "[ alg : updatingschemeforukmuk ] + * input : * @xmath33 - current point .",
    "+ @xmath212 - vertex representation of @xmath33 , + @xmath213 - current direction and stepsize , + @xmath214 - candidate vertices .",
    "+ * output : * updated vertex representation @xmath207 of @xmath215 .",
    "+ if @xmath216 ( away step ) then    1 .",
    "update @xmath217 for any @xmath218 .",
    "2 .   update @xmath219 .",
    "3 .   if @xmath220 ( drop step ) , then update @xmath221 , otherwise @xmath222 .",
    "else ( @xmath223 - forward step )    1 .",
    "update @xmath224 for any @xmath225 .",
    "2 .   update @xmath226 .",
    "3 .   if @xmath227 , then update @xmath228 , otherwise update @xmath229 .",
    "update @xmath230 with @xmath231 being a representation reduction procedure with constant @xmath232 .",
    "the vru scheme uses a representation reduction procedure @xmath231 with constant @xmath232 , which is a procedure that takes a representation @xmath233 of a point @xmath32 and replaces it by a representation @xmath234 of @xmath32 such that @xmath235 and @xmath236 .",
    "we consider two possible options for the representation reduction procedure :    1 .",
    "@xmath231 is the trivial procedure , meaning it does not change the representation , in which case its constant is @xmath237 .",
    "the procedure @xmath231 is some implementation of the carathodory theorem ( * ? ? ?",
    "* section 17 ) , in which case its constant is @xmath238 .",
    "using this option will accelerate the algorithm when the number of vertices is not polynomial in the problem s dimension .",
    "a full description of the incremental representation reduction ( irr ) scheme , which applies the carathodory theorem efficiently in this context , is presented in appendix  [ appx : caratheodory ] .",
    "we will now prove the linear rate of convergence for the ascg algorithm for problem . in the following we use @xmath239 to denote the _ index set of the active constraints at @xmath32 _",
    ", @xmath240 similarly , for a given set @xmath241 , the set of active constraints for all the points in @xmath241 is defined as @xmath242    we present the following technical lemma , which is similar to a result presented by jaggi and lacoste - julien @xcite . in @xcite",
    "the proof is based on geometrical considerations , and utilizes the so - called  pyramidal width constant \" , which is the optimal value of a complicated optimization problem , whose value is unknown even for simple sets such as the unit simplex .",
    "in contrast , the proof below relies on simple linear programming duality arguments , and in addition , the derived constant @xmath145 , which replaces the pyramidal width constant , is computable for a many choices of sets @xmath8 .",
    "[ lemma : pyrwidth2 ] given @xmath243 and @xmath244 .",
    "if there exists a @xmath245 such that @xmath246 and @xmath247 , then @xmath248 where @xmath249 for @xmath250    by the fundamental theorem of linear programming @xcite , we can maximize the function @xmath135 on @xmath8 instead of on @xmath146 and get the same optimal value .",
    "similarly , we can minimize the function @xmath251 on @xmath252 instead of on @xmath241 , and obtain the same optimal value .",
    "therefore , @xmath253 \\max_{{{\\bf",
    "p}}\\in v , { { \\bf u}}\\in u}{\\left\\langle{{{\\bf c}}},{{{\\bf p}}-{{\\bf u}}}\\right\\rangle}&=\\max_{{{\\bf p}}\\in v}{\\left\\langle{{{\\bf c}}},{{{\\bf p}}}\\right\\rangle}-\\min_{{{\\bf u}}\\in u}{\\left\\langle{{{\\bf c}}},{{{\\bf u}}}\\right\\rangle}\\\\ & = \\max_{{{\\bf x}}\\in x } { \\left\\langle{{{\\bf c}}},{{{\\bf x}}}\\right\\rangle}-\\min_{{{\\bf y}}\\in \\operatorname{conv}(u)}{\\left\\langle{{{\\bf c}}},{{{\\bf y}}}\\right\\rangle}\\\\ & = \\max_{{{\\bf x}}:{{\\bf a}}{{\\bf x}}\\leq{{\\bf a } } } { \\left\\langle{{{\\bf c}}},{{{\\bf x}}}\\right\\rangle}+\\max_{{{\\bf y}}\\in \\operatorname{conv}(u)}{\\left\\{{-{\\left\\langle{{{\\bf c}}},{{{\\bf y}}}\\right\\rangle}}\\right\\}}. \\end{aligned}\\ ] ]    since @xmath8 is nonempty and bounded , the problem in @xmath32 is feasible and bounded above .",
    "therefore , by strong duality for linear programming , @xmath254 plugging back into we obtain : @xmath255 \\max_{{{\\bf p}}\\in v , { { \\bf u}}\\in u}{\\left\\langle{{{\\bf c}}},{{{\\bf p}}-{{\\bf u}}}\\right\\rangle}&=\\min_{{{\\boldsymbol \\eta}}\\in{\\mathbb{r}}^m_+ : { { \\bf a}}^t{{\\boldsymbol \\eta}}={{\\bf c } } } { \\left\\langle{{{\\bf a}}},{{{\\boldsymbol \\eta}}}\\right\\rangle}+\\max_{{{\\bf y}}\\in \\operatorname{conv}(u)}{\\left\\{{-{\\left\\langle{{{\\bf c}}},{{{\\bf y}}}\\right\\rangle}}\\right\\}}\\\\ & = \\min_{{{\\boldsymbol \\eta}}\\in{\\mathbb{r}}^m_+ : { { \\bf a}}^t{{\\boldsymbol \\eta}}={{\\bf",
    "c}}}\\max_{{{\\bf y}}\\in \\operatorname{conv}(u ) } { \\left\\langle{{{\\bf a}}-{{\\bf a}}{{\\bf y}}},{{{\\boldsymbol \\eta}}}\\right\\rangle}. \\end{aligned}\\ ] ] since @xmath256 is in @xmath252 , we have that @xmath257 for any value of @xmath258 , and therefore , @xmath259    using strong duality on the rhs of , we obtain that @xmath260 denote @xmath261 and @xmath262 . from the definition of @xmath263 , it follows that @xmath264 for all @xmath265 , and that for any @xmath266 there exists at least one vertex @xmath265 such that @xmath267 , and hence , @xmath268 which in particular implies that @xmath269 since @xmath270 , we can conclude from ( [ 539 ] ) and ( [ 541 ] ) that @xmath271 therefore , replacing the rhs of the set of inequalities @xmath272 in by the bounds given in , we obtain that @xmath273    combining , , and it follows that @xmath274 where @xmath275 we will now show that it is not possible for @xmath276 to satisfy @xmath277 .",
    "suppose by contradiction @xmath276 satisfies does satisfy @xmath277 .",
    "then @xmath278 is a feasible solution of problem   for any @xmath279 , and since @xmath247 we obtain that @xmath280 as @xmath281 , and thus @xmath282 .",
    "however , since @xmath146 contains a finite number of points , the lhs of is bounded from above , and so @xmath283 in contradiction . therefore , there exists @xmath266 such that @xmath284 . since @xmath285",
    ", the vector @xmath286 is well defined .",
    "moreover , @xmath287 satisfies @xmath288 and @xmath289 { { \\bf a}}_i\\overline{{{\\bf x}}}={{\\bf a}}_i{{\\bf z}}\\frac{\\omega_x}{|u|{\\left\\|{{{\\bf z}}}\\right\\|}}&\\leq { \\left\\|{{{\\bf",
    "a}}_i}\\right\\|}{\\left\\|{{{\\bf z}}}\\right\\|}\\frac{\\zeta}{|u|{\\left\\|{{{\\bf z}}}\\right\\| } \\varphi   } \\leq",
    "\\frac{\\zeta}{|u|},\\quad \\forall i\\in \\overline{j } , \\end{aligned}\\ ] ] where the first inequality follows from the cauchy - schwartz inequality and the second inequality follows from the fact that if @xmath290 , then @xmath291 and so @xmath292 . consequently , and imply that @xmath287 is a feasible solution for problem .",
    "therefore , @xmath293 , which by yields @xmath294    the constant @xmath145 represents a normalized minimal distance between the hyperplanes that contain facets of @xmath8 and the vertices of @xmath8 which do not lie on those hyperplanes . we will refer to @xmath145 as _ the vertex - facet distance of @xmath8_. examples for the derivation of @xmath145 for some simple polyhedral sets can be found in section  [ sec : findingomegaforpolyhedrons ] .    the following lemma is a technical result stating that the active constraints at a given point are the same as the active constraints of the set of vertices in its compact representation .",
    "[ lemma : ix_iu ] let @xmath80 and the set @xmath243 satisfy @xmath295 , where @xmath296",
    ". then @xmath297 .",
    "it is trivially true that @xmath298 since @xmath32 is a convex combination of points in the affine space defined by @xmath299 .",
    "we will prove that @xmath300 .",
    "any @xmath301 satisfies @xmath302 .",
    "assume to the contrary , that there exists @xmath303 such that some @xmath304 satisfies @xmath305 .",
    "since @xmath306 and @xmath307 , it follows that @xmath308 in contradiction to the assumption that @xmath303 .",
    "[ cor : boundoninnerprod ] for any @xmath309 which can be represented as @xmath295 for some @xmath296 and @xmath243 , it holds that , @xmath310    for any @xmath309 define @xmath311 .",
    "it follows from lemma  [ lemma : ix_iu ] that @xmath312 . for any @xmath95 ,",
    "the vector @xmath313 satisfies @xmath314 and , from the convexity of @xmath6 , as well as the optimality of @xmath101 , @xmath315 .",
    "therefore , invoking lemma  [ lemma : pyrwidth2 ] achieves the desired result .",
    "we now present the main theorem of this section , which establishes the linear rate of convergence of ascg for problem  .",
    "this theorem is an extension of ( * ? ? ?",
    "* thorem 7 ) , and the proof follows the same general arguments , while incorporating the use of the error bound from lemma  [ lemma : errbound ] and the new constant @xmath145 .",
    "[ thm : convergenceofascg ] let @xmath316 be the sequence generated by the ascg algorithm for solving problem using a representation reduction to procedure @xmath231 with constant @xmath232 , and let @xmath98 be the optimal value of the problem .",
    "then for any @xmath317 @xmath318 where @xmath319 @xmath106 with @xmath78 being the hoffman constant associated with matrix @xmath107^t$ ] , @xmath100 , and @xmath145 is the vertex - facet distance of @xmath8 given in ( [ defomega ] ) .    for each @xmath34 we will denote the stepsize generated by exact line search as @xmath320 and the adaptive stepsize as @xmath321",
    ". then @xmath322 from lemma  [ lemma : descentlemma ] ( the descent lemma ) , we have that @xmath323 assuming that @xmath324 , then for any @xmath95 we have that @xmath325 { \\langle{\\nabla f({{\\bf x}}^k)},{{{\\bf d}}^k}\\rangle}&=\\min{\\left\\ { { { \\langle{\\nabla f({{\\bf x}}^k)},{{{\\bf p}}^k-{{\\bf x}}^k}\\rangle } , { \\langle{\\nabla f({{\\bf x}}^k)},{{{\\bf x}}^k-{{\\bf u}}^k}\\rangle}}\\right\\}}\\\\ & \\leq { \\langle{\\nabla f({{\\bf x}}^k)},{{{\\bf p}}^k-{{\\bf x}}^k}\\rangle}\\\\ & \\leq { \\langle{\\nabla f({{\\bf x}}^k)},{{{\\bf x}}^*-{{\\bf x}}^k}\\rangle}\\\\ & \\leq f^*-f({{\\bf x}}^k ) , \\end{aligned}\\ ] ]",
    "where the first equality is derived from the algorithm s specific choice of @xmath208 , the third line follows from the fact that @xmath326 , and the fourth line follows from the convexity of @xmath6 .",
    "in particular , @xmath327 , and by it follows that @xmath321 is equal to @xmath328 we now separate the analysis to three cases : ( a ) @xmath223 and @xmath329 , ( b ) @xmath216 and @xmath329 , and ( c ) @xmath330 .    in cases ( a ) and ( b ) , it follows from that @xmath331 using inequalities , and , we obtain @xmath332 subtracting @xmath98 from both sides of the inequality and using , we have that @xmath333            f({{\\bf x}}^{k+1})-f^ *          & \\leq   f({{\\bf x}}^k)-f^*+\\frac{\\overline{\\gamma}^k}{2}{\\langle{\\nabla f({{\\bf x}}^k)},{{{\\bf d}}^k}\\rangle}\\\\          & \\leq ( f({{\\bf x}}^k)-f^*)\\left(1-\\frac{\\overline{\\gamma}^k}{2}\\right ) .      \\end{aligned}\\ ] ] in case ( a ) , @xmath334 , and hence @xmath335 in case ( b ) , we have no positive lower bound on @xmath209 , and therefore we can only conclude , by the nonnegativity of @xmath209 , that @xmath336 however , case ( b ) is a drop step , meaning in particular that @xmath337 , since before applying the representation reduction procedure @xmath231 , we eliminate one of the vertices in the representation of @xmath338 . denoting the number of drop steps until iteration @xmath34 as @xmath339 , and the number of forward steps until iteration @xmath34 as @xmath340 , it follows from the algorithm s definition that @xmath341 ( at each iteration we add a vertex , remove a vertex , or neither ) and @xmath342 ( the number of removed vertices can not exceed the number of added vertices ) , and therefore @xmath343 .",
    "we arrive to case ( c ) . in this case",
    ", implies @xmath344 which combined with and results in @xmath345 from the algorithm s specific choice of @xmath208 , we obtain that @xmath346 applying the bound in and the inequality @xmath347 to , it follows that @xmath348 by the definitions of @xmath191 and @xmath349 , and since applying representation reduction procedure @xmath231 ensures that that @xmath350 , corollary  [ cor : boundoninnerprod ] implies that for any @xmath95 , @xmath351 lemma  [ lemma : errbound ] implies that there exists @xmath95 such that @xmath352 , which combined with convexity of @xmath6 , bounds from below as follows : @xmath353 { \\langle{\\nabla f({{\\bf x}}^k)},{{{\\bf u}}^k-{{\\bf p}}^k}\\rangle}^2 & \\geq \\left(\\frac{\\omega_x}{n}\\right)^2\\frac{{\\left\\langle{\\nabla f({{\\bf x}}^k)},{{{\\bf x}}^k-{{\\bf x}}^*}\\right\\rangle}^2}{{\\left\\|{{{\\bf x}}^k-{{\\bf x}}^*}\\right\\|}^2}\\\\ & \\geq \\left(\\frac{\\omega_x}{n}\\right)^2\\frac{(f({{\\bf x}}^k)-f({{\\bf x}}^*))^2}{{\\left\\|{{{\\bf x}}^k-{{\\bf x}}^*}\\right\\|}^2}\\\\ & \\geq \\left(\\frac{\\omega_x}{n}\\right)^2\\frac{(f({{\\bf x}}^k)-f^*)^2}{\\kappa(f({{\\bf x}}^k)-f^*)}\\\\ & = \\frac{(\\omega_x)^2}{n^2\\kappa}(f({{\\bf x}}^k)-f^ * ) , \\end{aligned}\\ ] ] which along with yields @xmath354                f({{\\bf x}}^{k+1})-f^ *                  & \\leq f({{\\bf x}}^k)-f^*-\\frac{{\\langle{\\nabla f({{\\bf x}}^k)},{{{\\bf u}}^k-{{\\bf p}}^k}\\rangle}^2}{8\\rho d^2}\\\\          & \\leq ( f({{\\bf x}}^k)-f^*)\\left(1-\\frac{(\\omega_x)^2}{8\\rho\\kappa d^2n^2}\\right ) \\end{aligned}\\ ] ] therefore , if either of the cases ( a ) or ( c ) occurs , then by ( [ 666 ] ) and , it follows that @xmath355 where @xmath356 is defined in ( [ defalpha ] ) .",
    "we can therefore conclude from cases ( a)-(c ) that until iteration @xmath34 we have at least @xmath357 iterations for which ( [ 718 ] ) holds , and therefore @xmath358 applying lemma  [ lemma : ofupperbound ] for @xmath359 we obtain @xmath360 , and the desired result ( [ 613 ] ) follows .      in this section ,",
    "we demonstrate how to compute the vertex - facet distance constant @xmath145 for a few simple polyhedral sets .",
    "we consider three sets : the unit simplex , the @xmath361 ball and the @xmath362 ball .",
    "we first describe each of the sets as a system of linear inequalities of the form @xmath363 . then , given the parameters @xmath45 and @xmath364 , as well as the vertex set @xmath146 , @xmath145 can be computed by its definition , given by ( [ defomega ] ) .",
    "_ _ the unit simplex @xmath365 can be represented by @xmath366 { { \\bf a}}=\\begin{bmatrix}-{{\\bf i}}_{n \\times n}\\\\{{\\bf 1}}_n^t\\\\-{{\\bf 1}}_n^t\\end{bmatrix}\\in { \\mathbb{r}}^{(n+2)\\times n},\\ ; { { \\bf a}}=\\begin{bmatrix}{{\\bf 0}}_n\\\\1\\\\1\\end{bmatrix}\\in{\\mathbb{r}}^{(n+2)}. \\end{aligned}\\ ] ] the set of extreme points is given by @xmath367 .",
    "notice that since there are only @xmath368 extreme points which are all affinely independent , using a rank reduction procedure which implements the carathodory theorem is the same as applying the trivial procedure that does not change the representation . in order to calculate @xmath145",
    ", we first note that @xmath369 , and therefore @xmath370 and @xmath371 which means that @xmath372 .",
    "_ _ the @xmath361 ball is given by the set @xmath373 therefore @xmath374 and each row of the matrix @xmath375 is a vector in @xmath376 .",
    "the set of extreme points is given by @xmath377 , and therefore has cardinality of @xmath378 .    finally , we have that @xmath379 and @xmath380 which means that @xmath381 .    _",
    "_ the @xmath362 ball is represented by @xmath366 { { \\bf a}}=\\begin{bmatrix}{{\\bf i}}\\\\-{{\\bf i}}\\end{bmatrix}\\in { \\mathbb{r}}^{2n\\times n},\\ ; { { \\bf a}}=\\begin{bmatrix}{{\\bf 1}}\\\\{{\\bf 1}}\\end{bmatrix}\\in{\\mathbb{r}}^{2n}. \\end{aligned}\\ ] ] the set of extreme points is given by @xmath382 , which in particular implies that @xmath383 .",
    "therefore , for large - scale problems , using the representation reduction procedure , which is based on carathodory theorem , is crucial in order to obtain a practical implementation .    from the definition of @xmath45 and @xmath146",
    ", it follows that @xmath384 and @xmath385 which implies that @xmath386 .    10    a.  beck and m.  teboulle . a conditional gradient method with linear rate of convergence for solving convex linear systems .",
    ", 59(2):235247 , 2004 .",
    "a.  beck and m.  teboulle .",
    "gradient - based algorithms with applications to signal recovery problems . in d.",
    "palomar and y.  eldar , editors , _ convex optimization in signal processing and communications _ , pages 139162 .",
    "cambridge university press , 2009 .",
    "d.  p. bertsekas . .",
    "athena scientific , belmont , ma , 2nd edition , 1999 .",
    "d.  bertsimas and j.  n. tsitsiklis . ,",
    "volume  6 .",
    "athena scientific belmont , ma , 1997 .",
    "m.  d. canon and c.  d. cullum . a tight upper bound on the rate of convergence of frank - wolfe algorithm .",
    ", 6(4):509516 , 1968 .",
    "j.  dunn and s.  harshbarger .",
    "conditional gradient algorithms with open loop step size rules .",
    ", 62(2):432  444 , 1978 .",
    "m.  epelman and r.  m. freund .",
    "condition number complexity of an elementary algorithm for computing a reliable solution of a conic linear system .",
    ", 88(3):451485 , 2000 .",
    "m.  frank and p.  wolfe .",
    "an algorithm for quadratic programming .",
    ", 3(1 - 2):95110 , 1956 .",
    "d.  garber and e.  hazan .",
    "a linearly convergent conditional gradient algorithm with applications to online and stochastic optimization .",
    "d.  goldfarb and m.  j. todd .",
    "chapter ii : linear programming . in g.",
    "nemhauser , a.  r. kan , and m.  todd , editors , _ optimization _ , volume  1 of _ handbooks in operations research and management science _ , pages 73  170 .",
    "elsevier , 1989 .",
    "j.  guelat and p.  marcotte . some comments on wolfe s ` away step ' .",
    ", 35(1):110119 , 1986 .",
    "o.  gler . .",
    "graduate texts in mathematics .",
    "springer , new york , ny , usa , 2010 .",
    "a.  j. hoffman .",
    "on approximate solutions of systems of linear inequalities . , 49(4):263265 , 1952 .",
    "s.  lacoste - julien and m.  jaggi .",
    "an affine invariant linear convergence analysis for frank - wolfe algorithms .",
    ", 2014 .",
    "e.  levitin and b.  t. polyak . constrained minimization methods .",
    ", 6(5):787823 , 1966 .",
    "y.  nesterov . , volume  87 .",
    "springer , 2004 .    z.  quan luo and p.  tseng .",
    "error bounds and convergence analysis of feasible descent methods : a general approach . , 46 - 47(1):157178 , 1993",
    ".    r.  t. rockafellar . .",
    "princeton university press , 2nd edition , 1970 .",
    "wang and c .- j .",
    "iteration complexity of feasible descent methods for convex optimization .",
    ", 15:15231548 , 2014 .",
    "p.  wolfe .",
    ", chapter chapter 1:convergence theory in nonlinear programming .",
    "north - holland publishing company , 1970 .",
    "in this section we will show a way to efficiently and incrementally implement the constructive proof of carathodory theorem , as part of the vru scheme , at each iteration of the ascg algorithm .",
    "we note that this reduction procedure does not have to be employed , and instead the trivial procedure , which does not change the representation can be used . in that case , the upper bound on the number of extreme points in the representation is just the number of extreme points of the feasible set @xmath8 .",
    "+ the implementation described in this section will allow maintaining a vertex representation set @xmath180 , with cardinality of at most @xmath387 , at a computational cost of @xmath388 operations per iteration .",
    "for this purpose , we assume that at the beginning of iteration @xmath34 , @xmath338 has a representation with vertex set @xmath389 , such that the vectors in the set are affinely independent .",
    "moreover , we assume that at the beginning of iteration @xmath34 , we have at our disposal two matrices @xmath390 and @xmath391 .",
    "we define @xmath392 to be the matrix whose @xmath31th column is the vector @xmath393 for @xmath394 , where @xmath395 is called the reference vertex .",
    "the matrix @xmath396 is a product of elementary matrices , which ensures that the matrix @xmath397 is in row echelon form .",
    "the implementation does not require to save the matrix @xmath398 , and so at each iteration , only the matrices @xmath396 and @xmath399 are updated .",
    "let @xmath400 be the vertex set and let @xmath401 be the coefficients vector at the end of iteration @xmath34 , before applying the rank reduction procedure .",
    "updating the matrices @xmath402 and @xmath403 , as well as @xmath400 and @xmath401 , is done according to the following _ incremental representation reduction _ scheme , which is partially based on the proof of carathodory theorem presented in ( * ? ? ?",
    "* section 17 ) .",
    "[ alg : caratheodory ] + * input * : representation @xmath207 of point @xmath404 , set @xmath405 of affinely +  independent vectors , and matrices @xmath406 and @xmath407 . + * output * : updated representation @xmath207 of @xmath404 , and matrices @xmath408 +   and @xmath409 .    1 .   set @xmath410 .",
    "2 .   update @xmath411 .",
    "3 .   if @xmath412 , then set the matrix @xmath402 to be empty and @xmath413 .",
    "4 .   else , if @xmath414 , then set @xmath415 .",
    "[ step : irr_dropstep ] else , if @xmath416 ( drop step ) , then 1 .",
    "find @xmath417 such that @xmath418 .",
    "[ step : irr_changereferencevertex ] if @xmath419 ( the reference vertex was removed ) , then remove the first column of @xmath399 and change reference vertex to @xmath420 , using the update formula @xmath421 where @xmath422 .",
    "( a non - reference vertex was removed ) , remove column @xmath423 from @xmath402 .",
    "6 .   else , if @xmath424 ( forward step ) , then 1 .",
    "find @xmath425 .",
    "2 .   compute @xmath426 .",
    "3 .   update the matrix @xmath427 $ ] .",
    "[ step : irr_findrowrank]compute @xmath428 - the row rank of @xmath429 .",
    "5 .   if @xmath430 , then 1 .",
    "[ step : irr_solsyseq ] find a solution @xmath431 of the following system + @xmath432 2 .",
    "set the vector @xmath433 to be + @xmath434 3 .",
    "compute @xmath435 and @xmath436 and set + @xmath437 4 .",
    "update @xmath438 for all @xmath439 .",
    "[ step : irr_findnullset ] compute @xmath440 .",
    "[ step : irr_removevertices ] for each @xmath441 remove column @xmath442 matrix @xmath402 .",
    "[ step : irr_updaterepresentation ] update @xmath443 . 7 .",
    "[ step : irr_rowelimination]if @xmath429 is not in row echelon form , then construct a matrix @xmath444 , as a composition of elementary matrices , such that @xmath445 is row echelon form , and update @xmath446 and @xmath447 .",
    "notice that in order to compute the row rank of the matrix @xmath402 in step  [ step : irr_findrowrank ] , we may simply convert the matrix to row echelon form , and then count the number of nonzero rows .",
    "this is done similarly to step  [ step : irr_rowelimination ] , and requires ranking of at most one column .",
    "we will need to rerank the matrix in step  [ step : irr_rowelimination ] only if @xmath430 , and subsequently at least one column is removed in step  [ step : irr_removevertices ] .",
    "the irr scheme may reduce the size of the input @xmath400 only in the case of a forward step , since otherwise the vertices in @xmath400 are all affinely independent .",
    "nonetheless , the irr scheme _ must _ be applied at each iteration in order to maintain the matrices @xmath448 and @xmath396 .",
    "the efficiency of the scheme relies on the fact that only a small number of vertices are either added to or removed from the representation .",
    "the potentially computationally expensive steps are : step [ step : irr_changereferencevertex ] - replacing the reference vertex , step  [ step : irr_findrowrank ] - finding the row rank of @xmath402 , step [ step : irr_solsyseq ] - solving the system of linear equalities , step [ step : irr_removevertices ] - removing columns corresponding with the vertices eliminated from the representation , and step [ step : irr_rowelimination ] - the ranking of the resulting matrix @xmath429 .",
    "step [ step : irr_changereferencevertex ] can be implemented without explicitly using matrix multiplication and therefore has a computational cost of @xmath388 .",
    "since @xmath448 was in row echelon form , step  [ step : irr_findrowrank ] requires a row elimination procedure , similar to step  [ step : irr_rowelimination ] , to be conducted only on the last column of @xmath429 , which involves at most @xmath449 operations and an additional @xmath388 operation for updating @xmath403 .",
    "moreover , since @xmath448 was full column rank , the irr scheme guarantees that in step @xmath450 the vector @xmath431 has a unique solution , and since @xmath429 is in row echelon form , it can be found in @xmath388 operations .",
    "moreover , in step [ step : irr_removevertices ] , the specific choice of @xmath451 ensures that the reference vertex @xmath395 is not eliminated from the representation , and so there is no need to change the reference vertex at this stage .",
    "furthermore , it is reasonable to assume that the set @xmath24 satisfies @xmath452 , since otherwise the vector @xmath404 , produced by a forward step , can be represented by significantly less vertices than @xmath33 , which , although possible , is numerically unlikely .",
    "therefore , assuming that indeed @xmath452 , the matrix @xmath444 , calculated in step [ step : irr_rowelimination ] , applies a row elimination procedure to at most @xmath453 rows ( one for each column removed from @xmath402 ) or one column ( if a column was added to @xmath402 ) .",
    "conducting such an elimination on either row or column takes at most @xmath388 operations , which may include row switching and at most @xmath368 row addition and multiplication .",
    "therefore , the total computational cost of the irr scheme amounts to @xmath388 ."
  ],
  "abstract_text": [
    "<S> we consider the problem of minimizing a function , which is the sum of a linear function and a composition of a strongly convex function with a linear transformation , over a compact polyhedral set . </S>",
    "<S> jaggi and lacoste - julien @xcite showed that the conditional gradient method with away steps employed on the aforementioned problem without the additional linear term has linear rate of convergence , depending on the so - called pyramidal width of the feasible set . </S>",
    "<S> we revisit this result and provide a variant of the algorithm and an analysis that is based on simple duality arguments , as well as corresponding error bounds . </S>",
    "<S> this new analysis ( a ) enables the incorporation of the additional linear term , ( b ) does not require a linear - oracle that outputs an extreme point of the linear mapping of the feasible set and ( c ) depends on a new constant , termed  the vertex - facet distance constant \" , which is explicitly expressed in terms of the problem s parameters and the geometry of the feasible set . </S>",
    "<S> this constant replaces the pyramidal width , which is difficult to evaluate . </S>"
  ]
}