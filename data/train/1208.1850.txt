{
  "article_text": [
    "despite convincing gravitational evidence for the existence of dark matter ( dm ) in our universe ( from galactic to cluster scales ) its nature remains a mystery .",
    "yet great progress has been made .",
    "in particular direct detection experiments have set progressively stronger limits on the properties of dark matter @xcite , gaining several orders of magnitude in less than a decade for masses in the @xmath1 gev to tev range .",
    "several direct detection experiments have reported dark matter - like events in their data ( e.g. cogent @xcite , cresst - ii @xcite and dama @xcite ) , with the most recent positive result coming from the cdms - si experiment @xcite . such hints are in tension with the limits published by the lux @xcite and xenon100 @xcite collaborations .",
    "however several authors have claimed that the systematic uncertainties inherent in their analysis may provide a way of reducing such tension @xcite .",
    "in addition if one moves beyond the most basic model of dm - quark scattering and considers e.g. inelastic scattering or isospin - violating dm , where the coupling to neutrons and protons is different , then such tension can also be greatly reduced @xcite .",
    "given the present situation , it is essential to exploit all the information contained in the data . in this article",
    "we propose a bayesian approach , based on the information hamiltonian , with a view to providing the community with a a novel and robust interpretation of these conflicting experimental signals .",
    "this is not the first bayesian analysis of direct detection data @xcite , however our method is distinct in that it extracts the maximum amount of information from the available data , by exploiting the differences between expected signal and background events . for the purpose of illustration",
    ", we will make use of data from the xenon100 calibration @xcite .",
    "this is an independent analysis of xenon100 data , and will enable us to check and also confront our new method with the collaboration s approach .",
    "this example is also highly relevant for the lux experiment , which works under a similar principle .    as we will show for the case where there are signal - like points in the data our method is particularly powerful , since one can simultaneously set an exclusion limit and define a potential signal region using bayesian regions of credibility",
    "this is in contrast to current analytical approaches , which usually involve methods designed only to set limits , such as the @xmath2 method @xcite , or the profile likelihood analysis with the cl@xmath3 method @xcite .",
    "we do not claim that our method is technically superior for all cases , however our approach is particularly transparent and easily generalised to many different data - sets .    in section [ sec : ih ]",
    "we first introduce our method and show how to apply it to direct detection experimental data in general ; this includes a discussion of when to set limits or claim discovery . in section [ sec : x100 ]",
    "we apply our method to data from the xenon100 experiment @xcite as a worked example and conclude in section [ sec : conc ] .",
    "our method is inspired from information theory , in the sense that it employs bayesian techniques ( see @xcite for a review ) with the aim to fully exploit the different expected distributions of signal and background events .",
    "ultimately this should either enhance the characteristics of a potential signal ( and therefore the evidence for a dark matter discovery ) , or place stringent bounds on dark matter models .    before proceeding",
    ", we would like to clarify the distinction between this approach , and the profile likelihood method used by e.g. the lux @xcite and xenon100 @xcite collaborations to set upper limits on the dm - nucleon cross section ( and also by cdms to fit to their data @xcite ) .",
    "the major difference is that our approach is bayesian and the profile likelihood is frequentist , and hence for example both methods have different ways of dealing with nuisance parameters .",
    "however , in most cases , with the same likelihood function the bayesian and profile likelihood results should agree , and each can provide an important cross - check of the other .    in section [ sec : x100 ]",
    "we discuss the xenon100 experiment as a worked example , and when referring to the profile likelihood in this context we mean specifically the likelihood used by the xenon100 collaboration @xcite to analyse their data .",
    "indeed , as an alternative , the lux collaboration @xcite also use a profile likelihood method , but not necessarily the same likelihood function as xenon100 . in the absence of any nuisance parameters , a profile likelihood analysis performed with our likelihood function should give similar limits to those derived in this work using a bayesian approach .",
    "even so , the two approaches are distinct and should be considered complimentary to each other .      our general strategy is to treat any 2d data - set effectively as an image , which we pixelate and exploit using pattern recognition .",
    "said differently , we map the data contained in a 2d plot onto a 2d data - space @xmath4 .",
    "a point @xmath5 in this space is identified by its two coordinates @xmath6 and @xmath7 , the coordinates of the initial plot and in fact the discrimination parameters used to identify events ( e.g. scintillation intensity , ionisation , phonon signals ) .",
    "the next key step is to then grid the data - space by pixelating it into @xmath8 two - dimensional bins of equal size in @xmath9 given by @xmath10 and labelled with the index @xmath11 .",
    "if such 2d - bins are chosen to be small enough , the ability of the analysis to discriminate between signal and background will be maximised . within a pixel @xmath11 at position @xmath12 in the @xmath6-@xmath7 plane",
    "there will be a certain number @xmath13 of _ experimental _ data - points , each of which are identified by their coordinates @xmath14 ( with @xmath15 running from 0 to @xmath16 , the total number of data - points in the whole space ) .",
    "for the same pixel , the _ theoretically expected _ number of points is given by @xmath17 .",
    "hence we can compare @xmath13 to @xmath18 given fluctuations in the latter , which we assume obey poisson statistics .",
    "the function @xmath19 is the expected distribution of events , which constitutes the theoretical expectation of both the background and possible signal in a pixel @xmath20 .. ]      we can now analyse the data using the method described above .",
    "the main issue is to find for which theoretical parameters is @xmath18 closest to @xmath13 for all pixels @xmath11 , within poisson fluctuations .",
    "if there is no dm signal in the data , one expects that for the configuration where @xmath18 is closest to @xmath13 that the former is equal to the theoretically expected number of background events in each pixel .    for this purpose , we will define a poisson likelihood to describe the theoretical number of background and signal - like events in each pixel @xmath11 . here",
    "@xmath18 represents the mean expectation value of the number of points expected in each pixel @xmath11 .",
    "such a likelihood is given by , @xmath21 in this expression , @xmath22 represents the data and @xmath23 the signal .    to make the interpretation easier",
    ", we decompose @xmath19 into a dm component @xmath24 composed entirely of nuclear recoils ( nr ) and a background component @xmath25 ( dominantly electronic recoils ( er ) , but with a possible nr component ) , leading to @xmath26 . the predominance of the signal @xmath24 over the background @xmath25 essentially depends on the number of signal events with respect to that from the background , at a given location in the data - space @xmath20 . since both the number of events and the location are important , and since the location depends on the dm mass ( i.e. can be computed once for each mass ) , we have explicitly separated out these two contributions .",
    "our calculations are therefore significantly speeded up by using the decomposition : @xmath27 where the term @xmath28 represents the signal position ( or shape ) in the data - space and @xmath29 its magnitude ( or intensity ) . for the standard picture of a non - relativistic wimp",
    ", the interaction rate depends linearly on cross section @xmath30 , and hence @xmath31 .",
    "the number of events is governed by the interaction cross section @xmath30 between the dark matter and the nucleons of the detector .",
    "if the shape of the signal matches that of the data points ( above background ) , then an inspection of the number of events should reveal the value of the cross section , and therefore the strength of the dm interactions .    on the other hand ,",
    "if the shape does not match the data - point distribution , one can set a limit on the dm interaction cross section . in practice",
    "the finite experimental sensitivity means we can only exclude values of @xmath30 which would lead to too large a signal . hence it is convenient to start with a value that is already excluded from previous experimental searches , namely @xmath32 , and decrease it until one reaches the experimental sensitivity . for this reason we will work with the ratio @xmath33 where @xmath34 , so that @xmath33 provides us with a direct measurement of the intensity of the signal .",
    "an exclusion limit is then set by determining the smallest @xmath35 value that still leads to too many signal - like events , so that all @xmath36 are excluded , while keeping values of @xmath29 which the experiment is not sensitive to .    the number of expected signal events in a pixel at @xmath20 is therefore given by @xmath37 .",
    "to proceed , we must now define a prior for @xmath29 .",
    "we have no theoretical prejudice on its value and therefore consider a flat prior i.e. assign to all possible cross section values @xmath38 $ ] the same a priori probability density function @xmath39 .",
    ", we would have to take @xmath40 .",
    "however in practice we can take @xmath41 to be very large but finite , such that we are confident that the probability of finding dm with this interaction strength is vanishingly small , given previous experimental knowledge . ]",
    "we can now combine the likelihood @xmath42 and prior @xmath43 into the joint data and signal probability @xmath44 .",
    "we will work with the information hamiltonian , @xmath45 where the @xmath46 indicates signal - independent terms , which do not contribute to the determination of the ratio @xmath29 .",
    "inserting our decomposition for @xmath19 ( cf eq.[lambdadecomposition ] ) and rearranging we obtain , @xmath47 + \\dots\\ ] ] the limit can now be taken where @xmath48 , can only contain either @xmath49 or @xmath50 data - points . hence in this limit @xmath13 tends to a delta - function and the hamiltonian becomes @xmath51 + \\dots % h",
    "\\ , \\widehat{= } \\int_{\\omega } \\mathrm{d}x \\bigg [ f(x ) s - \\mathrm{ln } \\left ( 1 + \\frac{f(x ) s}{b(x ) } \\right ) \\delta^n ( x - x^{\\mathrm{data}}_i ) \\bigg ] , \\label{eqn : ham_int}\\end{aligned}\\ ] ] where the @xmath52-function picks out the positions of the @xmath16 data - points @xmath14 .",
    "we define @xmath53 , the total number of reference signal ( nuclear - recoil from dark matter ) events in the data - space calculated at @xmath41 .",
    "with this likelihood we are ready to look for a dark matter signal in our data and we now outline this process explicitly ( see also @xcite ) .    as with standard @xmath54 methods ,",
    "we seek to minimise the hamiltonian .",
    "there is a positive identification of a dm signal in the experimental data only when the hamiltonian possesses a minimum . in this case",
    "the shape of the signal @xmath55 matches the distribution of the data points , in some region of data - space where @xmath56 is expected to be small .",
    "the strength of the dm - nucleon interaction is given by the intensity of the signal , @xmath57 , corresponding to @xmath58 , with @xmath59 representing the properties of the signal that fit the data best .",
    "to define the goodness of the fit in the standard approach , one would then consider all @xmath29 ( or equivalently @xmath30 ) values leading to @xmath60 where @xmath52 is fixed by the confidence level that one wants to have .",
    "here we shall proceed slightly differently ( but ultimately this is equivalent ) : we define the significance of the signal by integrating the posterior distribution @xmath61 over @xmath29 , retaining in particular @xmath29 values around @xmath57 .",
    "note that the last equality holds only for flat priors ( f.p . ) , and assuming that @xmath62 .",
    "however , in the following we will take out the normalisation of @xmath42 explicitly , such that : @xmath63    hence in our case a discovery will be established at a confidence level @xmath64 by using the definition , @xmath65 where the discovery region is bounded from below by @xmath66 and from above by @xmath67 .",
    "such a region is therefore a two - sided region of credibility , while an exclusion limit by contrast is said to be one - sided .",
    "one could also relate our two - sided bayesian region of credibility to a frquentist confidence interval with a certain number of ` sigmas ' , though this is only strictly possible for a gaussian likelihood and posterior and @xmath67 could be directly related to the distance from the best - fit point @xmath57 in units of the gaussian variance i.e. a number of ` sigmas ' . ] .",
    "however one may find that the hamiltonian possesses no minimum . in this case",
    "there is no value of @xmath29 for which the data is compatible with the signal distribution , no matter how intense this distribution becomes .",
    "one can not completely rule out dark matter however , since we know that our experiment has finite sensitivity , but we can set a limit , hereafter referred to as @xmath68 , on the dm interactions .",
    "since the experiment is not sensitive to dm cross section values smaller than @xmath69 , all @xmath29 values below @xmath68 are equally good ( or equally bad ) .",
    "hence there is a region of the parameter space corresponding to @xmath70 where the posterior probability @xmath71 is practically constant , as the experiment can not discriminate between these values of the cross section ( for a given exposure ) .",
    "the allowed region below @xmath68 is thus characterised by @xmath72 while the excluded region above @xmath68 ( where one expects too much signal ) is identified by a sharp cut - off in the posterior probability . to determine the exclusion limit ( i.e. @xmath68 )",
    ", we thus seek to quantify this cut - off .",
    "we have some freedom in choosing its value : it will depend on the confidence with which we set out limit .",
    "for example to set an exclusion limit at a confidence of @xmath73 ( e.g. for @xmath0 confidence we take @xmath74 ) , we define @xmath68 analogously with our best - fit region , as @xmath75 by integrating the constant region of the posterior probability until the integration reaches the value that we set , we identify @xmath68 and the cut - off .",
    "note also that for ease of calculation we tend to use the hamiltonian in the form of , @xmath76 where @xmath15 sums over all @xmath16 data - points at positions @xmath77 and @xmath78 are data weights with @xmath79 . for setting a limit the first term in eqn .",
    "( [ eq : sigma_limit ] ) @xmath80 is data - independent and gives the absolute limit in the case where no signal - like events are observed in the data , while the second term accounts for potential signal - like events present in the data , and weakens the limit .",
    "the statistical treatment is largely similar for setting limits or claiming discovery , and our method provides a natural transition between the two , though the approach to how one thinks about regions of credibility is different in either case . indeed",
    "both a signal region and an exclusion limit are equally valid regions of credibility , and so one may wish to highlight both if there is a hint of signal present in the data , but one wishes to remain conservative as to its interpretation .",
    "+   +    the strongest limits ( for @xmath81 ) on the spin independent cross section for dark matter elastic scattering with nuclei have been set by xenon - based experiments i.e. xenon100 and lux @xcite .",
    "we focus on the xenon100 experiment @xcite as a worked example , which operates using both liquid and gaseous xenon with a fiducial mass of @xmath82 ( for the most recent data - set @xcite ) .",
    "the xenon100 detector identifies events by using two distinct signals @xcite : primary ( s1 ) and secondary ( s2 ) scintillation , the former of which is due to scintillation light originating from the liquid part of the detector , while the latter comes from ionised electrons , which drift to the gaseous part of the detector under an electric field .",
    "the lux detector @xcite operates on a similar principle , but with a larger fiducial mass .",
    "the lux collaboration also employ different cuts ( e.g. a cut at s1 = 2 pe , instead of 3 pe ) and potentially a different likelihood function for their own analysis .",
    "otherwise , the following discussion should be interesting for an understanding of the analysis of lux data , as well as xenon100 .    in order to derive limits on the spin - independent cross section as a function of dark matter mass , the xenon100 collaboration employs a profile likelihood approach @xcite .",
    "such a method takes advantage of the distinct signatures in s1-s2 of electronic and nuclear recoils by splitting the data - space into a number of bands ( 23 in @xcite and 12 in @xcite ) .",
    "we can contrast this approach with our method , where the data - space is split into a grid of rectangular pixels , which are associated with a point in the data - space @xmath83 .",
    "hence , we expect our gridded approach to perform better than this method of bands used by the xenon100 collaboration , since we can exploit the difference between signal and background to the maximum amount , while they are limited by the rather coarse - grained resolution of their bands . this application should serve as a clear demonstration of the advantages to _ any _ direct detection experiment of using our method .",
    "we can identify s1 and s2 with our discrimination parameters @xmath6 and @xmath7 from section [ sec : method ] , though here we choose instead to take @xmath84 , @xmath85 , to match more closely the method used by the xenon100 collaboration themselves ( and also the lux collaboration @xcite ) .",
    "we will proceed first to discuss the determination of the signal @xmath55 and background @xmath56 distributions for the xenon100 experiment , before applying our method to data , both the more recent 225 live days data ( 225ld ) @xcite and the older 100 live days data - set ( 100ld ) @xcite .",
    "potential wimp events are characterised by their recoil spectra @xmath86 , parameterised as @xcite , @xmath87 where @xmath88 is the wimp - nucleus cross - section as a function of energy @xmath89 , @xmath90 is the wimp - nucleus reduced mass , @xmath91 is the local dark matter density and @xmath92 is the wimp mean velocity .",
    "the mean velocity is integrated over the distribution of wimp velocities in the galaxy @xmath93 boosted into the reference frame of the earth by @xmath94 .",
    "the lower limit of the integration is @xmath95 , which is the minimum wimp velocity required to induce a recoil of energy @xmath89 .",
    "we assume the standard halo model such that @xmath93 is given by a maxwell - boltzmann distribution cut off at an escape velocity of @xmath96 .",
    "we assume that wimps interact identically with protons and neutrons and @xmath97 . ] giving @xmath98 , where @xmath30 is the zero - momentum wimp - nucleon cross section , @xmath99 is the atomic mass of xenon , @xmath100 is the wimp - proton reduced mass and @xmath101 is the helm nuclear form factor @xcite . for 225ld ( 100ld )",
    "we use a value of @xmath102 days ( @xmath103 days ) for the exposure and @xmath104 kg ( @xmath105 kg ) for the target mass .      at a given nuclear - recoil energy @xmath89 the expected primary ( @xmath106 ) and secondary ( @xmath107 ) scintillation signals",
    "are obtained from the following formulae @xcite , @xmath108 where @xmath109 represents a poisson distribution with expectation value @xmath5 , @xmath110 , @xmath111 , @xmath73 is a gaussian - distributed value with mean @xmath112 pe per electron and width @xmath113 @xcite , @xmath114 is the relative scintillation efficiency and @xmath115 is the ionisation yield . for @xmath116 there is a degree of uncertainty on its functional form @xcite ; we use the model of @xcite in this work , however we have obtained similar results with the best - fit curve from @xcite .",
    "@xmath117 is obtained from a cubic spline fit to data from @xcite .    to obtain the @xmath118 and @xmath119 signals observed in the detector",
    ", we must include the finite detector resolution and the cuts imposed by the xenon100 collaboration on the data @xcite . both @xmath106 and @xmath107",
    "are blurred with a gaussian of width @xmath120 for @xmath121 photoelectrons ( pe ) to take account of the finite photomultiplier ( pmt ) resolution @xcite . the effect of cuts",
    "is then implemented using the cut - acceptance curve as a function of s1 @xcite after applying the resolution effect .",
    "additionally an s2 threshold cut is applied before gaussian blurring , cutting away all points with @xmath122 @xcite .      the expected signal distribution for a given wimp mass in the data - space @xmath55",
    "can now be calculated using @xmath123 of section [ sec : rec_spec ] , at a value of the reference cross - section @xmath124 ( or @xmath125 for @xmath126 ) .",
    "the energy range between @xmath127 and @xmath128 is separated into bins of size @xmath129 . for each binned energy @xmath130",
    "we calculate @xmath118 and @xmath119 a total of @xmath131 times , where @xmath132 , to obtain the full signal distribution as expected in xenon100 .",
    "the result is shown for two different masses in fig .",
    "[ fig : wimp_dists ]",
    ". similar simulations of the signal distribution expected from xenon100 have been performed in @xcite , however our method goes further and directly links these to the analysis through the weight function @xmath133 , as shown in figure [ fig : wimp_dists ] .",
    "the expected distribution of electronic - recoil background events @xmath134 is determined from fits to @xmath135co calibration data would improve were we to use the @xmath136th calibration data ( especially for the anomalous component ) , collected by the xenon100 collaboration for their most recent analysis @xcite , however this is not currently publicly available . ] , as is done in @xcite .",
    "although the electronic recoil events appear mostly gaussian distributed , the xenon100 collaboration noticed the presence of an anomalous ( non - gaussian ) background component @xcite . this could be due to double - scatter gamma events , where only one of the gammas contributes to the s2 signal .",
    "both such components of the er background are included , indeed the anomalous component can be seen in figure [ fig : wimp_dists ] predominantly at low - s1 .",
    "the distribution is normalised by the total number of expected background events , whose rate takes the constant value of .",
    "we also model the nuclear - recoil background due to neutrons @xmath137 .",
    "the distribution is calculated as for the signal distribution , but replacing @xmath123 with the expected energy spectrum of neutron scatters in the detector @xcite .",
    "hence the total background distribution is @xmath138 .",
    "now that we know how to calculate the expected signal and background distributions @xmath55 and @xmath56 , we are ready to apply our method to the data from the xenon100 experiment .",
    "all relevant ingredients are displayed in fig .",
    "[ fig : wimp_dists ] ; the left panels show the regions where the expected signal and background are expected to be largest , while the right panels show plots of @xmath139 as used directly for our analysis .",
    "the discrimination between signal and background is maximised provided the two - dimensional bins for @xmath133 are small enough : data - points where @xmath140 is large are more likely to be due to signal than background , while the opposite is true for points located where @xmath140 is small .",
    "this is then fed directly into our analysis , hence figure [ fig : wimp_dists ] contains all of the main ingredients of our method .    shown in figure [ fig : lims ] are the results of applying our method to the data . in order to understand the effect of data - points consistent with a signal interperetation , we have performed the analysis with both the full dataset ( with a lower cut on s1 at @xmath141 ) , and with a reduced dataset , where the two  hint \" data - points ( i.e. the starred points in figure [ fig : wimp_dists ] ) have been removed by cutting away the data - space below @xmath142 . to @xmath143 , as for the 100ld data - set , which would remove one of these points .",
    "] the former is displayed in the left panel of fig .",
    "[ fig : lims ] , while the results for the reduced dataset are shown on the central panel . results from the 100ld data are shown on the right .",
    "as discussed in section [ sec : sigs_lims ] we can define regions of credibility ( either exclusion limits or potential discovery regions ) by integrating under the normalised posterior @xmath144 .",
    "hence in the lower panels of figure [ fig : lims ] we show exclusion limits for various levels of confidence , between @xmath145 and @xmath0 , calculated by integrating the posterior from @xmath146 up to the limiting value of @xmath23 .",
    "one can equivalently consider the parameter space between these limits as a region of @xmath147 credibility .",
    "the @xmath0 limit for the full 225ld data - set can be compared with the result from @xcite , while the shaded band represents how the limit changes with different confidence .",
    "the upper panels show the dependence of the likelihood @xmath42 as a function of @xmath30 for various wimp masses .",
    "one can see directly that for the full 225ld dataset the likelihood function has a maximum ( corresponding to a minimum in the hamiltonian ) , indicating a preference for the data of a particular value of @xmath30 , which is strongest for lighter wimps .",
    "indeed this can also be observed in the exclusion curve as we change the significance value : particularly for lighter wimps the region of credibility between the @xmath145 and @xmath0 limits is denser as compared to heavier dm .",
    "this is due directly to the presence of a maximum in the posterior and likelihood .",
    "this is particularly interesting in the context of the potential hints of light dm in cdms @xcite and cogent @xcite ( and to some extent dama ) .",
    "however the significance of such a hint is weak . indeed the @xmath148 credible region for an @xmath149 wimp",
    "lies between @xmath150 and @xmath151 , with a best - fit cross section at @xmath152 .",
    "of course the cross - section is still inconsistent with the best - fit region from cdms @xcite , unless one changes the systematic parameters to a rather extreme degree @xcite or considers less standard interactions @xcite .",
    "claims that these points are consistent with a dm signal are likely to be overly optimistic .",
    "the significance of the signal is comparable to a @xmath153 fluctuation confidence interval as ( roughly ) comparable to a @xmath154 region of credibility , then we actually find the significance to be a bit less than @xmath153 .",
    "indeed our choice of @xmath148 was motivated by the fact that it is close to the largest two - sided interval we could set around the maximum - likelihood value of cross section .",
    "the sigma - level is only approximate though , as our likelihood is non - gaussian ( see fig .",
    "[ fig : lims ] ) .",
    "] , and hence these data - points may just be events from the non - gaussian er background , which we already model .",
    "we can additionally compute the bayes factor @xmath155 @xcite for e.g. an @xmath149 wimp , by calculating the ratio of the joint signal and data probability @xmath156 integrated over all @xmath29 , to @xmath157 i.e. the no - dm scenario , where @xmath158 .",
    "hence the size of @xmath155 should tell us to what degree a positive signal of dm is preferred , relative to the scenario where no signal is present ( see @xcite for details ) .",
    "we calculate @xmath159 , which is just on the boundary of being a positive result . hence , again we can conclude there is only a weak hint of signal for a low - mass wimp .",
    "there are also systematic uncertainties from @xmath117 and @xmath116 , though they are unlikely to result in a significant enhancement of the signal significance .    indeed , as can be seen from [ fig : wimp_dists ] if one attributes these points to a wimp signal , one must also explain why no data is seen where the signal from dm is expected to be even larger , at lower values of s1 for example .",
    "even so , the presence of consistency with signal , however weak , indicates some sort of new phenomenon may be present : either dm or an unknown ( or possibly misunderstood ) background .",
    "hence an interpretation of these points in terms of dark matter is possible but premature , however they are instructive as an example of the effect of signal - like points on our ability to set limits on light dm .    by contrast",
    "when the two  hint \" data - points are removed from the analysis by the more stringent low - s1 cut ( see figure [ fig : wimp_dists ] for details ) , there is no maximum in the likelihood and posterior for any wimp mass , as one would expect since all points are in a region where the weight @xmath160 is small . indeed the density of the posterior is now less for all masses than for the full data - set , with the contrast particularly stark for lighter dm . the same is seen for 100ld , for which no hint of signal is present .",
    "in addition , the limits without the  hint \" points are stronger since the data are now almost completely consistent with a negative result . if the xenon100 collaboration were to observe additional signal - like points in their data , one would expect the density of the posterior to increase around the best - fit region .    in any case",
    "this demonstrates the ability of our method to accurately set limits or define potential discovery regions .",
    "all of the relevant information is contained within the posterior @xmath144 , which can be integrated over to define the degree of belief that a given region of parameter space is consistent with the data .      before forming any firm conclusions on the efficacy of our method in searching for dark matter signals in direct detection data",
    ", we must compare our results to those previously found by the xenon100 collaboration .",
    "shown in figure [ fig : limit_comparison ] is our @xmath0 confidence limit ( identical to the one in figure [ fig : lims ] ) , compared with the limit derived by the xenon100 collaboration with the same 225 live days dataset @xcite , but their own profile likelihood analysis @xcite .",
    "uncertainties due to the relative scintillation efficiency @xmath117 are shown as a shaded region around our limit ( see e.g. @xcite for a review ) .",
    "in addition , in the lower panel of figure [ fig : limit_comparison ] we also show the results of applying our method to the 100 live days dataset , along with the limit from the xenon100 collaboration using their profile likelihood method , and a limit we have independently derived using the same method , but with identical inputs to our information theory analysis .     and @xmath56 .",
    "the limit from our bayesian information theory method agrees with the xenon100 published limit for 225ld , but is several times stronger for 100ld . ]",
    "the exclusion limit derived with our information hamiltonian method agrees with that derived by the xenon100 collaboration for the 225 live days data - set for large masses . for lighter wimps",
    "our limit is stronger , though this is likely due to uncertainty in the low - energy extrapolation of @xmath117 @xcite .",
    "indeed the xenon100 collaboration employ the most conservative approach and cut @xmath117 to zero below @xmath161 , where no data is available @xcite .",
    "our limit is derived using a constant extrapolation instead , though the uncertainty band shows the limit under different parameterisations of @xmath117@xcite .",
    "hence one can consider our result as an independent cross - check of the limit published by the xenon100 collaboration .",
    "there are undoubtably other small differences between our inputs and those used by the xenon100 collaboration , however the agreement of both limits indicates that our method does indeed perform correctly when analysing direct detection data .",
    "note also that for the  hint \"- removed data - set , where the low - s1 cut is moved to s1@xmath162 , the limit is stronger for heavy wimps due to the removal of the signal - like points by the cut .",
    "this is not so for lighter wimps , since much of the region where one expects to see signal is cut away in addition to the  hint \" points .",
    "we note however that when applying our method to the 100ld data @xcite that our information theory limit is stronger than that derived using the profile likelihood analysis , both performed directly by the xenon100 collaboration and from an independent analysis we have carried out .",
    "since the latter two limits are in agreement , it would be difficult to blame the inputs of the analysis on this discrepancy between the limits , hence it is likely that the coarse - graining ) .",
    "] of the profile likelihood analysis has resulted in the derivation of an over - conservative limit .",
    "to reiterate : we refer specifically to the profile likelihood analysis used by xenon100 here .",
    "the issue is not with the frequentist method itself , but rather with the choice of likelihood function used by the collaboration .",
    "hence , our limit is more accurate because we use a likelihood which exploits the whole data - space , and this should also be reflected in a profile likelihood analysis which followed the same principles .",
    "the reason for this discrepancy arising only for the 100ld dataset is not entirely clear , though it is likely that the increased background in this dataset relative to that from 225 live days @xcite ( due to the krypton leakage ) has effectively fooled the analysis into treating too many points as potential signal , thereby weakening the limit .",
    "hence we believe that this demonstrates the robustness of our method as compared to such a profile likelihood analysis , since it is less susceptible to leakage of background points into the signal region .",
    "in this work we have introduced a bayesian method of analysing data from dark matter direct detection experiments .",
    "our method takes as input the data itself and the expected signal and background distributions , defined over the whole data - space , which is divided into a grid of two - dimensional pixels .",
    "this enables us to take full advantage of the distinct expected distributions signal and background events , and hence to set limits ( or discovery regions ) without resorting to conservative approximations .    using data from the xenon100 experiment @xcite as a worked example we demonstrated how one would apply our method to direct detection data .",
    "this has direct relevance also to lux experiment @xcite , and any future runs of xenon100 .",
    "we have shown that there is merit in looking beyond the @xmath0 confidence limit , as hints of signal may be affecting the structure of the likelihood and posterior in a non - trivial manner .",
    "indeed an analysis of the xenon100 data from 225 live days indicates a weak preference in the data for a light dm particle . at 50@xmath163 confidence",
    "the best fit cross section is in between @xmath150 and @xmath151 for an @xmath149 wimp ; the error bars being relatively large , it is very premature to argue that this is evidence for dark matter .",
    "similar regions can be obtained for any dark matter particle with a mass below @xmath164 gev , with a possible evidence for a dark matter signal in the data vanishing for masses above about 20 gev .",
    "if indeed these points are due to a detection of dark matter , more data from the xenon100 experiment should increase the confidence level and shrink the error bars on the cross section .",
    "alternatively , these events may be found to be due to an additional background process or the anomalous component of the er background , in which case the signal significance would vanish with more data .",
    "considering the recent null result from the lux experiment @xcite , the latter would seem to be a more plausible explanation .",
    "we also demonstrated that our new method can produce a complementary analysis to the one currently used by the xenon100 collaboration , where the data are placed into bands .",
    "indeed our limit and theirs agree for the most recent 225 live days data - set @xcite , however ours is several times stronger for the data from 100 live days @xcite .",
    "the reason for this disagreement for the older data - set is not clear .",
    "however it is possible that since the background was higher due to krypton contamination , there was a greater proportion of background events leaking into the region where signal was expected ( i.e. the more signal - like bands of the analysis used by the xenon100 collaboration ) , which may have fooled their analysis into setting too weak a limit .",
    "additionally our method could be even more robust , especially if one exploits the full detector volume ( with @xmath165 and @xmath166 now depending on physical positions in the detector ) .",
    "our analysis can be seen as an independent analysis of the xenon100 data , and more importantly could be employed by any present or forthcoming experimental collaboration for such a purpose . in particular , our method can be easily applied to the lux experiment @xcite , since it operates on a similar principle to xenon100 . in this case",
    "one should hope to find agreement with our bayesian results and the frequentist method used by the lux collaboration , which should provide an important cross - check of the lux results .",
    "future experiments such as xenon1 t @xcite , lz @xcite and supercdms @xcite could also benefit from a bayesian cross - check",
    ".    the use of our formalism should be very convenient to set limits and potential regions of discovery simultaneously , allowing scenarios where the presence of a signal is ambiguous to be studied without bias .",
    "additionally , our method can be used to go beyond the conservative approach , and to set the strongest limit possible by exploiting the different distributions of signal and background events . with a consistent analytical method used by all dark matter direct detection experiments ,",
    "the current constraints on the wimp cross - section should be both stronger and clearer .",
    "jhd and cb are supported by the stfc ."
  ],
  "abstract_text": [
    "<S> the experimental situation of dark matter direct detection has reached an exciting cross - roads , with potential hints of a discovery of dark matter ( dm ) from the cdms , cogent , cresst - ii and dama experiments in tension with null - results from xenon - based experiments such as xenon100 and lux . </S>",
    "<S> given the present controversial experimental status , it is important that the analytical method used to search for dm in direct detection experiments is both robust and flexible enough to deal with data for which the distinction between signal and background points is difficult , and hence where the choice between setting a limit or defining a discovery region is debatable . in this article </S>",
    "<S> we propose a novel ( bayesian ) analytical method , which can be applied to all direct detection experiments and which extracts the maximum amount of information from the data . </S>",
    "<S> we apply our method to the xenon100 experiment data as a worked example , and show that firstly our exclusion limit at @xmath0 confidence is in agreement with their own for the 225 live days data , but is several times stronger for the 100 live days data . </S>",
    "<S> secondly we find that , due to the two points at low values of s1 and s2 in the 225 days data - set , our analysis points to either weak consistency with low - mass dark matter or the possible presence of an unknown background . given the null - result from lux , the latter scenario seems the more plausible . </S>"
  ]
}