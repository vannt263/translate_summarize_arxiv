{
  "article_text": [
    "simultaneously recovering the location of a robot and a map of its environment from sensor readings is a fundamental challenge in robotics  @xcite .",
    "well - known approaches to this problem , such as square root smoothing and mapping ( sam )  @xcite , have focused on regression - based methods that exploit the sparse structure of the problem to efficiently compute a solution .",
    "the main weakness of the original sam algorithm was that it was a _ batch _",
    "method : all of the data must be collected before a solution can be found . for a robot traversing an environment ,",
    "the inability to update an estimate of its trajectory online is a significant drawback . in response to this weakness",
    ", @xcite developed a critical extension to the batch sam algorithm , incremental smoothing and mapping ( isam ) , that overcomes this problem by _ incrementally _ computing a solution .",
    "the main drawback of isam , was that the approach required costly periodic batch steps for variable reordering to maintain sparsity and relinearization .",
    "this approach was extended in isam  2.0  @xcite , which employs an efficient data structure called the _ bayes tree _  @xcite to perform incremental variable reordering and just - in - time relinearization , thereby eliminating the bottleneck caused by batch variable reordering and relinearization .",
    "the isam  2.0 algorithm and its extensions are widely considered to be state - of - the - art in robot trajectory estimation and mapping .",
    "the majority of previous approaches to trajectory estimation and mapping , including the smoothing - based sam family of algorithms , have formulated the problem in discrete time  @xcite .",
    "however , discrete - time representations are restrictive : they are not easily extended to trajectories with irregularly spaced waypoints or asynchronously sampled measurements .",
    "a continuous - time formulation of the sam problem where measurements constrain the trajectory at any point in time , would elegantly contend with these difficulties .",
    "viewed from this perspective , the robot trajectory is a _ function _",
    "@xmath0 , that maps any time @xmath1 to a robot state",
    ". the problem of estimating this function along with landmark locations has been dubbed _ simultaneous trajectory estimation and mapping _ ( steam ) @xcite .",
    "tong et al .",
    "@xcite proposed a gaussian process ( gp ) regression approach to solving the steam problem .",
    "while their approach was able to accurately model and interpolate asynchronous data to recover a trajectory and landmark estimate , it suffered from significant computational challenges : naive gaussian process approaches to regression have notoriously high space and time complexity .",
    "additionally , tong et al.s approach is a _ batch _ method , so updating the solution necessitates saving all of the data and completely resolving the problem . in order to combat the computational burden ,",
    "tong et al.s approach was extended in barfoot et al .",
    "@xcite to take advantage of the sparse structure inherent in the steam problem .",
    "the resulting algorithm significantly speeds up solution time and can be viewed as a continuous - time analog of dellaert s original square - root sam algorithm  @xcite .",
    "unfortunately , like sam , barfoot et al.s gp - based algorithm remains a batch algorithm , which is a disadvantage for robots that need to continually update the estimate of their trajectory and environment .    in this work",
    ", we provide the critical extensions necessary to transform the existing gaussian process - based approach to solving the steam problem into an extremely efficient incremental approach .",
    "our algorithm elegantly combines the benefits of gaussian processes and isam  2.0 . like the gp regression approaches to steam , our approach can model continuous trajectories , handle asynchronous measurements , and naturally interpolate states to speed up computation and reduce storage requirements , and , like isam 2.0",
    ", our approach uses a bayes tree to efficiently calculate a _",
    "maximum a posteriori _",
    "( map ) estimate of the gp trajectory while performing incremental factorization , variable reordering , and just - in - time relinearization .",
    "the result is an online gp - based solution to the steam problem that remains computationally efficient while scaling up to large datasets .",
    "we begin by describing how the simultaneous trajectory estimation and mapping ( steam ) problem can be formulated in terms of gaussian process regression .",
    "following tong et al .",
    "@xcite and barfoot et al .",
    "@xcite , we represent robot trajectories as functions of time @xmath1 sampled from a gaussian process : @xmath2    here , @xmath0 is the continuous - time trajectory of the robot through state - space , represented by a gaussian process with mean @xmath3 and covariance @xmath4 functions .",
    "we next define a finite set of measurements : @xmath5    the measurement @xmath6 can be any linear or nonlinear functions of a set of related variables @xmath7 plus some gaussian noise @xmath8 .",
    "the related variables for a range measurement are the robot state at the corresponding measurement time @xmath9 and the associated landmark location @xmath10 .",
    "we assume the total number of measurements is @xmath11 , and the number of trajectory states at measurement times are @xmath12 .    based on the definition of gaussian processes , any finite collection of robot states has a joint gaussian distribution  @xcite .",
    "so the robot states at measurement times are normally distributed with mean @xmath13 and covariance @xmath14 .",
    "@xmath15^\\intercal\\\\ & \\bm{\\mu } =   [ \\begin{array}{ccc }   \\bm{\\mu}(t_1)^{\\intercal }   &    \\hdots & \\bm{\\mu}(t_m)^{\\intercal}\\end{array}]^\\intercal , \\hspace{12pt}\\bm{\\mathcal{k}}_{ij } = \\bm{\\mathcal{k}}(t_i , t_j ) \\end{split}\\ ] ]    note that any point along the continuous - time trajectory can be estimated from the gaussian process model .",
    "therefore , the trajectory does not need to be discretized and robot trajectory states do not need to be evenly spaced in time , which is an advantage of the gaussian process approach over discrete - time approaches ( e.g.  dellaert s square - root sam  @xcite ) .    the landmarks @xmath16 which represent the map are assumed to conform to a joint gaussian distribution with mean @xmath17 and covariance @xmath18 ( eq .  [ eqn_landmark_dist ] ) . the prior distribution of the combined state @xmath19 that consists of robot trajectory states at measurement times and landmarks is , therefore , a joint gaussian distribution ( eq .  [ eqn_prior_dist ] ) .",
    "@xmath20^\\intercal \\label{eqn_landmark_dist}\\\\ & \\bm{\\theta } \\sim \\mathcal{n}(\\bm{\\eta } , \\bm{\\mathcal{p } } ) , \\hspace{12pt}\\bm{\\eta } = [ \\begin{array}{cc }   \\bm{\\mu}^\\intercal &   \\bm{d}^\\intercal\\end{array}]^\\intercal , \\hspace{12pt } \\bm{\\mathcal{p } } =   \\begin{bmatrix }    \\bm{\\mathcal{k } } & \\\\    & \\bm{w }   \\end{bmatrix }     \\label{eqn_prior_dist}\\end{aligned}\\ ] ] to solve the steam problem , given the prior distribution of the combined state and the likelihood of measurements , we compute the _ maximum a posteriori _ ( map ) estimate of the combined state _ conditioned _ on measurements via bayes rule : @xmath21 where the norms are mahalanobis norms defined as : @xmath22 , and @xmath23 and @xmath24 are the mean and covariance of the measurements collected , respectively : @xmath25^\\intercal\\\\ \\bm{r } & = \\text{diag}(\\bm{r}_1 , \\bm{r}_2 , \\hdots , \\bm{r}_n)\\end{aligned}\\ ] ] because both covariance matrices @xmath26 and @xmath24 are positive definite , the objective in eq .",
    "[ eqn_map ] corresponds to a least squares problem .",
    "consequently , if some of the measurement functions @xmath27 are nonlinear , this becomes a nonlinear least squares problem , in which case iterative methods including gauss - newton and levenberg - marquardt  @xcite can be utilized .",
    "a linearization of a measurement function at current state estimate @xmath28 can be accomplished by a first - order taylor expansion : @xmath29 combining eq .",
    "[ eqn_measurement_linearization ] with eq .",
    "[ eqn_map ] , the optimal increment @xmath30 at the current combined state estimate @xmath31 is @xmath32 where @xmath33 is the measurement jacobian matrix : @xmath34 to solve the linear least squares problem in eq .",
    "[ eqn_map_linearized ] , we take the derivative with respect to @xmath35 , and set it to zero , which gives us @xmath30 embedded in a set of linear equations @xmath36 with covariance @xmath37 the positive definite matrix @xmath38 is the _ a posteriori _ information matrix , which we label @xmath39 . to solve this set of linear equations for @xmath30",
    ", we do not actually have to calculate the inverse @xmath40 . instead",
    ", factorization - based methods can provide a fast , numerically stable solution .",
    "for example , @xmath30 can be found by first performing a cholesky factorization @xmath41 , and then solving @xmath42 and @xmath43 by back substitution . at each iteration",
    "we perform a _ batch _",
    "state estimation update @xmath44 and repeat the process until convergence .    if @xmath39 is dense , the time complexity of a cholesky factorization and back substitution are @xmath45 and @xmath46 respectively , where @xmath47  @xcite .",
    "however , if @xmath39 has sparse structure , then the solution can be found much faster .",
    "for example , for a narrowly banded matrix , the computation time is @xmath48 instead of @xmath45  @xcite .",
    "fortunately , we can guarantee sparsity for the steam problem ( see section  [ subsec_sparse_gp_regression ] below ) .",
    "an advantage of the gaussian process representation of the robot trajectory is that any trajectory state can be interpolated from other states by computing the posterior mean  @xcite : @xmath49 with @xmath50^\\intercal \\quad \\text{and}\\nonumber\\\\ \\bm{\\mathcal{k}}(t ) & = [ \\begin{array}{ccc } \\bm{\\mathcal{k}}(t , t_1 ) & \\hdots & \\bm{\\mathcal{k}}(t , t_m ) \\end{array}].\\end{aligned}\\ ] ] by utilizing interpolation , we can reduce the number of robot trajectory states that we need to estimate in the optimization procedure  @xcite . for simplicity ,",
    "assume @xmath7 , the set of the related variables of the @xmath51th measurement according to the model ( eq .  [ eqn_measurement_model ] ) , is @xmath52 .",
    "then , after interpolation , eq .  [ eqn_measurement_linearization ] becomes : @xmath53 by employing eq .",
    "[ eqn_interpolated_measurement_linearization ] during optimization , we can make use of measurement @xmath51 without explicitly estimating the trajectory states that it relates to .",
    "we exploit this advantage to greatly speed up the solution to the steam problem in practice ( section  [ sec_experiment ] ) .",
    "the efficiency of the gaussian process gauss - newton algorithm presented in section  [ sec_trajest ] is heavily dependent on the choice of kernel .",
    "it is well - known that if the information matrix @xmath39 is sparse , then it is possible to very efficiently compute the solution to eq .",
    "[ eqn_linear_equations ]  @xcite .",
    "barfoot et al .",
    "suggest a kernel matrix with a sparse inverse that is well - suited to the simultaneous trajectory estimation and mapping problem  @xcite .",
    "in particular , barfoot et al . show that @xmath54 is exactly block - tridiagonal when the gp is assumed to be generated by linear , time - varying ( ltv ) stochastic differential equation ( sde ) which we describe here : @xmath55 where @xmath0 is trajectory , @xmath56 is known exogenous input , @xmath57 is process noise , and @xmath58 is time - varying system matrix .",
    "the process noise @xmath57 is modeled by a gaussian process , and @xmath59 is the _ dirac delta function_. ( see  @xcite for details ) .",
    "we consider a specific case of this model in the experimental results in section  [ subsec_synthetic_experiment ] .    assuming the gp is generated by eq .",
    "[ eqn_sde ] , the measurements are landmark and odometry measurements , and the variables are ordered in xl ordering xl ordering is an ordering where process variables come before landmarks variables . ] , the sparse information matrix becomes @xmath60\\end{aligned}\\ ] ] where @xmath61 is block - tridiagonal and @xmath62 is block - diagonal .",
    "s density depends on the frequency of landmark measurements , and how they are taken .",
    "see fig .",
    "[ fig_original_info_mat ] for an example .    when the gp is generated by ltv sde , barfoot et al .",
    "prove that @xmath64 in eq .",
    "[ eqn_batch_query ] has a specific sparsity pattern , only two column blocks that correspond to trajectory states at @xmath65 and @xmath66 , where @xmath67 , are nonzero . in other words , @xmath68 is an affine function of only two nearby states @xmath69 and @xmath70 : @xmath71 thus , it only takes @xmath72 time to query any @xmath68 using eq .",
    "[ eqn_sparse_query ] .",
    "moreover , because interpolation of a state is only determined by the two nearby states , measurement interpolation in eq .",
    "[ eqn_interpolated_measurement_linearization ] can be significantly simplified .",
    ".24   with xl ordering[foot_xl_ordering ] and symamd ordering[foot_symamd_ordering ] .",
    "both sparse matrices have the same number of non - zero elements , yet the second matrix can be factored much more efficiently due to the heuristic ordering of the matrix columns .",
    "( see table  [ tab_chol ] ) . for illustration ,",
    "only 200 trajectory states are shown here.,title=\"fig : \" ]    .24   with xl ordering[foot_xl_ordering ] and symamd ordering[foot_symamd_ordering ] .",
    "both sparse matrices have the same number of non - zero elements , yet the second matrix can be factored much more efficiently due to the heuristic ordering of the matrix columns .",
    "( see table  [ tab_chol ] ) . for illustration , only 200 trajectory states are shown here.,title=\"fig : \" ]",
    "previous work on batch continuous - time trajectory estimation as sparse gaussian process regression  @xcite assumes that the information matrix @xmath39 is sparse ( eq .  [ eqn_sparsei ] ) and applies standard block elimination to factor and solve eq .",
    "[ eqn_linear_equations ] . despite the sparsity of @xmath39 , for large numbers of landmarks",
    "this process can be very inefficient . inspired by square root sam  @xcite , which uses variable reordering for efficient cholesky factorization in a discrete - time context",
    ", we show that factorization - time can be dramatically improved by matrix column reordering in the sparse gaussian process context as well .",
    "it is reasonable to base our approach on sam because the information matrix and factor graph of the sparse gp  @xcite has structure similar to the sam formulations of the problem  @xcite , and the intuitions from previous discrete - time approaches apply here .",
    "if the cholesky decompositions are performed naively , fill - in can occur , where entries that are zero in the information matrix become non - zero in the cholesky factor .",
    "this occurs because the cholesky factor of a sparse matrix is guaranteed to be sparse for some variable orderings , but not all variable orderings  @xcite .",
    "therefore , we want to find a good variable ordering so that the cholesky factor is sparse .",
    "although finding the optimal ordering for a symmetric positive definite matrix is np - complete  @xcite , good heuristics do exist .",
    "one such heuristic is symmetric approximate minimum degree permutation ( symamd)symamd is a variant of column approximate minimum degree ordering ( colamd )  @xcite on positive definite matrix . ]",
    "@xcite . to demonstrate the benefits of variable reordering",
    ", we constructed a synthetic example and compared different approaches .",
    "the example , which is explained in detail in section  [ subsec_synthetic_experiment ] , consists of 1,500 time steps with trajectory states , @xmath73^\\intercal$ ] , @xmath74^\\intercal$ ] , and with odometry and range measurements .",
    "the total number of landmarks is 298 .",
    "the structure of the information matrix @xmath39 and cholesky factor @xmath75 , with and without variable reordering , are compared in fig .",
    "[ fig_info_mat ] and fig .",
    "[ fig_chol ] . although variable reordering does not change the sparsity of the information matrix @xmath39 ( fig .",
    "[ fig_info_mat ] ) , it dramatically increases the sparsity of the cholesky factor @xmath75 ( fig .",
    "[ fig_chol ] ) . table  [ tab_chol ] demonstrates this clear benefit of reordering .",
    "the cholesky factor after symamd ordering contains 10.6% non - zeroes of xl ordering  [ foot_xl_ordering ] , and takes 2.83% of the time , which are significant improvements in both time and space complexity .",
    "we also experimented with block symamd  @xcite , which exploits domain knowledge to group together variables belonging to a particular trajectory state @xmath9 or landmark location @xmath10 before performing symamd and empirically further improves performance .",
    ".24   of @xmath39 . in ( a )",
    ", @xmath75 is computed with xl ordering[foot_xl_ordering ] , which exhibits fill - in . when computed with symamd ordering in ( b ) , @xmath75 is more sparse . for illustration ,",
    "only 200 states are shown here.,title=\"fig : \" ]    .24   of @xmath39 . in ( a )",
    ", @xmath75 is computed with xl ordering[foot_xl_ordering ] , which exhibits fill - in . when computed with symamd ordering in ( b ) , @xmath75 is more sparse . for illustration ,",
    "only 200 states are shown here.,title=\"fig : \" ]    .cost of cholesky factorization with different ordering methods including ordering time [ cols= \" < , < , < , < \" , ]          this dataset consists of an exploration task with 1,500 time steps .",
    "each time step contains a trajectory state @xmath73^\\intercal$ ] , @xmath74^\\intercal$ ] , an odometry measurement , and a range measurement related to a nearby landmark .",
    "the total number of landmarks is 298 .",
    "the trajectory is randomly sampled from a gaussian process generated from white noise acceleration @xmath76 , i.e. constant velocity , and with zero mean . @xmath77 where @xmath78 note that velocity @xmath79 must to be included in trajectory state to represent the motion in ltv sde form  @xcite .",
    "the odometry and range measurements with gaussian noise are specified in eq .  [ eqn_odometry_measurements ] and eq .  [ eqn_range_measurements ] respectively .",
    "@xmath80 @xmath81 where @xmath82 consists of the robot - oriented velocity and heading angle velocity with gaussian noise , and @xmath83 is the distance between the robot and a specific landmark @xmath84 at @xmath65 with gaussian noise .",
    "we compare the computation time of the three approaches ( pb , pbvr and btgp ) in fig .",
    "[ fig_syn_performance ] .",
    "the incremental gaussian process regression ( btgp ) offers significant improvements in computation time compared to the batch approaches ( pbvr and pb ) .",
    "we also demonstrate that btgp can further increase speed over a naive application of the bayes tree ( e.g. isam  2.0 ) without sacrificing much accuracy by leveraging interpolation . to illustrate the trade - off between the accuracy and time efficiency due to interpolation , we plot rmse of distance errors and the total computation time by varying the time step difference ( the rate of interpolation ) between estimated states .",
    "the second experiment evaluates our approach on real data from a freely available range - only slam dataset collected from an autonomous lawn - mowing robot  @xcite .",
    "the `` plaza '' dataset consists of odometer data and range data to stationary landmarks collected via time - of - flight radio nodes .",
    "( additional details on the experimental setup can be found in  @xcite . )",
    "ground truth paths are computed from gps readings and have 2 cm accuracy according to  @xcite .",
    "the environment , including the locations of the landmarks and the ground truth paths , are shown in fig .",
    "[ fig_plaza_estimate ] .",
    "the robot travelled 1.9 km , occupied 9,658 poses , and received 3,529 range measurements , while following a typical path generated during mowing .",
    "the dataset has sparse range measurements , but contains odometry measurements at each time step .",
    "the results of incremental btgp are shown in fig .",
    "[ fig_plaza_estimate ] and demonstrate that we are able to estimate the robot s trajectory and map with a very high degree of accuracy .    as in section  [ subsec_synthetic_experiment ] , performance of three approaches  periodic batch relinearization ( pb ) , periodic batch relinearization with variable reordering ( pbvr ) and incremental bayes tree ( btgp )",
    "are compared in fig .",
    "[ fig_plaza_performance ] .    in this dataset ,",
    "the number of landmarks is 4 , which is extremely small relative to the number of trajectory states , so there is no performance gain from reordering .",
    "however , the bayes tree - based approach dramatically outperforms the other two approaches .",
    "as the problem size increases , there is negligible increase in computation time , even for close to 10,000 trajectory states .",
    "the third experiment evaluates our approach on the victoria park dataset  @xcite , which consists of range - bearing measurements to landmarks , and speed and steering odometry measurements .",
    "the data was collected from a vehicle equipped with a laser sensor driving through the sydney s victoria park .",
    "the environment contains a high number of trees as landmarks .",
    "the vehicle travelled @xmath85 km in 26 minutes . after repeated measurements , taken when the vehicle is stationary ,",
    "are dropped , the dataset consists of 6,969 time steps and 3,640 range - bearing measurements relative to 151 landmarks . the bearing measurement is specified in eq .",
    "[ eqn_bearing_measurements ] , as the relative angle from vehicle heading to the landmark direction with gaussian noise where @xmath86^\\intercal$ ] is location of landmark @xmath87 . @xmath88",
    "the results , shown in figure [ fig_victoria_park ] , further demonstrate the advantages of btgp . as seen from the upper right plot ,",
    "variable reordering drastically reduces computation time when used within batch optimization ( pbvr ) , and even further in the incremental algorithm ( btgp ) .",
    "we have introduced an incremental sparse gaussian process regression algorithm for computing the solution to the continuous - time simultaneous trajectory estimation and mapping ( steam ) problem .",
    "the proposed algorithm elegantly combines the benefits of gaussian process - based approaches to steam while simultaneously employing state - of - the - art innovations from incremental discrete - time algorithms for smoothing and mapping .",
    "our empirical results show that by parameterizing trajectories with a small number of states and utilizing gaussian process interpolation , our algorithm can realize large gains in speed over isam  2.0 with very little loss in accuracy ( e.g. reducing computation time by @xmath89 while increasing rmse by only 8 cm on the autonomous lawnmower dataset ) ."
  ],
  "abstract_text": [
    "<S> recent work on simultaneous trajectory estimation and mapping ( steam ) for mobile robots has found success by representing the trajectory as a gaussian process . </S>",
    "<S> gaussian processes can represent a continuous - time trajectory , elegantly handle asynchronous and sparse measurements , and allow the robot to query the trajectory to recover its estimated position at any time of interest . </S>",
    "<S> a major drawback of this approach is that steam is formulated as a _ batch _ estimation problem . in this paper </S>",
    "<S> we provide the critical extensions necessary to transform the existing batch algorithm into an extremely efficient incremental algorithm . </S>",
    "<S> in particular , we are able to vastly speed up the solution time through efficient variable reordering and incremental sparse updates , which we believe will greatly increase the practicality of gaussian process methods for robot mapping and localization . finally , we demonstrate the approach and its advantages on both synthetic and real datasets . </S>"
  ]
}