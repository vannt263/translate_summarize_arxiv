{
  "article_text": [
    "many situations in physics and beyond require the solution of np - hard optimization problems , for which the typical time needed to ascertain the exact solution apparently grows faster than any power of the system size  @xcite .",
    "examples in the sciences are the determination of ground states for disordered magnets  @xcite or of optimal arrangements of atoms in a compound  @xcite or a polymer  @xcite . with the advent of ever faster computers ,",
    "the exact study of such problems has become feasible  @xcite . yet , with typically exponential complexity of these problems , many questions regarding those systems still are only accessible via approximate , heuristic methods  @xcite .",
    "heuristics trade off the certainty of an exact result against finding optimal or near - optimal solutions with high probability in polynomial time .",
    "many of these heuristics have been inspired by physical optimization processes , for instance , simulated annealing  @xcite or genetic algorithms  @xcite .",
    "extremal optimization ( eo ) was proposed recently  @xcite , and has been used to treat a variety of combinatorial  @xcite and physical optimization problems  @xcite .",
    "comparative studies with simulated annealing  @xcite and other metropolis  @xcite based heuristics  @xcite have established eo as a successful alternative for the study of np - hard problems .",
    "eo has found a large number of applications , for instance , in pattern recognition  @xcite , signal filtering  @xcite , transport problems  @xcite molecular dynamics simulations  @xcite , artificial intelligence  @xcite , social modeling  @xcite , and @xmath0 spin glass models  @xcite .",
    "there are also a number of studies that have explored basic features of the algorithm  @xcite , extensions  @xcite , and rigorous performance properties  @xcite .    in this article",
    ", we will use a simple , annealed model of a generic combinatorial optimization problem , introduced in ref .",
    "@xcite , to compare _ analytically _ certain variations of local search with eo , and of metropolis algorithms such as simulated annealing ( sa )  @xcite .",
    "this comparison affirms the notion of `` optimization at the ergodic edge '' that motivated the @xmath1-eo implementation  @xcite .",
    "this implementation possesses a _ single _",
    "tunable parameter , @xmath1 , which separates a phase of `` greedy '' search from a phase of wide fluctuations , combining both features at the phase transition into an ideal search heuristic for rugged energy landscapes  @xcite .",
    "the model helps to identify the distinct characteristics of different search heuristics commonly observed in real optimization problems .",
    "in particular , revisiting the model with a `` jammed '' state from ref .",
    "@xcite proves the existence of the phase transition to be essential for the superiority of eo , at least within a one - parameter class of local search heuristics . at the phase boundary",
    ", eo descends sufficiently fast to the ground state with enough fluctuations to escape jams .",
    "this article is organized as follows : in the next section , we introduce the annealed optimization model , followed in sec .  [ search ] by a short review of the local search heuristics studied here , in particular , of one - parameter variations of eo and of metropolis - based search .",
    "then , in sec .  [ pedagogical ] we compare our analytical results for each heuristic in the annealed model . in sec .  [ theory ]",
    "we show why versions of eo lacking a phase transition fail to optimize well .",
    "we summarize our results and draw some conclusions in sec .",
    "[ conclusion ] .",
    "as described in ref .",
    "@xcite , we can abstract certain combinatorial optimization problems into a simple , analytically tractable model . to motivate this model",
    "we imagine a generic optimization problem as consisting of a number of variables @xmath2 , each of which contributes an amount @xmath3 to the overall cost per variable ( or energy density ) of the system , @xmath4 ( the factor @xmath5 arises because local cost are typically equally shared between neighboring variables . ) we call @xmath6 the `` fitness '' of the variable , where larger values are better and @xmath7 is optimal for each variable .",
    "correspondingly , @xmath8 is the ( optimal ) ground state of the system . in a realistic problem ,",
    "variables are correlated such that not all of them could be simultaneously of optimal fitness .",
    "but in our annealed model , those correlations are neglected .",
    "a concrete example for the above definitions is provided by a spin glass with the hamiltonian @xmath9 with some quenched random mix of bonds @xmath10 and spin variables @xmath11  @xcite . with @xmath12 , counting ( minus ) the number of violated bonds of each spin @xmath13 ( among its @xmath14 non - zero bonds ) ,",
    "it is @xmath15 , where @xmath16 is an insignificant constant .",
    "we will consider that each variable @xmath13 is in one of @xmath17 ( @xmath18 constant ) different fitness states @xmath19 .",
    "we can specify occupation numbers @xmath20 , @xmath21 , for each state @xmath22 , and define occupation densities @xmath23 .",
    "hence , any local search procedure  @xcite with single - variable updates , say , can be cast simply as a set of evolution equations for the @xmath24 , i.  e. @xmath25 the @xmath26 are the probabilities that a variable in state @xmath22 gets updated ; any local search process ( based on updating a finite number of variables ) _ defines _ a unique set of @xmath26 , as we will see below .",
    "the matrix @xmath27 specifies the net transition to state @xmath28 _ given _ that a variable in state @xmath22 is updated .",
    "this matrix allows us to _ design _ arbitrary , albeit annealed , optimization problems .",
    "both , @xmath29 and @xmath30 generally depend on the @xmath24 as well as on @xmath31 explicitly .",
    "we want to consider the different fitness states equally spaced , as in the spin glass example above , where variables in state @xmath22 contribute @xmath32 to the energy to the system . here",
    "@xmath33 is an arbitrary energy scale . thus minimizing the `` energy '' density @xmath34 defines the optimization problem in this model .",
    "conservation of probability and of variables implies the constraints @xmath35    while this annealed model eliminates most of the relevant features of a truly hard optimization problem , such as quenched randomness and frustration  @xcite , two basic features of the evolution equations in eq .",
    "( [ rhodoteq ] ) remain appealing : ( 1 ) the behavior of a system with a large number of variables can be abstracted into a relatively simple set of equations , describing their dynamics with a small set of unknowns , and ( 2 ) the separation of update process , @xmath29 , and update preference , @xmath30 , lends itself to an analytical comparison between different heuristics .",
    "the annealed optimization model is quite generic for a class of combinatorial optimization problems .",
    "but it was designed in particular to analyze the `` extremal optimization '' ( eo ) heuristic  @xcite , which we will review next .",
    "then we will present the update probabilities @xmath30 through which each local search heuristic enters into the annealed model in sec .",
    "[ evolution ] .",
    "finally , we also specify the update probabilities @xmath30 for metropolis - based local searches , such as sa .      here",
    "we only give a quick review of the eo heuristic as we will use it below .",
    "more substantive discussions of eo can be found elsewhere  @xcite .",
    "eo is simply implemented as follows : for a given configuration @xmath36 , assign to each variable @xmath37 an `` fitness '' @xmath38 ( e.  g. @xmath39 in the spin glass ) , so that eq .",
    "( [ defcosteq ] ) is satisfied .",
    "each variable falls into one of only @xmath17 possible states .",
    "say , currently there are @xmath40 variables with the worst fitness , @xmath41 , @xmath42 with @xmath43 , and so on up to @xmath44 variables with the best fitness @xmath45 .",
    "( note that @xmath46 . )",
    "select an integer @xmath47 ( @xmath48 ) from some distribution , preferably with a bias towards lower values of @xmath47 .",
    "determine @xmath21 such that @xmath49 .",
    "note that lower values of @xmath47 would select a `` pool '' @xmath20 with larger value of @xmath22 , containing variables of lower fitness .",
    "finally , select one of the @xmath20 variables in state @xmath22 and update it _ unconditionally .",
    "_ as a result , it and its neighboring variables change their fitness .",
    "after all the effected @xmath50 s and @xmath51 s are reevaluated , the next variables is chosen for an update , and the process is repeated .",
    "the process would continue to evolve , unless an extraneous stopping condition is imposed , such as a fixed number of updates .",
    "the output of local search with eo is the best configuration , with the lowest @xmath52 in eq .",
    "( [ lambdaeq ] ) , found up to the current update step .    clearly , a random selection of variables for such an update , without further input of information , would not advance the local search towards lower - cost states .",
    "thus , in the `` basic '' version of eo  @xcite , each update one variable among those of worst fitness would be made to change state ( typically chosen at random , if there is more than one such variable ) .",
    "this provides a _ parameter - free _ local search of some capability . but",
    "variants of this basic elimination - of - the - worst are easily conceived . in particular , ref .",
    "@xcite already proposed @xmath1-eo , a one - parameter ( @xmath1 ) selection with a bias for selecting variables of poor fitness on a slowly varying ( power - law ) scale over the _ ranking _",
    "@xmath48 of the variables by their @xmath19 . in detail",
    ", @xmath1-eo is characterized by a power - law distribution over the fitness - ranks @xmath47 , @xmath53 it is a major point of this paper to demonstrate the usefulness of this choice .",
    "hence , we will compare the effect of this choice with a plausible alternatives , @xmath54-eo , which uses an exponential scale , @xmath55 in fact , we show that the exponential cut - off @xmath54 in @xmath54-eo , which is fixed during a run , provides inferior results to @xmath1-eo . unlike @xmath1-eo , @xmath54-eo does not have a critical point affecting the behavior of the local search .",
    "although ref .",
    "@xcite has shown rigorously , that an optimal choice is given by using a sharp threshold when selecting ranks , the actual value of this threshold at any point in time is typically not obvious ( see also ref .",
    "we will simulate a sharp threshold @xmath56 ( @xmath57 ) via @xmath58 for @xmath59 . since we can only consider fixed thresholds @xmath56 , which gives results similar in character to @xmath54-eo , it is not apparent how to shape the rigorous results into a successful algorithm .      as described in sec .",
    "[ eoalgorithm ] ( and in ref .",
    "@xcite ) , each update of @xmath1-eo a variable is selected based on its rank according to the probability distribution in eq .",
    "( [ taueq ] ) . when a rank @xmath60 has been chosen , a variable is randomly picked from state @xmath61 , if @xmath62 , from state @xmath63 , if @xmath64 , and so on .",
    "we introduce a new , continuous variable @xmath65 , for large @xmath51 approximate sums by integrals , and rewrite @xmath66 in eq .",
    "( [ taueq ] ) as @xmath67 where the maintenance of the low-@xmath68 cut - off at @xmath69 will turn out to be crucial .",
    "now , the average likelihood in eo that a variable in a given state is updated is given by @xmath70,\\nonumber\\\\ \\medskip & \\ldots&\\nonumber\\\\ \\medskip q_0&=&\\int_{1-\\rho_0}^{1 } p(x)dx=\\frac{1}{1-n^{\\tau-1 } } \\left[1-\\left(1-\\rho_0\\right)^{1-\\tau}\\right ] , \\label{qeq}\\end{aligned}\\ ] ] where in the last line the norm @xmath71 was used .",
    "these values of the @xmath72 s completely describe the update preferences for @xmath1-eo at arbitrary @xmath1 .",
    "alternatively , if we consider the @xmath54-eo algorithm introduced in eq .",
    "( [ mueq ] ) , we have to replace the power - law distribution in eq .",
    "( [ newtaueq ] ) with an exponential distribution : @xmath73 hence , for @xmath54-eo we have @xmath74    similarly , we can proceed with the threshold distribution in eq .",
    "( [ seq ] ) to obtain @xmath75 with some proper normalization .",
    "while all the integrals to obtain @xmath30 are elementary , we do not display the rather lengthy results here .",
    "note that all the update probabilities in each variant of eo are _ independent _ of @xmath29 ( i.  e. any particular model ) , which remain to be specified .",
    "this is quite special , as the following case of metropolis algorithms shows .",
    "it is more difficult to construct @xmath30 for metropolis - based algorithms  @xcite like simulated annealing  @xcite .",
    "let s assume that we consider a variable in state @xmath22 for an update .",
    "certainly , @xmath26 would be proportional to @xmath76 , since variables are randomly selected for an update .",
    "the boltzmann factor @xmath77 for the potential update from time @xmath78 of a variable in @xmath22 , aside from the inverse temperature @xmath79 , only depends on the entries for @xmath80 : @xmath81_a,\\nonumber\\\\ \\medskip & \\sim&\\frac{n}{2}\\left[\\sum_bb{\\dot\\rho}_b(t)\\right]_a,\\nonumber\\\\ \\medskip & = & \\frac{n}{2}\\left[\\sum_bb\\sum_ct_{b , c}q_c \\right]_a,\\nonumber\\\\ \\medskip & = & \\frac{n}{2}\\sum_bbt_{b , a},\\end{aligned}\\ ] ] where the subscript @xmath22 expresses the fact that it is a _ given _ that a variable in state @xmath22 is considered for an update .",
    "hence , we find for the average probability of an update of a variable in state @xmath22 @xmath82\\right\\ } , \\label{saeq}\\end{aligned}\\ ] ] where the norm @xmath83 is determined via @xmath84 . unlike for eo ,",
    "the update probabilities for sa are model - specific , i.  e. depend on @xmath29 .",
    "to demonstrate the use of these equations , we consider a simple model of an energetic barrier with only three states @xmath85 and a constant flow matrix @xmath86/n$ ] , depicted in fig .  [ flowplot ] . here , variables in @xmath87 can only reach their lowest - energy state in @xmath88 by first jumping _ up _ in energy to @xmath89 .",
    "( [ rhodoteq ] ) gives @xmath90 with @xmath30 discussed in sec .",
    "[ eoupdates ] for the variants of eo .    given @xmath29",
    ", we can now also determine the update probabilities for metropolis according to eqs .",
    "( [ saeq ] ) . note that for @xmath91 we can evaluate the @xmath92 as @xmath93 , since @xmath94 always , while for @xmath95 the @xmath92 always evaluates to the exponential .",
    "properly normalized , we obtain @xmath96      therefore , according to eq .",
    "( [ costeq ] ) , metropolis reaches its best , albeit sub - optimal , cost @xmath105 at @xmath106 , due to the energetic barrier faced by the variables in @xmath87 , see fig .",
    "[ flowplot ] .",
    "( since fluctuations from the mean are suppressed in this model , even a slowly decreasing temperature schedule as in simulated annealing would not improve results . ) in turn , @xmath54-eo does reach optimality ( @xmath107 , hence @xmath8 ) , but only for @xmath108 .",
    "note that in this limit , @xmath54-eo reduces back to the `` basic '' version of eo discussed in sec .",
    "[ eoalgorithm ] . the result for threshold updating in eo",
    "are more promising : near - optimal results are obtained , to within @xmath109 , for any finite threshold @xmath56 . but",
    "again , results are best for small @xmath110 , in which limit we revert back to `` basic '' eo .",
    "the result for @xmath1-eo is most remarkable : for @xmath111 at @xmath112 eo remains sub - optimal , but reaches the optimal cost _ for all _ @xmath113 ! as discussed in ref .",
    "@xcite , this transition at @xmath114 separates an ( ergodic ) random walk phase with too much fluctuation , and a greedy descent phase with too little fluctuation , which would trap @xmath1-eo in problems with broken ergodicity @xcite .",
    "this transition derives _ generically _ from the scale - free power - law in eq .",
    "( [ taueq ] ) , as was already argued on the basis of numerical results for real np - hard problems in refs .",
    "in this section , we revisit the `` jammed '' model treated in ref .",
    "@xcite for @xmath1-eo and repeat that calculation for @xmath54-eo . as in the example in sec .",
    "[ pedagogical ] , @xmath54-eo proves inferior to @xmath1-eo : lacking the phase of optimal performance in the @xmath1-parameter space , the required fine - tuning of @xmath54 does not succeed in satisfying the conflicting constraints imposed on the search .",
    "naturally , the range of phenomena found in a local search of np - hard problems is not limited to energetic barriers .",
    "after all , so far we have only considered constant entries for @xmath27 .",
    "therefore , in our next model we want to consider the case of @xmath29 depending linearly on the @xmath115 discussed in ref .",
    "@xcite for @xmath1-eo .",
    "this model highlights significant differences between the @xmath1-eo and the @xmath54-eo implementation .    from fig .",
    "[ jamflowplot ] , we can read off @xmath29 and obtain for eq .",
    "( [ rhodoteq ] ) : @xmath116,\\nonumber\\\\ \\medskip   { \\dot\\rho}_1&=&\\frac{1}{n}\\left[\\frac{1}{2}q_0-q_1 + ( \\theta-\\rho_1)q_2\\right ] , \\label{thresheq}\\end{aligned}\\ ] ] . and",
    "@xmath117 from eq.([normeq ] ) .",
    "aside from the dependence of @xmath29 on @xmath87 , we have also introduced the threshold parameter @xmath118 .",
    "in fact , if @xmath119 , the model behaves effectively like the previous model , and for @xmath120 there can be no flow from state @xmath121 to the lower states at all .",
    "the interesting regime is the case @xmath122 , where further flow from state @xmath121 into state @xmath93 can be blocked for increasing @xmath87 , providing a negative feed - back to the system . in effect , the model is capable of exhibiting a `` jam '' as observed in many models of glassy dynamics  @xcite , and which is certainly an aspect of local search processes . indeed , the emergence of such a `` jam '' is characteristic of the low - temperature properties of spin glasses and real optimization problems : after many update steps most variables freeze into a near - perfect local arrangement and resist further change , while a finite fraction remains frustrated ( temporarily in this model , permanently in real problems ) in a poor local arrangement  @xcite .",
    "more and more of the frozen variables have to be dislodged collectively to accommodate the frustrated variables before the system as a whole can improve its state . in this highly correlated state ,",
    "frozen variables block the progression of frustrated variables , and a jam emerges .",
    "inserting the set of eqs .",
    "( [ muqeq ] ) for @xmath123 into the model in eqs .",
    "( [ thresheq ] ) , we obtain @xmath124,\\nonumber\\\\ \\medskip { \\dot\\rho}_1&=&\\frac{1}{n}\\frac{1}{e^{\\mu\\left(1 - 1/n\\right)}-1 } \\left[-\\frac{1}{2}+\\frac{3}{2}e^{\\mu\\rho_0}-e^{\\mu\\left(\\rho_0+\\rho_1\\right)}+ ( \\theta-\\rho_1)\\left(e^{\\mu\\left(1 - 1/n\\right ) } -e^{\\mu\\left(\\rho_0+\\rho_1\\right)}\\right)\\right ] , \\label{floweq}\\end{aligned}\\ ] ] at large times @xmath31 , the steady state solution , @xmath97 , yields for @xmath88 after eliminating @xmath87 the implicit equation @xmath125\\left(e^{\\mu\\left(1 - 1/n\\right)}- 3e^{\\mu\\rho_0}\\right ) , \\label{eigeneq}\\end{aligned}\\ ] ] and according to eq .",
    "( [ costeq ] ) , again eliminating @xmath87 and @xmath89 in favor of @xmath88 , we can express the cost per variable as @xmath126 \\quad(\\mu\\gg1 ) , \\label{sscosteq}\\end{aligned}\\ ] ] unlike the corresponding equations in ref .",
    "@xcite , which had a phase transition similar to the solution for @xmath1-eo in sec .",
    "[ pedagogical ] , eqs .",
    "( [ eigeneq]-[sscosteq ] ) have no distinct features .",
    "in fact , as shown in fig .",
    "[ jammuplot ] , @xmath127 behaves similar to the solution for @xmath54-eo in sec .",
    "[ pedagogical ] : the relation is independent of @xmath51 to leading order and only for @xmath108 , @xmath128 and @xmath129 .    while the steady state ( @xmath130 ) features of this model do not seem to be much different from the model in sec .",
    "[ pedagogical ] , the dynamics at intermediate times @xmath31 is more subtle . in particular , as was shown in ref .",
    "@xcite , a `` jam '' in the flow of variables towards better fitness may ensue under certain circumstances .",
    "the emergence of the jam depends on initial conditions , and its duration will prove to get longer for larger values of @xmath54 .",
    "if the initial conditions place a fraction @xmath131 already into the lowest state , most likely no jam will emerge , since @xmath132 for all times , and the ground state is reached in @xmath133 steps . but if initially @xmath134 , and @xmath54 is sufficiently large , @xmath54-eo will drive the system to a situation where @xmath135 by preferentially transferring variables from @xmath89 to @xmath87 .",
    "then , further evolution becomes extremely slow , delayed by the @xmath54-dependent , small probability that a variable in state @xmath93 is updated ahead of all variables in state @xmath121 .",
    "clearly , this jam is _ not _ a steady state solution of eq .",
    "( [ floweq ] ) .",
    "it is not even a meta - stable solution since there are no energetic barriers .",
    "for instance , simulated annealing at zero temperature would easily find the solution in @xmath133 without experiencing a jam .",
    "in reality , a hard problem would most certainly contain combinations of jams , barriers , and possibly other features .",
    "to analyze the jam , we consider initial conditions leading to a jam , @xmath136 and make the ansatz @xmath137 with @xmath138 for @xmath139 , where @xmath140 is the time at which @xmath128 . to determine @xmath140 , we apply eq .",
    "( [ rho1eq ] ) to the evolution equations in ( [ floweq ] ) to get @xmath141 , \\label{rho0eq}\\end{aligned}\\ ] ] where the relation for @xmath142 merely yields a self - consistent equation to determine sub - leading corrections",
    ".    5.0 in    we can now integrate eq .",
    "( [ rho0eq ] ) from @xmath143 ( assuming that any jam emerges almost instantly ) up to @xmath140 , where @xmath107 : @xmath144 the integral is easily evaluated , and we find for large values of @xmath54 : @xmath145 instead of repeating the lengthy calculation in ref .",
    "@xcite for the ground state energy averaged over all possible initial conditions for finite runtime @xmath146 , we can content ourselves here with a few obvious remarks : a finite fraction of the initial conditions will lead to a jam , hence will require a runtime @xmath147 to reach optimality . yet , to reach a quality minimum , say , @xmath148 , would require @xmath149 according to eq .",
    "( [ sscosteq ] ) .",
    "thus , the require runtime to resolve the jam would grow _ exponentially _ with system size @xmath51 , since from eq .",
    "( [ tjameq ] ) @xmath150 with @xmath151 , by definition of the jam above .    in conclusion",
    ", @xmath54-eo can never quite resolve the conflicting demands of pursuing quality ground states with a strong bias for selecting variables of low fitness ( i.  e. @xmath152 ) and the ensuing lack of fluctuations required to break out of a jam , which drives up @xmath140 .",
    "simulations of this model with @xmath54-eo in fig .",
    "[ jammuplot ] indeed show that the best results for @xmath153 are obtained at intermediate values of @xmath54 , which converge to a large , constant error for increasing @xmath51 .",
    "in contrast , @xmath1-eo provides a range near @xmath154  @xcite with small enough @xmath1 to fluctuate out of any jam in a time near - linear in @xmath51 while still attaining optimal results as it does for _ any _ @xmath113 , see e.  g. sec .",
    "[ pedagogical ] .",
    "we have presented a simple model to analyze the properties of local search heuristics . the model with a simple energetic barrier",
    "demonstrates the characteristics of a number of these heuristics , whether athermal ( eo and its variants ) or thermal ( metropolis )  @xcite .",
    "in particular , it plausibly describes a number of real phenomena previously observed for @xmath1-eo in a tractable way .",
    "finally , in a more substantive comparison on a model with jamming , the exponential distribution over fitnesses , @xmath54-eo proves unable to overcome the conflicting constraints of resolving the jam while finding good solutions .",
    "this is in stark contrast with the identical calculation in ref .",
    "@xcite using a scale - free approach with a power - law distribution over fitnesses in @xmath1-eo . in this approach",
    ", a sharp phase transition emerges generically between an expansive but unrefined exploration on one side ( `` ergodic '' phase ) , and a greedy but easily trapped search on the other ( `` non - ergodic '' phase ) , with optimal performance near the transition .                                                                                                        ref .",
    "@xcite has in error in eq .",
    "( 28 ) : the general expression for the energy @xmath52 in the integrand , @xmath155 , should be replaced by @xmath156 in the jam , which leads to this value for @xmath153 for large @xmath1 or @xmath54 , instead of @xmath157 quoted there ."
  ],
  "abstract_text": [
    "<S> using a simple , annealed model , some of the key features of the recently introduced extremal optimization heuristic are demonstrated . in particular , it is shown that the dynamics of local search possesses a generic critical point under the variation of its sole parameter , separating phases of too greedy ( non - ergodic , jammed ) and too random ( ergodic ) exploration . </S>",
    "<S> comparison of various local search methods within this model suggests that the existence of the critical point is essential for the optimal performance of the heuristic . </S>",
    "<S> pacs number(s ) : 02.60.pn , 05.40.-a , 64.60.cn , 75.10.nr . </S>"
  ]
}