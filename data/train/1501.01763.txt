{
  "article_text": [
    "in recent years , there is a great deal of attention paid to the development of high dimensional classification methods .",
    "many independence rules are proposed to deal with the situations where the correlations between variables are weak .",
    "tibshirani et al . (",
    "2002 ) proposed the nearest shrunken centroid ( nsc ) classifier .",
    "fan and fan ( 2008 ) proposed the features annealed independence rule ( fair ) .",
    "moreover , bickel and levina ( 2004 ) showed that the independence rule , naive bayes ( nb ) performs better than the naive fisher discriminant ( nfr ) where the variables are correlated . when the correlations are significant , nfr is about the same as random guess .",
    "they also showed that a classification procedure using a subset of well selected features is better than that using all the features , which typically accumulates much noise in estimating population centroids in high dimensional space .",
    "in addition , methods integrating the covariance structure have been proposed in the literature , such as support vector machines ( vapnik 1995 ) , shrunken centroids regularized discriminant analysis ( scrda ) ( guo et al . 2005 ) , sparse linear discriminant analysis ( shao et al .",
    "2011 ) and @xmath1procedure ( lange et al . 2014 ) .",
    "a recent work fan et al .",
    "( 2012 ) proposed a new method that involves correlation information , called regularized optimal affine discriminant ( road ) .",
    "interestingly enough , the classification error of the road decreases as the correlation coefficient increases .",
    "two variants are screening - based rules , named s - road1 and s - road2 , which select only 10 features and 20 features , respectively . in the simulation study , under the @xmath0large @xmath2-small @xmath3 `` and equal correlation setting , the road method outperforms the available classifiers mentioned above .",
    "s - road2 also performs well , while s - road1 fails for highly correlated variables .",
    "notice that the road and its variants have to select variables in the procedure of classification .",
    "although variable selection has been extensively developed in last decades , their practical implementation still faces several difficult issues such as the choice of turning parameter or thresholding values . in this paper , we investigate whether there are straightforward methods that can have competitive performances without preliminary variable selection .",
    "in addition , existing methods mainly focus on @xmath0large @xmath2-small @xmath3 '' case and the localized mean vector scenario ( see follows for exact definition ) . however , the case of @xmath0large @xmath2-large @xmath3 \" with comparable magnitude and the delocalized scenario are common issues in high dimensional classification .",
    "the classification rules proposed in the paper will help to handle these situations .",
    "saranadasa ( 1993 ) proposes the determinant - based ( d- ) and trace - based ( t- ) criteria .",
    "their asymptotic misclassification probabilities are established for normal populations . in this paper , we focus on the performance of these two criteria in the delocalized scenario without the normal assumption .",
    "specifically , consider two @xmath2-dimensional multivariate populations @xmath4 and @xmath5 with respective mean vectors @xmath6 , @xmath7 and common covariance matrix @xmath8 .",
    "the parameters @xmath6 , @xmath7 and @xmath8 are unknown and thus estimated using training samples @xmath9 from @xmath4 and @xmath10 from @xmath5 with respective sample size @xmath11 and @xmath12 . a new observation vector ,",
    "@xmath13 is known to belong to @xmath4 or @xmath5 and the aim is to find exactly its origin population .",
    "more complicated sample setting can refer to leung ( 2001 ) , which considers mixed continuous and discrete variables in each group .",
    "cheng ( 2004 ) studies the situation where the two populations have different covariance matrices .",
    "krzyko and skorzybut ( 2009 ) considers the multivariate repeated measures data with kronecker product covariance structures .",
    "let @xmath14 , @xmath15 be the two training sample mean vectors where @xmath16 if the vector @xmath17 is classified to the population @xmath4 , then the overall within group sum of squares and cross products matrix is @xmath18 while , if @xmath17 is classified to @xmath5 , then the sum is @xmath19 intuitively , one would decide @xmath20 when @xmath21 is in some sense @xmath0smaller \" than @xmath22 .",
    "the d - criterion defines this smallness to be @xmath23 and the t - criterion defines it to be @xmath24    two scenarios of mean difference @xmath25 are defined as follows :    1 .   _",
    "localized scenario _ : the difference @xmath26 is concentrated on a small number of variables .",
    "we set @xmath27 and @xmath7 equals to a sparse vector : @xmath28 , where @xmath29 is the sparsity size .",
    "notice that the location of the @xmath29 non - zero components does not influence the performance of various classifiers .",
    "_ delocalized scenario _",
    ": the difference @xmath26 is dispersed in most of the variables . to ease the comparison with the localized scenario",
    ", we choose the parameters such that the averaged mahalanobis distances are the same under these two scenarios .",
    "this is motivated by the fact that following fisher ( 1936 ) , the difficulty of classification mainly depends on the mahalanobis distance @xmath30 between two populations . more precisely , we set @xmath27 and the elements of @xmath7 are randomly drawn from the uniform distribution @xmath31 where @xmath32 is the mahalanobis distance under the localized scenario , and @xmath33 is a parameter chosen to fulfill the requirement @xmath34 where @xmath35 is the mahalanobis distance under the delocalized scenario .",
    "direct calculations lead to @xmath36 for an equal correlation structure , @xmath37 for @xmath38 and @xmath39 .",
    "for an autoregressive correlation structure , @xmath40 , we find @xmath41    by focusing on the delocalized scenario , simulation study is conducted to display the performances of proposed procedures .    as the main contribution of this paper",
    ", we generalize the d- and t- criteria from normality to general populations and establish their asymptotic misclassification probabilities .",
    "as it will be proven , the misclassification probability of the d - criterion will depend on the mahalanobis distance between the two populations , and the misclassification probability of the t - criterion will depend on the difference of two group mean vectors and the skewness and kurtosis coefficients of the two populations @xmath4 and @xmath5 .",
    "the rest of the paper is organized as follows . in section  [ sec:1 ] , the asymptotic misclassification probability of the d - criterion under general populations is derived and monte carlo experiments are conducted to compare the performance with that of several existing classification rules . in section  [ sec:7 ] , the asymptotic misclassification probability of the t - criterion under general populations is derived . and",
    "a real data is used to present the competitive performance of the t - criterion .",
    "the conclusion is made at the end of the paper .",
    "technical proofs are relegated to the appendix .",
    "unlike the normal populations assumed in saranadasa ( 1993 ) , we assume that the populations @xmath4 and @xmath5 have the general form as introduced in bai and saranadasa ( 1996 ) , i.e.    \\(a ) the population @xmath42 has the form @xmath43 , where @xmath44 is a @xmath45 mixing or loading matrix , and @xmath46 has @xmath2 independent and identically distributed , centered and standardized components .",
    "moreover , @xmath47 and we set @xmath48 .",
    "\\(b ) similarly , the population @xmath49 has the form @xmath50 , where @xmath51 has @xmath2 independent and identically distributed , centered and standardized components .",
    "we set @xmath52 and @xmath53 .    in consequence , the new observation @xmath54 where @xmath55 in distribution and @xmath56 if @xmath57 .",
    "throughout the paper , we set @xmath58 , @xmath59 and @xmath60 .",
    "notice that the data - generation model ( a)(b ) are quite general meaning that the population are linear combinations of some unobservable independent component .",
    "they are also adopted in overall recent studies on high - dimensional statistics , see chen et al .",
    "( 2010 ) , li and chen ( 2012 ) , srivastava et al .",
    "( 2011 ) and etc .",
    "the d - criterion ( [ e1 ] ) is easily seen equivalent to classifying @xmath17 into @xmath4 when @xmath61 where @xmath62 involves correlation information between variables .",
    "this criterion has a straightforward form and does not need a preliminarily selected subset of features or any thresholding parameter .    the associated error of misclassifying @xmath20 into @xmath5 is @xmath63 under the data - generation models ( a ) and ( b ) , since @xmath64 , we have @xmath65 , or @xmath66 , where @xmath67 the misclassification probability ( [ e5 ] ) is rewritten as @xmath68    here is the first main result of this paper .",
    "[ t1 ] under the data - generation models ( a ) and ( b ) , assume that the following hold :    1 .",
    "@xmath69 and @xmath70 , where @xmath71 ; 2 .",
    "@xmath72 and @xmath73 for some constant @xmath74 .",
    "then as @xmath75 , the misclassification probability ( [ e7 ] ) for the d - criterion satisfies @xmath76 where @xmath77 is the mahalanobis distance between the two populations @xmath4 and @xmath5 .",
    "the proof of the theorem is given in appendix 1 .",
    "the significance of the result is as follows .",
    "the asymptotic value of @xmath78 depends on the values of @xmath79 , @xmath80 and @xmath81 , and is independent of other characteristics of the distributions @xmath4 and @xmath5 .",
    "firstly , this asymptotic value is symmetric about @xmath80 , so the value remains unchanged under a switch of the populations @xmath4 and @xmath5 .",
    "secondly , if @xmath11 and @xmath12 do not have large difference , i.e. @xmath82 or @xmath83 , the asymptotic value of @xmath78 mainly depends on @xmath84 when @xmath79 is fixed . in other words ,",
    "the classification task becomes easier for the d - criterion when the mahalanobis distance between two populations increases as expected .",
    "when @xmath85 , the number of features is very close to the sample size , the classification task becomes harder for the d - criterion due to the instability of the inverse @xmath86 , a phenomenon well - noticed in high - dimensional statistical literature .    under normal assumption ,",
    "saranadasa ( 1993 ) derived another asymptotic value for @xmath78 @xmath87 notice that @xmath88 , with @xmath89    let us comment on the difference between @xmath90 and @xmath91 .",
    "the value of @xmath80 does not influence on the difference significantly . without loss of generality ,",
    "let @xmath92 .",
    "the factor @xmath93 is 1/2 when @xmath79 and @xmath81 satisfy @xmath94 . under this setting",
    ", figure  [ fig:1 ] shows the asymptotic values @xmath90 , @xmath91 and compares them to empirical values from simulations , as @xmath79 ranges from 0.1 to 0.9 with step 0.1 .",
    "obviously , the difference between the two values are non - negligible ranging from @xmath95 to @xmath96 .",
    "moreover , @xmath90 is much closer to the empirical values than @xmath91 .",
    "so our asymptotic result is more accurate .",
    "other experiments have shown that only when the ratio of @xmath79 and @xmath81 reaches some small values as of order @xmath97 , the difference between them can be negligible .",
    "( solid ) , @xmath91 ( dashes ) and empirical values ( dots ) with 10,000 replications under normal samples . @xmath98 and @xmath2 ranges from 50 to 450 with step 50 . ]",
    "additional experiments are conducted to check the accuracy of the asymptotic value @xmath90 .",
    "figure  [ fig:2 ] compares the values of @xmath90 to empirical values from simulations for normal samples .",
    "the empirical misclassification probabilities are very close to the theoretical values of @xmath90 .",
    "it s the same for both @xmath99 and @xmath100 situations .",
    "we conduct extensive tests to compare the d - criterion with several existing classification methods for high - dimensional data , the road method and its variants s - road1 ands - road2 , scrda , and the nb method , as well as the oracle .",
    "the oracle is defined following fan et al .",
    "( 2012 ) as the fisher s lda with true mean and true covariance matrix .    in all simulation studies ,",
    "the number of variables is @xmath101 , and the sample sizes of the training and testing data in two groups are @xmath102 .",
    "the sparsity size is set to be @xmath103 . a similar",
    "setting is used in fan et al .",
    "delocalized scenario is considered .      in this part",
    ", the covariance @xmath8 is set to be an equal correlation matrix and correlation coefficient @xmath104 ranges from 0 to 0.9 with step 0.1 .",
    "simulation results for normal samples are shown in table  [ tab:1 ] and a graphical summary is given in figure  [ fig:3 ] including the median classification errors and standard errors .",
    "the d - criterion performs similarly to the road in terms of classification errors and is more robust than road when @xmath104 is smaller than 0.5 .",
    "the nb and the t - criterion lose efficiency when correlation exists in this setting . notice that the results of scrda calculated using the r package provided by guo et al .",
    "( 2005 ) are not included .",
    "the package turns out to fail in some of our settings and report @xmath0na \" value .",
    "the percentage of failures in the simulations can reach 58% . therefore , it is unreliable to include scrda for comparison .",
    "c|cccccccc @xmath104 & d - criterion & road & s - road1 & s - road2 & nb & oracle & t - criterion + 0 & 9.6(1.55 ) & 9.4(2.91)&11.4(3.54)&9.6(3.24 ) & 6.6(1.23 ) & 5.6(1.13 ) & 6.2(1.18 ) + 0.1 & 9.2(1.52 ) & 8.4(2.50)&8.6(2.58)&8.4(2.50 ) & 12.4(1.57)&5.4(1.12 ) & 12.4(1.57 ) + 0.2 & 8.0(1.49 ) & 7.2(2.39)&7.4(2.42)&7.2(2.39 ) & 16.8(1.77)&4.4(1.06 ) & 16.8(1.76 ) + 0.3 & 6.4(1.37 ) & 6.0(1.87)&6.0(1.86)&6.0(1.87 ) & 20.2(1.88)&3.4(0.96 ) & 20.2(1.87 ) + 0.4 & 5.0(1.24 ) & 4.6(1.55)&4.6(1.55)&4.6(1.55 ) & 22.6(1.94)&2.4(0.82 ) & 22.6(1.94 ) + 0.5 & 3.4(1.04 ) & 3.2(1.02)&3.2(1.02)&3.2(1.02 ) & 24.6(2.00)&1.6(0.65 ) & 24.6(1.99 ) + 0.6 & 2.0(0.79 ) & 1.8(0.73)&1.8(0.74)&1.8(0.73 ) & 26.2(2.04)&0.8(0.46 ) & 26.2(2.03 ) + 0.7 & 0.8(0.51 ) & 0.8(0.47)&0.8(0.47)&0.8(0.47 ) & 27.4(2.06)&0.2(0.26 ) & 27.4(2.05 ) + 0.8 & 0.2(0.22 ) & 0.2(0.20)&0.2(0.20)&0.2(0.20 ) & 28.6(2.08)&0.0(0.09 ) & 28.6(2.07 ) + 0.9 & 0.0(0.02 ) & 0.0(0.02)&0.0(0.02)&0.0(0.02 ) & 29.6(2.10)&0.0(0.00 ) & 29.6(2.10 ) +    c|cccccccc @xmath104 & d - criterion & road & s - road1 & s - road2 & nb & oracle & t - criterion + 0 & 12.0(1.55 ) & 9.0(2.76)&9.0(2.80)&9.0(3.24 ) & 9.1(1.29 ) & 7.8(1.29 ) & 8.6(1.24 ) + 0.1 & 11.6(1.56 ) & 9.8(3.11)&15.2(6.32)&11.6(3.61 ) & 15.2(4.17)&7.6(1.27 ) & 14.8(3.40 ) + 0.2 & 10.4(1.48 ) & 8.6(2.81)&19.6(6.76)&11.4(3.44 ) & 19.2(7.00)&6.6(1.23 ) & 19.0(5.79 ) + 0.3 & 9.0(1.38 ) & 7.4(2.36)&24.0(7.26)&10.6(3.00 ) & 22.4(8.83)&5.6(1.16 ) & 22.0(7.58 ) + 0.4 & 7.6(1.27 ) & 6.0(1.50)&27.6(8.06)&9.2(2.73 ) & 24.8(10.15)&4.6(1.06 ) & 24.2(8.99 ) + 0.5 & 6.0(1.13 ) & 4.8(1.00)&28.9(9.35)&7.8(2.26 ) & 27.0(11.11)&3.4(0.91 ) & 26.2(10.11 ) + 0.6 & 4.4(0.97 ) & 3.4(0.84)&29.2(10.83)&6.0(1.73 ) & 29.0(11.90)&2.4(0.75 ) & 27.6(11.02 ) + 0.7 & 2.8(0.78 ) & 2.0(0.65)&29.2(12.32)&4.0(1.26 ) & 30.6(12.51)&1.4(0.57 ) & 29.0(11.79 ) + 0.8 & 1.2(0.53 ) & 0.8(0.43)&28.8(13.74)&2.0(0.90 ) & 32.0(13.01)&0.6(0.36 ) & 30.2(12.44 ) + 0.9 & 0.2(0.23 ) & 0.2(0.20)&28.6(15.06)&0.4(0.39 ) & 33.4(13.35)&0.0(0.14 ) & 31.2(12.96 ) +    simulation results for student s t ( degree of freedom is set to be 7 ) samples are shown in table  [ tab:2 ] .",
    "all classifiers have slightly higher misclassification rates for student s t samples",
    ". s - road1 and s - road2 have larger standard errors . and s - road1 , nb and t - criterion lose efficiency when correlation is significant .",
    "the d - criterion outperforms the others except road in term of classification error .",
    "but the d - criterion has the smallest standard error which is close to that of oracle .      in this part",
    ", the covariance @xmath8 is set to be an autoregressive correlation matrix and @xmath104 ranges from 0 to 0.9 with step 0.1 .",
    "previous the results have shown that nb is not a good rule when significant correlation exists . therefore , nb is no more included in comparison .",
    "since the comparison results are similar in normal samples and studentt t samples , we only use normal samples in this part .",
    "simulation results are shown in table  [ tab:3 ] and a graphical summary is given in figure  [ fig:4 ] .",
    "the t - criterion is only suitable for independent case @xmath105 , and loses efficiency when @xmath106 .",
    "the d - criterion has the same performance with road and s - road2 in terms of classification error .",
    "moreover , the d - criterion is much more robust and has a standard error close to that of the oracle .",
    "c|ccccccc @xmath104 & d - criterion & road & s - road1 & s - road2 & oracle & t - criterion + 0 & 9.6 ( 1.55 ) & 9.4 ( 2.91 ) & 11.6 ( 3.54)&9.6 ( 3.24 ) & 5.6(1.13)&6.2(1.18 ) + 0.1 & 11.8(1.68 ) & 11.4(3.42)&12.8(3.67)&11.6(3.61 ) & 0.0(0.09)&8.0(1.31 ) + 0.2 & 14.2(1.80 ) & 13.4(4.27)&14.4(4.02)&13.6(4.39)&0.0(0.15)&10.0(1.44 ) + 0.3 & 16.4(1.89 ) & 15.4(5.48)&16.0(4.61)&15.6(5.55)&0.4(0.33)&12.2(1.57 ) + 0.4 & 18.6(1.99 ) & 17.4(6.78)&17.8(5.95)&17.6(6.73)&1.8(0.64)&14.8(1.70 ) + 0.5 & 20.8(2.07 ) & 19.6(7.54)&20.0(7.29)&19.8(7.52)&4.6(1.02)&17.8(1.81 ) + 0.6 & 22.6(2.16 ) & 22.0(7.53)&22.6(7.34)&22.2(7.46)&8.6(1.38)&21.4(1.92 ) + 0.7 & 23.6(2.26 ) & 23.8(7.71)&26.0(7.54)&24.0(7.64)&12.6(1.71)&25.0(2.03 ) + 0.8 & 22.8(2.38 ) & 23.2(8.14)&30.6(7.67)&23.8(8.19)&14.6(1.94)&31.0(2.12 ) + 0.9 & 17.0(2.39 ) & 17.0(7.31)&33.4(9.13 ) & 18.0 ( 8.26)&11.4(1.93)&37.0(2.19 ) +    in conclusion , compared to these existing methods , the d - criterion is competitive for @xmath0large @xmath2-large @xmath3 \" situation specifically under delocalized scenario and autoregressive correlation structure . in such a scenario ,",
    "the d - criterion has a classification error comparable to that of the road - family classifiers while being the most robust with a much smaller standard error close to that of the oracle .",
    "notice that one limitation of the d - criterion is that the dimension @xmath2 must be smaller than the sample size @xmath3 .",
    "in addition , when the ratio @xmath107 is close to @xmath108 , the performance of this criterion becomes bad due to the matrix @xmath109 is close to singular .",
    "the t - criterion in contrast does not have such a limitation .",
    "the t - criterion ( [ e2 ] ) is easily seen equivalent to @xmath110 obviously , the t - criterion has a very simple form only involving the group mean vectors .",
    "in particular , it does not require to select a subset of features or to choose a threshold parameter .    when @xmath20 , the error of misclassifying @xmath17 into @xmath5 is @xmath111 here is the second main result of this paper . throughout the paper ,",
    "@xmath112 is a length @xmath113 vector with all entries 1 , @xmath114 is a length @xmath113 vector with all entries 0 .",
    "[ t2 ] under the data - generation models ( a ) and ( b ) , assume that the following hold :    1 .",
    "@xmath115 and @xmath116 for some constant @xmath74 ; 2 .",
    "the covariance matrix @xmath8 is diagonal , i.e. @xmath117 ; 3 .   @xmath118 ; and 4 .",
    "@xmath119 as @xmath120 , where @xmath121",
    ".    then we have as @xmath122 and @xmath123 , @xmath124 where @xmath125    the proof of the theorem is given in appendix 2 .",
    "assumption 1 is needed for dealing with non - normal populations .",
    "assumption 3 is a weak and technical condition without any practical limitation .",
    "assumption 4 is satisfied for most applications where typically @xmath126 and @xmath127 are all of order @xmath2 .",
    "the main term of @xmath128 is , @xmath129 since it has the order @xmath130 and other terms are @xmath131 . in order to get more accurate result in finite sample case ,",
    "these @xmath131 terms are kept in the theorem .",
    "notice that the main term of the approximation of @xmath78 depends on the ratio @xmath132 .",
    "if the components @xmath133 of @xmath26 satisfy @xmath134 , and @xmath135 for positive constants @xmath136 , then when @xmath122 , @xmath137 and @xmath138 in other words , the classification task becomes easier when the dimension grows .",
    "in other scenarios , this misclassification probability is not guaranteed to vanish .",
    "for example , under a localized scenario , @xmath139 , @xmath140 for @xmath141 and @xmath29 is fixed and independent of @xmath2 , then @xmath142    next , we provide below some simulation results to demonstrate the importance of keeping the @xmath131 terms in @xmath128 .",
    "the experiments use @xmath143 and various combinations of sample sizes @xmath144 with normal samples and gamma samples , respectively .",
    "empirical classification errors are compared in figure  [ fig:5 ] to the following three approximations of the variance @xmath128 :    * @xmath145 ; * @xmath146 ; * @xmath147 .    among the three ,",
    "the proposed approximation @xmath148 matches very well the empirical values , while @xmath149 is by far the worst in all tested cases . as for @xmath148 and @xmath150 , they are by definition the same for normal samples ( since @xmath151 ) . for gamma samples ,",
    "they remain close each other particularly when the relative difference of sample sizes @xmath152 become small , and @xmath148 has an overall slightly better performance than @xmath150 ( in these tested cases ) .",
    "notice that the gamma standardized variables are @xmath153 where @xmath154 is gamma distributed with unit shape and scale parameters so that @xmath155 .    under normal assumption",
    ", the expectation of @xmath156 ( defined in appendix ) is the same with ( [ e18 ] ) , and the variance simplifies to @xmath157 which coincides with the result established in saranadasa ( 1993 ) .",
    "we conduct simulations to show the performances of the t - criterion for normal distributions under delocalized scenario . in the simulation studies ,",
    "the number of variables is @xmath143 . without loss of generality , the sample sizes of the training and testing data in two groups are equal and range from 100 to 500 with step 50 .",
    "the covariance @xmath8 is set to be an identity matrix @xmath158 and the sparsity size is @xmath103 .",
    "simulation results are shown in table  [ tab:4 ] .",
    "the classification error decreases as sample size increases .",
    "meanwhile , small standard errors indicate that the t - criterion is robust with respect to the delocalization nature of mean differences .",
    "notice that the t - criterion is an independence rule .",
    "it s suitable for case where variables are independent or the correlations between variables are weak .",
    "as shown in tables 1 - 3 , the t - criterion has very high misclassification rate when variables have significant correlations .",
    "c|cccc|ccccc  & & + @xmath99 & 100 & 150 & 200 & 250 & 300 & 350 & 400 & 450 & 500 + median & 13.00 & 11.00 & 9.75 & 9.00 & 8.50 & 8.14 & 7.88 & 7.56 & 7.40 + s.e . &",
    "( 2.52 ) & ( 1.90 ) & ( 1.57 ) & ( 1.35 ) & ( 1.20 ) & ( 1.11 ) & ( 1.01 ) & ( 0.95 ) & ( 0.89 ) +      in this part , we analyze a popular gene expression data : @xmath159leukemia ( golub et al . 1999 ) .",
    "the leukemia data set contains @xmath160 genes for @xmath161 acute lymphoblastic leukemia and @xmath162 acute myeloid leukemia vectors in the training set .",
    "the testing set includes 20 acute lymphoblastic leukaemia and 14 acute myeloid leukemia vectors .",
    "obviously , this data set is a @xmath0large @xmath2-small @xmath3 \" case .",
    "the classification results for the t - criterion , road , scrda , falr , nsc and nb methods are shown in table  [ tab:5 ] .",
    "( the results for road , scrda , fair , nsc and nb are found in fan et al .",
    "( 2012 ) . )",
    "the t - criterion is as good as road and nb in terms of training error .",
    "road and fair perform better than t - criterion in terms of testing error .",
    "both of nb and t - criterion make use of all genes , but t - criterion outperforms nb .",
    "meanwhile , t - criterion performs better than nsc .",
    "overall , on this data set , t - criterion outperforms scrda , nsc and nb , equally well as fire , and is beaten only by road ( 2 v.s .",
    "1 errors ) .",
    "it s quite surprising that a @xmath0simple - minded@xmath163 rule like t - criterion has a performance comparable to a sophisticated rule like road",
    ".     used + t - criterion & 0 & 2 & 7129 + road & 0 & 1 & 40 + scrda & 1 & 2 & 264 + fair & 1 & 1 & 11 + nsc & 1 & 3 & 24 + nb & 0 & 5 & 7129 +",
    "we have proposed two new classification rules for high - dimensional data , namely the d - criterion and the t - criterion .",
    "both methods consider the overall within group sum of squares and cross products matrices .",
    "the d - criterion compares the determinants of these matrices and integrates correlation information between variables .",
    "the d - criterion performs well when correlations between variables become significant . when the correlation coefficient increases , the classification error of the d - criterion drops .",
    "the incorporation of covariance structure therefore strengthens the effectiveness in high dimensional classification . the t - criterion , on the other hand ,",
    "compares the traces of these matrices and involves only group mean vectors .",
    "the implementation of these two criteria is straightforward and it does not suffer from challenging issues such as variable selection , thresholding or control of the sparsity size that are required in the existing methods .",
    "we found d - criterion is particularly competitive in delocalized scenario . when @xmath164 , the t - criterion is quite effective as proven by the real data analysis .",
    "moreover , using the explicit forms of the criteria and recent results from random matrix theory , we are able to derive asymptotic approximations for the misclassification probability of both criteria . notice that such asymptotic approximations are unknown for most of the existing high - dimensional classifiers in the literature .",
    "simulation results have shown that the proposed approximations are quite accurate for both normal and non - normal populations .",
    "under the data - generation models ( a ) and ( b ) , let @xmath172 . conditioned on @xmath173 , the misclassification probability ( [ e7 ] ) can be rewritten as @xmath174 where @xmath175 therefore , @xmath176 where @xmath20 is assumed implicitly .",
    "it is easy to obtain the conditional expectation ( [ e13 ] ) . for the conditional variance of @xmath177 ,",
    "we first calculate the conditional second moment @xmath181 ^ 2 \\\\ & & \\quad \\quad + \\alpha_2 ^ 2 [ \\mathbf{z}^{\\ast \\prime } \\tilde{\\mathbf{a}}^{-1}\\mathbf{z}^{\\ast } -2 ( \\bar{\\mathbf{y}}^\\ast + \\tilde{\\vec{\\mu}})^\\prime \\tilde{\\mathbf{a}}^{-1}\\mathbf{z}^{\\ast } + ( \\bar{\\mathbf{y}}^\\ast + \\tilde{\\vec{\\mu}})^\\prime \\tilde{\\mathbf{a}}^{-1 } ( \\bar{\\mathbf{y}}^\\ast + \\tilde{\\vec{\\mu}})]^2 \\\\ & & \\quad \\quad-2\\alpha_1\\alpha_2 [ \\mathbf{z}^{\\ast \\prime } \\tilde{\\mathbf{a}}^{-1}\\mathbf{z}^{\\ast } - 2 \\bar{\\mathbf{x}}^{\\ast \\prime } \\tilde{\\mathbf{a}}^{-1}\\mathbf{z}^{\\ast } + \\bar{\\mathbf{x}}^{\\ast \\prime } \\tilde{\\mathbf{a}}^{-1 } \\bar{\\mathbf{x}}^{\\ast } ] \\\\ & & \\quad \\quad \\quad \\quad \\times [ \\mathbf{z}^{\\ast \\prime } \\tilde{\\mathbf{a}}^{-1}\\mathbf{z}^{\\ast } -2 ( \\bar{\\mathbf{y}}^\\ast + \\tilde{\\vec{\\mu}})^\\prime \\tilde{\\mathbf{a}}^{-1}\\mathbf{z}^{\\ast } + ( \\bar{\\mathbf{y}}^\\ast + \\tilde{\\vec{\\mu}})^\\prime \\tilde{\\mathbf{a}}^{-1 } ( \\bar{\\mathbf{y}}^\\ast + \\tilde{\\vec{\\mu } } ) ] \\big\\}.\\end{aligned}\\ ] ] since @xmath182 ^ 2 = ( \\gamma_x -3 ) \\sum_l b_{ll}^2 + \\big(\\textrm{tr } \\tilde{\\mathbf{a}}^{-1}\\big)^2 + 2 \\textrm{tr } ( \\tilde{\\mathbf{a}}^{-2});\\\\ & & \\mathit{e}_\\omega\\left[\\mathbf{z}^{\\ast \\prime } \\tilde{\\mathbf{a}}^{-1}\\mathbf{z}^{\\ast}\\cdot \\bar{\\mathbf{x}}^{\\ast \\prime } \\tilde{\\mathbf{a}}^{-1}\\mathbf{z}^{\\ast}\\right ] = \\theta_x \\sum_l b_{ll } \\big(\\tilde{\\mathbf{a}}^{-1}\\bar{\\mathbf{x}}^{\\ast}\\big)_l;\\\\ & & \\mathit{e}_\\omega\\left [ \\mathbf{z}^{\\ast \\prime } \\tilde{\\mathbf{a}}^{-1}\\mathbf{z}^{\\ast}\\cdot ( \\bar{\\mathbf{y}}^\\ast + \\tilde{\\vec{\\mu}})^\\prime \\tilde{\\mathbf{a}}^{-1}\\mathbf{z}^{\\ast } \\right ] = \\theta_x \\sum_l b_{ll } \\big(\\tilde{\\mathbf{a}}^{-1}(\\bar{\\mathbf{y}}^\\ast + \\tilde{\\vec{\\mu}})\\big)_l ; \\\\ & & \\mathit{e}_\\omega \\left [ \\bar{\\mathbf{x}}^{\\ast \\prime } \\tilde{\\mathbf{a}}^{-1}\\mathbf{z}^{\\ast } \\cdot \\mathbf{z}^{\\ast \\prime}\\tilde{\\mathbf{a}}^{-1 } \\bar{\\mathbf{x}}^{\\ast}\\right ] = \\bar{\\mathbf{x}}^{\\ast \\prime } \\tilde{\\mathbf{a}}^{-2}\\mathbf{x}^{\\ast};\\\\ & & \\mathit{e}_\\omega \\left [   ( \\bar{\\mathbf{y}}^\\ast + \\tilde{\\vec{\\mu}})^\\prime \\tilde{\\mathbf{a}}^{-1}\\mathbf{z}^{\\ast } \\cdot \\mathbf{z}^{\\ast \\prime } \\tilde{\\mathbf{a}}^{-1 } ( \\bar{\\mathbf{y}}^\\ast + \\tilde{\\vec{\\mu}})\\right ] = ( \\bar{\\mathbf{y}}^\\ast + \\tilde{\\vec{\\mu}})^\\prime \\tilde{\\mathbf{a}}^{-2 } ( \\bar{\\mathbf{y}}^\\ast + \\tilde{\\vec{\\mu}}),\\end{aligned}\\ ] ] we obtain @xmath183 finally , by @xmath184 equation ( [ e14 ] ) follows . the lemma  [ l2 ] is proved .",
    "the first step of the proof of theorem  [ t1 ] is similar to the one of the proof of theorem  [ t2 ] where we ensure that @xmath185 satisfies the lyapounov condition .",
    "the details are referred to ( [ e14 ] ) .",
    "therefore , conditioned on @xmath173 , as @xmath166 , the misclassification probability for the d - criterion satisfies @xmath186 next , we look for main terms in @xmath187 and @xmath188 , respectively , using lemma  [ l2 ] . for @xmath187 , we find the following equivalents for the three terms                  by the assumption 2 in theorem  [ t2 ] , the covariance matrix is @xmath207 . under the data - generation models ( a ) and ( b )",
    ", the misclassification probability ( [ e10 ] ) can be rewritten as @xmath208 where @xmath209        1 .",
    "@xmath210 + and @xmath211 2 .",
    "@xmath212 + and @xmath213\\textrm{tr}(\\mathbf{\\sigma}^2 ) + \\beta_2(\\theta ) \\mathbf{i}^\\prime \\gamma^3\\vec{\\delta } + 4\\alpha_2\\vec{\\delta}^\\prime \\mathbf{\\sigma}\\vec{\\delta},\\label{e19}\\end{aligned}\\ ] ] where @xmath214 if removing the small terms with order @xmath215 , then the formula of @xmath128 in theorem  [ t2 ] is obtained .        for the variance , we have @xmath221 ^ 2 \\nonumber \\\\ & = & \\sigma_{ll}^2 \\cdot \\mathit{e}\\left\\{\\alpha_1 ( z^\\ast_l-\\bar{x}^\\ast_l)^2-\\alpha_2(z^\\ast_l -\\bar{y}^\\ast_l-\\tilde{\\mu}_l)^2 + \\alpha_2\\tilde{\\mu}_l^2\\right\\}^2\\nonumber \\\\[1 mm ] & = & \\sigma_{ll}^2\\cdot \\big\\{\\alpha_1 ^ 2 \\mathit{e}(z^\\ast_l-\\bar{x}^\\ast_l)^4 + \\alpha_2 ^ 2 \\mathit{e}(z^\\ast_l-\\bar{y}^\\ast_l)^4 + 4 \\alpha_2 ^ 2\\tilde{\\mu}_l^2 \\mathit{e}(z^\\ast_l-\\bar{y}^\\ast_l)^2 \\nonumber \\\\[1 mm ] & & \\hskip1 cm -2\\alpha_1\\alpha_2 \\mathit{e}\\left[(z^\\ast_l-\\bar{x}^\\ast_l)^2 ( z^\\ast_l-\\bar{y}^\\ast_l)^2\\right ] -4 \\alpha_2 ^ 2 \\tilde{\\mu}_l \\mathit{e}(z^\\ast_l -\\bar{y}^\\ast_l)^3 \\nonumber \\\\[1 mm ] & & \\hskip1 cm + 4\\alpha_1 \\alpha_2 \\tilde{\\mu}_l \\mathit{e}[(z^\\ast_l-\\bar{x}^\\ast_l)^2(z^\\ast_l-\\bar{y}^\\ast_l)]\\big\\}.\\end{aligned}\\ ] ] moreover , @xmath222 ^ 4 & = & \\gamma_x\\left(1+\\frac{1}{n_1 ^ 3}\\right ) + \\frac{6n_1 ^ 2 + 3n_1 - 3}{n_1 ^ 3},\\\\ \\mathit{e}[z^\\ast_l-\\bar{y}^\\ast_l]^4 & = & \\gamma_x+\\frac{\\gamma_y}{n_2 ^ 3}+\\frac{6n_2 ^ 2 + 3n_2 - 3}{n_2 ^ 3},\\\\ \\mathit{e}[z^\\ast_l-\\bar{y}^\\ast_l]^2 & = & \\alpha_2^{-1},\\\\ \\mathit{e}[z^\\ast_l -\\bar{y}^\\ast_l]^3 & = & \\theta_x -\\frac{\\theta_y}{n_2 ^ 2},\\\\ \\mathit{e}\\left\\{[z^\\ast_l-\\bar{x}^\\ast_l]^2[z^\\ast_l-\\bar{y}^\\ast_l]^2\\right\\ } & = & \\gamma_x + \\frac{1}{\\alpha_1\\alpha_2}-1,\\end{aligned}\\ ] ] and @xmath223 finally , we obtain @xmath224 + \\alpha_2 ^ 2\\left[\\gamma_x+\\frac{\\gamma_y}{n_2 ^ 3 } + \\frac{6n_2 ^ 2 + 3n_2 - 3}{n_2 ^ 3}\\right]\\\\ & & \\quad \\quad + 4\\alpha_2 ^ 2\\tilde{\\mu}_l^2\\alpha_2^{-1 } -2\\alpha_1\\alpha_2 \\left[\\gamma_x + \\frac{1}{\\alpha_1\\alpha_2}-1\\right ] + 4\\alpha_1\\alpha_2\\tilde{\\mu}_l \\theta_x -4\\alpha_2 ^ 2 \\tilde{\\mu}_l \\left[\\theta_x-\\frac{\\theta_y}{n_2 ^ 2}\\right ] \\bigg\\}\\\\ & = & \\sigma_{ll}^2 \\bigg\\ { \\gamma_x\\left(\\alpha_1 ^ 2+\\frac{\\alpha_1 ^ 2}{n_1 ^ 3}+\\alpha_2 ^ 2 - 2\\alpha_1\\alpha_2\\right ) + \\frac{\\alpha_2 ^ 2\\gamma_y}{n_2 ^ 3 } + \\alpha_1 ^ 2\\frac{6n_1 ^ 2 + 3n_1 - 3}{n_1 ^ 3}\\\\ & & \\quad \\quad + \\alpha_2 ^ 2\\frac{6n_2 ^ 2 + 3n_2 - 3}{n_2 ^ 3}-2 + 4\\alpha_2\\tilde{\\mu}_l^2 + 2\\alpha_1\\alpha_2   + 4\\alpha_2(\\alpha_1-\\alpha_2)\\tilde{\\mu}_l \\theta_x + \\frac{4\\tilde{\\mu}_l}{n_2 ^ 2}\\theta_y \\bigg\\}\\\\ & = & \\sigma_{ll}^2\\big\\{\\beta_0",
    "+ \\beta_1(\\gamma ) + \\beta_2(\\theta)\\tilde{\\mu}_l + 4\\alpha_2\\tilde{\\mu}_l^2\\big\\}.\\end{aligned}\\ ] ] equation ( [ e19 ] ) follows .",
    "then @xmath128 can be rewritten as @xmath225 \\textrm{tr}(\\mathbf{\\sigma}^2)\\\\ & & + \\left [ 4\\frac{n_2}{n_2 + 1}\\left(\\frac{1}{n_2 + 1}-\\frac{1}{n_1 + 1}\\right)\\theta_x",
    "+ \\frac{4}{n_2 ^ 2}\\theta_y \\right]\\mathbf{1}_p^\\prime \\gamma^3",
    "\\vec{\\delta}\\\\ & & + 4\\frac{n_2}{n_2 + 1}\\vec{\\delta}^\\prime \\mathbf{\\sigma}\\vec{\\delta}\\\\ & \\approx & \\big [ \\frac{4}{n_1}+\\frac{4}{n_2}+\\frac{3}{n_1 ^ 2 } + \\frac{3}{n_2 ^ 2 } + \\frac{2}{n_1n_2 } -\\frac{3}{n_1 ^ 3 }",
    "-\\frac{3}{n_2 ^ 3 } + \\frac{\\gamma_x}{n_1 ^ 2 } + \\frac{\\gamma_y}{n_2 ^ 2 } -\\frac{2\\gamma_x}{n_1n_2 } + \\frac{\\gamma_x}{n_1 ^ 3 } -\\frac{\\gamma_y}{n_2 ^ 3}\\big]\\textrm{tr}(\\mathbf{\\sigma}^2)\\\\ & & + \\left[4\\left(\\frac{1}{n_2}-\\frac{1}{n_1}\\right)\\theta_x + \\frac{4}{n_2 ^ 2}\\theta_y\\right]\\mathbf{1}_p^\\prime \\gamma^3 \\vec{\\delta}\\\\ & & + 4\\left(1-\\frac{1}{n_2}\\right)\\vec{\\delta}^\\prime \\mathbf{\\sigma}\\vec{\\delta}.\\end{aligned}\\ ] ] only keep the terms with order @xmath130 and @xmath131 we can get the formula of @xmath128 in theorem  [ t2 ] . the lemma  [ l3 ] is proved .",
    "we know that @xmath226_{1\\leq l \\leq p}$ ] are independent variables with zero mean .",
    "we use the lyapounov criterion to establish a clt for @xmath227 $ ] , that is , there is a constant @xmath228 such that @xmath229 \\to 0.\\end{aligned}\\ ] ] since @xmath230 the @xmath231norm of @xmath226 $ ] is @xmath232 + \\big|\\tilde{\\mu}_l\\big|^2 \\right\\}\\\\ & \\leq & \\sigma_{ll } \\left\\ { 6\\left[2\\gamma_{4+b^\\prime , x}^{1/(4+b^\\prime ) } + \\gamma_{4+b^\\prime , y}^{1/(4+b^\\prime)}\\right ] + \\big|\\tilde{\\mu}_l\\big|^2 \\right\\}.\\end{aligned}\\ ] ] then @xmath233^{2+b } \\leq c_b \\sigma_{ll}^{2+b } \\cdot \\left\\ { 1 + \\big|\\tilde{\\mu}_l\\big|^{4+b^\\prime } \\right\\},\\end{aligned}\\ ] ] where @xmath234 is some constant depending on @xmath235 . therefore , as @xmath236 , @xmath237^{2+b } & \\leq & c_b \\cdot \\frac{\\sum_l \\sigma_{ll}^{2+b } + \\sum_l \\sigma_{ll}^{2+b}|\\tilde{\\mu}_l|^{4 + 2b}}{\\left ( \\sum_l \\sigma_{ll}\\delta_l^2 \\right)^{1+b/2}}\\\\ & = & c_b\\cdot \\frac{\\sum_l \\sigma_{ll}^{2+b } + \\sum_l \\delta_l^{4 + 2b}}{(\\sum_l \\sigma_{ll}\\delta_l^2)^{1+b/2 } } \\quad \\to 0,\\end{aligned}\\ ] ] by the assumption 4 in theorem  [ t2 ] . finally , we have @xmath238\\rightarrow n(0,1 ) , \\ as \\   p\\to \\infty ,",
    "\\ n_\\ast \\to \\infty.\\end{aligned}\\ ] ] this ends of the proof of theorem  [ t2 ] .",
    "golub tr , slonim dk , tamayo p , huard c , gaasenbeek m , mesirov jp , coller h , loh ml , downing jr , caligiuri ma , bloomfield cd , lander es ( 1999 ) molecular classification of cancer : class discovery and class prediction by gene expression monitoring .",
    "sci 286:531 - 537            saranadasa h ( 1993 ) asymptotic expansion of the misclassification probabilities of d- and a - criteria for discrimination from two high dimensional populations using the theory of large dimensional random matrices .",
    "j multivar anal 46:154 - 174"
  ],
  "abstract_text": [
    "<S> in this paper , we generalize two criteria , the determinant - based and trace - based criteria proposed by saranadasa ( 1993 ) , to general populations for high dimensional classification . </S>",
    "<S> these two criteria compare some distances between a new observation and several different known groups . </S>",
    "<S> the determinant - based criterion performs well for correlated variables by integrating the covariance structure and is competitive to many other existing rules . </S>",
    "<S> the criterion however requires the measurement dimension be smaller than the sample size . </S>",
    "<S> the trace - based criterion in contrast , is an independence rule and effective in the @xmath0large dimension - small sample size \" scenario . </S>",
    "<S> an appealing property of these two criteria is that their implementation is straightforward and there is no need for preliminary variable selection or use of turning parameters . </S>",
    "<S> their asymptotic misclassification probabilities are derived using the theory of large dimensional random matrices . </S>",
    "<S> their competitive performances are illustrated by intensive monte carlo experiments and a real data analysis .    </S>",
    "<S> example.pdf gsave newpath 20 20 moveto 20 220 lineto 220 220 lineto 220 20 lineto closepath 2 setlinewidth gsave .4 setgray fill grestore stroke grestore </S>"
  ]
}