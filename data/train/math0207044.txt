{
  "article_text": [
    "in this paper , we consider a tracking problem for smooth function @xmath0 , @xmath1 , under observation @xmath2 for @xmath3 ( @xmath4 is large ) , where @xmath5 is a sequence of i.i.d . random variables with @xmath6 , and @xmath7",
    "is a positive constant . without additional assumptions on the function",
    "@xmath8 it is difficult to create an estimator even for large @xmath4 .",
    "the filtering approach , see bar - shalom and li @xcite , proposes an estimator in the form of kalman filter corresponding to a stochastic model for @xmath8 , e.g. @xmath8 is differentiable @xmath9 times , and @xmath9-th derivatives of @xmath8 is simulated by a white noise with a certain intensity . since @xmath8 is deterministic function ,",
    "the non - trivial part of such approach is a choice of filter parameters and asymptotic analysis of estimation risk in @xmath10 . on the other hand ,",
    "nonparametric statistic approach to the regression estimation of a function @xmath8 assumes that @xmath8 belong to some limited class .",
    "we take @xmath8 from the class @xmath11 ( introduced by stone , @xcite , @xcite and ibragimov and khasminskii , @xcite , @xcite ) of @xmath9 times continuously differentiable functions with hlder continuous last derivative ( here @xmath12 , @xmath13 and @xmath14 are the same for any function from the class ) : @xmath15 ;   \\\\   \\beta = k+\\alpha \\end{array } \\right\\}.\\ ] ] it is known @xcite , @xcite , @xcite , @xcite that there are kernel type estimators @xmath16 of @xmath17 , @xmath18 such that for a wide class of loss functions @xmath19 and @xmath20 ( @xmath21 is positive constant ) : @xmath22 and no estimator provides a better rate of convergence to zero in @xmath23 uniformly in @xmath11 .",
    "the same rate in @xmath4 is valid under fixed value @xmath24 for the estimation risk @xmath25 , @xmath26 .",
    "this rate can not be exceeded uniformly on any nonempty open set from @xmath27 .",
    "parallel to kernel type estimators ( see , e.g. @xcite ) , @xcite , @xcite ) , khasminskii and liptser @xcite proposed an on - line estimator ( hereafter for brevity @xmath28 is replaced by @xmath29 and @xmath30 by @xmath31 ) : @xmath32 subject to the initial conditions @xmath33 .",
    "the initial conditions are chosen as arbitrary bounded constants independent of @xmath4 .",
    "the parameters @xmath34 are specifically chosen .",
    "the vector @xmath35 with these entries is called the filter gain .",
    "the rigorous result given in @xcite is formulated as :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ let filter gain @xmath35 be chosen such that all roots of characteristic polynomial @xmath36 are different and have negative real parts .",
    "let the observation model defined in ( [ model ] ) , @xmath37 and @xmath38 .",
    "then , for the estimator given in ( [ 1.4tri ] ) there exist positive constants @xmath39 , @xmath40 ( independent of @xmath4 ) such that for any @xmath41 the normalized in @xmath4 risk obeys @xmath42 the rates @xmath43 , @xmath18 can not be improved .",
    "the boundary layer @xmath44 , where might fail , is inevitable . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the left side boundary layer @xmath44 is due to on - line limitations of the above tracking system .",
    "one can readily suggest an off - line modification with the same recursion in the backward time subject to some boundary conditions independent of observation @xmath45 s .",
    "this modification possesses the right side boundary layer @xmath46 $ ] and accuracy ( [ main0 ] ) on @xmath47 $ ] .",
    "so , some combination of the forward and backward time tracking algorithms allows ( [ main0 ] ) accuracy on @xmath48 $ ] .",
    "for instance , a combination of forward time tracking on the interval @xmath49 $ ] and backward time tracking on @xmath50 $ ] can be used .    in this paper",
    ", we deal with the estimator given in ( [ 1.4tri ] ) and restrict ourselves by considering @xmath8 from the class @xmath51 , i.e. the class of @xmath9-times differentiable functions @xmath8 with lipschitz continuous @xmath52 .",
    "a suitable choice of filtering gain @xmath35 should satisfy multiple requirements regarding the cost function @xmath40 and parameter @xmath39 , involved in the description of boundary layer .",
    "moreover , a correct choice of @xmath35 should guarantee that the roots of characteristic polynomial @xmath53 are different and have negative real parts .",
    "these requirements might contradict each other . to avoid contradictions , we use the fact that estimator has a structure of kalman filter .",
    "we build a kalman filter according to bar - shalom and li @xcite , so that @xmath52 is generated by a white noise with intensity @xmath54 . for each @xmath54",
    ", we choose the kalman gain @xmath55 and use it for minimization of @xmath56 in @xmath54 .",
    "so , the minimization problem of the cost function is controlled by single parameter and allows to establish a reasonable relationship between @xmath40 and @xmath39 .",
    "moreover , this type of minimization automatically guarantees negative real parts of the roots for characteristic polynomial @xmath53 . for @xmath57 the roots of @xmath53 are different and numerical verification of the same fact for @xmath58 is available .",
    "henceforth @xmath59 . for notational convenience",
    "we describe our problem in matrix notation .",
    "introduce the following matrices : @xmath60    @xmath61 notice that by lemma 3.1 in @xcite the roots of @xmath53 and eigenvalues of @xmath62 coincide . in accordance with this remark , while eigenvalues of @xmath62 have negative real parts we may describe the cost function @xmath40 in terms of the bias @xmath63 and variance @xmath64 for tracking errors ( see , ( [ prosto ] ) ) ( hereafter @xmath65 is the transposition symbol ) : @xmath66 where @xmath67 and the matrix @xmath68 solves the lyapunov equation @xmath69 in section [ sec-4 ] , we select the filter gain @xmath55 from one - parameter family @xmath70 where @xmath71 is a positive definite matrix given by the algebraic riccati equation @xmath72 finally we choose @xmath73 and the filter gain @xmath74 .",
    "a relevant choice of @xmath75 allows to have an acceptable value of the constant @xmath76 .      in section [ sec-5 ]",
    ", we show that the cost function is expressed as : @xmath77\\bigg).\\ ] ] furthermore , we give the explicit structure of @xmath55 as a function of the control parameter @xmath78 .",
    "namely @xmath79 where @xmath80 , @xmath81 are entries of the matrix @xmath82 being solution of the algebraic riccati equation @xmath83      here , we consider the tracking problem for lipschitz continuous function @xmath8 . since @xmath84 , we have @xmath85 , @xmath86 , @xmath87 , @xmath88 and so , @xmath89 .",
    "therefore , @xmath90 with @xmath91 , we have @xmath92 and @xmath93 .",
    "the following estimator is constructed ( here @xmath94 ) @xmath95    notice that the direct minimization of @xmath96 with respect to @xmath97 provides the optimal @xmath98 . for @xmath99",
    ", this coincidence is not guaranteed . under the direct minimization of the cost function @xmath40 with respect to @xmath35 the eigenvalues of @xmath100 might have nonnegative real parts .",
    "let us consider a numerical solution for @xmath101 , i.e. @xmath8 is twice differentiable function .",
    "its second derivative is lipschitz continuous with constant @xmath102 . for @xmath103 ,",
    "we find ( see figure [ fig1 ] ) @xmath104 and @xmath105 . according to table 1 , @xmath106 , @xmath107 , @xmath108 .",
    "hence @xmath109 , @xmath110 , and @xmath111 .",
    "so , the following estimator is constructed @xmath112 the combination of forward and backward tracking practically allows to eliminate the boundary layer ( see figure [ fig2 ] ) .    ]",
    "in this section , we present a vector of normalized tracking errors and derive the expression for @xmath40 .      for notational convenience , set @xmath113 and introduce a diagonal matrix @xmath114 and vector @xmath115 : @xmath116 we use a vector - matrix form of estimator @xmath117 and obvious identity    @xmath118      denote @xmath119 and introduce normalized error @xmath120 recursions ( [ vmf ] ) and ( [ vmf0 ] ) provide @xmath121 multiplying both sides of this equation from the left by @xmath114 we find @xmath122 a special structure ( see @xcite ) of the objects involved in @xmath123 allows to simplify significantly : @xmath124 with @xmath125 we rewrite the recursion for @xmath126 s to @xmath127 in proposition 4.1 in @xcite , it is shown that for @xmath4 large enough the magnitudes of all eigenvalues of @xmath128 are strictly less than 1 .",
    "we shall use this property for asymptotic analysis and continuous time approximation .      denote the bias and variance of the normalized error @xmath126 : @xmath129 taking the expectation from both side of we find @xmath130 from and , we get @xmath131 so that @xmath132 is defined by the recursion @xmath133 since @xmath134 it is natural to choose @xmath8 with @xmath135 and substitute @xmath136 by @xmath137 henceforth , @xmath138 is defined by recursion ( [ 3.7 ] ) with such @xmath8 , i.e. @xmath139 where @xmath140 .",
    "thus , @xmath141 determines the normalized mean square tracking error @xmath142      to find `` @xmath143 '' in , we give a continuous time approximation of @xmath144 . to this end , let us introduce the time stretching @xmath145 with @xmath146 .",
    "the boundary layer @xmath147 $ ] is transformed to @xmath148 $ ] and the interval @xmath48 $ ] to @xmath149 $ ] .",
    "let us define @xmath150 and @xmath151 , @xmath152 and for @xmath153 @xmath154 we shall consider these recursions for @xmath155 from @xmath156 , $ ] where @xmath157 have entries bounded in @xmath4 ( see , @xcite ) .",
    "taking into account that recursions ( [ 3.15 ] ) are homogeneous in @xmath158 , let us replace @xmath159 and @xmath160 by @xmath161 and @xmath162 .",
    "then , the entries of @xmath163 are bounded in @xmath4 for @xmath164 . therefore without loss of generality we may consider ( [ 3.15 ] ) with initial conditions bounded in @xmath4 : @xmath165 to determine the right hand side of ( [ 3.17 ] ) , we apply the arzela - ascoli theorem . for any @xmath166 , the functions @xmath167 are uniformly bounded and equicontinuous .",
    "so , by the arzela - ascoli theorem , any converging subsequence @xmath168 obeys the limit @xmath169 in the local uniform topology : @xmath170 where @xmath171 since the eigenvalues of @xmath172 have negative real parts , the limits @xmath173 and @xmath174 exist and are defined as : @xmath175 that is @xmath63 and @xmath64 are independent of @xmath176 and so @xmath177",
    "for large values of @xmath9 a direct minimization of @xmath178 from would be a difficult problem . moreover",
    ", @xmath179 could not a priori guarantee negative real parts of eigenvalues for @xmath180 . to avoid implementation of a conditional minimization procedure , we propose to choose @xmath35 from some limited class given below .",
    "our estimator has a structure of kalman filter in the discrete time .",
    "we assume that @xmath181 is a random vector , @xmath182 , and @xmath183 is generated by stochastic recursion @xmath184 where @xmath185 is a white noise , independent of @xmath5 , with @xmath186 , @xmath187 and @xmath54 is an arbitrary nonzero parameter .",
    "for the observation model @xmath188 we apply the estimator given in ( [ vmf ] ) .",
    "the resulting errors @xmath189 , @xmath190 are defined by a recursion @xmath191 then , for @xmath192 we obtain @xmath193 and supply @xmath194 .",
    "denote @xmath195 . from ( [ 3.411 ] ) it follows @xmath196 similar to @xmath197 ( see previous section ) , let us introduce @xmath198 : @xmath199 applying the arzela - ascoli theorem technique it can be readily shown that @xmath200 converges in the local uniform topology to @xmath201 , where @xmath202 and @xmath203 with @xmath204 being the unique solution of lyapunov equation @xmath205 the matrix @xmath204 is a function of arguments @xmath35 and @xmath54 : @xmath206 .",
    "we choose @xmath207 so that for any @xmath54 @xmath208 due to the kalman filtering theory , the lower bound holds true for @xmath209 with @xmath71 being solution of the algebraic riccati equation @xmath210 it is well known ( see e.g. theorem 16.2 in @xcite ) that possesses a unique positive - definite solution provided that block - matrices @xmath211 have full ranks @xmath212 .",
    "notice that @xmath213 is a unite matrix and the rank of @xmath214 is @xmath215 .",
    "consequently , the eigenvalues of the matrix @xmath216 with @xmath55 defined in ( [ qg ] ) have negative real parts ( see , lemma 16.11 in @xcite ) .",
    "the one parameter family @xmath70 permits a simple numeric implementation and guarantees filtering stability mentioned above . in this class",
    "we use a constrain parameter @xmath75 to compensate unacceptably large boundary layer when the minimization procedure yields small values of @xmath217 with @xmath56 given in .",
    "the magnitude of @xmath75 is dictated by @xmath9 , @xmath218 , initial conditions and boundary layer specifications .",
    "the minimization in our class provides @xmath219",
    "in this section we describe a structure of @xmath74 . recall that @xmath220 and @xmath71 solves the riccati equation for any fixed @xmath218 . for notational convenience replace @xmath71 by @xmath221 and set @xmath222 clearly , @xmath82 solves the algebraic riccati equation @xmath223 kalachev @xcite shows that @xmath224 where @xmath225 and @xmath80 are entries of @xmath221 and @xmath82 respectively . hence , @xmath226 for @xmath57 , these values are given in the table below .",
    "@xmath227    the complex structure of @xmath56 does not provide an insight of @xmath54 and @xmath13 connection .",
    "numerical simulations show that for a wide range of values @xmath228 is almost proportional to @xmath229 ( see also figure [ fig1 ] ) .",
    "this remark enables to construct a simple interpolation tables for the values of @xmath230 and @xmath231 with respect to the parameter @xmath13 .",
    "although the eigenvalues of @xmath55 have negative real parts , we may not formally guarantee that they are different .",
    "so , for @xmath57 we give the eigenvalues : @xmath233 for @xmath58 , the fact that the polynomial has different roots should be verified .",
    "notice that @xmath78 is a natural control parameter defining the size of boundary layer and should be limited from below by @xmath234 with appropriate @xmath75 .     in logarithmic scale for various @xmath13 and @xmath218 ; @xmath101 . ]",
    "the index form of @xmath235 i.e. @xmath236 allows to find the solution @xmath237 thus , @xmath238 .",
    "$ ] from it follows @xmath239 .    as a result ,",
    "the final expression for the cost function is @xmath77\\bigg).\\ ] ]",
    "in this paper , we use the fact that a class of kalman filters , being adapted to a nonparametric statistic setting , provides the optimal rate of convergence in sample size ( @xmath23 ) .",
    "we show how to evaluate a normalized risk function for large sample size and minimize that value in some subclass of kalman filters with constant filter gain .",
    "the kalman type estimator , as any on - line estimator , has inevitable boundary layer .",
    "we suggest to reduce the boundary layer by interpolation procedure and limitation from below for filtering gain ."
  ],
  "abstract_text": [
    "<S> we construct an on - line estimator with equidistant design for tracking a smooth function from stone - ibragimov - khasminskii class . </S>",
    "<S> this estimator has the optimal convergence rate of risk to zero in sample size . </S>",
    "<S> the procedure for setting coefficients of the estimator is controlled by a single parameter and has a simple numerical solution . </S>",
    "<S> the off - line version of this estimator allows to eliminate a boundary layer . </S>",
    "<S> simulation results are given . </S>"
  ]
}