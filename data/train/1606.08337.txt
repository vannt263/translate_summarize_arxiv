{
  "article_text": [
    "we are interested in bayesian modelling approaches to sparsity in variance and precision matrices in multivariate normal distributions . with interests in parsimony and scalability of analyses of multivariate data in models such as gaussian mixtures for classification , priors that encourage sparse component covariance patterns are increasingly key as dimension increases .",
    "new modelling frameworks also need to enable efficient computational methods for model fitting , which can otherwise be a barrier to application .    among recent related developments , traditional sparsity priors from model selection in regression",
    "have been exploited in sparse extensions of bayesian factor analysis  @xcite , and in complementary approaches using gaussian graphical models  @xcite .",
    "the developments in the current work represent natural extensions of the thinking behind these models building sparsity into variance or precision matrices while naturally linking and bridging between factor models and graphical models .    the new sparse givens ",
    "models introduced and developed here arise from new theory of random sparse eigenmatrices ; these define eigenstructure of variance and precision matrices , and so induce new classes of priors over gaussian graphical models .",
    "compared to factor analysis , we avoid the assumption of a reduced dimensional latent factor structure , and the choices it involves .",
    "our new models arise from an inherent theoretical feature of eigenmatrices , rather than hypothesized model structures .",
    "we also face fewer challenges in hyper - parameter specification and tuning to fit models .",
    "our models can in fact be viewed as full - rank factor models with sparse , square factor loadings matrices .",
    "additional related work has explored new classes of priors over variance matrices through varying parametrizations , such as partial correlations or cholesky decompositions  @xcite , that could be extended with sparsity priors . some such extensions to time series contexts  @xcite show the utility of various cholesky - style approaches .",
    "our approach relates to this general literature in that it uses an inherent theoretical property of eigenmatrices that naturally defines the reparametrization as well as an underlying set of parameters that , when set to zero , define parsimonious models .",
    "section  [ sec : eigtheory ] introduces the theoretical and modelling ideas ; the approach is based on the givens rotation representation of full - rank eigenmatrices  @xcite .",
    "we describe how this can be exploited to define new classes of random sparse eigenmatrices , and relate these to decomposable graphical models .",
    "section  [ sec : priors ] considers prior specification over variance matrices using this new parametrization , in the context of normal random samples .",
    "section  [ sec : explore ] discusses properties of the likelihood and aspects of exploratory data analysis that give insights into sparsity structure of eigenmatrices in our framework , with an example using a @xmath0dimensional gene expression data set .",
    "section  [ sec : mcmc ] discusses full bayesian model fitting using a customized reversible jump markov chain monte carlo approach .",
    "we make a detailed , simulation - based comparison with traditional gaussian graphical modelling ( ggm ) in  [ sec : simstudy ] .",
    "section  [ sec : mixtures ] discusses embedding the basic model into more practicable contexts involving measurement errors and normal mixture models .",
    "that section concludes with a detailed example using breast cancer gene expression data , where underlying components relate to known , broad and intersecting cancer subtypes with expected sparsity in dependence , and conditional dependence patterns of subsets of the genes .",
    "section  [ sec : end ] concludes with additional comments and potential extensions .",
    "we discuss givens representations of variance matrices , introduce the general idea of sparsity modelling in this context , and explore aspects of the theoretical structures that emerge under priors over the resulting models .",
    "consider a random @xmath1vector @xmath2 with variance matrix @xmath3 the spectral representation ( principal component decomposition ) is @xmath4 where @xmath5 is the @xmath6 orthogonal matrix of eigenvectors the eigenmatrix and @xmath7 is the matrix of non - negative eigenvalues .",
    "the corresponding precision matrix is @xmath8 with @xmath9 the general givens rotator product representation of @xmath5  @xcite is @xmath10 where @xmath11 is diagonal with elements @xmath12 and each @xmath13 is a givens rotation matrix @xmath14 for some rotator angles @xmath15 , ( @xmath16 some comments and notation follow .    *",
    "the angles @xmath17 lie in @xmath18.$ ] write @xmath19 for the set of these @xmath20 angles .",
    "* this decomposition of @xmath5 into @xmath21 angles is unique and linked to the specific order of the variables in @xmath22 * for our goal of covariance modelling , note that @xmath11 cancels in @xmath23 ; hence , @xmath11 plays no role and we set @xmath24 with no loss when focused on modelling variance matrices via this decomposition .",
    "* covariance patterns in @xmath25 can be viewed as successively built - up by pairwise rotations of initial uncorrelated random variables .",
    "take a @xmath1vector @xmath26 with @xmath27 then dependencies are defined by successive left multiplication of @xmath26 by the rotator matrices : first by @xmath28 then @xmath29 and so on up to @xmath30 to define @xmath31 ( assuming @xmath24 as noted ) . *",
    "if @xmath32 for any @xmath33 then @xmath34 and that rotation has no contribution to the build - up of dependencies and is effectively removed from the representation of . * if @xmath35 for any @xmath33 then @xmath36 permutes rows @xmath37 and @xmath38 , and columns @xmath37 and @xmath38 , of any square matrix @xmath39 and hence does not affect the sparsity of @xmath40 * the spectral representations of @xmath25 and @xmath41 are unique only up to permutations of the columns @xmath42 i.e. , reordering of the eigenvalues .",
    "any reordering of the eigenvalues will generate a decomposition as in but with different values of the rotator angles . for identification , therefore , we will constrain to @xmath43 for variance matrices in models of data distributions , the @xmath44 will be distinct so a strict ordering can be assumed .",
    "the general representation above reparametrizes @xmath25 to the @xmath20 angles in @xmath19 and the @xmath45 eigenvalues in @xmath46 .",
    "we note above the role of zero angles , and this opens the path to defining sparse givens models , i.e. , products of fewer than the full set of rotators defining a resulting sparse eigenmatrix : if a large number of the angles are zero , then @xmath5 will become sparse .",
    "this can induce a sparse variance matrix @xmath25 and , equivalently , a sparse precision matrix @xmath41 as a result .",
    "let @xmath47 with @xmath48 . then is compactly written as @xmath49 where @xmath50 is the @xmath51 pair of dimensions in @xmath52",
    ". now allow exact zeros in @xmath53 define a sparsity defining index sequence @xmath54 with cardinality @xmath55 , and set @xmath56 with size @xmath57 . in words",
    ", @xmath58 is a sequence of @xmath59 ordered pairs @xmath60 denoting the relevant , non - identity givens rotation matrices in and @xmath61 assuming that priors support exact zeros in @xmath62 a primary modelling goal is then to learn @xmath58 and the corresponding non - zero angles .    among the features of this approach",
    "is that we are able to model full - rank , orthogonal matrices with a parsimonious set of angles , and we maintain the computational convenience of the full - rank spectral parametrization when inverting @xmath25 and @xmath63 this is especially useful in evaluating density functions in metropolis hastings acceptance ratios and , later , in computing normal mixture classification probabilities .",
    "the process of successively building dependencies by adding rotators ( from right to left ) in induces ties between the variables whose variance matrix is the resulting @xmath64 the resulting structure of @xmath65 connects to gaussian graphical modelling  @xcite .",
    "view the @xmath45 variables in @xmath2 as nodes of a graph in which conditional independencies are represented by lack of edges between node pairs .",
    "specifically , this is the undirected graph @xmath66 with the @xmath45 nodes , or vertices , in the vertex set @xmath67 ; two vertices @xmath68 are connected by an edge in the graph if , and only if , @xmath69 where @xmath70 is the @xmath71element of @xmath63 the edge set is @xmath72    any precision matrix @xmath73 having some off - diagonal zero elements has an implied graph @xmath74 now take @xmath75 where @xmath76 and @xmath77 with implied graph @xmath78 .",
    "notice that left multiplication of @xmath79 by @xmath80 simply replaces the @xmath81 and @xmath82 rows of with a linear combination of the two .",
    "therefore , the indices of the non - zero elements of the @xmath81 and @xmath82 rows of @xmath83 are the union of the indices of the @xmath81 and @xmath82 rows of @xmath79",
    ". similar comments apply to right multiplication . as a result",
    ", the sparsity pattern of @xmath84 is the same as that of @xmath73 except in rows and columns @xmath37 and @xmath38 .",
    "specifically , those rows and columns have sparsity indices that are the unions of the those in @xmath85 this shows that the additional rotator @xmath80 maps the graph @xmath86 to @xmath87 as follows . with @xmath88 , then @xmath89 . in words , @xmath78 takes @xmath90 , connects @xmath37 and @xmath38 , and unions their neighborhoods .",
    "this structure also generates constructive insights into the nature of the graphical models so defined .",
    "it shows that adding a new rotator to an existing sparse givens model merges the complete subgraphs ( cliques ) in which the rotators pair @xmath68 reside into one larger clique .",
    "starting at an empty graph , this leads to graphs that are decomposable , formally shown as follows .",
    "[ th : decomp ] the conditional independence graph @xmath91 implied by a sparse @xmath8 under @xmath58 is a decomposable graph .",
    "it is enough to show that @xmath91 has a perfect elimination ordering ; that is , an ordering of the vertices of the graph such that , for each vertex @xmath92 , the neighbors of @xmath93 that occur after @xmath93 in the order form a clique  @xcite .",
    "we do this by induction , beginning with no rotations : @xmath94 and @xmath95 this implies that @xmath90 is the empty graph and the perfect elimination ordering is trivial . for the inductive step , assume that an ordering exists for the graph implied by a current set of givens rotations defining @xmath79 and @xmath74 now take @xmath75 where @xmath76 and @xmath77 with implied graph @xmath78 .",
    "note that there is no loss of generality here ; @xmath35 would imply simply swapping the @xmath37 and @xmath38 rows and columns of @xmath73 to make @xmath96 and so always yields another decomposable graph .",
    "it is now enough to show that @xmath78 has a perfect elimination ordering .",
    "start with the ordering of @xmath91 given by @xmath97 now take the ordering for @xmath78 to be @xmath98 .",
    "it is enough to show that @xmath99 is a perfect elimination ordering for @xmath78 .",
    "take @xmath100 and let @xmath101 be @xmath102 and its neighbors that occur after @xmath102 in @xmath99 .",
    "we need to show that @xmath101 forms a clique in @xmath78 . if @xmath103 , this is trivial .",
    "if @xmath102 is not a neighbor of either @xmath104 or @xmath105 in @xmath90 , the rotation has no effect on the neighborhood of @xmath102 and @xmath101 is a clique .",
    "now suppose that @xmath102 is a neighbor of @xmath104 in @xmath90 .",
    "due to our construction of @xmath78 , the neighbors of @xmath102 in @xmath90 become neighbors of @xmath104 in @xmath106 since @xmath107 is a clique in @xmath90 , then @xmath101 will remain a clique in @xmath78 . since @xmath104 and @xmath105 were moved to the end of the ordering and @xmath107",
    "comes after @xmath102 in @xmath99 by the inductive hypothesis , then @xmath99 is a perfect elimination ordering of @xmath78",
    ".    note that the above concerns general , unrestricted values of the non - zero angles .",
    "furthermore , this applies to any ordering of the rotators where is a special case .",
    "there are sparse precision matrices whose graphs are decomposable but that do not have a sparse givens representations for their eigenmatrices .",
    "these arise , in particular , in parametric models where the variance and precision matrix are initially defined as functions of lower dimensional parameters to begin ; in such cases , the resulting eigenmatrices are inherently structured and typically not sparse , even though the precision matrices are sparse .",
    "the simplest example is that of the dependence structure for a set of @xmath45 consecutive values of a stationary , linear , gaussian first - order autoregressive process .",
    "there @xmath41 is tri - diagonal , and neither @xmath5 nor @xmath25 is sparse . while @xmath5 has the givens representation , all @xmath21 angles are required and they are deterministically related .    in the next section",
    "we define priors for the rotator angles @xmath53 this includes conditional priors for the effective angles excluding values of @xmath108 and @xmath109 under which these angle are a random sample from a continuous distribution . in such cases , which can be regarded as all practicable cases for applied data analysis",
    ", we find a surprising connection between sparse graphical models and sparse factor models ; that is , they coincide in this new sparse givens approach .",
    "[ th : covsparse ] if the angles @xmath110 defining a sparse eigenmatrix are a random sample from a continuous distribution , then the resulting patterns of zeros in @xmath25 and @xmath65 are the same with probability one .    for any @xmath68 pair , @xmath111 therefore ,",
    "zero values of @xmath112 and @xmath70 follow when @xmath113 however , any other case giving @xmath114 requires specific values of @xmath46 , and/or specific relationships among elements of @xmath25 and @xmath46 defining the deterministic constraint that the above sum be zero .",
    "such a constraint will not yield @xmath115 under a continuous prior over the angles .",
    "we overlay the theoretical framework above with priors that define interesting theoretical models of random variance matrices as well as the specifications necessary for bayesian analysis .",
    "we specify priors that give positive probability to zero values among the angles , allowing row and column flips via angles of @xmath109 , and that otherwise draw angles independently from a continuous distribution .",
    "specifically , the @xmath21 angles @xmath15 are a random sample from a distribution with density @xmath116 where @xmath117 is the indicator function and @xmath118 a continuous density on @xmath119    since @xmath35 does not effect the sparsity of @xmath120 and is needed for permuting the effects of the eigenvalues as discussed earlier , we do not want to penalize permutations in the same way as other non - zero angles .",
    "we specify the prior in three stages . first with probability @xmath121 @xmath35 to complement the constraint on eigenvalues @xmath44 being ordered .",
    "then , for angles that do not induce a permutation , we allow zero values with a non - zero conditional probability @xmath122 finally , conditional on @xmath123 or @xmath124 it follows a continuous prior @xmath125    there are various choices of the continuous prior component @xmath126 .",
    "our examples here use a specific form that seems relevant for use as a routine , namely @xmath127 where @xmath128 and @xmath129 is a normalizing constant . in bayesian analyses via reversible jump mcmc methods we need the value of @xmath130 and it can be easily evaluated using any standard numerical integration technique .",
    "this prior is unimodal and symmetric about zero , so represents appropriate centering relative to the null hypothesis \" value at zero . the prior concentrates more around zero for larger values of @xmath131 while @xmath132 leads to the limiting uniform distribution on @xmath119 the specific mathematical form is also suggested by the forms of conditional likelihood functions for angles in normal models , as noted below in section  [ sec : explore ] .    the prior is completed by specifying a distribution for the eigenvalues @xmath133 of @xmath64 as discussed above , we take them ordered as @xmath134 the natural , conditionally conjugate class of priors takes the @xmath135 as ordered values of @xmath45 independent draws from an inverse gamma distribution : given some chosen hyperparameters @xmath136 draw @xmath137 independently then impose the ordering .",
    "a specified prior over @xmath138 leads to the implied prior over @xmath25 and @xmath139 and within that a prior over the sparsity structure that relates to the random graphical model induced .",
    "simulation of @xmath140 yields simulations from the latter .",
    "one aspect of interest is to understand how sparsity in @xmath5 is related to the number of rotators .",
    "a follow - on question is how these then relate to sparsity in @xmath41 and hence the sparsity of the implied graph .",
    "this is trivially explored by simulation and then simply counting the number of zeros in @xmath5 and @xmath41 . for",
    "a given set of rotator pairs @xmath58 with @xmath141 randomly pick which rotations will be non - zero then sample their angles uniformly and generate @xmath5 and @xmath41 .",
    "we repeat this process 10,000 times for each @xmath142 . for each dimension",
    "@xmath143 , figure  [ fig : sparsity ] shows the median proportion of zeros in @xmath5 and @xmath41 as the proportion of non - zero rotators increases . note how quickly the sparsity of @xmath139 defining the sparsity of the underlying graph , decreases relative to @xmath5 .",
    "this gives some insights into how the choice of the prior sparsity probability @xmath144 plays a role in generating sparse graphs .",
    "[ cols= \" > , < \" , ]     finally , we see that , appropriately , the low probability component @xmath145 has really no structure at all , consistent with prior for a basically empty component .",
    "these conditional dependencies , and independencies , are better understood in terms of the estimated factor structure underlying eigenmatrices and eigenvalues of the sparse givens models in each component ; these are shown in the left column of figure  [ fig : postemean ] for the three main components .",
    "for the high er `` tumors in component @xmath146 we see one dominant and two subsidiary eigenvectors , indicating three er - related factors '' based on non - zero loadings of the er - related genes ; these presumably reflect several dimensions of the underlying patterns of variability in these genes as a result of the complexity of the er network .",
    "the second dominant eigenvector relates to the her2 cluster . for the high her2 \" tumors in @xmath147",
    "we see the dominant factor is indeed linked to the her2 gene cluster , while the fact that er related genes vary across the scale in these tumors leads to a natural set of three or four er - related factors . for the triple negative / basal - like tumors in component",
    "@xmath148 we see residual biological pathway activity highlighted involving hnf-3@xmath149 and c - myb genes , as well as important factors in both er and her2 pathways ; although these two pathways are less active in tumors in this group , there is still meaningful variation among subsets of some of these genes",
    ".    for comparison , the right column in figure  [ fig : postemean ] shows the corresponding eigenstructure extracted from an analysis using traditional inverse wishart priors on the @xmath150 , i.e. , in the standard analysis with no sparsity .",
    "it is very clear how the sharp factor - based groupings in the sparse givens mixture model cleans - up \" the much noisier standard results .",
    "in addition to cleaner and focused inference on dependency structures , we also found that the standard analysis by comparison with the sparse model generates over - diffuse estimates of the spread of mixture components and so less sharp classification of samples , as a result .",
    "in terms of modelling variations and extensions , one interesting question relates to the interpretation of the sparse givens model as factor analysis .",
    "our examples have stressed this interpretation from an applied viewpoint . theoretically , the givens model is a full - rank , orthogonal factor analysis model .",
    "we can imagine extensions to include reduced rank approximations that would be based on the use of priors giving positive probability to zero values among the @xmath151 relating more directly to alternative factor modelling frameworks  @xcite .",
    "we have experience in running the mcmc analysis for higher - dimensional variance matrices , including extensions of the gene expression examples with @xmath152 genes .",
    "the overall performance of the mcmc is scalable , in terms of acceptance rates , while of course the running time and implementation overheads increase . in particular , as the number of rotators grows , a number of computational challenges arise .",
    "first , the numerical optimization to define metropolis proposals @xmath153 becomes increasingly time consuming , so that one immediate area of research will be to explore more computationally efficient proposal strategies for the mcmc .",
    "second , based on our positive experience with the exploratory analysis to define ad - hoc starting values for increasingly high - dimensional problems , one direction for improving the mcmc would be to consider alternatives to the birth / death strategy based on more aggressive local search in neighborhoods of good \" sparsity configurations .",
    "some of the concepts and computational strategies underlying shotgun stochastic search in regression and graphical models  @xcite may be of real benefit here .",
    "the potential for distributed computation , including using gpu hardware  @xcite is also of interest .",
    "as noted in the text , code implementing the analyses reported here is ( freely ) available to interested readers at the authors web site .",
    "additional technical details of the mcmc algorithm in section  [ sec : mixtures ] are given here .",
    "\\(a ) _ starting values : _ we use @xmath154means clustering to define initial , crude classification of the data into @xmath155 groups , giving starting values for component indicators @xmath156 group means and proportions define starting values @xmath157 and @xmath158 initial values for the givens structures within each group are then created using the exploratory algorithm of section  [ sec : explore ] . beginning with the sample variance matrix of each group",
    "@xmath159 this algorithm produces a sparse givens structure with starting values for the rotator pairs , angles and eigenvalues , and hence @xmath160 , @xmath161 and @xmath162 the measurement error variances @xmath163 in @xmath164 are initialized at draws from the prior .",
    "\\(b ) _ rotator structure and angle updates : _ for each cluster @xmath165 defined at the current iterate of the mcmc , we update the rotators selected and corresponding angles using the rj - mcmc analysis of section  [ sec : rjmcmc ] .",
    "\\(c ) _ latent data @xmath166 : _ each @xmath167 is resampled from the complete conditional normal posterior whose mean vector @xmath168 and variance matrix @xmath169 are given by @xmath170 note that the @xmath171 can be calculated trivially even in high dimensional cases simply by inverting the eigenvalues .",
    "\\(d ) _ measurement error variances @xmath163 : _ each of the @xmath45 elements of the diagonal matrix @xmath164 is resampled from a complete conditional given by @xmath172 for @xmath173 where @xmath174 is the @xmath175the element of @xmath176    \\(e ) _ component indicators @xmath177 : _ the set of @xmath178 component classification indicators @xmath179 are drawn from conditionally independent multinomials , each with sample size 1 and probabilities over the @xmath155 cells defined by @xmath180 where @xmath181 denotes the multivariate normal pdf .",
    "\\(g ) _ component means @xmath157 : _ denote by @xmath186 the sample mean in group @xmath187 given a current set of component indicators",
    ". then the component means are sampled in parallel from the @xmath155 conditional normal posteriors with means @xmath188 and variance matrices @xmath189",
    "@xmath190    \\(h ) _ eigenvectors @xmath191 _ finally , the complete conditional distributions of the diagonal elements of @xmath192 are independent inverse gammas constrained by the ordering ; see , applied to each of the @xmath155 groups in parallel .",
    "these are sampled in sequence using the inverse cdf method ."
  ],
  "abstract_text": [
    "<S> we discuss probabilistic models of random covariance structures defined by distributions over sparse eigenmatrices . </S>",
    "<S> the decomposition of orthogonal matrices in terms of givens rotations defines a natural , interpretable framework for defining distributions on sparsity structure of random eigenmatrices . </S>",
    "<S> we explore theoretical aspects and implications for conditional independence structures arising in multivariate gaussian models , and discuss connections with sparse pca , factor analysis and gaussian graphical models . </S>",
    "<S> methodology includes model - based exploratory data analysis and bayesian analysis via reversible jump markov chain monte carlo . </S>",
    "<S> a simulation study examines the ability to identify sparse multivariate structures compared to the benchmark graphical modelling approach . </S>",
    "<S> extensions to multivariate normal mixture models with additional measurement errors move into the framework of latent structure analysis of broad practical interest . </S>",
    "<S> we explore the implications and utility of the new models with summaries of a detailed applied study of a @xmath0dimensional breast cancer genomics data set .    _ key words and phrases : _ bayesian sparsity models ; givens rotations ; graphical models ; mixtures of sparse factor analyzers ; mixtures of graphical models ; random orthogonal matrix ; sparse eigenmatrices ; sparse variance matrix ; sparse factor analysis ; sparse precision matrix </S>"
  ]
}