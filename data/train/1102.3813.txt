{
  "article_text": [
    "a _ hypergraph _ @xmath0 is a subset family defined on a vertex set @xmath1 , that is , each element ( called _ hyperedge _ ) @xmath5 of @xmath6 is a subset of @xmath1 .",
    "the hypergraph is a generalization of a graph so that edges can have more than two vertices .",
    "a _ hitting set _ is a subset @xmath2 of @xmath1 such that @xmath7 for any hyperedge @xmath8 .",
    "a hitting set is called _ minimal _ if it includes no other hitting set .",
    "the _ dual _ of a hypergraph is the set of all minimal hitting sets .",
    "the _ dualization _ of a hypergraph is to construct the dual of a given hypergraph .",
    "dualization is a fundamental problem in computer science , especially in machine learning , data mining , and optimization , etc .",
    "it is equivalent to ( 1 ) the minimal hitting set enumeration of given subset family , ( 2 ) minimal set cover enumeration of given set family , ( 3 ) enumeration of hypergraph transversal , ( 4 ) enumeration of minimal subsets that are not included in any of the given set family , etc .",
    "one of the research goals is to clarify the existence of a polynomial time algorithm for solving the problem .",
    "the size of dual can be exponential in the input hypergraph , thus the polynomial time algorithm for dualization usually means an algorithm running in time polynomial to the input size and the output size . although kachian et al.@xcite developed a quasi - polynomial time algorithm which runs in @xmath9 time , where @xmath10 is the input size plus output size",
    ", the existence of a polynomial time algorithm is still an open question .    from the importance of dualization in its application areas",
    ", a lot of research has aimed at algorithms that terminate in a short time on real world data .",
    "the size of the dual can be exponential , but in practice , it is huge but not intractable .",
    "thus , practically efficient algorithms aim to take a short time for each minimal hitting set .",
    "reduction of the search space was studied as a way to cope with this problem @xcite .",
    "finding a minimal hitting set is easy ; one removes vertices one by one unless each has an empty intersection with some hyperedges .",
    "however , finding exactly all minimal hitting sets is not easy ; we have to check a great many vertex subsets that can be minimal hitting sets .",
    "the past studies have succeeded in reducing the search space , but the computational cost was substantial , hence the current algorithms may take a long time when the size of the dual is large .    in this paper , we focus on developing an efficient computation for the case of large - scale input data with a large number of minimal hitting sets .",
    "we looked at the disadvantages of the existing methods and devised new algorithms to eliminate them .    * _ breadth - first search : _ a popular search method for dualization is hill climbing such that the algorithm starts from the emptyset , and recursively adds vertices one by one until it reaches minimal hitting sets .",
    "the minimal hitting sets already found are stored in memory and used to check the minimality .",
    "this minimality check is popular , but its memory usage is so inefficient so that we can not solve a problem with many minimal hitting sets .",
    "we alleviate this disadvantage by using a depth - first search algorithm with the use of the new minimality check algorithm explained below .",
    "the algorithm proposed in @xcite uses a depth - first search , but its minimality check takes a long time on large hypergraphs . * _ minimality check : _ the time for the minimality check in a breadth - first search is short when the hitting sets to be checked are small on average , but will be long for larger hitting sets ( such as size 20 or larger ) .",
    "we alleviate this disadvantage by using a new algorithm that does not need the hitting sets that have already been found .",
    "we introduce a new concept , called the _ critical hyperedge _ , that characterizes the minimality of hitting sets .",
    "computing and updating critical hyperedges can be done in a short time , thus we can efficiently check the minimality in a short time . * _ pruning : _ several algorithms use pruning methods to reduce the search space , but our experiments show that these pruning methods are not sufficient .",
    "we propose a simple but efficient pruning method .",
    "we introduce a lexicographic depth - first search , and thereby remove vertices that can never be used and prune branches without necessary vertices .",
    "the pruning drastically reduces the computation time . * _ sophisticated use of simple data structures : _ not many studies have mentioned the data structures or how to use them efficiently , despite this being a very important consideration to reduce the computation time .",
    "we use both the adjacency matrix ( characteristic vectors of hyperedges ) and doubly linked lists to speed up the operations of taking intersections and set differences .",
    "this accelerates the computation time in extremely sparse , extremely dense ( use complement as input ) , non - small minimal hitting sets ( over 10 vertices ) cases .",
    "the paper is organized as follows . in the following subsections ,",
    "we explain the related work and related problems .",
    "section 2 is for preliminaries , and section 3 describes the existing algorithms .",
    "we describe our new algorithms in section 4 and show the results of computational experiments in section 5 .",
    "we conclude the paper in section 6 .",
    "there have been several studies on the dualization problem , of which we shall briefly review the dl , bmr , ks and hbc algorithms .",
    "these algorithms are classified into two types according to their structure ; improved versions of the berge algorithm@xcite , and hill - climbing algorithms .",
    "the berge algorithm updates the set of minimal hitting sets iteratively , by adding hyperedges one by one to the current partial hypergraph .",
    "dl , bmr and ks are the algorithms of this type , and hbc is the hill - climbing type .",
    "the candidates for minimal hitting sets are generated by gathering vertices one - by - one until a minimality condition is violated .",
    "when a candidate becomes a hitting set , it is a minimal hitting set .",
    "the hbc algorithm does this operation in a breadth - first manner .",
    "the dl algorithm , proposed by dong and li @xcite , is a border - differential algorithm for data mining .",
    "the main difference from the berge algorithm is that it avoids generating non - minimal hitting sets by increasing the problem size incrementally .",
    "the dl algorithm starts from an empty hypergraph and adds a hyperedge iteratively while updating the set of minimal hitting sets .",
    "the sizes of the intermediate sets of minimal hitting sets are likely smaller than that of the original hypergraph , thus we can expect that there will be no combinatorial explosion .",
    "experiments on two small uci datasets @xcite have shown that the dl algorithm is much faster than their previous algorithm and the level - wise hill climbing algorithm .",
    "in general , the berge algorithm and dl algorithm are very useful when the hypergraph has few hyperedges , but for large hypergraphs , it may take a long time because of many updates . the bmr algorithm , proposed by bailey et al .",
    "@xcite , starts from a hypergraph with few vertices with hyperedges restricted to the vertex set ( the vertices not in the current vertex set are removed from the hyperedges ) .",
    "the hyperedges grow as the vertex set increases .",
    "the bmr algorithm first uses the berge algorithm to solve the problem of the initial hypergraph , and then it updates the minimal hitting sets .",
    "note that hagen tested a version of the dl algorithm instead of the berge algorithm @xcite .",
    "kavvadias and stavropoulos s algorithm ( ks algorithm ) @xcite embodies two ideas ; unifying the nodes contained in the same hyperedges and depth - first search .",
    "these ideas help to reduce the number of intermediate hitting sets and memory usage . to perform a depth - first search",
    ", they use a minimality check algorithm that does not need other hitting sets ; check whether the removal of each vertex results a hitting set or not .",
    "the ks algorithm uses an efficient algorithm for this task .",
    "hebert et al .",
    "proposed a level - wise algorithm ( hbc algorithm ) @xcite .",
    "their algorithm is a hill climbing algorithm which starts from the empty set and adds vertices one by one .",
    "it searches the vertex subsets satisfying a necessary condition to be a minimal hitting set , called a `` galois connection '' .",
    "a vertex subset satisfies the galois connection if the removal of any of its vertexes decreases the number of hyperedges intersecting with it .",
    "the sets satisfying the galois connection form a set system satisfying the monotone property ( independent set system ) , thus we can perform a breadth - first search in the usual way .",
    "dualization has many equivalent problems .",
    "we show some of them below",
    ". + _ ( 1 ) minimal set cover enumeration _ + for a subset family @xmath6 defined on a set @xmath11 , a set cover @xmath12 is a subset of @xmath6 such that the union of the members of @xmath12 is equal to @xmath11 , i.e. , @xmath13 .",
    "a set cover is called minimal if it is included in no other set cover .",
    "we consider @xmath6 to be a vertex set , and @xmath14 to be a hyperedge where @xmath14 is the set of @xmath4 that include @xmath15 .",
    "then , for the hyperedge set ( set family ) @xmath16 , a hitting set of @xmath5 is a set cover of @xmath6 , and vice versa .",
    "thus , enumerating minimal set covers is equivalent to dualization .",
    "+ _ ( 2 ) minimal uncovered set enumeration _",
    "+ for a subset family @xmath6 defined on a set @xmath11 , an uncovered set @xmath12 is a subset of @xmath11 such that @xmath12 is not included in any member of @xmath6 .",
    "let @xmath17 be the complement of @xmath6 , which is the set of the complement of members in @xmath6 , i.e. , @xmath18 .",
    "@xmath12 is not included in @xmath19 if and only if @xmath12 and @xmath20 have a non - empty intersection .",
    "an uncovered set of @xmath6 is a hitting set of @xmath17 , and vice versa , thus the minimal uncovered set enumeration is equivalent to the minimal hitting set enumeration .",
    "+ _ ( 3 ) circuit enumeration for independent system _ + a subset family @xmath6 defined on @xmath11 is called an independent system if for each member @xmath21 of @xmath6 , any of its subsets is also a member of @xmath6 . a subset of @xmath11 is called independent if it is a member of @xmath6 , and dependent otherwise .",
    "a circuit is a minimal dependent set , i.e. , a dependent set which properly contains no other dependent set .",
    "when an independent system is given by the set of maximal independent sets of @xmath6 , then the enumeration of circuits of @xmath6 is equivalent to the enumeration of uncovered sets of @xmath6 . + _ ( 4 ) computing negative border from positive border _ + a function is called boolean if it maps subsets in @xmath22 to @xmath23 .",
    "a boolean function @xmath24 is called monotone ( resp .",
    ", anti - monotone ) if it for any set @xmath21 with @xmath25 ( resp . , @xmath26 ) , any subset @xmath27 of @xmath21 satisfies @xmath28 ( resp . , @xmath29 ) .",
    "for a monotone function @xmath24 , a subset @xmath21 is called a positive border if @xmath25 and no its proper superset @xmath30 satisfies @xmath31 , and is called a negative border if @xmath26 and no its proper subset @xmath30 satisfies @xmath32 . when we are given a boolean function by the set of positive borders ,",
    "the problem is to enumerate all of its negative borders .",
    "this problem is equivalent to dualization , since the problem is equivalent to uncovered set enumeration .",
    "+ _ ( 5 ) dnf to cns transformation _ + dnf is a formula whose clauses are composed of literals connected by `` or '' and whose clauses are connected by `` and '' .",
    "cnf is a formula whose clauses are composed of literals connected by `` and '' , and whose clauses are connected by `` or '' . any formula can be represented as a dnf formula and a cnf formula .",
    "let @xmath33 be a dnf formula composed of variables @xmath34 and clauses @xmath35 .",
    "a dnf / cnf is called monotone if no clause contains a literal with `` not '' .",
    "then , @xmath12 is a hitting set of the clauses of @xmath33 if and only if the assignment obtained by setting the literals in @xmath12 to true gives a true assignment of @xmath33 .",
    "let @xmath2 be a minimal cnf formula equivalent to @xmath33 .",
    "@xmath2 has to include any minimal hitting set of @xmath33 as its clause , since any clause of @xmath2 has to contain at least one literal of any clause of @xmath33 .",
    "thus , a minimal cnf equivalent to @xmath33 has to include all minimal hitting sets of @xmath33 .",
    "for the same reason , computing the minimal dnf from a cnf is equivalent to dualization .",
    "a _ hypergraph _ @xmath0 is a subset family @xmath36 defined on a vertex set @xmath1 , that is , each element ( called _ hyperedge _ ) @xmath5 of @xmath6 is a subset of @xmath1 .",
    "the hypergraph is a generalization of a graph so that edges can contain more than two vertices .",
    "a subset @xmath2 of @xmath1 is called a _",
    "vertex subset_. a _ hitting set _ is a vertex subset @xmath2 such that @xmath7 for any hyperedge @xmath8 . a hitting set is called _ minimal _ if it includes no other hitting set .",
    "the _ dual _ of a hypergraph is the hypergraph whose hyperedge set is the set of all minimal hitting sets , and it is denoted by @xmath37 .",
    "for example , when @xmath38 @xmath39 is a hitting set but not minimal , and @xmath40 is a minimal hitting set .",
    "@xmath37 is @xmath41 .",
    "it is known that @xmath42 if no @xmath43 satisfy @xmath44 .",
    "the _ dualization _ of a hypergraph is to construct the dual of the given hypergraph .",
    "@xmath45 denotes the number of hyperedges in @xmath6 , that is @xmath46 , and @xmath47 denotes the sum of the sizes of hyperedges in @xmath6 , respectively .",
    "in particular , @xmath47 is called the _ size _ of @xmath6 .",
    "@xmath48 denotes the hypergraph composed of hyperedges @xmath49 . for @xmath50 ,",
    "let @xmath14 be the set of hyperedges in @xmath6 that includes @xmath15 , i.e. , @xmath51 . for vertex subset @xmath12 and vertex @xmath15 ,",
    "we respectively denote @xmath52 and @xmath53 by @xmath54 and @xmath55 .",
    "we introduce the new concept _ critical hyperedge _ in the following . for a vertex subset @xmath56",
    ", @xmath57 denotes the set of hyperedges that do not intersect with @xmath12 , i.e. , @xmath58 .",
    "@xmath12 is a hitting set if and only if @xmath59 . for a vertex @xmath60",
    ", a hyperedge @xmath4 is said to be _ critical _ for @xmath15 if @xmath61 .",
    "we denote the set of all critical hyperedges for @xmath15 by @xmath62 , i.e. , @xmath63 .",
    "suppose that @xmath12 is a hitting set .",
    "if @xmath15 has no critical hyperedge , every @xmath4 includes a vertex in @xmath12 other than @xmath15 , thus @xmath64 is also a hitting set .",
    "therefore , we have the following property .    [ crit ]",
    "@xmath12 is a minimal hitting set if and only if @xmath59 , and @xmath65 holds for any @xmath60 .",
    "if @xmath66 for any @xmath60 , we say that @xmath12 satisfies the _",
    "minimality condition_. our algorithm updates @xmath67 to check the minimality condition quickly , by utilizing the following lemmas .",
    "let us consider an example of @xmath67 .",
    "suppose that @xmath68 , and the hitting set @xmath12 is @xmath39 .",
    "we can see that @xmath69 , thus @xmath12 is not minimal , and we can remove either @xmath70 or @xmath71 . for @xmath72 , @xmath73 ,",
    "thus @xmath74 is a minimal hitting set .",
    "the following lemmas are the keys to our algorithms .    [ up ] for any vertex subset @xmath12 , @xmath60 and @xmath75 , @xmath76 .",
    "particularly , @xmath77 holds .    for any @xmath78",
    ", @xmath79 holds if @xmath5 is not in @xmath80 , and thus it is included in @xmath81 .",
    "conversely , @xmath79 holds for any @xmath82 .",
    "this means that @xmath83 , and @xmath84 .",
    "[ up2 ] for any vertex subset @xmath12 and @xmath75 , @xmath85 .",
    "since any hyperedge not in @xmath57 has a non - empty intersection with @xmath12 , @xmath5 can never be a critical hyperedge for @xmath86 .",
    "any critical hyperedge for @xmath86 includes @xmath86 thus , we can see that @xmath87 .",
    "conversely , for any hyperedge @xmath5 included in @xmath88 , @xmath89 , thereby @xmath90 .",
    "hence , the lemma holds .",
    "the next two lemmas follow directly from the above .",
    "[ mono]@xcite if a vertex subset @xmath12 satisfies the minimality condition , any of its subsets also satisfy the minimality condition , i.e. , the minimality condition satisfies the monotone property .",
    "[ min - cond]@xcite if a vertex subset @xmath12 does not satisfy the minimality condition , @xmath12 is not included in any minimal hitting set . in particular , any minimal hitting set @xmath12 is maximal in the set system composed of vertex subsets satisfying the minimality condition .",
    "[ crit - size ] for any vertex subset @xmath12 , @xmath91 .    from the definition of the critical hyperedge",
    ", any hyperedge @xmath4 can be critical for at most one vertex .",
    "thus , the lemma holds .",
    "this section is devoted to explaining the framework of the existing algorithms related to our algorithms : dl algorithm , ks algorithm , and hbc algorithm .",
    "the dl algorithm starts by computing @xmath92 and then iteratively computes @xmath93 from @xmath94 . for any @xmath95 , either @xmath96 holds , or @xmath97 holds for @xmath98 .",
    "note that when @xmath95 is not in @xmath94 , @xmath99 is composed of exactly one vertex , since @xmath62 must be @xmath100 .",
    "however , for any @xmath96 , @xmath95 if @xmath101 . when @xmath102 , @xmath54 with @xmath103 may be in @xmath93 .",
    "the algorithm is as follows .",
    "algorithm dl ( @xmath104 ) + 1 .",
    "@xmath105 + 2 . * for * @xmath106 * to * @xmath46 + 3 .",
    "@xmath107 + 4 .",
    "* for each * @xmath108 * do * + 5 .  * if * @xmath101 * then * insert @xmath12 to @xmath109 + 6 .  * else * * for each * @xmath103 * do * + 7 .  * if * no @xmath110 satisfies @xmath111 and @xmath112 * then * insert @xmath54 to @xmath109 + 8 .",
    "* end for * + 9 .  * end for * + 10 . *",
    "end for *    after the computation , @xmath113 is @xmath37 .",
    "line 7 is for checking whether @xmath54 is in @xmath109 or not by looking for a hitting set included in @xmath54 .",
    "this needs basically @xmath114 time and is a bottleneck computation of the algorithm .",
    "this part requires all of @xmath109 memory , thus we need to perform a breadth - first search .",
    "kavadias and stavropoulos@xcite proposed a depth - first version of this algorithm . according to the hitting sets generation rule ,",
    "each hitting set in @xmath48 is uniquely generated from a hitting set of @xmath115 .",
    "thus , starting from each hitting set in @xmath116 , we perform this generation rule in a depth - first manner , and visit all the minimal hitting sets of all @xmath48",
    ". the algorithm does not store each @xmath109 in memory , and it checks for the minimality of @xmath54 by checking whether @xmath117 is a hitting set or not for each @xmath118 .",
    "the algorithm is as follows .",
    "algorithm * ks * ( @xmath12 , @xmath119 ) + 1 . * if * @xmath120 * then * * output * @xmath12 ; * return * + 2 .",
    "* if * @xmath101 * then call * * ks*(@xmath12 , @xmath121 ) + 3 .",
    "* else * * for each * @xmath103 * do * + 4 .  * for each * @xmath122 * do * + 5 .  *",
    "if * @xmath123 is a hitting set * then go to * 8 .",
    "* end for * + 7 .",
    "* call * * ks*(@xmath54 , @xmath121 ) + 8 . *",
    "end for *    the bottleneck is also the minimality check on line 5 that basically needs to access all hyperedges in @xmath115 .",
    "the number hitting sets that are added a vertex is @xmath124 .",
    "the algorithms perform the minimality check for each addition , thus roughly speaking , the number of minimality checks in both algorithms is @xmath125 where @xmath126 is the average size of hyperedges . for @xmath127 and @xmath60 ,",
    "@xmath62 is non - empty , since @xmath15 always has a critical hyperedge in @xmath48 .",
    "it implies that any subset @xmath12 explored by the algorithm satisfies the minimality condition .",
    "the minimality check is usually one of the time - consuming parts of dualization algorithms .",
    "the check whether the current vertex subset is a hitting set or not is also a time consuming part , but it can be done by updating @xmath128 , thus for almost all vertex subsets to be operated on , its cost is much smaller than the minimality check . therefore , the number of minimality checks would be a good measure of the efficiency of the search strategy . here , we define the _ search space _ of an algorithm by the set of vertex subsets that are checked the minimality .",
    "the size of the search space is equal to the number of executed minimality checks .",
    "the cost for the minimality check increases with @xmath129 , for the dl algorithm , and with @xmath12 and @xmath47 for the ks algorithm .",
    "thus , the dl algorithm will be faster when the @xmath37 is small , whereas the ks algorithm will be faster when @xmath47 is small and @xmath12 is small on average .    the hbc algorithm is a kind of branch and bound algorithm .",
    "it starts from the emptyset , and chooses elements one by one . for each element",
    "@xmath15 , it generates two recursive calls concerned with a choice ; add @xmath15 to the current vertex subset , and do not add it . when the current vertex subset becomes a hitting set , it checks the minimality , and outputs it if minimal . to speed up the computation , the algorithm prunes branches through the use of the so called galois condition .",
    "the galois condition for @xmath12 and @xmath130 is @xmath131 , and when it holds , @xmath54 is never included in a minimal hitting set , thus we can terminate the recursive call with respect to @xmath54 .",
    "the galois condition is equivalent to our minimality condition , since it is equivalent to @xmath132 is proposed in 2003@xcite .",
    "the term `` minimality condition '' first appears in @xcite . ] .",
    "the algorithm is written as follows .",
    "algorithm hbc ( @xmath104 ) + 1 .",
    "@xmath105 ; @xmath133 + 2 .",
    "* while * @xmath134 + 3 .",
    "* for each * @xmath127 * do * + 4 .  * if * @xmath59 * then * output @xmath12 + 5 .",
    "* for each * @xmath15 larger than maximum vertex in @xmath12 * do * + 6 .  * if * @xmath54 satisfies the galois condition * then * insert @xmath12 to @xmath109 + 7 .",
    "* end for * + 8 .  *",
    "end for * + 9 . *",
    "end while * +    if the pruning method is only the galois condition , the vertex subsets to be explored by the algorithm is all the non - hitting sets satisfying the minimality condition .",
    "thus , the size of search space of hbc algorithm is no less than that of dl algorithm . on contrary , dl and ks algorithms has to update the minimal hitting sets even if they do not change , thus the hbc algorithm has an advantage in this point .",
    "we propose two depth - first search ( branch and bound ) algorithms for dualization problem .",
    "the main differences from the existing algorithms are to use @xmath67 for the minimality condition check , and pruning methods to avoid searching hopeless branches .",
    "the algorithms keep lists @xmath135 $ ] and @xmath128 representing @xmath136 and @xmath57 . when the algorithm adds a vertex @xmath15 to @xmath12 and generates a recursive call , it updates @xmath137 $ ] and @xmath128 by the following algorithm .",
    "update_crit_uncov ( @xmath138 , uncov$ ] ) + 1 .",
    "* for each * @xmath139 * do * + 2 .  *",
    "if * @xmath140 $ ] for a vertex @xmath122 * then * remove @xmath5 from @xmath135 $ ] + 3 .  * if * @xmath141 * then * @xmath142 ; @xmath143:= crit[e]\\cup \\{f\\}$ ] + 4 .",
    "* end for *    after execution , @xmath135 $ ] becomes @xmath144 .",
    "since each hyperedge @xmath5 can be critical hyperedge for at most one vertex , we put @xmath5 on the vertex as a mark and perform step 2 in a constant time .",
    "thus , the time complexity of this algorithm is @xmath145 . even though this algorithm is simple",
    ", we can reduce the time complexity of an iteration of the ks algorithm from @xmath146 to @xmath147 .",
    "one of our algorithms is based on the reverse search @xcite , and it can be regarded as an improved version of the ks algorithm .",
    "let @xmath148 , that is the set of vertex subsets that are operated by ks algorithm .",
    "let us denote the minimum @xmath119 such that @xmath149 by @xmath150 , and the minimum @xmath119 such that @xmath151 by @xmath152 .",
    "@xmath150 ( resp . , @xmath152 ) is defined as @xmath153 if @xmath62 ( resp . , @xmath57 )",
    "is empty . using these terms",
    ", we give a characterization of @xmath154 .",
    "[ cals ] @xmath155 belongs to @xmath154 if and only if @xmath156 holds for any @xmath60 .",
    "suppose that @xmath157 , thus @xmath158 for some @xmath119 .",
    "we can see that @xmath159 , @xmath160 includes a hyperedge @xmath161 with @xmath162 , and thus @xmath163 for any @xmath15 .",
    "thus , @xmath164 holds for any @xmath60 .",
    "conversely , suppose that @xmath164 holds for any @xmath60 .",
    "then , we can see that @xmath165 for any @xmath60 because @xmath166 .",
    "let @xmath167 .",
    "note that @xmath168 .",
    "we can then see that @xmath12 is a hitting set of @xmath48 and @xmath169 .",
    "this in turn implies that @xmath12 is a minimal hitting set in @xmath48 , and thus , it belongs to @xmath154 .    for @xmath157",
    ", @xmath170 is the minimum index @xmath119 such that @xmath12 is a minimal hitting set of @xmath48 , i.e. , @xmath171 .",
    "we define the _ parent _ @xmath172 of @xmath12 by @xmath64 , where @xmath15 is the vertex such that @xmath173 . since any @xmath174 is critical for at most one vertex , @xmath170 and the parent",
    "are uniquely defined .",
    "the parent - child relation given by this definition is acyclic , thus forms a tree spanning all the vertex subsets in @xmath154 and rooted at the emptyset .",
    "our algorithm performs a depth - first search on this tree starting from the emptyset .",
    "this kind of search strategy is called reverse search@xcite .",
    "this search strategy is essentially equivalent to ks algorithm if we skip all redundant iteration in which we add no vertex to the current vertex subset . in a straightforward implementation of ks algorithm , we have to iteratively compute the intersection of @xmath174 and the current vertex subset @xmath12 until we meet the @xmath174 that does not intersect with @xmath12 .",
    "when @xmath57 is not so large , it takes long time .",
    "particularly , when @xmath175 , we may spend @xmath176 time . on contrary , in our strategy , we have only to maintain @xmath57 , that is much lighter .",
    "the depth - first search starts from the emptyset .",
    "when it visits a vertex subset @xmath12 , it finds all children of @xmath12 iteratively and generates a recursive call for each child . in this way",
    ", we can perform a depth - first search only by finding children of the current vertex subset . the way to find the children",
    "is shown in the following lemma .    [ child ]",
    "let @xmath157 and @xmath177 .",
    "a vertex subset @xmath74 is a child of @xmath12 if and only if + ( 1 ) @xmath178 + ( 2 ) @xmath179 for some @xmath103 , and + ( 3 ) @xmath180 holds for any @xmath181 .",
    "suppose that @xmath74 is a child of @xmath12 .",
    "we can see that @xmath57 is not empty , and thus ( 1 ) holds . from the definition of the parent , @xmath12",
    "is obtained from @xmath74 by removing a vertex @xmath15 from @xmath74 . from @xmath182 and @xmath183 , we obtain @xmath184 .",
    "this means that @xmath185 , and thus ( 2 ) holds .",
    "this equation also implies that ( 3 ) holds .",
    "suppose that @xmath74 is a vertex subset satisfying ( 1 ) , ( 2 ) and ( 3 ) . from ( 2 )",
    ", we see that @xmath186 .",
    "since @xmath187 , this together with ( 3 ) implies that @xmath74 satisfies the conditions in lemma [ cals ] and thereby is included in @xmath188 .",
    "@xmath186 and ( 3 ) leads to @xmath189 and @xmath190 . note that condition ( 1 ) guarantees the existence of @xmath174 given condition ( 2 ) , thus it is implicitly used in the proof .    from lemma [ child ]",
    ", we can find all children of @xmath12 by adding each vertex @xmath103 to @xmath12 , and checking ( 3 ) .",
    "this can be done in a short time by updating @xmath67 .",
    "the algorithm is as follows .",
    "global variable : @xmath137 $ ] , @xmath128 + algorithm * rs * ( @xmath12 ) + 1 . * if * @xmath191 * then * * output * @xmath12 ; * return * + 2 . @xmath192 + 3 . * for each * @xmath103 * do * + 4 .",
    "call update_crit_uncov ( @xmath138 , uncov$ ] ) + 5 .  *",
    "if * @xmath193\\ } < i$ ] for each @xmath118 * then * * call * * rs*(@xmath74 ) + 6 .",
    "recover the change to @xmath137 $ ] and @xmath128 done in 4 + 7 .",
    "* end for *    algorithm rs enumerates all minimal hitting sets in @xmath194 time and @xmath147 space .",
    "since the parent - child relationship induces a rooted tree spanning all vertex subsets in @xmath154 , the algorithm certainly enumerates all vertex subsets in @xmath154 .",
    "since any minimal hitting set is included in @xmath154 , all minimal hitting sets are found by the algorithm .",
    "the update of @xmath137 $ ] and @xmath128 is done in @xmath195 time , thus an iteration of the algorithm takes @xmath147 time . in total , the algorithm takes @xmath196 time .",
    "the algorithm requires extra memory for storing @xmath137 $ ] and @xmath128 and for memorizing the hyperedges removed in step 4 .",
    "since @xmath137 $ ] and @xmath128 are pairwise disjoint , the total memory for @xmath67 and @xmath128 is @xmath197 .",
    "if a hyperedge is removed from a list , it will not be removed again in the deeper levels of the recursion , from the monotonicity of @xmath67 .",
    "thus , it also needs @xmath197 memory .",
    "the most memory is for @xmath14 of each @xmath15 , and takes @xmath147 space . *",
    "pruning method * suppose that in an iteration we are operating on a vertex subset @xmath12 , and have confirmed that @xmath54 does not satisfy the minimality condition .",
    "from lemma [ mono ] , we observe that @xmath198 does not satisfy the minimality condition if @xmath199 .",
    "this means that in the recursive call generated by the iteration with respect to @xmath12 , we do not have to care about the addition of @xmath15 , thus we remove @xmath15 from the candidate list for addition during the recursive call .",
    "this condition also holds when @xmath54 is a minimal hitting set , since no superset of a minimal hitting set satisfies the minimality condition .",
    "we call the vertex @xmath15 satisfying one of these conditions _",
    "we can apply this pruning method to the rs algorithm by finding all violating vertices before step 3 and can output all minimal hitting sets @xmath54 found in the process .",
    "we then execute the loop from step 3 to step 7 only for non - violating vertices , so that we can avoid unnecessary recursive calls .",
    "this subsection described a simple hill - climbing depth - first search algorithm , whose search space is contained in that of the hbc algorithm .",
    "we start from @xmath200 , and add vertices to @xmath12 recursively unless the minimality condition is violated . to avoid the duplication , we use a list of vertices @xmath201 that represents the vertices that can be added in the iteration .",
    "the vertices not included in @xmath201 will not be added , even if the addition satisfies the minimality condition , i.e. , the iteration given @xmath12 and @xmath201 enumerates all minimal hitting sets including @xmath12 and included in @xmath202 by recursively generating calls .",
    "suppose that an iteration is given @xmath12 and @xmath201 , and without loss of generality @xmath203 .",
    "for the first vertex @xmath204 , we make a recursive call with respect to @xmath205 , with @xmath206 , to enumerate all minimal hitting sets including @xmath205 . after the termination of the recursive call",
    ", we generate a recursive call for @xmath207 . to avoid finding the minimal hitting sets including @xmath204 , we give @xmath208 to the recursive call . in this way , for each vertex @xmath209",
    ", we generate a recursive call with @xmath210 and @xmath211 .",
    "this search strategy is common to many algorithms for enumerating members in a monotone set system , for example clique enumeration @xcite .",
    "that is , its correctness has already been proved .",
    "next , let us describe a pruning method coming from the necessary condition to be a hitting set .",
    "suppose that an iteration is given @xmath12 and @xmath201 , and let @xmath5 be a hyperedge in @xmath57 .",
    "we can see that any minimal hitting set including @xmath12 has to include at least one vertex in @xmath12 .",
    "thus , we have to generate recursive calls with respect to vertices in @xmath212 , but do not have to do so for vertices in @xmath213 . in the rs algorithm , we have to find all violating vertices before generating recursive calls .",
    "in contrast , we can omit this step from our dfs algorithm .",
    "suppose that an iteration is given @xmath12 and @xmath201 , and is going to generate recursive calls with respect to vertices in @xmath214 .",
    "then , we first set @xmath201 to @xmath215 .",
    "if @xmath216 is not a violating vertex , we generate a recursive call for @xmath217 , and add @xmath216 to @xmath201 .",
    "if @xmath216 is a violating vertex , we do not add @xmath216 to @xmath201 . in this way , when we generate a recursive call with respect to @xmath218 , all violating vertices @xmath219 have already been found , thus there is no need to find all them at the beginning .",
    "the algorithm is described as follows .",
    "global variable : @xmath137 $ ] , @xmath128 , @xmath201 + algorithm * dfs * ( @xmath12 ) + 1 . *",
    "if * @xmath191 * then output * @xmath12 ; * return * + 2 .",
    "choose a hyperedge @xmath5 from @xmath128 ; + 3 .",
    "@xmath220 ; @xmath221 + 4 . * for each * @xmath222 * do * + 5 .  call update_crit_uncov ( @xmath138 , uncov$ ] ) + 6 .  * if * @xmath223 for each @xmath118 * then * * call * * dfs*(@xmath54 ) ; @xmath224 + 7 .",
    "recover the change to @xmath137 $ ] and @xmath128 done in 5 + 8 .",
    "* end for *    similar to the case of the rs algorithm , the computation time of an iteration is bounded by @xmath147 .",
    "this section is devoted to the computational techniques for improving efficiency .",
    "our data structure for representing hyperedges and @xmath14 is an array list in which the ids of vertices or hyperedges are stored .",
    "using array list fastens the set operations with respect to @xmath14 and list vertices in a hyperedge .",
    "the data structure for @xmath67 and @xmath128 is a doubly linked list . in each iteration",
    ", we remove some hyperedge ids from these lists and reinsert them after the termination of a recursive call .",
    "a doubly linked list is a good data structure for these operations , as it preserves the order of ids in the list .",
    "when two subsets @xmath12 and @xmath74 are represented by lists of their including elements , the set operations such as intersection and set difference need @xmath225 time .",
    "however , when we have the characteristic vectors of @xmath12 and @xmath74 , we can do better .",
    "the characteristic vector of @xmath12 is a vector whose @xmath119th element is one if and only if @xmath119 is included in @xmath12 . to take the intersection , we scan @xmath12 ( or @xmath74 ) with the smaller size , and choose the elements included in @xmath74 ( or @xmath12 ) . this check can be done in @xmath226 time with using the characteristic vector of @xmath74 , thus the computation time is reduced to @xmath227 . for computing @xmath228 , we remove their intersection from @xmath12 , thus the computation time is also the same .    our algorithms take intersection of ( @xmath67 and @xmath128 ) and @xmath229 .",
    "updating the characteristic vectors of ( @xmath67 and @xmath128 ) uses @xmath230 memory and does not increase the time complexity .",
    "the characteristic vectors of @xmath229 for each @xmath15 requires a lot of memory to store , thus we use it only when @xmath47 is larger than @xmath231 , i.e. , @xmath6 is dense .",
    "note that in our experiments , all instances satisfied this condition .      in the dfs algorithm , we can choose arbitrary hyperedge in @xmath128 as @xmath5 , for restricting the vertices to be added .",
    "we choose a hyperedge including the smallest number of vertices which have not been pruned , so that the number of recursive calls generated will be small . counting such vertices in each hyperedge in @xmath128 may take time longer than the case just choosing one arbitrary , but our preliminary experiments showed that it reduced the computation time almost in half .      the pruning method described above can be applied to any vertex . however , applying it to all possible vertices may take a long time compared with other parts of an iteration .",
    "sometimes it occurs that pruning takes a long time but only few branches are pruned .",
    "thus , to make the computation time stable , we prune only the vertices in @xmath212 , which are the vertices to be added to the current solution .",
    "this takes a time proportional to the time spent by an iteration , thus it never needs a long time .      in some instances",
    ", @xmath0 is quite dense , e.g. , over 95% of vertices are included in many @xmath4 .",
    "this occurs when the data has no clear structure and has many minimal hitting sets .",
    "we can often find such instances in practice , such as in minimal infrequent vertex subset mining from maximal frequent vertex subsets .",
    "in such cases , the instance itself takes up a lot of memory , and needs a long time to be operated on . here",
    ", we can reduce the computation time by using the complement .    the complement version of our algorithm inputs the complement of each @xmath4 . the operations of each iteration change so that the vertices to be added are vertices not in @xmath5 , and taking difference in the @xmath67 update changes to taking the intersection .",
    "this substantially reduces the computation time , since we have to access only a small number of vertices / hyperedges . in our experiments",
    ", we found that this idea works well for very dense datasets .",
    "in this section , we show the results of our computational experiments comparing our algorithms with the existing algorithms .      our algorithms are implemented in c , without any sophisticated library such as binary tree .",
    "existing algorithms are implemented in c++ by using the vector class in stl .",
    "ks algorithm and fredman khachiyan algorithm ( begk@xcite ) are given by the authors .",
    "all tests were performed on a 3.2 ghz core i7 - 960 with a linux operating system with 24 gb of ram memory .",
    "note that none of the implementations used multi - cores .",
    "the codes and the instances are available at the author s web cite ( http://research.nii.ac.jp/  uno / dualization.html ) .",
    "we prepared several instances of problems in several categories as follows .",
    "the first category consists of randomly generated instances .",
    "each hyperedge includes a vertex @xmath119 with probability @xmath232 .",
    "the sizes and the probabilities are listed below .",
    "the instances in the second category were generated by the dataset `` connect-4 '' taken from the uci machine learning repository @xcite .",
    "connect-4 is a board game , and each row of the dataset corresponds to a minimal winning / losing stage of the first player , and a minimal hitting set of a set of winning stages is a minimal way to disturb wining / losing plays of the first player . from the dataset of",
    "winning / losing stages , we took the first @xmath46 rows to make problem instances of different sizes .",
    "the third instances are generated from the frequent itemset ( pattern ) mining problem .",
    "an itemset is a hyperedge in our terminology . for a set family @xmath6 and a support threshold @xmath233 ,",
    "an itemset is called _ frequent _ if it is included in at least @xmath233 hyperedges , and _",
    "infrequent _ otherwise .",
    "a frequent itemset included in no other frequent itemset is called a _ maximal frequent itemset _ , and an infrequent itemset including no other infrequent itemset is called a _ minimal infrequent itemset_. a minimal infrequent itemset is a minimal itemset included in no maximal frequent itemset , and any subset of it is included in at least one maximal frequent itemset .",
    "thus , the dual of the set of the complements of maximal frequent itemsets is the set of minimal infrequent itemsets .",
    "the problem instances are generated by enumerating all maximal frequent sets from the datasets `` bms - webview-2 '' and `` accidents '' , taken from the fimi repository @xcite .",
    "the profiles of the datasets are listed below .",
    "the fourth instances are used in previous studies @xcite .",
    "+ @xmath234 matching graph ( m(@xmath235 ) ) : a hypergraph with n vertices ( n is even ) and @xmath236 hyperedges forming a perfect matching , that is , hyperedge @xmath174 is @xmath237 .",
    "this instance has few hyperedges but a large number of minimal hitting sets @xmath238 .",
    "+ @xmath234 dual matching graph ( dm(@xmath235 ) ) : it is dual(m(@xmath235 ) ) .",
    "it has @xmath238 hyperedges on @xmath235 nodes .",
    "this instance has a large number of hyperedges but a small number of minimal hitting sets @xmath236 .",
    "+ @xmath234 threshold graph ( th(@xmath235 ) ) : a hypergraph with n vertices ( @xmath235 is even ) and hyperedge set @xmath239 is even}. this instance has a small number of hyperedges @xmath240 and a small number of minimal hitting sets @xmath241 .",
    "+ @xmath234 self - dual threshold graph ( sdth(@xmath235 ) ) : the hyperedge set of sdth(@xmath235 ) is given as @xmath242th(@xmath243th@xmath244 .",
    "sdth(@xmath235 ) has the same number of minimal hitting sets as its hyperedges , @xmath245 .",
    "+ @xmath234 self - dual fano - plane graph ( sdfp(@xmath235 ) ) : a hypergraph with n vertices and @xmath246 hyperedges , where @xmath247 .",
    "the construction starts with the set of lines in a fano plane @xmath248 @xmath249 @xmath250 @xmath251 @xmath252 @xmath253 @xmath254 .",
    "then we set @xmath255 , where @xmath256 are @xmath257 disjoint copies of @xmath258 .",
    "the dual of @xmath2 is the hypergraph of all @xmath259 unions obtained by taking one hyperedge from each of @xmath257 copies of @xmath260 .",
    "we finally obtain sdfp(@xmath235 ) , which is a hypergraph of @xmath261 hyperedges .      before showing the results ,",
    "we discuss the difference between the algorithms from the viewpoint of algorithmic structures . basically , the search space of dl , ks , and our rs algorithms are the same .",
    "however , @xmath262 and @xmath263 check the same hitting sets many times , while rs operates by one hitting set at most once .",
    "in addition , our rs has a pruning method , thus the number of hitting sets generated may be decreased .",
    "the search spaces of the hbc and dfs algorithms are basically the same , but dfs reduces it by using pruning methods .    for the minimality check , dl , bmr , and hbc algorithms access basically all members in @xmath109 .",
    "basically , this takes @xmath264 time .",
    "some heuristics can reduce the time , but the reduction ratio would be limited .",
    "in contrast , ks takes @xmath265 time , and @xmath266 takes @xmath145 time .",
    "thus , we can expect that    * dl , bmr , and hbc are faster when there are only a few minimal hitting sets , + * hbc is faster if the search space of dl is larger than the set of vertex subsets satisfying the minimality condition , for example , in the case that the sizes of minimal hitting sets are quite small + * ks , rs , and dfs are faster when @xmath45 is small , + * rs is faster than ks when the sizes of minimal hitting sets are not small , and vertex unification ( done by ks ) does not work .",
    "table [ winning ] - [ bms2 ] compare the computation times . in these tables",
    ", @xmath267 represents the number of hyperedges , @xmath268 represents the average size of hyperedges , @xmath269 represents the number of minimal hitting sets and @xmath270 represents the average size of minimal hitting sets .",
    "the computation time is in seconds .",
    "furthermore , `` - '' means that the computation time was more than 1000 seconds , and `` fail '' implies that the computation did not terminate normally because of a shortage of memory or some error .",
    "0 - note that the results of instance @xmath271 in our experiments are much different from that in previous works experiments @xcite .",
    "we have made sure that the instance @xmath271 of us is self - dual .",
    "thus , we think that our instance is not wrong , but the assignment of nodes number is different between the instance @xmath271 of us and that of previous works     .computation time on the dataset of winning stage in connect-4 [ cols= \" > , > , > , > , > , > , > , > , > \" , ]",
    "we proposed efficient algorithms for solving the dualization problem .",
    "the new depth - first search type algorithms are based on reverse search and branch and bound with a restricted search space .",
    "we also proposed an efficient minimality condition check method that exploits a new concept called `` critical hyperedges '' .",
    "computational experiments showed that our algorithms outperform the existing ones in almost all cases , while using less memory even for very large - scale problems with up to millions of hyperedges . in some cases ,",
    "though , our algorithms take a long time for the minimality check . shortening",
    "this time will be one of the future tasks .",
    "more efficient pruning methods are also an interesting topic of future work .",
    "part of this research is supported by the funding program for world - leading innovative r&d on science and technology , japan .",
    "we thank khaled elbassion and elias c. stavropoulos for providing us with the programs used in our experiments .",
    "99 d. avis and k. fukuda , `` reverse search for enumeration , '' _ discrete applied mathematics , _ vol . * 65 * , pp .",
    "21 - 46 ( 1996 ) . c. berge , `` hypergraphs , '' _ volume 45 of north - holland mathematical library , _",
    "e. boros , k. elbassioni , v. gurvich , and l. khachiyan , `` an efficient implementation of a quasi - polynomial algorithm for generating hypergraph transversals , '' _ in proc .",
    "of the 11th european symposium on algorithms ( esa 2003 ) _ , * 2432 * , pp .  556 - 567 , springer ( 2003 ) . g. dong and j. li , `` mining border descriptions of emerging patterns from dataset pairs , '' _ knowledge and information systems , _ vol . * 8 * , pp .  178 - 202 ( 2005 ) .",
    "j. bailey , t. manoukian , and k. ramamohanarao , `` a fast algorithm for computing hypergraph transversals and its application in mining emerging patterns , '' _ in proc . of the 3rd ieee international conference on mining ( icdm 2003 ) , _ , pp .",
    "485 - 488 , ieee computer society , december ( 2003 ) .",
    "m. l. fredman and l. khachiyan , `` on the complexity of dualization of monotone disjunctive normal forms , '' _ journal of algorithms , _ vol .",
    "* 21 * , pp .  618 - 628 ( 1996 ) . c. hbert , a. bretto and b. crmilleux , `` a data mining formalization to improve hypergraph minimal transversal computation '' , _ fundamental informaticae _ * 80 * , pp .  415 - 433 ( 2007 ) .",
    "d. j. kavvadias and e. c. stavropoulos , `` evaluation of an algorithm for the transversal hypergraph problem , '' _ algorithm engineering , _ pp .  72 - 84 ( 1999 ) .",
    "d. j. kavvadias and e. c. stavropoulos , `` an efficient algorithm for the transversal hypergraph generation , '' _ journal of graph algorithms and applications , _ vol .",
    "* 9 * , pp .  239 - 264 ( 2005 ) .",
    "frequent itemset mining dataset repository , http://fimi.cs.helsinki.fi/data/ m. hagen , `` algorithmic and computational complexity issues of monet , '' _ ph.d .",
    "thesis , friedrich - schiller - universitat jena , _ december ( 2008 ) .",
    "k. satoh , t. uno , `` enumerating maximal frequent sets using irredundant dualization '' , lecture notes in artificial intelligence * 2843 * , pp .  256 - 268 ( 2003 ) .",
    "e. tomita , a. tanaka , h. takahashi , `` the worst - case time complexity for generating all maximal cliques and computational experiments '' , theoretical computer science * 363 * , pp .",
    "28 - 42 ( 2006 ) .",
    "uci machine learning repository , http://archive.ics.uci.edu/ml/ t. uno , k. satoh , `` detailed description of an algorithm for enumeration of maximal frequent sets with irredundant dualization '' , icdm 2003 workshop on frequent itemset mining implementations , ceur workshop proceedings 90 ceur-ws.org , ( 2003 ) .",
    "l. khachiyan , e. boros , k. elbassioni and v. gurvich , `` an efficient implementation of a quasi - polynomial algorithm for generating hypergraph transversals and its application in joint generation '' , discrete applied mathematics * 154 * , pp .",
    "2350 - 2372 ( 2006 ) ."
  ],
  "abstract_text": [
    "<S> a hypergraph @xmath0 is a set family defined on vertex set @xmath1 . </S>",
    "<S> the dual of @xmath0 is the set of minimal subsets @xmath2 of @xmath1 such that @xmath3 for any @xmath4 . </S>",
    "<S> the computation of the dual is equivalent to many problems , such as minimal hitting set enumeration of a subset family , minimal set cover enumeration , and the enumeration of hypergraph transversals . </S>",
    "<S> although many algorithms have been proposed for solving the problem , to the best of our knowledge , none of them can work on large - scale input with a large number of output minimal hitting sets . </S>",
    "<S> this paper focuses on developing time- and space - efficient algorithms for solving the problem . </S>",
    "<S> we propose two new algorithms with new search methods , new pruning methods , and fast techniques for the minimality check . </S>",
    "<S> the computational experiments show that our algorithms are quite fast even for large - scale input for which existing algorithms do not terminate in a practical time . </S>"
  ]
}