{
  "article_text": [
    "our objective is to extract physics from lattice qcd with possibly minimal amount of computations .",
    "obviously , the required computing power exceeds that of any desktop personal computer currently available in the market .",
    "thus , for one without supercomputer resources , building a computational system @xcite seems to be inevitable if one really wishes to pursue a meaningful number of any physical quantity from lattice qcd .",
    "however , the feasibility of such a project depends not only on the funding , but also on the theoretical advancement of the subject , namely , the realization of exact chiral symmetry on the lattice @xcite .",
    "now , if we also take into account of the current price / performance of pc hardware components ( cpu + ram + hard disk gbyte ) ide hard disk turns out to be also rather crucial for this project , since the data storage is enormous . ] ) , it seems to be the right time to rejuvenate the project @xcite with a new goal - to build a computational system for lattice qcd with exact chiral symmetry . in this paper",
    ", we outline the essential features of a linux pc cluster ( 64 nodes ) which has been built at national taiwan university .",
    "in particular , we discuss how to optimize its hardware and software for lattice qcd with overlap dirac operator .",
    "first , we start from quenched qcd calculations ( i.e. , ignoring any internal quark loops by setting @xmath1 ) .",
    "thus , our first task is to compute quark propagators in the gluon field background , for a sequence of configurations generated stochastically with weight @xmath2 ( @xmath3 : pure gluon action )",
    ". then the hardronic observables such as meson and baryon correlation functions can be constructed , and from which the hadron masses and decay constants can be extracted .",
    "we use the creutz - cabbibo - marinari heat bath algorithm @xcite to generate ensembles of @xmath4 gauge configurations .",
    "the computation of quark propagators depends on the scheme of lattice fermions , the hard core of lattice qcd . in general",
    ", one requires that any quark propagator coupling to physical hadrons must be of the form @xcite ( d_c + m_q ) ^-1  , where @xmath5 is the bare quark mass , and @xmath6 is a chirally symmetric and anti - hermitian dirac operator [ @xmath7 and @xmath8 ] .",
    "here we assume that @xmath6 is doubler - free , has correct continuum behavior , and @xmath9 is exponentially local for smooth gauge backgrounds .",
    "note that the way @xmath5 coupling to @xmath6 is the same as that in the continuum .",
    "the chiral symmetry of @xmath6 ( even at finite lattice spacing ) is the crucial feature of any quark coupling to physical hadrons .",
    "otherwise , one could hardly reproduce the low energy strong interaction phenomenology from lattice qcd .    for any massless lattice dirac operator",
    "@xmath10 satisfying the ginsparg - wilson relation @xcite [ eq : gwr ] d _",
    "5 + _ 5 d = 2 r a d _ 5 d  , it can be written as @xcite d = d_c ( 1 + r a d_c ) ^-1  , and the bare quark mass is naturally added to the @xmath6 in the numerator @xcite , d(m_q ) = ( d_c + m_q ) ( 1 + r a d_c ) ^-1  . then the quenched quark propagator becomes ( d_c + m_q ) ^-1 = ( 1 - r m_q a ) ^-1 [ d(m_q)^-1 - r a ]    if we fix one of the end points at @xmath11 and use the hermitcity @xmath12 , then only 12 ( 3 colors times 4 dirac indices ) columns of [ eq : propagator ] d(m_q)^-1 = d^(m_q ) \\",
    "{ d(m_q ) d^(m_q ) } ^-1 are needed for computing the time correlation functions of hadrons .",
    "now our problem is how to optimize a pc cluster to compute @xmath13 for a set of bare quark masses .",
    "the outline of this paper is as follows . in section 2 ,",
    "we briefly review our scheme of computing quark propagators via the overlap dirac operator .",
    "the details have been given in ref .",
    "@xcite . in section 3",
    ", we discuss a simple scheme of memory management for the nested conjugate gradient loops . in section 4 ,",
    "we discuss how to implement the sse2 codes for the computationally intense parts of our program . in section 5",
    ", the performance of our system is measured in terms of a number of tests pertaining to the computation of quark propagators . in section 6 ,",
    "we conclude with some remarks and outlooks .",
    "the massless overlap dirac operator @xcite reads as [ eq : overlap ] d = m_0 a^-1 ( + _ 5 ) where @xmath14 denotes the hermitian wilson - dirac operator with a negative parameter @xmath15 , [ eq : hw ] h_w = _ 5 d_w = _ 5 ( -m_0 + _ t_+ w )  , @xmath16 the naive fermion operator , and @xmath17 the wilson term",
    ". then @xmath10 ( [ eq : overlap ] ) satisfies the ginsparg - wilson relation ( [ eq : gwr ] ) with @xmath18 . in this paper",
    ", we always fix @xmath19 for our computations .",
    "details of our implementation have been given in ref .",
    "@xcite .    basically , we need to solve the following linear system [ eq : outer_cg ] & & d(m_q ) d^(m_q ) y & = & \\ { m_q^2 + ( 2 m_0 ^ 2 - ) } y = by conjugate gradient ( cg )",
    ". then the quark propagators can be obtained through ( [ eq : propagator ] ) . with zolotarev optimal rational approximation @xcite to @xmath20 , the multiplication )",
    "is in the form @xmath21 which is different from @xmath22 used in ref .",
    "we refer to ref .",
    "@xcite for further discussions . ] & & h_w ( ) y , h_w & & h_w ( h_w^2 + c_2n ) _ l=1^n y = h_w ( h_w^2 + c_2n )",
    "_ l=1^n b_l z_l [ eq : mult_y ] can be evaluated by invoking another conjugate gradient process to the linear systems [ eq : inner_cg ] ( h_w^2 + c_2l-1 ) z_l = y , l = 1 , , n  . where c_l & = & + b_l & = & d_0 + d_0 & = & _ l=1^n + & = & _ l=1 ^",
    "2n+1  . here @xmath23 denotes the elliptic theta function , and the jacobian elliptic function @xmath24 is defined by the elliptic integral u = _ 0^  , and @xmath25 is the complete elliptic integral of the first kind with modulus @xmath26 , k = _ 0 ^ 1  , where @xmath27 , @xmath28 , and @xmath29 and @xmath30 are the maximum and the minimum of the eigenvalues of @xmath31 .    instead of solving each @xmath32 individually , one can use multi - shift cg algorithm @xcite , and obtain all @xmath32 altogether , with only a small fraction of the total time what one had computed each @xmath32 separately .",
    "evidently , one can also apply multi - shift cg algorithm to ( [ eq : outer_cg ] ) to obtain several quark propagators with different bare quark masses .    in order to improve the accuracy of the rational approximation as well as to reduce the number of iterations in the inner cg loop ,",
    "it is _ crucial _ to narrow the interval @xmath33 $ ] by projecting out the largest and some low - lying eigenmodes of @xmath31 .",
    "we use arnoldi algorithm @xcite to project these eigenmodes . denoting these eigenmodes by [ eq : eigen ] h_w u_j = _ j u_j , j = 1 , , k",
    ", then we project the linear systems ( [ eq : inner_cg ] ) to the complement of the vector space spanned by these eigenmodes [ eq : inner_cg1 ] ( h_w^2 + c_2l-1 ) |z_l = |y ( 1 - _ j=1^k u_j u_j^ ) y  , l = 1 , , n  .    in the set of projected eigenvalues of @xmath31 , @xmath34 , we use @xmath29 and @xmath30 to denote the least upper bound and the greatest lower bound of the eigenvalues of @xmath35 , where |h_w = h_w - _ j=1^k _ j u_j u_j^",
    ". then the eigenvalues of h_w^2 = |h_w^2 / _ min^2 fall into the interval @xmath36 , @xmath28 .",
    "now the matrix - vector multiplication ( [ eq : mult_y ] ) can be expressed in terms of the projected eigenmodes ( [ eq : eigen ] ) plus the solution obtained from the conjugate gradient loop ( [ eq : inner_cg1 ] ) in the complementary vector space , i.e. , [ eq : ss ] h_w y h_w ( h_w^2 + c_2n ) _ l=1^n",
    "b_l |z_l + _ j=1^k u_j u_j^ y s then the breaking of exact chiral symmetry ( [ eq : gwr ] ) can be measured in terms of [ eq : sigma ] =  . in practice",
    ", one has no difficulties to attain @xmath37 for most gauge configurations on a finite lattice @xcite .",
    "now the computation of quark propagators involves two nested conjugate gradient loops : the so - called inner cg loop ( [ eq : inner_cg1 ] ) , and the outer cg loop ( [ eq : outer_cg ] ) . the inner cg loop is the price what one pays for preserving the exact chiral symmetry at finite lattice spacing .",
    "in this section we discuss how to configure the hardware and software of a pc cluster such that it can attain the optimal price / performance for the execution of the nested cg loops , ( [ eq : outer_cg ] ) and ( [ eq : inner_cg1 ] ) .",
    "first , we examine how much memory is required for computing one of the 12 columns of the quark propagators for a set of bare quark masses , since each column can be computed independently .",
    "if the required memory can be allocated in a single node , then each node can be assigned to work on one of the 12 columns of the quark propagators .",
    "then the maximum speed of a pc cluster is attained since there is no communication overheads .",
    "nevertheless , the memory ( rdram ) is the most expensive component , thus its amount should be minimized even though the maximum memory at each node can be up to 4 gbyte . on the other hand , if one distributes the components of the nested cg loops across the nodes and performs parallel computations ( with mpi ) through a fast network switch , then the memory at each node can be minimal .",
    "however , the cost of a fast network switch and its accessories is rather expensive , and also the efficiency of the entire system will be greatly reduced due to the communication overheads . therefore , to optimize the price / performance of the pc cluster relies on what is the minimal memory required for computing one of the 12 columns of the quark propagators .",
    "let @xmath38 denote the total number of lattice sites , then each column of @xmath39 with double complex ( 16 bytes ) entries takes [ eq : nv ] n_v = n_s 12 16  .",
    "using @xmath40 or one column as the unit , we list the memory space of all components during the execution of the nested cg loops :    * gauge links : 3 . *",
    "number of projected low - lying eigenmodes : @xmath41 * quark propagators [ i.e. , @xmath42 in ( [ eq : outer_cg ] ) ] of @xmath43 masses : @xmath44 . + ( note that each @xmath42 only takes 1/2 column since it is chiral . ) * conjugate gradient vectors in the cg algorithm : @xmath44 . * residual vector for the outer cg loop : @xmath45 . * the vector @xmath46 ( of the smallest bare quark mass ) at the interface between the inner and the outer cg loops : 1 . * the inner cg loop : @xmath47 ( where @xmath48 is the degree of zolotarev rational polynomial ) , which consists of + ( i ) @xmath49 vectors : @xmath48 ; + ( ii ) conjugate gradient vectors @xmath50 : @xmath48 ; + ( iii ) residual vector ( r ) : 1 ; + ( iv ) @xmath51 : 1 ; + ( v ) @xmath52 : 1 .",
    "therefore , the memory space for all components of the nested cg loops is [ eq : ncg ] n_cg = ( n_m + 1/2 ) + ( 2 n + 3 ) + k + 3 = n_m + 2 n + k + 6.5 a schematic diagram of all components of the nested cg loops is sketched in fig . 1 .",
    "suppose we wish to compute quark propagators on the @xmath53 lattice ( at @xmath54 ) , with parameters @xmath55 , @xmath56 , and @xmath57 .",
    "then , according to ( [ eq : nv ] ) and ( [ eq : ncg ] ) , n_v & & 0.024   , + n_cg & = & 70.5   , the required memory for all components of the nested cg loops is n_cg n_v 70.5 0.024 = 1.7     this seems to imply that one should install four stripes of @xmath58 mbyte modules ( i.e. total 2 gbyte ) at each node , if one wishes to let each node compute independently , and to attain the maximum speed of the pc cluster .",
    "however , this is a rather expensive solution at this moment , in view of the current price of @xmath58 mbyte modules . on the other hand ,",
    "if one distributes the components of the nested cg loops across the nodes and performs parallel computations ( with mpi ) through a fast network switch , then the price / performance seems to be even worse than the former solution .",
    "fortunately , we observe that _ not _ all column vectors are used simultaneously at any step of the nested cg loops , and also the computationally intense part is at the inner cg loop .",
    "thus we can use the hard disk as the virtual memory for the storage of the intermediate solution vectors and their conjugate gradient vectors ( @xmath59 ) at each iteration of the outer cg loop , while the cpu is working on the inner cg loop .",
    "then the minimal physical memory required at each node can be greatly reduced .",
    "also , the projected eigenmodes are not required to be kept inside the memory , since they are only needed at the start of the inner cg loop to compute @xmath60 ( for the smallest bare quark mass ) , |y_1 ( 1 - _ j=1^k u_j u_j^ ) y_1  , and _",
    "j=1^k u_j u_j^ y_1 _ p y_1 where @xmath61 is only needed for computing @xmath62 ( [ eq : ss ] ) at the completion of the inner cg loop .",
    "thus one has the options to keep the vector @xmath61 inside the memory during the entire inner cg loop or save it to the hard disk and then retrieve it at the completion of the inner cg loop .",
    "further , since @xmath60 is only needed at the start of the inner cg loop , so it can share the same memory location with the residual vector @xmath63 .",
    "now it is clear that the minimum memory at each node ( without suffering a substantial loss in the performance ) is [ eq : ncg_min ] n_cg^min = ( 2n + 3 ) + 3 = 2n + 6   , which suffices to accommodate the link variables and all relevant vectors for the inner cg loop .",
    "after the completion of the inner cg loop and the vector @xmath62 ( [ eq : ss ] ) is computed , the memory space of @xmath64 column vectors is released , and the vectors @xmath65 and @xmath66 of the outer cg loop can be read from the hard disk , which are then updated to new values according to the cg algorithm .    with this simple scheme of memory management , the minimal memory for computing one of the 12 columns of the quark propagators ( for a set of bare quark masses ) on the @xmath53 lattice with @xmath67 ( degree of zolotarev rational polynomial ) becomes n_cg^min n_v = 38 0.024 = 0.912  .",
    "thus the computation can be performed at a single node with one gbyte of memory , which can be implemented by installing four stripes of 256 mbyte memory modules , a much more economic solution than using @xmath68 mbyte modules .",
    "moreover , the time for disk i / o ( at the interface of inner and outer cg loops ) only constitutes a few percent of the total time for the execution of the entire nested cg loops ( table [ tab : diskio ] ) .",
    "this is the optimal memory configuration for a pc cluster to compute quark propagators on the @xmath53 lattice , which of course is not necessarily the optimal one for other lattice sizes . however , our simple scheme of memory management for the nested cg loops should be applicable to any lattice sizes , as well as to other systems .    in passing ,",
    "we emphasize that the zolotarev optimal rational approximation to @xmath20 plays a crucial role to minimize the number of vectors required for the inner cg loop .",
    "if one had used other rational approximations , then it would require a very large @xmath48 to preserve exact chiral symmetry to a high precision ( e.g. , @xmath69 ) . in that case",
    ", it would be impossible to attain the optimal price / performance as what has been outlined above .",
    "with the optimal memory allocation for each node , we further enhance the performance of our lattice qcd codes ( in fortran ) by rewriting its computationally intense parts in the sse2 assembly codes of pentium 4 . in this section ,",
    "we briefly review the basic features of the vector unit ( sse2 ) of pentium 4 , and then describe how to implement sse2 codes in our lattice qcd program .",
    "the simplest and the most efficient scheme of parallel computation is single instruction multiple data ( simd ) .",
    "it can be implemented inside cpu through a set of long registers .",
    "if each register can accommodate several ( say , @xmath70 ) data entries , then any operation ( addition , subtraction , multiplication and division ) on these registers will act on all data entries in parallel , thus yields the speed - up by a factor of @xmath70 comparing with normal registers . a schematic diagram is shown in fig .",
    "[ f : mulpd ] .",
    "even though intel had implemented the vector unit in their cpus since pentium - mmx series , only in the most recent ia-32 pentium 4 and the advanced ia-64 itanium , the architecture has been extended to sse2 ( streamed simd extension 2 ) to incorporate double precision data entries .",
    "the pentium 4 processor has eight registers ( * * % * * xmm0 , % xmm1 ,  , % xmm7 ) for simd operations @xcite .",
    "each register is 128 bits wide and can accomodate 4 integers , or 4 single - precision or 2 double - precision floating point numbers .",
    "since we always use double precision floating point numbers in our program , the execution speed of our program can be almost doubled if sse2 is turned on judiciously in the computationally intensive parts .",
    "note that sse2 complies with the ieee 32-bit and 64-bit arithmetic , thus the precision is lower than the extended 80-bit precision of the normal registers in pentium 4 .",
    "however , the difference is less than one part in @xmath71 ( double precision ) , thus is negligible in our computations .",
    "since our lattice qcd codes were originally written in fortran 77 , it would be natural if sse2 codes can be directly embedded in our fortran program .",
    "however , to our knowledge , the fortran compilers currently available in the market do not support the option of inlining sse2 codes .",
    "moreover , for optimal performance of sse2 , the data should be aligned to 16-byte memory boundary .",
    "this can be easily carried out in c. therefore our strategy to implement sse2 codes is rewrite the main program unit in c such that the data arrays are allocated and aligned to 16 bytes memory boundary , then the sse2 codes are embedded in c subroutines which are then called by original routines in fortran .    of course",
    ", if one has written lattice qcd codes in c , then the sse2 codes can be embedded in c routines directly , without dealing with the interface of c and fortran .    in the following ,",
    "we illustrate our scheme of implementing sse2 codes with an example program .",
    "the default compilers are @xmath72 and @xmath73 in linux .    ....",
    "program main          implicit none          integer n          parameter ( n=100 )          double precision r(n ) , v(n )          call vxzero(n , r , v )          end            subroutine vxzero(n , r , v )          implicit none          integer n          double precision c , r ( * ) , v ( * )          ...          call vadd(n , c , r , v )              !   r = r + c v          ...                                !",
    "c :     scalar          end                                !",
    "r , v : vector ....    here the fortran * main * program calls the subroutine * vxzero * which in turn calls a computationally intensive routine * vadd*.    first , we rewrite the main program in c , with the data arrays allocated and properly aligned .    ....",
    "# include < malloc.h >          int main(int argc , char * * argv )          {              int n=100 ;              double * r , * v ;          / * setup the environment for fortran * /              f_setarg(argc , argv ) ;              f_setsig ( ) ;              f_init ( ) ;          / * allocate r & v , and align them to 16-byte boundary * /              r = memalign(16 , n*sizeof(double ) ) ;              v = memalign(16 , n*sizeof(double ) ) ;          / * call the fortran subroutine * /              vxzero_(&n , r , v ) ;          / * shutdown the i / o channels of fortran * /              f_exit ( ) ;              exit(0 ) ;              return 0 ;          } ....    the function call * memalign ( ) * dynamically allocates 16 bytes aligned pointers * r * and * v*. then the aligned arrays * v [ ] * and * r [ ] * can be passed to c subroutines for sse2 operations .    next we rewrite the computationally intensive routine * vadd * in c with embedded sse2 codes .    ....          / * load variable a into % % xmm0 * /          # define sse_load(a )   \\              _ _ asm _ _ _ _ volatile _ _ ( \" movapd % 0 , % % xmm0 \" : : \" m \" ( a ) )            / * r = r + % % xmm0 x v           * /          # define sse_add(r , v )                \\              _ _ asm _ _ _ _ volatile _ _ (           \\                  \" movapd % 1 , % % xmm1 \\n\\t \"     \\                  \" movapd % 2 , % % xmm2 \\n\\t \"     \\                  \" mulpd % % xmm0 , % % xmm2 \\n\\t \" \\                  \" addpd % % xmm1 , % % xmm2 \\n\\t \" \\                  \" movapd % % xmm2 , % 0 \"          \\                  :                            \\              / * store to address ( r ) , which is indexed as % 0 * /   \\                  \" = m \" ( r )                     \\                  :                            \\              / * load from address ( r ) and ( v ) , which are   \\                 indexed as % 1 and % 2 , respectively * /      \\                  \" m \" ( r ) , \" m \" ( v ) )            # define align16 _ _ attribute _ _ ( ( aligned ( 16 ) ) )            void vadd_(int * n , double * coeff , double * r , double * v )          {              int i , len ;              static double cc[2 ] align16 ;              / * the array cc is aligned to 16-byte boundary * /                cc[0 ] = cc[1 ] = * coeff ;              sse_load(cc[0 ] ) ;              len = ( * n)/2 ;              for ( i=0 ; i < len*2 ; i+=2 ) {                  sse_add(r[i ] , v[i ] ) ;              }              if ( * n % 2 ! = 0 )                  r[len*2 ] = r[len*2 ] + cc[0 ] * v[len*2 ] ;          } ....    note that we have added the keyword `` _ _ volatile _ _ '' ( an gnu extension ) in the macro `` _ _ asm _ _ '' . its purpose is to ensure that the compiler does not rearrange the order of execution of the codes during compilation .",
    "finally , all object modules are linked by @xmath72 with the option `` -lg2c '' .      in our lattice qcd program ,",
    "most of the execution time is spent in solving quark propagators via the nested cg loops .",
    "thus the execution time is dominated by the operation @xmath14 times @xmath75 , which is performed many times ( @xmath76 in most cases ) before the final results of quark propagators can be obtained .",
    "thus it is crucial to optimize this operation with sse2 codes .",
    "first , we have to set up the correspondence between the data structures used by c and fortran routines in our program , in particular , for the link variables and the relevant vectors in the nested cg loops .",
    "suppose we write the arrays of link variables and a column vector @xmath77 in the syntax of fortran as u(i , j,,x ) , + v(i , k , x ) , where @xmath78 and @xmath79 are the color indices , @xmath80 is the space - time direction , @xmath41 is the spinor index , and @xmath81 is the site index . now",
    "the question is how to access the elements of these arrays in c routines . to resolve this problem",
    ", we define some data structures in c as follows .    ....      / * su(3 ) matrix , ( c01,c02 ) forms the complex number of u11 , and                       ( c03,c04 ) of u21 , etc .                            * /      typedef struct {          double c01 , c02 , c03 , c04 , c05 , c06 ;          double c07 , c08 , c09 , c10 , c11 , c12 ;          double c13 , c14 , c15 , c16 , c17 , c18 ;      } su3_t ;        / * there are 4 link variables at each site .",
    "* /      typedef struct {          su3_t mu1 , mu2 , mu3 , mu4 ;      } ulink_t ;        / * su(3 ) vector , ( c1,c2 ) forms the complex number of v1 ,                       ( c3,c4 ) of v2 , and ( c5,c6 ) of v3 . * /      typedef struct {          double c1 , c2 , c3 , c4 , c5 , c6 ;      } vector_t ;        / * su(3 ) dirac spinor .",
    "* /      typedef struct {          vector_t s1 , s2 , s3 , s4 ;      } spinor_t ; ....    then the correspondence can be easily established .",
    "for example , the elements @xmath82 and @xmath83 can be accessed by c routines as ( @xmath84$].mu1.c11 , @xmath84$].mu1.c12 ) and ( @xmath85$].s4.c3 , @xmath85$].s4.c4 ) respectively .",
    "now we rewrite @xmath14 as h_w(x , y ) = _ 5 \\ { ( 4-m_0 ) _ x , y + _ = 1 ^ 4 [ ( -1 + _ ) u_(x ) _ x+,y - ( 1 + _ )",
    "u^_(x- ) _ x-,y ] }    then the multiplication of @xmath14 to a column vector @xmath75 can be optimized by minimizing the number of multiplications involving the link variables .",
    "for example , the multiplication in @xmath86 can be written as ( in the spinor space ) ( -+ _ 1 ) u |v > = (    c r_1 + r_2 + r_3 + r_4    ) = (    cccc -u & 0 & 0 & u + 0 & -u & u & 0 + 0 & u & -u & 0 + u & 0 & 0 & -u    ) (    c v_1 + v_2 + v_3 + v_4    ) = (    c u(v_4-v_1 ) + u(v_3-v_2 ) + -r_2 + -r_1    ) where all indices are suppressed except the spinor indices .",
    "it is clear that the vectors @xmath87 and @xmath88 should be computed first , before they are multiplied by link variable @xmath89 ( generic symbol for @xmath90 ) .",
    "for example , the operation @xmath91 can be performed by the following macros with sse2 .    .... # define mvpv(v1 , v2 ) \\      _ _ asm _ _ _ _ volatile _ _ ( \\          \" movapd % 0 , % % xmm0 \\n\\t \" \\          \" movapd % 1 , % % xmm1 \\n\\t \" \\          \" movapd % 2 , % % xmm2 \\n\\t \" \\          \" subpd % 3 , % % xmm0 \\n\\t \" \\          \" subpd % 4 , % % xmm1 \\n\\t \" \\          \" subpd % 5 , % % xmm2 \" \\          : : \\          \" m \" ( ( v2).c1 ) , \\          \" m \" ( ( v2).c3 ) , \\          \" m \" ( ( v2).c5 ) , \\          \" m \" ( ( v1).c1 ) , \\          \" m \" ( ( v1).c3 ) , \\          \" m \" ( ( v1).c5 ) ) ....    similarly , we have ( -- _ 1 ) u^ |v > & = & (    c r_1 + r_2 + r_3 + r_4    ) = (    c -u^(v_4+v_1 ) + -u^(v_3+v_2 ) + r_2 + r_1    ) , + ( -+ _ 2 ) u |v > & = & (    c r_1 + r_2 + r_3 + r_4    ) = (    c -u(v_1+iv_4 ) + -ir_3 + -u(v_3+iv_2 ) + -ir_1    ) , + ( -- _ 2 ) u^ |v > & = & (    c r_1 + r_2 + r_3 + r_4    ) = (    c -ir_4 + -u^(v_2+iv_3 ) + -ir_2 + -u^(v_4+iv_1 )    ) , + ( -+ _ 3 ) u |v > & = & (    c r_1 + r_2 + r_3 + r_4    ) = (    c u(v_3-v_1 ) + -u(v_2+v_4 ) + -r_1 + r_2    ) , + ( -- _ 3 ) u^ |v > & = & (    c r_1 + r_2 + r_3 + r_4    ) = (    c -u^(v_1+v_3 ) + u^(v_4-v_2 ) + r_1 + -r_2    ) , + ( -+ _ 4 ) u |v > & = & (    c r_1 + r_2 + r_3 + r_4    ) = (    c -ir_3 + -ir_4 + -u(v_3+iv_1 ) + -u(v_4+iv_2 )    ) , + ( -- _ 4 ) u^ |v > & = & (    c r_1 + r_2 + r_3 + r_4    ) = (    c -u^(v_1+iv_3 ) + -u^(v_2+iv_4 ) + -ir_1 + -ir_2    ) .",
    "so the multiplications involving the link variables can be implemented as    ....",
    "/ * for each lattice size * /      for ( x=0 ; x < ldim ; x++ ) {      / * prefetch for the current multiplication * /          y = iup[x].mu1 - 1 ;          _ prefetch_su3(&(u[x].mu1 ) ) ;          _ prefetch_spinor(&(v[y ] ) ) ;      / * prefetch for the next multiplication * /          z = idn[x].mu1 - 1 ;          _ prefetch_su3(&(u[z].mu1 ) ) ;          _ prefetch_spinor(&(v[z ] ) ) ;      / * r1.s1 = u[x].mu1 * ( v[y].s4 - v[y].s1 ) * /          mvpv(v[y].s1 , v[y].s4 ) ;          su3mul(r1.s1 , u[x].mu1 ) ;      / * r1.s2 = u[x].mu1 * ( v[y].s3 - v[y].s2 ) * /          mvpv(v[y].s2 , v[y].s3 ) ;          su3mul(r1.s2 , u[x].mu1 ) ;      / * r1.s3 = -r1.s2 * /          mvset(r1.s3 , r1.s2 ) ;      / * r1.s4 = -r1.s1 * /          mvset(r1.s4 , r1.s1 ) ;        / * prefetch for the next multiplication * /          y = iup[x].mu2 - 1 ;          _ prefetch_su3(&(u[x].mu2 ) ) ;          _ prefetch_spinor(&(v[y ] ) ) ;      / * r2.s1 = -(u[x].mu1)^{\\dagger } * ( v[y].s1 + v[y].s4 ) * /          mvmv(v[z].s1 , v[z].s4 ) ;          su3hmul(r2.s1 , u[z].mu1 ) ;      / * r2.s2 = -(u[x].mu1)^{\\dagger } * ( v[y].s2 + v[y].s3 ) * /          mvmv(v[z].s2 , v[z].s3 ) ;          su3hmul(r2.s2 , u[z].mu1 ) ;      / * r2.s3 = r2.s2 * /          pvset(r2.s3 , r2.s2 ) ;      / * r2.s4 = r2.s1 * /          pvset(r2.s4 , r2.s1 ) ;          ... ....    where * r1 * , * r2 * ,  , and * v [ ] * are declared as the type * spinor_t * , and * u [ ] * is declared as the type * ulink_t*. note that prefetching has been inserted in order to attain the optimal performance . finally , we have 8 vector segments * r1 * ,  , * r8 * , and a diagonal term . they are summed over to give the final result of @xmath92 $ ] , v[y].s1 & = & r1.s1 + r2.s1 + r3.s1 + r4.s1 + r5.s1 + r6.s1 + r7.s1 + r8.s1 + & & + ( 4-m_0 ) * v[x].s1 , + v[y].s2 & = & r1.s2 + r2.s2 + r3.s2 + r4.s2 + r5.s2 + r6.s2 + r7.s2 + r8.s2 + & & + ( 4-m_0 ) * v[x].s2 , + v[y].s3 & = & -(r1.s3 + r2.s3 + r3.s3 + r4.s3 + r5.s3 + r6.s3 + r7.s3 + r8.s3 + & & + ( 4-m_0 ) * v[x].s3 ) , + v[y].s4 & = & -(r1.s4 + r2.s4 + r3.s4 + r4.s4 + r5.s4 + r6.s4 + r7.s4 + r8.s4 + & & + ( 4-m_0 ) * v[x].s4 ) ,",
    "next we come to the question how to implement sse2 codes for a @xmath93 matrix times a vector , the most crucial part in @xmath14 times @xmath94 .",
    "this problem has been solved by lscher @xcite , and his sse2 codes is available in the public domain @xcite .",
    "we found that lscher s code is quite efficient , and have adopted it in our program . for completeness , we briefly outline lscher s algorithm as follows .    consider (    ccc u_11 & u_12 & u_13 + u_21 & u_22 & u_23 + u_31 & u_32 & u_33    ) (    c y_1 + y_2 + y_3    ) = (    c r_1 + r_2 + r_3    ) + .",
    "first , the elements ( @xmath95 ) of the vector @xmath96 are copied to the registers % xmm0 , % xmm1 , and % xmm2 , respectively .",
    "then the real part of the su(3 ) matrix @xmath97 is read sequentially , and is multiplied to @xmath96 at % xmm0 , % xmm1 , and % xmm2 , and the result is stored at % xmm3 , % xmm4 , and % xmm5 , % & = & ( ( y_1 ) , ( y_1 ) ) , % & = & ( ( y_2 ) , ( y_2 ) ) , % & = & ( ( y_3 ) , ( y_3 ) ) , % & = & ( t_1 , t_2 ) , % & = & ( t_3 , t_4 ) , % & = & ( t_5 , t_6 ) , where t_1 & = & ( u_11)(y_1)+(u_12)(y_2)+(u_13)(y_3),t_2 & = & ( u_11)(y_1)+(u_12)(y_2)+(u_13)(y_3),t_3 & = & ( u_21)(y_1)+(u_22)(y_2)+(u_23)(y_3),t_4 & = & ( u_21)(y_1)+(u_22)(y_2)+(u_23)(y_3),t_5 & = & ( u_31)(y_1)+(u_32)(y_2)+(u_33)(y_3),t_6 & = & ( u_31)(y_1)+(u_32)(y_2)+(u_33)(y_3 ) .",
    "next , multiply the vector @xmath98 by @xmath99 , i.e. ,    lclcl % & & ( ( y_1 ) , ( y_1 ) ) & & ( -(y_1 ) , ( y_1 ) ) , + % & & ( ( y_2 ) , ( y_2 ) ) & & ( -(y_2 ) , ( y_2 ) ) , + % & & ( ( y_3 ) , ( y_3 ) ) & & ( -(y_3 ) , ( y_3 ) ) ,    which is implemented by the following sse2 code    ....      static int sn3[4 ] align16 = { 0x0,0x80000000,0x0,0x0 } ;      # define su3mul(r , u ) \\          ...                      \\          \" xorpd % 9 , % % xmm0 \\n\\t \" \\          \" xorpd % 9 , % % xmm1 \\n\\t \" \\          \" xorpd % 9 , % % xmm2 \\n\\t \" \\          ...                      \\          : :                       \\          ...                      \\          \" m \" ( sn3[0 ] ) ) ; ....    then the imaginary part of @xmath97 is read and multiplied to @xmath100 , and the final result is % & = & ( t_1+s_1 , t_2+s_2 ) , + % & = & ( t_3+s_3 , t_4+s_4 ) , + % & = & ( t_5+s_5 , t_6+s_6 ) , where s_1 & = & -(u_11)(y_1)-(u_12)(y_2)-(u_13)(y_3 ) , s_2 & = & + ( u_11)(y_1)+(u_12)(y_2)+(u_13)(y_3 ) , s_3 & = & -(u_21)(y_1)-(u_22)(y_2)-(u_23)(y_3 ) , s_4 & = & + ( u_21)(y_1)+(u_22)(y_2)+(u_23)(y_3 ) , s_5 & = & -(u_31)(y_1)-(u_32)(y_2)-(u_33)(y_3 ) , s_6 & = & + ( u_31)(y_1)+(u_32)(y_2)+(u_33)(y_3 ) .    . the execution time ( in unit of second ) for @xmath14 multiplying a column vector @xmath42 , with sse2 turned on and off . the test is performed at a pentium 4 ( 2 ghz ) node . [ cols=\"^,^,^,^\",options=\"header \" , ]",
    "in this paper , we outline the essential features of a linux pc cluster ( 64 nodes ) which has been built at national taiwan university , and discuss how to optimize its hardware and software for lattice qcd with exact chiral symmetry . at present",
    ", all nodes are working around the clock on lattice qcd computations .    with zolotarev optimal rational approximation to @xmath101 , projections of high and low - lying eigenmodes of @xmath102 , the multi - mass cg algorithm , the sse2 acceleration , and our simple scheme of memory management , we are able to compute quark propagators of @xmath103 bare quark masses on the @xmath104 lattice , with the precision of quark propagators up to @xmath105 and the precision of exact chiral symmetry up to @xmath106 , at the rate of 2.5 gauge configuration ( @xmath54 ) per day , with our present system of 64 nodes .",
    "this demonstrates that an optimized linux pc cluster can be a viable computational system to extract physical quantities from lattice qcd with exact chiral symmetry @xcite .",
    "the speed of our system is higher than 70 gflops , and the total cost of the hardware is less than us$60000 .",
    "this amounts to price / performance ratio better than $ 1.0/mflops for 64-bit ( double precision ) computations .",
    "the basic idea of optimization is to let each node work independently on one of the 12 columns of the quark propagators ( for a set of bare quark masses ) , and also use the hard disk as the virtual memory for the vectors in the outer cg loop , while the cpu is working on the inner cg loop .",
    "our simple scheme of memory management for the nested cg loops may also be useful to other systems .    in future , we will add more nodes to our system , and will also work on larger lattices , say @xmath107 .",
    "then one gbyte memory at each node is not sufficient to accommodate all relevant vectors in the inner cg loop , even for 12 zolotarev terms .",
    "however , there are several ways to circumvent this problem .",
    "first , our memory management scheme is quite versatile , which is more than just for swapping the vectors at the interface of inner and outer cg loops .",
    "in fact , it can handle any number of zolotarev terms for any lattice size , and can automatically minimize disk i / o at any step of the nested cg loops , according to the amount of physical memory of a node . as long as the percentage of the disk i / o time is less than 30% , it is still a better option than distributing the nested cg loops across the nodes and performing parallel computations ( with mpi ) through a fast network switch , since the communication overheads is expected to be more than 30% of the total time , especially for a system of 100 nodes or more .",
    "secondly , we can increase the amount of memory at each node , which depends on the specification of the motherboard as well as the price and the capacity of the memory modules .",
    "finally , we can also exploit algorithms @xcite which only use five vectors rather than @xmath47 vectors for the inner cg loop , or the lanczos algorithm as described in ref .",
    "now it is clear that a linux pc cluster is a viable platform to tackle lattice qcd with exact chiral symmetry even for a large lattice ( e.g. , @xmath108 ) , though more studies are needed before one reaches an optimal design for dynamical quarks .",
    "recently , it has been shown @xcite that the speed of neuberger s double pass algorithm @xcite for computing the matrix - vector product @xmath109 is almost independent of the degree @xmath48 of the rational polynomial , and it is faster than the single pass for @xmath110 ( for pentium 4 with sse2 ) .",
    "thus the single pass has been replaced with the double pass algorithm in our linux pc cluster .",
    "t.  w.  chiu , `` a parallel computer for lattice gauge theories , '' proceedings of the third conference on hypercube concurrent computers and applications , edited by g.c .",
    "fox , published by acm , new york , n.y.(1988 ) p. 81",
    "d.  b.  kaplan , phys .",
    "b * 288 * , 342 ( 1992 ) h.  neuberger , phys .",
    "b * 417 * , 141 ( 1998 ) ; r.  narayanan and h.  neuberger , nucl .",
    "b * 443 * , 305 ( 1995 ) m.  creutz , phys .",
    "d * 21 * ( 1980 ) 2308 .",
    "n.  cabibbo and e.  marinari , phys .",
    "b * 119 * ( 1982 ) 387 .",
    "e.  i.  zolotarev , `` application of elliptic functions to the questions of functions deviating least and most from zero '' , zap .",
    "petersburg , 30 ( 1877 ) , no .",
    "5 ; reprinted in his collected works , vol .",
    "2 , izdat , akad .",
    "nauk sssr , moscow , 1932 , p. 1 - 59 .",
    "n.  i.  akhiezer , _ theory of approximation _ , reprint of 1956 english translation ( dover , new york , 1992 ) ; _ elements of the theory of elliptic functions _ , translations of mathematical monographs , 79 ( american mathematical society , providence , r.i .",
    "1990 )    j.  van den eshof , a.  frommer , t.  lippert , k.  schilling and h.  a.  van der vorst , nucl .",
    "suppl .   * 106 * , 1070 ( 2002 ) t.  w.  chiu , t.  h.  hsieh , c.  h.  huang and t.  r.  huang , phys .  rev .",
    "d * 66 * , 114502 ( 2002 ) a.  frommer , b.  nockel , s.  gusken , t.  lippert and k.  schilling , int .",
    "j.  mod .",
    "c * 6 * , 627 ( 1995 ) b.  jegerlehner , `` krylov space solvers for shifted linear systems , '' hep - lat/9612014 .",
    "r.  lehoucq , d.  sorensen , c.  yang , `` arpack users guide : solution of large scale eigenvalue problems with implicitly restarted arnoldi methods '' , _ philadelphia : siam , 1998_."
  ],
  "abstract_text": [
    "<S> a computational system for lattice qcd with exact chiral symmetry is described . </S>",
    "<S> the platform is a home - made linux pc cluster , built with off - the - shelf components . at present </S>",
    "<S> the system constitutes of 64 nodes , with each node consisting of one pentium 4 processor ( 1.6/2.0/2.5 ghz ) , one gbyte of pc800/1066 rdram , one @xmath0 gbyte hard disk , and a network card . </S>",
    "<S> the computationally intensive parts of our program are written in sse2 codes . </S>",
    "<S> the speed of our system is estimated to be 70 gflops , and its price / performance ratio is better than $ 1.0/mflops for 64-bit ( double precision ) computations in quenched qcd . </S>",
    "<S> we discuss how to optimize its hardware and software for computing quark propagators via the overlap dirac operator .    </S>",
    "<S> pacs numbers : 11.15.ha , 11.30.rd , 12.38.gc + keywords : lattice qcd , overlap dirac operator , linux pc cluster    ntuth-02 - 505d + august 2002 +    2.5truecm    a linux pc cluster for lattice qcd with + exact chiral symmetry    1.0truecm    ting - wai chiu , tung - han hsieh , chao - hsi huang , tsung - ren huang    department of physics , national taiwan university    taipei , taiwan 106 , taiwan .    </S>",
    "<S> _ e - mail : twchiu@phys.ntu.edu.tw_    1.5 cm    1 </S>"
  ]
}