{
  "article_text": [
    "some @xmath0-body simulations can be sped up in various ways , by using faster algorithms such as tree codes @xcite and/or special purpose hardware such as the grape family @xcite .",
    "for some regimes , such as low @xmath0 values , these speed - up methods are not very efficient , and it would be nice to find other ways to improve the speed of such calculations",
    ". it would be even better if these alternative ways can be combined with other methods of speed - up .",
    "we explore here a general approach based on speeding up the inner loop of gravitational force calculations , namely the interactions between one pair of particles . also when using tree codes this approach will be useful , since in that case the calculation cost is still dominated by force calculations . even for grape applications ,",
    "this approach will still be useful in many cases , since there are always some calculations which are done more efficiently on the front end .",
    "in particular , we consider the optimization of the inner force loop on the x86 - 64 ( or amd64 or em64 t ) architecture , the newest incarnation of the architecture that originated with the intel 8080 microprocessor .",
    "processors with an x86 - 64 instruction set are currently the most widely used .",
    "athlon 64 and opteron microprocessors from amd , and many recent models of pentium 4 and xeon microprocessors from intel , support this instruction set .",
    "as will be shown in section [ sec : baseline ] , a straightforward implementation of the inner force loop using either fortran or c , compiled with standard compilers like gcc or icc for x86 - 64 processors , results in a performance that is significantly lower than the theoretical peak value one can expect from the hardware . in the following ,",
    "we discuss how we can improve the performance of the force loop on processors with an x86 - 64 instruction set .",
    "our approach is based on the use of new features added to the x86 microprocessors in the last eight years .",
    "the first one is the sse2 instruction set for double - precision floating - point arithmetic .",
    "traditionally , the instruction set for x86 microprocessors has included the so - called x87 instruction set , which was originally designed for the 8087 math coprocessor for the 8086 16-bit microprocessor .",
    "this instruction set is stack - based , in the sense that it does not have any explicit way to specify registers .",
    "instead , registers are indirectly accessed as a stack , where two operands for an arithmetic operation are taken from the top of the stack ( popped ) and the result is placed back at the top of the stack ( pushed ) .",
    "memory access also takes place through the top of the stack .",
    "this x87 instruction set had the advantage that the instructions are simple and few in number , but for the last fifteen years the design of a fast floating - point unit for this x87 instruction set has been a major problem for all x86 microprocessors .",
    "if one would really design stack - based hardware , any pipelining would be practically impossible . in order to allow pipelining , current x86-based microprocessors , from intel as well as amd ,",
    "translate the stack - based x87 instructions to risc - like , presumably standard three - address register - to - register instructions in hardware at execution time .",
    "this approach has given quite high performance , certainly much higher than what would have been possible with the original stack - based implementation .",
    "however , it was clear that pipelining and better use of hardware registers would be much easier if one could use an instruction set with explicit reference to the registers . in 2001 , with the introduction of pentium 4 microprocessors , intel added such a new floating - point instruction set , which is called sse2 .",
    "it is still not a real three - address instruction set ; it rather uses a two - address form where the address of the source register and that of the destination register are the same .",
    "moreover , sse2 still supports operations between the data in the main memory and that in the register , as was the case with the ibm system/360 .",
    "thus , it still has the look and feel of an instruction set from the 1960s .",
    "even though operations between operands in memory and operands in registers are supported , clearly execution would be much faster if all operands could reside in registers .",
    "however , with the original sse2 instruction set it was difficult to eliminate memory access for intermediate results , because there were only eight registers available for sse2 instructions . for whatever reason ,",
    "these registers are called `` xmm '' registers in the manufacturer s document , and we follow this convention . with the new x86_64 instruction set , the number of these `` xmm '' registers was doubled from 8 to 16 .",
    "the implication for @xmath0-body calculations was that it now became possible to minimize memory access during the inner force loop .",
    "a form of optimization using this approach will be discussed in section [ sec : c - level ] .",
    "another important feature of sse2 ( which is actually streaming simd extension 2 ) is that it is defined to operate on a pair of two 64-bit floating point words , instead of a single floating - point word .",
    "this effectively means that the use of sse2 instructions automatically result in the execution of two floating - point operations in parallel .",
    "while this feature can not be easily used with any compiler - based optimization , it is possible to gain considerable profit from this feature through judicious hand coding .",
    "we discuss the use of this parallel nature of the sse2 instruction set in section [ sec : sse2 ] .",
    "sse2 is not the only new floating - point instruction set that has been made available for the x86 hardware .",
    "as the name sse2 already suggests , there is an earlier sse instruction set , which is similar to sse2 but works only on single - precision floating - point numbers . as is the case with sse2",
    ", sse also works on multiple data in parallel , but instead of the two double - precision data of sse2 , sse works on four single - precision floating - point numbers simultaneously .",
    "thus , the peak calculation speed of sse is at least a factor of two higher than that of sse2 . for those force calculations where single precision gives us a sufficient degree of accuracy",
    ", we can make use of sse , gaining a performance that is even higher than what would have been possible with sse2 , as we discuss in section [ sec : mixed ] .",
    "sse was designed mainly to speed up coordinate transformation in three - dimensional graphics . as a result",
    ", it has a special instruction for the very fast calculation of an approximate inverse square root , which is intended as a good initial value for newton - raphson iteration .",
    "this is exactly what we need for the calculation of gravitational forces .",
    "we discuss the use of this approximate inverse square root for double - precision calculation , in section [ sec : sse2 ] .",
    "this paper is organized as follows . in section [ sec :",
    "baseline ] , we give the standard c - language implementation of the force loop .",
    "we consider the code fragment which calculates both the acceleration and its first time derivative .",
    "it is used with hermite integration scheme @xcite .",
    "we present the assembly language output and measured performance , and describe possible room for improvements .",
    "we call these implementations the baseline implementation .    in section [ sec",
    ": c - level ] , we discuss the optimized c - language implementation of the force loop for the hermite scheme .",
    "the difference from the baseline implementation is that the c - language code is hand - tuned so that the load and store of intermediate results are eliminated from the generated assembly - language output .",
    "this version gives us the speed - up of 46% compared to the baseline method .    in section 4",
    ", we discuss a more efficient use of sse2 instruction , where forces on two particles are calculated in parallel using the simd nature of the sse2 instruction set . here",
    ", we use the intrinsic data types defined in gcc , which allows us to use this simd nature within the syntax of c - language .",
    "also , we use the fast approximate square root instruction and a newton - raphson iteration .",
    "this implementation is 88% faster than baseline .    in section 5",
    ", we discuss a mixed sse - sse2 implementation of the force calculation for the hermite scheme . in many applications ,",
    "full double - precision accuracy is not necessary , except for the first subtraction between positions and final accumulation of the forces .",
    "`` high - accuracy '' grape hardwares ( grape-2 , 4 and 6 ) rely on this mixed - accuracy calculation .",
    "thus , it is possible to perform most of the force - calculation operations using sse single - precision instructions , thereby further speeding up the force calculation . in this way",
    ", we can speed - up the calculation by another 67 % from the sse2 parallel implementation of section 4 , achieving 219% speedup ( 3.19 times faster ) from the baseline implementation .",
    "the target functions that we want to calculate are :    @xmath3 \\\\",
    "\\phi_i & = & -\\sum_j { m_j \\over ( r_{ij}^2 + \\varepsilon^2)^{1/2}}\\end{aligned}\\ ] ]    where @xmath4 and @xmath5 are the gravitational acceleration and the potential of particle @xmath6 , the jerk @xmath7 is the time derivative of the acceleration , and @xmath8 , @xmath9 , and @xmath10 are the position , velocity and mass of particle @xmath6 , @xmath11 and @xmath12 .",
    "the calculation of @xmath13 and @xmath14 requires 9 multiplications , 10 addition / subtraction operations , 1 division and 1 square root calculation . clearly , calculation of an inverse square root is more expensive than addition / multiplication operations .",
    "therefore , if we want to measure the speed of a force calculation loop in terms of the number of floating - point operations per second , we need to introduce some conversion factor for division and square root calculations . in this paper",
    ", we use 38 as the total number of floating point operations for the calculation of @xmath13 and @xmath14 .",
    "this implies that we effectively assign around 10 operations to each division and to each square root calculation .",
    "this particular convention was introduced by @xcite , and we follow it here since it seems to be a reasonable representation of the actual computational costs of force calculations on typical scalar processors .    in addition , it requires 11 multiplications and 11 addition / subtraction operations to calculate @xmath15 .",
    "thus , the total number of floating - point operations per inner force loop for the hermite scheme can be given as 60 .",
    "this is the number that we will use here .",
    "note that we have previously used numbers that were slightly less , by about 5% , using 57 instead of 60 @xcite .",
    "in the case of a simple leapfrog method or traditional `` aarseth '' scheme , we need to calculate only @xmath4 and @xmath5 . for a hermite scheme we need to determine @xmath7 as well .",
    "the following code fragments contain what we regard as the baseline implementation of the force calculations , where we use the word ` force ' loosely , to indicate the calculations for accelerations , jerks as well as the potential .",
    "in other words , we consider ` force calculations ' to comprise all the basic low - level dynamical calculations governing the interactions between pairs of particles .    ....",
    "# define dim 3    typedef struct {      double x[dim ] ;      double v[dim ] ;      double m ; } predictor ; typedef predictor * ppredictor ;    typedef struct {      double a[dim ] ;      double j[dim ] ;      double pot ; } accjerk ; typedef accjerk * paccjerk ;    void calcaccjerk_base ( ppredictor pr , paccjerk aj ,                          double eps2 , int i , int nbody ) {      int j , k ;      double r[3 ] , v[3 ] , acc[3 ] , jerk[3 ] , pot ;      double r2 , rv ;      double rinv , rinv2 ;      double mrinv , mrinv3 , rvrinv2 ;        pot = 0.0 ;      for(k=0 ; k<3 ; k++ ) acc[k ] = jerk[k ] = 0.0 ;        for(j=0 ; j < nbody ; j++ ) {          if(j = = i ) continue ;          for(k=0 ; k<3 ; k++ ) {              r[k ] = pr[j].x[k ] - pr[i].x[k ] ;              v[k ] = pr[j].v[k ] - pr[i].v[k ] ;          }            r2 = eps2 ;          for(k=0 ; k<3 ; k++ ) r2 + = r[k ] * r[k ] ;          rv = r[0 ] * v[0 ] ;          for(k=1 ; k<3 ; k++ ) rv + = r[k ] * v[k ] ;            rinv2 = 1./r2 ;          rinv = sqrt(rinv2 ) ;          rvrinv2 = 3.0 * rv * rinv2 ; ;          mrinv = rinv * pr[j].m ;          mrinv3 = mrinv * rinv2 ;            pot -= mrinv ;            for(k=0 ; k<3 ; k++ ) {              double temp = r[k ] * mrinv3 ;              acc [ k ] + = temp ;              jerk[k ] + = v[k ] * mrinv3 - temp * rvrinv2 ;          }      }        for(k=0 ; k<3 ; k++ ) {          aj->a[k ] = acc[k ] ;          aj->j[k ] = jerk[k ] ;          aj->pot = pot ;      } } ....    line 29 in list [ c : baseline ] is a branch to avoid self interaction .",
    "we might remove this branch when we can use softening , but in this case branch prediction of the microprocessor works ideally , hence the performance changes little if we remove this .",
    "l > l r r & & & + gcc 3.3.1 & -o3 -ffast - math -funroll - loops & 94.8 & 1.27 + gcc 3.3.1 & -o3 -ffast - math -funroll - loops -mfpmath=387 & 119 & 1.01 + gcc 4.0.1 & -o3 -ffast - math -funroll - loops & 100 & 1.20 + pgi 5.1 & -fastsse & 97.6 & 1.23 + pathscale 2.0 & -o3 -ffast - math & 95.0 & 1.26 + icc 9.0 & -o3 & 95.8 & 1.25 +    table [ table : baseline ] shows the performance of this code on an amd athlon 64 3000 + ( 2.0ghz ) processor with several different compilers .",
    "the first column gives the compiler used , the second the compiler options , the third the clock cycles per pairwise force calculation , and the fourth column gives the speed in gflops .",
    "all compilers generate sse2 instructions instead of x87 instructions unless we explicitly set options to use x87 .",
    "the performance is fairly good , but not ideal . in the following",
    "we will investigate how we can improve the performance .",
    "list [ asm : baseline ] is the assembly language output of the hermite force loop using the gcc compiler :    ....      xorpd    % xmm9 , % xmm9      xorl     % r8d , % r8d      subq     $ 16 , % rsp .lcfi0 :",
    "movsd    % xmm0 , % xmm10      movl     % edx , % r11d      cmpl     % ecx , % r8d      movl     % ecx , % r10d      movsd    % xmm9 , -120(%rsp )      movsd    % xmm9 , -88(%rsp )      movsd    % xmm9 , -112(%rsp )      movsd    % xmm9 , -80(%rsp )      movsd    % xmm9 , -104(%rsp )      movsd    % xmm9 , -72(%rsp )      jge .l41",
    "movslq   % edx,%rdx      movlpd   .lc4(%rip ) , % xmm11      movlpd   .lc5(%rip ) , % xmm12      leaq     0(,%rdx,8 ) , % r9      subq     % rdx , % r9      .p2align 4,,7 .l60 :      cmpl     % r11d , % r8d      je   .l9                          # continue      movslq   % r8d,%rcx      leaq     0(,%rcx,8 ) , % rdx      subq     % rcx , % rdx               # % rdx = 7 * % rcx      leaq     8(,%r9,8 ) , % rcx      movlpd   ( % rdi,%rdx,8 ) , % xmm6     # % xmm6 = pr[j].x[0 ]      leaq     8(,%rdx,8 ) , % rax      subsd    ( % rdi,%r9,8 ) , % xmm6      # % xmm6 -= pr[i].x[0 ]      movsd    % xmm6 , -24(%rsp )         # * ( % rsp-24 ) = % xmm6      movsd    % xmm6 , % xmm3      movlpd   24(%rdi,%rdx,8 ) , % xmm4      subsd    24(%rdi,%r9,8 ) , % xmm4    # % xmm4 = pr[j].v[0 ] - pr[i].v[0 ]      mulsd    % xmm6 , % xmm3             # % xmm3 = x*x      addsd    % xmm10 , % xmm3            # % xmm3 + = eps2      movsd    % xmm4 , -56(%rsp )      movlpd   ( % rdi,%rax ) , % xmm7      subsd    ( % rdi,%rcx ) , % xmm7       # % xmm7 = pr[j].x[1 ] - pr[i].x[1 ]      movsd    % xmm7 , -16(%rsp )      movsd    % xmm7 , % xmm15      movsd    % xmm7 , % xmm2      movlpd   24(%rdi,%rax ) , % xmm14      leaq     16(,%rdx,8 ) , % rax      subsd    24(%rdi,%rcx ) , % xmm14    # % xmm14 = pr[j].v[1 ] - pr[i].v[1 ]      mulsd    % xmm7 , % xmm15      leaq     16(,%r9,8 ) , % rcx      addsd    % xmm15 , % xmm3            # % xmm3 + = y*y      movlpd   -88(%rsp ) , % xmm15        # load acc[0 ]      movsd    % xmm14 , -48(%rsp )      mulsd    % xmm14 , % xmm2      movlpd   ( % rdi,%rax ) , % xmm8      subsd    ( % rdi,%rcx ) , % xmm8       # % xmm8 = pr[j].x[2 ] - pr[i].x[2 ]      movsd    % xmm8 , % xmm5      movsd    % xmm8 , -8(%rsp )      movsd    % xmm8 , % xmm0      movlpd   24(%rdi,%rax ) , % xmm13      mulsd    % xmm8 , % xmm5      subsd    24(%rdi,%rcx ) , % xmm13    # % xmm13 = pr[j].v[2 ] - pr[i].v[2 ]      addsd    % xmm5 , % xmm3             # % xmm3 + = z*z      movsd    % xmm6 , % xmm5      mulsd    % xmm4 , % xmm5             # % xmm5 = x*vx      movsd    % xmm13 , -40(%rsp )      mulsd    % xmm13 , % xmm0      addsd    % xmm2 , % xmm5             # % xmm5 + = y*vy      movsd    % xmm11 , % xmm2            # % xmm2 = 1.0      divsd    % xmm3 , % xmm2             # % xmm2 = 1.0/r2      movlpd   -80(%rsp ) , % xmm3      addsd    % xmm0 , % xmm5             # % xmm5 + = z*vz      sqrtsd   % xmm2 , % xmm1             # % xmm1 = 1.0/r      mulsd    % xmm2 , % xmm5             # % xmm5 = rv / r2      mulsd    48(%rdi,%rdx,8 ) , % xmm1   # % xmm1 * = pr[j].m      mulsd    % xmm12 , % xmm5            # % xmm5 = 3rv / r2      mulsd    % xmm1 , % xmm2             # % xmm2 = m / r3      subsd    % xmm1 , % xmm9             # pot -= m / r      movlpd   -72(%rsp ) , % xmm1      mulsd    % xmm2 , % xmm6      mulsd    % xmm2 , % xmm7      mulsd    % xmm2 , % xmm8      mulsd    % xmm2 , % xmm4      addsd    % xmm6 , % xmm15            # acc[0 ] + = x[0 ] * m / r3      mulsd    % xmm2 , % xmm14      addsd    % xmm7 , % xmm3      mulsd    % xmm5 , % xmm6      addsd    % xmm8 , % xmm1      mulsd    % xmm5 , % xmm7      mulsd    % xmm2 , % xmm13      mulsd    % xmm5 , % xmm8      movsd    % xmm15 , -88(%rsp )        # store acc[0 ]      subsd    % xmm6 , % xmm4      movsd    % xmm3 , -80(%rsp )         # store acc[1 ]      subsd    % xmm7 , % xmm14      addsd    -120(%rsp ) , % xmm4      movsd    % xmm1 , -72(%rsp )         # store acc[2 ]      addsd    -112(%rsp ) , % xmm14      subsd    % xmm8 , % xmm13      addsd    -104(%rsp ) , % xmm13      movsd    % xmm4 , -120(%rsp )        # store jerk[0 ]      movsd    % xmm14 , -112(%rsp )       # store jerk[1 ]      movsd    % xmm13 , -104(%rsp )       # store jerk[2 ] .l9 :      incl     % r8d      cmpl     % r10d , % r8d      jl   .l60                         # if(++j < nbody ) goto .l60 : .l41 :      movq     -88(%rsp ) , % r11      movq     -120(%rsp ) , % r10      movsd    % xmm9 , 48(%rsi )      movq     -80(%rsp ) , % r9      movq     -112(%rsp ) , % r8      movq     -72(%rsp ) , % rdi      movq     -104(%rsp ) , % rdx      movq     % r11 , ( % rsi )      movq     % r10 , 24(%rsi )      movq     % r9 , 8(%rsi )      movq     % r8 , 32(%rsi )      movq     % rdi , 16(%rsi )      movq     % rdx , 40(%rsi )      addq     $ 16 , % rsp      ret ....    one can see that there are quite a few unnecessary load / store instructions .",
    "for example , the store instructions in lines 32 , 38 , 41 , 51 , 56 and 64 of list [ asm : baseline ] serve no purpose . also arrays acc[3 ] and jerk[3 ] could be placed in registers like variable pot , but they are placed in memory instead .",
    "as we have seen in the previous section , the assembly language output shows a significant amount of unnecessary load / store instructions . in principle , if the compilers were clever enough they would be able to eliminate these unnecessary operations . in practice , we need to guide the compilers so that they do not generate unnecessary code .",
    "we have achieved significant speedup using the following two guiding principles :    1 .",
    "eliminate assignment to any array element in the force loop .",
    "reuse variables as much as possible in order to minimize the number of registers used .    apparently , present - day compilers are not clever enough to eliminate load / store operations , if elements of arrays are used as left - hand - side values .",
    "therefore , we hand - unroll all loops of length three and use scalar variables instead of arrays for three - dimensional vectors .",
    "in well - written programs , variables which contain the values for different physical quantities should have different names .",
    "however , the use of many different names prevents optimization , since it results in a number of variables too large to be fitted into the register set of sse2 instructions ( there are 16 `` xmm '' registers for sse2 instructions ) .",
    "therefore , we explicitly reuse variables such as x , y , z so that these can hold both the position difference components as well as the pairwise force components , at different times in the computation .",
    "the resulted c - language code is the following :    .... typedef struct {      double x[3 ] ;      double v[3 ] ;      double m ;      double pad ; } predictor ; typedef predictor * ppredictor ;    void calcaccjerk_fast ( ppredictor pr , paccjerk aj ,                          double eps2 , int i , int nj ) {      int j ;      double x , y , z , vx , vy , vz ;      double ax , ay , az , jx , jy , jz , pot ;      double r2,rv ;      double rinv , rinv2,rinv3 ;      ppredictor jpr = pr ;      ppredictor ipr = pr + i ;        pot = ax = ay = az = jx = jy = jz=0.0 ;        for(j=0;j < nj;j++,jpr++ ) {          _ _ builtin_prefetch(jpr+2 , 0 , 3 ) ;          if(j = = i ) continue ;          x = jpr->x[0 ] - ipr->x[0 ] ;          y = jpr->x[1 ] - ipr->x[1 ] ;          z = jpr->x[2 ] - ipr->x[2 ] ;            vx= jpr->v[0 ] - ipr->v[0 ] ;          vy= jpr->v[1 ] - ipr->v[1 ] ;          vz= jpr->v[2 ] - ipr->v[2 ] ;            r2 = eps2 + x*x + y*y + z*z ;            rv = vx*x + vy*y + vz*z ;            rinv2 = 1./r2 ;          rinv = sqrt(rinv2 ) ;          rv * = rinv2 * 3 . ;          rinv * = jpr->m ;          rinv3 = rinv * rinv2 ;            pot -= rinv ;            x * = rinv3 ; ax + = x ;          y * = rinv3 ; ay + = y ;          z * = rinv3 ; az + = z ;            vx * = rinv3 ; jx + = vx ;          vy * = rinv3 ; jy + = vy ;          vz * = rinv3 ; jz + = vz ;            x   * = rv ; jx -= x ;          y   * = rv ; jy -= y ;          z   * = rv ; jz -= z ;      }        aj->a[0 ] = ax ;      aj->a[1 ] = ay ;      aj->a[2 ] = az ;      aj->j[0 ] = jx ;      aj->j[1 ] = jy ;      aj->j[2 ] = jz ;      aj->pot = pot ; } ....    this code is compiled into the following assembly code using the gcc 3.3.1 compiler ( flag -o3 -ffast - math ) :    .... calcaccjerk_fast : .lfb3 :      movslq   % edx,%r8      xorpd    % xmm9 , % xmm9      salq     $ 6 , % r8      movsd    % xmm0 , -8(%rsp )      leaq     ( % r8,%rdi ) , % rax      movsd    % xmm9 , % xmm11      movsd    % xmm9 , % xmm10      xorl     % r8d , % r8d      movsd    % xmm9 , % xmm14      movsd    % xmm9 , % xmm13      cmpl     % ecx , % r8d      movsd    % xmm9 , % xmm12      movsd    % xmm9 , % xmm15      jge .l9",
    ".p2align 4,,7 .l11 :      cmpl     % edx , % r8d      prefetcht0   128(%rdi )",
    "je   .l4",
    "movlpd   ( % rdi ) , % xmm3      movlpd   8(%rdi ) , % xmm4      subsd    ( % rax ) , % xmm3      movlpd   16(%rdi ) , % xmm5      subsd    8(%rax ) , % xmm4      movlpd   24(%rdi ) , % xmm6      subsd    16(%rax ) , % xmm5      movlpd   32(%rdi ) , % xmm7      subsd    24(%rax ) , % xmm6      movlpd   40(%rdi ) , % xmm8      subsd    32(%rax ) , % xmm7      subsd    40(%rax ) , % xmm8      movsd    % xmm3 , % xmm0      movsd    % xmm4 , % xmm2      mulsd    % xmm3 , % xmm0      addsd    -8(%rsp ) , % xmm0      mulsd    % xmm4 , % xmm2      movsd    % xmm7 , % xmm1      mulsd    % xmm4 , % xmm1      addsd    % xmm2 , % xmm0      movsd    % xmm5 , % xmm2      mulsd    % xmm5 , % xmm2      addsd    % xmm2 , % xmm0      movsd    % xmm6 , % xmm2      mulsd    % xmm3 , % xmm2      addsd    % xmm1 , % xmm2      movsd    % xmm8 , % xmm1      mulsd    % xmm5 , % xmm1      addsd    % xmm1 , % xmm2      movlpd   .lc4(%rip ) , % xmm1      divsd    % xmm0 , % xmm1      sqrtsd   % xmm1 , % xmm0      mulsd    % xmm1 , % xmm2      mulsd    48(%rdi ) , % xmm0      mulsd    .lc5(%rip ) , % xmm2      mulsd    % xmm0 , % xmm1      subsd    % xmm0 , % xmm15      mulsd    % xmm1 , % xmm3      mulsd    % xmm1 , % xmm4      mulsd    % xmm1 , % xmm5      mulsd    % xmm1 , % xmm6      mulsd    % xmm1 , % xmm7      addsd    % xmm3 , % xmm12      mulsd    % xmm1 , % xmm8      addsd    % xmm4 , % xmm13      addsd    % xmm5 , % xmm14      mulsd    % xmm2 , % xmm3      addsd    % xmm6 , % xmm10      mulsd    % xmm2 , % xmm4      addsd    % xmm7 , % xmm11      mulsd    % xmm2 , % xmm5      addsd    % xmm8 , % xmm9      subsd    % xmm3 , % xmm10      subsd    % xmm4 , % xmm11      subsd    % xmm5 , % xmm9",
    ".l4 :      incl     % r8d      addq     $ 64 , % rdi      cmpl     % ecx , % r8d      jl   .l11 .l9 :      movsd    % xmm12 , ( % rsi )      movsd    % xmm13 , 8(%rsi )      movsd    % xmm14 , 16(%rsi )      movsd    % xmm10 , 24(%rsi )      movsd    % xmm11 , 32(%rsi )      movsd    % xmm9 , 40(%rsi )      movsd    % xmm15 , 48(%rsi )      ret ....     % xmm1 & rinv2 , rinv3 , y*vy , z*vz + % xmm2 & rv , y*y , z*z + % xmm3 & x + % xmm4 & y + % xmm5 & z + % xmm6 & vx + % xmm7 & vy + % xmm8 & vz + % xmm9 & jz + % xmm10 & jx + % xmm11 & jy + % xmm12 & ax + % xmm13 & ay + % xmm14 & az + % xmm15 & pot +    l > l r r & & & + gcc 3.3.1 & -o3 -ffast - math & 64.8 & 1.85 + gcc 4.0.1 & -o3 -ffast - math & 64.7 & 1.85 + icc 9.0 & -o3 & 79.4 & 1.51 +    [ table : scalar ]    we can see that now all load / store instructions of the intermediate results are eliminated . figure [ fig : regmap ] shows the use of registers and table [ table : scalar ] presents the performance data for this code .",
    "this version is 46% faster than the code in section [ sec : baseline ] .",
    "very roughly , this speed - up is consistent with the reduction in the number of assembly language instructions ( from 82 to 62 ) , but is somewhat larger .",
    "this is partly because the instructions which take memory arguments have a larger latency than register - only instructions .",
    "line 22 of list [ c : scalar ] shows a built - in function for gcc , which is compiled into a prefetch instruction . in this case , the prefetch instruction loads the data which will be needed two iterations after the current one .",
    "the second and third parameters are read / write flags for which zero indicates preparing for a read and the degree of temporal locality takes value from 1 to 3 .",
    "line 6 of list [ c : scalar ] shows a pad to make the size of the predictor structure to be exactly 64-byte , which is the cache - line size of athlon 64 processor . to make the predictor structure align at 64-byte boundary",
    ", we use memalign ( ) or posix_memalign ( ) instead of malloc ( ) .     on athlon 64 3000 + 2.0 ghz .",
    "open squares and triangles show the results of using code with prefetch and with and without 64-byte alignment .",
    "filled squares and triangles show the result of using code without prefetch and with and without 64-byte alignment .",
    ", height=384 ]    figure [ fig : prefetch ] shows the performance of the force loop as the function of the loop length @xmath0 , with and without this prefetch instruction and 64-byte alignment .",
    "the fastest case depends on @xmath0 . for the region @xmath16 , in which data fits into 64 kb of l1 data cache ,",
    "the code with prefetch and 64-byte alignment is the best . for larger @xmath0 ,",
    "the code with prefetch and without alignment is the best .",
    "in this section , we give two extensions specialized for x86/x86_64 architecture for the force loop .",
    "the first is using sse2 vector ( simd ) mode instead of sse2 scalar ( sisd ) mode .",
    "the second is replacing one division and one square root , by a special instruction for fast approximate inverse square root in sse and newton - raphson iteration .      in the previous section",
    "we improved the performance of the c - language implementation of the force loop , essentially by hand - optimizing the c code so that the generated assembly code becomes optimal .",
    "however , in this way , we did not use the full capability of sse2 .",
    "while sse2 instructions can process two double - precision numbers in parallel , the force loop discussed in the previous section uses only one of these two words . clearly , we did not yet use the `` simd '' nature of the instruction set .",
    "whether or not we can gain by using this simd nature depends on the particular code , and also on the particular processor . on intel",
    "p4 architecture the simd mode can offer up to a factor of two speedup , while on amd athlon 64 or intel pentium m , the speed increase can be small ( or even negative ) .",
    "there are many ways to use this simd feature .",
    "since there are similarities with the vector instructions of some old vector processors , in particular the cyber 205 , one could make use of an automatic vectorizing compiler ( such as icc version 6.0 and later ) .",
    "however , just as was the case with the old vectorizing compilers , the vectorizing capability of modern compilers are still very limited , and it is hard to rewrite the force loop so that the compiler can make use of the simd capability .",
    "in fact , part of the reason why vectorization is difficult is that the load / store capability of sse2 instructions is rather limited : it can work efficiently only on a pair of two double - precision words in consecutive and 16-byte aligned addresses .",
    "this means that the basic loop structure can not work , and one needs to copy the data of two particles into some special data structure , in order to let the compiler generate the appropriate simd instructions .",
    "here we have adopted a low - level approach , in which we make use of the special data type of pairs of double precision words defined in gcc .",
    "basically , this data type , which we call v2df , corresponds to what is in one xmm register ( a pair of double - precision words ) .",
    "we can perform the usual arithmetic operations and even function calls for this data type .",
    "here is the code which makes use of this v2df data type :    .... typedef double v2df _ _ attribute _ _ ( ( mode(v2df ) ) ) ; typedef float   v4sf _ _ attribute _ _ ( ( mode(v4sf ) ) ) ; ....    the v4sf data type packs four single - precision words for sse which we will use later .",
    "the basic idea here is to calculate the forces from one particle on two different particles in parallel .",
    "one could try to calculate the forces from , rather than on , two different particles on one particle in parallel , but that would result in a more complicated program since then we will need to add up the two partial forces in the end .",
    "also , from the point of view of memory bandwidth , our approach is more efficient , since we need to load only one particle per iteration .",
    "note that this is the same approach as what is called @xmath6-parallelism in the various versions of grape hardware , where parallel pipelines calculate the forces on different particles from the same set of particles .    in this code , we use macros for gathering load and scattering store using instructions for loading / storing the higher / lower half of an xmm register :    .... # define v2df_gather(reg , ptr0 , ptr1 ) \\      reg = _ _ builtin_ia32_loadlpd(reg , ( void * ) ( ptr0 ) ) ; \\      reg =",
    "_ _ builtin_ia32_loadhpd(reg , ( void * ) ( ptr1 ) ) ;    # define v2df_scatter(reg , ptr0 , ptr1 ) \\      _ _ builtin_ia32_storelpd((void * ) ( ptr0 ) , reg ) ; \\      _ _ builtin_ia32_storehpd((void * ) ( ptr1 ) , reg ) ; ....    we should be careful about the fact that these built - in functions are undocumented feature of gcc .",
    "the gcc document describes built - in functions for most sse / sse3 instructions but not for any of sse2 instruction .",
    "we can call most sse2 instruction through the prefix _ _ builtin_ia32_. note that , for whatever reason , instructions such as `` movxxx '' from / to memory are changed to `` loadxxx''/``storexxx '' . but this does not mean that gcc always generates correct assembly output .",
    "for example , gcc generates wrong code for the first macro in list [ list : gather ] when reg is not a register variable .",
    "this can be considered either a bug or a feature , and it shows the risk of using these undocumented aspects of gcc .",
    "we also use macros for numerical values since we can not use numerical values like `` 3.0 '' for vector operations :    .... # define v2df_op_scalar(reg , op , imm ) { \\",
    "v2df v2df_temp = { imm , imm } ; \\      reg op v2df_temp ; \\ } #",
    "define v2df_op_vector(reg , op , imm0 , imm1 ) { \\      v2df v2df_temp = { imm0 , imm1 } ; \\",
    "reg op v2df_temp ; \\ } # define v4sf_op_scalar(reg , op , imm ) { \\",
    "v4sf v4sf_temp = { imm , imm , imm , imm } ; \\      reg op v4sf_temp ; \\ } ....    note that we need to copy the data of a single particle into both high and low words of an xmm register .",
    "the new sse3 extension supports this `` broadcasting '' , while the original sse2 set did not .",
    "therefore , we use the following macro :    .... # ifdef _ _ sse3 _ _ # define loadddup(reg , ptr ) \\      reg = _ _",
    "builtin_ia32_loadddup(ptr ) ; # else # define loadddup(reg , ptr ) \\      reg = _",
    "_ builtin_ia32_loadlpd(reg , ( void * ) ( ptr ) ) ; \\      reg = _ _ builtin_ia32_loadhpd(reg , ( void * ) ( ptr ) ) ;",
    "....    we show the vectorized force loop using these data types and macros in list [ c : vector ] .",
    ".... typedef struct {      double x[3 ] ;      double v[3 ] ;      double m[2 ] ; } predictor ; typedef predictor * ppredictor ;    void calcaccjerk_vector ( ppredictor pr , paccjerk accjerkout ,                           double eps2 , int index [ ] , int nj ) {      int j , k ;      v2df x , y , z , vx , vy , vz , ax , ay , az , jx , jy , jz , pot ;      v2df r2,rv ;      v2df rinv1,rinv2,rinv3 ;      static v2df zero = { 0.0 , 0.0 } ;      ppredictor prj = pr ;      int i0 = index[0 ] ;      int i1 = index[1 ] ;      v2df xi[7 ] ;      v2df * vi     = xi+3 ;      v2df * eps2p = xi+6 ;        pot = ax = ay = az = jx = jy = jz = zero ;        for(k=0;k<3;k++ ) {          v2df_op_vector(xi[k ] , = , pr[i0].x[k ] , pr[i1].x[k ] ) ;          v2df_op_vector(vi[k ] , = , pr[i0].v[k ] , pr[i1].v[k ] ) ;      }      v2df_op_scalar(*eps2p , = , eps2 ) ;      for(j=0;j < nj;j++,prj++ ) {          loadddup(x , prj->x ) ;          x -= xi[0 ] ;          loadddup(y , prj->x+1 ) ;          y -= xi[1 ] ;          loadddup(z , prj->x+2 ) ;          z -= xi[2 ] ;          loadddup(vx , prj->v ) ;          vx -= vi[0 ] ;          loadddup(vy , prj->v+1 ) ;          vy -= vi[1 ] ;          loadddup(vz , prj->v+2 ) ;          vz -= vi[2 ] ;            r2 = x*x + y*y + z*z + * eps2p ;          rv = vx*x + vy*y + vz*z ;          _ _ builtin_prefetch(prj+2 , 0 , 3 ) ;            v2df_op_scalar(rinv2 , = , 1.0 ) ;          rinv2 /= r2 ;          rinv1 = _ _ builtin_ia32_sqrtpd(rinv2 ) ;            rv * = rinv2 ;          v2df_op_scalar(rv , * = , 3.0 ) ;            rinv1 * = * ( v2df * ) ( prj->m ) ;          pot -= rinv1 ;          rinv3 = rinv1 * rinv2 ;            x * = rinv3 ; ax + = x ;          y * = rinv3 ; ay + = y ;          z * = rinv3 ; az + = z ;            vx * = rinv3 ; jx + = vx ;          vy * = rinv3 ; jy + = vy ;          vz * = rinv3 ; jz + = vz ;            x   * = rv ; jx -= x ;          y   * = rv ; jy -= y ;          z   * = rv ; jz -= z ;      }        {    / * correct self interaction * /          v2df mass = { pr[i0].m[0 ] , pr[i1].m[0 ] } ;          v2df_op_scalar(mass , * = , 1.0/sqrt(eps2 ) ) ;          pot + = mass ;      }      v2df_scatter(ax , accjerkout[0].a ,    accjerkout[1].a ) ;      v2df_scatter(ay , accjerkout[0].a+1 , accjerkout[1].a+1 ) ;      v2df_scatter(az , accjerkout[0].a+2 , accjerkout[1].a+2 ) ;      v2df_scatter(jx , accjerkout[0].j ,    accjerkout[1].j ) ;      v2df_scatter(jy , accjerkout[0].j+1 , accjerkout[1].j+1 ) ;      v2df_scatter(jz , accjerkout[0].j+2 , accjerkout[1].j+2 ) ;      v2df_scatter(pot , & accjerkout[0].pot , & accjerkout[1].pot ) ; } ....    the predictor structure has changed to store the same mass value in two places ( line 4 ) , in order to save an instruction ( line 54 ) .      the most expensive part of the force calculation , for recent microprocessors , is the calculation of an inverse square root which requires one division and one square root caluclations . several attempts to speed this up using table lookup , polynomial approximation and newton - raphson iteration have been reported ( e.g. karp 1992 ) .",
    "the main difficulty in these approaches is how to get a good approximation for the starting value of a newton - raphson iteration quickly .    here , we use rsqrtss / ps instruction in the sse instruction set , which provide approximate values for the inverse square root for scalar / vector single - precision floating - point numbers , to an accuracy of about 12 bits . with one newton - raphson iteration",
    ", we can obtain 24-bit accuracy , which is sufficient for most `` high - accuracy '' calculations .",
    "if higher accuracy is really necessary , we could apply a second iteration .",
    "the newton - raphson iteration formula is expressed as : @xmath17 here , @xmath18 is an initial guess for @xmath19 .",
    "we show the implementation for a scalar version using inline assembly code with gcc extensions :    ....      double r2 , rinv ;      float ftmp ;        ftmp = ( float)r2 ;      asm(\"rsqrtss % 1 , % 0\":\" = x\"(ftmp):\"x\"(ftmp ) ) ;      rinv = ( double)ftmp ;      rinv * = r2*rinv*rinv - 3.0 ; ....    and for a vector version using built - in functions :    ....      v2df r2 , rinv ;      v4sf ftmp ;        ftmp = _ _ builtin_ia32_cvtpd2ps(r2 ) ;      ftmp = _ _ builtin_ia32_rsqrtps(ftmp ) ;      rinv = _ _ builtin_ia32_cvtps2pd(ftmp ) ;      r2 * = rinv ;      r2 * = rinv ;      v2df_op_scalar(r2 , -= , 3.0 ) ;      rinv * = r2 ; ....    note that we skip here the multiplication by @xmath20 in equation ( [ eqn : nr ] ) .",
    "this can be done after the total force is obtained .    to use rsqrtss / ps instructions",
    ", we first convert @xmath21 into single precision , then apply these instructions , and finally convert the result back to double precision .",
    "16 ( upper ) and @xmath22 ( lower ) .",
    "the errors is periodic for powers of 4 .",
    ", height=480 ]    note that the actual value returned by this rsqrtss / ps instruction is implementation dependent .",
    "in particular , the amd athlon 64 processors and the intel pentium 4 processors return different values .",
    "figure [ fig : rsqrt ] shows the errors of the return values as a function of the input values .",
    "the amd implementation has smaller average errors , but at the same time they show a relatively large systematic bias . even after one newton - raphson iteration in double precision ,",
    "results from both implementations show relatively large biases",
    ". table [ table : error ] presents the root mean square error , max error , and bias ( mean error ) of the approximate value @xmath23 , on a intel pentium 4 and an amd athlon 64 before / after one newton - raphson iteration . here the error is measured as @xmath24 ^ 2 - 1)$ ] , and the weight is @xmath25 , where @xmath26 is the normalized fraction of the floating - point number @xmath27 .",
    "we can correct these biases , at least statistically , by multiplying the resulted force by constants which depend on the processor type used .    .",
    "errors of approximate inverse square root .",
    "[ cols= \" < , > , > , > \" , ]     table [ table : mixedperf ] gives the performance of the code listed above .",
    "the second column gives the performance of the code after hand - tuning the assembly output .",
    "the best performance we have measured is 4.05 gflops , or 3.19 times faster than the original c code described in section 2 .",
    "in this paper we describe in detail various ways of improving the performance of the force - calculation loop for gravitational interactions between particles .",
    "since modern microprocessors have many instructions which can not be easily exploited by existing compilers , we can achieve quite a significant performance improvement if we write a few small library in assembly language and/or using instruction - set specific extensions for the compiler offered to us .",
    "our implementation will give a significant speedup for almost any @xmath0-body integration program .",
    "in addition , we believe that similar optimization is possible in many other compute - intensive applications , within astrophysics as well as in other areas of physics and science in general .",
    "the source code and documentation are available at : + http://grape.astron.s.u-tokyo.ac.jp/~nitadori/phantom/",
    "we thank jumpei niwa for making his opteron machines available for the development of the optimized force loop and also for his helpful advice .",
    "we are also grateful to eiichiro kokubo and toshiyuki fukushige for their encouragement during the development of the code .",
    "we would like to thank all of those who have been involved in the grape project , which has given us a lot of hints for speeding - up of the gravity calculation .",
    "thanks prof .",
    "ninomiya for his kind hospitality at the yukawa institute at kyoto university , through the grants - in - aid for scientific research on priority areas , number 763 , `` dynamics of strings and fields '' , from the ministry of education , culture , sports , science and technology , japan .",
    "this research is partially supported by the special coordination fund for promoting science and technology ( grape - dr project ) , also from the ministry of education , culture , sports , science and technology , japan .",
    "we show sample @xmath0-body code using hermite hierarchical time step scheme and the library in subsection [ subsec : wholecode ] .",
    "int gravitydriver(char * parmfile ) {      int i , k ;      int nbody ;      double epsinv , timeend , deouttime , maxstep , eta_s , sqrteta ;      double systemtime = 0.0 ;      double eps2 ;      double initenergy ;      file * fpin = null , * fpout = null ;      struct particle * ptcl ;      struct accjerk * ajnew ;      int * iindex ;      long numptclstep = 0 , numstep = 0 ;      long tsc0 , tsc1 ;      double cycle ;      long cyclepredict = 0 , cycleforce = 0           cyclecorrect = 0 , cycletot = 0 ;      double walltime ;                    / * initial step * /      for(i=0;i < nbody;i++ ) iindex[i ] = i ;      rdtscll(tsc0 ) ;      calcaccjerkusing_iindex(nbody , iindex , nbody , ajnew ) ;      rdtscll(tsc1 ) ;      cycle = ( tsc1 - tsc0 ) / ( ( double)nbody * nbody ) ;      printf(\"%f cycle per interruction , % f gflops\\n\\n \" ,              cycle , megaheltz/1.e3/cycle*60.0 ) ;        for(i=0;i < nbody;i++ ) {          for(k=0;k<3;k++ ) {              ptcl[i].a[k ] = ajnew[i].a[k ] ;              ptcl[i].j[k ] = ajnew[i].j[k ] ;          }          ptcl[i].pot = ajnew[i].pot ;          inittimestep(ptcl+i , eta_s , maxstep ) ;          bstep_pushaparticle(i , ptcl[i].timestep , & ptcl[i].timestep ) ;      }      initenergy = getenergy(ptcl , nbody ) ;      printenegy(ptcl , nbody , initenergy , systemtime ) ;                    cyclecorrect -= tsc0 ;          for(k=0;k < inum;k++ ) {              int ii = iindex[k ] ;              char * addr = ( char * ) ( ptcl + iindex[k+1 ] ) ;              prefetchw(addr ) ;              prefetchw(addr+64 ) ;              prefetch(ajnew+k+1 ) ;              hermitecorrect(ptcl+ii , ajnew+k , sqrteta ) ;              bstep_pushaparticle                  ( ii , ptcl[ii].timestep , & ptcl[ii].timestep ) ;          }          if(fmod(systemtime , deouttime ) = = 0.0 ) {              for(i=0;i < nbody;i++ ) ptcl[i].pot = ajnew[i].pot ;              printenegy(ptcl , nbody , initenergy , systemtime ) ;          }          numptclstep + = inum ;          numstep++ ;      }      rdtscll(tsc1 ) ;      cycletot + = tsc1 ;        walltime = ( double)cycletot / megaheltz / 1.e6 ;      printf(\"wall clock time : % f sec\\n \" , walltime ) ;      printf(\"predict : % f usec / blockstep , % f cycle / loop\\n \" ,              ( double)cyclepredict / numstep / megaheltz ,              ( double)cyclepredict/(numstep*nbody ) ) ;      printf(\"force   : % f usec / blockstep , % f cycle / loop\\n \" ,              ( double)cycleforce / numstep / megaheltz ,              ( double)cycleforce/(numptclstep*nbody ) ) ;      printf(\"correct : % f usec / blockstep , % f cycle / loop\\n \" ,              ( double)cyclecorrect / numstep / megaheltz ,              ( double)cyclecorrect/(numptclstep ) ) ;      printf(\"%f particles / blockstep\\n\\n \" , ( double)numptclstep / numstep ) ;        / * final synchronizing step * /      ssegrav_predict(ptcl , timeend/ * , nbody*/ ) ;      for(i=0;i <",
    "nbody;i++ ) iindex[i ] = i ;      calcaccjerkusing_iindex(nbody , iindex , nbody , ajnew ) ;      for(i=0;i <",
    "nbody;i++ ) {          ptcl[i].pot = ajnew[i].pot ;          ptcl[i].timestep = timeend - ptcl[i].time ;          hermitecorrect(ptcl+i , ajnew+i , sqrteta ) ;      }      printenegy(ptcl , nbody , initenergy , timeend ) ;"
  ],
  "abstract_text": [
    "<S> the main performance bottleneck of gravitational @xmath0-body codes is the force calculation between two particles . </S>",
    "<S> we have succeeded in speeding up this pair - wise force calculation by factors between two and ten , depending on the code and the processor on which the code is run . </S>",
    "<S> these speedups were obtained by writing highly fine - tuned code for x86_64 microprocessors . </S>",
    "<S> any existing @xmath0-body code , running on these chips , can easily incorporate our assembly code programs .    in the current paper , we present an outline of our overall approach , which we illustrate with one specific example : the use of a hermite scheme for a direct @xmath1 type integration on a single 2.0 ghz athlon 64 processor , for which we obtain an effective performance of 4.05 gflops , for double precision accuracy . in subsequent papers , we will discuss other variations , including the combinations of @xmath2 codes , single precision implementations , and performance on other microprocessors .    </S>",
    "<S> stellar dynamics , methods : numerical </S>"
  ]
}