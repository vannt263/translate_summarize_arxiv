{
  "article_text": [
    "an often - held opinion on intrinsic dimensionality of data sampled from submanifolds of the euclidean space is expressed in @xcite thus : `` ... the goal of estimating the dimension of a submanifold is a well - defined mathematical problem .",
    "indeed all the notions of dimensionality like e.g. topological , hausdorff , or correlation dimension agree for submanifolds in @xmath1 . ''",
    "we will argue that it may be useful to have at one s disposal a concept of intrinsic dimension of data which behaves in a different fashion from the more traditional concepts .",
    "our approach is shaped up by the following five goals .",
    "we want a high value of intrinsic dimension to be indicative of the presence of the curse of dimensionality .",
    "the concept should make no distinction between continuous and discrete objects , and the intrinsic dimension of a discrete sample should be close to that of the underlying manifold .",
    "the intrinsic dimension should agree with our geometric intuition and return standard values for familiar objects such as euclidean spheres or hamming cubes .",
    "we want the concept to be insensitive to high - dimensional random noise of moderate amplitude ( on the same order of magnitude as the size of the manifold ) .",
    "finally , in order to be useful , the intrinsic dimension should be computationally feasible .",
    "for the moment , we have managed to attain the goals ( 1),(2),(3 ) , while ( 4 ) and ( 5 ) are not met .",
    "however , it appears that in both cases the problem is the same , and we outline a promising way to address it .    among the existing approaches to intrinsic dimension , that of @xcite comes closest to meeting the goals ( 2),(3),(5 ) and to some extent ( 1 ) , cf .",
    "a discussion in @xcite .",
    "( lemma 1 in @xcite seems to imply that ( 4 ) does not hold for moderate noise with @xmath2 , i.e. , @xmath3 . )",
    "we work in a setting of metric spaces with measure ( @xmath4-spaces ) , i.e. , triples @xmath5 consisting of a set , @xmath6 , furnished with a distance , @xmath7 , satisfying axioms of a metric , and a probability measure @xmath8 .",
    "this concept is broad enough so as to include submanifolds of @xmath9 ( equipped with the induced , or minkowski , measure , or with some other probability distribution ) , as well as data samples themselves ( with their empirical , that is normalized counting , measure ) . in section [ s : conc ] , we describe this setting and discuss in some detail the phenomenon of concentration of measure on high dimensional structures , presenting it from a number of different viewpoints , including an approach of soft margin classification .",
    "the curse of dimensionality is understood as a geometric property of @xmath4-spaces whereby features ( @xmath10-lipschitz , or non - expanding , functions ) sharply concentrate near their means and become non - discriminating .",
    "this way , the curse of dimensionality is equated with the phenomenon of concentration of measure on high - dimensional structures @xcite , and can be dealt with an a precise mathematical fashion , adopting ( 1 ) as an axiom .    the intrinsic dimension , @xmath11 , is defined for @xmath4-spaces in an axiomatic way in section [ s : axiom ] , following @xcite .    to deal with goal ( 2 ) , we resort to the notion of a distance , @xmath12 , between two @xmath4-spaces , @xmath6 and @xmath13 , measuring their similarity @xcite .",
    "this forms the subject of section [ s : gromov ] .",
    "our second axiom says that if two @xmath4-spaces are close to each other in the above distance , then their intrinsic dimension values are also close . in this article",
    ", we show that if a dataset @xmath6 is sampled with regard to a probability measure @xmath8 on a manifold @xmath14 , then , with high confidence , the distance between @xmath6 and @xmath14 is small , and so @xmath15 and @xmath16 are close to each other",
    ".    the goal ( 3 ) can be made into an axiom in a more or less straightforward way .",
    "we give a new example of a dimension function @xmath11 satisfying our axioms .",
    "we show that the gromov distance between a low - dimensional manifold @xmath14 and its corruption by high - dimensional gaussian noise of moderate amplitude is close to @xmath14 in the gromov distance",
    ". however , this property does not carry over to the samples unless their size is exponential in the dimension of @xmath1 ( unrealistic assumption ) , and thus our approach suffers from high sensitivity to noise ( section [ s : noise ] . )",
    "another drawback is computational complexity : we show that computing the intrinsic dimension of a finite sample is an @xmath0-complete problem ( sect .",
    "[ s : complexity ] . )    however , we believe that the underlying cause of both problems is the same : allowing _ arbitrary _ non - expanding functions as features is clearly too generous . restricting the class of features to that of low - complexity functions",
    "whose capacity is manageable and rewriting the entire theory in this setting opens up a possibility to use statistical learning theory and offers a promising way to solve both problems , which we discuss in conclusion .",
    "as in @xcite , we model datasets within the framework of spaces with metric and measure ( @xmath4-spaces ) .",
    "so is called a triple @xmath5 , consisting of a ( finite or infinite ) set @xmath6 , a metric @xmath7 on @xmath6 , and a probability measure @xmath8 defined on the family @xmath17 of all borel subsets is the smallest family of subsets of @xmath6 closed under countable unions and complements and containing every open ball @xmath18 , @xmath19 , @xmath20 . ] of the metric space @xmath21 .",
    "the setting of @xmath4-spaces is natural for at least three reasons .",
    "first , a finite dataset @xmath6 sitting in a euclidean space @xmath1 forms an @xmath4-space in a natural way , as it comes equipped with a distance and a probability measure ( the empirical measure @xmath22 , where @xmath23 denotes the number of elements in @xmath24 ) .",
    "second , if one wants to view datasets as random samples , then the domain @xmath25 , equipped with the sampling measure @xmath8 and a distance , also forms an @xmath4-space . and",
    "finally , theory of @xmath4-spaces is an important and fast developing part of mathematics , the object of study of asymptotic geometric analysis , see @xcite and references therein .",
    "_ features _ of a dataset @xmath6 are functions on @xmath6 that in some sense respect the intrinsic structure of @xmath6 . in the presence of a metric , they are usually understood to be _ 1-lipschitz , _ or _",
    "non - expanding , _",
    "functions @xmath26 , that is , having the property @xmath27 we will denote the collection of all real - valued 1-lipschitz functions on @xmath6 by @xmath28 .",
    "the curse of dimensionality is a name given to the situation where all or some of the important features of a dataset sharply concentrate near their median ( or mean ) values and thus become non - discriminating . in such cases ,",
    "@xmath6 is perceived as intrinsically high - dimensional .",
    "this set of circumstances covers a whole range of well - known high - dimensional phenomena such as for instance sparseness of points ( the distance to the nearest neighbour is comparable to the average distance between two points @xcite ) , etc .",
    "it has been argued in @xcite that a mathematical counterpart of the curse of dimensionality is the well - known _ concentration phenomenon _ @xcite , which can be expressed , for instance , using gromov s concept of the _ observable diameter _ @xcite .",
    "let @xmath5 be a metric space with measure , and let @xmath29 be a small fixed threshold value .",
    "the _ observable diameter _ of @xmath6 is the smallest real number , @xmath30 , with the following property : for every two points @xmath31 , randomly drawn from @xmath6 with regard to the measure @xmath8 , and for any given @xmath10-lipschitz function @xmath32 ( a feature ) , the probability of the event that values of @xmath26 at @xmath33 and @xmath34 differ by more than @xmath35 is below the threshold : @xmath36<\\kappa.\\ ] ] informally , the observable diameter @xmath37 is the size of a dataset @xmath6 as perceived by us through a series of randomized measurements using arbitrary features and continuing until the probability to improve on the previous observation gets too small .",
    "the observable diameter has little ( logarithmic ) sensitivity to @xmath38 .",
    "the _ characteristic size _",
    "@xmath39 of @xmath6 as the median value of distances between two elements of @xmath6 .",
    "the concentration of measure phenomenon refers to the observation that `` natural '' families of geometric objects @xmath40 often satisfy @xmath41 a family of spaces with metric and measure having the above property is called a _",
    "lvy family_. here the parameter @xmath42 usually corresponds to dimension of an object defined in one or another sense .    for the euclidean spheres @xmath43 of unit radius , equipped with the usual euclidean distance and the ( unique ) rotation - invariant probability measure , one has , asymptotically as @xmath44 , @xmath45 , while @xmath46 .",
    "[ fig : obs - diam ] shows observable diameters ( indicated by inner circles ) corresponding to the threshold value @xmath47 of spheres @xmath43 in dimensions @xmath48 , along with projections to the two - dimensional screen of randomly sampled 1000 points .",
    "[ 0.271 ] , @xmath48.,title=\"fig : \" ] [ 0.271 ] , @xmath48.,title=\"fig : \" ]    [ 0.271 ] , @xmath48.,title=\"fig : \" ] [ 0.271 ] , @xmath48.,title=\"fig : \" ]    some other important examples of lvy families @xcite include : + @xmath49 hamming cubes @xmath50 of two - bit @xmath42-strings equipped with the normalized hamming distance @xmath51 and the counting measure .",
    "the law of large numbers is a particular consequence of this fact , hence the name geometric law of large numbers sometimes used in place of concentration phenomenon ; + @xmath49 groups @xmath52 of special unitary @xmath53 matrices , with the geodesic distance and haar measure ( unique invariant probability measure ) ; + @xmath49 spaces @xmath1 equipped with the guassian measure with standard deviation @xmath54 , + @xmath49 any family of expander graphs ( @xcite , p. 197 ) with the normalized counting measure on the set of vertices and the path metric .",
    "any dataset whose observable diameter is small relative to the characteristic size will be suffering from dimensionality curse .",
    "for some recent work on this link in the context of data engineering , cf .",
    "@xcite and references therein .",
    "one of many equivalent ways to reformulate the concentration phenomenon is this :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ for a typical `` high - dimensional '' structure @xmath6 , if @xmath24 is a subset containing at least half of all points , then the measure of the @xmath55-neighbourhood @xmath56 of @xmath24 is overwhelmingly close to @xmath10 already for small values of @xmath19 . _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    more formally , one can prove that a family @xmath57 of @xmath4-spaces is a lvy family if and only if , whenever a borel subset @xmath58 is picked up in every @xmath59 in such a way that @xmath60 , one has @xmath61 for every @xmath19 .",
    "this reformulation allows to define the most often used quantitative measure of concentration phenomenon , the _ concentration function _ , @xmath62 , of an @xmath4-space @xmath5 , cf .",
    "one sets @xmath63 and for all @xmath19 , @xmath64 where @xmath24 runs over borel subsets of @xmath6 .",
    "clearly , a family of @xmath4-spaces @xmath40 is lvy if and only if the concentration functions @xmath65 converge to zero pointwise for all @xmath19 .",
    "another such quantitative measure is the _ separation distance _ @xcite .",
    "let @xmath29 .",
    "the value @xmath66 of @xmath38-separation distance of the @xmath4-space @xmath6 is the supremum of all @xmath67 for which there are borel sets @xmath68 at a distance @xmath69 from each other which are both sufficiently large :",
    "@xmath70        by setting in addition @xmath71 , one gets the _ separation function _ of @xmath6 , @xmath72 , which is a non - increasing function from the interval @xmath73 $ ] to @xmath74 , vanishing at the right endpoint .    , @xmath75 . ]",
    "it is a simple exercise to verify that for all @xmath76 @xmath77 thus , a family @xmath40 of @xmath4-spaces is a lvy family if and only if @xmath78 converge to zero pointwise , cf .",
    "[ fig : sepfunctions ] .      here",
    "we will explain the concentration phenomenon in the language of soft margin classifiers .",
    "we will work in the setting of @xcite , subs .",
    "9.2 , assuming that the training dataset for a binary classification problem is modelled by a sequence of i.i.d .",
    "random variables distributed according to a probability measure @xmath79 on @xmath80 . here",
    "@xmath25 is the domain , in our case a metric space , and the classifying functions @xmath26 will be assumed @xmath10-lipschitz .",
    "( for a detailed treatment of large margin classification problem for such functions , see @xcite . ) the _ margin _ of a function @xmath81 on @xmath82 is defined as @xmath83 for a @xmath84 ( margin parameter ) , define the _ error _ of @xmath26 with respect to @xmath79 and @xmath85 as the probability @xmath86 the value @xmath87 is a measure of how many datapoints admit a confident correct classification .",
    "let @xmath88 be a metric space with measure , and let @xmath79 be a probability distribution on @xmath80 with the marginals equal to @xmath8 on @xmath25 and the bernoulli distribution on @xmath89 .",
    "let @xmath90 .",
    "then for every @xmath10-lipschitz function @xmath26 , @xmath91 where @xmath92 denotes the concentration function of the domain @xmath88 .",
    "[ th : gamma ]    the result easily follows from the definition of the concentration function if one takes into account that the distribution @xmath79 induces a partition of @xmath25 in two borel subsets of measure @xmath93 each .",
    "conversely , one can bound the concentration function in terms of the uniform error : @xmath94 where the supremum is taken over all @xmath10-lipschitz functions on @xmath25 .",
    "this formalizes the observation that in datasets suffering from dimensionality curse large margin classification with @xmath10-lipschitz functions becomes impossible .",
    "gromov s distance between two @xmath4-spaces satisfies the usual axioms of a metric and is introduced in such a way that a family @xmath40 of @xmath4-spaces forms a lvy family if it converges to a one - point space with regard to gromov s distance .",
    "thus , one can say that a dataset @xmath6 suffers from the curse of dimensionality if it is close to a one - point space in gromov s distance .",
    "intuitively , it means that the features of @xmath6 give away as little useful information about the intrinsic structure of @xmath6 as the features of the trivial one - point set , that is , not much more can be derived about @xmath6 from observations than about a one - point set .",
    "( for a formalization of this discussion in terms of gromov s _ observable diameter _ of an @xmath4-space , see @xcite and gromov s original book @xcite . )",
    "gromov s distance allows one to talk of _ concentration to a non - trivial space_. in a sense , this is what happens in the context of principal manifold analysis , where one expects a dataset to concentrate to a low - dimensional manifold .",
    "let @xmath95 and @xmath96 be two @xmath4-spaces .",
    "the idea of gromov s distance is that @xmath6 and @xmath13 are close if every feature of @xmath6 can be matched against a similar feature of @xmath13 , and vice versa .",
    "for this purpose , one needs to represent all the features as functions on a common third space .",
    "this is achieved through a standard result in measure theory .",
    "every @xmath4-space @xmath6 can be _",
    "parametrized _ by the unit interval : there is a measurable map @xmath97\\to x$ ] with the property that whenever @xmath98 is a borel subset , one has @xmath99 where @xmath100 is the lebesgue measure on @xmath101 $ ] and @xmath102\\colon \\phi(t)\\in a\\}$ ] is the inverse image of @xmath24 under @xmath103 .",
    "introduce the distance @xmath104 between measurable functions on @xmath101 $ ] as follows : @xmath105\\colon { \\vertf(t)-g(t)\\vert } > \\e\\}<\\e\\right\\}.\\ ] ] this is indeed a metric , determining the well - known _ convergence in measure_.    now define the gromov distance @xmath12 between two @xmath4-spaces @xmath6 and @xmath13 as the infimum of all @xmath19 for which there exist some suitable parametrizations @xmath106 and @xmath107 of @xmath6 and of @xmath13 respectively , with the following property . for every @xmath108",
    "there is a @xmath109 with @xmath110 and vice versa : for every @xmath109 there is an @xmath108 satisfying eq .",
    "( [ eq : me1 ] ) .",
    ".2 cm    let @xmath6 be an @xmath4-space .",
    "then @xmath111    suppose @xmath112 and let @xmath98 , @xmath113 .",
    "the distance function @xmath114 is 1-lipschitz and so differs from a suitable constant function @xmath115 by less than @xmath116 on a set of measure @xmath117 .",
    "clearly , @xmath118 , and so @xmath119 can possibly take value @xmath120 on a set of measure @xmath121 , meaning @xmath122 .",
    "conversely , if @xmath123 , there exists a 1-lipschitz function @xmath26 on @xmath6 which differs from its median value @xmath124 by at least @xmath55 on a set of measure @xmath125 .",
    "it means the existence of two sets , @xmath24 and @xmath126 , such that @xmath113 , @xmath127 , and for all @xmath128 , @xmath129 one has @xmath130 , that is , @xmath131 .",
    "e.g. for the spheres the gromov distance to a point is exactly the solution to the equation @xmath132 , fig .",
    "[ fig : distances ] .    , @xmath75 , and the straight line @xmath133 .",
    "]    a family @xmath40 of @xmath4-spaces is a lvy family if and only if it converges with regard to gromov s distance to the trivial one - point space @xmath134 .",
    "[ r : enough ] notice that in the definition of gromov s distance @xmath12 one can replace throughout the sets @xmath28 of all @xmath10-lipschitz functions with the sets of all 1-lipschitz functions @xmath26 satisfying @xmath135 , where @xmath35 is an upper bound on the diameter of two metric spaces in question and @xmath136 is the supremum norm of @xmath26 .",
    "let us compare the gromov distance to the well - known _ monge - kantorovich , _ or _",
    "mass transportation , _ distance ( also known in computer science as the _ wasserstein _ , or _",
    "earth - mover s distance _ ) , see @xcite . given two probability measures @xmath8 and @xmath79 on a metric space @xmath21 ,",
    "the mass transportation distance between them is @xmath137 where @xmath138 runs over all probability measures on @xmath139 whose marginals are @xmath8 and @xmath79 , respectively .",
    "thinking of @xmath8 and @xmath79 as piles of sand of equal mass , @xmath140 is the smallest average distance that a grain of sand has to travel when the first pile is moved to take place of the second .    [",
    "p : mass ] @xmath141 .    without loss in generality , one can assume both @xmath8 and @xmath79 to be non - atomic , and use the coordinate projections @xmath142 , @xmath143 , from the measure space @xmath144 to parametrize the two @xmath4-spaces in question .",
    "for every @xmath145 the @xmath146-norm of the difference @xmath147 satisfies @xmath148 whence the desired estimate follows easily .",
    "no bound in the opposite direction is possible .",
    "for instance , @xmath149 , while the mass transportation distance between the haar measure on the sphere @xmath43 and any dirac point mass will be at least @xmath10 .",
    "if @xmath88 is an @xmath4-space and @xmath6 is a @xmath8-sample of @xmath25 , then @xmath6 becomes an @xmath4-space on its own right if equipped with the restriction of the distance @xmath7 and the normalized counting measure .",
    "the following theorem states that random samples of an @xmath4-space @xmath25 will concentrate to it with confidence approaching one as the sample size increases .",
    "recall that a metric space @xmath150 is _ totally bounded _ if for every @xmath151 it can be covered with finitely many open balls of radius @xmath152 , and the smallest such number , the _ covering number _",
    ", is denoted @xmath153 .",
    "for instance , every compact metric space is totally bounded .",
    "[ th : sampling ] let @xmath88 be a totally bounded metric space of diameter one equipped with a non - atomic borel probability measure @xmath8 .",
    "let @xmath154 , and let @xmath6 be a random @xmath8-sample of @xmath25 of size @xmath155 where @xmath156 is an absolute constant .",
    "then with confidence @xmath157 one has : @xmath158    the rademacher averages of a class @xmath159 of functions are capacity measures defined as follows : for every @xmath160 , @xmath161 where @xmath162 , @xmath163 are i.i.d .",
    "sample points according to the sample distribution @xmath8 , and @xmath164 are i.i.d .",
    "rademacher random variables assuming equiprobable values @xmath165 .",
    "making use of remark [ r : enough ] , denote by @xmath166 the space of all @xmath10-lipschitz functions on @xmath25 with @xmath167 , and similarly for @xmath168 . by theorem 18 in @xcite , for a suitable constant @xmath156 , @xmath169 since the diameter and covering numbers of @xmath6 are majorized by those of @xmath25 , the inequality ( [ eq : bound ] ) remains true if @xmath166 is replaced with @xmath170 .    because @xmath171 is a standard borel non - atomic probability space ,",
    "it can be used instead of the unit interval to parametrize @xmath6 ( with the normalized counting measure ) .",
    "choose a borel measurable parametrization @xmath172 with the property @xmath173 for each @xmath20 .",
    "let @xmath174 denote the set of all pull - back functions on @xmath25 of the form @xmath175 , @xmath176 .",
    "such functions are borel measurable , though not necessarily lipschitz . by the choice of @xmath103 ,",
    "the rademachar averages of @xmath168 and of @xmath174 coincide , and so eq . ( [ eq : bound ] ) continues to hold with @xmath174 in place of @xmath166 .",
    "corollary 3 on p. 19 in @xcite , applied to the function class @xmath177 , together with the inequality ( [ eq : bound ] ) , implies that , under the condition @xmath178 one has with confidence @xmath157 that the empirical mean and the expected value of each @xmath179 differ by less than @xmath55 : @xmath180 ( notice that in eq .",
    "( [ eq : rademacher ] ) we use the normalization by @xmath181 as e.g. in @xcite , while the normalization in @xcite is by @xmath182 . also , the constant @xmath183 in eq .",
    "( [ eq : mendelson ] ) is different from that in eq .",
    "( [ eq : bound ] ) . )",
    "an analogous statement is true of the class @xmath184 .",
    "consequently , under the assumption ( [ eq : mendelson ] ) , with confidence @xmath185 , if @xmath186 and @xmath187 coincide on @xmath6 , then @xmath188 , which , in its turn , easily implies @xmath189 .    for every function @xmath190",
    "there is a function from the class @xmath174 taking the same values as @xmath26 at all points of @xmath6 : this is the function @xmath191 .",
    "the converse is also true : as is well known , every 1-lipschitz function on a subspace of a metric space ( e.g. @xmath6 ) admits an extension to a 1-lipschitz function on the entire space ( in our case , @xmath25 ) , cf .",
    "e.g. lemma 7 in @xcite .",
    "if @xmath192 , there exists a @xmath193 extending @xmath194 , and now @xmath195 .",
    "we conclude : with confidence @xmath185 , the hausdorff distance between @xmath166 and the pull - back of @xmath168 to @xmath25 is bounded by @xmath196 .",
    "therefore , @xmath197 with confidence @xmath185 . making a substitution @xmath198 , @xmath199",
    ", we obtain the desired result .",
    "since the above result is meant to be applied to low - dimensional manifolds , the values of the covering numbers are relatively low , and the theorem gives meaningful estimates for realistically sized sample sets . for @xmath200 and @xmath201",
    "they are on the order of thousands of points for @xmath202 ( principal curves ) , tens of thousands for @xmath203 and millions for @xmath204 .",
    "the estimates can be no doubt significantly improved .",
    "let @xmath205 denote some class of spaces with metric and measure ( possibly including all of them ) , containing a family @xmath40 of spaces asymptotically approaching the @xmath42-dimensional unit euclidean spheres @xmath43 with their standard rotation - invariant probability measures : @xmath206 these can be euclidean cubes @xmath101^n$ ] with the lebesgue measure and the distance normalized by @xmath182 , the hamming cubes @xmath50 with the normalized hamming ( @xmath207 ) distance and normalized counting measure , etc .",
    "let @xmath11 be a function defined for every member of @xmath205 and assuming values in @xmath208 .",
    "we call @xmath11 an _ intrinsic dimension function _ if it satisfies the following axioms :    1 .",
    "( _ [ ax : conc]axiom of concentration _ ) a family @xmath40 of members of @xmath205 is a lvy family if and only if @xmath209 .",
    "( _ axiom of smooth dependence on datasets _ ) if @xmath210 and @xmath211 , then @xmath212 .",
    "( _ axiom of normalization _ ) if there exist constants @xmath213 and an @xmath214 with @xmath215 for all @xmath216 .",
    "one says that the functions @xmath26 and @xmath194 asymptotically have the same order of magnitude . ] for some ( hence every ) family @xmath217 with the property @xmath218 one has @xmath219 .",
    "the first axiom formalizes a requirement that the intrinsic dimension is high if and only if a dataset suffers from the curse of dimensionality .",
    "the second axiom assures that a dataset @xmath6 well - approximated by a non - linear manifold @xmath14 has an intrinsic dimension close to that of @xmath14 .",
    "the role of the third axiom is just to calibrate the values of the intrinsic dimension .    as explained in @xcite , the axioms lead to a paradoxical conclusion : every dimension function defined for all @xmath4-spaces must assign to the trivial one - point space @xmath134 the value @xmath220 .",
    "this paradox is harmless and does not lead to any contradictions , furthermore one can avoid it is by restricting the class @xmath205 to @xmath4-spaces of a given characteristic size ( i.e. , the median value of distances between two points ) , which does not lead to any real loss in generality .",
    "in @xcite we gave an example of a dimension function , the _ concentration dimension _ of @xmath6 : @xmath221 ^ 2}.\\ ] ] here is another dimension function .",
    "the quantity @xmath222 ^ 2}\\ ] ] defines an intrinsic dimension function on the class of all @xmath4-spaces @xmath6 for which the above integral is proper ( including , in particular , all spaces of bounded diameter ) .",
    "we call it the _ separation dimension_. _ ( cf . fig .",
    "[ fig : sepdimham ] . ) _     of the hamming cube @xmath223 , equipped with the normalized hamming distance and normalized counting measure , @xmath224 , @xmath7 odd . ]    by judiciously choosing a normalizing constant , one can no doubt make the separation dimension of @xmath50 fit the values of @xmath42 much closer .",
    "in fact , practically every concentration invariant from theory of @xmath4-spaces leads to an example of an intrinsic dimension function , and the chapter 3@xmath225 of @xcite is a particularly rich source of such invariants .",
    "most existing approaches to intrinsic dimension of a dataset have to confront the problem that , strictly speaking , the value of dimension of a finite dataset is zero , because it is a discrete object . on the contrary , as examplified by the hamming cube @xmath50 ( fig .",
    "[ fig : sepdimham ] ) , our dimension functions make no difference between discrete and continuous @xmath4-spaces . moreover , the dimension of randomly sampled finite subsets approaches the dimension of the domain .",
    "the following is a consequence of theorem [ th : sampling ] and axiom 2 of dimension function .",
    "let @xmath11 be a dimension function , and let @xmath226 be a non - atomic @xmath4-space .",
    "for every @xmath19 , @xmath227 there is a value @xmath228 such that , whenever @xmath6 is a set of cardinality @xmath229 randomly sampled from @xmath25 with regard to the measure @xmath8 , one has with confidence @xmath157 @xmath230 [ c : sampling ]    jointly with theorems [ th : gamma ] and [ th : sampling ] , the above corollary implies the following result which we state in a qualitative version .",
    "let @xmath11 be a dimension function , and let @xmath88 be a non - atomic metric space with measure .",
    "let @xmath79 be a probability distribution on @xmath80 with the marginals equal to @xmath8 on @xmath25 and the bernoulli distribution on @xmath89 .",
    "then for every @xmath231 there are natural numbers @xmath232 and @xmath233 with the following property .",
    "assume @xmath234 .",
    "let @xmath235 training datapoints be sampled from @xmath236 according to the distribution @xmath79 an an i.i.d . fashion . then with confidence @xmath237 , for every @xmath10-lipschitz function @xmath26 the empirical error satisfies @xmath238 where @xmath239 is the empirical measure supported on the sample .    in other words , an intrinsically high - dimensional",
    "dataset does not admit large margin classifiers .",
    "for the moment , we do nt have any example of a dimension function that would be computationally feasible other than for well - understood geometrical objects ( spheres , cubes ... ) .    fix a value @xmath240 . determining the value @xmath241 of the separation function for finite metric spaces @xmath6 ( with the normalized counting measure ) is an @xmath0-complete problem .    to a given finite metric space @xmath6 associate a graph with @xmath6 as the vertex set and two vertices @xmath31 being adjacent if and only if @xmath242 . now the problem of determining @xmath241 is equivalent to solving the largest balanced complete bipartite subgraph problem which is known to be @xmath0-complete , cf .",
    "gt24 in @xcite .",
    "another deficiency of our approach in its present form is its sensitivity to noise .",
    "we will consider an idealized situation where data is corrupted by high - dimensional gaussian noise , as follows .",
    "let @xmath8 be a probability measure on the euclidean space @xmath1 .",
    "assume that @xmath8 is supported on a compact submanifold @xmath14 of @xmath1 of lower dimension @xmath243 .",
    "if @xmath8 has density @xmath244 ( that is , is absolutely continuous with regard to the lebesgue measure ) , a dataset @xmath6 being sampled in the presence of gaussian noise means @xmath245 where @xmath246 is the density of the gaussian distribution @xmath247 .",
    "equivalently , @xmath6 is sampled with regard to the convolution of @xmath8 with the @xmath7-dimensional gaussian measure : @xmath248 in which form the assumption of absolute continuity of @xmath8 becomes superfluous .",
    "one can think of the @xmath4-space @xmath249 with the euclidean distance as a _ corruption _ of the original domain @xmath250 .",
    "we will further assume that the amplitude of the corrupting noise is on the same order of magnitude as the size of @xmath14 , that is , @xmath251 , or @xmath252 .",
    "here is a result in the positive direction .",
    "[ th : corruption ] let @xmath14 be a compact topological manifold supporting a probability measure @xmath8 . consider a family of embeddings of @xmath14 into the euclidean space @xmath1 , @xmath253 as a submanifold in such a way that the euclidean covering numbers @xmath254 , @xmath19 , grow as @xmath255 .",
    "let @xmath250 be corrupted by the gaussian noise @xmath256 of constant amplitude , that is , @xmath252 .",
    "then the gromov distance between the image of @xmath250 in @xmath1 and its corruption by @xmath256 tends to zero as @xmath253 .    for an @xmath19 ,",
    "let @xmath257 be a finite @xmath55-net for @xmath14 .",
    "denote by @xmath258 the orthogonal projection from @xmath1 to the linear subspace @xmath259 spanned by @xmath257 .",
    "let @xmath260 denote the push - forward of the measure @xmath8 to @xmath259 , that is , for every borel @xmath261 one has @xmath262 .",
    "the mass transportation distance between @xmath8 and @xmath260 is bounded by @xmath55 , and by proposition [ p : mass ] the gromov distance between @xmath14 and @xmath263 is bounded by @xmath264 .",
    "a similar argument gives the same upper bound for the gromov distance between the gaussian corruption of @xmath14 and that of @xmath263 .",
    "the @xmath4-space @xmath265 can be parametrized by the identity mapping of itself ( because the measure is non - atomic and has full support ) , while the projection @xmath258 parametrizes the space @xmath263 by its very definition . if @xmath266 , then @xmath267 .",
    "conversely , let @xmath268 .",
    "the fibers @xmath269 , @xmath270 are @xmath271-dimensional affine subspaces , and the measure induced on each fiber by the measure @xmath272 approaches the gaussian measure @xmath273 with regard to the mass transportation distance as @xmath253 .",
    "the function @xmath274 obtained from @xmath26 by integration over all fibers @xmath269 , @xmath270 belongs to @xmath275 , and since @xmath276 , the concentration of measure for gaussians ( p. 140 in @xcite ) implies that for some absolute constant @xmath183 , the functions @xmath277 and @xmath278 differ by less than @xmath55 on a set of @xmath279-measure @xmath280 .",
    "in particular , if @xmath7 is large enough , the gromov distance between @xmath14 and its gaussian corruption will not exceed @xmath281 , whence the result follows since @xmath19 was arbitrary .    under the assumptions of theorem [ th :",
    "corruption ] , the value of any dimension function @xmath11 for the corruption of @xmath14 converges to @xmath15 as @xmath253 .",
    "unfortunately , this result does not extend to finite samples , because the required size of a random sample of @xmath14 in the presence of noise is unrealistically high : the covering numbers of @xmath282 go to infinity exponentially fast ( in @xmath7 ) , and theorem [ th : sampling ] becomes useless .    as an illustration , consider the simplest case possible .",
    "let @xmath283 be a singular one - point manifold , and let @xmath6 be sampled from @xmath14 in the presence of gaussian random noise of moderate amplitude , that is , @xmath284 where @xmath285 .",
    "assume the cardinality of the sample @xmath6 to be constant , @xmath286 .",
    "then the gromov distance between @xmath283 and @xmath6 tends to a positive constant ( @xmath287 ) as @xmath253 .",
    "it is a well - known manifestation of the curse of dimensionality that , as @xmath253 , the distances between pairs of points of @xmath6 strongly concentrate near the median value , which in this case will tend to @xmath288 .",
    "thus , a typical random sample @xmath6 will form , for all practical purposes , a discrete metric space of diameter @xmath289 .",
    "in particular , @xmath28 will contain numerous @xmath10-lipschitz functions that are highly non - constant , and the gromov distance from @xmath6 to the one - point space @xmath14 is seen to tend to the value @xmath290 .    for manageable sample sizes ( up to millions of points ) the above will already happen in moderate to high dimensions .",
    "for @xmath291 , a random sample @xmath6 as above of @xmath292 points will contain , with confidence @xmath293 , a @xmath10-separated subset @xmath294 containing @xmath295 % of all points ( that is , every two points of @xmath294 are at a distance @xmath296 from each other ) .",
    "consequently , @xmath297 , and the separation dimension @xmath298 will not exceed @xmath299 .",
    "( at the same time , @xmath300 . )",
    "we conclude : the proposed intrinsic dimension of discrete datasets of realistic size is unstable under random high - dimensional noise of moderate amplitude .",
    "the following interesting version of intrinsic dimension was proposed by chvez _",
    "@xcite who called it simply _ intrinsic dimensionality_. let @xmath5 be a space with metric and measure .",
    "denote by @xmath301 the mean of the distance function @xmath302 on the space @xmath139 with the product measure .",
    "assume @xmath303 .",
    "let @xmath304 be the standard deviation of the same function .",
    "the intrinsic dimensionality of @xmath6 is defined as @xmath305    the intrinsic dimensionality satisfies :    * a weaker version of axiom 1 : if @xmath57 is a lvy family of spaces with bounded metrics , then @xmath306 , * a weaker version of axiom 2 : if@xmath211 and @xmath307 , then @xmath308 , * axiom 3 .",
    "[ th : chavez ]    for a proof , as well as a more detailed discussion , see @xcite , where in particular it is shown on a number of examples that the dimension chvez _ et al .",
    "_ and our dimension can behave in quite different ways between themselves ( and of course from the topological dimension ) .",
    "the approaches to intrinsic dimension listed below are all quite different both from our approach and from that of chvez _ et al .",
    "_ , in that they are set to emulate various versions of _ topological _",
    "( i.e. essentially local ) dimension .",
    "in particular , all of them fail both our axioms 1 and 2 .",
    "@xmath49 _ correlation dimension , _ which is a computationally efficient version of the box - counting dimension , see @xcite .",
    "@xmath49 _ packing dimension _ , or rather its computable version as proposed and explored in @xcite .",
    "@xmath49 _ distance exponent _ @xcite , which is a version of the well - known minkowski dimension .",
    "@xmath49 an algorithm for estimating the intrinsic dimension based on the takens theorem from differential geometry @xcite .",
    "@xmath49 a non - local approach to intrinsic dimension estimation based on entropy - theoretic results is proposed in @xcite , however in case of manifolds the algorithm will still return the topological dimension , so the same conclusions apply .",
    "we have proposed a new concept of the intrinsic dimension of a dataset or , more generally , of a metric space equipped with a probability measure .",
    "dimension functions of the new type behave in a very different way from the more traditional approaches , and are closer in spirit to , though still different from , the notion put forward in @xcite ( cf . a comparative discussion in @xcite ) . in particular , high intrinsic dimension indicates the presence of the curse of dimensionality , while lower dimension expresses the existence of a small set of well - dissipating features and a possibility of dimension reduction of @xmath6 to a low - dimensional feature space .",
    "the intrinsic dimension of a random sample of a manifold is close to that of the manifold itself , and for standard geometric objects such as spheres or cubes the values returned by our dimension are `` correct ''",
    ".    two main problems pinpointed in this article are prohibitively high computational complexity of the new concepts , as well as their instability under random high - dimensional noise .",
    "the root cause of both problems is essentially the same : the class of all @xmath10-lipschitz functions is just too broad to serve as the set of admissible features .",
    "the richness of the spaces @xmath28 explains why computing concentration invariants of an @xmath4-space is hard : roughly speaking , there are just too many feature functions on the space that are to be examined one by one .",
    "the abundance of lipschitz functions on a discrete metric space @xmath6 is exactly what makes the gromov distance from a random gaussian sample to a manifold large .    at the same time",
    ", there is clearly no point in taking into account , as a potential feature , say , a typical polynomial function of degree @xmath309 on the ambient space @xmath310 , because such a function may contain up to @xmath311 monomials .",
    "since we can not store , let even compute , such a function , why should we care of it at all ?    a way out , as we see it , consists in refining the approach and modelling a dataset as a pair @xmath312 , consisting of an @xmath4-space @xmath6 _ together with a class of admissible features , _",
    "@xmath313 , whose statistical learning capacity measures ( vc - dimension , covering numbers , rademacher averages , etc . ) are limited .",
    "this will accurately reflect the fact that in practice one only uses features that are computationally cheap , and will allow a systematic use of vapnik - chervonenkis theory .",
    "all the main concepts of asymptotic geometric analysis will have to be rewritten in the new framework , and this seems to be a potentially rewarding subject for further investigation .",
    "a theoretical challenge would to be obtain noise stability results under the general statistical assumptions of @xcite .",
    "finally , the gromov distance between two @xmath4-spaces , @xmath6 and @xmath13 , is determined on the basis of comparing the features of @xmath6 and @xmath13 rather than the spaces themselves , which opens a possibility to try and construct an approximating principal manifold to @xmath6 by methods of unsupervised machine learning by optimizing over suitable sets of lipschitz functions , as in @xcite .",
    "the concept of dimension in mathematics admits a very rich spectrum of interpretations .",
    "we feel that the topological versions of dimension have been dominating applications in computing to the detriment of other approaches .",
    "we feel that the concept of dimension based on the viewpoint of asymptotic geometric analysis could be highly relevant to analysis of large sets of data , and we consider this article as a small step in the direction of developing this approach .",
    "the author is grateful to three anonymous referees of this paper for a number of suggested improvements .",
    "research was supported by nserc discovery grant and university of ottawa internal grants .",
    "blanchard , g. , kawanabe , m. , sugiyama , m. , spokoiny , v. , & mller , k .-",
    "( 2006 ) . in search of non - gaussian component of a high - dimensional distribution .",
    "journal of machine learning research , 7 , 247282 .",
    "hein , m. , & audibert , j .- y .",
    "( 2005 ) . intrinsic dimensionality estimation of submanifolds in @xmath1 . in : l. de raedt and s. wrobel ( eds . ) , proc .",
    "22nd intern .",
    "conf . on machine learning ( icml ) ( pp .",
    "289296 ) , amc press .",
    "hein , m. & maier , m. ( 2007 ) .",
    "manifold denoising as preprocessing for finding natural representations of data . in proc",
    ". twenty - second aaai conference on artificial intelligence ( vancouver , b.c . ) , pp .",
    "16461649 .",
    "traina , c. , jr . , traina , a.j.m . & faloutsos , c. ( 1999 ) .",
    "distance exponent : a new concept for selectivity estimation in metric trees .",
    "technical report cmu - cs-99 - 110 , computer science department , carnegie mellon university ."
  ],
  "abstract_text": [
    "<S> we perform a deeper analysis of an axiomatic approach to the concept of intrinsic dimension of a dataset proposed by us in the ijcnn07 paper . </S>",
    "<S> the main features of our approach are that a high intrinsic dimension of a dataset reflects the presence of the curse of dimensionality ( in a certain mathematically precise sense ) , and that dimension of a discrete i.i.d . </S>",
    "<S> sample of a low - dimensional manifold is , with high probability , close to that of the manifold . at the same time , the intrinsic dimension of a sample is easily corrupted by moderate high - dimensional noise ( of the same amplitude as the size of the manifold ) and suffers from prohibitevely high computational complexity ( computing it is an @xmath0-complete problem ) . </S>",
    "<S> we outline a possible way to overcome these difficulties </S>",
    "<S> .    intrinsic dimension of datasets , concentration of measure , curse of dimensionaity , space with metric and measure , features , gromov distance , random sample of a manifold , high - dimensional noise </S>"
  ]
}