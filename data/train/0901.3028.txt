{
  "article_text": [
    "the brain has a highly developed and complex self - generated dynamical neural activity , and this fact raises a series of interesting issues . does this self - sustained neural dynamics , its eigendynamics , have a central functional role , organizing overall cognitive computational activities ? or does this ongoing autonomous activity just serve as a kind of background with secondary computational task , like non - linear signal amplification or time encoding of neural codes ?",
    "the answer to this question is important not only to system neurobiology , but also for research in the field of cognitive computation in general .",
    "we will review here approaches based on the notion that the autonomous neural dynamics has a central regulating role for cognitive information processing .",
    "we will then argue , that this line of research constitutes an emerging field in both computational neuroscience and cognitive system research .",
    "some preliminaries , before we start .",
    "this is a mostly non - technical review with emphasis on content , an exhaustive and complete discussion of the published work on the subject is not the objective here . centrally important equations will be given and explained , but for the numerical values of the parameters involved , and for the details of the simulation set - ups , we will refer to the literature .",
    "the discussion will be given generally from the perspective of cognitive system theory , _ viz _ bearing in mind the overall requirements of prospective complete cognitive systems , akin to ones of real - world living animals @xcite .",
    "on the experimental side , the study of self - induced or autonomous neural activity in the brain has seen several developments in recent years , especially by fmri studies @xcite , and we will start by discussing some key issues arising in this respect .    the vast majority of experiments in cognitive neuroscience study the evoked neural response to certain artificial or natural sensory stimuli , often involving a given task which has been trained previously . it has been realized early on , that the neural response shows strong trial - to - trial variation , which is often as large as the response itself .",
    "this variability in the response to identical stimuli is a consequence of the ongoing internal neural activities ( for a discussion see @xcite ) .",
    "experimentally one has typically no control over the details of the internal neural state and it is custom to consider it as a source of noise , averaging it out by performing identical experiments many times over .",
    "it is on the other side well known that the majority of energy consumption of the brain is spent on internal processes @xcite , indicating that the ongoing and self - sustained brain dynamics has an important functional role .",
    "two possibilities are currently discussed :    _ ( a ) _  the internal neural activity could be in essence a random process with secondary functional roles , such as non - linear signal amplification or reservoir computing for the spatiotemporal encoding of neural signals ( for a theory review see @xcite ) .",
    "_ ( b ) _  the internal neural activity could represent the core of the cognitive information processing , being modulated by sensory stimuli , but not directly and forcefully driven by the input signals .",
    "indications for this scenario arise , e.g. , from studies of the visual information processing @xcite and of the attention system @xcite .",
    "the overall brain dynamics is still poorly understood and both possibilities ( a ) and ( b ) are likely to be relevant functionally in different areas . in this review",
    "we will focus on the ramifications resulting from the second hypothesis .",
    "there are indications , in this regard , that distinct classes of internal states generated autonomously correspond to dynamical switching cortical states , and that the time series of the spontaneous neural activity patterns is not random but determined by the degree of mutual relations @xcite .",
    "additionally , these spontaneous cortical state may be semantic in nature , having a close relation to states evoked by sensory stimuli @xcite and to neural activity patterns induced via thalamic stimulation @xcite .",
    "a second characteristics recurrently found in experimental studies is the organization of the spontaneously active states into spatially anticorrelated networks @xcite , being transiently stable in time , in terms of firing rates , with rapid switching between subsequent states @xcite .",
    "these results indicate that certain aspects of the time evolution of the self - sustained neural activity in the brain have the form of transient state dynamics , which we will discuss in detail in sect .",
    "[ sect_transient_state_dynamics ] , together with a high associative relation between subsequent states of mind .",
    "this form of spontaneous cognitive process has been termed ` associative thought process ' @xcite .",
    "it is currently under debate which aspects of the intrinsic brain dynamics is related to consciousness .",
    "the global organization of neural activity in anticorrelated and transiently stable states has been suggested , on one side , to be of relevance also for the neural foundations of consciousness @xcite , _ viz _ the ` observing self ' @xcite .",
    "the persistent default - mode network ( for a critical perspective see @xcite ) , _ viz _ the network of brain areas active in the absence of explicit stimuli processing and task performance , has been found , on the other side , to be active also under anesthetization @xcite and light sedation @xcite . it is interesting to note , in this context , that certain aspects of the default resting mode can be influenced by meditational practices @xcite .          the term ` neural transients ' characterizes evoked periods of neural activities , remaining transiently stable after the disappearance of the primary stimulating signal . in the prolonged absence of stimuli , neural architectures based on neural transients",
    "relax back to the quiescent default state .",
    "network setups based on neural transients therefore occupy a role functionally in between pure stimulus - response architectures and systems exhibiting continuously ongoing autonomous neural activity . an important class of neural architectures based on neural transients are neural reservoirs , which we discuss now briefly .    a recurrent neural net is termed a reservoir , if it is not involved in the primary cognitive information processing , having a supporting role . a typical architecture is illustrated in fig .",
    "[ figure_reservoir_dynamics ] .",
    "the reservoir is a randomly connected network of artificial neurons which generally has only a transiently stable activity in the absence of inputs , _",
    "the reservoir has a short - term memory .    in the standard mode of operation an input signal",
    "stimulates the network , giving raise to complex spatiotemporal reservoir activities .",
    "normally , there is no internal learning inside the reservoir , the intra - reservoir synaptic strengths are considered fixed .",
    "time prediction is the standard application range for reservoir computing . for this purpose the reservoir",
    "is connected to an output layer and the activities of the output neurons are compared to a teaching signal . with supervised learning , either online or off - line , the links leading from the reservoir to the output then acquire a suitable synaptic plasticity .",
    "there are two basic formulations of reservoir computing .",
    "the ` echo - state ' approach using discrete - time rate - encoding neurons @xcite , and the ` liquid state machine ' using continuous - time spiking neurons @xcite . in both cases",
    "the dimensionality of the input signal , consisting normally of just a single line , is small relative to the size of the reservoir , which may contain up to a few hundred neurons .",
    "many nonlinear signal transformations are therefore performed by the reservoir in parallel and the subsequent perceptron - like output neurons may solve complex tasks via efficient linear learning rules .",
    "neural reservoirs are possible candidates for local cortical networks like microcolumns .",
    "the bare - bones reservoir network is not self - active , but feedback links from the output to the reservoir may stabilize ongoing dynamical activity @xcite . in any case ,",
    "reservoir nets are examples of network architectures of type ( a ) , as defined in the previous section .",
    "the task of the reservoir , non - linear signal transformation , is performed automatically and has no semantic content .",
    "all information is stored in the efferent synaptic links .",
    "there is an interesting similarity , on a functional level , of reservoir computing with the notion of a ` global workspace ' @xcite .",
    "the global workspace has been proposed as a global distributed computational cortical reservoir , interacting with a multitude of peripheral local networks involving tasks like sensory preprocessing or motor output .",
    "the global workspace has also been postulated to have a central mediating role for conscious processes @xcite , representing the dominating hub nodes of a large - scale , small - world cortical network @xcite .",
    "a central question in neuroscience regards the neural code , that is the way information is transmitted and encoded ( see @xcite for reviews ) . keeping in mind that there is probably no pure information transmission in the brain , as this would be a waste of resources , that information is also processed when transmitted",
    ", one may then distinguish two issues regarding the encoding problem .    on one hand there is the question on how sensory signals are reflected , on relative short timescales , in subsequent neural activities .",
    "available neural degrees of freedom for this type of short - time encoding are the average firing rates ( rate encoding ) , transient bursts of spikes and the temporal sequence of spikes ( temporal encoding ) .",
    "in addition , the response of either individual neurons may be important , or the response of local ensembles @xcite .",
    "the subsequent sensory signal processing , on timescales typically exceeding 25 - 100ms , may , on the other hand , involve neural dynamics in terms of transiently stable activity patterns , as discussed earlier in sect .",
    "[ subsect_autonomous_brain_dynamics ] . in fig .",
    "[ figure_transient_states ] two types of model transient state activities are illustrated .",
    "alternating subsets of neurons are either active , to various degrees , or essentially silent , resulting in well characterized transient states having a certain degree of discreteness .",
    "this discreteness should be reflected , on a higher level , on the properties of the corresponding cognitive processes . of interest in this context",
    "is therefore the ongoing discussion , whether visual perception is continuous or discrete in the time domain @xcite , on timescales of the order of about 100ms , with the discrete component of perception possibly related to object recognition @xcite .",
    "transient state dynamics in the brain may therefore be related to semantic recognition , a connection also found in models for transient state dynamics based on competitive neural dynamics . in the following",
    "we will examine the occurrence and the semantic content of autonomous transient state dynamics in several proposed cognitive architectures .",
    "the concept of saddle point networks is based on the premises , ( a ) that the internal ongoing autonomous dynamics organizes the cognitive computation and ( b ) that the cognitive behavior is reproducible and deterministic in identical environments @xcite .",
    "as we will discuss in the next section , the first assumption is shared with attractor relic networks , while the second is not .",
    "technically , one considers a dynamical system , _ viz _ a set of @xmath0 first - order differential equations and the set of the respective saddle points , compare fig .  [ figure_scenarios ] .",
    "the precondition is now that every saddle point has only a single unstable direction and @xmath1 stable directions .",
    "any trajectory approaching the saddle point will then leave it with high probability close to the unique unstable separatrix and the system therefore has a unique limiting cycle attractor .",
    "this limiting cycle does not need to be a global attractor , but normally has a large basin of attraction . during one passage most ,",
    "if not all , saddle points are visited one after the other , giving raise to a transient state dynamics illustrated in fig .",
    "[ figure_transient_states ] , with the trajectory slowing down close to a saddle point .",
    "another condition for this concept to function is the formation of a heteroclinic cycle , which is of a set in phase space invariant under time evolution @xcite .",
    "implying , as illustrated in fig .",
    "[ figure_scenarios ] , that the unstable separatrix of a given saddle point needs to end up as a stable separatrix of another saddle point .",
    "such a behavior occurs usually only when the underlying differential equations are invariant under certain symmetry operations , like the exchange of variables @xcite . for any practical application",
    ", these symmetries need to be broken and the limiting cycle will vanish together with the heteroclinic sequence .",
    "it can however be restored in form of a heteroclinic channel , if the strength of the symmetry - breaking is not too strong , by adding a stochastic component to the dynamics . with noise ,",
    "a trajectory loitering around a saddle point can explore a finite region of phase space close to the saddle point until it finds the unstable direction .",
    "once the trajectory has found stochastically the unstable direction , it will leave the saddle point quickly along this direction in phase space and a heteroclinic channel is restored functionally . cognitive computation on the backbone of saddle point networks",
    "is therefore essentially based on an appropriate noise level .",
    "cognitive computation with saddle point networks has been termed ` winnerless competition ' in the context of time encoding of natural stimuli @xcite and applied to the decision making problem . in the later case interaction with the environment",
    "may generate a second unstable direction at the saddle points and decision taking corresponds to the choice of unstable separatrix taken by the trajectory @xcite .",
    "a trivial form of self - sustained neural activity occurs in attractor networks @xcite .",
    "starting with any given initial state the network state will move to the next attractor and stay there , with all neurons having a varying degree of constant firing rates , the very reason attractor nets have been widely discussed as prototypes for the neural memory @xcite . as such , an attractor network is useless for a cognitive system , as it needs outside help , or stimuli from other parts of the system , to leave the current attractor .",
    "there is a general strategy which transforms an attractor network into one exhibiting transient state dynamics , with the transient neural states corresponding to the fixpoints of the original attractor network .",
    "this procedure is applicable to a wide range of attractor networks and consists in expanding the phase space by introducing additional local variables akin to local activity reservoirs @xcite .    to be concrete ,",
    "let us denote with @xmath2 the set of dynamical variables of the attractor network , as illustrated in fig .",
    "[ figure_scenarios ] , and by @xmath3 the additional reservoir variables .",
    "we assume that the reservoirs are depleted / filled when the neuron is active / inactive , @xmath4 together with a suitable coupling of the reservoir variables @xmath3 to the neural activities @xmath2 one can easily achieve that the fixpoints of the attractor networks become unstable , _ viz _ that they are destroyed , turning into attractor ruins or attractor relics .",
    "this situation is illustrated in fig .",
    "[ figure_scenarios ] . in the expanded phase space @xmath5",
    "there are no fixpoints left .",
    "it is not the case that the attractors would just acquire additional unstable directions , upon enlargement of the phase space , turning them into saddle points .",
    "instead , the enlargement of the phase space destroys the original attractors completely .",
    "the trajectories will however still slow down considerably close to the attractor ruins , as illustrated in fig .",
    "[ figure_transient_states ] , if the reservoirs are slow variables , changing only relatively slowly with respect to the typical time constants of the original attractor network . in this case the time constant @xmath6 entering the time evolution of the reservoir , eq .",
    "( [ eq_general_dot_phi ] ) , is large . in the limit @xmath7",
    "the reservoir becomes static and the dynamics is reduced to the one of the original attractor network .",
    "the dynamics exhibited by attractor relic networks is related to the notion of chaotic itinerancy @xcite , which is characterized by trajectories wandering around chaotically in phase space , having intermittent transient periods of stability close to attractor ruins .",
    "here we consider the case of attractor relics arising from destroyed point attractors . in the general case one",
    "may also consider , e.g. , limit cycles or strange attractors .",
    "the coupling to slow variables outlined here is a standard procedure for controlling dynamical systems @xcite , and has been employed in various fashions for the generation and stabilization of transient state dynamics .",
    "one possibility is the use of dynamical thresholds for discrete - time rate - encoding neural nets @xcite . in this case one",
    "considers as a slow variable the sliding - time averaged activity of a neuron and the threshold of a neuron is",
    "increased / decreased whenever the neuron is active / inactive for a prolonged period .",
    "another approach is to add slow components to all synaptic weights for the generation of an externally provided temporal sequence of neural patterns @xcite . in the following",
    "we will outline in some detail an approach for the generation of transient state dynamics which takes an unbiased clique encoding neural net as its starting point @xcite , with the clique encoding network being a dense and homogeneous associative network ( dhan ) .",
    "transient state dynamics is intrinsically competitive in nature .",
    "when the current transient attractor becomes unstable , the subsequent transient state is selected via a competitive process .",
    "transient - state dynamics is a form of ` multi - winners - take - all ' process , with the winning coalition of dynamical variables suppressing all other competing activities @xcite .",
    "competitive processes resulting in quasi - stationary states with intermittent burst of changes are widespread , occurring in many spheres of the natural or the social sciences . in the context of darwinian evolution , to give an example , this type of dynamics has been termed ` punctuated equilibrium ' @xcite . in the context of research on the neural correlates of consciousness",
    ", these transiently stable states in form of winning coalitions of competing neural ensembles have been proposed as essential building blocks for human states of the mind @xcite .    the competitive nature of transient state dynamics",
    "is illustrated in fig .",
    "[ figure_competition ] , where a representative result of a simulation for a dhan net is presented . during the transition from one winning coalition to the subsequent",
    ", many neurons try to become members of the next winning coalition , which in the end is determined by the network geometry , the synaptic strengths and the current reservoir levels of the participating neurons .",
    "the transition periods from one transient state to the next are periods of increased dynamical sensibility . when coupling the network to sensory inputs , the input signal may tilt the balance in this competition for the next winning coalition , modulating in this way the ongoing internal dynamical activity .",
    "transient state dynamics therefore opens a natural pathway for implementing neural architectures for which , as discussed in the introduction , the eigendynamics is modulated , but not driven , by the sensory data input stream .",
    "a concrete example of how to implement this procedure will be discussed in sect .",
    "[ sect_influence_stimuli ] .",
    "only a small fraction of all neurons are active at any time in the brain in general , and in areas important for the memory consolidation in particular @xcite . for various reasons , like the optimization of energy consumption and the maximization of computational capabilities @xcite , sparse coding is an ubiquitous and powerful coding strategy @xcite .",
    "sparse coding may be realized in two ways , either by small non - overlapping neural ensembles , as in the single - winner - take - all architecture , or by overlapping neural ensembles .",
    "the latter pathway draws support from both theory considerations @xcite , and from experimental findings .",
    "experimentally , several studies of the hippocampus indicate that overlapping neural ensembles constitute important building blocks for the real - time encoding of episodic experiences and representations @xcite .",
    "these overlapping representations are not random superpositions but associatively connected .",
    "a hippocampal neuron could response , e.g. , to various pictures of female faces , but these pictures would tend to be semantically connected , e.g.  they could be the pictures of actresses from the same tv series @xcite .",
    "it is therefore likely that the memory encoding overlapping representations form an associative network , a conjecture that is also consistent with studies of free associations @xcite .",
    "there are various ways to implement overlapping neural encoding with neural nets . here",
    "we discuss the case of clique encoding .",
    "the term clique stems from graph theory and denotes , just as a clique of friends , a subgraph where ( a ) every member of the clique is connected with all other members of the clique and where ( b ) all other vertices of the graph are not connected to each member of the clique . in fig .",
    "[ figure_clique_encoding ] a small graph is given together with all of its cliques .",
    "also shown in fig .",
    "[ figure_clique_encoding ] are the associative interconnections between the cliques .",
    "one may view the resulting graph , with the cliques as vertices and with the inter - clique associative connections as edges , as a higher - level representation of an implicit hierarchical object definition @xcite . the clique ( 4,5,9 ) in the original graph in fig .",
    "[ figure_clique_encoding ] corresponds to a primary object and the meta - clique [ ( 4,5,9)-(2,4,6,7)-(4,5,6,8 ) ] in the graph of the cliques would in this interpretation encode a meta object , composed of the primary objects ( 4,5,9 ) , ( 2,4,6,7 ) and ( 4,5,6,8 ) .",
    "this intrinsic possibility of hierarchical object definitions when using clique encoding has however not yet be explored in simulations and may be of interest for future studies .",
    "cliques can be highly overlapping and there can be a very large number of cliques in any given graph @xcite .",
    "we will construct now a neural net where the cliques of the network are the attractors .",
    "it is a homogeneously random and dense associative network ( dhan ) , where the associative relations between cliques are given by the number of common vertices . starting from this attractor network we will introduce slow variables , as discussed in sect .  [ subsec_attractor_relic_networks ] , in terms of local reservoirs .",
    "the network will then show spontaneously generated transient state dynamics , with the neural cliques as the attractor ruins . in a second step",
    "we will couple the dhan net to sensory stimuli and study the interplay between the internal autonomous dynamical activity and the data input stream .",
    "we will find that the cliques acquire semantic content in this way , being mapped autonomously to the statistically independent patterns of the data input stream .",
    "the starting point of our considerations is the underlying attractor network , for which we employ a continuous time formulation , with rate encoding neurons , characterized by normalized activity levels @xmath8 $ ] .",
    "the growth rates @xmath9 govern the respective time developments , @xmath10 when @xmath11 , the respective neural activity @xmath12 increases , approaching rapidly the upper bound ; when @xmath13 , it decays to zero .",
    "we split the rates into three contributions : @xmath14 an internal positive contribution @xmath15 , an internal negative contribution @xmath16 , and the influence of external stimuli , @xmath17 .",
    "we will discuss the influence of non - trivial external stimuli in sect .",
    "[ sect_influence_stimuli ] , for the moment we consider @xmath18 . the division into an exciting and a depressing contribution in eq .",
    "( [ eq_r_pos_neg_contribution ] ) reflects on one side the well known asymmetry between excitatory neurons and inhibitory interneurons in the brain @xcite and is on the other side essential for clique encoding .",
    "the @xmath19 are determined via @xmath20 by the influence of the excitatory synaptic weights , @xmath21 .",
    "the function @xmath22 entering eq .",
    "( [ eq_r_i_plus ] ) couples the dynamics of the neurons locally to the slow variables @xmath23 .",
    "we will examine the reservoir function @xmath22 in the next section .",
    "for the time being we set @xmath24 , the primary neural dynamics is then decoupled from the reservoir dynamics and we will retain the starting attractor network . the @xmath25 are given by @xmath26 where the @xmath27 are the inhibitory synaptic weights and where @xmath28 sets the scale for the inhibition .",
    "( [ eq_r_i_neg ] ) leads to a normalization @xmath29 .",
    "we postpone the discussion of the reservoir function @xmath30 and consider for the time being @xmath31 .",
    "clique encoding corresponds to a multi - winners - take - all formulation .",
    "an inhibitory background is therefore necessary .",
    "the dhan architecture contains hence an inhibitory link @xmath32 whenever there is no excitatory link @xmath33 , @xmath34 _ viz _ the excitatory links are shunting the inhibitory synapses .",
    "this inhibitory background is implicitly present for the 9-site network shown in fig .  [ figure_clique_encoding ] .",
    "the edges of the network shown in fig .",
    "[ figure_clique_encoding ] .",
    "correspond to excitatory links @xmath35 .",
    "all pairs of sites not connected by an edge in fig .  [ figure_clique_encoding ] inhibit each other via @xmath36 .",
    "the formulation of the attractor network with clique encoding is such complete @xcite .",
    "all members of a given clique excite each other via intra - clique @xmath35 .",
    "neurons which are not members of the current active clique are suppressed by at least one inhibitory link @xmath36 .",
    "this suppression @xmath37 , compare eq .",
    "( [ eq_r_i_neg ] ) , dominates the residual positive signal the out - of - clique neuron may receive , whenever @xmath38 is large enough .",
    "an interesting feature of the dhan architecture is the absence of a bias in eq .",
    "( [ eq_x_dot ] ) .",
    "there is no self excitation or suppression , @xmath39 .",
    "the dynamics of an individual neuron is exclusively driven by the influence of the network , it has no preferred firing state .",
    "this feature would correspond for real - world neurons to the existence of a background of afferent activities with a level close to the firing threshold .",
    "next we note , that the separation of scales @xmath40 implies that hebbian - type modification of the inhibitory links @xmath32 would be meaningless , small changes of a relatively large quantity will not lead to a substantial effect .",
    "hebbian learning in the dhan architecture is therefore operational only for the excitatory links @xmath33 , in accordance to the general assumption that most learning taking place in the brain involves synapses interconnecting neurons and not interneurons @xcite . in sect .",
    "[ sect_influence_stimuli ] we will consider the synaptic plasticity of links afferent to the dhan layer .",
    "unsupervised and local hebbian - style learning can however be implemented easily for the intra - dhan excitatory links @xmath33 for unsupervised and homeostatic calibration of the excitatory links @xcite .",
    "it is however not essential for the occurrence and for the stabilization of transient state dynamics , our focus here .",
    "we consider normalized slow variables @xmath41 $ ] , with the time evolution @xmath42 where a neuron is active / inactive whenever its activity level @xmath12 is close to unity / zero .",
    "the @xmath23 behave functionally as reservoirs , being depleted / refilled for active / inactive neurons .",
    "the term @xmath43 on the rhs of eq .",
    "( [ eq_dot_phi ] ) is not essential for the establishment of transient state dynamics , but opens an interesting alternative interpretation for the slow variables .",
    "@xmath43 vanishes for inactive neurons and takes the value @xmath44 for active neurons .",
    "the reservoir levels @xmath45 of all active neurons are drawn together consequently .",
    "all members of the currently active winning coalition have then similar reservoir levels after a short time , on the order of @xmath46 .",
    "this is a behavior similar to what one would expect for groups of spiking neurons forming winning coalitions via synchronization of their spiking times . for each neuron of the winning coalitions",
    "one could define a degree of synchronization , given by the extent this neuron contributes to the overall synchronization .",
    "initially , this degree of synchronization would have a different value for each participating neuron . on a certain timescale , denoted here by @xmath46 , the spiking times",
    "would then get drawn together , synchronized , and all members of the winning coalition of active neurons would then participate to a similar degree in the synchronized firing .",
    "the firing of the winning coalition would however not remain coherent forever .",
    "internal noise and external influences would lead to a desynchronization on a somewhat longer time scale @xmath47 .",
    "when desynchronized , the winning coalition would loose stability , giving way to a new winning coalition .",
    "in this interpretation the reservoirs allow for a `` poor man s '' implementation of self organized dynamical synchronization of neural ensembles , a prerequisite for the temporal coding hypothesis of neural object definition @xcite .",
    "finally we need to specify the reservoir coupling functions @xmath22 and @xmath30 entering eqs .",
    "( [ eq_r_i_plus ] ) and ( [ eq_r_i_neg ] ) .",
    "they have sigmoidal form with @xmath48 and a straightforward interpretation : it is harder to excite a neuron with depleted reservoir , compare eq .",
    "( [ eq_r_i_plus ] ) , and a neuron with a low reservoir level has less power to suppress other neurons , see eq .",
    "( [ eq_r_i_neg ] ) .",
    "reservoir functions obeying the relation ( [ eq_res_functions ] ) therefore lead in a quite natural way to transient state dynamics . on a short time scale the system relaxes towards the next attractor ruin in the form of a neural clique .",
    "their reservoirs then slowly decrease and when depleted they can neither continue to mutually excite each other , nor can they suppress the activity of out - of - clique neurons anymore . at this point , the winning coalition becomes unstable and a new winning coalition is selected via a competitive process , as illustrated in fig .",
    "[ figure_competition ] .",
    "any finite @xmath49 leads to the destruction of the fixpoints of the original attractor network , which is thus turned into an attractor relic network .",
    "the sequence of winning coalitions , given by the cliques of the network , is however not random .",
    "subsequent active cliques are associatively connected . the clique ( 1,9 ) of the 9-site network shown in fig .  [ figure_clique_encoding ] , to give an example ,",
    "could be followed by either ( 4,5,9 ) or by ( 1,2,3 ) , since they share common sites .",
    "the competition between these two cliques will be decided by the strengths of the excitatory links and by the history of previous winning coalitions .",
    "if one of the two cliques had been activated recently , the constituent sites will still have a depressed reservoir and resist a renewed reactivation .     and cliques ( 0,1,2 ) ,",
    "... receives sensory signals via the input layer ( middle ) in the form of certain input patterns ( bottom ) .",
    ", scaledwidth=48.0% ]    the finite state dynamics of the dhan architecture is robust . for the isolated network",
    ", we will discuss the coupling to sensory input in the next section , the dynamics is relaxational and dissipative @xcite .",
    "the system relaxes to the next attractor relic and the reservoirs are relaxing either to zero or to unity , depending on the respective neural activity levels . for a network with a finite number of sites",
    ", the long - time state will be a long limiting cycle of transient states .    the simulation results shown in fig .",
    "[ figure_competition ] are for a set of parameters resulting in quite narrow transitions and long plateaus @xcite .",
    "the formulation presented here allows for the modelling of the shape of the plateaus and of other characteristics of the transient state dynamics .",
    "a smaller @xmath6 would result in shorter plateaus , a longer @xmath46 in longer transition times .",
    "one can , in addition , adjust the shape of the reservoir functions and details of eqs .",
    "( [ eq_r_i_plus ] ) and ( [ eq_r_i_neg ] ) in order to tune the overall competition for the next winning coalition .",
    "the dhan architecture providing therefore a robust framework for the generation of transient state dynamics , offering at the same time ample flexibility and room for fine tuning , paving the way for a range of different applications .",
    "the transient state dynamics generated by the dhan architecture is dynamically robust .",
    "the dhan dynamics has at the same time windows of increased sensibility to outside influences during the transition periods from one transient state to the subsequent , as shown in fig .",
    "[ figure_competition ] .",
    "these transition periods are phases of active inter - neural competition , reacting sensibly to the influence of afferent signals .",
    "we couple the input signals via an appropriate input layer , as illustrated in fig .",
    "[ figure_dhan_input ] , denoting by @xmath50 $ ] the time dependent input signals , which we will take as black - and - white or grey - scaled patterns .",
    "we denote by @xmath51 the afferent links to the dhan layer , with the external contribution to the dhan - layer growth rates , compare eq .",
    "( [ eq_r_pos_neg_contribution ] ) , given by @xmath52 the rationale behind this formulation is the following .",
    "the role of the input signal is not to destabilize the current winning coalition , the afferent signal is therefore shunted off in this case , eq .",
    "( [ eq_r_i_ext ] ) .",
    "the input signal should influence the competition for the next winning coalition , modulating but not driving directly the dhan dynamics .",
    "this rational is realized by the above formulation .",
    "inactive neurons will receive a bias @xmath53 from the input layer which increases / decreases its chance of joining the next winning coalition for @xmath54 / @xmath55 .",
    "a cognitive system with a non - trivial and self - sustained internal neural activity has to decide how and when correlations with the sensory data input stream are generated via correlations encoded in the respective synaptic plasticities .",
    "this is clearly a central issue , since the input data stream constitutes the only source for semantic content for a cognitive system .",
    "it makes clearly no sense if the afferent links to the dhan layer , _ viz _ the links leading from the input to the internal network supporting a self - sustained dynamical activity , would be modified continuously via hebbian - type rules , since the two processes , the internal and the environmental dynamics , are per se unrelated .",
    "it makes however sense to build up correlation whenever the input has an influence on the internal activity , modulating the ongoing associative thought process .",
    "from the perspective of the cognitive system such a modulation of the internal dynamics by environmental stimuli corresponds to something novel and unexpected happening .",
    "novelty detection is therefore vital for neural networks with a non - trival eigendynamics processing sensory data .",
    "the importance of novelty detection for human cognition has been acknowledged indeed since long @xcite , and a possible role of dopamine , traditionally associated with reinforcement reward transmission @xcite , for the signalling of novelty has been suggested recently @xcite .",
    "the influence of modulating and of not modulating sensory signals is illustrated in fig .",
    "[ figure_input_yes_no ] , where simulation results for a dhan layer containing seven neurons coupled to an intermittent input signal are presented .",
    "the signal is not able to deactivate a currently stable winning coalition , compare eq .",
    "( [ eq_delta_r_i ] ) , but makes an impact when active during a transition period .",
    "the system has the possibility to figure out whenever the later has happened .",
    "when the input signal @xmath17 is relevant then @xmath56 in this case the internal contribution @xmath57 to the growth rate is negative and the input makes a qualitative difference .",
    "we may therefore define a global novelty signal @xmath58 obeying @xmath59 where we have used eq .",
    "( [ eq_r_pos_neg_contribution ] ) , @xmath60 , and where a @xmath61 is implicit on the rhs of the equation .",
    "the novelty signal needs to be activated quickly , with @xmath62 .",
    "learning then takes place whenever the novelty signal @xmath63 exceeds a certain threshold .     of a dhan layer containing seven neurons ,",
    "compare fig .",
    "[ figure_dhan_input ] , the growth rates @xmath9 and the contributions from the input - layer @xmath53 , see eq .",
    "( [ eq_delta_r_i ] ) .",
    "the first input stimulus does not lead to a deviation of the transient state dynamics of the dhan layer .",
    "the second stimulus modulates the ongoing transient state dynamics , influencing the neural competition during the sensitive phase .",
    ", scaledwidth=48.0% ]     bars problem .",
    ", scaledwidth=30.0% ]     bars problem .",
    ", scaledwidth=48.0% ]      having determined when learning takes place , we have now to formulate the rules governing how learning modifies the links afferent to the dhan layer .",
    "for this purpose we will use the hebbian principle , that positive interneural correlations are enforced and negative correlations weakened .",
    "our system is however continuously active , at no point are activities or synaptic strengths reset .",
    "the hebbian principle therefore needs to be implemented as an optimization process @xcite , and not as a maximization process , which would lead to a potentially hazardous runaway growth of synaptic strengths .",
    "there are four quadrants in the @xmath64 hebbian learning matrix , corresponding to active / inactive pre- and post - synaptic neurons , out of which we use the following three optimization rules :    \\(a ) the sum over active afferent links leading to active dhan neurons is optimized to a large but finite value @xmath65 , @xmath66    \\(b ) the sum over inactive afferent links leading to active dhan neurons is optimized to a small value @xmath67 , @xmath68    \\(c ) the sum over active afferent links leading to inactive dhan neurons is optimized to a small value @xmath69 , @xmath70    the @xmath65 , @xmath69 and @xmath67 are the target values for the respective optimization processes , where the superscripts stand for ` active ' , ` inactive ' and ` orthogonal ' @xcite .",
    "these three optimization rules correspond to fan - in normalizations of the afferent synapses .",
    "positive correlations are build up whenever @xmath65 dominates in magnitude , and orthogonalization of the receptive fields to other stimuli is supported by @xmath67 .",
    "a small but non - vanishing value for @xmath69 helps to generate a certain , effective , fan - out normalization , avoiding the uncontrolled downscaling of temporarily not needed synapses .",
    "knowledge about the environment lies at the basis of all cognition , before any meaningful action can be taken by a cognitive system . for simple organisms this knowledge",
    "is implicitly encoded in the genes , but in general a cognitive system needs to extract this information autonomously from the sensory data input stream , via unsupervised online learning .",
    "this task includes signal separation and features extraction , the identification of recurrently appearing patterns , i.e.  of objects , in the background of fluctuation and of combinations of distinct and noisy patterns . for the case of linear signal superposition",
    "this problem is addressed by the independent component analysis @xcite and blind source separation @xcite , which seeks to find distinct representations of statistically independent input patterns . in order to examine how our system of an input layer coupled to a dhan layer , as illustrated in fig .",
    "[ figure_dhan_input ] , analyzes the incoming environmental signals , we have selected the bars problem @xcite .",
    "the bars problem constitutes a standard non - linear reference task for feature extraction via a non - linear independent component analysis for an @xmath71 input layer .",
    "the basic patterns are the @xmath72 vertical and @xmath72 horizontal bars and the individual input patterns are made up of a non - linear superposition of the @xmath73 basic bars , containing any of them with a certain probability @xmath74 , typically @xmath75 , as illustrated in fig .",
    "[ figure_star_pattern ] .",
    "our full system then consist of the dhan layer , which is continuously active , and an input layer coding the input patterns consisting of randomly superimposed black / white bars .",
    "for the dhan network we have taken a regular 20-site ring , containing a total of 10 cliques @xmath76 , each clique having @xmath77 sites , as illustrated in fig .",
    "[ figure_star_pattern ] .",
    "the self - sustained transient - state process is continuously active in the dhan layer , modulated by the contributions @xmath78 it receives via the links @xmath51 from the input layer . for the simulation",
    "a few thousands of input patterns were presented to the system @xcite .    in fig .",
    "[ figure_crp_graph ] we present for the @xmath79 bars problem the simulation results for the susceptibility @xmath80 of the 10 cliques @xmath81 in the dhan layer to the 10 basic patterns @xmath82 , the 10 individual horizontal and vertical bars , with @xmath83 , @xmath84 , and so on .",
    "all cliques have the size @xmath85 and the notation @xmath86 denotes the set of all sites defining the clique @xmath81 . at the start",
    "all @xmath51 are drawn randomly .",
    "the result is quite remarkable . at the beginning of the simulation the system undergoes an associative thought process without semantic",
    "content . during the course of the simulation , via the competitive novelty learning scheme , the individual attractor relics of the transient state dynamics , the cliques of the dhan layer , acquire a semantic connotation , having developed pronounced susceptibilities to statistically distinct objects in the sensory data input stream .",
    "this can be seen directly inspecting the clique receptive fields @xmath87 of the @xmath88 cliques in the dhan layer with respect to the @xmath89 input neurons , which are also presented in fig .",
    "[ figure_crp_graph ] .",
    "the clique receptive fields @xmath90 correspond to the averaged receptive fields of their constituent neurons .",
    "the data presented in fig .  [ figure_crp_graph ]",
    "are for the @xmath79 bars problem .",
    "we note that simulation for larger systems can be performed as well , with similar results @xcite .",
    "the learning scheme employed here is based on optimization and not on maximization , as stressed in sect .",
    "[ subsec_afferent_link_plasticity ] .",
    "the clique receptive fields , shown in fig",
    ".  [ figure_crp_graph ] , are therefore not of black / white type , but differentiated .",
    "synaptic modifications are turned progressively off when sufficient signal separation has been achieved .",
    "this behavior is consistent with the ` learning by error ' paradigm @xcite , which states that a cognitive system learns mostly when making errors and not when performing well .",
    "we may take a look at the results presented in fig .",
    "[ figure_crp_graph ] from a somewhat larger perspective .",
    "the neural activity of newborn animals consists of instinct - like reflexes and homeostatic regulation of bodily functions .",
    "the processing of the sensory signals has not yet any semantic content and internal neural activity states do not correspond yet to environmental features like shapes , colors and objects .",
    "the neural activity can acquire semantic content , philosophical niceties apart , only through interaction with the environment .",
    "this is a demanding task , since the optical or acoustical sensory signals are normally overloaded with a multitude of overlapping primary objects .",
    "the animal therefore needs to separate these non - linearly superposed signals for the acquisition of primary knowledge about the environment and to map the independent signals , the environmental object to distinct neural activity patters .",
    "this very basic requirement is performed by the dhan architecture .",
    "the internal transient states have , at the start of the simulation , no relation to environmental objects and are therfore void of semantic content . in the simulation presented here",
    ", there are 10 primary environmental objects , the 5 horizontal and vertical bars of the @xmath79 bars problem . in the setting",
    "used these 10 objects are independent and statistically uncorrelated . during the course of the unsupervised and online learning process",
    ", the receptive fields of the transiently stable neural states , the cliques in the dhan layer , acquire distinct susceptibilities not to arbitrary superpositions of the primary objects but to the individual primary bars themselves . a sensory signal consisting of the non - linear superposition of two or more bars",
    "will therefore lead , in general , to the activation of one of the corresponding cliques . to be concrete , comparing fig .",
    "[ figure_crp_graph ] , an input signal containing both the top - most and the bottom - most horizontal bar would activate either the clique @xmath91 or the clique @xmath92 .",
    "these two cliques will enter the competition for the next winning coalition whenever the input is not too weak and when it overlapps with a sensitive period .",
    "the present state together with its dynamical attention field @xcite will then determine the outcome of this competitions and one of the two objects present in this input signal is then recognized .",
    "the vast majority of neural nets considered to date for either research purposes , or for applications , are generalized stimulus - response networks @xcite .",
    "one has typically an input signal and an output result , as , e.g. , in speech recognition . in most settings the network is reset to a predefined default state after a given task",
    "is completed , and before the next input signal is provided .",
    "this approach is highly successful , in many instances , but it is clearly not the way the brain works on higher levels . it is therefore important to examine a range of paradigmal formulations for the non - trivial eigendynamics of cognitive systems , evaluating their characteristics and computational capabilities .    as an example for a concept situated somewhere in between a pure stimulus response net and systems with a fully developed eigendynamics , we have discussed in sect .  [ subsec_reservoir_computing ] the notion of reservoir computing . for reservoir networks",
    "the dynamics is , in general , still induced by the input signal and decays slowly in the absence of any input .",
    "any given stimulus encounters however an already active reservoir net , with the current reservoir activity caused by the preceding stimuli .",
    "the response of the network therefore depends on the full history of input signals and time prediction tasks constitute consequently the standard applications scenaria for reservoir computing .",
    "a somewhat traditional view , often presumed implicitly , is that the eigendynamics of the brain results from the recurrent interlinking of specialized individual cognitive modules .",
    "this viewpoint would imply , that attempts to model the autonomous brain dynamics can be considered only after a thorough understanding of the individual constituent modules has been achieved .",
    "here we have examined an alternative route , considering it to be important to examine the mutual benefits and computational capabilities of a range of theory proposals for the overall organization of the eigendynamics .    in sect .",
    "[ subsec_saddle_point_networks ] we have examined a first proposal for the organization of the eigendynamics in terms of saddle point networks . in this framework",
    "the internal neural dynamics is guided by heteroclines in a process denoted winnerless competition .",
    "this neural architecture aims to model reproducible cognitive behavior and a single robust attractor in terms of a heteroclinic channel constitutes the eigendynamics in the absence of sensory inputs .    in sect .",
    "[ subsec_attractor_relic_networks ] we have examined the viewpoint that a non - trivial associative thought process constitutes the autonomous dynamics in the absence of sensory input . for any finite ( and isolated ) network",
    "these thought processes turn eventually into limiting cycles of transient states . in this architecture",
    "there is however not a unique limiting cycle , but many possible and overlapping thought processes , every one having its respective basin of attractions .",
    "the transient state dynamics required for this approach is obtained by coupling an attractor network to slow variables , with the neural time evolution slowing down near the such obtained attractor relics .",
    "this is a quite general procedure and a wide range of concrete implementations are feasible for this concept .",
    "the coupling of neural nets having a non - trivial eigendynamics to the sensory input is clearly a central issue , which we have discussed in depth in sect .",
    "[ sect_influence_stimuli ] , for the case of networks with transient state dynamics based on attractor ruins , emphasizing two functional principles in this context :    \\(a )  the internal transient state dynamics is based intrinsically on the notion of competitive neural dynamics .",
    "it is therefore consistent to assume that the sensory input contributes to this neural competition , modulating the already ongoing internal neural competition .",
    "the sensory input would therefore have a modulating and not a forcing influence .",
    "the sensory signals would in particular not deactivate a currently stable winning coalition , influencing however the transition from one transiently stable state to the subsequent winning coalition .",
    "\\(b )  the eigendynamics of the cognitive system and of the sensory signals resulting from environmental activities are , a priori , unrelated dynamically",
    ". correlations between these two dynamically independent processes should therefore be built up only when a modulation of the internal neural activity through the sensory signal has actually occurred .",
    "this modulation of the eigendynamics by the input data stream should then generate an internal reinforcement signal , which corresponds to a novelty signal , as the deviation of the internal thought process by the input is equivalent , from the perspective of the cognitive system , to something unexpected happening .",
    "we have shown , that these two principles can be implemented in a straightforward manner , resulting in what one could call an ` emergent cognitive capability ' .",
    "the system performs , under the influence of the above two general operating guidelines , autonomously a non - linear independent component analysis .",
    "statistically independent object in the sensory data input stream are mapped during the life time of the cognitive system to the attractor relics of the transient state network .",
    "the internal associative thought process acquires thus semantic content , with the time series of transient states , the attractor ruins , now corresponding to objects in the environment .",
    "we believe that these results are encouraging and that the field of cognitive computation with autonomously active neural nets is an emerging field of growing importance",
    ". it will be important to study alternative guiding principles for the neural eigendynamics , for the coupling of the internal autonomous dynamics to sensory signals and for the decision making process leading to motor output .",
    "architectures built up of interconnected modules of autonomously active neural nets may in the end open a pathway towards the development of evolving cognitive systems .",
    "gros c. emotions , diffusive emotional control and the motivational problem for autonomous cognitive systems . in : vallverdu j , casacuberta d , editors .",
    "handbook of research on synthetic emotions and sociable robotics : new applications in affective computing and artificial intelligence .",
    "igi - global ; 2009 , in press .",
    "fox md , corbetta m , snyder az , vincent jl , raichle me .",
    "spontaneous neuronal activity distinguishes human dorsal and ventral attention systems .",
    "proceedings of the national academy of sciences 2003;103:10046 - 10051 .",
    "fox md , snyder az , vincent jl , corbetta m , van essen dc , raichle me . the human brain is intrinsically organized into dynamic , anticorrelated functional networks .",
    "proceedings of the national academy of sciences 2005;102:96739678 .",
    "lin l , osan r , shoham s , jin w , zuo w , tsien jz .",
    "identification of network - level coding units for real - time representation of episodic experiences in the hippocampus .",
    "proceedings of the national academy of sciences 2005;102:6125 - 613 ."
  ],
  "abstract_text": [
    "<S> the human brain is autonomously active . to understand the functional role of this self - sustained neural activity , and its interplay with the sensory data input stream , is an important question in cognitive system research and we review here the present state of theoretical modelling </S>",
    "<S> .    this review will start with a brief overview of the experimental efforts , together with a discussion of transient vs.  self - sustained neural activity in the framework of reservoir computing </S>",
    "<S> . the main emphasis will be then on two paradigmal neural network architectures showing continuously ongoing transient - state dynamics : saddle point networks and networks of attractor relics .    </S>",
    "<S> self - active neural networks are confronted with two seemingly contrasting demands : a stable internal dynamical state and sensitivity to incoming stimuli . </S>",
    "<S> we show , that this dilemma can be solved by networks of attractor relics based on competitive neural dynamics , where the attractor relics compete on one side with each other for transient dominance , and on the other side with the dynamical influence of the input signals .    </S>",
    "<S> unsupervised and local hebbian - style online learning then allows the system to build up correlations between the internal dynamical transient states and the sensory input stream . </S>",
    "<S> an emergent cognitive capability results from this set - up . </S>",
    "<S> the system performs online , and on its own , a non - linear independent component analysis of the sensory data stream , all the time being continuously and autonomously active . </S>",
    "<S> this process maps the independent components of the sensory input onto the attractor relics , which acquire in this way a semantic meaning . </S>"
  ]
}