{
  "article_text": [
    "in the past decade , high - dimensional data analysis has received broad research interests in data mining and scientific discovery , with many significant results obtained in theory , algorithm and applications .",
    "the major driven force is the rapid development of data collection technologies in many applications domains such as social networks , natural language processing , bioinformatics and computer vision . in these applications",
    "it is not unusual that data samples are represented with millions or even billions of features using which an underlying statistical learning model must be fit . in many circumstances , however , the number of collected samples is substantially smaller than the dimensionality of the feature , implying that consistent estimators can not be hoped for unless additional assumptions are imposed on the model .",
    "one of the widely acknowledged prior assumptions is that the data exhibit low - dimensional structure , which can often be captured by imposing sparsity constraint on the model parameter space .",
    "it is thus crucial to develop robust and efficient computational procedures for solving , even just approximately , these optimization problems with sparsity constraint .    in this paper",
    ", we focus on the following generic sparsity - constrained optimization problem @xmath0 where @xmath1 is a smooth convex cost function . among others ,",
    "several examples falling into this model include : ( i ) sparsity - constrained linear regression model  @xcite where the residual error is used to measure data reconstruction error ; ( ii ) sparsity - constrained logistic regression model  @xcite where the sigmoid loss is used to measure prediction error ; ( iii ) sparsity - constrained graphical model learning  @xcite where the likelihood of samples drawn from an underlying probabilistic model is used to measure data fidelity .    unfortunately , due to the non - convex cardinality constraint , the problem   is generally np - hard even for the quadratic cost function  @xcite .",
    "thus , one must instead seek approximate solutions . in particular ,",
    "the special case of   in least square regression models has gained significant attention in the area of compressed sensing  @xcite .",
    "a vast body of greedy selection algorithms for compressing sensing have been proposed including matching pursuit  @xcite , orthogonal matching pursuit  @xcite , compressive sampling matching pursuit  @xcite , hard thresholding pursuit  @xcite , iterative hard thresholding  @xcite and subspace pursuit  @xcite to name a few .",
    "these algorithms successively select the position of nonzero entries and estimate their values via exploring the residual error from the previous iteration . comparing to those first - order convex optimization methods developed for @xmath2-regularized sparse learning  @xcite",
    ", these greedy selection algorithms often exhibit similar accuracy guarantees but more attractive computational efficiency .",
    "the least square error used in compressive sensing , however , is not an appropriate measure of discrepancy in a variety of applications beyond signal processing .",
    "for example , in statistical machine learning the log - likelihood function is commonly used in logistic regression problems  @xcite and graphical models learning  @xcite .",
    "thus , it is desirable to investigate theory and algorithms applicable to a broader class of sparsity - constrained learning problems as given in  . to this end",
    ", several forward selection algorithms have been proposed to select the nonzero entries in a sequential fashion  @xcite .",
    "this category of methods date back to the frank - wolfe method  @xcite .",
    "the forward greedy selection method has also been generalized to minimize a convex objective over the linear hull of a collection of atoms  @xcite . to make the greedy selection procedure more adaptive , @xcite proposed a forward - backward algorithm which takes backward steps adaptively whenever beneficial .",
    "@xcite have applied this forward - backward selection method to learn the structure of a sparse graphical model .",
    "more recently , @xcite proposed a gradient hard - thresholding method which generalizes the compressive sampling matching pursuit method  @xcite from compressive sensing to the general sparsity - constrained optimization problem .",
    "the hard - threshholding - type methods have also been shown to be statistically and computationally efficient for sparse principal component analysis  @xcite .      in this paper , inspired by the success of hard thresholding pursuit ( htp )  @xcite in compressive",
    "sensing , we propose the gradient hard thresholding pursuit ( grahtp ) method to encompass the sparse estimation problems arising from applications with general nonlinear models . at each iteration , grahtp performs standard gradient descent followed by a hard thresholding operation which first selects the top @xmath3 ( in magnitude ) entries of the resultant vector and then ( optionally ) conducts debiasing on the selected entries .",
    "we prove that under mild conditions grahtp ( with or without debiasing ) has strong theoretical guarantees analogous to htp in terms of convergence rate and parameter estimation accuracy .",
    "we have applied grahtp to the sparse logistic regression model and the sparse precision matrix estimation model , verifying that the guarantees of htp are valid for these two models .",
    "empirically we demonstrate that grahtp is comparable or superior to the state - of - the - art greedy selection methods in these two sparse learning models . to our knowledge , grahtp is the first gradient - descent - truncation - type method for sparsity constrained nonlinear problems .      in the following ,",
    "@xmath4 is a vector , @xmath5 is an index set and @xmath6 is a matrix .",
    "the following notations will be used in the text .",
    "* @xmath7_i$ ] : the @xmath8th entry of vector @xmath9 .",
    "* @xmath10 : the restriction of @xmath9 to index set @xmath5 , i.e. , @xmath11_{i}=[x]_i$ ] if @xmath12 , and @xmath11_{i}=0 $ ] otherwise . *",
    "@xmath13 : the restriction of @xmath9 to the top @xmath3 ( in modulus ) entries .",
    "we will simplify @xmath14 to @xmath13 without ambiguity in the context .",
    "* @xmath15 : the euclidean norm of @xmath9 .",
    "* @xmath16 : the @xmath2-norm of @xmath9 . *",
    "@xmath17 : the number of nonzero entries of @xmath9 . * @xmath18 : the index set of nonzero entries of @xmath9 .",
    "* @xmath19 : the index set of the top @xmath3 ( in modulus ) entries of @xmath9 . * @xmath20_{ij}$ ] : the element on the @xmath8th row and @xmath21th column of matrix @xmath6 .",
    "* @xmath22 : the spectral norm of matrix @xmath6 . * @xmath23 ( @xmath24 ): the rows ( columns ) of matrix @xmath6 indexed in @xmath5 . * @xmath25_{ij}|$ ] : the element - wise @xmath2-norm of @xmath6 .",
    "* @xmath26 : the trace ( sum of diagonal elements ) of a square matrix @xmath6 .",
    "* @xmath27 : the restriction of a square matrix @xmath6 on its off - diagonal entries * @xmath28 : ( column wise ) vectorization of a matrix @xmath6 .",
    "this paper proceeds as follows : we present in   [ sect : grahtp ] the grahtp algorithm .",
    "the convergence guarantees of grahtp are provided in   [ sect : theory ] .",
    "the specializations of grahtp in logistic regression and gaussian graphical models learning are investigated in   [ sect : applications ] .",
    "monte - carlo simulations and experimental results on real data are presented in   [ sect : experiments ] .",
    "we conclude this paper in   [ sect : conclusion ] .",
    "grahtp is an iterative greedy selection procedure for approximately optimizing the non - convex problem  .",
    "a high level summary of grahtp is described in the top panel of algorithm  [ alg : grahtp ] .",
    "the procedure generates a sequence of intermediate @xmath3-sparse vectors @xmath29 from an initial sparse approximation @xmath30 ( typically @xmath31 ) . at the @xmath32-th iteration",
    ", the first step * s1 * , @xmath33 , computes the gradient descent at the point @xmath34 with step - size @xmath35 .",
    "then in the second step , * s2 * , the @xmath3 coordinates of the vector @xmath36 that have the largest magnitude are chosen as the support in which pursuing the minimization will be most effective . in the third step , * s3 * , we find a vector with this support which minimizes the objective function , which becomes @xmath37 .",
    "this last step , often referred to as _ debiasing _ , has been shown to improve the performance in other algorithms too .",
    "the iterations continue until the algorithm reaches a terminating condition , e.g. , on the change of the cost function or the change of the estimated minimum from the previous iteration .",
    "a natural criterion here is @xmath38 ( see * s2 * for the definition of @xmath39 ) , since then @xmath40 for all @xmath41 , although there is no guarantee that this should occur .",
    "it will be assumed throughout the paper that the cardinality @xmath3 is known . in practice this quantity may be regarded as a tuning parameter of the algorithm via , for example , cross - validations .    in the standard form of grahtp",
    ", the debiasing step * s3 * requires to minimize @xmath42 over the support @xmath39 .",
    "if this step is judged too costly , we may consider instead a fast variant of grahtp , where the debiasing is replaced by a simple truncation operation @xmath43 .",
    "this leads to the fast grahtp ( fgrahtp ) described in the bottom panel of algorithm  [ alg : grahtp ] .",
    "it is interesting to note that fgrahtp can be regarded as a projected gradient descent procedure for optimizing the non - convex problem  .",
    "its per - iteration computational overload is almost identical to that of the standard gradient descent procedure .",
    "while in this paper we only study the fgrahtp outlined in algorithm 1 , we should mention that other fast variants of grahtp can also be considered .",
    "for instance , to reduce the computational cost of * s3 * , we can take a restricted newton step or a restricted gradient descent step to calculate @xmath37 .",
    "we close this section by pointing out that , in the special case where the squared error @xmath44 is the cost function , grahtp reduces to htp  @xcite .",
    "specifically , the gradient descent step s1 reduces to @xmath45 and the debiasing step s3 reduces to the orthogonal projection @xmath46 . in the meanwhile",
    ", fgrahtp reduces to iht  @xcite in which the iteration is defined by @xmath47 .",
    "[ alg : grahtp ]    @xmath48 _ fast grahtp _",
    "in this section , we analyze the theoretical properties of grahtp and fgrahtp .",
    "we first study the convergence of these two algorithms .",
    "next , we investigate their performances for the task of sparse recovery in terms of convergence rate and parameter estimation accuracy . we require the following key technical condition under which the convergence and parameter estimation accuracy of grahtp / fgrahtp can be guaranteed . to simplify the notation in the following analysis , we abbreviate @xmath49 and @xmath50 .    [ assump_1 ] for any integer @xmath51",
    ", we say @xmath52 satisfies condition @xmath53 if for any index set @xmath5 with cardinality @xmath54 and any @xmath55 with @xmath56 , the following inequality holds for some @xmath57 and @xmath58 : @xmath59    in the special case where @xmath42 is least square loss function and @xmath60 , condition @xmath53 reduces to the well known _ restricted isometry property _ ( rip ) condition in compressive sensing .",
    "we may establish the connections between condition @xmath53 and the conditions of restricted strong convexity / smoothness which are key to the analysis of several previous greedy selection methods  @xcite .    for any integer @xmath51",
    ", we say @xmath42 is restricted @xmath61-strongly convex and @xmath62-strongly smooth if there exist @xmath63 such that @xmath64    the following lemma connects condition @xmath53 to the restricted strong convexity / smoothness conditions .",
    "[ lemma : strong_smooth ] assume that @xmath52 is a differentiable function .",
    "* if @xmath52 satisfies condition @xmath53 , then for all @xmath65 the following two inequalities hold : @xmath66 * if @xmath52 is @xmath61-strongly convex and @xmath62-strongly smooth , then @xmath52 satisfies condition @xmath53 with any @xmath67    a proof of this lemma is provided in appendix  [ append : proof_lemma_strong_smooth ] .",
    "the part(a ) of lemma  [ lemma : strong_smooth ] indicates that if condition @xmath53 holds , then @xmath52 is strongly smooth .",
    "the part(b ) of lemma  [ lemma : strong_smooth ] shows that the strong smoothness / convexity conditions imply condition @xmath53 .",
    "therefore , condition @xmath53 is no stronger than the strong smoothness / conveixy conditions .",
    "we now analyze the convergence properties of grahtp and fgrahtp .",
    "first and foremost , we make a simple observation about grahtp : since there is only a finite number of subsets of @xmath68 of size @xmath3 , the sequence defined by grahtp is eventually periodic .",
    "the importance of this observation lies in the fact that , as soon as the convergence of grahtp is established , then we can certify that the limit is exactly achieved after a finite number of iterations .",
    "we establish in theorem  [ thrm : convergence ] the convergence of grahtp and fgrahtp under proper conditions . a proof of this theorem is provided in appendix  [ append : proof_thrm_convergence ] .",
    "[ thrm : convergence ] assume that @xmath52 satisfies condition @xmath69 and the step - size @xmath70 .",
    "then the sequence @xmath71 defined by grahtp converges in a finite number of iterations .",
    "moreover , the sequence @xmath72 defined by fgrahtp converges .    since @xmath73",
    ", we have that the convergence results in theorem  [ thrm : convergence ] hold whenever the step - size @xmath74 . if @xmath52 is @xmath61-strongly convex and @xmath62-strongly smooth , then from part(b ) of lemma  [ lemma : strong_smooth ] we know that theorem  [ thrm : convergence ] holds whenever the step - size @xmath75 .",
    "the following theorem is our main result on the parameter estimation accuracy of grahtp and fgrahtp when the target solution is sparse .",
    "[ thrm : recovery_grahtp ] let @xmath76 be an arbitrary @xmath77-sparse vector and @xmath78 .",
    "let @xmath79 .",
    "if @xmath52 satisfies condition @xmath53 and @xmath80 ,    * if @xmath81 , then at iteration @xmath32 , grahtp will recover an approximation @xmath37 satisfying @xmath82 * if @xmath83 , then at iteration @xmath32 , fgrahtp will recover an approximation @xmath37 satisfying @xmath84    a proof of this theorem is provided in appendix  [ append : proof_thrm_recovery_grahtp ] . note that we did not make any attempt to optimize the constants in theorem  [ thrm : recovery_grahtp ] , which are relatively loose . in the discussion , we ignore the constants and focus on the main message theorem  [ thrm : recovery_grahtp ] conveys .",
    "the part ( a ) of theorem  [ thrm : recovery_grahtp ] indicates that under proper conditions , the estimation error of grahtp is determined by the multiple of @xmath85 , and the rate of convergence before reaching this error level is geometric . particularly , if the sparse vector @xmath76 is sufficiently close to an unconstrained minimum of @xmath52 then the estimation error floor is negligible because @xmath86 has small magnitude . in the ideal case where @xmath87 ( i.e.",
    ", the sparse vector @xmath76 is an unconstrained minimum of @xmath52 ) , this result guarantees that we can recover @xmath76 to arbitrary precision .",
    "in this case , if we further assume that @xmath35 satisfies the conditions in theorem  [ thrm : convergence ] , then exact recovery can be achieved in a _",
    "finite _ number of iterations .",
    "the part ( b ) of theorem  [ thrm : recovery_grahtp ] shows that fgrahtp enjoys a similar geometric rate of convergence and the estimation error is determined by the multiple of @xmath88 with @xmath79 .",
    "the shrinkage rates @xmath89 ( see part ( a ) ) and @xmath90 ( see part ( b ) ) respectively control the convergence rate of grahtp and fgrahtp .",
    "for grahtp , the condition @xmath91 implies @xmath92 by combining this condition with @xmath93 , we can see that @xmath94 is a necessary condition to guarantee @xmath91 . on the other side ,",
    "if @xmath95 , then we can always find a step - size @xmath96 satisfying   such that @xmath91 .",
    "this condition of @xmath97 is analogous to the rip condition for estimation from noisy measurements in compressive sensing  @xcite .",
    "indeed , in this setup our grahtp algorithm reduces to htp which requires weaker rip condition than prior compressive sensing algorithms .",
    "the guarantees of grahtp and htp are almost identical , although we did not make any attempt to optimize the rip sufficient constants , which are @xmath98 ( for grahtp ) versus @xmath99 ( for htp ) .",
    "we would like to emphasize that the condition @xmath100 derived for grahtp also holds in fairly general setups beyond compressive sensing .",
    "for fgrahtp we have similar discussions .    for the general sparsity - constrained optimization problem , we note that a similar estimation error bound has been established for the grasp ( gradient support pursuit ) method  @xcite which is another hard - thresholding - type method . at time",
    "stamp @xmath32 , grasp first conducts debiasing over the union of the top @xmath3 entries of @xmath34 and the top @xmath101 entries of @xmath102 , then it selects the top @xmath3 entries of the resultant vector and updates their values via debiasing , which becomes @xmath37 .",
    "our grahtp is connected to grasp in the sense that the @xmath3 largest absolute elements after the gradient descent step ( see * s1 * and * s2 * of algorithm  [ alg : grahtp ] ) will come from some combination of the largest elements in @xmath34 and the largest elements in the gradient @xmath102 .",
    "although the convergence rate are of the same order , the per - iteration cost of grahtp is cheaper than grasp . indeed , at each iteration , grasp needs to minimize the objective over a support of size @xmath103 while that size for grahtp is @xmath3 .",
    "fgrahtp is even cheaper for iteration as it does not need any debiasing operation .",
    "we will compare the actual numerical performances of these methods in our empirical study .",
    "in this section , we will specialize grahtp / fgrahtp to two popular statistical learning models : the sparse logistic regression ( in  [ ssect : logistic_model ] ) and the sparse precision matrix estimation ( in  [ ssect : ggm ] ) .      logistic regression is one of the most popular models in statistics and machine learning  @xcite . in this model",
    "the relation between the random feature vector @xmath105 and its associated random binary label @xmath106 is determined by the conditional probability @xmath107 where @xmath108 denotes a parameter vector .",
    "given a set of @xmath109 independently drawn data samples @xmath110 , logistic regression learns the parameters @xmath111 so as to minimize the logistic log - likelihood given by @xmath112 it is well - known that @xmath113 is convex .",
    "unfortunately , in high - dimensional setting , i.e. , @xmath114 , the problem can be underdetermined and thus its minimum is not unique .",
    "a conventional way to handle this issue is to impose @xmath104-regularization to the logistic loss to avoid singularity .",
    "the @xmath104-penalty , however , does not promote sparse solutions which are often desirable in high - dimensional learning tasks .",
    "the sparsity - constrained @xmath104-regularized logistic regression is then given by : @xmath115 where @xmath116 is the regularization strength parameter .",
    "obviously @xmath117 is @xmath118-strongly convex and hence it has a unique minimum .",
    "the cardinality constraint enforces the solution to be sparse .",
    "let @xmath119\\in\\mathbb{r}^{p\\times n}$ ] be the design matrix and @xmath120 be the sigmoid function .",
    "in the case of @xmath104-regularized logistic loss considered in this section we have @xmath121 where the vector @xmath122 is given by @xmath123_i=-2v^{(i)}(1-\\sigma(2v^{(i)}w^\\top u^{(i)}))$ ] .",
    "the following result verifies that @xmath117 satisfies condition @xmath53 under mild conditions .",
    "[ prop : logistic_assump1 ] assume that for any index set @xmath5 with @xmath124 we have @xmath125 , @xmath126 .",
    "then the @xmath104-regularized logistic loss satisfies condition @xmath53 with any @xmath127    a proof of this result is given in appendix  [ append : proof_prop_logistic_assump1 ] .",
    "we are going to bound @xmath128 which we obtain from theorem  [ thrm : recovery_grahtp ] that controls the estimation error bounds of grahtp ( with @xmath129 ) and fgrahtp ( with @xmath130 ) . in the following deviation ,",
    "we assume that the joint density of the random vector @xmath131 is given by the following exponential family distribution : @xmath132 where @xmath133 is the log - partition function .",
    "the term @xmath134 characterizes the marginal behavior of @xmath135 .",
    "obviously , the conditional distribution of @xmath136 given @xmath135 , @xmath137 , is given by the logistical model  . by trivial algebra",
    "we can obtain the following standard result which shows that the first derivative of the logistic log - likelihood @xmath113 yields the cumulants of the random variables @xmath138_j$ ]  ( see , e.g. , * ? ? ?",
    "* ) : @xmath139_j } = \\frac{1}{n}\\sum_{i=1}^n \\left\\{-v^{(i ) } [ u^{(i)}]_j + \\mathbb{e}_{v}[v[u^{(i)}]_j\\mid u^{(i)}]\\right\\}.\\ ] ] here the expectation @xmath140 $ ] is taken over the conditional distribution  .",
    "we introduce the following sub - gaussian condition on the random variate @xmath138_j$ ] .",
    "[ assump : tail_1 ] for all @xmath21 , we assume that there exists constant @xmath141 such that for all @xmath35 , @xmath142_j ) ] \\le \\exp\\left(\\sigma^2\\eta^2/2\\right).\\ ] ]    this assumption holds when @xmath143_j$ ] are sub - gaussian ( e.g. , gaussian or bounded ) random variables .",
    "the following result establishes the bound of @xmath128 .",
    "[ prop_logsitic ] if assumption  [ assump : tail_1 ] holds , then with probability at least @xmath144 , @xmath145    a proof of this result can be found in appendix  [ append : proof_prop_logsitic ] .",
    "if we choose @xmath146 , then with overwhelming probability @xmath128 vanishes at the rate of @xmath147 .",
    "this bound is superior to the bound provided by  ( * ? ? ?",
    "* section 4.2 ) which is non - vanishing .",
    "an important class of sparse learning problems involves estimating the precision ( inverse covariance ) matrix of high dimensional random vectors under the assumption that the true precision matrix is sparse .",
    "this problem arises in a variety of applications , among them computational biology , natural language processing and document analysis , where the model dimension may be comparable or substantially larger than the sample size .",
    "let @xmath9 be a @xmath148-variate random vector with zero - mean gaussian distribution @xmath149 .",
    "its density is parameterized by the precision matrix @xmath150 as follows : @xmath151 it is well known that the conditional independence between the variables @xmath7_i$ ] and @xmath7_j$ ] given @xmath152_k , k \\neq i , j\\}$ ] is equivalent to @xmath153_{ij } = 0 $ ] .",
    "the conditional independence relations between components of @xmath9 , on the other hand , can be represented by a graph @xmath154 in which the vertex set @xmath155 has @xmath148 elements corresponding to @xmath7_1, ...",
    ",[x]_p$ ] , and the edge set @xmath156 consists of edges between node pairs @xmath152_i,[x]_j\\}$ ] .",
    "the edge between @xmath7_i$ ] and @xmath7_j$ ] is excluded from @xmath156 if and only if @xmath7_i$ ] and @xmath7_j$ ] are conditionally independent given other variables .",
    "this graph is known as gaussian markov random field ( gmrf )  @xcite .",
    "thus for multivariate gaussian distribution , estimating the support of the precision matrix @xmath157 is equivalent to learning the structure of gmrf @xmath158 .",
    "given i.i.d .",
    "samples @xmath159 drawn from @xmath160 , the negative log - likelihood , up to a constant , can be written in terms of the precision matrix as @xmath161 where @xmath162 is the sample covariance matrix .",
    "we are interested in the problem of estimating a sparse precision @xmath163 with no more than a pre - specified number of off - diagonal non - zero entries . for this purpose , we consider the following cardinality constrained log - determinant program : @xmath164 where @xmath165 is the restriction of @xmath166 on the off - diagonal entries , @xmath167 is the cardinality of the support set of @xmath165 and integer @xmath168 bounds the number of edges @xmath169 in gmrf .",
    "it is easy to show that the hessian @xmath170 , where @xmath171 is the kronecker product operator .",
    "the following result shows that @xmath172 satisfies condition @xmath53 if the eigenvalues of @xmath166 are lower bounded from zero and upper bounded .",
    "[ prop : logdet_srh ] suppose that @xmath173 and @xmath174 for some @xmath175 .",
    "then @xmath176 satisfies condition @xmath53 with any @xmath177    due to the fact that the eigenvalues of kronecker products of symmetric matrices are the products of the eigenvalues of their factors , it holds that @xmath178 therefore we have @xmath179 which implies that @xmath176 is @xmath180-strongly convex and @xmath181-strongly smooth .",
    "the desired result follows directly from the part(b ) of lemma  [ lemma : strong_smooth ] .",
    "motivated by proposition  [ prop : logdet_srh ] , we consider applying grahtp to the following modified version of problem  : @xmath182 where @xmath183 are two constants which respectively lower and upper bound the eigenvalues of the desired solution . to roughly estimate @xmath184 and @xmath185",
    ", we employ a rule proposed by  ( * ? ? ?",
    "* proposition 3.1 ) for the @xmath2 log - determinant program . specifically , we set @xmath186 where @xmath187 is a small enough positive number ( e.g. , @xmath188 as utilized in our experiments ) .",
    "let @xmath189 .",
    "it is known from theorem  [ thrm : recovery_grahtp ] that the estimation error is controlled by @xmath190 . since @xmath191 , we have @xmath192 .",
    "it is known that @xmath193 with probability at least @xmath194 for some positive constants @xmath195 and @xmath196 and sufficiently large @xmath109  ( see , e.g. , * ? ? ?",
    "* lemma 1 ) . therefore with overwhelming probability",
    "we have @xmath197 when @xmath109 is sufficiently large .",
    "unfortunately , grahtp is not directly applicable to the problem   due to the presence of the constraint @xmath198 in addition to the sparsity constraint . to address this issue",
    ", we need to modify the debiasing step ( s3 ) of grahtp to minimize @xmath176 over the constraint of @xmath198 as well as the support set @xmath39 : @xmath199 since this problem is convex , any off - the - shelf convex solver can be applied for optimization . in our implementation , we resort to alternating direction method ( adm ) for solving this subproblem because of its reported efficiency  @xcite .",
    "the implementation details of adm for solving   are deferred to appendix  [ append : adm ] .",
    "the modified grahtp for the precision matrix estimation problem is formally described in algorithm  [ alg : grahtp_modified ] .    generally speaking , the guarantees in theorem  [ thrm : convergence ] and theorem  [ thrm : recovery_grahtp ] are not valid for the modified grahtp . however ,",
    "if @xmath200 and @xmath201 are diagonally dominant , then with a slight modification of proof , we can prove that the part ( a ) of theorem  [ thrm : recovery_grahtp ] is still valid for the modified grahtp .",
    "we sketchily describe the proof idea as follows : let @xmath202_{f^{(t)}}$ ] . since @xmath200 and @xmath203 are diagonally dominant , we have @xmath204 and @xmath205 are also diagonally dominant and thus @xmath206 . since @xmath207 is the minimum of @xmath176 restricted over the union of the cone @xmath208 and the supporting set @xmath39 , we have @xmath209 .",
    "the remaining of the arguments follows that of the part(a ) of theorem  [ thrm : recovery_grahtp ] .",
    "[ alg : grahtp_modified ]",
    "this section is devoted to show the empirical performances of grahtp and fgrahtp when applied to sparse logistic regression and sparse precision matrix estimation problems . here",
    "we do not report the results of our algorithms in compressive sensing tasks because in these tasks grahtp and fgrahtp reduce to the well studied htp  @xcite and iht  @xcite , respectively .",
    "our algorithms are implemented in matlab 7.12 running on a desktop with intel core i7 3.2 g cpu and 16 g ram .",
    "we consider a synthetic data model identical to the one used in  @xcite .",
    "the sparse parameter @xmath210 is a @xmath211 dimensional vector that has @xmath212 nonzero entries drawn independently from the standard gaussian distribution .",
    "each data sample is an independent instance of the random vector @xmath135 generated by an autoregressive process @xmath213_{i+1 } = \\rho [ u]_i + \\sqrt{1-\\rho^2 } [ a]_i$ ] with @xmath143_1 \\sim \\mathcal { n}(0,1)$ ] , @xmath214_i \\sim \\mathcal { n}(0,1)$ ] , and @xmath215 being the correlation .",
    "the data labels , @xmath216 , are then generated randomly according to the bernoulli distribution @xmath217 we fix the regularization parameter @xmath218 in the objective of  .",
    "we are interested in the following two cases :    1 .",
    "* case 1 * : cardinality @xmath3 is fixed and sample size @xmath109 is varying : we test with @xmath219 and @xmath220 .",
    "* case 2 * : sample size @xmath109 is fixed and cardinality @xmath3 is varying : we test with @xmath221 and @xmath222 .    for each case , we compare grahtp and fgrahtp with two state - of - the - art greedy selection methods : grasp  @xcite and fbs ( forward basis selection )  @xcite . as aforementioned ,",
    "grasp is also a hard - thresholding - type method .",
    "this method simultaneously selects at each iteration @xmath3 nonzero entries and update their values via exploring the top @xmath3 entries in the previous iterate as well as the top @xmath101 entries in the previous gradient .",
    "fbs is a forward - selection - type method .",
    "this method iteratively selects an atom from the dictionary and minimizes the objective function over the linear combinations of all the selected atoms .",
    "note that all the considered algorithms have geometric rate of convergence .",
    "we will compare the computational efficiency of these methods in our empirical study .",
    "we initialize @xmath223 . throughout our experiment , we set the stopping criterion as @xmath224 . + * results .",
    "* figure  [ fig : solution_lr](a ) presents the estimation errors of the considered algorithms . from the left panel of figure  [ fig : solution_lr](a ) ( i.e. , case 1 ) we observe that : ( i ) when cardinality @xmath3 is fixed , the estimation errors of all the considered algorithms tend to decrease as sample size @xmath109 increases ; and ( ii ) in this case grahtp and fgrahtp are comparable and both are superior to grasp and fbs . from the right panel of figure",
    "[ fig : solution_lr](a ) ( i.e. , case 2 ) we observe that : ( i ) when @xmath109 is fixed , the estimation errors of all the considered algorithms but fbs tend to increase as @xmath3 increases ( fbs is relatively insensitive to @xmath3 because it is a forward selection method ) ; and ( ii ) in this case grahtp and fgrahtp are comparable and both are superior to grasp and fbs at relatively small @xmath225 . figure  [ fig : solution_lr](b ) shows the cpu times of the considered algorithms . from this group of results we observe that in most cases , fbs is the fastest one while grahtp and fgrahtp are superior or comparable to grasp in computational time . we also observe that when @xmath3 is relatively small , grahtp is even faster than fgrahtp although fgrahtp is cheaper in per - iteration overhead .",
    "this is partially because when @xmath3 is small , grahtp tends to need fewer iterations than fgrahtp to converge .",
    "the algorithms are also compared on the dataset ( @xmath148 = 47,236 ) which is a popular dataset for binary classification on sparse data . a training subset of size @xmath109 = 20,242 and a testing subset of size 20,000",
    "we test with sparsity parameters @xmath226 and fix the regularization parameter @xmath227 .",
    "the initial vector is @xmath223 for all the considered algorithms .",
    "we set the stopping criterion as @xmath224 or the iteration stamp @xmath228 .",
    "figure  [ fig : rcv1 ] shows the evolving curves of empirical logistic loss for @xmath229 .",
    "it can be observed from this figure that grahtp and grasp are comparable in terms of convergence rate and they are superior to fgrahtp and fbs .",
    "the testing classification errors and cpu running time of the considered algorithms are provided in figure  [ fig : rcv1_cpu_error ] : ( i ) in terms of accuracy , all the considered methods are comparable ; and ( ii ) in terms of running time , fgrahtp is the most efficient one ; grahtp is significantly faster than grasp and fbs .",
    "the reason that fgrahtp runs fastest is because it has very low per - iteration cost although its convergence curve is slightly less sharper than grahtp and grasp ( see figure  [ fig : rcv1 ] ) . to summarize , grahtp and fgrahtp achieve better trade - offs between accuracy and efficiency than grahtp and fbs on dataset .",
    "-regularized logistic loss _ vs. _ number of iterations.,title=\"fig:\",width=288 ] -regularized logistic loss _ vs.",
    "_ number of iterations.,title=\"fig:\",width=288 ] -regularized logistic loss _ vs. _ number of iterations.,title=\"fig:\",width=288 ] -regularized logistic loss _ vs. _ number of iterations.,title=\"fig:\",width=288 ]            our simulation study employs the sparse precision matrix model @xmath230 where each off - diagonal entry in @xmath231 is generated independently and equals 1 with probability @xmath232 or 0 with probability @xmath233 .",
    "@xmath231 has zeros on the diagonal , and @xmath234 is chosen so that the condition number of @xmath163 is @xmath148 .",
    "we generate a training sample of size @xmath235 from @xmath236 , and an independent sample of size 100 from the same distribution for tuning the parameter @xmath3 . we compare performance for different values of @xmath237 , replicated 100 times each .",
    "we compare the modified grahtp ( see algorithm  [ alg : grahtp_modified ] ) with grasp and fbs . to adopt grasp to sparse precision matrix estimation , we modify the algorithm with a similar two - stage strategy as used in the modified grahtp such that it can handle the eigenvalue bounding constraint in addition to the sparsity constraint . in the work of  @xcite",
    ", fbs has already been applied to sparse precision matrix estimation . also , we compare grahtp with glasso ( graphical lasso ) which is a representative convex method for @xmath2-penalized log - determinant program  @xcite .",
    "the quality of precision matrix estimation is measured by its distance to the truth in frobenius norm and the support recovery f - score .",
    "the larger the f - score , the better the support recovery performance . the numerical values over @xmath238 in magnitude are considered to be nonzero .    [ fig : model3_frob ] [ fig : model3_fscore ] [ fig : model3_cpu ]    figure  [ fig : model3 ] compares the matrix error in frobenius norm , sparse recovery f - score and cpu running time achieved by each of the considered algorithms for different @xmath148 .",
    "the results show that grahtp performs favorably in terms of estimation error and support recovery accuracy .",
    "we note that the standard errors of grahtp is relatively larger than glasso .",
    "this is as expected since grahtp approximately solves a non - convex problem via greedy selection at each iteration ; the procedure is less stable than convex methods such as glasso .",
    "similar phenomenon of instability is observed for grasp and fbs .",
    "figure  [ fig : model3_cpu ] shows the computational time of the considered algorithms .",
    "we observed that glasso , as a convex solver , is computationally superior to the three considered greedy selection methods .",
    "although inferior to glasso , grahtp is computationally much more efficient than grasp and fbs .    to visually inspect the support recovery performance of the considered algorithms , we show in figure  [ fig : synthetic_heatmap ] the heatmaps corresponding to the percentage of each matrix entry being identified as a nonzero element with @xmath239 .",
    "visual inspection on these heatmaps shows that the three greedy selection methods , grahtp , grasp , fbs , tend to be sparser than glasso .",
    "similar phenomenon is observed in our experiments with other values of @xmath148 .",
    "we consider the task of lda ( linear discriminant analysis ) classification of tumors using the dataset  @xcite .",
    "this dataset consists of 133 subjects , each of which is associated with 22,283 gene expression levels . among these subjects ,",
    "34 are with pathological complete response ( pcr ) and 99 are with residual disease ( rd ) .",
    "the pcr subjects are considered to have a high chance of cancer free survival in the long term .",
    "based on the estimated precision matrix of the gene expression levels , we apply lda to predict whether a subject can achieve the pcr state or the rd state .    in our experiment",
    ", we follow the same protocol used by  @xcite as well as references therein . for the sake of readers",
    ", we briefly review this experimental setup .",
    "the data are randomly divided into the training and testing sets . in each random division , 5 pcr subjects and 16 rd subjects",
    "are randomly selected to constitute the testing data , and the remaining subjects form the training set with size @xmath240 . by using two - sample @xmath32 test ,",
    "@xmath241 most significant genes are selected as covariates . following the lda framework , we assume that the normalized gene expression data are normally distributed as @xmath242 , where the two classes are assumed to have the same covariance matrix , @xmath243 , but different means , @xmath244 , @xmath245 for pcr state and @xmath246 for rd state . given a testing data sample @xmath9 , we calculate its lda scores , @xmath247 , @xmath248 , using the precision matrix @xmath249 estimated by the considered methods . here",
    "@xmath250 is the within - class mean in the training set and @xmath251 is the proportion of class @xmath252 subjects in the training set .",
    "the classification rule is set as @xmath253 .",
    "clearly , the classification performance is directly affected by the estimation quality of @xmath254 .",
    "hence , we assess the precision matrix estimation performance on the testing data and compare ( modified ) grahtp with grasp and fbs .",
    "we also compare grahtp with glasso ( graphical lasso )  @xcite .",
    "we use a 6-fold cross - validation on the training data for tuning @xmath3 .",
    "we repeat the experiment 100 times .",
    "+ * results . *",
    "to compare the classification performance , we use specificity , sensitivity ( or recall ) , and mathews correlation coefficient ( mcc ) criteria as in  @xcite : @xmath255 where tp and tn stand for true positives ( pcr ) and true negatives ( rd ) , respectively , and fp and fn stand for false positives/ negatives , respectively . the larger the criterion value , the better the classification performance .",
    "since one can adjust decision threshold in any specific algorithm to trade - off specificity and sensitivity ( increase one while reduce the other ) , the mcc is more meaningful as a single performance metric .    .",
    "data : comparison of average ( std ) classification accuracy and average cpu running time over 100 replications . [",
    "tab : breast_cancer_results ] [ cols=\"^,^,^,^,^\",options=\"header \" , ]     table  [ tab : breast_cancer_results ] reports the averages and standard deviations , in the parentheses , of the three classification criteria over 100 replications .",
    "it can be observed that grahtp is quite competitive to the leading methods in terms of the three metrics .",
    "the averages of cpu running time ( in seconds ) of the considered methods are listed in the last column of table  [ tab : breast_cancer_results ] .",
    "figure  [ fig : breast_cancer ] shows the evolving curves of log - determinant loss verses number of iterations .",
    "we observed that on this data , grahtp converges much faster than grasp and fbs .",
    "note that we did not draw the curve of glasso in figure  [ fig : breast_cancer ] because its objective function is different from that of the problem  .",
    ", width=288 ]",
    "in this paper , we propose grahtp as a generalization of htp from compressive sensing to the generic problem of sparsity - constrained optimization .",
    "the main idea is to force the gradient descent iteration to be sparse via hard threshloding .",
    "theoretically , we prove that under mild conditions , grahtp converges geometrically in finite steps of iteration and its estimation error is controlled by the restricted norm of gradient at the target sparse solution .",
    "we also propose and analyze the fgrahtp algorithm as a fast variant of grahtp without the debiasing step .",
    "empirically , we compare grahtp and fgrahtp with several representative greedy selection methods when applied to sparse logistic regression and sparse precision matrix estimation tasks .",
    "our theoretical results and empirical evidences show that simply combing gradient descent with hard threshloding , with or without debiasing , leads to efficient and accurate computational procedures for sparsity - constrained optimization problems .",
    "xiao - tong yuan was a postdoctoral research associate supported by nsf - dms 0808864 and nsf - eager 1249316 .",
    "ping li is supported by onr - n00014 - 13 - 1 - 0764 , afosr - fa9550 - 13 - 1 - 0137 , and nsf - bigdata 1250914 .",
    "tong zhang is supported by nsf - iis 1016061 , nsf - dms 1007527 , and nsf - iis 1250985 .",
    "* part ( a ) * : the first inequality follows from the triangle inequality .",
    "the second inequality can be derived by combining the first one and the integration @xmath256 .    *",
    "part ( b ) * : by adding two copies of the inequality   with @xmath9 and @xmath257 interchanged and using the theorem 2.1.5 in  @xcite , we know that @xmath258 for any @xmath259 we have @xmath260 if @xmath261 , then @xmath262 .",
    "this proves the desired result .",
    "we first prove the finite iteration guarantee of grahtp .",
    "let us consider the vector @xmath263 which is the restriction of @xmath36 on @xmath39 . according to the definition of @xmath37 we have @xmath264 .",
    "it follows that @xmath265 where the second inequality follows from lemma  [ lemma : strong_smooth ] and the third inequality follows from the fact that @xmath263 is a better @xmath3-sparse approximation to @xmath36 than @xmath34 so that @xmath266 , which implies @xmath267",
    ". since @xmath268 , it follows that the sequence @xmath72 is nonincreasing , hence it is convergent . since it is also eventually periodic , it must be eventually constant . in view of",
    ", we deduce that @xmath269 , and in particular that @xmath270 , for @xmath32 large enough .",
    "this implies that @xmath271 for @xmath32 large enough , which implies the desired result .    by noting that @xmath272 and from the inequality  , we immediately establish the convergence of the sequence @xmath72 defined by fgrahtp .      *",
    "part ( a ) * : the first step of the proof is a consequence of the debiasing step s3 . since @xmath37 is the minimum of @xmath42 restricted over the supporting set @xmath39 , we have @xmath273 whenever @xmath274 .",
    "let @xmath275 and @xmath276 .",
    "it follows that @xmath277 where the last inequality is from condition @xmath53 , @xmath278 and @xmath279 .",
    "after simplification , we have @xmath280 .",
    "it follows that @xmath281 after rearrangement we obtain @xmath282    the second step of the proof is a consequence of steps s1 and s2 .",
    "we notice that @xmath283 by eliminating the contribution on @xmath284 , we derive @xmath285 for the right - hand side , we have @xmath286 as for the left - hand side , we have @xmath287 with @xmath288 denoting the symmetric difference of the set @xmath289 and @xmath39 , it follows that @xmath290 where the last inequality follows from condition @xmath53 , @xmath291 and lemma  [ lemma : strong_smooth ] . as a final step",
    ", we put   and   together to obtain @xmath292 since @xmath293 , by recursively applying the above inequality we obtain the desired inequality in part ( a ) . + * part ( b ) : * recall that @xmath294 and @xmath295 .",
    "consider the following vector @xmath296 by using triangular inequality we have @xmath297 where the last inequality follows from condition @xmath53 , @xmath93 and @xmath298 .",
    "for fgrahtp , we note that @xmath299 , and thus @xmath300 .",
    "it follows that @xmath301 since @xmath302 , by recursively applying the above inequality we obtain the desired inequality in part ( b ) .      obviously , @xmath117 is @xmath118-strongly convex .",
    "consider an index set @xmath5 with cardinality @xmath54 and all @xmath303 with @xmath304 .",
    "since @xmath305 is lipschitz continuous with constant @xmath306 , we have @xmath307 which implies @xmath308 therefore we have @xmath309 where the second `` @xmath310 '' follows @xmath311 , the third `` @xmath310 '' follows from @xmath312 , and",
    "last `` @xmath310 '' follows from @xmath313_f\\| \\le n\\sqrt{s } r_s$ ] .",
    "therefore @xmath52 is @xmath314-strongly smooth .",
    "the desired result follows directly from part(b ) of lemma  [ lemma : strong_smooth ] .      for any index set @xmath5 with @xmath54",
    ", we can deduce @xmath315_f \\| + \\lambda\\|\\bar w_f\\| \\le \\sqrt{s}\\|\\nabla l(\\bar",
    "w)\\|_\\infty + \\lambda \\|\\bar w_s\\|.\\end{aligned}\\ ] ] we next bound the term @xmath316 . from   we have @xmath317_j}\\right| & = & \\left|\\frac{1}{n}\\sum_{i=1}^n -v^{(i ) } [ u^{(i)}]_j + \\mathbb{e}_{v}[v[u^{(i)}]_j",
    "\\mid u^{(i)}]\\right| \\nonumber \\\\ & \\le & \\left|\\frac{1}{n}\\sum_{i=1}^n   v^{(i ) } [ u^{(i)}]_j - \\mathbb{e}[v[u]_j ] \\right| + \\left| \\frac{1}{n}\\sum_{i=1}^n \\mathbb{e}_{v}[v[u^{(i)}]_j \\mid u^{(i ) } ] - \\mathbb{e}[v[u]_j ] \\right| \\nonumber,\\end{aligned}\\ ] ] where @xmath318 $ ] is taken over the distribution  . therefore , for any @xmath319 , @xmath320_j}\\right| > \\varepsilon",
    "\\right ) & \\le & \\mathbb{p}\\left(\\left|\\frac{1}{n}\\sum_{i=1}^n   v^{(i ) } [ u^{(i)}]_j - \\mathbb{e}[v[u]_j ] \\right| >",
    "\\frac{\\varepsilon}{2}\\right ) \\nonumber \\\\ & & + \\mathbb{p}\\left(\\left| \\frac{1}{n}\\sum_{i=1}^n \\mathbb{e}_{v}[v[u^{(i)}]_j \\mid u^{(i ) } ] - \\mathbb{e}[v[u]_j ] \\right| > \\frac{\\varepsilon}{2 } \\right ) \\nonumber \\\\ & \\le &   4 \\exp\\left\\{-\\frac{n\\varepsilon^2}{8\\sigma^2}\\right\\ } \\nonumber,\\end{aligned}\\ ] ] where the last `` @xmath310 '' follows from the large deviation inequality of sub - gaussian random variables which is standard  ( see , e.g. , * ? ? ?",
    "* ) . by the union bound we",
    "have @xmath321 by letting @xmath322 , we know that with probability at least @xmath144 , @xmath323 combing the above inequality with   yields the desired result .",
    "in this appendix section , we provide our implementation details of adm for solving the subproblem  . by introducing an auxiliary variable @xmath324 ,",
    "the problem   is obviously equivalent to the following problem : @xmath325 then , the augmented lagrangian function of   is @xmath326 where @xmath327 is the multiplier of the linear constraint @xmath328 and @xmath329 is the penalty parameter for the violation of the linear constraint .",
    "the adm solves the following problems to generate the new iterate : @xmath330 let us first consider the minimization problem  .",
    "it is easy to verify that it is equivalent to the following minimization problem : @xmath331 where @xmath332 let the singular value decomposition of @xmath333 be @xmath334 it is easy to verify that the solution of problem   is given by @xmath335 where @xmath336 next , we consider the minimization problem  .",
    "it is straightforward to see that the solution of problem   is given by @xmath337_f.\\ ] ]      agarwal , a. , negahban , s. , and wainwright , m. fast global convergence rates of gradient methods for high - dimensional statistical recovery . in",
    "_ proceedings of the 24th annual conference on neural information processing systems ( nips10 ) _ , 2010 .",
    "boyd , s. , parikh , n. , chu , e. , peleato , b. , and eckstein , j. distributed optimization and statistical learning via the alternating direction method of multipliers . _",
    "foundations and trends in machine learning _",
    ", 3:0 1122 , 2010 .",
    "foucart , s. sparse recovery algorithms : sufficient conditions in terms of restricted isometry constants . in _",
    "approximation theory xiii : san antonio 2010 , volume 13 of springer proceedings in mathematics _ , pp .   6577 , 2012 .",
    "hess , k.  r. , anderson , k. , symmans , w.  f. , and _",
    "_ pharmacogenomic predictor of snesitivity to preoperative chemotherapy with paclitaxel and fluorouracil , doxorubicin , and cyclophosphamide in breast cancer .",
    "_ journal of clinical oncology _ , 24:0 42364244 , 2006 .",
    "jalali , a. , johnson , c.  c. , and ravikumar , p.  k. on learning discrete graphical models using greedy methods . in _ proceedings of the 25th annual conference on neural information processing systems ( nips11 )",
    "_ , 2011 .",
    "ravikumar , p. , wainwright , m.  j. , raskutti , g. , and yu , b. high - dimensional covariance estimation by minimizing @xmath2-penalized log - determinant divergence .",
    "_ electronic journal of statistics _ , 5:0 935980 , 2011 .",
    "tewari , a. , ravikumar , p. , and dhillon , i.  s. greedy algorithms for structurally constrained high dimensional problems . in _ proceedings of the 25th annual conference on neural information processing systems ( nips11 )",
    "_ , 2011 .",
    "yuan , x .-",
    "yan , s. forward basis selection for sparse approximation over dictionary .",
    "in _ proceedings of the fifteenth international conference on artificial intelligence and statistics ( aistats12 ) _ , 2012 ."
  ],
  "abstract_text": [
    "<S> hard thresholding pursuit ( htp ) is an iterative greedy selection procedure for finding sparse solutions of underdetermined linear systems . </S>",
    "<S> this method has been shown to have strong theoretical guarantee and impressive numerical performance . in this paper </S>",
    "<S> , we generalize htp from compressive sensing to a generic problem setup of sparsity - constrained convex optimization . </S>",
    "<S> the proposed algorithm iterates between a standard gradient descent step and a hard thresholding step with or without debiasing . </S>",
    "<S> we prove that our method enjoys the strong guarantees analogous to htp in terms of rate of convergence and parameter estimation accuracy . </S>",
    "<S> numerical evidences show that our method is superior to the state - of - the - art greedy selection methods in sparse logistic regression and sparse precision matrix estimation tasks .    </S>",
    "<S> [ [ key - words . ] ] key words .    </S>",
    "<S> sparsity , greedy selection , hard thresholding pursuit , gradient descent . </S>"
  ]
}