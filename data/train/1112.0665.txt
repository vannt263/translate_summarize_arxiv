{
  "article_text": [
    "sparsity - aware learning has been a topic at the forefront of research over the last ten years or so @xcite .",
    "considerable effort has been invested in developing efficient schemes for the recovery of sparse signal / parameter vectors .",
    "however , most of these efforts have focussed on batch processing , via the _ compressed sensing _ or _ sampling ( cs ) _ framework . in cs , an iterative algorithm is mobilized to solve the estimation task once _ all _ measurements ( training data ) have been collected by the processing unit @xcite .",
    "it is only very recently that online ( time - adaptive ) algorithms have been developed , where the training data are processed sequentially , and the sparse signal to be recovered has the freedom to be time - varying @xcite .",
    "both cs and online techniques share a common strategy , namely _ thresholding _ ; i.e , a thresholding rule is used to impose sparsity - aware a - priori knowledge : some of the components of the signal / vector to be estimated are kept intact , while the rest of them are shrunk under some user - defined rule .",
    "two thresholding operators dominate the literature : ( i ) _ hard _ thresholding , a brute force method , where shrinking is achieved by setting the size of some of the vector components to zero , and ( ii ) _ soft _ thresholding , where the shrinking operation is based on the ( weighted ) @xmath0-norm of the vector .",
    "a large number of thresholding operators have been studied thoroughly , both in theoretical and experimental contexts , mainly within the statistics community @xcite .",
    "it is by now well - established that hard thresholding , a discontinuous operator , has a tendency for larger variance of the estimates .",
    "moreover , due to its discontinuity , hard thresholding can lead to instabilities , in the sense of being sensitive to small changes in the training data @xcite .",
    "soft - thresholding , is a continuous operator , that tends to introduce bias in the estimates .",
    "therefore , alternative thresholding rules have been proposed in an effort to overcome these drawbacks @xcite .",
    "these advances in thresholding operators are strongly connected to optimization tasks ; they are obtained by minimizing squared error terms regularized by , usually , _",
    "non - convex _ penalty functions .",
    "the contribution of this paper is threefold .",
    "first , the _ generalized thresholding _ ( gt ) operator is introduced , which encompasses classical hard and soft thresholding rules , as well as the recent advances of @xcite .",
    "moreover , the proposed framework , motivated by the rich _ fixed point _ theory @xcite , is general enough to provide means for designing novel thresholding rules and/or incorporating a priori information associated with the sparsity level , i.e. , the number of nonzero components , of the sparse vector to be recovered .",
    "more importantly , gt is also allowed to _ non - convexly _ constrain the unknown vector .",
    "second , the gt operator is incorporated into a signal / parameter estimation framework . here , we choose the set theoretic estimation approach @xcite , and in particular its online version , introduced in @xcite and extended in @xcite .",
    "in particular , the _ adaptive projection - based generalized thresholding ( apgt ) _ algorithm is proposed having three important merits .",
    "a ) it is an online algorithm , b ) it promotes sparse solutions effectively via the flexibility provided by the gt operator and c ) its computational complexity _ scales linearly to the number of unknowns_. with respect to performance , although apgt shows a low computational load , the experimental validation of section  [ sec : simulations ] demonstrates that it exhibits a competitive performance even when compared to very recently developed , sparsity - promoting , and computationally more demanding alternatives , such as the apa- and rls - based techniques @xcite .    it should be noted that the adopted set theoretic estimation framework was also utilized in @xcite , where sparsity was induced via @xmath0-based constraints , well - known to be convex and intimately connected to soft thresholding operations . in contrast , the fact that the gt operator is a `` non - convex '' mapping poses certain challenges for the convergence analysis of the algorithm .",
    "specifically , the existing theory @xcite which , so far , has been developed around convex sets and constraints is not rich enough to cover the apgt case . in order to theoretically support the incorporation of gt into learning mechanisms , such as the apgt",
    ", a novel family of operators , hereafter referred to as _ partially quasi - nonexpansive mappings , _ is introduced , to the best of our knowledge , for the first time .",
    "it is the introduction of the partially quasi - nonexpansive mappings and their nice properties , which allowed the convergence analysis of apgt to be developed .",
    "these operators serve as a sound theoretical tool which allows the use of variational analysis @xcite and fixed point theory @xcite to attack _ non - convexly _ constrained learning problems .",
    "it is shown that gt belongs to this class of nonlinear mappings , with its _ fixed point set _ being a union of subspaces ; a non - convex object which lies at the heart of any sparsity - promoting technique @xcite .",
    "it should be stressed that , propelled by such a generic operator theoretical framework , the proposed gt mapping offers a sound mathematical basis for infusing sparsity arguments into both batch ( cs ) and online approaches , beyond the set - theoretic framework adopted here .",
    "moreover , the present manuscript shows a value beyond sparsity - aware learning . through the novel concept of the partially quasi - nonexpansive mappings",
    ", this study stands also as the first step toward the extension of @xcite to non - convexly constrained online learning tasks .",
    "the remainder of the paper is organized as follows .",
    "the problem under consideration is stated in section [ sec : problem ] . in section [ sec : gt ] , the gt operator is introduced .",
    "the proposed apgt algorithm is given in section [ sec : algo ] , together with its properties and the definition of the novel family of partially quasi - nonexpansive mappings .",
    "section [ sec : simulations ] contains the experimental validation of apgt .",
    "a number of appendices support theoretically the developments exposed throughout the paper .",
    "more specifically , in app .",
    "[ sec : properties.t_k ] the properties of the generalized thresholding operator are studied rigorously , and the convergence analysis of the proposed algorithm is performed in app  [ sec : analysis.algo ] .",
    "a preliminary version of this study was presented in @xcite .",
    "we will denote the set of all non - negative integers , positive integers , and real numbers by @xmath1 , @xmath2 , and @xmath3 , respectively . given any integers @xmath4 , such that @xmath5 , let @xmath6",
    ".    the stage for discussion will be the euclidean space @xmath7 , where @xmath8 . given any pair of vectors @xmath9 , the inner product in @xmath7 is defined as the classical vector - dot product @xmath10 , where @xmath11 stands for vector / matrix transposition .",
    "the induced norm is @xmath12 .",
    "our discussion will revolve around the following celebrated linear model : @xmath13 where @xmath14 is an unknown vector / signal , @xmath15 is a sequence of known training data , and @xmath16 stands for the noise process . in other words ,",
    "the unknown @xmath17 is `` sensed '' by a sequence of input vectors @xmath18 , via the inner product of @xmath7 , in order to produce the noisy outputs @xmath19 .",
    "the vector @xmath17 is considered to be _ sparse _ , i.e. , most of its components are zero .",
    "if we define @xmath20 to stand for the number of non - zero components of @xmath17 , then the assumption that @xmath17 is sparse can be equivalently given by @xmath21 , and the vector @xmath17 will be called _ @xmath22-sparse .",
    "_    this study attacks the following inverse problem : estimate the unknown sparse vector @xmath17 by utilizing the sequence of training data @xmath23 .",
    "a family of algorithms which shares a similar objective is the _ compressed sensing _ or _ sampling ( cs ) _",
    "framework @xcite .",
    "given a fixed number @xmath24 of training data @xmath25 , a cs algorithm is mobilized in order to compute an estimate @xmath26 of @xmath17 .",
    "cs belongs to the class of batch algorithms , i.e. , in the case where the datum @xmath27 enters the system , a cs algorithm starts _ from scratch _ , and triggers a generally time consuming iterative procedure which operates on the data @xmath28 for computing the updated estimate @xmath29 of @xmath17 .",
    "in contrast to _ batch learning _ approaches , this manuscript focuses on sparsity - aware _ online learning _ ,",
    "i.e. , an algorithmic framework which satisfies the following requirements",
    ".    1 .   the estimates of @xmath17 should be updated in a simple and efficient way every time that a new datum @xmath30 enters the system .",
    "the need to mobilize an optimization procedure from scratch , for every new datum @xmath30 , as in cs , should be avoided .",
    "2 .   the operations needed in order to update the estimate should be of low computational complexity ; hopefully of _ linear complexity with respect to the number of unknowns ,",
    "_ i.e. , @xmath31 .",
    "the unknown @xmath17 has also the freedom to be _ time - varying_. thus , an online learning scheme should be also able to _ quickly track _ any variations of @xmath17 .",
    "the mainstream of sparsity - promoting online methods exploits training data @xmath32 in the context of classical adaptive filtering @xcite ; a quadratic objective function is used to quantify the designer s perception of loss .",
    "additionally , a convex differentiable function is regularized by a sparsity promoting term , usually one that builds around the @xmath0 norm penalty function , and a minimizer of the resulting optimization task is sought either in the rls or the lms rationale , e.g. , @xcite .",
    "another sparsity - promoting methodology , where different components of the vector estimates are weighted under several user - defined rules , is given by _ proportionate - type _",
    "schemes @xcite .",
    "very recently , a novel online method for the recovery of sparse signals , based on set theoretic estimation arguments @xcite , was developed in @xcite , and extended for distributed learning in @xcite .",
    "the set theoretic estimation philosophy departs from the standard approach of constructing a loss function first ; instead , it initially identifies a _ set _ of solutions which are in _ agreement _ with the available measurements as well as the available a - priori knowledge .",
    "a popular strategy is to define , at each time instance @xmath33 , a closed convex subset of @xmath7 , by means of the training data pair @xmath30 , to contain the unknown @xmath17 with high probability .",
    "different alternatives exist on how to `` construct '' such convex regions .",
    "a popular choice takes the form of a _ hyperslab _ around @xmath30 , which is defined as : @xmath34 \\coloneqq \\bigl \\{\\bm{a}\\in{\\mathbb{r}}^l:\\ \\bigl|\\bm{u}_n^{\\top } \\bm{a } - y_n \\bigr| \\leq \\epsilon_n \\bigr\\},\\quad \\forall",
    "n \\in{\\mathbb{n}},\\label{eq : hyperslab}\\ ] ] for some user - defined tolerance @xmath35 , and for @xmath36 .",
    "the parameter @xmath37 determines , essentially , the width of the hyperslabs , and it implicitly models the effects of the noise , as well as various other uncertainties , like measurement inaccuracies , calibration errors , etc .",
    "for example , if the noise were bounded , i.e. , @xmath38 such that @xmath39 , @xmath40 , then for any choice of @xmath41 it is easy to verify that @xmath42 $ ] , @xmath40 . a rigorous stochastic analysis in the case of bounded noise , where almost sure convergence of the sequence of estimates is proved for a special member of the rich family of the _ adaptive projected subgradient method ( apsm ) _",
    "@xcite , can be found in @xcite . in the case of unbounded noise",
    ", the well - known tchebichev inequality @xcite suggests that for any @xmath43 , @xmath44\\bigr\\ } = \\operatorname{prob}\\bigl\\ { \\bigl|\\bm{u}_n^\\top \\bm{a } _ * - y_n \\bigr| \\leq \\epsilon_n \\bigr\\ } \\geq 1 - \\frac{\\operatorname{\\mathsf{e}}\\bigl\\{\\bigl|\\bm{u}_n^\\top \\bm{a } _ * - y_n    \\bigr|^{2}\\bigr\\}}{\\epsilon_n^2 } = 1 - \\frac{\\operatorname{\\mathsf{e}}\\bigl\\{|v_n|^{2}\\bigr\\}}{\\epsilon_n^2},\\ ] ] where @xmath45 denotes probability , and @xmath46 stands for the expectation operator . in other words , @xmath37 defines also a measure of _ confidence _ in having the unknown @xmath17 in the hyperslabs .",
    "the ( metric ) projection mapping @xmath47}$ ] @xcite onto the hyperslab @xmath48 $ ] is given by the following simple analytic formula : @xmath49}(\\bm{a } ) = \\bm{a } + \\begin{cases } \\frac{y_n - \\epsilon_n - \\bm{u}_n^{\\top } \\bm{a } }    { { \\left\\|{\\bm{u}_n}\\right\\|}^2}\\bm{u}_n , & \\text{if}\\ y_n - \\epsilon_n > \\bm{u}_n^{\\top } \\bm{a } , \\\\ 0 , & \\text{if}\\ |\\bm{u}_n^{\\top } \\bm{a } - y_n |\\leq    \\epsilon_n,\\\\ \\frac{y_n + \\epsilon_n - \\bm{u}_n^{\\top } \\bm{a } }            { { \\left\\|{\\bm{u}_n}\\right\\|}^2}\\bm{u}_n , & \\text{if}\\ y_n + \\epsilon_n <            \\bm{u}_n^{\\top } \\bm{a}. \\end{cases } \\label{project.hyperslab}\\ ] ]    in @xcite sparsity was induced within the convex analytic framework , and particularly via projections onto convex @xmath0-balls . here ,",
    "the fixed point theoretical framework @xcite is used in order to generalize the set theoretic estimation approach to support sparsity promoting constraints , which do not lie under the umbrella of convexity .",
    "this is realized via a novel operator theoretic framework , which embraces a wide range of thresholding rules referred to as generalized thresholding ( gt ) operators , described next .",
    "a couple of definitions are necessary prior to introducing gt mapping .",
    "[ def : ord.tuple ] given @xmath50 , define the set of all _ ascending tuples of length @xmath51 _ as @xmath52 .",
    "clearly , the cardinality of @xmath53 is @xmath54 .",
    "an example of such an ordered tuple is the support of a vector @xmath55 , defined by @xmath56 , where @xmath57 stands for the cardinality of a set .",
    "[ def : special.subspace ] given @xmath58 , let @xmath59 .",
    "clearly , @xmath60 is a linear subspace of @xmath7 .",
    "moreover , notice that if @xmath61 , then @xmath62 .",
    "in particular , if @xmath63 , then @xmath64 .",
    "an illustration of @xmath60 can be found in fig .",
    "[ fig : gt ] .    motivated by the hard thresholding operator ,",
    "let us introduce here the main object of this study .",
    "[ def : gt ] fix a positive integer @xmath65 and define @xmath66 as follows . for any @xmath55 , the output @xmath67 is obtained according to the following steps :    1 .",
    "[ compute.tuple ] compute , first , the tuple @xmath68 which contains the indices of the @xmath51 largest , in absolute value , components of @xmath69 . to avoid any ambiguity , in the case where we identify more than one component of @xmath69 with the same absolute value , we always choose the one with smallest index .",
    "2 .   define @xmath70 . in words",
    ", @xmath71 is the smallest among the @xmath51 largest absolute values of the components of @xmath69 .",
    "clearly , @xmath72 , @xmath73 .",
    "[ thres.components ] compute the components of @xmath74 as : @xmath75 , if @xmath76 , and @xmath77 , if @xmath78 , where the function @xmath79 , with @xmath80 $ ] , satisfies the following properties : 4 .",
    "[ same.sign ] @xmath81 , @xmath82 .",
    "[ shrinks ] @xmath83 , @xmath84 .",
    "[ strictly.shrinks ] going a step further than the previous property , we assume also that given any sufficiently small @xmath85 , there exists a @xmath86 , such that for any @xmath55 , and @xmath87 , @xmath88 .",
    "in other words , @xmath89 could be a user - defined parameter which guarantees that the function @xmath90 acts as a _ strict _ shrinkage operator for all the components of @xmath69 with indexes not in @xmath91 .",
    "the @xmath92 parameter is introduced in order to exclude @xmath93 from the picture , since at this point the @xmath90 function usually takes the value of @xmath93 , i.e. , @xmath94 ( see fig .  [",
    "fig : variousthresholds1 ] ) .    put in other words ,",
    "the gt mapping operates as follows ; given the input vector @xmath69 , a number of @xmath51 components of @xmath69 , i.e. , those with the @xmath51 largest absolute values , are kept intact , while the rest of them are shrunk according to the @xmath90 function .",
    "see , for example , fig .  [ fig : gt ] .    ,",
    "for the case of a @xmath95-dimensional space , i.e. , @xmath96 , and @xmath97 .",
    "take for example the point @xmath98 .",
    "the @xmath99 largest , in magnitude , coordinates of @xmath98 are the first two ones , i.e. , @xmath100 .",
    "the linear subspace @xmath101 stands for all those vectors in @xmath102 where all the components , except from those in the positions @xmath103 , are equal to @xmath93 .",
    "the first two components of @xmath98 stay unaffected by @xmath104 , while the third one is shrinked by the function @xmath90 . if this third coordinate is set to @xmath93 , then @xmath104 acts as the hard - thresholding mapping @xmath105 . on the other hand , the point @xmath106",
    "is already located in @xmath107 , i.e. , its first coordinate is @xmath93 .",
    "hence , the application of @xmath104 to @xmath106 has no effect , and @xmath106 stays _ fixed _ to its original position.,scaledwidth=30.0% ]    the function @xmath90 is user - defined and it can get various forms as long as it complies with the properties described before . as an example , a thresholding operator in the gt family based on an arbitrary @xmath90 function is shown in fig .",
    "[ fig : variousthresholds1]a .",
    "note that it comprises both discontinuities and nonlinear regions .",
    "a more systematic way to built gt s is via the univariate _",
    "penalized least squares _",
    "optimization task ; given @xmath108 , @xmath109 where @xmath110 is nonnegative , nondecreasing and differentiable function on @xmath111 .",
    "this problem is at the heart of many batch sparsity promoting algorithms as it is discussed in app .",
    "[ sec : plsto ] .",
    "it turns out , that has , in general , a unique solution which is obtained when @xmath112 is properly thresholded / shrinked @xcite .",
    "accordingly , let us define the _ penalized least - squares thresholding operator ( plsto ) _ as the mapping which maps a given @xmath112 to the previous unique minimizer : @xmath113 in simple words , the plsto of _ shrinks , _ in some sense that is dictated by @xmath114 , the size of @xmath112 .",
    "examples of plsto s exhibiting different characteristics are shown in fig .",
    "[ fig : variousthresholds1](b - d ) and details together with the corresponding literature review can be found in app .",
    "[ sec : plsto ] .",
    "all the thresholding rules of fig .",
    "[ fig : variousthresholds1](b - d ) satisfy the properties of def .",
    "[ def : gt].[same.sign ] and def .",
    "[ def : gt].[shrinks ] .",
    "moreover , they also satisfy the property of def .",
    "[ def : gt].[strictly.shrinks ] in their respective strict - shrinkage region , i.e. , in the case where the @xmath71 lies in the domain of all those @xmath115 such that @xmath116 .",
    "notice , also , that we do not impose any regularity conditions on @xmath90 , like continuity or differentiability , unlike most of the known plsto do @xcite . as a result , _ any _ plsto , i.e. , , can be used in the place of the @xmath90 function in the gt operator .",
    "examples of gt having plsto s as their @xmath90 function are shown in figs .",
    "[ fig : variousthresholds1]a and [ fig : variousthresholds1]e .",
    "moreover , gt where the @xmath90 function is the bridge @xmath117 and the _ smoothly clipped absolute deviation penalty ( scad ) _ threshold are used and further discussed in the numerical experiments section .",
    "[ apgt ] given the user - defined sparsity level @xmath118 , the sequence of non - negative parameters @xmath119 , the number @xmath120 of the hyperslabs to be processed concurrently at every time instant , the function @xmath90 for the generalized thresholding operation , and an arbitrary initial point , @xmath121 , execute the following , for every @xmath122 .",
    "define the sliding window @xmath123 on the time axis , of size at most @xmath124 .",
    "the set @xmath125 defines all the indices corresponding to the hyperslabs , which are to be processed at the time instant @xmath126 . among these , identify @xmath127}(\\bm{a}_n)\\neq \\bm{a}_n \\bigr\\}$ ] , which correspond to the _ active _ hyperslabs .",
    "moreover , for every @xmath128 , define the weight @xmath129 , where @xmath130 denotes the cardinality of @xmath131 , in order to weigh uniformly the importance of the information carried by each hyperslab , @xmath132 $ ] .",
    "other , more general , scenarios regarding the choice of @xmath133 are also possible .",
    "collect the projections @xmath134}(\\bm{a}_n)$ ] , @xmath135 ( see ) .",
    "choose an @xmath136 $ ] , and let the _ extrapolation parameter _ @xmath137 take values from the interval @xmath138 $ ] , where    [ algo ] @xmath139}(\\bm{a}_n ) - \\bm{a}_n}\\right\\|}^2 } {   { \\left\\|{\\sum_{i\\in\\mathcal{i}_n } \\omega_i^{(n ) }     p_{s_i[\\epsilon_i]}(\\bm{a}_n ) - \\bm{a}_n}\\right\\|}^2 } , & \\\\",
    "& \\hspace{-100pt}\\text{if}\\ \\sum_{i\\in\\mathcal{i}_n } \\omega_i^{(n ) }     p_{s_i[\\epsilon_i]}(\\bm{a}_n ) \\neq \\bm{a}_n,\\\\ 1 , & \\hspace{-100pt } \\text{otherwise}. \\label{mn } \\end{cases}\\ ] ] notice that due to the convexity of the function @xmath140 , we always have @xmath141 . as such",
    ", the parameter @xmath137 takes values larger than or equal to @xmath142 . in general",
    ", the larger the @xmath137 , the larger the convergence speed of the proposed algorithm .",
    "compute the next estimate by @xmath143}(\\bm{a}_n ) - \\bm{a}_n\\biggr)\\biggr ) , & \\\\",
    "& \\hspace{-50pt}\\text{if}\\ \\mathcal{i}_n\\neq\\emptyset,\\\\ t_{\\text{gt}}^{(k)}(\\bm{a}_n ) , & \\hspace{-50pt } \\text{if}\\ \\mathcal{i}_n = \\emptyset . \\end{cases}\\label{main.recursion}\\ ] ]    in order to theoretically support the incorporation of gt into parameter estimation schemes , a novel family of mappings , called the _ partially quasi - nonexpansive mappings _ , which , to the best of our knowledge , appears for the first time in the related literature @xcite .",
    "the reasons for defining this new class of mappings are : ( i ) this family includes as a special case the previously defined generalized thresholding operator @xmath104 , and , thus , it establishes a general theoretical framework for sparsity - promoting mappings , ( ii ) it introduces sound theoretical tools , which help to attack _ non - convexly _ constrained learning problems , and ( iii ) it generalizes the very recent results , obtained for the _ adaptive projected subgradient method ( apsm ) _",
    "@xcite , to _ non - convexly _ constrained online learning tasks ( see app .  [",
    "sec : analysis.algo ] ) .",
    "although the following discussion can be naturally extended to general hilbert spaces , for the sake of simplicity we focus here on the euclidean space @xmath7 , i.e. , @xmath144 .",
    "a concept of fundamental importance , associated to every mapping @xmath145 , is its _ fixed point set _",
    "in other words , @xmath147 reveals the hidden modes of @xmath145 , by putting together all those points unaffected by @xmath145 . to leave no place for ambiguity , every @xmath147 that appears in the sequel is assumed nonempty .",
    "[ def : pqne ] a mapping @xmath145 is called _ partially quasi - nonexpansive , _ if @xmath148 the fixed point set @xmath147 is _ not _ necessarily a convex set .",
    "let us also define a stronger version of ; the mapping @xmath145 will be called _ strongly _ or _ @xmath149-attracting partially quasi - nonexpansive mapping _ if there exists an @xmath150 such that @xmath151    an example of such a mapping is the novel generalized thresholding mapping of section [ sec : gt ] ( for a proof see app .",
    "[ sec : properties.t_k ] ) . in app .",
    "[ sec : properties.t_k ] , we will also verify that @xmath152 is a union of subspaces , which is indeed a non - convex set . recall that at the heart of any sparsity - promoting learning method lies the search for a solution in a union of subspaces @xcite .",
    "it must be pointed out that a number of well - known mappings , e.g. , @xcite , are special cases of the previously defined class of partially quasi - nonexpansive ones .",
    "the convergence analysis of the apgt is given by the following thm .",
    "[ thm : algo ] .",
    "this analysis is based on a set of deterministic assumptions , given below .",
    "since the apgt is based on the mapping @xmath104 , whose fixed point set ( see app .",
    "[ sec : properties.t_k ] ) is non - convex , this is the first time that the results of @xcite are generalized to _ non - convexly constrained online learning tasks . _    [ assumptions ]    1 .   [ ass : omega.n ]",
    "assume that @xmath153 such that @xmath154 \\neq \\emptyset$ ] .",
    "let us explain here the physical reasoning behind this assumption .",
    "recall , here , that @xmath155\\}_{i\\in\\mathcal{i}_n}$ ] is the set of all active hyperslabs ( see alg .",
    "[ apgt ] ) , at the time instant @xmath126 . for an appropriate choice of the parameters @xmath119 ( see ) ,",
    "the hyperslabs contain the desired @xmath17 with high probability .",
    "moreover , as time goes by , and due to a long sequence of projections in , the orbit @xmath156 is attracted closer and closer to the hyperslabs ; and as a consequence , closer to @xmath17 .",
    "for this reason , it is natural to expect that @xmath157 is similar to @xmath158 , and hence @xmath159 to @xmath160 , at some time @xmath126 .",
    "since @xmath160 enjoys a non - empty intersection with @xmath161 $ ] , with high probability , we anticipate that the same also happens to @xmath159 .",
    "[ ass : finite.omega ] assume that there exists a time instant @xmath162 , and an @xmath24 , such that @xmath163 .",
    "[ ass : omega ] assume that @xmath164 .",
    "in other words , we assume that the set of all points , which belong to all but a finite number of @xmath165s , is nonempty .",
    "[ thm : algo ]    1 .",
    "[ simple.monotinicity ] let assumption [ assumptions].[ass : omega.n ] hold true .",
    "then , @xmath166 , where @xmath167 stands for the ( metric ) distance function @xcite to @xmath165 .",
    "[ bound ] let assumption [ assumptions].[ass : finite.omega ] hold true .",
    "then , @xmath168 ) : j\\in\\mathcal{j}_n\\bigr\\}.\\end{aligned}\\ ] ] in other words , the previous inequality establishes a bound on the distance of the estimates from a _",
    "finite _ intersection of the @xmath165s . if we assume , also , that there exists an estimate @xmath26 which does not belong to such an intersection , i.e. , @xmath169 such that @xmath170 ) : j\\in\\mathcal{j}_{n'}\\bigr\\ } > 0 $ ] , then the previous result claims that the apgt forces @xmath171 to be located _ strictly _ closer to @xmath172 than @xmath173 is",
    "let assumption [ assumptions].[ass : omega ] holds true .",
    "then , 1 .   [ thm : exist.cluster ] the set of all cluster points of the sequence @xmath156 is nonempty , i.e. , @xmath174 .",
    "[ thm : distance.goes.2.zero ] @xmath175 ) = 0 $ ] . in other words , as the time advances , the orbit @xmath156 approaches @xmath176)_{n\\in{\\mathbb{n}}}$ ] .",
    "[ thm : inclusion.4.cluster.points ] @xmath177 . in words ,",
    "the apgt generates a sequence of estimates @xmath156 , whose _",
    "cluster points are sparse vectors , _ of sparsity level no larger than @xmath51 .",
    "see appendix [ sec : analysis.algo ] .",
    "in this section , our main intention is to provide the proof of concept of the theoretical findings presented in thm .",
    "[ thm : algo ] .",
    "this is realized via the performance evaluation of , where the shrinkage function @xmath90 , in def .",
    "[ def : gt ] , assumes any form of @xmath178 , defined in .",
    "this study is not meant to be exhaustive , and in order to demonstrate the potential of the proposed technique , the hard thresholding ( ht ) as well as the plstos corresponding to the scad @xcite and the @xmath179 penalty ( @xmath180 ) @xcite are examined , since they exhibit distinct characteristics , as it is illustrated in figs .  [",
    "fig : variousthresholds1]b and [ fig : variousthresholds1]e , respectively .",
    "notice that the associated penalty functions are non - convex .",
    "the resulting thresholding rules are called the _",
    "scad _ and the _ bridge thresholding ( bt ) _ , respectively",
    ". notice , also , that scad is a piece - wise linear thresholding operator , whereas , the bt exhibits strong discontinuity and non - linearity .    in order to comply with the theory , the scad , the bt , and the ht are used as shrinkage functions @xmath90 in def .",
    "[ def : gt ] , for all the components @xmath181 with index @xmath182 , where @xmath51 stands for an estimate of the true @xmath183 . to this end , we have slightly modified the classical scad , bt , and ht rules in order to fit our need to keep a number of @xmath51 components of a vector intact . as such",
    ", the scad thresholding operates according to the following rule ; given the input @xmath55 and the output vector @xmath184 , the @xmath185-th coordinate of @xmath74 , where @xmath186 , is given by the next rule : @xmath187 , \\\\",
    "\\operatorname{sgn}(x_i ) \\bigl(\\frac{(\\alpha-1)|x_i|-\\alpha \\lambda}{\\alpha -2 } -    \\delta\\bigr)_+ , & \\\\ & \\hspace{-60pt}\\text{if}\\ |x_i| \\in \\bigl(2\\lambda , \\min\\bigl\\{\\xi_{\\bm{x}}^{(k)},\\alpha \\lambda\\bigr\\}\\bigr ] , \\end{cases}\\ ] ] where @xmath188 is the regularization parameter , which appears in the definition of the plsto in , @xmath189 is a user - defined parameter , inherent to scad @xcite , @xmath86 is a sufficiently small user - defined parameter motivated by definition  [ def : gt].[strictly.shrinks ] , and @xmath190 , introduced here in order to leave no place for ambiguities",
    ". our modification on the classical scad can be seen by the introduction of @xmath89 , @xmath71 , and @xmath191 .",
    "similarly , given the classical version of the bt rule @xcite , our modified bt is given as follows by involving the quantity @xmath71 in the computations : @xmath192 , @xmath193 where @xmath188 is the corresponding regularization parameter in , @xmath194 is a user - defined parameter , and @xmath195 the term @xmath196 stands for the solution of the equation @xmath197 .",
    "when @xmath198 is set equal to @xmath199 , @xmath196 is obtained in closed form by solving a third order polynomial equation .",
    "similarly , ht is given by the following rule ; @xmath200 , @xmath201 where the @xmath188 is introduced here in order to be compliant also to a definition of the ht used often in the literature ( see the discussion in appendix  [ sec : plsto ] ) .    in the following experiments , unless otherwise stated , the signal under consideration has @xmath202 and @xmath203 .",
    "moreover , the classical cs signal recovery problem is considered , where the input ( sensing ) vectors have independent components drawn from a normal distribution @xmath204 , and the observations are corrupted by additive white gaussian noise of variance @xmath205 . regarding apgt , the extrapolation parameter @xmath137",
    "is set equal to @xmath206 , and the hyperslab parameter @xmath207 , @xmath208 . in this paper , for all the techniques employed , configurations leading to the fastest convergence rate are of principal interest .",
    "from this perspective , unless otherwise stated , @xmath124 is fixed to @xmath209 since this appeared to be the lowest @xmath124 value leading to enhanced convergence speed for the specific @xmath210 and @xmath51 values .",
    "it should be stressed out that the method is not sensitive to the parameter @xmath124 . a larger @xmath124 value would only add to computational complexity without any significant contribution to performance .",
    "an extensive and complementary experimental study of the apgt performance , in the case where @xmath124 is confined to small values , which relates to very low computational complexity techniques , can be found in @xcite . in all of the succeeding figures",
    ", the mse stands for @xmath211 , where @xmath212 is the sequence generated by the @xmath185-th realization of alg .",
    "[ apgt ] , and @xmath213 is the number of independent realizations in order to smooth out the obtained performance curves .      by the modifier `` time - invariant '' ,",
    "we mean that the user - defined parameter @xmath188 in remains fixed for all the time instants @xmath122 .",
    "the performance of all the employed methods is given in fig .",
    "[ fig : constthres ] . in all cases , @xmath214 .",
    "the regularization parameter @xmath188 was optimized leading to the values shown in the corresponding figure legend . moreover , apgt - scad , without being considerably sensitive to parameter @xmath189 , appeared to perform best when adopting the relatively large value @xmath215 .    for comparison , the _ improved proportionate adaptive projection algorithm ( ipapa ) ,",
    "_ described in @xcite , is employed .",
    "the _ projection order _ of the ipapa , which plays a similar role to @xmath124 , and therefore the same notation is used , is the major factor which dictates its performance .",
    "dashed curves indicated with triangles , stars and squares correspond to values of @xmath124 equal to @xmath216 , @xmath217 , and @xmath218 , respectively .",
    "the step parameter of the ipapa is denoted by @xmath219 . the best ipapa performance , i.e.",
    ", the one depicted with a dashed curve with diamonds , is achieved with @xmath220 and @xmath221 . for lower @xmath124 values ,",
    "such a large @xmath219 led to unstable performance . in all cases , the parameter @xmath222 , which tunes the weights in the proportionate algorithm in @xmath223 of @xcite .",
    "we call it here @xmath222 in order to avoid confusion with the parameter @xmath189 of scad .",
    "] , was given the large value @xmath224 in order to exhibit enhanced sparsity promoting behavior .",
    "when larger @xmath124 values are used , e.g. , @xmath225 , the performance turned to become somewhat faster , but with a quite elevated steady - state error floor , so the corresponding performance curves are not shown .",
    "moreover , a set - membership counterpart of ipapa @xcite was also examined .",
    "this algorithm performed similarly to ipapa , so the results are not shown to ease visualization .",
    "it is clear that the apgt-@xmath117 performs as well as ipapa .",
    "however , this is achieved under a significantly lower computational burden , as will be discussed in section  [ sec : complexity ] .      in the previous section ,",
    "the exact shape of the thresholding function was determined in advance using _ fixed _",
    "values for the associated parameters , e.g. , @xmath188 , @xmath198 , @xmath189 , etc .",
    "this is quite limiting , since the proposed technique has the potential to incorporate time - adaptive a - priori information , in the form of time - varying thresholding operators .",
    "this section demonstrates that exploiting this freedom leads apgt to enhanced performance .",
    "in particular , @xmath188 in changes as time @xmath126 advances .",
    "in order to explicitly describe this dependency of @xmath188 to @xmath126 , we will use hereafter the notation @xmath226 . assuming that an estimate @xmath51 of the true sparsity level @xmath22 is available at each iteration @xmath126 , parameter @xmath226",
    "is properly tuned in order to guarantee that after thresholding , a fixed number of components will be set equal to zero . with respect to the ht operator , in order to achieve a sparsity level equal to @xmath51 , i.e. , @xmath227 components are zero , the quantity @xmath226 should be set equal to @xmath228 , @xmath208 .",
    "for the scad case , @xmath229 , @xmath208 , ( refer to ) . in this way ,",
    "the scad shrinkage behavior is preserved and tuned by the user - defined parameter @xmath189 . in a similar manner",
    ", an adaptive bt can be built .",
    "going even further , apart from the @xmath51 larger in magnitude components which remain unaltered , the next , say @xmath230 , smaller in magnitude components could be shrunk according to the bridge rule .",
    "this is achieved if we notice that , by definition , @xmath231 , @xmath232 , and that the parameter @xmath226 is defined here as the solution of the following equation @xmath233 . in particular , for @xmath234 , this solution obtains a closed form : @xmath235 for convenience , the full gt operator involving the @xmath117 shrinkage is given next : @xmath200 , @xmath236 where @xmath196 satisfies @xmath237 , and @xmath226 is given by .    the performance of apgt methods , using the previous time - adaptive thresholding strategy ,",
    "hereafter abbreviated as apgt - at , is shown in fig .",
    "[ fig : adaptthres390 ] . for reference ,",
    "the dotted curve marked with open circles is the one from fig .",
    "[ fig : constthres ] corresponding to the best apgt method with a fixed @xmath188 .",
    "moreover , the best results for the apgt - at-@xmath117 are obtained when @xmath230 assumes a small integer value , such as @xmath238 .",
    "a conclusion that can be easily drawn is that the incorporation of adaptive thresholding led to a performance boost .",
    "moreover , the performance achieved depends on the thresholding operator that is adopted , with the bt leading to somewhat faster convergence speed compared to scad and ht .",
    "the performance of apwl1 , proposed in @xcite , is also shown with solid line marked with triangles .",
    "it appears that the newly proposed algorithms , and especially apgt - at-@xmath117 , succeeds in achieving a similar convergence behavior and speed compared to apwl1 and , as it will be discussed in section  [ sec : complexity ] , with half the computational complexity . for completeness ,",
    "the _ online cyclic coordinate descent - time weighted lasso ( occd - twl ) , _ presented in @xcite , is depicted with solid line marked with squares .",
    "the latter is an online algorithm approximating the lasso problem solution .",
    "it is observed , that apgt ( @xmath239 ) , demonstrates a performance competitive to occd - twl , which is an @xmath240 complexity algorithm .",
    "the advantages of the apgt algorithm over the apwl1 are not limited to the performance improvements and/or to computational complexity savings .",
    "the proposed theoretical framework is general enough in order to include other thresholding operators as well , either existing or newly defined .",
    "however , the scope of this paper is not a simulation study of all these alternatives of thresholding , and such a route will be studied elsewhere . for example , in @xcite , implementations of the proposed scheme driven by a different set of plstos , suitable for low complexity operation , and a novel specially customized thresholding operator are presented . in that case ,",
    "comparison with linear complexity sparsity inducing algorithms , such as the reweighted zero attracting - least mean square ( rza - lms ) @xcite , @xmath241-lms @xcite , and the sparse adaptive orthogonal matching pursuit ( spadomp ) @xcite is made in more advanced scenarios , such as system identification with correlated input signal ( see @xcite ) and sparse signal estimation corrupted by non - symmetric and/or impulsive noise .      with the aid of fig .",
    "[ fig : wrongsparsity20 ] , the effect of over- and under - estimation of @xmath22 is discussed for the reduced complexity case of @xmath242 .",
    "we choose a low value for @xmath124 , since we noticed that such a scenario reveals more distinctly the performance sensitivity and related behavior of the apgt with over- or under - estimations of @xmath22 .",
    "moreover , the use of a low value of @xmath124 , reveals the performance advantages of the gt , compared to other linear complexity algorithms , such as the @xmath241-lms @xcite . as it is seen from the fig .",
    "[ fig : wrongsparsity20 ] , the use of the gt mapping results in enhanced performance w.r.t .",
    "both apwl1 and @xmath241-lms , where the latter was fine - tuned for best convergence speed / error floor trade off .",
    "in order to have a reference of the performance achieved when the true sparsity level is given , the apgt - at-@xmath117 with @xmath243 , is also provided in fig .  [",
    "fig : wrongsparsity20 ] .",
    "let us start with the under - estimation case and assume that @xmath244 , i.e. , @xmath245 lower compared to the true sparsity level .",
    "let us take , for example , the apgt - at - scad curve , which shows an elevated error floor .",
    "notice that the case of under - estimations of @xmath22 is not supported theoretically by thm .",
    "[ thm : algo ] . with respect to over - estimation",
    ", apgt is shown to be very robust .",
    "for example , let us see the case where @xmath22 is over - estimated by @xmath246 , i.e. , @xmath247 . the performance achieved by apgt - at-@xmath117 ( solid line with open circles ) is still much better compared to the apwl1 , even if apwl1 uses an accurate estimate for the @xmath22 .",
    "moreover , the degradation resulted from such a large over - estimation appears to be limited .",
    "remarkably , in this low @xmath124 case , both apgt - at - ht and apgt - at - scad , drawn with solid lines marked with x - crosses and diamonds , respectively , have benefited from the over - estimation .",
    "the reason for this is that when @xmath124 is small , the tentative estimates of the unknown vector in each iteration are likely to be not accurate enough in order for the @xmath22 larger of them to reveal the true support of the vector .",
    "an over - estimated @xmath22 leads to less strict ht and scad thresholding operators , which allow components that would otherwise be set equal to zero , to survive .",
    "all the results above have been confirmed with higher levels of over - estimation .",
    "the results are similar when the algorithms operate with higher complexity , i.e. , @xmath239 , with the difference that the performance of apgt - at - ht and apgt - at - scad does not benefited as much as previously by an over - estimation of @xmath51 .",
    "the apgt - at - scad and apgt - at-@xmath117 perform similarly , so the corresponding curves are not shown . a thorough examination of several scenarios , in the case where @xmath124 attains low values , is deferred to a future work .",
    "[ fig : timevar ] shows the ability of the tested algorithms to track an abrupt change of the unknown vector @xmath17 , which is realized here after @xmath248 observations is examined .",
    "this is a typical setting used in adaptive filtering @xcite community to study the tracking agility of an algorithm . here , in order to give an essence from the cs paradigm , we consider the vector to be not sparse itself but to have a sparse wavelet representation . in the first half ,",
    "the signal under consideration is of length @xmath202 , with @xmath249 non zero wavelet coefficients .",
    "however , at the @xmath248 time instant , ten randomly selected wavelet coefficients change their values from @xmath93 to a randomly selected nonzero one . since the sparsity level of the signal changes ( from @xmath217 to @xmath250 , at most ) and it is not possible to know @xmath22 exactly in advance , taking into account that the methods we propose are quite robust to @xmath22 over - estimations , we set @xmath251 throughout the whole experiment",
    "moreover , @xmath124 is set to @xmath209 .    for the occd - twl , an rls - like forgetting factor lower than @xmath252 is adopted , in order to succeed in re - estimating the unknown signal after the abrupt change",
    ". more specifically , the value of @xmath253 appeared to offer a good trade - off between convergence speed and steady - state error floor .",
    "however , the occd - twl convergence speed slows down after the @xmath248 time instant , something which was observed and discussed in @xcite as well .",
    "the ipapa method , catches up quickly after the abrupt change ; however , the attained error floor is higher than that of the apgt .",
    "the choice of the thresholding operator affects significantly the overall computational burden for two reasons .",
    "first , the thresholding function itself requires a larger or smaller number of mathematical operations depending on the specific thresholding rule .",
    "such operations can be multiplications , divisions , as well as sorting operations .",
    "additions are ignored since they are considered to be much less costly .",
    "a second attribute of the thresholding rule , which affects complexity , is whether its outcome is a sparse vector with a certain sparsity level or not . indeed , if the thresholding operator produces vectors which are , say , @xmath254-sparse , then projections in apgt involve inner products with sparse vectors where the number of required multiplications equal to @xmath254 instead of @xmath210 .",
    "the ht and the gt with bridge-@xmath117 shrinkage function , as they where presented in [ sec : performance_adaptplsto ] , belong to this category with @xmath255 and @xmath256 , respectively .",
    "the scad thresholding rule does not guarantee a fixed number of zeros after its application .",
    "this is also the case of the apwl1 @xcite .",
    "moreover , in the case of the apwl1 , exact projections onto the weighted @xmath0-ball need to be computed , and in order to do so , the sorting of a vector is necessary , which requires in general @xmath257 operations .",
    "however , by adopting a divide - and - conquer approach , as in @xcite , one might reduce the above computational complexity down to @xmath258 operations .",
    "the worst - case computational complexities of all the methods employed are given in table  [ table : complexities.theory ] .",
    "the parameter @xmath259 is either 1 or 2 , depending on whether all @xmath260 of the apgt are given the same value or not . in the examples of this paper",
    "the former is the case , i.e. , @xmath261 .",
    "moreover , parameter @xmath262 is either @xmath252 , if the @xmath263 norm of the input vectors @xmath18 is arbitrary , or @xmath93 , if it is normalized to unity .    [ cols=\"<,^,^,^,^ \" , ]",
    "the present paper contributed to sparsity - aware online learning tasks in the following three ways : ( i ) it established a generalized thresholding ( gt ) mapping , which can incorporate as a shrinkage function the majority of the thresholding rules found in the literature , ( iii ) it proposed a non - convexly constrained , online learning algorithm for sparse signal recovery tasks with a computational complexity which scales linearly to the number of unknowns , and ( iii ) it introduced a family of mappings which serves as the wide functional analytic stage for the study of the previous gt operator .",
    "rigorous discussions on the properties of all the previous functional analytic tools , as well as a convergence analysis of the proposed algorithm were provided . to validate the theoretical findings regarding our algorithm ,",
    "extensive experiments were conducted , which showed that the proposed methodology offers a sound theoretical , and very competitive time - adaptive technique , with lower computational complexity than several of the state - of - the - art , sparsity - promoting , online learning algorithms .",
    "a subset @xmath264 of @xmath7 will be called convex , if for any @xmath265 , the line segment @xmath266 \\}$ ] lies in @xmath264 .",
    "a function @xmath267 is called convex if @xmath268 , and @xmath269 $ ] , we have @xmath270 .",
    "the _ @xmath93-th level set _ of the convex @xmath271 is defined as @xmath272 .",
    "a subgradient of the convex function @xmath271 at a point @xmath273 , denoted as @xmath274 , is an @xmath210-dimensional vector such that @xmath275 , @xmath276 . in general , the number of the subgradients of @xmath271 at @xmath273 is infinite .",
    "the set of all subgradients of @xmath271 at a point @xmath273 is called subdifferential , and it is denoted by @xmath277 . in the case",
    "where @xmath271 is differentiable at @xmath273 , then the subgradient @xmath274 is unique , and it is nothing but the gradient of @xmath271 at @xmath273 .    given a closed convex @xmath278 , define the _",
    "( metric ) distance function @xmath279 to @xmath264 _ as follows : @xmath280 , @xmath281 . notice that @xmath282 is convex with @xmath283 .",
    "the _ ( metric ) projection onto @xmath264 _ is defined as the mapping @xmath284 , which maps an @xmath285 to the _ unique _ @xmath286 , such that @xmath287 .",
    "for example , the subdifferential of @xmath282 is given as follows : @xmath288 , & \\text{if}\\ \\bm{a}\\in c,\\\\   \\frac{\\bm{a}-p_c(\\bm{a})}{d(\\bm{a},c ) } , & \\text{if}\\ \\bm{a}\\notin c ,   \\end{cases } \\label{subdiff.distance}\\ ] ] where @xmath289 .",
    "going back to , choose @xmath24 , and define @xmath290\\in { \\mathbb{r}}^{l\\times n}$ ] , as well as @xmath291^{\\top}\\in{\\mathbb{r}}^n$ ] , and @xmath292^{\\top } \\in { \\mathbb{r}}^n$ ] .",
    "then , it can be easily verified that takes the form of @xmath293 , @xmath40 .",
    "the mainstream of the batch sparsity - promoting algorithms utilize all the gathered @xmath294 training data to find an exact or approximate solution , in most cases iteratively , to the following _ penalized least - squares _ minimization task , @xmath295 where @xmath296 stands for a sparsity - promoting and non - convex , in general , _ penalty _ function , @xmath297 is the _ regularization _ parameter , and @xmath298 stands for the @xmath185-th coordinate of the vector @xmath273 .",
    "choices for @xmath114 are numerous ; if , for example , @xmath299 , @xmath300 , where @xmath301 stands for the characteristic function with respect to @xmath302 , i.e. , @xmath303 , if @xmath304 , and @xmath305 , if @xmath306 , then the regularization term @xmath307 becomes the @xmath241-norm of @xmath273 . in the case",
    "where @xmath308 , @xmath309 , then the regularization term is nothing but the @xmath0-norm @xmath310 , and the task becomes the celebrated lasso @xcite .",
    "however , it has been observed that if some of the lasso s regularity conditions are violated , then lasso is sub - optimal for model selection @xcite . such a behavior has motivated the search for _ non - convex _ penalty functions @xmath114 , which bridge the gap between the @xmath241- and @xmath0-norm ; for example , the @xmath179 penalty , for @xmath194 , @xcite , the @xmath311 @xcite , the scad @xcite , the mc+ @xcite , and the transformed @xmath0 @xcite penalties .",
    "recently , sparsity - promoting coordinate - wise optimization techniques for solving the task are attracting a lot of interest @xcite . to be more concrete , assume , for example , that @xmath312 , and that the matrix @xmath313 is orthogonal .",
    "byy defining @xmath314 , can be equivalently viewed as the following separable optimization task @xcite , @xmath315 under some mild regularity conditions on @xmath114 @xcite , the minimization task of possesses a unique minimizer . due to the separability of in coordinates , the minimization task of",
    "can be viewed as a task defined on an @xmath252-dimensional axis , instead of an @xmath210-dimensional domain .",
    "accordingly , the problem reduces to the univariate pls task described in .    figs .",
    "[ fig : variousthresholds1](b - d ) , show the thresholding functions ( plsto , see ) , which solve for some of the most commonly employed penalty functions .",
    "for example , if @xmath316/\\lambda$ ] , @xmath309 , then the resulting plsto is the celebrated _ hard thresholding ( ht ) _ mapping @xcite , which is depicted in fig .",
    "[ fig : variousthresholds1]a together with the well - known _ soft thresholding ( st ) _ mapping which results in the case where @xmath308 , i.e. is chosen such that to lead to the lasso task . note that both st and ht operators have been effectively employed in iterative thresholding schemes for fast sparse signal recovery under the compressed sensing framework @xcite . the rest of the thresholding rules , shown in fig .",
    "[ fig : variousthresholds1]b correspond to the mc+ penalty @xcite and the scad @xcite , respectively . both scad and",
    "mc+ leave large components unchanged , like ht , while avoiding being discontinuous and at the same time allowing a linear / gradual transition between the `` kill '' and the `` keep '' areas of ht .",
    "ht is far from being the only discontinuous thresholding operator .",
    "an example is shown in figs .",
    "[ fig : variousthresholds1]c , by the widely known bridge threshold @xcite , which is related to the @xmath179 penalty , @xmath180 @xcite .",
    "note that this thresholding rule comprise nonlinear segments .",
    "continuous thresholding functions , that contain nonlinear parts , are shown in fig .",
    "[ fig : variousthresholds1](d ) .",
    "more specifically , the non - negative garrote @xcite and representatives of the n - degree garrote threshold are shown .",
    "similar thresholding functions are also the hyperbolic shrinkage rule @xcite and plsto s stemming from the nonlinear diffusive filtering approach @xcite .",
    "[ thm : properties.t_k ]    1 .",
    "[ thm : same.support ] @xmath317 , @xmath318 .",
    "[ thm : fix.t_k ] @xmath319 .",
    "notice , here , that @xmath152 , as a union of subspaces , is non - convex .",
    "[ thm : demiclosed ] let a sequence @xmath320 and an @xmath321 .",
    "if @xmath322 , and @xmath323 , then @xmath324 .",
    "this property can be rephrased as @xmath325 being _ demiclosed at @xmath326 _ @xcite .",
    "[ thm : t_k.pqne ] @xmath104 is _ @xmath252-attracting partially quasi - nonexpansive ,",
    "_ i.e. , @xmath327 , @xmath328 .",
    "_ proof : _    1 .",
    "define @xmath329 . in order to derive a contradiction ,",
    "assume that @xmath330 .",
    "since both @xmath331 have the same cardinality , the previous assumption means that there exist @xmath332 such that @xmath333 , and @xmath334 . hence , @xmath335 , which , in turn , suggests by the definition of @xmath91 that @xmath336 .",
    "moreover , @xmath337 and @xmath338 by the definition of @xmath339 .",
    "thus , @xmath340 , which is absurd .",
    "this contradiction establishes the claim of thm .",
    "[ thm : properties.t_k].[thm : same.support ] .",
    "2 .   pick any @xmath341 .",
    "it is easy to verify by def .",
    "[ def : gt ] that @xmath342 , i.e. , @xmath343 . to prove the opposite inclusion , assume any @xmath344 , i.e. , @xmath342 .",
    "since @xmath345 , the relation @xmath342 leads to the trivial result @xmath346 , we deal here only with the more interesting case of @xmath78 .",
    "for such an @xmath347 , according to def .",
    "[ def : gt ] , we must have @xmath348 , which implies that @xmath349 . however , by the properties of @xmath90 , given in defs .",
    "[ def : gt].[shrinks ] and [ def : gt].[strictly.shrinks ] , we necessarily obtain that @xmath350 . since this holds @xmath351 , def .",
    "[ def : special.subspace ] suggests that @xmath352 .",
    "now , recall that @xmath353 to establish the inclusion @xmath354 .",
    "3 .   1 .   assume , for a contradiction , that there exists an @xmath355 and a subsequence @xmath356 , such that @xmath357 , @xmath358 , @xmath359 .",
    "+ by def .",
    "[ def : gt].[strictly.shrinks ] , @xmath360 such that @xmath361 , @xmath362 .",
    "then , it is easy to verify that @xmath362 , @xmath363 .",
    "this implies that @xmath362 , @xmath364 + notice that @xmath365 .",
    "hence , the assumption that @xmath366 implies that for the @xmath89 of , @xmath367 such that @xmath368 , @xmath369 .",
    "this contradicts . in other words ,",
    "our initial claim is wrong , and the contrary proposition becomes : @xmath370 , there exists an @xmath162 such that @xmath371 , @xmath372 , @xmath368 .",
    "this can be equivalently written in a more compact form as follows :",
    "@xmath373 2 .   let us define here @xmath374 in words , @xmath375 contains all those points which belong to all but a finite number of @xmath376s .",
    "there are two cases regarding @xmath377 and @xmath375 ; either @xmath378 or @xmath379 .",
    "notice that the latter covers also the case where @xmath380 .",
    "let us examine each case separately",
    "the case of @xmath381",
    "assume that @xmath382 .",
    "this implies that there exists an @xmath383 such that @xmath384 .",
    "since both @xmath377 and @xmath376 have the same cardinality , i.e. , @xmath51 , we obtain that @xmath368 , @xmath385 .",
    "choose any @xmath386 . by , @xmath387 .",
    "thus , @xmath388 , @xmath389 , or equivalently , @xmath390 .",
    "2 .   assume now that @xmath391 .",
    "hence , there exists an @xmath392 and a subsequence @xmath356 such that @xmath393 , @xmath359 . by ,",
    "since @xmath392 , we clearly have that @xmath395 , @xmath396 .",
    "hence , @xmath390 .",
    "the case of @xmath397 .",
    "this means that there exists an @xmath398 and a subsequence @xmath356 such that @xmath399 , @xmath359 .",
    "thus , similarly to our previous arguments , @xmath400 .",
    "4 .   define @xmath401 .",
    "given any @xmath55 , let @xmath402 , as in def .",
    "[ def : gt ] . then , verify that @xmath403 , @xmath404 the previous inequality is obtained from the observation that the properties of @xmath90 in def .",
    "[ def : gt ] suggest @xmath405 , and from the following elementary calculations : @xmath406 .",
    "hence , @xmath407 , @xmath408 , @xmath409 , where in order to obtain the last equivalence we used some elementary algebra , and the fact + @xmath410 .",
    "this establishes the claim of thm .",
    "[ thm : properties.t_k].[thm : t_k.pqne ] .",
    "let us define first a sequence of convex functions @xmath411 in an inductive way . given the time index @xmath126 , and the estimate @xmath412 , define the following convex function ; @xmath413 , @xmath414 ) } { \\sum_{j\\in\\mathcal{i}_n}\\omega_j^{(n ) } d(\\bm{a}_n ,    s_j[\\epsilon_j ] ) } d(\\bm{a } , s_i[\\epsilon_i ] ) & \\\\",
    "\\quad = \\frac{1}{l_n}\\sum_{i\\in\\mathcal{i}_n } \\omega_i^{(n ) } d(\\bm{a}_n ,    s_i[\\epsilon_i ] ) d(\\bm{a } , s_i[\\epsilon_i ] ) , \\\\ & \\hspace{-50pt}\\text{if}\\ \\mathcal{i}_n \\neq \\emptyset,\\\\ 0 , & \\hspace{-50pt } \\text{otherwise } , \\end{cases}\\ ] ] where @xmath415)$ ] .",
    "it is easy to verify by the definition of @xmath131 , that if @xmath416 , then @xmath417 , @xmath418)>0 $ ] , and thus @xmath419 .",
    "moreover , if @xmath420 , then @xmath421 , @xmath422 .",
    "let us look closer to @xmath423 , and especially only the interesting case of @xmath416 . by standard subgradient calculus",
    ", it can be verified by that @xmath424 ) \\frac{\\bm{a}_n -    p_{s_i[\\epsilon_i]}(\\bm{a}_n)}{d(\\bm{a}_n , s_i[\\epsilon_i ] ) } = \\frac{1}{l_n } \\sum_{i\\in\\mathcal{i}_n } \\omega_i^{(n ) } \\bigl(\\bm{a}_n - p_{s_i[\\epsilon_i]}(\\bm{a}_n)\\bigr).\\ ] ] thus , whenever @xmath425 , we have @xmath426 iff @xmath427}(\\bm{a}_n)\\bigr)= \\bm{0}$ ] .",
    "hence , for some user - defined parameter @xmath428 , it is straightforward to see that",
    "@xmath429)}{{\\left\\|{\\sum_{i\\in\\mathcal{i}_n } \\omega_i^{(n ) }    \\bigl(\\bm{a}_n - p_{s_i[\\epsilon_i]}(\\bm{a}_n)\\bigr)}\\right\\|}^2 } \\sum_{i\\in\\mathcal{i}_n } \\omega_i^{(n ) } \\bigl(\\bm{a}_n - p_{s_i[\\epsilon_i]}(\\bm{a}_n)\\bigr).\\ ] ] if we let @xmath430 , then an examination of , for both the cases of @xmath431 and @xmath432 , implies that the proposed algorithm can be rephrased as follows ; for @xmath433 $ ] , @xmath434    1 .   fix any @xmath435 , and assume that @xmath436 .",
    "then , since @xmath437 , @xmath438 where we used the property of thm .  [ thm : properties.t_k].[thm : t_k.pqne ] , and the definition of the subgradient @xmath439 . as a result ,",
    "notice , that this holds true also for the case where @xmath441 .",
    "now , if we apply @xmath442 on both sides of the previous inequality , then we establish the claim of thm .",
    "[ thm : algo].[simple.monotinicity ] .",
    "2 .   fix @xmath443 .",
    "assume that @xmath436 .",
    "then , notice by the convexity of the function @xmath140 that @xmath444 ) \\sum\\limits_{i\\in\\mathcal{i}_n } \\omega_i^{(n ) } d^2(\\bm{a}_n ,      s_i[\\epsilon_i])}{{\\left\\|{\\sum_{i\\in\\mathcal{i}_n } \\omega_i^{(n ) }          \\bigl(\\bm{a}_n - p_{s_i[\\epsilon_i]}(\\bm{a}_n)\\bigr)}\\right\\|}^2}\\nonumber\\\\       & \\geq \\frac{1}{q } \\sum_{i\\in\\mathcal{i}_n }      d^2(\\bm{a}_n , s_i[\\epsilon_i ] ) \\nonumber\\\\ & \\geq \\frac{1}{q } \\max    \\bigl\\{d^2(\\bm{a}_n , s_j[\\epsilon_j ] ) :    j\\in\\mathcal{j}_n\\bigr\\}. \\label{fraction.theta }    \\end{aligned}\\ ] ] hence , by , @xmath445 ) : j\\in\\mathcal{j}_n\\bigr\\}. \\label{2nd.inequality}\\end{aligned}\\ ] ] notice also that holds true also for the case where @xmath441 . if we take the infimum over all @xmath446 on both sides of , and if we add the resulting inequality for all values of @xmath447 , then the claim of thm .",
    "[ thm : algo].[bound ] is established .",
    "3 .   1 .   choose arbitrarily any @xmath448 .",
    "then , by definition , there exists an @xmath383 such that @xmath449 .",
    "clearly , leads to @xmath450 , @xmath451 , i.e. , @xmath452 is monotonically non - increasing , and thus convergent .",
    "this result implies also that the sequence @xmath156 is bounded , and that @xmath453 @xcite .",
    "2 .   let @xmath448 and @xmath383 as previously .",
    "we have already seen that @xmath454 , @xmath455 is convergent .",
    "thus , it is cauchy , and @xmath456 + now , a simple inspection of and establish the claim of thm .",
    "[ thm : algo].[thm : distance.goes.2.zero ] .",
    "3 .   notice that @xmath368 , @xmath426 iff @xmath457\\neq \\emptyset$ ] .",
    "hence , for such @xmath126 , takes the following equivalent form : @xmath458 where the mapping @xmath459 is the _ subgradient projection mapping with respect to the convex @xmath423 _ defined as @xcite : @xmath460 , if @xmath461 , and @xmath462 , if @xmath463 . the equations and imply that @xmath464 .",
    "it is a matter of simple algebra to show also that @xmath465 . as such , @xmath466 a remarkable property of the subgradient projection mapping",
    "is the following @xcite : @xmath467 , @xmath468 , @xmath469 + by and thm .",
    "[ thm : properties.t_k].[thm : same.support ] , we can see that @xmath470 .",
    "now , notice by thm .",
    "[ thm : properties.t_k].[thm : t_k.pqne ] and that @xmath454 , @xmath471 thus , by , we obtain @xmath472 + by thm .",
    "[ thm : algo].[thm : exist.cluster ] , choose any @xmath473 .",
    "thus , there exists a subsequence @xmath474 such that @xmath475 .",
    "hence , by , @xmath476 .",
    "this result , , and thm .",
    "[ thm : properties.t_k].[thm : demiclosed ] lead to @xmath477 . since @xmath478 was chosen arbitrarily , we obtain the desired @xmath479 .",
    "y.  murakami , m.  yamagishi , m.  yukawa , and i.  yamada . a sparse adaptive filtering using time - varying soft - thresholding techniques . in _ proceedings of the ieee icassp _ , pages 37343737 , dallas : usa , mar . 2010 .",
    "k.  slavakis and i.  yamada .",
    "the adaptive projected subgradient method constrained by families of quasi - nonexpansive mappings and its application to online learning . to appear in the siam journal of optimization , 2013 .",
    "o.  hoshuyama , r.  a. goubran , and a.  sugiyama .",
    "a generalized proportionate variable step - size algorithm for fast changing acoustic environments . in _ proceedings of ieee icassp _ , volume  4 , pages 161164 , 2004 .",
    "y.  kopsinis , k.  slavakis , s.  theodoridis , and s.  mclaughlin .",
    "generalized thresholding sparsity - aware algorithm for low complexity online learning . in _ proceedings of ieee icassp _ , pages 32773280 , kyoto : japan , mar .",
    "2012 .",
    "s.  chouvardas , k.  slavakis , s.  theodoridis , and i.  yamada . a stochastic analysis of the hyperslab - based adaptive projected subgradient method . submitted for publication to the ieee signal processing letters ( manusrcipt i d : spl-11972 - 2012 ) .",
    "y.  kopsinis , k.  slavakis , s.  theodoridis , and s.  mclaughlin .",
    "thresholding - based online algorithms of complexity comparable to sparse lms methods . submitted to the 2013 ieee international symposium on circuits and systems ( iscas ) , oct .",
    "j.  duchi , s.  s - shwartz , y.  singer , and t.  chandra .",
    "efficient projections onto the @xmath0-ball for learning in high dimensions . in _ proceedings of international conference on machine learning ( icml ) _ , pages 272279 , 2008 ."
  ],
  "abstract_text": [
    "<S> this paper studies a sparse signal recovery task in time - varying ( time - adaptive ) environments . </S>",
    "<S> the contribution of the paper to sparsity - aware online learning is threefold ; first , a generalized thresholding ( gt ) operator , which relates to both convex and non - convex penalty functions , is introduced . </S>",
    "<S> this operator embodies , in a unified way , the majority of well - known thresholding rules which promote sparsity . </S>",
    "<S> second , a non - convexly constrained , sparsity - promoting , online learning scheme , namely the adaptive projection - based generalized thresholding ( apgt ) , is developed that incorporates the gt operator with a computational complexity that scales linearly to the number of unknowns . </S>",
    "<S> third , the novel family of partially quasi - nonexpansive mappings is introduced as a functional analytic tool for treating the gt operator . by building upon the rich fixed point theory , </S>",
    "<S> the previous class of mappings helps us , also , to establish a link between the gt operator and a union of linear subspaces ; a non - convex object which lies at the heart of any sparsity promoting technique , batch or online . </S>",
    "<S> based on such a functional analytic framework , a convergence analysis of the apgt is provided . </S>",
    "<S> furthermore , extensive experiments suggest that the apgt exhibits competitive performance when compared to computationally more demanding alternatives , such as the sparsity - promoting affine projection algorithm ( apa)- and recursive least squares ( rls)-based techniques . </S>"
  ]
}