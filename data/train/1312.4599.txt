{
  "article_text": [
    "darwin said , evolution consists of many complex mechanisms , which can come into existence without any unlikely events to occur .",
    "evolution consists of a path consisting of many interdependent small stages , but what are the conditions which make these paths to be taken and others , not ?",
    "it may take exponential time , if evolution just randomly searched all the possible paths , but since we know that the time to evolve is a large finite polynomially bounded value it means there may exist an efficient learning mechanism , which can learn certain function classes and can not learn others .",
    "so , basically mechanisms are treated as mathematical functions in this analysis , such that some functions can be learned in polynomial time , but others can not due to their inherent computational intractability .",
    "we describe some notions , which may help us to formally analyze the quantitative theory of evolvability . we all know",
    ", that a cell consists of various types of proteins and maybe , other chemicals and complex circuits and thus , its working depends on many variables .",
    "so , in order to define some formalized theory of evolution , mechanisms need to be represented as many argument functions .",
    "hence , a mechanism can take a lot of input parameters to properly function .",
    "so , what are many argument functions and how do we represent a mechanism through many argument functions ?",
    "well , a many argument function is a function @xmath0 , which takes more than one input parameters to produce an output .",
    "the complex structures of living cells , have to respond to wide variations in both external and internal conditions .",
    "say the conditions are represented by @xmath1 boolean variables , @xmath2 and let us have a function , say @xmath0 whose output shows some particular desirable response under a particular combination of the @xmath3s , for all @xmath4 $ ] .",
    "we say , the function @xmath5 is an ideal function and since , it depends on many input parameters , so it is a many argument function.later in the text , it is explicitly shown that the class of parity functions is not evolvable , while the class of monotone conjunctions over the uniform distribution is .",
    "let @xmath6 be the set of all @xmath7 possible values that all the @xmath3s can take .",
    "let @xmath8 be the probability distribution over @xmath6 , which basically gives the relative frequency of the occurrence of certain combinations of the @xmath3s.now , let us have the notion of performance._definition 1.1 .",
    "_  let us have a function @xmath9 .",
    "the performance of function @xmath10 with respect to the ideal function @xmath11 for the probability distribution @xmath8 over @xmath6 is @xmath12 say for some points @xmath13 which have non - zero probability in @xmath8 , we have @xmath14 . now ,",
    "if the ideal function @xmath0 completely agrees with @xmath10 on these set of points , then we have @xmath15 for all of them and thus,@xmath16 . again ,",
    "if @xmath0 does not agree with @xmath10 for all these @xmath17 , then we have @xmath18 and so @xmath19.so , the range of @xmath20 is @xmath21 $ ] and can be viewed as a fitness landscape over all the genomes @xmath10.all the points in @xmath6 can be thought of as life experiences .",
    "when @xmath10 agrees with the ideal function , we have a benefit , otherwise we have a penalty in case of disagreement.so , over a sequence of life experiences , the organisms or groups which have high values of the performance function are selected preferentially for survival over organisms which have low values of the performance function .",
    "it is thus , a basic mathematical definition of the darwinian concept of _ survival of the fittest_. an organism or a group can test the performance of its genome @xmath10 against the ideal function , by sampling a set @xmath22 , of poly(n ) size , say @xmath23 life experiences . let us have a definition concerning empirical performance , which concerns the size of the independent selections , @xmath24._definition 1.2 . _  the empirical performance , @xmath25 , for some positive integer @xmath24 is a random variable which makes @xmath24 independent selections from the set @xmath26 with replacements according to the distribution @xmath8 and has the value @xmath27 .",
    "we take @xmath23 to be the upper bound of population size , as the life experiences @xmath28 may correspond to one or more organisms.moreover , let us also insist that evolution is able to proceed from any starting point , otherwise proceeding back to the reinitialized state from the current state may heavily decrease the value of the performance function.let us discuss in short , the final two notions before proceeding to the definition of the concept of evolvability.since , the organisms that can exist at any time is finite and is polynomially bounded , so for a function only a limited number of variants can be explored per generation , whether through mutations or recombination.and finally , we say that mechanisms with significant improvements in the value of performance function , evolve in a limited number of generations.let us now have an idea of evolvability and some of its definitions , in terms of learning theory .",
    "from the perspective of learning theory , we can ask whether there exists a hypothesis @xmath10 for a target concept @xmath0 , such that @xmath10 closely approximates @xmath0 , the ideal function.let the concept class @xmath29 consists of all the ideal functions . since , evolution has a path and follows a finite number of steps , before it tries to be as close to @xmath0 as possible , so",
    "let us say that the hypothesis for the initial stage be @xmath30 , the hypothesis for the second stage be @xmath31 and continuing like this , the hypothesis for the @xmath32 stage is @xmath33 .",
    "the path of evolutionary sequence can be represented by @xmath34 , where we have @xmath35 and each hypothesis tries to approximate the ideal function better than its predecessor .",
    "if after a certain number of steps , say @xmath36 , such that the @xmath37 hypothesis is @xmath38 and we have @xmath39 , for some positive threshold @xmath40 , then the process of evolution starts from any function .",
    "hence , in short we have to get a good evolved representation @xmath10 , from the representation class , say @xmath41 , such that @xmath20 is very close to 1.we assume that any representation from @xmath41 is polynomially evaluatable , i.e @xmath42 can be computed in polynomial time , where @xmath13.let the error parameter of the evolved representation be @xmath43._definition 2.1 . _",
    "a @xmath44-neighborhood n on r , for polynomial @xmath44 and representation class @xmath45 is a pair of two randomized turing machines @xmath46 and @xmath47 , such that @xmath46 outputs all the @xmath48neighbors of a representation @xmath49 so that they all have error parameters of @xmath43 , using @xmath1 and @xmath50 as its input and @xmath47 takes all the outputs of @xmath46 as its input and returns some representation with probability at least equal to @xmath51 .",
    "let the representations , which are generated by @xmath46 be put in the set @xmath52 , so that we have @xmath53 .",
    "@xmath46 may also do random coin tosses to output members from the set @xmath52 .",
    "after this , @xmath47 takes all the members of @xmath52 as its input and returns some @xmath54 with probability , @xmath55.since , the size of @xmath52 is polynomially bounded , it means that the number of variants which can be searched is not unlimited , which is logical as the population at any time is finite .",
    "@xmath47 makes sure that differences in performance can be detected reliably.most of the exponentially many variants are considered to be impractical and hence , are discarded , while only @xmath56 variants are assumed to be feasible for evolution.it can be seen from the analysis , that all the resources , computation time , population and generation sizes are upper bounded by some polynomial , which depends on the number of variables and the inverse of the error parameter and this makes pac learning possible , which we will analyze later . to give a basic intuitive understanding of definition 2.1 , consider the diagram below .",
    "the blue region corresponds to the representation class , @xmath45 , the yellow region consists of all the neighboring representations of @xmath10 having error parameter of @xmath43 , generated by the randomized turing machine , @xmath46 and the red region consists of @xmath10 , which is taken as input by @xmath46 .",
    "the yellow region has a size of @xmath48 and @xmath47 randomly selects some representation @xmath57 from this region with probability at least equal to the inverse of the size of this region .",
    "now , let us define the mutator function , which helps in genetic mutation .",
    "_ definition 2.2 .",
    "_  the mutator function , @xmath58(where all of the input parameters represent same concepts as before and there exists a @xmath48 neighborhood @xmath59 on @xmath45 ) for the current representation , @xmath49 is a random variable , which outputs some variant @xmath54 with a certain probability , such that the performance value of @xmath57 with respect to the ideal function , @xmath0 never goes below the performance value of @xmath10 with respect to @xmath0 by some threshold @xmath60 .  for some @xmath61",
    ", if we have @xmath62 in the evolutionary path , we say that @xmath63.the above definition captures the notion of feasibility of an evolutionary path , somewhat in a lower level .",
    "say , for some representation , @xmath10 the mutator function @xmath64 computes some variant @xmath57 of it and computes the value of the performance , @xmath65.(we will represent the performance of a representation @xmath10 with respect to the ideal function over a distribution @xmath66 , by @xmath67 from now onwards ) .",
    "it is easy to see that if @xmath68 , then it is beneficial for evolution and we create a set @xmath69 , which contains all the positive variants of @xmath10 , capable of mutation .",
    "similarly , let us create a set @xmath70 , to keep all the variants , which have performance values , at least equal to @xmath71 , but not exceeding @xmath67 .",
    "hence , @xmath72.now , if the cardinality of the set @xmath69 is at least equal to 1 , we output a representation , @xmath73 with probability equal to @xmath74 , otherwise we output a representation @xmath75 , with probability @xmath76.we refer to @xmath77 as the tolerance variable of a representation.let @xmath78 denote the tolerance variable of each representation , @xmath33 , such that it is generated by a turing machine on inputs @xmath79 and the error parameter @xmath43 .",
    "it is logical to assume that each @xmath78 diminishes with @xmath43 , as we want an accuracy of @xmath80 , whereas population size or the no .",
    "of generations is inversely proportional to @xmath43 .",
    "so , we can bound each @xmath78 by two polynomially related polynomials @xmath81 and @xmath82 , such that we have @xmath83.we also have the number of experiences represented by the polynomial @xmath84.so , basically , if we need higher accuracy , then @xmath43 needs to be small and tolerance levels,@xmath78 should be less , so each variant must be closer to the parent genome with respect to performance.however , we need more life experiences , @xmath24 for analyzing the conditions of evolvability , which is logical.(at least @xmath24 is polynomially bounded ) .",
    "so , we have the following equation , representing evolvability of variants .",
    "definition 2.3 . _",
    "a class @xmath29 is _ tolerance evolvable _ iff there exists a polynomial @xmath86(generation size for evolution)and a turing machine @xmath87 , which computes @xmath78 for every @xmath88 , where @xmath83 , such that for every positive integer @xmath1 , every @xmath89 , every @xmath90 and every initial representation , @xmath91 , the evolution sequence @xmath92 , where @xmath93 must have a representation , @xmath94 , such that its performance value with respect to @xmath0 is at least @xmath80._definition 2.4 .",
    "_  a class @xmath29 is _ poly evolvable _ over @xmath66 iff there exists polynomially related variables , @xmath81 and @xmath82 , such that @xmath29 is _ tolerance evolvable _ by @xmath95 and @xmath59 over @xmath66._definition 2.5 . _",
    "a class @xmath29 is _ r - evolvable _ iff @xmath29 is _ poly evolvable _ over @xmath66 for some polynomials @xmath96 and some @xmath48 neighborhood @xmath59 on @xmath45 . _",
    "definition 2.6 .",
    "_  a class @xmath29 is _ d - evolvable _ , if for some representation class , @xmath45 it is _ r - evolvable _ over @xmath66 .",
    "_ definition 2.7 .",
    "_  a class @xmath29 is _ perfectly - evolvable _ if it is _ d - evolvable _ over all @xmath66 .",
    "thus , to give a low level intuitive overview of the definitions , consider the diagram below.(note that the diagram looks like a decision list , but it is not a decision list by any means .",
    "the diagram is just for basic intuitive understanding of the definitions above ) from the definitions it is clear that one notion follows from the previous , only if the previous is true .",
    "thus , a concept class which is _ perfectly - evolvable _ is the broadest of all the notions and is independent of any distribution .",
    "_ proposition 3.1 . _  if there exists a concept class , @xmath29 which is _ r - evolvable _ over a distribution @xmath66 , then @xmath29 is efficiently pac - learnable by @xmath45 over @xmath66._proof . _",
    "recall the basic definition of probably approximately correct ( pac ) learning model from [ 2 ] and [ 4]._definition . _  if @xmath29 is a concept class over some domain @xmath97 , then @xmath29 is pac learnable if there exists an algorithm @xmath98 , such that for every target concept @xmath99 , for every distribution @xmath66 on @xmath97 and for all @xmath100 , it outputs a hypothesis @xmath101 from the hypothesis class @xmath102 with probability at least @xmath103 so that the error of @xmath101 is upper bounded by @xmath43 , provided the algorithm is given access to an example oracle @xmath104 and supplied with the parameters @xmath43 and @xmath105.the above proposition says that the concept class @xmath29 is _ r - evolvable _ over @xmath66 , so this means that @xmath29 is _ poly - evolvable _ over @xmath66 for polynomials @xmath106 and some @xmath48 neighborhood @xmath59 on @xmath45 and since , it is _",
    "poly - evolvable _ , this means there exists two polynomially bounded polynomials @xmath81 and @xmath82 , such that @xmath29 is _ tolerance evolvable _ by @xmath107 and @xmath59 over @xmath66 .",
    "this means there exists some evolutionary sequence @xmath108 , which satisfies equation @xmath109 , such that after @xmath36 evolutionary steps , where @xmath110 , the final hypothesis @xmath38 satisfies @xmath111 , provided the tolerance values for each representation , @xmath78 are generated by some turing machine @xmath87 , on inputs @xmath79 and @xmath43 .",
    "the evolution algorithm runs in many stages and is fed with labelled examples from the distribution , @xmath66 .",
    "say , for some stage",
    "@xmath112 , let us have a set @xmath113 of labelled examples , such that its cardinality , @xmath114 is bounded by the polynomial @xmath84 . in this case , our target concept is the ideal function , @xmath0 .",
    "we have @xmath115 , where @xmath116 and the algorithm returns a hypothesis , @xmath33 in each stage , @xmath112 under polynomial time as per our assumptions.now , the empirical performance of all the possible hypotheses for the current stage is computed in polynomial time and the best one is made the new genome , so that it has the capacity to evolve .",
    "thus , the new hypothesis is generated in polynomial amount of time , using polynomially bounded resources and polynomially bounded input parameters.but , the computation of the performance value for the current hypothesis , is nothing but measuring the probability that the hypothesis matches the target concept on some example point from the distribution @xmath66 .",
    "so , for a stage @xmath112 , input domain @xmath6 and distribution @xmath8 , we have @xmath117 $ ] .",
    "moreover , since the final hypothesis , @xmath38 has a performance of at least @xmath80 , where @xmath110 , this means we have @xmath118\\geq 1-\\epsilon$ ] , which tells us that the error of the final evolved genome is at most @xmath43 , i.e @xmath119 .",
    "all of these statements completely agree with the basic definition of the pac learning model , provided above and thus , it can be said that @xmath29 is efficiently pac learnable by the hypothesis class , @xmath45 over the distribution @xmath66 .",
    "_ proposition 3.2 .",
    "_  if there exists a concept class @xmath29 , such that it is _ r - evolvable _ over distribution @xmath66 , then it is also efficiently learnable using the statistical query model using @xmath45 over @xmath66._proof . _  in the statistical query learning model , the pac oracle , @xmath104 , which gives random examples of the target concept,@xmath0 with respect to an input distribution @xmath66 over the domain , @xmath97 is replaced by a weaker oracle , say @xmath120 .",
    "the new oracle , @xmath120 does not provide the learning algorithm , with individual random examples , but provides accurate probability estimates within arbitrary inverse polynomial additive error , over the sample space generated by the pac oracle @xmath104.say , we have a query @xmath121 , where @xmath122 is any boolean function over inputs @xmath123 , such that @xmath17 is drawn from @xmath66 and @xmath124 , the oracle @xmath120 will return a probability estimate that @xmath125 , which is accurate within additive error @xmath126$].hence , it can be said that the statistical query learning model is a weaker form of learning than valiant s pac model .",
    "the diagram below shows a block representation of statistical query learning , such that it acts as a messenger between the pac oracle and the learning algorithm , which have no direct contacts between them .",
    "let @xmath57 be a variant of the current representation @xmath127 , such that @xmath54 and we want to measure the chances of @xmath57 being the next hypothesis of @xmath10 , which is denoted by @xmath128 $ ] .",
    "so , we need pairs @xmath129 from the set @xmath69 . in other words",
    ", we need to find the probability that for certain pairs @xmath129 , the empirical performance for the samples , whose size is upper bounded by @xmath24 , follows @xmath130 .",
    "let there be four events , whose corresponding queries are denoted by @xmath131 and additive errors by @xmath132 , for all @xmath133 $ ] .",
    "if the ideal function is @xmath0 , the four events are @xmath134 and @xmath135 respectively for @xmath136 .",
    "each query , @xmath131 is a request for the probability of event @xmath112 on the distribution generated by the pac oracle , @xmath137 .",
    "hence , a query @xmath138 is interpreted as a request for the value @xmath139=pr_{ex(f , d)}[q_{i}=1]$ ] and since , @xmath132 is the additive error of the probability estimates , so the statistical query oracle , @xmath140 actually returns @xmath141 , where we have @xmath142 . since",
    ", each @xmath132 is bounded by inverse polynomial , so we have @xmath143 , for some polynomial @xmath144 .",
    "now , let us write the equations below , for the four events to give a clear picture .",
    "d)}[r(x)=f(x)\\ and\\ r_{c}(x)=f(x)]\\ ] ] @xmath146=pr_{ex(f , d)}[r(x)=f(x)\\ and\\ r_{c}(x)\\not = f(x)]\\ ] ] @xmath147=pr_{ex(f , d)}[r(x)\\not = f(x)\\ and\\   r_{c}(x)=f(x)]\\ ] ] @xmath148=pr_{ex(f , d)}[r(x)\\not = f(x)\\ and\\ r_{c}(x)\\not = f(x)]\\ ] ]    again for each additive error , @xmath132 , such that @xmath143 , we have the following equations below .",
    "@xmath149 @xmath150 @xmath151 @xmath152    hence , we can say that the statistical query oracle generates probabilities as its output over the random sample space of the pac oracle . it is very easy to simulate the oracle , @xmath140 on some query , say @xmath138 , with probability at least @xmath80 .",
    "a sufficient number of random labeled examples  @xmath153 , polynomial in @xmath1 and @xmath50 can be drawn from @xmath137 , such that we only use the fraction of examples for which @xmath154 as the estimate , @xmath141 of @xmath155.(note that we always assume that the number of calls to the pac oracle is bounded by the polynomial @xmath144 and each @xmath131 is evaluatable in polynomial time , so that the efficiency is maintained ) .",
    "thus , if the learning algorithm is given access to @xmath140 , it can be easily simulated given access to the pac oracle , @xmath137.so , we can say that , the statistical query oracle is some sort of a midman between the learning algorithm and the pac oracle , such that the learning algorithm has access to the statistical query oracle , which in turn uses the pac oracle but there does not exist any direct contact between the algorithm and the pac oracle.thus , the proof of the proposition is complete . follow the diagram below to have an intuitive understanding of the mechanism discussed above  ( all the @xmath156s are replaced by @xmath155 for simplicity ) .",
    "so , uptil now , we discussed whether a class is learnable or not , provided it is evolvable .",
    "but , what happens if we say the converse , i.e whether a class is evolvable or not , provided that it is learnable ?",
    "well , it is for sure , by simple intuition that if a class is not learnable , it ca nt be evolvable , which we will see in the following two propositions , with respect to the classes of parity functions and boolean threshold functions._proposition 3.3 . _  if @xmath157 is the class of all parity concepts over @xmath1 boolean variables , then there does not exist any polynomial time efficient statistical query learning algorithm for the class @xmath158 , where @xmath159 .",
    "_ proof_.  the proof has been analyzed from [ 3 ] .",
    "consider a query , @xmath160 , which takes a variable @xmath161 and @xmath162 as it input , to give a boolean output .",
    "( note that @xmath0 here denotes the parity concept and not the ideal function in the model of evolution )  let us have our target distribuion , @xmath66 , uniform over @xmath163 and represent the probability of the query that it is equal to 1 , on an input generated by the pac oracle @xmath137 , such that @xmath0 is drawn randomly from @xmath157 by the variable , @xmath164 .",
    "so , we have @xmath165 $ ] . as we are considering uniform distribution over @xmath163 , it is easy to see that we have @xmath166",
    ". hence , we have the following equation below if we take expectations on both sides of the previous equation . @xmath167=\\frac{1}{2^{n } } e[\\sum_{x\\in \\{0,1\\}^{n}}q(x , f(x))]\\ ] ] using the additive property of expectations  @xmath168=\\sum e[k])$ ] , we have the following equation below .",
    "@xmath167=\\frac{1}{2^{n}}\\sum_{x\\in \\{0,1\\}^{n}}e[q(x , f(x))]\\ ] ] let the cardinality of the set , say @xmath169 consisting of all @xmath17 , such that @xmath170 is always zero ( independent of @xmath171s value ) be equal to @xmath172 and that of the set , say @xmath173 consisting of all @xmath17 , such that @xmath170 is always 1 ( ( independent of @xmath171s value))be equal to @xmath174.this means , when we have some @xmath175 , then @xmath176 and when we have some @xmath177 , then @xmath178=1 $ ] .",
    "let @xmath179 be the the cardinality of the set , say @xmath180 , where @xmath180 contains all such @xmath17 , for which we have @xmath181 .",
    "thus , we get @xmath167=\\frac{1}{2^{n}}\\left(c_{1}+\\frac{1}{2}c_{2}\\right)\\ ] ] again , we have @xmath182=\\frac{1}{2^{2n}}\\sum_{x , y\\in \\{0,1\\}^{n}}e[q(x , f(x))q(y , f(y))]\\ ] ] intuitively it can be written as , @xmath182=\\frac{1}{2^{2n}}\\sum_{x\\in s_{0},y\\in \\{0,1\\}^{n}}e[q(x , f(x))q(y , f(y ) ) ] + \\ \\frac{1}{2^{2n}}\\sum_{x\\in",
    "s_{1},y\\in \\{0,1\\}^{n}}e[q(x , f(x))q(y , f(y))]\\ + \\ ] ] @xmath183\\ ] ] after analyzing the results for each set , we finally get @xmath182=\\frac{1}{2^{2n}}(c_{1}(c_{1}+1/2\\ c_{2})+c_{2}(1/2 + 1/2\\ c_{1}+1/4\\ ( c_{2}-1)))\\ ] ] again , squaring both the sides of the equation for @xmath184 $ ] , we get @xmath167^{2}=\\frac{1}{2^{2n}}\\left(c_{1}+\\frac{1}{2}c_{2}\\right)^{2}\\ ] ] hence , the variance can be calculated as , @xmath185=e[p_{q}^{2}]-e[p_{q}]^{2}=\\frac{c_{2}}{2^{2n+2}}\\leq \\frac{1}{2^{n+2}}\\ ] ] hence , for any query , @xmath170 , the variance of the quantity , @xmath164 is exponentially small with respect to the random draw of the target concept and thus , it can be proved by contradiction , by taking the parameter @xmath43 to be any constant lesser than 1/4 , fixing the parameter for @xmath186 and then showing that a randomly chosen parity concept will be consistent with the query responses received by the learning algorithm , say @xmath98 , by using chebyshev s inequality and the bound for variance , calculated above .  hence , the error of @xmath98 s hypothesis must be large with respect to the random draw of the target concept , as many parity concepts are consistent with the responses received by @xmath98 and this , proves that the class of parity concepts is not efficiently sq learnable .",
    "in this section , we will show that the class of monotone conjunctions is evolvable , under some conditions , unlike that of the parity class and the class of boolean threshold functions._theorem_.  the class of monotone conjunctions is provably evolvable over the uniform distribution for their natural representations._proof_.  let us have a total of @xmath1 boolean variables , say @xmath2 .",
    "a monotone @xmath187 conjunction is a conjunction of at most @xmath36 non - negated literals over the @xmath1 boolean variables .",
    "if we want to apply our discussed evolution model to this case , it is very important that we have proper notion of neighborhood @xmath59 , value of tolerance , @xmath77 and the sample sizes , before we evaluate the mutator random variable at each step .",
    "we take our hypothesis class ( i.e the representation class ) @xmath45 to be the class of all monotone @xmath187 conjunctions , such that we want an accuracy of @xmath43 on the evolution of this class.for some positive constant @xmath188 , fix the value of @xmath36 to be equal to @xmath189 .",
    "( we will see later , why we assumed this value)let for any representation @xmath127 , we have a set @xmath190 , which contains all the sets of conjunctions of literals in @xmath10",
    ". then we have to either remove some literal from @xmath10 or add something to it or maybe add and then remove something to get a better hypothesis ( representation ) , which has performance better than @xmath10 and gets one step closer to realizing the ideal function , @xmath0 .",
    "so , it is logical to assume that the neighborhood @xmath59 , which is represented by the notation , @xmath191 , encompasses all these cases and thus , we get some better variant of @xmath10 from this set .",
    "let us have the following sets below.@xmath192  -  sets of all the conjunctions which consist all the literals of @xmath10 , with one extra literal added @xmath193  -  sets of all the conjunctions which consist all the literals of @xmath10 , with one literal removed @xmath194  -  sets of all the conjunctions which consist all the literals of @xmath10 , with one extra literal added and then another literal removedso , we have @xmath195 .",
    "thus , either we can stay at @xmath10 and add or remove a literal to @xmath10 and then output a variant from them ( this has a probability of 1/2 )  or we can add a literal to @xmath10 and then remove another literal from it , to get a better variant  ( this also has a probability of 1/2 ) .",
    "hence , from above , clearly @xmath59 is an @xmath196 neighborhood of @xmath10 .",
    "now , we have to discuss about the tolerance , sample size and performance issues of this construction.(note that we will represent the tolerance , @xmath197 and the number of samples , @xmath84 by simply @xmath77 and @xmath24 respectively . )",
    "let us take @xmath198 and @xmath199 .  since",
    ", we have taken @xmath200 , so we get @xmath201 .",
    "the construction is done in such a way that every mutation in the neighborhood @xmath59 will cause a performance improvement of at least @xmath202 or causes no improvement .",
    "the test , which has been devised identifies the right mutation except with some small exponential probability ( which we get by hoeffding bound ) from the one which gives no improvement in performance.recall the theorem of the hoeffding bound .",
    "it says that the probability that the mean of @xmath24 independent random variables , with each taking values in the range @xmath203 $ ] , is greater than or less than the mean of their expectations by more than @xmath105 is at most equal to @xmath204 . in our case",
    ", we have @xmath205 and @xmath206 , which we have seen before . again , since the variable @xmath105 captures deviations , so it must be equal to the tolerance , @xmath77 .",
    "so , we have @xmath207 and hence , the number of trials , which is nothing but the sample size , @xmath24 is equal to @xmath208 .",
    "so , by hoeffding bound , the probability that all the @xmath24 mutation trials each with an expected improvement of @xmath209 will produce a mean improvement of less than @xmath210 is at most equal to @xmath211 , which is indeed very small .",
    "now , if we take @xmath86 to be the number of stages ( equivalent to saying number of generations ) and in each stage , we have a total of @xmath44 variants , which are to be tested , so if we calculate @xmath212,we see that it is strictly less than @xmath213 if we fix some @xmath214 properly , which is indeed what we want.now , finally we prove a set of claims about the testing and performance by modifying and manipulating the current representation , @xmath10 as a whole and try to give combined short proofs of them , according to [ 1 ] .",
    "we say , that @xmath10 is a monotone conjunction of @xmath215 literals , where @xmath216 .",
    "consider @xmath217 , where each @xmath218 .",
    "let @xmath98 be the set of @xmath219 literals , which form the conjunction of the ideal representation , @xmath0 and @xmath220 be the set of @xmath215 literals , which form the conjunction of the current hypothesis @xmath10 .",
    "so , let us have a set of cases and try to prove them .",
    "_ case  1_.  if we have @xmath221 , i.e the number of literals in the current hypothesis is strictly less than that of the ideal conjunction , then intuitively we can see that if we add a literal from the true function to the hypothesis , then our performance will increase and it is indeed so , mathematically .",
    "so , consider a literal , @xmath222 in the set @xmath223 , which will be added to the present hypothesis , @xmath217 .",
    "hence , adding @xmath222 to @xmath10 will change the value of the hypothesis from + 1 to -1 on the points which satisfy the conjunction , @xmath224 and thus , @xmath225 , which makes @xmath226 and so , the ideal function must be equal to -1 on these points as we have @xmath226 and @xmath227 .  as , we know that over the uniform distribution , @xmath228 the probability that a conjunction of @xmath40 literals will be satisfied is equal to @xmath229  ( and hence for a disjunction to be satisfied , it is equal to @xmath230 ) , so in our case , the @xmath231 points in @xmath10 will have a probability of @xmath232 .",
    "thus , performance increase will be at least equal to @xmath233 .",
    "( notice here that the value of the hypothesis is equal to the target concept , i.e @xmath234 and hence , the performance is bound to increase)._case  2_.  consider a literal , @xmath222 in @xmath235 , which will be removed from @xmath10 .  now , since the literal is also present in the ideal function , so it will surely decrease the performance .",
    "let us take @xmath222 to be equal to @xmath236 , without loss of generality.hence , the hypothesis changes the value from -1 to + 1 , such that it satisfies the conjunction @xmath237 and thus , @xmath226 , which makes value of the ideal function to be equal to -1 .",
    "hence , the probability of the points in the hypothesis is equal to @xmath238 and , since @xmath239 , the performance value decreases by at least @xmath233 .",
    "_ case  3_.  consider two literals , @xmath240 and @xmath241 , such that we add @xmath242 to @xmath10 and then remove @xmath243 from @xmath10 .",
    "now , from the previous two cases , we see that adding @xmath242 to @xmath10 will change the hypothesis from incorrect to correct and has a probability of @xmath244 and removing of @xmath243 , also applies with a probability of @xmath244 and the change in some of the points is from correct to incorrect .",
    "now , we have to show that the net change is not neutral , but positive for the evolution performance , which is very easy .",
    "consider the set of literals , @xmath245 , such that they are missing from @xmath10 , but are present in @xmath0 ( of course , other than @xmath242 ) and we have @xmath246 . now , if we consider that @xmath243 was removed , so we have a satisfying conjunction of @xmath247 and it is also known that @xmath17 fraction of these have a value of 1 in the ideal function , with a probability equal to @xmath248 .",
    "thus , the improvement in performance is at least equal to @xmath249._case  4_.  let us have two literals , @xmath250 and @xmath251 , so that @xmath242 is added to @xmath10 and @xmath243 is removed from @xmath10 .",
    "if we look carefully into this , we can see that it is exactly the opposite of the previous case .",
    "now , removing @xmath243 is an incorrect change at every point , as it belongs to both the sets @xmath98 and @xmath220 and occurs with probability , @xmath244 .",
    "consider the set of literals , @xmath245 such that they are absent from the ideal function , @xmath0 and let @xmath246 .",
    "then we can find that on the domain of points satisfying , @xmath252 , @xmath17 fraction of them have a correct value of 1 on the ideal function , with a probability of @xmath248 , which decreases the performance measure , by at least @xmath249._case  5_.  analyzing the above four cases , we find that the performance remains constant when we combine the addition of a literal in @xmath223 to @xmath10 from case 3 with the removal of a literal in @xmath235 from @xmath10 and the same thing happens when we combine the addition part of case 4 with the removal part of case 3 .",
    "consider two literals @xmath240 and @xmath251 .",
    "from these two , it is clear that @xmath242 belongs to the ideal function , but does not belong to the current hypothesis .",
    "again , @xmath243 belongs both to the target function and the current hypothesis . as we analyzed in the previous cases , the addition of @xmath242 to @xmath10 is a correct change at every point and the probability is equal to @xmath244 .",
    "also , the removal of @xmath243 from @xmath10 is an incorrect change at every point and has an equal probability of @xmath244 .",
    "so , the effect of two changes gets cancelled .",
    "the proof of the second case is similar._case  6_.  if the current representation , @xmath10 contains all the literals that are already present in the ideal conjunction , @xmath0 and it is logical , that if we remove all the irrelevant variables in @xmath10 , which are not present in the target , it will increase the performance measure.(_it seems that this should be the final step of evolution , in which it has been able to learn the target genome and now is ready to get rid of all the unworthy life experiences and mutate to an advanced life form .",
    "it is indeed the case , which we will see later_)without loss of generality , assume @xmath253 .",
    "now , removing @xmath242 from @xmath10 , will change the hypothesis s value from -1 to + 1 on the points satisfying @xmath254 . again , since @xmath10 contains all the literals in the target and all such points have a true value of + 1 with a probability of at least @xmath255 , hence the performance must increase by at least @xmath256._case  7_.  have a literal , @xmath242 in the set , @xmath257 , which is to be added to the current representation , @xmath10 , which already contains all the literals of @xmath0 .  therefore , comparing with the previous cases , the value of the hypothesis changes from + 1 to -1 on the points satisfying @xmath252 . similar to previous case , if @xmath10 contains all the literals in @xmath0 , then all such points must have true values of + 1 .",
    "hence , we got a disagreement between the hypothesis and the target concept , and so the performance must decrease by @xmath238.(_if we consider the intuitive thinking of this case , it is like , that the evolution mechanism is already prepared to mutate and getting rid of the unwanted variables , but some extraneous agent is trying to stuff an unwanted life experience to prevent the process of mutation , by decreasing its performance measure_)._case  8_.  what happens when we have @xmath258 , i.e the number of literals in the ideal function is greater than the number of literals in the initial representaton , of the whole evolution process ?",
    "well , then the fraction of points on which the prediction of -1 is there , increases by @xmath259 .",
    "now , if we have @xmath260 , then the fraction increases by at least @xmath261 and a literal is added .",
    "the fraction may also decrease by at least @xmath261 , with the removal of one literal if we have @xmath262 .",
    "hence , for the case when we have @xmath258 , the corresponding increase or decrease in the fraction of points on which the prediction is correct is at least @xmath263 .",
    "thus , according to the above mentioned rules , the evolution mechanism functions and tries to have the best possible representation , closest to the ideal function .",
    "so , it only chooses the cases , where there seems to be an increase of performance by adding or deleting literals to or from the conjunction of the current representation .",
    "now , it can be possible that some non - ideal literals may have been added to the representaion , without decreasing the performance , but if the ideal conjunction has a total of say , @xmath264 literals , then by the coupon collector s problem , after a total of @xmath265 generations , all @xmath264 would have been swapped or added in , thus giving the final genome , except with a probability of @xmath43 .",
    "thus , [ 1 ] says that if the initial number of literals in the representation , @xmath266 and the number of literals in the ideal conjunction is at most equal to @xmath36 , then the evolution mechanism gets completed in the above mentioned number of stages ( or generations ) , except with a probability of @xmath43 , where @xmath43 is the error parameter due to various factors , hindering evolution .",
    "again , if we have @xmath267 , then the removal of any literal from the hypothesis will change the value on at most , @xmath268 of the distribution and hence , the performance can decrease at the most by @xmath269 .",
    "thus , if we set the tolerance , @xmath77 to @xmath270 and hence , we have @xmath271 for the hoeffding bound analysis , for checking the probability when we decrease the number of literals from the hypothesis , we actually have a neutral condition of mutation .",
    "but , putting @xmath272 and @xmath273 , the probability is at most @xmath274 , which is very less .",
    "thus , after running the process for @xmath275 stages , we will be able to reach to a smaller conjunction of length @xmath36 , except with an exponentially small probability .",
    "we can also show optimality for the 8th case , by similar manner.by duality principle , we can convert a conjunction to a disjunction , by the laws of negation and applying de - morgan s laws , we can also prove evolvability for the class of disjunctions .",
    "i discuss some open problems and further suggestions to extend the model of evolution of [ 1 ] . though [ 1 ] is undoubtedly one of the very few best papers in this area of evolution and theoretical computer science , but according to me , there is still much to be done in this new field of study .",
    "let us discuss some of them below.*1*.  we have some forceful assumptions in the definitions of evolvability , such that it falls in the framework of pac learning model .",
    "though , we can say that a genome is evolvable only when its performance is close to 1 , i.e it , almost is same to the target function ( target genome ) and so can assume that the final performance is equal to @xmath80 , for some small @xmath276 , but is it necessary to bound the resources and time of computing the variants by polynomials .",
    "how can we be so sure , that everything is polynomially bounded ?",
    "we are basically assuming beforehand that evolutionary mechanism is a pac learning model in the definitions , that we define .",
    "maybe , we could have assumed that the final performance is some @xmath277 and then established the pac learnability of evolution . *",
    "2*.  maybe , there exists an algorithm with nature which efficiently searches all the exponentially possible many variants of the current genome .",
    "maybe , there are @xmath278 many representations , for some positive constant @xmath188 , but somehow the evolutionary mechanism knows beforehand which representation to pick and then may apply binary search , so that the search completes in @xmath275 time , which is very efficient",
    ".  we should always understand that evolution mechanism is itself learning for millions of years and through its experience , it may have some information of which representation to select ( or there may exist some probability distribution over the variants and there may exist many variants which are capable of evolution in the probability space ) for the next generation and hence , can perform a binary search over all the variants .",
    "the idea of discarding most of the variants seems to be too much of an assumption.*3*.  it may be possible that evolution has some efficient algorithm to search all the exponential variants in efficient polynomial time and finding the algorithm , will be one of the the first stepping stones towards the great p - np problem from biological evolution point of view",
    ".  it may be probable that nature has some algorithm , which outputs the evolved genome without even looking at all the variants .",
    "( here where , statistical query learning may have great importance to formulate a better theory of evolution , by generating a favorable probability space over the variants ) .",
    "it is the right time , that computer science theorists and mathematicians look more deeply into darwinian theories , to some how come close to one of the greatest problems ever posed by the humans.*4*.  does there exist any distribution free evolution model , is a very general and broad question and it may take quite a long time to get a satisfactory answer to this question.[1 ] * 5*.  finally , it is unknown whether we can pac learn the class of dnf using dnf hypothesis and find a consistent hypothesis finder in polynomial time .",
    "so , there is some possibility that we learn the evolvability of the dnf expressions , where we start our initial representation of the genome with a dnf and the target concept is also a dnf .  since",
    ", dnfs are extremely rich in their representations and can entail lots of valuable information , so it is possible that evolution mechanism uses this class of expressions and still evolves into a stronger individual within a finite number of generations .",
    "hence , evolvability with respect to the class of dnfs , will be an open problem for a great amount of time , according to me ."
  ],
  "abstract_text": [
    "<S> darwin s theory of evolution is considered to be one of the greatest scientific gems in modern science . </S>",
    "<S> it not only gives us a description of how living things evolve , but also shows how a population evolves through time and also , why only the fittest individuals continue the generation forward . </S>",
    "<S> the paper basically gives a high level analysis of the works of valiant[1 ] . </S>",
    "<S> though , we know the mechanisms of evolution , but it seems that there does not exist any strong quantitative and mathematical theory of the evolution of certain mechanisms . </S>",
    "<S> what is defined exactly as the fitness of an individual , why is that only certain individuals in a population tend to mutate , how computation is done in finite time when we have exponentially many examples : there seems to be a lot of questions which need to be answered . [ 1 ] basically treats darwinian theory as a form of computational learning theory , which calculates the net fitness of the hypotheses and thus distinguishes functions and their classes which could be evolvable using polynomial amount of resources . </S>",
    "<S> evolution is considered as a function of the environment and the previous evolutionary stages that chooses the best hypothesis using learning techniques that makes mutation possible and hence , gives a quantitative idea that why only the fittest individuals tend to survive and have the power to mutate . </S>"
  ]
}