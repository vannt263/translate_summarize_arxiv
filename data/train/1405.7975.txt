{
  "article_text": [
    "multi - document summarization ( mds ) aims to filter the most important information from a set of documents to generate a compressed summary .",
    "recently , the graph - based models and ranking algorithms have been extensively researched .",
    "+ while most work to date focuses on the sentence and the document level relations , in this work we present a novel 3-layered graph model that emphasizes not only the sentence and the document level relations but also the influence of the under sentence level relations . the document set d=@xmath0,@xmath1,  @xmath2 is represented as a weighted undirected frame graph g. as a difference to previous works in our graphs there is not only one kind of objects ( i.e. sentences ) ,",
    "but there are three kinds of objects : semantic role frames , sentences and documents . + even if humans do not always agree on the content to be added to a summary , they perform very well on this task",
    ". therefore our goal should be to find a way of mimicking the cognition behind the human like summarization process .",
    "for this challenge we consider using the psychology cognitive situation model , namely the event - indexing model @xcite . according to this model a human - like system",
    "should keep track of five indices while reading the document .",
    "those indices are _ protagonist _ , _ temporality _ , _ spatiality _ , _ causality _ and _ intention _ , with the given descending order of importance .",
    "one can also show that the semantic role parser s @xcite output can be mapped to the above proposed cognitive model .",
    "semantic roles are defined as the relationships between syntactic constituents and the predicates .",
    "most sentence components have semantic connections with the predicate , carrying answers to the questions such as who , what , when , where etc .",
    "+ the summarization method , we propose , works in the following way .",
    "first , the documents are given to the srl parser where the semantic arguments from each parsed sentence are extracted .",
    "based on the event - indexing model we calculate the composite similarity between all semantic frames . then we generate a semantic graph where nodes are semantic frames and edges are the composite similarity values . by using an intelligent weighting scheme",
    "we add two more layers , namely the sentence and the document layers , which yields the richer multilayered graph model with the inter and intra sentence and the documental level relations .",
    "next we use modified version of pagerank for identifying the significant edges in the graph .",
    "the next step aims to further remove redundant information in the summary by penalizing the sentences largely overlapping with other high ranked sentences .",
    "based on the text graph and the obtained rank scores , a greedy algorithm is applied to inflict the diversity penalty and compute the final rank scores of the sentences .",
    "later , we sum the pagerank scores of semantic frames , originating from the same sentence , and we use it as a score for sentence scoring .",
    "subsequently , the top scoring sentences are selected one - by - one and put into the summary .",
    "+ the remainder of this paper is organized as follows .",
    "section 2 reviews existing graph - based summarization models .",
    "section 3 and 4 introduces the proposed sentence ranking algorithm .",
    "after that , section 5 reports experiments and evaluation results . finally , section 6 concludes the paper .",
    "the graph - based models have been developed by the extractive document summarization community in the past years @xcite .",
    "conventionally , they model a document or a set of documents as a text graph composed by taking a text unit as a node and similarity between text units as edges .",
    "the significance of a node in a graph is estimated by graph - based ranking algorithms , such as pagerank @xcite or hits @xcite .",
    "sentences in document(s ) are ranked based on the computed node significance and the most salient ones are selected to form an extractive summary .",
    "an algorithm called lexrank @xcite , adapted from pagerank , was applied to calculate sentence significance , which was then used as the criterion to rank and select summary sentences .",
    "meanwhile , mihalcea and tarau @xcite presented their pagerank variation , called textrank , in the same year .",
    "in this section we present our novel graph model , which will be used in frame ranking algorithm presented in the next section .",
    "let a set of documents d be a text similarity graph @xmath3 , where @xmath4 and @xmath5 represent the frame , sentence and document vertex set , respectively . @xmath6 and @xmath7 are frame , sentence and document edge set . @xmath8 and @xmath9 are three functions defined to label frame , sentence and document vertices , while @xmath10 and @xmath11 are functions for labeling frame , sentence and document edges .",
    "+ adding new layers to the classical sentence similarity graph yields the richer multilayered graph model with the inter and intra sentence and the documental level relations that can be then used to enhance the existing pagerank algorithm .",
    "figure  [ mds system ] . shows the conventional text graph model before and after applying the concepts of multilayered graph representation .",
    "one can easily show that the new graph model brings to light the following previously ignored information : sentence to sentence similarity can now be distinguished in two groups , one within a document and one across two documents ; the document s significance can influence the sentence ratings ; there is also a completely new kind of objects ( i.e. semantic role labeler ( srl ) frames ) involved in representing the inner sentence relations .         + the sentence edge function @xmath12 and document edge function @xmath13 are formulated as the normalized similarity between the two sentences @xmath14 and @xmath15 , and the two documents @xmath16 and @xmath17 , respectively .",
    "the srl frame edge function @xmath18 , is formulated as the composite similarity function of two frames @xmath19 and @xmath20 .",
    "let n be the total number of frames in a documents set .",
    "the frame vertex function @xmath21 assigns to frame vertices the value of @xmath22 or @xmath23 , depending on their completeness , where incomplete frames have lower weight . the sentence @xmath24 and document vertex @xmath25 functions are defined by the normalized centroid - based weight of the sentence and document , respectively where @xmath26 denotes the centroid @xcite weight of the word u. + our goal is to capture the similarity and redundancy between sentences , but at a lower structural and a higher semantic level . to accomplish this",
    ", we use the event - indexing model as the base for calculations of semantic similarity between frames of semantic role parser outputs , namely frames .",
    "let us define the similarity measure for protagonist @xmath27 ; temporality @xmath28 ; spatiality @xmath29 and causality + @xmath30 . in order to have the flexible weighting scheme we use coefficients @xmath31 .",
    "the compose similarity is defined as : @xmath32 where @xmath33 the values for coefficients are chosen based on the cognitive model which gives an emphasis in the decreasing order to indices .",
    "in previous section the idea of multilayered text similarity graph is presented , based on it in this section we present a modified iterative graph - based sentence ranking algorithm .",
    "+ our algorithm is extended from those existing pagerank - like algorithms reported in the literature that calculate the graph only in the sentence level @xcite , or sentence and document level @xcite .",
    "+ in the summary , pagerank method ( in matrix notation ) as described in the original paper @xcite is    @xmath34    where * h * is a very sparse , raw sub stochastic hyperlink matrix , @xmath35 is a scaling parameter between 0 and 1 , @xmath36 is the stationary row vector of * h * called the pagerank vector , * * v**@xmath37 is a complete dense , rank - one teleportation matrix and a is a binary dangling node vector . in terms of the sentence ranking the matrix *",
    "h * is an adjacency matrix of similarities , * v@xmath37 * is the affinity vector and the resulting @xmath36 is the frame ranking vector . + for sake of simplicity , we just assume there are two documents ( e.g.@xmath38 ) and 4 sentences ( e.g.@xmath39 in @xmath40 and @xmath41 in @xmath42 ) involved in ranking .",
    "yet there are eight frames extracted from four sentences , let us show the document , sentence ( just the first one ) and frames ( again just the first one ) similarity matrices : + @xmath43 @xmath44 @xmath45 + @xmath46 @xmath47 +   + the block matrix @xmath48 refers to the similarity matrix of the sentences in document @xmath40 , while @xmath49 represents the fold - document ( @xmath40 and @xmath42 ) similarity matrix , and so on and so forth . similarly the block matrix @xmath50 in @xmath48 denotes the similarity matrix of the frames in sentence @xmath51 , while @xmath52 denotes the fold - sentence ( @xmath51 and @xmath53 ) affinity matrix .",
    "@xmath54 corresponds to the original sentence similarity matrix used in generic graph methods .",
    "the effective way of integrating the document and sentence dimension into the @xmath54 is to highlight the document and sentence influences on the sentence and frame edges that connect different documents and sentences as illustrated in formulas for @xmath55 and @xmath56 .",
    "the weight matrix @xmath57 is used to distinguish the cross - document sentence edges and the weight matrix @xmath58 is used to distinguish the cross - sentence frame edges .",
    "the diagonal elements in @xmath59 and @xmath60 are set to 1 to neutralize the influence of the intra - document sentence and intra - sentence frame edges . just on the opposite ,",
    "the non - diagonal elements are weighted by the connections between the two corresponding documents and sentences .",
    "we define @xmath59 as @xmath61 , where @xmath62 presents the document that contains the sentence @xmath14 .",
    "we also define @xmath60 as @xmath63 , and @xmath64 presents the sentence that contains the frame @xmath19 .",
    "+ based on assumption that a frame from the document and the sentence with higher significance should be ranked higher we reflect the influence of the document and sentence dimensions on the affinity vector @xmath65 .",
    "consequently , the centroid - based weight of the document and the sentence are taken as the weights on the affinity vector @xmath65 .",
    "see the following preference vectors : + @xmath66^t$ ] ; @xmath67\\cdot\\begin{bmatrix } w_{d_1 } & \\\\ & w_{d_2 } \\end{bmatrix}\\bigg)^t$ ] ; + @xmath68\\cdot\\begin{bmatrix } w_{s_1 } & \\\\ & w_{s_2 } \\end{bmatrix}\\bigg)^t$ ] ; @xmath69\\cdot\\begin{bmatrix } w_{s_3 } & \\\\ & w_{s_4 } \\end{bmatrix}\\bigg)^t$ ] ; + here @xmath70 represents the original preference vector , as used in lexrank , @xmath71 to @xmath72 denote the sub - preference vector of the frames from sentences @xmath73 to @xmath74 , respectively ; and @xmath75 denote the sub - preference vector of the sentences from documents @xmath0 and @xmath1 , respectively .",
    "the weight matrices @xmath76 , @xmath77 and @xmath78 are specified to introduce the bias to sentences from different documents and the bias towards frames from different sentences , respectively .",
    "the weighting functions are defined as @xmath79 , @xmath80 . to ensure the solution of proposed algorithm",
    ", we should first make @xmath65 a affinity probability vector .",
    "[ lema1 ] @xmath65 is a probability vector , if @xmath81 , @xmath82 and @xmath83 are positive and the diagonal elements in them sum to 1 ;    since @xmath71 , @xmath84 , @xmath85 and @xmath72 are probability vectors of frames from four sentences we have @xmath86 .",
    "then , @xmath87 @xmath88    afterward , we should make the matrix h column stochastic and irreducible by forcing each of four block matrices of sentences and two matrices of documents to be column stochastic simply by normalizing them by columns . to make h irreducible",
    ", we make the sixteen block matrices in h irreducible by adding additional links between any two frames , which is also adapted in pagerank .",
    "[ lema2 ] h is column stochastic and irreducible .",
    "h is column stochastic since the weight matrix w is column stochastic .",
    "let a , b , c , and d , donate any of the 4 column block in h , then @xmath89 @xmath88 since the four graphs corresponding to the four diagonal block matrices in h are strongly connected ( i.e. they are irreducible ) and the edges connecting the four graphs are bidirectional , the graph corresponding to h is obviously strongly connected .",
    "thus h must be also irreducible .",
    "notice that we must ensure @xmath90 and make the sum of the diagonal elements equal to 1 in order to ensure @xmath65 to be a probability vector .",
    "and we must make h column stochastic by setting @xmath91 and both matrices column stochastic .",
    "finally , we obtain that h is stochastic , irreducible and primitive , hence we can compute the unique dominant vector ( with 1 as the eigenvalue ) of h by using the power iteration method applied to h which converges to @xmath92 .",
    "the previous explanation is given as example with a two - document , a four sentences ( two of them in every document ) , and eight frames ( two of frames in every single sentence ) .",
    "however , we can come to the same conclusion when the number of the documents , sentences and frames involved extends from given values to arbitrary number .",
    "+ we can finally summarize the new ranking algorithm with following two functions .",
    "@xmath93 so far the document and sentence dimensions have been integrated into the pagerank - like algorithms for frame ranking with a solid mathematical foundation .",
    "the duc 2004 data set from duc was tested to analyze the efficiency of the proposed summarization method .",
    "the task 2 at the duc 2004 is to generate a short summary ( 665 bytes ) of an input set of topic - related news articles .",
    "l r systems & rouge-1 ( 95% confidence interval ) + [ 0.5ex ] avg . of human assessors & 0.403 [ 0.383,0.424 ] +   + best machine ( sysid = 65 ) & 0.382 [ 0.369,0.395 ] + median machine ( sysid = 138 ) & 0.343 [ 0.328,0.358 ] + worst machine ( sysid = 111 ) & 0.242 [ 0.230,0.253 ] +   + * our model * & 0.379 [ 0.361,0.389 ] + lexrank &",
    "0.369 [ 0.354,0.382 ] +   + 2 ( nist baseline ) ( rank : 25/35 ) & 0.324 [ 0.309,0.339 ] + random baseline : & 0.315 [ 0.303,0.328 ] +    [ tab : results ]    the total number of document groups is 50 , with each group containing 10 articles on average . for each group , four nist assessors were asked to create a brief summary .",
    "machine - generated summaries are evaluated using rouge @xcite automatic n - gram matching which measures performance based on the number of co - occurrences between machine - generated and ideal summaries in different word units .",
    "the 1-gram rouge score ( a.k.a.rouge-1 ) has been found to correlate very well with human judgements at a confidence level of 95% , based on various statistical metrics .",
    "even though in this version of method we did not consider sentence positions or other summary quality improvement techniques such as sentence reduction , its overall performance is promising , please see table  [ tab : results ] .",
    "the use of multilayered model in summarization can make considerable improvements even though the results presented here do not report a significant difference .",
    "we have presented a multilayered graph model and a ranking algorithm , for generic mds .",
    "the main contributions of our work is introducing the concept of 3-layered graph model .",
    "the results of applying this model on extractive summarization are quite promising .",
    "there is work still left to be done , however .",
    "we are now working on further improvements of the model , and it s adaptation to other summarization tasks , such as the query and update summarization .                          xiaojun wan .",
    "document - based hits model for multi - document summarization . in tu",
    "bao ho and zhi - hua zhou , editors , _ pricai _ , volume 5351 of _ lecture notes in computer science _ , pages 454465 .",
    "springer , 2008 ."
  ],
  "abstract_text": [
    "<S> multi - document summarization is a process of automatic generation of a compressed version of the given collection of documents . </S>",
    "<S> recently , the graph - based models and ranking algorithms have been actively investigated by the extractive document summarization community . </S>",
    "<S> while most work to date focuses on homogeneous connecteness of sentences and heterogeneous connecteness of documents and sentences ( e.g. sentence similarity weighted by document importance ) , in this paper we present a novel 3-layered graph model that emphasizes not only sentence and document level relations but also the influence of under sentence level relations ( e.g. a part of sentence similarity ) . </S>"
  ]
}