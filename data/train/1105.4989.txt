{
  "article_text": [
    "shannon s rate - distortion function ( rdf ) for a source @xmath0 and distortion measure @xmath1 is given by @xmath2 where the infimum is over all reconstructions @xmath3 such that the expected distortion satisfies @xmath4 \\leq d$ ] . even though  ( [ eq : rdf ] ) perhaps appears simple and innocent , it is well - known that it is generally very hard to explicitly compute .",
    "in fact , there exists only very few cases where  ( [ eq : rdf ] ) is known in closed - form , e.g. , gaussian sources and mse , binary sources and hamming distances etc . in the information theoretic literature , several methods have been proposed to approximate the rdf s e.g. , iterative numeric solutions , high - resolution source coding , and ( universal ) bounds . in the first case ,",
    "the arimoto - blahut algorithm is able to numerically obtain the rate - distortion function for arbitrary finite input / output alphabet sources and single - letter distortion measures  @xcite . in the second case , for continuous alphabet sources",
    ", it was shown by linder and zamir that the shannon lower bound ( slb ) is asymptotically tight for norm - based distortion metrics  @xcite .",
    "thus , at asymptotically high coding rates , the rdfs can be approximated by simple formulaes . in the third case , alternative rdfs , which are easier to compute and analyze , are used to bound the true rdfs .",
    "for example , at general resolution and for difference distortion measures , the slb provides a _ lower _ bound to the true rdf for many sources . on the other hand , zamir presented in  @xcite an additive rdf ( ardf ) , which consists of an additive test channel followed by estimation .",
    "the ardf has been shown to be a convenient tool for _ upper _ bounding the rate loss in many source coding problems .",
    "in particular , it was shown in  @xcite that the additive rate loss in the wyner - ziv problem is at most 1/2 bit for all sources .",
    "similarly , it was shown by lastras and berger in  @xcite , that the additive rate loss in the successive refinement problem is at most 1/2 bit per stage .",
    "the ardf has also been successfully applied to upper bound the rate loss in other multi - terminal problems , cf .",
    "@xcite . in the limit of high resolution , the ardf coincides with the true rdf for many sources and fidelity criterions  @xcite . in the other extreme , i.e. , in the limit of low resolutions ,",
    "the behavior of the ardf has not been rigorously addressed",
    ". there has , however , been a great interest in the counter part to low resolution source coding , i.e. , communication at low snr , e.g. , ultra - wideband communication  @xcite . a motivating factor for considering the low snr regime in communications , is that the absolute value of the slope of the capacity - cost function is large ( and therefore small for the cost - capacity function ) , which indicates that one gets the most channel capacity per unit cost at low snr , as was shown by verd  @xcite .",
    "interestingly , verd also showed that for rate - distortion at low rates , the most cost effective operating point in terms of bits per unit distortion , is near zero rate  @xcite .",
    "this follows since the absolute value of the slope of the rdf is minimized when the distortion approaches its maximum .    in this paper , we are interested in analyzing the ardf at low resolutions .",
    "we consider the special case of the ardf where the test channel s noise is gaussian and the distortion measure is the mse .",
    "we establish a link to the mutual information  minimum mean squared estimation ( i - mmse ) relation of guo et al .",
    "@xcite and use this to show that for any source the slope of the ardf near zero rate , converges to the slope of the gaussian rdf near zero rate .",
    "we then consider the multiplicative rate loss of this ardf and show that for bursty sources it may be unbounded .",
    "we also show that unconditional incremental refinement , i.e. , where each refinement is encoded independently of the other refinements , is ardf optimal in the limit of low resolution , independently of the source distribution . in particular ,",
    "let an arbitrarily distributed source @xmath0 be encoded into @xmath5 representations @xmath6 where @xmath7 are mutually independent , gaussian distributed , and independent of @xmath0 .",
    "then we show that @xmath8 at low rates .",
    "moreover , the joint reconstruction follows by simple linear estimation of @xmath0 from @xmath9 . if side information @xmath10 , where @xmath10 is independent of @xmath11 , but arbitrarily jointly distributed with @xmath0 , is available both at the encoder and decoder , we show that @xmath12 . in this case , however , the best conditional estimator @xmath13 $ ] is generally not linear .",
    "we provide the exact conditions for ardf optimality of linear estimation in the low rate regime .",
    "in this section , we present two existing important concepts that we will be needing in the sequel , i.e. , the additive rdf and the i - mmse relation .",
    "the additive ( noise ) rdf , as defined by zamir in  @xcite , describes the best rate - distortion performance achievable for any additive noise followed by optimum estimation , including the possibility of time sharing ( convexification ) . in the current paper , we restrict attention to gaussian noise , mmse estimation ( mse distortion ) , and no time - sharing , so we take the `` freedom '' to use the notation _ additive rdf _ , @xmath14 , for this special case ( i.e.  no minimization over free parameters ) .",
    "specifically , let @xmath15 denote the minimum possible mse in estimating @xmath0 from @xmath3 , i.e. , @xmath16 - x)^2].\\ ] ] moreover , let the additive noise @xmath17 be zero - mean gaussian distributed with variance @xmath18 .",
    "then , @xmath19 where the noise variance @xmath20 is chosen such that @xmath21 .      using an _ incremental _ gaussian channel , guo et al .",
    "@xcite was able to establish an explicit connection between information theory and estimation theory . for future reference",
    ", we include this result below :    [ theo : gsv ] let @xmath17 be zero - mean gaussian of unit variance , independent of @xmath0 , and let @xmath0 have an arbitrary distribution @xmath22 that satisfies @xmath23 . then @xmath24 where @xmath25 ) ^2 ] = \\mathrm{var}(x|\\sqrt{\\gamma}x+n).\\ ] ]",
    "we will show that the slope of @xmath14 at @xmath26 for a source @xmath0 with variance @xmath27 is independent of the distribution of @xmath0 .",
    "in fact , the slope is identical to the slope of the rdf of a gaussian source @xmath28 with variance @xmath29 .",
    "this is interesting since the rdf of any zero - mean source @xmath0 with a variance @xmath30 meets the gaussian rdf at @xmath31 .",
    "thus , since the gaussian rdf can be obtained by linear estimation , it follows that @xmath14 can also be obtained by linear estimation near @xmath32 .",
    "[ lem : slope_fx ] let @xmath33 where @xmath34 , @xmath0 is arbitrarily distributed with variance @xmath27 and @xmath17 is gaussian distributed according to @xmath35 .",
    "moreover , let @xmath14 be the additive rdf .",
    "then @xmath36 irrespective of the distribution on @xmath0 .",
    "interestingly , it was shown by marco and neuhoff  @xcite that in the quadratic memoryless gaussian case , the operational rate - distortion function of the scalar uniform quantizer ( followed by entropy coding ) has the same slope as  ( [ eq : slope ] ) .",
    "thus , in this particular case , the optimal scalar quantizer is as good as any vector quantizer .",
    "recall that in e.g. , the successive refinement problem , the _ additive _ rate loss is no more than 0.5 bits per stage .",
    "we will now show that the _ multiplicative _ rate loss may be unbounded .",
    "let @xmath0 be a gaussian mixture source with a density @xmath37 given by @xmath38 , where @xmath39 .",
    "the variance @xmath27 of @xmath0 is @xmath40 .",
    "the components contribution can be parametrized by @xmath41 $ ] as follows : @xmath42",
    ". it will be convenient to let @xmath43 and @xmath44 .",
    "moreover , we shall assume that @xmath45 . notice that as @xmath46 we have that @xmath47 , and @xmath48 .    at this point ,",
    "let @xmath49 with probability @xmath50 and @xmath51 with probability @xmath52 , and let @xmath53 be an indicator of the two components , i.e. , @xmath54 , if @xmath55 , and @xmath56 , if @xmath51 .",
    "the rdf , conditional on the indicator @xmath53 , is given by @xmath57 \\displaystyle\\frac{p_1}{2}\\log_2\\bigg ( \\frac{p_1\\sigma_1 ^ 2}{d - p_0\\sigma_0 ^ 2 } \\bigg ) , &   \\text{if $ \\sigma_0 ^ 2 < d < 1$}. \\end{cases}\\end{aligned}\\ ] ] thus , the slope of @xmath58 w.r.t .",
    "@xmath59 is given by @xmath60 which tends to zero as @xmath61 and @xmath62 .",
    "it follows from this fact and from lemma  [ lem : slope_fx ] that the ratio of the slope of the conditional rdf and the slope of the ardf grows unboundedly as @xmath63 .",
    "moreover , as @xmath61 , @xmath64 , which implies that it becomes increasingly easier for the uninformed encoder / decoder to guess the correct component of the source .",
    "thus , the conditional rdf converges towards the true rdf @xmath65 , from which it follows that the ratio @xmath66 .",
    "we will now show that unconditional incremental refinement , i.e. , where each refinement is encoded independently of the other refinements , is ardf optimal in the limit of low resolution , independently of the source distribution .",
    "this result is not only of theoretical value but is also useful in practice , since conditional source coding is generally more complicated than unconditional source coding , i.e. , creating descriptions that are individually optimal and at the same time jointly optimal is a long standing problem in information theory , where it is known as the multiple descriptions problem  @xcite .",
    "[ lem : oversampling ] let @xmath0 be arbitrarily distributed with variance @xmath27 , and let @xmath67 be a sequence of zero - mean mutually independent gaussian sources each with variance @xmath68 .",
    "then @xmath69    [ lem : uncond ] let @xmath70 , where @xmath71 .",
    "moreover , let @xmath0 be arbitrarily distributed with variance @xmath27 and let @xmath72 be zero - mean unit - variance i.i.d .",
    "gaussian distributed .",
    "then @xmath73 and @xmath74 = k.\\ ] ]    to illustrate the importance of lemma  [ lem : uncond ] , let us consider the situation of a zero - mean unit - variance memoryless gaussian source @xmath0 , which is to be encoded successively in @xmath75 stages . in stage @xmath76 ,",
    "@xmath77 descriptions @xmath78 , are constructed unconditionally of each other .",
    "thus , for the same coding rate ( at each stage ) , the joint distortion @xmath79 in the @xmath76th stage is worse than if only a single joint description within each stage had been created .",
    "in fact , in the symmetric case where all individual descriptions within stage @xmath76 has the same distortion @xmath80 and rate @xmath81 , it can be shown that the joint distortion @xmath82 of the @xmath76th stage is given by @xmath83 and the sum - rate at stage @xmath76 is given by @xmath84 where @xmath85 . since the gaussian source is successively refinable , using conditional refinements will achieve the true rdf given by @xmath86 , where @xmath82 is given by  ( [ eq : dsuc ] ) . on the other hand ,",
    "the rate required when unconditional coding is used is given by  ( [ eq : rsuc ] ) . for comparison ,",
    "we have illustrated the performance of unconditional and conditional coding when the source is encoded into @xmath87 descriptions per stage , for the case of of @xmath88 and @xmath89 increments ( stages ) , respectively , see fig .",
    "[ fig : uncond1 ] . in this example , @xmath90 and @xmath91 .",
    "notice that when using smaller increments , i.e. , when @xmath89 as compared to when @xmath88 , the resulting rate loss due to using unconditional coding is significantly reduced .",
    "the case of additional side information available at the encoder and the decoder was not considered by guo et al .  in  @xcite .",
    "below we generalize theorem  [ theo : gsv ] to include side information :    [ lem : condmi ] let @xmath92 where @xmath93 and @xmath0 is arbitrarily distributed , independent of @xmath17 and of variance @xmath27 .",
    "let @xmath10 be arbitrarily distributed and correlated with @xmath0 but independent of @xmath17 .",
    "then @xmath94    let @xmath70 , where @xmath95 and @xmath96 .",
    "let @xmath0 be arbitrarily distributed with variance @xmath27 and let @xmath72 be zero - mean unit - variance i.i.d .",
    "gaussian distributed .",
    "let @xmath10 be arbitrarily distributed and correlated with @xmath0 but independent of @xmath97 .",
    "then @xmath98      it was recently shown by akyol et al .",
    "@xcite , that for an arbitrarily distributed source @xmath0 , contaminated by gaussian noise @xmath17 , the mmse estimator of @xmath0 given @xmath92 , converges in probability to a linear estimator , in the limit where @xmath99 .",
    "contrary to this result , we show that the conditional mmse estimator @xmath100 $ ] with side information @xmath10 , where @xmath10 is independent of @xmath17 but is arbitrarily correlated with @xmath0 is generally not linear .",
    "[ lem : nonlinear ] let @xmath33 where @xmath34 , @xmath0 is arbitrarily distributed with variance @xmath27 and @xmath17 is gaussian distributed according to @xmath35 .",
    "moreover , let @xmath10 be arbitrarily distributed , independent of @xmath17 but arbitrarily correlated with @xmath0 .",
    "then the conditional mmse estimator @xmath100 $ ] is linear if and only if @xmath101 = \\mathrm{var}(x|z)^2,\\ ] ] where @xmath102 \\triangleq   \\mathbb{e}_z [ ( \\mathbb{e}_x [ ( \\mathbb{e}_x[x|z = z ] - x)^2 ] ) ^2]\\ ] ] and @xmath103 - x)^2])^2.\\ ] ]    in the case where @xmath104 are jointly gaussian , it is easy to show that  ( [ eq : mmse_equiv ] ) is satisfied and , thus , the mmse estimator @xmath100 $ ] is trivially linear in both @xmath10 and @xmath17 .",
    "the authors would like to thank uri erez who initially proposed the idea of incremental refinements in the context of multiple descriptions with feedback .",
    "the additive rdf is defined parametrically as @xmath14 , by @xmath105 , which implies that @xmath106 from the derivative of a composite function , it follows that @xmath107 we know that @xmath108 can be expanded as  @xcite @xmath109\\gamma^4\\sigma_x^8 + \\mathcal{o}(\\gamma^5)\\bigg ] , \\end{split}\\ ] ] and that @xmath110 it follows from  ( [ eq : ig ] ) that @xmath111 from  @xcite , @xmath112 . moreover , since @xmath113 and since @xmath99 implies @xmath114 , we have that the slope of @xmath14 with respect to @xmath59 at @xmath115 is @xmath116    let @xmath117^t,$ ] and let @xmath118 be the dft of @xmath119 , i.e. , @xmath120 the dc term is given by @xmath121 .",
    "the other terms , i.e. , @xmath122 , are ac terms and do not contain @xmath0 ( since @xmath0 is dc ) .",
    "the ac terms are orthogonal to the dc component of the noise , i.e. , @xmath123 , and since the gaussianity of the noise implies independence , we are left with only the dc term . since the @xmath124 s are mutually independent , the resulting sum - noise component @xmath125 of the dc term has variance @xmath126 .",
    "thus , the dc term is equivalent to @xmath127 , where @xmath17 is distributed as @xmath124 .",
    "this shows that @xmath128 .",
    "the lemma is proved .    from lemma  [ lem :",
    "oversampling ] , it is clear that @xmath129 to get to the standard form with unit - variance noise , we may scale both @xmath0 and @xmath130 by @xmath131 without affecting their mutual information , i.e. , @xmath132 at this point we use that  @xcite @xmath133 where @xmath134 and @xmath135 .",
    "this proves the first part of the lemma . by using well - known linear estimation theory ,",
    "it is easy to show that @xmath136 where @xmath137 denotes the mse due to estimating @xmath0 from @xmath138 using linear estimation .",
    "we now invoke the fact that linear estimation is optimal in the limit @xmath99 and re - order the terms in  ( [ eq : jointd1 ] ) to get  ( [ eq : jointd ] )",
    ".    we will extend the proof technique used in  ( * ? ? ?",
    "* lemma 1 ) to allow for arbitrary conditional distributions . to do this , we make use of the fact @xmath139 forms a markov chain ( in that order ) , which will allow us to simplify the decomposition of their joint distribution .",
    "let @xmath140 denote expectation with respect to @xmath141 .",
    "we first expand the conditional mutual information in terms of the divergence , i.e. @xmath142,\\end{aligned}\\ ] ] where @xmath143 can be chosen arbitrary as long as @xmath144 and @xmath145 are both well - defined .",
    "let @xmath146 , 1 + \\gamma\\ , \\mathrm{var}(x|z))$ ] .",
    "the first term in  ( [ eq : divsplit ] ) is the divergence between two gaussian distributions , since @xmath147 = \\mathbb{e}[y|x ] = n$ ] is gaussian distributed and @xmath148 $ ] is gaussian since a linear combination of gaussians remain gaussian . in this case",
    "we have  @xcite @xmath149 where we used that @xmath150 .",
    "we now look at the second expression in ( [ eq : divsplit ] ) and use the markov condition to get to @xmath151 = \\mathbb{e}_{x|z } [ p_{y|x}]$ ] . with this , we may adapt the proof technique of  @xcite to obtain : @xmath152 } { \\frac{1}{\\sqrt{2\\pi \\sigma_0 ^ 2 } }   \\exp\\big(-\\frac{1}{2\\sigma_0 ^ 2}(y    - \\gamma \\mathbb{e}[x|z])^2\\big ) } \\bigg ) \\\\   & =    \\log\\bigg ( \\mathbb{e}_{x|z = z}\\bigg[\\exp\\bigg\\ { \\frac{1}{2\\sigma_0 ^ 2}(y   - \\mathbb{e}[x|z])^2 \\\\ & \\qquad   -\\frac{1}{2\\sigma_n^2}(y-\\sqrt{\\gamma}x)^2   \\bigg\\}\\bigg ]   \\bigg )   + \\frac{1}{2}\\log\\bigg(\\frac{\\sigma_0 ^ 2}{\\sigma_n^2}\\bigg ) \\\\ & =   \\log\\bigg ( \\mathbb{e}_{x|z = z}\\bigg[\\exp\\bigg\\ {   \\frac{(y - \\sqrt{\\gamma}\\,\\mathbb{e}[x|z])^2}{2(1 + \\gamma\\,\\mathrm{var}(x|z ) ) } \\\\ & \\qquad   -\\frac{(y-\\sqrt{\\gamma}x)^2}{2\\sigma_n^2 }   \\bigg\\}\\bigg ]   \\bigg )   + \\frac{1}{2}\\log\\bigg(\\frac{\\sigma_0 ^ 2}{\\sigma_n^2}\\bigg ) \\\\ & \\overset{(a)}{= } \\log\\bigg ( \\mathbb{e}_{x|z = z}\\bigg [ 1 + \\sqrt{\\gamma } y(x-\\mathbb{e}[x|z ] ) \\\\ & \\quad + \\frac{\\gamma}{2}(y^2(x-\\mathbb{e}[x|z])^2 - y^2\\mathrm{var}(x|z ) \\\\ & \\quad   -x^2 + \\mathbb{e}[x|z]^2   + o(\\gamma)\\bigg ]   \\bigg )   + \\frac{1}{2}\\log(1+\\gamma\\ , \\mathrm{var}(x|z ) ) \\\\ & =   \\log(1-\\frac{\\gamma}2\\ , \\mathrm{var}(x|z ) ) + \\frac{1}{2}\\log(1+\\gamma\\ , \\mathrm{var}(x|z ) ) + o(\\gamma ) \\\\ & = o(\\gamma),\\end{aligned}\\ ] ] where @xmath153 follows by using a series expansion of @xmath154 in terms of @xmath155 .",
    "we have thus established that the second term of  ( [ eq : divsplit ] ) goes to zero ( as a function of @xmath155 ) faster than the first term .",
    "thus , the first term dominates the conditional mutual information for small @xmath155 .",
    "this completes the proof .",
    "we first consider the unconditional case , where @xmath156 .",
    "let us assume that @xmath157 . recall that @xmath158 , where @xmath159 and @xmath160 . for small @xmath155 ,",
    "the optimal estimator is linear , and we have that @xmath161 \\approx \\mu_x + \\alpha(y - \\mu_x),\\ ] ] where @xmath162 is the wiener coefficient given by @xmath163=\\sqrt{\\gamma}\\sigma_x^2 $ ] . from  ( [ eq : mmse_g ] ) , we know that the mmse behaves as : @xmath164 on the other hand , in the conditional case with side information @xmath3 , for each @xmath165 the source has mean @xmath166 $ ] and variance @xmath167 . using this in  ( [ eq : linest ] ) , and fixing @xmath165 , leads to @xmath168 \\approx \\mathbb{e}[x|z = z ] + \\alpha_z ( y - \\mathbb{e}[x|z = z]),\\ ] ] where the wiener coefficient depends on @xmath169 , i.e. , @xmath170 . using  ( [ eq : mmse_g ] ) for a fixed @xmath165 yields @xmath171 taking the average over @xmath10 results in @xmath172,\\ ] ] where @xmath173 $ ] . by jensen s inequality",
    ", it follows that @xmath174 \\geq \\mathrm{var}(x|z)^2,\\ ] ] with equality if and only if the conditional variance @xmath167 is independent of the realization of @xmath169 .",
    "thus , comparing  ( [ eq : var_cond ] ) to  ( [ eq : var_uncond ] ) shows that the linear estimator is generally not optimal ."
  ],
  "abstract_text": [
    "<S> the additive rate - distortion function ( ardf ) was developed in order to universally bound the rate loss in the wyner - ziv problem , and has since then been instrumental in e.g. , bounding the rate loss in successive refinements , universal quantization , and other multi - terminal source coding settings . </S>",
    "<S> the ardf is defined as the minimum mutual information over an additive test channel followed by estimation . in the limit of high resolution , </S>",
    "<S> the adrf coincides with the true rdf for many sources and fidelity criterions . in the other extreme , i.e. , the limit of low resolutions , </S>",
    "<S> the behavior of the ardf has not previously been rigorously addressed .    in this work </S>",
    "<S> , we consider the special case of quadratic distortion and where the noise in the test channel is gaussian distributed . </S>",
    "<S> we first establish a link to the i - mmse relation of guo et al .  and use this to show that for any source the slope of the ardf near zero rate , converges to the slope of the gaussian rdf near zero rate . </S>",
    "<S> we then consider the multiplicative rate loss of the ardf , and show that for bursty sources it may be unbounded , contrary to the additive rate loss , which is upper bounded by 1/2 bit for all sources . </S>",
    "<S> we finally show that unconditional incremental refinement , i.e. , where each refinement is encoded independently of the other refinements , is ardf optimal in the limit of low resolution , independently of the source distribution . </S>",
    "<S> our results also reveal under which conditions linear estimation is ardf optimal in the low rate regime . </S>"
  ]
}