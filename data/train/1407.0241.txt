{
  "article_text": [
    "the statistical study of stochastic processes with jumps , from high frequency data , has been the subject of many recent works .",
    "a major issue is to determine if the jump part is relevant to model the observed phenomenon .",
    "especially , for modelling of asset prices , the assessment of the part due to the jumps in the price is an important question .",
    "this has been addressed in several works , either by considering multi - power variations @xcite or by truncation methods ( see @xcite ) .",
    "another issue is to test statistically if the stochastic process has continuous paths .",
    "the question has been addressed in many works ( see @xcite ) and is crucial to the hedging of options .",
    "a clearly related question is to determine the degree of activity of the jump component of the process .",
    "estimators of the blumenthal  getoor index of the lvy measure of the process are proposed in several papers @xcite .    in that context",
    ", the main statistical difficulty comes from the fact that one observes a discrete sampling of the process , and consequently , the exact values of the jumps are unobserved . as a matter of fact ,",
    "a lot of statistical procedures rely on the estimation of a functional of the jumps . in @xcite",
    ", jacod considers the estimation , from a high frequency sampling , of the functional of the jumps @xmath4 for a smooth function @xmath5 vanishing at zero ( see theorems 2.11 and 2.12 in @xcite for precise assumptions ) .",
    "in particular , he studies the difference between the unobserved quantity @xmath6 and the observed one @xmath7 .",
    "when @xmath8 is a semi - martingale , it is shown that the difference between the two quantities goes to zero with rate @xmath3 .",
    "rescaled by this rate , the difference is asymptotically distributed as @xmath9,\\ ] ] where the variables @xmath10 are uniform variables on @xmath11 $ ] and @xmath12 , @xmath13 are standard gaussian variables . the quantity @xmath14 ( resp . , @xmath15 )",
    "is the local volatility of the semi martingale @xmath8 before ( resp . , after )",
    "the jump at time @xmath16 .",
    "this result serves as the basis for studying the statistical procedures developed in @xcite",
    ".    however , the problem of the efficiency of these methods seems to have never been addressed .",
    "motivated by these facts , we discuss , in this paper , the notion of efficiency to estimate the jumps from the discrete sampling @xmath17 .",
    "let us stress , that the meaning of efficiency is not straightforward here .",
    "indeed , we are not dealing with a standard parametric statistical problem , and it is not clear which quantity can stand for the fisher s information . in this paper , we restrict ourself to processes @xmath8 solutions of @xmath18 where we assume that the number of jumps on @xmath11 $ ] , denoted by @xmath19 , is finite .",
    "we note @xmath20 the vector of jumps , and @xmath21 the random marks . the notion of efficiency",
    "will be stated in this context as a convolution result in theorem [ toptimaliteconv ] .",
    "more precisely , we prove that for any estimator @xmath22 such that the error @xmath23 converges in law to some variable @xmath24 , the law of @xmath24 is a convolution between the law of the vector @xmath25_{k=1,\\ldots , k}\\ ] ] and some other law .",
    "contrarily to the standard convolution theorem , we do not need the usual regularity assumption on the estimator .",
    "the explanation is that we are not estimating a deterministic ( unknown ) parameter , but we estimate some random ( unobserved ) variable @xmath26 .",
    "the proof of this convolution result relies on the study of a preliminary parametric model : we consider the parametric model where the values of the marks @xmath27 are considered as an unknown deterministic parameter @xmath28 .",
    "the resulting model is a stochastic differential equation with jumps , whose coefficients depend on this parameter @xmath29 . we establish then in theorem [ th - lamn ] , that this statistical experiment satisfies the lamn property , with rate @xmath3 and some explicit fisher s information matrix @xmath30 .    by hajek s theorem ,",
    "it is well known that the lamn property implies a convolution theorem for any regular estimator of the parameter @xmath31 ( see @xcite ) . however",
    ", our context differs from the usual hajek s convolution theorem on at least two points .",
    "first , the parameter @xmath29 is randomized and second the target of the estimator @xmath32 depends both on the randomized parameter and on some unobserved quantities @xmath33 . as a result , the connection between the minimal law of the convolution theorem and the fisher s information of the parametric model is not straightforward .",
    "the proof of the convolution theorem , when @xmath34 does not depend on @xmath33 , is simpler and is given in theorem [ tconvosimple ] .",
    "remark that it is certainly possible to state a general result about the connection between the lamn property and convolution theorems for the estimation of unobserved random quantities .",
    "the proof of the proposition [ pindepw ] is a step in this direction . however , giving such general results is beyond the scope of the paper .",
    "the outline of the paper is as follows . in section  [ soptim ] , we state a convolution theorem , which establishes an asymptotic lower bound for the asymptotic error of any estimator of the jumps .",
    "the lamn property is enounced in section  [ slamn ] . in section  [ sestimresult ] , we show that the threshold estimator , introduced by mancini ( see @xcite ) , reaches the lower bound of theorem [ toptimaliteconv ] .",
    "this proves that this lower bound is optimal .",
    "the proofs of these results are postponed to the section  [ sproofs ] .",
    "consider @xmath0}$ ] an adapted c..d.l..g .",
    ", one dimensional , stochastic process defined on some filtered probability space @xmath35 } , \\mathbb{p})$ ] .",
    "we assume that the sample paths of @xmath8 almost surely admit a finite number of jumps .",
    "we denote by @xmath19 the random number of jumps on @xmath11 $ ] and @xmath36 the instants of these jumps .",
    "we assume that the process @xmath8 is a solution of the stochastic differential equation with jumps @xmath37 where @xmath38 is a standard @xmath39 brownian motion .",
    "the vector of marks @xmath40 is random .",
    "the brownian motion , the jump times and the marks are independent .",
    "we will note @xmath41 the sequence of the jumps of the process , defined by @xmath42 , for @xmath43 and @xmath44 , for @xmath45 .",
    "remark that if @xmath46 is exponentially distributed , the jumps times are arrival times of a poisson process .",
    "then , if the marks @xmath47 are i.i.d .",
    "variables , the process @xmath48 is a compound poisson process . in this particular case , the equation ( [ emodelbornesup ] ) becomes a standard sde with jumps based on a random poisson measure with finite intensity .",
    "it is convenient to assume that the process is realized on the canonical product space of the brownian part and the jumps parts @xmath49 , @xmath50 .",
    "more precisely , we note @xmath51 ) , \\mathcal{b } , \\mathbb{w})$ ] , the space of continuous functions endowed with the wiener measure on the borelian sigma - field and @xmath52}$ ] the filtration generated by the canonical process .",
    "we introduce @xmath53 , where @xmath54 is the law of two independent sequences of random variables @xmath55 , @xmath56 .",
    "we assume that , @xmath54-almost surely , the sequence @xmath55 is nondecreasing and such @xmath57 is finite .",
    "then , @xmath58},(t_k)_{k \\ge1},(\\lambda_k)_{k \\ge1})$ ] are the canonical variables on @xmath59 .",
    "we assume that @xmath60 is the right continuous , completed , filtration based on @xmath61 and @xmath62 .    in order to describe the asymptotic law of any estimator of the jumps , we need some additional notation .",
    "following @xcite , we introduce an extension of our initial probability space .",
    "we consider an auxiliary probability space @xmath63 which contains @xmath64 a sequence of independent variables with uniform law on @xmath11 $ ] , and @xmath65 , @xmath66 two sequences of independent variables with standard gaussian law .",
    "all these variables are mutually independent .",
    "we extend the initial probability space by setting @xmath67 , @xmath68 , @xmath69 , @xmath70 .",
    "we need some more assumptions on the process .",
    "especially , to avoid cumbersome notation we will first assume in the next subsection that the number of jumps is deterministic .",
    "we will show in section  [ ssrandom ] that this is not a real restriction , since we can reformulate our result by conditioning on the number of jumps @xmath19 .",
    "since the number of jumps @xmath19 is deterministic , the probability space @xmath59 introduced in section  [ snotation ] is simplified accordingly : @xmath71 , @xmath72)$ ] and @xmath73 .",
    "the space @xmath74 with @xmath75 extends the initial space with the sequences @xmath76 , @xmath77 , @xmath78 .",
    "[ ha0lower ] ( _ law of the jump times _ ) .",
    "the number of jumps @xmath19 is deterministic and the law of @xmath79 is absolutely continuous with respect to the lebesgue measure .",
    "we note @xmath80 its density .",
    "[ ha1lower ] ( _ smoothness assumption _ ) .",
    "the functions @xmath81 and @xmath82 are @xmath83 on @xmath11 \\times\\mathbb{r}$ ] .",
    "we note @xmath84 and @xmath85 their derivatives with respect to @xmath86 and we assume that @xmath84 and @xmath85 are @xmath83 on @xmath11 \\times \\mathbb{r}$ ] . moreover , the functions @xmath87 , @xmath88 , and their derivatives are uniformly bounded .",
    "the function @xmath89 is @xmath90 on @xmath91 , with bounded derivatives .",
    "we note @xmath92 its derivative with respect to @xmath86 and @xmath93 its derivative with respect to @xmath94 .",
    "we assume moreover that @xmath93 is @xmath95 with bounded derivatives .",
    "[ ha2lower ] ( _ non - degeneracy assumption _ ) .",
    "we assume that there exist two constants @xmath96 and @xmath97 such that @xmath98 \\times\\mathbb{r}\\qquad 0 < \\underline{a } \\leq a(t , x ) \\leq \\overline{a } ; & \\\\ & \\forall(x,\\theta ) \\in\\mathbb{r } \\times\\mathbb{r}\\qquad \\bigl|1+c'(x , \\theta ) \\bigr|\\geq\\underline{a}.&\\end{aligned}\\ ] ]    [ ha3lower ] ( `` _ _ randomness _ _ '' _ of the jump sizes _ ) . the law of @xmath21 is absolutely continuous with respect to the lebesgue measure and",
    "we note @xmath99 its density .",
    "we assume also @xmath100    let us comment on these assumptions .",
    "first , the assumption that the vector of jump times admits a density , hypothesis [ ha0lower ] , is crucial to prove the convergence in law of the fractional part of @xmath101 to the vector of uniform laws @xmath102 . in order to find a lower bound ,",
    "we need to deal with a kind of regular model , this explains the assumption [ ha1lower ] .",
    "moreover , it is clear that if the diffusion coefficient @xmath87 is equal to zero , one will expect a rate of convergence for the estimation of the jumps faster than @xmath3 . in that case , the lamn property will not be satisfied with rate @xmath3 .",
    "this clarifies why we assume a strictly positive lower bound on @xmath87 .",
    "remark that the non - degeneracy of @xmath103 is a standard assumption which implies that the equation ( [ emodelbornesup ] ) admits a flow .",
    "the assumption [ ha3lower ] is more specifically related to our statistical problem .",
    "we want to prove a lower bound for the estimation of the random jump sizes .",
    "indeed , if these quantities do not exhibit enough randomness , it could be possible to estimate them with a rate faster than @xmath3 .",
    "for instance , the condition [ ha3lower ] excludes that the jump sizes do not depend on the underlying random marks .",
    "we can now state our main result .",
    "we recall that @xmath104 is the sequence of the jumps of the process .",
    "we will call @xmath105 a sequence of estimators if for each @xmath106 , @xmath107 is a measurable function of the observations @xmath108 .",
    "[ toptimaliteconv ] assume .",
    "let @xmath22 be any sequence of estimators such that @xmath109 for some variable @xmath110 .",
    "then , the law of @xmath110 is necessarily a convolution : @xmath111 where conditionally on @xmath112 } , ( u_k)_k)$ ] , the random vector @xmath113 is independent of @xmath114 .",
    "we will say that an estimator @xmath22 of the jumps is efficient if the asymptotic distribution of @xmath23 is equal in law to @xmath115 ( which corresponds to @xmath116 ) .",
    "it is well known that in parametric models , the hajek s convolution theorem usually requires a regularity assumption on the estimator ( see @xcite ) . here , our theorem does not require any assumption on the estimator , apart its convergence with rate @xmath3 .",
    "this comes from the fact that the target @xmath26 of the estimator is random , yielding to some additional regularity properties , compared with the usual parametric setting ( see a related situation in jeganathan @xcite ) .",
    "[ ropti ] we can observe that @xmath117 where @xmath118 is the diagonal random matrix of size @xmath119 , defined on the extended probability space @xmath120 , with diagonal entries : @xmath121^{-1}\\qquad\\mbox{for } k=1,\\ldots , k.\\ ] ] conditionally on @xmath112 } , ( u_k)_k)$ ] , the vector @xmath122 is a standard gaussian vector on @xmath123 and consequently @xmath122 is independent of @xmath118 .",
    "[ riopt ] the theorem [ toptimaliteconv ] states , in particular , that any estimator of the jumps with rate @xmath3 must have an asymptotic conditional variance greater than @xmath124 .",
    "let us stress that if the rate of convergence is faster than @xmath3 , then ( [ epseudoregest ] ) is still true with @xmath125 and consequently the theorem [ toptimaliteconv ] proves that a convergence faster than @xmath3 is impossible .",
    "now if instead estimating @xmath26 , we estimate a function of the vector of jumps , we can prove in a similar way the following result . for the sake of shortness , we will omit the proof of the following proposition .",
    "[ poptimaliteconv ] assume .",
    "let @xmath126 be a @xmath127 function from @xmath123 to @xmath128 and let @xmath129 be any sequence of estimators of @xmath130 such that @xmath131 for some variable @xmath132 .",
    "then , the law of @xmath132 is necessarily a convolution : @xmath133 where , conditionally on @xmath112 } , ( u_k)_k)$ ] , the real random variable @xmath134 is independent of @xmath114 .",
    "[ roptijacod ] from the results of jacod ( theorems 2.11 and 2.12 in @xcite ) , we deduce that the lower bound of proposition [ poptimaliteconv ] is optimal , and that the estimators of @xcite are efficient .",
    "if the number of jumps is random , we need to modify some assumptions accordingly .",
    "we note @xmath135 \\}$ ] .",
    "conditionally on @xmath19 the law of the vector of jump times @xmath136 admits a density .",
    "conditionally on @xmath19 , the law of @xmath137 is absolutely continuous with respect to the lebesgue measure .",
    "we assume also @xmath138 @xmath139",
    ".    we can extend theorem [ toptimaliteconv ] .",
    "[ ckrandom ] assume @xmath140 , and @xmath141 .",
    "let @xmath22 be any sequence of estimators with values in @xmath142 such that @xmath143 for some variable @xmath110 .",
    "then , the law of @xmath110 admits the decomposition : @xmath144 1_{\\ { 1 \\le k \\le k\\ } } \\bigr)_k + \\widetilde{r},\\ ] ] where conditionally on @xmath145 } , ( u_k)_{1\\le k \\le k})$ ] the random vector of the @xmath19 first components of @xmath113 is independent of @xmath146 .    in section  [ slamn ]",
    ", we consider a parametric model related to the process ( [ emodelbornesup ] ) , and enounce the associated lamn property .",
    "this is the key step before proving theorem [ toptimaliteconv ] and corollary [ ckrandom ] .",
    "remark that , directly considering the values of the jumps size as the parameter , is not the right choice .",
    "the reason is that the jump sizes are not independent of the brownian motion @xmath147 .",
    "instead , we prefer to consider the values of the marks @xmath27 as the statistical parameter .",
    "we focus on the parametric model where the values of the marks @xmath148 are considered as the unknown ( deterministic ) parameters , and @xmath19 is deterministic .",
    "this is the crucial step before proving our convolution theorem .",
    "more precisely , our aim is to obtain the lamn property for the parametric model @xmath149 where the parameter @xmath150 .",
    "we note @xmath79 the vector of jump times such that @xmath151 .",
    "let us remark that , under the assumption [ ha0lower ] , the solutions of ( [ eq - par ] ) might be defined on the probability space @xmath152 endowed with the product of the wiener measure and the law of the jumps times .",
    "but , to avoid new notation , we can assume that , for all @xmath28 , the process @xmath153}$ ] is defined on the space @xmath154 of section  [ soptim ] .    in this model",
    ", we assume that we observe both the regular discretization @xmath155 of the process solution of ( [ eq - par ] ) on the time interval @xmath11 $ ] and the jump times vector @xmath156 .",
    "the observation of @xmath156 leads to a more tractable computation of the likelihood .",
    "this is not restrictive to add some observations to the statistical experiment , since our aim is to derive an asymptotic lower bound . under [ ha0lower ] and [ ha1lower ] , the law of the observations @xmath157 admits a density @xmath158 .",
    "we note @xmath159 the density of @xmath160 conditionally on @xmath156 . for @xmath161",
    "we introduce the log - likelihood ratio : @xmath162    [ th - lamn ] assume and .",
    "then , the statistical experiment @xmath163 satisfies a lamn property . for @xmath28 , @xmath164",
    "we have : @xmath165\\\\[-8pt ] & & \\quad= \\sum_{k=1}^k h_k i_n(\\lambda)_k^{1/2 } n_n(\\lambda)_k - \\frac{1}{2 } \\sum _ { k=1}^k h_k^2 i_n(\\lambda)_k + { \\mathrm{o}}_{\\mathbf { p}^{n,\\lambda}}(1),\\nonumber\\end{aligned}\\ ] ] where @xmath166 is a diagonal random matrix and @xmath167 are random vectors in @xmath123 such that @xmath168 with : @xmath169 ^ 2 u_k+ a^2(t_k , x^{\\lambda}_{t_k-}+c(x^{\\lambda}_{t_k-},\\lambda_k ) ) ( 1-u_k)},\\hspace*{3pt}\\ ] ] where @xmath170 is a vector of independent uniform laws on @xmath11 $ ] such that @xmath171 , @xmath156 and @xmath172}$ ] are independent , and conditionally on @xmath173})$ ] , @xmath122 is a standard gaussian vector in @xmath123 .    actually , we can complete the statement of the theorem by giving explicit expressions for @xmath166 and @xmath167 : @xmath174 where @xmath175 is the integer part of @xmath176 .",
    "[ rlamn ] we remark that from a direct application of hajek s theorem ( see van der vaart  @xcite , corollary 9.9 , page 132 ) , any regular estimator of @xmath29 has an asymptotic conditional variance greater than @xmath177 . here",
    ", an estimator of @xmath29 is a measurable function of @xmath157 , and so we deduce that , a fortiori , any measurable function of @xmath160 satisfies the same asymptotic lower bound .",
    "we use the notation of section  [ snotation ] and since we just propose an estimator of the jumps , we can weaken the assumptions of the previous sections .",
    "[ hexistestcoef ] ( _ smoothness assumption _ ) . the functions @xmath178\\times\\mathbb{r } \\to\\mathbb{r}$ ] , @xmath179\\times\\mathbb{r } \\to \\mathbb{r}$ ] and @xmath180 are continuous .",
    "[ hpositjumpestcoef ] ( _ identifiability of the jumps _ )",
    ". we have almost surely : @xmath181 .",
    "this last condition ensures that the jump times of @xmath8 are exactly the times @xmath16 .",
    "recall that @xmath182 is the sequence of jumps of @xmath8 ( on @xmath11 $ ] ) : we set @xmath183 for @xmath184 and we define @xmath44 for @xmath185 .",
    "we construct an estimator of @xmath26 following the threshold estimation method proposed by mancini @xcite .",
    "let @xmath186 be a sequence of positive numbers tending to @xmath187 .",
    "we set @xmath188 with the convention @xmath189 .",
    "we recursively define for @xmath190 , @xmath191 we set @xmath192 the number of increments of the jump diffusion exceeding the threshold @xmath193 .",
    "we then define for @xmath194 , @xmath195 the sequence @xmath196 is an estimator of the vector of jumps @xmath26 , and @xmath197 estimates the number of jumps .",
    "[ pconsistence ] let us assume @xmath198 , and @xmath199 with @xmath200 .",
    "then , we have almost surely , @xmath201    the consistency result concerning the estimator @xmath202 is a special case of mancini ( @xcite , theorem  1 ) and the jump sizes @xmath203 were consistently estimated in mancini @xcite with exactly the same estimator but when the observation time goes to infinity .",
    "we now describe the asymptotic law of the error between @xmath204 and @xmath26 .",
    "note that theorem 3 in @xcite gives the asymptotic distribution of the estimator of the sum of the jumps assuming that the diffusion coefficient @xmath87 is independent of the brownian process @xmath38 and the jump part , this is the reason why the uniform laws do not appear in the asymptotic law .",
    "the situation is completely different here , since the diffusion coefficient @xmath87 depends on the process @xmath8 , and is more related to jacod s results ( see @xcite ) .",
    "[ ttclsaut ] let us assume @xmath198 , and @xmath199 with @xmath200 .",
    "then @xmath205 converges in law to @xmath206 where the limit can be described on the extended space @xmath120 by : @xmath207 moreover the convergence is stable with respect to the sigma - field @xmath208 .",
    "let us precise that , here , the convergence in law of the infinite dimensional vector @xmath205 means the convergence of any finite dimensional marginals .",
    "[ rrandomvariance ] the theorem [ ttclsaut ] shows that the error for the estimation of the jump @xmath209 is asymptotically conditionally gaussian and that the estimator @xmath210 is efficient .",
    "in particular , the conditional variance on @xmath211 } , ( u_k)_k)$ ] of the error is equal to the lower bound @xmath212 , and consequently this lower bound is optimal .",
    "we divide the proofs into three sections .",
    "we first prove the lamn property of the parametric model in section  [ sslamnpf ] .",
    "then , the convolution result is established in section  [ ssconvolutionpf ] .",
    "finally , the section  [ ssconsistencepf ] is devoted to the proof of the convergence and normality of the estimator @xmath210 .",
    "we first state a lemma which will be useful in the next sections .",
    "[ l - tcltime ] let @xmath213 and consider @xmath214 a random variable on @xmath11^{k_0}$ ] with density @xmath80 . for @xmath215 , we note @xmath216 $ ] the integer part of @xmath176 .",
    "let @xmath217}$ ] be a standard brownian motion independent of @xmath156 .",
    "then , we have the convergence in law of the variables @xmath218}\\biggr)\\ ] ] to @xmath219}\\bigr),\\ ] ] where @xmath220 is a vector of independent uniform laws on @xmath11 $ ] , @xmath221 and @xmath222 are independent standard gaussian vectors such that @xmath156 , @xmath171 , @xmath223 , @xmath224 and @xmath225 are independent .    the convergence of the vector @xmath226}\\bigr)\\ ] ]",
    "is a direct consequence of lemma 6.2 in @xcite ( see also lemma 5.8 in @xcite ) and following this proof ( which is simpler in our case ) , there is no difficulty to add the variables @xmath227 in the vector .",
    "we use the framework of section  [ slamn ] and we introduce some more notation . for @xmath228 , we note @xmath216 $ ] the integer part of @xmath176 and for @xmath229 $ ] , we note @xmath230 the process solution of the following jump - diffusion equation with only one jump at time @xmath16 : @xmath231 under [ ha1lower ] and [ ha2lower ] and conditionally on @xmath156 , this process admits a strictly positive conditional density , which is @xmath232 with respect to @xmath94",
    ". we will note @xmath233 the density of @xmath234 conditionally on @xmath156 and @xmath235 and @xmath236 its derivative with respect to @xmath94 .",
    "we observe that the log - likelihood ratio @xmath237 only involves the transition densities of @xmath238 on a time interval where a jump occurs .",
    "this transition is @xmath233 if there is exactly one jump in the corresponding interval .",
    "then , one can easily see that the following decomposition holds for @xmath237 : @xmath239 where @xmath240 is the indicator function that there is at most one jump in each time interval @xmath241 for @xmath242 .",
    "we have now to study the asymptotic behaviour of ( [ eq - znk ] ) .",
    "this is divided into several lemmas .",
    "the lemmas [ l - malliavin][l - reste ] give an expansion for the score function , with an uniform control in @xmath94 .",
    "we deduce then an explicit expansion for @xmath243 in lemma [ l - lamn ] , and conclude by passing through the limit in lemma [ l - tcl ] .",
    "we begin with a representation of @xmath244 as a conditional expectation , using malliavin calculus .",
    "we refer to nualart @xcite for a detailed presentation of malliavin calculus .",
    "the malliavin calculus techniques to derive lamn properties have been introduced by gobet @xcite in the case of multi - dimensional diffusion processes and then used by gloter and gobet @xcite for integrated diffusions .    in all what follows",
    ", we will denote by @xmath245 a constant ( independent on @xmath106 , @xmath246 and @xmath94 ) which value may change from line to line .",
    "[ l - malliavin ] assuming and , we have @xmath247 : @xmath248 where @xmath249 is the conditional expectation on @xmath156 and @xmath250 , @xmath251 is the malliavin divergence operator and @xmath252 is the process given on @xmath253 $ ] by @xmath254 where @xmath255 is the process solution of @xmath256    we remark that under [ ha1lower ] , the process @xmath255 and its inverse satisfy @xmath257 ,",
    "@xmath258    the proof of lemma [ l - malliavin ] is based on malliavin calculus on the time interval @xmath259 $ ] , conditionally on @xmath156 and @xmath260 .",
    "we first observe that under [ ha1lower ] and [ ha2lower ] , the process @xmath230 solution of ( [ eq - par - thetak ] ) admits a derivative with respect to @xmath94 that we will denote by @xmath261 ( see , e.g. , kunita @xcite since this problem is similar to the derivative with respect to the initial condition ) .",
    "moreover @xmath230 and @xmath261 belong , respectively , to the malliavin spaces @xmath262 and @xmath263 , @xmath257 . now , let @xmath264 be a smooth function with compact support , we have : @xmath265 using the integration by part formula ( see nualart @xcite , proposition 2.1.4 , page 100 ) , we can write @xmath266 where the weight @xmath267 can be expressed in terms of the malliavin derivative of @xmath268 , the inverse of its malliavin variance  covariance matrix and the divergence operator as follows : @xmath269 where @xmath270 on the other hand , from lebesgue derivative theorem , we have : @xmath271 this leads to the following representation @xmath272 it remains to give a more tractable expression of @xmath273 .",
    "we first observe that : @xmath274 and consequently @xmath275 where @xmath276 is solution of ( [ eq - exp ] ) .",
    "turning to the malliavin derivative of @xmath268 , we first observe that @xmath277)$ ] and so we just have to explicit @xmath278 for @xmath279 . assuming first that @xmath280 , we have for @xmath281 $ ] : @xmath282 and then @xmath283 .    now , if @xmath284 , we have for @xmath285 @xmath286 and we deduce that @xmath287 .",
    "it follows that : @xmath288 from ( [ eq - xpoint ] ) and ( [ eq - dx ] ) , we obtain @xmath289\\\\[-12pt ] & = & p^{n,\\theta , k}_s,\\nonumber\\end{aligned}\\ ] ] and the lemma [ l - malliavin ] is proved .    in the next lemma ,",
    "we explicit the conditional expectation appearing in the decomposition of @xmath244 .",
    "[ l - divcond ] assuming and , we have @xmath290\\\\[-8pt ] & & { } + e^{x , t , k}\\bigl(q^{n,\\theta , k } | x_{(i_k+1)/{n}}^{\\theta , k}=y \\bigr)\\nonumber\\end{aligned}\\ ] ] with @xmath291\\\\[-8.5pt ] & & { } + a^2\\biggl ( \\frac{i_k}{n } , x+c(x,\\theta)\\biggr ) \\biggl(\\frac{i_k+1}{n}-t_k \\biggr)\\nonumber\\end{aligned}\\ ] ] and where @xmath292 satisfies @xmath293 for a constant @xmath245 independent of @xmath294 and @xmath94 .",
    "the first term in the right - hand side of ( [ eq - divcond ] ) is the main term and we will prove later that the contribution of the conditional expectation of @xmath295 is negligible .",
    "we first give an approximation of the process @xmath296 which depends on the position of @xmath297 with respect to the jump time @xmath16 .",
    "we have : @xmath298}(s ) \\nonumber\\\\[-1pt ] & & \\hspace*{5pt}{}+ a\\biggl(\\frac{i_k}{n } , x_{{i_k}/{n}}^{\\theta , k } + c\\bigl(x_{i_k / n}^{\\theta , k } , \\theta\\bigr)\\biggr)1_{(t_k , ( { i_k+1})/{n}]}(s ) \\biggr ) \\dot{c}\\bigl(x_{{i_k}/{n}}^{\\theta , k } , \\theta\\bigr)\\nonumber\\\\[-8.5pt]\\\\[-8.5pt ] & & { } / d^{n,\\theta , k}\\bigl(x_{{i_k}/{n}}^{\\theta , k}\\bigr ) \\nonumber\\\\[-1pt ] & & { } + u^{n,\\theta , k}_s,\\nonumber\\end{aligned}\\ ] ] where @xmath299 is defined by ( [ eq - den ] ) and @xmath300 is a remainder term . we deduce then that @xmath301 & & \\hspace*{5pt}{}+ a\\biggl(\\frac{i_k}{n } , x_{{i_k}/{n}}^{\\theta , k}+c\\bigl(x_{i_k / n}^{\\theta , k } , \\theta\\bigr)\\biggr ) ( w_{(i_k+1)/{n}}-w_{t_k } ) \\biggr ) \\dot{c}\\bigl(x_{{i_k}/{n}}^{\\theta , k } , \\theta\\bigr)\\nonumber\\\\[-8.5pt]\\\\[-8.5pt ] & & { } / d^{n,\\theta , k}\\bigl(x_{{i_k}/{n}}^{\\theta , k}\\bigr ) \\nonumber\\\\[-1pt ] & & { } + \\delta\\bigl(u^{n,\\theta , k}\\bigr).\\nonumber\\end{aligned}\\ ] ]    now , we can approximate @xmath234 in the following way : @xmath302 & & { } + a\\biggl(\\frac{i_k}{n } , x_{{i_k}/{n}}^{\\theta , k}+c \\bigl(x_{i_k / n}^{\\theta , k } , \\theta\\bigr)\\biggr ) ( w_{(i_k+1)/{n}}-w_{t_k } ) + r_1^{n,\\theta , k},\\end{aligned}\\ ] ] but observing that @xmath303 we finally obtain @xmath304\\\\[-8pt ] & & { } + a\\biggl(\\frac{i_k}{n } , x_{{i_k}/{n}}^{\\theta , k}+c \\bigl(x_{i_k / n}^{\\theta , k } , \\theta\\bigr)\\biggr ) ( w_{(i_k+1)/{n}}-w_{t_k } ) + r^{n,\\theta , k}\\nonumber\\end{aligned}\\ ] ] with @xmath305 .",
    "putting together ( [ eq - approxdelta ] ) and ( [ eq - approxx ] ) , this yields @xmath306\\\\[-8pt ] & & { } -r^{n , \\theta , k } \\frac { \\dot{c}(x_{{i_k}/{n}}^{\\theta , k } , \\theta ) } { d^{n,\\theta , k}(x_{{i_k}/{n}}^{\\theta , k})}+ \\delta\\bigl(u^{n,\\theta , k}\\bigr).\\nonumber\\end{aligned}\\ ] ] letting @xmath292 be the random variable defined by @xmath307 where @xmath308 and @xmath309 are , respectively , defined by ( [ eq - delta ] ) and ( [ eq - approxx ] ) , we deduce easily the first part of lemma [ l - divcond ] .",
    "it remains to bound @xmath310 , @xmath257 .",
    "we remark that from [ ha1lower ] and [ ha2lower ] @xmath311 for a constant @xmath312 independent on @xmath106 , @xmath246 and @xmath94 .",
    "moreover , we have @xmath313\\\\[-8pt ] \\bigl(e\\sup_{t_k \\leq s \\leq ( { i_k+1})/{n } } \\bigl| x_s^{\\theta , k } - x_{t_k}^{\\theta , k } \\bigr|^p\\bigr)^{1/p } & \\leq&\\frac { c_p}{\\sqrt{n}}.\\nonumber\\end{aligned}\\ ] ] so , one can easily deduce that , assuming [ ha1lower ] , @xmath314 and combining this with ( [ eq - bdn ] ) , we derive @xmath315 turning to @xmath316 , we first recall that , from the continuity property of the divergence operator ( see nualart @xcite , proposition 1.5.8 , page 80 ) , we have @xmath317 where @xmath318 to bound @xmath308 , we first observe that from ( [ eq - nu ] ) @xmath319 so we just have to prove @xmath320 the error term @xmath308 is defined by ( [ eq - delta ] ) as the difference between @xmath321 , given in ( [ eq - expn ] ) , and an explicit ratio : @xmath322}(s ) \\\\ & & \\hspace*{17.7pt}{}+ a\\biggl(\\frac{i_k}{n } , x_{{i_k}/{n}}^{\\theta , k}+c\\bigl(x_{i_k / n}^{\\theta , k } , \\theta\\bigr)\\biggr ) 1_{(t_k , ( { i_k+1})/{n}]}(s ) \\biggr ) \\dot{c}\\bigl(x_{{i_k}/{n}}^{\\theta , k } , \\theta\\bigr)\\\\ & & { } / d^{n,\\theta , k}\\bigl(x_{{i_k}/{n}}^{\\theta , k}\\bigr).\\end{aligned}\\ ] ] since @xmath93 and @xmath92 are bounded , we see easily from ( [ eq - euler ] ) that the difference between the numerators is of order @xmath323 .",
    "now , we remark that @xmath324 and that , using the non - degeneracy assumption [ ha2lower ] @xmath325\\\\[-8pt ] & & \\quad\\geq\\frac { \\underline{a}{}^2 \\min(1 , \\underline{a}{}^2)}{n \\sup_{{i_k}/{n } \\leq u \\leq({i_k+1})/{n } } ( y_u^{\\theta , k})^{2}}.\\nonumber\\end{aligned}\\ ] ] so , combining ( [ eq - by ] ) , ( [ eq - bdn ] ) , ( [ eq - bexp ] ) and ( [ eqbdeno ] ) , we obtain @xmath326 this proves ( [ eqbnuint ] ) and consequently @xmath327 it remains to bound the malliavin derivative of @xmath308 . from ( [ eq - delta ] ) and ( [ eq - expn ] ) , we have for @xmath328 $ ] @xmath329 under [ ha1lower ] , the malliavin derivatives of @xmath330 and @xmath331 are bounded in @xmath332 .",
    "turning to the inverse of the malliavin variance  covariance matrix @xmath333 , given by ( [ eq - gamma ] ) , we have @xmath334 and from ( [ eq - by ] ) and ( [ eqbdeno ] ) , it is easy to see that @xmath335\\\\[-8pt ] \\bigl(e^{x , t , k } \\sup_{{i_k}/{n } \\leq v \\leq({i_k+1})/{n } } \\bigl| d_v \\gamma^{\\theta , k } \\bigr|^p\\bigr)^{1/p } & \\leq & n c_p.\\nonumber\\end{aligned}\\ ] ] putting this together , we obtain @xmath336 and then @xmath337 from ( [ eq - bdiv ] ) , ( [ eq - bnu ] ) and ( [ eq - bndu ] ) , we deduce @xmath338 and the lemma [ l - divcond ] is proved .    the bound on @xmath292 given in lemma [ l - divcond ] is not sufficient , since to obtain the lamn property , we have to compute the conditional expectation with @xmath339 and @xmath340 .",
    "so we complete the lemma [ l - divcond ] with the following bound .",
    "[ l - reste ] with the assumptions and notations of lemma [ l - divcond ] , we have for @xmath94 such that @xmath341 @xmath342 where the constant @xmath343 is independent of @xmath294 and @xmath94 .",
    "we first remark that @xmath344\\\\[-8pt ] & & \\quad\\leq e^{x , t , k } \\bigl| q^{n,\\theta , k } \\bigr|\\frac{p^{\\lambda _ k , t}}{p^{\\theta , t}}\\biggl ( \\frac{i_k}{n } , \\frac{i_k+1}{n},x , x^{\\theta , k}_{({i_k+1})/{n}}\\biggr).\\nonumber\\vadjust{\\goodbreak}\\end{aligned}\\ ] ] from hlder s inequality and lemma [ l - divcond ] , we obtain for @xmath345 , @xmath346 such that , @xmath347\\\\[-8pt ] & & \\quad\\leq c_p \\biggl(e^{x , t , k } \\biggl(\\frac{p^{\\lambda_k , t}}{p^{\\theta , t } } \\biggl(\\frac{i_k}{n } , \\frac{i_k+1}{n},x , x^{\\theta , k}_{({i_k+1})/{n } } \\biggr ) \\biggr)^q \\biggr)^{1/q},\\nonumber\\end{aligned}\\ ] ] and the result of lemma [ l - reste ] reduces to prove that there exists @xmath348 such that @xmath349 where @xmath312 is independent of @xmath106 , @xmath86 and @xmath94 .",
    "we can write : @xmath350\\\\[-8pt ] & & \\quad= \\int p^{\\lambda_k , t}\\biggl(\\frac{i_k}{n } , \\frac { i_k+1}{n},x , y\\biggr)^{q_0}p^{\\theta , t}\\biggl ( \\frac{i_k}{n } , \\frac { i_k+1}{n},x , y\\biggr)^{1-q_0}{\\,\\mathrm{d}}y,\\nonumber\\end{aligned}\\ ] ] and we can express the transition @xmath351 by decomposing it in terms on the transitions of a diffusion without jump on the time intervals @xmath352 and @xmath353 @xmath354 now , assuming [ ha1lower ] and [ ha2lower ] , we have the following classical estimates of the transition probabilities of a diffusion process ( see azencott @xcite , page 478 ) , for some constants @xmath355 ,  @xmath356 : @xmath357 where @xmath358 denotes the density of the gaussian law with mean @xmath359 and variance @xmath360 .",
    "to simplify the notation , we note @xmath361 and @xmath362 . plugging this in ( [ eq - trans ] ) , we obtain @xmath363 we get analogously , @xmath364 observe that , in order to bound ( [ eq - qi ] ) , we have to compute an upper bound for @xmath365 and a lower bound for @xmath366 , since @xmath367 .",
    "our aim now is to give more tractable bounds for the transition density @xmath366 . for this , we make the following change of variables in the integrals @xmath368 and @xmath369 defined in ( [ eq - btranssup ] ) and ( [ eq - btransinf ] ) .",
    "we put @xmath370 .",
    "we observe that @xmath371 .",
    "moreover , from [ ha1lower ] and [ ha2lower ] , @xmath264 is invertible and its derivative satisfies , for some constant @xmath372 : @xmath373 and consequently @xmath374 so we obtain , for some constant @xmath355 @xmath375 proceeding similarly , @xmath376 turning back to ( [ eq - qi ] ) , it follows that @xmath377\\\\[-8pt ] & & \\quad\\leq c \\int g^ { q_0}\\bigl(x+c(x,\\lambda_k ) , \\sigma_{k , n}^1,y\\bigr ) g^{1-q_0}\\bigl(x+c(x,\\theta ) , \\sigma_{k , n}^2,y\\bigr ) { \\,\\mathrm{d}}y,\\nonumber\\end{aligned}\\ ] ] where @xmath378 and @xmath379 . since @xmath380",
    ", we check that @xmath381 and @xmath382 are both lower and upper bound by some constants over @xmath106 .",
    "moreover , we have @xmath383 with @xmath384 and @xmath385 , so @xmath386 .",
    "turning back to the right - hand side term of ( [ eq - qib ] ) , we have to bound @xmath387 with @xmath388 .",
    "first we observe that this integral is finite if @xmath389 , that is @xmath390 .",
    "this choice of @xmath391 is possible since @xmath386 .",
    "after some calculus , we get @xmath392 with @xmath393 recalling that @xmath381 and @xmath382 are of order @xmath1 , we observe that @xmath394 is bounded by some constant times @xmath106 and assuming that @xmath341 , we finally obtain @xmath395 for a constant @xmath343 independent on @xmath86 , @xmath106 and @xmath94 and the lemma [ l - reste ] is proved .",
    "[ l - lamn ] assuming and , we have : @xmath396    we deduce easily from lemmas [ l - malliavin ] and [ l - divcond ] that @xmath397 with @xmath398 . from lemma [ l - reste ] , the second term on the right - hand side of the preceding equation tends to zero in probability .",
    "now , from a taylor expansion of @xmath399 , we have the approximation for @xmath400 $ ] @xmath401 from [ ha1lower ] , and using ( [ eq - bdn ] ) , we have @xmath402 $ ] @xmath403 where @xmath312 does not depend on @xmath86 .",
    "so we deduce that @xmath404 $ ] @xmath405 for a constant @xmath312 independent on @xmath86 and @xmath406 .",
    "consequently , it follows that @xmath407 goes to zero in probability as @xmath106 goes to infinity , and the thesis follows .    [ l - tcl ] let us assume .",
    "let @xmath166 be the diagonal matrix of size @xmath408 , and @xmath167 be the random vector of size @xmath19 , defined by the entries , @xmath409 then , we have , @xmath168 with @xmath30 the diagonal matrix , @xmath410 ^ 2 u_k+ a^2(t_k , x^{\\lambda}_{t_k-}+c(x^{\\lambda}_{t_k-},\\lambda_k ) ) ( 1-u_k)},\\ ] ] and @xmath170 is a vector of independent uniform laws on @xmath11 $ ] such that @xmath171 , @xmath156 and @xmath172}$ ] are independent , and conditionally on @xmath173})$ ] , @xmath122 is a standard gaussian vector in @xmath123 .",
    "we just have to prove the convergence in law of the couple @xmath411 we have from ( [ eq - den ] ) @xmath412 and from ( [ eq - approxx ] ) @xmath413 where @xmath414 is bounded in @xmath332 by @xmath415 ( see the proof of lemma [ l - divcond ] ) .",
    "so as a straightforward consequence of lemma [ l - tcltime ] , we obtain that @xmath416 converges in law to @xmath417 with @xmath418 ^ 2 u_k+ a^2\\bigl(t_k , x^{\\lambda}_{t_k-}+c \\bigl(x^{\\lambda}_{t_k-},\\lambda_k\\bigr ) \\bigr ) ( 1-u_k).\\ ] ] this gives the result of lemma [ l - tcl ] .",
    "as noticed earlier , the proof of theorem [ th - lamn ] follows from the decomposition ( [ eq - znk ] ) with @xmath419 , and lemmas [ l - lamn ] and [ l - tcl ] .      in this section",
    ", we prove the theorem [ toptimaliteconv ] and some related results .",
    "we recall the framework described in section  [ soptim ] .",
    "@xmath420 is the canonical product space , on which are defined the independent variables @xmath172}$ ] , @xmath421 , @xmath21 .",
    "the probability @xmath422 is the simple product of the corresponding probabilities . from this simple disintegration of the measure @xmath422 as a product",
    ", we can introduce @xmath423 the probability @xmath422 conditional on @xmath424 .",
    "the process @xmath8 is solution of ( [ emodelbornesup ] ) , and we may assume that for any @xmath28 the law of @xmath8 under @xmath425 is equal to the law of @xmath238 solution of ( [ eq - par ] ) .",
    "we recall that @xmath120 is the extension of @xmath426 which contains the uniform variables @xmath427 , and the gaussian variables , @xmath428 , @xmath429 .    with these notations , the lamn expansion of theorem [ th - lamn ] writes , @xmath430\\\\[-8pt ] & & \\quad= \\sum _ { k=1}^k h_k i_n ( \\lambda)_k^{1/2 } n_n(\\lambda)_k - \\frac{1}{2 } \\sum_{k=1}^k h_k^2 i_n(\\lambda)_k + { \\mathrm{o}}_{\\mathbb { p}^{\\lambda}}(1)\\nonumber\\end{aligned}\\ ] ] with @xmath431\\\\[-8pt ] n_n(\\lambda)_k&=&\\frac{\\sqrt{n}(x_{({i_k+1})/{n}}-x_{i_k / n}-c(x_{{i_k}/{n}},\\lambda_k ) ) } { \\sqrt{nd^{n , \\lambda_k , k } ( x_{{i_k}/{n } } ) } } , \\nonumber\\\\ d^{n,\\lambda_k , k } ( x_{{i_k}/{n}})&=&a^2\\biggl(\\frac{i_k}{n } , x_{i_k / n}\\biggr ) \\bigl(1+c ' ( x_{{i_k}/{n } } , \\lambda_k)\\bigr)^2 \\biggl(t_k- \\frac{i_k}{n}\\biggr ) \\nonumber\\\\ & & { } + a^2\\biggl(\\frac{i_k}{n } , x_{{i_k}/{n}}+c ( x_{{i_k}/{n}},\\lambda_k)\\biggr ) \\biggl(\\frac{i_k+1}{n}-t_k \\biggr).\\nonumber\\end{aligned}\\ ] ] the theorem [ th - lamn ] states the convergence in law of @xmath432 to @xmath433 under @xmath434 . actually , from the proof of lemma [ l - tcl ] , we get the following convergence result under @xmath422 .",
    "[ plamnbis ] assuming , we have the convergence @xmath435\\\\[-8pt ] & & \\quad{{}\\mathop{\\hbox to 1cm{\\rightarrowfill}}^{n \\to\\infty}_{\\mathit{law } } { } } \\bigl((u_k)_{k } , \\bigl(\\sqrt{u_k } n_k^{- } \\bigr)_{k } , \\bigl(\\sqrt{1-u_k } n_k^{+ } \\bigr)_{k } , i(\\lambda ) , n(\\lambda)\\bigr ) , \\nonumber\\end{aligned}\\ ] ] where @xmath436 is distributed as a standard gaussian variable in @xmath123 .",
    "moreover this convergence is stable with respect to @xmath208 , and the last two limit variables can be represented on the extended space @xmath120 as , @xmath437^{1/2}}.\\end{aligned}\\ ] ]    remark that the matrix @xmath438 is not equal to the matrix @xmath118 appearing in the statement of the convolution theorem [ toptimaliteconv ] .",
    "comparing the expression ( [ edefi ] ) of @xmath118 with the expression ( [ eipara ] ) of @xmath30 , we see that in the parametric case , the information is proportional to @xmath439 .",
    "this is quite natural . if instead of estimating the `` mark ''",
    "@xmath440 we estimate the jump , equal to @xmath441 in the parametric model , we can expect that the effect of @xmath442 vanishes ( by a simple first order expansion of the error of estimation ) .",
    "this gives some insight on why @xmath443 disappears in the expression of @xmath118.=-1    on the other hand , it is not immediate why the expression of the parametric information involves the quantity @xmath444 , which is not present in the expression of @xmath118 .",
    "we will see that it is due to the fact that the value of the jump @xmath445 depends on the unobserved quantity @xmath33 and thus is not a simple functional of the parameter @xmath440 .",
    "if @xmath399 does not depend on @xmath8 , the situation is simpler and the proof of theorem [ toptimaliteconv ] is much easier .",
    "for this reason , in the next section we prove the convolution theorem in this easier setting .",
    "the general proof is given in section  [ sssproofgeneral ] and some intermediate results are stated in section  [ sssintermediate ] .",
    "we start with a simple lemma .",
    "[ lstabin ] assume then for all @xmath446 , @xmath447    this follows easily from the expressions ( [ einconv ] ) .",
    "assume that @xmath22 is a sequence of estimators ( based on @xmath108 ) such that @xmath448 in law under @xmath422 .",
    "then , the theorem [ toptimaliteconv ] is an immediate consequence of the following result .",
    "[ tconvosimple ] assume and that @xmath449 .",
    "denote @xmath450 the diagonal matrix of size @xmath408 such that @xmath451 .",
    "then , we have the decomposition for all @xmath106 , @xmath452 for @xmath453 a sequence of random variables with values in @xmath123 .    along a subsequence @xmath454",
    "we have the convergence in law , @xmath455 where @xmath456 is gaussian , and @xmath457 is independent of @xmath122 conditionally on @xmath118 .    in particular , we have @xmath458 .    we set @xmath459 and define , @xmath460 so that @xmath461 .",
    "since @xmath22 is a measurable function of the @xmath462 , @xmath463 and @xmath450 are measurable functions of the marks , and from the expression ( [ einconv ] ) , we deduce that @xmath464 for some borelian function @xmath465 .    using lemma [ lstabin ] and the expression ( [ edefwnlambda ] )",
    ", we easily get : @xmath466    remark now that by proposition [ plamnbis ] and the convergence of @xmath23 , we get that @xmath467 is a tight sequence of variables",
    ".    hence , we can apply proposition [ pindepw ] below .",
    "we deduce that @xmath468 where the limit can be represented on an extension @xmath469 of the space @xmath120 , and the convergence is stable with respect to @xmath112})$ ] . on this extension ,",
    "the variable @xmath457 is independent of @xmath436 conditionally on @xmath112 } , ( u_k)_k ) $ ] .",
    "this implies ( [ ecvsubprop ] ) , and thus the theorem .    [ pindepw ]",
    "let @xmath470 where @xmath471 is a sequence of borelian functions .",
    "set @xmath472 , and assume :    * @xmath473 , in @xmath423 probability for any @xmath474 , * the sequence @xmath467 is tight .",
    "then , one has the convergence in law , along a subsequence , @xmath475\\\\[-8pt ] & & \\quad{{}\\mathop{\\hbox to 1cm{\\rightarrowfill}}^{(n ) \\to\\infty}_{\\mathit{law } } { } } \\bigl((u_k)_{k } , \\bigl(\\sqrt{u_k } n_k^{- } \\bigr)_{k } , \\bigl(\\sqrt{1-u_k } n_k^{+ } \\bigr)_{k } , i(\\lambda ) , n(\\lambda ) , r\\bigr ) .",
    "\\nonumber\\end{aligned}\\ ] ] the limit can be represented on a extension @xmath476 of the space @xmath120 . on this space ,",
    "the variable @xmath457 is independent of @xmath436 conditionally on @xmath112 } , ( u_k)_k ) $ ] .",
    "moreover the convergence ( [ ecvzindep ] ) is stable with respect to @xmath112})$ ] .    consider the joint law of the random variables , @xmath477},(nt_k - i_k)_{k=1,\\ldots , k } , \\biggl(\\frac{(w_{t_k}-w_{i_k / n})}{\\sqrt{t_k - i_k / n}}\\biggr)_{k=1,\\ldots , k } , \\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\quad\\hspace*{0pt}\\biggl(\\frac{(w_{(i_k+1)/n}-w_{t_k})}{\\sqrt{(i_k+1)/n - t_k}}\\biggr ) _ { k=1,\\ldots , k } , i_n(\\lambda ) , n_n(\\lambda ) , r_n\\biggr ) \\nonumber\\end{aligned}\\ ] ] defined on the corresponding canonical product space , endowed with the usual product topology . from the assumption , all the components of this vector are tight , and thus the joint law is tight . along some subsequence",
    ", it converges in law to some limit , and thus ( [ ecvzindep ] ) holds true .",
    "the stability of the convergence with respect to @xmath478}$ ] is immediate .",
    "remark that from proposition [ plamnbis ] , the law of the limit @xmath479},(u_k)_{k=1,\\ldots , k } , \\bigl(n_k^{-}\\bigr)_{k=1,\\ldots , k } , \\bigl(n_k^{+}\\bigr)_{k=1,\\ldots , k } , i(\\lambda ) , n ( \\lambda ) , r\\bigr)\\ ] ] is known , apart for the last component @xmath457 . it can be clearly represented on an extension @xmath480 of @xmath120 .",
    "to determine some information on the law of @xmath457 , we use techniques inspired from the proof of convolution theorems in @xcite .    consider the following set of random variables defined on the space @xmath59 , @xmath481^r$ } , \\vspace*{1pt}\\cr g_n= g(x_{{[ns_1]}/{n } } , \\ldots , x_{{[ns_r]}/{n } } ) , \\vspace*{1pt}\\cr \\kappa = k(t_1,\\ldots , t_k ) , \\vspace*{1pt}\\cr l_n = l(nt_1-i_1,\\ldots , nt_k - i_k ) , \\vspace*{1pt}\\cr m = m(\\lambda_1,\\ldots,\\lambda_k),}\\ ] ] where @xmath482 , @xmath246 , @xmath483 , @xmath359 are bounded continuous functions .",
    "for @xmath484 we set @xmath485.\\ ] ] clearly @xmath486 in probability , and from the convergence , along a subsequence , of ( [ egrosvecteur ] ) , it is simple to show @xmath487.\\ ] ] by conditioning on the variable @xmath27 , whose law admits a density , we have @xmath488 f_{\\lambda}(\\lambda ) { \\,\\mathrm{d}}\\lambda.\\ ] ] for @xmath164 , we make a simple change of variable in the integral , @xmath489m(\\lambda+h/\\sqrt{n } ) f_{\\lambda}(\\lambda+h/\\sqrt{n } ) { \\,\\mathrm{d}}\\lambda.\\end{aligned}\\ ] ] now the translation is a continuous operator in @xmath490 and by assumption @xmath491 is integrable .",
    "thus , we easily deduce , @xmath492m(\\lambda ) f_{\\lambda}(\\lambda ) { \\,\\mathrm{d}}\\lambda+ { \\mathrm{o}}(1).\\ ] ] from the assumptions , we know the expansion @xmath493 , and from lemma [ lstabin ] , we have the expansion @xmath494 . in these expansions",
    ", all the random variables are only depending on @xmath495 .",
    "but , from the lamn property , we know that the measures @xmath425 and @xmath496 , restricted to @xmath495 , are contiguous .",
    "hence , in these expansions , one can replace @xmath497 with @xmath498 .",
    "then , using dominated convergence theorem , one can get @xmath499m(\\lambda ) f_{\\lambda}(\\lambda ) { \\,\\mathrm{d}}\\lambda+ { \\mathrm{o}}(1).\\ ] ] remark that the random variables appearing in the inner expectation only depend on the observations @xmath495 , and thus the likelihood ratio @xmath500 might be used to change the measure , @xmath501\\quad \\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\hspace*{16.1pt}{}\\times m(\\lambda ) f_{\\lambda}(\\lambda ) { \\,\\mathrm{d}}\\lambda+ { \\mathrm{o}}(1 )",
    ". \\nonumber\\end{aligned}\\ ] ] we deduce , @xmath502 + { \\mathrm{o}}(1).\\ ] ] but from the lamn expansion ( [ elamnrecall ] ) , one can easily get @xmath503 where @xmath504 is the transpose of the vector @xmath505 . hence , using the convergence in law of ( [ egrosvecteur ] ) , and uniform integrability of the sequence @xmath506 , it can be seen that @xmath507\\\\[-8pt ] & & \\quad{{}\\mathop{\\hbox to 1cm{\\rightarrowfill}}^{(n ) \\to\\infty } _ { } { } } e \\bigl [ { \\mathrm{e}}^{{\\mathrm{i}}\\mu_1 \\cdot r } { \\mathrm{e}}^{{\\mathrm{i}}\\mu_2 \\cdot(n(\\lambda)-i(\\lambda ) ^{1/2}h ) } { \\mathrm{e}}^ { h^ * i(\\lambda)^{1/2 } n(\\lambda ) - h^ * i(\\lambda ) h/2 } g \\kappa l(u_1,\\ldots , u_k ) m \\bigr].\\qquad\\quad \\nonumber\\end{aligned}\\ ] ] comparing the expressions ( [ ecara1 ] ) and ( [ ecara2 ] ) , it comes @xmath508 , @xmath509 \\\\ & & \\quad = e \\bigl [ { \\mathrm{e}}^{{\\mathrm{i}}\\mu_1 \\cdot r } { \\mathrm{e}}^{{\\mathrm{i}}\\mu_2 \\cdot(n(\\lambda)-i(\\lambda ) ^{1/2}h ) } { \\mathrm{e}}^ { h^ * i(\\lambda)^{1/2 } n(\\lambda ) - h^ * i(\\lambda ) h/2 } g \\kappa l(u_1,\\ldots , u_k ) m \\bigr].\\end{aligned}\\ ] ] we deduce that @xmath508 , the two following conditional expectations are almost surely equal , @xmath510 \\\\ & & \\quad = e \\bigl [ { \\mathrm{e}}^{{\\mathrm{i}}\\mu_1 \\cdot r } { \\mathrm{e}}^{{\\mathrm{i}}\\mu_2 \\cdot(n(\\lambda)-i(\\lambda ) ^{1/2}h ) } { \\mathrm{e}}^ { h^ * i(\\lambda)^{1/2 } n(\\lambda ) -   h^ * i(\\lambda ) h/2 } { |}x , t , ( u_k)_k , \\lambda\\bigr].\\end{aligned}\\ ] ] but from continuity and analyticity arguments , it can be seen that this equality holds , almost surely , for any @xmath511 .    hence , we can set @xmath512 in the above relation , and find @xmath513= e \\bigl [ { \\mathrm{e}}^{{\\mathrm{i}}\\mu_1 \\cdot r } { |}x , t , ( u_k)_k , \\lambda\\bigr ] { \\mathrm{e}}^{-\\mu_2^*\\mu^{}_2/2}.\\ ] ] this precisely states that , conditionally on @xmath514 , the random variables @xmath457 and @xmath436 are independent .",
    "the proposition is proved after remarking that the brownian motion @xmath225 can be recovered as a measurable functional of @xmath515 .",
    "the assumption @xmath516 is crucial for the proof of theorem [ tconvosimple ] . indeed",
    "if @xmath399 depends on the jump - diffusion , then @xmath517 , and instead of ( [ edefwnlambda ] ) , we have @xmath518 where @xmath519 .",
    "this quantity depends on @xmath33 which is unobserved . however , the assumption that @xmath520 is only function of @xmath495 is essential in the proposition [ pindepw ] ( at the step just before equation ( [ echangproba ] ) ) .",
    "but if instead of @xmath520 we consider @xmath521 where @xmath522 , then , the proposition [ pindepw ] can be applied , and we can prove the following modification of theorem [ tconvosimple ] .",
    "[ tciblediff ] let @xmath22 be any sequence of estimators such that @xmath523 for some variable @xmath524 .",
    "then , the law of @xmath524 is necessarily a convolution , @xmath525 where @xmath436 is a standard gaussian vector independent of @xmath526 , and @xmath457 is some random variable independent of @xmath436 conditionally on @xmath527 .",
    "a simple expression for the entries of the diagonal matrix @xmath528 is @xmath529\\\\[-8pt ] & & \\hspace*{3.5pt}{}+ ( 1-u_k)a(t_k , x_{t_k})^2 \\bigr]^{-1}\\qquad \\mbox{for } k=1,\\ldots , k.\\nonumber\\end{aligned}\\ ] ]    actually , to prove the convolution theorem when the coefficient @xmath530 depends on @xmath86 , we need a strengthened version of the proposition [ pindepw ] . indeed , we will show that the variable @xmath457 , in the statement of proposition [ pindepw ] , is independent of @xmath122 conditionally on any variable that can be obtained as a limit of the observations .",
    "this yields some additional knowledge on the dependence between the variable @xmath457 and the other variables .",
    "[ pindepwrenfo ] let us make the same assumptions as in proposition [ pindepw ] .",
    "assume furthermore that there exist a continuous function @xmath531 with values in @xmath123 and @xmath532 a sequence of random variables depending on the observations @xmath533 , such that @xmath534 then , in the description of the limit ( [ ecvzindep ] ) , the variable @xmath457 is independent of @xmath436 conditionally on @xmath112 } , ( u_k)_k ) $ ] and @xmath535 .",
    "the proof is a slight modification of the proof of proposition [ pindepw ] .",
    "we simply add to the list of random variables ( [ etestfonc ] ) , the new one @xmath536 , with @xmath297 being any continuous bounded function .",
    "accordingly , we set @xmath537 $ ]",
    ". then , the proof follows the same lines as the proof of proposition [ pindepw ] .",
    "we prove the theorem [ toptimaliteconv ] in the general situation where @xmath530 depends on @xmath86 .",
    "as seen in the previous section , a difficulty comes from the fact that the target of the estimator @xmath538 depends on the unobserved value @xmath33 .",
    "we introduce @xmath539 , and with simple computations , one can write the following expansion , for any sequence of estimators @xmath22 , @xmath540 if @xmath541 is tight we can use theorem [ tciblediff ] and deduce , @xmath542 .",
    "after a few algebra , involving the expressions ( [ eilambda])([enlambda ] ) , it could be seen that this reduces to the algebric relation ( [ econvolforme ] ) , with @xmath122 being some standard normal variable .",
    "however by this method , we can not deduce the conditional independence of @xmath457 with @xmath122 .",
    "indeed , only the conditional independence of @xmath457 with @xmath436 is known , and we have no information about the joint law of @xmath457 and @xmath223 .    to solve this problem , we consider two new statistical experiments where we add the observation of the jump - diffusion just before ( or just after ) the jump .",
    "we first state the lamn properties for these new experiments .",
    "we omit the proof , which is similar to the proof of theorem [ th - lamn ] .",
    "[ plamnsurm ] assume and .",
    "denote @xmath543 the density on @xmath544 of the augmented vector of observations @xmath545 under @xmath546 . for @xmath28 , @xmath547 , define the log - likelihood ratio @xmath548 .",
    "we have the expansion : @xmath549\\\\[-8pt ] & & { } - \\frac{1}{2 } \\sum_{k=1}^k h_k^2 i^{\\mathrm{aug}^-}_n(\\lambda)_k + { \\mathrm{o}}_{\\mathbb { p}^\\lambda}(1),\\nonumber\\end{aligned}\\ ] ] where @xmath550 moreover , @xmath551 where @xmath552 is the diagonal information matrix whose entries are @xmath553 and @xmath554 is a standard gaussian vector in @xmath123 .    [ plamnsurp ]",
    "assume and .",
    "denote @xmath555 the density on @xmath544 of the augmented vector of observations @xmath556 under @xmath546 . for @xmath28 , @xmath547 , define the log - likelihood ratio @xmath557 .",
    "we have the expansion : @xmath558\\\\[-8pt ] & & { } - \\frac{1}{2 } \\sum_{k=1}^k h_k^2 i^{\\mathrm{aug}^+}_n(\\lambda)_k + { \\mathrm{o}}_{\\mathbb { p}^\\lambda}(1),\\nonumber\\end{aligned}\\ ] ] where @xmath559 moreover , @xmath560 where @xmath561 is the diagonal information matrix whose entries are @xmath562 and @xmath563 is a standard gaussian vector in @xmath123 .",
    "we now deduce convolution results from these lamn properties .",
    "[ pconvdouble ] let @xmath22 be a sequence of estimator based on the observations of @xmath564 and denote @xmath565 .",
    "suppose that the sequence @xmath566 is tight and define @xmath567 and @xmath568 by the following expansions @xmath569 where @xmath570 ( resp . , @xmath571 )",
    "is the diagonal matrix with entries @xmath572 ( resp .",
    ", @xmath573 ) and @xmath574 is diagonal with entries @xmath575 .",
    "then , we have the convergence in law @xmath576 \\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\quad{{}\\mathop{\\hbox to 1cm{\\rightarrowfill}}^{(n ) \\to\\infty}_{}{}}\\bigl [ \\bigl(a(t_k , x_{t_k } ) \\sqrt{1-u_k}n_k^+ \\bigr)_k,\\nonumber\\\\ & & \\hspace*{36.5pt}\\quad \\bigl(a(t_k , x_{t_k- } ) \\bigl(1+c'(x_{t_k- } , \\lambda_k)\\bigr ) \\sqrt{u_k } n_k^-\\bigr)_k , r^{\\mathrm{aug}^- } , r^{\\mathrm{aug}^+}\\bigr ]",
    ". \\nonumber\\end{aligned}\\ ] ] this convergence holds jointly with ( [ econvbase ] ) and the limit variables can be represented on an extension of @xmath120 . on this space ,",
    "one has , @xmath577 , @xmath578\\\\[-8pt ] & & { } + a(t_k , x_{t_k } ) \\sqrt{1-u_k } n_k^+.\\nonumber\\end{aligned}\\ ] ] moreover , conditionally on @xmath112 } , ( u_k)_k , ( n_k^-)_k)$ ] , the variable @xmath579 is independent of @xmath580 . in a symmetric way , conditionally on @xmath112 } , ( u_k)_k , ( n_k^+)_k)$ ] , the variable @xmath581 is independent of @xmath582 .    from the definition of the variables @xmath583 and @xmath584 given by equations ( [ edefrsurm ] ) and ( [ edefrsurp ] ) , we deduce immediately the relations @xmath585_k",
    "+ r_n^{\\mathrm{aug}^-}+ { \\mathrm{o}}_{\\mathbb{p}}(1 ) , \\\\",
    "\\label{ersurp } \\sqrt{n}\\bigl(\\widetilde{j}{}^{n}- \\overline{j}{}^n\\bigr ) & = & \\bigl[\\sqrt{n}\\bigl(x_{t_k}-x_{i_k / n}-c(x_{{i_k}/{n } } , \\lambda_k)\\bigr)\\bigr]_k+ r_n^{\\mathrm{aug}^+}.\\end{aligned}\\ ] ]    by a tightness argument the joint convergence , along a subsequence , of ( [ econvbase ] ) and ( [ ecvlawcompli ] ) is clear .",
    "the relation ( [ erelrr ] ) is a consequence of the equality between the quantities ( [ ersurm ] ) and ( [ ersurp ] ) .",
    "now , we can deduce , from the lamn property ( proposition [ plamnsurm ] ) , a result analogous to proposition [ pindepw ] .",
    "hence @xmath579 is independent of the limit of @xmath586 , conditionally on @xmath112 } , ( u_k)_k)$ ] .",
    "moreover , remark that in the experiment @xmath587 , the sequence of variables @xmath588 is observed . but",
    "@xmath589 converges to zero in @xmath422-probability . showing a result analogous to proposition [ pindepwrenfo ]",
    ", we deduce that @xmath579 is independent of the limit of @xmath586 , conditionally on @xmath112 } , ( u_k)_k , ( \\sqrt{u_k } n_k^{-})_k)$ ] .",
    "this shows immediately that @xmath579 is independent of @xmath590 conditionally on @xmath591 } , ( u_k)_k , ( n_k^{-})_k)$ ] , since the sigma - fields generated by the two vectors are the same .",
    "the conditional independence between @xmath581 and @xmath582 is obtained in a symmetric way : one uses the lamn property of proposition [ plamnsurp ] , and the fact that the sequence @xmath592 is observed in the experiment based on @xmath593 .    finally , we are able to prove theorem [ toptimaliteconv ] .",
    "proof of theorem [ toptimaliteconv ] first , we write @xmath594\\\\[-8pt ] & = & \\sqrt{n}\\bigl ( \\widetilde{j}{}^{n}_k - \\overline{j}{}^n_k \\bigr ) - c'(x_{i_k / n},\\lambda_k ) \\sqrt{n } ( x_{t_k-}-x_{{i_k}/{n } } ) + { \\mathrm{o}}_{\\mathbb{p}}(1).\\nonumber\\end{aligned}\\ ] ] but the sequence @xmath595 is tight , and we can apply proposition [ pconvdouble ] . using ( [ ersurm ] ) , ( [ ecvlawcompli ] ) , and ( [ epreuvefincv ] ) we deduce @xmath596 we write the last equation as @xmath597 where @xmath598 .",
    "using proposition [ pconvdouble ] , we deduce that @xmath113 is independent of @xmath224 conditionally on @xmath112 } , ( u_k)_k , ( n_k^-)_k)$ ] .    from ( [ erelrr ] )",
    ", we have @xmath599 and we deduce that @xmath600 is independent of @xmath223 conditionally on @xmath601 } , ( u_k)_k , ( n_k^+)_k)$ ] .    remarking that @xmath223 and @xmath224 are independent conditionally on @xmath602 } , ( u_k)_k)$ ]",
    ", we deduce that @xmath113 is independent of @xmath603 conditionally on @xmath112 } , ( u_k)_k)$ ] .",
    "the theorem is proved .",
    "proof of corollary [ ckrandom ] we introduce the conditional probability @xmath604 for any @xmath605 such that @xmath606 . for any @xmath607 , the sequence @xmath608 is tight ( for the product topology on @xmath609 ) under @xmath610 .",
    "so , on a subsequence , one has the convergence in law @xmath611 , moreover the subsequence may be chosen independent of @xmath612 from a diagonal extraction argument .",
    "fix @xmath613 , under the probability @xmath610 , the assumptions [ ha0lower][ha3lower ] are satisfied and we can apply theorem [ toptimaliteconv ] to the @xmath612 first components of the vector @xmath614 .",
    "the corollary follows from the decomposition of the law of @xmath615 .",
    "proof of proposition [ pconsistence ] for @xmath616 , let us note @xmath175 the integer such that @xmath617 .",
    "we set @xmath618 and consider a variable which counts the number of false discovery of a jump by the estimator , @xmath619    for @xmath620 , we define @xmath621 as the event @xmath622 } [ { \\vert}b(s , x_s){\\vert}+{\\vert}a(s , x_s){\\vert } ] \\le m \\}$ ] .",
    "we have @xmath623 & & \\quad\\le \\mathbb{e } [ e_n{1}_{\\omega_m } ] \\nonumber\\\\[-1pt ] & & \\quad = \\sum_{i=0}^{n-1 } \\mathbb{e } [ { 1}_{{\\vert}x_{(i+1)/n}-x_{i / n } { \\vert}\\ge u_n } { 1 } _ { i \\notin\\mathcal{i } } { 1}_{\\omega_m } ] \\\\[-1pt ] & & \\quad\\le\\sum_{i=0}^{n-1 } \\mathbb{p } \\biggl [ \\biggl\\ { \\biggl{\\vert}\\int_{i / n}^{({i+1})/{n } } a(s , x_s ) { \\,\\mathrm{d}}w_s + \\int _ { { i}/{n}}^{({i+1})/{n } } b(s , x_s ) { \\,\\mathrm{d}}s \\biggr{\\vert}\\ge u_n \\biggr\\ } \\cap \\omega_m \\biggr ] \\nonumber\\\\[-1pt ] & & \\quad \\le\\sum_{i=0}^{n-1 } \\mathbb{p } \\biggl [ \\biggl\\ { \\biggl{\\vert}\\int_{i / n}^{(i+1)/n } a(s ,",
    "x_s ) { \\,\\mathrm{d}}w_s \\biggr{\\vert}\\ge u_n - \\frac{m}{n } \\biggr\\ } \\cap\\omega_m \\biggr].\\nonumber\\end{aligned}\\ ] ] with @xmath624 one has , using markov and burkholder  davis  gundy inequalities : @xmath625",
    "\\nonumber\\\\ & & \\quad\\le \\mathbb{p } \\biggl [ \\biggl{\\vert}\\int_{i / n}^{({i+1})/{n } } a_m(s , x_s ) { \\,\\mathrm{d}}w_s \\biggr{\\vert}\\ge u_n - \\frac{m}{n } \\biggr ] \\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\quad\\le c_p \\biggl(u_n - \\frac{m}{n } \\biggr)^{-p } n^{-p/2 } \\qquad\\forall p>0 \\nonumber\\\\ & & \\quad = c_p n^{p(\\varpi-1/2 ) } \\qquad\\forall p>0.\\nonumber\\end{aligned}\\ ] ] since @xmath626 , we get , from ( [ eborcant1 ] ) and ( [ eborcant2 ] ) by choosing @xmath627 large enough , @xmath628 , and by borel cantelli s lemma we deduce that @xmath629",
    ". it immediately implies @xmath630 and since @xmath631 , we easily deduce that almost surely , there exists @xmath106 , such that @xmath632 , @xmath633 . recalling the definitions ( [ edefihat ] ) and ( [ edefenproofconsistence ] ) , we conclude that almost surely , if @xmath106 is large enough , @xmath634 and , as a consequence , @xmath635 .",
    "now , remark that we have almost surely the convergence , for all @xmath184 , @xmath636 from the assumption [ hpositjumpestcoef ] , we have @xmath637 and using that @xmath638 , we deduce that for @xmath106 large enough , @xmath639 .    as a consequence",
    ", we have shown that , @xmath640 eventually , the proposition follows from ( [ edefjhat ] ) , ( [ einctodelta ] ) and ( [ eihatequali ] ) .    proof of theorem [ ttclsaut ] we use the notation introduced in the proof of proposition [ pconsistence ] : for @xmath616 , we have @xmath641 .",
    "let us define for @xmath642 , @xmath643 and @xmath644 for @xmath645 . using ( [ edefjhat ] ) and ( [ eihatequali ] )",
    ", we see that , almost surely , for @xmath106 large enough , we have @xmath646 . hence , it is sufficient to study the limit in law of @xmath647 .",
    "consider any @xmath648 such that @xmath606 and define @xmath649 , the conditional probability .",
    "actually , we will prove the convergence of @xmath650 conditionally on the event @xmath651 , to the law of @xmath24 conditional on @xmath651 , which is sufficient to prove the theorem .",
    "for @xmath652 we have @xmath644 , hence we focus only on the components @xmath653 with @xmath654 .",
    "define @xmath655 .",
    "we have @xmath656 on @xmath657 , the following decomposition holds true @xmath610 almost surely , for any @xmath654 , @xmath658 where @xmath659 first , we show that @xmath660 converges to zero in @xmath661 probability as @xmath662 . using [ hexistestcoef ] ,",
    "the ordinary integral converges almost surely to zero .",
    "it remains to see that the two stochastic integrals converge to zero .",
    "using that the jumps times are @xmath663-measurable , we can write the stochastic integral @xmath664 as a local martingale increment @xmath665}(s ) \\bigl(a(s , x_s)-a(i_k / n , x_{i_k / n})\\bigr ) { \\,\\mathrm{d}}w_s.\\ ] ] the bracket of this local martingale is @xmath666 which converges to zero almost surely , using the right continuity of the process @xmath8 .",
    "we deduce that @xmath667 converge to zero in probability .",
    "we proceed in the same way to prove that @xmath668 in probability .",
    "this yields to the relation , @xmath669 using @xmath670 , and the independence between @xmath671}$ ] and @xmath156 under @xmath610 , we can apply lemma  [ l - tcltime ] .",
    "we get the convergence in law , under @xmath610 , @xmath672 } \\bigr ) \\\\ & & \\quad{{}\\mathop{\\hbox to 1cm{\\rightarrowfill}}^{n \\to\\infty } _ { } { } } \\bigl((t_k)_{k=1,\\ldots , k_0},\\bigl ( \\sqrt{u_k } n^{-}_k\\bigr)_{k=1,\\ldots , k_0 } , \\bigl(\\sqrt{1-u_k } n^{+}_k \\bigr)_{k=1,\\ldots , k_0},(w_t)_{t\\in[0,1]}\\bigr).\\end{aligned}\\ ] ] since the marks @xmath40 , the brownian motion , and the jump times are independent , we have that , under @xmath610 , @xmath673 converges in law to @xmath674 stably with respect to the sigma - field generated by @xmath671}$ ] , @xmath675 and @xmath40 .",
    "the limit can be represented on the extended space @xmath120 endowed with the probability @xmath676 conditional on @xmath677 .",
    "but the process @xmath8 is measurable with respect to @xmath678 , and we deduce the stable convergence , @xmath679 for @xmath680 , under @xmath610 .    by simple computations , this implies the convergence of @xmath681 under @xmath422 , and the theorem is proved .",
    "we would like to thank the referees for their careful reading and suggestions which improved the presentation of the paper ."
  ],
  "abstract_text": [
    "<S> we study the problem of the efficient estimation of the jumps for stochastic processes . </S>",
    "<S> we assume that the stochastic jump process @xmath0}$ ] is observed discretely , with a sampling step of size @xmath1 . in the spirit of hajek </S>",
    "<S> s convolution theorem , we show some lower bounds for the estimation error of the sequence of the jumps @xmath2 . as an intermediate result </S>",
    "<S> , we prove a lamn property , with rate @xmath3 , when the marks of the underlying jump component are deterministic . </S>",
    "<S> we deduce then a convolution theorem , with an explicit asymptotic minimal variance , in the case where the marks of the jump component are random . to prove that this lower bound is optimal , </S>",
    "<S> we show that a threshold estimator of the sequence of jumps @xmath2 based on the discrete observations , reaches the minimal variance of the previous convolution theorem .    , </S>"
  ]
}