{
  "article_text": [
    "our knowledge about some of the most fundamental parameters of physics is derived from a vast number of measurements produced by different experiments using several complementary techniques .",
    "many statistical methods are routinely used  @xcite to combine the available data and extract the most appropriate estimates of the values and uncertainties for these parameters , properly taking into account all correlations between the measurements .",
    "one the most popular methods for performing these combinations is the best linear unbiased estimate ( blue ) technique , an approach first introduced in the 1930 s  @xcite and whose reformulation in the context of high - energy physics  @xcite has been routinely used for the combination of the precision measurements performed by experiments at the lep  @xcite , tevatron  @xcite and lhc  @xcite colliders , as well as in other domains .    to quantify the `` relative importance '' of each measurement in its contribution to the combined knowledge about the measured physical quantity , its coefficient in the blue weighted average is traditionally used . in many examples in the literature where the blue technique has been used ,",
    "the combinations are dominated by systematic uncertainties , often assumed as fully correlated among different measurements .",
    "this often leads to situations where one or more measurements contribute with a negative blue coefficient , pushing experimentalists to redefine the `` relative importance '' of a measurement as the absolute value of its blue coefficient , normalised to the sum of the absolute values of all coefficients  @xcite . in our opinion",
    ", this approach is .    in this paper",
    ", we propose a different approach for comparing the relative contributions of the measurements to the combined knowledge about the unknown parameter , using the well - established concept of fisher information  @xcite .",
    "we also show that negative coefficients in the blue weighted average invariably indicate the presence of very high correlations , whose marginal effect is that of reducing the error on the combined estimate , rather than increasing it . in these regimes",
    ", we stress that taking systematic uncertainties to be fully ( i.e. 100% ) correlated is not a conservative assumption , and we therefore argue that the correlations provided as inputs to blue combinations need to be assessed with extreme care . in those situations where their precise evaluation is impossible , we offer a few guidelines and tools for critically re - evaluating these correlations , in order to help experimental physicists perform more `` conservative '' combinations . in our discussion , we will generally limit ourselves to blue combinations of a single measured parameter and where the correlations used as inputs to the combination are positive .",
    "many of the concepts and tools we present could be applied also to the more general cases of blue combinations of several measured parameters , and/or involving also negative correlations between measurements , but this discussion is beyond the scope of this paper .",
    "the outline of this article is the following . in sec .",
    "[ sec : ri ] we review the definition of `` relative importance '' of a measurement in a blue combination as presented by some papers in the literature and we present our objections to it by using a simple numerical example .",
    "we then present our alternative definitions of information weights in sec .",
    "[ sec : information ] , after a brief recall of the definition of fisher information and of its relevant features . by studying marginal information and information derivatives , in sec .",
    "[ sec : highcorr ] we show that negative blue coefficients in the combination of several measurements of one parameter are always a sign of a `` high - correlation '' regime , thus generalising the results presented for two measurements by the authors of ref .  @xcite . in sec .",
    "[ sec : conservativeness ] we go on to discuss practical guidelines and tools , illustrated by numerical examples , to identify correlations that may have been overestimated and to review them in a more `` conservative '' way . in sec .  [ sec : conclusions ] we summarize our discussion and present some concluding remarks .",
    "in the blue technique , the best linear unbiased estimate of each unknown parameter is built as a weighed average of all available measurements .",
    "the coefficients multiplying the measurements in each linear combination are determined as those that minimize its variance , under the constraint of a normalisation condition which ensures that this represents an unbiased estimate of the corresponding parameter . as discussed extensively in refs .",
    "@xcite , this technique is equivalent to minimizing the weighted sum of squared distances of the measurements from the combined estimates , using as weighting matrix the input covariance matrix of the measurements , which is assumed to be known a priori .    in the case of @xmath0 measurements @xmath1 of a single parameter",
    "whose true value is @xmath2 , in particular , the best linear unbiased estimate @xmath3 can be determined as follows .",
    "first , the blue should be a linear combination @xmath4 of the available measurements .",
    "second , the blue should be an unbiased estimator , i.e. its expectation value @xmath5 $ ] should be equal to the true value @xmath2 of the unknown parameter . assuming that each measurement is also an unbiased estimator , i.e. that its outcomes are distributed as random variables with expectation values @xmath6={y}$ ] , this is equivalent to requiring a normalisation condition @xmath7 for the coefficients @xmath8 in the linear combination .",
    "third , the blue should be the best of such unbiased linear combinations , i.e. that for which the combined variance @xmath9 , where @xmath10 is the covariance matrix of the measurements , is minimized .",
    "it is then easy to show  @xcite that @xmath3 is the best linear unbiased estimate if the coefficients @xmath8 are equal to @xmath11 where @xmath12 is a vector whose elements are all equal to 1 .    while the normalisation condition ensures that the coefficients @xmath8 sum up to 1 , one peculiar and somewhat counter - intuitive feature of this method is that some of these individual coefficients may be negative .",
    "negative coefficients in the blue weighted averages apparently still pose a problem of interpretation sometimes , especially if these coefficients are used to compare the contributions of the different measurements to the combined knowledge about the measured observable .",
    "for instance , the `` relative importance '' of each measurement in the combination of atlas and cms results on the top quark mass  @xcite was defined as the absolute value of its coefficient in the blue weighted average , divided by the sum of the absolute values of the coefficients for all input measurements ,    @xmath13    the same procedure had already been used to visualize the `` weight that each measurement carries in the combination '' of cdf and d0 results on the top quark mass  @xcite . in both cases , the relative importances of the @xmath0 measurements sum up to 1 by definition , @xmath14 .    in our opinion , this procedure is an artefact that is conceptually wrong and suffers from two important limitations : first , it is not internally self - consistent and may easily lead to numerical conclusions which go against common sense ; second , it does not help to understand in which way the results with negative coefficients contribute to reducing the uncertainties on the combined estimates .",
    "we will use a simple example to illustrate the first objection . consider the combination of two uncorrelated measurements @xmath15 and @xmath16 of an observable @xmath2 in the appropriate units .",
    "the covariance matrix is then @xmath17 and the blue for their combination is @xmath18 , where the coefficients of these two uncorrelated measurements in the blue weighted average , @xmath19 and @xmath20 , are proportional to the inverses of the variances @xmath21 and @xmath22 as expected from simple error propagation .",
    "it is rather intuitive in this case to claim that the relative contributions to the knowledge about @xmath2 contributed by the two independent measurements a and b can be quantified by their blue coefficients , 40% for a and 60% for b. as @xmath23 and @xmath24 are both positive , these are also the `` relative importances '' of a and b according to  eq .",
    "[ eq : ri ] .",
    "imagine now that @xmath25 is not the result of a direct measurement , but is itself the result of the combination of two measurements @xmath26 and @xmath27 , where a high positive correlation @xmath28 between them leads to negative blue coefficients in their weighted average @xmath29 . instead of combining first @xmath30 and @xmath31 and then adding @xmath32 , one could also combine @xmath32 , @xmath30 and @xmath31 directly using the full covariance matrix @xmath33 this yields @xmath34 , where the blue coefficients in this overall weighted average are given by @xmath19 , @xmath35 and @xmath36 .    as expected , the final numerical result for @xmath3 is of course the same whether it is obtained from the combination of @xmath32 and @xmath25 or from the combination of @xmath32 , @xmath30 and @xmath31 .",
    "it is also not surprising that the blue coefficient @xmath19 for @xmath32 is the same in both combinations , as this is an independent measurement that is not correlated to either @xmath30 or @xmath31 ( the sum of whose blue coefficients , @xmath37 , of course , equals the blue coefficient @xmath24 of @xmath25 ) . what is rather surprising , however , is that the `` relative importance '' of @xmath32 computed using normalised absolute values of the blue coefficients is very different in the two cases : @xmath38 in our opinion , this is an internal inconsistency of eq .",
    "[ eq : ri ] , as common sense suggests that the relative contribution of @xmath32 to the knowledge about @xmath2 is the same in both combinations . in particular , we consider that the contribution of @xmath32 is indeed 40% , and that this is underestimated as 25% in the second combination because the relative contributions of @xmath30 and @xmath31 in the presence of negative blue coefficients are not being properly assessed and are overall overestimated .    more generally , the problem with defining the `` relative importances '' of measurements according to  eq .",
    "[ eq : ri ] is that the coefficient with which a measurement enters the linear combination of all measurements in the blue , i.e. its `` weight '' in the blue weighted average , is being confused with the impact or `` weight '' of its relative contribution to the knowledge about the measured observable . in the following we will therefore clearly distinguish between these two categories of `` weights '' : we will sometimes refer to the blue coefficient @xmath8 of a measurement as its `` central value weight ''  ( cvw ) , while we will use the term `` information weight ''  ( iw ) to refer to , using the same words as in refs .",
    "@xcite , its `` relative importance '' or the `` weight it carries in the combination '' . we will propose and discuss our definitions of intrinsic and marginal information weights in the next section , using the well - established concept of fisher information .",
    "in this section , we present our definitions of intrinsic and marginal information weights , after briefly recalling the definition of fisher information and summarizing its main relevant features . a more general discussion of fisher information and its role in parameter estimation in experimental science is well beyond the scope of this paper and can be found in many textbooks on statistics such as the two excellent reviews in refs .",
    "@xcite , which will largely be the basis of the overview presented in this section .",
    "consider @xmath0 experimental measurements @xmath39 that we want to use to infer the true values @xmath40 of @xmath41 unknown parameters , with @xmath42 ( though each of the @xmath1 need not necessarily be a direct measurement of one of the parameters @xmath43 ) .",
    "we will use the symbols @xmath44 and @xmath45 to indicate the vectors of all @xmath1 and of all @xmath43 , respectively .",
    "the measurements @xmath44 are random variables distributed according to a probability density function @xmath46 that is defined under the normalisation condition @xmath47 . the sensitivity of the measurements to the unknown parameters can be represented by the fisher `` score vector '' @xmath48 , which is itself a random variable , defined in the @xmath0-dimensional space of the measurements and whose value in general also depends on the parameters @xmath45 . under certain regularity conditions ( in summary",
    ", the ranges of values of @xmath44 must be independent of @xmath45 , and @xmath49 must be regular enough to allow @xmath50 and @xmath51 to commute ) , it can be shown  @xcite that the expectation value of the fisher score is the null vector , @xmath52 .",
    "the fisher information matrix , which in the following we will generally refer to simply as `` information '' , is defined as the covariance of the score vector : as the expectation value of the score is null , this can simply be written as @xmath53\\\\ = \\int\\,\\frac{\\partial\\log{p}({{\\mathbf{{y}}}};{{\\mathbf{{x}}}})}{\\partial{x}{_\\alpha } } \\,\\,\\frac{\\partial\\log{p}({{\\mathbf{{y}}}};{{\\mathbf{{x}}}})}{\\partial{x}{_\\beta } } \\,\\,{p}({{\\mathbf{{y}}}};{{\\mathbf{{x } } } } ) \\ , d{y}_1 \\ldots d{y}_{n}{}. \\label{eq : infodef}\\end{gathered}\\ ] ] information is thus defined as the result of an integral over @xmath54 and does not depend on the specific numerical outcomes of the measurements @xmath1 , although in general it is a function of the parameters @xmath43 instead . in other words , information is a property of the measurement process , and more particularly of the errors on the measurements and of the correlations between them , rather than of the specific measured central values @xmath1 .    as pointed out in ref .",
    "@xcite , fisher information is a valuable tool for assessing quantitatively the contribution of an individual measurement to our knowledge about an unknown parameter inferred from it , because it possesses three remarkable properties .",
    "first , information increases with the number of observations @xmath1 and in particular it is additive , i.e. the total information yielded by two independent experiments is the sum of the information from each experiment taken separately .",
    "second , the definition of the `` information obtained from a set of measurements '' depends on which parameters we want to infer from them .",
    "this is clear from eq .",
    "[ eq : infodef ] , which defines fisher information @xmath55 about @xmath45 in terms of a set of derivatives with respect to the parameters @xmath45 .",
    "finally , information is related to precision : the greater the information available from a set of measurements about some unknown parameters , the lower the uncertainty that can be achieved from the measurements on the estimation of these parameters . more formally , if @xmath56 is any unbiased estimator of the parameter vector @xmath45 derived from the measurements @xmath44 , then under the same regularity conditions previously assumed it can be shown that @xmath57 , where the symbol @xmath58 indicates that the difference between the matrices on the left and right hand sides is positive semidefinite . in particular , for the diagonal elements of these matrices , @xmath59 in other words , the quantity @xmath60 represents a lower bound ( called cramer - rao lower bound ) on the variance of any unbiased estimator of each parameter @xmath43 .",
    "an unbiased estimator whose variance is equal to its cramer - rao lower bound , i.e. one for which the equality in eq .",
    "[ eq : crlbdiag ] holds , is called an efficient unbiased estimator . while in the general case it is not always possible to build one , an efficient unbiased estimator does exist under the assumption that the @xmath0 measurements @xmath44 are multivariate gaussian distributed with a positive definite covariance matrix that is known a priori and does not depend on the unknown parameters @xmath45 .",
    "this is the same assumption that had been used for the description of the blue method in ref .",
    "@xcite and we will take it as valid throughout the rest of this paper .    as discussed at length in refs .",
    "@xcite , such distributions possess in fact a number of special properties that significantly simplify all statistical calculations involving them .",
    "in particular , it is easy to show , in the general case with several unknown parameters , that the best linear unbiased estimator is under these assumptions an unbiased efficient estimator , i.e. that its covariance matrix is equal to the inverse of the fisher information matrix .",
    "moreover , the fisher information matrix and the combined covariance do not depend on the unknown parameters @xmath45 under these assumptions , while this is not true in the general case . for gaussian distributions",
    ", the best linear unbiased estimator also coincides with the maximum likelihood estimator  @xcite , while this is not true in most other cases , including the case of poisson distributions .    in the case of one unknown parameter , in particular , i.e. when the parameter vector @xmath45 reduces to a scalar @xmath2 , the probability density function is simply @xmath61{}.\\end{gathered}\\ ] ] remembering that @xmath62 $ ] is the covariance of the unbiased measurements @xmath1 and @xmath63 , the fisher information for @xmath2 , which also reduces to a scalar @xmath64 , can simply be written as @xmath65 this is clearly the inverse of the variance of the blue for @xmath2 corresponding to the central value weights @xmath8 given in eq .",
    "[ eq : coeff1 ] , @xmath66 to further simplify the notation , in the following by @xmath67 we will always indicate the information @xmath64 relative to @xmath2 , dropping the superscript @xmath2 .",
    "having recalled the relevance of the fisher information concept to quantitatively assess the contribution of a set of measurements to the knowledge about an unknown parameter , we may now introduce our proposal about how to best represent the `` weight that a measurement carries in the combination '' or its `` relative importance '' .",
    "we define this in terms of intrinsic and marginal information weights .",
    "our approach is radically different from that of refs .",
    "@xcite , because we do not attempt to make sure that the @xmath0 weights for the different measurements sum up to 1 .",
    "[ cols=\"<,^,^,^,^,^\",options=\"header \" , ]     note also that the onionization prescription leads to the only combination where the blue coefficient for measurement @xmath68 becomes strictly positive . as mentioned earlier",
    ", this may be a consequence of the fact that this prescription may reduce correlations even more than their `` most conservative '' values , trespassing well into the low - correlation regime . in this respect , it is interesting to have a look at the effect of onionization on the partial covariance matrices , and more generally at the effect on the total covariance matrices of all procedures presented in this section : these are shown in tables  [ tab : covxseonion ] and  [ tab : covxse ] , respectively .",
    "in particular , note in table  [ tab : covxseonion ] that the onionization procedure ( but the same is true for minimizations ) affects correlations for the bkgd  and lumi  error sources in exactly the same way without distinctions .",
    "if this was a real combination , instead , one would most likely keep the lumi  correlation unchanged ( because a common luminosity measurement would indeed result in a 100% correlation between @xmath69 , @xmath68 and @xmath70 , and these three measurements together could even help to constrain the error on it ) , concentrating instead on the re - assessment of the bkgd  correlation alone ( because the initial `` nominal '' estimate of 100% correlation is neither conservative nor realistic in the presence of different sensitivities to differential distributions ) .",
    "it should finally be added that the total covariance matrix derived from the onionization prescription is used as the starting point of the `` byoffdiagelem '' minimization in the bluefin  software , as we have found this to improve the efficiency of the minimization procedure . as an additional cross - check of the onionization prescription",
    ", we also tested a fourth type of minimization , where information is independently minimized for each source of uncertainty as if this was the only one , varying each time only the correlations in that error source ( after removing those measurements not affected by it and slightly reducing the allowed correlation ranges to keep the partial covariance positive definite ) .",
    "the preliminary results of this test ( not included in table  [ tab : blue5xse ] ) indicate that these minimizations do not seem to significantly move partial covariances or the final result away from those obtained through the onionization prescription , which are used as a starting point also in this case .",
    "we conclude this section by reminding that the prescriptions presented here are only empirical recipes that assume no prior knowledge of the physics involved and , for this reason , can never represent valid substitutes for a careful quantitative analysis of correlations using real or simulated data . a precise estimate of correlations is important in general , but absolutely necessary in high correlation regimes , where it may be as important as a precise assessment of measurement errors themselves .",
    "combining many correlated measurements is a fundamental and unavoidable step in the scientific process to improve our knowledge about a physical quantity . in this paper",
    ", we recalled the relevance of the concept of fisher information to quantify and better understand this knowledge .",
    "we stressed that it is extremely important to understand how the information available from several measurements is effectively used in their combination , not only because this allows a fairer recognition of their relative merit in their contribution to the knowledge about the unknown parameter , but especially because this makes it possible to produce a more robust scientific result by critically reassessing the assumptions made in the combination .    in this context , we described how the correlations between the different measurements play a critical role in their combination .",
    "we demonstrated , in particular , that the presence of negative coefficients in the blue weighted average of any number of measurements is a sign of a `` high - correlation regime '' , where the effect of increasing correlations is that of reducing the error on the combined result .",
    "we showed that , in this regime , a large contribution to the combined knowledge about the parameter comes from the joint impact of several measurements through their correlation and we argued , as a consequence , that the merit for this particular contribution to information can not be claimed by any single measurement individually . in particular , we presented our objections to the standard practice of presenting the `` relative importances '' of different measurements based on the absolute values of their blue coefficients , and we proposed the use of ( `` intrinsic '' and `` marginal '' ) `` information weights '' instead .",
    "in the second part of the paper , we questioned under which circumstances assuming systematic errors as fully correlated can be considered a `` conservative '' procedure .",
    "we proposed the use of information derivatives with respect to inter - measurement correlations as a tool to identify those `` nominal '' correlations for which this assumption is wrong and a more careful evaluation is necessary .",
    "we also suggested a few procedures for trying to make a combination more `` conservative '' when a precise estimate of correlations is simply impossible .",
    "we should finally note that blue combinations are not the only way to combine different measurements , but they are actually the simplest to understand when combinations are performed under the most favorable assumptions that measurements are multivariate gaussian distributed with covariances known a priori , as in this case all relevant quantities become easily calculable by matrix algebra .",
    "we therefore stress that , while the results in this paper were obtained under these assumptions and using the blue technique , large positive correlations are guaranteed to have a big impact , and should be watched out for , also in combinations performed with other methods or under other assumptions .",
    "this work has been inspired by many discussions , during private and public meetings , on the need for critically reviewing the assumptions about correlations and the meaning of `` weights '' , when combining several measurements in the presence of high correlations between them .",
    "it would be difficult to mention and acknowledge all those colleagues who have hinted us towards the right direction and with whom we have had very fruitful discussions .",
    "we are particularly grateful to the members of the toplhcwg and to the atlas and cms members who have helped in the reviews of the recent top mass combinations at the lhc .",
    "we would also like to thank our colleagues who have sent us comments about the first two public versions of this paper .",
    "in particular , it is a pleasure to thank louis lyons for his extensive feedback and his very useful suggestions .",
    "we are also grateful to the epjc referees for their detailed and insightful comments , as well as for making us aware of the research presented in ref .",
    "@xcite .",
    "finally , a.v .",
    "would like to thank the management of the cern it - es and it - sdc groups for allowing him the flexibility to work on this research alongside his other committments in computing support for the lhc experiments .",
    "the aleph , delphi , l3 and opal collaborations and the lep electroweak working group , _ electroweak measurements in electron - positron collisions at w - boson - pair energies at lep _",
    "rep . 532 ( 2013 ) 119 the tevatron electroweak working group for the cdf and d0 collaborations , _ combination of cdf and do results on the mass of the top quark using up to 5.8  fb@xmath71 of data _ , arxiv:1107.5255v3 ( 2011 )"
  ],
  "abstract_text": [
    "<S> we discuss the effect of large positive correlations in the combinations of several measurements of a single physical quantity using the best linear unbiased estimate ( blue ) method . </S>",
    "<S> we suggest a new approach for comparing the relative weights of the different measurements in their contributions to the combined knowledge about the unknown parameter , using the well - established concept of fisher information . </S>",
    "<S> we argue , in particular , that one contribution to information comes from the collective interplay of the measurements through their correlations and that this contribution can not be attributed to any of the individual measurements alone . </S>",
    "<S> we show that negative coefficients in the blue weighted average invariably indicate the presence of a regime of high correlations , where the effect of further increasing some of these correlations is that of reducing the error on the combined estimate . in these regimes </S>",
    "<S> , we stress that assuming fully correlated systematic uncertainties is not a truly conservative choice , and that the correlations provided as input to blue combinations need to be assessed with extreme care instead . in situations where the precise evaluation of these correlations is impractical , or even impossible </S>",
    "<S> , we provide tools to help experimental physicists perform more conservative combinations . </S>"
  ]
}