{
  "article_text": [
    "suppose that we observe @xmath1 where @xmath2 is a @xmath3-dimensional predictor and @xmath4 is the response variable .",
    "we consider a standard linear model for each of @xmath5 observations @xmath6 with e@xmath7 and var@xmath8 .",
    "we also assume the predictors are standardized and the response variable is centered , @xmath9    with the dramatic increase in the amount of data collected in many fields comes a corresponding increase in the number of predictors @xmath3 available in data analyses . for simpler interpretation of the underlying processes generating the data ,",
    "it is often desired to have a relatively parsimonious model .",
    "it is often a challenge to identify important predictors out of the many that are available .",
    "this becomes more so when the predictors are correlated .",
    "as a motivating example , consider a study involving near infrared ( nir ) spectroscopy data measurements of cookie dough @xcite .",
    "near infrared reflectance spectral measurements were made at 700 wavelengths from 1100 to 2498 nanometers ( nm ) in steps of 2 nm for each of 72 cookie doughs made with a standard recipe .",
    "the study aims to predict dough chemical composition using the spectral characteristics of nir reflectance wavelength measurements . here , the number of wavelengths @xmath3 is much bigger than the sample size @xmath5 .",
    "many methods have been developed to address this issue of high dimensionality .",
    "section [ sect : review ] contains a brief review .",
    "most of these methods involve minimizing an objective function , like the negative log - likelihood , subject to certain constraints , and the methods in section [ sect : review ] mainly differ in the constraints used .    in this paper , we propose a variable selection procedure that can cluster predictors using the positive correlation structure and is also applicable to data with @xmath10 .",
    "the constraints we use balance between an @xmath0 norm of the coefficients and an @xmath0 norm for pairwise differences of the coefficients .",
    "we call this procedure a _ hexagonal operator for regression with shrinkage and equality selection _ , horses for short , because the constraint region can be represented by a hexagon",
    ". the hexagonal shape of the constraint region focuses selection of groups of predictors that are positively correlated .",
    "the goal is to obtain a homogeneous subgroup structure within the high dimensional predictor space .",
    "this grouping is done by focusing on spatial and/or positive correlation in the predictors , similar to supervised clustering .",
    "the benefits of our procedure are a combination of variance reduction and higher predictive power .",
    "the remainder of the paper is organized as follows .",
    "we introduce the horses procedure and its geometric interpretation in section [ sect : model ] .",
    "we provide an overview of some other methods in section [ sect : review ] , relating our procedure with some of these methods . in section [",
    "sect : compute ] we describe the computational algorithm that we constructed to apply horses to data and address the issue of selection of the tuning parameters .",
    "a simulation study is presented in section [ sect : simstudy ] .",
    "two data analyses using horses are presented in section [ sect : analysis ] .",
    "we conclude the paper with discussion in section [ sect : conclusion ] .",
    "in this section we describe our method for variable selection for regression with _ positively _ correlated predictors .",
    "our penalty terms involve a linear combination of an @xmath0 penalty for the coefficients and another @xmath0 penalty for pairwise differences of coefficients .",
    "computation is done by solving a constrained least - squares problem .",
    "specifically , estimates for the horses procedure are obtained by solving @xmath11 with @xmath12 and @xmath13 is a thresholding parameter .    .",
    "[ fig : region1 ]    as we describe in section [ sect : review ] , some methods like elastic net and oscar can group correlated predictors , but they can also put negatively correlated predictors into the same group .",
    "our method s novelty is its grouping of _ positively _ correlated predictors in addition to achieving a sparsity solution .",
    "figure [ fig : region1](c ) shows the hexagonal shape of the constraint region induced by ( [ eqn : horses ] ) , showing schematically the tendency of the procedure to equalize coefficients only in the direction of @xmath14 .",
    "the lower bound @xmath15 of @xmath16 prevents the estimates from being a solution only via the second penalty function , so the horses method always achieves sparsity .",
    "we recommend @xmath17 , where @xmath3 is the number of predictors .",
    "this ensures that the constraint parameter region lies between that of the @xmath0 norm and of the elastic net method , i.e.  the set of possible estimates for the horses procedure is a subset of that of elastic net . in other words ,",
    "horses accounts for positive correlations up to the level of elastic net . with @xmath18 ,",
    "the horses parameter region lies within that of the oscar method .",
    "[ fig : likelihood ]    in a graphical representation in the @xmath19 plane , the solution is the first time the contours of the sum of squares loss function hit the constraint regions .",
    "figure 2 gives a schematic view .",
    "figure [ fig : horses2 ] shows the solution for horses when there is negative correlation between predictors .",
    "horses treats them separately by making @xmath20 . on the other hand ,",
    "horses yields @xmath21 when predictors are positively correlated , as in figure [ fig : horses3 ] .",
    "the following theorem shows that horses has the exact grouping property . as the correlation between two predictors increases , the predictors are more likely to be grouped together .",
    "our proof follows closely the proof of theorem 1 in @xcite and is hence relegated to an appendix .",
    "let @xmath22 and @xmath23 be the two tuning parameters in the horses criterion . given data @xmath24 with centered response @xmath25 and standardized predictors @xmath26 , let @xmath27 be the horses estimate using the tuning parameters @xmath28 .",
    "let @xmath29 be the sample correlation between covariates @xmath30 and @xmath31 .",
    "for a given pair of predictors @xmath32 and @xmath33 , suppose that both @xmath34 and @xmath35 are distinct from the other @xmath36 .",
    "then there exists @xmath37 such that if @xmath38 then @xmath39$.}\\ ] ] furthermore , it must be that @xmath40    the strength with which the predictors are grouped is controlled by @xmath41 .",
    "if @xmath41 is increased , any two coefficients are morely likely to be equal .",
    "when @xmath42 and @xmath43 are positively correlated , theorem 1 implies that predictors @xmath44 and @xmath45 will be grouped and their coefficient estimates almost identical .",
    "this brief review can not do justice to the many variable selection methods that have been developed .",
    "we highlight several of them , especially those that have links to our horses procedure .",
    "while variable selection in regression is an increasingly important problem , it is also very challenging , particularly when there is a large number of highly correlated predictors . since the important contribution of the least absolute shrinkage and selection operator ( lasso ) method by @xcite ,",
    "many other methods based on regularized or penalized regression have been proposed for parsimonious model selection , particularly in high dimensions , e.g. elastic net , fused lasso , oscar and group pursuit methods @xcite .",
    "briefly , these methods involve penalization to fit a model to data , resulting in shrinkage of the estimators .",
    "many methods have focused on addressing various possible shortcomings of the lasso method , for instance when there is dependence or collinearity between predictors .    in the lasso ,",
    "a bound is imposed on the sum of the absolute values of the coefficients : @xmath46 where @xmath47 and @xmath48 .",
    "the lasso method is a shrinkage method like ridge regression @xcite , with automatic variable selection . due to the nature of the @xmath0 penalty term",
    ", lasso shrinks each coefficient and selects variables simultaneously .",
    "however , a major drawback of lasso is that if there exists collinearity among a subset of the predictors , it usually only selects one to represent the entire collinear group .",
    "furthermore , lasso can not select more than @xmath5 variables when @xmath10 .",
    "one possible approach is to cluster predictors based on the correlation structure and to use averages of the predictors in each cluster as new predictors .",
    "@xcite used this approach for gene expression data analysis and introduce the concept of a _ super gene_. however , it is sometimes desirable to keep all relevant predictors separate while achieving better predictive performance , rather than to use an average of the predictors .",
    "the hierarchical clustering used in @xcite for grouping does not account for the correlation structure of the predictors .",
    "other penalized regression methods have also been proposed for grouped predictors @xcite .",
    "all these methods except group pursuit work by introducing a new penalty term in addition to the @xmath0 penalty term of lasso to account for correlation structure .",
    "for example , based on the fact that ridge regression tends to shrink the correlated predictors toward each other , elastic net @xcite uses a linear combination of ridge and lasso penalties for group predictor selection and can be computed by solving the following constrained least squares optimization problem , @xmath49 the second term forces highly correlated predictors to be averaged while the first term leads to a sparse solution of these averaged predictors .",
    "@xcite proposed oscar ( octagonal shrinkage and clustering algorithm for regression ) , which is defined by @xmath50 by using a pairwise @xmath51 norm as the second penalty term , oscar encourages equality of coefficients .",
    "the constraint region for the oscar procedure is represented by an octagon ( see figure [ fig : region1 ] ) . unlike the hexagonal shape of the horses procedure , the octagonal shape of the constraint region allows for grouping of negatively as well as positively correlated predictors .",
    "while this is not necessarily an undesirable property , there may be instances when a separation of positively and negatively correlated predictors is preferred .    unlike elastic net and oscar , fused lasso @xcite",
    "was introduced to account for _ spatial _ correlation of predictors .",
    "a key assumption in fused lasso is that the predictors have a certain type of ordering .",
    "fused lasso solves @xmath52 the second constraint , called a _ fusion penalty _ , encourages sparsity in the differences of coefficients .",
    "the method can theoretically be extended to multivariate data , although with a corresponding increase in computational requirements .",
    "note that the fused lasso signal approximator ( flsa ) in @xcite can be considered as a special case of horses with design matrix @xmath53 .",
    "we also want to point out that our penalty function is a convex combination of the @xmath0 norm of the coefficients and the @xmath0 norm of the pairwise differenc es of coefficients .",
    "therefore , it is not a straightforward extension of fused lasso in which each penalty function is constrained separately .",
    "@xcite extended fused lasso by considering all possible pairwise differences and called it clustered lasso .",
    "however , the constraint region of clustered lasso does not have a hexagonal shape . as a result , clustered lasso",
    "does not have the _ exact _ grouping property of oscar .",
    "consequently , @xcite suggested to use a data - argumentation modification such as elastic net to achieve exact grouping .",
    "finally , the group pursuit method of @xcite is a kind of supervised clustering . with a regularization parameter @xmath54 and a threshold parameter @xmath41",
    ", they define @xmath55 and estimate @xmath56 using @xmath57    horses is a hybrid of the group pursuit and fused lasso methods and addresses some limitations of the various methods described above . for example",
    ", oscar can not handle the high - dimensional data while elastic net does not have the exact grouping property .",
    "a crucial component of any variable selection procedure is an efficient algorithm for its implementation . in this section",
    "we describe how we developed such an algorithm for the horses procedure .",
    "the matlab code for this algorithm is available upon request .",
    "we also discuss here the choice of optimal tuning parameters for the algorithm .      solving the equations for the horses procedure ( [ eqn : horses ] )",
    "is equivalent to solving its lagrangian counterpart @xmath58 where @xmath59 and @xmath60 with @xmath61 .    to solve ( [ lagr_obj ] ) to obtain estimates for the horses procedure , we modify the pathwise coordinate descent algorithm of @xcite .",
    "the pathwise coordinate descent algorithm is an adaptation of the coordinate - wise descent algorithm for solving the 2-dimensional fused lasso problem with a non - separable penalty ( objective ) function .",
    "our extension involves modifying the pathwise coordinate descent algorithm to solve the regression problem with a fusion penalty .",
    "as shown in @xcite , the proposed algorithm is much faster than a general quadratic program solver .",
    "furthermore , it allows the horses procedure to run in situations where @xmath62 .",
    "our modified pathwise coordinate descent algorithm has two steps , the descent and the fusion steps . in the descent step ,",
    "we run an ordinary coordinate - wise descent procedure to sequentially update each parameter @xmath63 given the others .",
    "the fusion step is considered when the descent step fails to improve the objective function . in the fusion step , we add an equality constraint on pairs of @xmath63s to take into account potential fusions and do the descent step along with the constraint . in other words ,",
    "the fusion step moves given pairs of parameters together under equality constraints to improve the objective function .",
    "the details of the algorithm are as follows :    * descent step : + the derivative of ( [ lagr_obj ] ) with respect to @xmath63 given @xmath64 , @xmath65 , is @xmath66 where the @xmath67 s are current estimates of the @xmath68 s and @xmath69 is a subgradient of @xmath70 .",
    "the derivative ( [ eqn : deriv_obj ] ) is piecewise linear in @xmath63 with breaks at @xmath71 unless @xmath72 . *",
    "* if there exists a solution to @xmath73 , we can find an interval @xmath74 which contains it , and further show that the solution is @xmath75 where @xmath76 , and @xmath77 . *",
    "* if there is no solution to @xmath73 , we let @xmath78 * fusion step : + if the descent step fails to improve the objective function @xmath79 , we consider the fusion of pairs of @xmath63s . for every single pair @xmath80 , we consider the equality constraint @xmath81 and try a descent move in @xmath82 .",
    "the derivative of ( [ lagr_obj ] ) with respect to @xmath82 becomes @xmath83 where @xmath84 .",
    "if the optimal value of @xmath82 obtained from the descent step improves the objective function , we accept the move @xmath81 .",
    "estimation of the tuning parameters @xmath16 and @xmath54 used in the algorithm above is very important for its successful implementation , as it is for the other methods of penalized regression .",
    "several methods have been proposed in the literature , and any of these can be used to tune the parameters of the horses procedure . @xmath85-fold",
    "cross - validation ( cv ) randomly divides the data into @xmath85 roughly equally sized and disjoint subsets @xmath86 , @xmath87 ; @xmath88 .",
    "the cv error is defined by @xmath89 where @xmath90 is the estimate of @xmath68 for a given @xmath16 and @xmath54 using the data set without @xmath86 .",
    "generalized cross - validation ( gcv ) and bayesian information criterion ( bic ) @xcite are other popular methods .",
    "these are defined by @xmath91 where @xmath92 is the estimate of @xmath68 for a given @xmath16 and @xmath54 , @xmath93 is the degrees of freedom and @xmath94 here , the degrees of freedom is a measure of model complexity . to apply these methods , one must estimate the degrees of freedom @xcite . following @xcite for fused lasso",
    ", we use the number of distinct groups of non - zero regression coefficients as an estimate of the degrees of freedom .",
    "we numerically compare the performance of horses and several other penalized methods : ridge regression , lasso , elastic net , and oscar .",
    "we do this by generating data based on six models that differ on the number of data points @xmath5 , number of predictors @xmath3 , the correlation structure @xmath95 and the true values of the coefficients @xmath56 .",
    "the parameters for these six models are given in table [ table : models ] .",
    ".parameters for the models used in the simulation study . [ cols=\"^,^,^,^,^,<\",options=\"header \" , ]     we analyze the data with the horses and oscar procedures and report the results in table 4 .",
    "although oscar and horses use the same definition of df , the oscar procedure groups predictors based on the _ absolute _ values of the coefficients . therefore the number of groups is not the same as the df in oscar . the results for lasso using 5-fold cross - validation and gcv can be found in @xcite .",
    "the 5-fold cross - validation oscar and horses solutions are similar .",
    "they select the exact same variables , but with slightly different coefficient estimates .",
    "since the sample size is only 20 and the number of predictors is 15 , the 5-fold cross - validation method may not be the best choice for selecting tuning parameters .",
    "however , using gcv , oscar and horses provide different answers .",
    "compared to the 5-fold cross - validation solutions , the oscar solution has one more predictor ( % base saturation ) while the horses solution has 3 additional predictors ( % base saturation , zinc , exchangeable acidity ) .",
    "more interestingly , in the oscar solution , % base saturation is not in the group measuring _ abundance of cations _ , while ph is .    on the other hand ,",
    "the % base saturation variable is included in the _ abundance of cations _ group .",
    "the horses solution also produces an additional group of variables consisting of phosophorus and ph .",
    "we proposed a new group variable selection procedure in regression that produces a sparse solution and also groups positively correlated variables together .",
    "we developed a modified pathwise coordinate optimization for applying the procedure to data .",
    "our algorithm is much faster than a quadratic program solver and can handle cases with @xmath96 .",
    "such a procedure is useful relative to other available methods in a number of ways .",
    "first , it selects groups of variables , rather than randomly selecting one variable in the group as the lasso method does .",
    "second , it groups positively correlated rather than both positively and negatively correlated variables .",
    "this can be useful when studying the mechanisms underlying a process , since the variables within each group behave similarly , and may indicate that they measure characteristics that affect a system through the same pathways .",
    "third , the penalty function used ensures that the positively correlated variables do not need to be spatially close .",
    "this is particularly relevant in applications where spatial contiguity is not the only indicator of functional relation , such as brain imaging or genetics .",
    "a simulation study comparing the horses procedure with ridge regression , lasso , elastic net and oscar methods over a variety of scenarios showed its superiority in terms of sparsity , effective grouping of predictors and mse .",
    "it is desirable to achieve a theoretical optimality such as the oracle property of @xcite in high dimensional cases .",
    "one possibility is to extend the idea of the adaptive elastic net @xcite to the horses procedure",
    ". then we may consider the following penalty form :    @xmath97    where @xmath98 are the adaptive data - driven weights .",
    "investigating theoretical properties of the above estimator will be a topic of future research .",
    "* proof of theorem 1 : *      suppose the covariates @xmath100 are ordered such that their corresponding coefficient estimates satisfy @xmath101 and @xmath102 .",
    "let @xmath103 denote the @xmath104 unique nonzero values of the set of @xmath105 , so that @xmath106 . for each @xmath107 ,",
    "let @xmath108 denote the set of indices of the covariates whose estimates of regression coefficients are @xmath109 .",
    "let also @xmath110 be the number of elements in the set @xmath111    suppose that @xmath112 and both are non - zero . in addtion , let assume @xmath113 and @xmath114 for @xmath115 without loss of generality .",
    "the differentiation of the objective function ( [ eqn : horses - obj ] ) with respect to @xmath63 gives @xmath116 where @xmath117 and @xmath118 , and @xmath119 in the same way , the differentiation of ( [ eqn : horses - obj ] ) with respect to @xmath120 is @xmath121 and we have , by taking their differences , @xmath122    since @xmath123 is standardized , @xmath124 .",
    "this together with the fact that @xmath125 gives @xmath126 however , we find that @xmath127 is always larger than or equal to @xmath128 . thus , if @xmath129 - equivalently , @xmath130 - then we encounter a contradiction",
    ".                        osborne , b.g .",
    ", fearn , t. , miller , a.r . ,",
    "and douglas , s. ( 1984 ) . application of near infrared reflectance spectroscopy to compositional analysis of biscuits and biscuit doughs .",
    "food agr . _",
    "* 35 * 99 - 105 ."
  ],
  "abstract_text": [
    "<S> identifying homogeneous subgroups of variables can be challenging in high dimensional data analysis with highly correlated predictors . </S>",
    "<S> we propose a new method called hexagonal operator for regression with shrinkage and equality selection , horses for short , that simultaneously selects positively correlated variables and identifies them as predictive clusters . </S>",
    "<S> this is achieved via a constrained least - squares problem with regularization that consists of a linear combination of an @xmath0 penalty for the coefficients and another @xmath0 penalty for pairwise differences of the coefficients . </S>",
    "<S> this specification of the penalty function encourages grouping of positively correlated predictors combined with a sparsity solution . </S>",
    "<S> we construct an efficient algorithm to implement the horses procedure . </S>",
    "<S> we show via simulation that the proposed method outperforms other variable selection methods in terms of prediction error and parsimony . </S>",
    "<S> the technique is demonstrated on two data sets , a small data set from analysis of soil in appalachia , and a high dimensional data set from a near infrared ( nir ) spectroscopy study , showing the flexibility of the methodology .    </S>",
    "<S> _ keywords and phrases : prediction ; regularization ; spatial correlation ; supervised clustering ; variable selection _ </S>"
  ]
}