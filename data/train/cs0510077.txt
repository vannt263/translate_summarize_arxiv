{
  "article_text": [
    "a dynamical network it is essential to keep track of the connection state information in order to ensure efficient transmission of data .",
    "this requires additional information , in the form a connection state overhead , to be sent through the network . for networks with rapid dynamics ( e.g.  mobile networks )",
    "this overhead may be large , and it is therefore of relevance to find some quantitative measure of the required bandwidth .    in this paper",
    "we study a simple model of a one - dimensional network introduced by dey @xcite , in which the links form identical , independent and time - homogeneous discrete - time markov processes in an open / closed - binary space . in this case",
    "the required connectivity information at a given node is simply the length of the path of open links in either direction .",
    "the ensuing connection state overhead is then quantified using information - theoretic methods .",
    "the relevant quantity is the smallest possible number of bits per second required for the connectivity overhead .",
    "our main result is a sequence of upper and lower bounds converging exponentially to this quantity , as well as a simple and efficient method for their computation .    to our knowledge  @xcite besides  @xcite is the only other work with the theme of quantifying the connection state overhead by information theory .",
    "the outline of the paper is as follows . in section  [ section : model ]",
    "we introduce the network model and the connection state variables .",
    "the overhead is quantified in section  [ section : entropy rate ] ; we also introduce a sequence of bounds for this quantity , derive an algorithm for their computation and show their exponential convergence towards the exact optimal overhead cost .",
    "the one - dimensional network is composed of nodes and links connecting neighbouring nodes .",
    "the nodes are labelled using the spatial variable @xmath0 .",
    "we choose @xmath1 to increase to the right ( see figure [ figure : network ] ) .",
    "the links are labelled using the index @xmath2 such that the link @xmath1 connects the nodes @xmath1 and @xmath3 .",
    "the dynamics of the network is described using a discretised time variable @xmath4 .",
    "the initial time is @xmath5 .",
    "the probability space @xmath6 contains elements @xmath7 of the form @xmath8 .",
    "the state of a link @xmath1 at time @xmath9 is described by the random variable @xmath10 which is by definition equal to @xmath11 ; `` @xmath12 '' stands for up or open , and `` @xmath13 '' for down or closed .",
    "we shall introduce a probability measure @xmath14 of @xmath15 on the @xmath16-field generated by the finite - dimensional cylindrical subsets of @xmath15 .",
    "all links @xmath17 are assumed to have identical and independent statistics : @xmath18 is a product over each @xmath0 .",
    "we now consider @xmath19 , i.e.  the time evolution of a single link @xmath1 .",
    "since all links @xmath1 have identical statistics , we consider only the the link @xmath20 and write @xmath21 .",
    "the time evolution of @xmath22 ( and consequently of @xmath23 ) is given by an autonomous   markov process .",
    "using the abbreviation @xmath24\\,,\\ ] ] where @xmath25 , the distribution of the markov process @xmath26 is determined by the transition matrix @xmath27[c]{,c / c , } p(1\\,|\\,1 )   &   p(1\\,|\\,0 )   \\\\",
    "p(0\\,|\\,1 )   &   p(0\\,|\\,0 )   \\end{ieeeeqnarraybox*}\\right )   \\;:=\\ ; \\left(\\begin{ieeeeqnarraybox*}[][c]{,c / c , } \\overline{d } &   u   \\\\             d   &   \\overline{u }",
    "\\end{ieeeeqnarraybox*}\\right ) \\,,\\ ] ] where @xmath28 are the free parameters of the model , and @xmath29 for any @xmath30 $ ] .",
    "thus @xmath31 ( resp .",
    "@xmath32 ) is the probability that a closed ( resp .",
    "open ) link is opened ( resp",
    ".  closed ) after one time step .",
    "the above markov chain has a steady state probability distribution on @xmath33 .",
    "for @xmath34 we have @xmath35\\,,\\ ] ] regardless of the initial condition @xmath36 . from above",
    "we get    @xmath37    thus @xmath38 ( resp .",
    "@xmath39 ) is the steady state probability of a link being up ( resp .  down ) .    for simplicity",
    "we assume that at time @xmath40 all the link variables @xmath41 are distributed according to the stationary distribution .",
    "autonomity of the links implies then that @xmath42 & \\;:=\\ ; u\\ , , \\\\ { \\mathrm{p}}[x_t(x )",
    "= 0 ] & \\;:=\\ ; d\\ , ,            \\end{split}\\ ] ] for all @xmath43 and @xmath4 .",
    "note that this restriction can always be lifted since all our results concern the limit @xmath44 .",
    "for any given initial distribution , conditions will hold with arbitrary accuracy for large enough times .",
    "these remarks define @xmath14 uniquely .",
    "figure [ figure : space - time diagram ] shows a space - time diagram of a typical evolution of the link variables .",
    "we make the following assumptions about the communication capabilities of the nodes .    *",
    "a node @xmath17 is able to send a one message to its left neighbour @xmath45 and another ( independent ) message to its right member @xmath46 at each time @xmath47 via links @xmath48 and @xmath17 respectively .",
    "* if the link @xmath17 is up at time @xmath47 , i.e. , @xmath49 , then the nodes @xmath17 and @xmath46 can receive the messages they have ( possibly ) sent to each others at the previous time @xmath50 . if the link @xmath17 is down at time @xmath47 then these messages are lost . however , the nodes @xmath51 and @xmath46 are able to observe that @xmath52 in this case . *",
    "if a node @xmath17 receives a message at time @xmath47 it may resend it immediately , i.e. , the destination neighbour is able to receive the message at the time @xmath53 provided the link between it and @xmath17 is up at @xmath47 .",
    "distant nodes are able to communicate by using the nodes between them as relays .",
    "we assume that when a link is open it forms a communication channel that has some finite transfer capacity .",
    "this last fact is not used for any calculations but is stated here to make the subsequent considerations meaningful .      in order to use efficient routing schemes",
    "it is important that a fresh connectivity status of each node is known at all times .",
    "since the network is linear the relevant information is , for each node @xmath17 , how far there exists an open path of links in both directions .",
    "because of the finite data propagation speed this connection state information can not be based on the current state of the network ; rather , it is extracted from the newest available data at @xmath17 on each link of the network .",
    "since the network model is symmetric with respect to reflection about @xmath17 and the states of the links on left and right of @xmath17 are independent we may restrict ourselves to the right direction only .",
    "the quantity @xmath54   and write @xmath55 for non - negative integers . ]",
    "expresses how many successive links are believed to be open on the right - hand side of node @xmath17 at time @xmath9 .",
    "a natural definition of @xmath56 in the light of the above remarks is then as follows .    at the initial time",
    "@xmath5 we set for all @xmath43 @xmath57 as time advances nodes transmit information to their neighbours according to the recursive scheme @xmath58\\,.\\ ] ] therefore @xmath59 which , by the independence of the links , has a stationary distribution @xmath60 \\;=\\ ; d\\ , u^m \\,.\\ ] ] note that   implies that the equality in   holds even without the limit whenever @xmath61 .",
    "because of translation symmetry , we restrict ourselves to the studying of the node @xmath20 and abbreviate @xmath62 . then becomes @xmath63 a glance at figure [ figure : space - time diagram 2 ] shows that the value of @xmath64 depends only on the link variables in the time - space - diagonal @xmath65 .    to simplify notation we re - index link states on the diagonals @xmath66 ,",
    "@xmath67 so that by   @xmath68 becomes a deterministic function of the infinite dimensional random vector @xmath69 .",
    "similarly , we define re - indexed messages on @xmath70 by setting @xmath71 so that @xmath72 and the recursion relation simplifies to @xmath73 \\,.\\ ] ]    note that the effect of the transformation of the variables @xmath74 is equivalent to setting the information propagation speed to infinity , as can be seen by comparing the recursion relations and . we may also consider a more general network model in which each link @xmath1 transmits with a certain ( constant ) speed @xmath75 , @xmath76 . by a similar variable transformation we can map this model to the infinite speed model in the @xmath77 variables .",
    "thus all following results are equally valid for such more general networks .",
    "the relevance of the value @xmath68 for the prediction of the true length of the open path for data sent at time @xmath47 depends on the parameters @xmath78 ( and of course @xmath79 ) .",
    "the entropy of a single link ( say @xmath20 , @xmath5 ) is @xmath80 where @xmath81 is the entropy functional on random variables and @xmath82\\,.\\ ] ]    the entropy rate of the process @xmath83 is by definition given by @xmath84 using the chain rule for entropy and markovity ( see @xcite for details ) we may write @xmath85 since we assumed that @xmath86 is distributed according to the stationary distribution , we get @xmath87 this may be easily evaluated to give @xmath88    in the following we shall also encounter markov chains @xmath89 defined by @xmath90 we therefore `` skip '' over @xmath91 links at each time step .",
    "the corresponding transition probabilities are characterised by the two off - diagonal elements of @xmath92 , denoted by    @xmath93\\ , , \\\\",
    "d_j & \\;:=\\ ; { \\mathrm{p}}[x_{t + j } = 0 \\ ; | \\ ; x_t = 1]\\ , .",
    "\\end{aligned}\\ ] ]    precisely as above , we find for the entropy rate of this process : @xmath94 where we used the fact that the stationary distribution of @xmath95 is the same as that of @xmath26 .",
    "we now quantify the optimal ( i.e.smallest possible ) cost of the connection state information overhead by the entropy rate of the stochastic process @xmath96 .",
    "this corresponds to the minimum amount of bits that need to be used on average to keep up to date on the number of consecutive up - links in the right direction from a fixed node @xmath1 ( for more details see for instance @xcite ) .",
    "the rate is @xmath97 where the second equality follows by applying the chain rule of entropy ( note that both limits exist since @xmath98 is an autonomous ergodic aperiodic process ; see @xcite for details ) .",
    "the evaluation of is tedious .",
    "a more practical approach is to compute lower and upper bounds that can be made as accurate as desired .",
    "define for @xmath99 @xmath100 it should not come as a surprise that @xmath101 ( resp.@xmath102 ) is an upper ( resp .  lower ) bound for @xmath103 that becomes arbitrarily accurate in the limit @xmath104 .",
    "this is the content of the following .",
    "[ lem : bounds ] the sequence @xmath105 is non - increasing and @xmath106 is nondecreasing .",
    "furthermore for all @xmath99 we have @xmath107 finally , @xmath108 for some constant @xmath109 .",
    "we omit the ( easy ) proof of monotonicity of the sequences as well as the fact that they are bounds for @xmath103 ( see for instance lemma 4.4.1 in  @xcite ) .",
    "the convergence of the bounds is postponed to theorem  [ thm : convergence ] , as it is easiest to prove using results from the following section .      in this section",
    "we derive the main result : a recursive algorithm for computing the bounds @xmath102 , @xmath101 and thus for approximating the exact entropy rate @xmath103 to an arbitrary accuracy .    for the proof it will be useful to rewrite the entropy by partitioning the probability space @xmath15 .",
    "let @xmath110 be an event .",
    "define @xmath111 as the entropy functional computed using the conditional probability measure @xmath112 $ ] .",
    "for two random variables @xmath113 we have , for example , @xmath114 \\",
    ", \\log { \\mathrm{p}}[x = x \\,|\\ , y = y , a]\\,.\\end{aligned}\\ ] ]    [ lemma : partitioning the entropy ] if @xmath115 lies in the @xmath16-field @xmath116 generated by @xmath117 , then @xmath118 \\ , { \\mathrm{h}}(x \\,|\\ , y \\,:\\ , a ) \\,\\\\ + \\ ,   & { \\mathrm{p}}[a^{\\mathrm{c } } ] \\ , { \\mathrm{h}}(x \\,|\\ , y \\,:\\ , a^{\\mathrm{c } } ) \\label{equation : partitioning the entropy }   \\,,\\end{aligned}\\ ] ] where @xmath119 is the indicator function of the event @xmath115 , and @xmath120 denotes the complement of the set @xmath115 .",
    "note that if @xmath121 the first term of vanishes .",
    "using the fact that @xmath122 is a deterministic function of @xmath123 as well as the chain rule we have @xmath124 the second term is equal to @xmath125 \\,\\notag\\\\   & { \\mspace{100 mu } } \\log { \\mathrm{p}}[x = x \\,|\\ , y = y , i_a = i ] \\notag \\\\ & = \\ ; - \\sum_{i \\in \\{0,1\\ } } { \\mathrm{p}}[i_a = i ] \\sum_{x , y } \\ ; { \\mathrm{p}}[x = x , y = y \\,|\\ , i_a = i ] \\ , \\notag\\\\ & { \\mspace{177 mu } } \\log { \\mathrm{p}}[x = x \\,|\\ , y = y , i_a = i ] \\notag \\\\ & = \\ , { \\mathrm{p}}[a ] \\ , { \\mathrm{h}}(x \\,|\\ , y \\,:\\ , a ) + { \\mathrm{p}}[a^{\\mathrm{c } } ] \\ , { \\mathrm{h}}(x \\,|\\ , y \\,:\\ , a^{\\mathrm{c } } ) \\,.\\end{aligned}\\ ] ]    we now introduce two sequences that will play a key role in the following . for @xmath126",
    "define @xmath127 \\,;\\ ] ] we also set @xmath128 .",
    "define furthermore the differences @xmath129 for @xmath99 .    in order to avoid writing explicit limits in the following",
    "we introduce the equivalence relation @xmath130 to denote asymptotic equality : @xmath131 means @xmath132 .",
    "[ thm : main result ] the sequence of bounds @xmath133 , @xmath134 can be computed recursively from    [ the recursion relation ] @xmath135\\ , ,   \\\\",
    "\\label{the recursion relation of upper bounds } \\mathscr{u}_{j+1 } & \\;=\\ ; \\mathscr{l}_j \\,+\\;\\frac{p_j}{d } \\",
    ", \\bigl [ { \\mathrm{h}}(x_1 ) - \\mathscr{h}\\bigl(x^{(j)}\\bigr)\\bigr ] \\ , , \\end{aligned}\\ ] ]    and    [ initial value of recursion ] @xmath136    note that the probabilities @xmath137 ( or , equivalently , the differences @xmath138 ) must still be computed ; this is done in appendix [ sect : calculation of p_j ] .",
    "everything else in the above expressions is known : @xmath139 was computed in , and @xmath140 .",
    "a direct consequence of the theorem is an expression for the exact entropy rate : from and we get @xmath141    we first introduce some notation .",
    "define the vector @xmath142 and the @xmath143-norm @xmath144 defined by @xmath145    the key idea of the proof is to partition the probability space @xmath146 , where @xmath147 and use lemma [ lemma : partitioning the entropy ] .",
    "some of the rigorous proofs of the intuitively plausible steps ( a - f ) are postponed to lemma [ lemma : proof of steps ] .",
    "we have @xmath148 \\ , { \\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j ) } , { \\underline{z } \\!\\,}_{t - j-1 } \\,:\\ , a}\\bigr ) } \\notag \\\\ & + \\,{\\mathrm{p}}[a^{\\mathrm{c } } ] \\ , { \\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j ) } , { \\underline{z } \\!\\,}_{t - j-1 } \\,:\\ ,",
    "a^{\\mathrm{c}}}\\bigr ) } \\notag \\\\ \\overset{\\text{(b)}}{=}\\;&{\\mathrm{h}}{\\bigl({i_a } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j ) } , { \\underline{z } \\!\\,}_{t - j-1}}\\bigr ) } \\notag \\\\ & + \\,{\\mathrm{p}}[a ] \\ , { \\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j ) } , { \\underline{z } \\!\\,}_{t - j-1 } \\,:\\ , a}\\bigr ) } \\notag \\\\ & + \\,{\\mathrm{p}}[a^{\\mathrm{c } } ] \\ , { \\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j-1 ) } , { \\underline{z } \\!\\,}_{t - j } \\,:\\ , a^{\\mathrm{c}}}\\bigr ) } \\notag \\\\ \\overset{\\text{(c)}}{=}\\;&{\\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j-1 ) } , { \\underline{z } \\!\\,}_{t - j}}\\bigr ) } \\notag \\\\ & + \\,{\\mathrm{h}}{\\bigl({i_a } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j ) } , { \\underline{z } \\!\\,}_{t - j-1}}\\bigr ) } \\,-\\,{\\mathrm{h}}{\\bigl({i_a } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j-1 ) } , { \\underline{z } \\!\\,}_{t - j}}\\bigr ) } \\notag \\\\ & + \\ , { \\mathrm{p}}[a ] \\ ,   \\begin{aligned}[t ] \\bigl[\\,&{\\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j ) } , { \\underline{z } \\!\\,}_{t - j-1 } \\,:\\ , a}\\bigr ) } \\\\ & -\\ , { \\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j-1 ) } , { \\underline{z } \\!\\,}_{t - j } \\,:\\ , a}\\bigr)}\\,\\bigr ] \\end{aligned } \\notag \\\\ \\overset{\\text{(d)}}{\\sim}\\;&\\mathscr{l}_j + 0 + { \\mathrm{p}}[a ] \\ , { \\bigl[{{\\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{z } \\!\\,}_{t - j-1}}\\bigr ) } -   { \\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{z } \\!\\,}_{t - j}}\\bigr)}}\\bigr ] } \\notag \\\\ \\overset{\\text{(f)}}{\\sim}\\;&\\mathscr{l}_j + \\frac{p_j}{d } \\ , \\bigl[\\mathscr{h}{\\bigl({x^{(j+1)}}\\bigr ) } -   \\mathscr{h}\\bigl(x^{(j)}\\bigr)\\bigr]\\ , , \\notag\\end{aligned}\\ ] ] where ( a ) follows from lemma [ lemma : partitioning the entropy ] ; ( b ) from lemma [ lemma : proof of steps ] ( i ) ; ( c ) from lemma [ lemma : partitioning the entropy ] applied to @xmath149 and @xmath150 ; ( d ) from lemma [ lemma : proof of steps ] ( ii),(iv),(v ) ; and ( f ) from lemma [ lem : first order bounds ] .    similarly , @xmath151 \\,+\\,&{\\mathrm{p}}[a ] \\ , { \\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j ) } \\,:\\ , a}\\bigr ) } \\\\ \\,+\\,&{\\mathrm{p}}[a^{\\mathrm{c } } ] \\ , { \\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j ) } \\,:\\ , a^{\\mathrm{c}}}\\bigr ) } \\end{aligned } \\notag \\\\ \\overset{\\text{(b)}}{=}\\;&{\\mathrm{h}}{\\bigl({i_a } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j)}}\\bigr ) } \\begin{aligned}[t ] \\,+\\ , & { \\mathrm{p}}[a ] \\ , { \\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j ) } \\,:\\ , a}\\bigr ) } \\\\",
    "\\,+\\ , & { \\mathrm{p}}[a^{\\mathrm{c } } ] \\ , { \\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j-1 ) } , { \\underline{z } \\!\\,}_{t - j } \\,:\\ , a^{\\mathrm{c}}}\\bigr ) } \\end{aligned } \\notag \\\\ \\overset{\\text{(c)}}{=}\\;&{\\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j-1 ) } , { \\underline{z } \\!\\,}_{t - j}}\\bigr ) } \\notag \\\\ & + \\,{\\mathrm{h}}{\\bigl({i_a } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j)}}\\bigr ) } \\,-\\,{\\mathrm{h}}{\\bigl({i_a } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j-1 ) } , { \\underline{z } \\!\\,}_{t - j}}\\bigr ) } \\notag \\\\ & + \\,{\\mathrm{p}}[a ] \\ ,",
    "\\begin{aligned}[t ] \\bigl[\\,&{\\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j ) } \\,:\\ , a}\\bigr ) } \\\\ & -\\,{\\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j-1 ) } , { \\underline{z } \\!\\,}_{t - j } \\,:\\ , a}\\bigr)}\\,\\bigr ] \\end{aligned } \\\\",
    "\\overset{\\text{(d)}}{\\sim}\\ ;   & \\mathscr{l}_j + 0 + { \\mathrm{p}}[a ] \\ , { \\bigl[{{\\mathrm{h}}{\\bigl({m_t}\\bigr ) } -   { \\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{z } \\!\\,}_{t - j}}\\bigr)}}\\bigr ] } \\\\ \\overset{\\text{(f)}}{\\sim}\\ ;   & \\mathscr{l}_j + \\frac{p_j}{d } \\ , { \\bigl[{{\\mathrm{h}}(x_1 ) -   \\mathscr{h}{\\bigl({x^{(j)}}\\bigr)}}\\bigr ] } \\,,\\end{aligned}\\ ] ] where ( a ) follows from lemma [ lemma : partitioning the entropy ] ; ( b ) from lemma [ lemma : proof of steps ] ( i ) ; ( c ) from lemma [ lemma : partitioning the entropy ] ; ( d ) from lemma [ lemma : proof of steps ] ( ii),(iii),(v ) ; and ( f ) from lemma [ lem : first order bounds ] .",
    "the initial values follow from lemma [ lem : first order bounds ] .",
    "[ lemma : proof of steps ] using the notation of the proof of theorem [ thm : main result ] , we have @xmath152 & = \\ , { \\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j ) } , { \\underline{z } \\!\\,}_{t - j-1 } \\,:\\ , a^{\\mathrm{c}}}\\bigr ) } \\\\ & = \\ , { \\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j-1 ) } , { \\underline{z } \\!\\,}_{t - j } \\,:\\ , a^{\\mathrm{c}}}\\bigr)}\\ , , \\end{aligned } \\\\",
    "\\text{(ii ) } \\;\\;\\ ; & { \\mathrm{h}}{\\bigl({i_a } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j)}}\\bigr ) } \\begin{aligned}[t ] & = \\,{\\mathrm{h}}{\\bigl({i_a } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j ) } , { \\underline{z } \\!\\,}_{t - j-1}}\\bigr ) }   \\\\ & = \\ , { \\mathrm{h}}{\\bigl({i_a } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j-1 ) } , { \\underline{z } \\!\\,}_{t - j}}\\bigr)}\\ , , \\end{aligned } \\\\",
    "\\text{(iii ) } \\;\\;\\ ; & { \\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t-1}^{(j ) } \\,:\\ , a}\\bigr ) } \\,\\sim\\ , { \\mathrm{h}}(m_t)\\ , , \\\\",
    "\\text{(iv ) } \\;\\;\\ ; & { \\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j ) } , { \\underline{z } \\!\\,}_{t - j-1 } \\,:\\ ,",
    "a}\\bigr ) } \\,\\sim\\ , { \\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{z } \\!\\,}_{t - j-1}}\\bigr ) } \\ , , \\\\ \\text{(v ) } \\;\\;\\ ; & { \\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{m } \\!\\,}_{t - 1}^{(j-1 ) } , { \\underline{z } \\!\\,}_{t - j } \\,:\\ ,",
    "a}\\bigr ) } \\,\\sim\\ , { \\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{z } \\!\\,}_{t - j}}\\bigr ) } \\,.\\end{aligned}\\ ] ]    the proof of lemma [ lemma : proof of steps ] is banished to appendix [ app : proof of steps ] . to complete the proof of theorem [ thm : main result ]",
    "we still need the first order bounds @xmath153 , @xmath154 .",
    "[ lem : first order bounds ] for any @xmath99 we have @xmath155 in particular , @xmath156 . furthermore , @xmath157 so that @xmath158 .    by the recursion relation we have @xmath159}$ ] . since @xmath160 we have by bijectivity and the chain rule @xmath161 } } \\,\\big|\\ , { { \\underline{z } \\!\\,}_{t - j}}\\bigr ) }",
    "\\\\ & = \\;{\\mathrm{h}}{\\bigl({z_t(1 ) } \\,\\big|\\ , { { \\underline{z } \\!\\,}_{t - j}}\\bigr ) } + { \\mathrm{h}}{\\bigl({z_t(1 ) \\cdot { [ { \\,{\\widetilde{m}}_t(2 ) + 1\\ , } ] } } \\,\\big|\\ , { z_t(1),{\\underline{z } \\!\\,}_{t - j}}\\bigr)}\\\\ & = \\;\\mathscr{h } { \\bigl({x^{(j)}}\\bigr ) } \\,+\\,{\\mathrm{p}}{\\bigl[{z_t(1 ) = 1}\\bigr ] }   \\\\ & { \\mspace{40 mu}}\\cdot\\ , { \\mathrm{h}}{\\bigl({z_t(1 ) \\cdot { [ { \\,{\\widetilde{m}}_t(2 ) + 1\\ , } ] } } \\,\\big|\\ , { { \\underline{z } \\!\\,}_{t - j } \\,:\\ , { \\{{z_t(1 ) = 1}\\}}}\\bigr ) }   \\\\ & { \\mspace{25 mu}}+\\,{\\mathrm{p}}{\\bigl[{z_t(1 ) = 0}\\bigr ] } \\\\   & { \\mspace{40 mu}}\\cdot\\ , { \\mathrm{h}}{\\bigl({z_t(1 ) \\cdot { [ { \\,{\\widetilde{m}}_t(2 ) + 1\\ , } ] } } \\,\\big|\\ , { { \\underline{z } \\!\\,}_{t - j } \\,:\\ , { \\{{z_t(1 ) = 0}\\}}}\\bigr ) } \\\\ & = \\ ; \\mathscr{h } { \\bigl({x^{(j)}}\\bigr ) } + u \\ , { \\mathrm{h}}{\\bigl({{\\widetilde{m}}_t(2 ) }",
    "\\,\\big|\\ , { { \\underline{z } \\!\\,}_{t - j } \\,:\\ , { \\{{z_t(1 ) = 1}\\}}}\\bigr ) } \\\\ & = \\ ; \\mathscr{h } { \\bigl({x^{(j)}}\\bigr ) } + u \\ , { \\mathrm{h}}{\\bigl({{\\widetilde{m}}_t(2 ) }",
    "\\,\\big|\\ , { { \\underline{z } \\!\\,}_{t - j}}\\bigr)}\\,,\\end{aligned}\\ ] ] where the last step follows from the fact @xmath162 is independent of @xmath163 .",
    "using translation invariance we therefore get @xmath164 and follows .",
    "furthermore from @xmath165 \\sim u^m \\ , d $ ] we get @xmath166 where the function @xmath167 is defined in  .",
    "we now address the convergence of the bounds , thus completing the proof of lemma [ lem : bounds ] .",
    "[ thm : convergence ] for any @xmath168 there exists a constant @xmath169 such that @xmath170    we start with three auxiliary results .",
    "first , we notice that the eigenvalues of the single link transition matrix @xmath171 in   are @xmath172 , and since @xmath173 the limit @xmath174 exists .",
    "( this is just a restatement that the @xmath26 has a unique stationary distribution . )",
    "the convergence is exponentially fast , i.e. , @xmath175 where @xmath176 is a matrix norm and @xmath177 is some finite constant .",
    "second , the smooth function @xmath178 on @xmath179 matrices @xmath180 , defined by @xmath181 is lipschitz continuous on closed subdomains .",
    "in particular , for all @xmath182 holds @xmath183 provided that @xmath184 is small enough that the closure of the ball @xmath185 is contained in @xmath186 , and the finite constant @xmath187 is large enough .",
    "third , by a direct calculation we see that @xmath178 satisfies @xmath188    therefore , by expressing the difference of the recursion relations   and   with these identities and using the trivial bound @xmath189 , we get @xmath190 } \\notag \\\\ & \\leq\\ ; \\frac{1}{d } \\ , { \\bigl [ { g(t^j ) - g(t_\\ast)}\\bigr]}\\ , .",
    "\\label{first convergence estimate}\\end{aligned}\\ ] ] if @xmath191 is large enough the estimates   and   can be combined to yield @xmath192 which together with   completes the proof .",
    "finally some remarks about convergence . from the theorem",
    "it is clear that if @xmath193 the convergence is fast . indeed ,",
    "if @xmath194 the first order terms @xmath195 are exact .",
    "this can also be seen directly : we have @xmath196 , @xmath197 , so that @xmath198 and therefore @xmath199 . on the other hand , the convergence becomes slower if @xmath200 or @xmath201 .",
    "the limiting case @xmath202 corresponds to a static network and @xmath203 is physically meaningless , which is also why we excluded both cases from our discussion .",
    "in a dynamic network information about connectivity must be sent through the network regularly .",
    "this connection state overhead consumes the available bandwidth of the network .",
    "it is therefore natural to ask what is the smallest possible ( in the context of information theory ) bandwidth required for the connection state overhead . in this work",
    "we provide the answer in the special case of a simple linear network model : as a main result we have presented an exact and rapidly converging series expression for the best achievable overhead data rate .    we have only considered a linear network model .",
    "however , the results derived here are also applicable to the case of a tree with the connectivity information at each node being whether or not it is connected to the root , since this model is fully equivalent to the one - dimensional network .",
    "the generalisation of our results to linear networks with more general links that have a larger state space is probably possible by using the same or very similar techniques as here .",
    "however , the most interesting generalisations , such as more complex network topologies , seem to pose a far greater challenge .",
    "a `` brute force '' computation of @xmath204 is too complex to be of any practical use if @xmath205 .",
    "we present here a more convenient method .",
    "the result is a simple recursive algorithm for calculating @xmath206 .",
    "the probabilities @xmath204 can then be computed from @xmath207    for @xmath99 we have @xmath208 } \\notag \\\\   \\label{computation of r_j } \\;=\\ ;   & \\begin{aligned}[t ] \\sum_{m = 0}^\\infty \\;{\\mathrm{p}}{\\bigl[{m_t = m}\\bigr]}\\ , { \\mathrm{p}}\\bigl[\\ , & m_{t - j } \\geq m , m_{t - j + 1 } < m , \\cdots \\\\   & \\cdots , m_{t - 1 } < m\\ , \\big|\\ , m_t = m\\ , \\bigr ] \\ , .   \\end{aligned}\\end{aligned}\\ ] ] define the new random variable @xmath209 so that @xmath210 then we get from above @xmath211   { \\mathrm{p}}\\big[&z^{(m)}_{t - j } = 1 , z^{(m)}_{t - j + 1 } = \\dots = z^{(m)}_{t - 1 } = 0   \\big| \\\\      & z^{(m)}_t = 1 , z_t(m + 1 ) = 0 \\bigr ] \\ , { \\mathrm{p}}{\\bigl[{m_t = m}\\bigr]}\\end{aligned } \\notag \\\\ \\label{r as a sum over m } & \\;\\sim\\ ; d \\sum_{m = 0}^\\infty   \\ ; r_j^{(m)}\\ , u^m   \\,,\\end{aligned}\\ ] ] where we have used   and @xmath212 is the limit @xmath213 } \\,.\\ ] ] the above discussion is meaningless if @xmath214 ; from , however , we see that we must define @xmath215 for to hold .",
    "define now for @xmath99 @xmath216 } \\,.\\ ] ] for the following we note that the process obtained from @xmath83 by reversing the time is also a markov process with transition probabilities identical to those of @xmath83 ; for example @xmath217 } \\,=\\ , u$ ] .",
    "thus @xmath218    the recursion relation for @xmath219 arises as follows .",
    "we rewrite @xmath220 by decomposing the event @xmath221 : by successively conditioning on the values of @xmath222 , @xmath223 , we get @xmath224 where the sum means a union of disjoint events . taking the probability measure of both sides and using markovity",
    "then _ all _ of the relevant first @xmath225 links of @xmath226 are known ( to equal 1 ) . ] of the time - reversed @xmath83 process we have @xmath227 which gives @xmath228 this is the desired recursion relation expressing @xmath219 as a function of @xmath229 .",
    "using @xmath230 we may therefore find @xmath231 .",
    "we summarise the results :    the quantity @xmath138 , @xmath99 , may be computed from @xmath232 where @xmath231 , @xmath233 , satisfies the recursion relation    @xmath234    as an example , we compute @xmath235 , @xmath236 and @xmath237 : @xmath238 } \\ , ,",
    "\\\\ r_3 \\;=\\ ;   & d \\ , \\sum_{m = 0}^\\infty \\ ; { \\bigl({\\overline{d}_3^m -   2 \\ , \\overline{d}^m \\ , \\overline{d}_2^m + \\overline{d}^{3m}}\\bigr ) } u^m \\\\ \\;=\\ ;   & d \\ , { \\biggl[{\\frac{1}{1 - \\overline{d}_3 \\ , u } - \\frac{2}{1 - \\overline{d } \\ ,   \\overline{d}_2 \\ , u } + \\frac{1}{1 - \\overline{d}^3 \\ u}}\\biggr ] } \\,.\\end{aligned}\\ ] ]",
    "the proof involves deriving equalities for conditional probabilities .",
    "these then induce equalities of the conditional entropies according to the following lemma .",
    "[ lemma : prob - > entropy ] let @xmath113 be random variables , @xmath239 a function on the range of @xmath240 , and suppose that , for all @xmath241 , @xmath242 \\;=\\ ; { \\mathrm{p}}\\bigl[x = x \\,|\\ , y \\in \\phi^{-1 } { \\bigl({\\phi(y)}\\bigr)}\\bigr]\\,.\\ ] ] then @xmath243    the proof is based on writing out the definition of the conditional entropy @xmath244 , rewriting the sum @xmath245 as @xmath246 and using the assumption . we omit further details",
    "let us begin with ( i ) .",
    "the conditioning event is @xmath247 let @xmath248 and define @xmath249 let furthermore @xmath250 be chosen so that @xmath251 and @xmath252 , where @xmath253 is a deterministic function that gives @xmath68 as a function of @xmath254 .",
    "then we have , for @xmath255 and @xmath256 , @xmath257   \\\\ \\overset{\\text{(a)}}{=}\\ ;   { \\mathrm{p}}\\bigl[\\,&z_t(1 ) = \\dots= z_t(m ) = 1 , z_t(m + 1 ) = 0 \\;\\big| \\\\ & { \\underline{m } \\!\\,}_{t - 1}^{(j ) } = { \\underline{m } \\!\\ , } , { \\underline{z } \\!\\,}_{t -j - 1 } = { \\underline{z } \\!\\ , } , m_t \\leq { \\lvert \\overline{m } \\rvert } , \\\\ & z_{t - i({\\underline{m } \\!\\,})}(1 ) = \\dots = z_{t - i({\\underline{m } \\!\\,})}(1 ) = 1,\\\\ & z_{t - i({\\underline{m } \\!\\,})}({\\lvert { \\underline{m } \\!\\ , } \\rvert } + 1 ) = 0\\;\\bigr ] \\\\ \\overset{\\text{(b)}}{=}\\ ;   { \\mathrm{p}}\\bigl[\\,&z_t(1 ) = \\dots= z_t(m ) = 1 , z_t(m + 1 ) = 0 \\,\\big| \\notag \\\\ & { \\underline{m } \\!\\,}_{t - 1}^{(j ) } = { \\underline{m } \\!\\ , } , m_t \\leq { \\lvert { \\underline{m } \\!\\ , } \\rvert } , z_{t - i({\\underline{m } \\!\\,})}(1 ) = \\cdots \\\\ & \\dots = z_{t - i({\\underline{m } \\!\\,})}(1 ) = 1 , z_{t - i({\\underline{m } \\!\\,})}({\\lvert { \\underline{m } \\!\\ , } \\rvert } + 1 ) = 0\\,\\bigr ] \\notag \\\\ = \\;{\\mathrm{p}}\\bigl[\\,&m_t = m\\,\\big|\\ , { \\underline{m } \\!\\,}_{t - 1}^{(j ) } = { \\underline{m } \\!\\ , } , m_t \\leq { \\lvert { \\underline{m } \\!\\ , } \\rvert}\\,\\bigr ] \\notag \\\\ \\overset{\\text{(c)}}{= } \\ ;   { \\mathrm{p}}\\bigl[\\,&z_t(1 ) = \\dots= z_t(m ) = 1 , z_t(m + 1 ) = 0 \\,\\big|\\\\ & { \\underline{m } \\!\\,}_{t - 1}^{(j ) } = { \\underline{m } \\!\\ , } , m_t \\leq { \\lvert { \\underline{m } \\!\\ , } \\rvert } , { \\underline{z } \\!\\,}_{t - i({\\underline{m } \\!\\ , } ) } = { \\underline{z } \\!\\,}''\\,\\bigr ] \\\\ \\overset{\\text{(d)}}{= } \\ ; { \\mathrm{p}}\\bigl[\\,&z_t(1 ) = \\dots= z_t(m ) = 1 , z_t(m + 1 ) = 0 \\,\\big| \\\\ & { \\underline{m } \\!\\,}_{t - 1}^{(j ) } = { \\underline{m } \\!\\ , } , m_t \\leq { \\lvert { \\underline{m } \\!\\ , } \\rvert } , { \\underline{z } \\!\\,}_{t - i({\\underline{m } \\!\\ , } ) } = { \\underline{z } \\!\\ , } '' , { \\underline{z } \\!\\,}_{t - j } = { \\underline{z } \\!\\ , } ' \\,\\bigr ] \\\\",
    "\\overset{\\text{(e)}}{= } \\ ; { \\mathrm{p}}\\bigl[\\,&m_t = m \\,\\big|\\ , { \\underline{m } \\!\\,}_{t - 1}^{(j ) } = { \\underline{m } \\!\\ , } , m_t \\leq { \\lvert { \\underline{m } \\!\\ , } \\rvert } , { \\underline{z } \\!\\,}_{t - j } = { \\underline{z } \\!\\,}'\\,\\bigr ] \\notag \\\\ = \\ ; { \\mathrm{p}}\\bigl[\\,&m_t = m \\,\\big|\\ , { \\underline{m } \\!\\,}_{t - 1}^{(j - 1 ) } = { \\underline{m } \\!\\,}^{(j - 1 ) } , m_t \\leq { \\lvert { \\underline{m } \\!\\ , } \\rvert } , { \\underline{z } \\!\\,}_{t - j } = { \\underline{z } \\!\\,}'\\,\\bigr ] \\,,\\end{aligned}\\ ] ] where @xmath258 denotes the @xmath259 first components of @xmath260 ; ( a ) follows from rewriting the conditions @xmath261 , @xmath262 ; ( b ) from markovity , independence and the fact that @xmath263 ; ( c ) from independence and @xmath264 ; ( d ) from markovity ; and ( e ) from independence and @xmath265 .",
    "then the assertion follows from lemma [ lemma : prob - > entropy ] by choosing the functions @xmath266 , @xmath267 , and @xmath268 .    to prove ( ii ) choose @xmath260 , @xmath269 and @xmath270 as above and write @xmath271   \\\\ \\overset{\\text{(a)}}{=}\\ ;   { \\mathrm{p}}\\bigl[\\ , & z_t(1 ) = \\dots = z_t({\\lvert { \\underline{m } \\!\\ , } \\rvert } + 1 ) = 1 \\,\\big|\\,\\\\   & { \\underline{m } \\!\\,}_{t-1}^{(j ) } = { \\underline{m } \\!\\ , } , { \\underline{z } \\!\\,}_{t - j-1 } = { \\underline{z } \\!\\,}\\,\\bigr ] \\\\",
    "\\overset{\\text{(b)}}{=}\\ ;   { \\mathrm{p}}\\bigl[\\,&z_t(1 ) = \\dots = z_t({\\lvert { \\underline{m } \\!\\ , } \\rvert } + 1 ) = 1 \\,\\big|\\ ,   { \\underline{m } \\!\\,}_{t-1}^{(j ) } = { \\underline{m } \\!\\ , } \\,\\bigr ] \\\\",
    "\\overset{\\text{(c)}}{=}\\ ;   { \\mathrm{p}}\\bigl[\\,&z_t(1 ) = \\dots = z_t({\\lvert { \\underline{m } \\!\\ , } \\rvert } + 1 ) = 1 \\,\\big|\\,\\\\   & { \\underline{m } \\!\\,}_{t-1}^{(j ) } = { \\underline{m } \\!\\ , } , { \\underline{z } \\!\\,}_{t - j } = { \\underline{z } \\!\\ , } ' \\,\\bigr ] \\\\",
    "= \\ ; { \\mathrm{p}}\\bigl[\\,&m_t > { \\lvert { \\underline{m } \\!\\ , } \\rvert } \\,\\big|\\ ,   { \\underline{m } \\!\\,}_{t-1}^{(j-1 ) } = { \\underline{m } \\!\\,}^{(j-1 ) } , { \\underline{z } \\!\\,}_{t - j } = { \\underline{z } \\!\\ , } ' \\,\\bigr ] \\,,\\end{aligned}\\ ] ] where ( a ) follows from rewriting @xmath272 ; ( b ) and ( c ) from independence and markovity ( the full details are exactly as above using the index variable @xmath273 ) .",
    "the proofs of ( iii ) , ( iv ) and ( v ) are almost identical ; we only show ( iv ) .",
    "let @xmath225 , @xmath260 and @xmath269 be as above . first note that under the conditions @xmath274 and @xmath275 there is a bijective map between @xmath64 and @xmath276 : @xmath277 so that @xmath278 now @xmath279",
    "\\\\ \\overset{\\text{(a)}}{=}\\ ;   { \\mathrm{p}}\\bigl[\\,&{\\widetilde{m}}_t({\\lvert { \\underline{m } \\!\\ , } \\rvert } + 2 ) = m \\,\\big|\\ , { \\underline{m } \\!\\,}_{t - 1}^{(j ) } = { \\underline{m } \\!\\ , } , { \\underline{z } \\!\\,}_{t - j - 1 } = { \\underline{z } \\!\\,},z_t(1 ) = \\\\ & \\cdots= z_t({\\lvert { \\underline{m } \\!\\ , } \\rvert } + 1 ) = 1\\,\\bigr ] \\\\",
    "\\overset{\\text{(b)}}{=}\\ ; { \\mathrm{p}}\\bigl[\\,&{\\widetilde{m}}_t({\\lvert { \\underline{m } \\!\\ , } \\rvert } + 2 ) = m   \\,\\big|\\ , { \\underline{z } \\!\\,}_{t - j - 1 } = { \\underline{z } \\!\\,}\\,\\bigr ] \\,,\\end{aligned}\\ ] ] where ( a ) follows from rewriting the condition @xmath272 , and ( b ) from independence .",
    "now by lemma [ lemma : prob - > entropy ] and we get @xmath280 where the last step follows from translation invariance",
    ". therefore @xmath281 & { \\mathrm{p}}{\\bigl[{{\\underline{m } \\!\\,}_{t - 1}^{(j ) } = { \\underline{m } \\!\\ , } } \\,\\big|\\ , { a}\\bigr ] } \\\\",
    "\\cdot\\;&{\\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{z } \\!\\,}_{t - j - 1 } \\,:\\ , { \\bigl\\{{{\\underline{m } \\!\\,}_{t - 1}^{(j ) } = { \\underline{m } \\!\\ , } , m_t > { \\lvert { \\underline{m } \\!\\ , } \\rvert}}\\bigr\\}}}\\bigr ) } \\end{aligned } \\\\ & \\sim\\ ; { \\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{z } \\!\\,}_{t - j-1}}\\bigr ) } \\sum_{{\\underline{m } \\!\\ , } } \\ ; { \\mathrm{p}}{\\bigl[{{\\underline{m } \\!\\,}_{t - 1}^{(j ) } = { \\underline{m } \\!\\ , } } \\,\\big|\\ , { a}\\bigr ] } \\\\ & = \\ ; { \\mathrm{h}}{\\bigl({m_t } \\,\\big|\\ , { { \\underline{z } \\!\\,}_{t - j-1}}\\bigr)}\\,.\\end{aligned}\\ ] ]",
    "the authors are grateful for ari hottinen , antti kupiainen , paolo muratore - ginanneschi and olav tirkkonen for fruitful discussions during the preparation of this paper ."
  ],
  "abstract_text": [
    "<S> we consider a dynamical linear network where nearest neighbours communicate via links whose states form binary ( open / closed ) valued independent and identically distributed markov processes .    </S>",
    "<S> our main result is the tight information - theoretic lower bound on the network traffic required by the connection state overhead , or the information required for all nodes to know their connected neighbourhood .    these results , and especially their possible generalisations to more realistic network models , </S>",
    "<S> could give us valuable understanding of the unavoidable protocol overheads in rapidly changing ad hoc and sensor networks .    </S>",
    "<S> connection state overhead , dynamic linear network , exact series solution , entropy rate of an infinite dimensional hidden markov process . </S>"
  ]
}