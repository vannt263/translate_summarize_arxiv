{
  "article_text": [
    "it is desired that most events to be safe , stable or rather predictable . identifying an unusual event",
    "is a typical and vital topic in different fields , such as intrusion detection in cyber security , fraud detection for credit cards , insurance or health care and fault detection in safety critical systems @xcite .",
    "we may call this unusual event as outlier or abnormality which is different from a usual event .",
    "outlier is usually minor in a group of events / data while inlier is the majority .",
    "od refers to detect any abnormal element in data which is not consistent with an expected behavior @xcite .",
    "a good od method should be accurate to detect outlier and less erroneous judgment on inlier data .",
    "many od methods @xcite have been developed in recent years . in @xcite,@xcite ,",
    "nearest neighbor approach was proposed to consider the distance or the similarity between two data instances .",
    "the assumption behind the approach is that inliers in data should be dense and outlier(s ) is far from these dense inliers .",
    "nb classifiers were developed in the 1950s .",
    "the nb classifiers are constructed based on bayes theorem , and are widely used for text categorization with superior performance @xcite .",
    "another popular application is spam filtering among numerical emails @xcite .",
    "the kernel smooth nb method in this paper supposes the traffic data could be modeled by kernel distributions so that the bayes theorem is applied .",
    "in contrast , the gmm method presumed a mixture of gaussian distributions and confidence region is employed .",
    "gmm , firstly introduced by aitkin and wilson ( 1980 ) @xcite , is  an appropriate way of handle data with multiple outliers @xcite .",
    "the gmm method is useful in dealing with speech recognition @xcite .    for traffic data , detecting anomaly traffic event would be better to deal with the traffic problems",
    "this research aims at detecting outliers for a large - scale traffic database from hong kong .",
    "the original video was taken at a four - arm junction as shown in fig .",
    "[ fig1:a ] .",
    "the 4-arm junction can be expressed as an ideal map like fig .",
    "[ fig1:b ] . in total , the traffic data for 31 days was recorded with two sessions per day : am ( 07:00 - 10:00 ) and pm ( 17:00 - 20:00 ) . for each session , the original video data are dividing into 19 spatial temporal ( st ) signals .",
    "[ fig1:c ] demonstrates an sample of 4 normal st signals .",
    "these st signals have different number of traffic cycles because smoother traffic flow would lead to a shorter traffic cycle and result in more traffic cycles in one session .",
    "also , the st signals in each direction suffer a high degree of similarities ( fig .",
    "[ fig1:d ] ) among each other or across signals in different traffic direction .",
    "therefore , it requires a truncation process ( to standardize the length of a signal s cycles ) and a signal domain transformation ( to remedy the signal similarities ) .",
    "herein , each st signal was processed by principal component analysis ( pca ) @xcite to reduce the dimension of signal representation .",
    "then , an od method was carried out based on the pca - processed ( x , y)-coordinates plane .    0.5     0.5     0.5     0.5     in this paper , a detail investigation of nb and gmm classifiers in large - scale traffic data",
    "is carried out , for these two methods are newly applied in traffic od .",
    "experimental results demonstrate that the nb and gmm methods can achieve 93.78% and 94.50% detection success rates ( dsrs ) , respectively . these performance is comparable to previous evaluation on other od methods @xcite , @xcite including gaussian mixture model ( 80.86% ) , one - class svm ( 59.27% ) , s - estimator ( 76.20% ) and kernel density estimation ( 95.20% ) .",
    "the organization of this paper is as follows : section ii gives a review of related work about od .",
    "sections iii and iv present details of two proposed od methods and their experimental results , and section v concludes this paper .",
    "the popular od methods include statistical , nearest neighborhood , spectral approaches and learning based approaches .",
    "details about these methods are given as follows .",
    "statistical approach is one of the earliest approaches in od .",
    "it assumes that all inliers occur in a high probability region of this distribution model when outliers deviate strongly from the inliers .",
    "statistical od methods can perform parametric techniques like gmm or regression , or non - parametric techniques like histogram based or kernel function based .",
    "this approach would perform well if data distribution is assumed well . however , if the data distribution assumption is false , the result would be far away from the correct situation @xcite , @xcite .",
    "nearest neighbor approach considers the distance or the similarity between two data instances .",
    "the assumption behind the approach is that inliers in data should be dense and outlier(s ) is far from these dense inliers @xcite , @xcite .",
    "this approach can be easily realized in an unsupervised way that could require high computational complexity . as a result , it is hard to deal with very complex data @xcite .",
    "there are two major variant methods related to this approach , density measurement based methods , such as local outlier factor ( lof ) @xcite , influenced outlierness ( inflo ) @xcite and local outlier correlation integral ( loci ) @xcite , or distance measurement based methods , like mahalanobis distance @xcite , db(@xmath0)-outliers , outlier scoring based on k - nn distances @xcite , resolution - based outlier factor ( rof ) @xcite .",
    "spectral approach suggests that inliers and outliers could appear significantly different in a spectral domain .",
    "the approach detects outliers by embedding data into a lower dimensional subspace .",
    "the approach is useless if inliers and outliers in data are not separable in the lower dimensional subspace .",
    "in addition , it requires high computational complexity @xcite .",
    "learning approach is using various training methodologies @xcite ( or so - called trained machines ) to train the input data .",
    "the related od methods are neural networks , nave bayesian network , support vector machines ( svm ) .",
    "the assumption is that inliers and outliers in data can be easily distinguished by the trained machines .",
    "therefore , the machines can test and classify a test instance into either an inlier group or an outlier group in od @xcite .",
    "there are many algorithms that can be used in multi - learning class approach , and they have a faster testing phase than other approach .",
    "however , accurate labeling for various normal classes is often impossible @xcite .    in this paper , an investigation of nb classifier and gmm classifier to model the pca - processed ( x , y)-data and their performance for od in traffic data",
    "would be carried out .",
    "nb classifier was developed in 1950s for text retrieval @xcite .",
    "it is also widely used for spam filtering among numerical emails @xcite .",
    "nb classifier @xcite is developed based on bayes theorem which takes the form of @xmath1 based on the probability p(e),p(h ) , and the conditional probability p(eh ) , the posteriori probability p(he ) , which denotes the possibility of event h conditioned on an occurred event e , can be obtained .",
    "naive bayes classifiers are based on the information of the training data , and then determined the highest possible class of testing data from their information .",
    "kernel distribution is a non - parametric distribution to estimate each training point as some independent distributions into the whole distribution @xcite .",
    "@xmath2 kernel distribution is estimated by  sample data points with a kernel density .",
    "the commonly used kernels are listed as follows : + box kernel : @xmath3 triangle kernel : @xmath4 epanechnikov kernel : @xmath5 normal kernel : @xmath6 where _ x _ is replaced by _",
    "y _ for y - axis kernel .",
    "kernel smoothing nb ( ksnb ) classifier is a trained kernel distribution based classifier .",
    "different kernels with different widths for each predictor or class are available in ksnb classifier .",
    "we use the trained ksnb classifiers to detect outliers , where the classifiers would automatically set a bandwidth value for each feature and class which is optimal for a gaussian model .",
    "then , a corresponding region from contours surrounding various kernel distributions is formed an inlier region e , therefore any data out of that region is classified as an outlier . in this method , box ,",
    "triangle , epanechnikov and normal kernels are used for od .",
    "the od procedures based on ksnb classifier are given as follows : + fig .",
    "[ fignbflowchart ] illustrates the flowchart of the proposed nb based method , which includes a training stage and a testing stage .",
    "the details are as below .",
    "[ [ a .- training - stage ] ] a. training stage + + + + + + + + + + + + + + + + +    step 1 : feed training data ( all inliers ) into nb classifiers .",
    "+ step 2 : fit training data into the specified kernel distributions defined in eqns .",
    "( 3 - 6 ) .",
    "+ step 3 : combine each kernel distribution into a whole distribution .",
    "+ step 4 : set a bandwidth ( bib ) of the inlier region automatically from the kernel distribution .",
    "[ [ b .- testing - stage ] ] b. testing stage + + + + + + + + + + + + + + + +    step 1 : input testing data into the trained nb classifiers .",
    "+ step 2 : data label as outlier if the data is out of the inlier region .",
    "+ step 3 : output result .",
    "+ in addition , * algorithm [ nbalg ] * is provided below .",
    "the inliers data set @xmath7 and testing data set @xmath8 @xmath9 @xmath10 every data point @xmath11 @xmath12 @xmath13 @xmath14 @xmath15 @xmath16 @xmath17 in @xmath18 @xmath19 @xmath20 @xmath13      experimental results based on the kernel smoothing nb method are listed in table [ diffnbkernelres ] .",
    "accuracy ( acc . ) , positive predictive value ( ppv ) , negative predictive value ( npv ) , sensitivity ( sen . ) , false positive rate ( fpr ) are employed as measurement metrics and their definitions can be referred to @xcite . from the results , we can see that overall accuracy among all kernels is almost higher than * 90%*. overall npv among all kernels is higher than * 95% .",
    "*    .performance of the nb method among different kernels . [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     [ diffgmm ]",
    "in this paper , we present two od methods , kernel smoothing nb and gmm , to detect outliers in large - scale traffic data .",
    "the kernel smoothing nb method utilizes the trained kernel smoothing nb classifiers to detect outliers , in which the classifiers set the best bandwidth value with the inlier region .",
    "any data points out of that region are classified as outliers . in the gmm method ,",
    "the rectangular confidence region is formed in bonferroni adjustment way .",
    "true @xmath21  significant level region of the gmm can be constructed as the inlier region for the more accuracy .",
    "experimental results show that the two algorithms can achieve pleasing detection accuracies compared with the od methods in our previous studies @xcite , @xcite , including gaussian mixture model , one - class svm , s - estimator and kernel density estimation .",
    "this research is supported by hong kong rgc grf : 12201814 and hkbu frg2/14 - 15/075 .",
    "g.h . john and p.",
    "langley ,  estimating continuous distributions in bayesian classifiers , \" _ in : hanks , p. b. eds . ,",
    "11th conf . uncertainty in artificial intelligence _ , morgan kaufmann , san francisco , ca , pp .",
    "338 - 345 , 1995 .",
    "ngan , n.h.c .",
    "yung and a.g.o .",
    "yeh ,  a comparative study of outlier detection for large - scale traffic data by one - class svm and kernel density estimation , \" _",
    "is&t / spie electronic imaging _",
    ", 94050i-94050i-10 , 2015 ."
  ],
  "abstract_text": [
    "<S> it is meaningful to detect outliers in traffic data for traffic management . </S>",
    "<S> however , this is a massive task for people from large - scale database to distinguish outliers . in this paper </S>",
    "<S> , we present two methods : kernel smoothing nave bayes ( nb ) method and gaussian mixture model ( gmm ) method to automatically detect any hardware errors as well as abnormal traffic events in traffic data collected at a four - arm junction in hong kong . </S>",
    "<S> traffic data was recorded in a video format , and converted to spatial - temporal ( st ) traffic signals by statistics . </S>",
    "<S> the st signals are then projected to a two - dimensional ( 2d ) ( x , y)-coordinate plane by principal component analysis ( pca ) for dimension reduction . </S>",
    "<S> we assume that inlier data are normal distributed . </S>",
    "<S> as such , the nb and gmm methods are successfully applied in od ( outlier detection ) for traffic data . </S>",
    "<S> the kernel smooth nb method assumes the existence of kernel distributions in traffic data and uses bayes theorem to perform od . </S>",
    "<S> in contrast , the gmm method believes the traffic data is formed by the mixture of gaussian distributions and exploits confidence region for od . </S>",
    "<S> this paper would address the modeling of each method and evaluate their respective performances .  </S>",
    "<S> experimental results show that the nb algorithm with triangle kernel and gmm method achieve up to * 93.78% * and * 94.50% * </S>",
    "<S> accuracies , respectively . </S>"
  ]
}