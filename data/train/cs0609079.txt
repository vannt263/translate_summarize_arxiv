{
  "article_text": [
    "[ sec:1 ] * remark .",
    "* notation @xmath0 is equivalent to @xmath1 where @xmath2 } _ { n\\times 1 } \\ , & \\rho & = & \\rho_{ij } & = & \\underbrace { \\left [ \\begin{array}{c } \\rho_{1j } \\\\ \\vdots   \\\\ \\rho_{nj } \\\\ \\end{array } \\right ]   } _ { n \\times 1 } \\ . \\end{array}\\ ] ] notation @xmath3 is equivalent to @xmath4 where @xmath5}_{n \\times n } } \\ .",
    "\\end{array}\\ ] ] let us consider ( e.g. for @xmath6 ) variance of the difference @xmath7 of two random variables @xmath8 and @xmath9 , where @xmath10 , in terms of covariance @xmath11 introducing the estimation statistics @xmath12 @xmath13 in terms of correlation function @xmath14 @xmath15 if @xmath16 and @xmath17 or @xmath18 if @xmath19 and @xmath20 the unbiasedness constraint ( i condition ) @xmath21 is equivalent to @xmath22 the minimization constraint @xmath23 where @xmath24 produces @xmath25 equations in the @xmath26 unknowns : kriging weights @xmath27 and a lagrange parameter @xmath28 ( ii condition ) @xmath29}_{n\\times(n+1 ) } } &   \\cdot   & \\underbrace { \\left [ \\begin{array}{c } \\omega_j^1",
    "\\\\ \\vdots   \\\\ \\omega_j^n \\\\ \\mu_j \\\\ \\end{array } \\right ] } _ { ( n+1)\\times 1 } & = & \\underbrace { \\left [ \\begin{array}{c } \\rho_{1j } \\\\ \\vdots   \\\\ \\rho_{nj } \\\\ \\end{array } \\right ]   } _ { n \\times 1 }   \\end{array}\\ ] ] multiplied by @xmath30 @xmath31 and substituted into @xmath32 @xmath33 ^ 2\\}-\\underbrace{e^2\\{v_j-\\hat{v}_j\\}}_0 \\\\    & = & e\\{[(v_j - m)-(\\hat{v}_j - m)]^2\\ } \\\\    & = & e\\{[v_j - m]^2\\}-2(e\\{v_j\\hat{v}_j\\}-m^2)+e\\{[\\hat{v}_j - m]^2\\ } \\\\    & = & \\sigma^2 -2 \\sigma^2 |\\omega^i_j \\rho_{ij}|      + \\sigma^2 |\\omega^i_j \\rho_{ii } \\omega^i_j| \\\\     & = & \\sigma^2 \\pm 2 \\sigma^2 \\omega^i_j \\rho_{ij }     \\mp \\sigma^2 \\omega^i_j \\rho_{ii } \\omega^i_j   \\end{array}\\ ] ] give the minimized variance of the field @xmath8 under estimation @xmath34 ^ 2\\ } = \\sigma^2 ( 1 \\pm ( \\omega^i_j \\rho_{ij } + \\mu_j ) ) \\ ] ] and these two conditions produce @xmath26 equations in the @xmath26 unknowns @xmath35}_{(n+1)\\times(n+1 ) } } &   \\cdot   & \\underbrace { \\left [ \\begin{array}{c } \\omega_j^1 \\\\ \\vdots   \\\\ \\omega_j^n \\\\ \\mu_j \\\\ \\end{array } \\right ] } _ { ( n+1)\\times 1 } & = & \\underbrace { \\left [ \\begin{array}{c } \\rho_{1j } \\\\ \\vdots   \\\\ \\rho_{nj } \\\\ 1 \\\\ \\end{array } \\right ]   } _ { ( n+1 ) \\times 1 } \\ .   \\end{array}\\ ] ]",
    "since @xmath36 then @xmath37 and ( since ) @xmath38 then @xmath39 the minimized variance of the field @xmath8 under estimation @xmath34 ^ 2\\ } = \\sigma^2 ( 1\\pm(\\omega^i_j \\rho_{ij } + \\mu_j))\\ ] ] has known asymptotic property @xmath40 ^ 2\\ }    = \\lim_{n \\rightarrow \\infty }   e\\{[v_j-\\omega^i_j v_i]^2\\ }   = e\\{[v_j - m]^2\\ }   = \\sigma^2 \\ .\\ ] ]",
    "let us consider the field @xmath8 under estimation @xmath41 where for auto - estimation holds @xmath42 with minimized variance of the estimation statistics @xmath43 ^ 2\\ } & = & cov\\{(\\omega^i_j v_i)(\\omega^i_j v_i)\\ }   \\\\    & = & \\sum_i\\sum_l\\omega^i_j \\omega^l_j cov\\{v_i v_l\\ }   \\\\    & = & \\sigma^2 |\\omega^i_j \\rho_{ii } \\omega^i_j|   \\\\    & = & \\mp\\sigma^2(\\omega^i_j \\rho_{ij}-\\mu_j ) \\ , \\end{array}\\ ] ] where for auto - estimation holds @xmath44 ^ 2\\ } = e\\{[v_i - m]^2\\ } = \\sigma^2\\ ] ] that means outcoming of input value is unknown for mathematical model , with minimized variance of the field @xmath8 under estimation @xmath45 ^ 2\\ } & = & \\sigma^2(1\\pm(\\omega^i_j \\rho_{ij } + \\mu_j ) )      \\end{array}\\ ] ] where for auto - estimation holds @xmath46 ^ 2\\ } = \\underbrace{e\\{[v_i - m]^2\\}}_{\\sigma^2 } - \\underbrace{2(e\\{v_i\\hat{v}_i\\}-m^2)}_{2\\sigma^2 } + \\underbrace{e\\{[\\hat{v}_i - m]^2\\}}_{\\sigma^2 } = 0\\ ] ] that means variance of the field is equal to variance of the ( auto-)estimation statistics ( not that auto - estimation matches observation ) .",
    "for @xmath47 @xmath48   } _ { n \\times 1 } = \\xi \\underbrace { \\left [ \\begin{array}{c } 1 \\\\",
    "\\vdots   \\\\ 1 \\\\ \\end{array } \\right ]   } _ { n \\times 1 } \\qquad \\xi \\rightarrow 0 ^ - ~(\\mbox{or } ~\\xi \\rightarrow 0^+ ) \\ ] ] and a disjunction of the minimized variance of the field @xmath8 under estimation @xmath49 ^ 2\\ } - \\underbrace{(e\\{v_j\\hat{v}_j\\}-m^2)}_{\\mp\\sigma^2\\xi } + \\underbrace{e\\{\\hat{v}_j[\\hat{v}_j - v_j]\\}}_{\\mp\\sigma^2\\xi } \\quad \\mbox{if } \\quad \\rho_{ij } \\omega^i_j + \\mu_j = \\xi+ \\mu_j=0\\ ] ] which fulfills its asymptotic property the kriging system @xmath50}_{(n+1)\\times(n+1 ) } } &   \\cdot   & \\underbrace { \\left [ \\begin{array}{c } \\omega^1 \\\\ \\vdots   \\\\ \\omega^n \\\\ - \\xi \\\\ \\end{array } \\right ] } _ { ( n+1)\\times 1 } & = & \\underbrace { \\left [ \\begin{array}{c } \\xi \\\\ \\vdots   \\\\ \\xi \\\\ 1 \\\\ \\end{array } \\right ]   } _ { ( n+1 ) \\times 1 } & \\end{array}\\ ] ] equivalent to @xmath51 and @xmath52 where : @xmath53 , @xmath54 , @xmath55 , has the least squares solution @xmath56 and @xmath57 with a mean squared error of mean estimation @xmath58 ^ 2\\ } = \\mp\\sigma^2 2\\xi \\ .\\ ] ]",
    "for white noise @xmath45 ^ 2\\ } & = & e\\{[v_j - m]^2\\}+e\\{[\\hat{v}_j - m]^2\\ } \\\\        * remark .",
    "* precession of arithmetic mean can not be identical to @xmath62 cause a straight line fitted to high - noised data by ordinary least squares estimator can not have the slope identical to @xmath62 .",
    "for this reason the estimator of an unknown constant variance @xmath63 in fact is the lower bound for precession of the minimized variance of the field under estimation @xmath34 ^ 2\\ } = \\sigma^2\\left(1+\\frac{1}{n}\\right)\\ ] ] to increase ` a bit ' the lower bound for @xmath64 we can effect on weight and reduce total counts @xmath25 by @xmath65 because @xmath65 is the closest positive integer number to @xmath62 so it is easy to find the closest weight such that @xmath66 then the so - called unbiased variance @xmath67 in fact is the simplest estimator of minimized variance of the field under estimation ."
  ],
  "abstract_text": [
    "<S> we present statistics ( s - statistics ) based only on random variable ( not random value ) with a mean squared error of mean estimation as a concept of error . </S>"
  ]
}