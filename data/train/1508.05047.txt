{
  "article_text": [
    "we focus on rare - event simulation for addressing reliability problems corresponding to dynamic systems . to compute the rare event ( failure ) probability for a dynamic system , both input ( excitation ) and modeling uncertainties",
    "should be quantified and propagated .",
    "therefore , a probability model must be chosen to describe the uncertainty in the future input for the system and then a chosen deterministic or stochastic system model is used , preferably in conjunction with a probability model describing the associated modeling uncertainties , to propagate these uncertainties .",
    "these input and system models define a probabilistic description of the system output ( response ) . for example",
    ", the problem of interest might be to compute the small failure probability for a highly reliable dynamic system such as a bridge or building under uncertain future earthquake excitation , or for an aircraft under uncertain excitation by turbulence , using a finite - element structural model to approximate the dynamics of the system .",
    "this model will usually be subject to both parametric uncertainty ( what values of the model parameters best represent the behavior of the system ? ) and non - parametric modeling uncertainty ( what are the effects of the aspects of the system behavior not captured by the dynamic model ? ) .",
    "the treatment of input uncertainty has a long history in dynamic reliability theory and random vibrations , now more commonly called stochastic dynamics , but the treatment of modeling uncertainty is more recent .",
    "usually the dynamic model of the system is represented by a time - dependent bvp ( boundary - value problem ) involving pdes ( partial differential equations ) or by a set of coupled odes ( ordinary differential equations ) .",
    "typically the failure event is defined as any one of a set of performance quantities of interest exceeding its specified threshold over some time interval .",
    "this is the so - called _ first - passage problem_. this challenging problem is characterized by a lack of analytical solutions , even for the simplest case of a single - degree - of - freedom linear oscillator subject to excitation that is modeled as a gaussian process .",
    "approximate analytical methods exist that are usually limited in scope and their accuracy is difficult to assess in a given application @xcite .",
    "semi - analytical methods from structural reliability theory such as form and sorm ( first- and second - order reliability methods ) @xcite can not be applied directly to the first - passage problem and are inapplicable , anyway , because of the high - dimensional nature of the discrete - time input history @xcite .",
    "standard monte carlo simulation has general applicability but it is computationally very inefficient because of the low failure probabilities . as a consequence , advanced stochastic simulation schemes are needed .",
    "we assume that initially there is a continuous - time deterministic model of the real dynamic system that consists of a state - space model with a finite - dimensional state @xmath1 at time @xmath2 and this is converted to a discrete - time state - space model using a numerical time - stepping method to give : @xmath3 where @xmath4 is the input at discrete time @xmath2 .",
    "if the original model consists of a bvp with pdes describing a response @xmath5 where @xmath6 , then we assume that a finite set of basis functions @xmath7 is chosen ( e.g. global bases such as fourier and hermite polynomials or localized ones such as finite - element interpolation functions ) so that the solution is well approximated by : @xmath8 then a numerical method is applied to the bvp pdes to establish time - dependent equations for the vector of coefficients @xmath9 $ ] so that the standard state - space equation in ( [ model ] ) still applies .",
    "for example , for a finite - element model of a structural system , @xmath7 would be local interpolation functions over the elements .",
    "then , expressing the bvp in weak form , a weighted residual or galerkin method could be applied to give a state - space equation for the vector of coefficients @xmath10 @xcite .",
    "suppose that a positive scalar performance function @xmath11 is a quantity of interest and that the rare event @xmath12 of concern is that @xmath11 exceeds a threshold @xmath13 over some discrete - time interval @xmath14 : @xmath15 where @xmath10 satisfies ( [ model ] ) .",
    "the performance function @xmath11 may involve exceedance of multiple performance quantities of interest @xmath16 above their corresponding thresholds @xmath17 .",
    "this can be accomplished by aggregating them using the _ max _ and _ min _ operators in an appropriate combination on the set of @xmath18 s ; for example , for a _",
    "pure series _ failure criterion , where the threshold exceedance of _ any _ @xmath19 represents failure , one takes the aggregate performance failure criterion as @xmath20 , while for a _ pure parallel _ failure criterion , where _ all _ of the @xmath18 must exceed their thresholds before failure is considered to have occurred , one takes the aggregate performance failure criterion as @xmath21 .",
    "if the uncertainty in the input time history vector @xmath22\\in\\mathbb{r}^d$ ] @xmath23 is quantified by a probability distribution for @xmath24 that has a pdf ( probability density function ) @xmath25 with respect to lebesgue integration over @xmath26 , then the rare - event probability is given by : @xmath27 the pdf @xmath25 is assumed to be readily sampled . although _ direct sampling _ from a high - dimensional pdf is not possible in most cases , multi - dimensional gaussians are an exception because the gaussian vector can be readily transformed so that the components are independent and the pdf is a product of one - dimensional gaussian pdfs . in many applications ,",
    "the discrete - time stochastic input history is modeled by running discrete - time gaussian white noise through a digital filter to shape its spectrum in the frequency domain , and then multiplying the filtered sequence by an envelope function to shape it in the time domain , if it is non - stationary .    the model in ( [ model ] ) may also depend on uncertain parameters @xmath28 which includes the initial values @xmath29 if they are uncertain .",
    "then a prior pdf @xmath30 may be chosen to quantify the uncertainty in the value of vector @xmath31 .",
    "some of the parameters may characterize the pdf for input @xmath24 which can then be denoted @xmath32 .",
    "it is convenient to re - define vector @xmath24 to also include @xmath31 , then the new pdf @xmath25 is @xmath33 in terms of the previous pdfs .",
    "we assume that model parameter uncertainty is incorporated in this way , so the basic equations remain the same as ( [ model ] ) , ( [ f ] ) , and ( [ pf ] ) . when model uncertainty is incorporated , the calculated @xmath34 has been referred to as the robust rare - event probability @xcite , meaning robust to model uncertainty , as in robust control theory .",
    "the standard _ monte carlo simulation",
    "_ method ( mcs ) is one of the most robust and straightforward ways to simulate rare events and estimate their probabilities .",
    "the method was originally developed in @xcite for solving problems in mathematical physics . since",
    "then mcs has been used in many applications in physics , statistics , computer science , and engineering , and currently it lays at the heart of all random sampling - based techniques @xcite .    the basic idea behind mcs is to observe that the probability in ( [ pf ] ) can be written as an expectation : @xmath35 } p_\\mathcal{e}=\\int_{\\mathbb{r}^{d}}{i_\\mathcal{e}(u)p(u)du}=\\mathbb{e}_p[i_\\mathcal{e}],\\ ] ] where @xmath36 is the indicator function of @xmath12 , that is @xmath37 if @xmath38 and @xmath39 otherwise , and @xmath40 is the dimension of the integral . recall that the strong law of large numbers @xcite states that if @xmath41 are independent and identically distributed ( i.i.d . )",
    "samples of vector @xmath24 drawn from the distribution @xmath25 , then for any function @xmath42 with finite mean @xmath43 $ ] , the sample average @xmath44 converges to the true value @xmath43 $ ] as @xmath45 almost surely ( i.e. with probability 1 ) .",
    "therefore , setting @xmath46 , the probability in ( [ pf = e[i ] ] ) can be estimated as follows : @xmath47 it is straightforward to show that @xmath48 is an _ unbiased _ estimator of @xmath34 with mean and variance : @xmath49&= \\mathbb{e}_p\\left[\\frac{1}{n}\\sum_{i=1}^ni_\\mathcal{e}(u_i)\\right]\\\\ & = \\frac{1}{n}\\sum_{i=1}^n\\mathbb{e}_p[i_\\mathcal{e}]=p_\\mathcal{e},\\\\ \\mathrm{var}_p[{p}_\\mathcal{e}^{mcs}]&=\\mathrm{var}_p\\left[\\frac{1}{n}\\sum_{i=1}^ni_\\mathcal{e}(u_i)\\right]\\\\ & = \\frac{1}{n^2}\\sum_{i=1}^n\\mathrm{var}_p[i_\\mathcal{e}]=\\frac{p_\\mathcal{e}(1-p_\\mathcal{e})}{n}. \\end{split}\\ ] ] furthermore , by the central limit theorem @xcite , as @xmath45 , @xmath48 is distributed asymptotically as gaussian with this mean and variance .    _ frequentist interpretation of mcs : _ the frequentist interpretation of mcs focuses on the forward problem , arguing that if @xmath50 is large so that the variance of @xmath48 is relatively small , then the value @xmath51 based on ( [ mcsestimate ] ) for a specific set of @xmath50 samples @xmath52 drawn from @xmath25 should be close to the mean @xmath34 of @xmath48 .",
    "the sample mean estimate @xmath51 is very intuitive and , in fact , simply reflects the frequentist definition of probability : @xmath51 is the ratio between the number of trials where the event @xmath12 occurred , @xmath53 , and the total number of trials @xmath50 .    _",
    "bayesian interpretation of mcs : _ the same mcs estimate @xmath51 has a simple bayesian interpretation ( e.g. @xcite ) , which focuses on the inverse problem for the specific set of @xmath50 samples @xmath52 drawn from @xmath25 . following the bayesian approach @xcite , the unknown probability @xmath34 is considered as a stochastic variable whose value in @xmath54 $ ] is uncertain .",
    "the _ principle of maximum entropy _",
    "@xcite leads to the uniform prior distribution for @xmath34 , @xmath55 , @xmath56 , which implies that all values are taken as equally plausible a priori .",
    "since samples @xmath41 are i.i.d , the binary sequence @xmath57 is a sequence of bernoulli trials , and so for the forward problem , @xmath58 is distributed according to the binomial distribution with parameters @xmath50 and @xmath34 , @xmath59 .",
    "therefore , for the set of @xmath50 samples , the likelihood function is @xmath60 . using _",
    "bayes theorem _ , the posterior distribution for @xmath34 , @xmath61 , is therefore the beta distribution @xmath62 , i.e. @xmath63 where the beta function @xmath64 is the normalizing constant that equals @xmath65 here . the mcs estimate is the _",
    "maximum a posteriori ( map ) estimate _ , which is the mode of the posterior distribution ( [ posterior ] ) and therefore the most probable value of @xmath34 a posteriori : @xmath66 notice that the posterior pdf in ( [ posterior ] ) gives a complete description of the uncertainty in the value of @xmath34 based on the specific set of @xmath50 samples of @xmath24 drawn from @xmath25 .",
    "the posterior distribution in ( [ posterior ] ) is in fact the original bayes result @xcite , although bayes theorem was developed in full generality by laplace @xcite .",
    "the standard mcs method for estimating the probability in ( [ pf ] ) is summarized in the following pseudo - code .",
    "+    ' '' ''    height 0.6pt    ' '' ''    * monte carlo simulation *    ' '' ''    ' '' ''    ` input : `    @xmath67 @xmath50 , total number of samples .    `",
    "algorithm : `    set @xmath68 , number of trials where the event @xmath12    occurred .    * for * @xmath69 * do *    sample the input excitation    @xmath70 .    compute the system trajectory    @xmath71    using the system model ( [ model ] ) with @xmath72 .",
    "* if * @xmath73    @xmath74    * end if *    * end for *    ` output : `    @xmath75 @xmath76 , mcs estimate of @xmath34    @xmath75 @xmath77 , posterior pdf    of @xmath34    ' '' ''    _ assessment of accuracy of mcs estimate : _ for the frequentist interpretation , the _ coefficient of variation _ ( c.o.v . ) for the estimator @xmath48 given by ( [ mcsestimate ] ) , conditional on @xmath34 and @xmath50 , is given by ( [ mean_var ] ) : @xmath78}}{\\mathbb{e}_p[{p}_\\mathcal{e}^{mcs}]}=\\sqrt{\\frac{1-p_\\mathcal{e}}{np_\\mathcal{e}}}.\\ ] ] this can be approximated by replacing @xmath34 by the estimate @xmath79 for a given set of @xmath50 samples",
    "@xmath52 : @xmath80    for the bayesian interpretation , the posterior c.o.v . for the stochastic variable @xmath34 ,",
    "conditional on the set of @xmath50 samples , follows from ( [ posterior ] ) : @xmath81}}{\\mathbb{e}[{p}_\\mathcal{e}|\\widehat{n}_\\mathcal{e},n]}\\\\ & = \\frac{\\sqrt{1-\\frac{\\widehat{n}_\\mathcal{e}+1}{n+2}}}{\\sqrt{(n+3)\\left(\\frac{\\widehat{n}_\\mathcal{e}+1}{n+2}\\right)}}\\longrightarrow\\sqrt{\\frac{1-\\widehat{p}_\\mathcal{e}^{mcs}}{n\\widehat{p}_\\mathcal{e}^{mcs}}}=\\widehat{\\delta}_n^{mcs } , \\end{split}\\ ] ] as @xmath45 .",
    "therefore , the same expression @xmath82 can be used to assess the accuracy of the mcs estimate , even though the two c.o.v.s have distinct interpretations .",
    "the approximation @xmath82 for the two c.o.v.s reveals both the main advantage of the standard mcs method and its main drawback .",
    "the main strength of mcs , which makes it very robust , is that its accuracy does not depend on the geometry of the domain @xmath83 and its dimension @xmath84 .",
    "as long as an algorithm for generating i.i.d .",
    "samples from @xmath25 is available , mcs , unlike many other methods ( e.g. numerical integration ) , does not suffer from the `` curse of dimensionality . ''",
    "moreover , an irregular , or even fractal - like , shape of @xmath12 will not affect the accuracy of mcs .",
    "on the other hand , the serious drawback of mcs is that this method is not computationally efficient in estimating the _ small probabilities _",
    "@xmath34 corresponding to _ rare events _ , where from ( [ cov ] ) , @xmath85 therefore , to achieve a prescribed level of accuracy @xmath86 , the required total number of samples is @xmath87 . for each sampled excitation @xmath88 , a system analysis  usually computationally very intensive  is required to compute the corresponding system trajectory @xmath89 and to check whether @xmath88 belongs to @xmath12 .",
    "this makes mcs excessively costly and inapplicable for generating rare events and estimating their small probabilities .",
    "nevertheless , essentially all sampling - based methods for estimation of rare event probability are either based on mcs ( e.g. importance sampling ) or have it as a part of the algorithm ( e.g. subset simulation ) .",
    "the _ importance sampling _ ( is ) method belongs to the class of _ variance reduction techniques _ that aim to increase the accuracy of the estimates by constructing ( sometimes biased ) estimators with a smaller variance @xcite .",
    "it seems it was first proposed in @xcite , soon after the standard mcs method appeared .",
    "the inefficiency of mcs for rare event estimation stems from the fact that most of the generated samples @xmath90 do not belong to @xmath12 so that the vast majority of the terms in the sum ( [ mcsestimate ] ) are zero and only very few ( if any ) are equal to one . the basic idea of is is to make use of the information available about the rare event @xmath12 to generate samples that lie more frequently in @xmath12 or in the _ important region _ @xmath91 that accounts for most of the probability content in ( [ pf ] ) . rather than estimating @xmath34 as an average of many 0 s and very few 1 s like in @xmath51 , is seeks to reduce the variance by constructing an estimator of the form @xmath92 , where @xmath93 is an appreciable fraction of @xmath50 and the @xmath94 are small but not zero , ideally of the same order as the target probability , @xmath95 .",
    "specifically , for an appropriate pdf @xmath96 on the excitation space @xmath26 , the integral in ( [ pf = e[i ] ] ) can be re - written as follows : @xmath97 .",
    "\\end{split}\\ ] ] the is estimator is now constructed similarly to ( [ mcsestimate ] ) by utilizing the law of large numbers : @xmath98 where @xmath41 are i.i.d .",
    "samples from @xmath96 , called the _ importance sampling density _ ( isd ) , and @xmath99 is the _ importance weight _ of sample @xmath88 .",
    "the is estimator @xmath100 converges almost surely as @xmath45 to @xmath34 by the strong law of large numbers , provided that the support of @xmath96 , i.e. the domain in @xmath26 where @xmath101 , contains the support of @xmath102 .",
    "intuitively , the latter condition guarantees that all points of @xmath12 that can be generated by sampling from the original pdf @xmath25 , can also be generated by sampling from the isd @xmath96 .",
    "note that if @xmath103 , then @xmath104 and is simply reduces to mcs , @xmath105 . by choosing the isd @xmath96 appropriately , is aims to obtain an estimator with a smaller variance .    the is estimator @xmath100 is also _ unbiased _ with mean and variance : @xmath106&=\\mathbb{e}_q\\left[\\frac{1}{n}\\sum_{i=1}^ni_\\mathcal{e}(u_i)w(u_i)\\right]\\\\ & = \\frac{1}{n}\\sum_{i=1}^n\\mathbb{e}_q\\left[\\frac{i_\\mathcal{e}p}{q}\\right]=p_\\mathcal{e},\\\\ \\mathrm{var}_q[{p}_\\mathcal{e}^{is}]&=\\frac{1}{n^2}\\sum_{i=1}^n\\mathrm{var}_q\\left[\\frac{i_\\mathcal{e}p}{q}\\right]\\\\ & = \\frac{1}{n}\\left(\\mathbb{e}_q\\left[\\frac{i_\\mathcal{e}p^2}{q^2}\\right]-p_\\mathcal{e}^2\\right ) . \\end{split}\\ ] ] the is method is summarized in the following pseudo - code",
    ". +    ' '' ''    height 0.6pt    ' '' ''    * importance sampling *    ' '' ''    ' '' ''    ` input : `    @xmath67 @xmath50 , total number of samples .",
    "@xmath67 @xmath96 , importance sampling density .    `",
    "algorithm : `    set @xmath107 , counter for the number of samples in @xmath12 .    * for * @xmath69 * do *    sample the input excitation    @xmath108 .    compute the system trajectory    @xmath71    using the system model ( [ model ] ) with @xmath72 .    *",
    "if * @xmath73    @xmath109    compute the importance weight of the @xmath110    sample in @xmath12 , @xmath111 .",
    "* end if *    * end for *    @xmath112 , the total number of trials where the event    @xmath12 occurred .    `",
    "output : `    @xmath75 @xmath113 , is estimate of @xmath34 .    ' '' ''    the most important task in applying is for estimating small probabilities of rare events is the construction of the isd , since the accuracy of @xmath114 depends critically on @xmath96 .",
    "if the isd is `` good '' , then one can get great improvement in efficiency over standard mcs . if , however , the isd is chosen inappropriately so that for instance @xmath68 or the importance weights have a large variation",
    ", then is will yield a very poor estimate .",
    "both scenarios are demonstrated below in the section `` illustrative example '' .",
    "it is straightforward to show that the _ optimal _ isd , which minimizes the variance in ( [ mean var is ] ) , is simply the original pdf @xmath25 conditional on the domain @xmath12 : @xmath115 indeed , in this case , all generated sample excitations satisfy @xmath116 , so their importance weights @xmath117 , and the is estimate @xmath118 .",
    "moreover , just one sample ( @xmath119 ) generated from @xmath120 is enough to find the probability @xmath34 exactly .",
    "note , however , that this is a purely theoretical result since in practice sampling from the conditional distribution @xmath121 is challenging , and , most importantly , it is impossible to compute @xmath120 : this would require the knowledge of @xmath34 , which is unknown .",
    "nevertheless , this result indicates that the isd @xmath96 should be chosen as close to @xmath120 as possible .",
    "in particular , most of the probability mass of @xmath96 should be concentrated on @xmath12 .",
    "based on these considerations , several ad hoc techniques for constructing isds have been developed , e.g. variance scaling and mean shifting @xcite .    in the special case of linear dynamics and gaussian excitation ,",
    "an extremely efficient algorithm for estimating the rare - event probability @xmath34 in ( [ pf ] ) , referred to as isee ( _ importance sampling using elementary events _ ) , has been presented @xcite .",
    "the choice of the isd exploits known information about each elementary event , defined as an outcrossing of the performance threshold @xmath13 in ( [ f ] ) at a specific time @xmath122 . the c.o.v .",
    "of the isee estimator for @xmath50 samples of @xmath24 from @xmath25 is given by @xmath123 where the proportionality constant @xmath124 is close to @xmath125 , regardless of how small the value of @xmath34 .",
    "in fact , @xmath124 decreases slightly as @xmath34 decreases , exhibiting the opposite behavior to mcs .",
    "in general , it is known that in many practical cases of rare event estimation it is difficult to construct a good isd that leads to a low - variance is estimator , especially if the dimension of the uncertain excitation space @xmath26 is large , as it is in dynamic reliability problems @xcite . a geometric explanation as to why is in often inefficient in high dimensions is given in @xcite .",
    "au @xcite has presented an efficient is method for estimating @xmath34 in ( [ pf ] ) for elasto - plastic systems subject to gaussian excitation . in recent years",
    ", substantial progress has been made by tailoring the _ sequential importance sampling _ ( sis ) methods @xcite , where the isd is iteratively refined , to rare event problems .",
    "sis and its modifications have been successfully used for estimating rare events in dynamic portfolio credit risk @xcite , structural reliability @xcite , and other areas .",
    "the _ subset simulation _ ( ss ) method @xcite is an advanced stochastic simulation method for estimating rare events which is based on _ markov chain monte carlo _ ( mcmc ) @xcite .",
    "the basic idea behind ss is to represent a very small probability @xmath34 of the rare event @xmath12 as a product of larger probabilities of `` more - frequent '' events and then estimate these larger probabilities separately . to implement this idea ,",
    "let @xmath126 be a sequence of nested subsets of the uncertain excitation space starting from the entire space @xmath127 and shrinking to the target rare event @xmath128 .",
    "by analogy with ( [ f ] ) , subsets @xmath129 can be defined by relaxing the value of the critical threshold @xmath13 : @xmath130 where @xmath131 . in the actual implementation of ss ,",
    "the number of subsets @xmath132 and the values of intermediate thresholds @xmath133 are chosen adaptively .    using the notion of conditional probability and exploiting the nesting of the subsets",
    ", the target probability @xmath34 can be factorized as follows : @xmath134 an important observation is that by choosing the intermediate thresholds @xmath133 appropriately , the conditional events @xmath135 can be made more frequent , and their probabilities can be made large enough to be amenable to efficient estimation by mcs - like methods .",
    "the first probability @xmath136 can be readily estimated by standard mcs : @xmath137 where @xmath138 are i.i.d .",
    "samples from @xmath25 . estimating the remaining probabilities",
    "@xmath139 , @xmath140 , is more challenging since one needs to generate samples from the conditional distribution @xmath141 , which , in general , is not a trivial task . notice that a sample @xmath24 from @xmath142 is one drawn from @xmath25 that lies in @xmath143 .",
    "however , it is not efficient to use mcs for generating samples from @xmath142 : sampling from @xmath25 and accepting only those samples that belong to @xmath143 is computationally very expensive , especially at higher levels  @xmath144 .    in standard ss ,",
    "samples from the conditional distribution @xmath142 are generated by the _ modified metropolis algorithm _",
    "( mma ) @xcite which belongs to the family of mcmc methods for sampling from complex probability distributions that are difficult to sample from directly @xcite .",
    "an alternative strategy  _ splitting _  is described in the next section .",
    "the mma algorithm is a component - wise version of the original metropolis algorithm @xcite .",
    "it is specifically tailored for sampling from high - dimensional conditional distributions and works as follows .",
    "first , without loss of generality , assume that @xmath145 , i.e. components of @xmath24 are independent .",
    "this assumption is indeed not a limitation , since in simulation one always starts from independent variables to generate correlated excitation histories @xmath24 .",
    "suppose further , that some vector @xmath146 is already distributed according to the target conditional distribution , @xmath147 .",
    "mma prescribes how to generate another vector @xmath148 and it consists of two steps :    1 .",
    "generate a `` candidate '' state @xmath149 as follows : first , for each component @xmath150 of @xmath149 , sample @xmath151 from the symmetric univariate _",
    "proposal distribution _",
    "@xmath152 centered on the @xmath153 component of @xmath154 , where symmetry means that @xmath155 ; then , compute the _ acceptance ratio _",
    "@xmath156 ; finally , set @xmath157 2 .",
    "accept or reject the candidate state @xmath149 : @xmath158    it can be shown that @xmath159 generated by mma is indeed distributed according to the target conditional distribution @xmath142 when @xmath154 is @xcite . for a detailed discussion of mma ,",
    "the reader is referred to @xcite .",
    "the procedure for generating conditional samples at level @xmath144 is as follows .",
    "starting from a `` seed '' @xmath160 , one can now use mma to generate a sequence of random vectors @xmath138 , called a _",
    "markov chain _",
    ", distributed according to @xmath142 . at each step ,",
    "@xmath161 is used to generate the next state @xmath162 .",
    "note that although these mcmc samples are identically distributed , they are clearly not independent : the correlation between successive samples is due to the proposal pdfs @xmath163 at level @xmath144 that govern the generation of @xmath162 from @xmath164 .",
    "nevertheless , @xmath138 can still be used for statistical averaging as if they were i.i.d , although with certain reduction in efficiency @xcite . in particular , similarly to ( [ p1 ] ) , the conditional probability @xmath139 , can be estimated as follows : @xmath165    to obtain an estimator for the target probability @xmath34 , it remains to multiply the mcs ( [ p1 ] ) and mcmc ( [ pi ] ) estimators of all factors in ( [ product ] ) .",
    "in real applications , however , it is often difficult to rationally define the subsets @xmath166 in advance , since it is not clear how to specify the values of the intermediate thresholds @xmath133 . in ss , this is done adaptively .",
    "specifically , let @xmath167 be the mcs samples from @xmath25 , @xmath168 be the corresponding trajectories from ( [ model ] ) , and @xmath169 be the resulting performance values .",
    "assume that the sequence @xmath170 is ordered in non - increasing order , i.e. @xmath171 , renumbering the samples where necessary .",
    "define the first intermediate threshold @xmath172 as follows : @xmath173 where @xmath174 is a chosen probability satisfying @xmath175 .",
    "this choice of @xmath172 has two immediate consequences : first , the mcs estimate of @xmath176 in ( [ p1 ] ) is exactly @xmath174 , and , second , @xmath177 not only belong to @xmath178 , but also are distributed according to the conditional distribution @xmath179 .",
    "each of these @xmath180 samples can now be used as mother seeds in mma to generate @xmath181 offspring , giving a total of @xmath182 samples @xmath183 .",
    "since these seeds start in the stationary state @xmath179 of the markov chain , this mcmc method gives _ perfect sampling _",
    ", i.e. no wasteful burn - in period is needed .",
    "similarly , @xmath184 is defined as @xmath185 where @xmath186 are the ( ordered ) performance values corresponding to excitations @xmath187 . again by construction , the estimate ( [ pi ] ) gives @xmath188 , and @xmath189 .",
    "the ss method proceeds in this manner until the target rare event @xmath12 is reached and is sufficiently sampled .",
    "all but the last factor in ( [ product ] ) are approximated by @xmath174 , and the last factor @xmath190 , where @xmath191 is the number of samples in @xmath12 among @xmath192 .",
    "the method is more formally summarized in the following pseudo - code .    ' '' ''    height 0.6pt    ' '' ''    * subset simulation *    ' '' ''    ' '' ''    ` input : `    @xmath67 @xmath182 , number of samples per conditional level .",
    "@xmath67 @xmath174 , level probability ; e.g. @xmath193 .",
    "@xmath67 @xmath163 , proposal distributions ;    e.g. @xmath194 .",
    "` algorithm : `    set @xmath195 , number of conditional level .",
    "set @xmath196 , number of the mcs samples in @xmath12 .    sample the input excitations @xmath197 .",
    "compute the corresponding trajectories    @xmath198 .",
    "* for * @xmath199 * do *    * if * @xmath200 * do *    @xmath201    * end if *    * end for *    * while * @xmath202 * do *    @xmath203 , a new subset @xmath129 is needed .",
    "sort @xmath204 so that    @xmath205 .",
    "define the @xmath206 intermediate threshold : @xmath207 .",
    "* for * @xmath208 * do *    using @xmath209 as a    seed , use mma to generate @xmath181    additional states of a markov chain    @xmath210 .    * end for *    renumber :    @xmath211 .",
    "compute the corresponding trajectories    @xmath212 .    * for * @xmath199",
    "* do *    * if * @xmath213 * do *    @xmath214    * end if *    * end for *    * end while *    @xmath215 , number of levels , i.e. subsets @xmath129 in ( [ filtration ] )    and ( [ subsets ] ) .",
    "@xmath216 , total number of samples .",
    "` output : `    @xmath75 @xmath217 , ss estimate of @xmath218 .    ' '' ''    implementation details of ss , in particular the choice of level probability @xmath174 and proposal distributions @xmath219 , are thoroughly discussed in @xcite .",
    "it has been confirmed that @xmath193 proposed in the original paper @xcite is a nearly optimal value .",
    "the choice of @xmath163 is more delicate , since the efficiency of mma strongly depends on the proposal pdf variances in a non - trivial way : proposal pdfs with both small and large variance tend to increase the correlation between successive samples , making statistical averaging in ( [ pi ] ) less efficient . in general , finding the optimal variance of proposal distributions is a challenging task not only for mma , but also for almost all mcmc algorithms .",
    "nevertheless , it has been found in many applications that using @xmath194 , the gaussian distribution with mean @xmath220 and variance @xmath221 , yields good efficiency if @xmath222 and @xmath25 is a multi - dimensional gaussian with all variances equal to @xmath223 .",
    "for an adaptive strategy for choosing @xmath163 , the reader is referred to @xcite ; for example , @xmath224 can be chosen so that the observed average acceptance rate in mma , based on a subset of samples at level @xmath144 , lies in the interval @xmath225 $ ] .",
    "it can be shown @xcite that , given @xmath34 , @xmath174 , and the total number of samples @xmath50 , the c.o.v .",
    "of the ss estimator @xmath226 is given by @xmath227 where @xmath228 and @xmath229 is approximately a constant that depends on the state correlation of the markov chain at each level .",
    "numerical experiments show that @xmath230 gives a good approximation to the c.o.v . and that @xmath231 if the proposal variance @xmath232 for each level is appropriately chosen @xcite .",
    "it follows from ( [ cov2 ] ) that @xmath233 for mcs , while for ss , @xmath234 .",
    "this drastically different scaling behavior of the c.o.v.s with small @xmath34 directly exhibits the improvement in efficiency .    to compare an advanced stochastic simulation algorithm directly with mcs , which is always applicable ( but not efficient ) for rare event estimation",
    ", @xcite introduced the relative computation efficiency of an algorithm , @xmath235 , which is defined as the ratio of the number of samples @xmath236 required by mcs to the number of samples @xmath237 required by the algorithm for the same c.o.v .",
    "the _ relative efficiency _ of ss is then @xmath239 for @xmath230 , @xmath240 , and @xmath193 . for rare events",
    ", @xmath241 is very large , and , as expected , ss outperforms mcs ; for example , if @xmath242 , then @xmath243 .",
    "in recent years , a number of modifications of ss have been proposed , including ss with splitting @xcite ( described in the next section ) , hybrid ss @xcite , two - stage ss @xcite , spherical ss @xcite , and ss with delayed rejection  @xcite .",
    "a bayesian post - processor for ss , which generalizes the bayesian interpretation of mcs described above , was developed in @xcite . in the original paper @xcite ,",
    "ss was developed for estimating reliability of complex civil engineering structures such as tall buildings and bridges at risk from earthquakes .",
    "it was applied for this purpose in @xcite and @xcite .",
    "ss and its modifications have also been successfully applied to rare event simulation in fire risk analysis @xcite , aerospace @xcite , nuclear @xcite , wind @xcite and geotechnical engineering @xcite , and other fields .",
    "a detailed exposition of ss on an introductory level and a matlab code implementing the above pseudo - code is given in @xcite .",
    "for more advanced and complete reading , the fundamental monograph on ss @xcite is strongly recommended .",
    "in the previously presented stochastic simulation methods , samples of the input and output discrete - time histories , @xmath244 and @xmath245 , are viewed geometrically as vectors @xmath24 and @xmath246 that define points in the vector spaces @xmath247 and @xmath248 , respectively . in the splitting method , however , samples of the input and output histories are viewed as trajectories defining paths of length @xmath249 in @xmath250 and @xmath251 , respectively .",
    "samples that reach a certain designated subset in the input or output spaces at some time are treated as `` mothers '' and are then split into multiple offspring trajectories by separate sampling of the input histories subsequent to the splitting time .",
    "these multiple trajectories can themselves subsequently be treated as mothers if they reach another designated subset nested inside the first subset at some later time , and so be split into multiple offspring trajectories .",
    "this is continued until a certain number of the trajectories reach the smallest nested subset corresponding to the rare event of interest .",
    "splitting methods were originally introduced by kahn and harris @xcite and they have been extensively studied ( for example , @xcite ) .",
    "we describe splitting here by using the framework of subset simulation where the only change is that the conditional sampling in the nested subsets is done by splitting the trajectories that reach each subset , rather than using them as seeds to generate more samples from markov chains in their stationary state . as a result ,",
    "only standard monte carlo simulation is needed , instead of mcmc simulation .",
    "the procedure in @xcite is followed here to generate offspring trajectories at the @xmath206 level @xmath252 of subset simulation from each of the mother trajectories in @xmath129 constructed from samples from the previous level , except that we present it from the viewpoint of trajectories in the input space , rather than the output space .",
    "therefore , at the @xmath206 level , each of the @xmath180 sampled input histories @xmath164 , @xmath208 , from the previous level that satisfy @xmath253 , as defined in ( [ subsets ] ) ( so the corresponding output history @xmath254 satisfies @xmath255 ) , are split at their first - passage time @xmath256 this means that the mother trajectory @xmath164 is partitioned as @xmath257 $ ] where @xmath258 $ ] and @xmath259 $ ] ; then a subtrajectory sample @xmath260 $ ] is drawn from @xmath261 where the last equation follows if one assumes independence of the @xmath262 ( although it is not necessary ) . also , @xmath263 . note that the new input sample @xmath264 $ ] also lies in @xmath129 since it has the subtrajectory @xmath265 in common with @xmath164 , which implies that the corresponding outputs at the first - passage time @xmath266 are equal : @xmath267 .",
    "the offspring trajectory @xmath268 is a sample from @xmath25 lying in @xmath129 and so , like its mother @xmath164 , it is a sample from @xmath269 .",
    "this process is repeated to generate @xmath181 such offspring trajectories from each mother trajectory , giving a total of @xmath270 input histories that are samples from @xmath269 at the @xmath206 level .",
    "the pseudo - code for the splitting version of subset simulation is the same as the previously presented pseudo - code for the mcmc version except that the part describing the generation of conditional samples at level @xmath144 using the mma algorithm is replaced by :    ' '' ''    height 0.6pt    ' '' ''    * generation of conditional samples *     +    * at level @xmath144 with splitting *    ' '' ''    ' '' ''    * for * @xmath208 * do *    using @xmath271 as a mother trajectory ,    generate @xmath181 offspring trajectories by splitting    of this input trajectory .",
    "* end for *    ' '' ''    to generate the same number of samples @xmath182 at a level , the splitting version of subset simulation is slightly more efficient than the mcmc version using mma because when generating the conditional samples , the input offspring trajectories @xmath272 $ ] already have available the first part @xmath273 of the corresponding output trajectory @xmath274 $ ] .",
    "thus , ( [ model ] ) need only be solved for @xmath275 starting from the final value of @xmath273 ( which corresponds to the first - passage time of the trajectory ) .",
    "a disadvantage of the splitting version is that it can not handle parameter uncertainty in the model in ( [ model ] ) since the offspring trajectories must use ( [ model ] ) with the same parameter values as their mothers .",
    "furthermore , the splitting version applies only to dynamic problems , as considered here .",
    "the mcmc version of subset simulation can handle parameter uncertainty and is applicable to both static and dynamic uncertainty quantification problems .",
    "ching , au and beck @xcite discuss the statistical properties of the estimators corresponding to ( [ p1 ] ) and ( [ pi ] ) when the sampling at each level is done by the trajectory splitting method .",
    "they show that as long as the conditional probability in subset simulation satisfies @xmath276 , the coefficient of variation for @xmath34 when estimating it by ( [ product ] ) and ( [ pi ] ) is insensitive to @xmath174 .",
    "ching , beck and au @xcite also introduce a hybrid version of subset simulation that combines some advantages of the splitting and mcmc versions when generating the conditional samples @xmath277 at each level .",
    "it is limited to dynamic problems because of the splitting but it can handle parameter uncertainty through using mcmc .",
    "all three variants of subset simulation are applied to a series of benchmark reliability problems in @xcite ; their results imply that for the same computational effort in the dynamic benchmark problems , the hybrid version gives slightly better accuracy for the rare - event probability than the mcmc version . for a comparison between these results and",
    "those of other stochastic simulation methods that are applied to some of the same benchmark problems ( e.g. spherical subset simulation , auxiliary domain method and line sampling ) , the reader may wish to check @xcite .",
    "to illustrate mcs , is , and ss with mcmc and splitting for rare event estimation , consider the following forced lorenz system of ordinary differential equations : @xmath278 where @xmath279 defines the system state at time @xmath2 and @xmath280 is the external excitation to the system .",
    "if @xmath281 , these are the original equations due to e.  n.  lorenz that he derived from a model of fluid convection @xcite . in this example , the three parameters @xmath282 , and @xmath13 are set to @xmath283 , @xmath284 , and @xmath285 .",
    "it is well - know ( e.g. @xcite ) that in this case , the lorenz system has three unstable equilibrium points , one of which is @xmath286 that lies on one `` wing '' of the `` butterfly '' attractor .",
    "let @xmath287 be the initial condition , and @xmath10 be the corresponding solution .",
    "lorenz showed @xcite that the solution of ( [ lorenz1],[lorenz2],[lorenz3 ] ) with @xmath281 always ( for any @xmath2 ) stays inside the bounding ellipsoid @xmath288 : @xmath289    ) enclosed in the bounding ellipsoid @xmath288 ( top ) and the corresponding response function @xmath290 ( bottom ) , where @xmath291 $ ] , @xmath292 .",
    "the right top panel shows the solution of the forced lorenz system ( @xmath293 ) that corresponds to an excitation @xmath294 .",
    "as it is clearly seen , this solution leaves the ellipsoid @xmath288 .",
    "according to the response function @xmath290 shown in the right bottom panel , this first - passage event happens around @xmath295 . ]",
    "suppose that the system is now excited by @xmath296 , where @xmath297 is the standard brownian process ( gaussian white noise ) and @xmath124 is some scaling constant .",
    "the uncertain stochastic excitation @xmath280 makes the corresponding system trajectory @xmath10 also stochastic .",
    "let us say that the event @xmath12 occurs if @xmath10 leaves the bounding ellipsoid @xmath288 during the time interval of interest @xmath298 $ ] .",
    "the discretization of the excitation @xmath24 is obtained by the standard discretization of the brownian process : @xmath299 where @xmath300s is the sampling interval , @xmath301 , and @xmath302 are i.i.d .",
    "standard gaussian random variables .",
    "the target domain @xmath83 is then @xmath303 where the system response @xmath304 at time @xmath305 is @xmath306    figure  [ fig1 ] shows the solution of the unforced lorenz system ( with @xmath307 so @xmath308 ) , and an example of the solution of the forced system ( with @xmath293 ) that corresponds to excitation @xmath294 ( slightly abusing notation , @xmath309 means that the corresponding gaussian vector @xmath310 ) .",
    "_ monte carlo simulation : _ for @xmath293 , figure  [ fig2 ] shows the probability @xmath34 of event @xmath12 as a function of @xmath311 estimated using standard mcs : @xmath312 where @xmath313 are i.i.d .",
    "samples from the standard @xmath84-dimensional gaussian pdf @xmath314 . for each value of @xmath311 , @xmath315 samples were used .",
    "when @xmath316 the accuracy of the mcs estimate ( [ eq : mcs_ex ] ) begins to degenerate since the total number of samples @xmath50 becomes too small for the corresponding target probability .",
    "moreover , for @xmath317 , none of the @xmath50 generated mcs samples belong to the target domain @xmath12 , making the mcs estimate zero .",
    "figure  [ fig2 ] shows , as expected , that @xmath34 is an increasing function of @xmath311 , since the more time the system has , the more likely its trajectory eventually penetrates the boundary of ellipsoid @xmath288 .     of event @xmath12 where @xmath293 as a function of duration time @xmath311 . for each value of @xmath318 $ ] ,",
    "@xmath315 samples were used in mcs and @xmath319 samples per conditional level were used in the two versions of ss . the mcs and ss / splitting estimates for @xmath34 are zero for @xmath317 and @xmath320 , respectively .",
    "the bottom panel shows the total computational effort automatically chosen by both ss algorithms . ]    _ importance sampling : _ is is a variance reduction technique and , as it was discussed in previous sections , its efficiency critically depends on the choice of the isd @xmath321 . usually some geometric information about the target domain @xmath12 is needed for constructing a good isd .",
    "to get some intuition , figure  [ fig3 ] shows the domain @xmath12 for two lower dimensional cases : @xmath322 , @xmath323 ( @xmath324 ) and @xmath325 , @xmath323 ( @xmath326 ) .",
    "notice that in both cases , @xmath12 consists of two well separated subsets , @xmath327 , which are approximately symmetric about the origin .",
    "this suggests that a good isd must be a mixture of two distributions @xmath328 and @xmath329 , that effectively sample @xmath330 and @xmath331 , @xmath332     in two dimensional case @xmath324 , where @xmath322 , @xmath323 , and @xmath333 .",
    "@xmath315 samples were generated and marked by red circles ( respectively , green dots ) if they do ( respectively , do not ) belong to @xmath12 .",
    "right panel : the same as on the left panel but with @xmath326 and @xmath325 . ]    in this example , three different isds , denoted @xmath334 and @xmath335 , are considered :    case 1 : : :    @xmath336 , where    @xmath337 .",
    "that is , we    first generate a sample @xmath338    and then take isd @xmath339 as the mixture of gaussian pdfs    centered at @xmath340 and    @xmath341 .",
    "case 2 : : :    @xmath342 , where    @xmath343 is obtained as follows .",
    "first we    generate @xmath344 samples from @xmath314 , and    define @xmath343 to be the sample in    @xmath12 with the smallest norm .",
    "sample    @xmath343 can be interpreted as the `` best    representative '' of @xmath330 ( or    @xmath331 ) , since    @xmath345 has the largest ( among generated    samples ) value .",
    "we then take isd @xmath346 as the mixture of    gaussian pdfs centered at @xmath343 and    @xmath347 .",
    "case 3 : : :    to illustrate what happens if one ignores the geometric information    about two components of @xmath12 , we choose    @xmath348 , as given in case 2 .",
    "let @xmath322 and @xmath333 .",
    "the dimension of the uncertain excitation space is then @xmath349 .",
    "table  [ tab1 ] shows the simulation results for the above three cases as well as for standard mcs .",
    "the is method with @xmath339 , on average , correctly estimates @xmath34",
    ". however the c.o.v . of the estimate is very large , which results in large fluctuations of the estimate in independent runs .",
    "is with @xmath346 works very well and outperforms mcs : the c.o.v .",
    "is reduced by half .",
    "finally , is with @xmath335 completely misses one component part of the target domain @xmath12 , and the resulting estimate is about half of the correct value .",
    "note that the c.o.v . in this case",
    "is very small , which is very misleading .",
    "lll & @xmath350 & @xmath351 + mcs & @xmath352 & 17@xmath353 + is @xmath339 & @xmath354 & 132.4@xmath353 + is @xmath346 & @xmath352 & 8.3@xmath353 + is @xmath335 & @xmath355 & 5.5@xmath353 +     as a function of duration time @xmath311 .",
    "solid red and dashed blue curves correspond to mcs and is with @xmath346 , respectively . in this example ,",
    "@xmath333 and @xmath315 samples for each value of @xmath311 is used .",
    "it is clearly visible how the is estimate degenerates as the dimension @xmath84 goes from 10 ( @xmath322 ) to 100 ( @xmath356 ) . ]",
    "it was mentioned in previous sections that is is often not efficient in high dimensions because it becomes more difficult to construct a good isd @xcite . to illustrate this effect , is with @xmath346 was used to estimate @xmath34 for a sequence of problems where the total duration time gradually grows from @xmath322 to @xmath356 .",
    "this results in an increase of the dimension @xmath84 of the underlying uncertain excitation space from 10 to 100 .",
    "figure  [ fig4 ] shows how the is estimate degenerates as the dimension @xmath84 of the problem increases .",
    "while is is accurate when @xmath349 ( @xmath322 ) , it strongly underestimates the true value of @xmath34 as @xmath84 approaches @xmath357 ( @xmath356 ) .    _",
    "subset simulation : _ ss is a more advanced simulation method and , unlike is , it does not suffer from the curse of dimensionality . for @xmath293 ,",
    "figure  [ fig2 ] shows the estimate of the target probability @xmath34 as a function of @xmath311 using ss with mcmc and splitting . for each value of @xmath311",
    ", @xmath319 samples were used in each conditional level in ss .",
    "unlike mcs , ss is capable of efficiently simulating very rare events and estimating their small probabilities .",
    "the total computational effort , i.e. the total number @xmath50 of samples automatically chosen by ss , is shown in the bottom panel of figure  [ fig2 ] .",
    "note that the larger the value of @xmath34 , the smaller the number of conditional levels in ss , and , therefore , the smaller the total number of samples @xmath50 .",
    "the total computational effort in ss is thus a decreasing function of @xmath311 . in this example",
    ", the original mcmc strategy @xcite for generating conditional samples outperforms the splitting strategy @xcite that exploits the causality of the system : while the ss / mcmc method works even in the most extreme case ( @xmath358 ) , the ss / splitting estimate for @xmath34 becomes zero for @xmath320 .",
    "this chapter examines computational methods for rare - event simulation in the context of uncertainty quantification for dynamic systems that are subject to future uncertain excitation modeled as a stochastic process .",
    "the rare events are assumed to correspond to some time - varying performance quantity exceeding a specified threshold over a specified time duration , which usually means that the system performance fails to meet some design or operation specifications .    to analyze the reliability of the system against this performance failure , a computational model for the input - output behavior of the system",
    "is used to predict the performance of interest as a function of the input stochastic process discretized in time .",
    "this dynamic model may involve explicit treatment of parametric and non - parametric uncertainties that arise because the model only approximately describes the real system s behavior , implying that there are usually no true values of the model parameters and the accuracy of its predictions are uncertain . in the engineering literature , the mathematical problem to be solved numerically for the probability of performance failure , commonly called the failure probability , is referred to as the first - passage reliability problem .",
    "it does not have an analytical solution and numerical solutions must face two challenging aspects :    1 .   the vector representing the time - discretized stochastic process that models the future system excitation lies in an input space of high dimension ; 2 .   the dynamic systems of interest are assume to be highly reliable so that their performance failure is a rare event , that is , the probability of its occurrence , @xmath34 , is very small .    as a result , standard monte carlo simulation and importance sampling methods are not computationally efficient for first - passage reliability problems . on the other hand ,",
    "subset simulation has proved to be a general and powerful method for numerical solution of these problems .",
    "like mcs , it is not affected by the dimension of the input space and for a single run , it produces a plot of @xmath34 vs threshold @xmath13 covering @xmath359 $ ] , where @xmath132 is the number of levels used . for a critical appraisal of methods for first - passage reliability problems in high dimensions ,",
    "the reader may wish to check schuller et al @xcite .",
    "several variants of subset simulation have been developed motivated by the goal of further improving the computational efficiency of the original version , although the efficiency gains , if any , are modest .",
    "all of them have an accuracy described by a coefficient of variation for the estimate of the rare - event probability that depends on @xmath360 rather than @xmath361 as in standard monte carlo simulation .",
    "for all methods covered in this section , the dependence of this coefficient of variation on the number of samples @xmath50 is proportional to @xmath362 .",
    "therefore , in the case of very low probabilities , @xmath34 , it still requires thousands of simulations ( large @xmath50 ) of the response time history based on a dynamic model as in ( [ model ] ) in order to get acceptable accuracy . for complex models ,",
    "this computational effort may be prohibitive .",
    "one approach to reduce the computational effort when estimating very low rare - event probabilities is to utilize additional information about the nature of the problem for specific classes of reliability problems ( e.g. @xcite ) .",
    "another more general approach is to construct surrogate models ( meta - models ) based on using a relatively small number of complex - model simulations as training data .",
    "the idea is to use a trained surrogate model to rapidly calculate an approximation of the response of the complex computational model as a substitute when drawing new samples .",
    "various methods for constructing surrogate models have been applied in reliability engineering , including response surfaces @xcite , support vector machines @xcite , neural networks @xcite , and gaussian process modeling ( kriging ) @xcite .",
    "the latter method is a particularly powerful one because it also provides a probabilistic assessment of the approximation error .",
    "it deserves further exploration , especially with regard to the optimal balance between the accuracy of the surrogate model as a function of the number of training samples from the complex model , and the accuracy of the estimate of the rare - event probability as a function of the total number of samples from both the complex model and the surrogate model .",
    "ching , j. , au , s.k .",
    ", beck , j.l . : reliability estimation for dynamical systems subject to stochastic excitation using subset simulation with splitting . comput .",
    "methods appl .",
    "194 , 1557 - 1579 ( 2005 ) .",
    "jalayer , f. , beck , j.l . :",
    "effects of two alternative representations of ground - motion uncertainty on probabilistic seismic demand assessment of structures .",
    "earthquake eng . and structural dynamics 37 , 61 - 79 ( 2008 ) .",
    "katafygiotis , l.s . , cheung , s.h .",
    ": a two - stage subset simulation - based approach for calculating the reliability of inelastic structural systems subjected to gaussian random excitations .",
    "methods appl .",
    "194 , 1581 - 1595 ( 2005 ) .",
    "pellissetti , m.f . ,",
    "schuller , g.i . ,",
    "pradlwarter , h.j .",
    ", calvi , a. , fransen , s. , klein , m. : reliability analysis of spacecraft structures under static and dynamic loading .",
    "computers @xmath363 structures 84 , 1313 - 1325 ( 2006 ) .",
    "taflanidis , a.a . ,",
    "beck , j.l . : analytical approximation for stationary reliability of certain and uncertain linear dynamic systems with higher dimensional output .",
    "earthquake engineering and structural dynamics 35 , 1247 - 1267 ( 2006 ) .",
    "valdebenito , m.a .",
    ", pradlwarter , h.j . , schuller , g.i . : the role of the design point for calculating failure probabilities in view of dimensionality and structural nonlinearities .",
    "structural safety 32 , 101 - 111 ( 2010 ) .",
    "zuev , k. : subset simulation method for rare event estimation : an introduction . in : m. beer",
    "et al ( eds . ) encyclopedia of earthquake engineering .",
    "springer - verlag berlin heidelberg , ( 2015 ) .",
    "available on - line at http://arxiv.org/abs/1505.03506 .",
    "zuev , k.m . ,",
    "beck , j.l .",
    "katafygiotis , l.s . : bayesian post - processor and other enhancements of subset simulation for estimating failure probabilities in high dimensions .",
    "computers @xmath363 structures 92 - 93 , 283 - 296 ( 2012 ) ."
  ],
  "abstract_text": [
    "<S> rare events are events that are expected to occur infrequently , or more technically , those that have low probabilities ( say , order of @xmath0 or less ) of occurring according to a probability model . in the context of uncertainty quantification , the rare events often correspond to failure of systems designed for high reliability , meaning that the system performance fails to meet some design or operation specifications . </S>",
    "<S> as reviewed in this section , computation of such rare - event probabilities is challenging . </S>",
    "<S> analytical solutions are usually not available for non - trivial problems and standard monte carlo simulation is computationally inefficient . </S>",
    "<S> therefore , much research effort has focused on developing advanced stochastic simulation methods that are more efficient . in this section , </S>",
    "<S> we address the problem of estimating rare - event probabilities by monte carlo simulation , importance sampling and subset simulation for highly reliable dynamic systems . </S>"
  ]
}