{
  "article_text": [
    "similar document detection is the problem of finding similar documents of two parties , alice and bob , and it has been widely used in version management of files , copyright protection , and plagiarism detection@xcite .",
    "recently , secure similar document detection(ssdd)@xcite has been introduced to identify similar documents while preserving privacy of each party s documents as shown in figure [ fig : fig1 ] .",
    "that is , ssdd finds similar document pairs whose cosine similarity@xcite exceeds the given tolerance while not disclosing document vectors to each other party .",
    "ssdd is a typical example of privacy - preserving data mining(ppdm)@xcite , and has the following applications@xcite .",
    "first , in two or more conferences that are not allowing double submissions , ssdd finds the double - submitted papers while not disclosing the papers to each other conference .",
    "second , in the insurance fraud detection system , ssdd searches similar accident cases of two or more insurance companies while not providing sensitive and private cases to each other company .",
    "+    jiang  et  al.@xcite have proposed a novel solution for ssdd by exploiting secure multiparty computation(smc)@xcite in a semi - honest model .",
    "their solution has preserved privacy of two parties by using the secure scalar product in computing cosine similarity between document vectors . as the secure scalar product ,",
    "they have suggested random matrix and homomorphic encryption methods@xcite . in this paper , we use the random matrix method as a base protocol , and we call it ssdd - base . however , ssdd - base has a critical problem of incurring severe computation and communication overhead .",
    "let alice s and bob s document sets be @xmath2 and @xmath3 , respectively , then ssdd - base requires @xmath4 secure scalar products . in many cases ,",
    "the dimension @xmath5 of document vectors reaches tens of thousands or even hundreds of thousands , and ssdd - base incurs a very high complexity of @xmath6 , which is not practical to support a large volume of document databases .",
    "in particular , if there are many parties or frequent changes in document databases , the overhead becomes much more critical .    to alleviate the computation and communication overhead of ssdd - base , in this paper we present a 2-step protocol that exploits the feature selection of lower - dimensional transformation .",
    "the feature selection transforms high dimensional document vectors to low dimensional feature vectors , and in general it selects tens to hundreds dimensions from thousands to tens of thousands dimensions .",
    "we call the feature selection _ fs _ in short .",
    "representative fs includes rp(random projection)@xcite , df(document frequency)@xcite , and lda(linear discriminant analysis)@xcite . in this paper , we use rp and df since they are known as simple but efficient feature selections@xcite . to devise a 2-step protocol , we need to find an upper bound of cosine similarity for the filtering process",
    "thus , we first present an upper bound of fs and formally prove its correctness . using the upper bound property of fs ,",
    "we then propose a generic 2-step protocol , called ssdd - fs .",
    "the proposed ssdd - fs works as follows : in the first _ filtering _ step , it converts @xmath5-dimensional vectors to @xmath7-dimensional vectors and applies the secure protocol to @xmath8-dimensional vectors to filter out non - similar @xmath5-dimensional vectors ; in the second _ post - processing _ step , it applies the base protocol ssdd - base to the non - filtered @xmath5-dimensional vectors . in the filtering step , ssdd - fs prunes many non - similar _ high _ dimensional vectors by comparing _",
    "low _ dimensional vectors with relatively less complexity of @xmath9 , and thus , it significantly improves the performance compared with ssdd - base .    to make ssdd - fs be efficient , fs should be highly discriminative , i.e. , fs should filter out as many high dimensional vectors as possible if they are non - similar . in this paper , we analyze ssdd protocols in detail and propose four different techniques as the discriminative implementation of fs .",
    "we can think rp first as an easiest way of implementing fs .",
    "rp randomly selects @xmath8 dimensions from @xmath5 dimensions .",
    "rp is easy , but its filtering effect will be very low due to the randomness . to solve the problem of rp , we exploit df that selects feature dimensions based on frequencies in all document vectors . in particular , by referring the concept of df , we present three variants of df , called lf(local frequency ) , gf(global frequency ) , and hf(hybrid frequency ) . first , lf considers term frequencies of alice s current querying vector(we call it the _ current vector _ ) , and it selects dimensions whose frequencies higher than the others in the current vector .",
    "lf focuses on the _ locality _ , which means that considering the current vector only might be enough to decrease the upper bound of cosine similarity .",
    "second , gf means df itself , that is , gf counts the number of documents containing each term(dimension ) , constructs a frequency vector from those counts(we call it the _ whole vector _ ) , and selects high frequency dimensions from the whole vector .",
    "gf focuses on the _ globality _ since it considers all the document vectors . to implement gf ,",
    "however , we need to make a secure protocol for obtaining the whole vector from both alice s and bob s document sets . for this",
    ", we propose a secure protocol securedf as a secure implementation of df .",
    "third , hf takes advantage of both locality of lf and globality of gf .",
    "hf computes a _ difference vector _ between current and whole vectors and selects high - valued dimensions from the difference vector .",
    "this is because hf tries to maximize the value difference between alice s and bob s vectors for each selected dimension and eventually decrease the upper bound of cosine similarity .",
    "table [ tbl : tbl1 ] summarizes these four feature selections and their corresponding ssdd protocols , ssdd - rp , ssdd - lf , ssdd - gf , and ssdd - hf , to be proposed in section [ sec : sec4 ] .",
    ".feature selection methods to be used for ssdd - fs .",
    "[ cols=\"^,<\",options=\"header \" , ]     in this paper , we empirically evaluate the base protocol , ssdd - base , and our four ssdd - fs protocols(ssdd - rp , ssdd - lf , ssdd - gf , ssdd - hf ) using various data sets .",
    "experimental results show that the ssdd - fs protocols significantly outperform ssdd - base .",
    "this means that the proposed 2-step protocols effectively prune a large number of non - similar sequences early in the filtering step .",
    "in particular , ssdd - hf that takes advantage of both locality of ssdd - lf and globality of ssdd - gf shows the best performance .",
    "compared with ssdd - base , ssdd - hf reduces the execution time of ssdd by three or four orders of magnitude .",
    "the rest of this paper is organized as follows .",
    "section  [ sec : sec2 ] explains related work and background of the research .",
    "section  [ sec : sec3 ] presents the fs - based 2-step protocol , ssdd - fs , and proves its correctness .",
    "section  [ sec : sec4 ] introduces four novel feature selections , rp , lf , gf , and hf , and it proposes their corresponding secure protocols . section  [ sec : sec5 ] explains experimental results on various data sets .",
    "we finally summarize and conclude the paper in section  [ sec : sec6 ] .",
    "we use cosine similarity as the basic operation of similar document detection .",
    "the cosine similarity of two @xmath5-dimensional vectors @xmath10 and @xmath11 is computed as @xmath12 , where @xmath13 is the scalar product of @xmath14 and @xmath15 , that is , @xmath16 . if we can compute @xmath13 securely in two parties , we can also compute @xmath17 securely .",
    "there are two representative methods for the secure scalar product@xcite .",
    "the first one is the random matrix method@xcite , where two parties share the same random matrix and compute the scalar product securely using the matrix .",
    "the second one is the homomorphic encryption method@xcite , where two parties use the homomorphic probability key system for the secure computation of scalar products . in this paper",
    ", we use the random matrix method since it is more efficient than the homomorphic encryption one , but we can also instead use the homomorphic encryption method for the protocols to be discussed later . without loss of generality , we assume that vectors @xmath14 and @xmath15 are normalized to size @xmath18 .",
    "that is , @xmath19 , and thus , simply @xmath20 .",
    "figure  [ fig : fig2 ] shows the protocol of ssdd - base , the recent solution of ssdd by jiang  et  al.@xcite .",
    "ssdd - base uses the random matrix method@xcite for secure scalar products , where alice and bob share the same matrix @xmath21 and securely determine whether two vectors @xmath14 and @xmath15 are similar or not . for the correctness and detailed explanation on * protocol * ssdd - base ,",
    "readers are referred to @xcite . in ssdd , we perform ssdd - base for each pair of document vectors . more formally , if @xmath2 and @xmath3 are sets of document vectors owned by alice and bob , respectively , we perform ssdd - base for each pair @xmath22 , where @xmath23 and @xmath24 . as we mentioned in section",
    "[ sec : sec1 ] , however , ssdd - base incurs the severe computation and communication overhead of @xmath25 , which will be much serious if there are several parties , or a large number of documents are changed dynamically . to alleviate this critical overhead , in this paper",
    "we discuss the 2-step solution for ssdd .",
    "+    in text mining and time - series mining , many lower - dimensional transformations have been proposed to solve the dimensionality curse problem@xcite of high dimensional vectors .",
    "we can classify lower - dimensional transformations into feature extractions and feature selections@xcite .",
    "first , the feature extraction _ creates _ a few _ new _ features from an original high dimensional vector .",
    "representative examples of feature extractions include lsi(latent semantic indexing)@xcite , lpi(locality preserving indexing)@xcite , dft(discrete fourier transform)@xcite , dwt(discrete wavelet transform)@xcite , and paa(piecewise aggregate approximation)@xcite . in contrary",
    ", the feature selection _ selects _ a few _ discriminative _ features from an original ( or transformed ) high dimensional vectors .",
    "representative examples of feature selections include rp , df , lda , and pca(principal component analysis)@xcite . in this paper",
    ", we use rp and df with appropriate variations .",
    "this is because rp and df are much simpler than other transformations , and accordingly , they are easily applied to ssdd with low complexity ; on the other hand , lsi , lpi , lda , and pca may provide very accurate feature vectors , but they are too complex to be applied to ssdd . for the detailed explanation on lower - dimensional transformations for text mining ,",
    "readers are referred to @xcite .",
    "there have been many efforts on ppdm@xcite .",
    "ppdm solutions can be classified into four categories : data perturbation , @xmath26-anonymization , distributed privacy preservation , and privacy preservation of mining results@xcite .",
    "ssdd can be regarded as an application of distributed privacy privation . for the detailed explanation on problems and solutions of data perturbation and @xmath26-anonymization ,",
    "readers are referred to survey papers@xcite .",
    "in this paper , we use fs , feature selection , for the secure 2-step protocol . to transform an @xmath5-dimensional vector to an @xmath8-dimensional vector",
    ", fs chooses randomly or highly frequent @xmath8 dimensions from @xmath5 dimensions , and thus , its transformation process is very simple . in this section",
    ", we first assume that fs can select @xmath8 dimensions from @xmath5 dimensions in a secure manner , and we then propose the secure 2-step protocol of ssdd by using the secure fs .    to use a lower - dimensional transformation @xmath27 for ssdd , we need to find an upper bound function @xmath28 that satisfies eq .",
    "( [ eq : eq1 ] ) , where @xmath29 and @xmath30 are @xmath8-dimensional feature vectors selected from @xmath5-dimensional vectors , @xmath14 and @xmath15 , respectively , by the transformation @xmath27 . in eq .",
    "( [ eq : eq1 ] ) , @xmath10 , @xmath11 , @xmath31 , and @xmath32 .",
    "@xmath33 the reason why the transformation @xmath27 should satisfy eq .",
    "( [ eq : eq1 ] ) is that ssdd of using @xmath27 should not incur any false dismissal , and this is known as parseval s theorem(the lower bound property of euclidean distances ) in time - series matching@xcite . to obtain an upper bound of the lower - dimensional transformation @xmath27",
    ", we first define an upper bound of @xmath27 as follows .",
    "[ def : def1 ] if a lower - dimensional transformation @xmath27 transforms @xmath5-dimensional vectors , @xmath14 and @xmath15 , to @xmath8-dimensional vectors , @xmath29 and @xmath30 , respectively , we define an _ upper bound function _ of @xmath27 , denoted by @xmath28 , as eq .",
    "( [ eq : eq2 ] ) .",
    "@xmath34 where @xmath35 is the squared euclidean distance between @xmath29 and @xmath30 , i.e. , @xmath36 .",
    "@xmath37    in this paper , we want to use fs as a lower - dimensional transformation @xmath27 , and thus , we formally prove that the upper bound function of fs satisfies eq .",
    "( [ eq : eq1 ] ) , the upper bound property of cosine similarity .",
    "[ th : th1 ] if a feature selection fs transforms @xmath5-dimensional vectors , @xmath14 and @xmath15 , to @xmath8-dimensional vectors , @xmath38 and @xmath39 , respectively , @xmath40 is an upper bound of @xmath17 , that is , eq .  ( [ eq : eq3 ] ) holds .",
    "proof : first , let @xmath10 , @xmath11 , @xmath42 , and @xmath43 . then ,",
    "( [ eq : eq4 ] ) and ( [ eq : eq5 ] ) hold for @xmath14 and @xmath15 .",
    "@xmath44 @xmath45 we note that all entry values of @xmath14 and @xmath15 are non - negative , and fs constructs @xmath38 and @xmath39 by choosing @xmath8 features from @xmath14 and @xmath15 .",
    "based on this property , eq .",
    "( [ eq : eq6 ] ) holds .",
    "@xmath46 finally , eq .  ( [ eq : eq7 ] ) holds by eqs .",
    "( [ eq : eq5 ] ) , ( [ eq : eq6 ] ) , and eq .  ( [ eq : eq2 ] ) of definition  [ def : def1 ] .",
    "@xmath47 therefore , @xmath40 is an upper bound of @xmath17 .",
    "@xmath37    by using the upper bound property of fs , we now propose a generic 2-step protocol ssdd - fs .",
    "figure  [ fig : fig3 ] shows * protocol * ssdd - fs . as shown in the protocol , ssdd - fs maintains @xmath8-dimensional @xmath38 and @xmath38 as well as @xmath5-dimensional @xmath14 and @xmath15 of ssdd - base . also , alice and bob share an @xmath48 matrix @xmath49 as well as an @xmath50 matrix @xmath21 of ssdd - base .",
    "lines 1 to 7 of ssdd - fs are the first step of discarding non - similar @xmath5-dimensional vectors in the @xmath8-dimensional space . first , lines 1 to 4 securely compute the scalar product @xmath51 for @xmath8-dimensional vectors @xmath38 and @xmath39 . except using @xmath8-dimensional vectors instead of @xmath5-dimensional vectors ,",
    "these steps are the same as those of ssdd - base .",
    "the only difference from ssdd - base is that bob additionally sends @xmath52 to alice in line 3 for computing @xmath53 . in line 5 ,",
    "alice computes @xmath54 by using eq .",
    "( [ eq : eq8 ] ) .",
    "@xmath55 after then , alice computes an upper bound function of fs , @xmath40 , in line 6 . in line 7",
    ", we perform the filtering process by comparing the upper bound(@xmath56 ) and the given tolerance(@xmath57 ) .",
    "if the upper bound is less than the tolerance , i.e. if @xmath58 , the actual cosine similarity will also be less than the tolerance , and we do nt need to compute it in the next @xmath5-dimensional space .",
    "that is , if @xmath58 , we can skip line 8 of the second step .",
    "thus , line 8 is executed only if @xmath5-dimensional vectors of @xmath22 are not filtered out by the upper bound . in line 8",
    ", we compute the actual cosine similarity for @xmath22 by using ssdd - base .",
    "+    we here note that how ssdd - fs improves the performance compared with ssdd - base depends on how many @xmath5-dimensional vectors are discarded in the first step .",
    "this filtering effect largely depends on the discriminative power of the feature selection , i.e. , efficiency of fs . in other words ,",
    "if fs exploits the filtering effect largely , ssdd - fs can reduce the computation and communication overhead from @xmath59 to @xmath60 .",
    "based on this observation , we need to maximize the filtering effect of fs , and this can be seen a problem of how we choose @xmath8 dimensions from @xmath5 dimensions for maximizing the discriminative power of fs .",
    "therefore , we propose efficient fs variants and their ssdd protocols in section  [ sec : sec4 ] and evaluate their performance in section  [ sec : sec5 ] .",
    "in this section , we propose four methods to implement fs of * protocol * ssdd - fs .",
    "figure  [ fig : fig4 ] shows the procedure of ssdd - fs including the feature selection step . as shown in the figure , we first obtain @xmath38 and @xmath39 from @xmath14 and @xmath15 through the feature selection which should also be done securely . as mentioned in section  [ sec : sec1 ] , we present rp , lf , gf , and hf as the feature selection method , and we explain how they work in detail in sections  [ ssec : sec41 ] to [ ssec : sec44 ] . in figure",
    "[ fig : fig4 ] , the secure feature selection corresponds to line ( 1 ) of * protocol * ssdd - fs , and the other two steps correspond to the first step(lines 1 to 7 ) and the second step(line 8) , respectively .",
    "+      rp is an easiest way of implementing fs , which selects @xmath8 dimensions randomly from @xmath5 dimensions .",
    "we can think two different methods in applying rp to ssdd - fs .",
    "the first one selects @xmath8 dimensions dynamically for each document pair @xmath22 ; the second one first determines @xmath8 dimensions and then uses those pre - determined dimensions for all document pairs .    to use the first rp method ,",
    "alice and bob should share @xmath8 indexes , @xmath61 , of randomly selected @xmath8 dimensions for each @xmath22 before starting the first step of ssdd - fs .",
    "this sharing process can be implemented as alice randomly selects @xmath8 dimensions and sends their indexes to bob , or alice and bob share the same seed of the random function .",
    "that is , we can implement the first rp method by modifying line ( 1 ) of * protocol * ssdd - fs as lines ( 1 - 1 ) to ( 1 - 3 ) of figure  [ fig : fig5 ] .",
    "+    the second rp method uses the same @xmath8 dimensions for all @xmath22 pairs .",
    "we can easily implement this method as alice and bob share the same @xmath8 indexes only once before starting ssdd - fs .",
    "these first and second rp methods do not disclose any values of alice s and bob s document vectors , and thus , they are said to be secure .",
    "also , these two methods have the same effect in selecting @xmath8 dimensions randomly .",
    "thus , we use the second one since it is much simpler than the first one , and we call the second one ssdd - rp by differentiating it from ssdd - fs .",
    "ssdd - rp proposed in section  [ ssec : sec41 ] has a problem of exploiting only a little filtering effect in the first filtering step .",
    "this low filtering effect is due to that rp chooses features without any consideration of characteristics of document vectors .",
    "according to the real experiments , ssdd - rp shows a very little improvement in ssdd performance compared with ssdd - base . to solve the problem of ssdd - rp and to enlarge the filtering effect , in this paper",
    "we consider how frequent each term is in the document or document set , i.e. , we use the term frequency(tf ) . in general",
    ", we use the tf concept as follows : we first compute the number of occurrences(i.e . ,",
    "frequency ) of each term throughout the whole data set and then choose the highly frequent dimensions .",
    "we call this selection method df(document frequency ) as in @xcite . the reason why we consider tf(or df ) in ssdd - fs",
    "is that , if we select the highly frequent @xmath8 dimensions , we can obtain relatively small upper bounds @xmath28 s by relatively large @xmath35 s of eq .",
    "( [ eq : eq2 ] ) , and accordingly , we can exploit the filtering effect largely .    as a feature selection using term frequencies ,",
    "we first consider how frequent each term is in an individual document rather than the whole document set , that is , we first propose the feature selection of exploiting _ locality _ of each document .",
    "more precisely , for a pair of documents @xmath22 , the locality - based selection chooses @xmath8 dimensions highly frequent in alice s current vector @xmath14 .",
    "this selection is based on the simple intuition that , even without considering whole vectors of the document set , the current vector itself will make a big influence on the upper bound @xmath62 . in this selection , we can instead use bob s vector @xmath15 rather than alice s vector @xmath14 as the current vector , or we can also use both alice s and bob s vectors @xmath14 and @xmath15 . using @xmath15 , however , incurs the additional communication overhead , and thus , in this paper we consider a simple method of using alice s @xmath14 as the current vector .",
    "we call this selection method _ lf_(local frequency ) since it considers individual ( i.e. , local ) documents rather than whole documents , and we denote the protocol of applying lf to ssdd - fs as ssdd - lf .",
    "ssdd - lf exploits the locality by selecting @xmath8 dimensions for each document at every start time .",
    "figure  [ fig : fig6 ] shows how we implement ssdd - lf by modifying line ( 1 ) of ssdd - fs of figure  [ fig : fig3 ] . in line ( 1 - 2 ) , alice first selects top @xmath8 frequent dimensions from her current vector @xmath14 .",
    "she sends those indexes of the selected @xmath8 dimensions to bob in line ( 1 - 3 ) .",
    "thus , they can share the same indexes and obtain @xmath8-dimensional feature vectors by using the same @xmath8 indexes in line ( 1 - 4 ) .",
    "+    we now analyze the computation and communication overhead of feature selection in ssdd - lf .",
    "as shown in figure  [ fig : fig6 ] , for each vector @xmath14 , alice ( 1 ) chooses the top @xmath8 frequent dimensions from @xmath5 dimensions of @xmath14 and ( 2 ) communicates with bob to share those @xmath8 indexes .",
    "first , alice needs the additional computation overhead of @xmath63 to select top @xmath8 frequent dimensions from the current @xmath5-dimensional vector .",
    "second , alice and bob need the additional communication overhead to share the @xmath8 indexes .",
    "however , this communication process can be done with line ( 3 ) of ssdd - base of figure  [ fig : fig2 ] , that is , alice can send @xmath8 indexes together with the encrypted vector @xmath64 to bob .",
    "the amount of @xmath8 indexes is much smaller than that of the @xmath5-dimensional vector , and the overhead of @xmath8 indexes can be negligible .",
    "thus , we can say that ssdd - lf causes the computation overhead of @xmath63 , but the communication overhead can be ignored . in particular",
    ", we compare each vector @xmath14 of alice with a large number of vectors @xmath65 of bob , and thus , the computation overhead of @xmath63 can also be ignored as a pre - processing step .    another considering point in ssdd - lf",
    "is whether its feature selection process is secure or not .",
    "that is , there should be no privacy disclosure when alice selects @xmath8 indexes and shares them with bob .",
    "fortunately , alice sends only indexes @xmath66 to bob rather than entry values @xmath67 of @xmath14 , and the sensitive values @xmath67 are not disclosed in the selection process .",
    "unfortunately , however , the information that which @xmath8 dimensions are frequent in @xmath14 is revealed to bob .",
    "if the user can not be allowable even this limited disclosure of information , s / he can not use ssdd - lf as a secure protocol . in this case",
    ", we recommend to use the previous ssdd - rp or the next ssdd - gf or ssdd - hf as the more secure protocol .",
    "ssdd - lf of section  [ ssec : sec42 ] has a problem of considering only alice s current vector but ignoring all the other vectors of bob . due to this problem ,",
    "ssdd - lf exploits the filtering effect for only a part of bob s vectors , but it does not for most of other vectors . to overcome this problem , in this section",
    "we propose another feature selection that uses the whole vector of which each element represents the number of documents containing the corresponding term .",
    "unlike lf of focusing on the current vector only , it considers whole document vectors , and it has characteristics of globality .",
    "we call this feature selection _ gf_(global frequency ) and denote the gf - based secure protocol as ssdd - gf . actually , gf is the same as df , which has been widely used as the representative feature selection , and it works as follows .",
    "first , let @xmath68 be a whole vector and @xmath69 be a number of documents containing the @xmath26-th term , that is , @xmath69 be the df value of the @xmath26-th term .",
    "then , to reduce the number of dimensions from @xmath5 to @xmath8 , gf simply selects @xmath8 dimensions whose df values are larger than those of the other @xmath70 dimensions .",
    "we can get the whole vector by scanning all the document vectors once .",
    "the traditional df constructs the whole vector based on the assumption that all the document vectors are maintained in a single computer . in ssdd , however , document vectors are distributed in alice and bob , and they do not want to provide their own vectors to each other .",
    "thus , to use gf in ssdd , we first need to present a secure protocol of constructing the whole vector from the document vectors distributively stored in alice and bob .",
    "figure  [ fig : fig7 ] shows * protocol * securedf that securely constructs a whole vector @xmath71 from alice s and bob s document vectors and gets @xmath8 frequent dimensions from @xmath71 . in lines 1 to 8 ,",
    "alice and bob computes their own whole vectors independently . that is , alice computes her own whole vector @xmath72 from her own document set @xmath2 , and bob gets @xmath73 from @xmath3 . in lines 4 and 8 , they share those whole vectors @xmath72 and @xmath73 with each other . in lines 9 to 11 , they then compute the aggregated whole vector @xmath71 from those vectors .",
    "after obtaining the whole vector @xmath71 , alice and bob can select @xmath8 frequent dimensions from @xmath71 .",
    "we note that alice sends @xmath72 to bob in line 4 , and bob sends @xmath73 to alice in line 8 .",
    "vectors @xmath72 and @xmath73 , however , are not exact values of document vectors , but simple statistics , and thus , we can say that securedf does not reveal any privacy of individual documents .",
    "computation and communication complexities of securedf are merely @xmath74 and @xmath75 , respectively .",
    "also , securedf can be seen as a pre - processing step executed only once for all document vectors of alice and bob .",
    "thus , its complexity can be negligible compared with the complexity @xmath76 of ssdd - base .",
    "+    we now explain ssdd - gf which exploits securedf as the feature selection .",
    "figure  [ fig : fig8 ] shows how we modify line ( 1 ) of figure  [ fig : fig3 ] for converting ssdd - fs to ssdd - gf . in line ( 1 - 0 ) , we first perform securedf to obtain the whole vector @xmath71 and determine @xmath8 indexes which are most frequent in @xmath71 . for current @xmath5-dimensional vectors @xmath14 and @xmath15 , alice and bob get @xmath8-dimensional vectors @xmath38 and @xmath39 by using the determined @xmath8 indexes .",
    "as shown in figure  [ fig : fig8 ] , the current vectors and even their term frequencies are not disclosed to each other , and thus , we can say that ssdd - gf is a secure protocol of ssdd .",
    "+      lf and gf proposed in sections  [ ssec : sec42 ] and [ ssec : sec43 ] have the following characteristics in a viewpoint of the filtering effect .",
    "first , lf considers alice s current vector @xmath14 only , and thus , the filtering effect will be large for only a part of bob s vectors whose tf patterns much differ from the current vector , but the effect are less exploited for most of the other vectors .",
    "in other words , lf can exploit the better filtering effect than gf when alice s current vector quite differs from the whole vector in tf patterns .",
    "second , gf considers the whole vector @xmath71 obtained by securedf without considering the current vector , and it thus can exploit the filtering effect relatively evenly on many of bob s document vectors .",
    "that is , gf can exploit the better filtering effect than lf when alice s current vector has the similar characteristics with the whole vector in tf patterns .    to take advantage of both locality of lf and globality of gf ,",
    "we now propose a hybrid feature selection , called _",
    "hf_(hybrid frequency ) .",
    "that is , hf uses the current vector for exploiting locality of lf , and at the same time it also use the whole vector for exploiting globality of gf .",
    "we then present an advanced secure protocol ssdd - hf by applying hf to the ssdd - fs . simply speaking ,",
    "hf compares current and whole vectors and selects feature dimensions whose differences are larger than those of the other dimensions . in more detail",
    ", we select feature dimensions which have one of the following two characteristics : ( 1 ) the dimensions which frequently occur in alice s current vector but seldom occur in the whole vector(i.e . , whose values are relatively large in the current vector but relatively small in the whole vector ) ; or on the contrary , ( 2 ) the dimensions which seldom occur in alice s current vector but frequently occur in the whole vector .",
    "this is because the larger @xmath77(= the difference between values of the selected feature dimension ) , the smaller @xmath78 , i.e. , the larger @xmath79 of eq .",
    "( [ eq : eq2 ] ) , which exploits the larger filtering effect",
    ".    however , we can not directly compare alice s current vector @xmath14 and the whole vector @xmath71 by securedf .",
    "the reason is that @xmath14 represents `` frequencies of terms '' in a single vector while @xmath71 represents `` frequencies of documents '' containing those terms .",
    "that is , the meaning of frequencies in @xmath14 differs from that of @xmath71 , and thus , their scales are also different . to resolve this problem , before comparing two vectors @xmath14 and @xmath71 ,",
    "we first normalize them using their mean(@xmath80 ) and standard deviation(@xmath81 ) .",
    "more precisely , we first normalize @xmath14 and @xmath71 to @xmath82 and @xmath83 by eq .",
    "( [ eq : eq9 ] ) , and we next obtain the difference vector @xmath84 . after then , we select the largest @xmath8 dimensions from @xmath85 and use them as the features of ssdd - hf .",
    "@xmath86    figure  [ fig : fig9 ] shows how we modify line ( 1 ) of ssdd - fs in figure [ fig : fig3 ] to implement ssdd - hf .",
    "first , as in ssdd - gf , line ( 1 - 0 ) constructs the whole vector @xmath71 by executing securedf .",
    "next , in lines ( 1 - 2 ) and ( 1 - 3 ) , we normalize the current and whole vectors and obtain the difference vector @xmath85 from those normalized vectors . finally , in lines ( 1 - 4 )",
    "to ( 1 - 6 ) , alice chooses @xmath8 dimensions from the difference vector @xmath85 and shares those dimensions with bob . that is , lines ( 1 - 4 ) to ( 1 - 6 ) are the same as lines ( 1 - 2 ) to ( 1 - 4 ) of ssdd - lf in figure  [ fig : fig6 ] except that ssdd - lf uses the current vector @xmath14 while ssdd - hf uses the difference vector @xmath85 .",
    "+    the overhead of feature selection in ssdd - hf can be seen as the summation of those in ssdd - lf and ssdd - gf .",
    "that is , like ssdd - gf , it has the overhead of performing securedf to obtain the whole vector @xmath71 , and at the same time , like ssdd - lf , it has the overhead of choosing the largest @xmath8 dimensions from the @xmath5-dimensional difference vector @xmath85",
    ". these overheads , however , can be negligible by the following reasons : ( 1 ) as we explained in ssdd - gf of section  [ ssec : sec43 ] , securedf having @xmath74 and @xmath75 of computation and communication complexities can be seen as a pre - processing step executed only once for all document vectors , and its overhead can be negligible in the whole process of ssdd ; ( 2 ) as we explained in ssdd - lf of section  [ ssec : sec42 ] , the computation complexity @xmath63 of choosing @xmath8 dimensions from an @xmath5-dimensional vector can be ignored since it can also be seen as the pre - processing step .",
    "one more notable point is that ssdd - hf is a secure protocol like ssdd - gf since it uses securedf and the difference vector which are secure and do not disclose any original values or any sensitive indexes of individual vectors .",
    "in this section , we empirically evaluate feature selection - based ssdd protocols proposed in section 4 . as the experimental data , we use three datasets obtained from the document sets of uci repository@xcite .",
    "these datasets are kos blog entries , nips full papers , and enron emails , which have been frequently used in text mining .",
    "the first dataset consists of kos blog entries collected from dailykos.com , and we call it _ kos_. kos consists of 3,430 documents with 6,906 different terms(dimensions ) , and it has total 467,714 terms",
    ". the second dataset contains nips full papers published in neural information processing systems conference , and we call it _ nips_. nips consists of 1,500 documents with 12,419 different terms , and it has about 1.9 million terms in total .",
    "the third dataset contains e - mail messages of enron , and we call it _ emails_. emails consists of 39,861 e - mails with 28,102 different terms , and it has about 6.4 million terms in total .    we experiment five ssdd protocols : ssdd - base as the basic one and four proposed ones of ssdd - rp , ssdd - lf , ssdd - gf , and ssdd - hf . in the experiment , we basically measure the elapsed time of executing ssdd for each protocol . in the first experiment , we vary the number of dimensions for a fixed tolerance , where the number of dimensions means @xmath8 , i.e. , the number of _ selected _ features(dimensions ) by the feature selection . in the second experiment , we vary the tolerance for a fixed number of dimensions . for these two experiments ,",
    "we use kos and nips , which have a relatively small number of documents compared with emails . on the other hand ,",
    "the third experiment is to test scalability of each protocol , and we thus use emails whose number of documents is much larger than those of kos and nips .",
    "the hardware platform is hp proliant ml110 g7 workstation equipped with intel(r ) xeon(r ) quad core cpu e31220 3.10ghz , 16 gb ram , and 250 gb hdd ; its software platform is centos 6.5 linux .",
    "we use c language for implementing all the protocols .",
    "we perform ssdd in a single machine using a local loop for network communication .",
    "the reason why we use the local loop is that we want to intentionally ignore the network speed since different network speeds or environments may largely distort the actual execution time of each protocol .",
    "we measure the execution time spent for that alice sends each document to bob and identifies its similarity securely .",
    "more precisely , we store the whole dataset in bob and select ten query documents for alice .",
    "after then , we execute each ssdd protocol for those ten query documents and use their sum as the experimental result .      figure  [ fig : fig10 ] shows the experimental results for kos .",
    "first , in figure  [ fig : fig10](a ) , we set the tolerance to 0.80 and vary the number of documents by 70 , 210 , 350 , 490 , and 640 , which correspond to 1% , 3% , 5% , 7% , and 9% of kos documents .",
    "as shown in the figure , @xmath87 axis shows the number of ( selected ) dimensions , and @xmath88 axis does the actual execution time .",
    "note that the @xmath88 axis is a log scale .",
    "+    figure  [ fig : fig10](a ) shows that all proposed protocols significantly outperform the basic ssdd - base .",
    "even ssdd - rp of selecting features randomly beats ssdd - base by exploiting the filtering effect in the first step of the 2-step protocol .",
    "next , ssdd - gf shows the better performance than ssdd - rp since it selects the frequently occurred features throughout the whole dataset by using df . in case of ssdd - rp and ssdd - gf",
    ", we note that , as the number of dimensions increases , the execution time decreases .",
    "this is because the more number of dimensions we use , the larger filtering effect we can exploit .",
    "ssdd - lf of using locality of the current vector also outperforms ssdd - rp as well as ssdd - base .",
    "in particular , ssdd - lf is better than ssdd - gf for a small number of dimensions , but it is worse than ssdd - gf for a large number of dimensions .",
    "this is because only a small number of dimensions make a big influence on the locality of the current vector .",
    "finally , ssdd - hf of taking advantage of both ssdd - lf and ssdd - gf shows the best performance for all dimensions . in figure  [ fig : fig10](a ) , we note that the execution time of ssdd - lf and ssdd - hf slightly increases as the number of dimensions increases .",
    "the reason is that , as the number @xmath8 of dimensions increases , the filtering effect increases relatively slowly , but the overhead of obtaining a current / difference vector and choosing @xmath8 dimensions from that vector increases relatively quickly .",
    "second , in figure  [ fig : fig10](b ) , we set the number of dimensions to 70(1% of total dimensions ) and vary the tolerance from 0.95 to 0.75 by decreasing 0.05 . note that the closer to 1.0",
    "the tolerance is , the stronger similarity we use . as shown in the figure ,",
    "all proposed protocols significantly improve the performance compared with ssdd - base .",
    "in particular , ssdd - lf and ssdd - hf , which exploits the locality , show the better performance than the other two proposed ones .",
    "we here note that , as the tolerance decreases , execution times of all proposed protocols gradually increase .",
    "this is because the smaller tolerance we use , the more documents we get as similar ones .",
    "that is , as the tolerance decreases , the more documents pass the first step , and thus , the more time is spent in the second step . in summary of figure",
    "[ fig : fig10 ] , the proposed ssdd - lf and ssdd - hf significantly outperform ssdd - base by up to 726.6 and 9858 times , respectively .    figure  [ fig : fig11 ] shows the experimental results for nips .",
    "as in figure  [ fig : fig10 ] of kos , we measure the execution time of ssdd by varying the number of dimensions and the tolerance . in figure",
    "[ fig : fig11](a ) , we set the tolerance 0.80 and increase the number of dimensions from 120(1% ) to 600(5% ) by 120(1% ) , where 120 means 1% of total 12,419 documents .",
    "next , in figure  [ fig : fig11](b ) , we set the number of dimensions to 120 and decrease the tolerance from 0.95 to 0.75 by 0.05 . the experimental results of figures  [ fig : fig11](a ) and [ fig : fig11](b ) show a very similar trend with those of figures  [ fig : fig10](a ) and [ fig : fig10](b ) .",
    "that is , all proposed protocols significantly outperform ssdd - base , and ssdd - hf shows the best performance . in figure",
    "[ fig : fig11 ] , ssdd - hf extremely improves the performance compared with ssdd - base by up to 16620 times .",
    "+    figure [ fig : fig12 ] shows the results for scalability test using a large volume of high dimensional dataset , emails .",
    "we set the tolerance and the number of dimensions to 0.80 and 70 , respectively , and we increase the number of documents(emails ) from 40(0.1% ) to 39,861(100% ) by 10 times . in this experiment , we exclude the results of ssdd - base , ssdd - rp , and ssdd - gf for the case of 39,861 documents due to excessive execution time .",
    "as shown in the figure , like the results of kos and nips , our feature selection - based protocols outperform ssdd - base at all cases , and in particular , ssdd - lf and ssdd - hf show the best performance regardless of the number of documents .",
    "we also note that all proposed protocols show a pseudo linear trend on the number of documents .",
    "( please note that @xmath87- and @xmath88-axis are all log scales . )",
    "that is , the protocols are pseudo linear solutions on the number of documents , and we can say that they are excellent in scalability as well as performance .",
    "in this paper , we addressed an efficient method of significantly reducing computation and communication overhead in secure similar document detection .",
    "contributions of the paper can be summarized as follows .",
    "first , we thoroughly analyzed the previous 1-step protocol and pointed out that it incurred serious performance overhead for high dimensional document vectors .",
    "second , to alleviate the overhead , we presented the feature selection - based 2-step protocol and formally proved its correctness .",
    "third , to improve the filtering efficiency of the 2-step protocol , we proposed four feature selections : ( 1 ) rp of selecting features randomly , ( 2 ) lf of exploiting locality of a current vector , ( 3 ) gf of exploiting globality of all document vectors , and ( 4 ) hf of considering both locality and globality .",
    "fourth , for each feature selection , we presented its formal protocol and analyzed its secureness and overhead .",
    "fifth , through experiments on three real datasets , we showed that all proposed protocols significantly outperformed the base protocol , and in particular , the hf - based secure protocol improved performance by up to three or four orders of magnitude .",
    "as the future work , we will consider two issues : ( 1 ) use of feature extraction(feature creation ) instead of feature selection for dimensionality reduction and ( 2 ) use of homomorphic encryption rather than random matrix for the secure scalar product .",
    "s. berchtold , c. bohm , and h .-",
    "kriegel , `` the pyramid - technique : towards breaking the curse of dimensionality , '' in _ proc .",
    "intl conf . on management of data",
    "_ , acm sigmod , seattle , washington , pp .",
    "142 - 153 , june 1998 .",
    "e. bertino , d. lin , and w. jiang , `` a survey of quantification of privacy preserving data mining algorithms , '' in _ privacy - preserving data mining : models and algorithms _ , c. c. aggarwal and p. s. yu ( eds . ) , pp . 183 - 205 , kluwer academic publishers , june 2008 .",
    "e. bingam and h. mannila , `` random projection in dimensionality reduction : applications to image and text data , '' in _ proc . the 7th intl conf . on knowledge discovery and data mining _ , acm sigkdd , san francisco , california , pp .",
    "245 - 250 , aug . 2001 .",
    "chan , a. w .- c .",
    "fu , and c. t. yu , `` haar wavelets for efficient similarity search of time - series : with and without time warping , '' _ ieee trans . on knowledge and data engineering _ , vol .",
    "686 - 705 , jan./feb . 2003 .",
    "s. deerwester , t. dumais , w. furnas , k. landauer , and r. harshman , `` indexing by latent semantic analysis , '' _ journal of the american society for information science _",
    "41 , no . 6 , pp .",
    "391 - 407 , sept .",
    "1990 .    c. faloutsos , m. ranganathan , and y. manolopoulos ,",
    "`` fast subsequence matching in time - series databases , '' in _ proc . of intl conf . on management of data",
    "_ , acm sigmod , minneapolis , minnesota , pp .",
    "419 - 429 , may 1994 .",
    "b. goethals , s. laur , h. lipmaa , and t. mielikainen , `` on secure scalar product computation for privacy - preserving data mining , '' in _ proc . of the 7th annual intl conf . in information security & cryptology",
    "_ , seoul , korea , pp .",
    "104 - 120 , dec . 2004 .",
    "han , j. lee , y .- s .",
    "moon , s. hwang , h. yu , `` a new approach for processing ranked subsequence matching based on ranked union , '' in _ proc .",
    "of intl conf . on management of data",
    "_ , acm sigmod , athens , greece , pp .",
    "457 - 468 , june 2011 .",
    "w. jiang , m. murugesan , c. clifton , and l. si , `` similar document detection with limited information disclosure , '' in _ proc .",
    "of the 24th ieee intl conf . on data",
    "engineering _ , cancun , mexico , pp .",
    "735 - 743 , apr . 2008 .",
    "moon , k .- y .",
    "whang , and w .- s .",
    "han , `` generalmatch : a subsequence matching method in time - series databases based on generalized windows , '' in _ proc . of intl conf .",
    "on management of data _ , acm sigmod , madison , wisconsin , pp . 382 - 393 , june 2002 .",
    "moon , h .- s .",
    "kim , and e. bertino , `` publishing time - series data under preservation of privacy and distance orders , '' in _ proc . of the 21st intl conf . on database and expert systems applications _ ,",
    "part ii , pp .",
    "17 - 31 , bilbao , spain , aug . 2010 .",
    "moon , b .-",
    "kim , m. s. kim , and k .- y .",
    "whang , `` scaling - invariant boundary image matching using time - series matching techniques '' , _ data & knowledge engineering _ , vol .",
    "1022 - 1042 , oct .",
    "2010 .",
    "n. shivakumar and h. garcia - molina , `` scam : a copy detection mechanism for digital documents , '' in _ proc .",
    "of the 2nd intl conf . in theory and practice of digital libraries _ ,",
    "austin , texas , pp . 398 - 409 , june 1995 .",
    "b. tang , m. shepherd , and e. milios , `` comparing and combining dimension reduction techniques for efficient text clustering , '' in _ proc . of intl workshop on feature selection for data mining : interfacing machine learning and statistics _ , newport beach , california , pp .",
    "17 - 26 , apr . 2005 .",
    "j. vaidya and c. clifton , `` privacy preserving association rule mining in vertically partitioned data , '' in _ proc . of the 8th intl conf . on knowledge discovery and data mining _ , acm sigkdd , alberta , canada , pp .",
    "639 - 644 , july 2002 ."
  ],
  "abstract_text": [
    "<S> secure similar document detection(ssdd ) identifies similar documents of two parties while each party does not disclose its own _ sensitive _ documents to another party . in this paper </S>",
    "<S> , we propose an efficient 2-step protocol that exploits a feature selection as the lower - dimensional transformation and presents discriminative feature selections to maximize the performance of the protocol . </S>",
    "<S> for this , we first analyze that the existing 1-step protocol causes serious computation and communication overhead for high dimensional document vectors . to alleviate the overhead , we next present the feature selection - based 2-step protocol and formally prove its correctness . </S>",
    "<S> the proposed 2-step protocol works as follows : ( 1 ) in the _ filtering _ step , it uses low dimensional vectors obtained by the feature selection to filter out non - similar documents ; ( 2 ) in the _ post - processing _ step , it identifies similar documents only from the non - filtered documents by using the 1-step protocol . as the feature selection , we first consider the simplest one , random projection(rp ) , and propose its 2-step solution ssdd - rp . </S>",
    "<S> we then present two discriminative feature selections and their solutions : ssdd - lf(local frequency ) which selects a few dimensions locally frequent in the current querying vector and ssdd - gf(global frequency ) which selects ones globally frequent in the set of all document vectors . </S>",
    "<S> we finally propose a hybrid one , ssdd - hf(hybrid frequency ) , that takes advantage of both ssdd - lf and ssdd - gf . </S>",
    "<S> we empirically show that the proposed 2-step protocol outperforms the 1-step protocol by three or four orders of magnitude . + * keywords * : secure similar document detection , cosine similarity , feature selection , lower - dimensional transformation , term frequency , document frequency    * efficient 2-step protocol and its discriminative feature selections + in secure similar document detection *    sang - pil kim@xmath0 , myeong - sun gil@xmath0 , yang - sae moon@xmath0 , and hee - sun won@xmath1    @xmath0department of computer science , kangwon national university + 1 kangwondaehak - gil , chuncheon - si , gangwon 200 - 701 , republic of korea @xmath1electronics and telecommunications research institute + 218 gajeong - ro , yuseong - gu , daejeon 305 - 701 , republic of korea e - mail : \\{spkim , gils , ysmoon}@kangwon.ac.kr , hswon@etri.re.kr </S>"
  ]
}