{
  "article_text": [
    "as a dual of the compressed sensing ( cs ) problem @xcite , the matrix rank minimization problem has been extensively studied in recent years @xcite .",
    "this problem arises in many fields such as system identification @xcite , computer vision @xcite and quantum state tomography @xcite , where notions of order , dimensionality or complexity can be expressed in terms of the rank of a matrix .",
    "let @xmath4 be an @xmath5 matrix of low rank rank @xmath3 , the sampling operation can be expressed as @xmath6^t = \\mathbf{a } \\text{vec}(\\mathbf{x}),\\ ] ] where @xmath7 is a vector of measurements , @xmath0 is a linear transformation , @xmath8 denotes the transform in matrix format in which @xmath9 ( without loss of generality we assume that @xmath10 ) ; and @xmath11 , ( @xmath12 ) is the operator that vectorizes the matrix @xmath4 by concatenating the columns as a long vertical vector @xmath13 .    in compressed sensing , besides conventional gaussian",
    "/ bernoulli random sensing approaches , many structured / deterministic sensing matrices have been proven suitable for recovery of compressible signals .",
    "the interest in using structured sensing matrices in cs stems from the application needs , mainly due to their low complexity in computation and memory , as well as hardware implementation @xcite .    while we note several attempts in cs to use structured sensing matrices , contributions to low rank matrix reconstruction are more scarce .",
    "there are some efforts such as in @xcite .",
    "an existing key condition of @xmath14 is the so called rank restricted isometry property ( r - rip ) @xcite .",
    "however , examining the r - rip of a given operator @xmath14 is np - hard . in this paper",
    ", we show that if the @xmath8 consists of independent identically distributed ( i.i.d . )",
    "random variables forming a piecewise toeplitz structure , it is feasible to recover the objective low rank matrix from its measurements uniquely . instead of studying its r - rip , we convert the uniqueness problem of rank minimization into a compressed sensing problem , and analyze the performance bounds of proposed matrices by using the tools from structured sensing matrix analysis in cs .",
    "the extension is not trivial , since the vectorized low rank matrix is no longer sparse .",
    "by utilizing such technique we may reduce the memory required to store the sampling operator from @xmath1 to @xmath15 at the expense of a few measurements under mild assumptions .",
    "the rest of the paper is organized as follows . in section",
    "ii we formulate the problem and introduce the proposed piecewise toeplitz matrices .",
    "the main result and the corresponding proof are presented in section iii .",
    "simulations are given in section iv and finally section v addresses the conclusion .",
    "there are existing random or structured matrices for the rank minimization problem , such as @xcite .",
    "the idea of this paper different from the previous work is to use toeplitz structured operators inspired from structured sensing matrices in cs @xcite .",
    "after decomposing the low rank matrix , we analyze the uniqueness of recovering a block sparse vector ( as defined in @xcite ) . in @xcite",
    "the authors also adopted the gershgorin circle theorem to bound the eigenvalues of toeplitz matrices , while in this paper the different coherence expression makes the analysis much more complicated .",
    "the idea of decomposing low rank matrix by its columns also relates to cur matrix decomposition and the nystrom method @xcite .",
    "in contrast , to these previous approaches , the core problem here is the unique reconstruction rather than the decomposition , so the coherence and rip analyses are adopted .",
    "to exploit the low rank property , we suppose that the low rank matrix @xmath16 $ ] has rank @xmath3 so that @xmath3 of its columns @xmath17 can represent the remaining @xmath18 columns explicitly by their linear combination .",
    "denote the selected @xmath3 columns as @xmath19 , @xmath20 , @xmath21 , @xmath22 ; and the remaining @xmath18 columns as @xmath23 , @xmath24 , @xmath25 .",
    "we call them primary columns and secondary columns , respectively",
    ". then the @xmath18 secondary columns can be represented as @xmath26 @xmath27 .",
    "its matrix multiplication form is    @xmath28 = \\left\\{\\underbrace { \\left [ \\begin{array}{ccc } \\alpha_{{*_{1}}\\diamond_{1 } }   & \\cdots & \\alpha_{*_{1}\\diamond_{r } } \\\\",
    "\\alpha_{*_{2}\\diamond_{1 } }   & \\cdots & \\alpha_{*_{2}\\diamond_{r } } \\\\ \\vdots & \\ddots & \\vdots \\\\   \\alpha_{*_{n_2-r}\\diamond_{1 } }   & \\cdots & \\alpha_{*_{n_2-r}\\diamond_{r } } \\end{array } \\right ]   } \\otimes \\mathbf{i}_{n_1 } \\right\\ } \\left [ \\begin{array}{c } \\mathbf{x}_{\\diamond_1 } \\\\ \\mathbf{x}_{\\diamond_2 } \\\\ \\vdots \\\\",
    "\\mathbf{x}_{\\diamond_{r } } \\end{array } \\right],\\\\ { \\boldsymbol\\alpha }   \\end{array}\\ ] ]    where @xmath29 denotes the kronecker product .",
    "normally @xmath30 , i.e. @xmath31 is a tall matrix .",
    "thus @xmath32 can also be decomposed in a block sparse manner as @xmath33 where @xmath34 is a sparse matrix with block diagonal matrices @xmath35 or @xmath36 of size @xmath37 , @xmath38 is a block @xmath3-sparse vector ( adopting the definition in @xcite ) @xmath39^t$ ] .",
    "then ( [ eq : y = avec(x ) ] ) can be written as : @xmath40 where @xmath41 .",
    "this block sparse problem has been studied by analyzing the block - restricted isometry constant @xcite . however , here the matrix @xmath42 is formed by the multiplication of @xmath8 and an unknown structured sparsifying matrix @xmath34 in ( [ eq : x = psif ] ) . and",
    "obviously this @xmath34 is neither unitary , nor can be constructed delicately from complete bases .",
    "it is an unknown block diagonal sparse matrix determined by the low rank matrix @xmath4 .",
    "the same low rank matrix may even lead to multiple @xmath34 and @xmath38 .",
    "thus the conventional theory of block cs may encounter difficulties for such a problem .",
    "a set of matrices @xmath43 of size @xmath5 are defined as piecewise toeplitz matrices if @xmath44 } \\begin{split } \\mathbf{a}_1= \\text{vec}(\\mathbf{a}_1)^t&= [ \\mathbf{a}_{11}^t,\\mathbf{a}_{12}^t , \\cdots , \\mathbf{a}_{1n_2}^t ] , \\\\ & \\vdots \\\\",
    "\\text{vec}(\\mathbf{a}_m)^t&= [ \\mathbf{a}_{m1}^t,\\mathbf{a}_{m2}^t , \\cdots , \\mathbf{a}_{mn_2}^t ] , \\end{split}\\ ] ] where @xmath45 denotes the @xmath46th column of the matrix @xmath47 , and their piecewise concatenation matrices @xmath48 } \\begin{array}{cccc } \\mathbf{a}[1 ] = \\left [ \\begin{array}{c } \\mathbf{a}_{11}^t \\\\ \\vdots \\\\ \\mathbf{a}_{m1}^t \\\\ \\end{array } \\right ] & \\cdots & \\mathbf{a}[n_2 ] = \\left [ \\begin{array}{c } \\mathbf{a}_{1n_2}^t \\\\ \\vdots \\\\ \\mathbf{a}_{mn_2}^t \\\\ \\end{array } \\right ]    \\end{array}\\ ] ] are all toeplitz .",
    "[ prop : theta expression ] for the measurement process @xmath49 , and a matrix @xmath4 of size @xmath50 and rank @xmath3 , ( [ eq : y = avec(x ) ] ) is equivalent to @xmath51 , where @xmath38 is a block @xmath3-sparse vector , and @xmath52 of size @xmath53 has the structure @xmath54 & \\mathbf{\\theta}[2 ] & \\cdots & \\mathbf{\\theta}[i ] & \\cdots & \\mathbf{\\theta}[n ] \\end{array } \\right]$ ] in which @xmath55= \\left\\ { \\begin{array}{ccl } \\mathbf{a}[i]+ \\sum_{*}{\\alpha_{*i}\\mathbf{a } [ * ] } & \\mbox{if } & i \\in \\{\\diamond\\ } \\\\",
    "\\mathbf{0 } & \\mbox{if } & i \\notin \\{\\diamond\\ } \\end{array}\\right.\\ ] ] for all @xmath56 .",
    "@xmath57\\}$ ] are matrices derived from @xmath58 by concatenating their columns piecewisely as in ( [ eq : a[i ] ] ) .",
    "@xmath59 represent the sets of primary columns and secondary columns indexes , respectively .",
    "@xmath61 $ ] are also toeplitz when @xmath62 .    because @xmath4 has rank @xmath3 , @xmath63 \\",
    "\\mathbf{a}[2 ] \\ \\cdots \\",
    "\\mathbf{a}[n ] \\right ]   \\mathbf{\\psi }   \\mathbf{f } = \\mathbf{\\theta f}.\\ ] ] then it is straightforward to verify the expression in _ prop . _ 1 .",
    "because @xmath64 $ ] are toeplitz , @xmath61 $ ] must be toeplitz as well when @xmath62 .",
    "* remark * : since the decomposition @xmath65 is not unique , @xmath42 have different expressions corresponding to various @xmath38 , which distinguishes ( [ eq : y = aaapsif ] ) from cs with multiple solutions @xmath38 . fortunately what we need to recover is not @xmath38 but @xmath66 .",
    "multiple @xmath38 may lead to a unique solution @xmath66 .",
    "[ prop : unique - rec ] the reconstruction of matrix @xmath4 with size @xmath5 and rank @xmath3 in ( [ eq : y = a psi f ] ) has a unique solution @xmath67 if @xmath68 holds for every @xmath69 in ( [ eq : y = a psi f ] ) which is block @xmath70-sparse , where @xmath71 has the structure in ( [ eq : theta0 ] ) .",
    "assume that there is a new solution @xmath72 to ( [ eq : y = a psi f ] ) with @xmath73 , @xmath74 , which means that @xmath75 let @xmath76 , where @xmath77 is a nonzero matrix of rank at most @xmath70 .",
    "then there must exist an @xmath78 that is block @xmath70-sparse such that @xmath79 , where @xmath80 , which contradicts the assumption .",
    "please note that @xmath81 if @xmath82 .",
    "the result is arrived from the above analyses .    * remark * : _ prop .",
    "_ [ prop : unique - rec ] is an extension of the uniqueness guarantee from a sparse vector in cs to a low rank matrix .",
    "@xmath42 depends on the unknown matrix @xmath4 , @xmath38 is not unique .",
    "however , because @xmath77 has rank at most @xmath70 , it must be decomposed into @xmath83 and a @xmath70-sparse vector @xmath78 , which is in contradiction to the assumption .",
    "[ prop : epsilon_bound ] consider @xmath84 with the structure in ( [ eq : theta0 ] ) .",
    "denote by @xmath85 $ ] the submatrix formed by retaining the column blocks of @xmath42 indexed by @xmath86 . if the normalized gram matrix @xmath87 of every @xmath85 $ ] has bound @xmath88 with some positive values @xmath89 , then the eigenvalues of @xmath90)$ ] are bounded by @xmath91 , @xmath92 , and ( [ eq : y = a psi f ] ) has the unique block @xmath70-sparse solution @xmath93 when @xmath94 is bounded by the rip constant .    the proof is based on the gershgorin circle theorem @xcite , and can be derived from the rip and _ prop . _",
    "[ prop : unique - rec ] .",
    "now we give the main result of this paper .",
    "the details of _ condition 1 _ and _ assumption 1 _ are provided in the appendix .",
    "consider the measurements @xmath49 .",
    "let @xmath95 be piecewise random toeplitz matrices whose entries satisfy _ condition 1_. @xmath4 is a rank @xmath3 matrix satisfying _ assumption 1 _ , when @xmath96 , @xmath97 , then there exists a constant @xmath98 such that for any fixed @xmath4 has a unique solution @xmath99 with probability exceeding @xmath100 as @xmath101 .",
    "the proof exploits inner products of any two columns of @xmath42 in order to bound the eigenvalues .",
    "denote the @xmath102th column in matrix @xmath103 $ ] as @xmath104 , p \\in \\{1,\\cdots , n_2\\ } , q \\in \\{1,\\cdots , n_1\\}$ ] .",
    "for the entries in one row of the gram matrix @xmath87 , there are four circumstances of non - zero @xmath105^t \\mathbf{\\theta}[p_2 , q_2]$ ] : ( 1 ) @xmath106 ; ( 2 ) @xmath107 ; ( 3)@xmath108 ; ( 4 ) @xmath109 .",
    "we will analyze them case by case .",
    "\\(1 ) when @xmath106 , they are the diagonal entries of the gram matrix . without loss of generality , we calculate @xmath110 , which implies @xmath111 . following the expressions in _ prop . _",
    "[ prop : theta expression ] , the @xmath112th column of @xmath42 can be calculated is @xmath113= \\mathbf{a}[\\diamond_i , q ] + \\sum_{*}{\\alpha _ { * \\diamond_i } \\mathbf{a } [ * , q]}.\\ ] ] suppose @xmath114(k)| \\leq a$ ] where @xmath115 is a positive bound .",
    "there is some positive value @xmath116 that @xmath117 ^ 2(k ) ) = \\gamma_{i}^2 \\sigma^2 $ ] for every @xmath118 , @xmath119 . by exploiting hoeffding s inequality",
    "we have @xmath120 ^ 2(k ) } - \\gamma_i^2 \\sigma^2 m \\right| \\geq t_0 \\right\\ }    \\leq 2 \\exp{\\left ( -\\frac{2 t_0 ^ 2 } { m a^4 } \\right ) } .\\ ] ]    [ fig : matrix1 ]    ( 20,17)(8,0 )    ( 2,1)(5,0)2(0,1)13 ( 11,1)(5,0)2(0,1)13 ( 20,1)(5,0)2(0,1)13 ( 29,1)(5,0)2 ( 0,1)13 ( 2,1)(1,0)32.01 ( 2,6)(1,0)32.01 ( 2,7)(1,0)32.01 ( 2,13)(1,0)32.01 ( 2,14)(1,0)32.01 ( 0,0)@xmath121 ( 0,13)@xmath122 ( 0,3)@xmath123 ( 0,6)@xmath124 ( 0,9)@xmath123    ( 8,15)@xmath125 ( 17,15)@xmath125 ( 26,15)@xmath125 ( 8,9)@xmath125 ( 17,9)@xmath125 ( 26,9)@xmath125 ( 8,3)@xmath125 ( 17,3)@xmath125 ( 26,3)@xmath125    ( 3.2,15)@xmath126 $ ] ( 11.6,15)@xmath127 $ ] ( 20.6,15)@xmath128 $ ] ( 29.6,15)@xmath129 $ ]    ( 13.4,1)(1,0)2(0,1)13 ( 22.4,1)(1,0)2(0,1)13 ( 13.7,0)@xmath130 ( 22.7,0)@xmath131 ( 0,9)@xmath123    ( 13.4,5.9)@xmath132 ( 22.4,5.9)@xmath132    \\(2 ) when @xmath133 , they represent two columns in different block @xmath134 , \\mathbf{\\theta}[p_2]$ ] but have relatively the same internal positions .",
    "let @xmath135 , the two columns @xmath136 , \\mathbf{\\theta}[\\diamond_j , q]= \\mathbf{a}[\\diamond_j , q]$ ] can be expressed as @xmath137= \\mathbf{a}[\\diamond_i , q ] + \\sum_{*}{\\alpha _ { * \\diamond_i } \\mathbf{a } [ * , q ] } \\\\",
    "\\mathbf{\\theta}[\\diamond_j , q]= \\mathbf{a}[\\diamond_j , q ] + \\sum_{*}{\\alpha _ { * \\diamond_j } \\mathbf{a } [ * , q]}.   \\end{split}\\ ] ] because all entries in @xmath8 are i.i.d . in different blocks , @xmath138(k )",
    "\\cdot \\mathbf{\\theta}[\\diamond_j , q](k ) \\right ) } \\\\ & = \\mathbb{e}{\\left ( \\sum_{*}{\\alpha _ { * \\diamond_i } \\mathbf{a } [ * , q](k ) } \\sum_{*}{\\alpha _ { * \\diamond_j } \\mathbf{a } [ * , q](k ) } \\right ) } \\\\ & = \\sum_{*}{\\alpha _ { * \\diamond_i } \\alpha _ { * \\diamond_j } \\cdot \\sigma^2 } \\end{split}\\ ] ] although @xmath136(k ) ,   \\mathbf{\\theta}[\\diamond_j , q](k)$ ] are dependent due to the mutual combination terms , we still can use hoeffding s inequality to bound the summation , because for different @xmath139 , @xmath136(k ) \\cdot   \\mathbf{\\theta}[\\diamond_j , q](k)$ ] are i.i.d .. let @xmath140 , then there exists some positive @xmath141 such that @xmath142^t \\mathbf{\\theta}[\\diamond_j , q ] - \\kappa_{ij}\\sigma^2 m \\right| \\geq t_1 \\right\\ } } \\leq 2\\exp{\\left ( - \\frac{t_1 ^ 2}{2 m a^4 } \\right)}.\\ ] ]    \\(3 ) when @xmath143 , the two columns are in the same block @xmath144 with different internal index @xmath145 : @xmath146= \\mathbf{a}[\\diamond_i , q_1 ] + \\sum_{*}{\\alpha _ { * \\diamond_i } \\mathbf{a } [ * , q_1 ] } \\\\",
    "\\mathbf{\\theta}[\\diamond_i , q_2]= \\mathbf{a}[\\diamond_i , q_2 ] + \\sum_{*}{\\alpha _ { * \\diamond_i } \\mathbf{a } [ * , q_2]}.   \\end{split}\\ ] ] here a natural problem comes out : for these two columns , they are not independent any more due to the toeplitz structure . here",
    "we use  divide and conquer \" technique that separates the sum into two groups that have no mutual terms .",
    "for instance , if @xmath147 , the sum can be divided as @xmath148^t \\mathbf{\\theta}[\\diamond_i , q_2]&=\\begin{array}{c } \\underbrace{\\sum_{k=1}^{d}\\theta[\\diamond_i , q_1](k)\\theta[\\diamond_i , q_2](k+d ) + \\cdots } \\\\ \\text{first group } \\end{array }   \\\\ & \\begin{array}{c } \\underbrace{+\\sum_{k = d+1}^{2d}\\theta[\\diamond_i , q_1](k)\\theta[\\diamond_i , q_2](k+d ) + \\cdots } \\\\ \\text{second group } \\end{array } ,   \\end{split}\\ ] ] and it is always possible to find a partition that divides @xmath149^t \\mathbf{\\theta}[\\diamond_i , q_2]$ ] into two parts as sums with size @xmath150 for even @xmath151 and @xmath152 for odd @xmath151 .",
    "thus for some positive value @xmath153 , @xmath154^t \\mathbf{\\theta}[\\diamond_i , q_2 ] \\right| \\geq t_2 \\right\\ } \\leq 4 \\exp{\\left ( - \\frac{t_2 ^ 2}{8 m a^4 } \\right)}.\\ ] ]    \\(4 ) when @xmath155 , similar to case ( 3 ) , we divide @xmath156 into @xmath157 parts without mutual terms , giving @xmath154^t \\mathbf{\\theta}[\\diamond_j , q_2 ] \\right| \\geq t_3 \\right\\ } \\leq 4 \\exp{\\left ( - \\frac{t_3 ^ 2}{8 m a^4 } \\right)}.\\ ] ]    finally , we normalize the gram matrix and summarize the diagonal elements and off - diagonal elements .",
    "assume that @xmath4 satisfies the _ statistical low rank property _ defined in the _ def .",
    "[ def : slrp ] _ ( see appendix ) .",
    "suppose @xmath158 , @xmath159 .",
    "let @xmath160 , @xmath161 , and let @xmath162 .",
    "after tedious calculation of bounding bias factor @xmath163 and @xmath164 , we obtain the normalized result by summarizing the four cases above and adopting the gershgorin theorem @xmath165 hence there must exist a constant @xmath166 that @xmath167 whenever @xmath168 as @xmath169 then @xmath170 . use",
    "_ prop . _ [ prop : unique - rec],[prop : epsilon_bound ] to derive the last step , which completes the proof .    *",
    "remark * : 1 ) for gaussian matrices with i.i.d .",
    "entries , the measurements for the recovery of a low rank matrix should be at least @xmath171 @xcite . in our case , the measurement price is the extra factor @xmath3 .",
    "\\2 ) for random gaussian matrices , one needs @xmath1 memory to store the operator . by using the piecewise toeplitz structure , we are able to reduce the memory requirement to @xmath172 as @xmath173 .",
    "\\3 ) the proof holds when @xmath174 in order to avoid the situation that @xmath175 . in practice",
    "it is feasible to apply @xmath8 to the case when @xmath176 are close .",
    "our simulations verify this conjecture numerically .",
    "extensive simulations have been carried out to compare the reconstruction performances of random and proposed operators . here",
    "we present some results .",
    "we utilize @xmath177 different algorithms , including a ) cvx toolbox to minimize the nuclear norm @xcite ; b ) alternating least - squares ( als ) algorithm @xcite c ) directional - als algorithm @xcite , to compare the reconstruction results , respectively . for each algorithm",
    ", we recover a @xmath178 random low rank matrix using different @xmath8 such as gaussian / bernoulli operators , 3-valued operators @xcite and finally random piecewise toeplitz operators with truncated gaussian entries .",
    "1 depicts a comparison of reconstruction errors with increasing rank @xmath3 at sampling rate @xmath179 .",
    "each point is recorded as an average of 200 trials . from these curves",
    "one can observe that the performance of the proposed operator is close to that of random matrices , which are typically considered as the optimal universal operators .",
    "in addition , the proposed operators may be equipped with fast reconstruction algorithms potentially by exploiting the toeplitz structure like in cs @xcite .",
    "this paper proposes piecewise toeplitz matrices as structured linear operators in the matrix minimization problem , and proves that it is feasible to recover the low rank matrix uniquely when the number of measurements exceeds @xmath180 under mild assumptions .",
    "experimental results show that the proposed operators compare favorably with existing random operators .",
    "( b ) using alternating least - squares ( als ) algorithm ( c ) using directional - als algorithm . , width=283 ]    ( a )     ( b ) using alternating least - squares ( als ) algorithm ( c ) using directional - als algorithm . , width=283 ]    ( b )     ( b ) using alternating least - squares ( als ) algorithm ( c ) using directional - als algorithm . , width=283 ]    ( c )    [ fig : m - als_vs_als2 ]",
    "_ condition 1 _ ( @xmath95 ) + @xmath181 is an @xmath5 matrix whose entries are bounded i.i.d .",
    "@xmath182-mean @xmath183-variance random variables satisfying @xmath184 for some @xmath185 , @xmath186 ; in addition , @xmath187 are a set of piecewise toeplitz matrices defined in sec .",
    "[ sec : ptm ] .",
    "j.  haupt , w.  bajwa , g.  raz , and r.  nowak , `` toeplitz compressed sensing matrices with applications to sparse channel estimation , '' _ ieee trans .",
    "inform . theory _",
    "56 , no .  11 , pp .",
    "58625875 , nov ."
  ],
  "abstract_text": [
    "<S> this paper proposes a set of piecewise toeplitz matrices as the linear mapping / sensing operator @xmath0 for recovering low rank matrices from few measurements . </S>",
    "<S> we prove that such operators efficiently encode the information so there exists a unique reconstruction matrix under mild assumptions . </S>",
    "<S> this work provides a significant extension of the compressed sensing and rank minimization theory , and it achieves a tradeoff between reducing the memory required for storing the sampling operator from @xmath1 to @xmath2 but at the expense of increasing the number of measurements by @xmath3 . </S>",
    "<S> simulation results show that the proposed operator can recover low rank matrices efficiently with a reconstruction performance close to the cases of using random unstructured operators .    </S>",
    "<S> rank minimization , toeplitz matrix , compressed sensing , coherence </S>"
  ]
}