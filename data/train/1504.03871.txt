{
  "article_text": [
    "humans can effortlessly and rapidly recognize surrounding objects  @xcite , despite the tremendous variations in the projection of each object on the retina  @xcite caused by various transformations such as changes in object position , size , pose , illumination condition and background context  @xcite .",
    "this invariant recognition is presumably handled through hierarchical processing in the so - called ventral pathway .",
    "such hierarchical processing starts in v1 layers , which extract simple features such as bars and edges in different orientations  @xcite , continues in intermediate layers such as v2 and v4 , which are responsive to more complex features  @xcite , and culminates in the inferior temporal cortex ( it ) , where the neurons are selective to object parts or whole objects  @xcite . by moving from the lower layers to the higher layers , the feature complexity , receptive field size and transformation invariance increase , in such a way that the it neurons can invariantly represent the objects in a linearly separable manner  @xcite .",
    "another amazing feature of the primates visual system is its high processing speed . the first wave of image - driven neuronal responses in it appears around 100 ms after the stimulus onset  @xcite .",
    "recordings from monkey it cortex have demonstrated that the first spikes ( over a short time window of 12.5 ms ) , about 100 ms after the image presentation , carry accurate information about the nature of the visual stimulus  @xcite .",
    "hence , ultra - rapid object recognition is presumably performed in a feedforward manner  @xcite . moreover , although there exist various intra- and inter - area feedback connections in the visual cortex , some neurophysiological  @xcite and theoretical  @xcite studies have also suggested that the feedforward information is usually sufficient for invariant object categorization . appealed by the impressive speed and performance of the primates visual system ,",
    "computer vision scientists have long tried to `` copy '' it .",
    "so far , it is mostly the architecture of the visual system that has been mimicked .",
    "for instance , using hierarchical feedforward networks with restricted receptive fields , like in the brain , has been proven useful  @xcite . in comparison , the way that biological visual systems learn the appropriate features has attracted much less attention .",
    "all the above - mentioned approaches somehow use non biologically plausible learning rules .",
    "yet the ability of the visual cortex to wire itself , mostly in an unsupervised manner , is remarkable  @xcite .    here",
    ", we propose that adding bio - inspired learning to bio - inspired architectures could improve the models behavior . to this end , we focused on a particular form of synaptic plasticity known as spike timing - dependent plasticity ( stdp ) , which has been observed in the mamalian visual cortex  @xcite .",
    "briefly , stdp reinforces the connections with afferents that significantly contributed to make a neuron fire , while it depresses the others  @xcite .",
    "a recent psychophysical study provided some indirect evidence for this form of plasticity in the human visual cortex  @xcite .    in an earlier study  @xcite , it is shown that a combination of a temporal coding scheme  where in the entry layer of a spiking neural network the most strongly activated neurons fire first  with stdp leads to a situation where neurons in higher visual areas will gradually become selective to complex visual features in an unsupervised manner .",
    "these features are both salient and consistently present in the inputs .",
    "furthermore , as learning progresses , the neurons responses rapidly accelerates .",
    "these responses can then be fed to a classifier to do a categorization task .    in this study , we show that such an approach strongly outperforms state - of - the - art computer vision algorithms on view - invariant object recognition benchmark tasks including 3d - object  @xcite and eth-80  @xcite datasets .",
    "these datasets contain natural and unsegmented images , where objects have large variations in scale , viewpoint , and tilt , which makes their recognition hard  @xcite , and probably out of reach for most of the other bio - inspired models  @xcite .",
    "yet our algorithm generalizes surprisingly well , even when `` simple classifiers '' are used , because stdp naturally extracts features that are class specific .",
    "this point was further confirmed using mutual information  @xcite and representational dissimilarity matrix ( rdm )  @xcite .",
    "moreover , the distribution of objects in the obtained feature space was analyzed using hierarchical clustering  @xcite , and objects of the same category tended to cluster together .",
    "the algorithm we used here is a scaled - up version of the one presented in  @xcite .",
    "essentially , many more c2 features and iterations were used .",
    "our code is available upon request .",
    "we used a five - layer hierarchical network @xmath0 @xmath1 @xmath2 @xmath1 @xmath3 @xmath1 @xmath4 @xmath1 @xmath5 , largely inspired by the hmax model  @xcite ( see fig .",
    "[ model_figure ] ) .",
    "specifically , we alternated simple cells that gain selectivity through a sum operation , and complex cells that gain shift and scale invariance through a max operation .",
    "however , our network uses spiking neurons and operates in the temporal domain : when presented with an image , the first layer s @xmath0 cells , detect oriented edges and the more strongly a cell is stimulated the earlier it fires .",
    "these @xmath0 spikes are then propagated asynchronously through the feedforward network .",
    "we only compute the first spike fired by each neuron ( if any ) , which leads to efficient implementations .",
    "the justification for this is that later spikes are probably not used in ultra - rapid visual categorization tasks in primates  @xcite .",
    "we used restricted receptive fields and a weight sharing mechanism ( i.e. convolutional network ) . in our model ,",
    "images are presented sequentially and the resulting spike waves are propagated through to the @xmath3 layer , where stdp is used to extract diagnostic features .    more specifically , the first layer s @xmath0 cells detect bars and edges using gabor filters . here",
    "we used @xmath6 convolutional kernels corresponding to gabor filters with the wavelength of 5 and four different preferred orientations ( @xmath7 ) .",
    "these filters are applied to five scaled versions of the original ) .",
    "hence , for each scaled version of the input image we have four @xmath0 maps ( one for each orientation ) , and overall , there are 4@xmath85 @xmath9 20 maps of @xmath10 cells ( see the @xmath10 maps of fig .",
    "[ model_figure ] ) . evidently , the @xmath0 cells of larger scales detect edges with higher spatial frequencies while the smaller scales extract edges with lower spatial frequencies . indeed , instead of changing the size and spatial frequency of gabor filters",
    ", we are changing the size of input image .",
    "this is a way to implement scale invariance at a low computational cost .",
    "each @xmath10 cell emits a spike with a latency that is inversely proportional to the absolute value of the convolution .",
    "thus , the more strongly a cell is stimulated the earlier it fires ( intensity - to - latency conversion , as observed experimentally  @xcite ) . to increase the sparsity at a given scale and location ( corresponding to one cortical column ) , only the spike corresponding to the best matching orientation",
    "is propagated ( i.e. a winner - take - all inhibition is employed ) . in other word , for each position in the four @xmath0 orientation maps of a given scale , the @xmath0 cell with highest convolution value emits a spike and prevents the other three @xmath0 cells from firing .    for each @xmath10 map , there is a corresponding @xmath11 map .",
    "each @xmath11 cell propagates the first spike emitted by the @xmath10 cells in a @xmath12 square neighborhood of the @xmath10 map which corresponds to one specific orientation and one scale ( see the @xmath11 maps of fig .  [ model_figure ] ) .",
    "@xmath11 cells thus execute a maximum operation over the @xmath10 cells with the same preferred feature across a portion of the visual field , which is a biologically plausible way to gain local shift invariance  @xcite .",
    "the overlap between the afferents of two adjacent @xmath2 cells is just one @xmath0 row , hence a subsampling over the @xmath0 maps is done by the @xmath2 layers as well .",
    "therefore , each @xmath2 map has @xmath13 fewer cells than the corresponding @xmath0 map .",
    "@xmath14 features correspond to intermediate - complexity visual features which are optimum for object classification  @xcite .",
    "each @xmath14 feature has a prototype @xmath14 cell ( specified by a @xmath11-@xmath14 synaptic weight matrix ) , which is a weighted combination of bars ( @xmath11 cells ) with different orientations in a @xmath15 square neighborhood .",
    "each prototype @xmath14 cell is retinotopically duplicated in the five scale maps ( i.e. weight - sharing is used ) . within those maps",
    ", the @xmath14 cells can integrate spikes only from the four @xmath11 maps of their corresponding processing scales .",
    "this way , a given @xmath14 feature is simultaneously explored in all positions and scales ( see @xmath14 maps of fig .",
    "[ model_figure ] with same feature prototype but in different processing scales specified by different colors ) .",
    "indeed , duplicated cells in all positions of all scale maps integrate the spike train in parallel and compete with each other . the first duplicate reaching its threshold , if any , is the winner .",
    "the winner fires and prevents the other duplicated cells in all other positions and scales from firing through a winner - take - all inhibition mechanism .",
    "then , for each prototype , the winner @xmath14 cell triggers the unsupervised stdp rule and its weight matrix is updated .",
    "the changes in its weights are applied over all other duplicate cells in different positions and scales ( weight sharing mechanism ) .",
    "this allows the system to learn frequent patterns , independently of their position and size in the training images .",
    "the learning process begins with @xmath14 features initialized by random numbers drawn from a normal distribution with mean @xmath16 and std @xmath17 , and the threshold of all @xmath14 cells is set to 64 ( @xmath18 ) . through the learning process , a local inhibition between different @xmath14 prototype cells is used to prevent the convergence of different @xmath14 prototypes to similar features : when a cell fires at a given position and scale , it prevents all the other cells ( independently of their preferred prototype ) from firing later at the same scale and within a neighborhood around the firing position .",
    "thus , the cell population self - organizes , each cell trying to learn a distinct pattern so as to cover the whole variability of the inputs .",
    "moreover , we applied a k - winner - take - all strategy in @xmath14 layer to ensure that at most two cells can fire for each processing scale .",
    "this mechanism , only used in the learning phase , helps the cells to learn patterns with different real sizes . without it ,",
    "there is a natural bias toward  small \" patterns ( i.e. , large scales ) , simply because corresponding maps are larger , and so likeliness of firing with random weights at the beginning of the stdp process is higher .    a simplified version of stdp is used to learn the @xmath19 weights as follows :    @xmath20 where @xmath21 and @xmath22 respectively refer to the index of post- and presynaptic neurons , @xmath23 and @xmath24 are the corresponding spike times , @xmath25 is the synaptic weight modification , and @xmath26 and @xmath27 are two parameters specifying the learning rate .",
    "note that the exact time difference between two spikes ( @xmath28 ) does not affect the weight change , but only its sign is considered .",
    "these simplifications are equivalent to assuming that the intensity - to - latency conversion of @xmath10 cells compresses the whole spike wave in a relatively short time interval ( say , @xmath29 ms ) , so that all presynaptic spikes necessarily fall close to the postsynaptic spike time , and the time lags are negligible .",
    "the multiplicative term @xmath30 ensures the weights remain in the range [ 0,1 ] and maintains all synapses in an excitatory mode .",
    "the learning phase starts by @xmath31 which is multiplied by 2 after each 400 postsynaptic spikes up to a maximum value of @xmath32 . a fixed @xmath33 ratio ( -4/3 ) is used .",
    "this allows us to speed up the convergence of @xmath14 features as the learning progresses .",
    "initiation of the learning phase with high learning rates would lead to erratic results .    for each @xmath14 prototype",
    ", a @xmath34 cell propagates the first spike emitted by the corresponding @xmath14 cells over all positions and processing scales , leading to the global shift- and scale - invariant cells ( see the @xmath34 layer of fig .",
    "[ model_figure ] ) .",
    "to study the robustness of our model with respect to different transformations such as scale and viewpoint , we evaluated it on the _ 3d - object _ and _ eth-80 _ datasets .",
    "the 3d - object is provided by savarese et al . at cvglab , stanford university  @xcite .",
    "this dataset contains 10 different object classes : bicycle , car , cellphone , head , iron , monitor , mouse , shoe , stapler , and toaster .",
    "there are about 10 different instances for each object class .",
    "the object instances are photographed in about 72 different conditions : eight view angles , three distances ( scales ) , and three different tilts .",
    "the images are not segmented and the objects are located in different backgrounds ( the background changes even for different conditions of the same object instance ) .",
    "figure  [ objects ] presents some examples of objects in this dataset .",
    "the eth-80 dataset includes 80 3d objects in eight different object categories including apple , car , toy cow , cup , toy dog , toy horse , pear , and tomato .",
    "each object is photographed in 41 viewpoints with different view angles and different tilts .",
    "figure s1 in supplementary information provides some examples of objects in this dataset from different viewpoints .    for both datasets ,",
    "five instances of each object category are selected for the training set to be used in the learning phase .",
    "the remaining instances constitute the testing set which is not seen during the learning phase , but is used afterward to evaluate the recognition performance .",
    "this standard cross - validation procedure allows to measure the generalization ability of the model beyond the specific training examples .",
    "note that for 3d - object dataset , the original size of all images were preserved , while the images of eth-80 dataset are resized to @xmath35 pixels in height while preserving the aspect ratio .",
    "the images of both datasets were converted to grayscale values .        [ cols=\"^,^,^,^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     in an other experiment , we analyzed the class dependency of the @xmath4 features for our model . to this end , the 50 most informative features , when classifying a specific class against all the other classes ,",
    "are selected by employing the mutual information technique . in other words , for each class , we selected those 50 features which have the highest activity for samples of that class and have less activity for other classes .",
    "afterwards , the number of common features among the informative features of each pair of classes are computed as provided in table  [ table_example2 ] . on average , there are only about 5.4 common features between pairs of classes .",
    "although there are some common features between any two classes , their co - occurrence with the other features help the classifier to separate them from each other . in this way , our model can represent various object classes with a relatively small number of features .",
    "indeed , exploiting the intermediate complexity features , which are not common in all classes and are not very rare , can help the classifier to discriminate instances of different classes  @xcite .      in a previous study  @xcite",
    ", it has been shown that using the hmax model with random dot patterns in the @xmath14 layer can reach a reasonable performance , comparable to the one obtained with random patches cropped from the training images .",
    "it seems that this is due to the dependency of hmax to the application of a powerful classifier .",
    "indeed , the use of both random dot or randomly selected patches transform the images into a complex and nested feature space and it is the classifier which looks for a complex signature to separate object classes .",
    "the deficiencies emerge when the classification problem gets harder ( such as invariant or multiclass object recognition problems ) and then even a powerful classifier is not able to discriminate the classes  @xcite . here",
    ", we show that the superiority of our model is due to the informative feature extraction through a bio - inspired learning rule . to this end , we have compared the performances on 3d - object dataset obtained with random features versus stdp features , as well as a very simple classifier versus svm .    to generate random features ,",
    "we have set the weight matrix of each @xmath14 feature of our model with random values .",
    "first , we have computed the mean and standard deviation ( std ) ( @xmath36 ) of the number of active ( nonzero ) weights in the features learned by stdp .",
    "second , for each random feature , the number of active weights , @xmath37 , is computed by generating a random number based on the obtained mean and std .",
    "finally , a random feature is constructed by uniformly distributing the @xmath37 randomly generated values in the weight matrix .",
    "in addition , we designed a simple classifier comprised of several one - versus - one classifiers .",
    "for each binary classifier , two subset of @xmath34 features with high occurrence probabilities in one of the two classes are selected . in more details , to select suitable features for the first class , the occurrence probabilities of @xmath34 features in this class are divided by the corresponding occurrence probabilities in the second class .",
    "then , a feature is selected if this ratio is higher than a threshold .",
    "the optimum threshold value is computed by a trial and error search in which the performance over the training samples is maximized . to assign a class label to the input test sample",
    ", we performed an inner product on the feature value and feature probability vectors .",
    "finally , the class with the highest probability is reported to the combined classifier .",
    "the combined classifier selects the winner class based on a simple majority voting .    for 500 random features , using the svm and the simple classifier , our model reached classification performances of 71% and 21% on average , respectively .",
    "whereas , for the learned @xmath14 features , both the svm and simple classifiers attained reasonable performances of 96% and 79% , respectively . based on these results , it can be concluded that the features obtained through the bio - inspired unsupervised learning projects the objects into an easily separable space , while the feature extraction by selection of random patches ( drawn from the training images ) or by generation of random patterns leads to a complex object representation .",
    "position and scale invariance in our model are built - in , thanks to weight sharing and scaling process .",
    "conversely , view - invariance must be obtained through the learning process . here , we used all images of five object instances from each category ( varied in all dimensions ) to learn the @xmath14 visual features , while images of all other object instances of each category were used to test the network .",
    "hence , the model was exposed to all possible variations during the learning to gain view - invariance . moreover , near or opposite views of the same object shares some features which are suitable for invariant object recognition .",
    "for instance , consider the overall shape of a head , or close views of a bike wheel which could be a complete circle or an ellipse .",
    "regarding the fact that stdp tends to learn more frequent features in different images , different views of an object could be invariantly represented based on more common features .",
    "our model appears to be the best choice when dealing with few object classes , but huge variations in view points .",
    "as pointed out in previous studies , both hmax and deepconvnet models could not handle these variations perfectly  @xcite .",
    "conversely , our model is not appropriate to handle many classes , which requires thousands of features , like in the imagenet contest , because its time complexity is roughly in @xmath38 , where @xmath37 is the number of features ( briefly : since the number of firing neurons per image is limited , if the number of features is doubled , reaching convergence will take roughly twice as many images , and the processing time for each of them will be doubled as well ) .",
    "for example , extracting 4096 features in our model , the same number of features in deepconvnet , would take about 67 times it took us to extract 500 .",
    "however , parallel implementation of our algorithm could speed - up the computation time by several orders of magnitude  @xcite . even in this case",
    ", we do not expect to outperform the deepconvnet model on the imagenet database , since only the shape similarities are taken into account in our model and the other cues such as color or texture are ignored .",
    "importantly , our algorithm has a natural tendency to learn salient contrasted regions  @xcite , which is desirable as these are typically the most informative  @xcite .",
    "most of our @xmath4 features turned out to be class - specific , and we could guess what they represent by doing the reconstructions ( see fig .  [ features ] and fig .",
    "since each feature results from averaging multiple input images , the specificity of each instance is averaged out , leading to class archetypes .",
    "consequently , good classification results can be obtained using only a few features , or even using ` simple ' decision rules like feature counts  @xcite and majority voting ( here ) , as opposed to a ` smart classifier ' such as svm .",
    "there are some similarities between stdp - based feature learning , and non - negative matrix factorization  @xcite , as first intuited in  @xcite , and later demonstrated mathematically in  @xcite . within both approaches ,",
    "objects are represented as ( positive ) sums of their parts , and the parts are learned by detecting consistently co - active input units .",
    "our model could be efficiently implemented in hardware , for example using address event representation ( aer )  @xcite . with aer ,",
    "the spikes are carried as addresses of sending or receiving neurons on a digital bus .",
    "time ` represents itself ' as the asynchronous occurrence of the event  @xcite .",
    "thus the use of stdp will lead to a system which effectively becomes more and more reactive , in addition to becoming more and more selective .",
    "furthermore , since biological hardware is known to be incredibly slow , simulations could run several order of magnitude faster than real time  @xcite .",
    "as mentioned earlier , the primate visual system extracts the rough content of an image in about 100ms .",
    "we thus speculate that some dedicated hardware will be able to do the same in the order of a millisecond or less .",
    "recent computational  @xcite , psychophysical  @xcite , and fmri  @xcite experiments demonstrate that the informative intermediate complexity features are optimal for object categorization tasks .",
    "but the possible neural mechanisms to extract such features remain largely unknown .",
    "the hmax model ignores these learning mechanisms and imprints its features with random crops from the training images  @xcite , or even uses random filters  @xcite .",
    "most individual features are thus not very informative , yet in some cases , a ` smart ' classifier such as svm can efficiently separate the high - dimensional vectors of population responses .",
    "many other models use supervised learning rules  @xcite , sometimes reaching impressive performance on natural image classification tasks  @xcite .",
    "the main drawback of these supervised methods , however , is that learning is slow and requires numerous labeled samples ( e.g. , about 1 million in  @xcite ) , because of the credit assignment problem  @xcite .",
    "this contrasts with humans who can generalize efficiently from just a few training examples  @xcite .",
    "we avoid the credit assignment problem by keeping the @xmath4 features fixed when training the final classifier ( that being said , fine - tuning them for a given classification problem would probably increase the performance of our model  @xcite ; we will test this in future studies ) .",
    "even if the efficiency of such hybrid unsupervised - supervised learning schemes has been known for a long time , few alternative unsupervised learning algorithms have been shown to be able to extract complex and high - level visual features ( see  @xcite ) .",
    "finding better representational learning algorithms is thus an important direction for future research and seeking for inspiration in the biological visual systems is likely to be fruitful  @xcite .",
    "we suggest here that the physiological mechanism known as stdp is an appealing start point .    considering the time relation among the incoming inputs is an important aspect of spiking neural networks .",
    "this property is critical to promote the existing models from static vision to continuous vision  @xcite .",
    "a prominent example is the trace learning rule  @xcite , suggesting that the invariant object representation in ventral visual system is instructed by the implicit temporal contiguity of vision .",
    "also , in various motion processing and action recognition problems  @xcite , the important information lies in the appearance timing of input features .",
    "our model has this potential to be extended for continuous and dynamic vision  something that we will further explore .",
    "to date , various bio - inspired network architectures for object recognition have been introduced , but the learning mechanism of biological visual systems has been neglected . in this paper , we demonstrate that the association of both bio - inspired network architecture and learning rule results in a robust object recognition system .",
    "the stdp - based feature learning , used in our model , extracts frequent diagnostic and class specific features that are robust to deformations in stimulus appearance .",
    "it has previously been shown that the trivial models can not tolerate the identity preserving transformations such as changes in view , scale , and position . to study the behavior of our model confronted with these difficulties , we have tested our model over two challenging invariant object recognition databases which includes instances of 10 different object classes photographed in different views , scales , and tilts .",
    "the categorization performances indicate that our model is able to robustly recognize objects in such a severe situation .",
    "in addition , several analytical techniques have been employed to prove that the main contribution to this success is provided by the unsupervised stdp feature learning , not by the classifier . using representational dissimilarity matrix ,",
    "we have shown that the representation of input images in @xmath34 layer are more similar for within - category and dissimilar for between - category objects . in this way , as confirmed by the hierarchical clustering , the objects with the same category are represented in neighboring regions of @xmath34 feature space . hence , even if using a simple classifier , our model is able to reach an acceptable performance , while the random features fail .",
    "we would like to thank mr .",
    "majid changi ashtiani at the math computing center of ipm ( http://math.ipm.ac.ir/mcc ) for letting us to perform some parts of the calculations on their computing cluster .",
    "we also thank dr .",
    "reza ebrahimpour for his helpful discussions and suggestions .",
    "10 url # 1`#1`urlprefixhref # 1#2#2 # 1#1    s.  thorpe , d.  fize , c.  marlot , et  al . ,",
    "speed of processing in the human visual system , nature 381  ( 6582 ) ( 1996 ) 520522 .",
    "i.  biederman , recognition - by - components : a theory of human image understanding .",
    ", psychological review 94  ( 2 ) ( 1987 ) 115 .",
    "j.  j. dicarlo , d.  zoccolan , n.  c. rust , how does the brain solve visual object recognition ? , neuron 73  ( 3 ) ( 2012 ) 415434 .",
    "p.  lennie , j.  a. movshon , coding of color and form in the geniculostriate visual pathway ( invited review ) , journal of the optical society of america a 22  ( 10 ) ( 2005 ) 20132033 .",
    "a.  s. nandy , t.  o. sharpee , j.  h. reynolds , j.  f. mitchell , the fine structure of shape tuning in area v4 , neuron 78  ( 6 ) ( 2013 ) 11021115 .",
    "k.  tanaka , h .- a .",
    "saito , y.  fukada , m.  moriya , coding visual images of objects in the inferotemporal cortex of the macaque monkey , journal of neurophysiology 66  ( 1 ) ( 1991 ) 170189 .",
    "c.  p. hung , g.  kreiman , t.  poggio , j.  j. dicarlo , fast readout of object identity from macaque inferior temporal cortex , science 310  ( 5749 ) ( 2005 ) 863866 .",
    "n.  c. rust , j.  j. dicarlo , selectivity and tolerance ( `` invariance '' ) both increase as visual information propagates from cortical area v4 to it , journal of neuroscience 30  ( 39 ) ( 2010 ) 1297812995 .",
    "h.  liu , y.  agam , j.  r. madsen , g.  kreiman , timing , timing , timing : fast decoding of object information from intracranial field potentials in human visual cortex , neuron 62  ( 2 ) ( 2009 ) 281290 .",
    "w.  a. freiwald , d.  y. tsao , functional compartmentalization and viewpoint generalization within the macaque face - processing system , science 330  ( 6005 ) ( 2010 ) 845851 .",
    "f.  anselmi , j.  z. leibo , l.  rosasco , j.  mutch , a.  tacchetti , t.  poggio , unsupervised learning of invariant representations with low sample complexity : the magic of sensory cortex or a new framework for machine learning ? , arxiv preprint arxiv:1311.4158 ( 2014 ) 123 .",
    "k.  fukushima , neocognitron : a self organizing neural network model for a mechanism of pattern recognition unaffected by shift in position . ,",
    "biological cybernetics 36  ( 4 ) ( 1980 ) 193202 .",
    "y.  lecun , y.  bengio , convolutional networks for images , speech , and time series , in : m.  a. arbib ( ed . ) , the handbook of brain theory and neural networks , cambridge , ma : mit press , 1998 , pp .",
    "255258 .",
    "t.  serre , l.  wolf , s.  bileschi , m.  riesenhuber , t.  poggio , robust object recognition with cortex - like mechanisms , ieee transactions on pattern analysis machine intelligence 29  ( 3 ) ( 2007 ) 411426 . http://dx.doi.org/10.1109/tpami.2007.56 [ ] .",
    "h.  lee , r.  grosse , r.  ranganath , a.  y. ng , convolutional deep belief networks for scalable unsupervised learning of hierarchical representations , proceedings of the 26th annual international conference on machine learning ( icml ) ( 2009 ) 18http://dx.doi.org/10.1145/1553374.1553453 [ ] .",
    "a.  krizhevsky , i.  sutskever , g.  hinton , imagenet classification with deep convolutional neural networks . , in : neural information processing systems ( nips ) , lake tahoe , nevada , 2012 , pp .",
    "q.  v. le , building high - level features using large scale unsupervised learning , in : 2013 ieee international conference on acoustics , speech and signal processing , ieee , 2013 , pp .",
    "http://dx.doi.org/10.1109/icassp.2013.6639343 [ ] .",
    "g.  m. ghose , learning in mammalian sensory cortex .",
    ", current opinion in neurobiology 14  ( 4 ) ( 2004 ) 513518 .",
    "z.  kourtzi , j.  j. dicarlo , learning and neural plasticity in visual object recognition .",
    ", current opinion in neurobiology 16  ( 2 ) ( 2006 ) 152158 . http://dx.doi.org/10.1016/j.conb.2006.03.012 [ ] .    c.  d. meliza , y.  dan , receptive - field modification in rat visual cortex induced by paired visual stimulation and single - cell spiking .",
    ", neuron 49  ( 2 ) ( 2006 ) 183189 . http://dx.doi.org/10.1016/j.neuron.2005.12.009 [ ] .",
    "s.  huang , c.  rozas , m.  trevino , j.  contreras , s.  yang , l.  song , t.  yoshioka , h .- k .",
    "lee , a.  kirkwood , associative hebbian synaptic plasticity in primate visual cortex , journal of neuroscience 34  ( 22 ) ( 2014 ) 75757579 .",
    "http://dx.doi.org/10.1523/jneurosci.0983-14.2014 [ ] .",
    "d.  e. feldman , the spike - timing dependence of plasticity .",
    ", neuron 75  ( 4 ) ( 2012 ) 556571 . http://dx.doi.org/10.1016/j.neuron.2012.08.001 [ ] .",
    "d.  b.  t. mcmahon , d.  a. leopold , stimulus timing - dependent plasticity in high - level vision .",
    ", current biology 22  ( 4 ) ( 2012 ) 332337 .",
    "http://dx.doi.org/10.1016/j.cub.2012.01.003 [ ] .",
    "t.  masquelier , s.  j. thorpe , unsupervised learning of visual features through spike timing dependent plasticity .",
    ", plos computational biology 3  ( 2 ) ( 2007 ) e31 .",
    "s.  savarese , l.  fei - fei , 3d generic object categorization , localization and pose estimation , in : ieee 11th international conference on computer vision ( iccv ) , 2007 , pp .",
    "http://dx.doi.org/10.1109/iccv.2007.4408987 [ ] .",
    "b.  pepik , m.  stark , p.  gehler , b.  schiele , multi - view priors for learning detectors from sparse viewpoint data , in : international conference on learning representations ( iclr ) , banff , ab , canada , 2014 , pp .",
    "b.  leibe , b.  schiele , analyzing appearance and contour based methods for object categorization , in : computer vision and pattern recognition , 2003 . proceedings .",
    "2003 ieee computer society conference on , vol .  2 , ieee , 2003 , pp .",
    "ii409 .",
    "n.  pinto , d.  d. cox , j.  j. dicarlo , why is real - world visual object recognition hard ? , plos computational biology 4  ( 1 ) ( 2008 )",
    "http://dx.doi.org/10.1371/journal.pcbi.0040027 [ ] .",
    "n.  pinto , y.  barhomi , d.  d. cox , j.  j. dicarlo , comparing state - of - the - art visual features on invariant object recognition tasks , in : ieee workshop on applications of computer vision ( wacv ) , ieee , 2011 , pp .",
    "463470 .",
    "m.  ghodrati , a.  farzmahdi , k.  rajaei , r.  ebrahimpour , s .- m .",
    "khaligh - razavi , feedforward object - vision models only tolerate small image variations compared to human , frontiers in computational neuroscience 8 ( 2014 ) 74 .",
    "j.  pohjalainen , o.  rsnen , s.  kadioglu , feature selection methods and their combinations in high - dimensional classification of speaker likability , intelligibility and personality traits , computer speech & language .",
    "n.  kriegeskorte , m.  mur , p.  bandettini , representational similarity analysis  connecting the branches of systems neuroscience , frontiers in systems neuroscience 2  ( 4 ) .",
    "f.  murtagh , p.  contreras , algorithms for hierarchical clustering : an overview , wiley interdisciplinary reviews : data mining and knowledge discovery 2  ( 1 ) ( 2012 ) 8697 .",
    "s.  j. thorpe , m.  imbert , biological constraints on connectionist modelling , in : connectionism in perspective , amsterdam : elsevier , 1989 , pp .",
    "s.  celebrini , s.  thorpe , y.  trotter , m.  imbert , dynamics of orientation coding in area v1 of the awake primate . ,",
    "visual neuroscience 10  ( 5 ) ( 1993 ) 811825 .",
    "d.  g. albrecht , w.  s. geisler , r.  a. frazor , a.  m. crane , visual cortex neurons of monkeys and cats : temporal dynamics of the contrast response function , journal of neurophysiology 88  ( 2 ) ( 2002 ) 888913 .",
    "o.  shriki , a.  kohn , m.  shamir , fast coding of orientation in primary visual cortex . ,",
    "plos computational biology 8  ( 6 ) ( 2012 ) e1002536 .",
    "m.  riesenhuber , t.  poggio , hierarchical models of object recognition in cortex .",
    ", nature neuroscience 2  ( 11 ) ( 1999 ) 10191025 .",
    "g.  a. rousselet , s.  j. thorpe , m.  fabre - thorpe , taking the max from neuronal responses .",
    ", trends cogn sci 7  ( 3 ) ( 2003 ) 99102 .",
    "s.  ullman , m.  vidal - naquet , e.  sali , visual features of intermediate complexity and their use in classification .",
    ", nature neuroscience 5  ( 7 ) ( 2002 ) 682687 .",
    "j.  mutch , u.  knoblich , t.  poggio , cns : a gpu - based framework for simulating cortically - organized networks , tech .",
    "mit - csail - tr-2010 - 013 / cbcl-286 , massachusetts institute of technology , cambridge , ma ( february 2010 ) .",
    "y.  jia , e.  shelhamer , j.  donahue , s.  karayev , j.  long , r.  girshick , s.  guadarrama , t.  darrell , caffe : convolutional architecture for fast feature embedding , arxiv preprint arxiv:1408.5093 .",
    "d.  d. cox , t.  dean , neural networks and neuroscience - inspired computer vision , current biology 24  ( 18 ) ( 2014 ) r921r929 .",
    "j.  z. leibo , j.  mutch , s.  ullman , t.  poggio , from primal templates to invariant recognition , mit - csail - tr-2010 - 057 , cbcl-293 , massachusetts institute of technology , cambridge , ma .",
    "b.  lemoine , a.  s. maida , gpu facilitated unsupervised visual feature acquisition in spiking neural networks , in : neural networks ( ijcnn ) , the 2013 international joint conference on , ieee , 2013 , pp .",
    "r.  vanrullen , s.  j. thorpe , rate coding versus temporal order coding : what the retinal ganglion cells tell the visual cortex . , neural comput 13  ( 6 ) ( 2001 ) 12551283 .",
    "d.  d. lee , h.  s. seung , learning the parts of objects by non - negative matrix factorization . , nature 401  ( 6755 ) ( 1999 ) 788791 .",
    "t.  masquelier , s.  j. thorpe , learning to recognize objects using waves of spikes and spike timing - dependent plasticity , in : the 2010 international joint conference on neural networks ( ijcnn ) , 2010 , pp . 18 .",
    "http://dx.doi.org/10.1109/ijcnn.2010.5596934 [ ] .",
    "k.  carlson , m.  richert , n.  dutt , j.  krichmar , biologically plausible models of homeostasis and stdp : stability and learning in spiking neural networks , in : the 2013 international joint conference on neural networks ( ijcnn ) , 2013 , pp .",
    "18 .    c.  zamarreo ramos , l.  a. camuas mesa , j.  a. prez - carrasco , t.  masquelier , t.  serrano - gotarredona , b.  linares - barranco , on spike - timing - dependent - plasticity , memristive devices , and building a self - learning visual cortex , frontiers in neuroscience 5  ( march ) ( 2011 ) 22 .",
    "o.  bichler , d.  querlioz , s.  j. thorpe , j .-",
    "bourgoin , c.  gamrat , extraction of temporally correlated features from dynamic vision sensors with spike - timing - dependent plasticity . , neural networks : the official journal of the international neural network society 32 ( 2012 ) 33948 . http://dx.doi.org/10.1016/j.neunet.2012.02.022 [ ] .",
    "t.  dorta , m.  zapata , j.  madrenas , g.  snchez , aer - srt : scalable spike distribution by means of synchronous serial ring topology address event representation , neurocomputing 171 ( 2016 ) 16841690 .",
    "c.  diaz , g.  sanchez , g.  duchen , m.  nakano , h.  perez , an efficient hardware implementation of a novel unary spiking neural network multiplier with variable dendritic delays , neurocomputing .",
    "m.  sivilotti , wiring considerations in analog vlsi systems with application to field - programmable networks , ph.d .",
    "thesis , comput .",
    ", california inst .",
    "technol . , pasadena , ca ( 1991 ) .    t.  serrano - gotarredona , t.  masquelier , t.  prodromakis , g.  indiveri , b.  linares - barranco , stdp and stdp variations with memristors for spiking neuromorphic learning systems . ,",
    "frontiers in neuroscience 7  ( february ) ( 2013 ) 2 .",
    "http://dx.doi.org/10.3389/fnins.2013.00002 [ ] .",
    "a.  harel , s.  ullman , d.  harari , s.  bentin , basic - level categorization of intermediate complexity fragments reveals top - down effects of expertise in visual perception , journal of vision 11  ( 8) ( 2011 ) 18 .",
    "y.  lerner , b.  epshtein , s.  ullman , r.  malach , class information predicts activation by object fragments in human object areas , journal of cognitive neuroscience 20  ( 7 ) ( 2008 ) 11891206 .",
    "t.  serre , a.  oliva , t.  poggio , a feedforward architecture accounts for rapid categorization .",
    ", proc natl acad sci u s a 104  ( 15 ) ( 2007 ) 64246429 .",
    "http://dx.doi.org/10.1073/pnas.0700622104 [ ] .",
    "d.  l.  k. yamins , h.  hong , c.  f. cadieu , e.  a. solomon , d.  seibert , j.  j. dicarlo , performance - optimized hierarchical models predict neural responses in higher visual cortex . , proceedings of the national academy of sciences of the united states of americahttp://dx.doi.org/10.1073/pnas.1403112111 [ ] .",
    "e.  t. rolls , g.  deco , computational neuroscience of vision , oxford university press , oxford , uk , 2002 .",
    "m.  a. ranzato , f.  j. huang , y .- l .",
    "boureau , y.  lecun , unsupervised learning of invariant feature hierarchies with applications to object recognition , in : ieee conference on computer vision and pattern recognition ( cvpr ) , 2007 , pp .",
    "http://dx.doi.org/10.1109/cvpr.2007.383157 [ ] .",
    "h.  goh , n.  thome , m.  cord , j .- h .",
    "lim , learning deep hierarchical visual feature coding , neural networks and learning systems , ieee transactions on pp  ( 99 ) ( 2014 ) 11 .",
    "http://dx.doi.org/10.1109/tnnls.2014.2307532 [ ] .",
    "t.  masquelier , relative spike time coding and stdp - based orientation selectivity in the early visual system in natural continuous and saccadic vision : a computational model . , journal of computational neuroscience 32  ( 3 ) ( 2012 ) 42541 . http://dx.doi.org/10.1007/s10827-011-0361-9 [ ] .",
    "p.  fldik , learning invariance from transformation sequences , neural computation 3 ( 1991 ) 194200 .",
    "escobar , g.  s. masson , t.  vieville , p.  kornprobst , action recognition using a bio - inspired feedforward spiking network , international journal of computer vision 82  ( 3 ) ( 2009 ) 284301 .",
    "here we provide the results of feature analysis techniques such as rdm and hierarchical clustering on eth-80 dataset for both hmax and our model .",
    "some sample images of eth-80 dataset are shown in fig .",
    "[ eth_samples ] . in fig .",
    "[ rdms_supliment_tims ] and fig .",
    "[ rdms_supliment_hmax ] the rdms of @xmath39 features of our model and hmax in eight view angels are presented , respectively .",
    "it can be seen that our model can better represent classes with high shape similarities such as tomato , apple , and pear or cow , horse , and dog with respect to the hmax model .",
    "also , the hierarchical clustering of whole training data based on their representations on feature spaces of our model and hmax are demonstrated in fig .",
    "[ cluster_tim_eth80 ] and fig.[cluster_hmax_eth80 ] , respectively . as for the 3d - object dataset",
    ", hmax feature extraction leads to a nested representation of different object classes which causes a poor classification accuracy . here",
    "again a huge number of images which belong to different classes are assigned to a large cluster with lower than 0.14 internal dissimilarities .",
    "on the other hand , our model has distributed images of different classes in different regions of @xmath39 feature space .",
    "note that the largest cluster of our model includes the instances of tomato , apple , and pear classes which their shapes are so similar ."
  ],
  "abstract_text": [
    "<S> retinal image of surrounding objects varies tremendously due to the changes in position , size , pose , illumination condition , background context , occlusion , noise , and nonrigid deformations . but </S>",
    "<S> despite these huge variations , our visual system is able to invariantly recognize any object in just a fraction of a second . to date , various computational models have been proposed to mimic the hierarchical processing of the ventral visual pathway , with limited success . here , we show that the association of both biologically inspired network architecture and learning rule significantly improves the models performance when facing challenging invariant object recognition problems . </S>",
    "<S> our model is an asynchronous feedforward spiking neural network . </S>",
    "<S> when the network is presented with natural images , the neurons in the entry layers detect edges , and the most activated ones fire first , while neurons in higher layers are equipped with spike timing - dependent plasticity . </S>",
    "<S> these neurons progressively become selective to intermediate complexity visual features appropriate for object categorization . </S>",
    "<S> the model is evaluated on _ 3d - object _ and _ eth-80 _ datasets which are two benchmarks for invariant object recognition , and is shown to outperform state - of - the - art models , including deepconvnet and hmax . </S>",
    "<S> this demonstrates its ability to accurately recognize different instances of multiple object classes even under various appearance conditions ( different views , scales , tilts , and backgrounds ) . </S>",
    "<S> several statistical analysis techniques are used to show that our model extracts class specific and highly informative features .    </S>",
    "<S> * keywords : * view - invariant object recognition , visual cortex , stdp , spiking neurons , temporal coding </S>"
  ]
}