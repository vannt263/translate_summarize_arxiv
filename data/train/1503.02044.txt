{
  "article_text": [
    "image compression is a topic of interest since the beginning of digital imaging , remaining relevant continuously due to the ongoing improvement in image resolution .",
    "while standard approaches are based on orthogonal bases and frames like cosine transforms or wavelets , an alternative route based on ideas from partial differential equations has emerged recently ( cf .",
    "@xcite ) . in the latter case particular attention",
    "is paid to compressions from which cartoons can be reconstructed accurately avoiding the artefacts of the above mentioned standard approaches by a direct treatment of edges .",
    "roughly speaking , their idea is to detect edges and store the image value in pixels on both sides of an edge .",
    "the remaining parts of the image are completed by harmonic inpainting .",
    "an alternative interpretation of the two - sided pixel values , worked out more clearly in the osmosis setting of @xcite , is that a vector field @xmath0 corresponding to the normal derivatives of the image at the edge location is stored .",
    "the inpainting of the image @xmath1 then corresponds to a solution of @xmath2 where @xmath3 is the image domain into which @xmath0 is extended by zero off the edges .    in this paper",
    "we start from reinterpretation of the pde - based compression in terms of sparsity , which directly translates into a variational framework .",
    "the key observation is that with the edge detection and zero extension of @xmath0 one essentially looks for a sparse vector field that leads to a certain precision in the reconstruction via ( [ eq : inpainting ] ) . in the spirit of the predominant sparsity regularization of imaging problems we study a direct variational approach",
    ": we minimize an @xmath4-type norm of the vector field subject to the constraint that @xmath1 reconstructed via ( [ eq : inpainting ] ) approximates a given image @xmath5 up to a certain tolerance , i.e. @xmath6 as we shall discuss below a more rigorous statement of the problem takes into account that @xmath0 needs to be interpreted as a vectorial radon measure on @xmath3 ( similar to gradients of @xmath7-functions ) in a continuum setting .",
    "the properties of a limiting continuum model appear to be of particular advantage for the compression issues , since one expects to concentrate the measure @xmath0 on a set of lebesgue measure zero .",
    "this means that for a suitable discrete approximation of the model the number of pixels we need to store the vector field in divided by the total number of pixels tends to zero .",
    "a direct consequence is an increase in the compression rates as the image resolution increases , a highly desirable property .",
    "apart from compression we shall investigate models like the above one as a regularization for more general imaging problems .",
    "the relation to denoising models can readily be seen when the above constraint of approximating @xmath5 is incorporated via a lagrange functional .",
    "indeed , there exists a lagrange parameter @xmath8 such that ( [ eq : model0 ] ) is equivalent to @xmath9 hence , we may also interpret the norm of @xmath0 as an implicit regularization of @xmath0 and simply replace the first term by an arbitrary data term to treat other imaging problems .",
    "this is more apparent when we replace the constraint by a natural special solution @xmath10 . in this case ( [ eq :",
    "model1 ] ) is just the rof - model for denoising ( cf .",
    "@xcite ) and indeed an equivalence relation holds in spatial dimension one . in higher spatial dimensions",
    "it becomes apparent that a key role is played by the curl @xmath11 .",
    "if the curl of @xmath0 vanishes , it simply becomes a gradient vector field and hence we again recover the rof - model .",
    "this motivates to study further generalizations of the model also penalizing @xmath11 . with the interpretation that @xmath0 is related to the gradient of @xmath1 ,",
    "the additional term becomes a higher order regularization effectively .",
    "functionals of this type are currently studied in particular to reduce staircasing artefacts in total variation regularization ( cf .",
    "@xcite ) , and indeed we shall be able to draw very close analogies to the recently very popular tgv - approach ( cf .",
    "the reduction of staircasing is confirmed by computational experiments .",
    "however , in the denoising case we shall see that the divergence part alone does not suffice for appropriate smoothing .",
    "the remainder of the paper is organized as follows : in section 2 we discuss the model and its variants .",
    "then , we proceed to a discussion of some theoretical properties in section 3 , which provide some insights into the sparse vector field model .",
    "section 4 introduces a numerical solution based on primal - dual optimization methods , which is used for some experimental studies in section 5 .",
    "finally , we provide a conclusion and directions for future research .",
    "in order to obtain an appropriate formulation of ( [ eq : model1 ] ) we proceed as in @xcite and interpret @xmath0 as a @xmath12-dimensional radon measure on @xmath13 . the regularization functional is then @xmath14 where ( [ eq : inpainting ] ) is to be understood in a weak form as well .",
    "hence , ( [ eq : model1 ] ) is rewritten as @xmath15    the model can be formulated as in recent approaches for denoising by defining @xmath16 ( in the sense of distributions ) . with @xmath17 being the characteristic function of the set @xmath18 we obtain @xmath19 one observes that the regularization functional is now an infimal convolution of total variation and a functional of @xmath20 .",
    "the same structure is apparent in the recently popularized tgv - model ( cf .",
    "@xcite ) , which in the analogous setting reads @xmath21 a major difference of the tgv approach to our new model is the fact that in our case only the divergence of @xmath0 respectively @xmath22 is penalized , which might be too weak to achieve suitable regularization properties .",
    "this can obviously be realized if we add additional regularization terms depending on @xmath11 , which is natural since a divergence - free vector field is constant if and only if its curl vanishes .",
    "note that @xmath23 , hence @xmath24 , i.e. we can formulate regularization either on @xmath0 or on @xmath22 .",
    "the most general formulation is given by @xmath25 with convex functionals @xmath26 and @xmath27 , which however exceeds the scope of this paper and is left as subject of future research .",
    "we finally recast the above results in terms of the regularization they induce on @xmath1 . in this respect",
    "we also discuss the boundary conditions in ( [ eq : inpainting ] ) , respectively its weak formulation .",
    "natural boundary conditions are no - flux conditions @xmath28 on @xmath29 , which means the used weak formulation of ( [ eq : inpainting ] ) is @xmath30 hence , we can define the regularization functional @xmath31 $ ] @xmath32    once we have defined the regularization terms it is straight - forward to extend the variational model to other imaging tasks , e.g. by just changing the data fidelity .",
    "moreover , we can consider bregman iterations ( cf .",
    "@xcite ) @xmath33 as well as other scale space methods such as the gradient flow ( cf .",
    "@xcite ) @xmath34 and the inverse scale space method ( cf .",
    "in the following we further discuss some properties of the regularization functional @xmath35 defined via ( [ eq : rdefinition ] ) . to avoid obvious technicalities with constants ,",
    "we restrict ourselves to the space @xmath36 if @xmath3 is a bounded domain .",
    "we start with some topological properties induced by @xmath35 :    let @xmath3 be a sufficiently regular domain . then there exists a constant @xmath37 such that @xmath38 for all @xmath39 .",
    "moreover , dom@xmath40 is a subspace of @xmath41 and @xmath35 is a norm on dom@xmath42 . finally , @xmath43    we have @xmath44 for @xmath45 we define @xmath22 as the weak solution of the poisson equation @xmath46 with homogeneous neumann boundary conditions and mean value zero .",
    "thus , using the weak formulations we have @xmath47 regularity of solutions of the poisson equation yields continuity of @xmath22 and the existence of a constant @xmath48 such that @xmath49 for all @xmath45 .",
    "hence , @xmath50 which yields the estimate of the @xmath4-norm .",
    "the one - homogeneity and triangle inequality follow in a straight - forward way from the definition , consequently @xmath35 is a norm on a subspace of @xmath51 .",
    "estimate ( [ rtvestimate ] ) is obtained since @xmath52 satisfies ( [ eq : inpaintingweak1 ] ) , hence the infimum over all admissible @xmath0 is less or equal to the total variation .    a next step towards the understanding of properties of @xmath35",
    "is an investigation of its subdifferential , with subsequent consequences for optimality conditions of ( [ eq : model1a ] ) .",
    "for brevity we use a formal approach based on lagrange multipliers .",
    "we have @xmath53 if and only if @xmath54 for solutions @xmath55 of the saddle - point problem @xmath56 for given @xmath1 and the lagrangian is defined as @xmath57 thus , we find @xmath58 and the optimality conditions for the saddle point problem yield ( [ eq : inpaintingweak1 ] ) and @xmath59    it is instructive to compare the subgradients of @xmath35 with those of tv . indeed with similar reasoning",
    "one can show that @xmath60 if @xmath61 for a vector field @xmath62 and @xmath10 .",
    "this means that if we can write @xmath63 we also obtain @xmath53 . in particular",
    "this opens the door towards a simple verification whether solutions of the rof - model are also solutions of the sparse vector field model ( [ eq : model1a ] ) .",
    "one simply has to inspect the subgradient in the optimality condition and check whether the associated vector field @xmath64 can be written as a gradient .",
    "we will exemplify this in the case of the most well - known example for the rof model , the reconstruction of the indicator function of a ball on @xmath65 ( cf .",
    "this function is an eigenfunction of tv , i.e. there exists @xmath66 ( depending on the radius @xmath35 of the ball ) such that @xmath67 it is easy to see that @xmath68 , where @xmath69 is the signed distance function of the ball ( cf .",
    "@xcite ) and @xmath26 satisfies @xmath70 hence , we have @xmath71 for @xmath72 , which implies that @xmath73 , with the same value .",
    "the results in @xcite imply that the variational model ( [ eq : model1a ] ) reconstructs data @xmath5 being the indicator function of a ball in the form @xmath74 , with @xmath75 depending on @xmath66 and @xmath76 .",
    "moreover , the bregman iteration and inverse scale space methods reconstruct @xmath5 exactly after a finite number of iterations respectively finite time .",
    "finally we return to the original idea of compressing an image by encoding a sparse vector field . for this sake",
    "it is desireable that @xmath0 has support on a set of small ( or even zero ) lebesgue measure .",
    "thinking about the continuum case as a limit of discrete pixel images , the asymptotic property of zero lebesgue measure means that the image ( in 2d ) can be encoded by a number of values proportional to the square root of the number of pixels .",
    "consequently the compression rate of such a pde - based approach should improve with higher image resolution , which is highly relevant given the current trend of screen and camera resolution .",
    "we already see from the example of the indicator function of a ball above that we can expect the method to encode a piecewise constant image by vector fields concentrated on the edge sets . for more complicated images the vector field potentially needs to have a larger support to obtain a suitable reconstruction , since away from the support of @xmath0 the function @xmath1 is just harmonic . a better understanding of the compression properties would need a characterization of the structure of minimizers , similar to @xcite . while the one - dimensional case is equivalent to total variation regularization and hence always yields @xmath0 concentrated on a set of zero lebesgue measure ( cf .",
    "@xcite ) , the multi - dimensional case is less clear and left as an interesting topic of future research .",
    "further studies on the compression properties will be carried out below by computational experiments .",
    "in order to solve our minimization problem ( [ eq : model1a ] ) numerically , we at first need to discretize it . thereto we will adopt the notation of the continuous functions @xmath1 , @xmath0 , @xmath5 and the operators @xmath77 , @xmath78 and @xmath79 , but from now on , we are thereby referring to their discretized versions , which we shall comment on in the following .    for simplification , we assume the normalized images to be quadratic , i.e. @xmath80^{n \\times n}$ ] .",
    "the pixel grid can be written as @xmath81 , where @xmath82 denotes the spacing size .",
    "we use forward finite differences with neumann boundary conditions for the discretization of the gradient of @xmath1 and in order to preserve the adjoint structure the divergence is discretized with backward finite differences .",
    "moreover , in this discrete setting the @xmath12-dimensional radon measure on @xmath13 becomes the discrete l1-norm .",
    "considering @xmath0 as being related to the gradient of @xmath1 , the regularization term in problem can be interpreted as the discrete total variation norm .",
    "we decided to use its isotropic version which is given by @xmath83 consequently , the minimization problem reads : @xmath84 or equivalently : @xmath85 where @xmath17 is again the characteristic function of the set @xmath18 . defining @xmath86",
    "one can easily see that we can calculate a solution of the above problem by applying a version of the recently very popular primal - dual algorithms ( cf . )",
    "designed for efficiently solving general minimization problems of the form @xmath87 where f and g are proper convex lower - semicontinuous functionals .",
    "+    @xmath88 , @xmath89 $ ] @xmath90 , @xmath91 , @xmath92 @xmath93 @xmath94 @xmath95    we decided to use the first - order primal - dual algorithm as proposed by chambolle and pock ( cf .",
    "@xcite ) given by algorithm [ alg : cp ] . adopting their notation",
    "we can now derive the updates concerning our minimization problem .",
    "thereto we at first have to calculate the dual functional @xmath96 . since in our case @xmath26 is the characteristic function of the set @xmath97 , it is straightforward to see that @xmath98 equals zero .",
    "hence the update for the dual variable @xmath99 simplifies to @xmath100 .",
    "next we consider the update for the primal variable @xmath101 . as the subdifferentials of @xmath27 with respect to @xmath1 and @xmath0",
    "are independent of @xmath0 and @xmath1 , respectively , we can update each component of @xmath101 separately ( see algorithm 2 ) . using the norm of the sum of the primal and dual residual given by ( cf .",
    "@xcite ) @xmath102 where @xmath103 as a stopping criterion the implementation of our minimization problem can be summarized by algorithm [ alg : model1 ] , where the two - dimensional isotropic shrinkage operator is defined by : @xmath104    image @xmath5 , @xmath8 , @xmath88 , @xmath89 $ ] , max no of iterations , @xmath105 @xmath106 , @xmath107 , @xmath91 , @xmath108 , @xmath109 @xmath110 @xmath111 @xmath112 shrinkage(@xmath113 )",
    "@xmath114 @xmath115    as already mentioned in section [ chap : variational_model ] the model discussed so far can be further extended by considering for example bregman iterations as proposed by osher and coworkers @xcite . to incorporate this iterative regularization method in our algorithm",
    ", we use their `` adding - back - the - noise '' formulation such that the update for @xmath1 in the previously introduced routine is replaced by @xmath116 besides , the existing implementation is extended by an outer loop over a given number of bregman iterations in which @xmath82 is updated by @xmath117 after each complete cycle of the inner loop .",
    "in the following we present some results for the cases of image compression and denoising discussed above . as an example we chose the frequently used image `` trui '' ( @xmath118 pixels ) , making the approach comparable to previous results such as @xcite .",
    "however , since the size of this image does not correspond to modern hd resolutions , we also created two similar images with sizes of @xmath119 and @xmath120 pixels , respectively .",
    "we start by discussing the compression of the cartoon part of an image , which is illustrated in figure [ fig : comparisonofparametervalues ] for the trui test image .",
    "we plot the relative error vs. the non - zero gradient ratio , which means the number of pixels with non - zero @xmath0 divided by the total number of pixels . on the left",
    "we show a comparison of the sparse vector field ( svf ) model with the classical rof model , which demonstrates the improved compression properties . on the right",
    "we plot the results for the variational svf model compared to the bregman iteration ( numbers correspond to bregman iterations ) , which illustrates that no significant improvement can be obtained with respect to compression by the latter . in figure",
    "[ fig : principleofimagecompressionalgorithm ] we display the results of the compression and the corresponding vector fields for @xmath121 .",
    "one observes that the support of @xmath0 corresponds well to an edge indicator , confirming the relation to the approach in @xcite .",
    "the reconstructed image seems to preserve the main edges well , but does not have the strict piecewise constant behaviour as total variation regularization , which seems attractive for further reconstruction tasks .",
    "we also investigate the behaviour for higher resolution . in order to mimic increasing resolution we simply downscale the test images to @xmath122 times the number of original pixels ,",
    "@xmath123 $ ] .",
    "we then perform compression at fixed error tolerance ( corresponding to constant @xmath66 when appropriately scaled ) for the images of different size and finally plot the non - zero gradient ratio vs. @xmath122 in figure [ comparisonofnon - zerogradientratios ] .",
    "our expectation that due to continuum limit and the potential convergence towards a concentrated measure the ratio decreases with increasing resolution is well - confirmed for the trui image as well as for a similar image at higher resolution .",
    "finally we display the result of the svf model for image compression performed on a high definition image and compare it to a jpg image with the same compression rate in figure [ fig : comparisonwithjpg_highresolutiontestjoana ] .",
    "indeed we achieve an improved psnr with the svf model .",
    "we also mention that several further compression steps on @xmath0 can be carried out in an analogous way to @xcite , which will lead to highly improved rates , but is beyond the scope of this paper .",
    "+     +      we finally give an outlook towards other tasks such as denoising with sparse vector fields .",
    "for this sake we compare the model with results of the classical rof model and choose in both cases @xmath66 such that the psnr to the original image is maximized .",
    "we illustrate the result in figure [ fig : denoisingtrui ] , which appears to be representative for all our tests .",
    "one observes that the reduced staircasing in the svf model compared to the rof model is less visible , which is due to point like artefacts that were not present without noise .",
    "this results in a lower psnr than for the rof model , which is consistent for all our tests .",
    "the reason for the artefacts is that @xmath0 is too sparse in this case and does not encode the contours anymore .",
    "this can be seen in the last image of figure [ fig : denoisingtrui ] .    ,",
    "middle left ) , svf denoising result ( @xmath124 , middle right ) , norm of vector field @xmath0 in svf model ( right).,title=\"fig:\",height=132 ] , middle left ) , svf denoising result ( @xmath124 , middle right ) , norm of vector field @xmath0 in svf model ( right).,title=\"fig:\",height=132 ] , middle left ) , svf denoising result ( @xmath124 , middle right ) , norm of vector field @xmath0 in svf model ( right).,title=\"fig:\",height=132 ] , middle left ) , svf denoising result ( @xmath124 , middle right ) , norm of vector field @xmath0 in svf model ( right).,title=\"fig:\",height=132 ]",
    "we have introduced the svf model for image compression motivated by diffusion inpainting and found several interesting connections to tv - type regularization methods .",
    "the svf approach leads to significantly sparser vector fields than the gradients of total variation , which appears quite useful for compression tasks , and seems not to suffer from staircasing artefacts , which appears attractive for other reconstruction tasks .",
    "however , the denoising performance of the svf model is not convincing , since it creates point artefacts at reasonable choice of the regularization parameter .",
    "this is probably due to the fact that the norm induced by the corresponding regularization is too weak ( there is an upper but no lower bound in terms of tv ) . for future improvement it seems natural to consider regularization on the curl of the vector field as well , such that @xmath0 becomes again concentrated on contours rather than scattered points .",
    "in particular we suggest to study the more general problem @xmath125 for positive @xmath126 and @xmath127 where again @xmath128 ( see ) . due to the exact penalization properties of one - norms we expect that the rof model corresponds to the case of @xmath126 and @xmath127 sufficiently large , while the svf model in this paper is @xmath129 and @xmath127 sufficiently large .",
    "this work has been supported by erc via grant eu fp 7 - erc consolidator grant 615216 lifeinverse .",
    "mb acknowledges support by the german science foundation dfg via exc 1003 cells in motion cluster of excellence , mnster , germany .",
    "jg acknowledges support by the cambridge biomedical research centre .",
    "the authors thank tim lpmeier ( mnster ) and johannes hjorth ( cambridge ) for the acquisition of hd photographs used as test data .",
    "chambolle , a. , caselles , v. , cremers , d. , novaga , m. , pock , t. .",
    "an introduction to total variation for image analysis . in : fornasier , m. ( ed . ) , theoretical foundations and numerical methods for sparse recovery , degruyter , berlin , 263 - 340 ( 2010 ) .",
    "mainberger , m. , weickert , j. : edge - based image compression with homogeneous diffusion . in : computer analysis of images and patterns . ed .",
    "by jiang , x. , petkov , n. , pp .",
    "springer , berlin , heidelberg ( 2009 )              weickert , j. , hagenburg , k. , breuss , m. , vogel , o. : linear osmosis models for visual computing .",
    "energy minimization methods in computer vision and pattern recognition .",
    "springer berlin heidelberg ( 2013 )"
  ],
  "abstract_text": [
    "<S> this paper introduces a novel variational approach for image compression motivated by recent pde - based approaches combining edge detection and laplacian inpainting . </S>",
    "<S> the essential feature is to encode the image via a sparse vector field , ideally concentrating on a set of measure zero . </S>",
    "<S> an equivalent reformulation of the compression approach leads to a variational model resembling the rof - model for image denoising , hence we further study the properties of the effective regularization functional introduced by the novel approach and discuss similarities to tv and tgv functionals . </S>",
    "<S> moreover we computationally investigate the behaviour of the model with sparse vector fields for compression in particular for high resolution images and give an outlook towards denoising . </S>",
    "<S> +   + * keywords : * image compression , denoising , reconstruction , diffusion inpainting , sparsity , total variation </S>"
  ]
}