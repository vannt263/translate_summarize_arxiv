{
  "article_text": [
    "luc moreau is a postdoctoral fellow of the fund for scientific research - flanders . work done while a recipient of an honorary fellowship of the belgian american educational foundation , while visiting the princeton university mechanical and aerospace engineering department .",
    "this paper presents research results of the belgian programme on inter - university poles of attraction , initiated by the belgian state , prime minister s office for science , technology and culture .",
    "the scientific responsibility rests with its authors ."
  ],
  "abstract_text": [
    "<S> some biological systems operate at the critical point between stability and instability and this requires a fine - tuning of parameters . </S>",
    "<S> we bring together two examples from the literature that illustrate this : neural integration in the nervous system and hair cell oscillations in the auditory system . in both examples </S>",
    "<S> the question arises as to how the required fine - tuning may be achieved and maintained in a robust and reliable way . </S>",
    "<S> we study this question using tools from nonlinear and adaptive control theory . </S>",
    "<S> we illustrate our approach on a simple model which captures some of the essential features of neural integration . as a result </S>",
    "<S> , we propose a large class of feedback adaptation rules that may be responsible for the experimentally observed robustness of neural integration . </S>",
    "<S> we mention extensions of our approach to the case of hair cell oscillations in the ear .    </S>",
    "<S> persistent neural activity is prevalent throughout the nervous system . </S>",
    "<S> numerous experiments have demonstrated that persistent neural activity is correlated with short - term memory . </S>",
    "<S> a prominent example concerns the oculomotor system  </S>",
    "<S> see @xcite for a review and experimental facts . </S>",
    "<S> the brain moves the eyes with quick saccadic movements . between saccades , it keeps the eyes still by generating a continuous and constant contraction of the eye muscles ; thus requiring a constant level of neural activity in the motor neurons controlling the eye muscles . </S>",
    "<S> this constant neural activity level serves as a short - term memory for the desired eye position . during a saccade </S>",
    "<S> , a brief burst of neural activity in premotor command neurons induces a persistent change in the neural activity of the motor neurons , via a mechanism equivalent to integration in the sense of calculus . </S>",
    "<S> neural activity of an individual neuron , however , has a natural tendency to decay with a relaxation time of the order of milliseconds . </S>",
    "<S> therefore the question arises as to how a transient stimulus can cause persistent changes in neural activity . according to a long - standing hypothesis , persistent neural activity </S>",
    "<S> is maintained by synaptic feedback loops . </S>",
    "<S> positive feedback can oppose the tendency of a pattern of neural activity to decay . </S>",
    "<S> if the feedback is weak , then the natural tendency to decay dominates and neural activity decays . as the feedback strength is increased , the neural dynamics undergo a bifurcation and become unstable . </S>",
    "<S> when the feedback is tuned to exactly balance the decay , then neural activity neither increases nor decreases but persists without change . </S>",
    "<S> this , however , requires a fine - tuning of the synaptic feedback strength and the question arises as to how a biological system can achieve and maintain this fine - tuning  @xcite . some gradient descent and function approximation algorithms performing this fine - tuning </S>",
    "<S> have been proposed  @xcite and a feedback learning mechanism based on differential anti - hebbian synaptic plasticity has been studied in  @xcite . </S>",
    "<S> nevertheless , it is still unclear how the required fine - tuning is physiologically feasible . for this reason , a different model for neural integration based upon bistability </S>",
    "<S> has recently been proposed in  @xcite . in the present paper </S>",
    "<S> , we do not follow the line of research based upon bistability . </S>",
    "<S> instead , we pursue the hypothesis of precisely tuned synaptic feedback . </S>",
    "<S> the present paper proposes an adaptation mechanism that may be responsible for the fine - tuning of neural integrators and that may explain the experimentally observed robustness of neural integrators with respect to perturbations . </S>",
    "<S> before we present this adaptation mechanism in detail , we first discuss a similar phenomenon in the auditory system .    in order to detect the sounds of the outside world , hair cells in the cochlea operate as nanosensors which transform acoustic stimuli into electric signals . </S>",
    "<S> in @xcite these hair cells are described as active systems capable of generating spontaneous oscillations . </S>",
    "<S> ions such as @xmath0 are believed to contribute to the hair cell s tendency to self - oscillate . for low concentrations of the ions , </S>",
    "<S> damping forces dominate and the hair cell oscillations are damped . </S>",
    "<S> as the concentration increases the system undergoes a hopf bifurcation , the dynamics become unstable , and the hair cells exhibit spontaneous oscillations . in @xcite </S>",
    "<S> the hair cells are postulated to operate near the critical point , where the activity of the ions exactly compensates for the damping effects . as before , this requires a fine - tuning of parameters ( the ion concentrations ) and again the question arises as to how this fine - tuning can be achieved and maintained . in @xcite </S>",
    "<S> a feedback mechanism has been proposed which could be responsible for maintaining this fine - tuning .    </S>",
    "<S> it thus seems that operating in the vicinity of a bifurcation is a recurrent theme in biology . and </S>",
    "<S> the question as to how proximity to the bifurcation point may be achieved and maintained in a noisy environment may be of considerable , general interest . </S>",
    "<S> we view the two presented examples as special instances of the following general problem . </S>",
    "<S> consider a forced dynamical system , described by a differential equation @xmath1 . </S>",
    "<S> the right - hand side of this equation depends on a parameter  @xmath2 and the unforced dynamics @xmath3 are assumed to exhibit a bifurcation when @xmath2 equals a critical value  @xmath4 . </S>",
    "<S> the problem consists of finding a feedback adaptation rule for the parameter  @xmath2 which guarantees proximity to the bifurcation point ; that is , which steers @xmath2 toward its critical value  @xmath4 . </S>",
    "<S> this adaptation law may depend on @xmath2 and @xmath5 but should be independent of @xmath4 , since this critical value is not known precisely . </S>",
    "<S> this abstract formulation captures common features of both biological examples and suggests some unexpected links with the literature . </S>",
    "<S> questions very similar to the present one have been studied extensively in the literature on adaptive control  @xcite and stabilization  @xcite ; and the general problem is closely related to extremum seeking  @xcite , and to instability detection  @xcite , where an operating parameter is adapted on - line in order to experimentally locate bifurcations .    </S>",
    "<S> although the above general formulation is convenient , there is little hope that a complete and satisfactory theory can be developed that applies to all possible instances of the problem . simplifying assumptions </S>",
    "<S> make it more tractable . in this letter </S>",
    "<S> , we study in detail what is probably the most simple but nontrivial instance of the general problem . </S>",
    "<S> we consider the one - dimensional system @xmath6 which captures some of the essential features of neural integration and is in fact closely related to the autapse model from @xcite . with this interpretation </S>",
    "<S> , @xmath5 is a strictly positive variable representing neural activity in the integrator network and @xmath7 represents the signal generated by the premotor command neurons . </S>",
    "<S> the term @xmath8 corresponds to the natural decay of neural activity and @xmath9 represents a positive , synaptic feedback loop . </S>",
    "<S> of course , when studying neural integration , questions can be investigated at varying levels of detail . </S>",
    "<S> it is clear that a simple model as ( [ e : ni ] ) has several limitations . </S>",
    "<S> because of its one - dimensional nature , the present model is , for example , unable of reproducing the distributed nature of persistent activity patterns observed in the brain . </S>",
    "<S> nevertheless , eq .  ( [ e : ni ] ) captures a key feature of neural integration : when the feedback is tuned to exactly balance the decay , eq .  </S>",
    "<S> ( [ e : ni ] ) behaves as an integrator and produces persistent neural activity . </S>",
    "<S> eq .  </S>",
    "<S> ( [ e : ni ] ) is therefore a valuable model when studying fine - tuning of neural integrator networks  @xcite . </S>",
    "<S> we are interested in the fine - tuning of eq .  </S>",
    "<S> ( [ e : ni ] ) and study this question using tools from nonlinear and adaptive control theory . </S>",
    "<S> first , we ignore the presence of the input  @xmath7 and consider the simpler equation @xmath10 we present a large class of feedback adaptation laws for  ( [ e : pa ] ) which steer @xmath2 to its critical value  @xmath4 ; thus enabling the automatic self - tuning of parameters and the spontaneous generation of persistent neural activity . </S>",
    "<S> we consider adaptation laws could come from synaptic plasticity . in particular , the term @xmath11 might be related to types of synaptic plasticity that depend on the temporal ordering of presynaptic and postsynaptic spiking , as in @xcite . </S>",
    "<S> ] of the form @xmath12 we show that , under three very mild conditions , this adaptation rule guarantees convergence to the bifurcation point for  ( [ e : pa ] ) . </S>",
    "<S> the first condition requires that  @xmath13 is a strictly increasing function . </S>",
    "<S> this means that the term  @xmath14 in  ( [ e : adaptation ] ) acts as a negative feedback . as a consequence , if the neural activity  @xmath5 were constant in  ( [ e : adaptation ] ) , then the synaptic feedback gain  @xmath2 would naturally relax to a rest value depending on  @xmath5 via the equation  @xmath15 . </S>",
    "<S> the second condition states that there exists  @xmath16 such that @xmath17 . </S>",
    "<S> this condition implies that , if the neural activity would be constant and equal to  @xmath16 in  ( [ e : adaptation ] ) , then the synaptic feedback gain  @xmath2 would naturally relax to its critical , desired value  @xmath4 . </S>",
    "<S> of course there is no guarantee that the neural activity would be equal to , or even converge to , this special value  @xmath16 . instead </S>",
    "<S> , the level of neural activity is governed by eq .  </S>",
    "<S> ( [ e : pa ] ) . therefore , in order for the adaptation law  ( [ e : adaptation ] ) to work , we need to impose a last condition , that  @xmath18 is a decreasing function . </S>",
    "<S> this means that the level of neural activity negatively regulates the synaptic feedback strength .    </S>",
    "<S> we now show that , under these three conditions , the feedback adaptation law  ( [ e : adaptation ] ) indeed tunes the synaptic feedback gain  @xmath2 to exactly balance the natural decay rate  @xmath4 . </S>",
    "<S> we begin with noticing that the combined system of equations  ( [ e : pa])([e : adaptation ] ) has a unique rest point . </S>",
    "<S> this equilibrium is determined by setting the right - hand sides of  ( [ e : pa])([e : adaptation ] ) equal to zero , yielding @xmath19 and @xmath20 . </S>",
    "<S> although the precise value of  @xmath4 is unknown , if we are able to prove that all trajectories of  ( [ e : pa])([e : adaptation ] ) converge to this ( unknown ) fixed point , then it follows that  @xmath2 indeed converges to its desired , critical value  @xmath4 . in order to prove this </S>",
    "<S> , we introduce a coordinate transformation  @xmath21 and  @xmath22 . </S>",
    "<S> this transforms  ( [ e : pa])([e : adaptation ] ) into @xmath23 and @xmath24 . in these new coordinates , </S>",
    "<S> the dynamics take the form of a nonlinear mass - spring - damper system ( with unit mass , nonlinear spring characteristic  @xmath25 and nonlinear damping function  @xmath26 ) . </S>",
    "<S> it follows from physical energy considerations that this system exhibits damped oscillations  @xcite . </S>",
    "<S> this shows that all trajectories of  ( [ e : pa])([e : adaptation ] ) indeed converge to the unique fixed point , where @xmath20 .    </S>",
    "<S> the above coordinate transformation reveals a subtle relationship between self - tuning of bifurcations and the internal model principle ( `` imp '' ) from robust control theory ( see  @xcite for a discussion of the imp from a systems biology perspective ) . </S>",
    "<S> this relation is made explicit by the equation  @xmath23 , which represents an integrator and corresponds to integral action studied in robust control theory . </S>",
    "<S> one regards the constant  @xmath4 as an unknown perturbation acting on the system . </S>",
    "<S> the imp implies that , in order to track this constant perturbation , the system dynamics should contain integral action . </S>",
    "<S> the integral action is generated by the biological system itself , and not by the feedback adaptation law .    </S>",
    "<S> we have so far ignored the presence of the signal  @xmath7 . </S>",
    "<S> we showed that the adaptation law  ( [ e : adaptation ] ) tunes the synaptic feedback gain to exactly compensate for the natural decay rate , resulting in the spontaneous generation of persistent neural activity . at these equilibrium conditions , </S>",
    "<S> the action potential firing rate equals  @xmath16 , which is related to  @xmath4 by @xmath17 . in the next paragraphs </S>",
    "<S> , we take into account the effect of the input  @xmath7 . in this case </S>",
    "<S> , the value  @xmath16 will play the role of a parameter that influences the accuracy with which the feedback adaptation law guarantees proximity to the bifurcation point . </S>",
    "<S> the signal  @xmath7 will in general result in a time - varying action potential firing rate  @xmath27 . </S>",
    "<S> the mechanism with which this happens , is determined by the neural integrator equation  ( [ e : ni ] ) and the adaptation law  ( [ e : adaptation ] ) . for the purpose of analysis , </S>",
    "<S> we make two simplifying assumptions , both of which seem to be natural and physically relevant for neural integration . </S>",
    "<S> first , we assume that , over any sufficiently large time interval  @xmath28 $ ] , the time spent by  @xmath27 in any interval  @xmath29 $ ] is approximately independent of  @xmath30 . in more mathematical terms , we assume the existence of a function  @xmath31 such that for every test function  @xmath32 , the time average @xmath33 converges to @xmath34 as @xmath35 , uniformly with respect to  @xmath30 . </S>",
    "<S> secondly , we assume that the adaptation law acts on a much slower time scale than the time variations in  @xmath27 . under these assumptions , </S>",
    "<S> the effect of the action potential firing rate  @xmath27 on the adaptation law  ( [ e : adaptation ] ) may be approximated by the average effect @xmath36 . </S>",
    "<S> it is now clear when the adaptation law guarantees proximity to the bifurcation point : if the compatibility condition  @xmath37 is satisfied , then time scale separation arguments suggest that @xmath2 will converge approximately to @xmath4 and the neural integrator will approximately behave as a perfect integrator . </S>",
    "<S> the compatibility condition may by interpreted as follows  @xcite . when the premotor command signal @xmath7 has zero time - average and the adaptation law acts on a slow time scale , then eq .  </S>",
    "<S> ( [ e : ni ] ) behaves as a good integrator and the firing rate @xmath27 equals the time - integral of @xmath7 plus an integration constant . </S>",
    "<S> the compatibility condition ensures that this integration constant is compatible with the desired range for the firing rate @xmath27 . </S>",
    "<S> we illustrate this result on a particular example representative for saccadic eye movements . </S>",
    "<S> we consider the case of periodic saccadic eye movements asking for an action potential firing rate in the motor neurons alternating between @xmath38 and @xmath39 every second . at each saccade </S>",
    "<S> , a brief burst of neural activity in premotor command neurons changes the actual firing rate . </S>",
    "<S> we assume that this change is such that immediately after each saccade , the actual firing rate equals the desired firing rate . between saccades </S>",
    "<S> , we assume that no input is applied . at its desired level between saccades , </S>",
    "<S> which is consistent with experimental observations . ] </S>",
    "<S> if the neural integrator is perfectly tuned , then the actual firing rate will remain constant between saccades and equal to the desired firing rate ( eyes are fixed ) . </S>",
    "<S> if the neural integrator is not perfectly tuned , then the actual firing rate will deviate from the desired firing rate ( eyes drift ) until a new saccade occurs which brings the actual firing rate to its new desired value . </S>",
    "<S> fig .  </S>",
    "<S> [ f2 ] shows the results of a simulation where the adaptation law satisfies the compatibility condition of the previous paragraph .    in the beginning of the simulation </S>",
    "<S> , we have mis - tuned the neural integrator . </S>",
    "<S> clearly , after a short transient , the adaptation law achieves excellent tuning and the drift between two successive saccades becomes negligible .    </S>",
    "<S> we have thus shown that an adaptation law can tune a neural integrator with great accuracy to its bifurcation point . in order to achieve perfect tuning </S>",
    "<S> , however , the adaptation law itself needs to satisfy a compatibility condition .  </S>",
    "<S> it seems that we have merely moved the problem of fine - tuning from the neural integrator to the adaptation law . the crucial observation and one of the main contributions of the present paper , however , is that this results in a significant decrease in sensitivity . _ </S>",
    "<S> the adaptation law is robust with respect to perturbations in its parameters . _    in order to illustrate this significant increase in robustness , let us first summarize the well - known  @xcite sensitivity properties of neural integration . </S>",
    "<S> experiments suggest that the actual time constant obtained in a tuned neural integrator circuit is typically greater than  @xmath40 ; that is , @xmath41 . </S>",
    "<S> this requires for the fine - tuning of @xmath2 a relative precision  @xmath42 ranging from  @xmath43 to  @xmath44 , depending on whether the intrinsic time constant  @xmath45 equals @xmath46 or @xmath47 ( typical values suggested in the literature ) . the required precision for  @xmath2 </S>",
    "<S> should be contrasted with the required precision for the parameters of the adaptation law proposed in the present paper . </S>",
    "<S> the simulations of fig .  </S>",
    "<S> [ f3 ] show that , in order to have @xmath41 as observed in experiments , the parameters of the adaptation law need to be tuned with a precision of @xmath48 , independent of the intrinsic time constant  @xmath45 . comparing this with the originally required precision for the synaptic feedback strength  @xmath2 </S>",
    "<S> , we conclude that _ the proposed adaptation mechanism could improve the robustness of neural integration with a factor ranging from  @xmath49 to  @xmath50_.    we have studied a simple model for neural integration and proposed a class of feedback adaptation rules that could explain the experimentally observed robustness of neural integration with respect to perturbations . the analysis tools that we have introduced extend to the study of fine - tuning involved in other systems such as hair cell oscillations in the ear  @xcite .    </S>",
    "<S> consider the nonlinear oscillator equation @xmath51 , which captures some of the essential features of hair cell oscillations  @xcite . </S>",
    "<S> inspired by our previous analysis , we consider a feedback adaptation law for the parameter  @xmath2 of the form @xmath52 , with @xmath53 a positive variable characterizing the magnitude of oscillations and related to  @xmath5 and  @xmath54 via the expression @xmath55 . </S>",
    "<S> fig .  </S>",
    "<S> [ f4 ] shows that , in the absence of the stimulus  @xmath7 , this type of adaptation law is indeed able to bring and keep the bifurcation parameter close to its critical value , resulting in the spontaneous generation of oscillations . </S>"
  ]
}