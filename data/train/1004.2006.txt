{
  "article_text": [
    "an experimentally measured distribution differs from the true physical distribution because of the limited efficiency of event registration and the finite resolution of a particular set - up .",
    "to identify a physical distribution , an unfolding procedure is typically applied @xcite . unfolding is an underspecified problem .",
    "any approach to solving the problem requires a priori information about the solution .",
    "methods for unfolding differ , directly or indirectly , in the use of this a priori information .",
    "unfolding when the apparatus function or transformation model for a true distribution from the measured one is unknown has been considered previously @xcite . in this paper these ideas are further developed and the problem of simultaneously identifying a transformation model and inverse problem is solved . to obtain a robust solution for an unfolding problem , information about the shape of the distribution to be measured",
    "is used to create a training sample in monte - carlo simulations of an experiment .",
    "an approximation of the apparatus function is calculated for the set of distributions for the training sample .",
    "use of this type approximation can minimize the statistical errors and biases of the unfolded distribution for distributions used to create the training sample .",
    "there is no restriction on the size and shape of bins , linearization of the problem is simple ( if the set - up has non - linear distortions ) , and multidimensional data can be unfolded .",
    "a machine learning approach provides a method for validating the unfolding procedure and for improving the results .",
    "the remainder of the paper is organized as follows . in section 2 the main equation for solving an unfolding problem",
    "is proposed .",
    "a formal method for solving the unfolding problem and estimating the statistical errors for the unfolded distribution is discussed .",
    "section 3 presents the algorithm for calculating the transformation matrix . in section 4 the overall unfolding procedure",
    "is described .",
    "this consists of bin choice , system identification , solution of the basic equation and validation of the unfolding procedure .",
    "section 5 presents a numerical example . for comparison ,",
    "an example reported elsewhere is used @xcite . to investigate biases in the unfolding distribution , a numerical experiment with 1000 runs",
    "is performed .",
    "the results show that biases for the unfolded distribution is small . to demonstrate the robustness of the unfolding method for distributions used to create the training sample , the same investigation is performed for eight distributions randomly chosen from training sample .",
    "the results reveal that there are small biases and low statistical errors for all the unfolding distributions , which confirms that the procedure is robust .",
    "statistical errors are as small as possible in all cases because of application of the least mean square method and the method for system identification .",
    "in this work we use a linear model to transform a true distribution to the measured one : @xmath0 where @xmath1 is an @xmath2-component column vector of an experimentally measured histogram , @xmath3 is an @xmath4 matrix , with @xmath5 , @xmath6 is an @xmath7-component vector of some true histogram and @xmath8 is an @xmath2-component vector of random residuals with expectation value @xmath9 and a diagonal variance matrix @xmath10 , where @xmath11 is the statistical error of the measured distribution for the @xmath12th bin .",
    "the linear model ( [ basic ] ) is reasonable for the majority of set - ups in particle and nuclear physics .",
    "it is only an approximate model for set - ups with a non - linear transformation from a true to a measured distribution .",
    "a least squares method @xcite can give an estimate of the true distribution @xmath13 , @xmath14 where @xmath15 , the estimate , is the _ unfolded distribution _ and the variance matrix of the unfolded distribution @xmath16 is given by @xmath17 the diagonal element @xmath18 of the matrix is the variance of component @xmath19 of the unfolded vector and @xmath20 is the statistical error .",
    "to realize the scheme described in section 2 , the matrix @xmath21 must be defined .",
    "this problem can be solved using system identification methods @xcite .",
    "system identification can be defined as a process for determining a model of a dynamic system using observed input  output data . in our case , this is the model for transforming a true physical distribution into the experimentally measured distribution , represented by the matrix @xmath21 .",
    "monte - carlo simulation of a set - up can be used to obtain input  output data .",
    "control input signals are used for system identification .",
    "the most popular choice is to use impulse control signals @xcite .",
    "an impulse input control signal is a generated ( input ) distribution in which the histogram with @xmath7 bins has only one bin with non - zero content . for model ( [ basic ] ) , there are @xmath7 different impulse inputs that can be presented as the diagonal matrix @xmath22 , where each row contains the content from a generated histogram . denote the corresponding values of the @xmath12th component of the reconstructed ( output ) vector as @xmath23 .",
    "each element of the @xmath12th row of the matrix @xmath24 can be found from the equation @xmath25 where @xmath26 , and @xmath27 .",
    "equation ( [ unfolded ] ) , with the matrix @xmath21 calculated in this way , gives a highly fluctuating unfolded function with large statistical errors .",
    "in addition , it is possible that the matrix @xmath28 is singular , in which case a solution does not exist .",
    "the effect of this type of instability is well known .",
    "there are many methods for solving this type of system , all of which use a priori information to obtain a stable solution to eq .",
    "( [ basic ] ) .    for system identification , instead of using impulse control distributions",
    ", we use a training sample of distributions based on a priori physically motivated information that may be known from theory or from some other experimental data .",
    "assume that we have a training sample with @xmath29 generated ( input ) distributions and presented as a @xmath30 matrix @xmath31 [ matrix ] where each row represents a generated histogram content .",
    "for each @xmath12th row of the matrix @xmath21 , we can write the following equation @xcite : @xmath32 where @xmath33 , @xmath34 is a @xmath29-component vector of the content for the reconstructed ( output ) @xmath12th bin for different generated distributions , and @xmath35 is a @xmath29-component vector of random residuals with expectation value @xmath36 and a diagonal variance matrix @xmath37 , where @xmath38 is the statistical error for the reconstructed distribution for the @xmath12th bin and the @xmath39th generated distribution .",
    "formally a least squares method gives an estimate for @xmath40 : @xmath41 the whole matrix @xmath42 is found by producing calculations defined by formula ( [ pmatr ] ) for all rows .",
    "similarity of shapes of distributions of the training sample leads to high correlations between columns of matrix @xmath43 .",
    "this means that transformation of generated distribution to the @xmath12th bin of the reconstructed distribution can be parameterized using the subset of elements of row @xmath44 .",
    "elements of a row that do not belong to the subset are set to 0 .",
    "the training sample contained copies of the same distribution is example of the singular case of the similarity .",
    "the transformation can be reduced to only one non - zero element of vector @xmath45 for this example .",
    "another example is the training sample that contains any possible distributions .",
    "the number of non - zero elements can not be reduced and matrix @xmath21 coincides with matrix calculated using impulse control signals .    a forward stepwise regression algorithm can be used @xcite to find non - zero elements of a row @xmath44 .",
    "stepwise algorithm combines fs and be steps .",
    "steps are followed by each other and repeated until the process is terminated .",
    "steps are defined as : + * step fs .",
    "* suppose there is @xmath46 elements of row @xmath12 included into the model of transformation .",
    "subvector of elements @xmath47 is calculated according to formula ( [ pmatr ] ) with submatrixes @xmath48 and @xmath49 that correspond to this subvector .",
    "a new element is added if : @xmath50 where @xmath51'\\,\\ , \\mathsf{\\gamma_i}^{-1}(l)\\,\\,[\\bm{f_i^c}-\\mathsf{\\phi^c\\mathbf{}}(l)\\bm { p_{i}(l)}]\\ ] ] and @xmath52 is constant ( threshold ) . +",
    "* step be .",
    "* let there be @xmath46 elements of row @xmath12 included into model of transformation then an element is excluded from model of transformation if : @xmath53 where @xmath54 is another constant .",
    "the algorithm is terminated when there can not be found any elements that satisfy inequality ( [ eneq ] ) or inequality ( [ eneqout ] ) .",
    "good results give thresholds @xmath55 that have some theoretical background , see @xcite .",
    "position @xmath56 of the first element @xmath57 for our case is defined by the maximum value of the correlation between vector @xmath34 and columns of matrix @xmath43 : @xmath58 , \\ ] ] where @xmath59 .",
    "the whole matrix @xmath42 is found by stepwise algorithm calculations for all rows .",
    "it is possible that for each row exist more than one subset of non - zero matrix elements that describe the transformation in a sufficiently good manner .",
    "this case can be , for example , when all distributions of training sample are rather close to each other .",
    "thus , for each @xmath12th reconstructed bin we will have a set of @xmath60 candidate rows , and for all reconstructed bins a set of @xmath61 candidate matrices @xmath21 .",
    "we need to choose a matrix @xmath21 that is good or optimal in some sense .",
    "the most convenient criterion in our case is @xmath62-optimality @xcite , which is related to minimization of @xmath63 there are many algorithms and programs for minimization of ( [ det ] ) .",
    "the matrix @xmath21 that minimizes function ( [ det ] ) gives a stable solution to unfolding problem ( [ unfolded ] ) with a minimum volume for the confidence ellipsoid .",
    "there are three possibilities to further improve the quality of the solution :    1 .",
    "introduce selection criteria for models of distributions used to create the training sample . the previously described goodness - of - fit test can be used for this purpose @xcite .",
    "2 .   each training distribution has a reconstructed distribution that can be compared with the experimentally measured distribution using a @xmath64 test @xcite .",
    "improvement is achieved by selecting distributions for the training sample that satisfy @xmath65 , where @xmath66 is the test statistic for comparison of the reconstructed and experimentally measured distributions @xcite .",
    "the threshold @xmath67 defines how close a reconstructed distribution is to the experimental distribution .",
    "note that any threshold @xmath67 corresponds to a particular significance level for the test .",
    "it is reasonable that a decrease in parameter @xmath67 represents a decrease in bias and statistical error for the solution .",
    "3 .   a leave - one - out validation procedure @xcite for @xmath29 runs can be performed . during a run",
    "the unfolding procedure is applied for each of @xmath29 a reconstructed distributions .",
    "each unfolded distribution is then compared with the corresponding generated distribution using a @xmath64 test @xcite .",
    "a boosting procedure @xcite can be used for distributions of the training sample with a low @xmath68-value .",
    "this involves adding to the training sample the same distribution with a statistically independent realization of the corresponding reconstructed histogram .",
    "this section provides a description of the complete unfolding procedure .",
    "the procedure can be divided into four parts : initialization , system identification , solution of the basic equation , and validation . + * initialization *    * _ define the binning for the experimental ( reconstructed ) data . _",
    "the strategy for selecting the bin size involves starting with a large bin size and then increasing the number of bins incrementally until the error for the unfolded distribution stops decreasing . *",
    "_ define the binning for the unfolded ( generated ) distribution . _",
    "the bin size should be chosen by picking a reasonably large size first and then decreasing the size in further steps until the correlation between adjusted bins becomes too large . the number of bins for an unfolded distribution , @xmath7 , must be less than the number of bins for the experimentally measured distribution , @xmath2 , because the least squares method is used to solve the main equation .    * system identification *    * _ choose a training sample of generated distributions . _",
    "generated distributions for the training sample must be chosen as described in the previous section .",
    "a second iteration can be made to find a better set of distributions .",
    "the number of generated distributions must be greater than the expected number of non - zero elements in any row of matrix @xmath42 ( for reasons related to use of the least squares method ) .",
    "* _ calculate the matrix _ @xmath21 .",
    "the matrix is calculated according to the algorithm described in the previous section . *",
    "_ calculate the d - optimal matrix _ @xmath21 .",
    "optimization can be performed using fedorov s reliable ea algorithm @xcite with the initial matrix @xmath42 calculated in the previous step .",
    "* solution of the basic equation *    * _ calculate the unfolded distribution eq .",
    "( [ unfolded ] ) with the variance matrix eq .",
    "( [ error])_. the correlation matrix calculated from the variance matrix can give hints for improved binning of the unfolding distribution .",
    "for example , if the correlation between two adjacent bins is high , they should be combined .",
    "* validation of the unfolding procedure *    * _ fit the unfolded distribution , and then use this fit to generate a reconstructed distribution ( including the effects of resolution and acceptance ) to compare with the real data . _ * _ leave - one - out procedure @xcite for @xmath29 runs . during a run",
    ", the unfolded procedure is applied for each of @xmath29 reconstructed distributions . _ the unfolded distributions",
    "are then compared with the corresponding generated distributions @xcite .",
    "this procedure yields an unfolded distribution with minimal statistical errors and minimal bias for the true distributions closed to distributions of the training sample .",
    "this follows from the properties of the least mean square method and the method used to calculate the transformation matrix @xmath42 .",
    "the method described above is now illustrated with an example proposed by blobel @xcite and used for illustration elsewhere @xcite .",
    "we take a true distribution @xmath69 with the same parameters as in a previous study @xcite ( table 1 , first row ) , where @xmath70 is defined on the interval @xmath71 $ ] .",
    "+    .values of the parameters and intervals used for training sample simulations [ cols=\"^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     to demonstrate that the algorithm is robust , eight sets of parameters ( table 5 ) were randomly simulated according to uniform distributions on the intervals represented in table 1 . for each set , a random experiment with 1000 runs was performed using matrix @xmath42 defined in the first example .",
    "the results presented in fig .",
    "7 demonstrate the robustness of the method , with rather low bias for the unfolded distribution in all eight cases .",
    "the main difficulties of the unfolding problem , which is a particular case of the inverse problem , are widely known .",
    "information is lost in measuring owing to the inefficiency of registration in the frequency domain because of the low - pass filter defined by the resolution function and to the inefficiency of events registration defined by the acceptance function of the set - up .",
    "thus , there are an infinite number of true distributions that give the same measured distribution and therefore a priori information about the solution must be used to solve an unfolding problem ( inverse problem ) .    one way",
    "to solve an unfolding problem is to replace the original problem by a problem for a smoothed original true distribution and to use a sliding window ( bin ) for a smoothing .",
    "this is equivalent to solving the unfolding problem for the true distribution in some binning .",
    "smoothing is low - pass filtering and the loss of information for a smoothed distribution due to the resolution function effect , which is another low - pass filter , is lower than for the original true distribution .",
    "solution of the unfolding problem is easier , but no information is obtained about the structure of the original true distribution inside the bin .    in practical applications of the unfolding procedure , the transformation matrix @xmath42 must be calculated .",
    "simulation of the measurement process is used for this , especially in nuclear and particle physics .",
    "this process is very time - consuming and the sample size for simulated events is often of the same order as for measured events .",
    "the calculated matrix will have many noisy matrix elements in this case , which is another source of instability in solving the inverse problem .",
    "main points related with difficulties of the unfolding problem have formulated above on physical level of rigor permit us summarize results of given paper and define place of proposed unfolding method among other known methods .",
    "the method presented here is a completely new approach to unfolding problems using machine learning concepts , including a training sample , a validation procedure and boosting .",
    "all a priori information about the solution is contained in the training sample , which is a set of physically motivated true distributions known from theory and other experiments .",
    "methods for selecting distributions for the training sample were presented in section 3 and are supported by previous research @xcite .    in the proposed method , an unfolded distribution can be calculated for a grid of points or for bins .",
    "there are no restrictions imposed by the dimensionality of the problem or the configuration of the bins or the grid .",
    "the method for identification provides a linear approximation of a transformation from the true distribution to the measured distribution if this transformation is non - linear .",
    "the numerical example presented demonstrates the robustness of the new unfolding procedure and the possibility of unfolding a whole set of distributions with a single calculated matrix for transformation @xmath42 .",
    "the set is defined as distributions used to create the training sample .",
    "biases and statistical errors for components of the unfolded distribution were calculated using a monte - carlo method with 1000 runs .",
    "the examples demonstrate that the bias is small for components of the unfolded distribution and for estimates of the statistical errors .",
    "it should be noted that such biases were investigated for unfolding for the first time .",
    "the unfolding procedure is validated using a machine learning approach and has a good statistical interpretation .",
    "the proposed method has wide potential for applications in nuclear and particle physics , where models for training samples can be proposed and monte - carlo simulations can be used to calculate transformation matrices",
    ".    9 v.p .",
    "zhigunov , improvement of resolution function as an inverse problem , nucl .",
    "methods 216 ( 1983 ) 183190 .",
    "v. blobel , unfolding methods in high energy physics experiments , cern 85 - 02 , 1985 .",
    "anikeev , a.a .",
    "spiridonov , v.p .",
    "zhigunov , correcting factors method as one of unfolding technique , nucl .",
    "methods a 322 ( 1992 ) 280285 .",
    "n. gagunashvili , unfolding of true distributions from experimental data distorted by detectors with finite resolutions , nucl .",
    ". methods a 343 ( 1993 ) 606609 .",
    "m. schmelling , the method of reduced cross entropy : a general approach to unfold probability distributions , nucl .",
    "instrum . methods a 340 ( 1994 ) 400412 .",
    "a. hcker , v. kartvelishvili , svd approach to data unfolding , nucl .",
    "methods a 372 ( 1996 ) 469481 .",
    "l. lindemann , g. zech , unfolding by weighting monte - carlo events , nucl .",
    "methods a 354 ( 1995 ) 516521 .",
    "g. dagostini , a multidimensional unfolding method based on bayes theorem , nucl .",
    "methods a 362 ( 1995 ) 487498 .",
    "v. blobel , an unfolding method for high - energy physics experiments , in : proceedings of the conference on statistical problems in particle physics , durham , england , 1822 march 2002 , pp .",
    "n. gagunashvili , unfolding with system identification , in : proceedings of the conference on statistical problems in particle physics , astrophysics and cosmology , 1215 september , 2005 , oxford , imperial college press , london , 2006 , pp .",
    "j. albert et al .",
    "unfolding of differential energy spectra in the magic experiment , nucl .",
    "methods a 583 ( 2007 ) 494506 .",
    "seber and a.j .",
    "lee , linear regression analysis , john wiley & sons , 2003 . n. draper and h. smith , applied regression analysis , john wiley & sons , 1998 .",
    "d. graupe , identification of systems , van nostrand reinhold and robert e. kreiger , huntington , new york , 1976 .",
    "l. ljung , system identification : theory for user , prentice hall , 1999 .",
    "fedorov , theory of optimal experiments , academic press , new york , 1972 .",
    "n. gagunashvili , parametric fitting of data obtained from detectors with finite resolution and limited acceptance , nucl .",
    "methods a 635 ( 2011 ) 8691 .",
    "n. gagunashvili , chi - square tests for comparing weighted histograms , nucl .",
    "methods a 614 ( 2010 ) 287296 .",
    "tan , m. steinbach , v. kumar , introduction to data mining , addison wesley , 2006 .",
    "witten , e. frank , data mining practical machine learning tools and techniques , morgan kaufman publishers , 2010 ."
  ],
  "abstract_text": [
    "<S> a procedure for unfolding the true distribution from experimental data is presented . </S>",
    "<S> machine learning methods are applied for simultaneous identification of an apparatus function and solving of an inverse problem . </S>",
    "<S> a priori information about the true distribution from theory or previous experiments is used for monte - carlo simulation of the training sample . </S>",
    "<S> the training sample can be used to calculate a transformation from the true distribution to the measured one . </S>",
    "<S> this transformation provides a robust solution for an unfolding problem with minimal biases and statistical errors for the set of distributions used to create the training sample . </S>",
    "<S> the dimensionality of the solved problem can be arbitrary . </S>",
    "<S> a numerical example is presented to illustrate and validate the procedure .    </S>",
    "<S> unfolding , system identification , d - optimization , apparatus function , deconvolution , robustness , boosting 02.30.zz , 07.05.kf , 07.05.fb </S>"
  ]
}