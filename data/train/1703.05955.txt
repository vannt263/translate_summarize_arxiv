{
  "article_text": [
    "researchers in science and engineering have paid their continuous concern on addressing the problem of online linear time - invariant equations , which is formulated as : @xmath0 where @xmath1 and @xmath2 are coefficients , and the unknown vector @xmath3 denotes the real - time solution with time @xmath4 evolves .",
    "when coefficient @xmath5 is non - singular , @xmath6 has an unique theoretical solution @xmath7 ; when coefficient @xmath5 is singular , linear equations ( [ eqn.le ] ) can have no solution or multiple solutions . a number of related problems such as matrix inversion @xcite , quadratic programming / minimisation @xcite , sylvester equations @xcite , lyapunov matrix equation @xcite and linear matrix equations @xmath8 @xcite can be transformed into linear equations ( [ eqn.le ] ) via vectorisation and kronecker product @xcite .",
    "such a problem is widely encountered in other fields of science and engineering such as ridge regression in machine learning @xcite , signal processing @xcite , optical flow in computer vision @xcite , and robotic inverse kinematics @xcite .    to address such a problem",
    ", the existing algorithms generally can fall into three categories : direct solution based on matrix inversion , serial - processing numerical algorithms , and the parallel - processing algorithms .",
    "evidently , either inverse based direct solution or numerical algorithms are less favourable to cope with large - scale or real - time problems because of its high computational cost ( typically @xmath9 operations , i.e. the cube of the dimensionality @xmath10 of coefficient - matrix @xmath5 ) . for less computational efficiency ,",
    "some @xmath11-operation algorithms are proposed in @xcite .",
    "analogue recurrent neural networks are thus proposed and analysed owing to its high computational efficiency , rich information distributed in neurons and the feasibility to implement the model into circuits @xcite .",
    "a kind of gradient - based explicit dynamical network ( termed as gradient neural networks ) was proposed for real - time linear equations solving @xcite .",
    "gradient neural networks in the form of an explicit system ( @xmath12 ) can be obtained based on the negative gradient of a scalar - valued energy function @xmath13 as follows @xcite : @xmath14 where design parameter @xmath15 is utilised to control the speed of convergence .",
    "note that , @xmath16 should be set as large as possible and we select @xmath17 for illustrative purpose in the simulative verification in section [ sec.simulation ] .    with the advantages of adopting an implicit - dynamics form ( i.e. , @xmath18 @xmath19 with the mass matrix @xmath20 )",
    ", zhang neural networks for solving online linear equations can be achieved by time derivative of a vector - formed error function @xmath21 @xcite as follows : @xmath22    recently , a novel implicit neural mode ( termed as improved zhang neural networks ) @xcite was proposed to achieve superior convergence performance in comparison with the explicit gradient net ( [ eqn.wnn ] ) and implicit zhang net ( [ eqn.znn ] ) , which can be depicted as follows : @xmath23 where @xmath24 is the identity matrix .    compared to explicit dynamic models ( e.g. , gradient neural networks ( [ eqn.wnn ] ) ) , implicit dynamic systems ( e.g. , conventional zhang model ( [ eqn.znn ] ) and improved zhang model ( [ eqn.cnn ] ) ) have higher abilities in representing dynamic systems because of preserving physical parameters in the coefficient matrices , e.g. , @xmath5 on the left - hand side of ( [ eqn.znn ] ) and ( [ eqn.cnn ] ) @xcite . in this sense , owing to the advantages of implicit systems , zhang models ( [ eqn.znn ] ) and ( [ eqn.cnn ] ) can be much superior to gradient neural networks ( [ eqn.wnn ] ) in the form of explicit dynamics . however ,",
    "in view of mass matrix @xmath25 , the existing implicit neural systems ( [ eqn.znn ] ) and ( [ eqn.cnn ] ) ) become differential - algebraic - equations ( can be formulated as constrained ordinary - differential - equations and abbreviated as dae ) , which can not be completely solvable and might become an initial state problem @xcite , when coefficient matrix @xmath5 is not invertible ( i.e. , the mass matrix is singular ) .",
    "this letter proposes a novel model ( termed as implicit gradient neural networks ) , which can both keep the advantages of implicit neural dynamics about rich representation of neural dynamics for circuit realisation @xcite and also overcome the drawbacks of the existing implicit neural models ( [ eqn.znn ] ) and ( [ eqn.cnn ] ) on the difficulty in coping with the problem of singular mass matrices .",
    "owing to adopting a positive - definite mass matrix , our model can still achieve global stability even when coefficient matrix @xmath5 is singular ( i.e. , no - solution and multi - solution cases ) .",
    "computer simulation results can demonstrate theoretical analysis on the proposed analogue network for online solution of simultaneous linear equations ( [ eqn.le ] ) .",
    "the contributions and novelties of this letter is as follows .",
    "* to the best of our knowledge , a positive - definite mass matrix is for the first time introduced to the implicit neural dynamics , which can effectively cope with singular coefficient matrix .",
    "* the proposed model can achieve global exponential convergence when linear equations have a unique solution , while it can still be global stable even for no - solution and multi - solution cases .",
    "based on the explicit gradient neural model ( [ eqn.wnn ] ) , the following design procedure is adopted to obtain the proposed implicit gradient neural networks .",
    "* we firstly define a scalar - valued norm - based energy function @xmath26",
    ". such an object function can reach its minimum point if and only if the solution @xmath6 of linear equations ( [ eqn.le ] ) is equal to its theoretical solution @xmath27 . *",
    "secondly , our model is designed to evolve along the negative gradient of this energy function @xmath26 until the minimum is reached @xcite : @xmath28 . * finally , different from explicit gradient model ( [ eqn.wnn ] ) adopting an identity mass matrix @xmath29 , a positive - definite mass matrix @xmath30 is introduced into our implicit dynamical model .",
    "consequently , we can obtain the following implicit gradient neural networks : @xmath31    evidently , our model ( [ eqn.iwnn ] ) remains ordinary - differential - equations ( ode ) owing to the usage of the positive - definite mass matrix @xmath30 even when @xmath5 is singular and still achieve global stability for no - solution and multi - solution cases .",
    "in addition , for hardware realisation , the proposed implicit neural dynamics ( [ eqn.iwnn ] ) has higher representing capability of dynamic systems in comparison with explicit gradient neural networks ( [ eqn.wnn ] ) .",
    "we analyse the global exponential convergence of the proposed implicit gradient neural networks ( [ eqn.iwnn ] ) when linear equations ( [ eqn.le ] ) have a unique solution , i.e. , coefficient @xmath5 in linear equations ( [ eqn.le ] ) is non - singular .    _",
    "theorem 1 : _ given a non - singular coefficient matrix @xmath32 and a coefficient vector @xmath33 , the state solution @xmath34 of implicit gradient neural networks ( [ eqn.iwnn ] ) can achieve globally exponential convergence to a unique exact solution @xmath7 of linear equations ( [ eqn.le ] ) .",
    "@xmath35    _ proof : _ given the exact solution @xmath36 , we can define @xmath37 as the solution error by implicit gradient neural networks ( [ eqn.iwnn ] ) .",
    "consequently , time derivation of @xmath38 can be as the following : @xmath39 .",
    "let us substitute @xmath38 as well as @xmath40 to implicit model ( [ eqn.iwnn ] ) and we can get the following equation : @xmath41 let us define a positive - definite lyapunov function candidate @xmath42 .",
    "it is evident that @xmath43 is positive definite in the sense that @xmath44 for any @xmath45 , and @xmath46 only for @xmath47 ( i.e. , @xmath6 is equal to the exact solution @xmath7 of linear equations @xmath48 ) . as a result ,",
    "@xmath49 along the state trajectory of the reformulated implicit model ( [ eqn.iwnn_err ] ) using the solution error @xmath38 is written as follows : @xmath50 , where @xmath51 and @xmath52 denote the minimum eigenvalue of @xmath53 and @xmath54 respectively . in the light of lyapunov stability theory",
    "@xcite , the proposed implicit neural model ( [ eqn.iwnn_err ] ) thus reaches globally asymptotic stability . for more desirable exponential convergence @xcite",
    ", @xmath49 can be reformulated as @xmath55 the above formulation can fall into two sub - situation : 1 ) @xmath56 and 2 ) @xmath57 .    * for the former situation @xmath56 , @xmath58 and thus exponential convergence rate is @xmath59 . * for the latter situation @xmath57 , @xmath60 and thus exponential convergence rate of ( [ eqn.iwnn ] ) to @xmath36 is @xmath16 .",
    "proof of theorem 1 is thus completed .",
    "[ c][c][1]time @xmath4 , ms   [ c][c][1 ]  @xmath61    global stability of the proposed implicit dynamical model ( [ eqn.iwnn ] ) can also be achieved when coefficient @xmath5 is singular as follows .",
    "_ theorem 2 : _ given a singular coefficient matrix @xmath32 and a coefficient vector @xmath33 , the state vector @xmath34 of implicit gradient neural networks ( [ eqn.iwnn ] ) can still be globally stable .",
    "@xmath35    _ proof : _ without the unique solution for singular matrix @xmath5 , the following lyapunov function candidate @xmath62 is considered .",
    "then , along the state trajectory of implicit gradient neural networks ( [ eqn.iwnn ] ) , time derivative of @xmath43 can be derived as : @xmath63 according to lyapunov stability theory @xcite , the implicit gradient neural networks ( [ eqn.iwnn ] ) is globally stable .",
    "[ c][c][1]time @xmath4 , ms [ c][c][1 ]  @xmath61    for verifying the aforementioned analysis , we consider the same illustrative example in @xcite . related to the non - singular coefficient matrix @xmath5 , the minimal eigenvalue @xmath64 of @xmath53 is @xmath65 , which falls into the first situation @xmath56 . as global exponential convergence rate @xmath66 of the implicit neural networks ( [ eqn.iwnn ] )",
    "can be @xmath67 with the design parameter @xmath68 according to the analysis of theorem 1 , convergence time to a tiny residual error @xmath69 is 29.9 @xmath70 .",
    "simulative result about the residual error @xmath61 is shown in figure [ fig.1 ] to demonstrate globally exponential convergence of the proposed implicit dynamical model ( [ eqn.iwnn ] ) , from six initial states randomly - selected within @xmath71 $ ] .    to show the global stability of the proposed model ( [ eqn.iwnn ] ) in theorem 2",
    ", we consider the singular coefficient matrix @xmath5 ( i.e. , @xmath72 ) and two coefficient vector @xmath73 corresponding to no - solution and multi - solution cases respectively : @xmath74 where , if @xmath75 linear equations @xmath48 has no theoretical solution ; if @xmath76 there is multiple solutions . in figure",
    "[ fig.2 ] , we visualise simulation results about the residual error @xmath61 for both no - solution and multi - solution cases , which can verify theoretical analysis on the implicit dynamical model ( [ eqn.iwnn ] ) in theorem 2 .",
    "a novel implicit gradient neural network for real - time solution of linear equations is proposed and analysed under different situations . global exponential convergence and stability on implicit dynamical models",
    "can be achieved owing to the introduction of a positive - definite mass matrix .",
    "simulation results substantiate theoretical analysis on the proposed neural networks .",
    "y. zhang , z. li , c. yi , k. chen , zhang neural network versus gradient neural network for online time - varying quadratic function minimization , advanced intelligent computing theories and applications ( 2008 ) 807 - 814 .",
    "y. zhang , y. yang , g. ruan , performance analysis of gradient neural network exploited for online time - varying quadratic minimization and equality - constrained quadratic programming , neurocomputing 74 ( 2011 ) 1710 - 1719 .",
    "k.chen , s. yue , y. zhang , matlab simulation and comparison of zhang neural network and gradient neural network for online solution of linear time - varying matrix equation axb - c=0 , advanced intelligent computing theories and applications ( 2008 ) 68 - 75 .",
    "y. zhang , k. chen , comparison on zhang neural network and gradient neural network for time - varying linear matrix equation axb = c solving , proceedings of ieee international conference on industrial technology , ( 2008 ) 1 - 6 .",
    "k. chen , s. gong , t. xiang , c.c .",
    "loy , cumulative attribute space for age and crowd density estimation , in : proceedings of ieee conference on computer vision and pattern recognition , 2013 , pp . 2467 - 2474 .",
    "k. chen , j. a. tuhtan , j. f. fuentes - prez , g. toming , m. musall , n. strokina , j.k .",
    "kmrinen , m. kruusmaa , estimation of flow turbulence metrics with a lateral line probe and regression , ieee transactions on instrumentation and measurement 66(4 ) ( 2017 ) 651660 .",
    "k. chen , l. zhang , y. zhang , cyclic motion generation of multi - link planar robot performing square end - effector trajectory analyzed via gradient - descent and zhang et al s neural - dynamic methods , proceedings of international symposium on systems and control in aerospace and astronautics ( 2008 ) 1 - 6 .",
    "y. zhang , d. guo , b. cai , k. chen , remedy scheme and theoretical analysis of joint - angle drift phenomenon for redundant robot manipulators , robotics and computer - integrated manufacturing 27 ( 2011 ) 860 - 869 .",
    "y. zhang , w. e. leithead , d. j. leith , time - series gaussian process regression based on toeplitz computation of @xmath77 operations and @xmath78-level storage , in : proceedings of the 44th ieee conference on decision and control , 2005 , pp .",
    "3711 - 3716 .",
    "w. e. leithead , y. zhang , @xmath77-operation approximation of covariance matrix inverse in gaussian process regression based on quasi - newton bfgs methods , communications in statistics - simulation and computation 36 ( 2007 ) 367 - 380 ."
  ],
  "abstract_text": [
    "<S> motivated by the advantages achieved by implicit analogue net for solving online linear equations , a novel implicit neural model is designed based on conventional explicit gradient neural networks in this letter by introducing a positive - definite mass matrix . </S>",
    "<S> in addition to taking the advantages of the implicit neural dynamics , the proposed implicit gradient neural networks can still achieve globally exponential convergence to the unique theoretical solution of linear equations and also global stability even under no - solution and multi - solution situations . </S>",
    "<S> simulative results verify theoretical convergence analysis on the proposed neural dynamics . </S>"
  ]
}