{
  "article_text": [
    "playing a vital role in quantum information processing , as well as being fundamental for characterizing quantum objects , quantum state tomography focuses on reconstructing the ( unknown ) state of a physical quantum system  @xcite , usually represented by the so - called density matrix @xmath2 ( the exact definition of a density matrix is given in section  [ set - up ] ) .",
    "this task is done by using outcomes of measurements performed on many independent systems identically prepared in the same state .",
    "the tomographic method , also named as linear / direct inversion  @xcite , is the simplest and oldest estimation procedure .",
    "it is actually the analogous of the least - square estimator in the quantum setting .",
    "although easy in computation and providing unbiased estimate  @xcite , it does not generate a physical density matrix as an output  @xcite .",
    "maximum likelihood estimation  @xcite is the current procedure of choice .",
    "unfortunately , it has some critical flaws detailed in  @xcite , including a huge computational complexity .",
    "furthermore , both these methods are not adaptive to the case where a system is in a state @xmath2 for which some additional information is available .",
    "note especially that , physicists focus on so - called pure states , for which @xmath3 .",
    "the problem of rank - adaptivity was tackled thanks to adequate penalization .",
    "rank - penalized maximum likelihood ( bic ) was introduced in  @xcite while a rank - penalized least - square estimator @xmath4 was proposed in  @xcite , together with a proof of its consistency . more specifically , when the density matrix of the system is @xmath5 with @xmath6 , the authors of @xcite proved that the frobenius norm of the estimation error satisfies @xmath7 where @xmath8 is the number of quantum measurements .",
    "the rate was improved to @xmath9 by  @xcite , using a thresholding method .",
    "note that the rate @xmath10 was first claimed in the paper , but in the corrigendum  @xcite , the authors acknowledge that this is not the case .",
    "the paper however contains a proof that no method can reach a rate smaller than @xmath11 .",
    "so , the minimax - optimal rate is somewhere in between @xmath12 and @xmath13 .",
    "note that all the aforementioned papers only cover the complete measurement case ( the definition is given in section  [ set - up ] , basically it means that we have observations for all the observables given by the pauli basis ) .",
    "the statistical relationship between matrix completion and quantum tomography with incomplete measurements ( in the le cam paradigm ) has been investigated in  @xcite . thus compressed sensing",
    "ideas have been successfully proposed in estimating a density state from incomplete measurements  @xcite .    on the other hand ,",
    "bayesian estimation has been considered in this context .",
    "the papers  @xcite compare bayesian methods to other methods on simulated data .",
    "more recently ,  @xcite discuss efficient algorithms for computing bayesian estimators .",
    "importantly , @xcite showed that bayesian method comes with natural error bars and is the most accurate scheme w.r.t . the expected error ( operational divergence ) ( even ) with finite samples .",
    "however , there is no theoretical guarantee on the convergence of these estimators .",
    "more works on quantum state tomography in various settings include  @xcite .    in this paper",
    ", we consider a pseudo - bayesian estimation , where the likelihood is replaced by pseudo - likelihoods based on various moments ( two estimators , corresponding to two different pseudo - likelihood , are actually proposed ) .",
    "using pac - bayesian theory  @xcite , we derive oracle inequalities for the pseudo - posterior mean .",
    "we obtain rates of convergence for these estimators in the complete measurement setting .",
    "one of them has a rate as good as the best known rate up to date @xmath14 ( still , the other one is interesting for computationnal reasons that are discussed in the paper ) .",
    "the rest of the paper is organized as follow .",
    "we recall the standard notations and basics about quantum theory in section [ set - up ] .",
    "then the definition of the prior and of the estimators are presented in section  [ def prior ] .",
    "the statistical analysis of the estimators are given in section  [ main ] , while all the proofs are delayed to the appendix  [ appendix ] .",
    "some numerical experiments on simulated and real datasets are given in section  [ num ] .",
    "a very good introduction to the notations and problems of quantum statistics is given in  @xcite .",
    "here , we only provide the basic definitions required for the paper .    in quantum physics , all the information on the physical state of a system can be encoded in its _ density matrix _ @xmath2 .",
    "depending on the system in hand , this matrix can have a finite or infinite number of entries .",
    "a two - level system of @xmath1-qubits is represented by a @xmath15 density matrix @xmath2 , with coefficients in @xmath16 . for the sake of simplicity",
    ", the notation @xmath17 is used in  @xcite , so note that @xmath2 is a @xmath18 matrix .",
    "this matrix is hermitian @xmath19 ( i.e. self - adjoint ) , semidefinite positive @xmath20 and has @xmath21 .",
    "additionally , it often makes sense to assume that the rank of @xmath2 is small @xcite . in theory",
    ", the rank can be any integer between @xmath0 and @xmath22 , but physicists are especially interested in pure states and a pure state @xmath23 can be defined by @xmath3 .    the objective of quantum tomography is to estimate @xmath2 on the basis of experimental observations of many independent and identically systems prepared in the state @xmath2 by the same experimental device .    for each particle ( qubit ) , one can measure one of the three pauli observables @xmath24 . the outcome for each",
    "will be @xmath25 , or @xmath26 , randomly ( the corresponding probability depends on the state @xmath2 and will be given in   below ) .",
    "thus for a @xmath27-qubits system , we consider @xmath28 possible experimental observables .",
    "the set of all possible performed observables is @xmath29 where vector @xmath30 identifies the experiment .",
    "the outcome for each fixed observable setting will be a random vector @xmath31 , thus there are @xmath32 outcomes in total .",
    "let us denote @xmath33 a @xmath34-valued random vector that is the outcome of an experiment indexed by @xmath35 . from the basic principles of quantum mechanics ( born s rule ) ,",
    "its probability distribution is given by @xmath36 where @xmath37 and @xmath38 is the orthogonal projection associated to the eigenvalue @xmath39 in the diagonalization of @xmath40 for @xmath41 and @xmath42  that is @xmath43 .",
    "the quantum state tomography problem is as follows : a physicist has access to an experimental device that produces @xmath1-qubits in a state @xmath5 , and @xmath5 is assumed to be unknown .",
    "he / she can produce a large number of replications of the @xmath1-qubits and wants to infer @xmath5 from this .",
    "in the complete measurement case , for _ each _ experiment setting @xmath44 , the experimenter repeats @xmath45 times the experiment corresponding to @xmath35 and thus collects @xmath46 independent random copies of @xmath47 , say @xmath48 .",
    "as there are @xmath28 possible experiment settings @xmath49 , we define the * quantum sample * size as @xmath50",
    ". we will refer to @xmath51 as @xmath52 ( for data ) .",
    "note that the case where we would only have access to experiments @xmath53 where @xmath54 is some proper subset of @xmath55 ( @xmath56 ) is referred to as the incomplete measurement case . in this paper",
    ", we focus on the complete measurement case , but the extension to the incomplete case is discussed in section  [ conclusion ] .",
    "a natural idea is to define the empirical frequencies @xmath57 note that @xmath58 is an unbiased estimator of the probability @xmath59 . the inversion method is based on solving the linear system of equations @xmath60 as mentioned above , the computation of @xmath61 is quite straighforward .",
    "explicit formulas are classical , see e.g.  @xcite .",
    "another commonly used method is maximum likelihood ( ml ) estimation , where the likelihood is @xmath62^ { n_{\\mathbf{a},\\mathbf{s } } } , \\ ] ] where @xmath63 is the number of times we observed output @xmath64 in experiment @xmath35 ( obviously , @xmath65 ) .",
    "as mentioned in the introduction , both methods suffer many drawbacks .",
    "the inversion method returns a matrix @xmath61 that usually does not satisfy the axioms of a density matrix .",
    "ml becomes expensive ( inpractical ) for @xmath66 .",
    "moreover , these two methods can not take advantage of a prior knowledge ( e.x .",
    "low - rank state ) .",
    "considering the expansion of the density matrix @xmath23 in the @xmath1pauli basis , i.e. @xmath67 @xmath68 one can also estimate the density matrix via estimating the coefficients in the pauli expansion .",
    "this was studied in  @xcite where the authors also make a sparsity assumption : that is , most of @xmath69 are small or very close to @xmath70 .",
    "note that , this is not related to the setting we explore ( low - rank assumption ) .",
    "we now turn to the definition of a prior distribution on density matrices that will allow to perform ( pseudo-)bayesian estimation .",
    "we remind that the idea of bayesian statistics is to encode the prior information on density matrices through a prior distribution @xmath71 .",
    "inference is then done through the posterior distribution @xmath72 . here , for computational reasons ,",
    "we replace the likelihood by a pseudo - likelihood .",
    "this is an increasingly popular method in bayesian statistics  @xcite and in machine learning  @xcite .",
    "we define the pseudo - posterior by @xmath73   \\pi({\\rm d}\\nu ) ,   \\label{pseudo_post}\\end{aligned}\\ ] ] the pseudo - likelihood being @xmath74 $ ] . the term @xmath75 can be specified by the user .",
    "two examples are provided in section  [ main ] . as a replacement of the likelihood ,",
    "this term plays the role of the empirical evidence .",
    "more specially    * the role of @xmath74 $ ] is to give more weight to the density @xmath76 when it fits the data well ; * the role of @xmath77 , the prior , is to restrict the posterior to the space of densities ( and even give more weight to low - rank matrices if needed ) ; * @xmath78 is a free parameter that allows to tune the balance between evidence from the data and prior information .",
    "we finally define the pseudo - posterior mean ( also refered to as gibbs estimator , pac - bayesian estimator or ewa , for exponentially weighted aggregate  @xcite ) : @xmath79 the definition of the estimator @xmath80 based on the pseudo - posterior @xmath81 is actually validated by the theoretical results from section  [ main ] .      in the single qubit state estimation @xmath82 ,",
    "the representation of the quantum constraints is explicit @xcite .",
    "thus , one can place a prior distribution on the polar reparametrization of the density .",
    "up to our knowledge , this has not been extended to the case @xmath83 , and this extension seems not straightforward . for general n - qubit densities , uninformative priors ( e.g the haar measure ) are put on @xmath84 matrices ( @xmath85 ) and the density state is built by @xmath86 @xcite .",
    "one could also define a prior on the coefficients @xmath87 of @xmath2 on the pauli basis .",
    "nevertheless , none of these approaches seem helpful for rank adaptation .    the idea for our prior",
    "is inspired by the priors used for low - rank matrix estimation in machine learning , e.g.  @xcite and the references therein .",
    "hereafter , we describe in details the prior construction .",
    "let @xmath88 be a vector in @xmath89 ( @xmath90 in our model ) , then @xmath91 is a hermitian , semi - definite positive matrix in @xmath92 with @xmath93 .",
    "additionally , we can normalize @xmath88 ( that is replace @xmath88 by @xmath94 ) , this lead to @xmath95 .",
    "so , @xmath91 satisfies the conditions of a density matrix ( with rank-@xmath0 ) .",
    "now , let @xmath96 be @xmath97 normalized vectors in @xmath98 and @xmath99 be non - negative weights with @xmath100",
    ". put @xmath101 then @xmath102 is clearly a density matrix : it is hermitian ( as a sum of hermitian matrices ) , it is semi - definite positive ( same reason ) and @xmath103    moreover , note that any density matrix can be written in such way , as we know that for any density matrix @xmath2 , @xmath104 and just write @xmath105 with the @xmath106 s being _ orthogonal _ , where @xmath107 .",
    "the only difference in  ( [ formula ] ) is that we do not require that the @xmath108 s are orthogonal .",
    "thus , it is easier to simulate a matrix @xmath2 by simulating the @xmath108 s and @xmath109 in   than by simulating @xmath110 and @xmath111 in  .",
    "also , note that the @xmath112 s are not necessarily the eigenvalues of @xmath2 .",
    "we define the prior definition on @xmath2 , @xmath71 , by @xmath113 where @xmath114 is the dirichlet distribution with parameters @xmath115 .    to get an approximate rank-@xmath0 matrix @xmath2",
    ", one can take all parameters of the dirichlet distribution equal to a constant that is very closed to 0 ( e.g @xmath116 ) . and",
    "a typical drawing will lead to one of the @xmath117 close to @xmath118 and the others close to @xmath119 .",
    "see  @xcite for more discussion on choosing the parameters for dirichlet distribution .",
    "theoretical recommendations for the @xmath120 s are given in section  [ main ] below .",
    "we could impose the @xmath108 s to be orthogonal in practice .",
    "the theoretical results would be unchanged , however , the implementation of our method would become trickier . note that to sample from the uniform distribution on the sphere is rather easy .",
    "we can for example simulate @xmath121 from any isotropic distribution , e.g. @xmath122 and define @xmath123 .",
    "here , we consider two natural ways to compare a theoretical density @xmath124 and the observations : first @xmath125 should be close to the empirical part @xmath126 ; second @xmath124 should be close to the least square ( invert ) estimator @xmath127 . as we have no reason to prefer one in advance , we define and study @xmath128 estimators .",
    "we consider @xmath129 ^ 2\\ ] ] and @xmath130 \\pi({\\rm d}\\nu).\\end{aligned}\\ ] ] note that if we use the shortened notation @xmath131_{\\mathbf{a},\\mathbf{s}}$ ] and @xmath132_{\\mathbf{a},\\mathbf{s}}$ ] then @xmath133 ( frobenius norm ) .",
    "this distance quantifies how far the probabilities and the empirical frequencies in the sample are .",
    "now , let us take : @xmath134 and @xmath135 \\pi({\\rm d}\\nu).\\end{aligned}\\ ] ] in another words , this estimator finds a balance between prior information and closeness to the least square estimate @xmath61 . from a computational point of view",
    ", this estimator is easier to implement than the previous estimator .",
    "[ a1 ] fix some constants @xmath136 and @xmath137 ( that do not depend on @xmath46 nor @xmath1 ) .",
    "we assume that the parameters of the dirichlet prior distribution @xmath138 satisfy    * @xmath139 , * @xmath140 , * @xmath141 .",
    "note that this assumption is satisfied for @xmath142 with @xmath143 .",
    "the first theorem provides the concentration bound on the square error of the first estimator @xmath144 .",
    "the proof of this theorem is left to the .",
    "[ thrm : rate 1 ] fix a small @xmath145 . under , for @xmath146 , with probability at least @xmath147 ,",
    "one has @xmath148 where @xmath149 is a constant that depends only on @xmath150 .    as said in the introduction , the best known rate up - to - date in this problem is @xmath151 , so our estimator @xmath152 reaches this rate ( up to log terms ) .",
    "this rate is actually @xmath153 and the best lower bound known in this case is @xmath154  @xcite ( we remind that @xmath17 ) .",
    "the next theorem presents the square error bound of the second estimator @xmath155 . here again , see the appendix for the proof .",
    "[ rate 2 ] fix a small @xmath145 . under , for @xmath156 , with probability at least @xmath147 , @xmath157 where @xmath158 is a constant that depends only on @xmath150 .",
    "the guarantee for @xmath159 is far less satisfactory .",
    "however , as this estimator is easier to compute , we think it is interesting to provide a convergence rate , even if it is far from optimal : note that for a fixed @xmath97 , the bound goes to @xmath70 when @xmath160 .",
    "experiments show that @xmath156 is actually not the best choice for dens - estimator . the choice @xmath161 ( heuristically motivated by  @xcite ) leads to results comparable to the prob - estimator in section [ num ] .",
    "this leads to the conjecture that the rate of @xmath162 is much better than @xmath163 but this is still an open question .",
    "we implement the two proposed estimators via the metropolis - hasting ( mh ) algorithm  @xcite .",
    "note that to draw @xmath164 is equivalent to draw @xmath165 with @xmath166 .",
    "thus , instead of @xmath117 , we conduct a mh updating for @xmath167 .",
    "so the objective is to produce a markov chain @xmath168 . from this , we deduce obviously the sequence @xmath169 and use the following empirical mean as the monte - carlo approximation of our estimator : @xmath170    for @xmath171 from @xmath172 to @xmath173 , we iteratively update through the following steps :    updating for @xmath174 : : :    for @xmath175 from @xmath118 to @xmath176 , +    sample @xmath177 where    @xmath178 is a proposal distribution given explicitely below .",
    "+    calculate    @xmath179 .",
    "+    set @xmath180 where @xmath181 is the    acceptance ratio given below .",
    "+    put    @xmath182 .",
    "updating for @xmath183 : : :    for @xmath175 from @xmath118 to @xmath176 , +    sample @xmath184 from the uniform distribution on    the unit sphere . +    set @xmath185 where @xmath186 is the    acceptance ratio given below .",
    "let us now give precisely @xmath178 , @xmath187 and @xmath188 .",
    "we define @xmath189 as the probability distribution of @xmath190 where @xmath191 . following  @xcite the acceptance ratios",
    "are then given by : @xmath192 and @xmath193 where @xmath194 stands for @xmath195 or @xmath196 depending on the estimator we are computing .",
    "we study the numerical performance of the prob - estimators with @xmath197 , i.e. @xmath198 and the dens - estimator with @xmath199 , i.e. @xmath200 on the following settings , all with @xmath201 ( @xmath202 ) :    * a pure state density ( rank-@xmath172 ) @xmath203 with @xmath204 , * a rank-@xmath128 density matrix that @xmath205 with @xmath206 being two normalized orthogonal vectors in @xmath207 , * an ",
    "approximate rank-@xmath208 \" density matrix : @xmath209 .",
    "note that by ",
    "approximate rank-@xmath208 \" , we mean that @xmath124 is very well approximated by a rank-@xmath208 matrix @xmath210 ( in the sense that @xmath211 is small ) , but in general @xmath124 itself is full rank , * a maximal mixed state ( rank-@xmath97 ) .",
    "the experiments are done for @xmath212 .",
    "the parameter for @xmath213 is @xmath214 .",
    "we repeat each experiment @xmath215 times , and compute the mean of the square error , mse , @xmath216 for each estimator , together with the associated standard deviation ( between brackets in tables  [ tablen_4],[tablen_3],[tablen_2 ] ) .",
    "we compare the prob- and dens - estimator to the simple inversion procedure and to the thresholding estimator of @xcite .",
    "the results are given in tables  [ tablen_4],[tablen_3],[tablen_2 ] ( outputs from the * r * software ) .",
    "the conclusions are :    * the prob - estimator seems to be the most accurate but also comes with a larger standard deviation .",
    "this might be due to slow convergence of the mcmc procedure .",
    "indeed each step is computationally highly expensive . *",
    "the dens - estimator is easier to compute and while it is less accurate than the prob - estimator , it still shows better results than the direct inversion method . *",
    "the thresholding estimator of @xcite works well for rank-1 states but seems to bring too much bias for other states .    besides the square error ,",
    "the eigenvalues of the estimates are also important when reconstructing density matrices . in figure",
    "[ eigenvalues_simulated ] , the dens - estimator returns with eigenvalues similar to the true eigenvalues of the true density matrix , while the prob - estimator seems not to shrink enough .    . ]",
    "@xmath217 & @xmath218 +   + inversion & 175 ( 4e-4 ) & 14.8 ( 2e-5 ) & 2.71 ( 8e-6 ) & 1.55 ( 5e-6 ) + thresholding & 93.5 ( 3e-4 ) & * 12.6 * ( 3e-5 ) & * .596 * ( 2e-6 ) & * .412 * ( 2e-6 ) + prob & 86.3 ( 6e-4 ) & 22.4 ( 2e-4 ) & 10.5 ( 6e-5 ) & 5.13 ( 2e-5 ) + dens & * 51.5 * ( 2e-4 ) & 21.7 ( 7e-5 ) & 13.1 ( 3e-5 ) & 13.2 ( 2e-5 ) +   + inversion & 16.8 ( 8e-4 ) & 15.9 ( 3e-4 ) & 15.9 ( 1e-4 ) & 15.8 ( 7e-5 ) + thresholding & 14.9 ( 3e-4 ) & 15.5 ( 7e-5 ) & 15.5 ( 9e-6 ) & 15.5 ( 7e-6 ) + prob & * 9.29 * ( 2e-3 ) & * 7.90 * ( 1e-3 ) & * 8.46 * ( 1e-3 ) & * 7.84 * ( 8e-4 ) + dens & 14.5 ( 3e-4 ) & 14.6 ( 3e-4 ) & 14.4 ( 3e-4 ) & 14.5 ( 4e-4 ) +   + inversion & 15.9 ( 8e-4 ) & 15.4 ( 2e-4 ) & 15.3 ( 1e-4 ) & 15.2 ( 4e-5 ) + thresholding & 14.3 ( 2e-4 ) & 14.2 ( 3e-4 ) & 15.0 ( 1e-5 ) & 15.0 ( 6e-6 ) + prob & * 8.88 * ( 9e-4 ) & * 7.68 * ( 2e-3 ) & * 8.11 * ( 1e-3 ) & * 7.39 * ( 1e-3 ) + dens & 13.9 ( 4e-4 ) & 15.1 ( 2e-4 ) & 14.2 ( 3e-4 ) & 14.2 ( 2e-4 ) +   + inversion & 15.9 ( 4e-4 ) & 6.57 ( 7e-5 ) & 5.09 ( 5e-5 ) & 4.76 ( 2e-5 ) + thresholding & * 4.67 * ( 9e-5 ) & 5.59 ( 5e-5 ) & 5.34 ( 8e-5 ) & 6.06 ( 8e-5 ) + prob & 5.44 ( 2e-4 ) & * 3.37 * ( 8e-5 ) & * 3.31 * ( 8e-5 ) & * 3.20 * ( 8e-5 ) + dens & 5.72 ( 9e-5 ) & 4.47 ( 6e-5 ) & 4.56 ( 4e-5 ) & 4.24 ( 2e-5 ) +     @xmath217 & @xmath218 +   + inversion & 39.5 ( 9e-4 ) & 3.17 ( 9e-5 ) & .559 ( 1e-5 ) & .343 ( 1e-5 ) + thresholding & 21.4 ( 6e-4 ) & * 2.26 * ( 1e-4 ) & * .196 * ( 1e-5 ) & * .152 * ( 1e-5 ) + prob & 40.3 ( 2e-2 ) & 5.79 ( 4e-4 ) & 2.95 ( 2e-4 ) & 1.78 ( 1e-4 ) + dens & * 12.8 * ( 5e-4 ) & 2.73 ( 2e-4 ) & 1.24 ( 4e-5 ) & 1.07 ( 4e-5 ) +   + inversion & 3.69 ( 3e-3 ) & 3.35 ( 6e-4 ) & 3.32 ( 4e-4 ) & 3.31 ( 2e-4 ) + thresholding & 2.94 ( 1e-3 ) & 3.05 ( 2e-4 ) & 3.04 ( 6e-5 ) & 3.05 ( 5e-5 ) + prob & * 1.91 * ( 5e-3 ) & * 1.17 * ( 3e-3 ) & * 1.18 * ( 3e-3 ) & * 1.14 * ( 2e-3 ) + dens & 2.83 ( 8e-4 ) & 2.89 ( 3e-4 ) & 2.89 ( 3e-4 ) & 3.00 ( 1e-4 ) +   + inversion & 3.33 ( 2e-4 ) & 3.22 ( 8e-4 ) & 3.19 ( 3e-4 ) & 3.18 ( 2e-4 ) + thresholding & 2.81 ( 1e-3 ) & 2.96 ( 1e-4 ) & 2.97 ( 8e-5 ) & 2.97 ( 9e-5 ) + prob & * 1.10 * ( 5e-3 ) & * .551 * ( 5e-3 ) & * .189 * ( 2e-3 ) & * .113 * ( 1e-3 ) + dens & 2.74 ( 6e-4 ) & 2.88 ( 3e-4 ) & 2.91 ( 3e-4 ) & 2.91 ( 2e-4 ) +   + inversion & 6.98 ( 2e-3 ) & 3.19 ( 4e-4 ) & 2.88 ( 2e-4 ) & 3.01 ( 1e-4 ) + thresholding & 4.41 ( 6e-4 ) & 3.26 ( 6e-4 ) & 3.19 ( 2e-4 ) & 3.29 ( 1e-4 ) + prob & 3.63 ( 1e-3 ) & * 2.70 * ( 7e-4 ) & * 2.28 * ( 7e-4 ) & * 2.29 * ( 1e-3 ) + dens & * 3.18 * ( 6e-4 ) & 2.99 ( 4e-4 ) & 2.90 ( 2e-4 ) & 3.04 ( 1e-4 ) +     @xmath217 & @xmath218 +   + inversion & 61.9 ( 3e-3 ) & 9.22 ( 5e-4 ) & .802 ( 4e-5 ) & .772 ( 6e-5 ) + thresholding & * 49.4 * ( 3e-3 ) & * 4.06 * ( 3e-4 ) & * .737 * ( 4e-5 ) & * .356 * ( 2e-5 ) + prob & 102 ( 8e-3 ) & 39.7 ( 2e-3 ) & 9.37 ( 8e-4 ) & 7.19 ( 5e-4 ) + dens & 52.2 ( 3e-3 ) & 7.57 ( 5e-4 ) & 1.91 ( 9e-5 ) & 1.08 ( 2e-5 ) +   + inversion & 8.24 ( 2e-2 ) & 7.91 ( 3.2e-3 ) & 7.81 ( 2e-3 ) & 7.74 ( 7e-4 ) + thresholding & 5.13 ( 3e-3 ) & 5.34 ( 1.1e-3 ) & 5.32 ( 5e-4 ) & 5.33 ( 4e-4 ) + prob & * 2.62 * ( 2e-2 ) & * 1.77 * ( 7.4e-3 ) & * 1.79 * ( 8e-3 ) & * 1.73 * ( 5e-3 ) + dens & 4.53 ( 3e-3 ) & 5.20 ( 1.5e-3 ) & 5.24 ( 9e-4 ) & 5.24 ( 9e-4 ) +   + inversion & 8.12 ( 2e-2 ) & 7.54 ( 4e-3 ) & 7.54 ( 1.2e-3 ) & 7.56 ( 6e-4 ) + thresholding & 4.95 ( 4e-3 ) & 5.19 ( 8e-4 ) & 5.23 ( 5e-4 ) & 5.22 ( 4e-4 ) + prob & * 2.69 * ( 2e-2 ) & * 1.82 * ( 1.1e-2 ) & * 1.52 * ( 6e-3 ) & * 1.58 * ( 6e-3 ) + dens & 4.40 ( 4e-3 ) & 5.02 ( 1.3e-3 ) & 5.11 ( 1e-3 ) & 5.15 ( 6e-4 ) +   + inversion & 3.03 ( 9e-3 ) & 2.12 ( 2e-3 ) & 2.11 ( 2e-3 ) & 2.11 ( 1e-3 ) + thresholding & 2.78 ( 8e-3 ) & 2.36 ( 2e-3 ) & 2.21 ( 2e-3 ) & 2.25 ( 1e-3 ) + prob & 2.32 ( 2e-2 ) & * 1.15 * ( 5e-3 ) & * 1.19 * ( 5e-3 ) & * 1.07 * ( 4e-3 ) + dens & * 2.30 * ( 6e-3 ) & 2.11 ( 2e-3 ) & 2.06 ( 2e-3 ) & 2.09 ( 1e-3 ) +      the experiments performed to produce the data is explained in @xcite .",
    "the data was kindly provided by m.  gu and t. monz .",
    "it had been used in @xcite .",
    "we apply two proposed estimators to the real data set of a system of 4 ions which is smolin state further manipulated . in figure [ realdata ]",
    "we plot the eigenvalues of the inversion estimator and our ones .    ]",
    "note that the distribution of the eigenvalues of the three estimators are rather different .",
    "still , it seems that all estimators return results compatible with a rank-2 state .",
    "we propose a novel prior and introduce two pseudo - bayesian estimators for the density matrix : the dens - estimator and the prob - estimator .",
    "the prob - estimator reaches the best up - to - date rate of convergence in the low - rank case . on the other hand , computation of the dens - estimator",
    "is an easier task . in practice",
    ", we recommend the prob - estimator .",
    "however , in cases where the mcmc shows activities of lacking of convergence , the dens - estimator can be used as a reasonable alternative .    note also that the prob - estimator can be extended to the incomplete measurement case .",
    "we consider the ( incomplete ) pseudo - likelihood as @xmath219 ^ 2,\\end{aligned}\\ ] ] where @xmath220 .",
    "the study in this case will be the object of future works .",
    "open questions include faster algorithms based on optimization ( in the spirit of @xcite ) .",
    "also , from a theoretical perspective , the most important question is the minimax lower bound .",
    "both authors gratefully acknowledge financial support from genes and by the french national research agency ( anr ) under the grant labex ecodec ( anr-11- labex-0047 ) . p.alquier",
    "gratefully acknowledges financial support from the research programme _",
    "new challenges for new data _ from lcl and genes , hosted by the _ fondation du risque_.",
    "we first remind here a version of hoeffding s inequality for bounded random variables .",
    "first inequality : @xmath227}_{= : c(a , s ) }        [ p^0_{a , s}-\\hat{p}_{a , s } ] \\right )    \\\\    & = \\prod_{a\\in    \\mathcal{e}^n } \\mathbb{e }    \\exp\\left(\\lambda   \\sum_{s \\in \\mathcal{r}^n } c(a , s )        \\left[p^0_{a , s}-\\frac{1}{m}\\sum_{i=1}^m \\mathbf{1}(r_i^a = s)\\right ] \\right )   \\\\   & = \\prod_{a\\in \\mathcal{e}^n } \\mathbb{e }   \\exp\\bigg(\\frac{\\lambda}{m }   \\sum_{i=1}^m \\underbrace{\\left [ \\sum_{s   \\in \\mathcal{r}^n } c(a , s ) \\{p^0_{a , s } -   \\mathbf{1}(r_i^a = s)\\ } \\right]}_{= : y_{i , a}}\\bigg )   \\end{aligned}\\ ] ] we have that @xmath228 . then , using cauchy - schwartz inequality @xmath229 ^ 2\\right )     \\\\     \\leq \\left(\\sum_{s \\in \\mathcal{r}^n } c(a , s)^2\\right )    \\left(\\sum_{s \\in \\mathcal{r}^n }   |p^0_{a , s } -     \\mathbf{1}(r_i^a = s ) | \\right )   \\leq 2 \\left(\\sum_{s \\in \\mathcal{r}^n } c(a , s)^2\\right ) .",
    "\\end{gathered}\\ ] ] so we can apply hoeffding s inequality ( lemma  [ bernstein ine ] ) : @xmath230    \\\\    & \\leq   \\exp\\left [     \\frac{\\lambda^2}{4 m } \\|p - p_\\nu\\|_f^2   \\right].\\end{aligned}\\ ] ] second inequality : same proof , just replace @xmath231 by @xmath232 .    [ lem3 ] for @xmath233 , we have @xmath234\\|p^0-p_\\nu\\|_f^2        \\right\\ }      \\leq 1 ,   \\\\         \\mathbb{e}\\exp\\left\\{\\lambda\\left[1-         \\frac{\\lambda}{m }    \\right]\\|p^0-p_\\nu\\|_f^2      -\\lambda \\left(\\|p_\\nu - \\hat{p}\\|_f^2      - \\|p^0-\\hat{p}\\|_f^2\\right)\\right\\ } \\leq 1 .",
    "\\label{apbern2 }   \\end{aligned}\\ ] ]          we rewrite ( [ apbern2 ] ) in lemma  [ lem3 ] as follows @xmath240 \\|p^0 -p_\\nu\\|_f^2   - \\lambda   \\left(\\| p_\\nu - \\hat{p } \\|_f^2   - \\| p^0 - \\hat{p}\\|_f^2\\right )   \\bigg\\ } \\pi ( d\\nu )    \\leq 1 .",
    "\\end{aligned}\\ ] ] by using fubini s theorem @xmath241   \\|p^0-p_\\nu\\|_f^2 - \\lambda   \\left(\\| p_\\nu - \\hat{p } \\|_f^2   - \\| p^0- \\hat{p}\\|_f^2\\right )   \\bigg\\ } \\pi ( d\\nu )    \\leq 1 .",
    "\\end{aligned}\\ ] ] now , using  ( * ? ? ?",
    "* lemma 1.1.3 ) , for any distribution @xmath242 , we have @xmath243   \\int \\|p^0-p_\\nu\\|_f^2 \\hat{\\pi}(d\\nu )    - \\log\\left(2 / \\epsilon \\right )     - \\mathcal{k}(\\hat{\\pi},\\pi ) \\hspace*{1 cm } \\\\ - \\lambda   \\left(\\int \\| p_{\\nu } - \\hat{p}\\|_f^2    \\hat{\\pi}(d\\nu )   - \\| p^0- \\hat{\\rho}\\|_f^2\\right )   \\bigg\\ }    \\leq \\frac{\\epsilon}{2 }   \\end{aligned}\\ ] ] and with @xmath244 , one has @xmath245   \\int \\|p^0 -p_\\nu\\|_f^2 \\hat{\\pi}(d\\nu )    - \\log\\left(2 / \\epsilon \\right )     - \\mathcal{k}(\\hat{\\pi},\\pi ) \\hspace*{1 cm } \\\\ - \\lambda   \\left(\\int \\| p_{\\nu } - \\hat{p}\\|_f^2    \\hat{\\pi}(d\\nu )   - \\| p^0- \\hat{\\rho}\\|_f^2\\right )   \\bigg ] \\geq 0   \\bigg\\ }    \\leq \\frac{\\epsilon}{2}.   \\end{aligned}\\ ] ] taking the complementary yields successfully the results .",
    "[ bound1 ] for @xmath246 s.t @xmath247 , with probability @xmath248 we have : @xmath249\\int \\|p_\\nu    -p^0\\|_f^2\\tilde{\\pi}({\\rm d}\\nu )             + \\frac{2\\mathcal{k}(\\hat{\\pi},\\pi )             +    2\\log\\left(\\frac{2}{\\epsilon }              \\right)}{\\lambda }   } { 1 - \\frac{\\lambda}{m } } \\end{aligned}\\ ] ] and @xmath250    \\int \\|\\nu-\\rho^0\\|_f^2\\hat{\\pi}({\\rm d}\\nu )             + \\frac{2\\mathcal{k}(\\hat{\\pi},\\pi )             +   2\\log\\left(\\frac{2}{\\epsilon }              \\right)}{2^n \\lambda }     } { 1- \\frac{\\lambda}{m}}.   \\end{aligned}\\ ] ]    [ proofbound1 ] using the same proof of lemma  [ pac - bound : empirical ] for inequality ( [ apbern1 ] ) in lemma  [ lem3 ] , we obtain with probability at least @xmath237 , for any distribution @xmath238 that @xmath251 \\int \\| p_{\\nu}- p^0\\|_f^2   \\hat{\\pi}(d\\nu )   + \\| p^0- \\hat{p}\\|_f^2 + \\frac{\\mathcal{k}(\\hat{\\pi},\\pi ) + \\log(\\frac{2}{\\epsilon})}{\\lambda }   \\end{aligned}\\ ] ] with a union argument , combining the lemma  [ pac - bound : empirical ] and the above inequality yields the following inequality with probability at least @xmath252 , for any @xmath242 @xmath253 \\int \\|p_{\\nu}-p^0\\|_f^2\\hat{\\pi}({\\rm d}\\nu )    + \\frac{2\\mathcal{k}(\\hat{\\pi},\\pi )   +     2\\log(2/\\epsilon ) } { \\lambda } } { 1-\\frac{\\lambda}{m } }   \\end{aligned}\\ ] ] taking @xmath254 ( once again ,  ( * ? ? ?",
    "* lemma 1.1.3 ) ) be the minimizer of the right hand side of the above inequality , we obtain .",
    "moreover , in  ( * ? ?",
    "* equation  ( 5 ) ) states that , for any @xmath76 : @xmath255 for some operator @xmath256 .",
    "therefore @xmath257 the eigenvalues of @xmath258 are known , they range between @xmath22 and @xmath259 according to  ( * ? ? ?",
    "* proposition 1 ) .",
    "thus , for any @xmath76 , @xmath260 and so we obtain ( [ oracle main ] ) .    in the following , we will consider @xmath242 as a restriction of the prior to a local set around the true density matrix @xmath261 .",
    "this allows us to obtain an explicit bound of the left hand side of ( [ oracle main ] ) .",
    "let @xmath262 be the spectral decomposition of @xmath5 .",
    "now , the kullback - leibler term @xmath275 the first log term @xmath276   , d = 2^n   \\\\   & \\geq   \\bigg [ \\dfrac{c^{d-1}}{2^d\\pi }   \\bigg]^r    \\geq    \\dfrac{c^{r(d-1)}}{2^{4rd } }    .\\end{aligned}\\ ] ] note for the above calculation : it is greater or equal to the volume of the ( d-1)-``circle '' with radius @xmath277 over the surface area of the @xmath97-unit - sphere \" .    the second log term in the kullback - leibler term @xmath278 for some constant @xmath279 that depends only on @xmath271 . since @xmath280 for every @xmath281 , we can lower bound the integrand by @xmath0 and also @xmath282 .",
    "the interval of integration contains at least an interval of length @xmath283 .",
    "this trick was presented in ( * ? ? ?",
    "* lemma 6.1 , page 518 )        substituting  ( [ kl1]),([estimate norm 1 ] ) into  ( [ oracle main ] ) , we obtain @xmath287    ( 3d\\delta + 2 rc)^2}{1-\\frac{\\lambda}{m } } \\\\ &   +   \\frac{a rd\\log(\\frac{1}{c } ) + c_{d_1,d_2}d(\\log(d)+ \\log(\\frac{1}{\\delta } ) )    + 2\\log(2/\\epsilon   ) } { \\lambda 2^n [ 1-\\frac{\\lambda}{m } ] } \\bigg\\}.\\end{aligned}\\ ] ] by taking @xmath288 leads to @xmath289 for some absolute constant @xmath188 .",
    "finally , by jensen inequality , one has @xmath290 this completes the proof of the theorem .      rewriting equation  ( [ prob.fomala ] ) , by plugging  ( [ pauli expansion ] ) in , as follow @xmath291 where @xmath292 and @xmath293 , see  @xcite for technical details .",
    "we are now ready to handle with the proofs .      first inequality @xmath296    \\\\ & = \\mathbb{e}\\exp\\left[d \\lambda \\sum_{b } ( \\rho^0_b-\\nu_b )   \\sum_s \\sum_a    \\frac{\\mathbf{p}_{(s , a ) ,   b}}{3^{d(b)}2^n }   ( p^0_{a , s } -   \\hat{p}_{a , s } ) \\right ]   \\\\ & = \\prod_a \\mathbb{e}\\exp\\left[\\lambda \\sum_{b } ( \\rho^0_b-\\nu_b )   \\sum_s \\frac{1}{m }   \\sum_{i=1}^m     \\frac{\\mathbf{p}_{(s ,",
    "a ) ,    b}}{3^{d(b ) } }     ( p^0_{a , s } - \\mathbf{1}_{r_i^a = s } ) \\right ]   \\\\ & = \\prod_a \\prod_i   \\mathbb{e}\\exp\\bigg[\\frac{\\lambda}{m }     \\underbrace{\\sum_{b } ( \\rho^0_b-\\nu_b ) \\sum_s \\frac{\\mathbf{p}_{(s , a ) ,    b}}{3^{d(b ) } } ( p^0_{a , s}- \\mathbf{1}_{r_i^a = s } ) } _ { : = y_{i , a}}\\bigg ] .   \\end{aligned}\\ ] ] remark that @xmath228 . also , from the definitions above , the absolute value @xmath297 does not depend on @xmath298 so @xmath299 so we can apply hoeffding s inquality ( lemma  [ bernstein ine ] ) : @xmath300.\\end{aligned}\\ ] ] second inequality : same proof , just replace @xmath301 by @xmath302 .",
    "[ lem exp_bound 2 ] we have @xmath303    \\| \\nu - \\rho^0\\|_f^2 -\\lambda \\left(\\|\\nu - \\hat{\\rho}\\|_f^2 - \\|\\rho^0-\\hat{\\rho }   \\|_f^2\\right )   \\big\\ }    \\leq 1 , \\\\    \\mathbb{e}\\exp \\big\\ { \\lambda   \\left(\\| \\nu - \\hat{\\rho}\\|_f^2   - \\| \\rho^0- \\hat{\\rho}\\|_f^2\\right )    - \\lambda\\left [   1 + \\frac{2\\lambda}{m }    \\left(\\frac{5}{3}\\right)^n \\right ]    \\|\\nu-\\rho^0\\|_f^2     \\big\\ }   \\leq 1 .",
    "\\end{aligned}\\ ] ]      [ pacbound_2 ] for @xmath246 s.t @xmath305 , with probability at least @xmath306 , we have @xmath307 \\int \\|\\nu-\\rho^0\\|_f^2\\hat{\\pi}({\\rm d}\\nu )    + \\frac{2\\mathcal{k}(\\hat{\\pi},\\pi )   +     2\\log(2/\\epsilon ) } { \\lambda } } { 1-\\frac{2\\lambda } { m } \\left(\\frac{5}{3}\\right)^n}.      \\label{pac bound 2 }   \\end{aligned}\\ ] ]        substituting  ( [ kl1]),([estimate norm 1 ] ) into  ( [ pac bound 2 ] ) @xmath308    ( 3d\\delta + 2 rc)^2}{1-\\frac{2\\lambda}{m } \\left(\\frac{5}{3}\\right)^n } \\\\   & +   \\frac{a rd\\log(\\frac{1}{c } ) + c_{d_1,d_2}d(\\log(d)+ \\log(\\frac{1}{\\delta } ) ) + 2\\log(2/\\epsilon   ) } { \\lambda [ 1-\\frac{2\\lambda}{m } \\left(\\frac{5}{3}\\right)^n ] } \\bigg\\}.\\end{aligned}\\ ] ] taking @xmath309 lead to @xmath310 for some constant @xmath311 . simultaneously , by jensen inequality",
    ", one has @xmath290 this complete the proof of the theorem .",
    "t.  baier , d.  petz , k.  m. hangos , and a.  magyar .",
    "comparison of some methods of quantum state estimation . in _ quantum probability and infinite dimensional analysis _ ,",
    "volume  20 of _ qp  pq : quantum probab .",
    "white noise anal .",
    "_ , pages 6478 .",
    "world sci .",
    "hackensack , nj , 2007 .",
    "l.  bgin , p.  germain , f.  laviolette , and j .- f .",
    "pac - bayesian bounds based on the rnyi divergence . in _ proceedings of the 19th international conference on artificial intelligence and statistics _ , pages 435444 , 2016 ."
  ],
  "abstract_text": [
    "<S> quantum state tomography , an important task in quantum information processing , aims at reconstructing a state from prepared measurement data . </S>",
    "<S> bayesian methods are recognized to be one of the good and reliable choices in estimating quantum states  @xcite . </S>",
    "<S> several numerical works showed that bayesian estimations are comparable to , and even better than other methods in the problem of @xmath0-qubit state recovery . </S>",
    "<S> however , the problem of choosing prior distribution in the general case of @xmath1 qubits is not straightforward . </S>",
    "<S> more importantly , the statistical performance of bayesian type estimators have not been studied from a theoretical perspective yet . in this paper </S>",
    "<S> , we propose a novel prior for quantum states ( density matrices ) , and we define pseudo - bayesian estimators of the density matrix . </S>",
    "<S> then , using pac - bayesian theorems  @xcite , we derive rates of convergence for the posterior mean . </S>",
    "<S> the numerical performance of these estimators are tested on simulated and real datasets . </S>"
  ]
}