{
  "article_text": [
    "human annotation is expensive , yet it is needed in many tasks in order to create training data . given the high cost , it is critical to improve the efficiency of such annotation .",
    "active learning involves selecting samples `` intelligently '' rather than randomly , for human annotation . by doing so , it is possible for systems to attain better performance with the same amount of annotation or achieve the same level of performance with a lot less annotated data .    in this paper",
    ", we present several new active learning strategies for the task of mention detection ( md ) . here",
    ", we employ the terminology used in the automatic content extraction conferences  @xcite .",
    "mentions are references to real - world entities that can be _ named _",
    "( e.g. `` john '' ) , _ nominal _ ( e.g. `` survivor '' ) or _ pronominal _ ( e.g. `` he '' ) .",
    "we propose and investigate a variety of sentence selection criteria for active learning , including various sentence scoring metrics that combine uncertainty - based and query - by - committee like measurements .",
    "experimental results show that these sentence selection strategies are quite effective for mention detection : compared to the random selection strategy , the best strategy reduces the amount of required annotated training data by over 50% while achieving the same performance .",
    "the effect is even more significant when only named mentions are considered : the system achieves the same performance by using only 42% of the training data required by the random strategy .    in the next section ,",
    "we discuss related work on active learning .",
    "section  [ setup ] describes our framework in detail and presents our experimental setup .",
    "section  [ results ] presents the results of the different experiments .",
    "finally , we discuss observations from our experiments and present some ideas for future work .",
    "active learning has been utilized for many nlp applications , most noticeably for text classification  .",
    "it has also been applied for part - of - speech tagging  , statistical parsing  , noun phrase chunking  , japanese word segmentation  @xcite , and confusion set disambiguation  .    there are few reported instances of applying active learning techniques to mention detection",
    ". investigated several document , rather than sentence , selection strategies for information extraction .",
    "they observed that some strategies lead to improvement in recall while others improve precision , but it is difficult to get significant improvement in both recall and precision for an active learner to perform better than random selection .",
    "proposed a multi - criteria - based active learning approach for named entity recognition .",
    "the multiple criteria include informativeness , representativeness , and diversity .",
    "they proposed measures to quantify these properties and investigated different selection strategies .",
    "they showed that the labeling cost can be reduced by at least 80% without degrading the performance for their data sets .",
    "investigated a query - by - committee - based active learner for information extraction in the astronomy domain .",
    "it studied the effects of selective sampling on human annotators .",
    "although active learning improved annotation efficiency overall , they observed lower inter - annotator agreement and higher per - token annotation times for the data selected by active learning .",
    "active learning techniques are usually divided into two types : uncertainty sampling for a single learner  , or disagreement measurement between a committee of learners  . in each case ,",
    "seed data needs to be provided to build an initial model or models . in the uncertainly - based approach ,",
    "a single learner labels unlabeled examples and provides a confidence score for each predicted label .",
    "samples that have the lowest confidence scores are chosen for manual labeling . in the query - by - committee approach ,",
    "a committee of learners is built and each learner labels the unlabeled samples .",
    "samples that have the highest disagreement among committee members are chosen for manual labeling .    in our work ,",
    "we experiment with query - by - committee - based approaches as well as hybrid approaches in which we employ a weighted combination of multiple learners .",
    "we also explore the effect of using different sets of committee learners .",
    "we carry out a number of experiments to compare the selection strategies that we propose . in this section",
    ", we first describe the corpus used in the experiments and then present the statistical classifiers used in the committees , along with the scoring metrics that we use .",
    "the malach collection  @xcite contains 116,000 hours of digitized interviews and testimonies in 32 languages from 52,000 survivors , liberators , rescuers and witnesses of the holocaust .",
    "we are interested in automatic information extraction from this corpus of spontaneous conversational speech . for a small number of english testimonies",
    ", we manually transcribed them and manually annotated the transcripts with three types of information :    * * mentions*. named , nominal and pronominal mentions of 20 categories of entities .",
    "* * co - reference*. sets of mentions that refer to the same real - world entity . * * relations*. sets of relationships between pairs of mentions .",
    "for instance , given the sentence `` i was in auschwitz for a year '' , the following relation exists : ` locatedat(i , auschwitz ) ` .    for the experiments described in this paper , we chose to focus on mention annotation only .",
    "we plan to explore co - reference and relation annotation in the future . for mention annotation ,",
    "we excluded pronouns ; their inclusion leads to inflated numbers since detection of pronominal mentions is easy and they occur with high frequency .",
    "we split the annotated data into two parts : the pool from which the learners select the next batch of data for annotation , and the development set .",
    "the first pool consists of 99 documents , including 4772 sentences and 198k words .",
    "a total of 43k mentions have been annotated for the documents in the pool ( 16k mentions if pronouns are excluded ) .",
    "the development evaluation set consists of 5 complete testimonies , which cover over 10 hours of speech .",
    "it consists of 1700 sentences , 73k words and 16k total mentions ( 6k mentions if pronouns are excluded ) .      when using active learning techniques to select samples for human annotation , we need to first decide on the granularity of sample selection .",
    "the samples can be documents , sentences , or tokens .",
    "a possible problem with selecting samples at the document level is that a document may only be partially useful from a learning point of view and and it is impossible to add only the interesting examples so as to maximize the effect of active learning .",
    "this problem is particularly acute in our corpus since the documents , which are testimonies of survivors , are very long and contain a lot of redundant information . selecting tokens as samples , as in , has the advantage that all samples are of equal length but suffers from the problem that a human annotator has to annotate mentions by looking at only the tokens .",
    "the selected tokens may also contain partial mentions when mention boundaries are incorrectly identified .",
    "sentences , on the other hand , contain enough non - redundant contextual information for effective annotation .",
    "therefore , we use sentence - level blocks for active learning , as in .      we propose a new active learning based framework for",
    "sample selection which provides considerable savings in the annotation task and can provide better performance for the same amount of annotation .",
    "figure  [ fig : framework ] shows the architecture of the framework . as mentioned before",
    ", the central idea is to use an ensemble of classifiers -each one differing from the others in some respect .",
    "once trained , the classifiers can then be used to detect mentions in the unlabeled sentences .",
    "these detected mentions can then be compared , for each sentence , and a score assigned to this sentence indicating the agreement , or lack thereof , among the ensemble classifiers .",
    "the sentences on which the classifiers disagree the most are , intuitively , the ones that can contribute the most to the learning .",
    "the sentences are then ranked according to this metric and the required number sampled from this ranked list .",
    "+ in the case of true active learning , these samples would be provided to the annotator to label and then added to the training data for the main classifier used for the actual task of mention detection .",
    "however , for this paper , we consider controlled experiments where the entire dataset has been annotated in advance .",
    "therefore , the path between the selection of the samples to the annotator has effectively been short - circuited and the samples are added directly to the training data for the main classifier .",
    "we structured our setup in such a way since we wanted to experiment with adding successively increasing amounts of training data without incurring the overhead of annotation at each step . the same exact advantages and savings will apply to a real setting where the selected samples need to be annotated at each step of learning . at each step ,",
    "the set of sampled sentences is split into equal parts and added to the seed data for the ensemble classifiers as well .",
    "+ our approach differs from previous approaches to the same problem in two significant ways .",
    "first , we measure the disagreement at the granularity of sentences rather than documents , which allows us to be much more discerning when selecting samples .",
    "the second point is that our framework allows targeting the sample selection towards specific types of mention that can be specified by the user .",
    "this is important because for some tasks , like ace , some types of mentions carry more weight than the others .          at each step in the active learning process , we build * two * maximum - entropy based statistical classifiers  @xcite using the labeled data available at that step .",
    "there are two dimensions of classifier training - the feature set of the classifier and the data used for training . for our experiments , we use two different combinations of these dimensions :    * each classifier is trained with the _ same _ features but on a _ different _ half of the available labeled data .",
    "we refer to this as the _ data - different _ ( dd ) setting . *",
    "each classifier is trained on the entire available labeled data but using a _ different _ feature set : the `` inside '' classifier uses lexical features derived solely from the current token , and the `` outside '' classifier uses features involving surrounding tokens only .",
    "we refer to this experimental setting as _ feature - different _ ( fd ) .",
    "we employ a variety of metrics for measuring the degree of disagreement between two classifiers over the output labels for a given sentence .",
    "the simplest metrics look at only the output labels predicted by the classifiers .",
    "however , we propose another set of metrics that utilizes the confidence values associated with those labels as well .    * * f - measure * as the harmonic mean of precision and recall , the f - measure is often used to assess the agreement between two classifiers for the named entity recognition task .",
    "the value of f - measure is between 0 and 1 , with higher values indicating greater agreement . during sentence selection",
    ", we compute the f - measure of the two classifiers on each unannotated sentence , and select sentences with the lowest f - measure values for annotation . *",
    "* macro - averaged f - measure * instead of computing the f - measure over the entire set of mentions , as in the previous metric , another option is to compute the f - measure for each mention category and then take the average over all categories .",
    "this metric allows categories with a small number of mentions to be weighted equally . *",
    "* confidence sum*. our statistical classifiers can provide a probability value for each output label , indicating its confidence in choosing the label . to leverage this information",
    ", we use the normalized sum of the confidence values of the two classifiers as another metric .",
    "the higher this value , the more confident both classifiers are about a particular sentence .",
    "therefore , that sentence will provide little or no information if added to the training set and should be ranked lower for selection .",
    "* * confidence difference*. we also use the absolute difference of the two confidence values . a sentence with a higher confidence difference value indicates an explicit disagreement between the two classifiers when detecting mentions and , therefore , would prove to be more useful for annotation .",
    "in addition to the above selection metrics , there are three additional parameters that were used in our experiments :    * * minimum number of mentions per sentence*. this parameter is used to deal with the sparse mention problem in sentence selection . for instance , for a given sentence",
    "@xmath0 , if the first classifier finds only one mention in it , and the second classifier finds zero mentions , then the f - measure for the sentence is 0 , which puts the given sentence at the top of the selection list .",
    "in contrast , for another sentence @xmath1 , if the first classifier finds 5 mentions , and the second classifier finds 3 mentions , two of which overlap with the output by the first classifier , then the f - measure is 50% .",
    "therefore , @xmath1 is erroneously ranked lower than @xmath0 on the selection list . to eliminate sentences with very few mentions ,",
    "a user can set the minimal number of mentions per sentence parameter to @xmath2 , where @xmath2 is a non - negative number .",
    "any sentence with less than @xmath2 mentions is not considered as a candidate for active learning . *",
    "* mention category weights*. each mention category is associated with a weight in the above metrics .",
    "this gives us the flexibility to focus on certain categories during active learning .",
    "for instance , if a system performs weakly in the organization category , we may want to customize the scoring metric so that the active learner can pick up samples that are particularly useful in improving the performance in this category .",
    "the categories with higher weights are considered more important ; the categories with zero weights are not considered for active learning . *",
    "* mention level weights*. the mention level indicates whether a mention is named , nominal , or pronominal .",
    "we can customize our learner to focus on a particular mention level by adjusting the weight associated with it .",
    "we carried out a number of experiments to evaluate the different sentence selection strategies presented in the previous section .",
    "all the strategies perform better than random selection , but the confidence sum metric with the feature - different classifier training setting gives the greatest improvement .    ideally , the active learners should be retrained each time a new sample has been selected by active learning and annotated by a human . in reality , this is rarely done due to time and computational cost . instead , active learners usually operate in `` batch '' mode  a set of samples , instead of a single sample , is selected at each step and the active learners are retrained by adding all the samples in the set . given our granularity of selection , one way to define the size of this batch",
    "can be in term of number of sentences to be added at each step .",
    "however , this approach does not provide a way to differentiate between a sentence that is @xmath3 words long and another that is only @xmath4 words long ; both are equally important",
    ". therefore , we define the batch in terms of the number of words contained in the sentences . at each step , we add just enough complete sentences that , among themselves , contain the given number of words or as close to that number as possible . in the experiments described in this section ,",
    "this size is @xmath5 words .",
    "first , a set of @xmath5 words is randomly selected to build seed models . then at each step of active learning , an additional @xmath5 words are selected .",
    "we also describe the effect of this _ step size _ at the end of this section .",
    "all the results presented in this section are on our development evaluation set  after each step of active learning , we train a model using all the data that have been selected as of that step ( including the seed data ) , and the model s performance on the development set is reported .      in the experiments described here , we consider only named and nominal mentions .",
    "the weights for all mention categories are set to 1 .                            *",
    "random selection*. we established a baseline performance by selecting sentences at random from the unlabeled data pool .",
    "we performed @xmath6 runs and averaged the numbers from each run .",
    "the performance for the random selection strategy is shown in figure  [ baseline ] , including each run and their average .",
    "the average performance is used as the baseline and shown in later graphs .",
    "the x - axis represents the data size in number of words , and the y - axis presents the f - measure on the development set .    *",
    "f - measure and macro - averaged f - measure*. figure  [ fmeas ] shows the curves for the f - measure and macro - averaged f - measure metrics compared to the baseline . as we can see , the f - measure metric works very well .",
    "even at 40k words ( after only one step of active learning ) , we observe an increase of about 2 absolute f - measure points with the same amount of labeled data , compared to random sentence selection . to attain the same performance",
    ", the random strategy needs @xmath7 times as much labeled data .",
    "the selection strategy based on macro - averaged f - measure also performs better than random selection , but the improvement is smaller than that for the f - measure metric . as indicated , these metrics were computed only for the data - different classifier setting .    *",
    "confidence sum*. figure  [ confsum ] shows the curves for the confidence sum metric .",
    "this metric is computed for both the data - different and the feature - different setting . as shown in the figure , the classifiers trained on different features ( fd ) lead to slightly higher improvement in the first two steps than the classifiers with the same feature set ( dd ) .",
    "overall , the confidence sum performs much better than random selection : at 60k words ( after only two steps of active learning ) , the confidence sum strategy attains better performance than the baseline does with twice this amount of labeled data .    *",
    "confidence difference*. figure  [ confdiff ] shows the results for the confidence difference metric .",
    "similar to the confidence sum metric , using the classifiers trained on different features ( fd ) leads to better improvements in the first two steps . at 60k words",
    ", the system achieves the same performance as the random strategy at 100k words .",
    "* the best 3 strategies*. figure  [ best3 ] compares the curves for the @xmath8 best strategies - fd confidence sum , fd confidence difference , and dd f - measure .",
    "the fd confidence sum strategy outperforms the other two strategies in the first three active learning steps . at the fourth step ,",
    "the fd confidence difference strategy catches up . at 60k words ,",
    "the fd confidence sum strategy achieves the performance of the random strategy at about 130k words .",
    "this indicates a reduction of over 50% in the amount of annotated data and demonstrates the effectiveness of active learning for making annotation more efficient .",
    "as we have described in the previous section , our system allows us to weight the different types and levels of mentions differently in order to focus our learning on specific mention types and levels .",
    "we provide the results of two such experiments : one focusing on the person category and one focusing on named mentions .",
    "first , we target the learning to mentions of type person . to achieve this",
    ", we weight the person category twice as much as any of the other mention categories .",
    "the results are shown in figure  [ person ] .",
    "note that the f - measure along the y - axis in the graph is the performance for the person category .",
    "the second experiment was for named mentions . for this experiment",
    ", we set the weights to all other levels to @xmath9 .",
    "the result is an active learner specifically tuned to maximizing the performance of the system on named mentions .",
    "figure  [ named ] shows the results for this experiment .",
    "this plot clearly illustrates the advantages of active learning .",
    "we see a @xmath6-point f - measure increase with only one step of learning , which could otherwise have only been achieved by annotating almost @xmath10 times as much data .",
    "the size of the step used in the learning needs to be optimized . on the one hand , getting higher performance gains from smaller amounts of labeled data to be added at each step represents a huge savings in the annotation task ; on the other hand , retraining active learners at each step takes time and resources .",
    "we need to balance the cost of retraining active learners and the gains from smaller batches .",
    "we experimented with different step sizes for the feature - different ( fd ) confidence - sum metric .",
    "the results are shown in figure  [ stepsize ] . from that graph",
    ", it seems that a step size of @xmath11 words might have provided the best balance between annotation and performance .",
    "step sizes higher than @xmath5 do not seem to have the right granularity for effective learning .",
    "we conducted several active learning experiments for the task of detecting mentions of entities in human transcripts of spontaneous conversational speech .",
    "specifically , we proposed and compared a variety of sentence selection strategies for active learning .",
    "the best strategy uses the sum of the confidence values of a pair of classifiers trained with different feature sets .",
    "compared to random sentence selection , this strategy required 50% of the data to achieve the same performance . for named mentions ,",
    "the strategy required only 42% of the data needed by random selection to achieve the same performance .    in the future , we would like to test our active learning strategies on data from a different domain and data in languages other than english , such as the ace mention annotation corpus . we would also like to explore active learning for co - reference and relation annotation .",
    "this project was part of a research effort funded by nsf .",
    "any opinions , findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the nsf .",
    "mccallum , a.k .",
    ", nigam , k. : employing em in pool - based active learning for text classification . in : proceedings of icml 1998 , 15th international conference on machine learning , madison , us ( 1998 ) 350358          thompson , c.a . ,",
    "califf , m.e . ,",
    "mooney , r.j . :",
    "active learning for natural language parsing and information extraction . in : proceedings of icml 1999 ,",
    "16th international conference on machine learning .",
    "( 1999 ) 406414    tang , m. , luo , x. , roukos , s. : active learning for statistical natural language parsing . in : proceedings of the 40th annual meeting of the association for computational lingusitics , philadelphia , pa , usa ( 2002 ) 120127    steedman , m. , hwa , r. , clark , s. , osborne , m. , sarkar , a. , hockenmacier , j. , ruhlen , p. , baker , s. , crim , j. : example selection for bootstrapping statistical parsers .",
    "in : proceedings of hlt - naacl 2003 , edmonton , canada ( 2003 ) 236243      ngai , g. , yarowsky , d. : rule writing or annotation : cost - efficient resource usage for base noun phrase chunking . in",
    ": proceedings of 38th annual meeting of the association for computational linguistics , hong kong ( 2000 )    sassano , m. : an empirical study of active learning with support vector machines for japanese word segmentation . in : proceedings of the 40th annual meeting of the association for computational linguistics , philadelphia , pa , usa ( 2002 ) 505512    banko , m. , brill , e. : scaling to very very large corpora for natural language disambiguation . in : proceedings of 39th annual meeting of the association for computational linguistics , toulouse , france ( 2001 ) 2633      shen , d. , zhang , j. , su , j. , zhou , g. , tan , c.l .",
    ": multi - criteria - based active learning for named entity recognition . in : proceedings of the 42nd meeting of the association for computational linguistics ( acl04 ) , main volume , barcelona , spain ( 2004 ) 589596    hachey , b. , alex , b. , becker , m. : investigating the effects of selective sampling on the annotation task . in : proceedings of the 9th conference on compuational natural language learning(conll ) , ann arbor ( 2005 ) 144151        gustman , s. , oard , d.s.d . , byrne , w. , picheny , m. , ramabhadran , b. , greenberg , d. : supporting access to large digital oral history archives . in : proceedings of the joint conference on digital libraries .",
    "( 2002 ) 1827    oard , d. , soergel , d. , doermann , d. , huang , x. , murray , g. , wang , j. , ramabhadran , b. , franz , m. , gustman , s. , mayfield , j. , kharevych , l. , strassel , s. : building an information retrieval test collection for spontaneous conversational speech . in : proceedings of sigir04 ,",
    "sheffield , u.k .",
    "( 2004 )    florian , r. , hassan , h. , ittycheriah , a. , jing , h. , kambhatla , n. , luo , x. , nicolov , n. , roukos , s. : a statistical model for multilingual entity detection and tracking . in : proceedings of hlt - naacl 2004 ,",
    "boston , massachusetts , usa ( 2004 ) 18"
  ],
  "abstract_text": [
    "<S> we propose and compare various sentence selection strategies for active learning for the task of detecting mentions of entities . </S>",
    "<S> the best strategy employs the sum of confidences of two statistical classifiers trained on different views of the data . </S>",
    "<S> our experimental results show that , compared to the random selection strategy , this strategy reduces the amount of required labeled training data by over 50% while achieving the same performance . </S>",
    "<S> the effect is even more significant when only named mentions are considered : the system achieves the same performance by using only 42% of the training data required by the random selection strategy . </S>"
  ]
}