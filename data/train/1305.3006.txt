{
  "article_text": [
    "images contaminated by the additive gaussian noise are widely investigated in the field of image restoration . however , for coherent imaging systems such as laser or ultrasound imaging , synthetic aperture radar ( sar ) and optical coherence tomography , image acquisition processes are different from the usual optical imaging , and thus the multiplicative noise ( speckle ) , rather than the additive noise , provides an appropriate description of these imaging systems . in this paper",
    ", we mainly consider the problem of the multiplicative noise removal .",
    "let @xmath0 be the observed image corrupted by the multiplicative noise .",
    "our goal is to recover the original image @xmath1 from the observed image .",
    "the corresponding relationship can be expressed as follows : @xmath2 where @xmath3 is the multiplicative noise that follows the gamma distribution , i.e. , @xmath4    in the last several years , various variational models and iterative algorithms @xcite based on the tv regularization have been proposed for the multiplicative noise removal .",
    "aubert and aujol @xcite used a maximum a posteriori ( map ) estimator and established the following tv regularized variational model : @xmath5 where @xmath6 is a fixed regularization parameter , and @xmath7 is the ( isotropic ) total variation of @xmath8 , i.e. , @xmath9 note that @xmath10 and @xmath11 denote the horizontal and vertical first order differences at pixel @xmath12 respectively , @xmath13 denotes a matrix of ones with the same size as @xmath8 , the multiplication and division are performed in componentwise , and neumann boundary conditions are used for the computation of the gradient operator @xmath14 and its adjoint ( just the negative divergence operator ) @xmath15 ; see @xcite for more details .    the objective function in the aa model ( [ equ1.3 ] )",
    "is nonconvex , and hence it is difficult to find a global optimal solution .",
    "recently , in many existing literatures @xcite , the log transformation was used to resolve the non - convexity . the variational model based on the log transformed image",
    "can be reformulated as follows : @xmath16 the above model overcomes the drawback of the aa model , and numerical experiments verify its efficiency @xcite . in @xcite , the exponential of the solution of the exponential model ( [ equ1.4 ] ) is proved to be equal to the solution of the classical i - divergence model @xmath17 very recently , new convex methods such as shifting technique @xcite and m - th root transformation @xcite were also investigated for this problem .",
    "the augmented lagrangian framework has recently been proposed to solve the exponential model ( [ equ1.4 ] ) and the i - divergence model ( [ equ1.5 ] ) . although this iterative technique is useful , inner iteration @xcite or inverses involving the laplacian operator @xcite are required at each iteration .",
    "the computational cost of the inner iteration or the inverse operation is still expensive .",
    "very recently , linearized techniques @xcite were widely used for accelerating the alternating minimization algorithm for solving the variational model in image processing . in @xcite ,",
    "a fast proximal linearized alternating direction ( plad ) algorithm , which linearized both the fidelity term and the quadratic term of the augmented lagrangian function , were proposed to solve the tv models ( [ equ1.4 ] ) and ( [ equ1.5 ] ) .",
    "numerical results there demonstrate that the plad algorithm overall outperforms the augmented lagrangian method for multiplicative noise removal .",
    "the regularization parameter @xmath18 in ( [ equ1.4 ] ) controls the trade - off between the goodness - of - fit of @xmath19 and a smoothness requirement due to the tv regularization .",
    "the regularized solution highly depends on the selection of @xmath18 .",
    "specifically , large @xmath18 leads to little smoothing , and thus , noise will remain almost unchanged in the denoised image or the regularized solution , whereas small @xmath18 leads to oversmoothing solution , so that fine details in the image are destroyed . therefore , choosing a suitable @xmath18 is a key issue for the variational model ( [ equ1.4 ] ) . in the existing literatures ,",
    "the main approaches for automatic parameter setting include the generalized cross validation @xcite , the l - curve @xcite , the stein unbiased risk estimator @xcite and the discrepancy principle @xcite .",
    "these methods are all based on the assumption of gaussian noise and not suitable for the special denoised problem here . in @xcite , a new discrepancy principle based on the statistical characteristics of the multiplicative noise",
    "was proposed for selecting a proper value of @xmath18 .",
    "however , it needs to solve ( [ equ1.4 ] ) or the corresponding i - divergence model several times for a sequence of @xmath18 s .",
    "hence the computational cost is expensive .    in this paper",
    ", we use the special discrepancy principle for the multiplicative noise to compute @xmath18 . a linearized alternating direction minimization algorithm with auto - adjusting of the regularization parameter",
    "is proposed to solve ( [ equ1.4 ] ) . in each iteration of the proposed algorithm ,",
    "the regularization parameter is updated in order to guarantee that the denoised image satisfies the discrepancy principle .",
    "due to the use of the linearization technique , the solution of the subproblem in the iteration step has a closed - form expression , and therefore the zero point of the discrepancy function with respect to @xmath18 can be computed by the newton method efficiently .",
    "numerical experiments show that our algorithm is very effective in finding a proper @xmath18 .",
    "moreover , the proposed algorithm can be extended to the case of the spatially adapted regularization parameter without extra computational burden almost",
    ".    we will give the convergence proof of the proposed algorithms with adaptive parameter estimation .",
    "with a fixed regularization parameter , our algorithms are reduced to the plad algorithm proposed in @xcite .",
    "as we known , the convergence of the original plad method is unsolved at present .",
    "therefore , one contribution of our work is to supply a complete convergence proof for the plad method .",
    "the rest of this paper is organized as follows . in section [ sec2 ]",
    "we briefly review the plad algorithms proposed in @xcite . in section [ sec3 ] , we utilize the statistical characteristic of the gamma noise to define a special discrepancy function , and then propose a new linearized alternating direction minimization algorithm , which automatically estimate the value of the regularization parameter by computing the zero point of the corresponding discrepancy function .",
    "finally the convergence properties of the proposed iterative algorithm are proved . in section [ sec4 ]",
    "we extend the proposed algorithm to the case of the spatially - adaptive regularization parameter . in section [ sec5 ] the numerical results are reported to compare the proposed algorithms with some of the recent state - of - the - art methods .",
    "in this section , we briefly review the plad algorithm proposed in @xcite , which will serve as the foundation for the algorithm presented in the next section . consider the following tv regularized minimization problem : @xmath20 where @xmath21 is a real - valued , convex and smooth function .",
    "the corresponding constrained optimization problem can be reformulated as : @xmath22    the augmented lagrangian function for ( [ equ2.2 ] ) is defined as @xmath23    then the well - known admm ( alternating direction method of multipliers ) for solving ( [ equ2.1 ] ) can be expressed as follows : @xmath24 the first subproblem in ( [ equ2.4 ] ) is difficult to solve .",
    "therefore , the authors in @xcite used the second - order taylor expansion of @xmath25 at @xmath26 , and meanwhile replaced the hessian matrix @xmath27 with @xmath28 ( @xmath29 is a constant ) .",
    "then the first subproblem in ( [ equ2.4 ] ) can be simplified as follows : @xmath30 replacing the first subproblem in ( [ equ2.4 ] ) with ( [ equ2.5 ] ) we obtain the plad algorithm for the minimization problem ( [ equ2.1 ] ) .",
    "now consider the variational models ( [ equ1.4 ] ) and ( [ equ1.5 ] ) .",
    "they can be reformulated as @xmath31 where @xmath32 , and @xmath33 or @xmath34 . while the plad algorithm is applied to solve the minimization problem in ( [ equ2.6 ] ) ,",
    "it can be expressed as the following simple form : @xmath35 where @xmath36 for the exponential model and @xmath37 for the i - divergence model .",
    "the shrinkage operator is defined as : @xmath38 if we further assume that @xmath39^{m\\times n}$ ] , and @xmath40 , then the first formula in ( [ equ2.7 ] ) should be replaced by @xmath41 where @xmath42 denotes the projection onto the set @xmath43 .",
    "in this section , we describe the proposed linearized alternating minimization algorithm with adaptive parameter selection . to this end",
    ", we first introduce the statistical characteristic of some random variable with respect to gamma noise @xmath3 .",
    "it will play an important role in developing our algorithm .",
    "[ lem1 ] let @xmath3 be a gamma random variable ( r.v ) with mean 1 and standard deviation @xmath44 .",
    "consider the following discrepancy function with respect to @xmath3 @xmath45 then the following estimate of the expected value of @xmath46 holds true for large @xmath47 : @xmath48    the above conclusion has appeared in the existing literature @xcite , and therefore we omit the proof here .",
    "next , we consider the plad algorithm for solving the exponential model ( [ equ1.4 ] ) .",
    "choose @xmath49 .",
    "the solution @xmath50 in ( [ equ2.5 ] ) has a closed expression as follows : @xmath51 which can be seen as a linear function with respect to @xmath18 . since @xmath50 is an approximation of the log transformed original image",
    ", we have @xmath52 denote @xmath53 @xmath54 then @xmath55 . let @xmath56 . then according to lemma [ lem1 ] and the relation ( [ equ3.4 ] ) , for large @xmath47 we have @xmath57    on the other hand , considering the function @xmath58",
    ", it is monotonically increasing for @xmath59 and decreasing for @xmath60 .",
    "besides , @xmath61 . the denoised image @xmath50 is a smoothed version of the noisy image @xmath62 , and its value will be far from @xmath62 under the over - smoothness condition ( with small @xmath18 ) .",
    "therefore , we demand that @xmath50 obtained by ( [ equ3.3 ] ) satisfies the following inequality in order to avoid over - smoothness : @xmath63 where @xmath64 and @xmath65 is a constant for each iteration .",
    "according to ( [ equ3.5 ] ) we infer that @xmath66 is proper for large @xmath47 .",
    "in fact , we use this setting for most experiments in section [ sec5 ] .",
    "the function @xmath67 in ( [ equ3.6 ] ) is called as the discrepancy function defined for gamma noise .",
    "the relation in ( [ equ3.6 ] ) inspires us to update the value of @xmath18 in the iteration step @xmath68 as follows : if @xmath69 , then @xmath70 .",
    "otherwise update @xmath71 just as the zero point of the function @xmath72 , i.e. , @xmath73 .",
    "the following lemma guarantees the existence of the solution of @xmath74 .",
    "[ lem2 ] assume that @xmath75 , @xmath76 and @xmath77 , then the equation @xmath74 has at least one solution .    obviously , @xmath72 is continuous with respect to @xmath18 . if @xmath78 , then @xmath79 .",
    "hence there exists @xmath80 such that @xmath81 for any @xmath82 . if @xmath83 , then there exists @xmath84 for some @xmath85 , and thus @xmath86 .",
    "this shows that there exists @xmath87 such that @xmath81 for any @xmath88 .",
    "moreover , due to @xmath77 , there exists @xmath89 satisfies @xmath90 .",
    "therefore , the equation @xmath74 has at least one solution .",
    "based on the above analysis , we obtain the linearized alternating direction minimization algorithm with adaptive parameter selection , which is summarized in algorithm 1 .",
    "noisy image @xmath19 ; choose @xmath91 , @xmath92 . + * output * : denoised log transformed image @xmath8 ; regularization parameter @xmath18 . + * initialization * : @xmath93 ; @xmath94 .",
    "+ * iteration * : +  if @xmath69 ; then +  @xmath70 ; +  else +  compute the solution of @xmath74 by the newton iteration ; +  end if +  @xmath95 ; +  @xmath96 ; +  @xmath97 ; +  @xmath98 ; + until some stopping criterion is satisfied . + return @xmath99 and @xmath100",
    ".      here we investigate the convergence properties of the sequence @xmath101 generated by algorithm 1 .",
    "assume that @xmath102 , and @xmath103 is one solution of the variational model @xmath104 choose @xmath105 in algorithm 1 .",
    "denote @xmath106 , and @xmath107 as a lipschitz constant that satisfies @xmath108 for any @xmath109 ( @xmath110 denotes @xmath111 unless otherwise specified in the following ) .",
    "then we have the following convergence result .    [ the1 ]",
    "let @xmath112 be the sequence generated by algorithm 1 with @xmath113 .",
    "then it converges to a point @xmath114 where the first - order optimality conditions for ( [ equ3.7 ] ) are satisfied .",
    "the first - order optimality conditions for the sequence @xmath101 generated by algorithm 1 are @xmath115 where @xmath116 .",
    "since @xmath103 is one solution of ( [ equ3.7 ] ) , the first - order optimality conditions can be rewritten as @xmath117 for some @xmath118 .",
    "rearrange ( [ equ3.9 ] ) to get @xmath119    denote the errors by @xmath120 , @xmath121 , @xmath122 and @xmath123 . subtracting ( [ equ3.10 ] ) from ( [ equ3.8 ] )",
    "we obtain @xmath124 taking the inner product with @xmath125 , @xmath126 and @xmath127 on both sides of the three equations of ( [ equ3.11 ] ) respectively , we obtain @xmath128 due to @xmath129 , according to ( [ equ3.12 ] ) we get @xmath130 the last equation in ( [ equ3.13 ] ) uses the relation @xmath131 .",
    "adding the equations in ( [ equ3.13 ] ) we obtain @xmath132 besides , @xmath133 where @xmath134 and @xmath135(@xmath136 is a positive - definite matrix due to @xmath76 ) .    substituting it in ( [ equ3.14 ] )",
    "we obtain @xmath137 since @xmath113 , we have @xmath138 moreover , by the convexity of @xmath139 we have @xmath140 . by the update criterion of @xmath71 in algorithm 1",
    "we infer that @xmath141 , and therefore @xmath142 which implies that @xmath143 by the definition of @xmath144 . due to @xmath145",
    ", we infer that @xmath146 . by the convexity of @xmath147 we also have @xmath148 .",
    "after removing the nonnegative terms except the first four terms in the left side of the equation ( [ equ3.15 ] ) , we obtain @xmath149 since @xmath150 , we know that @xmath151 for any @xmath68 . according to ( [ equ3.16 ] ) we conclude that @xmath152 is bounded and hence @xmath112 is also bounded .    by summing ( [ equ3.15 ] ) from some @xmath153 to @xmath154",
    ", we can derive    @xmath155    and hence we obtain @xmath156 this implies that @xmath157 therefore , @xmath158 according to ( [ equ3.19 ] ) and ( [ equ3.20 ] ) we also have @xmath159 moreover , by the relation of @xmath131 and ( [ equ3.20 ] ) we have @xmath160    in ( [ equ3.16 ] ) we have shown that @xmath112 is bounded . then there exists a convergent subsequence , also denoted by @xmath112 for convenience , converging to a limit @xmath161 .",
    "due to @xmath162 , it follows that @xmath163 $ ] and hence there exists a subsequence of @xmath164 converging to a limit @xmath165 .",
    "besides , as @xmath166 , there exists a subsequence of @xmath167 converging to @xmath168 .    for convenience ,",
    "let @xmath169 be a subsequence converging to @xmath170 .",
    "since @xmath147 is a closed proper convex function , according to theorem 24.4 in @xcite we have @xmath171 .",
    "let @xmath172 in ( [ equ3.8 ] ) while utilizing the convergence results in ( [ equ3.19])-([equ3.22 ] ) to obtain    @xmath173    which is equivalent to @xmath174 hence the limit point @xmath161 satisfies the first - order optimality conditions for ( [ equ3.7 ] ) .",
    "the proof of this theorem started with any saddle point @xmath114 . here",
    "we consider the special case of @xmath175 which is the limit of a convergent subsequence @xmath176 . according to ( [ equ3.16 ] ) we know",
    "that , for any @xmath177 , @xmath178 let @xmath179 tend to infinity to deduce that @xmath180 which demonstrates that @xmath112 converges to @xmath114 .",
    "this completes the proof .",
    "if we fix @xmath181 , then algorithm 1 reduces to the plad method proposed in @xcite .",
    "the proof above can be used for the special case with little modification .",
    "therefore , we can obtain the following convergence result .",
    "[ cor1 ] ( the convergence property of the plad method ) let @xmath112 be the sequence generated by the plad method with @xmath182 .",
    "then it converges to a point @xmath114 where the first - order optimality conditions for ( [ equ3.7 ] ) ( @xmath183 in this case ) are satisfied .    in this proof",
    "we consider the constraint @xmath39^{m\\times n}$ ] .",
    "we can obtain the first - order optimality conditions similar to those in ( [ equ3.8 ] ) and ( [ equ3.10 ] ) .",
    "the differences lie in @xmath184 and @xmath185 are added to the first formulas of ( [ equ3.8 ] ) and ( [ equ3.10 ] ) respectively , and @xmath186 .",
    "here @xmath187 denotes the normal cone @xcite .",
    "then similarly to ( [ equ3.14 ] ) we can obtain that @xmath188 where @xmath189 .",
    "similarly to ( [ equ3.15 ] ) we can also get @xmath190 due to @xmath191 , by the definition of normal cone we infer that @xmath192 and @xmath193 add the two inequalities we obtain that @xmath194 . then similarly to the proof below ( [ equ3.15 ] ) we can complete the proof here .    * remark * :",
    "if we consider the constraint @xmath195 in algorithm 1 , the iterative formula with respect to @xmath26 is rewritten as @xmath196 assume that @xmath50 still satisfies that the discrepancy function @xmath197 , then the convergence properties can be still guaranteed . in section [ sec5 ] ,",
    "we choose @xmath198^{m\\times n}$ ] for the exponential model , and find that the convergence of algorithm 1 still exists .    in algorithm 1 ,",
    "a fixed step @xmath199 is used .",
    "however , through further investigation we observe that a variable step @xmath200 can be used to accelerate our algorithm .",
    "this can be stated by the following conclusion .",
    "[ cor2 ] ( the convergence property of the proposed method with variable step ) let @xmath112 be the sequence generated by algorithm 1 with a variable step @xmath200 that satisfies @xmath201 ( @xmath202 is a small constant ) and @xmath200 is monotonically increasing for sufficiently large @xmath68 .",
    "then it converges to a point @xmath114 where the first - order optimality conditions for ( [ equ3.7 ] ) are satisfied .",
    "the proof is analogous to that presented in theorem [ the1 ] , and we only illustrate the difference due to limited space .",
    "substitute @xmath200 for @xmath199 in the formulas ( [ equ3.8])-([equ3.16 ] ) . since @xmath200 is monotonically increasing for sufficiently large @xmath68 , there exists @xmath203 satisfies @xmath204 for any @xmath205 .",
    "hence we obtain that @xmath206 due to @xmath201 , we have @xmath207 , and then according to ( [ equ3.24 ] ) we conclude that @xmath152 is bounded and hence @xmath112 is also bounded .",
    "choose @xmath208 .",
    "similarly to ( [ equ3.17])-([equ3.18 ] ) we obtain that @xmath209 then similarly to the proof below ( [ equ3.18 ] ) we can complete the proof here .",
    "the parameter @xmath18 in ( [ equ1.4 ] ) controls the smoothness of the denoised image generated by the tv regularizer . as we know , small @xmath18 is more suitable for the homogeneous regions of the image to remove the noise sufficiently , while large @xmath18 is more appropriate in the regions richly contain the textures and details .",
    "therefore , tv models with the spatially - adaptive regularization parameter were widely researched recently , see @xcite for instance . however , the parameter is difficult to compute and needs to solve the corresponding tv regularized variational model for many times .    in this section ,",
    "we use the newton iteration to update the regularization parameter and extend algorithm 1 to the case of the tv model with spatially - adaptive parameter . to this end , we first define the local discrepancy function as follows : @xmath210 where @xmath211 denotes the size of the local region centered at the pixel @xmath212 . the formula ( [ equ4.1 ] ) is just the local version of the discrepancy function @xmath72 defined in section [ sec3 ] , and the essential difference lies in a regularization parameter @xmath213 , rather than the scale parameter @xmath6 , is used here .    for the convenience of description , we column - wise stack @xmath18 into a vector , and denote @xmath214_{s } : \\mathbb{r}^{mn}\\rightarrow \\mathbb{r}^{mn}$ ] . here the index @xmath215 corresponds to the pixel @xmath212 in the image . similarly to the discussion in section [ sec3 ] , @xmath18 is computed by solve the system of equations @xmath216 , which can be solved by the newton iteration as follows : @xmath217 where @xmath218 .",
    "therefore , the computation of the inverse of a @xmath219 matrix is required in each iteration step of the newton method and the computational cost is rather expensive .    in this paper",
    ", we simplify the update of @xmath18 by the following strategy . due to",
    "the properties of local regions in the image are similar , we assume that the value of @xmath18 is constant in the local regions . specifically , for the index @xmath12 , assume that @xmath220 for any @xmath221 .",
    "then the local discrepancy function in ( [ equ4.1 ] ) is only a function with respect to @xmath222 , and hence @xmath222 can be updated by @xmath223 let @xmath224 denote the mean operator with size @xmath211 in convolution - form , and @xmath225 .",
    "then based on the patch - wise constant assumption of @xmath18 , the update formula of @xmath18 can be further approximated by @xmath226 where @xmath227 denotes the convolution operation , and @xmath228 denotes componentwise division . here we use the function @xmath229 , which means that @xmath70 while @xmath230 .    due to the parameter @xmath18 obtained by ( [ equ4.4 ] )",
    "is based on the patch - wise constant assumption , the estimator of @xmath222 should be computed by the average of the estimation values of @xmath18 around the index @xmath12 .",
    "therefore , the final @xmath18 is obtained by @xmath231    the whole process can be described by algorithm 2 as follows .",
    "noisy image @xmath19 ; choose @xmath91 , @xmath92 ; maximum inner iteration number @xmath232 ; + * output * : denoised log transformed image @xmath8 ; regularization parameter @xmath18 . + * initialization * : @xmath233 ; @xmath94 .",
    "+ * iteration * : +  @xmath234 +  while @xmath235 +  @xmath236 ; +  @xmath237 +  end while +  @xmath238 ; +  @xmath95 ; +  @xmath239 ; +  @xmath97 ; +  @xmath98 ; + until some stopping criterion is satisfied .",
    "+ return @xmath99 and @xmath100 .",
    "in this section , we evaluate the performance of the proposed algorithms by various experiments .",
    "first , the ability of auto - selection of the regularization parameter @xmath18 in the proposed dp - ladm algorithm is investigated .",
    "second , the proposed algorithms are compared with those of the current state - of - the - art methods : one is the plad algorithm proposed in @xcite , which was proved to be more efficient than the widely used augmented lagrangian method @xcite for multiplicative noise removal at present ; the other is the spatially adapted total variation model proposed in @xcite , which solved the tv model with spatially - adapted regularization parameter by the augmented lagrangian method for several times .",
    "the codes of the proposed algorithms and the methods used for comparison are entirely written in matlab , and all the experiments are performed under windows xp and matlab r2008a running on a laptop with an intel pentium dual cpu ( 2.0 g hz ) and 1 gb memory .",
    "we use six test images , which include three remote sensing images , in figure 5.1 for the experiments below .",
    "the size of figure [ fig5.1:subfig : a]-[fig5.1:subfig : e ] is @xmath240 , and the size of figure [ fig5.1:subfig : f ] is @xmath241 .",
    "+      we now evaluate the performance of the dp - ladm algorithm , including the influence of the initial regularization parameter @xmath242 to the final denoised images by algorithm 1 , and verification of the convergence properties proved in section [ sec3 ] .",
    "three images called  cameraman \" ,  barbara \" and  remote2 \" ( see figure [ fig5.1 ] ) are used for our test .",
    "the corresponding noisy images are generated by multiplying the original image by a realization of gamma noise based on the formulas in ( [ equ1.1])-([equ1.2 ] ) .",
    "the performance of denoised images is measured quantitatively by means of the peak signal - to - noise ratio ( psnr ) , which is defined by @xmath243 where @xmath8 and @xmath244 denote the original image and the denoised image respectively . during the implementation of algorithm 1 ,",
    "the parameters @xmath245 is chosen to be @xmath246 , the value of @xmath247 is updated by the newton method every three iteration , the iteration number @xmath232 of the newton method is fixed as @xmath248 , and the constant @xmath144 is chosen to be @xmath249 for @xmath250 and @xmath251 for @xmath252 ( they are also used for algorithm 2 ) . moreover , in order to accelerate the iteration , we use a variable step @xmath200 in our algorithm . specifically , we set @xmath253 , and update @xmath200 as @xmath254 after @xmath247 is recalculated .",
    "we use the stopping criterion for the proposed iterative algorithm as follows @xmath255 where @xmath50 denotes the iterate of the scheme ( correspond to the denoised log transformed image ) .",
    "note that @xmath256 in ( [ equ5.2 ] ) should be replaced by @xmath26 for the i - divergence model . in the experiments we choose @xmath257 .",
    "in order to evaluate the influence of @xmath242 to the final results of algorithm 1 , we use images with different noise level for our test .",
    "the parameter @xmath47 in ( [ equ1.2 ] ) reflects the noise level of gamma noise .",
    "here we choose @xmath258 . in figure",
    "[ fig5.2 ] , we plot the psnr values of the images denoised by our algorithm with @xmath242 varying from @xmath259 to @xmath260 . from the plots we observe that the psnr values are rather stable for @xmath261 under different noise conditions .",
    "in fact , the value of @xmath18 converges to more or less the same value very quickly ( refer to figure [ fig5.3 ] for the convergence case of @xmath18 ) , and therefore the influence of initial @xmath242 is negligible .",
    "we choose @xmath262 in the following experiments .    in the next ,",
    "we verify the convergence properties proved in subsection [ subsec3.1 ] .",
    "figure [ fig5.3:subfig : a]-[fig5.3:subfig : c ] show the evolution curves of @xmath247 with the increasing iteration number @xmath68 , which indicate that @xmath247 converges to some @xmath168 very quickly .",
    "moreover , we plot the evolution curves of the relative error @xmath263 and the psnr values versus the iteration number in figure [ fig5.3:subfig : d]-[fig5.3:subfig : i ] . from the plots we observe that the convergence properties of the proposed dp - ladm algorithm are really guaranteed by our experiments .",
    "finally , we show the advantage of the dp - ladm algorithm compared with the plad methods .",
    "three images contaminated by gamma noise with @xmath264 are used for comparison . for the plad method of solving the exponential model",
    ", we choose the regularization parameter @xmath265 to be varying from @xmath266 to @xmath267 .",
    "moreover , @xmath268 , @xmath269 are chosen for @xmath270 , and @xmath271 is used for @xmath272 .",
    "table [ tab5.1 ] shows the denoised results of the plad algorithm with different regularization parameters . from the psnr values we observe that the plad algorithm is rather sensitive to the parameter @xmath265 and needs many trials to obtain the optimal value . on the contrary ,",
    "the proposed dp - ladm algorithm is able to estimate the value of the regularization parameter during the iteration , and obtains satisfactory results with the estimator ( see the last column in table [ tab5.1 ] ) .",
    "[ htbp ]    [ tab5.1 ]      in this subsection , we further report the experiment results comparing our algorithms with those of the current state - of - the - art algorithms @xcite for multiplicative noise removal .",
    "the stopping criterion in ( [ equ5.2 ] ) is used for our test here .",
    "table [ tab5.2 ] lists the psnr values , the number of iterations and the cpu time of different algorithms for the natural images in figure [ fig5.1 ] . in this table ,  plad_exp \" and  plad_div \" represent the plad algorithms for the exponential model and i - divergence model respectively ( they were verified to be more efficient than the plad method with the correction step ( pladc ) , see table 4.4 in @xcite , thus we do nt consider the pladc algorithms here ) . ",
    "stv_al \" represents the spatially - adapted total variation model proposed in @xcite . here",
    "@xmath273 denotes the psnr values , iteration numbers and cpu time in sequence .",
    "specifically , the outer iteration number of  stv_al \" is fixed to be @xmath248 .",
    "note that these methods are all manual parameter models .",
    "for the plad algorithms , we use the same setting as that adopted in @xcite , i.e. , we fix @xmath274 , @xmath269 for the plad_exp and @xmath275 , @xmath276 for the plad_div . besides , @xmath265 is chosen to be @xmath277 for @xmath278 and @xmath279 for @xmath280 . for the spatially - adapted total variation model , the initial regularization parameter @xmath242 , the local window size @xmath211 and the step size @xmath199",
    "are chosen to be @xmath281 , @xmath282 and @xmath47 respectively .",
    "moreover , @xmath242 is set to be @xmath259 in our algorithms . in algorithm 2 ,",
    "the values of @xmath247 are updated by the newton iteration every twenty iteration for the cameraman and lena images , and computed every three iteration for the barbara image with @xmath283 .",
    "the local window size @xmath211 is chosen to be @xmath282 .",
    "in fact , algorithm 2 is unsensitive to the different values of @xmath211 while @xmath211 ranges from @xmath284 to @xmath285 , see figure [ fig5.4 ] for instance .",
    "[ htbp ]    [ tab5.2 ]    from the table we observe that algorithm 2 overall outperforms other algorithms while considering both the psnr values and cpu time . on the one hand , the ldp - ladm algorithm obtains better psnr values than the plad methods , and on the other hand , it takes less cpu time than the stv_al method .    in figure [ fig5.5 ] , the noisy and restored images corresponding to the cameraman image with @xmath286 are shown .",
    "the results in figure [ fig5.5](b)(c)(e ) are generated by tv models with scale regularization parameter , and those in figure [ fig5.5](d)(f ) are produced by tv models with spatially - adaptive regularization parameter .",
    "note that in figure [ fig5.5:subfig : f ] fine details such as the camera and the tripod are sharper as compared to [ fig5.5](b)(c)(e ) due to the use of the spatially varying parameter .",
    "meanwhile , we also observe that some artificial effects appear in the edges of the camera and the tripod of figure [ fig5.5:subfig : d ] owing to the over - fitting .",
    "however , it does not exist in figure [ fig5.5:subfig : f ] . in order to make the comparison clearer , we zoom into certain regions of these results in figure [ fig5.6 ] .    figures [ fig5.7 ] and [ fig5.9 ] show the denoised results of the barbara image with @xmath287 and the lena image with @xmath286 respectively . in figure [ fig5.7 ] , we observe that the details in the scarf are sharper in [ fig5.7:subfig : d ] and [ fig5.7:subfig : f ] than in [ fig5.7:subfig : b ] , [ fig5.7:subfig : c ] and [ fig5.7:subfig : e ] .",
    "meanwhile , the noise spots in the background are removed sufficiently in [ fig5.7:subfig : d ] and [ fig5.7:subfig : f ] , and not in the other results .",
    "part of the texture regions of these results are zoomed in figure [ fig5.8 ] .",
    "similarly , we observe that the fairs of the lena image are better recovered in [ fig5.9:subfig : d ] and [ fig5.9:subfig : f ] than in [ fig5.9:subfig : b ] , [ fig5.9:subfig : c ] and [ fig5.9:subfig : e ] .    finally , we quantify the denoising performance of different algorithms for the remote images in figure [ fig5.1 ] .",
    "table [ tab5.3 ] lists the psnr values , the number of iterations and the cpu time .",
    "the parameter setting of the plad algorithms is the same as that adopted for the experiments in table [ tab5.2 ] . for the spatially - adapted total variation model",
    ", we set @xmath288 for remote1 and remote2 with @xmath289 respectively , and fix @xmath290 for the nimes image .",
    "the other parameter setting refers to those used for the experiments in table [ tab5.2 ] . for algorithm 2 ,",
    "@xmath247 is updated by the newton method every three iteration .",
    "similarly to the experiment results in table [ tab5.2 ] , we observe that algorithm 2 obtains the best effect while comparing both the psnr values and cpu time .",
    "figure [ fig5.10 ] and [ fig5.11 ] shows the noisy image and the restored results of plad algorithms and our algorithm respectively . due to the use of the spatially varying parameter ,",
    "our algorithm is able to improve the quality of the restored images compared with the plad algorithms .",
    "meanwhile , since the parameter can be easily computed by the newton method during the iteration , it takes much less time than the stv_al method .",
    "[ htbp ]    [ tab5.3 ]",
    "in this paper , we propose two fast linearized alternating direction minimization algorithms that simultaneously estimate the regularization parameter and recover the image contaminated by gamma noise .",
    "the new approaches are base on the statistical characteristics of some random variable with respect to the gamma noise . by utilizing the linearized technique and the ( local ) discrepancy principle , we establish nonlinear equation(s ) with respect to the regularization parameter",
    ". then fast iterative algorithms , which update regularization parameter through computing the solution of the established equation(s ) , are proposed to remove the multiplicative noise .",
    "numerical experiments demonstrate that the proposed methods are able to obtain a suitable value for the regularization parameter , and overall outperform those of the current state - of - the - art methods while considering both the psnr values and cpu time .",
    "the work was supported in part by the national natural science foundation of china under grant 61271014 ."
  ],
  "abstract_text": [
    "<S> owing to the edge preserving ability and low computational cost of the total variation ( tv ) , variational models with the tv regularization have been widely investigated in the field of multiplicative noise removal . </S>",
    "<S> the key points of the successful application of these models lie in : the optimal selection of the regularization parameter which balances the data - fidelity term with the tv regularizer ; the efficient algorithm to compute the solution . in this paper , we propose two fast algorithms based on the linearized technique , which are able to estimate the regularization parameter and recover the image simultaneously . in the iteration step of the proposed algorithms , </S>",
    "<S> the regularization parameter is adjusted by a special discrepancy function defined for multiplicative noise . </S>",
    "<S> the convergence properties of the proposed algorithms are proved under certain conditions , and numerical experiments demonstrate that the proposed algorithms overall outperform some state - of - the - art methods in the psnr values and computational time .    </S>",
    "<S> total variation ; regularization parameter ; discrepancy principle ; linearized alternating direction minimization ; multiplicative noise </S>"
  ]
}