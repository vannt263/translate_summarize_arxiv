{
  "article_text": [
    "data sparse representations of tensors and efficient operations in the corresponding formats play increasingly important role in many applications . in the paper we consider a 3-_tensor _ @xmath3 $ ] that is an array with three indices . the number of allowed values of each index is called _",
    "mode size_. to specify tensor indices explicitly , we use _",
    "square brackets_. this notation allows to easily specify different _",
    "index transformations_. for instance , _ unfoldings _ of @xmath4 tensor @xmath5,$ ] are _ matricizations _ of sizes @xmath6 @xmath7 and @xmath8 that consist of columns , rows and tube fibers of @xmath9 @xmath10 , \\qquad a^{(2 ) } = a[j , ki ] , \\qquad a^{(3 ) } = a[k , ij].\\ ] ] here we set row / column / fiber index of the tensor @xmath5 $ ] as row index and join the two others in one multiindex for columns of the unfolding .",
    "the result is considered as a two - index object ( matrix ) , with row and column indices separated by comma .",
    "the difference between matrices and tensors is additionally stressed by use of uppercase letter instead of bold uppercase .",
    "the _ reshape _ of tensor elements assumes as well a change of the index ordering .",
    "for example , transposition of matrix reads @xmath11)^\\t",
    "= a[j , i],$ ] vectorization reads @xmath12",
    "= a[i , j].$ ] we see that the square bracket notation is rather self - explaining and suits for description of algorithms working with multidimensional data .",
    "we also will use the matlab - style _ round bracket _",
    "notation @xmath13 to point to individual element of @xmath5 $ ] and @xmath14 to select a mode vector ( i.e. row ) from tensor @xmath15 scalars and vectors are denoted by lowercase and bold lowercase letters .    in numerical work with tensors of large _ mode size",
    "_ it is crucial to use data sparse formats . for 3-tensors ,",
    "the most useful are the following .    the _ canonical decomposition _",
    "@xcite ( or _ canonical approximation _ to some other tensor ) reads @xmath16 = \\sum_{s=1}^r \\u_s[i ] \\o \\v_s[j ] \\o \\w_s[k ] , \\qquad    a(i , j , k ) = \\sum_{s=1}^r u(i , s ) v(j , s ) w(k , s).\\ ] ] the minimal possible number of summands is called _ tensor rank _ or _ canonical rank _ of the given tensor @xmath15 however , canonical decomposition / approximation of a tensor with minimal value of @xmath17 is a rather ill - posed and computationally unstable problem @xcite . this explains why among many algorithms of canonical approximation ( cf .",
    "@xcite ) none is known as absolutely reliable , and no robust tools for linear algebra operations maintaining the canonical format ( linear combinations , etc . )",
    "are proposed .    the ( truncated ) _ tucker decomposition / approximation",
    "_ @xcite reads @xmath18 & = \\g[p , q , s ] \\x_1 u[i , p ] \\x_2 v[j , q ] \\x_3 w[k , s ] , \\\\",
    "a(i , j , k ) & = \\sum_{p=1}^{r_1}\\sum_{q=1}^{r_2 } \\sum_{s=1}^{r_3 } g(p , q , s ) u(i , p ) v(j , q ) w(k , s ) .",
    "\\end{split}\\ ] ] the quantities @xmath19 are referred to as _ tucker ranks _ or _ mode ranks _ , the tensor @xmath20 $ ] of size @xmath21 is called the _",
    "tucker core _ , the symbol @xmath22 designates the multiplication of a tensor by a matrix along the @xmath23-th mode , the _ mode factors _ @xmath24 have orthonormal columns . in @xmath25 dimensions ,",
    "the memory to store @xmath26 core is @xmath27 that is usually beyond affordable for large @xmath25 and even for very small @xmath28 ( so - called _ curse of dimensionality _ ) . for @xmath29",
    "the storage is small and tucker decomposition can be used efficiently . in  @xcite",
    "the efficient operations with 3-tensors in canonical and tucker formats are discussed , with approximation of the result in the tucker format .",
    "simple operations like linear combination of small number of structured tensors can be done using _ multilinear svd _",
    "@xcite ( or high - order svd , hosvd ) , with quasi - optimal ranks and guaranteed accuracy .",
    "linear combination of many tensors , convolution , hadamard ( pointwise ) product of tensors and many other bilinear operations reduce to recompression of the following structured tensor @xmath30 = & \\kron(\\g , \\h)[ap , bq , cs ] \\x_1 u[i , ap ] \\x_2 v[j , bq ] \\x_3 w[k , cs ] , \\\\",
    "f(i , j , k )   = & \\sum_{pqs } \\sum_{abc } g(p , q , s ) h(a , b , c ) u(i , ap ) v(j , bq ) w(k , cs ) ,   \\end{split}\\ ] ] with @xmath31 core @xmath32,$ ] @xmath33 core @xmath34 $ ] and non - orthogonal factors @xmath35 and @xmath36 formally   is a tucker - like format with larger mode ranks @xmath37 that should be reduced ( truncated ) maintaining the desired accuracy . due to memory limitations , @xmath38 $ ] can not be assembled for mode sizes @xmath39 and auxiliary @xmath40 core can not be assembled for ranks @xmath41 ( see tab .  [ tab : mem ] ) . and",
    "@xmath42 in complexity estimates ] the structure of @xmath43 should be exploited without explicit evaluation of large temporary arrays .",
    ".memory for @xmath44 elements , mb [ cols=\"^,^,^,^,^\",options=\"header \" , ]     we conclude that proposed algorithm is faster that other methods for this purpose and return approximation of dominant subspaces that allows to construct approximation with accuracy about square root of machine precision . using the subspaces , computed by alg .",
    "[ alg ] as initial guess , rank revealing tucker - als converges to almost machine precision in one iteration .",
    "this work was supported by rfbr grants 08 - 01 - 00115 , 09 - 01 - 12058 , 10 - 01 - 00757 , rfbr / dfg grant 09 - 01 - 91332 , russian federation gov .",
    "contract @xmath45 and priority research program of dep .",
    "math . ras .",
    "the first author was supported by rfbr travel grant 10 - 01 - 09201 to present the results of this paper on icsmt ( hong kong , january 2010 ) .",
    "part of this work was done during the stay of the first author in max - plank institute for mathematics in sciences in leipzig ( germany ) .",
    "authors are grateful to heinz - jrgen flad and rao chinnamsettey for providing input data for the electron density functions ."
  ],
  "abstract_text": [
    "<S> we propose a fast algorithm for mode rank truncation of the result of a bilinear operation on 3-tensors given in the tucker or canonical form . if the arguments and the result have mode sizes @xmath0 and mode ranks @xmath1 the computation costs  @xmath2 the algorithm is based on the cross approximation of gram matrices , and the accuracy of the resulted tucker approximation is limited by square root of machine precision .    _ </S>",
    "<S> ams classification : _ </S>",
    "<S> 15a21 , 15a69 , 65f99 </S>"
  ]
}