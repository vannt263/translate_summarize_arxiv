{
  "article_text": [
    "the amount of information that a system is able to process ( and/or store ) plays an essential role when one tries to quantify the level of `` complexity '' of a system , and indeed often the mutual information [ 1 ] stored in the system ( or a concept derived from it , such as the past - future mutual information ) is used as a measure of its statistical complexity [ 2 ] .    over the last decade",
    "a number of authors have carried out work towards understanding under what conditions can we expect to maximize the information procesing capabilities of different types of complex systems . for instance , langton and others [ 3,4 ] investigated the behavior of cellular automata ( ca ) , while crutchfield , young and others [ 5 ] have been concerned mainly with iterated function systems and computational complexity in this area .",
    "the definitions used for complexity were rather problem dependent , and not surprisingly two main approaches to measuring statistical complexity have been developed over the years , as well as a large number of other `` ad hoc '' methods for describing structure .",
    "the first line of work uses information theory [ 6 - 9 ] , whereas the second approach defines complexity using computation theoretic tools [ 5,10 ] .    in spite of this model dependence ,",
    "the common picture that seemed to emerge from this work was that complex systems were able to show a maximally varied and self - organizative behaviour ( i.e. , maximally complex behaviour ) in the vicinity of sharp phase transitions [ 11 ] .",
    "since these transitions often belonged to the class commonly known in statistical mechanics as order - disorder phase transitions , this naturally led to the notion that maximally interesting behaviour of complex systems takes place `` at the edge of chaos '' , in an expression coined by langton [ 3 ] .",
    "( note however that the disordered phase does not neccesarily need to be chaotic in the strict sense of the word , i.e. , ergodyc . )",
    "the underlying reason was simple and appeling enough , neither very ordered systems with static structures , nor disordered systems in which information can not be persistently stored are capable of complex information processing tasks .    the actual verification of the fact that the mutual information ( or definitions of statistical complexity based on other approaches ) had a maximum in the vicinity of the relevant phase transitions were a trickier business though .",
    "early results by langton for ca s [ 3,4 ] and by crutchfield [ 5,10 ] for iterated dynamics showing sharp peaks in complexity as a function of the degree of order in the system at what appeared to be phase transitions were subsequently shown to be critically dependent on the particular measure of order choosen [ 2 ] .",
    "after this , arnold [ 12 ] showed numerically that the 2-dimensional ising model indeed had a maximum of statistical complexity ( defined through past - future mutual information ) at its order - disorder transition .    without wanting to go into the debate of what exactly constitutes a good measure of complexity -a debate often riddled with the specifics of the particular problem at hand-",
    ", it would seem clear though that complexity and information must bear a close relationship .",
    "we will thus concern ourselves in this paper with the mutual information contained in random boolean networks ( rbn ) [ 13 ] and its behavior as the networks undergo their order - disorder phase transition ( for a view point computational see [ 18 ] ) . by using a mean field approximation and assuming markovian behaviour of the automata",
    ", we will show both numerically and analytically that the mutual information stored in the network indeed has a maximum at the transition point .",
    "random boolean networks ( rbn ) [ 13 ] are systems composed of a number n of automata ( @xmath0 ) with only two states available ( say @xmath1 and @xmath2 for instance ) , each having associated a boolean function @xmath3 of @xmath4 boolean arguments that will be used to update the automaton state at each time step .",
    "each automaton @xmath5 will then have associated @xmath4 other automata @xmath6 ( the inputs or vicinity of @xmath5 ) , whose states @xmath7 will be the entries of @xmath8 . that is , the automaton @xmath5 will change its state @xmath9 at each time step according to the rule @xmath10    both @xmath8 and the identity of its @xmath4 inputs are initially assigned to the automaton @xmath5 at random .",
    "( in particular , the @xmath11 @xmath12 s are created by randomly generating outputs of value one with a probability @xmath13 , and of value zero with a probability @xmath14 , where @xmath13 is called the bias of the network ) .",
    "this initial assignation will be maintained throught the evolution of the system , so we will be dealing with a quenched system . even keeping this assignation fixed ,",
    "the number of possible networks that we can form for given values of @xmath11 and @xmath4 is extraordinarily high ( a total of @xmath15 possible networks ) .",
    "thus , if we want to study general characteristics of rbn systems we are inevitably led to an statistical approach .",
    "one fact that can be observed for all rbn s is that although the number of available states for a network of size @xmath11 grows like @xmath16 , the dynamics of the net separates the possible states into disjoint sets , attractor basins .",
    "each basin will lead the system to a different attractor .",
    "however , since the number of states available is finite and the quenched system is fully deterministic , we can be sure that the system will at some point retrace its steps in the form of periodic cycles .",
    "thus attractors will neccesarily be periodic sets of states .",
    "since after a transient any initial state will end up in one attractor or another , their period ( or rather their average period ) will set the typical time scale characterizing an rbn .",
    "it has been known for some time now [ 13 ] that rbn s show two different phases separated , for a given value of @xmath13 , by a critical value of @xmath4 , @xmath17 : this behaviour naturally induced the conjecture that at @xmath17 the rbn s undergo a second order phase transition .",
    "this conjecture has been prooven correct and some more information about the transition has been gained [ 15 ] .",
    "for instance , as we change the value of @xmath13 the critical value @xmath18 at which the transtion takes place also changes and a `` critical line '' appears , as shown in figure 1 . as was shown by [ 16 ] this line corresponds to    @xmath19    in the insets of figure 1 , three sets of states of a network with @xmath20 and @xmath21 are also shown as we move from the disordered state to the ordered one by changing @xmath13 , showing a typical order - disorder transition .",
    "each set of states contains 50 consecutive states , time running upwards along the vertical axys .",
    "[ criticalline - fig ]",
    "since rbn s appear undergo an order - disorder phase transition , a useful way to caharacterize the state of the system will be its `` self - overlap '' @xmath22 .",
    "this is simply defined to be one minus the hamming distance between an automaton at time @xmath23 and itself at time @xmath24 , averaged over all automata and times .",
    "let us expand on this .",
    "let us suppose that we generate an rbn with bias @xmath13 , and a random initial condition .",
    "we let the system evolve until the transient dies out and we are inside an attractor cycle , and then compute the states of the system for a number of time steps equal to the number of automata in the system ( that is , from @xmath25 to @xmath26 for the @xmath27 network that we have used .",
    "each experimental computer point in all figures is the average of @xmath28 differents networks with random initial conditions ) .",
    "let us suppose that we count the number of times that an automata is in the state @xmath2 both at time @xmath23 and @xmath24 , and average over all automata and time steps .",
    "this will give us the `` 1 state self - overlap '' , @xmath29 . repeating this procedure with the @xmath1 state",
    "will then obviously give us the `` zero state self - overlap '' , @xmath30 .",
    "then , @xmath22 will simply be given by @xmath31 on the other hand , we can analogously define @xmath32 and @xmath33 . note that by symmetry we have to have @xmath34 even with @xmath35 , since @xmath32 and @xmath32 are the joint probability distributions , not the conditional probabilities of transitioning from @xmath2 to @xmath1 or viceversa .",
    "it is then fairly easy to find the equation that describes the evolution of @xmath22 .",
    "if we define @xmath36 to be @xmath37 then it is not difficult to convince oneself that in a mean field approximation we must have @xmath38 where @xmath4 is the connectivity of the net",
    ". this equation forces @xmath22 to evolve towards fixed points , @xmath39 , that will depend on @xmath4 and @xmath13 .",
    "the stability analysis of ( 5 ) for @xmath40 gives the critical line ( 2 ) separating the ordered phase ( @xmath40 ) from the disordered phase ( @xmath41 ) .",
    "this is shown in figure 2 , where the evolution of @xmath22 given by ( 5 ) ( solid line ) is plotted against the results of the numerical simulations ( dots ) .",
    "the evolution lasts for as long as it takes the transient to die out , and once the system is in the attractor cycle @xmath22 takes on its fixed point value ( from now on we drop the star and designate the fixed point value simply by @xmath22 ) .",
    "[ auto - fig ]    let us now obtain analitycal expressions for the @xmath42 from our knowledge of @xmath22 , the normalization conditions and the fact that @xmath34 . by definition ( 3 ) and by normalization @xmath43 so that @xmath44 but then , by symmetry , @xmath45 we still have two more normalization conditions , derived from the fact that the probability of finding a mean field automaton in the state 1 is @xmath13 , and @xmath14 for the state @xmath1",
    "@xmath46 @xmath47 whence @xmath48 @xmath49 which satisfy ( 3 ) above .",
    "figure[autos - fig ] shows the analytical expressions for the @xmath50 ( solid lines ) together with the results from the numerical simulations ( dots ) .",
    "[ autos - fig ]    so far we have simply approximated the whole network by a set of mean field automata . however , since the @xmath42 are equivalent to @xmath51 we can now calculate the conditional probabilities @xmath52 if we now assume that our mean field automata are markovian these conditional probabilities will completely characterize their transition probabilities [ 1 ] . therefore , the transition matrix for the mean field markovian automaton is : @xmath53 which satisfy @xmath54 @xmath55 where @xmath56 are the probabilities of transitioning from the states @xmath57 to the state @xmath58 .",
    "thus , we have now reduced the whole network to a set of mean field automata evolving independently under markovian conditions , all the effects of their interactions being encoded in @xmath22 . to compute the past - future mutual information stored in the system we only have to apply information theory [ 2,17 ] .",
    "the one - automaton entropy is simply @xmath59 whereas the shannon uncertainty associated to the markovian evolution of this automaton will be @xmath60 with @xmath61 @xmath62 and @xmath63 @xmath64 the uncertainty is thus , @xmath65 @xmath66 @xmath67 whence the past - future mutual information will be : @xmath68 @xmath69 @xmath70 @xmath71 figure 4 shows the analytical expressions ( solid lines ) as well the experimental results from the simulations for both the one - automaton entropy ( dots ) and the shannon uncertainty ( triangles ) . note how the uncertainty is always smaller and decays faster than the one - automaton entropy .",
    "in particular , for @xmath72 ( where @xmath73 for our net with @xmath21 ) , we have @xmath74 and @xmath75 .",
    "thus in the ordered phase the mutual information becomes simply the one - automaton entropy .",
    "given this discussion , it is obvious that the mutual information that can be stored in the system has to have a maximum precisely at @xmath76 .",
    "this is shown in figure 4 , where the mutual information @xmath77 is plotted against @xmath13 ( again , both the analytical expression above as well as the experimental results ) .",
    "[ informacion - fig ]    finally , in figure 5 the mutual information is plotted against the one - automaton entropy @xmath78 . from @xmath79 corresponding to @xmath80 to @xmath81 which corresponds to the critical value @xmath82 , we see that @xmath77 is just a straight line of slope @xmath2 .",
    "this is as it should be , since as we just saw @xmath83 is zero for @xmath13 beyond @xmath76 , and @xmath84 in this region . precisely at @xmath85 ,",
    "@xmath77 reaches a maximum , and beyond this point it starts to decay non - lineary as the shanon uncertainty switches on .",
    "[ hversusi - fig ]",
    "by using a mean field approximation and a markovian ansatz for the evolution of an rbn , we have been able to show with a few , back of the envelope type of calculations , that the past - future mutual information contained in a rbn reaches a maximum at the point at which this system undergoes its order - disorder phase transition .",
    "also , in figure 5 we can see how the mutual information as a function of the amount of disorder present in the system ( the one - automaton entropy ) reaches a maximum at the point that corresponds to the phase transition .",
    "similar results obtained in [ 3,4 ] ( for ca s ) and in [ 5 ] ( for symbolic dynamics of the logistic map ) were criticized by li [ 2 ] on the ground that the peak was a artifact created by the particular quantity chosen to measure the disorder of the system .",
    "thus for instance li criticizes langton arguing that since in the ordered phase we have @xmath86 , it is only natural for him to find a straight line as the boundary of his plot of complexity against disorder ( as we do ) .",
    "li surmises that if instead of using @xmath87 as a measure of the disorder of the system one chooses to use the shanon uncertainty of the source @xmath88 ( @xmath89 for short from now on ) the left side of the plot would no longer be a straight line , and the maximum of @xmath77 would not be reached for intermediate values of the disorder .",
    "rather , in the @xmath90 plot the maximum of @xmath77 falls over the @xmath91 axis since @xmath77 reaches a maximum at zero @xmath89 , and @xmath77 monotonically decreases as @xmath89 increases .",
    "thus , the intuitive picture of the relationship between complexity and disorder proposed by langton and others ( i.e. , unimodal relationship between complexity and disorder with complexity reaching a maximum at intermediate values of the latter ) would no longer seem to be correct .",
    "this li takes as support to his conclusion that the dependence of @xmath77 on the amount of disorder in the system can take many varied forms .",
    "we think that the argument just presented , although trivially correct , fails to capture the essence behind the idea of unimodal dependence between @xmath77 and the amount of disorder in the system .",
    "we should first note that @xmath92 is not a single valued function .",
    "rather , since @xmath93 for @xmath94 , at @xmath93 @xmath77 grows from zero ( corresponding to @xmath80 ) to its maximum value ( corresponding to @xmath82 ) .",
    "that is , we have not got rid of the straight line in the @xmath95 graph , we have merely made it into a vertical line placed at @xmath93 .",
    "note however that the maximum of @xmath77 would still be reached at the transition point between the two phases of the system .",
    "this is in fact the central point of the issue at hand .",
    "the postulated unimodal dependence between @xmath77 ( or complexity ) and disorder rests under the assumption that , as we vary the order parameter , the system goes from an ordered phase into a disordered one with @xmath77 attainning its maximum value neither at one phase nor the other , but precisely at the transition point between them . if the quantity chosen as the order parameter varies over both phases then @xmath77 will reach this maximum for intermediate values of the parameter .",
    "if , on the other hand , a whole phase of the system is mapped into a single value of the order parameter , then quite obviously the maximum will be at one of the edges of the graph .",
    "thus one could say that the essence of ` unimodality ' lyes not on @xmath77 reaching its maximum for intermediate values of the order parameter , but on such maximum being at the transition point between the ordered and the disordered phases .    [ main - text ]",
    "the authors would like to thank juan prez mercader and r. v. sol for help . this work has been supported by the centro de astrobiologa ."
  ],
  "abstract_text": [
    "<S> during the last few years an area of active research in the field of complex systems is that of their information storing and processing abilities . </S>",
    "<S> common opinion has it that the most interesting beaviour of these systems is found `` at the edge of chaos '' , which would seem to suggest that complex systems may have inherently non - trivial information proccesing abilities in the vicinity of sharp phase transitions . </S>",
    "<S> a comprenhensive , quantitative understanding of why this is the case is however still lacking . </S>",
    "<S> indeed , even `` experimental '' ( i.e. , often numerical ) evidence that this is so has been questioned for a number of systems . in this paper </S>",
    "<S> we will investigate , both numerically and analitically , the behavior of random boolean networks ( rbn s ) as they undergo their order - disorder phase transition . </S>",
    "<S> we will use a simple mean field approximation to treat the problem , and without lack of generality we will concentrate on a particular value for the connectivity of the system . in spite of the simplicity of our arguments , we will be able to reproduce analitically the amount of mutual information contained in the system as measured from numerical simulations . </S>"
  ]
}