{
  "article_text": [
    "semi supervised learning methods are mainly classified into two broad classes : inductive and transductive . in both inductive and",
    "transductive the learner has both labeled training data set @xmath0 and unlabeled training data set @xmath1 , where @xmath2 .",
    "the inductive learner learns a predictor f : x @xmath3 y , f @xmath4 f where f is the hypothesis space , @xmath5 is an input instance , @xmath6 its class label .",
    "the predictor learns in such a way that it predicts the future test data better than the predictor learned from the labeled data alone . in transductive learning",
    ", it is expected to predict the unlabeled data @xmath7 without any expectations of generalizing the model to future test data .",
    "gaussian processes , transductive svm and graph - based methods fall in the latter category . on the other hand ,",
    "the former models are based on joint distribution and examples include expectation maximization .",
    "in many real world scenarios , it is easy to collect a large amount of unlabeled data \\{x}. for example , the catalogue of celestial objects can be obtained from sky surveys , geo spatial data can be received from satellites , the documents can be browsed through the web . however , their corresponding labels \\{y } for the prediction , such as classification of the galaxies , prediction of the climate conditions , categories of documents often requires expensive laboratory experiments , human expertise and a lot of time .",
    "this labeling obstruction results in an insufficiency in labeled data with an excess of unlabeled data left over . utilizing this unlabeled data along with",
    "the limited labeled data in constructing the generalized predictive models is desirable .",
    "semi supervised learning model can either be built by first training the model on unlabeled data and using labeled data to induce class labels or vice versa .",
    "the proposed inductive approach labels the unlabeled data using a hybrid model which involves both label propagation and svm . at every step in the process",
    ", it fits the model to minimize error and thus improve the prediction quality .",
    "the rest of the paper is organized as follows .",
    "the related work is discussed in section 2.label propagation , svm are discussed in section 3 and the proposed approach is presented in section 4 .",
    "section 5 contains the experimental results and comparison of the proposed approach with the label propagation algorithm followed by conclusion and future work in section 6 .",
    "a decent amount of work has been done in the field of semi - supervised learning in which major role has been played by the unlabeled data which is in huge amount as compared to the labeled data.castelli et al.[1],[2 ] and ratsaby et al.[3 ] showed that unlabeled data can predict better if the model assumption is correct .",
    "but if the model assumption is wrong , unlabeled data may actually hurt accuracy.cozman et al.[4 ] provide theoretical analysis of deterioration in performance with an increase in unlabeled data and argue that bias is adversely affected in such situations .",
    "another technique that can be used to get the model correct is to down weight the unlabeled data by corduneanu et al.[5].callison - burch et al [ 6 ] used the down - weighing scheme to estimate word alignment for machine translation .",
    "a lot of algorithms have been designed to make use of abundant unlabeled data .",
    "nigam et al.[7 ] apply the expectation maximization[8 ] algorithm on mixture of multinomial for the task of text classification and showed that the resulting classifiers predict better than classifier trained only on labeled data .",
    "clustering has also been employed over the years to make use of unlabeled data along with the labeled data.the dataset is clustered and then each cluster is labeled with the help of labeled data.demiriz et al.[9 ] and dara et al.[10 ] used this cluster and label approach successfully to increase prediction performance .",
    "a commonly used technique for semi - supervised learning is self - training . in this , a classifier is initially trained with the small quantity of labeled data . the classifier is then used to classify the unlabeled data and unlabeled points which are classified with most confidence are added to the training set .",
    "the classifier is re - trained and the procedure is repeated .",
    "word sense disambiguation is successfully achieved by yarowsky et al.[11 ] using self - training .",
    "subjective nouns are identified by riloff et.al[12 ] .",
    "parsing and machine translation is also done with the help of self - training methods as shown by rosenberg et al.[13 ] in detection of object systems from images .",
    "a method which sits apart from all already mentioned methods in the field of semi supervised learning is co - training .",
    "co - training [ 14 ] assumes that ( i ) features can be split into two sets ; ( ii ) each sub - feature set is sufficient to train a good classifier ; ( iii ) the two sets are conditionally independent given the class .",
    "balcan et al.[15 ] show that co - training can be quite effective and that in the extreme case only one labeled point is needed to learn the classifier .    a very effective way to combine labeled data with unlabeled data is described by xiaojin zhu et al.[16 ] which propagated labels from labeled data points to unlabeled ones .",
    "an approach based on a linear neighborhood model is discussed by fei wang et al.[17 ] which can propagate the labels from the labeled points to the whole data set using these linear neighborhoods with sufficient smoothness .",
    "graph based method is proposed in [ 18 ] wherein vertices represent the labeled and unlabeled records and edge weights denote similarity between them .",
    "their extensive work involves using the label propagation to label the unlabeled data , role of active learning in choosing labeled data , using hyper parameter learning to learn good graphs and handling scalability using harmonic mixtures .",
    "let @xmath8 be labeled data , where @xmath9 are the instances , y = @xmath10 are the corresponding class labels .",
    "let @xmath11 be unlabeled data where @xmath12 are unobserved .",
    "@xmath13 has to be estimated using @xmath14 and @xmath15 , where @xmath16 + @xmath17 @xmath18 is similarity between i and j , where @xmath19 f should minimize the energy function + @xmath20 and @xmath21 should be similar for a high @xmath18 .",
    "label propagation assumes that the number of class labels are known , and all classes present in the labeled data .",
    "the learning task in binary svm can be represented as the following @xmath22    subject to @xmath23 where @xmath24 and @xmath25 are the parameters of the model for total @xmath26 number of instances .",
    "+ using legrange multiplier method the following equation to be solved ,    @xmath27 are called legrange multipliers . by solving the following partial derivatives",
    ", we will be able to derive the decision boundary of the svm .",
    "the proposed algorithm is an inductive semi supervised learning , an iterative approach in which at each iteration the following steps are executed . in the first step label propagation is run on the training data and the probability matrix is computed for the unlabeled data .",
    "the second step is to train the svm on the available labeled data . now in the third step , the classes for unlabeled data are predicted using svm and class probabilities computed in step 1 are compared with the threshold . in step 4 ,",
    "all the records for which both label propagation and svm agree on the class label , are labeled with corresponding class.this continues till all the records are labeled or no new records are labeled in an iteration .",
    "one - one multi class svm is used as the data consists of multi classes .",
    "the records of each of the data sets are shuffled , 70% is considered for training and the rest is considered for testing .",
    "80% of the training data is unlabeled .",
    "the algorithm is implemented in both the serial and parallel versions .",
    "input : classifier , threshold + output : f - measure    1 .",
    "( labeled_records , unlabeled_records ) = select_next_train_folds ( ) + # _ each fold of data is split into labeled and unlabeled records with 20:80 ratio _",
    "+ # _ unlabeled_records have their class field set to -1 _ 2 .",
    "test_records = select_next_test_fold ( ) + # _ concatenate labeled and unlabeled records to get train_records _ 3 .   train_records = labeled_records + unlabeled_records 4 .   newly_labeled = 0 5 .   while len(labeled_records ) @xmath30 len ( train_records ) : 1 .",
    "lp_probability_matrix = run_lp(labeled_records + unlabeled_records ) 2 .",
    "model = fit_classifier(classifier , labeled_records ) 3 .",
    "labeled_atleast_one = false 4 .   for record in unlabeled_records",
    ": a.   classifier_out = model.predict_class(record.feature_vector ) + # _ test for lp and classifier agreement _ b.   if lp_probability_matrix[record.feature_vector][classifier_out ] @xmath31 threshold : a.   unlabeled_records.remove(record ) b.   record.class_label = classifier_out # label the record c.   labeled_records.add(record ) # add the newly labeled record to set of labeled records d.   newly_added + = 1 + # _ set labeled_atleast_one flag to true if at least one new record is labeled in current iteration of while loop _",
    "e.   labeled_atleast_one = true + # _ break the loop if no new record is labeled in current iteration of while loop _ 5 .",
    "if labeled_atleast_one = = false : 1 .",
    "break + # _ compute f - measure of constructed model _ 6 .",
    "test_records_features = test_records.get_feature_vectors ( ) 7 .   test_records_labels = test_records.get_labels ( ) 8 .   predicted_labels = model.predict(test_records_features ) 9 .",
    "f - measure = compute_fmeasure(predicted_labels , test_records_labels )      input : classifier , threshold , no_of_tasks # number of parallel processes + output : f - measure    1 .   newly_labeled = 0 2 .   while len(labeled_records ) @xmath32 len(train_records ) : 1 .",
    "lp_train_records = labeled_records + unlabeled_records 2 .",
    "lp_probability_matrix = [ ] ; classifier_out = [ ] 3 .",
    "lp_process = new_process(target = run_lp , args = ( lp_train_records , lp_probability_matrix ) ) 4 .",
    "lp_process.start ( ) 5 .   classifier_process = new_process(target = fit_classifier , args = ( classifier , labeled _ records , unlabeled_records , classifier_all_out ) ) 6 .",
    "classifier_process.start ( ) 7 .",
    "lp_process.join ( ) 8 .",
    "classifier_process.join ( ) 9 .",
    "atleast_one_labeled = false 10 . chunk_size = len(unlabeled_records ) / no_of_tasks 11 .",
    "all_pids = [ ] 12 .",
    "none_initialize(labeled_lists , no_of_tasks ) 13 .",
    "none_initialize(unlabeled_copies , no_of_tasks ) 14 . for i in range(len(labeled_lists ) ) : a.   start = i * chunk_size b.   end = ( i+1 ) * chunk_size c.   unlabeled_copies = unlabeled_records[start : end ] d.   lp_probabilities = lp_probability_matrix[start : end ] e.   classifier_outs = classifier_all_outs[start : end ] f.   label_records_process = new_process(func = label_data , args = ( unlabeled_copies[i ] , labeled_lists[i ] , lp_probabilities , classifier_outs , threshold ) ) g.   label_records_process.start ( ) h.   all_pids.append(label_records_process ) 15 .",
    "unlabeled_records = [ ] 16 .",
    "done_processes = [ ] 17 . while len(done_pids ) @xmath33 len(all_pids ) : a.   for i in range(len(all_pids ) ) : + . if not all_pids[i].is_alive ( ) and ( i not in done_pids ) : a.   done_processes.append(i ) b.   unlabeled_records + = unlabeled_copies[i ] c.   labeled_records + = labeled_lists[i ] 18 .",
    "if atleast_one_labeled = = false : 1 .",
    "break + # _ compute f - measure of constructed model _ 3 .",
    "predicted_labels = [ ] 4 .",
    "test_records_features = test_records.get_feature_vectors ( ) 5 .   test_records_labels = test_records.get_labels ( ) 6 .",
    "run_parallel_classifier(predicted_labels , labeled_records , test_records_features , classifier , no_of_tasks ) 7 .",
    "f - measure = compute_fmeasure(predicted_labels , test_records_labels )",
    "in our experimental analysis , we considered 12 different datasets . the datasets along with their number of attributes ( excluding the class label ) and number of instances are as follows ( in the format_dataset : ( no of attributes , no of records _ ) ): vowel : ( 10 , 528 ) , letter : ( 16 , 10500 ) , segment : ( 18 , 2310 ) , iris scale random : ( 4 , 149 ) , sat .",
    "2.serial version of our hybrid approach with supervised learning classifier svm .    3.parallelization of our algorithm with our own serial implementation .",
    "before performing the above comparisons , the serial version of our hybrid approach is examined on the following aspects .",
    "the algorithm is run for different values of threshold of the probability matrix of label propagation and percentage of initially labeled data in each of the training data set .",
    "we observed the values of percentage of increase in labeled records for each iteration , percentage of labeled data at the final iteration and finally the training time and f-measure.an alternative classifier logreg is also implemented and its performance is compared with svm based on factors like f - measure and training time .    as it can be seen from fig 1 , varying the threshold of the probability matrix of label propagation has little impact on the f - measure .",
    "considering only label propagation increase in threshold would lead to stricter labeling resulting in increase in precision and decrease in recall .",
    "similarly , the decrease in probability threshold results in an increase in number of unlabeled records being considered for labeling .",
    "hence , it should increase the labeling rate of the records .",
    "but when svm is used with label propagation precision and recall are not allowed to vary significantly because a record can be labeled only when svm and label propagation agree on the class label .",
    "so , unlabeled records marked by label propagation for labeling with low confidence are discarded by output of svm .",
    "thus the percentage of labeled data at the end of the final iteration fluctuates very little for all the thresholds ( as it is shown in next graph fig 2 ) .",
    "so change in thresholds , has little effect on f - measure of the model .    to see the effect of the dimension and the number of records in the dataset , in fig 3 , we plotted training time with respect to the cube of number of records in the dataset for the best performing classifier and threshold .",
    "the axis were chosen by keeping in mind the o ( dim*@xmath34 ) complexities of both svm and logreg .",
    "the graph tends to show polynomial increase in training time as n increases ( instead of linear ) .",
    "this may be the effect of neglecting lower order terms in the complexity expression of svm and logreg .",
    "the analysis of fig 1 and fig 2 explains the following feature of the algorithm .",
    "for the datasets : 1000 records  sloan digital sky survey ( sdss ) , 10000 records sdss , mfeat , pendigits and shuttle , percentage of labeled data is very low 0 - 20% .",
    "but f - measures are reasonably high between 0.67 to 0.9 .",
    "this shows that their high f - measure does not always require high amount of unlabeled data to be labeled .",
    "as long as the algorithm is able to label representative records in the dataset , it is likely to give good f - measure .",
    "we observed the percentage of increase in labeled records for every iteration for the best choice of classifier and threshold ( according to f - measure ) . in fig 4 , it shows that percentage of increase in labeled records decreases exponentially ( note the log scale ) as the iterations progress .",
    "this means not much data gets labeled as loop progresses .",
    "this is the consequence of label propagation and svm not agreeing on deciding the class label which is to be assigned to the unlabeled record . while labeling an unlabeled record",
    ", there is a low chance of misclassification by svm , since it is always trained on labeled data .",
    "this means that the quality of labeling done by label propagation decreases significantly as the iterations progresses .",
    "this deterioration in label propagation s quality has very little effect on algorithm s overall prediction quality because of the right predictions done by svm at every step while labeling the unlabeled records leading to better performance than label propagation .",
    "the performance of the proposed approach is compared with the label propagation algorithm[16 ] for all the datasets and shown in fig 5 .",
    "f - measures of the proposed approach were noted for best choice of classifier and threshold . for label propagation",
    ", all the unlabeled records were labeled according to the class corresponding to highest label propagation probability .",
    "in all the cases , the proposed approach outperforms label propagation by a large margin .",
    "this high quality of performance can be attributed use of svm together with label propagation to label the unlabeled examples .",
    "no unlabeled example is labeled without the agreement of both  svm and label propagation .",
    "this significantly reduces the pitfalls caused by label propagation increasing the prediction quality of overall approach .",
    "the analysis of our approach(semi supervised learning ) with the supervised svm is also studied .",
    "results in fig 6 show that the f - measure of our approach is comparable .",
    "fig 7 to 12 are plotted for different percentages of initially unlabeled data for the data sets vowel , irisscalerandom , satimage , 1000-sdss , glassscalrandom , mfeat respectively  considering best choice of threshold ( as per the f - measure ) .",
    "as can be seen , f - measure of the model tends to fall as percentage of initially unlabeled data increases .",
    "this is intuitive . as the amount of labeled data in the initial data increases ,",
    "the algorithm is able to learn the pattern present in a representative sample of the dataset .",
    "thus it can successfully generalize the pattern to the test set leading to a overall increase in f - measure .    .",
    "]                                    finally , the parallel version of the proposed approach is also implemented .",
    "as the results can be seen from fig 13 , parallelizing the algorithm helps to improve training time of the algorithm .",
    "two of the most expensive steps in the algorithm are training svm and label propagation on the data .",
    "doing these two in parallel reduces training time significantly ( note that training time is in log scale ) .",
    "the analysis is done for sdss dataset for samples of different sizes.the results are shown in fig 14 . for each sample",
    ", we ran different number of parallel tasks and training time is observed .",
    "results show that number of parallel tasks have reasonable effect on training time only when dataset size exceeds a certain threshold ( around 60000 records in this case ) .",
    "further , for each dataset , there is an optimum number of parallel tasks which yields minimum training time .",
    "if number of parallel tasks is above this optimum level , the cost of maintaining these parallel tasks exceeds the gain earned by parallel computation . on the other hand ,",
    "if number of parallel tasks is set to a value less than optimum level , system resources are poorly utilized .",
    "so it is necessary to set number of parallel tasks to optimum value for maximum benefit .",
    "the proposed approach is tested on skewed datasets also .",
    "the proportion of class labels in the skewed dataset is at least 1:8 .",
    "f - measure of 10000-sdss drops by approximately 0.1 when it has skewed class proportions . but",
    "skewed version of shuttle shows exceptional behavior .",
    "its f - measure remains almost the same .",
    "this shows that skewness of the data has little or no effect on f - measure of algorithm .",
    "it can be inferred that the distribution of the data plays a major role in performance of semi - supervised algorithm .",
    "the results are shown in fig 15 .",
    "the proposed approach uses svm along with label propagation algorithm to yield a very high overall prediction quality .",
    "it can use a small amount of labeled data along with a large quantity of unlabeled data to yield a high f - measure on test data .",
    "it has a very small margin for error since it labels the unlabeled data using consent of both - svm and label propagation .",
    "on testing both the algorithms on 12 different datasets we can conclude that the proposed approach performs much better than label propagation[16 ] alone .",
    "it yields f - measure values which are almost twice as compared to label propagation .",
    "further , we designed the parallel version of the approach and were able to decrease the training time significantly . in future",
    ", the parallel algorithm can be further enhanced to yield linear or super linear scale up",
    ". further research on the role of supervised algorithms in the field of semi supervised learning could be beneficial .        4 castelli , v. , & cover , t. ( 1995 ) . the exponential value of labeled samples .",
    "pattern recognition letters , 16 , 105111 .",
    "castelli , v. , & cover , t. ( 1996 ) .",
    "the relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter .",
    "ieee transactions on information theory , 42 , 21012117 .",
    "ratsaby , j. , & venkatesh , s. ( 1995 ) .",
    "learning from a mixture of labeled and unlabeled examples with parametric side information .",
    "proceedings of the eighth annual conference on computational learning theory , 412417 .",
    "cozman , fabio gagliardi , ira cohen , and marcelo cesar cirelo .",
    "`` semi - supervised learning of mixture models . ''",
    "2003 corduneanu , a. , & jaakkola , t. ( 2001 ) .",
    "stable mixing of complete and incomplete information ( technical report aim-2001 - 030 ) .",
    "mit ai memo .",
    "callison - burch , c. , talbot , d. , & osborne , m. ( 2004 ) .",
    "statistical machine translation with word- and sentence - aligned parallel corpora .",
    "proceedings of the acl .",
    "nigam , k. , mccallum , a. k. , thrun , s. , & mitchell , t. ( 2000 ) .",
    "text classification from labeled and unlabeled documents using em .",
    "machine learning , 39 , 103134 .",
    "dempster , a. , laird , n. , & rubin , d. ( 1977 ) . maximum likelihood from incomplete data via the em algorithm .",
    "journal of the royal statistical society , series b. bennett , k. , & demiriz , a. ( 1999 ) .",
    "semi - supervised support vector machines.advances in neural information processing systems , 11 , 368374 .",
    "dara , r. , kremer , s. , & stacey , d. ( 2002 ) . clsutering unlabeled data with soms improves classification of labeled real - world data .",
    "proceedings of the world congress on computational intelligence ( wcci ) .",
    "yarowsky , d. ( 1995 ) .",
    "unsupervised word sense disambiguation rivaling supervised methods .",
    "proceedings of the 33rd annual meeting of the association for computational linguistics ( pp .",
    "189196 ) .",
    "riloff , e. , wiebe , j. , & wilson , t. ( 2003 ) .",
    "learning subjective nouns using extraction pattern bootstrapping .",
    "proceedings of the seventh conference on natural language learning ( conll-2003 ) .",
    "rosenberg , c. , hebert , m. , & schneiderman , h. ( 2005 ) .",
    "semi - supervised self training of object detection models .",
    "seventh ieee workshop on applications of computer vision .",
    "blum , a. , & mitchell , t. ( 1998 ) . combining labeled and unlabeled data with co - training .",
    "colt : proceedings of the workshop on computational learning theory .",
    "balcan , m .- f . , & blum , a. ( 2006 ) . an augmented pac model for semi - supervised learning . in o. chapelle ,",
    "b. sch  olkopf and a. zien ( eds . ) , semi - supervised learning .",
    "mit press .",
    "xiaojin zhu and zoubin ghahramani .",
    "learning from labeled and unlabeled data with label propagation .",
    "technical report cmu - cald-02 - 107 , carnegie mellon university , 2002 .",
    "fei wang , changshui zhang , label propagation through linear neighborhoods - 2008 http://machinelearning.wustl.edu/mlpapers/paper_files/icml2006_wangz06.pdf xiaojin zhu , john lafferty , and ronald rosenfeld .",
    "semi - supervised learning with graphs . diss.carnegie mellon university , language technologies institute , school of computer science , 2005 hearst , m.a . ; dumais , s.t . ; osman , e. ; platt , j. ; scholkopf , b. , `` support vector machines , '' intelligent systems and their applications , ieee , vol.13 , no.4 , pp.18,28 , jul / aug 1998"
  ],
  "abstract_text": [
    "<S> semi supervised learning methods have gained importance in today s world because of large expenses and time involved in labeling the unlabeled data by human experts . </S>",
    "<S> the proposed hybrid approach uses svm and label propagation to label the unlabeled data . in the process , at each step svm is trained to minimize the error and thus improve the prediction quality . </S>",
    "<S> experiments are conducted by using svm and logistic regression(logreg ) . </S>",
    "<S> results prove that svm performs tremendously better than logreg . </S>",
    "<S> the approach is tested using 12 datasets of different sizes ranging from the order of 1000s to the order of 10000s . </S>",
    "<S> results show that the proposed approach outperforms label propagation by a large margin with f - measure of almost twice on average . the parallel version of the proposed approach is also designed and implemented </S>",
    "<S> , the analysis shows that the training time decreases significantly when parallel version is used .    </S>",
    "<S> -supervised learning , data mining , support vector machine , label propagation </S>"
  ]
}