{
  "article_text": [
    "scalability of checkpointing for petascale and future exascale computing is a critical question for fault tolerance on future supercomputers .",
    "a stream of publications by researchers has been concerned with this question of fault tolerance for future supercomputers  @xcite .",
    "system - level transparent checkpointing has been avoided at larger scale in hpc because of the need for a full - memory dump .",
    "for example , a 2014  report  @xcite on software resilience presents a typically pessimistic outlook for pure full - memory checkpoints :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ `` the norm in 2009 was to store the application state on remote storage , generally a parallel file system , through i / o nodes .",
    "checkpoint time was significant ( often 15-30 minutes ) , because of the limited bandwidth of the parallel file system .",
    "when checkpoint time is close to the mtbf , the system spends all its time checkpointing and restarting , with little forward progress .",
    "since the [ mtbf ] may be an hour or less on exascale platforms , new techniques are needed in order to reduce checkpoint time . ''",
    "@xcite _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    nevertheless , prior work on transparent , system - level checkpointing is only used at moderate - scale ( e.g. , 128  mpi processes in  @xcite ) .",
    "the single - node checkpointing package blcr  @xcite is used in combination with the checkpoint - restart service of a given mpi implementation such as-@xcite for open  mpi or @xcite for lam / mpi . in this approach ,",
    "the checkpoint - restart service temporarily tears down the infiniband network , and delegates the single - process checkpointing to blcr .",
    "this approach does not scale , since blcr does not support sysv shared memory objects  @xcite .",
    "most modern mpi implementations require such shared memory for efficient communication among mpi processes on the same node  to avoid the delay in going through kernel system calls .    moreover , an important work on transparent , system - level checkpointing is  @xcite , which supported only infiniband  rc ( reliable connection ) mode .",
    "while that result sufficed for earlier mpi implementations , modern mpi implementations require infiniband  ud for optimal performance when running with more than about 64  processes .",
    "this is because a pure point - to - point rc  mode implementation would require up to @xmath0 connections for @xmath1  mpi ranks ( for @xmath1  mpi processes ) .",
    "mpi requires infiniband  ud for the scales considered in this work , such as  24,000 mpi processes on 24,000 cpu cores .",
    "setting up @xmath2 , or 576  million , point - to - point rc connections is unacceptable both due to large memory resources and long times for initialization .    to advance in realizing transparent checkpointing on large - scale supercomputers , we need to address the fundamental problem for transparent checkpointing over infiniband : how to save or replay `` in - flight data '' that is present in the infiniband network at the time of checkpointing , while at the same time not penalizing standard runtime performance . in particular , we need to address ( a )  how to enable transparent checkpointing to support the infiniband ud ( unreliable datagram ) mode ; and ( b )  how to remove most of the runtime overhead that is seen at larger scales .",
    "note that this second issue of runtime overhead even affects performance when no checkpoints are taken .",
    "the earlier result  @xcite had already shown that runtime overhead grew as high as 1.7% when using 2k  cores .",
    "when we then scaled to 4k  cores on the stampede supercomputer in this work , we even saw overhead grow to an unacceptable  9% ( see section  [ sec : runtime - overhead ] for a discussion and solution ) .",
    "the primary contribution of this paper is to demonstrate the practicality of petascale system - level checkpointing through the use of full - memory dumps . in order to achieve this ,",
    "dmtcp  @xcite was used as a vehicle for checkpointing .",
    "we have extended the designs for dmtcp software to have a tight interaction with modern mpi runtimes by taking advantage of some important scalability features .",
    "the proposed enhancements are along these directions :    1 .",
    "this is the first checkpoint support for a hybrid infiniband communication mode that uses both reliable connection ( rc ) and unreliable datagram ( ud ) .",
    "a hybrid rc / ud  mode provides better performance than a pure connection - oriented rc  mode , and is a commonly used optimization among modern mpi implementations .",
    "see section  [ sec : udmodel ] for details .",
    "a secondary contribution is to lower the runtime overhead for checkpointing rc  mode connections themselves .",
    "the previous work supported only rc  mode  @xcite using runtime tracing of infiniband send messages .",
    "the runtime overhead was shown to be 1.7% for 2k  cores",
    "( see table  2 in  @xcite ) , which grew to 9% for 4k  cores in current experiments on stampede .",
    "we use a new checkpoint - time strategy that reduces the runtime overhead to under  1% even for 4k  cores on stampede ( see section  [ subsec : runtime ] ) .",
    "the current work represents an advance in the state - of - the - art . by transparently supporting both infiniband rc and ud mode",
    ", this work demonstrates a pure system - level checkpoint over 24,000 cpu cores at the petascale stampede supercomputer  @xcite in just 10.6  minutes , during which 29  tb are saved to stable storage on a lustre filesystem .",
    "in contrast , an earlier report  @xcite described the 2009 state - of - the - art for checkpointing to be 1530 minutes for a supercomputer from that era , and had argued that checkpointing times would increase even further from the times of that era .",
    "of course checkpointing through full - memory dumps is just one component of a software resilience strategy of the future , and is compatible with other complementary approaches .",
    "these include multi - level checkpointing  @xcite , incremental checkpointing , partial restart , mitigation of silent data corruption ( sdc ) , tuning of checkpoint frequencies  @xcite , and alternate approaches to error prevention , prediction , tolerance , detection , containment and recovery ( forward or backward )  @xcite .",
    "going beyond the petascale level presented here , there is an important question of scalability to support future exascale computing . in order to address this",
    ", we enunciate a simple formula , the checkpoint fill - time law , for predicting the checkpoint time using fundamental specifications for a given supercomputer ( see section  [ sec : ssd ] ) .",
    "this law predicts an ideal checkpoint time , and underestimates the checkpoint time for two real - world applications ( hpcg and namd ) by as much as a factor of ten .",
    "nevertheless , this formula predicts that ssd - based exascale supercomputers of the future will enable checkpointing through a full - memory dump in just 1.6  minutes ( ideally ) , or a real - world 16  minutes if one extrapolates using the same factor of ten that is seen at the petascale level .    in order to gain confidence in the predictions for an ssd - based supercomputer",
    ", we also tested on a single ssd - based computer in table  [ tbl : ckptfilltime ] .",
    "a 3  gb checkpoint image was created there in 7.2  seconds ( and restart required 6.2  seconds ) .",
    "this is an i / o bandwidth of 416  mb / s , close to the ideal bandwidth of 500  mb / s for sata 3.0 interface . since 3  gb is 2.3% of the 128  gb ssd disk , the predicted ideal checkpoint time is 2.3% of 4.3  minutes , or 5.9  seconds .",
    "so , the predicted time of 5.9  seconds compares well with the actual time of 7.2  seconds .",
    "in addition to the research contributions above , we were surprised to discover a counter - intuitive practical issue in checkpointing at petascale levels",
    ". simply launching a new computation was found to be excessively slow with 8k  cores , and was found to fail at 16k  cores ( see section  [ sec : socketlimitations ] ) .",
    "this was tracked down to limitations in the hardware / software system .",
    "the simple act of creating 16k  cores was found to overwhelm the hardware / software system on the stampede supercomputer . in discussions with sysadmins",
    ", we found that in the emphasis on infiniband over ethernet meant that each rack at stampede was provided with just a single 10  gb  ethernet backbone from each rack .",
    "hence , this appears to have led to longer delays in the processing of ethernet by the kernel at larger scales , and we directly observed processes receiving a sigkill signal from the kernel at 16k  cores .",
    "the rest of this paper is organized as follows . the relevant background on lustre , dmtcp , and the various modes used by mvapich2 are presented in section  [ sec : background ] .",
    "section  [ sec : implementation ] describes the methodology used to achieve petascale level and some implications for extending checkpointing to the next level .",
    "the experimental evaluation is presented in section  [ sec : experiment ] .",
    "section  [ sec : discussion ] describes the scalability issues associated with petascale checkpointing .",
    "the related work is presented in section  [ sec : related ] , and conclusions appear in section  [ sec : conclusion ] .",
    "the following three subsections review three critical components that affect the performance in the experiments : the mpi implementation ( mvapich2 at tacc , and intel  mpi and open  mpi at ccr  see section  [ sec : experiment ] ) , dmtcp itself as the checkpointing software , and lustre as the back - end filesystem .",
    "we highlight mvapich2  @xcite as the mpi used in the majority of experiments .",
    "other mpi implementations typically have similar features to those described here .",
    "mvapich2 uses the tcp / ip - based process management interface ( pmi ) to bootstrap the infiniband end - points using infiniband  rc .",
    "while pmi is the most straightforward way to establish infiniband connectivity , it leads to poor startup performance due to the @xmath0 point - to - point connections referred to in the introduction . for mpi jobs with more than 64  processes",
    ", mvapich2 also uses the lazy establishment of `` on - demand '' connections using infiniband  ud  @xcite ( although the 64-process threshold can be configured using the mv2_on_demand_threshold environment variable ) .",
    "distributed multithreaded checkpointing ( dmtcp )  @xcite provides a framework for coordinated checkpointing of distributed computations via a centralized coordinator .",
    "each client process of the application communicates with the coordinator via a tcp socket .",
    "dmtcp includes a checkpointing library that is injected into each process of the target application .",
    "this library creates a checkpoint thread in each process , to communicate with the coordinator and to copy process memory and other state to a checkpoint image .    the coordinator implements global barriers to synchronize checkpoint / restart between multiple nodes , and it provides a publish - subscribe scheme for peer discovery ( e.g. , discover new tcp peer addresses for infiniband i d during restart ) .",
    "these are used in combination with wrappers around library functions to build plugin libraries .",
    "the plugin libraries are injected along with the checkpoint library .",
    "they serve to translate real ids into virtual ids seen by the application , and to update the virtual address translation table with the new real ids that are seen on restart  @xcite .",
    "this virtualization capability is used to virtualize below the level of the mpi library  [ subsec : ompiandimpi ] ) . a new plugin capability for this work serves to virtualize the infiniband  ud mode .",
    "the lustre filesystem at stampede plays a critical role in supporting high - bandwidth writes of checkpoint image files .",
    "lustre  @xcite is a parallel object - based filesystem in widespread use that was developed to support large - scale operations on modern supercomputers .",
    "lustre attains high i / o performance by simultaneously striping a single file across multiple object storage targets ( osts ) that manage the system s spinning disks .",
    "lustre clients run the lustre file system and interact with osts for file data i / o and with metadata servers ( mds ) for namespace operations .",
    "the lustre protocol features authenticated access and write - behind caching for all metadata updates .",
    "in the first subsection , we discuss a key barrier to petascale checkpointing and its solution : support for infiniband ud  mode . in the nature of lessons learned , we also present two additional and unexpected barriers to scalability within the context of running on the stampede supercomputer : excessive runtime overhead at large scale , and the lack of support for processes employing many tcp sockets .    finally , the scalability of this approach for future exascale supercomputers is a key concern .",
    "the key question here is the write bandwidth to the storage subsystem for a full - memory dump from ram .",
    "section  [ sec : ssd ] presents a simple , empirical model , the checkpoint - fill - time law , to extrapolate trends , and predicts that with the adoption of ssd for secondary storage in supercomputers ( and with hard disks being relegated to tertiary storage ) , expected checkpoint times in the exascale generation are estimated at 1.6  minutes , ideally , and 16  minutes in real - world applications .",
    "recall from section  [ sec : mvapich2 ] that the infiniband ud communication mode is for connectionless unreliable datagrams .",
    "newer versions of mpi use a hybrid rc / ud scheme for balancing performance with the memory requirements for the queue pairs .",
    "thus , transparent checkpointing of modern mpi requires support for ud and in particular for hybrid rc / ud mode ( in which both types of communication operate in parallel ) .",
    "virtualization of address handler and lid ( local i d ) of remote hca ( hardware channel adapter ) . ]",
    "the key to checkpointing ud is to virtualize remote address of the queue pairs , so that the actual address can be replaced by a different address after restart .",
    "figure  [ fig : ibv - ud - ah ] presents an overview of the situation , to accompany the detailed description that follows .",
    "the approach here maintains a translation table between virtual and actual addresses , and is implemented using dmtcp plugins  @xcite .",
    "further , on each ud - based send , the infiniband lid ( local identifier ) must also be updated for a possibly different remote queue - pair address .    in detail",
    ", each computer node includes an infiniband hca ( host channel adapter ) .",
    "the hca provides hardware support for a _",
    "queue pair _ , which refers to a send - receive pair of queues .",
    "unlike the connection - oriented rc communication mode , ud has no end - to - end connections .",
    "so a local queue pair can send messages to different remote queue pairs .",
    "the problem of virtualizing a remote ud address is made more difficult because of the dynamic change of the address of the remote queue pair , which is identified by a unique pair ( lid ( local identifier ) , qp_num ( queue pair number ) ) .",
    "the lid is assigned by the subnet manager , which is unique only within the local subnet , while the queue pair number is assigned by the hardware driver , which is unique only within the local hca .",
    "since both fields can change after restart , we need to virtualize both fields in order to identify the remote ud address .    at the time of restart ,",
    "all previously created ud queue pairs ( as well as the ones created after restart ) will send their address pairs to the checkpoint coordinator . after the application resumes , each node must discover the ( remote - lid , queue - pair - number ) .",
    "it was decided to do this by querying the checkpoint coordinator at runtime prior to each ud - based send for the latest destination lid .",
    "although this adds to the runtime overhead , ud is not used as the primary communication channel by mpi implementations , and so this runtime querying overhead is negligible in practice .",
    "note that ud presents a very different situation from the older rc  mode work in  @xcite . for rc  mode",
    "it s safe to build the connection at restart , because the peer wo nt change .",
    "but the destination of a given lid can change after restart . in the extreme case ,",
    "each ud - based send can change its peer queue - pair address , and so there s no fixed peer .",
    "instead , we are forced to patch the right remote address on every send .",
    "finally , the ud protocol refers to an address handler ( for which the remote lid is just one field ) .",
    "so , instead of virtualizing the remote lid , we must virtualize the address handler ( ah ) . hence",
    ", we create wrapper functions around all infiniband library calls that refer to an ah . whenever an ah is created , we also create a shadow ah .",
    "thus , the application code only sees pointers to our shadow ah , and our wrapper functions make corresponding modifications to the actual ah struct that is passed to the infiniband library . on restart , the shadow ah is redirected to a new , actual ah constructed by the infiniband library . in particular , this technique can account for any hidden fields in the actual ah , or other examples of data hiding that an infiniband library may use .      in testing on the lu.e benchmark , we saw runtime overhead rise to 9% for 4k  cpu cores compared to the 1.7% runtime overhead at 2k  cores reported by  @xcite .",
    "this was due to the non - scalable tracing of send / receive requests required by the infiniband checkpointing code to shadow hardware state because infiniband devices do nt provide a way to `` peek '' at the current state .",
    "we address this scaling problem by updating the model by relaxing some of the guarantees around send / receive queues . instead of computing the exact number of pending",
    "send messages at checkpoint time , we poll the receive queues for a `` reasonable '' amount time during checkpointing .",
    "if a message arrives during this time , we wait again . if no messages arrive , we assume that there are no more messages in flight",
    ". for practical purposes , most message will arrive in the first time window",
    ". there might be a small number of messages arriving in the second time window if there is a slow network switch .",
    "since the infiniband network is quiesced at this point ( because all processes are going through checkpoint ) , no new messages are being scheduled to send . in our experiments",
    ", we used a `` one - second window '' for draining in - flight messages and noticed that all `` pending '' messages arrived within the first window .",
    "no messages arrived in the second window .      while startup time was reasonable for dmtcp with 8k  clients , when running with 16k  clients , some of the clients randomly died .",
    "we were able to diagnose this by creating an artificial application unrelated to dmtcp .",
    "the application similarly created a tcp connection between a coordinator and each client .",
    "we observed a sigkill being sent to each of the random processes that died .",
    "since standard user - space tools failed to indicate the sender of the sigkill , the behavior was reproducible : dmtcp ran well with 8k  clients , but was never observed to run with 16k  clients .    in discussions with the sysadmins , they pointed out that there was a single 10  gb  ethernet backbone from each rack , since the cluster emphasized infiniband over ethernet .",
    "we speculate that the linux kernel had sent the sigkill due to excessive delays seen by the kernel on top of an overloaded ethernet backbone .",
    "we then implemented two standard solutions .",
    "first , a `` staggered sleep '' ( i.e. , network backoff ) was used to avoid bursts of communication over the network when initializing the tcp sockets .",
    "this worked .",
    "however , stampede also sets a per - user limit of 16k sockets per process ( which can be individually overridden by the system administrator on a per - user basis ) .",
    "so , in order to scale to 16k  mpi processes and beyond , we then employed a second solution .",
    "we created a new mode using a two - level tree of coordinators .",
    "specifically , a `` sub - coordinator '' process was created on each computer node to relay messages to the main coordinator . in certain cases ,",
    "the messages of clients were aggregated by the sub - coordinator into a single message in order to optimize network overhead .",
    "as shown in section  [ tbl : launch ] , the launch time improved significantly when using this two - level tree of coordinators .",
    "as is well known , the bottleneck for a transparent checkpoint employing a full - memory dump is the sustained write bandwidth ( sustained transfer rate ) to the storage subsystem .",
    "this observation yields a simple equation for predicting the ideal checkpoint time for a full - memory dump of all of the aggregate ram .",
    "the relationship is well known , and we formalize it here as the checkpoint - fill - time law .",
    "we assume here a write bandwidth ( transfer rate ) of 100 mb / s for a _ single _ disk .    [ cols=\"^,^,^,^,^,^,^,^,^ \" , ]     + 6pt    -6pt",
    "our approach uses a single - threaded central coordinator .",
    "it appears that this design does _ not _ insert a central point of contention . in these experiments ,",
    "the checkpoint coordinator was always run on a separate compute node with no competing processes .",
    "total network traffic on each socket was estimated to be a total of 20  kb during a checkpoint - restart .",
    "this traffic was primarily related to the publish - subscribe database maintained by the infiniband checkpointing code .",
    "nevertheless , the cpu load was always measured at less than 5% of the time for one cpu core .",
    "separately , the approach uses tcp sockets to communicate with the peer processes .",
    "this represents a design flaw at the petascale level .",
    "two issues were encountered .",
    "first , the use of multiple tcp writes without an intervening read forced us to invoke tcp_nodelay to turn off nagle s algorithm  @xcite .",
    "second , there was a need at larger scale to use a staggered sleep ( network backoff ) during initialization of tcp sockets , so that the many peers would not overwhelm the operating system ( or possibly the switch hardware ) in a burst of requests to create new socket connections .",
    "additionally , most linux - based operating systems include a limit on the number of socket connections per process .",
    "our implementation needed to be extended with a tree - of - coordinators so that the many peers connecting to the coordinator would not exceed this limit .",
    "it is in part for this reason that mvapich2 switches to an on - demand connect mode when more than 64  processes are present .",
    "a related issue was encountered specifically for stampede . while a single 10  gb  ethernet backbone is provided for each compute rack , that network is often overloaded . because it is required for the administration of the stampede supercomputer , administrative measures are sometimes taken to kill processes that excessively use the ethernet network for communication .    in an effort to alleviate this",
    ", we experimented with ip over infiniband ( ipoib )  @xcite for all tcp communication , in order to avoid using the ethernet network .",
    "the ipoib implementation at stampede is based entirely on a kernel module , and does not expose the underlying infiniband layer in the user space . for some unknown reason , we found that the stampede system continued to kill processes when there were too many socket connections ( albeit now of the ipoib flavor ) .",
    "also , it was observed that ipoib at stampede used the second , lower - bandwidth port of the hca infiniband adapter .",
    "hence , ipoib provided better latency , but did not provide better bandwidth than tcp over ethernet .      initially , we were worried whether the lustre filesystem could handle the large bandwidth during checkpoint .",
    "as related in section  [ sec : experiment ] , the use of 24,000 cpu cores was allowed on stampede only through a special reservation .",
    "system administrators monitoring the backend performance reported that there was no measurable interference with other concurrent users .",
    "as discussed in  [ sec : runtime - overhead ] , the shadow send / receive queues provide stronger correctness guarantees but impose a significant runtime overhead .",
    "the proposed alternative is to use a heuristic - based approach with relaxed correctness guarantees .",
    "a third alternative is possible if the infiniband device driver can provide an api to `` peek '' into the hardware to learn the current state of the send / receive queues . while being non - destructive , the peek operation could significantly simplify the logic around draining and refilling of the send / receive queues without imposing a runtime overhead .      during restart",
    ", there is an opportunity to use `` mmap '' to map the checkpoint image file back into process memory ( ram ) on - demand .",
    "instead , all memory was copied from the checkpoint image file to process memory ( ram ) during restart . with `` mmap '' , the restart could be significantly faster for a certain class of application that have a smaller working set .",
    "this would allow for some overlap of computation and demand - paging generated file  i / o .",
    "further , there is less of a `` burst '' demand on the lustre filesystem .",
    "this mode was not used , so that the worst - case time for restart could be directly measured .",
    "to the best of our knowledge , the largest previous checkpoint was carried out by cao  @xcite . that work demonstrated transparent checkpoint - restart over infiniband  rc ( but not ud  mode ) for the first time .",
    "scalable results were demonstrated for the nas npb lu benchmark for 2048  mpi processes over 2048  cpu cores .",
    "that work mostly used local disk rather than lustre , showing a maximum i / o bandwidth of 0.05  gb / s when using the local disks of 128  nodes .",
    "( one example with lustre over 512  processes reported an i / o bandwidth of 0.1  gb / s . )",
    "the previous work was demonstrated solely for open  mpi using rc  mode , while today most mpi implementations also take advantage of infiniband ud  mode during initialization .",
    "the most frequently used packages for system - level transparent checkpointing today are blcr  @xcite , criu  @xcite , cryopid2  @xcite , and dmtcp  @xcite . only dmtcp and blcr",
    "are used for checkpointing mpi computations .",
    "dmtcp is the only one of the four that supports transparent checkpointing of distributed computations , and so it supports our current mpi - agnostic approach toward checkpointing .",
    "in contrast , blcr is also often used for checkpointing mpi , but only in combination with an mpi - specific checkpointing service such as @xcite for open  mpi or @xcite for lam / mpi .",
    "blcr can only checkpoint the processes on a single node .",
    "hence , an mpi - specific checkpointing service temporarily tears down the infiniband network , and then uses blcr  @xcite to checkpoint individual nodes as standalone computations .",
    "afterwards , the infiniband connections are re - built .",
    "dmtcp is preferred over the combined use of blcr with an mpi implementation - specific checkpointing service for two reasons : ( a )  it is mpi - agnostic , operating without modification for most mpi implementations ; and ( b )  the alternative checkpointing service that tears down the network can incur long delays when re - initializing the infiniband connections upon resuming the computation and hence limits its performance .    in 2014 , cao  @xcite extended the dmtcp model from transparent support for tcp to include infiniband using the then dominant rc communication mode . that work was demonstrated for open  mpi  1.6",
    " mostly checkpointing to the local disk . they found that checkpointing to the lustre filesystem was 6.5  times faster than without lustre , although restart times were similar in the case of lu.e over 512  cpu cores .",
    "most of that work was done with the dmtcp default of `` on - the - fly '' gzip compression of checkpoint images , and with checkpointing to local disk with 2048  cpu cores",
    ".    there have been several surveys of the state of the art for software resilience in the push to petascale and then exascale computing  @xcite .",
    "one of the approaches is ftc - charm++  @xcite , which provides a fault - tolerant runtime base on an in - memory checkpointing scheme ( with a disk - based extension ) for both charm++ and ampi ( adaptive mpi ) .",
    "three categories of checkpointing are supported : uncoordinated , coordinated , and communication - induced .",
    "because of the potentially long times to checkpoint , a multi - level checkpointing approach  @xcite has been proposed .",
    "the key idea is to support local fault tolerance for the `` easy '' cases , so that a global checkpoint ( potentially including a full - memory dump ) is used as a last resort . since restart from a global checkpoint",
    "are needed less often , such checkpoints may also be taken less often .    a popular application - level or user - level mechanism is ulfm ( user - level failure mitigation ) . by applying recovery at the user - level",
    ", they offer different recovery models , such as backward versus forward , local versus global and shrinking versus non - shrinking .",
    "@xcite reviews the ulfm model , and adds an application - level model based on global rollback .    finally , rmpi ( redundant mpi ) has been proposed for exascale computing  @xcite .",
    "this has the potential to make checkpointing less frequent , and thus allow for longer times to checkpoint .",
    "the authors write , `` note that redundant computing @xmath3 reduces the overhead of checkpointing but does not eliminate it . ''",
    "the authors provide the example of a fully - redundant application for which daly s equation  @xcite predicts a run for 600  hours without failure over 50,000  _ nodes _ with a 5-year mtti / node .",
    "* figure  12 ) ( mtti is mean - time - to - interrupt . )",
    "the need for a fault - tolerance solution for exascale computing has been a long - time concern  @xcite .",
    "this work has demonstrated a practical petascale solution , and provided evidence that the approach scales into the exascale generation .",
    "specifically , system - initiated full - memory dumps for three modern mpi implementations over infiniband have been demonstrated .",
    "this required virtualization of infiniband  ud , since the previous simpler infiniband  rc point - to - point mode did not support modern mpi implementations at scale .",
    "testing on real - world - style applications of namd and hpcg stressed large memory footprints .",
    "the current lustre filesystems successfully supported many - terabyte full - memory dumps . a simple formula in section  [ sec : ssd ] allowed for extrapolation to future ssd - based exascale computers .",
    "the predicted ideal checkpoint time was 1.7  minutes , which extrapolates to under 17  minutes ( ten - fold increase ) after comparing the ideal formula against current supercomputers .",
    "in particular , special permission was received to run hpcg with 24,000 cpu cores ( one - third of the stampede supercomputer ) , and a 29  tb checkpoint image was created in 10.6  minutes .",
    "the system administrator manually monitored this special run , and reported that it did not affect the normal use of i / o by other concurrent users .",
    "we would like to acknowledge the comments and encouragement of raghu raja chandrasekar in integrating dmtcp with mvapich2 .",
    "we would also like to acknowledge zhixuan cao , peter desnoyers , and shadi ibrahim for helpful feedback .",
    "we also acknowledge the support of the texas advanced computing center ( tacc ) and the extreme science and engineering discovery environment ( xsede ) , which is supported by national science foundation grant number aci-1053575 .",
    "we especially would like to acknowledge the assistance of tommy minyard and bill barth from tacc in helping set up and monitor the large experiment there .",
    "also , we acknowledge the resources at the university at buffalo center for computational research ( www.buffalo.edu/ccr ) that were used in performing a portion of the numerical experiments .      e.  n. elnozahy and j.  s. plank , `` checkpointing for peta - scale systems : a look into the future of practical rollback - recovery , '' _ ieee trans . on dependable and secure computing _ ,",
    "vol .  1 , no .  2 , pp .",
    "97108 , 2004 .",
    "f.  cappello , `` fault tolerance in petascale / exascale systems : current knowledge , challenges and research opportunities , '' _ international journal of high performance computing applications _ , vol .",
    "23 , no .  3 , pp .",
    "212226 , 2009 .",
    "m.  snir , r.  w. wisniewski , j.  a. abraham , s.  v. adve , s.  bagchi , p.  balaji , j.  belak , p.  bose , f.  cappello , b.  carlson _",
    "_ , `` addressing failures in exascale computing , '' _ international journal of high performance computing applications _ , 2014 .",
    "f.  liu and j.  b. weissman , `` elastic job bundling : an adaptive resource request strategy for large - scale parallel applications , '' in _ proc . of int .",
    "conference for high performance computing , networking , storage and analysis ( sc15)_.1em plus 0.5em minus 0.4emacm , 2015 .",
    "f.  shahzad , m.  wittmann , t.  zeiser , g.  hager , and g.  wellein , `` an evaluation of different i / o techniques for checkpoint / restart , '' in _",
    "ieee 27th int .",
    "parallel and distributed processing symposium workshops & phd forum ( ipdpsw)_.1em plus 0.5em minus 0.4emieee , 2013 , pp .",
    "17081716 .",
    "j.  hursey , t.  i. mattox , and a.  lumsdaine , `` interconnect agnostic checkpoint / restart in open mpi , '' in _ proc . of the 18th acm int .",
    "symp . on high performance",
    "distributed computing_.1em plus 0.5em minus 0.4emacm , 2009 , pp .",
    "s.  sankaran , j.  m. squyres , b.  barrett , v.  sahay , a.  lumsdaine , j.  duell , p.  hargrove , and e.  roman , `` the lam / mpi checkpoint / restart framework : system - initiated checkpointing , '' _ international journal of high performance computing applications _ , vol .",
    "19 , no .  4 , pp . 479493 , 2005 .",
    "j.  cao , g.  kerr , k.  arya , and g.  cooperman , `` transparent checkpoint - restart over infiniband , '' in _ proc . of the 23rd int .",
    "symp . on high - performance parallel and",
    "distributed computing_.1em plus 0.5em minus 0.4emacm press , 2014 , pp .",
    "j.  ansel , k.  arya , and g.  cooperman , `` dmtcp : transparent checkpointing for cluster computations and the desktop , '' in _ ieee int . symp . on parallel and",
    "distributed processing ( ipdps)_.1em plus 0.5em minus 0.4em ieee press , 2009 , pp .",
    "112 .",
    "a.  moody , g.  bronevetsky , k.  mohror , and b.  r. de  supinski , `` design , modeling , and evaluation of a scalable multi - level checkpointing system , '' in _ 2010 int .",
    "conf . for high performance computing , networking , storage and analysis ( sc10)_.1em plus 0.5em minus 0.4emieee press , 2010 , pp .",
    "y.  liu , r.  nassar , c.  leangsuksun , n.  naksinehaboon , m.  paun , and s.  l. scott , `` an optimal checkpoint / restart model for a large scale high performance computing system , '' in _ ieee int .",
    "symp . on parallel and",
    "distributed processing_.1em plus 0.5em minus 0.4emieee press , 2008 , pp .",
    "d.  tiwari , s.  gupta , and s.  s. vazhkudai , `` lazy checkpointing : exploiting temporal locality in failures to mitigate checkpointing overheads on extreme - scale systems , '' in _",
    "44th annual int .",
    "conf . on dependable systems and networks ( dsn)_.1em plus",
    "minus 0.4emieee press , 2014 , pp . 2536 .    d.  k. panda , k.  tomko , k.  schulz , and a.  majumdar , `` the mvapich project : evolution and sustainability of an open source production quality mpi library for hpc , '' in _ int .",
    "workshop on sustainable software for science : practice and experiences , held in conjunction with int .",
    "conference on supercomputing ( sc13 ) _ ,",
    "november 2013 .",
    "s.  chakraborty , h.  subramoni , j.  perkins , a.  a. awan , and d.  k. panda , `` on - demand connection management for openshmem and openshmem+mpi , '' in _ parallel and distributed processing symposium workshop ( ipdpsw ) , 2015 ieee international _ , may 2015 , pp .",
    "235244 .",
    "k.  arya , r.  garg , a.  y. polyakov , and g.  cooperman , `` design and implementation for checkpointing of distributed resources using process - level virtualization , '' in _ ieee int .",
    "conf . on cluster computing ( cluster04)_.1em plus 0.5em minus 0.4emieee press , 2016",
    ".      j.  h. yoon , `` 3d nand technology  implications to enterprise storage applications , '' in _ proc . of flash memory summit _ , 2014 , memory technology , ibm systems supply chain ; http://www.flashmemorysummit.com/english/collaterals/proceedings/2015/20150811_fm12_yoon.pdf .",
    "j.  c. phillips , y.  sun , n.  jain , e.  j. bohm , and l.  v. kal , `` mapping to irregular torus topologies and other techniques for petascale biomolecular simulation , '' in _ proc . of int .",
    "conf . for high performance computing , networking , storage and analysis_.1em plus",
    "0.5em minus 0.4em ieee press , 2014 , pp . 8191 .      j.  c. phillips , r.  braun , w.  wang , j.  gumbart , e.  tajkhorshid , e.  villa , c.  chipot , r.  d. skeel , l.  kale , and k.  schulten , `` scalable molecular dynamics with namd , '' _ journal of computational chemistry _ , vol .",
    "26 , no .",
    "16 , pp . 17811802 , 2005 .",
    "g.  zheng , l.  shi , and l.  v. kal , `` ftc - charm++ : an in - memory checkpoint - based fault tolerant runtime for charm++ and mpi , '' in _ ieee int .",
    "conf . on cluster computing ( cluster04)_.1em plus 0.5em minus 0.4emieee press , 2004 , pp .",
    "93103 .",
    "i.  laguna , d.  f. richards , t.  gamblin , m.  schulz , b.  r. de  supinski , k.  mohror , and h.  pritchard , `` evaluating and extending user - level fault tolerance in mpi applications , '' _ international journal of high performance computing applications _ , 2016 .",
    "k.  ferreira , r.  riesen , r.  oldfield , j.  stearley , j.  laros , k.  pedretti , and t.  brightwell , `` rmpi : increasing fault resiliency in a message - passing environment , '' sandia national laboratories , tech . rep",
    ". sand2011 - 2488 , apr .",
    "2011 , http://prod.sandia.gov/techlib/access-control.cgi/2011/112488.pdf ."
  ],
  "abstract_text": [
    "<S> fault tolerance for the upcoming exascale generation has long been an area of active research . </S>",
    "<S> one of the components of a fault tolerance strategy is checkpointing . </S>",
    "<S> petascale - level checkpointing is demonstrated through a new mechanism for virtualization of the infiniband  ud ( unreliable datagram ) mode , and for updating the remote address on each ud - based send , due to lack of a fixed peer . </S>",
    "<S> note that infiniband  ud is required to support modern mpi implementations . </S>",
    "<S> an extrapolation from the current results to future ssd - based storage systems provides evidence that the current approach will remain practical in the exascale generation . </S>",
    "<S> this transparent checkpointing approach is evaluated using a framework of the dmtcp checkpointing package . </S>",
    "<S> results are shown for hpcg ( linear algebra ) , namd ( molecular dynamics ) , and the nas npb benchmarks . in tests up to 24,000 mpi processes on 24,000 cpu cores , checkpointing of a computation with a 29  tb memory footprint in 10  minutes </S>",
    "<S> is demonstrated . </S>",
    "<S> runtime overhead is reduced to less than 1% . </S>",
    "<S> the approach is also evaluated across three widely used mpi implementations . </S>"
  ]
}