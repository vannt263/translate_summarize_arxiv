{
  "article_text": [
    "predicting the accuracy of a trained machine learning system when presented with previously unseen test data is a widely studied research topic .",
    "techniques such as cross - validation are well established and understood both theoretically and empirically e.g.  @xcite .",
    "however , these techniques predict the accuracy on unseen data _ given the existing training set_. for example @xmath0-fold cross validation ( @xmath1 ) averages the fitness estimated from @xmath0 runs , each using a proportion @xmath2 of the available data to train a classifier and @xmath3 to evaluate it . therefore repeating with different values of @xmath0 can give the user some indication of how the error rate changed as the training set increased to the current size , since lower values of @xmath0 effectively equate to smaller training sets .",
    "however , @xmath1 does not predict what accuracy might be achievable after further training .",
    "thus if the current accuracy is not acceptable , and obtaining data comes at cost , @xmath1 and similar techniques do not offer any insights into whether it is worth incurring the cost of further training .",
    "this is of more than theoretical interest , because the successful application of machine learning techniques to `` real - world '' problems places various demands on the collaborators .",
    "not only must the management of the industrial or commercial partner must be sufficiently convinced of the potential benefits that they are prepared to invest money in equipment and time , but vitally , there must be a significant investment in time and commitment from the end - users in order to provide training data from which the system can learn .",
    "this poses a problem if the system developed is not sufficiently accurate , as the users and management may view their input as wasted effort , and lose faith with the process .    in some cases",
    "this effort may be re - usable  for example , the user has been labelling training examples that can be stored in their original form , and which come from a fairly stationary distribution . however , this is frequently not the case .",
    "for example , in many applications it may not be practical to store the physical training examples , rather it is necessary to characterise them by a number of variables .",
    "if the failure of the machine learning system in such cases stems from an inappropriate or inadequate choice of descriptors , then the whole process must be repeated .",
    "not only has the user s input been a costly waste of time and effort , but there may be a loss of faith in the process which can manifest in reduced attention and consistency when classifying further samples . to give a concrete example from the field of diagnostic visual inspection ( e.g.  manufacturing process control or medical images ) , it frequently turns out that it is not sufficient to store each relevant image  other information is necessary such as process variables , or patients history . if this data is not captured at the same time , and is not recoverable post - hoc , then the effort of collecting and labelling the database of examples has been wasted",
    ".    a significant factor that would help in gaining confidence and trust from end - users would be the ability to quickly and accurately predict whether the learning process was going to be successful . perhaps more importantly from a commercial viewpoint",
    ", it would be extremely valuable to have an early warning that the users can save their effort while the system designer refines the choice of data , algorithms etc .    in this paper",
    "we investigate a technique for making such early predictions of future error rates .",
    "we will consider that we are given @xmath4 samples , and that the system is still learning and refining its model at this stage .",
    "we are interested in predicting what final accuracy might be achievable if the users were to invest the time to create @xmath5 more samples .",
    "this leads us to focus on two questions .",
    "first , what are the most appropriate descriptors of the system s behaviour after some limited number @xmath4 of samples , and then later after an additional @xmath5 samples ?",
    "second , is it possible to find useful relationships for predicting the second of these quantities from the first ?    theoretical studies , backed up by empirical results , have suggested that the total error rate follows a power - law relationship , diminishing as extra training samples are provided . while these theoretic bounds on error are rather loose , they provide motivation for investigating practical approaches for quickly and reliably estimating the error rate that may be observed after future training . in general",
    "the error will be a complicated function , but the hypothesis of this paper is that we can deal with it more easily if we decompose it into a number of more stable functions .",
    "therefore this paper concentrates on the use of the well - known bias - variance decomposition @xcite as a source of predictors when an algorithm is used to build a classification model from a dataset .",
    "specifically , our hypothesis is that if the observed error is decomposed into bias and variance terms , then although these components may behave differently , their behaviour may be individually predictable .    to test our hypothesis",
    "we first apply a range of algorithms to a variety of datasets , for each combination periodically estimating the error components as more training samples are introduced , until the full dataset has been used .",
    "all of the data arising from this ( rather lengthy ) process is merged and regression analysis techniques are applied to produce three sets of predictive models - one each for bias , variance and total error . each of these models takes as input a measurement obtained from the classifier produced when only a few samples ( @xmath4 ) from a dataset have been presented to the learning algorithm , and predicts the value after all samples have been applied ( @xmath6 ) . as the data has been merged ,",
    "the intention is that these models are algorithm - dataset independent .",
    "we examine the stability and valid range of these models , and evaluate their predictive power when using previously unseen datasets and algorithms . moving on to consider trainable ensembles of different classifiers , we show how a similar approach can be applied to obtain estimates on the upper bound of the achievable accuracy , which can predict the progression of the ensemble s performance .",
    "the rest of this paper proceeds as follows . in section  [ sec : review ] we review related work in the field , including the bias - variance decomposition of error that we will use . following that",
    ", section  [ methodology ] describes the experimental methodology used to collect the initial statistics , and test the resulting models .",
    "sections  [ single_results ] and  [ unseen ] describe and discuss the results obtained . in section  [ ensembles ] we show how this approach may be extended to predict the future accuracy of trainable ensembles of classifiers .",
    "finally in section  [ conclusions ] we draw some conclusions and suggestions for further work .",
    "for the sake of clarity we will use a standard notation throughout this paper , reinterpreting results from other authors as necessary .",
    "we assume classification tasks , where we are given an instance space @xmath7 and a predicted categorical variable @xmath8 .",
    "the `` true '' underlying function @xmath9 is a mapping @xmath10 .",
    "let @xmath11 be the set of all possible training sets of size @xmath4 sampled from the instance space @xmath7 , and @xmath12 .",
    "when a machine learning algorithm @xmath13 is presented with @xmath14 it creates a classifier , which we may view as a hypothesis about the underlying mapping : @xmath15 .",
    "the subscripts @xmath13 and @xmath14 make it explicit that the specific classifier @xmath16 induced depends on the learning algorithm and the training set . for a specific learning algorithm @xmath13 the set of classifiers that it can induce",
    "is denoted @xmath17 .",
    "we consider a @xmath18 misclassification error - in other words the error is zero if @xmath16 correctly predicts the true class of an item @xmath19 , and 1 otherwise .",
    "more formally the misclassification cost of a single data item @xmath20 with a specific classifier @xmath16 is : @xmath21 the expected error of the classifier created from @xmath4 data points is then given by integrating over @xmath7 and @xmath14 , taking into account their conditional likelihood , i.e. : @xmath22    in practice of course it is not possible to exactly measure the true error , so approaches such as bootstrapping , hold - out and cross - validation are used to estimate the error , given a finite sized set of examples .",
    "we will use the lower case @xmath23 to denote an estimation is being used for the true error .",
    "cortes _ et al _",
    "@xcite presented an empirical study where they characterised the behaviour of classification algorithms using `` learning curves '' .",
    "these suggest that the predicted error of the classifier after @xmath4 samples have been presented will follow a power - law distribution in @xmath4 : @xmath24 where the constants @xmath25 ( the learning rate ) , @xmath26 ( the decay rate ) and @xmath27 ( the asymptotic bayes error rate ) depend on the particular combination of classification algorithm and data set , but @xmath26 is usually close to , or less than one .",
    "this suggests that given a particular classifier - dataset combination , it should be possible to commence training , take periodic estimates of the error as @xmath4 increased , and then use regression to find values for @xmath28 that fit the data , and can be used for predicted future error rates .",
    "et al _ @xcite have pointed out a problem with this approach , namely that for low values of @xmath4 the estimated error rates are subject to high variability , which leads to significant deviations when fitting the power - law curve .",
    "they have presented an extension of the method which uses a `` significance permutation test '' to establish the significance of the observed classifier error prior to curve fitting .",
    "these results fit in with theoretical bounds from `` probably approximately correct '' ( pac ) theory such as those presented by vapnik in @xcite .",
    "these begin with the assumption that a training set @xmath29 is drawn independently and identically distributed ( iid ) from a data set , and that future training and test data will be drawn from the dataset in the same way .",
    "given the restriction @xmath30 , the test error @xmath31 , ( the probability of misclassification ) is defined to be : @xmath32\\ ] ] where the division by two maps differences in @xmath8 onto costs , and the @xmath33 denotes that we are taking the expectation for the general case .",
    "the current empirically measured training error @xmath34 is : @xmath35    if @xmath36 represents the vc - dimension of classification algorithm @xmath13 , and @xmath37 vapnik @xcite showed that with probability @xmath38 : @xmath39    effectively this equation makes explicit an assumption that machine learning algorithms inherently produce classifiers which overfit the available training data .",
    "the vc - dimension @xmath36 may be thought of as the `` power '' of the machine learning algorithm @xmath13  it is the maximum number of points that can be arranged so that @xmath13 can `` shatter '' them .",
    "equation  [ eq : vc - dim - error ] makes it clear that more powerful algorithms ( higher @xmath36 ) are more likely to overfit the data , and so it may be used , for example , as grounds to select between two algorithms which produce the same training error but have different complexity ( related to @xmath36 ) .",
    "it also makes explicit the dependency on @xmath4 : for a given training set error , the maximum amount by which this will underestimate the true error decreases by approximately @xmath40 .",
    "however , in practice these bounds tend to be rather `` loose '' .",
    "there have been other more recent developments in statistical learning theory which use a similar approach but exploit rademacher complexity to provide tighter bounds , such as those in @xcite .",
    "common to all of these approaches , as with the use of vc - dimension results , is the idea that on the basis of the available training data , an algorithm selects a classifier @xmath41 from some class @xmath17 available to it . to analyse the learning outcomes , the `` error '' observed when the training data is classified by @xmath41",
    "is broken down into the bayes optimal error ( which can not be avoided ) plus an amount by which best ( @xmath42 ) in the current class of classifiers would be more than bayes optimal , plus an amount by which the classifier @xmath41 currently estimated by the algorithm to be `` best '' is different to the actual best @xmath43 .",
    "thus for example , approaches such as structural risk minimisation can be thought of as principled methods for increasing the size / complexity of the current class of classifiers @xmath17 until it includes the bayes optimal classifier .",
    "the underlying assumption is that the error is estimated using the current training set , and that this almost certainly overfits the true underlying distribution ( i.e. @xmath44 ) so the current estimates of error for the chosen classifier @xmath41 will be less than the `` true '' error that would be seen if it was applied to the whole data distribution .",
    "therefore bounds are derived which describe the extent to which the error on the training set underestimates the true error . since",
    "this can be described in terms of the search problem of identifying @xmath42 , it is understandable that they take into account the amount of information available to the search algorithm - i.e. the size @xmath4 of the training set .",
    "while this is a valid and worthwhile line of theoretical research , we would argue that it is not currently as useful for the practioner .",
    "consider the example of a user who is highly skilled in their domain , but knows nothing about machine learning , and is providing the training examples from which a classifier is constructed .",
    "the theory above effectively says : _",
    "`` based on what you have told me , i ve built a classifier which seems to have an error rate of @xmath20% .",
    "i can tell you with what probability the ' ' true `` error rate is worse than @xmath45% , for any positive @xmath46''_. if they have provided enough labelled data items to create what appears to be an accurate classifier , then this is valuable .",
    "however , if they are still early on in the process , and the current error rates are high , it gives no clues as to whether they will drop .",
    "instead we attempt to provide heuristics that answer a different question : _ `` based on what you have told me , i ve built some classifiers and although the current error rate is @xmath20% it will probably drop to @xmath46% where @xmath47 . '' _    to do this , we note that the analysis above relates the true test error to a specific estimated error from a given training set size , and as discussed above the variance in the predicted error depends strongly on @xmath4 . this has prompted us to examine different formulations that explicitly decompose the error into terms arising from the inherent bias of the algorithm ( related to its vc - dimension , or to the difference between @xmath43 and the bayes optimal classifier ) ) and the variability arising from the choice of @xmath48 .",
    "a number of recent studies have shown that the decomposition of a classifier s error into bias and variance terms can provide considerable insight into the prediction of the performance of the classifier @xcite .",
    "originally , it was proposed for regression @xcite but later , this decomposition has been successfully adapted for classification @xcite . while a single definition of bias and variance is adopted for regression , there is considerable debate about how the definition can be extended to classification @xcite . in this paper",
    ", we use kohavi and wolpert s @xcite definition of bias and variance on the basis that it is the most widely used definition @xcite , and has strictly non - negative variance terms .",
    "kohavi and wolpert define bias , variance and noise as follows @xcite :    squared bias : : :    `` this quantity measures how closely the learning algorithm s average    guess ( over all possible training sets of the given training set size )    matches the target '' .",
    "variance : : :    `` this quantity measures how much the learning algorithm s guess    bounces around for the different training sets of the given size '' . intrinsic noise : : :    `` this quantity is a lower bound on the expected cost of any learning    algorithm . it is the expected cost of the bayes - optimal classifier '' .    given these definitions , we can restate eq .",
    "[ eq : error - general ] as :    @xmath49    assuming a fixed cardinality for @xmath8 ( finite set of classes ) , and noting @xmath11 has finite cardinality , the summation terms are : + @xmath50 ^ 2 , \\nonumber \\\\",
    "variance_{x } & = & \\frac{1}{2}-\\frac{1}{2}\\sum_{y \\in y } \\sum_{d \\in d}p(d|f , n)p(h_{cd}(x)=y)^2 , \\nonumber \\\\",
    "\\sigma_{x}^{2 } & = & \\frac{1}{2}-\\frac{1}{2}\\sum_{y \\in y } p(f(x)=y)^2 .",
    "\\nonumber\\end{aligned}\\ ] ] where the terms @xmath51 make explicit that some terms are conditional probability distributions since the bayes error may be non - zero , the classification output may not be crisp , and the specific choice of training set depends on the underlying function and the number of samples .",
    "an alternative perspective on the above analysis is that the bias term reflects an inherent limit on a classifier s accuracy resulting from the way in which it forms decision boundaries .",
    "for example , an elliptical class boundary can never be exactly replicated by a classifier which divides the space using axis - parallel decisions .",
    "a number of studies have been made confirming the intuitive idea that the size of variance term drops as the number of training samples increases , whereas the estimated bias remains more stable , e.g.  @xcite .",
    "therefore we can treat the sum of the inherent noise and bias terms as an upper limit on the achievable accuracy for a given classifier .",
    "noting that in many prior works it is assumed that the inherent noise term is zero , and that for a single classifier it is not possible to distinguish between inherent noise and bias , we hereafter adopt the convention of referring to these collectively as bias .",
    "the hypothesis of the main part of this paper is that values of the bias and variance components estimated after @xmath4 training samples can be used to provide accurate predictions for their values after @xmath6 samples , and hence for the final error rate observed . to do this prediction we use statistical models built from a range of dataset - algorithm combinations . to create the model data we repeatedly draw training and test sets from the @xmath4 samples from which we can estimate the total error , together with its bias and variance components",
    "this raises the issue of how we should do this repeated process .",
    "if the variables in @xmath7 are continuous , or unbounded integers , then the underlying distribution over which the classifier may have to generalise is of course infinite . for bounded integer or categorical variables ,",
    "the number of potential training sets of size @xmath4 drawn iid from an underlying distribution of @xmath7 is of size @xmath52 , so in practice even for non - trivial datasets it is not possible to evaluate all possible training sets @xmath14 of size @xmath4 .",
    "however the success ( or otherwise ) of the approach proposed in this paper depends on the accuracy with which we can predict error components , particularly for when the training set sizes are low .",
    "this immediately raises the question of finding the most appropriate methodology for estimating the values of those quantities . to give a simple example of why this is important",
    ", a later result in this paper partially relies on being able to distinguish between those data items that are _ always _ going to be misclassified by a given classifier , and those which will _ sometimes _ be misclassified , depending on the choice of training set .",
    "since the well known @xmath0-fold cross - validation approach only classifies each data item once , it does not permit this type of decomposition and can not be used . in a preliminary paper @xcite we have examined two possible approaches : the `` hold - out '' method proposed by kohavi and wolpert @xcite and the `` sub - samples cross validation '' ( sscv ) method proposed by webb and conilione @xcite .",
    "the latter have argued that the hold - out approach proposed in @xcite is fundamentally flawed , partly because it results in small training sets , leading to instability in the estimates it derives .",
    "this was confirmed by our results @xcite which showed that the stability of the estimates , and hence the accuracy of the resulting prediction was far higher for the sub - sampling method .",
    "therefore we restrict ourselves to this approach .",
    "the sscv procedure is designed to address weaknesses in to both the hold - out and bootstrap procedures by providing a greater degree of variability between training sets .",
    "in essence , this procedure repeats @xmath0-fold cv @xmath53 times , thus ensuring that each sample @xmath20 from the training set of size @xmath4 is classified @xmath53 times by the classifier @xmath54 .",
    "the true @xmath55 and @xmath56 can be estimated as @xmath57 and @xmath58 from the resulting set of classifications .",
    "the final bias and variance is estimated from the average of all @xmath59 @xcite , thus using all @xmath5 samples .",
    "the following sections describe our choice of experimental methodology , algorithms and data sets",
    ". please note the distinction between those datasets and algorithms used to provide the data from which the statistical models were built , and those which were only used for evaluation purposes .      in order to obtain the data for modelling ten different classification algorithms",
    "were selected , each with different bias and variance characteristics .",
    "these were : naive bayes ( nai ) @xcite , c4.5 ( c4.5 ) @xcite , nearest neighbour ( 1nn ) @xcite , bagging ( bag ) @xcite , adaboost ( ada ) @xcite , random forest ( raf ) @xcite , decision table ( dtb ) @xcite , bayes network ( ban ) @xcite , support vector learning ( smo ) @xcite , and ripple - down rule learner ( rid ) @xcite .",
    "note that this set includes two methods for creating ensembles : adaboost ( using decision stumps as the base classifier ) and bagging ( using a decision tree with reduced error pruning ) . in these cases , since we are solely interested in the outputs , we treat the ensemble as a single entity , rather than attempt a bias - variance - noise - covariance decomposition .    in the evaluation",
    "we investigate how well the models can extrapolate when new classifiers are used .",
    "five classifiers , again with different bias - variance trade - offs , are used for this analysis namely cart ( cart ) @xcite , randomsubspace ( rss ) @xcite , logistic ( log ) @xcite , knn with @xmath60 ( 3nn ) , and complement naive bayes ( cnb ) @xcite .",
    "all these classifiers are implemented in the weka library @xcite , and the default parameters in weka are used for each classifier .",
    "the data collection required to build the statistical models is carried out on data sets derived from four artificial and five real - world visual surface inspection problems from the dynavis project .",
    "each artificial problem consists of 13000 contrast images created by a tuneable randomised image generator .",
    "class labels ( good / bad ) were assigned to the images by using different sets of rules of increasing complexity acting on the generator .",
    "the real world data sets came from cd - imprint and egg inspection problems .",
    "there are 1534 cd images , each labelled by 4 different operators , and 4238 labelled images from the egg inspection problem .",
    "the same set of image processing routines are applied to segment and measure regions of interest ( roi ) in each image . from each set of images",
    "are derived 2 data sets .",
    "the first has 17 features describing global characteristics of the image and the roi it contains . in the second these",
    "are augmented by the maximum value ( over all the roi ) for each of 57 roi descriptors .",
    "adding the labels available provides a total of 18 different data sets with a range of dimensionality and cardinality .    to build the models we used 14 of the data sets : the six derived from the first three artificial image sets , the six from the cd images labeled by the first three operators and the two from the egg data .",
    "the remaining four data sets , derived from the fourth artificial image set , and the cd labelled by operator 4 are reserved for evaluation purposes , as are three example datasets selected from the uci repository @xcite .    in each case",
    "we took @xmath61 , so @xmath5 differs between data sets .",
    "( 30,15 ) ( 2,13)@xmath4 samples ( 2,12)sscv ( 2.5,10)(2,1)5    ( 7.5 , 10 )    ( 10,5 )    [ cols= \" < \" , ]     ( 17.5,2.5)(1,0)5 ( 23,2.1)*compare *    figure  [ fig : unseendata_accuracy ] shows the observed errors and the predicted values with and without bias - variance decomposition using the 10 different classifiers described in section  [ methodology : choice ] and seven previously `` unseen '' datasets",
    ". the first four are image processing datasets artificial  04 and cd - operator  4 respectively with the two sizes of feature space . to investigate",
    "how well the models can extrapolate when the initial observed accuracies lie outside the range of values ( @xmath62 ) used to build the models , and when the datasets come from very different problems described by different numbers of features , we also used three well known data sets from the uci repository @xcite . these were satimage ( 4435 samples , 36 features ) , segment ( 2310 , 18 ) and cmc ( 1473 , 9 ) .    as can be seen",
    ", there is a close alignment between predicted and observed values and the method correctly indicates those cases ( e.g.  adaboost - satimage , adaboost - segment ) where the accuracies are low .",
    "this is a good example of providing an `` early warning mechanism '' that some remedial action might be needed . in general the predictions made via decomposition",
    "are more accurate than those made directly using the error - error regression models . specifically , the predictions of bias are highly accurate but there is a tendency for the variance terms to be overestimated , leading to an overestimation of the combined error .",
    "the most noticeable errors in prediction occur on the cmc dataset which has 9 features ( so is relatively `` dense '' ) and a total of 1437 data items . in this case",
    "there is a noticeable effect that the variance term is underestimated for most classifiers .",
    "this could be because the variance term is known to decrease with @xmath5  as echoed by the model @xmath63  and the value of @xmath5 for this dataset ( 473 ) was lower than for any of the data used to build the model .",
    "however , this effect of overestimating the variance is not apparent for the cd datasets ( @xmath64 ) .",
    "a more likely explanation is that the cmc dataset shows higher errors than were used to create the initial regression models , which suggests a weakness in extrapolation .",
    "it remains for future research to examine whether adding additional data during the model building process would create better linear regression models , or cause a need for more complex models .",
    "samples ( 1st , black bar)with that predicted from @xmath65 samples using decomposed ( middle , red bar ) or direct ( right , green bar ) prediction . for observed error and decomposed predictions , stacking within bars shows bias and variance components .",
    "[ fig : unseendata_accuracy ] ]     and the correlation coefficient between observed and predicted errors using ten `` known '' classifiers .",
    "[ fig : unseendata_correlation ] ]    to quantify the accuracy of the predictions , for any given dataset , we can pool the results for the ten classifiers , perform a regression analysis and then calculate the coefficient of determination ( @xmath66 ) between the observed and predicted values of error , bias and variance .",
    "figure  [ fig : unseendata_correlation ] shows the progress of @xmath66 against @xmath4 . because of the small number of samples , there is a lot of `` noise '' and the curves are not monotonically non - decreasing .",
    "however , common patterns can be seen  in each case the four values of @xmath66 rise fairly steadily with @xmath4 , and in most cases reach values of @xmath67 before @xmath68 .",
    "the exception is the variance for satimage , which is consistently low .",
    "notably , for the larger datasets ( artif , segment and satimage ) the correlations between the observed errors for different classifiers , and those predicted via decomposition has @xmath69 for @xmath70 , and in fact @xmath71 for @xmath72 . in every case except cmc , the correlation is consistently higher for decomposed error predictions than for direct predictions . to conclude , when using 1000 samples to make predictions of the classifier accuracy attained after using the full training set , with a set of ten diverse algorithms and seven datasets , the coefficient of determination between the predicted and observed errors is greater than 90% .",
    "to further evaluate the predictive performance , we treated each classifier - dataset combination as a potential item with pairs of ( predicted - observed ) measurements for bias , variance and total error .",
    "since there may in principle be a huge number of possible datasets used , it is reasonable that all of these measurements may be considered samples from an underlying normal distribution , and therefore it is appropriate to use paired samples t - tests .",
    "these confirm that at the 95% confidence level , the deviations for variance and directly predicted - observed error are significant .",
    "in contrast , there is a less than 0.01% probability that the deviations for bias and error predicted via decomposition and are only significant .",
    "samples ( middle , blue bar ) with that predicted from @xmath65 samples using decomposed ( right , green bar ) or direct ( left , red bar ) prediction . for observed error and decomposed predictions , stacking within bars shows bias and variance components .",
    "[ fig : error - unseenboth ] ]     and the correlation coefficient between observed and predicted errors using `` unknown '' data sets and algorithms .",
    "[ fig : unseenboth_correlation ] ]    we next investigate how well the models can extrapolate when new algorithms are used to build classifiers , i.e.  ones not used during training .",
    "five algorithms are used for this analysis , namely cart ( cart ) @xcite , randomsubspace ( rss ) @xcite , logistic ( log ) @xcite , knn ( 3nn ) , and complement naive bayes ( cnb ) @xcite .",
    "all these are implemented in the weka library @xcite , and the default parameters in weka are used for each .",
    "figure  [ fig : error - unseenboth ] compares the final observed error to the predictions made with and without bias - variance decomposition for the 5 new algorithms building classifiers for the seven `` unseen '' datasets . as seen before ,",
    "in almost every case the use of the separate model for bias - variance provides better estimates of the error than than the simple error model without decomposition . as before ,",
    "the exception is the cmc . nevertheless , in all cases , the predicted error is near to the actual error  note in particular the complement naive bayes ( cnb )  satimage and segment results .",
    "again we hypothesise that the _ relatively _ poor predictive accuracy on the cmc dataset arises from the very high bias components , and inaccurate extrapolation by the regression model from its original data to these high values .",
    "nevertheless it is worth pointing out that the decomposed approach correctly predicts the final rank order of the five classifiers .",
    "figure  [ fig : unseenboth_correlation ] shows the coefficient of determination ( @xmath66 ) between predicted and observed error as a function of the number of initial samples .",
    "again , the small number of observations ( 5  one for each unseen classifier ) for each value of @xmath4 cause some noise , but the correlation is high and stable for the artif-17f , artif-74f , satimage and segment data sets , rises for the artif-74f and cd-74f sets , and is more variable so for cmc , where the variance models do not perform well .    running the paired samples t - test as before showed that with more than 95% confidence the differences between predcited and observed values are not significantly different except for variance .    as a further way of analysing the data , the `` relative '' differences ( i.e. @xmath73 were calculated and plotted for the variables error , bias and variance .",
    "figure  [ fig : relative - byq ] shows plots of these values against the size of the `` extra '' data @xmath5 , with colours and markers to distinguish between data sets and classifiers .",
    "note the logarithmic @xmath20-axis , and the different scales on the y - axes which somewhat exaggerate the deviations .",
    "visually , there appears to be a slight trend towards overestimating bias that increases with the value of @xmath5 , and this causes a lesser but corresponding trend in the behaviour of the error predicted via decomposition .",
    "there is no apparent pattern for variance .",
    "however the influence of these trends are not borne out by statistical analysis  performing a linear regression showed a near zero correlation ( @xmath74 ) for each variable .",
    "the concept of decomposing error into different terms has also been used to help explain the behaviour of ensembles of algorithms . when the algorithms concerned are performing regression tasks , decomposing the error of an ensemble into terms representing the mean bias and variance of the individual algorithms , and the covariance between them is fairly straightforward .",
    "a good recent survey of both the bias - variance - covariance and ambiguity decompositions may be found in the first few pages of @xcite .",
    "however , just as defining bias and variance for 0/1 loss functions was non - trivial , and there were several versions before kohavi and wolpert @xcite created their formulation in which variance is always non - negative , the extension to handle covariance in a natural way is also problematic . to the best of our knowledge",
    "there has not been a successful model decomposing 0/1 loss functions for ensembles of classifiers , so it is not immediately possible to simply extend the approach we took for single classifiers .",
    "however , in this section we present some initial findings from an approach in which we treat the entire ensemble as a single classifier . revisiting the definitions of bias in section  [ bv_decomposition ] ,",
    "we next develop predictors for upper limits on its attainable accuracy based on simple observations of the behaviour of the individual classifiers in the ensemble .",
    "the analysis in section  [ bv_decomposition ] used a very general model predicated on the fact that the data items @xmath20 could be drawn from a large , potentially infinite universe of samples , corresponding to unlimited future use of the classifiers .",
    "here we are concerned with the more limited case where our future estimates are still drawn from a finite set of size @xmath75 .",
    "in particular we consider whether we can predict the values of those estimates , before completing the training process . in order to achieve this",
    "we can reformulate the models above slightly as follows .    to start with ,",
    "let us assume that we have a finite set @xmath7 of sample data points .",
    "for consistency with above note that @xmath76 . because we are treating the ensemble as a single high - level entity , we need not worry about the effects of boosting or bagging approaches to creating ensembles by repeatedly sampling from training sets .",
    "therefore , we assume that at our higher level training sets of size @xmath77 are created by sampling from @xmath7 uniformly without replacement .",
    "let @xmath11 denote the set of training sets created in this way , and @xmath14 be any member of @xmath11 , then we note that under these conditions @xmath78 .",
    "now let @xmath79 partition @xmath7 such that @xmath80 , where :    * @xmath81 is the ( possibly empty ) subset of data items where for all training sets a classifier trained on that set correctly predicts the class of item @xmath20 .",
    "@xmath82 * @xmath83 is the ( possibly empty ) subset of data items where for all training sets a classifier trained from that set incorrectly predicts the class of item @xmath20 . @xmath84 * @xmath85 is the ( possibly empty ) set of data items where @xmath86 , the hypothesis describing the predicted class of item @xmath20 depends on the choice of training sets @xmath14 .",
    "@xmath87    so now lets look at what this means in terms of our estimates of the bias of the classifier .",
    "this will of course depend on the methods used for the estimates .",
    "following well - established previous research , we will assume that each item in the data set is predicted exactly @xmath88 times .",
    "this is true with @xmath89 for @xmath0-fold cross - validation , and for @xmath90 for the webb and collione approach , in general although interestingly not for the kohavi approach @xcite .",
    "this means that when we sum over the data items @xmath20 in the counterpart of eq .",
    "[ kohaviandwolpert ] each term occurs with equal probability .",
    "note that @xmath91 as stated above is composed of terms which themselves depend on the choice of training sets , and that we are assuming a fixed set of data points and a fixed size training sets .",
    "we therefore refine the definition of bias to take these into account , and average over all possible training sets .",
    "@xmath92 ^ 2\\ ] ]    if we assume we are sampling iid then @xmath93 and @xmath94 .",
    "we now turn our attention to the case where each data item @xmath19 is unambiguously associated with one of two possible class labels @xmath95 , and we will further constrain our ensemble to output crisp decisions so that @xmath96 . partitioning the data set @xmath7 as above , we note that we make use of the following conditions when performing the summation . first , the set @xmath81 does not contribute to the bias since the predicted class for this subset of items is always correct .",
    "second , @xmath97 .",
    "this means that within the partition @xmath83 for each combination of @xmath20 and @xmath14 , there are exactly two values of @xmath46 which both contribute + 1 to the summation .",
    "this yields : @xmath98 ^ 2 \\\\               & = & \\frac{|a^-|}{|x| } + \\frac{1}{2}\\frac{|b|}{|x| } \\cdot \\frac{n!(|x|-n)!}{|x|!}\\sum_{x ,",
    "d}\\sum_{y \\in y}[p(f(x)=y)-p(h_{cd}(x)=y)]^2\\end{aligned}\\ ] ] the last term will take a value between 0 and @xmath99 since for each value of @xmath46 the difference will be 0 for some training sets and 1 for others which the gives bounds : @xmath100    this reformulation makes it explicit that considering the proportion of samples which the ensemble always misclassifies will yield a strict underestimate of the bias provided that there exist any items for which the prediction made is dependent on the training set . furthermore , since according to eq .",
    "[ kohaviandwolpert ] the variance term is always non - negative , we can say that the quantity @xmath101 constitutes a strict lower bound on the error rate of a classifier  or an ensemble treated as a single entity .",
    "previous sections illustrated the successful use of regression models built from a variety of dataset - classifier combinations to predict the error rates that could be attained after future training .",
    "however , decomposing the error into different components is not straightforward for ensembles of classifiers @xcite .",
    "moreover , this would require running @xmath0-fold cross validation a number of times to get accurate estimates of bias and variance components for each combination of dataset , algorithm , and @xmath4 .",
    "this becomes computationally expensive when extended to a heterogeneous ensemble , particularly if the ensemble is itself trainable .",
    "for this section we use a slightly different approach .",
    "previously we pooled the results from many experiments to build regression models relating observations of bias and variance after different values of @xmath4 training data to the same variables of @xmath6 items . here",
    "we treat each data set independently , and built regression models to characterise the ensemble s learning curve as a function of @xmath4 .",
    "as noted above , there is theoretical @xcite as well as empirical @xcite evidence that these learning curves have a power - law dependency on the number of training samples , i.e.  they are of the form @xmath102 where @xmath25 is the learning rate , @xmath27 the decay rate and @xmath103 the bayes error ( the minimum achievable error or , in the error - decomposition framework , the `` noise '' ) .    in our experiments ,",
    "the bound on the ensemble s error derived in section  [ sec : ens - lower_bounds ] , @xmath101 , was used as an estimate of the minimum achievable error . when faced with a new dataset - ensemble combination , we make observations of @xmath104 and the ensemble error at regular intervals , and then feed these into the power - law regression model in order to fine - tune the parameters of the model so that it fits the new data and predicts the future development of the ensemble error , as will be detailed in section  [ sec : results - learning_curves ] . before elaborating on these results , in section  [ sec : results - oracle ]",
    "we will provide an analysis of the stability of the estimation of the lower bound on the error by using @xmath101 .      for the experiments performed here",
    "22 machine vision datasets from the dynavis project were used ( 2 different feature spaces  17 and 74 features  for each of 5 cd - print , 5 artificial , and the egg image sets ) .",
    "the cart @xcite and c4.5 @xcite decision trees , the naive bayes @xcite , nearest neighbour @xcite and evq @xcite classifiers were used as base classifiers , the decisions of which were combined using the discounted dempster - shafer ensemble training method @xcite . for each data",
    "set , each classifier , and each value of @xmath105 samples , @xmath0-fold cross validation was repeated @xmath53 times to make @xmath53 predictions of the class of each item in the training set . from this data we calculated the values of @xmath101 as a function of @xmath4 for each data set ( i.e.  22 values for each value of @xmath4 ) . for clarity",
    "we denote the values @xmath101 hereafter as @xmath106 .    in order to examine the stability of the predicted bounds as @xmath4 increased , we plotted @xmath106 against @xmath107 and used linear regression as before to fit a model of the form @xmath108 , and to estimate the quality of the model via @xmath66 .",
    "figure  [ fig : oracle_progression ] shows the progression of the coefficients @xmath109 and @xmath110 and the corresponding values of @xmath66 as a function of @xmath4 .     vs.  @xmath107 together with the coefficient of determination @xmath66 as a function of @xmath4 .",
    "[ fig : oracle_progression ] ]    as can be seen in figure  [ fig : oracle_progression ] , the models generated as @xmath4 increases do produce predictions which correlate well to the observed values after further training .",
    "however , as can be seen by the progression of the coefficients , the nature of the regression models changes . for low values of @xmath4",
    "the models predict a high constant value for @xmath107 with a low component related to the observed value of @xmath106  essentially the system has not seen enough `` difficult '' samples .",
    "since the major component of the predicted value of @xmath107 is fixed for @xmath111 , the correlation is fairly low . as @xmath4 increases and a more representative sample of the data is seen , the situation changes .",
    "thus for training set sizes @xmath112 the predicted value is dominated by the observed value ( @xmath113 ) with only a low constant component ( @xmath114 ) . for these training set sizes @xmath66 increases to approximately 0.9 .",
    "the values @xmath106 for different @xmath4 can be used for predicting not just a lower bound on , but also an estimate of the error of a trained ensemble .",
    "the following procedure can be used :    1 .",
    "@xmath106 is measured for different @xmath4 and a constant regression is performed for these values , i.e.  we obtain the constant @xmath115 which minimises the mean square error with the values of @xmath106 across different values of @xmath4 .",
    "this value forms our estimate of the lower bound on the achievable error .",
    "the errors the ensemble makes are also recorded for different @xmath4 .",
    "a power - law regression is performed for the ensemble errors , asymptotically approaching the estimated constant @xmath115 as calculated in step 1 : @xmath116 where @xmath25 and @xmath27 are the regression parameters which are optimised in the regression procedure .",
    "an analogous procedure is used to model the standard deviation of the observed values .",
    "5 .   from the power - law regression model",
    "we can estimate the error of the ensemble after @xmath6 samples are presented , and also some estimates of how the variation changes .",
    "the results of this procedure are illustrated in figure  [ fig : oracle_example ] .",
    "the five base classifiers listed above are combined using the discounted dempster - shafer combination ensemble @xcite .",
    "@xmath106 was measured for @xmath117 samples .",
    "a constant regression was performed to model @xmath106 with a constant value and the obtained value is then used as an asymptote when modelling the ensemble errors .",
    "the errors the ensemble makes are again recorded for @xmath117 samples and a ( robust ) regression model is built according to equation  [ eq : regress ] .",
    "the results of this procedure are illustrated in figure  [ fig : oracle_example_sony4 ] for cd - operator  4 and in figure  [ fig : oracle_example_artif08 ] for artificial  04 .",
    "the values @xmath106 and the errors of the ensemble are shown for different @xmath4 , as well as the regression models that are built for them , together with the estimated standard errors .",
    "also the final error after evaluating the performance of the ensemble when it is trained on the entire data set is indicated , to show how accurately the errors are predicted for the ensemble when it would be trained using a larger number of training samples ( @xmath6 ) .",
    "first , in both cases the results show that the model of @xmath115 does as expected form a lower bound on the error .",
    "as can be seen from figure  [ fig : oracle_example_artif08 ] , the use of the secondary robust regression method to predict the mean and standard deviation of the observed ensemble error ( top set of curves ) for the artificial data set , extrapolates well and the final observed error ( large asterisk at @xmath118 ) falls inside these values . for the much smaller cd print data set the figure is less clear , and the estimated standard errors on the predicted asymptote @xmath106 ( bottom set of curves ) overlap those of the robust regression prediction",
    "nevertheless , again the observed final ensemble error lies within one standard deviation of the value predicted by the robust regression procedure .",
    "in this paper , we have investigated techniques for making early predictions of the error rate achievable after further interactions .",
    "we have provided several example scenarios where the ability to do this would be of great value in practical data mining applications .",
    "our approach is based on our observations that although the different components of the error progress in different ways as the number of training samples is increased , the behaviour displayed by each component appeared to be qualitatively similar across different combinations of dataset and classification algorithm . to investigate this finding ,",
    "we have created a large set of results for many different combinations of dataset , algorithm , and training set size ( @xmath4 ) and applied statistical techniques to examine the relationship between the values observed after partial training ( with @xmath4 samples ) and those after full training .    perhaps surprisingly , the results showed that in fact a simple linear model provided a highly accurately predictor for the subsequent behaviour of different components .",
    "results confirmed our hypotheses that these could be combined to produce highly accurate predictions of the total observed error .",
    "these findings are validated using a range of datasets and algorithms which were not used during the creation of our statistical models .",
    "we have also examined the extent to which our models can reliably extrapolate when new observations have values way outside the ranges of data used in our models .",
    "results such as the accurate predictions of poor performance for the cnb and adaboost algorithms on the satimage and segment datasets ) show that the bias models extrapolate extremely well",
    ". however , the final predictions are slightly less accurate if the nature of the data is such that a high variance component is observed ",
    "e.g.  the cmc data of the uci database .",
    "this suggests that a more complicated model , where the predicted value depends on the final number of samples available , may be necessary for variance .",
    "as there is no bias - variance(-covariance ) decomposition available for 0/1 loss functions for ensembles of classifiers , it is not straightforward to apply the methodology used to accurately predict the performance of classifiers after further training to ensembles of classifiers .",
    "we have shown how a reformulation of the bias component can provide an estimate of the lower bound on the achievable error which may be more easily computed .",
    "this is especially important when the cost of training is high  for example with trainable ensembles of classifiers .",
    "this bound is used as an asymptote in a power - law regression model to accurately predict the progression of the ensemble s error , independently for each data set .    for future work",
    ", we will focus in two directions .",
    "first we will combine previous theoretical findings and the successful results from the two different approaches here .",
    "taken together they suggest that for even more accurate predictions , it is worth combining the linear model for bias with an inverse power law model for variance using both the current estimates and period over which to predict ( @xmath5 ) as factors .",
    "this can be expected to prove particularly useful for classifiers where variance forms a major part of the observed error .",
    "second , the work presented in this paper used kohavi and wolpert s definition of bias and variance , and we will investigate whether using other definitions of bias and variance further improve the predicted accuracy .",
    "this work was supported by the european commission ( project contract no .",
    "strp016429 , acronym dynavis ) .",
    "this publication reflects only the authors views .                            c.  cortes , l.d .",
    "jackel , s.a .",
    "solla , v.  vapnik , and j.s .",
    "learning curves : asymptotic values and rate of convergence . in _ advances in neural information processing systems : 6 _ , pages 327334 , 1994 .",
    "b.  e. kong and t.  g. dietterich .",
    "error - correcting output coding corrects bias and variance . in _ proceedings of the 12th international conference on machine learning _ , pages 313321 , san francisco , 1995 .",
    "morgan kaufmann .",
    ". fast training of support vector machines using sequential minimal optimization . in b.",
    "schoelkopf , c.  burges , and a.  smola , editors , _ advances in kernel methods - support vector learning_. mit press , 1998 .",
    "j.  j. rodriguez , c.  j. alonso , and o.  j. prieto .",
    "bias and variance of rotation - based ensembles. in _ computational intelligence and bioinspired systems _ ,",
    "number 3512 in lecture notes in computer science , pages 779786 .",
    "springer , 2005 .",
    "d.  sannen , h.  van brussel , and m.  nuttin .",
    "classifier fusion using discounted dempster - shafer combination . in _ proceedings of the 5th international conference on machine learning and data mining _ , pages 216230 , 2007 .",
    "j.  e. smith and m.  a. tahir .",
    "stop wasting time : on predicting the success or failure of learning for industrial applications . in _ proceedings of the 8th international conference on intelligent data engineering and automated learning ( ideal08 ) , _ , number 4881 in lecture notes in computer science , pages 673683 .",
    "springer verlag , 2007 ."
  ],
  "abstract_text": [
    "<S> the accuracy of machine learning systems is a widely studied research topic . </S>",
    "<S> established techniques such as cross - validation predict the accuracy on unseen data of the classifier produced by applying a given learning method to a given training data set . </S>",
    "<S> however , they do not predict whether incurring the cost of obtaining more data and undergoing further training will lead to higher accuracy . in this paper </S>",
    "<S> we investigate techniques for making such early predictions . </S>",
    "<S> we note that when a machine learning algorithm is presented with a training set the classifier produced , and hence its error , will depend on the characteristics of the algorithm , on training set s size , and also on its specific composition . </S>",
    "<S> in particular we hypothesise that if a number of classifiers are produced , and their observed error is decomposed into bias and variance terms , then although these components may behave differently , their behaviour may be predictable .    </S>",
    "<S> we test our hypothesis by building models that , given a measurement taken from the classifier created from a limited number of samples , predict the values that would be measured from the classifier produced when the full data set is presented . </S>",
    "<S> we create separate models for bias , variance and total error . </S>",
    "<S> our models are built from the results of applying ten different machine learning algorithms to a range of data sets , and tested with `` unseen '' algorithms and datasets . </S>",
    "<S> we analyse the results for various numbers of initial training samples , and total dataset sizes . </S>",
    "<S> results show that our predictions are very highly correlated with the values observed after undertaking the extra training . </S>",
    "<S> finally we consider the more complex case where an ensemble of heterogeneous classifiers is trained , and show how we can accurately estimate an upper bound on the accuracy achievable after further training . </S>"
  ]
}