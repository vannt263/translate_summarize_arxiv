{
  "article_text": [
    "the concept of @xmath2-wise independence was introduced by wegman and carter  @xcite in focs79 and has been the cornerstone of our understanding of hash functions ever since .",
    "formally , a family @xmath3 \\to [ \\tsize ] \\}$ ] of hash functions is @xmath2-independent if ( 1 ) for any distinct keys @xmath4 $ ] , the hash codes @xmath5 are independent random variables ; and ( 2 ) for any fixed @xmath6 , @xmath7 is uniformly distributed in @xmath8 $ ] .",
    "as the concept of independence is fundamental to probabilistic analysis , @xmath2-independent functions are both natural and powerful in algorithm analysis .",
    "they allow us to replace the heuristic assumption of truly random hash functions with real ( implementable ) hash functions that are still `` independent enough '' to yield provable performance guarantees .",
    "we are then left with the natural goal of understanding the independence required by algorithms .",
    "when first we have proved that @xmath2-independence suffices for a hashing - based randomized algorithm , then we are free to use _ any _ @xmath2-independent hash function .",
    "the canonical construction of a @xmath2-independent family is based on polynomials of degree @xmath9 .",
    "let @xmath10 be prime . picking random @xmath11 ,",
    "the hash function is defined by : @xmath12 for @xmath13 , the hash function is statistically close to @xmath2-independent .",
    "sometimes 2-independence suffices .",
    "for instance , if one implements a hash table by chaining , the time it takes to query @xmath6 is proportional to the number of keys @xmath14 colliding with @xmath6 ( i.e.  @xmath15 ) .",
    "thus , pairwise independence of @xmath7 and @xmath16 is all we need for expected constant query time .",
    "we note that 2-independence also suffices for the 2-level hashing of fredman et al .",
    "@xcite , yielding static hash tables with constant query time .    at the other end of the spectrum",
    ", @xmath17-independence suffices in a vast majority of applications .",
    "one reason for this is the chernoff bounds of  @xcite for @xmath2-independent events , whose probability bounds differ from the full - independence chernoff bound by @xmath18 .",
    "another reason is that random graphs with @xmath19-independent edges  @xcite share many of the properties of truly random graphs .    in this paper , we study two compelling applications in which independence bigger than @xmath20 is currently needed : linear probing and minwise - independent hashing .",
    "( the reader unfamiliar with these applications will find more details below . ) for linear probing , pagh et al .",
    "@xcite showed that 5-independence suffices , thus giving the first realistic implementation of linear probing with formal guarantees .",
    "for minwise - independence , indyk  @xcite showed that @xmath21-approximation can be obtained using @xmath22-independence .    in both cases",
    ", it was known that 2-independence does not suffice  @xcite , and , indeed , the simplest family @xmath23 provides a counterexample .",
    "however , a significant gap remained to the upper bounds .",
    "in this paper , we close this gap , showing that both upper bounds are , in fact , tight .",
    "we do this by exhibiting carefully constructed families for which these algorithms fail : for linear probing , we give a 4-independent family that leads to @xmath24 expected query time ; and for minwise independence , we give an @xmath25-independent family that leads to @xmath26 approximation .",
    "in fact , we will present a complete understanding of linear probing with low independence as summarized in table  [ tab : lin - probe ] .",
    ".expected time bounds for linear probing with a bad family of @xmath2-independent hash functions .",
    "construction time refers to the total time to insert @xmath27 keys starting from an empty table . [ cols=\"<,^,^,^,^\",options=\"header \" , ]     [ [ concrete - schemes . ] ] concrete schemes .",
    "+ + + + + + + + + + + + + + + + +    our results give a powerful understanding of a natural combinatorial resource ( independence ) for two important algorithmic questions .",
    "in other words , they are limits on how far the _ paradigm _ of independence can bring us .",
    "note , however , that independence is only one property that concrete hash schemes have . in a particular application ,",
    "a hash scheme can behave much better than its independence guarantees , if it has some other probabilistic property unrelated to independence .",
    "obviously , proving that a concrete hashing scheme works is not as attractive as proving that every @xmath2-independent scheme works , including more efficient @xmath2-independent schemes found in the future .",
    "however , if low independence does not work , then a concrete scheme may be the best we can hope for .",
    "the most practical 2-independent hash function is not the standard @xmath28 , but dietzfelbinger s multiply - shift scheme  @xcite , which on some computers is 10 times as fast  @xcite . to hash @xmath29-bit integers to @xmath30-bit integers , @xmath31 ,",
    "the scheme picks two random @xmath32-bit integers @xmath33 and @xmath34 , and computes @xmath35 , where @xmath36 denotes unsigned shift .    in this paper",
    ", we prove that linear probing with multiply - shift hashing suffers from @xmath24 expected running times on some input .",
    "similarly , we show that minwise independent hashing may have a very large approximation error of @xmath37 . while these results are not surprising ,",
    "given the `` moral similarity '' of multiply - shift and @xmath38 schemes , they do require rather involved arguments .",
    "we feel this effort is justified , as it brings the theoretical lower bounds in line with programming reality .",
    "[ [ later - work . ] ] later work .",
    "+ + + + + + + + + + +    in our continued search for efficient hashing schemes with good theoretical properties , we later considered simple tabulation hashing  @xcite , which breaks fundamentally from polynomial hashing schemes . tabulation based hashing is comparable in speed to multiply - shift hashing  @xcite , but it uses much more space ( polynomial instead of constant ) .",
    "it is only 3-independent , yet it does give constant expected time for linear probing and @xmath39-approximate minwise hashing .",
    "it is the negative findings of the current paper that motivated us to look into the much more space consuming tabulation hashing .",
    "the problems discovered here for minwise hashing with low independence also lead the second author to consider alternatives more suitable for low independence @xcite .",
    "linear probing uses a hash function to map a set of keys into an array of size @xmath40 .",
    "when inserting @xmath6 , if the desired location @xmath7 is already occupied , the algorithm scans @xmath41 until an empty location is found , and places @xmath6 there .",
    "the query algorithm starts at @xmath7 and scans either until it finds @xmath6 , or runs into an empty position , which certifies that @xmath6 is not in the hash table .",
    "we assume constant load of the hash table , e.g.  the number of keys is @xmath42 .",
    "this classic data structure is one of the most popular implementation of hash tables , due to its unmatched simplicity and efficiency .",
    "the practical use of linear probing dates back at least to 1954 to an assembly program by samuel , amdahl , boehme ( c.f .",
    "@xcite ) . on modern architectures ,",
    "access to memory is done in cache lines ( of much more than a word ) , so inspecting a few consecutive values typically translates into just one memory probe .",
    "even if the scan straddles a cache line , the behavior will still be better than a second random memory access on architectures with prefetching .",
    "empirical evaluations  @xcite confirm the practical advantage of linear probing over other known schemes , while cautioning  @xcite that it behaves quite unreliably with weak hash functions ( such as 2-independent ) .",
    "taken together , these findings form a strong motivation for theoretical analysis .",
    "linear probing was first shown to take expected constant time per operation in 1963 by knuth  @xcite , in a report now considered the birth of algorithm analysis . however , this required truly random hash functions .",
    "a central open question of wegman and carter  @xcite was how linear probing behaves with @xmath2-independence .",
    "siegel and schmidt  @xcite showed that @xmath19-independence suffices .",
    "recently , pagh et al .",
    "@xcite showed that even @xmath43-independent hashing works .",
    "we now close this line of work , showing that @xmath44-independence is not enough .",
    "[ [ review - of - the-5-independence - upper - bound . ] ] review of the 5-independence upper bound . + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to better situate our lower bounds , we begin by reviewing the upper bound of  @xcite .",
    "our proof here is much simpler than the one in  @xcite but assumes a load factor below @xmath45 .",
    "a more elaborate proof considering all load factors is presented in  @xcite .",
    "the main probabilistic tool featuring in this analysis is a 4moment bound .",
    "consider throwing @xmath27 balls into @xmath40 bins uniformly .",
    "let @xmath46 be the probability that ball @xmath47 lands in the first bin , and @xmath48 the number of balls in the first bin .",
    "we have @xmath49 = \\frac{n}{\\tsize}$ ] .",
    "then , the @xmath2moment of @xmath50 is defined as @xmath51 $ ] .    as long as our placement of the balls",
    "is @xmath2-independent , the @xmath2moment is identical to the case of full independence .",
    "for instance , the 4moment is : @xmath52 = { \\mathbf{e}}\\big [ \\big ( \\sum_i ( x_i-\\tfrac{1}{\\tsize } ) \\big)^4 \\big ] = \\sum_{i , j , k , l } { \\mathbf{e}}\\big [ ( x_i-\\tfrac{1}{\\tsize } )    ( x_j-\\tfrac{1}{\\tsize})(x_k-\\tfrac{1}{\\tsize})(x_l-\\tfrac{1}{\\tsize } ) \\big].\\ ] ] the only question in calculating this quantity is the independence of sets of at most @xmath44 items .",
    "thus , 4-independence preserves the 4moment of full randomness . since @xmath53=0]$ ]",
    ", @xmath52   = \\sum_{i } { \\mathbf{e}}\\big [ ( x_i-\\tfrac{1}{\\tsize})^4 \\big ] .",
    "+ { 4\\choose 2}\\sum_{i , j } { \\mathbf{e}}\\big [ ( x_i-\\tfrac{1}{\\tsize})^2    ( x_j-\\tfrac{1}{\\tsize})^2 \\big].\\ ] ] moments are a standard approach for bounding the probability of large deviations . let s say that we expect @xmath54 items in the bin , but have capacity @xmath55 ; what is the probability of overflow ?",
    "a direct calculation shows that the 4moment is @xmath56 = o(\\mu^2)$ ] .",
    "then , by a markov bound , the probability of overflow is @xmath57 = \\pr [ ( x-\\mu)^4 \\ge \\mu^4 ] = o(1/\\mu^2)$ ] . by contrast",
    ", if we only have 2-independence , we can use the 2moment @xmath58 = o(\\mu)$ ] and obtain @xmath57 = o(1/\\mu)$ ] .",
    "observe that the 3moment is not useful for this approach , since @xmath59 can be negative , so markov does not apply .    to apply moments to linear probing , we consider a perfect binary tree spanning the array @xmath60 $ ] where @xmath61 is a power of two . for notational convenience ,",
    "we assume that the load factor is @xmath62 , that is , @xmath63 .",
    "a node at height @xmath64 has an interval of @xmath65 array positions below it , and is identified with this interval .",
    "we expect at most @xmath66 keys to be hashed to the interval ( but more or less keys may end in the interval , since items are not always placed at their hash position ) .",
    "call the node `` near - full '' if at least @xmath67 keys hash to its interval .",
    "we will now bound the total time it takes to construct the hash table ( the cost of inserting @xmath27 distinct items ) .",
    "a run is an maximal interval of filled locations .",
    "if the table consists of runs of @xmath68 keys ( @xmath69 ) , the cost of constructing it is bounded from above by @xmath70 . to bound these runs ,",
    "we make the following crucial observation : if a run contains between @xmath65 and @xmath71 keys , then some node at height @xmath72 above it is near - full . in fact , there will be such a near - full height @xmath72 node whose last position is in the run .    for a proof",
    ", we study a run of length at least @xmath65 . the run is preceded by an empty position ,",
    "so all keys in the run are hashed to the run ( but may appear later in the run than the position they hashed to ) .",
    "there are at least 4 consecutive height @xmath72 nodes with their last position in the interval .",
    "assume for a contradiction that none of these are near - full .",
    "the first node ( whose first positions may not be in the run ) contributes less than @xmath73 keys to the run ( in the most extreme case , this many keys hash to the last position of that node ) .",
    "the subsequent nodes have all @xmath74 positions in the run , but with less than @xmath75 keys hashing to these positions . even with the maximal excess from the first node",
    ", we can not fill the intervals of two subsequent nodes , so the run must stop before the end of the third node , contradicting that its last position was in the run .",
    "each node has its last position in at most one run , so the observation gives an upper bound on the cost : add @xmath76 for each near - full node at some height @xmath77 . denoting by @xmath78 the probability that a node on height @xmath77 is near - full , the expected total cost over all heights is thus bounded by @xmath79 . using the 2moment to bound @xmath78 , we obtain @xmath80 , so the total expected cost with 2-independence is @xmath81",
    ". however , the 4moment gives @xmath82 , so the total expected cost with 4-independence is @xmath83 .    to bound the running time of one particular operation ( query or insert @xmath84 ) , we want @xmath84 to be independent of the construction .",
    "thus , if the hash function is @xmath2-independent , then we view the hash of @xmath84 as independent of a @xmath85-independent construction .",
    "the expected time is then the average distance from a position to the end of the run containing it .",
    "there is also a constant to be added for empty positions , but we ignore that below .",
    "the average cost is then a fraction @xmath86 of the sum of the squared run lengths , which is exactly what we bounded above .",
    "thus , with 3-independent hashing , the expected operation time is @xmath87 while with 5-independent hashing , the expected operation time is @xmath88 . for load factors below @xmath45 ,",
    "this establishes the upper bounds from table  [ tab : lin - probe ] except for the @xmath89 search time with 2-independent hashing .    [",
    "[ our - results . ] ] our results .",
    "+ + + + + + + + + + + +    two intriguing questions pop out of this analysis .",
    "first , is the independence of the query really crucial ?",
    "perhaps one could argue that the query behaves like an average operation , even if it is not completely independent of everything else .",
    "secondly , one has to wonder whether 3-independence suffices ( by using something other than 3moment ) : all that is needed is a bound slightly stronger than 2moment in order to make the costs with increasing heights decay geometrically !",
    "we answer both questions in strong negative terms .",
    "the complete understanding of linear probing with low independence is summarized in table  [ tab : lin - probe ] . addressing the first question ,",
    "we show that 4-independence can not give expected time per operation better than @xmath24 , even though @xmath27 operations take @xmath83 time .",
    "our proof demonstrates an important phenomenon : even though most bins have low load , a particular key s hash code could be correlated with the ( uniformly random ) choice of _ which _ bins have high load .",
    "an even more striking illustration of this fact happens for 2-independence : the query time blows up to @xmath90 in expectation , since we are left with no independence at all after conditioning on the query s hash .",
    "this demonstrates a very large separation between linear probing and collision chaining , which enjoys @xmath88 query times even for 2-independent hash functions .    addressing the second question ,",
    "we show that 3-independence is not enough to guarantee even a construction time of @xmath83 .",
    "thus , in some sense , the 4moment analysis is the best one can hope for .",
    "this concept was introduced by two classic algorithms : detecting near - duplicate documents  @xcite and approximating the size of the transitive closure  @xcite .",
    "the basic step in these algorithms is estimating the size of the intersection of pairs of sets , relative to their union : for @xmath91 and @xmath92 , we want to find @xmath93 ( the _ jaccard similarity coefficient _ ) . to do this efficiently , one can choose a hash function @xmath77 and maintain @xmath94 as the sketch of an entire set @xmath91 .",
    "if the hash function is truly random , we have @xmath95 = \\frac{|a\\cap    b|}{|a \\cup b|}$ ] .",
    "thus , by repeating with several hash functions , or by keeping the bottom @xmath2 keys with one hash function , the jaccard coefficient can be estimated up to a small approximation .    to make this idea work ,",
    "the property that is required of the hash function is _",
    "minwise independence_. formally , a family of functions @xmath96 \\to [ u ] \\}$ ] is said to be minwise independent if , for any set @xmath97 $ ] and any @xmath98 , we have @xmath99 = \\frac{1}{|s|+1}$ ] . in other words , @xmath6 is the minimum of @xmath100 only with its `` fair '' probability @xmath101 .    as good implementations of exact minwise independent functions",
    "are not known , the definition is relaxed to @xmath21-minwise independent , where @xmath99 = \\frac{1 \\pm { \\varepsilon}}{|s|+1}$ ] . using such a function",
    ", we will have @xmath95 = ( 1\\pm { \\varepsilon } ) \\frac{|a\\cap b|}{|a \\cup b|}$ ] .",
    "thus , the @xmath21 parameter of the minwise family dictates the best approximation achievable in the algorithms ( which _ can not _",
    "be improved by repetition ) .",
    "indyk  @xcite gave the only implementation of minwise independence with provable guarantees , showing that @xmath102-independent functions are @xmath21-minwise independent .",
    "his proof uses another tool enabled by @xmath2-independence : the inclusion - exclusion principle .",
    "say we want to bound the probability that at least one of @xmath27 events is `` good . ''",
    "we can define @xmath103 , |s|=k } \\pr[\\textrm{all $ s$ are good}]$ ] .",
    "then , the probability that at least one event is good is , by inclusion - exclusion , @xmath104 .",
    "if we only have @xmath2-independence ( @xmath2 odd ) , we can upper bound the series by @xmath105 . in the common scenario that @xmath106 decays exponentially with @xmath2 , the trimmed series will only differ from the full independence case by @xmath18 .",
    "thus , @xmath2-independence achieves bounds exponentially close to full independence , whenever probabilities can be computed by inclusion - exclusion .",
    "this turns out to be the case for minwise independence : we can express the probability that at least some key in @xmath107 is below @xmath6 by inclusion - exclusion .    in this paper , we show that , for any @xmath108 , there exist @xmath25-independent hash functions that are no better than @xmath21-minwise independent . indyk s  @xcite simple analysis via inclusion - exclusion is therfore tight : @xmath21-minwise independence requires @xmath1 independence .",
    "we will now present our analysis of linear probing with different degrees of independence .",
    "we will present negative results complementing the positive findings from @xcite , reviewed above .",
    "when lower bounding query times , for simplicity we assume that the query is not among the stored keys .",
    "the cost of such an unsuccessful search is the distance to the end of the run the query hashes to .",
    "above we saw that the expected construction time with @xmath20-independence is @xmath110 , so the average cost per key is @xmath87 .",
    "we will now define a 2-independent hash family such that the expected query time for some concrete key is @xmath109 .",
    "the main idea of the proof is that the query can play a special role : even if most portions of the hash table are lightly loaded , the query can be correlated with the portions that _ are _ loaded .",
    "we assume that @xmath40 is an odd power of two , and we store @xmath111 keys .",
    "then @xmath112 is also a power of two .",
    "we think of the stored keys and the query key as given , and we want to find bad ways of distributing them @xmath20-independently into the range @xmath8 $ ] . to extend the hash function to the entire universe ,",
    "all other keys are hashed totally randomly .",
    "we consider unsuccessful searches , i.e.  the search key @xmath84 is not stored in the hash table .",
    "the query time for @xmath84 is the number of cells considered from @xmath113 up to the first empty cell . if , for some @xmath114 , the interval @xmath115 $ ] has @xmath116 keys , then the search time is @xmath117 .",
    "let @xmath118 ; this is a power of two dividing @xmath40 . in our construction , we first pick the hash @xmath113 uniformly .",
    "we then divide the range into @xmath119 intervals of length @xmath114 , of the form @xmath120 $ ] , wrapping around modulo @xmath40 .",
    "one of these intervals is exactly @xmath121 .",
    "we prescribe the distribution of keys between the intervals ; the distribution within each interval will be fully random . to place @xmath122 keys in the query interval with constant probability , we mix among two strategies with constant probabilities ( to be determined ) :    @xmath123 : : :    spread keys evenly , with @xmath119 keys in each interval . @xmath124",
    ": : :    pick the query interval @xmath121 and three random intervals .",
    "place @xmath125 keys in one of these 4 intervals , and    none in the others .",
    "all other intervals get @xmath119    keys",
    ".    +    from the perspective of the stored keys , the 4 intervals are    completely random . with probability @xmath126 , it is    @xmath121 that gets @xmath127 keys , overloading    it by a factor 2 . then",
    ", as described above , the search time is    @xmath90 .    to prove that the hash function is 2-independent",
    ", we need to consider pairs of two stored keys , and pairs involving the query and one stored key . in either case",
    ", we can just look at the distribution into intervals , since the position within an interval is truly random .",
    "moreover , by symmetry between intervals , we only need to understand the probability of the two keys landing in the same interval ( which we call a `` collision '' ) .",
    "we need to balance the strategies so that the collision probability is exactly @xmath128 .",
    "since stored keys are symmetric , the probability of @xmath84 and @xmath6 colliding is @xmath129 times the expected number of items in @xmath121 , which is exactly @xmath130 with both strategies .",
    "thus @xmath113 and @xmath7 are independent no matter how we mix @xmath123 and @xmath124 .",
    "to analyze pairs @xmath131 of stored keys , we compute the expected number of collisions among stored keys .",
    "we want this number to be @xmath132 . in strategy @xmath123",
    ", we get the smallest possible number of collisions : @xmath133 .",
    "this is too few by almost @xmath134 . in strategy @xmath124",
    ", we get @xmath135 collisions , which is too much by a bit more than @xmath136 . to get the right expected number of collisions , we use @xmath124 with probability @xmath137 . with this mix of strategies ,",
    "our hashing of keys is 2-independent , and since @xmath138 , our expected search cost is @xmath139 .",
    "[ [ upper - bound ] ] upper bound + + + + + + + + + + +    we will now prove a matching upper bound of @xmath89 on the expected query cost with any 2-independent scheme .",
    "as in the lower bound , we hash @xmath27 keys into @xmath60 $ ] , @xmath140 $ ] , and divide @xmath60 $ ] into @xmath130 intervals of length @xmath118 .",
    "we view keys as colliding if they hash to the same interval .",
    "we want to argue that long runs imply too many collisions for @xmath20-independence .",
    "the expected number of collisions is @xmath141 .",
    "the minimum number of collisions is with the distribution @xmath123 from the lower bound : a perfectly regular distribution with @xmath142 keys in each interval , hence @xmath143 collisions in total .",
    "an interval with @xmath144 keys has @xmath145 collisions and the derivative is @xmath146 .",
    "it follows that if we move a key from an interval with @xmath147 keys to one with @xmath148 keys , the number of collisions increases by more than @xmath149",
    ". any distribution can be obtained from the above minimal distribution by moving keys from intervals with at most @xmath130 keys to intervals with at least @xmath130 keys , and each such move increases the number of collisions .",
    "a run of length @xmath150 implies that this many keys hash to an interval of this length .",
    "the run is contained in less than @xmath151 of our length @xmath118 intervals . in the process of creating a distribution with this run from the minimum distribution , we have to move at least @xmath152 keys to intervals that have already been filled with at least @xmath153 keys .",
    "keys are always moved from intervals with less than @xmath130 keys , so each move gains at least @xmath154 collisions .",
    "thus our total gain is at least @xmath155 the total number of collisions with a run of length @xmath150 is therefore at least @xmath156 it follows that if the expected run length is bigger than @xmath157 , then the expected number of collisions is bigger than @xmath158 , contradicting that their expected number is only @xmath159 .      we will now construct a 3-independent family of hash functions , such that the time to insert @xmath27 items into a hash table is @xmath161 .",
    "the lower bound is based on overflowing intervals .",
    "[ lem : overflow - constr ] suppose an interval @xmath162 $ ] of length @xmath114 has @xmath163 stored keys hashing to it .",
    "then the insertion cost of these keys is @xmath164 .",
    "the overflowing @xmath165 keys will be part of a run containing @xmath166 $ ] .",
    "at least @xmath167 of them must end at position @xmath168 or later , i.e. , a displacement of at least @xmath167 . interference from stored keys hashing outside @xmath162 $ ] can only increase the displacement , so the insertion cost is @xmath164 .",
    "we will add up such squared overflow costs over disjoint intervals , demonstrating an expected total cost of @xmath160 .",
    "as before , we assume the array size @xmath40 is a power of two , and we set @xmath169 .",
    "we imagine a perfect binary tree spanning the array .",
    "the root is level @xmath170 and level @xmath30 is the nodes at depth @xmath30 . our hash function will recursively distribute keys from a node to its two children , starting at the root .",
    "nodes run independent random distribution processes .",
    "then , if each node makes a @xmath2-independent distribution , overall the function is @xmath2-independent .    for a node",
    ", we mix between two strategies for distributing @xmath171 keys between the two children :    @xmath123 : : :    distribute the keys evenly between the children .",
    "if @xmath171    is odd , a random child gets @xmath172 keys . @xmath124",
    ": : :    give all the keys to a random child .",
    "our goal is to determine the correct probability for the second strategy , @xmath173 , such that the distribution process is 3-independent .",
    "then we will calculate the cost it induces on linear probing .",
    "first , however , we need some basic facts about @xmath2-independence .",
    "our randomized procedure treats keys symmetrically , and ignores the distinction between left / right children .",
    "we call such distributions _",
    "fully symmetric_. say the current node has to distribute @xmath171 keys to its two children ( @xmath144 need not be integral ) .",
    "let @xmath174 be the indicator random variable for key @xmath33 ending in the left child , and @xmath175 .",
    "by symmetry of the children , @xmath176 = \\frac{1}{2}$ ] , so @xmath177 = m$ ] .",
    "the @xmath2moment is @xmath178 $ ] .",
    "also define @xmath179 $ ] ( by symmetry , any @xmath2 distinct keys yield the same value ) .",
    "[ lem : pk ] a fully symmetric distribution is @xmath2-independent iff @xmath180 for all @xmath181 .    for the non - trivial direction , assume @xmath180 for all @xmath181 .",
    "we need to show that , for any @xmath182 , @xmath183 = 2^{-k}$ ] . by symmetry of the keys ,",
    "we can sort the vector to @xmath184 and @xmath185 .",
    "let @xmath186 be the probability that such a vector is seen .",
    "we use induction on @xmath2 . in the base case , @xmath187 by symmetry . for @xmath188 , we start with @xmath189 .",
    "we then use induction for @xmath190 down to @xmath191 .",
    "the induction step is simply : @xmath192 .",
    "indeed , @xmath193 $ ] can be computed as the difference between @xmath194 $ ] ( measured by @xmath195 ) and @xmath196 $ ] ( measured by @xmath197 ) .",
    "based on this lemma , we can also give a characterization based on moments .",
    "first observe that any odd moment is necessarily zero , as @xmath198 = \\pr[x = m-\\delta]$ ] by symmetry of the children .",
    "[ lem : fk ] a fully symmetric distribution is @xmath2-independent iff its even moments up to @xmath199 coincide with the moments of the truly random distribution .",
    "we will show that @xmath200 are determined by @xmath201 , and vice versa .",
    "thus , any distribution that has the same moments as a truly random distribution , will have the same values @xmath202 as the truly random distribution ( @xmath180 as in lemma  [ lem : pk ] ) .",
    "let @xmath203 be the falling factorial .",
    "the complete dependence between @xmath200 and @xmath201 follows inductively from the following statement : @xmath204 to see this , note that @xmath178 = { \\mathbf{e}}[x^k ] + f\\big ( m , { \\mathbf{e}}[x^2 ] , \\dots , { \\mathbf{e}}[x^{k-1 } ] \\big)$ ] for some function @xmath205 .",
    "but @xmath206 = ( 2m)^{\\overline k } p_k + f_1(m , k ) p_{k-1 } + f_2(m , k ) p_{k-2 } + \\dots$ ] . here , the factors @xmath207 count the number of ways to select @xmath2 out of @xmath171 keys , with @xmath47 duplicates .      as a general convention ,",
    "when we are mixing strategies @xmath208 , we use @xmath209 to denote the probability of picking strategy @xmath208 while we use a superscript @xmath210 to denote measures within strategy @xmath208 , e.g. , @xmath211 is the second moment when strategy @xmath208 is applied .    by lemma [ lem : fk ]",
    ", a mix of @xmath123 and @xmath124 is 3-independent iff it has the correct 2moment @xmath212 . in strategy",
    "@xmath123 , @xmath213 ( due to rounding errors if @xmath171 is odd ) , so @xmath214 . in @xmath124 ( all to one child ) , @xmath215 so @xmath216 .",
    "for a correct 2moment of @xmath217 , we balance with @xmath218 .",
    "we now calculate the cost in terms of squared overflows .",
    "as long as the recursive steps spread the keys evenly with @xmath123 , the load factor stays around @xmath45 : at level @xmath30 , the intervals have length @xmath219 and @xmath220 keys .",
    "if now , for a node @xmath221 on level @xmath30 , we apply @xmath124 collecting all keys into one child , that child interval gets an overflow of @xmath222 keys . by lemma [ lem : overflow - constr ]",
    ", the keys at the child will have a total insertion cost of @xmath223 .",
    "since @xmath224 , the expected cost induced by @xmath221 is @xmath225 .",
    "this , however , assumes that no ancestor of @xmath221 was collected .",
    "note that it also avoids over - counting when we only charge @xmath221 is @xmath124 collection is applied to @xmath221 but to no ancestors of @xmath221 , for then the nodes charged all represent different keys .",
    "it remains to bound the probability that @xmath124 collection has been applied to an ancestor of a node @xmath221 on a given level @xmath226 .",
    "the collection probability for a node @xmath227 on level @xmath228 is @xmath229 assuming no collection among the ancestors of @xmath227 . by the union bound , the probability that any ancestor @xmath227 of @xmath221 is first to be collected is @xmath230 .",
    "we conclude that @xmath221 has no collected ancestors with probability @xmath231 , hence that the expected cost of @xmath221 is @xmath232 as above .",
    "the total expected cost over all @xmath233 level @xmath30 nodes is thus @xmath234 .",
    "summing over all levels @xmath235 , we get an expected total insertion cost of @xmath236 for our 3-independent scheme .",
    "proving high expected search cost with 4-independence combines the ideas for 2-independence and 3-independence .",
    "however , some quite severe complications will arise .",
    "the lower bound is based on an overflowing intervals .",
    "[ lem : overflow - query ] suppose an interval @xmath162 $ ] of length @xmath114 has @xmath163 , @xmath237 , stored keys hashing to it .",
    "assuming that the interval has even length and that the stored keys hash symmetrically to the first and second half of @xmath162 $ ] .",
    "moreover , assume that the query key hash uniformly in @xmath162 $ ] .",
    "then the expected query time is @xmath238 .    by symmetry between the first and the second half , with probability @xmath239 , the first half gets half the keys , hence an overflow of @xmath240 keys , and a run containing @xmath241 .",
    "since @xmath237 , the probability that the query key hits the first half of this run is @xmath242 , and then the expected query cost is @xmath238 .    as for 2-independence , we will first choose @xmath113 and then make the stored keys cluster preferentially around @xmath113 . as for 3-independence , the distribution will be described using a perfectly balanced binary tree over @xmath60 $ ] .",
    "the basic idea is to use the 3-independent distribution from section [ sec:3wise ] along the query path .",
    "for brevity , we call nodes on the query path for query nodes .",
    "the overflows that lead to an @xmath236 construction cost , will yield an @xmath243 expected query time .",
    "however , the clustering of this 3-independent distribution is far too strong for 4-independence .",
    "we can not really use it in the top of the tree , but further down , we can balance it with an anti - clustering distributions at most of the nodes outside the query path .      for a node that has @xmath171 keys to distribute , we consider three basic strategies :    @xmath123 : : :    distribute the keys evenly between the two children .",
    "if    @xmath171 is odd , a random child gets    @xmath172 keys . @xmath124",
    ": : :    give all the keys to a random child .",
    "@xmath244 : : :    pick a child randomly , and give it @xmath245 keys .    by mixing among these ,",
    "we define two super - strategies :    @xmath246 : :    @xmath247 ; @xmath248 : :    @xmath249 .",
    "the above notation means that strategy @xmath250 picks strategy @xmath124 with probability @xmath173 ; @xmath123 otherwise .",
    "likewise @xmath251 picks @xmath244 with probability @xmath252 ; @xmath123 otherwise .",
    "the probabilities @xmath173 and @xmath252 are chosen such that @xmath250 and @xmath251 are 3-independent .",
    "the strategy @xmath250 is the 3-independent strategy from section  [ sec:3wise ] where we determined @xmath253 .",
    "this will be our preferred strategy on the query path .",
    "to compute @xmath252 , we employ the 2moments : @xmath214 and @xmath254 .",
    "( if one ignored rounding , we would have the precise bounds @xmath255 and @xmath256 . ) by lemma  [ lem : fk ] , we need a 2moment of @xmath217 .",
    "thus , we have @xmath257 .",
    "we are going to get 4-independence by an appropriate mix of our 3-independent strategies @xmath250 and @xmath251 .",
    "our first step is to hash the query uniformly into @xmath60 $ ] .",
    "this defines the query path .",
    "we will do the mixing top - down , one level @xmath30 at the time .",
    "the individual node will not distribute its keys 4-independently .",
    "nodes on the query path will prefer @xmath250 while keys outside the query path will prefer @xmath251 , all in a mix that leads to global 4-independence .",
    "there will also be neutral nodes for which we use a truly random distribution . since all distributions are 3-independent regardless of the query path , the query hashes independently of any 3 stored keys .",
    "we are therefore only concerned about the 4-independence among stored keys .",
    "it is tempting to try balancing of @xmath250 and @xmath251 via 4moments using lemma  [ lem : fk ] .",
    "however , even on the same level @xmath30 , the distribution of the number of keys at the node on the query path will be different from the distributions outside the query path , and this makes balancing via 4moments non - obvious .",
    "instead , we will argue independence via lemma  [ lem : pk ] : since we already have 3-independence and all distributions are symmetric , we only need to show @xmath258 .",
    "thus , conditioned on 4 given keys @xmath259 being together on level @xmath30 , we want them all to go to the left child with probability @xmath260 . by symmetry ,",
    "our 4-tuple @xmath261 is uniformly random among all 4-tuples surviving together on level @xmath30 . on the average",
    "we thus want such 4-tuples to go left together with probability @xmath260 .",
    "our aim now is to compute @xmath262 and @xmath263 for a node with @xmath171 keys to be split between its children .",
    "first we note : @xmath264 indeed , the first key will go to the left child with probability @xmath265 . conditioned on this",
    ", the second key will go to the left child with probability @xmath266 , etc . in @xmath124 ,",
    "all keys go to the left child with probability a half , so @xmath267 .",
    "since @xmath268 , we get @xmath269 to avoid a rather involved calculation , we will not derive @xmath263 directly , but rather as a function of the 4moment .",
    "we have @xmath270 , @xmath271 , and @xmath257 , so @xmath272 from the proof of lemma  [ lem : fk ] , we know that @xmath273 with any distribution . since @xmath251 is 3-independent , it has the same @xmath274 and @xmath275 as a truly random distribution . thus , we can compute @xmath276 using the @xmath277 and @xmath278 values of a truly random distribution .",
    "the 4moment of a truly random distribution is : @xmath279 since @xmath258 in the truly random case , we have : @xmath280 $ ] .",
    "now we can return to @xmath263 : @xmath281 to get @xmath282 for a given node , we use a strategy @xmath283 that applies @xmath250 with probability @xmath284 ; @xmath251 otherwise .",
    "however , as stated earlier , we will often give preference to @xmath250 on the query path , and to @xmath251 elsewhere .",
    "we are now ready to describe the mix of strategies used in the binary tree . on the top @xmath285 levels",
    ", we use the above mentioned mix @xmath283 of @xmath250 and @xmath251 yielding a perfect 4-independent distribution of the keys at each node .    on the next levels @xmath286 , we will always use @xmath250 on the query path . for the other nodes , we use @xmath250 with the probability @xmath287 such that if all non - query nodes on level @xmath30 use the strategy    @xmath288 : :    @xmath289 ;    then we get @xmath282 for an average 4-tuple on level @xmath30 .",
    "we note that @xmath287 depends completely on the distribution of 4-tuples at the nodes on level @xmath30 and that @xmath287 has to compensate for the fact that @xmath250 is used at the query node .",
    "we shall prove the existence of @xmath287 shortly .",
    "finally , we have a stopping criteria : if at some level @xmath30 , we use the @xmath124 collection on the query path , or if @xmath290 , then we use a truly random distribution on all subsequent levels .",
    "we note that the @xmath124 collection could happen already on a top level @xmath291 .",
    "consider a level @xmath30 before the stopping criteria has been applied .",
    "we need to argue that the above mentioned probability @xmath287 exists .",
    "we will argue that @xmath292 implies @xmath293 while @xmath294 implies @xmath295 .",
    "then continuity implies that there exists a @xmath296 $ ] yielding @xmath282 .    with @xmath294",
    ", we use strategy @xmath250 for all nodes on the level , and we already know that @xmath297 .    now consider @xmath292 , that it , we use @xmath250 only at the query node . starting with a simplistic calculation ,",
    "assume that all @xmath233 nodes on level @xmath30 had exactly @xmath298 keys , hence the same number of 4-tuples .",
    "then the average is @xmath299 the inequality follows because @xmath286 implies @xmath300 while @xmath301 .",
    "however , the number of keys at different nodes on level @xmath30 is not expected to be the same , and we will handle this below .    we want to prove that the average @xmath277 over all 4-tuples on level @xmath30 is below @xmath260 . to simplify calculations",
    ", we can add @xmath302 for each 4-tuple using @xmath250 and @xmath303 for each tuple using @xmath251 , and show that the sum is negative .",
    "if the query node has @xmath171 keys , all using @xmath250 , we thus add @xmath304 .",
    "if a non - query node has @xmath171 keys , we subtract @xmath305 .",
    "we now want to bound the number of keys at the level @xmath30 query node . since the stopping criteria has not applied , we know that @xmath124 collection has not been applied to any of its ancestors .",
    "[ lem : no - collection ] if we have never applied @xmath124 collection on the path to a query node @xmath221 on level @xmath306 , then @xmath221 has @xmath307 keys .    on the path to @xmath221 ,",
    "we have only applied strategies @xmath123 and @xmath244 . hence ,",
    "if an ancestor of @xmath221 has @xmath171 keys , then each child gets @xmath308 keys .",
    "the bound follows by induction starting with @xmath309 keys at the root on level @xmath170 .",
    "our level @xmath30 query node thus has @xmath310 keys and contributes @xmath311 to the sum .",
    "to lower bound the negative contribution from the non - query nodes on level @xmath30 , we first note that they share all the @xmath312 keys not on the query path .",
    "the negative contribution for a node with @xmath171 keys is @xmath223 . by convexity ,",
    "the total negative contribution is minimized if the keys are evenly spread among the @xmath313 non - query nodes , and even less if we distributed on @xmath233 nodes .",
    "the total negative contribution is therefore at least @xmath314 .",
    "this dominates the positive contribution from the query node since @xmath315 .",
    "thus we conclude that @xmath293 when @xmath292 .",
    "this completes the proof that we for level @xmath30 can find a value of @xmath316 $ ] such that @xmath282 , hence the proof that the distribution tree described in section [ sec : tree ] exists , hashing all keys 4-independently .",
    "we will now study the expected query time .",
    "we only consider the cost in the event that @xmath124 collection is applied at the query node at some level @xmath317 $ ] .",
    "this implies that @xmath124 has not been applied previously on the query path , so the event can only happen once with a given distribution ( no over counting ) . by lemma [ lem : no - collection ] , our query node has @xmath318 keys . with probability @xmath239",
    ", these all go to the query child which represents an interval of length @xmath319 . since @xmath320",
    ", we conclude that the query child gets overloaded by almost a factor @xmath321 . by lemma [ lem : overflow - query ] ,",
    "the expected search cost is then @xmath322 .    on the query path on every level @xmath323 ,",
    "we know that the probability of applying @xmath124 provided that @xmath124 has not already been applied is @xmath324 where @xmath325 by lemma [ lem : no - collection ] .",
    "the probability of applying @xmath124 on level @xmath317 $ ] is therefore @xmath326 , so the expected search cost from this level is @xmath327 .",
    "since our event can only happen on one level for a given distribution , we sum this cost over the @xmath243 levels in @xmath328 $ ] .",
    "we conclude that the expected search cost of our 4-independent scheme is @xmath243 .",
    "we will show that it is limited how good minwise independence we can achieve based on @xmath2-independent hashing . for a given @xmath2 , our goal is to construct a @xmath2-independent distribution over @xmath27 regular keys and a query key @xmath84 , such that the probability that @xmath84 gets the minimal hash value is @xmath329 .",
    "we assume that @xmath2 is even and divides @xmath27 .",
    "each hash value will be uniformly distributed in the unit interval @xmath330 .",
    "discretizing this continuous interval does not affect any of the calculations below , as long as precision @xmath331 or more is used ( making the probability of a non - unique minimum vanishingly small ) .    for our construction",
    ", we divide the unit interval into @xmath332 subintervals of the form @xmath333 .",
    "the regular keys are distributed totally randomly between these subintervals .",
    "each subinterval @xmath334 gets @xmath2 regular keys in expectation .",
    "we say that @xmath334 is _ exact _ if it gets exactly @xmath2 regular keys . whenever @xmath334 is not exact , the regular keys are placed totally randomly within it .",
    "the distribution inside an exact interval @xmath334 is dictated by a parity parameter @xmath335 .",
    "we break @xmath334 into two equal halves , and distribute the @xmath2 keys into these halves randomly , conditioned on the parity in the first half being @xmath336 . within its half , each key gets an independent random value .",
    "if @xmath336 is fixed , this process is @xmath9 independent .",
    "indeed , one can always deduce the half of a key @xmath6 based on knowledge of @xmath9 keys , but the location of @xmath6 is totally uniform if we only know about @xmath337 keys . if the parity parameter @xmath336 is uniform in @xmath338 ( but possibly dependent among exact intervals ) , the overall distribution is still @xmath2-independent .",
    "the query is generated independently and uniformly .",
    "for each exact interval @xmath334 , if the query is inside it , we set its parity parameter @xmath339 .",
    "if @xmath334 is exact but the query is outside it , we toss a biased coin to determine the parity , with @xmath340 = ( \\frac{1}{2 } - \\frac{k}{n } ) / ( 1 - \\frac{k}{n})$ ] .",
    "any fixed exact interval receives the query with probability @xmath341 , so overall the distribution of @xmath342 is uniform .",
    "we claim that the overall process is @xmath2-independent .",
    "uniformity of @xmath342 implies that the distribution of regular keys is @xmath2-independent . in the case of @xmath84 and @xmath9 regular keys",
    ", we also have full independence , since the distribution in an interval is @xmath85-independent even conditioned on @xmath336 .",
    "it remains to calculate the probability of @xmath84 being the minimum under this distribution .",
    "first we assume that the query landed in an exact interval @xmath334 , and calculate @xmath343 , the probability that @xmath84 takes the minimum value within @xmath334 .",
    "define the random variable @xmath50 as the number of regular keys in the first half . by our process",
    ", @xmath50 is always even .    if @xmath344 , @xmath84 is the minimum only if it lands in the first half ( probability @xmath345 ) and is smaller than the @xmath6 keys already there ( probability @xmath346 ) . if @xmath347 , @xmath84 is the minimum either if it lands in the first half ( probability @xmath345 ) , or if it lands in the second half , but is smaller than everybody there ( probability @xmath348 ) .",
    "thus , @xmath349 \\cdot \\big ( \\tfrac{1}{2 } + \\tfrac{1}{2 ( k+1 ) } \\big ) + \\sum_{x=2,4,{\\mathinner{\\ldotp\\ldotp } } , k } \\pr[x = x ] \\cdot \\tfrac{1}{2(x+1)}\\ ] ]    to compute @xmath350 $ ] , we can think of the distribution into halves as a two step process : first @xmath9 keys are distributed randomly ; then , the last key is placed to make the parity of the first half even .",
    "thus , @xmath351 if either @xmath6 or @xmath352 of the first @xmath9 keys landed in the first half . in other words : @xmath353 = \\tbinom{k-1}{x } / 2^{k-1 } + \\tbinom{k-1}{x-1 } / 2^{k-1 }              = \\tbinom{k}{x } / 2^{k-1}\\ ] ]    no keys are placed in the first half iff none of the first @xmath9 keys land there ; thus @xmath354 = 1/2^{k-1}$ ] .",
    "we obtain : @xmath355 but @xmath356 .",
    "since @xmath357 is odd , the sum over all odd binomial coefficients is exactly @xmath358 ( it is equal to the sum over even binomial coefficients , and half the total ) .",
    "thus , @xmath359 , i.e.  @xmath84 is the minimum with a probability that is too large by a factor of @xmath360 .",
    "we are now almost done .",
    "for @xmath84 to be the minimum of all keys , it has to be in the minimum non - empty interval .",
    "if this interval is exact , our distribution increases the chance that @xmath84 is minimum by a factor @xmath360 ; otherwise , our distribution is completely random in the interval , so @xmath84 is minimum with its fair probability .",
    "let @xmath361 be the number of regular keys in @xmath84 s interval , and let @xmath362 be the event that @xmath84 s interval is the minimum non - empty interval .",
    "if the distribution were truly random , then @xmath84 would be minimum with probability : @xmath363 \\cdot \\pr[{\\mathcal{e}}\\mid z = z ] \\cdot \\frac{1}{z+1}\\ ] ] in our tweaked distribution , @xmath84 is minimum with probability : @xmath364 \\cdot \\pr[{\\mathcal{e}}\\mid z = z ] \\cdot \\frac{1}{z+1 }       + \\pr[z = k ] \\cdot \\pr[{\\mathcal{e}}\\mid z = k ] \\cdot \\frac{1 + 2^{-k}}{k+1 } \\\\ & = & \\frac{1}{n+1 } ~+~ \\pr[z = k]\\cdot \\pr[{\\mathcal{e}}\\mid z = k ] \\cdot \\frac{2^{-k}}{k+1}\\end{aligned}\\ ] ]    but @xmath361 is a binomial distribution with @xmath27 trials and mean @xmath2 ; thus @xmath365 = \\omega(1/\\sqrt{k})$ ] .",
    "furthermore , @xmath366 \\ge \\frac{k}{n}$ ] , since @xmath84 s interval is the very first with probability @xmath341 ( and there is also a nonzero chance that it is not the first , but all interval before are empty ) .",
    "thus , the probability is off by an additive term @xmath367 .",
    "this translates into a multiplicative factor of @xmath368 .",
    "we show that the simplest and fastest known universal hashing schemes have bad expected performance when used for linear probing on some of the most realistic structured data .",
    "this result is inspired by negative experimental findings from @xcite .",
    "the essential form of the schemes considered have the following basic form : we want to hash @xmath369-bit keys into @xmath370-bit indices . here",
    "@xmath371 , and the indices are used for the linear probing array . for the typical case of a half full table , we have @xmath372 .",
    "in particular , @xmath373 .",
    "depending on details of the scheme , for some @xmath374 , we pick a random multiplier @xmath375 $ ] , and compute @xmath376 we refer to this as the _ basic multiply - shift scheme_. if @xmath377 , the mod - operation is performed automatically as discarded overflow .",
    "the @xmath378 operation is just a right shift by @xmath379 , so in c we get the simple code ` ( ` @xmath33`*`@xmath6`)>>`@xmath380 and the cost is dominated by a single multiplication . for the plain universal hashing in @xcite , it suffices that @xmath381 but then the multiplier @xmath33 should be odd . for 2-independent hashing as in @xcite , we need @xmath382 . also we need to add a random number @xmath34 , but as we shall discuss in the end , these details have no essential impact on the derivation below .",
    "our basic bad example will be where the keys form the interval @xmath383=\\{0, ... ,n-1\\}$ ] .",
    "however , the problem will not go away if this interval is shifted or not totally full , or replaced by an arithmetic progression .    when analyzing the scheme , it is convenient to first consider it as a mapping into the unit interval @xmath330 via @xmath384 then @xmath385 .",
    "we think of the unit interval as circular , and for any @xmath386 , we define @xmath387 this is the distance from 0 in the circular unit interval .",
    "[ lem : avg - cost ] let the multiplier @xmath33 be given and suppose for some @xmath388 that @xmath389 .",
    "then , when we use @xmath390 to hash @xmath383 $ ] into a linear probing table , the average cost per key is @xmath391 .",
    "the case studied is illustrated in figure  [ fig:5cycle ] .",
    ".,scaledwidth=30.0% ]    for each @xmath392 $ ] , consider the set @xmath383^x_k=\\{y\\in[n]\\;|\\;y = k\\pmod x\\}$ ] .",
    "the @xmath393 keys from @xmath383^x_k$ ] map to an interval of length @xmath394 , which means that the @xmath390 distributes @xmath383^x_k$ ] on at most @xmath395 consecutive array locations . linear probing will have to spread @xmath383^x_k$ ] on @xmath84 locations , so",
    "on the average , the keys will get a displacement of @xmath396 .",
    "we get a corresponding double full interval for every equivalence class modulo @xmath6 .",
    "therefore we get an average insert cost of @xmath391 over all the keys .",
    "the above average costs only measures the interaction between keys from the same equivalence class modulo @xmath6 .",
    "if some of these classes overlapped , the cost would only grow .",
    "note that @xmath397 implies that @xmath398 is contained in an interval of size @xmath399 around 0 . from the universality arguments of @xcite",
    "we know that the probability of this event is roughly @xmath399 ( we shall return with an exact statement and proof later ) .",
    "we would like to conclude that the expected average cost is @xmath400 .",
    "the answer is correct , but the calculation cheats in the sense that we may have many different @xmath6 such that @xmath397 , and the associated costs should not all be added up .    to get a proper lower bound , for any given multiplier @xmath33 , we let @xmath401 denote the minimal positive value such that @xmath402 . if @xmath403 , then by lemma [ lem : avg - cost ] , the average insertion cost is @xmath404 .",
    "therefore , if @xmath33 is random over some probability distribution ( to be played with as we go along ) , the expected average insert cost is lower bounded by @xmath405\\right).\\ ] ]    [ lem : bad ] for a given multiplier @xmath33 , consider any @xmath406 such that @xmath397 .",
    "then @xmath407 if and only if for some prime factor @xmath408 of @xmath6 , @xmath409 .",
    "the `` if '' part is trivial . since @xmath410 , for any integer @xmath411 , we have @xmath412 .",
    "hence @xmath413 . on the other hand ,",
    "suppose @xmath414 where @xmath14 is not a multiple of @xmath401 .",
    "then @xmath415 maps @xmath416 to points in the cyclic unit interval that are at most @xmath417 apart ( c.f .",
    "figure  [ fig:5cycle ] ) .",
    "it follows that @xmath418 .",
    "however , we are only considering @xmath406 with @xmath397 . if @xmath419 , then @xmath420 while @xmath421 , so we conclude that @xmath6 is a multiple of @xmath401 .",
    "then @xmath422 where @xmath423 .",
    "let @xmath408 be any prime factor of @xmath47",
    ". then @xmath424 .    to illustrate the basic accounting idea ,",
    "assume for simplicity that we have a perfect distribution @xmath425 on @xmath33 that for any fixed @xmath426 distributes @xmath398 uniformly in the unit interval .",
    "then for any @xmath6 and @xmath427 , @xmath428=2{\\varepsilon}.\\ ] ] then by lemma [ lem : bad ] , @xmath429 & \\geq&\\pr_{a\\leftarrow{\\mathcal{u}}}[\\| h^0_a(x)\\|\\leq 1/(2m)]-\\sum_{p\\ { \\rm prime\\ factor\\ of}\\ x } \\pr_{a\\leftarrow{\\mathcal{u}}}[\\| h^0_a(x / p)\\|\\leq 1/(2pm)]\\nonumber\\\\ & = & 1/m-\\sum_{p\\ { \\rm prime\\ factor\\ of}\\ x } 1/(pm)\\nonumber\\\\ & = & \\left(1-\\sum_{p\\ { \\rm prime\\ factor\\ of}\\ x } 1/p\\right)/m\\label{eq : prop - min}\\end{aligned}\\ ] ] we note that the lower - bound may be negative since there are values of @xmath6 for which @xmath430",
    ". nevertheless suffices with an appropriate reordering of terms . from",
    "we get that the expected average insertion cost is : @xmath431\\right ) & = & \\omega\\left(\\sum_{x=1}^{n } \\left(n/(xm)\\,\\left(1-\\sum_{{\\rm prime\\ factor}\\ p{\\rm\\ of}\\ x } 1/p\\right)\\right)\\right)\\\\ & = & \\omega\\left ( \\sum_{x=1}^{n } \\left(n/(xm)\\,\\left(1-\\sum_{{\\rm prime}\\ p=2,3,5 , .. } 1/p^2\\right)\\right)\\right)\\end{aligned}\\ ] ] above we simply moved terms of the form @xmath432 where @xmath408 is a prime factor of @xmath6 to @xmath433 in the form @xmath434 .",
    "conservatively , we include @xmath434 for all primes @xmath408 even if @xmath435 .",
    "since @xmath436 , we get an expected average insertion cost of @xmath431\\right ) & = & \\omega\\left(\\sum_{x=1}^{n } 0.547n/(xm)\\right)\\\\ & = & \\omega(n / m\\,\\lg n).\\end{aligned}\\ ] ] we would now be done if we had the perfect distribution @xmath425 on @xmath33 so that the equality was satisfied .",
    "instead we will use the weaker statements of the following lemma :    [ lem : eq : approx ] let @xmath437 the uniform distribution on odd @xmath30-bit numbers . for any odd @xmath406 and @xmath438 , @xmath439\\leq 4{\\varepsilon}\\ ] ] however , if @xmath21 is an integer multiple @xmath440 , then @xmath441=2{\\varepsilon}.\\ ] ]    when @xmath6 is odd and @xmath33 is a uniformly distributed odd @xmath30-bit number , then @xmath442 is uniformly distributed odd @xmath30-bit number . to get @xmath398 , we divide by @xmath233 , and then we have a uniform distribution on odd multiples of @xmath443 .",
    "now is immediate because we have exactly one odd multiple in each interval @xmath444 $ ] .",
    "also , we maximize @xmath445/{\\varepsilon}$ ] when we capture the points closest to 0 with @xmath446 , that is , with @xmath447 = 4/2^\\ell$ ] .",
    "hence follows .",
    "we are now ready to prove our lower bound for the performance of linear probing with the basic multiply - shift scheme .",
    "[ thm : bad - multiply - shift ] suppose @xmath448 and that the multiplier @xmath33 is a uniformly distributed odd @xmath30-bit number .",
    "if we use @xmath390 to insert @xmath383 $ ] in a linear probing table , then expected average insertion cost is @xmath243 .    by assumption @xmath449",
    "is a multiple of @xmath440 , so for odd @xmath406 , implies @xmath450=2/m.\\ ] ] by lemma [ lem : bad ] combined with and , we get that @xmath429 & \\geq&\\pr_{a\\leftarrow{\\mathcal{u}}}[\\| h^0_a(x)\\|\\leq 1/(2m)]-\\sum_{p\\ { \\rm prime\\ factor\\ of}\\ x } \\pr_{a\\leftarrow{\\mathcal{u}}}[\\| h^0_a(x / p)\\|\\leq 1/(2pm)]\\nonumber\\\\ & \\geq&1/m-\\sum_{p\\ { \\rm prime\\ factor\\ of}\\ x } 2/(pm)\\nonumber\\\\\\end{aligned}\\ ] ] from we get that the expected average insertion cost is : @xmath431\\right ) & = & \\omega\\left(\\sum_{x=1}^{n } \\left(n/(xm)\\,\\left(1 - 2\\sum_{{\\rm prime\\ factor}\\ p{\\rm\\ of}\\ x } 1/p\\right)\\right)\\right)\\nonumber\\\\ & = & \\omega\\left ( \\sum_{x=1}^{n } \\left(n/(xm)\\,\\left(1 - 2\\sum_{{\\rm prime}\\ p=3,5 , .. } 1/p^2\\right)\\right)\\right)\\nonumber\\\\ & = & \\omega\\left ( \\sum_{\\textnormal{odd } x=1}^{n } 0.594n/(xm)\\right)\\nonumber\\\\ & = & \\omega(n / m\\,\\lg n).\\nonumber\\ ] ] above we again moved terms of the form @xmath432 where @xmath408 is a prime factor of @xmath6 to @xmath433 in the form @xmath434 . since @xmath6 is odd , we only have to consider odd primes factors @xmath408 , and then we used that @xmath451 .",
    "this completes the proof of theorem [ thm : bad - multiply - shift ] .",
    "we note that the plain universal hashing from @xcite also assumes an odd multiplier , so theorem  [ thm : bad - multiply - shift ] applies directly if @xmath448 .",
    "the condition @xmath448 is , in fact , necessary for bad performance .",
    "if @xmath452 , then @xmath390 is a permutation for any odd @xmath33 , and then linear probing works perfectly",
    ".    for the 2-universal hashing @xcite there are two differences .",
    "one is that the multiplier may also be even , but restricting it to be odd can only double the cost .",
    "the other difference is that we add an additional @xmath30-bit parameter @xmath34 , yielding a scheme of the form : @xmath453 the only effect of @xmath34 is a cyclic shift of the double full buckets , and this has no effect on the linear probing cost .",
    "for the 2-independent hashing , we have @xmath454 , so @xmath455 if @xmath456 . hence again",
    "we have an expected average linear probing cost of @xmath457 .    finally , we sketch some variations of our bad input . currently , we just considered the set @xmath383 $ ] of input keys , but it makes no essential difference if instead for some integer constants @xmath458 and @xmath459 , we consider the arithmetic sequence @xmath460+\\beta=\\{i\\alpha + \\beta\\,|\\,x\\in [ n]\\}$ ] .",
    "the @xmath459 is just adds a cyclic shift like the @xmath34 in 2-independent hashing . if @xmath458 is odd , then it is absorbed in the random multiplier @xmath33 .",
    "what we get now is that if for some @xmath461 $ ] , we have @xmath462 , then again we get an average cost @xmath391 .",
    "a consequence is that no odd multiplier @xmath33 is universally safe because there always exists an inverse @xmath458 ( with @xmath463 ) leading to a linear cost if @xmath390 is used to insert @xmath460+\\beta$ ] .",
    "it not hard to also construct bad examples for even @xmath458 .",
    "if @xmath458 is an odd multiple of @xmath464 , we just have to strengthen the condition @xmath448 to @xmath465 to get the expected average insertion cost of @xmath457 .",
    "this kind of arithmetic sequences could be a true practical problem .",
    "for example , in some denial - of - service attacks , one often just change some bits in the middle of a header key , and this gives an arithmetic sequence .",
    "another more practical concern is if the input set @xmath50 is an @xmath21-fraction of @xmath383 $ ] .",
    "as long as @xmath466 , the above proof works almost unchanged . for smaller @xmath21 ,",
    "our bad case is if @xmath467 . in that case , for each @xmath392 $ ] , the @xmath468 potential keys @xmath14 from @xmath383 $ ] with @xmath469 would map to an interval of length @xmath470 .",
    "this means that @xmath390 spreads these potential keys on at most @xmath471 consecutive array locations .",
    "a @xmath21-fraction of these keys are real , so on the average , these intervals become double full , leading to an average cost of @xmath472 . strengthening @xmath448 to @xmath473 , we essentially get that all probabilities are reduced by @xmath21 .",
    "thus we end with a cost of @xmath474 .",
    "we now consider the lack of minwise independence with a hashing scheme @xmath475 shifting out less significant bits does not make much sense since we are not hashing to entries in an array",
    ". the analysis will be very similar to the one done for linear probing in section [ sec : ms ] , and we will only sketch it . now",
    "the added @xmath34 is necessary to get anything meaningful , for without it , zero would always get the minimal hash value @xmath476 .",
    "the effect of adding @xmath34 is to randomly spin the wheel from figure [ fig:5cycle ] . as in section",
    "[ sec : ms ] , it is convenient to divide by @xmath233 to get fractions in the cyclic unit interval .",
    "we thus define @xmath477",
    ".    the bad case will be the interval @xmath383 $ ] versus a random query key @xmath84 .",
    "we assume that @xmath27 is a power of two . to see the parallels to section [ sec : ms ] , think of @xmath478 . for any @xmath33",
    ", we define @xmath479 to be the smallest value such that @xmath480 .",
    "then @xmath481)$ ] falls in @xmath401 equidistant intervals , each of length at most @xmath482 .",
    "this leaves us with @xmath401 equidistant empty intervals , each of length at least @xmath483 .",
    "together these empty intervals cover half the cyclic unit interval .",
    "when we add @xmath34 it has the effect of placing @xmath170 randomly on the cycle . having chosen @xmath33 and @xmath34 , a random @xmath84",
    "is also hashed to random place on the cycle .",
    "the probability that @xmath84 and @xmath170 end in the same empty interval and with @xmath84 after @xmath170 in the interval is @xmath484 . in this case , @xmath84 has the minimal hash value , but that should only have happened with probability @xmath129 .",
    "thus , relatively speaking , the probability is to high by a factor @xmath404 , matching the linear probing cost from section [ sec : ms ] . as a result",
    "we will also end up concluding that the expected min - probability is to high by a factor @xmath24 .",
    "as in section [ sec : ms ] , there are some details to consider .",
    "it is convenient to restrict ourselves to odd values of @xmath33 , @xmath34 , @xmath485 and @xmath84 . as a result ,",
    "all our hash values are odd , and at odd multiples of @xmath443 in the cyclic unit interval .",
    "the analysis goes through as long as @xmath486 , and in fact , we can shit our all but the @xmath487 most significant bits and yet have the same lower bound that the min - probability is too high by a factor @xmath24 .",
    "john  r. black , charles  u. martel , and hongbin qi .",
    "graph and hashing algorithms for modern architectures : design and performance . in _ proc .",
    "2nd international workshop on algorithm engineering ( wae ) _ ,",
    "pages 3748 , 1998 .",
    "martin dietzfelbinger .",
    "universal hashing and @xmath2-wise independent random variables via integer arithmetic without primes . in _ proc .",
    "13th symposium on theoretical aspects of computer science ( stacs ) _ , pages 569580 , 1996 .",
    "mihai ptracu and mikkel thorup . on the @xmath2-independence required by linear",
    "probing and minwise independence . in _ proc .",
    "37th international colloquium on automata , languages and programming ( icalp ) _ , pages 715726 , 2010 ."
  ],
  "abstract_text": [
    "<S> we show that linear probing requires 5-independent hash functions for expected constant - time performance , matching an upper bound of [ pagh et al .  </S>",
    "<S> stoc07 ] . </S>",
    "<S> more precisely , we construct a 4-independent hash functions yielding expected logarithmic search time . for @xmath0-approximate minwise independence , </S>",
    "<S> we show that @xmath1-independent hash functions are required , matching an upper bound of [ indyk , soda99 ] . </S>",
    "<S> we also show that the very fast 2-independent multiply - shift scheme of dietzfelbinger [ stacs96 ] fails badly in both applications . </S>"
  ]
}