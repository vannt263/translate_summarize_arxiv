{
  "article_text": [
    "analyzing spike trains from hundreds of neurons to find significant temporal patterns is an important current research problem @xcite . by using experimental techniques such as micro electrode arrays or imaging of neural currents ,",
    "spike data can be recorded simultaneously from many neurons @xcite .",
    "such multi - neuronal spike train data can now be routinely gathered _ in vitro _ from neural cultures or _ in vivo _ from brain slices , awake behaving animals and even humans . such data would be a mixture of stochastic spiking activities of individual neurons as well as that due to correlated activity of groups of neurons due to interconnections , possibly triggered by external inputs . automatically discovering patterns ( regularities ) in these spike trains can lead to better understanding of how interconnected neurons act in a coordinated manner to generate specific functions .",
    "there has been much interest in techniques for analyzing the spike data so as to infer functional connectivity or the functional relationships within the system that produced the spikes @xcite .",
    "in addition to contributing towards our knowledge of brain function , understanding of functional relations embedded in spike trains leads to many applications , e.g. , better brain - machine interfaces .",
    "such an analysis can also ultimately allow us to systematically address the question , `` is there a neural code ? '' .    in this paper , we consider the problem of discovering _ statistically significant _ patterns from multi - neuronal spike train data .",
    "the patterns we consider here are ordered firing sequences by a group of neurons with specific time - lags or delays between successive neurons .",
    "such a pattern ( when it repeats many times ) may denote a chain of triggering events and hence unearthing such patterns from spike data can help understand the underlying functional connectivity .",
    "for example , memory traces are probably embedded in such sequential activation of neurons and signals of this form have been found in hippocampal neurons @xcite .",
    "such patterns of ordered firing sequences with fairly constant delays between successive neuronal firings have been observed in many experiments and there is much interest in detecting such patterns and assessing their statistical significance .",
    "( see @xcite and references therein ) .    here , we will call patterns of ordered firing sequences as _",
    "sequential patterns_. symbolically , we denote such a pattern as , e.g. , @xmath0 .",
    "this represents the pattern of ordered firing sequence of @xmath1 followed by @xmath2 followed by @xmath3 with a delay of @xmath4 time units between @xmath1 & @xmath2 and @xmath5 time units between @xmath2 & @xmath3 .",
    "( we note here that within any occurrence of such a firing pattern , there could be spikes by other neurons ) .",
    "such a pattern of firings may occur repeatedly in the spike train data if , e.g. , there is an excitatory influence of total delay @xmath4 from @xmath1 to @xmath2 and an excitatory influence of delay @xmath5 between @xmath2 and @xmath3 .",
    "in general , the delays may not be exactly constant because synaptic transmission etc .",
    "could have some random variations .",
    "hence , in our sequential patterns , we will allow the delays to be intervals of small length . at the least",
    ", we can take the length of the interval as the time resolution in our measurements .",
    "in general , such patterns can involve more than three neurons .",
    "the _ size _ of a pattern is the number of neurons in it .",
    "thus , the above example is that of a size 3 pattern or a 3-node pattern .",
    "one of the main computational methods for detecting such patterns that repeat often enough , is due to abeles and gerstein @xcite .",
    "this essentially consists of sliding the spike train of one neuron with respect to another and noting coincidences at specific delays .",
    "there are also some recent variations of this method @xcite .",
    "most of the current methods for detecting such patterns essentially use correlations among time - shifted spike trains ( and some statistics computed from the correlation counts ) , and these are computationally expensive when detecting large - size ( typically greater than 4 ) patterns @xcite . another approach to detecting such ordered firing sequences",
    "is considered in @xcite while analyzing recordings from hippocampal neurons .",
    "given a specific ordering on a set of neurons , they look for longest sequences in the data that respect this order .",
    "this is similar to our sequential patterns which are somewhat more general because we can also specify different delays between consecutive elements of the pattern .    in this paper",
    "we use a method based on some temporal datamining techniques that we have recently proposed @xcite .",
    "this method can automatically detect all sequential patterns whose _ frequency _ in the data is above a ( user - specified ) threshold where _ frequency _ of the pattern is maximum number of non - overlapped occurrences of the pattern in the spike data .",
    "the essence of this algorithm is that instead of trying to count all occurrences of the pattern in the data we count only certain well - defined subset of occurrences and this makes the process computationally efficient .",
    "the method is effective in detecting long patterns and it would detect only those patterns that repeat more than a given threshold . also , the method can automatically decide on the most appropriate delays in each detected pattern by choosing from a set of possible delays supplied by the user .",
    "( see @xcite for details ) .",
    "the main contribution of this paper is a method for assessing the statistical significance of such sequential patterns .",
    "the objective is to have a method so that we will detect only those patterns that repeat often enough to be significant ( and thus fix the thresholds for the data mining algorithm automatically ) .",
    "we tackle this issue in a classical hypothesis testing framework .",
    "there have been many approaches for assessing the significance of detected firing patterns @xcite . in the current analytical approaches , one generally employs a null hypothesis that the different spike trains are generated by independent processes . in most cases",
    "one assumes ( possibly inhomogeneous ) bernoulli or poisson processes .",
    "then one can calculate the probability of observing the given number of repetitions of the pattern ( or of any other statistic derived from such counts ) under the null hypothesis of independent processes and hence calculate a minimum number of repetitions needed to conclude that a pattern is significant in the sense of being able to reject the null hypothesis .",
    "there are also some empirical approaches suggested for assessing significance @xcite .",
    "here one creates many surrogate data streams from the experimentally observed data by perturbing the individual spikes while keeping certain statistics same and then assessing significance by noting whether or not the patterns are preserved in the surrogate data .",
    "there are many possibilities for the perturbations to be imposed to generate surrogate data @xcite . in these empirical methods",
    "also , the implicit null hypothesis assumes independence .",
    "the main motivation for the approach presented here is the following .",
    "if a sequential pattern repeats often enough to be significant then one would like to think that there are strong influences among the neurons representing the pattern .",
    "however , different ( detected ) patterns may represent different levels or strengths of influences among their constituent neurons .",
    "hence it would be nice to have a method of significance analysis that can rank order different ( significant ) patterns in terms some ` strength of influence ' among the neurons of the pattern .",
    "for this , here we propose that the strength of influence of @xmath1 on @xmath2 is well represented by the conditional probability that @xmath2 will fire after some prescribed delay given that @xmath1 has fired .",
    "we then employ a composite null hypothesis specified through one parameter that denotes an upper bound on all such pairwise conditional probabilities . using this",
    "we would be able to decide whether or not a given pattern is significant at various values for this parameter in the null hypothesis and thus be able to rank - order different patterns .",
    "there is an additional and important advantage of the above approach that we propose here .",
    "our composite null hypothesis is such that any stochastic model for a set of spiking neurons would be in the null hypothesis if all the relevant pairwise conditional probabilities are below some bound . since",
    "this bound is a parameter that can be chosen by the user , the null hypothesis would include not only independent neuron models but also many models of interdependent neurons where the pair - wise influences among neurons are ` weak ' . hence rejecting such a null hypothesis is more attractive than rejecting a null hypothesis of independence when we want to conclude that a significant pattern indicates ` strong ' interactions among the neurons . in this sense ,",
    "the approach presented here extends the currently available methods for significance analysis .",
    "we analytically derive some bounds on the probability that our counting process would come up with a given number of repetitions of the firing pattern if the data is generated by any model that is contained in our compound null hypothesis .",
    "as mentioned earlier , we use the number of non - overlapped occurrences of a pattern as our test statistic instead of the total number of repetitions and employ a temporal datamining algorithm for counting non - overlapped occurrences of sequential patterns @xcite .",
    "this makes our method attractive for discovering significant patterns involving large number of neurons also .",
    "we show the effectiveness of the method through extensive simulation experiments on synthetic spike train data obtained through a model of inter - dependent non - homogeneous poisson processes .    the rest of the paper is organized as follows . in section  [ sec",
    ": epi ] we give a brief overview of temporal datamining and explain our algorithm for detecting sequential patterns whose frequency is above some threshold .",
    "the full details of the algorithm are available elsewhere @xcite and we provide only some details which are relevant for understanding the statistical significance analysis which is presented in section  [ sec : stat ] . in section  [ sec : simu ] , we present some simulation results on synthetic spike train data to show the effectiveness of our method .",
    "we present results to show that our method is capable of ranking different patterns in terms of the synaptic efficacy of the connections .",
    "while we confine our attention in this paper to only sequential patterns , the statistical method we present can be generalized to handle other types of patterns .",
    "we briefly indicate this and conclude the paper with a discussion in section  [ sec : dis ] .",
    "temporal datamining is concerned with analyzing symbolic time series data to discover ` interesting ' patterns of temporal dependencies @xcite .",
    "recently we have proposed that some datamining techniques , based on the so called frequent episodes framework , are well suited for analyzing multi - neuronal spike train data @xcite .",
    "many patterns of interest in spike data such as synchronous firings by groups of neurons , the sequential patterns explained in the previous section , and synfire chains which are a combination of synchrony and ordered firings , can be efficiently discovered from the data using these datamining techniques . while the algorithms are seen to be effective through simulations presented in @xcite , no statistical theory was presented there to address the question of whether the detected patterns are significant in any formal sense which is the main issue addressed in this paper . in this section",
    "we first briefly outline the frequent episodes framework and then qualitatively describe this datamining technique for discovering frequently occurring sequential patterns .    in the frequent episodes framework of temporal datamining .",
    "the data to be analyzed is a sequence of events denoted by @xmath6 where @xmath7 represents an _ event type _ and @xmath8 the _ time of occurrence _ of the @xmath9 event .",
    "@xmath10 s are drawn from a finite set of event types , @xmath11 .",
    "the sequence is ordered with respect to time of occurrences of the events so that , @xmath12 , @xmath13 .",
    "the following is an example event sequence containing 11 events with 5 event types .",
    "@xmath14    in multi - neuronal spike data , the event type of a spike event is the label of the neuron which generated the spike and the event has the associated time of occurrence .",
    "the neurons in the ensemble under observation fire action potentials at different times , that is , generate spike events .",
    "all these spike events are strung together , in time order , to give a single long data sequence as needed for frequent episode discovery .",
    "it may be noted that there can be more than one event with the same time because two neurons can spike at the same time .",
    "the temporal patterns that we wish to discover in this framework are called episodes . in general , episodes are partially ordered sets of event types . here",
    "we are only interested in _ serial episodes _ which are totally ordered .",
    "a _ serial episode _ is an ordered tuple of event types .",
    "for example , @xmath15 is a _ 3-node _ serial episode .",
    "( we also say that the size of this episode is 3 ) .",
    "the arrows in this notation indicate the order of the events .",
    "such an episode is said to _ occur _ in an event sequence if there are corresponding events in the prescribed order in the data sequence . in sequence ( [ eq : data - seq ] ) , the events \\{@xmath16 } constitute an occurrence of the serial episode @xmath17 while the events \\{@xmath18 } do not .",
    "we note here that occurrence of an episode does not require the associated event types to occur consecutively ; there can be other intervening events between them .    in the multi - neuronal data ,",
    "if neuron @xmath1 makes neuron @xmath2 to fire , then , we expect to see @xmath2 following @xmath1 often",
    ". however , in different occurrences of such a substring , there may be different number of other spikes between @xmath1 and @xmath2 because many other neurons may also be spiking during this time .",
    "thus , the episode structure allows us to unearth patterns in the presence of such noise in spike data .",
    "the objective in frequent episode discovery is to detect _ all _ frequent episodes ( of different lengths ) from the data .",
    "frequent episode _ is one whose _",
    "frequency _ exceeds a ( user specified ) _",
    "frequency threshold_. the frequency of an episode can be defined in many ways .",
    "it is intended to capture some measure of how often an episode occurs in an event sequence .",
    "one chooses a measure of frequency so that frequent episode discovery is computationally efficient and , at the same time , higher frequency would imply that an episode is occurring often .    here , we define frequency of an episode as the maximum number of non - overlapped occurrences of the episode in the data stream . two occurrences of an episode are said to be _ non - overlapped _",
    "if no event associated with one occurrence appears in between the events associated with the other .",
    "a set of occurrences is said to be non - overlapped if every pair of occurrences in it are non - overlapped . in our example sequence ( [ eq : data - seq ] ) , there are two non - overlapped occurrences of @xmath19 given by the events : @xmath20 and @xmath21 .",
    "note that there are three distinct occurrences of this episode in the data sequence though we can have only a maximum of two non - overlapped occurrences .",
    "we also note that if we take the occurrence of the episode given by @xmath22 , then there is no other occurrence that is non - overlapped with this occurrence .",
    "that is why we define the frequency to be the maximum number of non - overlapped occurrences .",
    "this definition of frequency results in very efficient counting algorithms with some interesting theoretical properties @xcite .",
    "in addition , in the context of our application , counting non - overlapped occurrences seems natural because we would then be looking at chains that happen at different times again and again .    in analyzing neuronal spike data ,",
    "it is useful to consider methods , where , while counting the frequency , we include only those occurrences which satisfy some additional temporal constraints . here",
    "we are interested in what we call inter - event time constraint which is specified by giving an interval of the form @xmath23 $ ] .",
    "the constraint requires that the difference between the times of every pair of successive events in any occurrence of a serial episode should be in this interval . in general",
    ", we may have different time intervals for different pairs of events in each serial episode . as is easy to see , a serial episode with inter - event time constraints corresponds to what we called a",
    "_ sequential pattern _ in the previous section .",
    "these are the temporal patterns of interest in this paper .",
    "the inter - event time constraint allows us to take care of delays involved in the process of one neuron influencing another through a synapse .",
    "suppose neuron @xmath1 is connected to neuron @xmath2 which , in turn , is connected to neuron @xmath3 , through excitatory connections with delays @xmath4 and @xmath5 respectively . then , we should be counting only those occurrences of the episode @xmath24 , where the inter - event times satisfy the delay constraint .",
    "this would be the sequential pattern @xmath25 .",
    "in general , the inter - event constraint could be an interval .",
    "occurrences of such serial episodes with inter - event constraints in spike data are shown schematically in fig .",
    "[ fig : ser - epi - fig ]     in the spike trains from neurons @xmath26 . a small interval ( usually 1 ms ) is shown around the second and third spike to indicate possible variation in the delay .",
    "note that within the duration of one occurrence of the pattern there may be other intervening spikes ( from any of the neurons ) . ]    in any occurrence of the episode or sequential pattern , we call the difference between the times of the first and last events as its _ span_. the span would be the total of all the delays . if , in the above episode , the span of all occurrences would be @xmath27 and hence we may call it the span of the episode . if the inter - event time constraints are intervals then the span of different occurrences could be different .",
    "there are efficient algorithms for discovering all frequent serial episodes with specified inter - event constraints @xcite .",
    "that is , for discovering all episodes whose frequency ( which is the number of non - overlapped occurrences of the episode ) is above a given threshold .",
    "_ conceptually , the algorithm does the following_. suppose , we are operating at a time resolution of @xmath28 .",
    "( that is , the times of of events or spikes are recorded to a resolution of @xmath28 ) .",
    "then we discretize the time axis into intervals of length @xmath28 .",
    "then , for each episode whose frequency we want to find we do the following .",
    "suppose the episode is the one mentioned above .",
    "we start with time instant 1 .",
    "we check to see whether there is an occurrence of the episode starting from the current instant . for this , we need an @xmath1 at that time instant and then we need a @xmath2 and a @xmath3 within appropriate time windows . if there are such @xmath2 and @xmath3 , then we take the earliest of the @xmath2 and @xmath3 to satisfy the time constraints , increment the counter for the episode and start looking for the occurrence again starting with the next time instant ( after @xmath3 ) . on the other hand ,",
    "if we can not find such an occurrence ( either because @xmath1 does not occur at the current time instant or because there are no @xmath2 or @xmath3 at appropriate times following @xmath1 ) , then we move by one time instant and start the search again",
    ".    the actual search process would be very inefficient if implemented as described above .",
    "the algorithm itself does the search in a much more efficient manner .",
    "there are two issues that the algorithm needs to address .",
    "since , a priori , we do not know what patterns to look for , we need to make a reasonable list of candidate patterns and then obtain their frequencies so as to output only those patterns whose frequency exceeds the preset threshold .",
    "the second issue is that in obtaining frequencies , the algorithm is required to count the frequencies of not one but a set of candidates in one pass through the data and we need to do this efficiently . in generating the candidates , we need to tackle the combinatorial explosion because all possible serial episodes of a given size increases exponentially with the size .",
    "this is tackled using an iterative procedure that is popular in datamining . to understand this ,",
    "consider our example 3-node pattern @xmath25 .",
    "this can not be frequent unless certain 2-node _ subepisodes _ of this , namely , @xmath29 and @xmath30 are frequent .",
    "( this is because any two non - overlapped occurrences of the 3-node pattern also gives us two non - overlapped occurrences of the two 2-node patterns mentioned above ) .",
    "thus , we should allow this 3-node episode to be a candidate only if the appropriate 2-node episodes are already found to be frequent . based on this idea",
    ", we have the following structure for the algorithm .",
    "we first get frequent 1-node episodes which are then used to make candidate 2-node episodes . then",
    ", by one more pass over data , we find frequent 2-node episodes which are then used to make candidate 3-node episodes and so on .",
    "such a technique is quite effective in controlling combinatorial explosion and the number of candidates comes down drastically as the size increases .",
    "this is because , as the size increases , many of the combinatorially possible serial episodes of that size would not be frequent .",
    "this allows the algorithm to find large size frequent episodes efficiently . at each stage of this process , we count frequencies of not one but a whole set of candidate episodes ( of a given size ) through one sequential pass over the data",
    ". we do not actually traverse the time axis in time ticks once for each pattern whose occurrences we want to count .",
    "we traverse the time - ordered data stream .",
    "as we traverse the data we remember enough from the data stream to correctly take care of all the occurrence possibilities of all episodes in the candidate set and thus compute all the frequent episodes of a given size through one pass over the data .",
    "the complete details of the algorithm are available in @xcite .",
    "in this section we address the issue of the statistical significance of the sequential patterns discovered by our algorithm .",
    "the question is when are the discovered episodes significant , or , equivalently , what frequency threshold should we choose so that all discovered frequent episodes would be statistically significant .    to answer this question",
    "we follow a classical hypothesis testing framework .",
    "intuitively we want significant sequential patterns to represent a chain of strong interactions among those neurons .",
    "so , we have to essentially choose a _ null hypothesis _ that asserts that there is no ` _ structure _ ' or ` _ strong influences _ ' in the system of neurons generating the data .",
    "also , as mentioned earlier , we want the null hypothesis to contain a parameter that allows us to specify what we mean by saying that the influence one neuron has on another is not ` strong ' .    for this , we capture the strength of interactions among the neurons in terms of conditional probabilities . let @xmath31 denote the conditional probability that @xmath2 fires in a time interval @xmath32 $ ] given that @xmath1 fired at time zero .",
    "@xmath28 is essentially the time resolution at which we operate .",
    "( for example , @xmath28 = 1ms ) .",
    "thus , @xmath31 is essentially , the conditional probability that @xmath2 fires @xmath33 time units after @xmath1 .",
    ", as a constant",
    ". however , in practice our method can easily take care of the case where the actual delay is uniformly distributed over a small interval with @xmath33 as its expected value . ]",
    "if there is a strong excitatory synapse of delay @xmath33 between @xmath1 and @xmath2 , then this conditional probability would be high . on the other hand",
    "if @xmath1 and @xmath2 are independent , then , this conditional probability is the same as the unconditional probability of @xmath2 firing in an interval of length @xmath28 .",
    "we denote the ( unconditional ) probability that a neuron , @xmath1 , fires in any interval of length @xmath28 by @xmath34 .",
    "( for example , if we take @xmath35 and that the average firing rate of @xmath2 is 20hz , then @xmath36 would be about 0.02 ) .",
    "the main assumption we make is that the conditional probability @xmath37 is not a function of time .",
    "that is , the conditional probability of @xmath2 firing at least once in an interval @xmath38 $ ] given that @xmath1 has fired at @xmath39 is same for all @xmath39 within the time window of the observations ( data stream ) that we are analyzing .",
    "we think this is a reasonable assumption and some recent analysis of spike trains from neural cultures suggests that such an assumption is justified @xcite .",
    "note that this assumption does not mean we are assuming that the firing rates of neurons are not time - varying . as a matter of fact ,",
    "one of the main mechanisms by which this conditional probability is realized is by having a spike from @xmath1 affect the rate of firing by @xmath2 for a short duration of time .",
    "thus , the neurons would be having time - varying firing rates even when the conditional probability is not time - varying .",
    "essentially , the constancy of @xmath40 would only mean that every time @xmath1 spikes , it has the same chance of eliciting a spike from @xmath2 after a delay of @xmath33 .",
    "thus our assumption only means that there is no appreciable change in synaptic efficacies during the period in which the data being analyzed is gathered .",
    "the intuitive idea behind our null hypothesis is that the conditional probability @xmath37 is a good indicator of the ` strength of interaction ' between @xmath1 and @xmath2 . for inferring functional connectivity from repeating sequential patterns",
    ", the constancy of delays ( between spikes by successive neurons ) in multiple repetitions is important .",
    "that is why we defined the conditional probability with respect to a specific delay .",
    "now , an assertion that the interactions among neurons is ` weak ' can be formalized in terms of an upper bound on all such conditional probabilities .",
    "we formulate our composite null hypothesis as follows .    _",
    "our composite null hypothesis includes all models of interacting neurons for which we have @xmath41 for all pairs of neurons @xmath42 and for a set of specified delays @xmath33 , where @xmath43 is a fixed user - chosen number in the interval @xmath44 .",
    "_    thus all models of inter - dependent neurons where the probability of @xmath1 causing @xmath2 to fire ( after a delay ) is less that @xmath43 , would be in our null hypothesis .",
    "the actual mechanism by which spikes from @xmath1 affect the firing by @xmath2 is immaterial to us .",
    "whatever may be this mechanism of interaction , if the resulting conditional probability is less than @xmath43 , then that model of interacting neurons would be in our null hypothesis . could be taken as a typical delay involved in the process ; otherwise it can be taken as some integral multiple of such delays . in any case , our interest is in deciding on the significance of sequential patterns with some given values for @xmath33 . ]",
    "the user specified number , @xmath43 , formalizes what we mean by interaction among neurons is strong .",
    "if @xmath1 and @xmath2 are independent then this conditional probability is same as @xmath36 . as mentioned earlier , if @xmath35 and average firing rate for @xmath2 as 20 hz , then @xmath45 .",
    "so , if we choose @xmath46 , it means that we agree to call the influence as strong if the conditional probability is 20 times what it would be if the neurons are independent . by having different values for @xmath43 in the null hypothesis , we can ask what patterns are significant at what value of @xmath43 and thus rank - order patterns . now",
    "if we are able to reject this null hypothesis then it is reasonable to assert that the episode(s ) discovered would indicate ` strong ' interactions among the appropriate neurons .",
    "the ` strength ' of interaction is essentially chosen by us in terms of the bound @xmath43 on the conditional probability in our null hypothesis .",
    "we now present a method for bounding the probability that the frequency ( number of non - overlapped occurrences ) of a given serial episode with inter - event constraints is more than a given threshold under the null hypothesis . to do this ,",
    "we first compute the expectation and variance ( under the null hypothesis ) of the random variable representing the number of non - overlapped occurrences of a serial episode with inter - event constraints by using the following stochastic model .",
    "let @xmath47 be _ iid _ random variables with distribution given by @xmath48 & = &   p \\nonumber \\\\",
    "p[x_i = 1 ] & = & 1 - p   \\label{eq : xi}\\end{aligned}\\ ] ] where @xmath33 is a fixed constant ( and @xmath49 ) .",
    "let @xmath50 be a random variable defined by @xmath51 where @xmath52 is a fixed constant .",
    "let the random variable @xmath53 denote the number of @xmath54 s out of the first @xmath50 which have value @xmath33 .",
    "define the random variable @xmath55 by @xmath56    all the random variables , @xmath57 depend on the parameters @xmath58 . when it is important to show this dependency we write @xmath59 and so on .",
    "now we will argue that @xmath60 is the random variable representing the number of non - overlapped occurrences of an episode where @xmath33 is the span ( or sum of all delays ) of the episode and @xmath52 is the length of data ( in terms of time duration ) .",
    "we would fix @xmath61 based on the bound @xmath43 in our null hypothesis as explained below .",
    "superimposed on the spike trains from neurons @xmath1 and @xmath2 . in the yellow region",
    "there are no occurrences of the pattern starting with that time instant and the counting scheme moves forward by one time step . in the blue region",
    "there is an occurrence and the counting process moves by @xmath33 time steps .",
    "the random variables @xmath54 , defined by eq .",
    "( [ eq : xi ] ) , capture the evolution of the counting process ]    consider an episode @xmath62 with an inter - event time constraint ( or delay ) of @xmath33 .",
    "now , the sequence @xmath54 essentially captures the counting process of our algorithm .",
    "a schematic of the counting process ( as relevant for this discussion ) is shown in fig .",
    "[ fig : counting ] .",
    "as explained earlier , the algorithm can be viewed as traversing a _ discretized _ time axis in steps of @xmath28 , looking for an occurrence of the episode starting at each time instant . at each time instant ( which , on the discretized time axis corresponds to an interval of length @xmath28 ) , let @xmath63 denote the probability of spiking by @xmath1 and let @xmath64 denote the conditional probability that @xmath2 generates a spike @xmath33 instants later given that @xmath1 has spiked now . in terms of our earlier notation , @xmath65 , @xmath66 .",
    "thus , at any instant , @xmath67 denotes the probability of occurrence of the episode starting at that instant .",
    "now , in eq.([eq : xi ] ) let @xmath68 .",
    "then @xmath61 represents the probability that this episode occurs starting with any given time instant . because we do not know the exact value for @xmath37 .",
    "but finally we would bound the relevant probability by using @xmath43 to bound @xmath37 . ]",
    "let @xmath52 in eq.([eq : n ] ) denote the data length ( in time units ) .",
    "then the sequence , @xmath69 , represents our counting process .",
    "if , at the first instant there is an occurrence of the episode starting at that instant then we advance by @xmath33 units on the time - axis and then look for another occurrence ( since we are counting non - overlapped occurrences ) ; if there is no occurrence starting at the first instant then we advance by one unit and look for an occurrence . also , whether or not there is an occurrence starting from the current instant is independent of how many occurrences are completed before the current instant ( because we are counting only non - overlapped occurrences )",
    "so , the counting process is well captured by accumulating the @xmath54 s defined above till we reach the end of data .",
    "hence @xmath50 captures the number of such @xmath54 that we accumulate because @xmath52 is the data length in terms of time . since @xmath54 take values @xmath70 or @xmath33 , the only way @xmath71 exceeds @xmath52 is if the last @xmath54 takes value @xmath33 which in turn implies that when we reached end of data we have a partial occurrence of the episode . in this case",
    "the total number of completed occurrences is one less than the number of @xmath54 ( out of @xmath50 ) that take value @xmath33 .",
    "if the last @xmath54 has taken value 1 ( and hence the sum is equal to @xmath52 ) then the number of completed occurrences is equal to the number of @xmath54 that take value @xmath33 .",
    "now , it is clear that @xmath55 is the number of non - overlapped occurrences counted .",
    "it is easy to see that the model captures counting of episodes of arbitrary length also .",
    "for example , if our episode is @xmath0 then @xmath33 is eq.([eq : xi ] ) would be @xmath72 and @xmath61 would be @xmath73 . after a delay of @xmath5 from @xmath2 , conditioned on firing of @xmath2 , is conditionally independent of earlier firing of @xmath1 . since our objective is to unearth significant triggering chains , this is a reasonable assumption .",
    "also , this allows us to capture the null hypothesis with a single parameter @xmath43 .",
    "we discuss this further in section  [ sec : dis ] . ]",
    "suppose in a @xmath74-node episode the conditional probability of @xmath75 neuron firing ( after the prescribed delay ) given that the previous one has fired , is equal to @xmath76 .",
    "let the successive delays be @xmath77 .",
    "let the ( unconditional ) probability of the first neuron ( of the episode ) firing at any instant ( that is , in any interval of length @xmath28 ) is @xmath78",
    ". then we will take ( for the @xmath74-node episode ) @xmath79 and @xmath80 .",
    "now , we first derive some recurrence relations to calculate the mean and variance of @xmath59 for a given episode . fixing an episode fixes the value of @xmath61 and @xmath33 .",
    "let @xmath81 where @xmath82 denotes expectation .",
    "we can derive a recurrence relation for @xmath83 as follows .",
    "@xmath84 \\ ; \\right ] \\nonumber \\\\   & = & e\\ : [ m(l , t , p ) \\ ; | \\ ; x_1 = 1 ] ( 1-p ) \\",
    "; + \\ : e[m(l , t , p ) \\ ; | \\ ; x_1 \\neq 1 ] p \\nonumber \\\\ & = & ( 1-p ) e\\:[m(l-1 , t , p ) ] \\ : + \\ : p ( 1 \\ : + \\ :",
    "e[m(l - t , t , p ) ] ) \\nonumber \\\\ & & \\end{aligned}\\ ] ]    in words what this says is : if the first @xmath54 is @xmath70 ( which happens with probability @xmath85 ) , then the expected number of occurrences is same as those in data of length @xmath86 ; on the other hand , if first @xmath54 is not @xmath70 ( which happens with probability @xmath61 ) then the expected number of occurrences are 1 plus the expected number of occurrences in data of length @xmath87 .    hence our recurrence relation is : @xmath88 the boundary conditions for this recurrence are : @xmath89    let @xmath90 $ ] .",
    "that is @xmath91 is the second moment of @xmath60 . using the same idea as in case of @xmath83 we can derive recurrence relation for @xmath92 as follows .",
    "@xmath93 & = & e\\ : \\left [ e\\ : [ m^2(l , t , p ) \\ ; | \\ ; x_1 ] \\right ] \\nonumber \\\\   & = & e\\ : [ m^2(l , t , p ) \\ ; | \\ ; x_1 = 1 ] ( 1-p ) \\ ; \\nonumber \\\\     & & \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ + \\ : e[m^2(l , t , p ) \\ ; | \\ ; x_1 \\neq 1 ] p \\nonumber \\\\ & = & ( 1-p ) e\\:[m^2(l-1 , t , p ) ] \\ : + \\ : p e ( 1 \\ : + \\ : m(l - t , t , p ) ) ^2 \\nonumber \\\\ & = & ( 1-p ) e\\:[m^2(l-1 , t , p ) ] \\ : + \\nonumber \\\\     & &   \\ \\ \\ \\ \\ \\ \\ \\ \\ \\   \\ : p e ( 1 \\ : + \\ : m^2(l - t , t , p ) \\ : +",
    "\\ : 2 m(l - t , t , p ) ) \\nonumber \\\\   & & \\end{aligned}\\ ] ] thus we get @xmath94    solving the above , we get the second moment of @xmath55 .",
    "let , @xmath95 be the variance of @xmath96 .",
    "then we have @xmath97    once we have the mean and variance we can bound the probability that the number of non - overlapped occurrences is beyond something .",
    "for example , we can use chebyshev inequality as @xmath98 \\leq \\frac{1}{k^2 } \\label{eq : cheb}\\ ] ] for any positive @xmath99 .. such bounds can be used for test of statistical significance as explained below .",
    "suppose we are considering @xmath74-node episodes .",
    "let the allowable type i error for the test be @xmath100 .",
    "then what we need is a threshold , say , @xmath101 for which we have @xmath102 \\leq \\epsilon ,    \\label{eq : test1}\\ ] ] where @xmath103 is the frequency of any @xmath74-node episode and @xmath104 denotes probability under the null hypothesis models .",
    "this would imply that if we find a @xmath74-node episode with frequency greater than @xmath101 then , with @xmath105 confidence we can reject our null hypothesis and hence assert that the discovered episode represents ` strong ' interactions among those neurons .",
    "now the above can be used for assessing statistical significance of any episode as follows .",
    "suppose we are considering an @xmath74-node ( serial ) episode .",
    "let the first node of this episode have event type @xmath1 .",
    "( that is , it corresponds to neuron @xmath1 ) .",
    "let @xmath34 be the probability that @xmath1 will spike in any interval of length @xmath28 .",
    "( we will fix @xmath28 by the time resolution being considered ) .",
    "let @xmath100 be the prescribed confidence level .",
    "let @xmath99 be such that @xmath106 .",
    "fix @xmath107 .",
    "let @xmath33 be the sum of all inter - event delay times in the episode .",
    "let @xmath52 be the total length of data ( as time span in units of @xmath28 ) .",
    "our null hypothesis is that the conditional probability for any pair of neurons is less than @xmath43 .",
    "further , our random variable @xmath55 is such that its probability of taking higher values increases monotonically with @xmath61 . hence , with the above @xmath61 , the probability of @xmath60 being greater than any value is an upper bound on the probability of the episode frequency being greater than that value under any of the models in our null hypothesis .    thus , a threshold for significance is @xmath108 because , from eq .",
    "( [ eq : cheb ] ) we have @xmath109 \\leq \\frac{1}{k^2 } \\leq   \\epsilon .",
    "\\label{eq : cheb1}\\ ] ]    though we do not have closed form expressions for @xmath83 and @xmath110 , using our recurrence relations , we can calculate @xmath111 and @xmath95 for any given values of @xmath112 and hence can calculate the above threshold .",
    "the only thing unspecified for this calculation is how do we get @xmath34 .",
    "we can obtain @xmath34 by either estimating the average rate of firing for this neuron from the data or from other prior knowledge .",
    "thus , we can use eq .",
    "( [ eq : cheb1 ] ) either for assessing the significance of a specific @xmath74-node episode or for fixing a threshold of any @xmath74-node episode in our datamining algorithm . in either case , this allows us to deduce the ` strong connections ' ( if any ) in the neural system being analyzed by using our datamining method .",
    "we can summarize the the test of significance as follows .",
    "suppose the allowed type - i error is @xmath100 .",
    "we choose integer @xmath99 such that @xmath113 .",
    "suppose we want to assess the significance of a n - node sequential pattern with the total delay being @xmath33 based on its count .",
    "suppose @xmath43 is the bound we use in our null hypothesis .",
    "let @xmath52 be the total data length in time units .",
    "let @xmath78 be the average firing rate of the first neuron in the data .",
    "let @xmath114 .",
    "we calculate @xmath111 and @xmath95 using ( [ eq : rec1 ] ) , ( [ eq : var - rec1 ] ) and ( [ eq : var ] )",
    ". then the pattern is declared significant if its count exceeds @xmath115 .",
    "_ we like to emphasize that the threshold frequency ( count ) given above for an episode to be significant ( and hence represent strong interactions ) is likely to be larger than that needed .",
    "this is because it is obtained through a chebyshev bound which is often loose . _",
    "thus , for example , if we choose @xmath46 then some strong connections which may result in the effective conditional probability value of up to 0.5 may not satisfy the test of significance at a particular significance level .",
    "this , in general , is usual in any hypothesis testing framework . in practice",
    ", we found that we can very accurately discover all connections whose strengths in terms of the conditional probabilities are about 0.2 more than @xmath43 at 5% confidence level . at @xmath116 ,",
    "the threshold is about 4.5 standard deviations above the mean . in a specific application , for example ,",
    "if we feel that three standard deviations above the mean is a good enough threshold , then correspondingly we will be able to discover even those connections whose effective conditional probability is only a little above @xmath43 .",
    "this test of significance allows us to rank order the discovered patterns . for this",
    ", we run our datamining method with different thresholds corresponding to different @xmath43 values .",
    "then , by looking at the sets of episodes found at different @xmath43 values , we can essentially rank order the strengths of different connections in the underlying system . since any manner of assigning numerical values to strengths of connections",
    "is bound to be somewhat arbitrary , this method of rank ordering different connections in terms of strengths can be much more useful in analyzing microcircuits .",
    "we illustrate all these through our simulation experiments in section  [ sec : simu ] .      so far in this section",
    "we have assumed that the individual delays and hence the span of an episode , @xmath33 , to be constant . in practice ,",
    "even if delay is random and varies over a small interval around @xmath33 , the threshold we calculated earlier would be adequate .",
    "in addition to this , it is possible to extend our model to take care of some random variations in such delays .    since we have assumed that @xmath28 is the time resolution at which we are working , it is reasonable to assume that the delay @xmath33 is actually specified in units of @xmath28 .",
    "then we can think of the delay as a random variable taking values in a set @xmath117 where @xmath118 is a small ( relative to t ) integer .",
    "for example , suppose the delay is uniformly distributed over @xmath119 .",
    "now we can change our model as follows :    the @xmath47 will now be _ iid _ random variables with distribution @xmath120   & = &   1 - p \\nonumber \\\\",
    "\\mbox{prob}[x_i = t-1 ]   =      \\mbox{prob}[x_i = t ]   =   \\mbox{prob}[x_i = t+1 ]   & = &   \\frac{p}{3 } \\nonumber   \\label{eq : xi - new}\\end{aligned}\\ ] ] where we now assume that @xmath121 .",
    "we will define @xmath50 as earlier by eq .",
    "( [ eq : n ] ) .",
    "we will now define @xmath53 as the number of @xmath54 out of first @xmath50 that * do not * take value 1 . in terms of this @xmath53 , we will define @xmath55 as earlier by eq .",
    "( [ eq : m ] ) .",
    "now it is easy to see that our @xmath60 would again be the random variable corresponding to number of non - overlapped occurrences in this new scenario where there are random variations in the delays . now",
    "the recurrence relation for @xmath111 would become @xmath122    the recurrence relation for variance of @xmath60 can also be similarly derived .",
    "now , we can easily implement the significance test as derived earlier .",
    "while the recurrence relations are a little more complicated , it makes no difference to our method of significance analysis because these recurrence relations are anyway to be solved numerically .",
    "it is easy to see that this method can , in principle , take care of any distribution of the total delay ( viewed as a random variable taking values in a finite set ) by modifying the recurrence relation suitably .",
    "in this section we describe some simulation experiments to show the effectiveness of our method of statistical significance analysis .",
    "we show that our stochastic model properly captures our counting process and that the frequency threshold we calculate is effective for separating connections that are ` strong ' ( in the sense of conditional probabilities ) .",
    "we also show that our frequency can properly rank order the strengths of connections in terms of conditional probabilities . as a matter of fact ,",
    "our results provide good justification for saying that conditional probabilities provide a very good scale for denoting connection strengths . for all our experiments we choose",
    "synthetically generated spike trains .",
    "this is because then we know the ground truth about connection strengths and hence can test the validity of our statistical theory . for the simulations we use a data generation scheme where we model the spiking of each neuron as an inhomogeneous poisson process",
    "on which is imposed an additional constraint of refractory period .",
    "( thus the actual spike trains are not truly poisson even if we keep the rate fixed ) .",
    "the inhomogeneity in the poisson process are due to the instantaneous firing rates being modified based on total input spikes received by a neuron through its synapses .",
    "we have shown elsewhere @xcite that our datamining algorithms are very efficient in discovering interesting patterns of firings from spike trains and that we can discover patterns of more than ten neurons also . since in this paper",
    "the focus is on statistical significance of the discovered patterns , we would not be presenting any results for showing the computational efficiency of the method .",
    "we use a simulator for generating the spike data from a network of interconnected neurons .",
    "let @xmath50 denote the number of neurons in the network .",
    "the spiking of each neuron is modelled as an inhomogeneous poisson process whose rate of firing is updated at time intervals of @xmath28 .",
    "( we normally take @xmath28 to be 1ms ) .",
    "the neurons are interconnected by synapses and each synapse is characterized by a delay ( which is in integral multiples of @xmath28 ) and a weight which is a real number .",
    "all neurons also have a refractory period .",
    "the rate of the poisson process is varied with time as follows .",
    "@xmath123 where @xmath124 is the firing rate of @xmath125 neuron at time @xmath126 , and @xmath127 are two parameters .",
    "@xmath128 is the total input into @xmath125 neuron at time @xmath126 and it is given by @xmath129 where @xmath130 is the output of @xmath9 neuron ( as seen by the @xmath125 neuron ) at time @xmath126 and @xmath131 is the weight of synapse from @xmath9 to @xmath125 neuron .",
    "@xmath130 is taken to be the number of spikes by the @xmath9 neuron in the time interval @xmath132 $ ] where @xmath133 represents the delay ( in units of @xmath28 ) for the synapse from @xmath134 to @xmath135 .",
    "the parameter @xmath136 is chosen based on the dynamic range of firing rates that we need to span .",
    "the parameter @xmath137 determines the ` background ' spiking rate , say , @xmath138 .",
    "this is the firing rate of the @xmath125 neuron under zero input . after choosing a suitable value for @xmath136",
    ", we fix the value of @xmath137 based on this background firing rate specified for each neuron .",
    "we first build a network that has many random interconnections with low weight values and a few strong interconnections with large weight values .",
    "we then generate spike data from the network and show how our method can detect all strong connections . to build the network",
    "we specify the background firing rate ( which we normally keep same for all neurons ) which then fixes the value of @xmath137 in ( [ eq : lambda - update ] ) .",
    "we specify all weights in terms of conditional probabilities . given a conditional probability we first calculate the needed instantaneous firing rate so that probability of at least one spike in the @xmath28 interval is equal to the specified conditional probability .",
    "then , using ( [ eq : lambda - update ] ) and ( [ eq : input ] ) , we calculate the value of @xmath131 needed so that the receiving neuron ( @xmath135 ) reaches this instantaneous rate given that the sending neuron ( @xmath134 ) spikes once in the appropriate interval and assuming that input into the receiving neurons from all other neurons is zero .",
    "we note here that the background firing rate as well as the effective conditional probabilities in our system would have some small random variations .",
    "as said above , we fix @xmath137 so that on zero input the neuron would have the background firing rate .",
    "however , all neurons would have synapses with randomly selected other neurons and the weights of these synapses are also random .",
    "hence , even in the absence of any strong connections , the firing rates of different neurons keep fluctuating around the background rate that is specified .",
    "since we choose random weights from a zero mean distribution , in an expected sense we can assume the input into a neuron to be zero and hence the average rate of spiking would be the background rate specified .",
    "we also note that the way we calculate the effective weight for a given conditional probability is also approximate and we chose it for simplicity . if we specify a conditional probability for the connection from @xmath1 to @xmath2 , then , the method stated in the previous paragraph fixes the weight of connection so that the probability of @xmath2 firing at least once in an appropriate interval given that @xmath1 has fired is equal to this conditional probability _ when all other input into @xmath2 is zero_. but since @xmath2 would be getting small random input from other neurons also , the effective conditional probability would also be fluctuating around the nominal value specified . further , even if the random weights have zero mean , the fluctuations in the conditional probability may not have zero mean due to the nonlinear sigmoidal relationship in ( [ eq : lambda - update ] ) .",
    "the nominal conditional probability value determines where we operate on this sigmoid curve and that determines the bias in the excursions in conditional probability for equal fluctuations in either directions in the random input into the neurons .",
    "we consider this as a noise in the system and show that our method of significance analysis is still effective .",
    "the simulator is run as follows .",
    "first , for any neuron we fix a fraction ( e.g. , 25% ) of all other neurons that it is connected to .",
    "the actual neurons that are connected to any neuron are then selected at random using a uniform distribution .",
    "we fix the delays and background firing rates for all neurons .",
    "we then assign random weights to connections by choosing uniformly from an interval . in our simulation experiments",
    "we specify this range in terms of conditional probabilities .",
    "for example suppose the background firing rate is 20 hz .",
    "then with @xmath35 , the probability of firing in any interval of length @xmath28 is ( approximately ) 0.02 .",
    "hence a conditional probability of 0.02 would correspond to a weight value of zero .",
    "then a range of conditional probabilities such as @xmath139 $ ] ( increase or decrease by a factor of 2 in either direction ) would correspond to a weight range around zero .",
    "after fixing these random weights , we incorporate a few strong connections which vary in different simulation experiments .",
    "these weight values are also specified in terms of conditional probabilities .",
    "we then generate a spike train by simulating all the inhomogeneous poisson processes where rates are updated every @xmath28 time instants .",
    "we also fix refractory period for neurons ( which is same for all neurons ) .",
    "once a neuron is fired , we will not let it fire till the refractory period is over .      for the results reported here we used a network of 100 neurons with the nominal firing rate being 20 hz .",
    "each neuron is connected to 25 randomly selected neurons with the effective conditional probability of the connection strength ranging over @xmath139 $ ] . with 20hz firing rate and 1ms time resolution , the effective conditional probability when two neurons are independent is 0.02 .",
    "thus the random connections have conditional probabilities that vary by a factor of two on either side as compared to the independent case .",
    "we then incorporated some strong connections among some neurons .",
    "for this we put in one 3-node episode , three 4-node episodes , three 5-node episodes and one 6-node episode with different strengths for the connections .",
    "the connection strengths are so chosen so that we have enough number of 3-node and 4-node episodes ( as possibly subepisodes of the embedded episodes ) spanning the range of conditional probabilities from 0.1 to 0.8 .",
    "all synaptic connections have a delay of 5ms . using our simulator described earlier ,",
    "we generated spike trains for 20 sec of time duration ( during which there are about 50,000 spikes typically ) , and obtained the counts of non - overlapped occurrences of episodes of all sizes using our datamining algorithms . in all results presented below , all statistics are calculated using 1000 repetitions of this simulation . typically , on a data sequence for 20 sec duration ,",
    "the mining algorithms ( run on a dual - core pentium machine ) take about a couple of minutes .    as explained earlier , in our simulator",
    ", the rate of the poisson process ( representing the spiking of a neuron ) is updated every 1ms based on the actual spike inputs received by that neuron .",
    "this would , in general , imply that many pairs of neurons ( especially those with strong connections ) are not spiking as independent processes .",
    "[ fig : corr ] shows this for a few pairs of neurons .",
    "the figure shows the cross correlograms ( with bin size of 1 ms and obtained using 1000 replications ) for pairs of neurons that have weak connections and for pairs of neurons that have strong connections .",
    "there is a marked peakiness in the cross correlogram for neurons with strong interconnections , as expected .",
    "[ fig : acc-3 - 4 - 5 ] shows that our theoretical model for calculating the mean and variance of of the non - overlapped count ( given by @xmath83 and @xmath110 determined through eqns .",
    "( [ eq : rec1 ] ) and ( [ eq : var ] ) ) are accurate .",
    "the figure shows plot of the mean ( @xmath83 ) and mean plus three times standard deviation ( @xmath140 ) for different values of the connection strength in terms of conditional probabilities ( @xmath43 ) , for the different episode sizes .",
    "also shown are the actual counts obtained for episodes of that size with different @xmath43 values . as is easily seen ,",
    "the theoretically calculated mean and standard deviations are very accurate .",
    "notice that most of the observed counts are below the @xmath141 threshold for @xmath142 even though this corresponds to a type - i error of just over 10% .",
    "thus our statistical test with @xmath142 or @xmath143 should be quite effective .    [ cols=\"^,^ \" , ]",
    "in this paper we addressed the problem of detecting statistically significant sequential patterns in multi - neuronal spike train data .",
    "we employed an efficient datamining algorithm that detects all frequently occurring sequential patterns with prescribed inter - neuron delays .",
    "a pattern is frequent if the number of non - overlapping occurrences of the pattern is above a threshold .",
    "the strategy of counting only the non - overlapped occurrences rather than all occurrences makes the method computationally attractive .",
    "the main contribution of the paper is a new statistical significance test to determine when the count obtained by our algorithm is statistically significant . or , equivalently",
    ", the method gives a threshold for different patterns so that the algorithm can detect only the significant patterns .",
    "the novelty in assessing the significance in our approach is in the structure of the null hypothesis .",
    "the idea is to use conditional probability as a mechanism to capture strength of influence of one neuron on another .",
    "our null hypothesis is specified in terms of a ( user - chosen ) bound on the conditional probability that @xmath2 will fire after a specified delay given that @xmath1 has fired , for any pair of neurons @xmath1 and @xmath2 .",
    "thus this compound null hypothesis includes many models of inter - dependent neurons where the influences among neurons are ` weak ' in the sense that all such pairwise conditional probabilities are below the bound .",
    "being able to reject such a null hypothesis makes a stronger case for concluding that the detected patterns represent significant functional connectivity .",
    "equally interestingly , such a null hypothesis allows us to rank order the different patterns in terms of their strengths of influence .",
    "if we chose this bound @xmath43 to be the value of the conditional probability when the different neurons are independent , then we get the usual null hypothesis of independent neuron model .",
    "but since we can choose the @xmath43 to be much higher , we can decide which patterns are significant at different levels of @xmath43 and hence get an idea of the strength of interaction they represent .",
    "thus , the method presented here extends the current techniques of significance analysis .",
    "while we specify our null hypothesis in terms of a bound on the conditional probability , note that we are not in any way estimating such conditional probabilities . estimating all relevant conditional probabilities would be computationally intensive .",
    "since our algorithm counts only non - overlapped occurrences and also uses the datamining idea of counting frequencies for only the relevant candidate patterns , our counts do not give us all the pair - wise conditional probabilities .",
    "however , the statistical analysis presented here allows us to obtain thresholds on the non - overlapped occurrences possible ( at the given confidence level ) if all the conditional probabilities are below our bound .",
    "this is what gives us the test of significance .",
    "we presented a method for bounding the probability that , under the null hypothesis , a pattern would have more than some number of non - overlapped occurrences . because we are counting non - overlapped occurrences , we are able to capture our counting process in an interesting model specified in terms of sums of independent random variables . this model allowed us to get recurrence relations for mean and variance of the random variable representing our count under the null hypothesis which allowed us to get the required threshold using chebyshev inequality .",
    "while this may be a loose bound , as shown through our simulation results , the bound we calculate is very effective .",
    "our method of analysis is quite general and it can be used in situations other than what we considered here . by choosing the value of @xmath61 in eq.([eq : xi ] )",
    "appropriately we can realize this generality in the model .    as an illustration of this",
    "we will briefly describe one extension of the model . in the method presented , while analyzing significance of a pattern @xmath25 , we are assuming that firing of @xmath3 after @xmath5 given that @xmath2 has fired is independent of @xmath1 having fired earlier .",
    "that is why we have used @xmath144 while calculating our threshold . but",
    "suppose we do not want to assume this .",
    "then we can have a null hypothesis that is specified by bounds on different conditional probabilities .",
    "suppose @xmath145 is the conditional probability that @xmath146 fires after @xmath33 given @xmath147 has fired and suppose @xmath148 be the probability that @xmath146 fires after @xmath4 and @xmath149 fires after another @xmath5 given @xmath147 has fired .",
    "now we specify the null hypothesis in terms of two parameters as : @xmath150 and @xmath151 . now for assessing significance of 3-node episodes we can use @xmath152 .",
    "our method of analysis is still applicable without any modifications .",
    "of course , now the user has to specify two bounds on different conditional probabilities and he has to have some reasons for distinguishing between the two conditional probabilities .",
    "but the main point here is that the model is fairly general and can accommodate many such extensions .",
    "there are many other ways in which the idea presented here can be extended .",
    "suppose we want to assess significance of synchronous firing patterns rather than sequential patterns based on the count of number of non - overlapped occurrences of the synchronous firing pattern .",
    "one possibility would be to use conditional probabilities of @xmath1 firing within an appropriate short time interval from @xmath2 in our null hypothesis and then use an appropriate expression for @xmath61 in our model .",
    "another example could be that of analyzing occurrences of neuronal firing sequences that respect a pre - set order on the neurons as discussed in @xcite .",
    "suppose we want to assess the significance of count of such patterns of a fixed length .",
    "if we use our type of non - overlapped occurrences count as the statistic , then the model presented here can be used to assess the significance .",
    "now the parameter @xmath61 would be the probability of occurrence of a sequence of that length ( which respects the global order on the neurons ) starting from any time instant . for a given null hypothesis , e.g. , of independence",
    ", this would be a combinatorial problem similar to the one tackled in @xcite .",
    "once we can derive an expression for @xmath61 we can use our method for assessing significance .    though we did not discuss the computational issues in this paper , the data mining algorithms used for discovering sequential patterns are computationally efficient ( see @xcite for details ) .",
    "one computational issue that may be relevant for this paper may be that of data sufficiency .",
    "all the results reported here are on spike data of 20 sec duration with background spiking rate of 20 hz .",
    "( that works out to about 400 spikes per neuron on the average in the data ) . from fig .",
    "[ fig : acc-3 - 4 - 5 ] we can see that , with this much of data , we can certainly distinguish between connection strengths that differ by about 0.2 on the conditional probability scale .",
    "( notice that , in the figure , the mean plus three sigma range of the count distribution at a connection strength is below our threshold ( with @xmath142 ) at a connection strength 0.2 more ) . in fig .",
    "[ fig : rankorder-3 - 4 - 5 ] we showed that we can reliably rank order connection strengths with about the same resolution .",
    "thus we can say that 20 sec of data is good enough for this level of discrimination .",
    "obviously , if we need to distinguish between only widely different strengths , much less data would suffice .    in terms of computational issues",
    ", we feel that one of the important conclusions from this paper is that temporal data mining may be an attractive approach for tackling the problem of discovering firing patterns ( or microcircuits ) in multi - neuronal spike trains . in temporal data mining literature , episodes are , in general , partially ordered sets of event types . here",
    "we used the methods for discovery of serial episodes which correspond to our sequential patterns .",
    "a general episode would correspond to a graph of interconnections among neurons .",
    "however , at present , there are no efficient algorithms for discovering frequently occurring graph patterns from a data stream of events .",
    "extending our data mining algorithm and our analysis technique to tackle such graph patterns is another interesting open problem .",
    "this would allow for discovery of more general microcircuits from spike trains .    in summary",
    ", we feel that the general approach presented here has a lot of potential and it can be specialized to handle many of the data analysis needs in multi - neuronal spike train data .",
    "we would be exploring many of these issues in our future work .",
    "we wish to thank mr .",
    "debprakash patnaik and mr .",
    "casey diekman for their help in preparing this paper .",
    "the simulator described here as well as the data mining package for analyzing data streams is written by mr .",
    "patnaik @xcite and he has helped in running the simulator .",
    "diekman has helped in generating all the figures .",
    "the work reported here is partially supported by a project funded by general motors r&d center , warren through sid , indian institute of science , bangalore .",
    "r.   agrawal and r.   srikant .",
    "mining sequential patterns . in philip",
    "s. yu and arbee s.  p. chen , editors , _ eleventh international conference on data engineering _ , pages 314 , taipei , taiwan , 1995 .",
    "ieee computer society press .            j. le feber , w.  l.  c.  rutten , j.  stegenga , p.",
    "s.  wolters , g.  j.  a.  ramakers and j. van pelt .",
    "conditional firing probabilities in cultured neuronal networks : a stable underlying structure in widely varying spontaneous activity patterns . , 4:5467 , 2007 .",
    "s. laxman , p.  s. sastry , and k.  p. unnikrishnan . a fast algorithm for finding frequent episodes in event streams . in _ proc .",
    "acm sigkdd int .",
    "conf . knowledge discovery and datamining _ , san jose , usa , august 2007 .",
    "k.  p. unnikrishnan , d. patnaik , and p.  s. sastry .",
    "discovering patterns in multi - neuronal spike trains using the frequent episode method .",
    "technical report , 2007 .",
    "available at http://www.citebase.org/abstract?id=oai:arxiv.org:0709.0566 ."
  ],
  "abstract_text": [
    "<S> in this paper we consider the problem of detecting statistically significant sequential patterns in multi - neuronal spike trains . </S>",
    "<S> these patterns are characterized by an ordered sequences of spikes from different neurons with specific delays between spikes . </S>",
    "<S> we have previously proposed a datamining scheme @xcite to efficiently discover such patterns which are frequent in the sense that the count of non - overlapping occurrences of the pattern in the data stream is above a threshold . here </S>",
    "<S> we propose a method to determine the statistical significance of these repeating patterns and to set the thresholds automatically . </S>",
    "<S> the novelty of our approach is that we use a compound null hypothesis that includes not only models of independent neurons but also models where neurons have weak dependencies . </S>",
    "<S> the strength of interaction among the neurons is represented in terms of certain pair - wise conditional probabilities . </S>",
    "<S> we specify our null hypothesis by putting an upper bound on all such conditional probabilities . </S>",
    "<S> we construct a probabilistic model that captures the counting process and use this to calculate the mean and variance of the count for any pattern . using this </S>",
    "<S> we derive a test of significance for rejecting such a null hypothesis . </S>",
    "<S> this also allows us to rank - order different significant patterns . </S>",
    "<S> we illustrate the effectiveness of our approach using spike trains generated from a non - homogeneous poisson model with embedded dependencies . </S>"
  ]
}