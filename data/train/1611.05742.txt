{
  "article_text": [
    "in a variety of computer vision and machine learning domains , deep neural networks have impressively surpassed conventional shallow learning models .",
    "the great success of deep neural networks is mainly attributed to the power of performing non - linear computations on the visual data , and the effectiveness of the gradient - descent training procedure based on backpropagation .    by leveraging the successful deep learning paradigm of traditional neural networks , increasing new neural networks",
    "@xcite have been built over general non - euclidean domains in recent years .",
    "for instance , @xcite proposed a spectral variant of convolution networks to graphs through graph fourier transform , which is in turn defined via a generalization of the laplacian operator on the grid to graph laplacian .",
    "more recently , @xcite presented a deep learning approach on spatio - temporal graphs by treating an arbitrary spatio - temporal graph as a recurrent neural network mixture that is feedforward , differential and jointly trainable . in @xcite , to deeply learn appropriate features on the manifolds of symmetric positive definite matrices , a deep network was build with some spectral layers , which can be trained by a variant of backpropagation .    in this paper",
    ", we focus on studying how to build a deep neural network architecture on grassmann manifolds , where the data have become a core representation of many computer vision techniques .",
    "for example , as a basic form of grassmannian data , linear subspace has proven beneficial for matching image sets due to its ability to encode the useful second - order information of one set of images from a certain object class @xcite .",
    "for video visual recognition , linear dynamic system ( i.e. , linear subspace ) of autoregressive and moving average model has shown its great power to model dynamics in spatio - temporal processing @xcite .",
    "in addition , grassmannian data also appear in image restoration @xcite , chromatic noise filtering @xcite and domain adaptation @xcite .",
    "the popular applications of grassmannian data motivate us to build a deep neural network architecture for grassmannian representation learning . for this purpose , the new network architecture",
    "is designed to take grassmannian data directly as input , and learns new favorable grassmannian data that are able to improve the final visual tasks . in other words",
    ", the new network aims to deeply learn grassmannian data on their underlying riemannian manifolds in an end - to - end learning architecture . in summary ,",
    "the main contributions of this paper are two - fold :    * we explore a new network architecture in the context of grassmann manifolds , where it has not been possible to apply deep neural networks .",
    "thanks to this network , a new direction of deeply learning grassmannian data has been opened up . *",
    "we study a procedure to train the new network . in the procedure ,",
    "we derive an update rule for the connection weights on a specific type of riemannian manifolds , and exploit matrix backpropogation to update the structured data within typical matrix factorizations .",
    "a grassmann manifold @xmath0 is a @xmath1 dimensional compact riemannian manifold , which is the set of @xmath2-dimensional linear subspaces of the @xmath3 .",
    "thus , each point on @xmath0 is a linear subspace that is spanned by its orthonormal basis matrix @xmath4 of size @xmath5 such that @xmath6 , where @xmath7 is the identity matrix of size @xmath8 .    on a grassmannian @xmath0",
    ", points are connected via smooth curves .",
    "the geodesic distance between two points is defined as the length of the shortest curve connecting them on the riemannian manifold .",
    "the shortest curve and its length are named geodesic and geodesic distance , respectively . on the grassmannian ,",
    "the geodesic distance between two points @xmath9 and @xmath10 is computed by @xmath11 where @xmath12 is the vector of principal angels between @xmath9 and @xmath10 . for its computation ,",
    "readers are referred to @xcite .",
    "one of the most popular approaches to approximate the true grassmannian geodesic distance is the projection mapping framework @xmath13 proposed by @xcite .",
    "as the projection @xmath14 is a @xmath15 symmetric matrix , a natural choice of inner product is @xmath16 .",
    "the inner product induces a distance named projection metric : @xmath17 where @xmath18 indicates the matrix frobenius norm .",
    "as proved in @xcite , the projection metric can approximate the true geodesic distance eqn.[eq0.6 ] up to a scale of @xmath19 .      to perform discriminant learning on grassmann manifolds , many works @xcite",
    "embed the grassmannian into a euclidean space .",
    "this can be achieved either by tangent space approximation of the underlying manifold , or by exploiting a positive definite kernel function to embed the manifold into a reproducing kernel hilbert space . in both of such two cases , any existing euclidean technique",
    "can then be applied to the embedded data , since hilbert spaces respect euclidean geometry .",
    "for example , @xcite first embeds the grassmannian into a high dimensional hilbert space , and then applies traditional fisher analysis method .",
    "obviously , most of these methods are limited to the mercer and hence restricted to use only kernel - based classifiers .",
    "moreover , their computational complexity increases steeply with the number of training samples .",
    "more recently , a new learning scheme was proposed in @xcite to perform a geometry - aware dimensionality reduction from the original grassmann manifold to a lower - dimensional , more discriminative grassmann manifold .",
    "this could better preserve the original riemannian data structure , which commonly leads to more favorable classification performances as studied in classical manifold learning .",
    "while @xcite has reached some success , it merely adopts a shallow learning scheme on grassmann manifolds , which is still far away from the best solution for the problem of representation learning on non - linear manifolds .",
    "accordingly , this paper attempts to open up a possibility of deep learning on grassmannians .",
    "the proposed grassmann network ( grnet ) performs deep learning on grassmannians for general visual recognition problems . as classical convolutional networks ( convnets ) , the proposed network also designs fully connected convolution - like layers and normalization layers , named full rank mapping ( frmap ) layers and orthogonal re - normalization ( reorth ) layers respectively .",
    "inspired by the geometry - aware manifold learning idea @xcite , the frmap layers are proposed to firstly perform transformations on input orthonormal matrices to generate new matrices by adopting a full rank mapping scheme .",
    "then , the reorth layers are developed to normalize the output matrices of the frmap layers so that they can become valid orthonormal matrices to form a grassmann manifold . with a specific non - linear matrix decomposition , to some extent ,",
    "the reorth layers serve as a role of introducing the non - linearity to the network as well . as traditional pooling layers",
    ", we also study special pooling layers for the grassmannian data to reduce the network complexity . as the pooling is performed on the projection matrix form of the orthonormal matrices , we call this kind of layer as projpooling layers . to enable the regular euclidean layers such as softmax layers to work for the grnet",
    ", we also exploit projection mapping ( projmap ) layers to map the resulting orthonormal matrices into projection matrices that lie in euclidean space .",
    "the proposed grassmann network is illustrated in fig.[fig1 ] .",
    "the grnet is basically to learn more compact and discriminative grassmannian data for better classification in an end - to - end learning manner . for this purpose , we design the frmap layers to first transform the input orthonormal matrices to new matrices by a linear mapping @xmath20 as @xmath21 where @xmath22 is the input orthonormal basis matrix of the @xmath23-th layer , @xmath24 is the transformation matrix ( connection weights ) that is basically required to be a full - rank matrix , @xmath25 is the resulting matrix .",
    "unfortunately , except the case @xmath26 is an orthogonal matrix , @xmath27 is not generally an orthonormal basis matrix . to tackle this problem , we exploit a normalization strategy of qr decomposition in the following reorth layer .",
    "in addition , as classical deep networks , multiple projections @xmath28 per frmap layer can be applied on each input orthonormal matrix as well , where @xmath29 is the number of transformation matrices .    as the weight space @xmath30 of full - rank matrices on each frmap layer is a non - compact stiefel manifold where the distance function has no upper bound , optimizing on the manifold directly is infeasible . to handle this problem , one possible",
    "solution is imposing orthogonality constraint on the transformation matrix @xmath31 so that it resides on a compact stiefel manifold @xmath32 .",
    "obviously , such orthogonal solution space is smaller than the original solution space @xmath33 , making the optimization theoretically yield suboptimal solution of @xmath31 . to achieve more faithful solution of the full - rank transformation matrix @xmath31 , following @xcite",
    ", we alternatively choose to do the optimization over the manifolds @xmath34 is the set of @xmath35-rank positive semidefinite matrices of size @xmath36 @xcite .",
    "] of the conjugates @xmath37 , which are actually positive semidefinite ( psd ) matrices . as studied in @xcite , since @xmath38 can be easily parametrized by @xmath31",
    ", optimizing on the psd manifold actually pursues optimal @xmath31 directly .      to make the resulting matrices reside on a valid grassmann manifold , the reorth layer normalizes the matrix @xmath39 to @xmath40 so that the columns of @xmath40 are orthonormal",
    "specifically , we perform qr decomposition on @xmath39 : @xmath41 where @xmath42 is the orthonormal matrix composed by the first @xmath2 columns , and @xmath43 is the invertible upper - triangular matrix . since @xmath44 is invertible and @xmath45 is orthonormal , we can make @xmath40 become an orthonormal basis matrix by normalizing @xmath39 in the @xmath23-th layer as : @xmath46    in the context of convnets , @xcite have presented various nonlinear activation functions , e.g. , rectified linear units ( relu ) , to improve discriminative performance .",
    "accordingly , exploiting this kind of layers to introduce the non - linearity to the domain of the proposed grnet is also necessary . in the light of this , to some extent , the function eqn.[eq1.0 ] also takes a role of performing a non - linear operation based on the qr factorization .",
    "the projmap layer is designed to perform riemannian computing on the resulting orthonormal matrices on one grassmannian . as well - studied in @xcite ,",
    "the projection metric is one of the most popular grassmannian metrics , and is able to endow the specific riemannian manifold with an inner product structure so that the manifold is reduced to a flat space . in the flat",
    ", classical euclidean computations can be applied to the projection domain of orthonormal matrices .",
    "formally , we apply the projection mapping @xcite on a orthonormal matrix in the @xmath23-th layer with the function @xmath47 being defined as @xmath48    as for other riemannian computations on grassmann manifolds , please refer to @xcite for more detailed discussions about their properties .",
    "it is known that classical pooling layers with max , min and mean pooling functions reduce the sizes of the representations to lower the model complexity and improve the regular convnets .",
    "motivated by this , we also design pooling layers tailored for the feature maps of grassmannian data . without loss of generality",
    ", we here study the mean pooling functions for the grassmannian points .",
    "as far as we investigate , there exist some approaches @xcite to compute mean points on grassmannians .",
    "one of the most popular methods is to compute the karcher mean @xcite , which is the intrinsic mean on grassmann manifolds by minimizing the geodesic distances of the difference defined in eqn.[eq0.6 ] .",
    "unfortunately , the karcher mean algorithms require very expensive iterative exponential and logarithm maps to move the data to and from the tangent space . to speed up the computation , @xcite proposed an alternative to the karcher mean by minimizing the frobenius norm squared of the difference of the projection maps of grassmannian data .    inspired by the idea @xcite",
    ", we propose three layers to implement the average pooling for grassmannian data .",
    "in particular , the grassmannian data are first mapped to the space of projection matrices by the projmap layer presented before .",
    "as the resulting projection matrices @xmath49 are euclidean , we then design a regular mean pooling layer for them .",
    "lastly , we devise an orthogonal map ( orthmap ) layer to map the mean projection matrix back to the underlying grassmann manifold .",
    "formally , the functions for the last two layers ( i.e. , projpooling and orthmap ) are defined as @xmath50 where @xmath51 is the first @xmath2 leadest eigenvectors achieved by symmetric eigenvalue ( eig ) computation on the input projection matrices @xmath52 , and @xmath53 is the number of instances for the pooling . note that the instances for pooling could be multiple projection matrices or multiple elements within an @xmath54 patch located in one projection matrix . in the experiments , we will further study these two different pooling schemes .",
    "after applying the projmap layer , the outputs ( i.e. , the projection matrices ) lie in euclidean space and thus can be transformed into vector forms .",
    "hence , on the top of the projmap layer , classical euclidean network layers can be employed .",
    "for instance , the regular fully connected ( fc ) layer could be utilized after the projmap layer .",
    "the dimensionality of the filters in the fc layer is typically set to @xmath55 , where @xmath35 and @xmath56 are the class number and the dimensionality of the input vector forms respectively . in the end",
    ", the common softmax layer or softmax log - loss layer can be equipped for the visual classification tasks .",
    "as most layers in the grnet model are expressed with the complex matrix factorization functions , they can not be simply reduced to a constructed bottom - up from element - wise calculations . in other words , the matrix backpropgation can not be derived by using traditional matrix that deals element - wise operations in matrix form . as a result , simply using the traditional backpropgation training procedure will break down in the setting . to solve the problem ,",
    "@xcite introduced manifold - valued connection weight update rule and matrix backpropagation ( backprop ) respectively .",
    "furthermore , the convergence of the stochastic gradient descent ( sgd ) algorithm on riemannian manifolds has also been studied well in @xcite .",
    "accordingly , we exploit the training procedure for the proposed grnet upon these two works .    to begin with , we represent the proposed grnet model with a sequence of successive function compositions @xmath57 with a parameter tuple @xmath58 , where @xmath59 and @xmath31 are the function and the weight matrix respectively for the @xmath23-th layer , and @xmath60 is the number of layers",
    ". the loss of the @xmath23-th layer can be denoted by a function as @xmath61 , where @xmath62 is the loss function for the last output layer .    then",
    ", we recall the definition of the matrix backprop and its properties studied in @xcite . in particular , @xcite exploits a function @xmath63 to describe the variations of the upper layer variables with respect to the lower layer variables , i.e. , @xmath64 .",
    "consequently , a new version of the chain rule for the matrix backprop is defined as @xmath65 where @xmath66 is the desired output , @xmath67 , @xmath68 is a non - linear adjoint operator of @xmath63 , i.e. , @xmath69 , the matrix inner product @xmath70 .    in the sequel",
    ", we will detail the connection weight update on manifolds and the matrix backprop process through some key layers in the context of the proposed grnet . for simplicity ,",
    "we uniformly let @xmath71 be @xmath72 , @xmath73 be @xmath45 , and @xmath74 be @xmath44 respectively in the following .      for the weight update in the frmap layers ,",
    "we propose a new way of updating the weights appeared in eqn.[eq1 ] by exploiting an sgd setting on psd manifolds .",
    "as studied in @xcite , the steepest descent direction for the corresponding loss function @xmath75 with respect to @xmath31 on one riemannian manifold is the riemannian gradient @xmath76 . in particular ,",
    "the parallel transport is first applied to transfer the euclidean gradient in the tangent space at the anchor point @xmath31 to the one in the tangent space at the point @xmath77 .",
    "then the resulting euclidean gradient is subtracted to the normal component of the euclidean gradient @xmath78 .",
    "after this operation , searching along the tangential direction yields the update in the tangent space of the psd manifold . finally , the resulting update is mapped back to the psd manifold with a retraction operation @xmath79 . for more details about the geometry of psd manifolds and its retraction operation ,",
    "readers are referred to @xcite .",
    "accordingly , the update of the connection weight @xmath31 on the psd manifold respects the following form @xmath80 where @xmath81 is the current weight , @xmath79 is the retraction operation , @xmath82 is the learning rate , @xmath83 is the normal component of the euclidean gradient @xmath78 . by employing the conventional backprop ,",
    "@xmath78 is computed by @xmath84      actually , the reorth layers involve qr decomposition eqn.[eq1.00 ] and the non - linear operation eqn.[eq1.0 ] .",
    "firstly , for eqn.[eq1.00 ] we introduce a virtual layer @xmath85 , which receives @xmath39 as input and produces a tuple @xmath86 , @xmath87 . following @xcite to deal with the case of outputting a tuple",
    ", we apply the new chain rule eqn.[eq10 ] with the equations @xmath69 and @xmath64 to achieve the update rule for the data : @xmath88 where the two variations @xmath89 and @xmath90 are derived by the variation of the qr operation @xmath91 as : @xmath92 where @xmath93 , @xmath94 is an identity matrix , @xmath95 , @xmath96 extracts the elements below the main diagonal of @xmath97 . for more details to derive eqn.[eq12 ] and",
    "eqn.[eq13 ] , readers are referred to the first part of the appendix .    as derived in the second part of the appendix ,",
    "plugging eqn.[eq12 ] and eqn.[eq13 ] into eqn.[eq11 ] gives the partial derivatives of the loss functions for the reorth layers : @xmath98 where @xmath99 , @xmath96 extracts the elements below the main diagonal of @xmath97 .",
    "@xmath100 and @xmath101 can then be obtained on the function eqn.[eq1.0 ] employed in the reorth layers .",
    "specially , its variation becomes @xmath102 .",
    "therefore , the involved partial derivatives with respect to @xmath45 and @xmath44 are computed by @xmath103      as presented before , the orthmap layers involve symmetric eigenvalue ( eig ) computation .",
    "thus , we adopt the proposition in @xcite to calculate the partial derivatives for the eigenvalue computation in the matrix backprop setting .",
    "* proposition 1 * _ let @xmath104 with @xmath105 , such that @xmath106 and @xmath107 owns a diagonal structure .",
    "the resulting partial derivative for the eig layer @xmath85 is given by _",
    "@xmath108 where @xmath109 ( here , @xmath110 is the diagonal element of @xmath107 ) , and the partial derivatives with respect to @xmath107 and @xmath111 in eqn.[eq4.1 ] for the orthmap layers can be calculated by @xmath112 , \\label{eq17.1}\\\\      \\frac{\\partial l^{(k^{'})}}{\\partial \\bm{\\sigma } } & = 0 , \\label{eq17.2 }      \\end{aligned}\\ ] ] where @xmath113 in eqn.[eq17.1 ] is the matrix of size @xmath114 with all elements being zero .",
    "in this section we study the effectiveness of the proposed grnet structure . in particular , we present results for three typical visual classification tasks being emotion recognition , action recognition and face verification . in the experiments , we compare four state - of - the - art grassmann learning methods : discriminative canonical correlations ( dcc ) @xcite , grassmann discriminant analysis ( gda ) @xcite , grassmannian graph - embedding discriminant analysis ( ggda ) @xcite and projection metric learning ( pml ) @xcite methods .",
    "for all of them , we use their source codes from authors with tuning their parameters as in the original papers . for the proposed grnet",
    ", we build its architecture with one or multiple block(s ) illustrated in fig.[fig1 ] before three output layers , that are projmap , fc and softmax layers .",
    "the learning rate @xmath82 is fixed as @xmath115 , the batch size is set to 30 .",
    "note that multiple projections per frmap layer ( m - frmap ) and matrix element - based projpooling ( e - projpooling ) are studied first before the discussion where other configurations are compared .",
    "the projection matrices per frmap layer are initialized as random full rank matrices , and the number of them per layer is set to 16 .",
    "for all the projpooling layers , the number of the instances for pooling are fixed as 4 . for training the grnet",
    ", we just use an i7 - 2600k ( 3.40ghz ) pc without any gpus .      for the task of emotion recognition",
    ", we employ the popular acted facial expression in wild ( afew ) @xcite dataset .",
    "the afew dataset contains a dynamic temporal facial expressions data in close to real world setting .",
    "it collects 1,345 video sequences of facial expressions acted by 330 actors .    the standard protocol @xcite splits the dataset into three data sets , i.e. , training , validation and test data sets . in the training and validation data sets , each video is classified into one of seven expressions , while the ground truth of the test set has not been released .",
    "as a result , we follow @xcite to present the results on the validation set . as done in many works such as @xcite for augmenting the training data , we split the training videos to 1,747 small subvideos . for the evaluation ,",
    "each facial frame is normalized to an image of size @xmath116 .",
    "then , following @xcite , we mode each sequence of facial expression with a linear subspace of order 10 . with this setting ,",
    "the linear subspace representations of the video samples span a grassmann manifold @xmath117 .",
    ".emotion / action recognition accuracies for the afew and hdm05 database . [ cols=\"<,^,^\",options=\"header \" , ]      for face verification , we use one standard video face dataset being point - and - shoot challenge ( pasc ) @xcite .",
    "it contains 2,802 videos of 265 subjects , two halves of which are taken by control and handheld cameras respectively .",
    "for the dataset , @xcite designs control and handheld face verification experiments , where the in the query video is verified by comparing with the target video . as done in @xcite , we use its 280 training videos and the external training data ( cox ) @xcite with 900 videos for training . as the last two experiments ,",
    "the whole training data are also split to 12,529 small subvideos . to extract the state - of - the - art deep face features ,",
    "we perform the approach of @xcite on the normalized face images of size @xmath118 . for speeding up the training",
    ", we employ pca to reduce the deep features to 400-dimensional features . following @xcite , a spd matrix of size @xmath119",
    "is computed by fusing the data covariance matrix and mean for each video sequence . as done in @xcite on each video , we finally compute a linear subspace of order 10 , which is on a grassmann manifold @xmath120 .    in the evaluation , we configure the sizes of the grnet weights in frmap layers to @xmath121 in the 1 block case , while setting those to @xmath122 and @xmath123 in the 2 block case .",
    "the time for training the grnet at each of 100 epoches is about 13 m .",
    "table.[tab1 ] compares the accuracies of the different methods including the state - of - the - art methods ( vggdeepface @xcite , deepo2p @xcite and spdnet @xcite ) on the pasc database .",
    "although the used softmax output layer in the grnet do not suit the verification task well , we find that it still reaches the highest performances in the case of 2 blocks , which learns more favorable grassmannian data .          for the manifolds of spd matrices , @xcite developed a deep network ( spdnet ) , which has shown its superiority over shallow spd matrix learning scheme as well as recent deep learning methods ( e.g. , deepo2p @xcite ) that work on the euclidean forms of manifold data . in this paper , the proposed grnet acts as a deep learning tool on grassmann manifolds",
    "obviously , different from spdnet , it designs the layers specially for grassmannian data .",
    "besides , the grnet optimizes the connection weights on psd manifolds rather than stiefel manifolds because the former could get better solutions . to validate this , we compare them on the three used datasets , where the stiefel optimization often achieves worse results ( 32.13% , 57.25% , 80.15% , 71.28% )",
    ". moreover , the grnet also exploits single and multiple projections per frmap layer ( i.e. , s - frmap , m - frmap ) simultaneously , and devises pooling layer on multiple projections ( p - projpooling ) and multiple elements ( e - projpooling ) .",
    "note that the projpooling works together with m - frmap .",
    "these properties also considerably differ from existing grassmann shallow learning methods e.g. , pml method @xcite . as presented in fig.[fig3 ] ( a ) , for the three used datasets , m - frmap typically beats s - frmap , and e - projpooling outperforms p - projpooling .",
    "finally , we also study that the grnet is able to converge after hundreds of epoches as shown in fig.[fig3 ] ( b ) .",
    "grassmannian data are becoming popular in computer vision community .",
    "motivated by this , in this paper we introduced grassmann networks , the first deep architecture that performs deep learning over grassmann manifolds .",
    "essentially , it is a natural exploration of classical deep convolutional neural networks to perform global convolution , normalization , pooling and remannian computing on grassmannian data . in three typical visual classification evaluations ,",
    "the proposed network significantly outperformed existing grassmann learning methods , and performed comparably with state - of - the - art methods .",
    "directions for future work include extending to more general riemannian manifolds ( e.g. , stiefel manifolds ) and applying to other computer vision problems ( e.g. , image restoration ) .",
    "for the qr operation , we differentiate its implicit system @xmath124 where @xmath125 returns the elements below the main diagonal of @xmath44 , and obtain @xmath126 which means @xmath127 is antisymmetric .",
    "multiplying eqn.[eq20 ] from the left with @xmath128 and the right with @xmath129 derives @xmath130          by substituting eqn.[eq25 ] into eqn.[eq22 ] , the gradient of the qr decomposition with respect to @xmath44 is derived as eqn.[eq13 ] . by pluging eqn.[eq13 ] into eqn.[eq23.1 ] , we can derive the gradient of the qr decomposition with respect to @xmath45 as eqn.[eq12 ] .      at first , we employ some properties of matrix inner product @xmath70 , that were studied in @xcite , to derive the following equivalent equation @xmath135 where @xmath136 , @xmath99 , @xmath96 extracts the elements below the main diagonal of @xmath97 .",
    "j.  r. beveridge , p.  j. phillips , d.  s. bolme , b.  a. draper , g.  h. given , y.  m. lui , m.  n. teli , h.  zhang , w.  t. scruggs , k.  w. bowyer , et  al .",
    "the challenge of face recognition from digital point - and - shoot cameras . in _ btas _ , 2013 .",
    "d.  boscaini , j.  masci , s.  melzi , m.  bronstein , u.  castellani , and p.  vandergheynst . learning class - specific descriptors for deformable shapes using localized spectral convolutional networks . in _",
    "computer graphics forum _ , volume  34 , pages 1323 .",
    "wiley online library , 2015 ."
  ],
  "abstract_text": [
    "<S> representing the data on grassmann manifolds is popular in quite a few image and video recognition tasks . in order to enable deep learning on grassmann manifolds , </S>",
    "<S> this paper proposes a deep network architecture which generalizes the euclidean network paradigm to grassmann manifolds . </S>",
    "<S> in particular , we design full rank mapping layers to transform input grassmannian data into more desirable ones , exploit orthogonal re - normalization layers to normalize the resulting matrices , study projection pooling layers to reduce the model complexity in the grassmannian context , and devise projection mapping layers to turn the resulting grassmannian data into euclidean forms for regular output layers . to train the deep network , </S>",
    "<S> we exploit a stochastic gradient descent setting on manifolds where the connection weights reside on , and study a matrix generalization of backpropagation to update the structured data . </S>",
    "<S> we experimentally evaluate the proposed network for three computer vision tasks , and show that it has clear advantages over existing grassmann learning methods , and achieves results comparable with state - of - the - art approaches . </S>"
  ]
}