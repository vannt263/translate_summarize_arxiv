{
  "article_text": [
    "let @xmath5 be a semimartingale , which is observed at discrete times @xmath6 for @xmath7 , over a finite time interval @xmath8 $ ] , with a discretization mesh @xmath9 which is small and eventually goes to @xmath10 ( high - frequency setting ) .",
    "one of the main problems encountered in practice is the estimation of the integrated ( squared ) volatility ( in finance terms ) , or equivalently of the continuous part of the quadratic variation @xmath11_t$ ] .    by now ,",
    "this is a well - understood problem , at least when @xmath5 is an it semimartingale .",
    "for example , in the continuous one - dimensional case , if @xmath5 takes the form @xmath12 the approximate quadratic variation @xmath13}(x_{i{\\delta}_n}-x_{(i-1){\\delta}_n})^2 $ ] , which of course converges to @xmath11_t=\\int_0^t{\\sigma}^2_s\\,ds$ ] , enjoys a central limit theorem ( clt ) : the difference between these two processes , normalized by @xmath14 , converges stably in law to a limit which is conditionally on @xmath5 a continuous gaussian martingale with quadratic variation ( equivalently , with variance ) twice the so - called `` quarticity , '' that is , @xmath15 .",
    "although later we consider a much more general framework , allowing @xmath5 to be multi - dimensional and with jumps , in the we pursue the discussion in this special one - dimensional continuous case . in various statistical problems",
    "one needs to estimate not only the quarticity , but functionals of the form @xmath16 ( for relatively general test functions @xmath17 , and to derive associated clts , see  @xcite ) ; notice that we plug in the `` spot '' squared volatility @xmath18 rather than @xmath19 , since in any case it is impossible to determine the sign of @xmath19 on the basis of the observation of the path @xmath20 .",
    "the case @xmath21 corresponds to the usual integrated volatility , and @xmath22 to the quarticity .    toward this aim ,",
    "two methods are currently at hand :    the first one is available if @xmath23 for all , where the @xmath24 s are independent @xmath25 variables and @xmath26 is a continuous function on @xmath27 , of polynomial growth .",
    "then we know that @xmath28-k+1 } f \\biggl(\\frac{{\\delta}^n_ix}{{\\sqrt{{\\delta}_n}}},\\ldots,\\frac{{\\delta}^n_{i+k-1}x}{{\\sqrt{{\\delta}_n } } } \\biggr)\\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\eqntext{\\displaystyle \\mbox{where } { \\delta}^n_ix = x_{i{\\delta}_n}-x_{(i-1){\\delta}_n},}\\end{aligned}\\ ] ] converges to @xmath29 in probability , and if @xmath26 is @xmath30 the rate of convergence is @xmath31 , and in the associated clt the limiting conditional variance is @xmath32 for a suitable function @xmath33 .",
    "the second one consists in using estimators for the spot volatility and approximating the integral @xmath29 by riemann sums , in which the spot volatility is replaced by its estimator ; that is , we set @xmath34-k_n+1}g\\bigl(\\widehat{c}{}^n_i \\bigr)\\qquad \\mbox{where } \\widehat{c}{}^n_i=\\frac1{k_n { \\delta}_n}\\sum_{j=0}^{k_n-1}\\bigl ( { \\delta}^n_{i+j}x\\bigr)^2\\ ] ] for an arbitrary sequence of integers such that @xmath35 and @xmath36 .",
    "then one knows that @xmath37 ( when @xmath17 is continuous and of polynomial growth ) .",
    "but so far nothing is known about the rate of convergence of these estimators when @xmath38 goes to infinity ( the situation @xmath39 not depending on @xmath40 is studied in  @xcite where the rate @xmath41 is obtained for power functions ) .",
    "the first method is quite powerful and gives optimal rates , but the special form of @xmath17 puts strong constraints on this function [ e.g. , it is @xmath42 on @xmath43 , and much more ] . to tell the truth , in the one - dimensional case , by far the most useful test functions @xmath17 are the powers @xmath44 ( recall that @xmath45 here ) for @xmath46 , which are associated as above with @xmath47 , where @xmath48 is the @xmath49th absolute moment of @xmath50 .",
    "nevertheless , some functions @xmath17 of interest might not be , or not in an obvious way , of this form or , more generally , linear combinations of functions of this form . in the multivariate case",
    ", however , with @xmath5 being @xmath51-dimensional and thus @xmath52 above as well , one typically finds asymptotic variances which are complicated functions of the @xmath53-dimensional spot volatility .",
    "this is , for instance , the case when studying multipower variations for integrated volatility estimation in the presence of jumps ; see , for example , @xcite .",
    "in this situation and more generally for an arbitrary ( smooth ) function @xmath17 on the set @xmath54 of all @xmath53 symmetric nonnegative matrices , it is rather a difficult task in practice to find an integer @xmath55 and a function @xmath26 on @xmath56 such that , for all @xmath57 , we have @xmath58 , where again the @xmath24 s are ( @xmath51-dimensional ) i.i.d . @xmath25 .    in addition",
    ", this first method does not provide efficient estimation in general . to see that , consider the toy example @xmath59 , where @xmath60 is a constant , @xmath61 , @xmath62 and @xmath63 .",
    "we thus observe the increments @xmath64 for @xmath65 , or equivalently the @xmath40 variables @xmath66 .",
    "these variables are i.i.d .",
    "@xmath67 , so the asymptotically best estimators for @xmath68 ( efficient in all possible senses , and also the mle ) are @xmath69 , with convergence rate @xmath70 and asymptotic variance @xmath71 .",
    "if instead one wants to estimate @xmath72 for some @xmath73 in @xmath43 , one can use @xmath74 , and the ordinary central limit theorem tells us that the rate of convergence is again @xmath70 , and the asymptotic variance is @xmath75 : this is exactly what the first method above does .",
    "but this is not optimal , the asymptotically optimal estimators being @xmath76 ( the mle again ) , with rate @xmath70 and asymptotic variance @xmath77 , smaller than the previous one when @xmath73 .",
    "now , taking @xmath76 is exactly what the second method ( [ i-2 ] ) does .",
    "the aim of this paper is to develop the second method , and in particular to provide a central limit theorem , with the rate @xmath31 ( as it is usually the case in a nonparametric setting for integrated functionals estimation ; see , e.g. ,  @xcite ) , and with an asymptotic variance always smaller than if one uses the first method .",
    "this can be viewed as an extension , in several directions , of the `` block method '' of mykland and zhang in  @xcite .",
    "about efficiency , and despite the title of the paper , we do not really examine the question in the general nonparametric or semi - parametric setting assumed below , since even for the simpler problem of estimating the integrated volatility , the concept of efficiency is not well established so far .",
    "instead , we will term as `` efficient '' a procedure which is efficient in the usual sense for the sub - model consisting in the toy model @xmath59 above , and efficient in the sense of the hajek convolution theorem , for the markov - type model recently studied by clment , delattre and gloter in  @xcite and of the form @xmath78 where @xmath79 are unknown smooth enough functions and @xmath80 arbitrary processes and where the two brownian motions @xmath81 are independent .    this will be done in the multivariate setting and when @xmath5 possibly has jumps ( upon suitably truncating the increments in ( [ i-2 ] ) if it is the case , in the spirit of ) , and under the additional assumptions that @xmath18 itself is an it semimartingale and that , when @xmath5 jumps , these jumps are summable , which are exactly the same assumptions under which the truncated versions of @xmath82 in ( [ i-1 ] ) converge with rate @xmath31 .",
    "the paper is organized as follows : section  [ sec - set ] is devoted to presenting the assumptions .",
    "results are given in section  [ sec - res ] , and all proofs are gathered in section  [ sec - p ] .",
    "the underlying process @xmath5 is @xmath51-dimensional , and observed at the times @xmath6 for @xmath7 , within a fixed interval of interest @xmath0 $ ] . for any process",
    "@xmath83 we use the notation @xmath84 defined in ( [ i-1 ] ) for the increment over the @xmath85th observation interval .",
    "we assume that the sequence @xmath9 goes to @xmath10 .",
    "the precise assumptions on @xmath5 are as follows :    first , @xmath5 is an it semimartingale on a filtered space @xmath86 .",
    "it can be written in its grigelionis form , using a @xmath51-dimensional brownian motion @xmath87 and a poisson random measure @xmath88 on @xmath89 , where @xmath90 is an auxiliary polish space and with the ( nonrandom ) intensity measure @xmath91 for some @xmath60-finite measure @xmath92 on @xmath90 , @xmath93 this is a vector - type notation : the process @xmath94 is @xmath95-valued optional , the process @xmath19 is @xmath96-valued optional , @xmath97 is a predictable @xmath95-valued function on @xmath98 and @xmath99 denotes the euclidean norm on any finite - dimensional linear space .",
    "besides the measurability requirements above , and for any @xmath100 $ ] , we introduce the assumption :    [ assumhr ] there are a sequence @xmath101 of nonnegative bounded @xmath92-integrable functions on @xmath90 and a sequence @xmath102 of stopping times increasing to @xmath103 , such that @xmath104\\\\[-8pt ] t&\\leq&\\tau_n(\\omega ) \\quad\\rightarrow\\quad\\bigl\\|\\delta(\\omega , t , z)\\bigr\\|^r\\wedge1\\leq j_n(z).\\nonumber\\end{aligned}\\ ] ]    the spot volatility process @xmath105 ( @xmath106 denotes transpose ) takes its values in the set @xmath54 of all nonnegative symmetric @xmath53 matrices .",
    "we will indeed suppose that @xmath18 is again an it semimartingale , and we consider the following assumption :    [ assumar ] the process @xmath5 satisfies assumption  [ assumhr ] , the associated volatility process @xmath68 satisfies ( h-@xmath107 ) and the processes @xmath94 and , when @xmath108 , @xmath109 are cgld or cdlg .",
    "the bigger @xmath110 is , the weaker assumption  [ assumar ] is , and when ( a-@xmath10 ) holds the process @xmath5 has finitely many jumps on each finite interval . since we suppose in the theorems of the next section that @xmath111 , the last condition in ( [ s-3 ] ) implies that @xmath112 is indeed well defined , and it is the `` genuine '' drift , in the sense that this is the drift after removing the sum @xmath113 of all jumps ( which here are summable , and we even have @xmath114 a.s . here ) .",
    "in order to define the estimators of the spot volatility , we need to fix a sequence @xmath38 of integers and a sequence @xmath115 of cut - off levels in @xmath116 $ ] .",
    "the @xmath117-valued variables @xmath118 are defined , componentwise , as @xmath119 and they implicitly depend on @xmath120 .",
    "a natural idea is to choose the sequence @xmath38 satisfying , as @xmath121 , @xmath122 indeed , one knows that @xmath123}{\\stackrel{{\\mathbb{p}}}{\\longrightarrow}}c_t$ ] for any @xmath124 , as soon as @xmath35 and @xmath36 , and there is an associated central limit theorem under assumption  [ assumar ] for some @xmath125 , with rate @xmath126 , which reaches its biggest value @xmath127 when @xmath128 : this choice of @xmath38 ensures a balance between the involved `` statistical error '' which is of order @xmath129 , and the variation of @xmath18 over the interval @xmath130 $ ] , which is of order @xmath131 because @xmath18 is an it semimartingale ( and even when it jumps ) ; see  @xcite .    by theorem 9.4.1 of  @xcite , and",
    "again as soon as @xmath35 and @xmath36 , one also knows that @xmath132-k_n+1}g\\bigl(\\widehat{c}{}^n_i \\bigr)\\quad{\\stackrel{\\mathrm{u.c.p.}}{\\longrightarrow}}\\quad v(g)_t:=\\int_0^tg(c_s ) \\,ds\\ ] ] ( convergence in probability , uniform over each compact interval ; by convention @xmath133 whenever @xmath134 ) , as soon as the function @xmath17 on @xmath54 is continuous with @xmath135 for some constants @xmath136 , and under either one of the following three conditions : @xmath137 notice the upper limit in definition ( [ r-4 ] ) of @xmath138 : this is to ensure that @xmath138 is actually computable from the observations up to the time horizon  @xmath124 .",
    "note also that when @xmath5 is continuous , the truncation in ( [ r-3 ] ) is useless : one may use ( [ r-3 ] ) with @xmath139 , which reduces to ( [ i-2 ] ) in the one - dimensional case .",
    "now , we want to determine at which rate convergence ( [ r-4 ] ) takes place",
    ". this amounts to proving an associated central limit theorem . under the restriction @xmath111 and an appropriate choice of the truncation levels , such a clt is available for @xmath140 , with the rate @xmath31 , but the limit exhibits a bias term .",
    "below , @xmath17 is a smooth function on @xmath54 , and the two first partial derivatives are denoted as @xmath141 and @xmath142 , since any @xmath57 has @xmath143 components @xmath144 .",
    "the family of all partial derivatives of order @xmath145 is simply denoted as @xmath146 .",
    "[ tr-01 ] assume assumption  [ assumar ] for some @xmath111 .",
    "let @xmath17 be a @xmath147 function on @xmath54 such that @xmath148 for some constants @xmath149 .",
    "either suppose that @xmath5 is continuous and @xmath150 for some @xmath151 ( e.g. , @xmath152 , so there is no truncation at all ) , or suppose that @xmath153 then we have the finite - dimensional ( in time ) stable convergence in law @xmath154 where @xmath155 is a process defined on an extension @xmath156 of @xmath157 , which conditionally on @xmath158 is a continuous centered gaussian martingale with variance @xmath159 and where @xmath160 where @xmath161 is the volatility process of @xmath18 , @xmath162 with @xmath163 .",
    "note that @xmath164 , so the sum defining @xmath165 is absolutely convergent , and vanishes when @xmath18 is continuous .",
    "the bias has four parts :    the first part @xmath166 is a border effect , easily eliminated by taking @xmath167-k_n+1 } \\bigr)\\bigr)\\ ] ] instead of @xmath168 : we then have @xmath169 , and this convergence is even functional in time when @xmath18 is continuous .",
    "the second part @xmath170 is continuous in time and is present even for the toy model @xmath171 with @xmath68 a constant and @xmath62 and @xmath63 . in this simple case",
    "it can be interpreted as follows : instead of taking the `` optimal '' @xmath172 for estimating @xmath173 , with @xmath174 , one takes @xmath175 with @xmath176 a `` local '' estimator of @xmath68 .",
    "this adds a statistical error which results in a bias .    the third and fourth parts @xmath177 and @xmath178 are , respectively , continuous and purely discontinuous , due to the continuous part and to the jumps of the volatility process @xmath18 itself .",
    "these two biases disappear if we take @xmath179 in ( [ r-01 ] ) ( with still @xmath180 ) .",
    "the only test function @xmath17 for which the biases",
    "@xmath181 disappear is the identity @xmath21 .",
    "this is because , in this case , and up to border terms , @xmath182 is nothing but the realized quadratic variation itself and the spot estimators @xmath176 actually merge together and disappear as such .",
    "it is possible to consistently estimate @xmath183 , and thus de - bias @xmath182 and obtain a clt with a conditionally centered gaussian limit .",
    "consistent estimators for @xmath184 are easy to derive , since @xmath185 for the function @xmath186 .",
    "consistent estimators for @xmath187 and @xmath188 , involving the volatility and the jumps of @xmath18 , are more complicated to describe , especially the last one , and also likely to have poor performances .",
    "all the details about the way to remove the bias together with the proof of theorem  [ tr-01 ] can be found in  @xcite .      in front of the difficulties involved in de - biasing the estimators @xmath168",
    "above , we in fact choose a window size @xmath38 smaller than the one in ( [ r-01 ] ) .",
    "namely , we choose @xmath38 such that , as @xmath189 , @xmath190 of course , the second condition enables us to make the first and last two bias terms in theorem  [ tr-01 ] vanish , which is technically very convenient .",
    "however , it amplifies the first bias term , which becomes the leading term in the difference @xmath191 , and thus a prior de - biasing is necessary if we want a rate @xmath31 .",
    "this leads us to consider the following estimator : @xmath192-k_n+1 } \\biggl(g\\bigl ( \\widehat{c}{}^n_i\\bigr)-\\frac1{2k_n } \\sum _ { j , k , l , m=1}^d \\partial^2_{jk , lm } g\\bigl(\\widehat{c}{}^n_i\\bigr)\\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\hspace*{161.7pt}{}\\times \\bigl(\\widehat{c}{}^{n , jl}_i \\widehat{c}{}^{n , km}_i + \\widehat{c}_i^{n , jm } \\widehat{c}_i^{n , kl } \\bigr ) \\biggr).\\hspace*{-22pt}\\nonumber\\end{aligned}\\ ] ]    this estimator uses overlapping intervals , in the sense that we estimate @xmath193 on the basis of the time window @xmath194 $ ] , and then sum over all @xmath85 s .",
    "another version is indeed possible , which does not use overlapping intervals and is as follows : @xmath195 - 1 } \\biggl(g \\bigl(\\widehat{c}{}^n_{ik_n+1}\\bigr ) \\nonumber\\\\ & & \\hspace*{73.3pt}{}-\\frac1{2k_n } \\sum_{j , k , l , m=1}^d \\partial^2_{jk , lm } g\\bigl(\\widehat{c}{}^n_{ik_n+1 } \\bigr ) \\\\ & & \\hspace*{141pt}{}\\times\\bigl(\\widehat{c}{}^{n , jl}_{ik_n+1 } \\widehat{c}{}^{n , km}_{ik_n+1 } + \\widehat{c}_{ik_n+1}^{n , jm } \\widehat{c}_{ik_n+1}^{n , kl } \\bigr ) \\biggr ) .",
    "\\nonumber\\end{aligned}\\ ] ]    we can now give the final version of our associated central limit theorems .    [ tr-1 ] assume assumption  [ assumar ] for some @xmath111 .",
    "let @xmath17 be a @xmath147 function on @xmath54 such that @xmath196 for some constants @xmath149 .",
    "either suppose that @xmath5 is continuous and @xmath150 for some @xmath151 ( e.g. , @xmath152 , so there is no truncation at all ) , or suppose that @xmath197 then under ( [ r-1 ] ) we have the two ( functional in time ) stable convergences in law @xmath198 where @xmath155 is a process defined on an extension @xmath156 of @xmath157 , which conditionally on @xmath158 is a continuous centered gaussian martingale with variance @xmath199    [ rr-02 ] when @xmath5 jumps , the requirement ( [ r-9 ] ) is exactly the same as in theorem  [ tr-01 ] , and it implies @xmath111 .",
    "this restriction is not a surprise , since one needs @xmath108 in order to estimate the integrated volatility by the ( truncated ) realized volatility , with a rate of convergence @xmath31 .",
    "indeed , it is shown in  @xcite that if @xmath200 , the optimal rate in the minimax sense is @xmath201 .",
    "when @xmath202 it is likely that the clt still holds for an appropriate choice of the sequence  @xmath115 , and with another additional bias ; see , for example ,  @xcite for a slightly different context . here",
    "we let this borderline case aside .",
    "[ rr-03 ] the limiting process @xmath155 is the same in both theorems  [ tr-01 ] and  [ tr-1 ] , but in the latter case the functional convergence always holds .",
    "it is also the same for ( the normalized versions of ) the processes @xmath203 and @xmath204 , which is somewhat a surprise since in many instances using overlapping intervals instead of nonoverlapping intervals results in a strictly smaller asymptotic variance ; this is for example the case for multipower variations , see theorem 11.2.1 in @xcite . however , in practice , it is probably advisable to use @xmath203 rather than @xmath204 , because the former estimator is likely to be less sensitive to way - off values of the spot estimators @xmath205 than the latter one , due to the `` smoothing '' embedded in its definition .",
    "[ rr-06 ] the @xmath147 property of @xmath17 is somewhat restrictive , as , for example , in the one - dimensional case it rules out the powers @xmath206 with @xmath207 .",
    "it could be proved that , in the one - dimensional case again , and if the processes @xmath18 and @xmath208 do not vanish ( equivalently , the process @xmath209 is locally bounded ) , the result still holds when @xmath17 is @xmath147 on @xmath43 and satisfies ( [ r-8 ] ) with an arbitrary @xmath46 : here again , the fact that @xmath209 is locally bounded is also necessary for having a clt for the functionals of ( [ i-1 ] ) ( say , with @xmath210 ) when the test function @xmath26 is @xmath30 outside @xmath10 only .    [ rr-07 ]",
    "one should compare this result with those of mykland and zhang in  @xcite : in that paper [ in which only the continuous one - dimensional case and the test functions @xmath206 are considered ] the authors propose to take @xmath39 in ( [ r-3 ] ) .",
    "of course ( [ r-1 ] ) fails , but @xmath140 in this case is actually of the form ( [ i-1 ] ) and a clt holds for @xmath211 [ without de - biasing term , but with an appropriate multiplicative factor @xmath212 , which is explicitly known ] : the asymptotic variance is bigger than in ( [ r-11 ] ) , but approaches this value when @xmath213 is large .    an advantage of mykland ",
    "zhang s approach is that when @xmath17 is positive , hence @xmath29 as well , the estimators are also positive . in contrast , @xmath214 in ( [ r0 - 1 ] ) may be negative even when @xmath215 everywhere .",
    "thus if this positivity issue is important for a specific application , taking @xmath39 `` large '' and the estimator @xmath216 might be advisable , although it seems to work only when @xmath17 is a power function .",
    "moreover , if @xmath214 is negative , it probably means that there is not enough data in order to obtain a relevant estimation .",
    "it is simple to make this clt `` feasible , '' that is , usable in practice for determining a confidence interval for @xmath29 at any time @xmath217 .",
    "indeed , we can define the following function on @xmath54 : @xmath218 which is continuous with @xmath219 , and nonnegative ( and positive at each @xmath220 such that @xmath221 ) .",
    "( [ r-9 ] ) implies the last condition in ( [ r-5 ] ) , and we have @xmath222 , with @xmath223 being the right - hand side of ( [ r-11 ] ) .",
    "then we readily deduce :    [ cr-1 ] under the assumptions of the previous theorem , for any @xmath217 we have the following stable convergence in law , where @xmath83 is an @xmath50 variable : @xmath224 and the same holds with @xmath225 instead of @xmath214 .",
    "we address now the question of the optimality of our procedures .    for simplicity ,",
    "we restrict our attention to the one - dimensional case @xmath226 .",
    "we denote by @xmath227 the class of all one - dimensional continuous semimartingales @xmath5 of the form ( [ i-3 ] ) , with @xmath79 being @xmath147 functions with bounded derivatives with further @xmath26 bounded away from @xmath10 , and @xmath81 being two independent brownian motions , and @xmath228 being lebesgue square - integrable processes , optional with respect to the filtration generated by @xmath229 , and with @xmath230 bounded away from @xmath10 .",
    "such an @xmath5 satisfies ( a-@xmath10 ) , with @xmath231 .",
    "let @xmath217 . in the following ,",
    "we say that a sequence of estimators @xmath232 of @xmath29 satisfy property @xmath233 over @xmath227 if :    the estimator @xmath234 is a function of @xmath235)$ ] ;    for any @xmath236 , the variables @xmath237 converge stably in law to a limit @xmath238 ( depending of @xmath17 of course ) , defined on an extension of the space .",
    "the following theorem gives three small steps toward optimality .",
    "[ tr-5 ] let @xmath226 and @xmath17 be a @xmath147 function on @xmath239 satisfying ( [ r-8 ] ) and which is strictly increasing , or strictly decreasing .    for the parametric model @xmath59 , where @xmath240 is a constant ( the toy example of the ) , for any @xmath217 , the estimators @xmath214 and @xmath225 are asymptotically efficient ( in le cam s sense ) for estimating the number @xmath241 .",
    "let @xmath232 be a sequence of estimators satisfying @xmath233 over the class of continuous processes @xmath5 for which holds .",
    "assume @xmath238 has a conditional variance of the form @xmath242 for some nonnegative borel function @xmath243 .",
    "then necessarily @xmath244 , as given by ( [ r-102 ] ) , and in particular , @xmath245    the estimators @xmath214 and @xmath225 are optimal over @xmath227 in the following sense : for any sequence @xmath246 of estimators satisfying @xmath233 over @xmath227 , the limiting variable @xmath238 can be realized as @xmath247 , where @xmath248 is the limiting process in ( [ r-10 ] ) , and the variable @xmath249 is independent of @xmath248 conditionally on @xmath158 .",
    "part ( b ) of theorem  [ tr-5 ] shows in particular that the estimators @xmath250 given in ( [ i-1 ] ) for estimating @xmath251 have always an asymptotic variance bigger than or equal to the variance ( [ r-11 ] ) .",
    "part ( c ) states that our estimators achieve the lower bounds of hajek _ convolution theorem _ over the class @xmath252 .",
    "this convolution theorem for the subclass @xmath252 is due to clment , delattre and gloter ; see  @xcite .",
    "it in particular implies that for given @xmath124 , any rate optimal estimator over @xmath227 has a limiting variance which is larger than those of @xmath248 the limiting process in ( [ r-10 ] ) .",
    "so far , however , a `` general '' theory of optimality in our nonparametric context seems still out of reach .",
    "suppose @xmath226 , and take @xmath22 , so we want to estimate the quarticity @xmath253 . in this case",
    "an `` optimal '' estimator for the quarticity is @xmath254-k_n+1}\\bigl(\\widehat{c}{}^n_i \\bigr)^2.\\ ] ] the asymptotic variance is @xmath255 , to be compared with the asymptotic variance of the more usual estimators @xmath256}({\\delta}^n_ix)^4 $ ] , which is @xmath257 .",
    "[ rr-15 ] although taking ( [ r-1 ] ) eliminates the bias terms @xmath258 , @xmath187 and @xmath188 showing in theorem  [ tr-01 ] , it might be judicious to still eliminate the ( asymptotically negligible ) bias @xmath258 by adding to @xmath214 the same correction term @xmath259-k_n+1 } ) $ ] as in ( [ r-6 ] ) .",
    "due to their probable instability , it does not seem advisable , though , to eliminate the biases @xmath187 and @xmath188 by using ( with the proper normalization ) the method of  @xcite .",
    "under assumption  [ assumar ] , not only do we have ( [ s-1 ] ) , but we can write @xmath18 in a similar fashion : @xmath260 ( here , @xmath261 is a @xmath143-dimensional brownian motion , possibly correlated with @xmath87 ) .",
    "then , according to the localization lemma 4.4.9 of  @xcite [ for the assumption  ( k ) in that lemma ] , it is enough to show theorem  [ tr-1 ] under the following stronger assumption :    [ assumsar ] we have assumption  [ assumar ] .",
    "moreover we have , for a @xmath92-integrable function @xmath262 on @xmath90 and a constant @xmath263 , @xmath264\\\\[-8pt ] & \\bigl\\|{\\widetilde{\\delta}}(\\omega , t , z)\\bigr\\|^2\\leq j(z).&\\nonumber\\end{aligned}\\ ] ]    in the sequel we suppose that @xmath5 satisfies assumption  [ assumsar ] , and also that ( [ r-1 ] ) holds : these assumptions are typically not recalled .",
    "below , all constants are denoted by @xmath265 , and they vary from line to line .",
    "they may implicitly depend on the process @xmath5 [ usually through @xmath263 in ( [ p-0 ] ) ] .",
    "when they depend on an additional parameter @xmath266 , we write @xmath267 .",
    "recall the notation @xmath112 in assumption  [ assumar ] .",
    "we will usually replace the discontinuous process @xmath5 by the continuous process @xmath268 connected with @xmath5 by @xmath269 . note that @xmath270 is bounded , and without loss of generality we will use below its cdlg version .",
    "\\(1 ) first , we recall well - known estimates for @xmath271 and @xmath68 . under ( [ p-0 ] ) and for @xmath272 and @xmath273 , we have @xmath274 } \\bigl\\|x'_{t+w}-x'_t \\bigr\\|^q\\bigm|{\\mathcal{f}}_t \\bigr)&\\leq & k_q s^{q/2},\\nonumber\\\\ \\bigl\\| { \\mathbb{e}}\\bigl(x'_{t+s}-x'_t \\mid{\\mathcal{f}}_s\\bigr)\\bigr\\|&\\leq & ks , \\nonumber\\\\[-8pt]\\\\[-8pt ] { \\mathbb{e}}\\bigl(\\sup_{w\\in[0,s ] } \\|c_{t+w}-c_t \\|^q\\bigm|{\\mathcal{f}}_t \\bigr ) & \\leq & k_q s^{1\\wedge(q/2)},\\nonumber\\\\ \\bigl\\| { \\mathbb{e}}(c_{t+s}-c_t\\mid{\\mathcal{f}}_s ) \\bigr\\|&\\leq & ks . \\nonumber\\end{aligned}\\ ] ]    we need slightly more refined estimates for @xmath271 , and before giving them we introduce some simplifying notation , @xmath275 \\bigr),\\\\ \\eta^n_{i , j}&=&\\sqrt { { \\mathbb{e}}\\bigl(\\eta_{(i-1){\\delta}_n , j{\\delta}_n}\\mid{\\mathcal{f}}^{n}_i\\bigr)},\\qquad \\eta^n_i=\\eta^n_{i , k_n}. \\nonumber\\end{aligned}\\ ] ]    [ lp-6 ] we have @xmath276    for simplicity we prove the result when @xmath277 , so @xmath278 , but upon shifting time the proof for @xmath279 is the same .",
    "first we have @xmath280 , where @xmath281 is a martingale with @xmath282 . taking the @xmath283-conditional expectation",
    "thus yields @xmath284 next , it s formula yields that @xmath285 is the sum of a martingale vanishing at @xmath10 , plus @xmath286 upon taking the conditional expectation , and using the cauchy ",
    "schwarz inequality and the first and the last parts of ( [ p-251 ] ) , plus ( [ p-503 ] ) , we readily deduce @xmath287    with @xmath288 , this gives the first claim .",
    "finally , for any indices @xmath289 it s formula yields a martingale @xmath281 vanishing at @xmath10 such that @xmath290 again , we take the @xmath283-conditional expectation and we deal with the second , the third and the last term in the right - hand side above by fubini s theorem and the cauchy  schwarz inequality . for the fourth term we use ( [ p-5030 ] ) , and",
    "a simple calculation yields the second claim .",
    "[ lp-1 ] for all @xmath217 we have @xmath291}\\eta^n_i ) \\to0 $ ] , and for all @xmath292 such that @xmath293 we have @xmath294 .",
    "the second claim follows from the definitions of @xmath295 and @xmath296 and the cauchy  schwarz inequality . for the first claim ,",
    "we observe that @xmath297 is smaller than a constant always , and than @xmath298 when @xmath299 . hence by the cauchy ",
    "schwarz inequality , @xmath300 } \\eta^n_{i } \\biggr ) & \\leq&\\biggl(t\\mathbb{e } \\biggl ( \\delta_{n}\\sum_{i=1}^{[t/\\delta_{n}]}\\bigl ( \\eta^n_{i}\\bigr)^{2 } \\biggr ) \\biggr)^{1/2 } \\\\ & \\leq&\\biggl(kt{\\delta}_n+\\mathbb{e } \\biggl(t\\int _ { 0}^{t}(\\eta_{s,2k_n+1 } ) ^2\\,ds \\biggr ) \\biggr)^{1/2}.\\end{aligned}\\ ] ] we have @xmath301 , and the cdlg property of @xmath270 yields that @xmath302 for all @xmath303 , and all @xmath304 except for countably many strictly positive values ( depending on @xmath303 ) .",
    "then , the first claim follows by the dominated convergence theorem .",
    "\\(2 ) it is much easier ( although unfeasible in practice ) to replace @xmath176 in ( [ r-4 ] ) by the estimators based on the process @xmath271 , as given by ( [ p-3 ] ) .",
    "namely , we will replace @xmath176 by the following : @xmath305    the comparison between @xmath176 and @xmath306 is based on the following consequence of lemma 13.2.6 of  @xcite , applied with @xmath307 , so @xmath210 and @xmath308 and @xmath309 and @xmath310 ( because @xmath111 ) with the notation of that lemma .",
    "namely , we have for all @xmath311 and for some sequence @xmath312 going to @xmath10 , @xmath313 since @xmath314 for any @xmath315 by classical estimates , implying by markov s inequality that @xmath316 for any @xmath317 , by taking @xmath318 , we then easily deduce @xmath319    \\(3 ) let us introduce the following @xmath96-valued variables : @xmath320\\\\[-8pt ] { \\beta}^n_i&=&\\widehat{c}'^n_i - c^n_i= \\frac1{k_n{\\delta}_n } \\sum_{j=0}^{k_n-1 } \\bigl({\\alpha}^n_{i+j}+\\bigl(c^n_{i+j}-c^n_i \\bigr){\\delta}_n \\bigr).\\nonumber\\end{aligned}\\ ] ]    from ( [ p-251 ] ) we get that for all @xmath273 , @xmath321 this and the burkholder  gundy and hlder inequalities give us , for @xmath322 , that @xmath323 .",
    "this and ( [ p-251 ] ) and again hlder s inequality yield @xmath324 lemma  [ lp-6 ] allows us for better estimates for @xmath325 , namely @xmath326\\\\[-8pt ] & \\bigl|{\\mathbb{e}}\\bigl({\\alpha}^{n , jk}_i{\\alpha}_i^{n , lm}\\mid { \\mathcal{f}}^{n}_i\\bigr ) -\\bigl(c^{n , jl}_ic^{n , km}_i+c^{n , jm}_ic^{n , kl}_i \\bigr){\\delta}_n^2 \\bigr|\\leq k{\\delta}_n^{5/2}. & \\nonumber\\end{aligned}\\ ] ]    [ lp-7 ] we have @xmath327    the first claim follows from ( [ p-251 ] ) , ( [ p-22 ] ) and the last part of lemma  [ lp-1 ] . for the second one , we set @xmath328 and @xmath329 and write @xmath330 as @xmath331\\\\[-8pt ] & & \\qquad{}+\\frac1{k_n^2 { \\delta}_n^2}\\sum_{u=0}^{k_n-2 } \\sum_{v = u+1}^{k_n-1 } { \\zeta}^{n , lm}_{i , u } { \\zeta}^{n , jk}_{i , v}.\\nonumber\\end{aligned}\\ ] ]    first , we have @xmath332 whose @xmath333-conditional expectation is less than @xmath334 by ( [ p-251 ] ) and ( [ p-21 ] ) .",
    "the boundedness of @xmath18 and ( [ p-251 ] ) yield @xmath335 .",
    "then ( [ p-22 ] ) gives us that the @xmath333-conditional expectation of the first term in ( [ p-2201 ] ) , minus @xmath336 , is less than @xmath337 .",
    "second , ( [ p-251 ] ) and ( [ p-22 ] ) , plus the first claim of lemma  [ lp-6 ] , yield , when @xmath338 , @xmath339 since @xmath340 is @xmath341-measurable , and using ( [ p-21 ] ) and the second part of lemma  [ lp-1 ] , the @xmath342-conditional expectation of the last term of ( [ p-2201 ] ) is smaller than @xmath343 .",
    "the same is obviously true for the second term , and we readily deduce the second claim of the lemma .      using the key property @xmath344 and the definition ( [ p-20 ] ) of @xmath345 , a simple calculation shows the decomposition @xmath346 , as soon as @xmath347 , and where @xmath348-k_n+1 } \\biggl(g\\bigl(\\widehat{c}{}^n_i\\bigr)-g\\bigl ( \\widehat{c}'^n_i\\bigr ) \\\\ & & \\hspace*{77.5pt}{}-\\frac1{2k_n}\\sum_{j , k , l , m=1}^d \\bigl(\\partial^2_{jk , lm } g\\bigl(\\widehat{c}{}^n_i \\bigr ) \\bigl(\\widehat{c}{}^{n , jl}_i \\widehat{c}{}^{n , km}_i + \\widehat{c}_i^{n , jm } \\widehat{c}_i^{n , kl } \\bigr ) \\\\ & & \\hspace*{128.5pt}{}-\\partial^2_{jk , lm } g\\bigl(\\widehat{c}'^n_i \\bigr ) \\bigl(\\widehat{c}'^{n , jl}_i \\widehat{c}'^{n , km}_i+\\widehat{c}_i'^{n , jm } \\widehat{c}_i'^{n , kl}\\bigr ) \\bigr ) \\biggr ) , \\\\",
    "v^{n,2}_t&=&\\frac{1}{{\\sqrt{{\\delta}_n } } } \\sum _ { i=1}^{[t/{\\delta}_n]-k_n+1 } \\int_{(i-1){\\delta}_n}^{i{\\delta}_n } \\bigl(g\\bigl(c_i^n\\bigr)-g(c_s)\\bigr)\\,ds\\\\ & & { } - \\frac{1}{{\\sqrt{{\\delta}_n } } } \\int_{{\\delta}_n([t/{\\delta}_n]-k_n+1)}^tg(c_s ) \\,ds , \\\\",
    "v^{n,3}_t&=&{\\sqrt{{\\delta}_n}}\\sum_{i=1}^{[t/{\\delta}_n]-k_n+1 } \\sum_{l , m=1}^d \\partial_{lm}g \\bigl(c^n_i\\bigr ) \\frac{1}{k_n } \\sum _ { u=0}^{k_n-1}\\bigl(c_{i+u}^{n , lm}-c_i^{n , lm } \\bigr ) , \\\\ v^{n,4}_t&=&{\\sqrt{{\\delta}_n}}\\sum_{i=1}^{[t/{\\delta}_n]-k_n+1 } \\biggl(g\\bigl(c^n_i+{\\beta}^n_i \\bigr)-g\\bigl(c^n_i\\bigr ) -\\sum _ { l , m=1}^d \\partial_{lm}g \\bigl(c^n_i\\bigr ) { \\beta}^{n , lm}_i \\\\ & & \\hspace*{77.5pt}{}-\\frac1{2k_n}\\sum_{j , k , l , m=1}^d \\partial^2_{jk , lm } g\\bigl(c^n_i+ { \\beta}^n_i\\bigr)\\\\ & & \\hspace*{145.6pt}{}\\times \\bigl(\\bigl(c^{n , jl}_i+ { \\beta}^{n , jl}_i\\bigr ) \\bigl(c^{n , km}_i+ { \\beta}^{n , km}_i\\bigr ) \\\\ & & \\hspace*{162.3pt}{}+\\bigl(c^{n , jm}_i+{\\beta}^{n , jm}_i \\bigr ) \\bigl(c^{n , kl}_i+{\\beta}^{n , kl}_i \\bigr ) \\bigr ) \\biggr ) , \\\\ v^{n,5}_t&=&\\frac1{k_n{\\sqrt{{\\delta}_n } } } \\sum _ { i=1}^{[t/{\\delta}_n]-k_n+1 } \\sum_{l , m=1}^d \\partial_{lm}g\\bigl(c^n_i\\bigr ) \\sum _ { u=0}^{k_n-1}{\\alpha}^{n , lm}_{i+u}.\\end{aligned}\\ ] ]    the leading term is @xmath349 , and the first claim in ( [ r-10 ] ) , about @xmath203 , is a consequence of the following two lemmas :    [ lp-10 ] for @xmath350 we have @xmath351 .",
    "[ lp-11 ] with @xmath155 as in theorem  [ tr-1 ] , we have the functional stable convergence in law @xmath352    proof of lemma  [ lp-10 ] _ the case _ @xmath353 : we define functions @xmath354 on @xmath54 by @xmath355 from ( [ r-8 ] ) we obtain @xmath356 ( uniformly in @xmath40 ) .",
    "so if @xmath295 is the @xmath85th summand in the definition of @xmath357 , we get @xmath358 recalling the last part of ( [ p-21 ] ) , and by ( [ p-6 ] ) , hlder s inequality and the fact that @xmath359 when @xmath360 is small enough , because @xmath361 , we deduce @xmath362 and thus @xmath363 in view of ( [ r-9 ] ) , we deduce the result for @xmath353 .    _",
    "@xmath364 : since @xmath365 is bounded , it is obvious that the absolute value of the last term in @xmath366 is smaller than @xmath367 , which goes to @xmath10 by ( [ r-1 ] ) . since @xmath17 is  @xmath368 , the convergence of the first term in @xmath366 to @xmath10 in probability , locally uniformly in @xmath124 , is well known ; see , for example , the proof of ( 5.3.24 ) in  @xcite , in which one replaces @xmath369 by @xmath365 . thus the result holds for @xmath364 .    _ the case _",
    "@xmath370 : letting @xmath371 be the @xmath85th summand in the definition of @xmath372 , and @xmath373 be the integer part of @xmath374-k_n - j+1)/k_n$ ] , we have @xmath375 from ( [ p-251 ] ) and the cauchy ",
    "schwarz inequality , we get @xmath376 then doob s inequality , the @xmath377-measurability of @xmath378 , and @xmath379 imply @xmath380 since @xmath381 and @xmath382 , we deduce the result for @xmath370 .    _ the case _",
    "@xmath383 : the @xmath85th summand in the definition of @xmath384 is @xmath385 , where @xmath386 [ use ( [ r-8 ] ) and @xmath387 repeatedly ] , and we thus have @xmath388 , with @xmath373 as in the previous step and @xmath389-k_n+1 } \\bigl(w^n_i+{\\mathbb{e}}\\bigl(v^n_i\\mid{\\mathcal{f}}^n_i\\bigr)\\bigr ) , \\\\ h(j)^n_t&=&\\sum_{i=0}^{n(n , j , t ) } { \\zeta}(j)^n_i,\\qquad { \\zeta}(j)^n_i={\\sqrt{{\\delta}_n}}\\bigl(v^n_{j+k_ni}-{\\mathbb{e}}\\bigl(v^n_{j+k_ni}\\mid { \\mathcal{f}}^n_{j+k_ni}\\bigr ) \\bigr).\\end{aligned}\\ ] ]    in view of lemma  [ lp-7 ] and ( [ p-23 ] ) , plus hlder s inequality , we have @xmath390 and thus ( [ r-1 ] ) and lemma  [ lp-1 ] yield @xmath391}{\\sqrt{{\\delta}_n}}\\bigl(\\bigl|w^n_i\\bigr|+\\bigl|{\\mathbb{e}}\\bigl(v^n_i\\mid { \\mathcal{f}}^n_i\\bigr)\\bigr| \\bigr ) \\biggr)\\to0.\\ ] ] moreover ( [ p-23 ] ) and @xmath392 yield @xmath393 , whereas @xmath394 is a martingale increment for the filtration @xmath395 , hence doob s inequality and @xmath379 imply @xmath396 since @xmath397 , we deduce the result for @xmath383 .",
    "proof of lemma  [ lp-11 ] we can rewrite @xmath349 as @xmath398}\\sum_{l , m=1}^d w^{n , lm}_i { \\alpha}^{n , lm}_i,\\ ] ] where @xmath399+k_n-1)^+}^{(i-1)\\wedge(k_n-1 ) } \\partial_{lm } g \\bigl(c_{i - j}^n\\bigr).\\ ] ] observe that @xmath400 and @xmath325 are measurable with respect to @xmath333 and @xmath401 , respectively , so by theorem ix.7.28 of  @xcite ( with @xmath402 and @xmath403 in the notation of that theorem ) it suffices to prove the following four convergences in probability , for all @xmath217 and all component indices : @xmath404-k_n+1}w^{n , lm}_i { \\mathbb{e}}\\bigl({\\alpha}^{n , lm}_i\\mid{\\mathcal{f}}^n_i \\bigr){\\stackrel{{\\mathbb{p}}}{\\longrightarrow}}0 , \\\\ \\label{p-52 } & & \\frac1{{\\delta}_n } \\sum_{i=1}^{[t/{\\delta}_n]-k_n+1 } w^{n , jk}_i w^{n , lm}_i { \\mathbb{e}}\\bigl ( { \\alpha}^{n , jk}_i { \\alpha}^{n , lm}_i\\mid { \\mathcal{f}}^n_i\\bigr ) \\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\qquad{\\stackrel{{\\mathbb{p}}}{\\longrightarrow}}\\int_0^t \\partial_{jk } g(c_s ) \\,\\partial_{lm } g(c_s ) \\bigl(c_s^{jl}c_s^{km}+c_s^{jm}c_s^{kl } \\bigr)\\,ds,\\nonumber \\\\ \\label{p-53 } & & \\hspace*{28.1pt}\\frac1{{\\delta}_n^2}\\sum _ { i=1}^{[t/{\\delta}_n]-k_n+1}\\bigl\\|w^n_i \\bigr\\|^4 { \\mathbb{e}}\\bigl(\\bigl\\|{\\alpha}^n_i\\bigr\\|^4\\mid { \\mathcal{f}}^n_i\\bigr ) { \\stackrel{{\\mathbb{p}}}{\\longrightarrow}}0 , \\\\ \\label{p-54 } & & \\frac{1}{{\\sqrt{{\\delta}_n}}}\\sum_{i=1}^{[t/{\\delta}_n]-k_n+1}w^{n , lm}_i { \\mathbb{e}}\\bigl({\\alpha}^{n , lm}_i { \\delta}^n_in\\mid { \\mathcal{f}}^n_i\\bigr ) { \\stackrel{{\\mathbb{p}}}{\\longrightarrow}}0,\\end{aligned}\\ ] ] where @xmath405 for some @xmath213 , or is an arbitrary bounded martingale , orthogonal to  @xmath87 .    lemma  [ lp-1 ] , ( [ p-21 ] ) , ( [ p-22 ] ) and the property @xmath406 readily imply ( [ p-51 ] ) and ( [ p-53 ] ) . in view of the form of @xmath325 , a usual argument ( see , e.g. ,",
    "@xcite ) shows that in fact @xmath407 for all @xmath408 as above , and hence ( [ p-54 ] ) holds.=-1    for ( [ p-52 ] ) , by ( [ p-22 ] ) it suffices to prove that @xmath409-k_n+1 } w^{n , jk}_i w^{n , lm}_i \\bigl(c^{n , jl}_ic^{n , km}_i+c^{n , jm}_ic^{n , kl}_i \\bigr ) \\\\ & & \\qquad{\\stackrel{{\\mathbb{p}}}{\\longrightarrow}}\\int_0^t \\partial_{jk } g(c_s ) \\,\\partial_{lm } g(c_s ) \\bigl(c_s^{jl}c_s^{km}+c_s^{jm}c_s^{kl } \\bigr)\\,ds.\\end{aligned}\\ ] ] in view of the definition of @xmath400 , for each @xmath124 we have @xmath410 and @xmath411 almost surely if @xmath412 , and the above convergence follows by the dominated convergence theorem , thus ending the proof of ( [ p-50 ] ) .",
    "proof of the second claim in ( [ r-10 ] ) the proof is basically the same as for the first claim .",
    "we have the decomposition @xmath413 , where @xmath414 - 1 } \\bigl(g\\bigl(\\widehat{c}{}^n_{k_ni+1 } \\bigr)-g\\bigl(\\widehat{c}'^n_{k_ni+1}\\bigr ) \\bigr ) , \\\\ { \\overline{v}{}}^{n,2}_t&=&\\frac{1}{{\\sqrt{{\\delta}_n } } } \\sum _ { i=0}^{[t / k_n{\\delta}_n]-1 } \\int_{k_ni{\\delta}_n}^{k_n(i+1){\\delta}_n } \\bigl(g(c_{k_ni{\\delta}_n})-g(c_s)\\bigr)\\,ds \\\\ & & { } -\\frac{1}{{\\sqrt{{\\delta}_n } } } \\int _ { k_n{\\delta}_n([t / k_n{\\delta}_n])}^tg(c_s)\\,ds , \\\\ { \\overline{v}{}}^{n,3}_t&=&k_n{\\sqrt{{\\delta}_n}}\\sum _ { i=0}^{[t / k_n{\\delta}_n]-1 } \\sum_{l , m=1}^d \\partial_{lm}g\\bigl(c^n_{k_ni+1}\\bigr ) \\frac{1}{k_n } \\sum_{u=0}^{k_n-1 } \\bigl(c_{k_ni+1+u}^{n , lm}-c_{k_ni+1}^{n , lm}\\bigr ) , \\\\ { \\overline{v}{}}^{n,4}_t&=&k_n{\\sqrt{{\\delta}_n}}\\\\ & & { } \\times\\sum _ { i=0}^{[t / k_n{\\delta}_n]-1 } \\biggl(g\\bigl(c^n_{k_ni+1}+ { \\beta}^n_{k_ni+1}\\bigr)-g\\bigl(c^n_{k_ni+1 } \\bigr ) \\\\ & & \\hspace*{82.2pt}\\hspace*{-22.5pt}{}-\\sum_{l , m=1}^d \\partial_{lm}g \\bigl(c^n_{k_ni+1}\\bigr ) { \\beta}^{n , lm}_{k_ni+1 } \\\\ & & \\hspace*{82.2pt}\\hspace*{-22.5pt}{}-\\frac1{2k_n}\\sum_{j , k , l , m=1}^d \\partial^2_{jk , lm } g\\bigl(c^n_{k_ni+1}+ { \\beta}^n_{k_ni+1}\\bigr ) \\\\ & & \\hspace*{149.8pt}\\hspace*{-22.5pt}{}\\times\\bigl(\\bigl(c^{n , jl}_{k_ni+1}+ { \\beta}^{n , jl}_{k_ni+1}\\bigr ) \\bigl(c^{n , km}_{k_ni+1}+ { \\beta}^{n , km}_{k_ni+1}\\bigr ) \\\\ & & \\hspace*{143.1pt } { } + \\bigl(c^{n , jm}_{k_ni+1}+{\\beta}^{n , jm}_{k_ni+1 } \\bigr ) \\bigl(c^{n , kl}_{k_ni+1}+{\\beta}^{n , kl}_{k_ni+1 }",
    "\\bigr ) \\bigr ) \\biggr ) , \\\\ { \\overline{v}{}}^{n,5}_t&=&\\frac1{{\\sqrt{{\\delta}_n } } }",
    "\\sum_{i=0}^{[t / k_n{\\delta}_n]-1 } \\sum_{l , m=1}^d \\partial_{lm}g \\bigl(c^n_{k_ni+1}\\bigr ) \\sum_{u=0}^{k_n-1 } { \\alpha}^{n , lm}_{k_ni+u+1}.\\end{aligned}\\ ] ] the proofs of lemmas  [ lp-10 ] and  [ lp-11 ] carry over to @xmath415 instead of @xmath416 , for @xmath417 , almost word for word , except for the following points :    \\(1 ) for lemma  [ lp-10 ] , cases @xmath418 , there is no need to consider the @xmath38 processes @xmath419 ; a single process @xmath420 is enough , and the proof is simpler .",
    "\\(2 ) for lemma  [ lp-10 ] , case @xmath364 , the proof of the u.c.p . convergence to @xmath10 of the first term in the definition of @xmath421 should be reworked as follows : the @xmath85th summand @xmath422 in this term is @xmath423-measurable , and by ( [ r-8 ] ) and ( [ p-251 ] ) it satisfies @xmath424 then the claim follows from the usual martingale argument and @xmath382 .",
    "\\(3 ) for lemma  [ lp-11 ] , we have @xmath425 } \\sum_{l , m=1}^d \\partial_{lm } g\\bigl(c_{1+k_n[(j-1)/k_n]}^n\\bigr ) { \\alpha}^{n , lm}_i,\\ ] ] and the rest of",
    "the proof is similar .",
    "\\(a ) is almost obvious : indeed , @xmath426 converges with the rate @xmath14 and is asymptotically normal with asymptotic variance @xmath427 ( @xmath428 is the derivative of @xmath17 ) .",
    "however , since @xmath17 is one - to - one , the model index by the new parameter @xmath241 is regular , and the mle is @xmath429 , where @xmath430}({\\delta}^n_i x)^2 $ ] , and clearly @xmath429 has the same asymptotic properties as @xmath426 : this proves the result .",
    "\\(b ) is also obvious : the properties of @xmath431 hold for all continuous processes @xmath5 satisfying ( a-@xmath10 ) .",
    "then , using the toy model of ( a ) , the optimality proved above implies that @xmath432 for any constant @xmath433 , that is , @xmath244 .",
    "finally , ( c ) is exactly theorem 3 of  @xcite applied to the present setting .",
    "we thank two anonymous referees for very constructive comments , which led to substantial improvements .",
    "we also thank jia li for pointing out some problems in an earlier draft ."
  ],
  "abstract_text": [
    "<S> we consider a multidimensional it semimartingale regularly sampled on @xmath0 $ ] at high frequency @xmath1 , with @xmath2 going to zero . </S>",
    "<S> the goal of this paper is to provide an estimator for the integral over @xmath0 $ ] of a given function of the volatility matrix . to approximate the integral </S>",
    "<S> , we simply use a riemann sum based on local estimators of the pointwise volatility . </S>",
    "<S> we show that although the accuracy of the pointwise estimation is at most @xmath3 , this procedure reaches the parametric rate @xmath4 , as it is usually the case in integrated functionals estimation . after a suitable bias correction </S>",
    "<S> , we obtain an unbiased central limit theorem for our estimator and show that it is asymptotically efficient within some classes of sub models . </S>"
  ]
}