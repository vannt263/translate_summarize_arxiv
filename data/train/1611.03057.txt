{
  "article_text": [
    "the emergence and abundance of social media texts has prompted the urge to develop tools that are able to process language which is often non - conventional , both in terms of lexicon as well as grammar .",
    "indeed , models trained on standard newswire data heavily suffer when used on data from a different language variety , especially twitter  @xcite .    as a way to equip microblog processing with efficient tools , two ways of developing twitter - compliant models",
    "have been explored .",
    "one option is to transform twitter language back to what pre - trained models already know via normalisation operations , so that existing tools are more successful on such different data .",
    "the other option is to create _ native _ models by training them on labelled twitter data .",
    "the drawback of the first option is that it s not clear what norm to target :  what is standard language ? \"",
    "@xcite , and implementing normalisation procedures requires quite a lot of manual intervention and subjective decisions .",
    "the drawback of the second option is that manually annotated twitter data is nt readily available , and it is costly to produce .    in this paper , we report on our participation in postwita , the evalita  2016 shared task on italian part - of - speech ( pos ) tagging for twitter  @xcite .",
    "we emphasise an approach geared to building a _ single model _ ( rather than an ensemble ) based on weakly supervised learning , thus favouring ( over normalisation ) the aforementioned second option of learning _ invariant representations _ , also for theoretical reasons .",
    "we address the bottleneck of acquiring manually annotated data by suggesting and showing that a semi - supervised approach that mainly focuses on tweaking data selection within a bootstrapping setting can be successfully pursued for this task .",
    "contextually , we show that large amounts of manually annotated data might not be helpful if data is nt `` of the right kind '' .",
    "in adapting a pos tagger to twitter , we mainly focus on ways of selectively enriching the training set with additional data . rather than simply adding large amounts of existing annotated data , we investigate ways of selecting smaller amounts of more appropriate training instances , possibly even tagged with silver rather than gold labels . as for the model itself , we simply take an off - the - shelf tagger , namely a bi - directional long short - term memory ( bi - lstm ) model @xcite , which we use with default parameters ( see section  [ sec : bilstm ] ) apart from initializing it with twitter - trained embeddings ( section  [ sec : setup ] ) .",
    "our first model is trained on the postwita training set plus additional gold data selected according to two criteria ( see below : _ two shades of gold _ ) .",
    "this model is used to tag a collection of facebook posts in a bootstrapping setting with two cycles ( see below : _ bootstrapping via facebook _ ) .",
    "the rationale behind using facebook as _ not - so - distant _ source when targeting twitter is the following : many facebook posts of public , non - personal pages resemble tweets in style , because of brevity and the use of hashtags .",
    "however , differently from random tweets , they are usually correctly formed grammatically and spelling - wise , and often provide more context , which allows for more accurate tagging .",
    "[ [ two - shades - of - gold ] ] two shades of gold + + + + + + + + + + + + + + + + + +    we used the italian portion of the latest release ( v1.3 ) of the universal dependency ( ud ) dataset  @xcite , from which we extracted two subsets , according to two different criteria .",
    "first , we selected data on the basis of its _ origin _ , trying to match the twitter training data as close as possible .",
    "for this reason , we used the facebook subportion ( ` ud_fb ` ) .",
    "these are 45 sentences that presumably stem from the italian facebook help pages and contain questions and short answers .",
    "second , by looking at the confusion matrix of one of the initial models , we saw that the model s performance was especially poor for cliticised verbs and interjections , tags that are also infrequent in the training set ( table  [ tab : postwita ] ) .",
    "therefore , from the italian ud portion we selected _ any _ data ( in terms of origin / genre ) which contained the ` verb_clit ` or ` intj ` tag , with the aim to boost the identification of these categories .",
    "we refer to this set of 933 sentences as ` ud_verb_clit+intj ` .",
    "0.2 cm    [ [ bootstrapping - via - facebook ] ] bootstrapping via facebook + + + + + + + + + + + + + + + + + + + + + + + + + +    we augmented our training set with silver - labelled data . with our best model trained on the original task data plus ` ud_verb_clit+intj ` and ` ud_fb ` , we tagged a collection of facebook posts , added those to the training pool , and retrained our tagger .",
    "we used two iterations of indelible self - training  @xcite , i.e. , adding automatically tagged data where labels do not change once added . using the facebook api through the facebook - sdk python library , we scraped an average of 100 posts for each of the following pages , selected on the basis of our intuition and on reasonable site popularity :    * sport : ` corrieredellosport ` * news : ` ansa.it ` , ` ilsole24ore ` , ` lastampa.it ` * politics : ` matteorenziufficiale ` * entertainment : ` novella2000 ` , ` alfemminile ` * travel : ` viaggiart `    we included a second cycle of bootstrapping , scraping a few more facebook pages ( ` sologossip.it ` , ` paesionline ` , ` espressonline ` , ` lagazzettadellosport ` , again with an average of 100 posts each ) , and tagging the posts with the model that had been re - trained on the original training set plus the first round of facebook data with silver labels ( we refer to the whole of the automatically - labelled facebook data as ` fb_silver ` ) . `",
    "fb_silver ` was added to the training pool to train the final model .",
    "statistics on the obtained data are given in table  [ tbl : stats ] .",
    "in this section we describe how we developed the two models of the final submission , including all preprocessing decisions .",
    "we highlight the importance of choosing an adequate development set to identify promising directions .",
    "[ [ postwita - data ] ] postwita data + + + + + + + + + + + + +    in the context of postwita , training data was provided to all participants in the form of manually labelled tweets .",
    "the tags comply with the ud tagset , with a couple of modifications due to the specific genre ( emoticons are labelled with a dedicated tag , for example ) , and subjective choices in the treatment of some morphological traits typical of italian .",
    "specifically , clitics and articulated prepositions are treated as one single form ( see below : _ ud fused forms _ ) .",
    "the training set contains 6438 tweets , for a total of ca .",
    "115k tokens . the distribution of tags together with examples is given in table  [ tab : postwita ] .",
    "the test set comprises 301 tweets ( ca .",
    "4800 tokens ) .",
    ".[tab : postwita]tag distribution in the original trainset . [ cols=\"<,<,>,<\",options=\"header \" , ]     [ [ ud - fused - forms ] ] ud fused forms + + + + + + + + + + + + + +    in the ud scheme for italian , articulated prepositions ( ` adp_a ` ) and cliticised verbs ( ` verb_clit ` ) are annotated as separate word forms , while in postwita the original word form ( e.g. , ` alla ' or ` arricchirsi ' ) is annotated as a whole . in order to get the postwita ` adp_a ` and ` verb_clit ` tags for these fused word forms from ud , we adjust the ucph ` ud - conversion - tools `  @xcite that propagates head pos information up to the original form .    [ [ pre - processing - of - unlabelled - data ] ] pre - processing of unlabelled data + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for the facebook data , we use a simplistic off - the - shelf rule - based tokeniser that segments sentences by punctuation and tokens by whitespace .",
    "we normalise urls to a single token ( ` http://www.someurl.org ` ) and add a rule for smileys . finally , we remove sentences from the facebook data were more than 90% of the tokens are in all caps .",
    "unlabelled data used for embeddings is preprocessed only with normalisation of usernames and urls .",
    "[ [ word - embeddings ] ] word embeddings + + + + + + + + + + + + + + +    we induced word embeddings from 5 million italian tweets ( twita ) from twita  @xcite .",
    "vectors were created using ` word2vec ` @xcite with default parameters , except for the fact that we set the dimensions to 64 , to match the vector size of the multilingual ( poly ) embeddings @xcite used by .",
    "we dealt with unknown words by adding a `` unk '' token computing the mean vector of three infrequent words (  vip!\",cuora \" ,  white \" ) .",
    "[ [ creation - of - a - realistic - internal - development - set ] ] creation of a _ realistic _ internal development set + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the original task data is distributed as a single training file . in initial experiments we saw that performance varied considerably for different random subsets .",
    "this was due to a large bias towards tweets about ` monti ' and ` grillo ' , see figure  [ fig : wordle ] , but also because of duplicate tweets . we opted to create _ the most difficult _ development set possible .",
    "this development set was achieved by removing duplicates , and randomly selecting a subset of tweets that do not mention ` grillo ' or ` monti ' while maximizing out - of - vocabulary ( oov ) rate with respect to the training data . hence , our internal development set consisted of 700 tweets with an oov approaching 50% .",
    "this represents a more realistic testing scenario .",
    "indeed , the baseline ( the basic bi - lstm model ) , dropped from 94.37 to 92.41 computed on the earlier development set were we had randomly selected 1/5 of the data , with an oov of 45% ( see table  [ tbl : resultsxlime ] ) .      the bidirectional long short - term memory model ` bilty ` is illustrated in figure  [ fig : model ] .",
    "it is a context bi - lstm taking as input word embeddings @xmath0 .",
    "character embeddings @xmath1 are incorporated via a hierarchical bi - lstm using a sequence bi - lstm at the lower level  @xcite .",
    "the character representation is concatenated with the ( learned ) word embeddings @xmath0 to form the input to the context bi - lstm at the upper layers .",
    "we took default parameters , i.e. , character embeddings set to 100 , word embeddings set to 64 , 20 iterations of training using stochastic gradient descent , a single bi - lstm layer and regularization using gaussian noise with @xmath2 ( ` cdim 100 , trainer sgd , indim 64 , iters 20 , h_layer 1 , sigma 0.2 ` ) .",
    "the model has been shown to achieve state - of - the - art performance on a range of languages , where the incorporation of character information was particularly effective  @xcite . with these features and settings",
    "we train two models on different training sets .     and character @xmath1 representations . ]",
    "[ [ goldpick ] ] goldpick + + + + + + + +    ` bilty ` with pre - initialised twita embeddings , trained on the postwita training set plus selected gold data ( ` ud_fb ` + ` ud_verb_clit+intj ` ) .",
    "[ [ silverboot ] ] silverboot + + + + + + + + + +    a bootstrapped version of goldpick , where ` fb_silver ` ( see  section  [ sec : approach ] ) is also added to the training pool , which thus includes both gold and silver data .",
    "participants were allowed to submit one official , and one additional ( unofficial ) run . because on development data silverboot performed better than goldpick , we selected the former for our official submission and the latter for the unofficial one , making it thus also possible to assess the specific contribution of bootstrapping to performance .",
    "table  [ tbl : resultstest ] shows the results on the official test data for both our models and tnt  @xcite .",
    "the results show that adding bootstrapped silver data outperforms the model trained on gold data alone .",
    "the additional training data included in silverboot reduced the oov rate for the testset to 41.2% ( compared to 46.9% with respect to the original postwita training set ) . note that , on the original randomly selected development set the results were less indicative of the contribution of the silver data ( see table  [ tbl : resultsxlime ] ) , showing the importance of a carefully selected development set .",
    "in addition to what we found to boost the tagger s performance , we also observed what did nt yield any improvements , and in some case even lowered global accuracy .",
    "what we experimented with was triggered by intuition and previous work , as well as what we had already found to be successful , such as selecting additional data to make up for under - represented tags in the training set .",
    "however , everything we report in this section turned out to be either pointless or detrimental .",
    "[ [ more - data ] ] more data + + + + + + + + +    we added to the training data _ all _ ( train , development , and test ) sections from the italian part of ud1.3 . while training on selected gold data ( 978 sentences ) yielded 95.06% accuracy , adding",
    "all of the ud - data ( 12k sentences of newswire , legal and wiki texts ) yielded a disappointing 94.88% in initial experiments ( see table  [ tbl : resultsxlime ] ) , also considerably slowing down training .",
    "next , we tried to add more twitter data from xlime , a publicly available corpus with multiple layers of manually assigned labels , including pos tags , for a total of ca .",
    "8600 tweets and 160k tokens  @xcite .",
    "the data is nt provided as a single gold standard file but in the form of separate annotations produced by different judges , so that we used mace  @xcite to adjudicate divergences . additionally , the tagset is slightly different from the ud set , so that we had to implement a mapping .",
    "the results in table  [ tbl : resultsxlime ] show that adding all of the xlime data declines performance , despite careful preprocessing to map the tags and resolve annotation divergences .",
    "lr * system * & * accuracy * +   + baseline ( w / o emb ) & 94.37 + + poly emb & 94.15 + + twita emb & * 94.69 * +   + + morphit ! coarse mtl & 94.61 + + morphit ! fine mtl & 94.68 + + ud all & 94.88 + + gold - picked & 95.06 + + gold - picked+silver ( 1st round ) & * 95.08 * +   + baseline ( incl .",
    "twita emb ) & 92.41 + + gold ( goldpick ) & 93.19 + + gold+silver ( silverboot ) & * 93.42 * +   + + xlime adjudicated ( 48 ) & 92.58 + + xlime single annot . & 91.67 + + xlime all ( 8k ) & 92.04 +    [ [ more - tag - specific - data ] ] more tag - specific data + + + + + + + + + + + + + + + + + + + + + +    from the matrix computed on the dev set , it emerged that the most confused categories were ` noun ` and ` propn ` .",
    "following the same principle that led us to add ` ud_verb_clit+intj ` , we tried to reduce such confusion by providing additional training data containing proper nouns .",
    "this did not yield any improvements , neither in terms of global accuracy , nor in terms of precision and recall of the two tags .",
    "[ [ multi - task - learning ] ] multi - task learning + + + + + + + + + + + + + + + + + + +    multi - task learning ( mtl ) @xcite , namely a learning setting where more than one task is learnt at the same time , has been shown to improve performance for several nlp tasks @xcite .",
    "often , what is learnt is one main task and , additionally , a number of auxiliary tasks , where the latter should help the model converge better and overfit less on the former . in this context , the additional signal we use to support the learning of each token s pos tag is the token s degree of ambiguity . using the information stored in _ morph - it ! _ , a lexicon of italian inflected forms with their lemma and morphological features @xcite , we obtained the _ number _ of all different tags potentially associated to each token . because the _ morph - it ! _ labels are highly fine - grained we derived two different ambiguity scores , one on the original and one on coarser tags . in neither case",
    "the additional signal contributed to the tagger s performance , but we have not explored this direction fully and leave it for future investigations .",
    "the main conclusion we draw from the experiments in this paper is that _ data selection matters _ , not only for training but also while developing for taking informed decisions .",
    "indeed , only after creating a carefully designed internal development set we obtained stronger evidence of the contribution of silver data which is also reflected in the official results .",
    "we also observe that choosing less but more targeted data is more effective .",
    "for instance , twita embeddings contribute more than generic poly embeddings which were trained on substantially larger amounts of wikipedia data .",
    "also , just blindly adding training data does not help .",
    "we have seen that using the whole of the ud corpus is not beneficial to performance when compared to a small amount of selected gold data , both in terms of origin and labels covered . finally , and most importantly , we have found that adding little amounts of not - so - distant silver data obtained via bootstrapping resulted in our best model .",
    "we believe the low performance observed when adding xlime data is likely due to the non - correspondence of tags in the two datasets , which required a heuristic - based mapping . while this is only a speculation that requires further investigation , it seems to indicate that exploring semi - supervised strategies is preferrable to producing idiosyncratic or project - specific gold annotations .",
    "valerio basile and malvina nissim . 2013 .",
    "sentiment analysis on italian tweets . in _ proceedings of the 4th workshop on computational approaches to subjectivity ,",
    "sentiment and social media analysis _ , pages 100107 .",
    "jacob eisenstein .",
    "what to do about bad language on the internet . in _ proceedings of the annual conference of the north american chapter of the association for computational linguistics ( naacl ) _ ,",
    "pages 359369 , atlanta .",
    "xiaodong liu , jianfeng gao , xiaodong he , li  deng , kevin duh , and ye - yi wang .",
    "representation learning using multi - task deep neural networks for semantic classification and information retrieval . in _ proc .",
    "naacl_.                fabio tamburini , cristina bosco , alessandro mazzei , and andrea bolioli .",
    "2016 . . in pierpaolo basile ,",
    "franco cutugno , malvina nissim , viviana patti , and rachele sprugnoli , editors , _ proceedings of the 5th evaluation campaign of natural language processing and speech tools for italian ( evalita 2016)_. aacademia university press ."
  ],
  "abstract_text": [
    "<S> * english . * we bootstrap a state - of - the - art part - of - speech tagger to tag italian twitter data , in the context of the evalita  2016 postwita shared task . </S>",
    "<S> we show that training the tagger on native twitter data enriched with little amounts of specifically selected gold data and additional silver - labelled data scraped from facebook , yields better results than using large amounts of manually annotated data from a mix of genres .    </S>",
    "<S> nellambito della campagna di valutazione postwita di evalita  2016 , addestriamo due modelli che differiscono nel grado di supervisione in fase di training . </S>",
    "<S> il modello addestrato con due cicli di bootstrapping usando post da facebook , e che quindi impara anche da etichette  silver \" , ha una performance superiore alla versione supervisionata che usa solo dati annotati manualmente . </S>",
    "<S> discutiamo limportanza della scelta dei dati di training e development . </S>"
  ]
}