{
  "article_text": [
    "around seven decades ago , an american scientist , warren weaver , classified scientific problems into three classes : problems of _ simplicity _ , problems of _ disorganized _ complexity , and problems of _ organized _ complexity @xcite .",
    "for example , the classical dynamics can be used to analyze and predict the motion of a few ivory balls as they move about on a billiard table .",
    "this is a typical problem of simplicity .",
    "imagine then , a large billiard table with millions of balls rolling over its surface , colliding with one another and with the side rails .",
    "although to be sure the detailed history of one specific ball can not be traced , statistical mechanics can analyze and predict the average motions .",
    "this is a typical problem of disorganized complexity .",
    "problems of organized complexity , however , deal with features of organization such as living things , ecosystems , and artificial things . here , cells in a living thing are interrelated into an organic whole in their positions and motions , whereas the balls in the above illustration of disorganized complexity are distributed in a helter - skelter manner .    in the tradition of lord kelvin ,",
    "the quantitative definition of complexity is the most fundamental and important notion in problems of complexity .    _",
    "`` i often say that when you can measure what you are speaking about , and express it in numbers , you know something about it ; but when you can not measure it , when you can not express it in numbers , your knowledge is of a meagre and unsatisfactory kind ; it may be the beginning of knowledge , but you have scarcely in your thoughts advanced to the state of science , whatever the matter may be . '' _",
    "_ lord kelvin , 1883 _    the quantitative definition of _ disorganized _ complexity of physical systems has been established to be _ entropy _ , which is defined in thermodynamics and statistical mechanics . in a similar manner ,",
    "disorganized complexity of information sources ( distributions ) can be quantitatively defined by shannon entropy @xcite .",
    "in contrast , there is no agreed upon quantitative definition of _ organized _ complexity .",
    "the difficulty comes from the notion that organized complexity could be greatly dependent on our senses or that the objects of organized complexity like living things , ecosystems , and artificial things may be recognized only by intelligent organisms like human beings , that is to say , it is vastly different from the measures of disorganized complexity such as entropy and shannon entropy which simply quantify the randomness of the objects",
    ".    we may therefore wonder whether such sensory and vague things can be rigorously defined in a unified manner covering various living things to artificial things .",
    "many investigations nonetheless have been pursued toward this aim in the last decades , e.g. , _ logical depth _ by bennett @xcite , _ effective complexity _ by gel - man @xcite , _ thermodynamic depth _ by lloyd and pagels @xcite , _ effective measure complexity _ by grassberger @xcite , and _ statistical complexity _ by crutchfield et al .",
    "@xcite , although no existing measure has been agreed on in the field .",
    "@xcite . in section [ sec : survey ] , we explain our understanding on why no existing measure is satisfactory to be agreed upon .",
    "the quantitative definition of complexity of an object is essentially related to the amount of information that the object possesses . for example , shannon entropy , which is the quantitative definition of disorganized complexity of an information source , was introduced to define the amount of information of a source in the sense of shannon s information theory @xcite .",
    "one year after shannon introduced his information theory ( and weaver published the aforementioned article @xcite ) , weaver proposed that there are three levels of information and communication problems @xcite :    - level a :  how accurately can the symbols of communication be transmitted ? ( the technical problem . )    - level b :  how precisely do the transmitted symbols convey the desired meaning ? ( the semantic problem . )    - level c :  how effectively does the received meaning affect conduct in the desired way ? ( the effectiveness problem . )",
    "interestingly , the classes of scientific problems classified by weaver @xcite are closely related to the above - mentioned three levels of information problems .",
    "the _ disorganized _ complexity measures , e.g. , shannon entropy , are related to the level a problem , _ technical or syntactic _ problem , e.g. , shannon information theory , and the _ organized _ complexity measures should be related to the level b and c problems , i.e. , _ semantic and effectiveness _ problems .",
    "there have been many studies on the level b ( semantic ) problem spanning more than six decades in terms of the semantic information theory @xcite , but no existing work has been recognized as a standard theory . in section [ sec : existing - semantic - theories ] , we show a fundamental problem common among the existing works for level b that forms the basis of our understanding of why no existing work could be a standard theory .",
    "roughly , all existing work assumes some a priori information which is not concretely specified , where informal observation might be possible but no formal result could be achieved rigorously without concrete specification of such a priori information .",
    "in addition , no observation has been presented in literature on the relation between the semantic ( level b ) problem and the organized complexity . to the best of our knowledge ,",
    "no study has been conducted seriously on the level c ( effectiveness ) problem .",
    "this paper presents a new quantitative definition of organized complexity .",
    "in contrast to existing measures such as kolmogorov complexity , logical depth , effective complexity and statistical complexity , this new definition simultaneously captures the three major features of complexity : computational feature ( similar to logical depth ) , descriptional feature ( similar to kolmogorov complexity and effective complexity ) and distributional feature ( similar to statistical complexity ) .",
    "in addition , the proposed definition is computable and can measure both probabilistic and deterministic forms of objects in a unified manner .",
    "the proposed definition is based on _ circuits _",
    "@xcite rather than turing machines @xcite and @xmath0-machines @xcite .",
    "our new measure is given by the shortest size of a stochastic finite - state automaton form of circuit , _ oc - circuit _",
    ", for simulating the object . here",
    "note that , given an object , the shortest size of an oc - circuit to simulate the object is computable and that the size of an oc - circuit can capture the computational , descriptional and distributional features of complexity of the object .",
    "we give several criteria required for organized complexity measures and show that the proposed definition is the first that satisfies all of the requirements .",
    "we then present the first semantic information theory for the level b ( semantic ) problem that overcomes the fundamental problem common among all previous works .",
    "that is , the proposed semantic information theory is constructed only on concretely defined notions .",
    "this theory is based on the proposed organized complexity measure .",
    "we then unveil several fundamental properties in the semantic information theory , e.g. , a semantic source coding theorem and semantic channel coding theorem . moreover , this paper , for the first time , develops a theory for the effectiveness ( level c ) problem , which is also constructed on our organized complexity measure .",
    "in other words , we clarify the relationship of organized complexity with the semantic and effectiveness ( level b and c ) problems of information and communication .",
    "thus , this paper presents the first unified paradigm for the organized complexity and the semantic information theory that covers the semantic and effectiveness problems .",
    "the sets of natural , rational , and real numbers are denoted by @xmath1 , @xmath2 , and @xmath3 , respectively .",
    "the set of @xmath4-bit strings is denoted by @xmath5 ( @xmath6 ) , @xmath7 , and the null string ( 0-bit string ) is denoted by @xmath8 .",
    "when @xmath9 , @xmath10 denotes the bit length of @xmath11 .",
    "when @xmath12 , @xmath13 $ ] denotes set @xmath14 .",
    "when @xmath15 , @xmath16 denotes the smallest integer greater than or equal to @xmath11 .",
    "when @xmath11 is a variable and @xmath17 is a value or @xmath18 denotes that @xmath11 is substituted or defined by @xmath17 .",
    "a probability distribution over @xmath5 is @xmath19 ,   \\sum_{a\\in \\{0,1\\}^n}p_a=1 \\}$ ] .",
    "when @xmath20 is a probability distribution , or the source ( machinery ) of the distribution , @xmath21 denotes that element @xmath22 is randomly selected from @xmath20 according to its probability distribution .",
    "when @xmath20 is a set , @xmath23 denotes that @xmath24 is randomly selected from @xmath20 with a uniform distribution .",
    "when @xmath25 and @xmath26 are two distributions , the statistical distance of @xmath25 and @xmath26 , @xmath27 , is defined by @xmath28 - \\pr[\\alpha { \\stackrel{\\ { \\sf r}}{\\leftarrow}}y ] |$ ] , and @xmath29 denotes that @xmath27 is bounded by @xmath30 .",
    "then we say @xmath25 and @xmath26 are statistically @xmath30-close .",
    "when @xmath26 is a distribution , @xmath31 denotes the @xmath4-bit restriction of @xmath26 , i.e. , @xmath31 is a distribution over @xmath5 and @xmath32 = \\sum_{z \\in \\{0,1\\}^ * } \\pr[(y , z ) { \\stackrel{\\ { \\sf r}}{\\leftarrow}}y ] $ ] .",
    "when @xmath33 is a set , @xmath34 denotes the number of elements of @xmath33 .",
    "the existing complexity measures can be categorized in two classes .",
    "one is a class of measures whose objects are _ deterministic strings _ , and the objects of the other class of measures are",
    "_ probability distributions_. as for the traditional complexity measures , the kolmogorov complexity is categorized in the former and ( shannon ) entropy is in the latter . among the above - mentioned organized complexity measures , logical depth and effective complexity",
    "are in the former , and thermodynamic depth , effective measure complexity , and statistical depth are in the latter .    which is more appropriate as the objects of organized complexity ?",
    "the objects of complexity are everything around us , stars and galaxies in space , living things , ecosystems , artificial things , and human societies .",
    "the existence of everything can be recognized by us only through observations .",
    "for example , the existence of many things are observed through devises such as the telescope , microscope , various observation apparatus and electronic devices .",
    "we can take things around us directly into our hands and sense them , but they are also recognized by our brains as electronic nerve signals transmitted from the sensors of our five senses through the nervous system .",
    "that is , all objects of complexity are recognized as the result of observations by various apparatus and devices including the human sensors of our five senses .",
    "since the micro world is governed by quantum mechanics , observed values are determined in a probabilistic manner .",
    "this is because observed values ( data ) obtained when observing micro phenomena ( quantum states ) in quantum mechanics are randomly selected according to a certain probability distribution corresponding to a quantum state ( e.g. , entangled superposition ) .    how then , are observed values in macro phenomena ?",
    "for example , if some sort of radio signals are received and measured , they would almost certainly be accompanied by noise .",
    "there are various reasons why noise becomes mixed in with signals , and one of them is thought to be the probabilistic phenomena of electrons , thermal noise .",
    "similarly , various types of noise will be present in the data obtained when observing distant astronomical bodies .",
    "this can be caused by the path taken by the light ( such as through the atmosphere ) and by factors associated with the observation equipment .    even in the case of deterministic physical phenomena ,",
    "chaos theory states that fluctuations in initial conditions can lead to diverse types of phenomena that behave similar to those of random systems . in short",
    ", even a deterministic system can appear to be a quasi - probabilistic system .",
    "but even a system of this type can become a true ( non - quasi ) probabilistic system if initial conditions fluctuate due to some noise , e.g. , thermal noise .",
    "there are also many cases in which a quasi - probabilistic system associated with chaos can not be distinguished from a true probabilistic system depending on the precision of the observation equipment . here",
    ", even quasi - probabilistic systems may be treated as true probabilistic systems .",
    "thus , when attempting to give a quantitative definition of the complexity of observed data , the source of those observed data would be a probability distribution and the observed data themselves would be values randomly selected according to that distribution . if we now consider the complexity of a phenomenon observed using certain observation equipment , the object of this complexity should not be the observed data selected by chance from the source but rather the probability distribution itself corresponding to the source of the observed data .",
    "it is known that some parts of genome patterns appear randomly distributed over a collection of many samples ( over generations ) . here",
    ", we can suppose a source ( probability distribution ) of genome patterns , from which each genome pattern is randomly selected .",
    "also in this case , the object of complexity should not be each individual genome pattern but rather the source ( probability distribution ) of the genome patterns .    therefore , hereafter in this paper , we consider that an object of complexity is a _",
    "probability distribution_. here note that a deterministic string can be considered to be a very special case of a probability distribution ( where only a value occurs with probability 1 and the others with 0 ) .    how can we determine a source or probability distribution from observed data ?",
    "it has been studied as the _ model selection theory _ in statistics and information theory , e.g. , aic ( akaike s information criterion ) by akaike @xcite and mdl ( minimum description length ) by rissanen @xcite , given a collection of data , to find the most likely and succinct model ( source , i.e. , probability distribution ) of the data .",
    "in this paper , however , it is outside the scope , i.e. , we do not consider how to find such a source from a collection of observed data ( through the model selection theory ) .",
    "we here suppose that a source ( probability distribution ) is given as an object of organized complexity , and focus on how to define quantitatively the complexity of such a given source .",
    "there are roughly two types of observed data , one type is data observed at a point in time and the other is time series data .",
    "genome pattern data are an example of the former , and data obtained from an observation apparatus for a certain time period are an example of the latter . in any case , without loss of generality , we here assume that observed data @xmath11 are bounded and expressed in binary form , i.e. , @xmath35 for some @xmath36 , since any physically observed data have only finite precision ( no infinite precision ) .",
    "then the _ source _ of the observed data , @xmath25 , which is an object of organized complexity in this paper , is a probability distribution over @xmath5 for some @xmath36 such that @xmath37 .",
    "we describe our attempt to define quantitatively the organized complexity . to begin with ,",
    "let us consider the following example .",
    "we give a chimpanzee a computer keyboard and prompt the chimpanzee to hit the keys freely resulting in the output of a string of characters .",
    "let us assume an output of 1000 alphabetical characters .",
    "at the same time , we select a string of 1000 characters from one of shakespeare s plays . naturally , the character string input by the chimpanzee is gibberish possessing no meaning , which undoubtedly makes it easy for us to distinguish that string from a portion of a shakespearean play .",
    "is there a way , however , to construct a mathematical formulation of the difference between these two strings that we ourselves can easily tell apart ?",
    "why is it so easy for us to make a distinction between these two strings ?",
    "the answer is likely that the chimpanzee s string is simply random ( or disorganized ) and meaningless to us while shakespeare s string is highly organized and meaningful . in short , if we can mathematically define the amount of organized complexity ( or meaningful information ) , we should be able to make a distinction between these two strings .",
    "what then are the sources of the observed data , the chimpanzee s string and shakespeare s string .",
    "let us return to the source of the chimpanzee s string creation without thinking of it as simply a deterministic string . here , for the sake of simplicity",
    ", we suppose that the scattered hitting of keys by the chimpanzee is the same as a random selection of hit keys . at this time , the source of the chimpanzee s string is the probability distribution in which any particular 1000-character string can be randomly selected from all possible 1000-character strings with equal probability .",
    "what , then , would be the source of shakespeare s string ?",
    "we can surmise that , when shakespeare wrote down this particular 1000-character string , a variety of expressions within his head would have been candidates for use , and that the 1000-character string used in the play would have finally been selected from those candidates with a certain probability .",
    "the candidates selected must certainly be connected by complex semantic relationships possessed by english words . accordingly",
    ", the source of shakespeare s string must be the complex probability distribution of candidate expressions connected by complex semantic relationships .",
    "for example ( case 1 ) , candidate expression 1 has the probability of 0.017 , candidate expression 2 has the probability of 0.105 , ... , candidate expression 327 has the probability of 0.053 and the other expressions have the probability of 0 , i.e. , hundreds of candidate expressions occurred in his head consciously or unconsciously and finally one of them was randomly chosen according to the distribution . as more simplified cases , case 2 is where candidate expression 1 has the probability of 2/7 , candidate expression 2 has the probability of 5/7 and the others have the probability of 0 , i.e. , only two candidate expressions occurred in his head and finally one of them was randomly chosen .",
    "case 3 is where only a single expression has the probability of 1 and the others have the probability of 0 , i.e. , a deterministic string case ; he selected the expression without hesitation .",
    "these expressions as well as the distributions should be highly organized and structured with complex semantic relationships .    considering the above - mentioned observation , we give the following criteria for formulating the organized complexity measures .    1",
    ".   the objects should be probability distributions .",
    "in addition , deterministic strings ( as a special case of distributions ) and more general distributions should be treated in a unified manner , e.g. , the complexity of cases 1 , 2 and 3 for the source of shakespeare s string should be measured in a unified manner .",
    "simple ( or very regular ) objects , which are treated as `` problems of simplicity '' based on weaver s classification , should have low organized complexity .",
    "3 .   simply random objects , which are treated as `` problems of disorganized complexity '' by weaver , e.g.",
    ", the source of the chimpanzee s string , should have low organized complexity .",
    "4 .   highly organized objects , which are treated as `` problems of organized complexity '' by weaver , e.g. , cases 1 , 2 and 3 for the source of shakespeare s string , should have high organized complexity . 5 .",
    "the organized complexity of an object should be computable ( or recursive in computation theory ) .      using these criteria",
    ", we now survey the typical quantitative definitions of organized complexity in literature .",
    "* objects are `` deterministic strings '' *    * * kolmogorov complexity * + the notion of the kolmogorov complexity was independently proposed by solomonoff , kolmogorov , and chaitin @xcite . + roughly , the kolmogorov complexity of string @xmath11 is the size of the shortest program ( on a computer ) to produce string @xmath11 .",
    "+ more precisely , let @xmath38 be a reference universal prefix ( turing ) machine ( see @xcite for the reference universal prefix machine ) .",
    "then , the kolmogorov complexity , @xmath39 , of string @xmath9 is defined by @xmath40 + in light of the above - mentioned criteria , the kolmogorov complexity has the following properties . 1 .   the objects are only deterministic strings ( bad ) .",
    "simple ( or very regular ) objects have low kolmogorov complexity ( good ) .",
    "simply random objects , deterministic @xmath4-bit strings , that are uniformly and randomly chosen from @xmath5 have high kolmogorov complexity ( bad ) .",
    "highly organized objects may have between high and low logical depth ( bad ) , since some highly organized complex objects that are generated from small strings through very long running - time and complex computations may have low kolmogorov complexity .",
    "in other words , some organized complexities may be characterized in a dynamic manner as logical depth rather than in a static manner as kolmogorov complexity .",
    "the kolmogorov complexity of an object ( string ) is not computable ( bad ) . * * logical depth * + bennett @xcite introduced _ logical depth _ with the intuition that complex objects are those whose most plausible explanations describe long causal processes . to formalize the intuition , bennett employs the methodology of algorithmic information theory , the kolmogorov complexity .",
    "+ the logical depth of a deterministic string , bennett s definition for measuring organized complexity , is dependent on the running time of the programs that produce the string and whose length is relatively close to the minimum in a sense .",
    "+ more precisely , the logical depth of string @xmath11 at significance level @xmath41 @xcite is @xmath42 where we define @xmath43 and @xmath44 by @xmath45   @xmath46 here , @xmath38 is the reference universal prefix ( turing ) machine ( for the kolmogorov complexity ) and @xmath47 is a specific class of @xmath38 whose running time is bounded by @xmath48 steps .",
    "@xmath49 is the length of program @xmath50 .",
    "+ in light of the above - mentioned criteria , the logical depth has the following properties . 1",
    ".   the objects are only deterministic strings ( bad ) .",
    "simple ( or very regular ) objects have low logical depth ( good ) .",
    "3 .   simply random objects have low logical depth ( good ) .",
    "4 .   highly organized objects may have between high and low logical depth ( bad ) , since some highly organized complex objects may have low logical depth with relatively high kolmogorov complexity , where the core part of the organized complexity is due to the kolmogorov complexity .",
    "in other words , some organized complexities may be characterized in a static manner as kolmogorov complexity rather than in a dynamic manner as logical depth , ( where @xmath51 is required but the logical depth is not dependent on the value of @xmath44 itself ( roughly , @xmath52 is close to the kolmogorov complexity of @xmath11 ) ) .",
    "the logical depth of an object ( string ) is not computable , since it is based on the kolmogorov complexity or universal turing machines ( bad ) . * * effective complexity * + effective complexity @xcite was introduced by gell - mann , and is based on the kolmogorov complexity . to define the complexity of an object",
    ", gell - mann considers the shortest description of the distribution in which the object is embedded as a typical member . here , ` typical ' means that the negative logarithm of its probability is approximately equal to the entropy of the distribution . + that is , the effective complexity of string @xmath11 is @xmath53 where @xmath54 is the kolmogorov complexity of distribution @xmath55 , i.e.",
    ", the length of the shortest program to list all members , @xmath56 , of @xmath55 together with their probabilities , @xmath57 , and @xmath58 is the ( shannon ) entropy of @xmath55 . + in light of the above - mentioned criteria , the effective complexity has the following properties . 1",
    ".   the objects are only deterministic strings ( bad ) .",
    "technically , however , we can consider distribution @xmath55 to be an object of the effective complexity .",
    "simple ( or very regular ) objects have low effective complexity ( good ) .",
    "simply random objects have low effective complexity ( good ) .",
    "4 .   highly organized objects may have between high and low effective complexity ( bad ) , since some highly organized complex objects may have low effective complexity with very high computational complexity of the universal machine to generate @xmath55 , where the core part of the organized complexity is due to the computational complexity in a dynamic manner ( e.g. , high logical depth of @xmath55 ) . 5 .",
    "the effective complexity of an object ( string ) is not computable , since it is based on the kolmogorov complexity or universal turing machines ( bad ) .",
    "* objects are `` probability distributions '' *    * * thermodynamic depth * + the _ thermodynamic depth _ was introduced by lloyd and pagels @xcite and shares some informal motivation with logical depth , where complexity is considered a property of the evolution of an object .",
    "+ we now assume the set of histories or trajectories that result in object ( distribution ) @xmath59 .",
    "a trajectory is an ordered set of macroscopic states ( distributions ) @xmath60 .",
    "the thermodynamic depth of object @xmath59 is @xmath61 where @xmath62 is the conditional entropy of combined distribution @xmath63 with condition @xmath64 .",
    "+ one of the major problems with this notion is that it is not defined how long the trajectories ( what value of @xmath65 ) should be .",
    "moreover , it is impossible to specify formally the trajectories , given an object , since there is no description on how to select macroscopic states in @xcite .",
    "if there are thousands of possible sets of macroscopic states , we would have thousands of different definitions of the thermodynamic depth . +",
    "another fundamental problem with this measure is that in order to measure the complexity of an object @xmath59 , a set of macroscopic states @xmath66 whose complexity is comparable to or more than that of @xmath59 should be established beforehand .",
    "hence , if @xmath67 is fixed , or the thermodynamic depth of @xmath67 is concretely defined , it can not measure the complexity of an object whose complexity is more than that of @xmath67 .",
    "that is , any concrete definition of this notion can measure only a restricted subset of objects , i.e. , any concrete and generic definition is impossible in thermodynamic depth .",
    "it should be a fundamental problem with this concept .",
    "+ as a result , it is difficult to define rigorously the thermodynamic depth and to characterize the definition .",
    "+ note that we have the same criticisms for the existing semantic information theories that are described in section [ sec : existing - semantic - theories ] . *",
    "* effective measure complexity * + the _ effective measure complexity _ was introduced by grassberger @xcite and measures the average amount by which the uncertainty of a symbol in a string decreases due to the knowledge of previous symbols . + for distribution @xmath68 over @xmath69 @xmath70 , @xmath71 is the shannon entropy of @xmath68 .",
    "let @xmath72 , and @xmath73",
    ". the effective measure complexity of @xmath74 is @xmath75 this difference quantifies the perceived randomness which , after further observation , is discovered to be order @xcite .",
    "+ in light of the above - mentioned criteria , the effective measure complexity has the following properties : 1 .",
    "the objects are only probability distributions , and deterministic strings are outside the scope of this measure ( the complexity is 0 for any deterministic string ) ( bad ) .",
    "simple ( or very regular ) objects have low effective measure complexity ( good ) .",
    "simply random objects have low effective measure complexity ( good ) .",
    "4 .   highly organized objects may have between high and low effective measure complexity ( bad ) , since some highly organized complex objects ( distributions ) may have low effective measure complexity with very high kolmogorov complexity or computational complexity of the universal machine to generate them , where the core of the organized complexity is due to some kolmogorov complexity or the computational complexity .",
    "in other words , the effective measure complexity cares only about distributions but not the computational features that logical depth and effective complexity care about .",
    "the effective measure complexity of an object ( distribution ) is computable , since it is not based on any turing machine ( good ) . *",
    "* statistical complexity * + the _ statistical complexity _ was introduced by crutchffeld and young @xcite . here , to define the complexity",
    ", the set of causal states @xmath33 and the probabilistic transitions between them are modeled in the so - called @xmath0-machine , which produces a stationary distribution of causal states , @xmath76 . the mathematical structure of the @xmath0-machine is a stochastic finite - state automaton or hidden markov model .",
    "+ let @xmath77 for @xmath78 be causal states , @xmath79 and @xmath80 be the probability of a transition from state @xmath77 to state @xmath81 , i.e. , @xmath82 $ ] .",
    "each transition from @xmath77 to @xmath81 is associated with an output symbol , @xmath83 , ( e.g. , @xmath84 ) .",
    "then , @xmath85 $ ] , the probability that @xmath77 occurs in the infinite run of the @xmath0-machine , is given by the eigenvector of matrix @xmath86 , since @xmath87 \\cdot t_{ij } = \\pr[s_j]$ ] , i.e. , @xmath88 , ... , \\pr[s_k ] ) \\cdot t = ( \\pr[s_1 ] , ... , \\pr[s_k])$ ] .",
    "hence , the machine produces a stationary distribution of states , @xmath76 .",
    "the output of the @xmath0-machine is the infinite sequence of @xmath83 induced by the infinite sequence of the transition of states .",
    "that is , @xmath0-machine outputs a distribution , @xmath89 , over @xmath90 , induced by @xmath76 .",
    "+ the statistical complexity of object @xmath25 ( distribution ) , denoted @xmath91 , is the minimum value of the shannon entropy of @xmath76 , @xmath92 , when @xmath93 : @xmath94 + a more generalized notion , @xmath95 ( @xmath96 ) , is defined by the reny entropy of @xmath76 in place of the shannon entropy , i.e. , @xmath97 where @xmath91 is the case where @xmath98 as @xmath99 is the shannon entropy , and @xmath100 ( @xmath101 ) ( @xmath34 is the number of elements of set @xmath33 ) ( for @xmath102 , @xmath103 is the mini - entropy ) .",
    "+ in light of the above - mentioned criteria , the statistical complexity has the following properties : 1 .",
    "statistical complexity @xmath91 can measure only probability distributions as objects , since @xmath104 for any deterministic string .",
    "complexity @xmath105 can not measure the deterministic strings well either , since the organized complexity may be around @xmath106 for high kolmogorov complexity or high logical depth deterministic strings but @xmath107 .",
    "moreover , @xmath105 can not capture the distribution of the @xmath0-machine , since it only depends on the number of vertexes of causal states .",
    "that is , none of @xmath95 with a value of @xmath108 ( @xmath96 ) can measure probability distributions and deterministic strings in a unified manner ( bad ) .",
    "simple ( or very regular ) objects have low statistical complexity ( good ) .",
    "simply random objects have low statistical complexity ( good ) .",
    "4 .   as for the standard definition of statistical complexity , i.e. , @xmath98 or the shannon entropy , highly organized objects may have between high and low statistical complexity ( bad ) , since ( 1 ) some highly organized complex objects ( almost deterministic strings ) may have low statistical complexity , almost zero , where the core of the organized complexity is due to the complexity of the almost deterministic data part , and ( 2 ) some highly organized complex objects ( distributions ) have relatively low statistical complexity with highly complex output mapping @xmath109 of @xmath0-machines , where the core of the organized complexity is due to the complexity of @xmath109 of @xmath0-machines ( statistical complexity @xmath95 depends on only @xmath76 but is independent of the complexity of output mapping @xmath109 ) .",
    "+ see remark [ rmk : statistical complexity ] for more precise observation .",
    "the statistical complexity of an object ( distribution / strings ) is computable , since it is not based on any turing machines ( good ) .    in summary ,",
    "the existing measures have the following drawbacks .    *",
    "every existing complexity measure focuses on a single feature of complexity , for example , logical depth focuses on the computational complexity feature , the kolmogorov complexity and effective complexity focus on the descriptional complexity feature , and statistical complexity focuses on the distributional complexity feature , but no existing measure captures all of them simultaneously . *",
    "every existing complexity measure can treat either a probabilistic or deterministic form of an object , but no measure can cover both in a unified manner .",
    "* some of the measures , the kolmogorov complexity , effective complexity and logical depth , that are based on turing machines are not computable .",
    "we now propose a new quantitative definition of organized complexity . roughly speaking ,",
    "the proposed quantitative definition is given by the shortest description size of a ( stochastic finite - state automaton form of ) _ circuit _",
    "@xcite that simulates an object ( probability distribution ) .    in the existing complexity measures surveyed in section [ sec : survey ] , some computing machineries are employed . in the logical depth and effective complexity ,",
    "universal turing machines are employed , which cause the uncomputability of their measures . in the effective measure complexity",
    ", no computational machinery is used ; hence , it can not capture the computational and descriptional features of organized complexities , which logical depth and effective complexity capture , respectively .",
    "the statistical complexity employs @xmath0-machines , whose mathematical model is a stochastic finite - state automaton or hidden markov model ; hence it captures the distributional features of organized complexities but not the computational , descriptional , and deterministic - object features .",
    "see remark [ rmk : statistical complexity ] for more details .    in the place of universal turing machines and @xmath0-machines",
    ", we employ another class of machinery , a stochastic finite - state automaton form of _ circuit _ , _ oc - circuit_. our new measure is given by the _",
    "shortest _ description size of an oc - circuit for simulating the object .",
    "that is , occam s razor plays a key role in our definition .",
    "the advantage of using circuits is that it can capture the computational and distributional features of complexity as the size of a circuit as well as the descriptional features of complexity as the input size of a circuit .",
    "moreover , the shortest description size of an oc - circuit for simulating an object is computable ( theorem [ thm : computable ] ) , in contrast to that in which the shortest program size on a turing machine is uncomputable @xcite .",
    "our approach is more general than the approach by @xmath0-machines in the statistical complexity , since our oc - circuit model can simulate any @xmath0-machine as a special case ( theorem [ thm : sim - epsilon ] ) .",
    "the major difference between circuits and turing machines is that a single ( universal ) turing machine can compute any size of input , while a single circuit can compute a fixed size of input . in spite of the difference ,",
    "any bounded time computation of a turing machine can be computed by a bounded size of a circuit @xcite .",
    "hence , the proposed complexity measure based on circuits captures general computational features in complexity .",
    "in addition , the finiteness of each circuit yields the computability of our measure , in contrast to the uncomputability of turing machine based measures such as logical depth and effective complexity as well as the kolmogorov complexity .",
    "we now define a new measure of organized complexity .",
    "first , we define our computation model , _ oc - circuit_.    ( oc - circuit ) [ def : oc - circuit ]   let circuit @xmath64 with @xmath110 input bits and @xmath65 output bits be a directed acyclic graph in which every vertex is either an input gate of in - degree 0 labeled by one of the @xmath110 input bits , or one of the basis of gates @xmath111 : = \\{and , or , not}. among them , @xmath65 gates are designated as the output gates .",
    "that is , circuit @xmath64 actualizes a boolean function : @xmath112 .",
    "let @xmath113 be a state at step @xmath114 ( @xmath115 ) , @xmath116 be an a priori input ( universe ) , @xmath117 be an input at step @xmath114 , @xmath118 be random bits at step @xmath114 , and @xmath119 .",
    "then , @xmath120 i.e. ,   @xmath121 where @xmath122 , @xmath123 is the output of @xmath64 at step @xmath114 , and @xmath124 .",
    "let @xmath125 be the number of vertexes of @xmath64 .",
    "let @xmath126 be a canonical description of @xmath64 , where @xmath127 is the adjacent matrix of directed graph @xmath64 , i.e. , @xmath128 iff there is an edge from vertex @xmath114 to vertex @xmath129 , and @xmath130 otherwise , @xmath131 ( @xmath132 ) is the label of the @xmath114-th vertex , @xmath133 , i.e. , each vertex @xmath114 is labeled by @xmath131 , and @xmath134 is the vertex designated to the @xmath114 s output , i.e. , @xmath135 is the sequence of output gates .",
    "hereafter , we abuse the notation of @xmath64 to denote @xmath136 , the canonical description of @xmath64 .",
    "let @xmath137 be an `` oc - circuit '' , and @xmath26 be the output of @xmath138 , where @xmath139 @xmath140 , @xmath141 @xmath142 @xmath143 , @xmath144 ( see section [ sec : notation ] for the notation of @xmath145 ) .",
    "the output , @xmath26 , of @xmath146 can be expressed by @xmath147 , i.e. , @xmath148 , where the probability of distribution @xmath26 is taken over the randomness of @xmath118 ( @xmath149 ) .",
    "then , @xmath150 , @xmath151 , and @xmath152 are called the `` logic , '' `` universe , '' and `` semantics '' of oc - circuit @xmath146 , respectively .",
    "[ rmk : random ] circuit @xmath64 of oc - circuit @xmath146 is a probabilistic circuit , where uniformly random strings , @xmath118 for @xmath149 , are input to @xmath64 and the output of @xmath146 is distributed over the random space of @xmath153 .    here note that @xmath153 , which is an input to @xmath64 , is not included in @xmath146 , while the other inputs to @xmath64 , @xmath151 and @xmath154 , are included in @xmath146 .",
    "in other words , the size of the randomness , @xmath155 , is ignored in the size of @xmath146 or the definition of the organized complexity ( see definition [ def : oc ] ) , while @xmath156 and a part of @xmath64 regarding the randomness are included in @xmath146 .",
    "this is because the randomness , @xmath153 , is just the random source of @xmath146 s output distribution and has no organized complexity itself .",
    "hence , simply random objects are characterized to have low organized complexity based on the size of oc - circuit @xmath146 ( see item 3 in the property summary of the proposed complexity measure in the end of this section ) .",
    "[ rmk : naming ] although the parts of an oc - circuit , @xmath150 , @xmath151 , and @xmath152 , are named logic , universe , and semantics , respectively , we do not care about the meanings of these names in section [ sec : proposed ] .",
    "we care more about these meanings in section [ sec : proposed - sit ] .",
    "( organized complexity ) [ def : oc ]    let @xmath25 be a distribution over @xmath5 for some @xmath6 .",
    "`` organized complexity '' @xmath157 of distribution @xmath25 at precision level @xmath30 ( @xmath158 ) is @xmath159 where @xmath160 is an oc - circuit , and @xmath161 denotes the bit length of the binary expression of @xmath138 ( see section [ sec : notation ] for the notations of @xmath162 ) .",
    "we call oc - circuit @xmath163 @xmath164 the shortest ( or proper ) oc - circuit of @xmath25 at precision level @xmath30 , if @xmath165 and @xmath166",
    ". if there are multiple shortest oc - circuits of @xmath25 , i.e. , they have the same bit length , the lexicographically first shortest one is selected as the shortest oc - circuit .    then , @xmath167 @xmath168 , and @xmath169 are called the `` proper logic , '' `` proper universe , '' and `` proper semantics '' of @xmath25 at precision level @xmath30 , respectively . here , @xmath170 .",
    "[ thm : computable ] for any distribution @xmath25 over @xmath5 ( @xmath36 ) and any precision level @xmath171 , @xmath172 can be computed .    * proof *    for any distribution @xmath173 ,",
    "\\   \\sum_{x \\in \\{0,1\\}^n } p_x = 1 \\}$ ] and any precision level @xmath174 , there always exists another distribution @xmath175 such that @xmath176 ( here note that @xmath177 is changed to @xmath178 provided that @xmath176 ) .",
    "we then construct the truth table of boolean function @xmath179 such that @xmath180 = p'_x$ ] for all @xmath35 , where the probability is taken over @xmath181 .",
    "such a function , @xmath182 , can be achieved by setting truth table @xmath183 such that @xmath184 for all @xmath35 .",
    "since any boolean function can be achieved by a circuit with basis @xmath111 : = \\{and , or , not } @xcite , we construct circuit @xmath185 for oc - circuit @xmath186 with @xmath187 ( i.e. , @xmath188 ) , @xmath189 , @xmath190 , @xmath191 , @xmath192 .",
    "that is , @xmath193 , and the output of @xmath186 is @xmath194 with the same distribution as that of @xmath195 over the randomness of @xmath196 .",
    "that is , @xmath197 and @xmath198 .    from the definition of @xmath157 , @xmath199 .",
    "we then , exhaustively check all values of @xmath200 with @xmath201 whether @xmath200 is an oc - circuit such that @xmath202 here note that we can syntactically check whether or not @xmath200 is the correct form of an oc - circuit .",
    "finally , we find the shortest one among the collection of @xmath200 ( and @xmath186 ) satisfying the condition . clearly , the size of the shortest one is @xmath172 .",
    "@xmath203    [ rmk : oc - circuit ] as clarified in this proof , given object ( distribution ) @xmath25 and precision level @xmath30 , the proposed definition of organized complexity uniquely determines ( computes ) not only organized complexity @xmath172 but also the shortest ( proper ) oc - circuit , @xmath204 , including proper logic @xmath167 , proper universe @xmath168 , and proper semantics @xmath169 of @xmath25 . in other words ,",
    "the definition characterizes the complexity features of object @xmath25 , i.e. , it characterizes not only organized complexity @xmath172 , but also structural complexity features of @xmath25 , e.g. , computational and distributional features by the size of @xmath167 and descriptional features by the size of @xmath168 and @xmath169 .    in the following theorem",
    ", we show that the notion of oc - circuit with the proposed organized complexity includes the @xmath0-machine with statistical complexity introduced in section [ sec : survey ] as a special case .",
    "[ thm : sim - epsilon ] any @xmath0-machine can be simulated by an oc - circuit .    * proof *    given @xmath0-machine , @xmath205 @xmath206",
    ", we construct oc - circuit @xmath138 @xmath207 @xmath208 such that @xmath209 @xmath210 @xmath211 ( i.e. , @xmath212 , @xmath213 , @xmath214 , @xmath215 @xmath216 @xmath217 ( initial causal state ) , @xmath218 , @xmath219 @xmath8 ( null string ) , @xmath220 @xmath8 ( @xmath221 ) and @xmath64 is achieved to satisfy @xmath222 = t_{ij } $ ] for @xmath223 , where @xmath224 is the bit length of the binary expression of @xmath80 , and the probability is taken over the randomness of @xmath225 in each execution of @xmath64 .",
    "it is clear that the behavior of this oc - circuit with respect to the causal states is exactly the same as that for the given @xmath0-machine . @xmath203    ( features of the proposed complexity )  [ rmk : statistical complexity ] the proposed organized complexity",
    "is characterized by the minimum length of the description of whole oc - circuit @xmath138 , but the statistical complexity is characterized by some partial information on @xmath138 , i.e. , only the average size of a compressed coding of a causal state , @xmath226 , for @xmath227 , or the ( uncompressed ) size of a causal state , @xmath228 , for @xmath229 .",
    "that is , our complexity measure captures the complexity of whole circuit ( logic ) @xmath150 , while the statistical complexity only captures a partial property of the `` distributional complexity '' of @xmath150 , the compressed or uncompressed size of a causal state , but ignores the distributional complexity of @xmath80 and @xmath83 ( expressed by @xmath156 and the complexity of @xmath150 ) .",
    "that is , even if we only focus on the distributional complexity features ( where @xmath230 ) , the statistical complexity only captures some of the features , while our definition captures the whole as the size of @xmath150 including @xmath228 and @xmath156 .",
    "in addition , our complexity measure can treat more general cases with @xmath231 and @xmath232 , while statistical complexity only considers a limited case with @xmath233 and @xmath234 , i.e. , it ignores the descriptional complexity features as well as the computational features . for example , a sequence in a genome pattern that is common to all individuals is considered to be determined in the evolution process , and has some biological meaning . the biological knowledge of dna necessary to understand the dna sequences can be captured by logic @xmath150 and universe @xmath151 of the oc - circuit @xmath146 ( where @xmath235 ) , and the characteristic information ( biological meaning ) on each genome pattern ( @xmath236 can be captured by semantics @xmath152 of @xmath146 ( where @xmath237 ) .",
    "moreover , our complexity definition covers more complexity features .",
    "the `` computational complexity '' features of an object , which are captured by the logical depth , are characterized by the size of logic @xmath150 of oc - circuit @xmath146 in our definition , and the `` descriptional complexity '' features of an object , which are captured by the effective complexity ( and kolmogorov complexity ) , are characterized by the size of universe @xmath151 and semantics @xmath238 of the oc - circuit @xmath146 .",
    "we can achieve a circuit @xmath64 using another basis of gates , e.g. , \\{nand } and \\{and , not } , in place of \\{and , or , not}. we express an oc - circuit using such a basis by @xmath239 and @xmath240 , respectively .",
    "we also express the organized complexity of @xmath25 using such a circuit by @xmath241 and @xmath242 , respectively .",
    "based on such a different basis of gates , a natural variant of the proposed organized complexity , _ structured organized complexity _ , is given below .",
    "( structured organized complexity ) [ def : soc ]    let @xmath25 be a distribution over @xmath5 for some @xmath36 .",
    "let @xmath243 be a structured oc - circuit , where @xmath244 @xmath245 and @xmath246 represents a set of macro gates ( subroutine circuits ) that are constructed from basis gates and that can be hierarchically constructed , where a level of macro gates are constructed from lower levels of macro gates .",
    "in addition , @xmath246 is notationally abused as the canonical description of macro gates in @xmath246 , which is specified in the same manner as that in the canonical description of a circuit .",
    "term @xmath247 is ( the canonical description of ) a circuit constructed from basis gates @xmath111 as well as macro gates in @xmath246 .",
    "structured organized complexity @xmath248 of distribution @xmath25 at precision level @xmath30 ( @xmath158 ) is @xmath249    [ thm : computable - str ] for any distribution @xmath25 over @xmath5 ( @xmath36 ) and any precision level @xmath171 , @xmath250 can be computed .",
    "* proof * given distribution @xmath25 , we can construct structured oc - circuit @xmath186 in the same manner as that shown in the proof of theorem [ thm : computable ] . here",
    "note that any ( basic ) oc - circuit @xmath138 can be expressed as structured oc - circuit @xmath251 where @xmath252 , with slightly relaxing the format for structured oc - circuits , or to allow @xmath252 .    from the definition of @xmath248 , for any value of @xmath253 , @xmath254 .",
    "we then , given @xmath30 , exhaustively check all values of @xmath200 with @xmath201 whether @xmath200 is a structured oc - circuit such that @xmath255 .",
    "finally , we select the shortest one among the collection of @xmath200 ( and @xmath186 ) satisfying the condition .",
    "clearly , the size of the shortest one is @xmath250 .",
    "@xmath203    [ rmk : s - oc - c ] as described in remark [ rmk : oc - circuit ] , the shortest structured oc - circuit with these parameters characterizes the properties of obeject @xmath25 .",
    "it especially shows the optimized hierarchically structured circuit @xmath251 .",
    "[ rmk : s - oc - example ] as mentioned above , we can construct a structured oc - circuit using another basis such as \\{nand } and \\{and , not } , e.g. , @xmath256 and @xmath257 .",
    "then , @xmath246 in @xmath256 can consist of macro gates of and , or , and not from nand gates . since the size of @xmath246 is a constant @xmath258 in @xmath4 , @xmath259    * variations : * [ phrase : variations ]    we have more variations of the organized complexity .    1 .   ( computational distance ) in definitions [ def : oc ] and [ def : soc ] , statistical distance is used for defining the closeness @xmath260 .",
    "we can replace this with the `` computational '' closeness @xmath261 . here , for two distributions , @xmath25 and @xmath26 , over @xmath5 ,",
    "the computational closeness of @xmath25 and @xmath26 is defined by + @xmath262  iff   @xmath263   @xmath264 - \\pr[1 { \\stackrel{\\ { \\sf r}}{\\leftarrow}}d(\\alpha ) \\mid \\alpha { \\stackrel{\\ { \\sf r}}{\\leftarrow}}y ] | < \\delta , $ ] + where @xmath265 is a class of machines @xmath266 .",
    "intuitively , the computational closeness means that @xmath25 and @xmath26 are indistinguishable at precision level @xmath30 by any machine in class @xmath265 .",
    "( quantum circuits ) circuit @xmath64 in definitions [ def : oc ] and [ def : soc ] can be replaced by a `` quantum '' circuit @xcite . in this variation , we assume that the source of a distribution ( observed data ) is principally given by a quantum phenomenon .",
    "+ there are several variations of the quantum complexity definition , typically : ( 1 ) all inputs and outputs of @xmath64 are classical strings , ( 2 ) only state @xmath267 is a quantum string and the others are classical , and ( 3 ) all inputs and outputs of @xmath64 except output @xmath268 are quantum strings .",
    "( interactions ) if an observation object actively reacts similar to a living thing , we often observe it in an interactive manner .",
    "+ so far in this paper we have assumed that an object is a distribution that we perceive passively .",
    "we can extend the object from such a passive one to an active one with interactive observation .",
    "suppose that the observation process is interactive between observer @xmath20 and observation object @xmath111 .",
    "for example , @xmath20 first sends @xmath269 to @xmath111 , which replies @xmath270 to @xmath20 , and we continue the interactive process , @xmath271 , @xmath272 , ... , @xmath273 , @xmath274",
    ". + let @xmath275 and @xmath276 be distributions .",
    "we then define the conditional organized complexity of @xmath25 under @xmath200 with precision level @xmath30 , @xmath277 , which can be defined as the shortest ( finite version of ) conditional oc - circuit to simulate @xmath25 ( with precision level @xmath30 ) under @xmath200 ( see definition [ def : cond - occ ] for the conditional oc - circuit ) .",
    "+ the extended notion of the organized complexity of interactive object @xmath278 can be defined by @xmath277 .    in light of the criteria described in section [ sec : criteria ] , the proposed complexity measure has the following properties .    1 .",
    "the proposed complexity definition covers probability distributions and deterministic strings ( as special cases of distributions ) in a unified manner ( good ) .",
    "+ for example , for any deterministic string ( as a special case of distribution ) , there exists an oc - circuit with circuit @xmath64 to simulate the deterministic string by @xmath279 , such that @xmath280 where @xmath281 , i.e. , @xmath282 for @xmath149 .",
    "simple ( or very regular ) objects have low complexity ( good ) .",
    "+ for example , in a very regular case ( ` @xmath283 ' @xmath284 ) , the object can be simulated by an oc - circuit @xmath285 with @xmath286 such that @xmath287 where @xmath288 , @xmath289 for @xmath290 ( i.e. , @xmath291 ) , @xmath292 ( i.e. , @xmath293 ) and @xmath294 . that is , input size @xmath295 and output size @xmath296 , i.e. , @xmath64 has 2 gates that are input gates labeled by @xmath297 and that are also output gates .",
    "@xmath298 , where @xmath299 is the 2-dimensional identity matrix . hence , @xmath300 , and @xmath301`@xmath302'@xmath303 , where @xmath304 is a small constant .",
    "simply random objects have low complexity ( good ) .",
    "+ for example , in the case of uniformly random distribution @xmath25 over @xmath5 , the object can be simulated by an oc circuit @xmath305 with @xmath306 such that @xmath307 where @xmath308 , @xmath289 for @xmath309 ( i.e. , @xmath291 ) , @xmath310 ( i.e. , @xmath311 ) , @xmath312 , @xmath313 , and @xmath294 .",
    "that is , input size @xmath295 and output size @xmath296 , i.e. , @xmath64 has 2 gates that are input gates labeled by @xmath297 and that are also output gates .",
    "hence , @xmath315 , and @xmath316 , where @xmath304 is a small constant .",
    "highly organized objects have high complexity ( good ) . +",
    "as described in remarks [ rmk : oc - circuit ] and [ rmk : statistical complexity ] , the proposed complexity definition simultaneously captures the distributional features of complexity ( similar to statistical complexity and effective measure complexity ) , computational features of complexity ( similar to logical depth ) , and descriptional features of complexity ( similar to the kolmogorov complexity and effective complexity ) .",
    "+ hence , our complexity measure does not have the drawbacks of the existing complexity measures described in section [ sec : survey ] , i.e. , our measure correctly evaluates the complexity of highly organized objects for which the existing measures miss - evaluate to be low .",
    "in addition , objects evaluated by any existing ( organized ) complexity measure to be high for some features of complexity are also evaluated to be high in our complexity measure .",
    "the proposed complexity of any object ( distributions / strings ) is computable as shown in theorems [ thm : computable ] and [ thm : computable - str ] ( good ) .",
    "as described in section [ sec : bacgroud ] , the semantic information theory has been studied for over six decades , e.g. , @xcite . among these studies , we here investigate the existing semantic information theories that offer a quantitative measure of semantic information , e.g. , @xcite .",
    "although they present many different ideas and approaches , a common paradigm among these theories assumes some a priori information , e.g. , world model , knowledge database , and logic , to measure the amount of semantic information of an object .",
    "for example , in @xcite , the following information is assumed to be established beforehand to define the semantic information measure .",
    "- a world model that is a set of interpretations for propositional logic sentences with probability distributions .    - an inference procedure for propositional logic .    - a message generator that generates messages using some coding strategy .",
    "it looks natural and inevitable to assume such a priori basic information such as the world model and logic to define the semantic information measure , but we have the same criticism for this paradigm as that for the thermodynamic depth @xcite described in section [ sec : survey ] .",
    "that is , any existing semantic information theory in literature assumes such a priori information but gives no concrete or precise specification of the assumed a priori information . without any concrete specification of the assumed information",
    ", we can not rigorously define the semantic information amount and such a quantitative definition is just a vague and obscure notion .",
    "if there are thousands of possible concrete specifications of the information , we would have thousands of possible quantitative definitions .",
    "in addition , when we try to measure the semantic information amount of an object , or we have no idea of its amount , such a priori information should be established beforehand and its complexity ( information amount ) should be comparable to or exceed that of the object .",
    "hence , if the a priori information is fixed , or the semantic information measure with this information is concretely defined , it can not measure the information amount of an object that has greater information amount than that of the a priori information .",
    "that is , any concrete definition in this paradigm can measure only a specific subset of objects , i.e. , any concrete and generic definition is impossible , or any concrete definition is ad hoc .",
    "it should be an essential problem in the existing semantic information theories .",
    "another criticism of the existing semantic information theories is that they are constructed only on some mathematical logic such as propositional and first order logics .",
    "our daily communications should be based on more complicated and fuzzy logic .",
    "it is well known that bees inform other bees of the direction and distance of flowers using their actions similar to dancing .",
    "clearly some semantic information is transferred among bees in this case , and the logic for the semantics should be much different from mathematical logic and the logic of humans .    given an object , e.g. , shakespeare s plays and bee s actions , we may roughly imagine which classes of information ( universe ) and logic are necessary or sufficient to understand the object .",
    "for example , in order to understand the semantic information of shakespeare s plays , the necessary universe and logic may be the knowledge of english sentences , the culture of that age and human daily logic . to understand the semantic information of bee communications ,",
    "a much more limited and specific type of universe and logic may be sufficient .",
    "we now have the following questions .    1 .",
    "can we quantitatively define the amount of semantic information of an object without assuming any a priori information ? or , can we quantitatively define the amount of semantic information of an object absolutely ( not relative to a priori information ) ? 2 .",
    "given an object , can we determine the minimum amount of universe and logic to understand the semantic information of the object ?      in this section , we present a mathematical theory of _ semantic _ information and communication that covers the semantic and effectiveness problems ( levels b and c of information and communication problems given by weaver , see section [ sec : bacgroud ] ) .",
    "the proposed theory is based on our organized complexity measure shown in section [ sec : proposed ] .",
    "the proposed semantic information theory offers a positive answer to the questions raised at the end of section [ sec : existing - semantic - theories ] .",
    "we first consider an example described in section [ sec : criteria ] , a source ( distribution ) from shakespeare s plays .",
    "let distribution @xmath25 over @xmath5 be the source of several plays that consists of several hundred pages of english sentences , and let @xmath317 be the shortest ( proper ) oc - circuit , @xmath164 to simulate source @xmath25 at precision level @xmath30 .",
    "the output , @xmath318 , is the distribution statistically @xmath30-close to @xmath25 , the distribution of the source of the whole sentences in the plays .",
    "the shortest oc - circuit , @xmath317 , for @xmath25 can characterize source @xmath25 such that    * the proper logic , @xmath167 of @xmath25 may capture the features of shakespeare s way of thinking and daily logic including english grammar , * the proper universe of @xmath25 , @xmath168 , may capture the knowledge of english words and expressions as well as aspects of the cultures necessary to understand the plays , * the proper semantics of @xmath25 , @xmath169 , may capture the semantics ( meanings ) of sentences of the plays .",
    "based on the observation above , we formalize the semantic information theory .    in section [ sec : proposed ]",
    ", we aim to define quantitatively the organized complexity of _ physical _ objects , i.e. , the sources of physically observed data . since any physical thing",
    "is essentially bounded finitely , we assume a source is a distribution over finite strings , @xmath5 ( @xmath6 ) , in section [ sec : proposed ] .",
    "in contrast , in this section , we aim to construct a _ mathematical _ theory of semantic information , where we focus on the _ asymptotic _ properties of an object when the size of the object is supposed to be increasing unlimitedly .",
    "this is because this theory focuses on the semantics part of the organized complexity , which consists of logic , universe , and semantics .",
    "the semantics part such as the proper semantics , @xmath169 , of object ( source ) @xmath25 in the example above can be characterized more clearly and simply using the asymptotic properties ( where the sizes of @xmath169 and @xmath25 are supposed to be increasing unlimitedly while the sizes of proper logic @xmath167 and proper universe @xmath168 are finitely bounded ) than by the finite - size properties ( where the size of @xmath169 is finitely bounded ) .",
    "note that shannon s information theory is also a theory for asymptotic properties .",
    "therefore , an object here is not a single distribution but a _ family _ of distributions , @xmath319 , where @xmath320 is a distribution over @xmath5 .",
    "we define several notions including the _ semantic information amount_. note that occam s razor also plays a key role in this definition , since it is based on organized complexity , @xmath321 .",
    "first we introduce the notion of a _ family of distributions _ and ( naturally ) extend the concept of an oc - circuit ( for a distribution on finite - size strings ) into that of an _ oc - circuit for a family of distributions _ , which is the same as the original one except that its output and semantics are unbounded in this concept , while they are bounded in the original .",
    "( family of distributions and oc - circuit for a family of distributions ) [ def : dist - family ]    let @xmath320 be a distribution such that @xmath322 , and @xmath323 be a `` family of distributions . ''",
    "let @xmath324 be an `` oc - circuit for a family of distributions '' such that @xmath325 where @xmath123 , @xmath117 , @xmath116 , @xmath326 , @xmath327 , @xmath328 and @xmath90 is the set of infinite - size binary strings .",
    "let @xmath329 , where @xmath330 .",
    "[ def : seq - fd ] ( sequential family of distributions )    a family of distributions , @xmath331 , is called a `` sequential family of distributions '' if for any @xmath332 and @xmath4 in @xmath1 with @xmath333 , distribution @xmath334 over @xmath5 is the @xmath4-bit restriction of distribution @xmath335 over @xmath336 ( see section [ sec : notation ] for the definition of @xmath4-bit restriction ) .",
    "the family of distributions output by an oc - circuit for a family of distributions , @xmath331 , is a sequential family of distributions .",
    "a value of @xmath337  ( @xmath338 for @xmath339 ) corresponds to a value in @xmath340 \\subset { \\mathbb{r}}$ ] by map @xmath341 $ ] ,   @xmath342 ",
    "@xmath343 $ ] , where @xmath344  is the binary expression of a value in @xmath340 $ ] .    through this correspondence ,",
    "if @xmath331 is a sequential family of distributions , @xmath345 corresponds to probability density function @xmath346 with support @xmath340 $ ] @xcite such that @xmath347 , and @xmath348 , where @xmath349 `` denotes @xmath350 '' @xmath343 $ ] , if @xmath351 .",
    "since there are a variety of unnatural or eccentric distribution families in general , we introduce a class of distribution families , _ semantic information sources _ , that are natural or appropriate as the object of the semantic information theory .",
    "although a family of distributions covers an unbounded number of distributions , the core mechanism , e.g. , logic and universe , of a source to produce a family of distributions should be _ bounded _ due to the physical constraints .",
    "in other words , such a natural family of distributions should be actualized as an unbounded series of distributions produced by a physically bounded mechanism , e.g. , logic and universe , along with an unbounded series of inputs , e.g. , semantics .",
    "since oc - circuits are sufficiently general to express any distribution ( as shown in theorem [ thm : computable ] ) , a natural and appropriate object in the semantic information theory should be expressed by an oc - circuit for a family of distributions , which is defined in definition [ def : dist - family ] .",
    "we have another condition for an appropriate object or its oc - circuit .",
    "roughly , the shortest oc - circuit for simulating an appropriate object should be converging asymptotically , since we aim to characterize an object by the asymptotic properties in our theory .",
    "then , the shortest expression ( logic , universe , and semantics ) of an oc - circuit simulating a _ family of distributions _ , @xmath352 , should be equivalent to the shortest one for _ each distribution _ @xmath320 asymptotically ( for sufficiently large @xmath4 ) .",
    "namely , the _ global _ shortest expression ( the proper logic , universe , and semantics of @xmath353 ) should be equivalent to the _ local _ shortest expression ( the proper logic , universe , and semantics of @xmath320 ) asymptotically .",
    "( semantic information source ) [ def : sis ]    a family of distributions , @xmath323 , is called a `` semantic information source '' at precision level @xmath354 if there exists an oc - circuit for a family of distributions , @xmath355 , where @xmath356 , and @xmath357 is the ( @xmath358-bit ) prefix of @xmath359 for @xmath4-bit output , that satisfies the following conditions .",
    "* for all @xmath6 ,   @xmath360 , and * there exists @xmath361 such that for all @xmath362 , @xmath363    if there are multiple oc - circuits , @xmath364 , that satisfy the above conditions , the lexicographically first one is selected as @xmath364 for @xmath353 .",
    "then , @xmath364 , @xmath365 , @xmath366 , and @xmath367 are called the `` proper oc - circuit , '' `` proper logic , '' `` proper universe , '' and `` proper semantics '' of @xmath353 at precision level @xmath354 , respectively . here , @xmath368 .",
    "for two semantic information sources , @xmath319 and @xmath369 , we say @xmath353 and @xmath370 are `` semantically equivalent '' at precision level @xmath354 iff they have the same proper oc - circuit @xmath146 at level @xmath354 .",
    "we denote this by @xmath371 .",
    "[ rmk : sis ] from the definition , for sufficiently large @xmath4 ( @xmath372 ) , @xmath373 where @xmath374 is the proper oc - circuit of @xmath320 at precision level @xmath375 ( see definition [ def : oc ] for the proper oc - circuit ) .",
    "the left term in eq .",
    "( [ eq : sis-2 ] ) is fixed by @xmath353 , while the right term varies with each @xmath320 .",
    "this definition says that , in semantic information source @xmath376 , @xmath320 for all sufficiently large @xmath4 is uniformly characterized by a single oc - circuit @xmath377 proper for @xmath353 .",
    "semantic information source @xmath376 is characterized by its proper oc - circuit @xmath378 , where @xmath379 with @xmath380 . since @xmath359 includes an infinite number of strings in @xmath381 , any value in @xmath381 could be @xmath382 for some @xmath383 , or the value of @xmath382 for @xmath383 could be any value in @xmath381 .",
    "hence , if @xmath378 is the proper oc - circuit of semantic information source @xmath353 , an oc - circuit @xmath384 with any other @xmath385 could be the proper oc - circuit of a semantic information source with the proper semantics @xmath385 .",
    "therefore , a semantic information source is characterized by its proper logic @xmath386 along with universe @xmath151 in the universe space @xmath387 and semantics @xmath385 in the semantics space @xmath388 .",
    "namely , the @xmath389 bit output , @xmath268 , should have @xmath390 bit semantic information , i.e. , as @xmath4 bit output should have @xmath391 ( or its rounded - up integer , @xmath392 ) bit semantic information .",
    "( semantic information amount and semantic information space ) [ def : sa ]    let @xmath323 be a semantic information source whose proper logic at precision level @xmath354 is @xmath393 .",
    "the `` semantic information amount , '' @xmath394 , of @xmath395 at precision level @xmath354 is defined by @xmath396    let @xmath388 be the `` semantic information ( meaning ) space '' of proper logic @xmath397 , and @xmath398 be  the ( @xmath399-bit ) prefix of @xmath400 for an @xmath4-bit output .",
    "    let @xmath387 be the `` universe space '' of proper logic @xmath150 .",
    "* examples of semantic information sources * [ par : example - sis ]    here we show some examples of semantic information sources .    1 .",
    "( example 1 ) + let us employ an example of shakespeare s plays again , and imaginarily suppose that there are an unbounded number of shakespeare s plays , but that the logic and universe ( knowledge ) of shakespeare are bounded .",
    "+ let @xmath376 be a sequential family of distributions of shakespeare s ( unbounded number of ) plays .",
    "+ given @xmath401 with @xmath402 , we compute an oc - circuit @xmath403 such that @xmath404 is the shortest ( proper ) oc - circuit to simulate @xmath401 at precision level @xmath30 , i.e. , @xmath405 + next , for some @xmath406 , we compute the shortest ( proper ) oc - circuit @xmath407 to simulate @xmath408 at precision level @xmath30 .",
    "+ if for any @xmath406   @xmath409 where @xmath410 is the @xmath411-prefix of @xmath412 , it could imply that @xmath413 , @xmath414 and @xmath415 are the proper logic , universe , and semantics of @xmath353 , respectively .",
    "+ if for some @xmath406 @xmath416 , let @xmath417 as a candidate of the proper oc - circuit of @xmath353 ( up to the size of @xmath418 ) . + we repeat the procedure for @xmath419 ( @xmath420 ) and update @xmath421 .",
    "+ if @xmath422 for some @xmath383 , it should hold that latexmath:[$|(\\overline{c^{(i*1)}},u^{(i+1)})| >     ( knowledge ) to understand the plays should increase as the amount of plays increases .",
    "+ in the updating process of @xmath421 , the @xmath114-th semantics part , @xmath424 , with @xmath425 has some redundancy in light of a longer ( more global ) context with @xmath426 and such redundancy could be eliminated in @xmath427 and absorbed into the @xmath428-th logic and universe , @xmath429 ( for a longer context ) , i.e. , the logic and universe part should increase in the process , while the semantics part becomes more compressed ( shorter ) and closer to a uniform one .",
    "in addition , block size @xmath430 of the output becomes longer , where a longer block with a longer context is processed using more complicated logic and a larger universe .",
    "+ the logic and universe part , @xmath431 , of oc - circuit @xmath432 should be finitely bounded for any @xmath383 .",
    "actually , @xmath433 should be the proper logic and universe of @xmath353 .",
    "+ hence , there exists @xmath434 such that for any @xmath435 ( @xmath436 @xmath437 , i.e. , there exists @xmath434 and @xmath438 @xmath439 such that for any @xmath435 ( @xmath436   @xmath440 .",
    "+ thus , @xmath353 is a semantic information source and @xmath441 is the proper oc - circuit of @xmath353 .",
    "( example 2 ) + let @xmath146 be an oc - circuit that outputs a sequential family of distributions , @xmath442 , such that @xmath443 and @xmath444 , @xmath445 . + for any precision level @xmath30 , for sufficiently large @xmath446 , we can compute @xmath447 which is the shortest ( proper ) oc - circuit to simulate @xmath448 at precision level @xmath30 .",
    "+ then , there exists @xmath449 with high probability such that @xmath450 , and for any @xmath451 @xmath452 is the shortest oc - circuit of @xmath320 at precision level @xmath30 , i.e. , @xmath453 is the proper oc - circuit of @xmath353 at precision level @xmath30 . + this is because @xmath445 and no more data compression on @xmath238 is possible for any sufficiently large @xmath451 with high probability . + that is",
    ", @xmath353 is a semantic information source with high probability . + the difference between this example and the first example is that the unbounded semantics sequence , @xmath385 , in this example is uniformly selected from the beginning , while , in the first example , the semantics sequence is gradually compressed as size @xmath4 of distribution @xmath320 becomes longer in the process of updating @xmath454 .",
    "3 .   ( other examples )",
    "+ the information sources modeled in the previous semantic information theories in literature ( in section [ sec : existing - semantic - theories ] ) are considered to be `` semantic information sources . ''",
    "+ for example in @xcite , fig .",
    "2 shows a model of semantic information communication . here ,",
    "@xmath455 ( inference procedure ) and the syntax and logic part of @xmath456 ( message generator ) can be considered as _ circuit _ @xmath64 of the oc - circuit and @xmath457 ( world model ) , @xmath458 ( background knowledge ) can be considered as _ universe _",
    "@xmath151 of the oc - circuit , and the semantics of @xmath456 ( message generator ) can be considered as semantics @xmath152 of the oc - circuit .",
    "that is , the messages from sender @xmath33 in fig .",
    "2 can be modeled as a source generated by an oc - circuit , or a semantic information source .",
    "we then consider the following problem .",
    "given semantic information source @xmath353 with @xmath354 , can we compute its proper oc - circuit and the related information ?",
    "the answer is no , since @xmath353 consists of an infinite number of distributions and it can not be described finitely .",
    "the following theorem however , shows that , given @xmath395 and @xmath354 with a sufficiently large @xmath4 , we can compute the proper oc - circuit of @xmath353 .",
    "[ thm : comp - sis ]    for any semantic information source @xmath353 at precision level @xmath459 , given @xmath395 and @xmath375 for some @xmath460 , where @xmath461 is given in definition [ def : sis ] , the proper oc - circuit ( proper logic , proper universe , and @xmath4-prefix of proper semantics ) of @xmath353 at precision level @xmath354 can be computed .",
    "the proof of this theorem is essentially the same as that for theorem [ thm : computable ] .    in our semantic information theory ,",
    "the concepts of `` proper oc - circuit , '' `` proper logic , '' `` proper universe , '' and `` proper semantics '' introduced in definition [ def : sis ] and the computability shown in theorem [ thm : comp - sis ] represent a positive answer to the questions raised at the end of section [ sec : existing - semantic - theories ] .",
    "we then introduce the concept of _ conditional oc - circuit _ , _ conditional semantic information source _ and _ conditional semantic information amount _ , which play central roles in our theory , especially in the semantic channel coding theorem ( section [ sec : s - channel - coding ] ) , the effectiveness coding theorem ( section [ sec : effectiveness ] ) and in the semantic source coding theorem ( section [ sec : s - source - coding ] ) .",
    "[ def : cond - occ ] ( conditional oc - circuits )    let @xmath462 be a sequential family of distributions .",
    "a `` conditional oc - circuit for a family of distributions under @xmath463 '' is @xmath464 , where @xmath465 where @xmath466 , @xmath467 , @xmath468 , @xmath469 , @xmath470 , and @xmath471 .",
    "the probability on @xmath370 is taken over the randomness of @xmath463 and @xmath472 ,    given a sample of sequential family of distributions @xmath463 , conditional oc - circuit @xmath473 divides the sample of @xmath463 into @xmath474-bit strings , @xmath475 , where the size , @xmath474 , is also determined by @xmath473 .",
    "[ def : csia ] ( conditional semantic information source and conditional semantic information amount )    let @xmath463 be a sequential family of distributions . a family of distributions , @xmath323 , is called a `` conditional semantic information source under @xmath463 '' at precision level @xmath354 if there exists an conditional oc - circuit for a family of distributions under @xmath463 , @xmath476 , where @xmath477 and @xmath478 is the ( @xmath479-bit ) prefix of @xmath480 for @xmath4-bit output , that satisfies the following conditions .    * for all @xmath6 ,   @xmath481 with any sampled value of @xmath482 , and * there exists @xmath461 such that for all @xmath460 , @xmath483    if there are multiple conditional oc - circuits , @xmath484 , that satisfy the above conditions , the lexicographically first one is selected as @xmath484 .",
    "then , the `` conditional semantic information amount , '' @xmath394 , of @xmath395 under @xmath463 at precision level @xmath375 is @xmath485 where @xmath486 @xmath487 @xmath488    eq.([eq : csia ] ) can be written as below in a manner similar to that for eq.([eq : sis-2 ] ) shown in remark [ rmk : sis ] .",
    "for sufficiently large @xmath4 , @xmath489 is the shortest one in @xmath490 .",
    "based on the conditional semantic information amount , we next introduce the concept of the _ semantic mutual information amount _ , which is employed in the _ effectiveness _ to be shown in section [ sec : effectiveness ] .",
    "( semantic mutual information amount )    let @xmath462 be a sequential family of distributions and @xmath376 be a semantic information source and a conditional semantic information source under @xmath463 .",
    "the `` semantic mutual information amount '' of distribution @xmath395 with @xmath463 , @xmath491 , is defined by @xmath492    semantic mutual information amount @xmath491 means the semantic information amount in @xmath463 with respect to @xmath320 .",
    "more precisely , it means the semantic information amount in @xmath493 with respect to @xmath320 , where @xmath494 .",
    "in contrast to the mutual information amount in the shannon information theory , the semantic mutual information amount is not symmetric , i.e. , @xmath491 is not always equivalent to @xmath495 for some @xmath496 , since even if some semantic information @xmath25 is useful for @xmath200 , @xmath200 may not be so useful for @xmath25 , e.g. , quantum physics is often useful to understand chemical phenomena but the converse is not always true .",
    "based on the notion of ( conditional ) semantic information sources , we next develop a semantic information theory for level b ( semantic ) problem as described by weaver @xcite .",
    "our theory answers two fundamental questions in semantic information theory : what is the ultimate semantic data compression , or ultimate data compression with preserving semantics , and what is the ultimate transmission rate of semantic data communication . the first question is answered by theorem [ thm : ssc ] , _ semantic source coding theorem _ , in this section , and the second question is answered by theorems [ thm : scc1 ] and [ thm : scc2 ] , _ semantic channel coding theorem _ , in section [ sec : s - channel - coding ] .    to begin with ,",
    "we introduce the notion of semantic data compression in the following definition .",
    "[ def : ssc ] ( semantic source coding system )    let @xmath376 be a semantic information source .",
    "semantic source coding system @xmath497 consists of sender @xmath498 , which outputs a sequential family of distributions , @xmath463 , on @xmath353 , and receiver @xmath499 , which is a conditional oc - circuit @xmath500 under @xmath463 without semantics input ( @xmath501 ) to output a family of distributions @xmath370 , where @xmath502 , and @xmath503 .    for parameter @xmath6 ,",
    "sender @xmath498 outputs @xmath493 on @xmath395 , which is directly input to receiver @xmath499 , and @xmath499 outputs @xmath504 , where @xmath505 , @xmath506 is a distribution over @xmath507 , and @xmath508 is a distribution over @xmath5 .    [",
    "cols=\"^,^,^,^,^,^ \" , ]     we say that @xmath509 correctly codes at precision level @xmath354 if there exists @xmath361 such that for all @xmath362 ,   @xmath510 .",
    "[ def : ec - er ] ( effectiveness capacity and effectiveness rate )    let @xmath511 be a `` effectiveness system '' of effectiveness coding system @xmath512 . given @xmath513 , @xmath498 in @xmath514 computes @xmath515 , i.e. , @xmath516 , where @xmath238 is the @xmath517-bit prefix of @xmath385 for @xmath4-bit output .",
    "then , @xmath518 is input to @xmath519 in @xmath514 , and @xmath520 outputs @xmath521 .    if for any @xmath513 , @xmath516 is a conditional semantic information source under @xmath463 in effectiveness system @xmath514 , we call @xmath514 `` _ normal _ ''",
    ".    let @xmath522 be a sequential family of distributions over @xmath400 , where @xmath523 is a distribution over @xmath524 , @xmath525 , i.e. , @xmath526 , ( see definition [ def : seq - fd ] for the sequential family of distributions ) .    when effectiveness system @xmath514 is normal , `` effectiveness capacity '' @xmath527 of @xmath514 for @xmath6 ( say @xmath528 ) is @xmath529 where @xmath530 is the class of the @xmath517-bit prefix of sequential families of distributions , i.e. , @xmath531 is the @xmath517-bit prefix of a sequential family of distributions , @xmath532 is the shannon entropy and @xmath533 is the expectation value over the distribution of @xmath534 .    `` effectiveness rate '' @xmath535 of effectiveness coding @xmath536 @xmath537 in effectiveness coding system @xmath538 @xmath539 for @xmath6 is @xmath540    we say a effectiveness coding system , @xmath541 , is `` normal , '' if the effectiveness system , @xmath514 , of @xmath541 is normal .",
    "we can also define the `` uniformity '' of normal effectiveness coding system @xmath541 in the same manner as that for the normal semantic channel coding system described in definition [ def : usc ] .",
    "[ thm : ec ] ( effectiveness coding theorem )    there exists a uniform effectiveness coding system , @xmath541 , that correctly codes at precision level @xmath354 and for any @xmath542 , there exits @xmath543 such that for all @xmath460 ,   @xmath544    there exists no normal effectiveness coding system , @xmath541 , that correctly codes at precision level @xmath354 ( with a negligible error probability ) and for any @xmath542 , there exits @xmath543 such that for all @xmath460 ,   @xmath545",
    "approximately seven decades have passed since warren weaver published his two insightful and prescient articles @xcite that clearly indicated two research directions in science , organized complexity and semantic information theory .",
    "although the articles stimulated and encouraged these research areas , it is hard to say that these areas have been well established in science , and weaver would be disappointed to know it .    moreover , he might be disappointed to learn that no study has been done on the relation and integration of these areas , since he could have realized the relationship between the areas considering that these articles were written at almost the same time .",
    "the aim of this paper is to pursue the research directions that weaver indicated .",
    "this paper first quantitatively defined the organized complexity .",
    "the proposed definition for the first time simultaneously captures the three major features of organized complexity and satisfies all of the criteria for organized complexity measures introduced in this paper .",
    "we then applied the organized complexity measure to develop our semantic information theory , where we presented the first formal definition of a semantic information amount that is based only on concretely defined notions , and unveil several fundamental properties in the semantic information theory . through this organized complexity measure",
    ", we offered a unified paradigm of organized complexity and semantic information theory .",
    "organized complexity is an interdisciplinary concept straddling physics , cosmology , biology , ecology , sociology , and informatics .",
    "thus , the proposed organized complexity measure could be a core notion in such interdisciplinary areas , and for example , offer some basis for tackling the problems posed in @xcite .",
    "h. akaike .",
    "information theory and an extension of the maximum likelihood principle , proceedings of the 2nd international symposium on information theory , petrov , b. n. , and caski , f .",
    "( eds . ) , akadimiai kiado , budapest : 267 - 281 ( 1973 ) .",
    "r. carnap , and y. bar - hillel .",
    "an outline of a theory of semantic information .",
    "rle technical reports 247 , research laboratory of electronics , massachusetts institute of technology , cambridge ma , oct ( 1952 ) .",
    "g. sommagura , ed .",
    "formal theories of information : from shannon to semantic information theory and general concepts of information [ muenchenwiler seminar ( switzerland ) , may 2009 ] , vol .",
    "5363 of lecture notes in computer science , springer ( 2009 ) ."
  ],
  "abstract_text": [
    "<S> one of the most fundamental problems in science is to define _ quantitatively _ the complexity of organized matters , i.e. , _ </S>",
    "<S> organized complexity_. although many measures have been proposed toward this aim in previous decades , there is no agreed upon definition . </S>",
    "<S> this paper presents a new quantitative definition of organized complexity . </S>",
    "<S> in contrast to existing measures such as the kolmogorov complexity , logical depth , effective complexity , and statistical complexity , this new definition _ simultaneously _ captures the three major features of complexity : computational ( similar to logical depth ) , descriptional ( similar to the kolmogorov complexity and effective complexity ) and distributional ( similar to statistical complexity ) . </S>",
    "<S> in addition , the proposed definition is computable and can measure both probabilistic and deterministic forms of objects in a unified manner . </S>",
    "<S> the proposed definition is based on circuits rather than turing machines and @xmath0-machines . </S>",
    "<S> we give several criteria required for organized complexity measures and show that the proposed definition satisfies all of them for the first time .    </S>",
    "<S> we then apply this quantitative definition to formulate a _ </S>",
    "<S> semantic information theory_. we present the first formal definition of a _ semantic information amount _ , which is the core concept of the semantic information theory , that is based only on concretely defined notions . </S>",
    "<S> previous semantic information theories defined this amount under some a priori information which is not concretely specified . </S>",
    "<S> we then unveil several fundamental properties in the semantic information theory , e.g. , a semantic source coding theorem , semantic channel coding theorem , and effectiveness coding theorem . </S>",
    "<S> although the semantic information theory has a long history of research going back more than six decades , there has been no study on its relation to organized complexity . </S>",
    "<S> this paper offers the first unified paradigm of organized complexity and semantic information theory . </S>"
  ]
}