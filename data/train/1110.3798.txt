{
  "article_text": [
    "deeply virtual compton scattering ( dvcs ) is recognized as the theoretically cleanest process for accessing generalized parton distributions ( gpds ) @xcite , which describe the three - dimensional structure of nucleon in terms of partonic degrees of freedom .",
    "determination of gpds , beside improving our general understanding of qcd dynamics , allows to address important questions such as the partonic decomposition of the nucleon spin @xcite and characterization of multiple - hard reactions in proton - proton collisions at lhc collider @xcite . concerning the latter , gpd - describable non - trivial transversal structure of proton , such as the correlation between parton s longitudinal momentum fraction and its transversal distance is already finding its way in popular event generators , such as pythia @xcite .",
    "similarly to extraction of normal parton distribution functions ( pdfs ) , extraction of gpds can be performed by global model or local fits to available data @xcite .",
    "however , compared to global pdf fits , extracting gpds from data is a much more intricate task and model ambiguities are much larger . due to the facts that gpds can not be fully constrained from data and that they depend at the input scale on three variables , the space of possible functions , although restricted by gpd constraints , is huge . as a result ,",
    "the theoretical systematic error induced by the choice of the fitting model is much more serious than in the pdf case , where the model functions depend at the input scale only on one variable , namely , the longitudinal momentum fraction @xmath1 .    here",
    "we report on some results obtained using alternative approach @xcite , in which _ neural networks _",
    "are used in place of specific models .",
    "this essentially eliminates the problem of model dependence and , as an additional advantage , facilitates a convenient method to propagate uncertainties from experimental measurements to the final result .",
    "our approach is mostly similar to the one already employed for @xmath2 structure function and pdf extraction @xcite and will be shortly described in the next section . to reduce the mathematical complexity of the problem",
    "we have fitted not the gpds itself , but the dominant compton form factor ( cff ) @xmath3 , depending on bjorken variable @xmath4 and momentum transfer @xmath5 . at leading order ,",
    "the imaginary part of this cff is related to the corresponding gpd @xmath6 at the cross - over line @xmath7 : @xmath8 so knowledge of cff @xmath0 provides us with direct information about the proton structure .",
    "each blob symbolizes a neuron and thickness of arrows represents the strengths of weights @xmath9 . ]    the neural network type used in this work , known as _ multilayer perceptron _ , is a mathematical structure consisting of a number of interconnected `` neurons '' organized in several layers .",
    "it is schematically shown on figure  [ fig : perceptron ] , where each blob symbolizes a single neuron .",
    "each neuron has several inputs and one output .",
    "the value at the output is given as a function @xmath10 of a sum of values at inputs @xmath11 , each weighted by a certain number @xmath9 . for _ activation function _ @xmath12 we employed logistic sigmoid function @xmath13 for neurons in inner ( `` hidden '' ) layer , while for input and output layers we used the identity function .    by iterating over the following steps",
    "the network is trained , _",
    "i.e. _ , it `` learns '' how to describe a certain set of data points :    1 .",
    "kinematic values ( two in our case : @xmath4 and @xmath5 ) of the first data point are presented to two input - layer neurons 2 .",
    "these values are then propagated through the network according to values of weights @xmath9 . in the first iteration , weights are set to some random value .",
    "3 .   as a result , the network produces some resulting values of output in its output - layer neurons .",
    "here we have two : @xmath14 and @xmath15 . obviously , after the first iteration , these will be some random functions of the input kinematic values : @xmath16 and @xmath17 .",
    "4 .   using these values of cff(s ) , the observable corresponding to the first data point is calculated and it is compared to actually measured value , with squared error used for building the standard @xmath18 function .",
    "the obtained error is then used to modify the network : it is propagated backwards through the layers of the network and each weight is adjusted such that this error is decreased .",
    "this procedure is then repeated with the next data point , until the whole training set is exhausted .",
    "this sequence of steps is repeated until the network is capable to describe experimental data with a sufficient accuracy .",
    "to guard against overfitting the data ( `` fitting to the noise '' ) , one ( randomly chosen ) subset of data is not used for training but only for monitoring the progress and stopping the training when error of network description of this data starts to increase significantly .",
    "this ensures that resulting neural network represents a function which is not too complex and which is thus expected to provide a reasonable estimate of the actual underlying physical law .    to propagate experimental uncertainties into the final result we used the `` monte carlo '' method @xcite where neural networks are not trained on actual data but on a collection of `` replica data sets '' .",
    "these sets are obtained from original data by generating random artificial data points according to gaussian probability distribution with a width defined by the error bar of experimental measurements . taking a large number @xmath19 of such replicas",
    ", the resulting collection of trained neural networks @xmath20 defines a probability distribution @xmath21 $ ] of the represented cff @xmath3 and of any functional @xmath22 $ ] thereof .",
    "thus , the mean value of such a functional and its variance are @xcite @xmath23 \\big\\rangle & =   \\int \\mathcal{d}\\mathcal{h }   \\ : \\mathcal{p}[\\mathcal{h } ] \\ , \\mathcal{f}[\\mathcal{h } ] =    \\frac{1}{n_{rep}}\\sum_{k=1}^{n_{rep } } \\mathcal{f}[\\mathcal{h}^{(k)}]\\ ; , \\label{eq : funcprob } \\\\ \\big(\\delta \\mathcal{f}[\\mathcal{h}]\\big)^2 & = \\big\\langle \\mathcal{f}[\\mathcal{h}]^2 \\big\\rangle   - \\big\\langle \\mathcal{f}[\\mathcal{h } ] \\big\\rangle^2 \\;. \\label{eq : variance}\\end{aligned}\\ ] ] more details about our procedure can be found in @xcite .",
    "we now present neural network fits @xcite to two sets of hermes collaboration measurements @xcite of leptoproduction of a real photon by scattering leptons off unpolarized protons ( of which dvcs is a subprocess ) .",
    "one set consists of 18 measurements of the first sine harmonic @xmath24 of the beam spin asymmetry ( bsa ) @xmath25 ( where @xmath26 is the azimuthal angle in the so - called trento convention ) , while in the other set there are 18 measurements of the first cosine harmonic @xmath27 of the beam charge asymmetry ( bca ) @xmath28 both sets cover the identical kinematic region @xmath29 and in this region bsa and bca are determined essentially by the imaginary and the real part of the compton form factor @xmath0 @xcite , respectively , so we ignored other cffs .",
    "furthermore , in this region the dependence of @xmath0 on the photon virtuality @xmath30 is weak and , therefore , we neglected it for simplicity .",
    "thus , at present , just a single cff @xmath3 , or two real - valued functions @xmath16 and @xmath17 , are extracted from data by neural networks .",
    "( [ eq : bca ] ) and first sine harmonic of beam spin asymmetry @xmath24 ( [ eq : bsa ] ) resulting from neural network fit , shown together with data @xcite , used for training . [",
    "fig : hermes09 ] ]    we fitted this data using the method described in the previous section where we constructed 50 neural networks with architecture ( 2 - 13 - 2 ) , _ i.e. _ , with two input neurons ( for two kinematic variables @xmath4 and @xmath5 ) , 13 neurons in the hidden layer ( where we convinced ourselves that adding or removing few neurons does nt significantly change the results ) , and 2 output neurons ( representing @xmath16 and @xmath17 ) . on figure",
    "[ fig : hermes09 ] we show the fit quality by presenting the data used for training together with the description of this data by the final set of 50 neural networks , using relations ( [ eq : funcprob ] ) and ( [ eq : variance ] ) .     and",
    "@xmath31 from hermes bca and bsa  @xcite data .",
    "actual data region is shown as vertical band in the middle of the left two panels .",
    "outside of this band and on the whole of the right two panels , neural networks are _ extrapolating _ from the data . [",
    "fig : cff ] ]    cff @xmath0 itself , which is our main result , is displayed in figure  [ fig : cff ] , where @xmath14 ( upper panels ) and @xmath15 ( lower panels ) are separately plotted .",
    "one notices that in the kinematic region of the measured data ( this is roughly the middle vertical third of the left panels ) , where neural networks are _ interpolating _ the data , cff @xmath0 is estimated with a reasonably small uncertainty .",
    "however , as one starts to _ extrapolate _ the fitted cff @xmath0 outside of the data region ( left and right thirds of left panels and the whole of the right panels ) , the neural network parameterization of cff @xmath0 is very unconstrained .",
    "this is particularly visible in the right panels , illustrating the difficulty of a model - independent extrapolation towards @xmath32 , which is a limit of particular interest for hadron structure studies .    ) in compass ii kinematics ( @xmath33 , @xmath34 ) . ]    finally , as an example of a proper prediction coming from our analysis we plot in figure  [ fig : compass ] the beam charge - spin asymmetry ( bcsa ) , @xmath35 as a function of momentum transfer @xmath5 , for several kinematic points that are characteristic for the compass ii experiment of scattering muons and antimuons off proton target ( where the muon is taken to be massless and the polarization is set equal to 0.8 ) .",
    "this experiment was chosen because its kinematics overlap with that of the hermes data used for neural network training .",
    "hence these predictions represent partly interpolation and partly extrapolation of hermes data , thus testing this whole approach in a nontrivial way .",
    "by explicit extraction of compton form factor @xmath0 from hermes data on beam spin and charge asymmetries we demonstrated that neural networks can be a powerful tool for studying hadron structure .",
    "they can interpolate experimental data in an unbiased way , eliminating thus the systematic error introduced by choosing a specific fitting function in the standard model - fitting approaches .",
    "since gpds and cffs are multivariate functions , this advantage is much more pronounced than in pdf fitting , where pdfs on the input scale depend only on a single variable .",
    "still , this feature of neural network approach cuts both ways : unbiased fitting of gpds and cffs to the precision of the present pdf fits would require orders of magnitude more data then presently available  to cover the larger dimension of the kinematic space . to overcome this problem",
    ", one may deliberately introduce some biases and constraints on neural networks , especially those that correspond to certain well established properties of represented functions ( e.g. dispersion relations between imaginary and real parts of cffs @xcite ) .",
    "furthermore , one could view neural network fits as intermediate results and use them as a tool for model - dependent studies .",
    "this work was supported by the bmbf grant under the contract no .",
    "06ry9191 , by eu fp7 grant hadronphysics2 , by dfg grant , contract no .",
    "436 kro 113/11/0 - 1 and by croatian ministry of science , education and sport , contract no .",
    "119 - 0982930 - 1016 ."
  ],
  "abstract_text": [
    "<S> neural networks are utilized to fit compton form factor @xmath0 to hermes data on deeply virtual compton scattering off unpolarized protons . </S>",
    "<S> we used this result to predict the beam charge - spin assymetry for muon scattering off proton at the kinematics of the compass ii experiment . </S>"
  ]
}