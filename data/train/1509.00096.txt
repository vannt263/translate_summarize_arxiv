{
  "article_text": [
    "the solution of the numerically ill - posed linear system of equations @xmath0 is considered .",
    "we suppose that the matrix @xmath1 has large dimension , may be over or underdetermined , @xmath2 , or @xmath3 , resp . and is severely ill - conditioned ; the singular values of @xmath1 decay exponentially to zero , or to the limits of the numerical precision .",
    "vectors @xmath4 and @xmath5 describe the data and model parameters respectively .",
    "noise in the data is represented by @xmath6 , i.e. @xmath7 for exact but unknown data @xmath8 .",
    "we assume components of @xmath9 are independently sampled from a gaussian distribution and have known diagonal covariance matrix , denoted by @xmath10 . given @xmath1 and @xmath4",
    "the aim is to compute an approximate solution for @xmath5 .",
    "discrete ill - posed problems of the form   may be obtained by discretizing linear ill - posed problems such as fredholm integral equations of the first kind and arise in many research areas including image deblurring , geophysics , etc .",
    "the presence of the noise in the data and the ill - conditioning of @xmath1 means that regularization is needed in order to compute an approximate solution for .",
    "probably the most well - known method is that of tikhonov regularization in which @xmath5 is estimated as @xmath11 here @xmath12 is the weighted data fidelity term and @xmath13 is a regularization term .",
    "the product @xmath14 predicts the data , @xmath15 is a regularization matrix and @xmath16 allows specification of a given reference vector of _ a priori _ information for the model @xmath5 . in @xmath17 is an unknown regularization parameter which trades - off between the data fidelity and regularization terms .",
    "the noise in the measurements @xmath18 is whitened when @xmath19 where @xmath20 approximates the standard deviation of the noise @xmath21 in the @xmath22th datum . introducing @xmath23 , @xmath24 , and shifting by the prior information through @xmath25 , yields @xmath26 under the assumption that the null spaces of @xmath27 and @xmath15 do not intersect @xmath28 is explicitly dependent on @xmath17 and is given by @xmath29",
    "it is well - known that when the matrix @xmath15 is invertible , the alternative but equivalent formulation uses @xmath30 yielding , with right preconditioned matrix @xmath31 and regularized inverse @xmath32 , @xmath33 although equivalent analytically , numerical techniques to solve and differ . for small scale problems , for example , we may solve using the generalized singular value decomposition ( gsvd ) , e.g. @xcite , for the matrix pair @xmath34 $ ] , but would use the singular value decomposition ( svd ) of @xmath35 for , e.g. @xcite , as given in [ svd ] .",
    "the solutions depend on the stability of these underlying decompositions , as well as the feasibility of calculating @xmath36 .",
    "still , the use of the svd or gsvd is not viable computationally for large scale problems unless the underlying operators possess a specific structure .",
    "for example , if the underlying system matrix , and associated regularization matrix are expressible via kronecker decompositions , e.g. @xcite , then the gsvd decomposition can be found via the gsvd for each dimension separately .",
    "here we consider the more general situation and turn to consideration of iterative krylov methods to estimate the solutions of and .",
    "forthwith we assume for simplicity of notation and the initial discussion that we solve the system @xmath37 , equivalent to using @xmath38 , weighting of @xmath4 and @xmath1 by @xmath39 , and right preconditioning of @xmath1 by @xmath36 , dependent on the context , i.e. we solve with @xmath40 , @xmath41 and @xmath38 .",
    "specifically , we assume that the components of the error @xmath21 are independently sampled from a normal distribution with variance @xmath42 , @xmath43 .      in principle",
    "iterative methods such as conjugate gradients ( cg ) , or other krylov methods , can be employed to solve for large scale problems in the presence of noise .",
    "results presented in @xcite demonstrate , however , that minres and gmres should not be used as regularizing krylov iterations due to the early transfer of noise to the krylov basis .",
    "recently , there has also been some interest in the lsmr modification of lsqr which is an implementation based on minres , @xcite . here",
    ", because our goal is to understand how to find regularization parameters for a well - studied reduced problem , we use the well - known golub - kahan bidiagonalization ( gkb ) , also known as the lsqr iteration which has been well - studied in the context of projected solutions of the least squares problem @xcite and for which the noise regularizing properties of the iteration are better understood , @xcite .",
    "effectively the gkb projects the solution of the inverse problem to a smaller subspace , say of size @xmath44 .    applying @xmath44 steps of the gkb on matrix @xmath1 with initial vector @xmath45 , of norm @xmath46 , and defining @xmath47 to be the unit vector of length @xmath48 with a @xmath42 in the first entry , lower bidiagonal matrix @xmath49 and column orthonormal matrices @xmath50 , @xmath51",
    "are generated such that , see @xcite , @xmath52 for @xmath53 , we define the full , @xmath54 , and projected , @xmath55 , residuals via @xmath56 by the column orthonormality of @xmath57 @xmath58 theoretically , therefore , an estimate for the solution @xmath5 with respect to a reduced subspace may be found by solving the normal equations for the projected problem and then projecting back to the full problem .    by the courant fischer minimax theorem and noting @xmath59 it is immediate that the eigenvalues of @xmath60 , the ritz values , are interlaced by those of @xmath61 and are bounded above and below by the largest and smallest non zero eigenvalues of @xmath61 , ( * ? ? ?",
    "* section 5 ) .",
    "likewise , the singular values , @xmath62 , of @xmath63 interlace the singular values @xmath64 of @xmath1 , @xmath65 and the singular vectors of @xmath1 are approximated , via @xmath66 and @xmath67 , for the singular value decomposition @xmath68 , given sufficient precision , e.g. @xcite .",
    "thus , for large enough @xmath44 , dependent on the spectrum for @xmath1 , the normal equations for the projected system of equations inherit the ill - conditioning of the normal equations for and regularization is also needed for the noise - contaminated projected problem , @xcite , despite the regularizing impact of the krylov iterations .    by the column orthonormality of @xmath69",
    ", we have @xmath70 . explicitly introducing regularization parameter @xmath71 , distinct from @xmath17 in order to emphasize regularization on the projected problem , yields the projected tikhonov problem @xmath72 the solution of has two equivalent forms",
    ", it will be helpful for theoretical analysis to give both , dependent on whether derived directly for , or without factoring out @xmath57 in @xmath73 in practice , one uses to find @xmath74 via the svd for @xmath63 , under the assumption that @xmath75 , noting that an explicit solution for @xmath76 is immediately available , see e.g. [ svd ] .",
    "as already observed in @xcite , the regularized lsqr algorithm now poses the problem of both detecting the appropriate number of steps @xmath44 as well as of finding the optimal parameter @xmath77 .",
    "one method of regularization is simply to avoid the introduction of the regularizer in and find an optimal @xmath44 at which to stop the iteration , equivalently regarding the lsqr iteration itself as sufficiently regularizing .",
    "here we assume that regularization is needed , and thus with respect to the regularization , it is evident that the problem may be considered in two ways , namely regularize and project , or project and then regularize , e.g. @xcite .",
    "if the regularization is applied for each step of the reduction , the method is regarded as a hybrid of the two techniques , e.g. @xcite .",
    "the problem of first determining the appropriate size @xmath44 for the projected space is discussed in e.g. @xcite and more recently for large scale geophysical inversion in @xcite .",
    "on the other hand , for a given @xmath44 the krylov subspace for @xmath74 is the same for all @xmath71 , i.e. the krylov subspace is invariant to shifts , which is useful for determining @xmath77 , @xcite .",
    "although the solutions obtained from the regularize then project , and project then regularize , for a given @xmath44 and @xmath78 are equivalent , ( * ? ? ?",
    "* theorem 3.1 ) which also points to ( * ? ? ?",
    "* p 301 ) , this does not mean that for given @xmath44 finding @xmath77 for the subspace problem will provide @xmath79 that is optimal for the full problem , @xcite .    determining to which degree certain regularization techniques provide a good estimate for @xmath79 from the subspace problem estimate , and the conditions under which this will hold , is the topic of this work and is the reason we denote regularization parameter on the subspace by @xmath71 distinct from @xmath17 .",
    "we should note that in our discussion we explicitly assume invertibility of the regularization operator @xmath15 in order to allow the right preconditioning of @xmath1 .",
    "practically we note that in applying the gkb reduction , applications of forward operations @xmath80 should be performed via solves for the systems @xmath81 to find @xmath82 .",
    "typically @xmath15 is sparse and structured , and such solves are efficient given a potential initial factorization for @xmath15 .",
    "the gkb also requires forward operations with @xmath83 which are again efficiently implemented via solution solves with system matrix @xmath84 .",
    "here we assume that such efficient solves are possible , and do not address this aspect of algorithmic development .",
    "we also refer to recent work in @xcite in which the most often used differential operators , themselves not invertible , are replaced by invertible versions by use of suitable boundary conditions . when @xmath15 is not invertible , the projected tikhonov problem lacks the immediate preservation of the regularization norm , yielding the subspace problem @xmath85 which unfortunately requires projecting the subspace solution back to the full space via @xmath69 , which is avoided in . on the other hand",
    "the regularizer @xmath86 may be achieved , as noted in @xcite , by finding the @xmath87 factorization @xmath88 , yielding @xmath89 by the column orthogonality of @xmath90 .",
    "the formulae to immediately find @xmath74 for small @xmath44 use in this case the gsvd , see e.g. @xcite .",
    "having now set the background for our investigation , we reiterate that a main goal of this work is to theoretically analyze in which cases determining @xmath77 from the projected problem will effectively regularize the full problem . the presented analysis is independent of whether the originating problem is over or under determined . for the full problem",
    "the question of determining an optimal parameter @xmath79 is well - studied , see e.g. @xcite , for a discussion of methods including the morozow discrepancy principle ( mdp ) , the l - curve ( lc ) , generalized cross validation ( gcv ) and unbiased predictive risk estimation ( upre ) .",
    "the use of the mdp , lc and gcv is also widely discussed for the projected problem , particularly starting with the work of kilmer et al , @xcite and continued in @xcite .",
    "further , extensions for windowed regularization , and hence multi - parameter regularization , @xcite are also applied for the projected problem @xcite .",
    "our attention is initially on the use of the upre , for which we find a useful connection between the full and projected formulations .",
    "this is not so immediately clear , particularly for the gcv and is , we believe , the reason why a weighted gcv ( wgcv ) was required in @xcite . in our work we are also able to heuristically explain the weighting parameter in the wgcv .",
    "we stress that the approach assumes that the projected system is calculated with full reorthogonalization , a point often overlooked in many discussions , although it is apparent than many references implicitly make this assumption .",
    "although our analysis should also be relevant for the case of windowed regularization , this is not a topic for this paper , and will be considered in future work .",
    "instead we extend the hybrid approach for use with an iteratively reweighted regularizer ( irr ) , which sharpens edges within the solution , @xcite , hence demonstrating that edge preserving regularization can be applied in the context of regularized lsqr solutions of the least squares problem on a projected subspace .",
    "the paper is organized as follows .",
    "the regularization parameter estimation techniques of interest are presented in section  [ sec : parameter estimation ] .",
    "the discussion in section  [ sec : parameter estimation ] is validated with one dimensional simulations in section  [ sec : simulationoned ] .",
    "image restoration problems presented in section  [ sec : simulationtwod ] illustrate the relevance for the two dimensional case .",
    "we then go further and demonstrate the use of irr @xcite , an approach for approximating the total variation regularization , in section  [ sec : irr ] . finally in section  [ sec : walnut ] we also illustrate the algorithms in the context of sparse tomographic reconstruction of a walnut data set , @xcite , demonstrating the more general use of the approach beyond deblurring of noisy data .",
    "our conclusions are presented in section  [ conclusions ] .",
    "it is of particular interest that our analysis applies for both over and under determined systems of equations and is thus potentially of future use for other algorithms also in which alternative regularizers are imposed and also require repeated tikhonov solves at each step .",
    "further , this work extends our analysis of the upre in the context of underdetermined but small scale problems in @xcite , and demonstrates that irr can be applied for projected solutions .",
    "although it is well - known that the mdp always leads to an over estimation of the regularization parameter , e.g. @xcite , it is still a widely used method for many applications , and is thus an important baseline for comparison . on the other hand ,",
    "the upre is less well - accepted but often leads to a better estimation of the regularization parameter , e.g. @xcite . in order to use any specific regularization parameter estimation method for the projected problem it is helpful to understand the derivation on the full problem . in the discussion that follows we explicitly assume that lsqr is implemented using sufficient precision , namely with full reorthogonalization of the columns of @xmath69 and @xmath57 so that holds .",
    "the predictive error , @xmath91 , for the solution @xmath92 , is defined by @xmath93 where @xmath94 is the influence matrix , and compares with the full residual @xmath95 in both equations the first term is deterministic , whereas the second is stochastic , through the assumption that @xmath96 is a random vector . for completeness",
    ", we give the trace lemma e.g. ( * ? ? ?",
    "* lemma 7.2 ) , as required for the following discussion .    for deterministic vector @xmath97 , random vector @xmath9 with diagonal covariance matrix @xmath98 , matrix @xmath99 , and expectation operator @xmath100 @xmath101 using @xmath102 to denote the trace of matrix @xmath1 .",
    "applying to both and with the assumption that @xmath103 , due to whitening of noise @xmath9 , and using the symmetry of the influence matrix , we obtain @xmath104 where @xmath105 is the defined to be the expected value of the risk of using the solution @xmath92 .",
    "although the first term on the right hand side in each case can not be obtained , we may use @xmath106 in .",
    "thus using linearity of the trace and eliminating the first term in the right hand side of gives the upre estimator for the optimal parameter @xmath107 typically , @xmath79 is found by evaluating for a range of @xmath17 , for example by the svd see e.g. [ appb ] , with the minimum found within that range of parameter values , as suggested in @xcite for the gcv .",
    "see also e.g. ( * ? ? ?",
    "* appendix , ( a.6 ) ) for the formulae for calculating the functional in terms of the svd of matrix @xmath1 .      for the projected case we consider two different approaches for minimizing the predictive risk using the solution of the projected problem .",
    "first observe that from @xmath108 and by the column orthogonality of @xmath69 , we have @xmath109 the residual , with respect to the solution of the projected problem explicitly depending on the regularization parameter @xmath71 , is now given by @xmath110 where @xmath111 , consistent with the definition of the influence matrix @xmath112 .",
    "similarly the predictive error is given by @xmath113 comparing with , and with , gives the upre functional for finding the regularization parameter for the solution for the full problem with the solution found with respect to the projected subspace @xmath114 expanding @xmath115 gives @xmath116 where the last equality follows from the cycle property of the trace operator for consistently sized matrices .",
    "hence @xmath117 can be evaluated without reprojecting the solution for every @xmath71 back to the full problem .",
    "when estimated by upre , the optimal @xmath71 for the solution on the projected space can be found from the projected solution alone .",
    "it remains to question whether has any relevance with respect to the projected solution , i.e. does this appropriately regularize the projected solution , otherwise it may not be appropriate to find @xmath71 to minimize this functional on the subspace .",
    "observe for @xmath118 , right hand side vector @xmath119 consists of a deterministic and stochastic part , @xmath120 , where for white noise vector @xmath9 and column orthogonal @xmath57 , @xmath121 is a random vector of length @xmath48 with covariance matrix @xmath122 .",
    "thus from the derivation of the upre for the full problem defined by system matrix @xmath1 , right hand side @xmath4 and white noise vector @xmath9 , we may immediately write down the upre for the projected problem with system matrix @xmath63 , right hand side @xmath123 and white noise vector @xmath121 . in particular",
    ", defining @xmath124 and comparing with , it is immediate that minimizing to minimize the risk for the projected solution , also minimizes the risk for the full solution , with respect to the given subspace .",
    "the shift by @xmath48 as compared to @xmath125 is irrelevant in the minimization of the functional .",
    "note that this does not immediately minimize the predictive risk ( [ optalpha2 ] ) for the full problem , i.e. @xmath126 , because @xmath79 is needed with respect to solutions in @xmath127 and not just restricted to @xmath128 .    by the linearity and cycle properties of the trace operator @xmath129 and @xmath130 in exact arithmetic the large singular values of @xmath63",
    "provide a good approximation of the large singular values of @xmath1 , ( * ? ? ?",
    "* section 9.3.3 ) .",
    "thus suppose @xmath44 is such that the first @xmath44 singular values are well - approximated by those of @xmath63 , and that the ill - conditioning of @xmath1 is effectively captured so that there is clear separation between @xmath131 and @xmath132 .",
    "then for regularizing the full problem , for which @xmath133 , @xcite , and with filter factor @xmath134 , @xmath135 , we have @xmath136 comparing and , with @xmath137 in , we see that we may interpret the determination of @xmath77 as giving a good estimate for @xmath79 if @xmath138 .",
    "further , if @xmath139 provides a good estimate for @xmath140 we may interpret the determination of @xmath77 as giving a good estimate for @xmath79 in the case in which the filter factors in the tikhonov regularization are determined for the truncated singular value decomposition ( tsvd ) of @xmath1 , with truncation at @xmath44 .",
    "this observation follows theorem 3.2 @xcite which connects the use of the tsvd of @xmath63 for the solution with the solution obtained using the tsvd of @xmath1 . to summarize :    if @xmath44 is such that @xmath141 for @xmath135 , so that @xmath142 and @xmath139 approximates @xmath140 , then @xmath143 when obtained using the upre",
    "further , the estimate is found without projecting the solution back to the full space , namely by minimizing .      the premise of the mdp , @xcite , to find @xmath17 is the assumption that the norm of the residual , @xmath144 follows a @xmath145 distribution with @xmath146 degrees of freedom , @xmath147 .",
    "heuristically , the rationale for this choice is seen by re - expressing @xmath148 so that if @xmath149 has been found as a good estimate for @xmath150 then the residual should be dominated by the whitened error vector @xmath9 . for white noise @xmath151",
    "is distributed as a @xmath145 distribution with @xmath125 degrees of freedom , from which @xmath152 , with variance @xmath153 .",
    "thus we seek a residual such that @xmath154 using a newton root - finding method , see [ appb ] , where we take safety parameter @xmath155 to handle the well - known over smoothing of the mdp .",
    "alternatively , we note @xmath156 , where the size of @xmath157 depends on the percentiles of the cumulative @xmath145 distribution with @xmath125 degrees of freedom . the larger @xmath157",
    "the less confidence we have in the distribution for @xmath9 , and of @xmath92 as an approximation to @xmath158 .    for the projected residual , @xmath159 where @xmath121 follows a @xmath145 distribution with @xmath48 degrees of freedom .",
    "this suggests setting @xmath160    a number of other suggestions for a projected discrepancy principle have been presented in the literature , but all imply using @xmath161 dependent on the noise level of the full problem , e.g. @xcite , with @xmath155 .",
    "it is reported in @xcite , however , that while the theory predicts choosing @xmath155 , numerical experiments support reducing @xmath162 .",
    "alternatively this may be seen as reducing the degrees of freedom , instead of reducing @xmath162 , consistent with .",
    "we deduce that the interpretation for finding the regularization parameter based on the statistical property of the projected residual in contrast to the full residual should be important in determing the size of @xmath163 .    for the mdp the degrees of freedom change from @xmath125 to @xmath48 when the residual is calculated on the full space as compared to the projected space .",
    "thus @xmath77 is not a good approximation for @xmath164 when obtained using @xmath163 as a guide for the actual size of the projected residual . if the full problem is effectively singular , so that @xmath165 for @xmath135 , the degrees of freedom for the full problem are reduced and again @xmath143 .      unlike the upre and mdp , the method of generalized cross validation ( gcv ) for finding the regularization parameter @xmath17 does not require any estimate of the noise level @xmath151 .",
    "it is , however , a statistical technique based on leave one out validation and has been well - studied in the context of tikhonov regularization , @xcite .",
    "the optimal parameter @xmath17 is found as the minimizer of the functional @xmath166 ignoring constant scaling of @xmath167 by @xmath168 .",
    "the use of the gcv for finding the optimal parameter for the projected problem , as well for finding the subspace parameter @xmath44 , has also received attention in the literature , @xcite .",
    "the obvious implementation for is the exact replacement in using the projected system @xmath169 as indicated in @xcite .",
    "it was recognized in ( * ? ? ?",
    "* section 5.4 ) , however , that this formulation , tends to lead to solutions which are over smoothed .",
    "there it was suggested instead to use the weighted gcv dependent on parameter @xmath170 @xmath171 experiments illustrated that @xmath170 should be smaller for high noise cases , but in all cases @xmath172 is required to avoid the potential of a zero in the denominator . although the choice for @xmath170 is argued heuristically , and an adaptive algorithm to find @xmath170 is given , no theoretical analysis for finding an optimal @xmath170 is discussed .",
    "moreover , there is apparently no study of the use of for projection of underdetermined problems .",
    "consider now the two denominators in and .",
    "first of all it is not difficult to show from , with the not very restrictive requirement @xmath173 , @xmath174 hence @xmath175 . thus picking @xmath17 to minimize the projected gcv",
    "will not minimize the full gcv term .",
    "for the weighted gcv , however , @xmath176 moreover , with @xmath141 for @xmath135 @xmath177 and factoring for @xmath178 and @xmath179 in and , respectively , gives the scaled denominators @xmath180 ignoring constant scaling the denominators are equilibrated by taking @xmath181 this result suggests that we need @xmath182 in order for @xmath77 to estimate @xmath79 found with respect to the projected space .",
    "if @xmath44 is such that @xmath141 for @xmath135 , and @xmath183 then @xmath143 when obtained using the wgcv .",
    "the estimate is found without projecting the solution back to the full space .",
    "note that without reorthogonalization of the columns of @xmath69 and @xmath57 clustering of the singular values means we should not expect the equilibration of the denominators to yield the correct weighting parameter .",
    "it is interesting that this reorthogonalization was regarded as less significant in @xcite , although it is clear from our discussion that it is useful for suggesting the choice @xmath184 .",
    "all formulae apply using the svd for @xmath63 replacing that for matrix @xmath1 .",
    "the upre functional is given by @xmath364      the mdp functional is given by @xmath365 for the projected case @xmath366 replaces @xmath146 .",
    "using the svd for @xmath63 the wgcv functional is given by @xmath367 with @xmath368 this reduces to the expression for the projected gcv , .",
    "to illustrate the discussion in section  [ sec : parameter estimation ] we investigate the properties of the projected system matrices @xmath63 in the context of the solution of ill - posed one dimensional problems with known solutions . because the regularization parameter estimation techniques rely on the determination of the subspace @xmath44 and on the properties of the spectra of @xmath63 we look at the spectra and at methods to estimate @xmath44 . in all experiments we use matlab 2014b with examples from the regularization tool box @xcite and the code",
    "` bidiag_gk ` associated with the software for the paper @xcite , for finding the factorization of matrix @xmath1 .",
    "for a given problem defined by without noise , noisy data are obtained as @xmath185 for noise level @xmath186 and with @xmath187 the @xmath188 column of error matrix @xmath189 with columns sampled from a random normal distribution using the matlab function ` randn`@xmath190 .",
    "examples presented here use the test problems ` phillips ` , ` shaw ` and ` gravity ` from the regularization toolbox , @xcite , all of which are discretizations of fredholm integral equations of the first kind . to show the impact of the ill - posedness of the problem we take problem ` phillips ` for which the picard condition does not hold ,",
    "problem ` shaw ` which is severely ill - posed , and problem ` gravity ` that depends on a parameter @xmath191 determining the conditioning of the problem , here we use @xmath192 for severe ill - conditioning and @xmath193 which is better conditioned .",
    "simulations for over and under sampled data are obtained by straightforward modification of the relevant functions in @xcite . in the examples we only show @xmath194 , @xmath195 illustrating the results for the under sampled cases , with @xmath196 samples . for the given method for calculating the noise level",
    "we observe that the signal to noise ratio for the data , given by @xmath197 is independent of the test problem .",
    "in particular @xmath198 and @xmath199 .",
    "the condition of each problem depends on the condition of the matrix @xmath1 , see table  [ conditiontable ] , and thus for the same choice of @xmath186 noise propagates differently for each test problem .",
    ".condition of the test matrices [ cols=\"^,^,^,^,^\",options=\"header \" , ]     these results are substantiated by consideration of figure  [ figrelerrs ] .",
    "it is well - known that the choice of estimator is non - trivial with different methods performing better under different conditions , these results do not contradict that conclusion .",
    "overall we deduce that gcv may work well when the subspace is detected by minimizing and also for problems not satisfying the picard condition .",
    "we consider two image deblurring problems , ` grain ` and ` satellite ` , both of size @xmath200 from restoretools @xcite .",
    "restore tools also provides overloaded matrix operations for calculation of matrix vector products @xmath201 and @xmath202 , where @xmath1 describes a point spread function blurring operation .",
    "the calculation of the factorization can be immediately obtained once the object @xmath1 is defined as a psf operator with the overloaded matrix operations . as indicated by the results for the one dimensional simulations it is important to note that all presented results use reorthogonalization in obtaining the factorization .",
    "our main point here is to first demonstrate the use of the regularization techniques pmdp , wgcv and upre for increasing @xmath44 , and then to examine a stabilizing technique using an irr , section  [ sec : irr ] .",
    "results without irr are presented for completeness in section  [ sec : lsqr ] and with irr in section  [ sec : resirr ] .",
    "for contrast with the results presented in @xcite we use noise levels @xmath203 and @xmath204 in which corresponds to noise levels @xmath205 and @xmath206 , respectively , in @xcite with @xmath207 , yielding @xmath208 . these correspond to bsnr @xmath209db and @xmath210db as calculated by . for immediate comparison with @xcite",
    "we indicate the results using the noise level @xmath211 rather than @xmath186 . in figure  [ fig2dexample ]",
    "we give the true solution , blurred and noisy data and the point spread function .",
    "one can observe in fact that even a noise level of @xmath212 is quite low , the main problem here being the blurring .",
    "+      in finding the restorations for the data indicated in figure  [ badgrain ] and [ badsatellite ] we note first that the matrices @xmath1 for the psfs indicated in figures  [ psfgrain ] and [ psfsatellite ] do not satisfy the picard condition .",
    "as illustrated in figure  [ rhotwod ] , @xmath213 does not show the increase within the shown range of @xmath44 as is clear for @xmath213 with large @xmath44 obtained for the one dimensional examples in figure  [ fig : rho ] .",
    "still , @xmath213 for ` grain ` does attain a minimum within this range and then exhibits a gradual increase .",
    "this suggests that noise is entering the data after the minimum and that one may use @xmath214 where again we advance @xmath215 steps under the assumption that noise enters after the minimum .",
    "when the picard condition is not satisfied we may also use to find @xmath216 . in figure  [ rhotwod ]",
    "the vertical lines indicate the positions of @xmath217 , @xmath218 and @xmath219 . for ` grain `",
    ", @xmath218 becomes quickly independent of the number of terms used , already stabilizing at @xmath220 with just @xmath221 terms , with no change even out to a maximum size of @xmath222 in the calculation . for ` satellite ` @xmath218 is less stable only reaching @xmath223 when @xmath224 terms are used in the estimation , but increasing to @xmath225 if @xmath222 terms are required .",
    "stability in the choice of @xmath217 with respect to @xmath226 also follows lack of stability in choice of @xmath218 , suggesting that it is preferable to use @xmath219 . in our experiments",
    "we have deduced that it is important to examine the characteristic shape of @xmath213 in determining the optimal choice for the size of the subspace , and will show results using @xmath219 , @xmath217 and @xmath218 .",
    "the range for the regularization parameter is also important as is indicated through the windowing approach based on .",
    "because there is no clear distinction between the singular values as they decay , we use a single window defined by @xmath227 and apply a filtered truncated svd for the solution which is dominant for the first @xmath227 terms , ie with filter factors @xmath228 for @xmath229 and @xmath230 for @xmath231 . with @xmath232 , @xmath233 . in our results",
    "we use @xmath234 , @xmath235 , and impose @xmath236 , for the range of @xmath71 in finding the optimal regularization parameter for each of the regularization parameter techniques . to find the _ optimal _ solution , denoted by min in the legends , solutions are found using @xmath237 sampled at @xmath238 points logarithmically on this range and scaled by the mean of the standard deviation of the noise in the data , consistent with the inverse covariance scaling of the problems , and consistent with the range for the regularization parameter used in @xcite .      in assessing the solutions we adopt the approach in @xcite and use both the relative error ( re ) and the mean structured similarity ( mssm ) of the obtained images , @xcite , for which larger values correspond to better solutions .",
    "the re and mssm for all cases are illustrated in figure  [ fig2derrorsstep1 ] .",
    "the results are consistent with the literature in terms of the semi convergence behavior of the lsqr and the difficulty for both the mdp and the gcv in estimating a useful regularization parameter . on the other hand ,",
    "the results with wgcv , pdmd and upre are consistent with each other and verify the analysis in section  [ sec : regest ] , providing a stable solution for increasing @xmath44 .",
    "the solutions do not achieve the minimal error of the projected solution without regularization , which depends on knowing the optimal @xmath44 for stability .",
    "solutions found at the noted @xmath239 as compared to the _ optimal _ solution with minimum error are illustrated in figure  [ fig2dsolutionsgrain_step1 ] , demonstrating that the restorations are inadequate at this level of noise .",
    "+      iteratively reweighted regularization provides a cost effective approach for sharpening images e.g. @xcite , and has been introduced and applied for focusing geophysical inversion , in this context denoted as minimum support regularization , @xcite .",
    "regularization operator @xmath15 is replaced by a solution dependent operator @xmath240 , initialized with @xmath241 , yielding iterative solution @xmath242 . for @xmath243 @xmath244 where @xmath245 is a focusing parameter which assures that @xmath240 is invertible .",
    "immediately @xmath246 and @xmath247 moves entries for which @xmath248 away from @xmath249 .",
    "given that @xmath240 is invertible , we can use with system matrix @xmath250 , to obtain the iterative solution @xmath251 , @xmath243 .",
    "furthermore , it is straightforward to modify the algorithm for calculation of the gkb factorization for matrix @xmath252 still using the overloaded matrix operations for matrix multiplication by @xmath1 and @xmath253 , and noting that operations with the diagonal matrix are simple component - wise products .",
    "the update given by is equivalent to regularizing the system of equations , here dropping the dependence on iteration @xmath254 , @xmath255 suppose that @xmath248 , for @xmath256 and @xmath257 , then @xmath258 for @xmath256 and we may solve the reduced system @xmath259 where @xmath260 is @xmath261 but with column @xmath22 removed for @xmath256 and all other columns scaled by the relevant diagonal entries from @xmath262 .",
    "matrix @xmath260 is of size @xmath263 where @xmath264 and vector @xmath265 is vector @xmath266 with entries @xmath256 removed .",
    "we can then solve for @xmath265 in @xmath267 yielding the update @xmath268 , where @xmath269 is obtained from @xmath15 with the same diagonal entries @xmath256 removed .",
    "the update for @xmath5 is therefore obtained using with entries @xmath270 , for @xmath271 and @xmath272 , for @xmath273 .",
    "therefore , to avoid any need to discuss the choice of @xmath245 , we simply use @xmath257 and obtain the gkb factorization for the reduced system with system matrix @xmath260 .",
    "the approach for the iteration requires some explanation as to how the range of @xmath44 is obtained at irr iterations @xmath243 , and requires consideration of @xmath274 , which depends on the subspace size @xmath275 from the prior step @xmath254 , and current subspace size @xmath44 .",
    "thus calculation of @xmath217 , @xmath219 and @xmath218 are all dependent on @xmath275 as well as @xmath276 and @xmath277 , ie given a specific subspace size at @xmath275 the minimum and maximum sizes to use at step @xmath254 need to be specified . because the update costs for irr should be kept minimal , the subspace size is maintained less that @xmath239 from the previous step , i.e. we pick @xmath278 . because we anticipate that further noise enters with increasing @xmath254 , we expect @xmath279 , which can be determined by examination of @xmath280",
    ". we will examine the choices for the case with @xmath206 noise .",
    "for each iteration the range for @xmath71 is constrained using the current singular values , by @xmath281 , where at step @xmath249 , @xmath282 and @xmath283 for the irr updates .",
    "having already noted that the pmdp , wgcv and upre give consistent solutions , while gcv and mdp generally lead to larger errors , the experiments reported with irr are given for the projected , _ optimal _ and upre solutions only . to demonstrate the effectiveness of the irr for stabilizing the solution obtained using tikhonov regularization with upre and to describe a manual approach for determining the sizes of the optimal subspaces we examine the process first for problem ` grain ` and then ` satellite ` with @xmath206 noise .",
    "function @xmath213 at the first step @xmath284 does not differ significantly from the case with @xmath212 noise , shown in figure  [ rhotwod ] .",
    "we reiterate that the calculation of @xmath216 depends on the maximum subspace considered , here we use @xmath285 .",
    "because the update costs for irr should be kept minimal , the subspace size is maintained less that @xmath239 from the first step , here @xmath286 .",
    "figure  [ grainrhostepk ] shows @xmath280 for the choices of @xmath44 in the legend .",
    "it is clear that @xmath280 is almost independent of @xmath275 for the first steps , but that noise enters for @xmath287 .",
    "this is also reflected in the re in figure  [ grainre ] , the re stabilizes for increasing @xmath44 , and decreases for the first three steps of irr , but increases at step @xmath288 .",
    "cropped images are shown in figure  [ grainstep3]-[grainstep4 ] . at @xmath289",
    "the contrast is increased and some small features not present without the irr become apparent .",
    "an equivalent process is detailed for problem ` satellite ` with @xmath206 noise , and for maximum subspace @xmath290 .",
    "using @xmath291 gives a choice very close to @xmath218 , but @xmath292 depends very much on identifying which peaks should be included , here we imposed @xmath293 .",
    "the plot of @xmath213 with increasing @xmath44 in figure  [ satelliterhostepk ] show that noise enters the solution sooner for smaller @xmath294 , but that the various choices for @xmath239 demonstrate similar characteristics , and suggest that no more than @xmath215 steps of irr may be needed . from these plots of @xmath213",
    "we select @xmath295 , yielding a subspace selection of @xmath296 for the first update and in subsequent updates with @xmath297 the subspace is maintained small just of size @xmath288 .",
    "the relative errors for the two simulations are detailed in table  [ tab : errortwodfive ] , in comparison to the _ optimal _ relative error .",
    "these results show that the stabilization leads to results which are comparable to those that are _ optimal _ but practically unknown .",
    "moreover , it is clear that one may not conclude that finding @xmath239 using @xmath219 is preferable to using @xmath218 or @xmath217 .",
    "provided that the solutions are stabilized with the irr , improvements in the solutions are obtained in a limited number of steps using iterative reweighting .",
    "further , effective irr steps can be obtained using relatively small subspaces for the iterative updates .",
    "iteration & @xmath291 & @xmath218 & min for upre&overall min + @xmath42&@xmath298 & @xmath299 & @xmath300 & @xmath301 + @xmath215&@xmath302 & @xmath303 & @xmath304 & @xmath305 + @xmath306&@xmath307 & @xmath308 & @xmath309 & @xmath310 + @xmath288&@xmath311 & @xmath312 & @xmath313 & @xmath314 + & + iteration & @xmath292 & @xmath216 & min for upre&overall min + @xmath42 & @xmath315 & @xmath316 & @xmath317 & @xmath318 + @xmath215&@xmath319 & @xmath320 & @xmath321 & @xmath322 + @xmath306&@xmath323 & @xmath324 & @xmath325 & @xmath326 + @xmath288&@xmath327 & @xmath328 & @xmath329 & @xmath326 +    the solutions at the first step are illustrated in figure  [ fig : twodfive ] , demonstrating that using @xmath217 leads to best solutions in one case , and @xmath218 in the other , although effectively the quality is comparable .",
    "the graphs of @xmath213 with increasing @xmath254 in figures  [ grainrhostepk ] - [ satelliterhostepk ] indicate that the properties of @xmath280 can be used to determine effective termination of the irr , based on the iteration @xmath254 when noise enters into @xmath280 .",
    "our experience has shown that the optimal solution in terms of image quality is achieved not at the step before noise enters in @xmath280 but two steps before .      in figure  [ fig2derrorsgrain ]",
    "we contrast the relative errors obtained by the upre , projected and _ optimal _",
    "solutions for two steps of irr as compared to the first step @xmath284 .",
    "then in figure  [ fig2dmeasuresgrain ] we illustrate how the re and the mssm change with the iteration count .",
    "the vertical lines in figures  [ grainreten]-[satelliessim ] demonstrate that using @xmath217 or @xmath218 makes little difference to the quality of the solution when measured with respect to relative error or mssm .",
    "example solutions are given in figure  [ fig2dsolutionsgrain_irrstep ] .     +      to contrast the success of the regularization parameter estimation techniques in the context of a @xmath215d projection problem , we present results for the reconstruction of projection data obtained from tomographic x ray data of a walnut , used for edge preserving reconstruction in @xcite and with the description of the data described in @xcite .",
    "the data are available at @xcite .",
    "datasets ` datan ` correspond to resolution @xmath330 in the image , and use @xmath331 projections , corresponding to @xmath332 sampling .",
    "data are provided with @xmath333 , @xmath334 and @xmath335 .",
    "we use resolution @xmath334 with @xmath331 projections , and then downsampled to @xmath336 projections , @xmath337 projections and @xmath338 projections , ie angles @xmath332 , @xmath339 , @xmath340 and @xmath341 .",
    "( results with resolutions @xmath342 and @xmath335 are comparable ) .",
    "results are presented using the solution at @xmath217 for the projected solution without regularization , and regularized using upre and gcv for comparison , figures  [ walnut16460]-[walnut16415 ] , where we do not show results for @xmath331 projections , for which the solutions are almost perfect due to the apparent limited noise in the provided data .",
    "the impact of using a reduced number of projections is first evident with just @xmath336 projections . in all the presented results the parameters @xmath217 are determined automatically , after manually picking @xmath343 from manual consideration of the plot for @xmath213 , see figure  [ walnutphi ] .",
    "all other parameters are estimated in the same way as for the image restoration cases .",
    "@xmath213 for increasing sparsity for resolution @xmath334 for the walnut data . ]    the results in figure  [ walnut16460]-[walnut16415 ] , which show results for one set of data at increasing sparsity , compare with ( * ? ? ?",
    "* figure 6.6 and figure 6.7 ) , which give results with resolution for @xmath344 and @xmath345 , respectively , and angle separation @xmath346 , @xmath347 , @xmath339 and @xmath340 .",
    "results there use selected choices for the regularization parameter based on a sparsity argument with prior information and seek to support the use of the sparsity argument for reconstruction of sparse data sets , although exhibiting the rather standard total variation blocky structures when applied for truly sparsely sampled data .",
    "our results show robust reconstructions with the automatically determined solutions , after first examining the plot for @xmath213 .",
    "irr generates marginally improvements in qualitative solutions , more so for the projected case without regularization . to show the impact of the correct choice of @xmath239 on the solution we show a set of results at iteration @xmath284 using @xmath348 in figure  [ walnut164tmin18 ] , with the positive constraint .",
    "there the projected solution is already significantly noise contaminated for all levels of sparsity , while the upre yields solutions qualitatively similar to the case with @xmath343 . in the examples here",
    "we do impose an additional positivity constraint on the solutions at each step , before calculating the iterative weighting matrix .",
    "the results demonstrate that the projected problem with automatic determination of @xmath77 can be used to reconstruct sparsely sampled tomographic data , provided that an initial estimate for @xmath226 is manually determined by consideration of the plot of @xmath213 .",
    "further , irr can stabilize the solution when @xmath239 has not been appropriately estimated for the non regularized solution .",
    "for the sparse data sets the solutions do not exhibit the characteristic blocky reconstructions of total variation image reconstructions , as seen in @xcite , although as there the solutions would be inadequate for precise usage .",
    "+   +   +     +   +   +     +   +   +     +   +",
    "we have demonstrated that regularization parameter estimation by the method of upre can be effectively applied for regularizing the projected problem .",
    "our results also explain the use of the weighting parameter in the wgcv , as well as the reduced safety parameter in the mdp when applied for the projected problem .",
    "further , edge preserving regularization via the iteratively reweighted regularizer can be applied to stabilize regularized solutions of the projected problem .",
    "our results suggest manual estimation of a minimal subspace size can then lead to useful estimates for an optimal projected space , with the use of the irr leading to improvements in the solutions when @xmath239 is found by different methods , including the use of @xmath217 , @xmath219 and @xmath218 , hence making the determination of this @xmath239 less crucial in providing an acceptable solution",
    ". future work on this topic should include extending the windowed regularization parameter techniques for finding a multiply weighted gcv for projected regularization , and use of more general iteratively reweighted regularizers accounting for edges in more than one direction in conjunction with the projected solutions .",
    "these are topics for further study .",
    "suppose the svd of matrix @xmath1 , @xmath349 , is given by @xmath350 , where the singular values are ordered @xmath351 and occur on the diagonal of @xmath352 with @xmath353 zero columns ( when @xmath354 ) or @xmath355 zero rows ( when @xmath356 ) , and @xmath357 , and @xmath358 are orthogonal matrices , @xcite .",
    "then @xmath359 for the projected case @xmath360 , i.e. @xmath356 , and the expression still applies with @xmath361 replacing @xmath4 , @xmath71 replacing @xmath17 , @xmath362 replacing @xmath64 and @xmath363 in .",
    "00 bjrck a 1986 _ numerical methods for least squares problems _ society for industrial and applied mathematics philadelphia pa chung j m easley g and oleary d p 2011 windowed spectral regularization of inverse problems _ siam j. sci .",
    "comput . _ * 33 * 6 3175 - 3200 chung j m nagy j and oleary d p 2008 a weighted gcv method for lanczos hybrid regularization _ etna _ ,",
    "* 28 * , 149 - 167 chung j m kilmer m e and oleary d p 2015 a framework for regularization via operator approximation _",
    "siam j. sci .",
    "* 37 * 2 b332-b359 donatelli m and reichel l 2014 square smoothing regularization matrices with accurate boundary conditions _ j computational and applied mathematics _",
    "* 272 * 334 - 349 fong d c - l and saunders m a 2011 lsmr : an iterative algorithm for sparse least - squares problems _",
    "siam j. sci .",
    "comput . _ * 33 * 5 , 2950 - 2971 golub g h heath m and wahba g 1979 generalized cross validation as a method for choosing a good ridge parameter",
    "_ technometrics _ * 21 * 2 215 - 223 .",
    "golub g h and van loan c 1996 _ matrix computations _ john hopkins press baltimore 3rd ed .",
    "hmlinen k harhanen l kallonen a kujanp a niemi e and siltanen s 2015 tomographic x - ray data of a walnut arxiv:1502.04064v1 , http://www.fips.fi/dataset.php .",
    "hmlinen k kallonen a kolehmainen v lassas m niinimki k and siltanen s 2013 sparse tomography , _ siam journal of scientific computing _ * 35 * 3 , b644- b665 hanke m and hansen p c 1993 regularization methods for large scale problems _ surveys math .",
    "indust . _ * 3 * 253 - 315 hansen p c 1998 _ rank - deficient and discrete ill - posed problems : numerical aspects of linear inversion _",
    "siam monographs on mathematical modeling and computation * 4 * philadelphia hansen p c 2007 regularization tools : a matlab package for analysis and solution of discrete ill - posed problems version 4.0 for matlab 7.3 , _ numerical algorithms _ * 46 * , 189 - 194 , and http://www2.imm.dtu.dk/~pcha/regutools/ hansen p c and jensen t k 2008 noise propagation in regularizing iterations for image deblurring _ etna _ * 31 * 204 - 220 hansen p c nagy j g and oleary d p 2006 _ deblurring images matrices spectra and filtering _",
    "siam philadelphia hntynkov i plesinger m and strakos , z 2009 the regularizing effect of the golub - kahan iterative bidiagonalization and revealing the noise level in the data _ bit numerical mathematics _ *",
    "49 * 4 669 - 696 hochstenbach m e and reichel l 2010 an iterative method for tikhonov regularization with general linear regularization operator _",
    "j. integral equations appl . _",
    "* 22 * 463 - 480 kilmer m e and oleary d p 2001 choosing regularization parameters in iterative methods for ill - posed problems _ siam journal on matrix analysis and applications _ * 22 * 1204 - 1221 morozov v a 1966 on the solution of functional equations by the method of regularization _ sov .",
    "dokl . _ * 7 * 414 - 417 nagy j g palmer k and perrone l 2004 iterative methods for image deblurring : an object oriented approach _ numerical algorithms _ * 36 * 73 - 93 neelamani r choi h and baraniuk r g 2004 forward : fourier - wavelet regularized deconvolution for ill - conditioned systems _ ieee transactions on signal processing _ * 52 * 2 418 - 433 paige c c and saunders m a 1981 towards a generalized singular value decomposition _",
    "siam journal on numerical analysis _ * 18 * 3 398 - 405 paige c c and saunders m a 1982 lsqr : an algorithm for sparse linear equations and sparse least squares _ acm trans .",
    "math . software _",
    "* 8 * 43 - 71 paige c c and saunders m a 1982 algorithm 583 lsqr : sparse linear equations and least squares problems _ acm trans .",
    "math . software _ * 8 * 195 - 209 portniaguine o and zhdanov m s 1999 focusing geophysical inversion images _ geophysics _ * 64 * 874 - 887 paoletti v hansen p c hansen m f and maurizio f 2014 a computationally efficient tool for assessing the depth resolution in large - scale potential - field inversion _ geophysics _ * 79 * 4 a33a38 reichel l sgallari f and ye q 2012 tikhonov regularization based on generalized krylov subspace methods _ appl . numer .",
    "_ , * 62 * 1215 - 1228 renaut r a hnetynkov i and mead j l 2010 regularization parameter estimation for large scale tikhonov regularization using a priori information _ computational statistics and data analysis _ * 54 * 12 3430 - 3445 doi:10.1016/j.csda.2009.05.026 vatankhah s ardestani v e and renaut r a 2014 automatic estimation of the regularization parameter in 2-d focusing gravity inversion : application of the method to the safo manganese mine in the northwest of iran _ journal of geophysics and engineering _ * * * 11 * 045001 vatankhah s ardestani v e and renaut r a 2015 application of the @xmath145 principle and unbiased predictive risk estimator for determining the regularization parameter in 3-d focusing gravity inversion _ geophysical j international _ * 200 * 265 - 277 doi : 10.1093/gji / ggu397 vatankhah s renaut r a and ardestani v e 2014 regularization parameter estimation for underdetermined problems by the @xmath145 principle with application to 2d focusing gravity inversion _ inverse problems _ * 30 * 085002 vogel c r 2002 _ computational methods for inverse problems _ siam frontiers in applied mathematics siam philadelphia u.s.a",
    ". wohlberg b and rodriguez p 2007 an iteratively reweighted norm algorithm for minimization of total variation functionals _ ieee signal processing letters _ * 14 * 948951 wang z bovik a c sheikh h r simoncelli e p 2004 image quality assessment : from error visibility to structural similarity _ ieee trans . image process .",
    "_ * 13 * 600 - 612 www.cns.nyu.edu/~lcv/ssim zhdanov m s 2002 _ geophysical inverse theory and regularization problems _ elsevier amsterdam ."
  ],
  "abstract_text": [
    "<S> tikhonov regularization for projected solutions of large - scale ill - posed problems is considered . </S>",
    "<S> the golub - kahan iterative bidiagonalization is used to project the problem onto a subspace and regularization then applied to find a subspace approximation to the full problem . </S>",
    "<S> determination of the regularization parameter using the method of unbiased predictive risk estimation is considered and contrasted with the generalized cross validation and discrepancy principle techniques . </S>",
    "<S> examining the unbiased predictive risk estimator for the projected problem , it is shown that the obtained regularized parameter provides a good estimate for that to be used for the full problem with the solution found on the projected space . </S>",
    "<S> the connection between regularization for full and projected systems for the discrepancy and generalized cross validation estimators is also discussed and an argument for the weight parameter in the weighted generalized cross validation approach is provided . </S>",
    "<S> all results are independent of whether systems are over or underdetermined , the latter of which has not been considered in discussions of regularization parameter estimation for projected systems . numerical simulations for standard one dimensional test problems and two dimensional data for both image restoration and tomographic image reconstruction support the analysis and validate the techniques . </S>",
    "<S> the size of the projected problem is found using an extension of a noise revealing function for the projected problem . </S>",
    "<S> furthermore , an iteratively reweighted regularization approach for edge preserving regularization is extended for projected systems , providing stabilization of the solutions of the projected systems with respect to the determination of the size of the projected subspace .    </S>",
    "<S> * keywords : * large - scale inverse problems , golub - kahan bidiagonalization , regularization parameter estimation , unbiased predictive risk estimator , discrepancy principle , generalized cross validation , iteratively reweighted schemes </S>"
  ]
}