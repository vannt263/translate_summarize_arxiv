{
  "article_text": [
    "the multivariate normal distribution is a fundamental building block in many machine learning algorithms , and its well - known density can compactly be written as @xmath1 where @xmath2 denotes the mahalanobis distance for covariance matrix @xmath3 .",
    "this distance measure corresponds to the length of the straight line connecting @xmath4 and @xmath5 , and consequently the normal distribution is often used to model _ linear _ phenomena .",
    "when data lies near a nonlinear manifold embedded in @xmath6 the normal distribution becomes inadequate due to its linear metric .",
    "we investigate if a useful distribution can be constructed by replacing the linear distance function with a nonlinear counterpart .",
    "this is similar in spirit to isomap @xcite that famously replace the linear distance with a geodesic distance measured over a neighborhood graph spanned by the data , thereby allowing for a nonlinear model .",
    "this is , however , a discrete distance measure that is only well - defined over the training data . for a generative model ,",
    "we need a continuously defined metric over the entire @xmath6 .",
    "following @xcite we learn a smoothly changing metric that favors regions of high density i.e. , geodesics tend to move near the data . under this metric",
    ", the data space is interpreted as a @xmath7-dimensional riemannian manifold .",
    "this `` manifold learning '' does not change dimensionality , but merely provides a local description of the data .",
    "the riemannian view - point , however , gives a strong mathematical foundation upon which the proposed distribution can be developed .",
    "our work , thus , bridges work on statistics on riemannian manifolds @xcite with manifold learning @xcite .",
    "0.31     0.31     0.31     we develop a _ locally adaptive normal distribution ( land ) _ as follows : first , we construct a metric that captures the nonlinear structure of the data and enables us to compute geodesics ; from this , an    unnormalized density is trivially defined .",
    "second , we propose a scalable monte carlo integration scheme for normalizing the density with respect to the measure induced by the metric .",
    "third , we develop a gradient - based algorithm for maximum likelihood estimation on the learned manifold .",
    "we further consider a mixture of lands and provide the corresponding em algorithm .",
    "the usefulness of the model is verified on both synthetic data and eeg measurements of human sleep stages .    *",
    "notation * : all points @xmath8 are considered as column vectors , and they are denoted with bold lowercase characters .",
    "@xmath9 represents the set of symmetric @xmath10 positive definite matrices .",
    "the learned riemannian manifold is denoted @xmath11 , and its tangent space at @xmath12 is denoted @xmath13 .",
    "we start our exposition with a brief review of _ riemannian manifolds _ @xcite .",
    "these smooth manifolds are naturally equipped with a distance measure , and are commonly used to model physical phenomena such as dynamical or periodic systems , and many problems that have a smooth behavior .    a smooth manifold @xmath11 together with a riemannian metric @xmath14 is called a riemannian manifold .",
    "the riemannian metric @xmath15 encodes a smoothly changing inner product @xmath16 on the tangent space @xmath17 of each point @xmath12 .",
    "the riemannian metric @xmath18 acts on tangent vectors , and may , thus , be interpreted as a standard mahalanobis metric restricted to an infinitesimal region around @xmath5 .",
    "the local inner product based on @xmath15 is a suitable model for capturing local behavior of data , i.e.  _ manifold learning_. from the inner product , we can define _ geodesics _ as length - minimizing curves connecting two points @xmath19 , i.e. @xmath20 here @xmath21 is the metric tensor at @xmath22 , and the tangent vector @xmath23 denotes the derivative ( velocity ) of @xmath24 .",
    "the distance between @xmath5 and @xmath25 is defined as the length of the geodesic .",
    "a standard result from differential geometry is that the geodesic can be found as the solution to a system of @xmath26 order ordinary differential equations ( odes ) @xcite :    r0.33     @xmath27}{\\partial{\\boldsymbol{\\gamma}}(t)}}\\right]^\\t \\left({\\boldsymbol{\\gamma}}'(t ) \\otimes { \\boldsymbol{\\gamma}}'(t ) \\right ) \\label{eq : geodesic_ode } \\end{aligned}\\ ] ]    subject to @xmath28 .",
    "here @xmath29 $ ] stacks the columns of a matrix into a vector and @xmath30 is the kronecker product .",
    "this differential equation allows us to define basic operations on the manifold .",
    "the _ exponential map _ at a point @xmath5 takes a tangent vector @xmath31 to @xmath32 such that the curve @xmath33 is a geodesic originating at @xmath5 with initial velocity @xmath34 and length @xmath35 .",
    "the inverse mapping , which takes @xmath25 to @xmath13 is known as the _ logarithm map _ and is denoted @xmath36 . by",
    "definition @xmath37 corresponds to the geodesic distance from @xmath5 to @xmath25 .",
    "these operations are illustrated in fig .",
    "[ fig : operations_illustration ] .",
    "the exponential and the logarithmic map can be computed by solving eq .",
    "[ eq : geodesic_ode ] numerically , as an _ initial value problem ( ivp ) _ or a _ boundary value problem ( bvp ) _ respectively . in practice",
    "the ivps are substantially faster to compute than the bvps .",
    "the mahalanobis distance is naturally extended to riemannian manifolds as @xmath38 . from this , @xcite considered the riemannian normal distribution @xmath39 and showed that it is the manifold - valued distribution with maximum entropy subject to a known mean and covariance .",
    "this distribution is an instance of eq .",
    "[ eq : normal_pdf_general ] and is the distribution we consider in this paper .",
    "next , we consider standard `` intrinsic least squares '' estimates of @xmath4 and @xmath3 .",
    "let the data be generated from an unknown probability distribution @xmath40 on a manifold .",
    "then it is common @xcite to define the _ intrinsic mean _ of the distribution as the point that minimize the variance @xmath41 where @xmath42 is the measure ( or infinitesimal volume element ) induced by the metric .",
    "based on the mean , a covariance matrix can be defined @xmath43 where @xmath44 is the domain over which @xmath45 is well - defined . for the manifolds we consider , the domain @xmath44 is @xmath6",
    ". practical estimators of @xmath46 rely on gradient - based optimization to find a local minimizer of eq .",
    "[ eq : def_mu ] , which is well - defined @xcite . for finite data @xmath47 ,",
    "the descent direction is proportional to @xmath48 , and the updated mean is a point on the geodesic curve @xmath49 . after estimating the mean , the empirical covariance matrix is estimated as @xmath50 .",
    "it is worth noting that even though these estimators are natural , they are not maximum likelihood estimates for the riemannian normal distribution .    in practice , the intrinsic mean often falls in regions of low data density @xcite .",
    "for instance , consider data distributed uniformly on the equator of a sphere , then the optima of eq .",
    "[ eq : def_mu ] is either of the poles .",
    "consequently , the empirical covariance is often overestimated .",
    "we now have the tools to define a locally adaptive normal distribution ( land ) : we replace the linear euclidean distance with a locally adaptive riemannian distance and study the corresponding riemannian normal distribution . by learning a riemannian manifold and using its structure to estimate distributions of the data , we provide a new and useful link between riemannian statistics and manifold learning .      in the context of manifold learning ,",
    "@xcite suggest to model the local behavior of the data manifold via a locally - defined riemannian metric . here",
    "we propose to use a local covariance matrix to represent the local structure of the data .",
    "we only consider diagonal covariances for computational efficiency and to prevent the overfitting .",
    "the locality of the covariance is defined via an isotropic gaussian kernel of size @xmath51 .",
    "thus , the metric tensor at @xmath12 is defined as the inverse of a local diagonal covariance matrix with entries @xmath52 here @xmath53 is the @xmath54 dimension of the @xmath55 observation , and @xmath56 a regularization parameter to avoid singular covariances .",
    "this defines a smoothly changing ( hence riemannian ) metric that captures the local structure of the data .",
    "it is easy to see that if @xmath5 is outside of the support of the data , then the metric tensor is large .",
    "thus , geodesics are `` pulled '' towards the data where the metric is small .",
    "note that the proposed metric is not invariant to linear transformations.while we restrict our attention to this particular choice , other learned metrics are equally applicable , c.f . @xcite .      the normalization constant of eq .",
    "[ eq : pennec_normal_distribution ] is by definition @xmath57 where @xmath42 denotes the measure induced by the riemannian metric .",
    "the constant @xmath58 depends not only on the covariance matrix , but also on the mean of the distribution , and the curvature of the manifold ( captured by the logarithm map ) . for a general learned manifold , @xmath58 is inaccessible in closed - form and we resort to numerical techniques .",
    "we start by rewriting eq .",
    "[ eq : norm_const_int ] as @xmath59 in effect , we integrate the distribution over the tangent space @xmath60 instead of directly over the manifold .",
    "this transformation relies on the fact that the volume of an infinitely small area on the manifold can be computed in the tangent space if we take the deformation of the metric into account @xcite .",
    "this deformation is captured by the measure which , in the tangent space , is @xmath61 . for notational simplicity",
    "we define the function @xmath62 , which intuitively captures the cost for a point to be outside the data support ( @xmath63 is large in low density areas and small where the density is high ) .",
    "r0.3     we estimate the normalization constant using monte carlo integration .",
    "we first multiply and divide the integral with the normalization constant of the euclidean normal distribution @xmath64 . then , the integral becomes an expectation estimation problem @xmath65 $ ] , which can be estimated numerically as @xmath66 and @xmath67 is the number of samples on @xmath60 .",
    "the computationally expensive element is to evaluate @xmath63 , which in turn requires evaluating @xmath68",
    ". this amounts to solving an ivp numerically , which is fairly fast .",
    "had we performed the integration directly on the manifold we would have had to evaluate the logarithm map , which is a much more expensive bvp .",
    "the tangent space integration , thus , scales better .      assuming an independent and identically distributed dataset @xmath47 , we can write their joint distribution as @xmath69 .",
    "we find parameters @xmath4 and @xmath3 by maximum likelihood , which we implement by minimizing the mean negative log - likelihood @xmath70 the first term of the objective function @xmath71 is a data - fitting term , while the second can be seen as a force that both pulls the mean closer to the high density areas and shrinks the covariance .",
    "specifically , when the mean is in low density areas , as well as when the covariance gives significant    r0.475    probability to those areas , the value of @xmath72 will by construction be large .",
    "consequently , @xmath58 will increase and these solutions will be penalized . in practice , we find that the maximum likelihood land mean generally avoids low density regions , which is in contrast to the standard intrinsic least squares mean , see fig .",
    "[ fig : means_comparison ] .    in practice",
    "we optimize @xmath73 using block coordinate descent : we optimize the mean keeping the covariance fixed and vice versa . unfortunately , both of the sub - problems are non - convex , and unlike the linear normal distribution , they lack a closed - form solution .",
    "since the logarithm map is a differentiable function , we can use gradient - based techniques to infer @xmath4 and @xmath3 .",
    "below we give the descent direction for @xmath4 and @xmath3 and the corresponding optimization scheme is given in algorithm  [ alg : mle ] .",
    "initialization is discussed in appx .",
    "[ appendix : initialization ] .    * optimizing @xmath4 * : the objective function is differentiable with respect to @xmath4 @xcite , and using that @xmath74 , we get the gradient @xmath75.\\ ] ] it is easy to see that this gradient is highly dependent on the condition number of @xmath3 .",
    "we find that this , at times , makes the gradient unstable , and choose to use the steepest descent direction instead of the gradient direction .",
    "this is equal to @xmath76 ( see appx . [",
    "appendix : gradient_mu ] ) .    * optimizing @xmath3 * : since the covariance matrix by definition is constrained to be in the space @xmath9 , a common trick is to decompose the matrix as @xmath77 , and optimize the objective with respect to @xmath78 .",
    "the gradient of this factor is ( see appx . [",
    "appendix : gradient_sigma ] for a derivation ) @xmath79.\\ ] ] here the first term fits the given data by increasing the size of the covariance matrix , while the second term regularizes the covariance towards a small matrix .      at this point",
    "we can find maximum likelihood estimates of the land model .",
    "we can easily extend this to mixtures of lands : following the derivation of the standard gaussian mixture model @xcite , our objective function for inferring the parameters of the land mixture model is formulated as follows @xmath80,\\ ] ] where @xmath81 , @xmath82 is the probability that @xmath83 is generated by the @xmath84 component , and @xmath85 .",
    "the corresponding em algorithm is in appx .",
    "[ appendix : algorithms ] .",
    "in this section we present both synthetic and real experiments to demonstrate the advantages of the land .",
    "we compare our model with both the gaussian mixture model ( gmm ) , and a mixture of lands using least squares ( ls ) estimators ( [ eq : def_mu ] , [ eq : def_cov ] ) . since the latter are not maximum likelihood estimates we use a riemannian @xmath86-means algorithm to find cluster centers . in all experiments we use @xmath87 samples in the monte carlo integration .",
    "this choice is investigated empirically in the supplements .",
    "furthermore , we choose @xmath51 as small as possible , while ensuring that the manifold is smooth enough that geodesics can be computed numerically .        as a first experiment , we generate a nonlinear data - manifold by sampling from a mixture of 20 gaussians positioned along a half - ellipsoidal curve ( see left panel of fig .",
    "[ fig : synthetic_1_contours ] ) .",
    "we generate 10 datasets with 300 points each , and fit for each dataset the three models with @xmath88 number of components .",
    "then , we generate 10000 samples from each fitted model , and we compute the mean negative log - likelihood of the true generative distribution using these samples . fig .  [ fig : synthetic_1_likelihood ] shows that the land learns faster the underlying true distribution , than the gmm .",
    "moreover , the land perform better than the least squares estimators , which overestimates the covariance . in the supplements we show , using the standard aic and bic criteria , that the optimal land is achieved for @xmath89 , while for the least squares estimators and the gmm , the optimal is achieved for @xmath90 and @xmath91 respectively .",
    "in addition , in fig .  [",
    "fig : synthetic_1_contours ] we show the contours for the land and the gmm for @xmath92 .",
    "there , we can observe that indeed , the land adapts locally to the data and reveals their underlying nonlinear structure .",
    "this is particularly evident near the `` boundaries '' of the data - manifold .",
    "we extend this experiment to a clustering task ( see left panel of fig .",
    "[ fig : synthetic_2 ] for data ) . the center and right panels of fig .",
    "[ fig : synthetic_2 ] show the contours of the land and gaussian mixtures , and it is evident that the land is substantially better at capturing non - ellipsoidal clusters . due to space limitations , we move further illustrative experiments to appx . [",
    "appendix : experiments ] and continue with real data .",
    "we consider electro - encephalography ( eeg ) measurements of human sleep from 10 subjects , part of the physionet database @xcite . for each subject",
    "we get eeg measurements during sleep from two electrodes on the front and the back of the head , respectively .",
    "measurements are sampled at @xmath93hz , and for each 30 second window a so - called sleep stage label is assigned from the set @xmath94 . rapid eye movement ( rem ) sleep is particularly interesting , characterized by having eeg patterns similar to the awake state but with a complex physiological pattern , involving e.g. , reduced muscle tone , rolling eye movements and erection @xcite .",
    "recent evidence points to the importance of rem sleep for memory consolidation @xcite",
    ". periods in which the sleeper is awake are typically happening in or near rem intervals .",
    "thus we here consider the characterization of sleep in terms of three categories rem , awake , and non - rem , the latter a merger of sleep stages @xmath95 .",
    "we extract features from eeg measurements as follows : for each subject we subdivide the 30 second windows to 10 seconds , and apply a short - time - fourier - transform to the eeg signal of the frontal electrode with @xmath96 overlapping windows . from this",
    "we compute the log magnitude of the spectrum @xmath97 of each window .",
    "the resulting data matrix is decomposed using non - negative matrix factorization ( 10 random starts ) into five factors , and we use the coefficients as 5@xmath7 features . in fig .",
    "[ fig : sleep_sigma_curve ] we illustrate the nonlinear manifold structure based on a three factor analysis .",
    "we perform clustering on the data and evaluate the alignment between cluster labels and sleep stages using the f - measure @xcite .",
    "the land depends on the parameter @xmath51 to construct the metric tensor , and in this experiment it is less straightforward to select @xmath51 because of significant intersubject variability .",
    "first , we fixed @xmath98 for all the subjects . from the results in table [ tab : eeg_results ]",
    "we observe that for @xmath98 the land(1 ) generally outperforms the gmm and achieves much better alignment . to further illustrate the effect of @xmath51 we fitted a land for @xmath99 $ ] and present the best result achieved by the land .",
    "selecting @xmath51 this way leads indeed to higher degrees of alignment further underlining that the conspicuous manifold structure and the rather compact sleep stage distributions in fig .",
    "[ fig : sleep_sigma_curve ] are both captured better with the land representation than with a linear gmm .            in order to show the consistency of the normalization constant estimation with respect to the number of samples , we conduct the following experiment .",
    "we used the data from the first synthetic experiment of the paper .",
    "then for a grid @xmath129 on the @xmath60 we computed the corresponding @xmath72 values .",
    "thus , we computed the numerical integral on the tangent space using trapezoidal numerical integration .",
    "then we estimated the normalization constant using our approach for sample sizes @xmath130 and for 10 different runs . from the result in fig .",
    "[ fig : c_behaviour ] we observe that the numerical scheme we provide , approximates well the normalization constant that we computed numerically .        in this experiment",
    "we used the digit 1 from the mnist dataset .",
    "we sample 200 points and using pca we projected them onto the first 2 principal components .",
    "then we fitted land , a least squares model , and a normal distribution . from the result fig .",
    "[ fig : mnist ] we observe that the land model approximates efficiently the underlying distribution of the data .",
    "also , the least squares model has a similar performance , since it takes under consideration the underlying manifold .",
    "however , is obvious that it overfits the given data , and gives significant probability to low density areas . on the other hand , the linear model has poor performance , due to the linear distance measure .            here",
    "we present the feature extraction result for 3 factors . from the fig .",
    "[ fig : sleep_features ] we observe that actually , the derived data have a manifold structure .",
    "moreover , we see that the characteristics of the data i.e. , the eeg measurement , varies a lot between the 3 subjects .",
    "due to space limitations , we were not able to present the result of the least squares mixture model for the clustering problem of the synthetic data , thus , we present here the result . from the fig .",
    "[ fig : synthetic_2 ] we observe that indeed the land can approximates efficiently the underlying distributions of the clusters .",
    "even thought the least squares mixture model takes under consideration the underlying structure of the data , it fails to reveal precisely the distributions of the clusters .",
    "thus , we argue that our maximum likelihood estimates are better than the least squares estimates . on the other hand , the gmm fails even to find the correct means of the distributions .",
    "additionally to the results presented in the main paper , in fig .",
    "[ fig : synthetic_1 ] we present the contours of all the fitted models and for all the numbers of components , where the advantages of the land are obvious .",
    "especially , when @xmath89 we observe that the land approximates well the underlying distribution , while even though the least squares estimator reveals the nonlinearity of the distribution , as we discussed in the paper the covariance overfits the given data .",
    "furthermore , when @xmath86 increases the land components locally become almost linear gaussians , since the geodesics will almost be straight lines .",
    "however , even in this case the land mixture model is more flexible than the gaussian mixture model , see the result for @xmath91 . also , the land does not overfit the given data , as the least squares mixture model does , since the probability mass is more concentrated around the means , see the result for @xmath92 .",
    "we conducted an experiment using motion capture data from _ _ cmu motion capture database__. specifically , we picked two movements motion : 16 from subject 22 ( jumping jag ) , and the subject 9 ( run ) .",
    "each data point corresponds to a human pose .",
    "we projected the data onto the first 2 and 3 principal and we fitted a land mixture model and a gaussian mixture model for @xmath92 . from the results in fig .",
    "[ fig : mocap_result ] we see that the land means fall inside the data , while the gmm means are actually outside of the manifold .",
    "a scalability concern is that the underlying odes are computationally more demanding in high dimensions , and more specifically , we are interested in the logarithm map .",
    "we conducted a supplementary experiment on the mnist data , reporting the ode solver running time as a function of input dimensionality .",
    "in particular , we fix a point and we compute the running time of the logarithm map between this point and 20 random chosen points , for a set of the dimensions of the feature space . from the result in fig . [",
    "fig : scalability ] we observe that the current implementation scales to approximately @xmath131 dimensions , where it becomes impractical .",
    "we used the standard aic and bic criteria , @xmath132 where @xmath133 is the log - likelihood of the model , and @xmath134 is the number of free parameters .",
    "the optimal number of components @xmath86 can then be chosen to minimize either criteria .",
    "note that the land and the gmm are not normalized under the same measure , so their likelihoods are not directly comparable . however , we can select the optimal @xmath86 for each method separately .",
    "we used the synthetic data from the first experiment in the paper . from the results in fig .",
    "[ fig : aic_bic ] we observe that the optimal land model is achieved for @xmath89 , while the for the least squares estimators and the gmm , the optimal is achieved for @xmath90 and @xmath91 respectively .",
    "thus , we argue that the less complex land model with only one component , is able to reveal the underlying distribution , while the other two methods need more components resulting to more complex models .",
    "we are not the first to consider riemannian normal distributions , e.g.  @xcite gives a theoretical analysis of the distribution , and @xcite consider the riemannian counterpart of probabilistic pca .",
    "both consider the scenario where the manifold is known a priori .",
    "we adapt the distribution to the `` manifold learning '' setting by constructing a riemannian metric that adapts to the data .",
    "this is our overarching contribution .",
    "traditionally , manifold learning is seen as an _ embedding problem _ where a low - dimensional representation of the data is sought .",
    "this is useful for visualization @xcite , clustering @xcite , semi - supervised learning @xcite and more .",
    "however , in embedding approaches , the relation between a new point and the embedded points are less well - defined , and consequently these approaches are less suited for building generative models . in contrast , the riemannian approach gives the ability to measure continuous geodesics that follow the structure of the data .",
    "this makes the learned riemannian manifold a suitable space for a generative model .",
    "@xcite consider mixtures of riemannian normal distributions on manifolds that are known a priori .",
    "structurally , their em algorithm is similar to ours , but they do not account for the normalization constants for different mixture components . consequently ,",
    "their approach is inconsistent with the probabilistic formulation .",
    "@xcite consider data on spherical manifolds , and further consider a dirichlet process prior for determining the number of components .",
    "such a prior could also be incorporated in our model . the key difference to our work is that we consider learned manifolds as well as the following complications .",
    "in this paper we have introduced a parametric locally adaptive normal distribution .",
    "the idea is to replace the euclidean distance in the ordinary normal distribution with a locally adaptive nonlinear distance measure . in principle",
    ", we learn a non - parametric metric space , by constructing a smoothly changing metric that induces a riemannian manifold , where we build our model .",
    "as such , we propose a parametric model over a non - parametric space .",
    "the non - parametric space is constructed using a local metric that is the inverse of a local covariance matrix . here",
    "locality is defined via a gaussian kernel , such that the manifold learning can be seen as a form of kernel smoothing .",
    "this indicates that our scheme for learning a manifold might not scale to high - dimensional input spaces . in these cases",
    "it may be more practical to learn the manifold probabilistically @xcite or as a mixture of metrics @xcite .",
    "this is feasible as the land estimation procedure is agnostic to the details of the learned manifold as long as exponential and logarithm maps can be evaluated .",
    "once a manifold is learned , the land is simply a riemannian normal distribution .",
    "this is a natural model , but more intriguing , it is a theoretical interesting model since it is the maximum entropy distribution for a fixed mean and covariance @xcite .",
    "it is generally difficult to build locally adaptive distributions with maximum entropy properties , yet the land does this in a fairly straight - forward manner .",
    "this is , however , only a partial truth as the distribution depends on the non - parametric space . the natural question , to which we currently do not have an answer , is whether a suitable maximum entropy manifold exist ?",
    "algorithmically , we have proposed a maximum likelihood estimation scheme for the land .",
    "this combines a gradient - based optimization with a scalable monte carlo integration method . once exponential and logarithm maps are available , this procedure is surprisingly simple to implement .",
    "we have demonstrated the algorithm on both real and synthetic data and results are encouraging .",
    "we almost always improve upon a standard gaussian mixture model as the land is better at capturing the local properties of the data .",
    "we note that both the manifold learning aspect and the algorithmic aspect of our work can be improved .",
    "it would be of great value to learn the parameter @xmath51 used for smoothing the riemannian metric , and in general , more adaptive learning schemes are of interest .",
    "computationally , the bottleneck of our work is evaluating the logarithm maps",
    ". this may be improved by specialized solvers , e.g.  probabilistic solvers @xcite , or manifold - specific heuristics .",
    "the ordinary normal distribution is a key element in many machine learning algorithms .",
    "we expect that many fundamental generative models can be extended to the `` manifold '' setting simply by replacing the normal distribution with a land .",
    "examples of this idea include nave bayes , linear discriminant analysis , principal component analysis and more .",
    "finally we note that standard hypothesis tests also extend to riemannian normal distributions @xcite and hence also to the land .",
    "* acknowledgements*. lkh was funded in part by the novo nordisk foundation interdisciplinary synergy program 2014 , biophysically adjusted state - informed cortex stimulation ( basics). sh was funded in part by the danish council for independent research , natural sciences .",
    "* notation * : all points @xmath8 are considered as column vectors , and they are denoted with bold lowercase characters .",
    "@xmath9 represents the set of symmetric @xmath10 positive definite matrices .",
    "the learned riemannian manifold is denoted @xmath11 , and its tangent space at point @xmath12 is denoted @xmath60 .",
    "the locally adaptive normal distribution is defined as @xmath104 therefore , the normalization constant is equal to @xmath105 \\simeq \\frac{\\z}{s } \\sum_{s=1}^s m({\\boldsymbol{\\mu}},{\\mathbf{v}}_s ) , \\qquad \\text{where } \\quad { \\mathbf{v}}_s \\sim \\n(0,{\\boldsymbol{\\sigma}}).\\end{aligned}\\ ] ] to simplify notation we have define the @xmath62 and @xmath106 .",
    "the integral is then estimated with a monte - carlo technique .",
    "the objective function is differentiable with respect to @xmath4 with @xmath107 then the gradient of the objective function @xmath108 is equal to @xmath109\\\\ & = -\\frac{1}{n } { \\boldsymbol{\\sigma}}^{-1 } \\sum_{n=1}^n { \\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}}_n ) } + \\frac{1}{\\c({\\boldsymbol{\\mu}},{\\boldsymbol{\\sigma}})}\\int_{\\m } { \\frac{\\partial}{\\partial{\\boldsymbol{\\mu } } } } \\left [    \\exp\\left(-\\frac{1}{2 } { \\langle{\\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}})},{\\boldsymbol{\\sigma}}^{-1}{\\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}})}\\rangle } \\right ) \\right]{\\mathrm{d } } \\m({\\mathbf{x } } ) \\\\ & = -\\frac{{\\boldsymbol{\\sigma}}^{-1 } } { n } \\sum_{n=1}^n { \\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}}_n ) } + \\frac { { \\boldsymbol{\\sigma}}^{-1}}{\\c({\\boldsymbol{\\mu}},{\\boldsymbol{\\sigma } } ) } \\int_{\\m } { \\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x } } ) } \\exp\\left(-\\frac{1}{2 } { \\langle{\\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}})},{\\boldsymbol{\\sigma}}^{-1}{\\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}})}\\rangle } \\right ) { \\mathrm{d } } \\m({\\mathbf{x } } ) \\\\ & = -\\frac{1}{n } { \\boldsymbol{\\sigma}}^{-1 } \\sum_{n=1}^n { \\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}}_n ) } + \\frac { { \\boldsymbol{\\sigma}}^{-1}}{\\c({\\boldsymbol{\\mu}},{\\boldsymbol{\\sigma } } ) } \\int_{{\\mathcal{t}_{\\boldsymbol{{\\boldsymbol{\\mu}}}}\\m } } m({\\boldsymbol{\\mu}},{\\mathbf{v}}){\\mathbf{v } } \\exp\\left(-\\frac{1}{2 } { \\langle{\\mathbf{v}},{\\boldsymbol{\\sigma}}^{-1}{\\mathbf{v}}\\rangle } \\right ) { \\mathrm{d } } { \\mathbf{v } } \\\\ & = -{\\boldsymbol{\\sigma}}^{-1}\\left [ \\frac{1}{n}\\sum_{n=1}^n { \\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}}_n ) } - \\frac{\\z}{\\c({\\boldsymbol{\\mu}},{\\boldsymbol{\\sigma}})\\cdot s}\\sum_{s=1}^s m({\\boldsymbol{\\mu}},{\\mathbf{v}}_s ) { \\mathbf{v}}_s\\right ]",
    ". \\label{eq : gradient_mu}\\end{aligned}\\ ] ]          using the cauchy - schwarz inequality @xmath111 for the optimization problem ( [ eq : steepest_optimization ] ) , we get that the minimizer is equal to @xmath112 and thus , by plugging the result of ( [ eq : steepest_minimizer ] ) in to ( [ eq : steepest_optimization ] ) , we get that the steepest descent direction is @xmath113    in our case , the @xmath114 , and thus , we get that the steepest descent direction of the objective function of the land model is @xmath115 where we omit the denominator @xmath116 , since this is just a scaling factor , which will be captured by the stepsize .",
    "this avoid problems that appears due to large condition numbers of @xmath3 .",
    "we decompose the @xmath77 . in addition , we rewrite the inner product as follows @xmath117 = 2 { \\mathbf{a } } { \\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}}_n)}{\\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}}_n)}^\\t,\\end{aligned}\\ ] ] where @xmath118 is the trace operator",
    ". then the gradient of the objective with respect the matrix @xmath78 is    @xmath119\\\\ & = \\frac{1}{2n}2 { \\mathbf{a}}\\sum_{n=1}^n { \\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}}_n ) } { \\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}}_n)}^\\t \\\\ & + \\frac{1}{\\c({\\boldsymbol{\\mu}},{\\boldsymbol{\\sigma}})}\\int_{\\m } { \\frac{\\partial}{\\partial{\\mathbf{a}}}}\\left [ \\exp\\left(-\\frac{1}{2 } { \\langle{\\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}})},{\\mathbf{a}}^\\t{\\mathbf{a}}{\\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}})}\\rangle}\\right ) \\right ] { \\mathrm{d}}\\m({\\mathbf{x}})\\\\ & = \\frac{1}{n}{\\mathbf{a}}\\sum_{n=1}^n { \\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}}_n ) } { \\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}}_n)}^\\t \\\\ & - \\frac{{\\mathbf{a } } } { \\c({\\boldsymbol{\\mu}},{\\boldsymbol{\\sigma } } ) } \\int_{\\m } { \\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}})}{\\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}})}^\\t \\exp\\left(-\\frac{1}{2 } { \\langle{\\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}})},{\\mathbf{a}}^\\t{\\mathbf{a}}{\\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}})}\\rangle}\\right ) { \\mathrm{d}}\\m({\\mathbf{x}})\\\\ & = \\frac{1}{n}{\\mathbf{a}}\\sum_{n=1}^n { \\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}}_n ) } { \\text{log}_{{\\boldsymbol{\\mu}}}({\\mathbf{x}}_n)}^\\t   \\\\ & - \\frac{{\\mathbf{a}}}{\\c({\\boldsymbol{\\mu}},{\\boldsymbol{\\sigma } } ) } \\int_{{\\mathcal{t}_{\\boldsymbol{{\\boldsymbol{\\mu}}}}\\m } } m({\\boldsymbol{\\mu}},{\\mathbf{v } } ) { \\mathbf{v}}{\\mathbf{v}}^\\t \\exp\\left(-\\frac{1}{2 } { \\langle{\\mathbf{v}},{\\boldsymbol{\\sigma}}^{-1}{\\mathbf{v}}\\rangle } \\right ) { \\mathrm{d}}{\\mathbf{v}}.\\end{aligned}\\ ] ]",
    "similarly the land mixture model are @xmath121 \\label{eq : grad_mu_mm } \\\\ & { \\nabla_{{\\mathbf{a}}_k}}\\psi({\\boldsymbol{\\theta } } )   = { \\mathbf{a}}_k \\left [ \\sum_{n=1}^n r_{nk } { \\text{log}_{{\\boldsymbol{\\mu}}_k}({\\mathbf{x}}_n ) } { \\text{log}_{{\\boldsymbol{\\mu}}_k}({\\mathbf{x}}_n)}^\\t   - \\frac{\\z\\cdot r_k}{\\c_k({\\boldsymbol{\\mu}}_k,{\\boldsymbol{\\sigma}}_k)\\cdot s } \\sum_{s=1}^s m({\\boldsymbol{\\mu}}_k,{\\mathbf{v}}_s){\\mathbf{v}}_s { \\mathbf{v}}_s^\\t \\right ] \\label{eq : grad_a_mm}\\end{aligned}\\ ] ] where @xmath122 , and the responsibilities @xmath82 .",
    "the land objective is expensive to evaluate due to the dependency on @xmath123 .",
    "this imply that a line - search is infeasible for selecting a stepsize .",
    "thus , we use the following common trick . each stepsize is given in the start of the algorithm .",
    "if the objective increased after an update , we reduce the corresponding stepsize as @xmath124 , and if the objective reduced , then @xmath125 .        1 .   * random * : we initialize the land mean with a random point on the manifold . the initial covariance is the empirical covariance of the tangent vectors .",
    "this initialization can be used also for the mixture model , with @xmath86 random starting points .",
    "then , we cluster the points and the covariances are initialized using empirical estimators .",
    "* least squares * : we initialize the land with the intrinsic least squares mean , and the covariance with the empirical estimator . this initialization can be used also for the mixture model , using the extension of the @xmath126-means on riemannian manifolds , and then , the points of each cluster for the empirical covariances .",
    "* gmm * : we initialize the land mixture model centres with the result of the gmm . for the empirical covariances ,",
    "we use the points that belong to each cluster from the gmm solution .",
    "our objective function is non - convex , thus , as stopping criterion we use the change of the objective value . in particular , we stop the optimization when @xmath127 , for some @xmath128 given by the user . the same stopping criterion is used for the mixture model ."
  ],
  "abstract_text": [
    "<S> the multivariate normal density is a monotonic function of the distance to the mean , and its ellipsoidal shape is due to the underlying euclidean metric . </S>",
    "<S> we suggest to replace this metric with a locally adaptive , smoothly changing ( riemannian ) metric that favors regions of high local density . </S>",
    "<S> the resulting _ locally adaptive normal distribution ( land ) _ is a generalization of the normal distribution to the `` manifold '' setting , where data is assumed to lie near a potentially low - dimensional manifold embedded in @xmath0 . </S>",
    "<S> the land is parametric , depending only on a mean and a covariance , and is the maximum entropy distribution under the given metric . the underlying metric is , however , non - parametric . </S>",
    "<S> we develop a maximum likelihood algorithm to infer the distribution parameters that relies on a combination of gradient descent and monte carlo integration . </S>",
    "<S> we further extend the land to mixture models , and provide the corresponding em algorithm . </S>",
    "<S> we demonstrate the efficiency of the land to fit non - trivial probability distributions over both synthetic data , and eeg measurements of human sleep . </S>"
  ]
}