{
  "article_text": [
    "facial expressions are a primary modality to understand the emotional status of an individual .",
    "the expression provides a useful contextual clue for social communication  @xcite .",
    "however , individuals do not always clearly reveal their facial expressions .",
    "when an individual reveals an ambiguous facial expression , a human may have an experience to compare his / her expression with other expressions observed in past in order to extract their facial expression differences .",
    "the related evidence is found in the literature of brain sciences . according to  @xcite , an individual can discern various facial expressions by recalling the memorized face shapes of a shown person .",
    "the neural pathways for detecting changeable aspects of faces ( e.g. , eye movements and emotional expressions ) and for memorizing the unique face shape are separately distributed  @xcite .",
    "these two processes are interacted in the core system of the brain  @xcite .",
    "we attempt to utilize a reference face image that indicates the memorized unique face in the brain to discriminate a facial expression input in a deep neural network framework ( see figure  [ fig : overview_concept ] ) .",
    "we assume that an expression factor can be extracted from the _ contrastive characteristics _ between the given image and the reference image . the reference image for an individual identity , however , is not always available in the wild .",
    "we start from the assumption that there is a generative artificial neural network that can be used to infer a reference image from the given facial expressions .",
    "if a single image is given , the reference image is generated using the generative ( encoder - decoder ) networks .",
    "the next required process is to model the _ contrastive characteristics _ mentioned above using deep neural networks .",
    "one of main concerns is to find out how to extract or encode the contrastive features between the reference image and the given expression image . in our proposed networks , two representation models are included , for 1 ) disentangling of expressions and 2 ) explaining of contrastive representation with a supervised setting . in general , deep networks disentangle multiple variation factors of an input image .",
    "several unknown or unintended factors are revealed in the networks and a useful factor is selected by a proper objective . in this paper",
    ", we attempt to disentangle directly the intended factor : a facial expressive factor .",
    "hence , disentangling of expression is conducted in two steps .",
    "first , through learning a generator network that estimates a reference expression image , expressive factors can be eliminated .",
    "this estimated reference image is used to measure a expressive representation by comparing with the original expression image in the feature space . in a later part",
    ", disentangling is assisted by contrastive metric learning and a supervised reconstruction .    from the approach in the literature , gradual changes of facial expressions are utilized to extract temporal information along the multiple frames .",
    "this multiple images ( video ) based model has abundant information of the expression transition , which can be used for the recognition . in this paper , we focus on exploring a representation from a pair of a generated frame and a given frame .",
    "our proposed framework could be easily extended to utilize multiple frames as well .    in this paper",
    ", we attempt to answer to a few questions quantitatively and qualitatively : 1 ) is a generated reference image useful for the discriminative task ?",
    "2 ) how generative networks are controlled by contrastive metric learning for a discriminative purpose ?",
    "3 ) how does facial generation affect expression recognition ?    the main contributions of this paper are as follows :    * we combine encoder - decoder networks and convolutional neural networks into a unified network that simultaneously learns to generate , compare , and classify samples on a dataset . *",
    "we show that the contrastive representation trained with contrastive metric learning and a supervised reconstruction is useful to achieve a better discriminative performance for a facial expression recognition task . *",
    "we show that the proposed method outperforms the state - of - the - art methods including the multiple images based approach in terms of facial expression recognition accuracy even when a single image is utilized in a test phase .",
    "facial expression recognition has been studied over decades .",
    "several different approaches exist that are based on local feature extraction , facial action units ( faus ) , temporal information , and convolutional neural networks .",
    "the local feature - based methods such as the gabor filter , lbp , hog , and bow are the most common and widely studied to extract good visual features @xcite . in the fau",
    "based methods @xcite , faus are detected and analyzed to classify an expression .",
    "this is mainly based on the facial action coding system ( facs ) proposed by paul ekman @xcite .",
    "temporal information - based methods @xcite utilize multiple images .",
    "these methods , however , achieve _ limited _ recognition accuracy performance because the designed features lost some information . to overcome the insufficient representations of the hand - crafted features ,",
    "deep learning based methods have been recently adopted .",
    "an ensemble of two deep networks models that handle temporal information including appearance and geometric features has been proposed  @xcite . a simple convolutional neural network has been used to analyze the fau in the learned filter of the networks  @xcite . to obtain discriminative spatiotemporal representation",
    ", facial action parts detection is performed using 3d - cnn  @xcite .",
    "however , it shows limited performance when compared to the state - of - the - art methods .",
    "this is because those cnn - based methods still could not show a good enough representation of a facial expression .",
    "another deep learning framework has been proposed to take advantage of the discriminative and generative models for realizing a better generalization performance .",
    "traditionally in generative networks such as the autoencoder , a popular approach is that the entire stack of encoders is finetuned using pre - trained autoencoders in a layer - wise manner for discriminative purposes .",
    "recently , a generative model was simultaneously learned with a discriminative model . in generative adversarial networks ( gans )",
    "@xcite , the generative model is learned against an adversary and a discriminative model that learns to determine whether a sample is from the model distribution or data distribution .",
    "the stacked what - where auto - encoders ( swwaes )  @xcite integrate discriminative and generative learning pathways and provide a unified approach to supervised , semi - supervised , and unsupervised learning . in this paper , we deploy a generative model with discriminative learning as well .",
    "we are mainly focusing on investigating a contrastive representation of a facial expression that is optimized with appropriate objectives .",
    "consider an input image matrix @xmath0 and a reference image matrix @xmath1 that are elements of a given set of image matrices @xmath2 .",
    "the corresponding expression label is denoted by @xmath3 and @xmath4 respectively . in a real world",
    ", an expressive face might be changed from a reference ground face ( due to emotional changes that incur facial muscle movements @xcite ) .",
    "we define a relationship between two images with a hidden factor denoted by @xmath5 formally as follows : @xmath6 where the addition indicates operations for facial expression change is added at the equation  ( [ eq : additive_epsilon ] ) : @xmath7 . in this paper , we omit the term @xmath8 for a simplicity in the notation . ] .    as a facial expression",
    "is not always apparently represented as an absolute value , a quantity of expression change obtained by comparing with a reference image might be useful .",
    "an expression image with a very small change could be recognized via difference maps ( e.g. , a pixel - wise distance and optical flows ) . as a human keeps a neutral - like or less - expressive face most of time , that face image",
    "could be considered as the reference image .",
    "a representation of a difference between expression images can appear in various ways .",
    "a simple approach is to compare image pixels of the faces .",
    "however , owing to distortions between the images ( e.g. , distortions by an affine transform ) , comparing the images at the pixel level is not effective . for example",
    ", a small translation in the image level might return large pixel - wise errors even though a human face shows no expression changes .",
    "the representation of the difference (  contrastive representation \" ) can be better extracted at the feature level , but not at the pixel level .",
    "the feature - wise representation can offer an invariance towards distortions ( e.g. , translation , scale , or rotation ) .",
    "we employ a contrastive representation in the networks to extract a latent difference factor between expressions .",
    "consider a pair of images @xmath9 , where @xmath0 is an input sample and @xmath1 is a reference sample .",
    "let @xmath10 ( abbreviation of an encoder ) be a transform function used to map an input matrix to the embedding space ( @xmath11 ) . in the transformed space , a latent factor in the feature level ( @xmath12 ) can be represented as follows : @xmath13 where @xmath14 is an element - wise distance formulation and @xmath15 . in this paper",
    ", we adopt a distance @xmath16 of the @xmath17-th element of the feature vector , @xmath18 ) for @xmath14 .    in the contrastive representation of  expressiveness , \"",
    "we expect that other factors ( e.g. , individual s identity , pose , and etc . ) than the expression will be eliminated .",
    "the contrastive representation @xmath12 is used for a discriminative task .      the reference face",
    "image , such as less - expressive face image of an individual , may not be available in the test phase . in this paper , therefore , we propose to generate the reference face using convolutional encoder - decoder networks . to estimate a reference face transformed from an expressive face , we apply the concept of the denoising auto - encoder ( dae ) . in the dae ,",
    "a term corresponding to corruption , i.e. , a gaussian distribution , added to the original input is eliminated via learning is corrupted into @xmath19 using the known conditional distribution @xmath20 in order to train the autoencoder to estimate the reverse conditional @xmath21  @xcite . in this section",
    ", we assume that the term corresponding to corruption should not be limited to a specific probability distribution . there might be a latent model ( or unknown transform ) that makes a face with a certain expression appear to be a reference face . without a definition of the latent distribution , in this paper , the model is represented using encoder - decoder networks . by disentangling facial expressive factors in feature learning , hence , information that is irrelevant or of negligible use for the discriminative purposes",
    "could be discarded  @xcite .      in this section ,",
    "we show how the generated reference image can be used in deep networks .",
    "multiple objectives are adopted to optimize parameters of the networks to generate a good contrastive representation .",
    "as shown in figure  [ fig : overview ]  ( f )  and  [ fig : overview_detail ] , a loss function ( @xmath22 ) of the proposed networks consists of three kinds of objectives .",
    "formally , the loss function is written as follows : @xmath23 where @xmath24 denotes a discriminative loss function , @xmath25 denotes a contrastive loss function , and @xmath26 denotes a reconstruction loss function .",
    "@xmath27 indicates a weight .",
    "the main purpose of the proposed networks is to classify a facial expression in the given input . for the discriminative objective @xmath24",
    ", we adopt the cross entropy loss function which is widely used for the classification task .",
    "consider a pair of features @xmath28 , @xmath29 extracted from encoder layers @xmath30 , where a subscript @xmath31 at @xmath30 indicates the second encoder layers ( layers shown in figure  [ fig : overview_detail ] ) .",
    "a contrastive representation feature @xmath32 where @xmath14 is the element - wise distance and @xmath33 is used for the classification task .    for learning a contrastive representation ,",
    "two learning objectives are deployed in the proposed networks : the first objective is contrastive metric learning ( @xmath25 ) to enlarge or to diminish the distance between the two feature vectors , and the second is reconstruction learning ( @xmath26 ) for a better representation .",
    "hence , the two objective functions are designed to jointly assist the classification task for realizing a good generalization performance .",
    "[ [ loss - for - contrastive - metric - learning - in - feature - space . ] ] loss for _ contrastive metric learning _ in feature space .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the objective of the loss @xmath25 is to optimize a similarity between two features @xmath34 according to an expression label . if the expression labels of @xmath35 and @xmath36 are not identical , the function optimizes to obtain dissimilar features within a predefined margin ; if the expressions are identical , it optimizes to similar features .",
    "hence , the contrastive loss @xcite is adopted for @xmath25 in a feature space as follows : @xmath37 where @xmath38 if the labels of a pair @xmath39 are not the same , @xmath40 otherwise , @xmath41 is a similarity measure , and @xmath42 is a margin . a feature space is defined as ( @xmath43 ) at the encoder layers .    [",
    "[ loss - for - generation - and - representation . ] ] loss for generation and representation .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the main objectives of the loss @xmath26 are two - fold : one is to generate a reference image , and the other is to supplement to represent a good contrastive feature in the embedding layer ( @xmath44 ) .",
    "hence , a reconstruction loss ( @xmath26 ) can be represented as a weighted summation of three terms as follows : @xmath45    where of the subscripts of @xmath46 , the first one @xmath47 indicates the stage for a generation ( @xmath48 ) or a reconstruction ( @xmath49 ) ( shown in figure  [ fig : overview_detail ] ) .",
    "the second subscript @xmath50 , indicates a target : @xmath51 for a _",
    "_ r__eference image , and @xmath52 for an _",
    "_ i__nput image .",
    "@xmath53 where @xmath54 is learned to estimate @xmath1 .",
    "in this section , we describe the experiments conducted to compare the proposed method with the state - of - the - arts on three publicly available face expression databases ( ck+ , mmi , and oulu - casia ) that are widely adopted in the literatures  @xcite .",
    "all models used in different databases share exactly the same architecture ( shown in figure  [ fig : overview_detail ] ) , including encoder - decoder networks depicted in table  [ table : ae_detail ] .",
    "all parameter settings are shared through the databases with the same value .",
    "the encoder - decoder networks in table  [ table : ae_detail ] are pre - trained with the reconstruction task using the casia - webface database  @xcite , and three convolutional layers in the encoder are adopted at @xmath55 ( @xmath56 ) of the proposed generative - contrastive networks ( gcnet ) shown in the figure [ fig : overview_detail ] .",
    "the baseline cnn consisting of three convolutional layers and two inner - product ( fc ) layers are pre - trained with the identification task using the same database , and convolutional layers are adopted at @xmath57 ( @xmath58 ) . during the training of the proposed networks , the learning rate at layers of the decoder networks is set to @xmath59 during fine - tuning . the number of outputs at the first fully - connected layer ( inner - product ) is empirically determined by @xmath60 where we set @xmath61 , @xmath62 .",
    "this is intended that a dimensionality of the vector decreases smoothly as the number of ( conv./pool ) layers increases .",
    "@xmath63 is related to a pooling size ( @xmath64 ) at each layer .",
    "the dropout is applied before this fully - connected layer with a ratio of @xmath65 . after the fc - layer",
    ", a softmax layer is connected with the number of outputs corresponding to the number of classes .",
    "we arbitrarily set @xmath66 for each loss function .",
    "the maximum iteration is set to @xmath67 .",
    "our models are trained with ` nesterov ' optimization using an ` inverse ' learning policy , a base learning rate of @xmath68 , a momentum of @xmath69 , a gamma term of 0.75 , a weight decay of @xmath70 , and a mini - batch size of @xmath71 .",
    "the proposed network model is implemented on _ python _ and the deep learning framework and run using the nvidia tesla k80 gpu .    to avoid over - fitting , we applied data augmentation during the training phase .",
    "we used input images on a gray level ( 1 channel ) where a facial region is cropped , normalized based on 5 points ( eyes , the end of a nose , and two ends of lips ) and resized into @xmath72 .",
    "the resized image is cropped again with the size of @xmath73 at a random location .",
    "each cropped image is manipulated using 2d affine transform such as scaling , rotation , and intensity multiplication , in addition to random flipping .      [",
    "[ ck - database ] ] ck+ database @xcite + + + + + + + + + + + + + + + + + + + +    this database is widely adopted in the benchmark for facial expression recognition tasks .",
    "this database consists of 593 sequences with 123 individuals .",
    "the images are captured expression transitions from a neutral face to peak facial expression acted by an individual .",
    "the 327 valid sequences with 118 individuals that maintain discrete emotion labels such as  anger , contempt , disgust , fear , happy , sad , and surprise \" are adopted for an experiment .",
    "we divide the valid sequences into ten different subsets with individual - independent way . according to individual",
    "i d in the database , individuals are grouped by sampling in i d ascending order with ten even intervals first .",
    "one subset out of ten subsets is used for validation ( test ) , the remains are used for training .",
    "this procedure is repeated ten times .",
    "this 10-fold cross - validation follows the previous works @xcite .",
    "[ [ mmi - database ] ] mmi database @xcite + + + + + + + + + + + + + + + + + + + +    this database consists of 312 sequences from 30 individuals with six basic expressions ( contempt included in the ck+ database is excluded ) .",
    "we selected 205 sequences captured in a front view .",
    "each sequence starts from a neutral face , and shows a peak expression within a single expression type in the middle of the sequence . at the end",
    ", it returns to a neutral face again . as a peak expression frame number is not given , we selected it manually .",
    "similar to the ck+ database settings , we divided the mmi database into ten different individual independent subsets . consequently",
    ", 10-fold cross validation was conducted .",
    "this database includes individuals who pose expressions non - uniformly , wear glasses / caps , and have mustaches / head movements .",
    "therefore , the facial expression recognition task is relatively challenging .",
    "moreover , the small number of sequences and individuals makes it difficult to achieve a good generalization performance .",
    "this database could be suitable to measure the recognition performance in realistic situations when compared to other databases .",
    "[ [ oulu - casia - vis - database ] ] oulu - casia vis database @xcite + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    this database consists of 480 image sequences with 80 individuals .",
    "this database is captured under the visible ( vis ) normal illumination conditions and is a subset of oulu - casia nir - vis database .",
    "each individual poses six basic expressions similar to mmi database .",
    "similar to the ck+ database , the sequence starts from a neutral face and ends with peak facial expression within a same emotion category .",
    "as done with the two databases above , individual - independent 10-fold cross - validation is conducted .",
    ".details of the convolutional encoder - decoder layers  @xcite embedded in the proposed networks .",
    "an encoder part consists of three convolutional layers ( conv . ) which is followed by batch normalization ( bnorm ) , relu , and max poooling layers .",
    "correspondingly , a decoder part consists of three de - convolutional ( transposed convolutional ) layers . in a conv and deconv .",
    "layers , ( @xmath74 , @xmath75 ) indicates that there is 32 sets of 5@xmath765 filters",
    ". in maxpool and maxunpool layers , ( @xmath74 ) indicates a pooling window size . [ cols=\"^\",options=\"header \" , ]          [ [ small - expression - images ] ] small expression images + + + + + + + + + + + + + + + + + + + + + + +    in figure  [ fig : ambiguous_images ] , several examples of recognition errors on the small expression images are shown .",
    "our proposed method shows approximately twice less recognition errors than baseline cnn method .",
    "however , both the baseline cnn and the proposed method still have a limitation to recognize the small ( or ambiguous ) expressions .",
    "[ [ visualization - in - the - feature - space ] ] visualization in the feature space + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to observe a discriminative distribution of the extracted features , we visualized the feature vectors from the first layer of the fully - connected layers of the proposed networks of the baseline cnn and our proposed networks .",
    "we visualize the 384 dimensional feature vectors using t - sne  @xcite .",
    "the feature points of original images are scattered within a narrow region .",
    "the point distribution of the baseline cnn forms partially overlapped clusters .",
    "the proposed network features are clustered well to discriminate individual expression further .",
    "[ [ patterns - of - the - learned - filters ] ] patterns of the learned filters + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we observe the characteristics of the filters learned in the proposed networks . as shown in figure  [ fig : learned_filter ] , the encode filters learned by contrastive metric learning , ( b ) , has more gabor like edge and blob detection filters than ( a ) .",
    "the decoder filters for the expression reconstruction , ( e ) , show a simpler patterns than that for the reference image generator , ( c ) , and the reconstruction decoder of a neutral image , ( d ) , as shown in figure  [ fig : learned_filter ] .    [ [ visualization - of - the - response - maps ] ] visualization of the response maps + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we observe the response maps resulted from generation and reconstruction layers of the proposed networks to understand what the networks have been conducted in the test phase . in figure",
    "[ fig : response_map_ck+ ] , a generated reference image , a reconstructed neutral image , and a reconstructed image of a given expression are shown .",
    "the generated reference image is affected by reconstruction and contrastive metric learning .",
    "in this paper , we proposed facial expression recognition method based on contrastive representation learning .",
    "the contrastive representation is calculated in the embedding layer of deep networks by comparing a single given image with a reference image .",
    "the reference image is generated by deep generative ( encoder - decoder ) networks .",
    "this approach is useful especially if an expressive depth of an emotional face is varied among individuals , expressions , or situations . in our proposed networks",
    ", we attempted to disentangle a facial expressive factor directly .",
    "disentangling of expression is conducted in two steps : 1 ) learning of a reference face by a generator network and 2 ) learning of a contrastive representation with a combination of contrastive and reconstruction objectives .",
    "extensive experiments were conducted on three face expression databases that are publicly available and widely adopted in the literature .",
    "the proposed method outperforms the known state - of - the arts , including both single image and multiple - image based methods .",
    "this study could be extended to effectively detect and recognize small changes of facial expressions from sequential images .",
    "m.  s. bartlett , g.  littlewort , m.  frank , c.  lainscsek , i.  fasel , and j.  movellan . recognizing facial expression : machine learning and application to spontaneous behavior . in _",
    "2005 ieee computer society conference on computer vision and pattern recognition ( cvpr05 ) _ , volume  2 , pages 568573 .",
    "ieee , 2005 .",
    "y.  bengio , l.  yao , g.  alain , and p.  vincent .",
    "generalized denoising auto - encoders as generative models . in _ proceedings of the 26th international conference on neural information processing systems _ , nips13 , pages 899907 , 2013 .",
    "i.  goodfellow , j.  pouget - abadie , m.  mirza , b.  xu , d.  warde - farley , s.  ozair , a.  courville , and y.  bengio .",
    "generative adversarial nets . in _ advances in neural information processing systems 27",
    "_ , pages 26722680 . 2014 .",
    "r.  hadsell , s.  chopra , and y.  lecun .",
    "dimensionality reduction by learning an invariant mapping . in _",
    "2006 ieee computer society conference on computer vision and pattern recognition ( cvpr06 ) _ , volume  2 , pages 17351742 .",
    "ieee , 2006 .",
    "s.  jain , c.  hu , and j.  k. aggarwal .",
    "facial expression recognition with temporal modeling of shapes . in _",
    "computer vision workshops ( iccv workshops ) , 2011 ieee international conference on _ , pages 16421649 .",
    "ieee , 2011 .",
    "h.  jung , s.  lee , j.  yim , s.  park , and j.  kim .",
    "joint fine - tuning in deep neural networks for facial expression recognition . in _ proceedings of the ieee international conference on computer vision _ , pages 29832991 , 2015 .",
    "p.  khorrami , t.  paine , and t.  huang .",
    "do deep neural networks learn facial action units when doing expression recognition ? in _ proceedings of the ieee international conference on computer vision workshops _ ,",
    "pages 1927 , 2015 .",
    "m.  liu , s.  li , s.  shan , and x.  chen .",
    "au - aware deep networks for facial expression recognition . in _ automatic face and gesture recognition ( fg ) , 2013 10th ieee international conference and workshops on _ , pages 16 .",
    "ieee , 2013 .",
    "m.  liu , s.  li , s.  shan , r.  wang , and x.  chen . deeply learning deformable facial action parts model for dynamic expression analysis . in _ asian conference on computer vision _ , pages 143157 .",
    "springer , 2014 .",
    "m.  liu , s.  shan , r.  wang , and x.  chen . learning expressionlets on spatio - temporal manifold for dynamic facial expression recognition . in _ proceedings of the ieee conference on computer vision and pattern recognition _ , pages 17491756 , 2014 .",
    "p.  lucey , j.  f. cohn , t.  kanade , j.  saragih , z.  ambadar , and i.  matthews .",
    "the extended cohn - kanade dataset ( ck+ ) : a complete dataset for action unit and emotion - specified expression . in _ 2010 ieee computer society conference on computer vision and pattern recognition - workshops _ , pages 94101 .",
    "ieee , 2010 .",
    "m.  valstar and m.  pantic .",
    "induced disgust , happiness and surprise : an addition to the mmi facial expression database . in _ proc .",
    "3rd intern .",
    "workshop on emotion ( satellite of lrec ) : corpora for research on emotion and affect _",
    ", page  65 , 2010 .",
    "z.  wang , s.  wang , and q.  ji . capturing complex spatio - temporal relations among facial muscles for facial expression recognition . in _ proceedings of the ieee conference on computer vision and pattern recognition _ ,",
    "pages 34223429 , 2013 .",
    "l.  zhong , q.  liu , p.  yang , b.  liu , j.  huang , and d.  n. metaxas .",
    "learning active facial patches for expression analysis . in _",
    "computer vision and pattern recognition ( cvpr ) , 2012 ieee conference on _ , pages 25622569 .",
    "ieee , 2012 ."
  ],
  "abstract_text": [
    "<S> as the expressive depth of an emotional face differs with individuals , expressions , or situations , recognizing an expression using a single facial image at a moment is difficult . </S>",
    "<S> one of the approaches to alleviate this difficulty is using a video - based method that utilizes multiple frames to extract temporal information between facial expression images . in this paper </S>",
    "<S> , we attempt to utilize a generative image that is estimated based on a given single image . then , we propose to utilize a contrastive representation that explains an expression difference for discriminative purposes . </S>",
    "<S> the contrastive representation is calculated at the embedding layer of a deep network by comparing a single given image with a reference sample generated by a deep encoder - decoder network . </S>",
    "<S> consequently , we deploy deep neural networks that embed a combination of a generative model , a contrastive model , and a discriminative model . in our proposed networks , </S>",
    "<S> we attempt to disentangle a facial expressive factor in two steps including learning of a reference generator network and learning of a contrastive encoder network . </S>",
    "<S> we conducted extensive experiments on three publicly available face expression databases ( ck+ , mmi , and oulu - casia ) that have been widely adopted in the recent literatures . </S>",
    "<S> the proposed method outperforms the known state - of - the art methods in terms of the recognition accuracy . </S>"
  ]
}