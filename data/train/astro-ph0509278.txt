{
  "article_text": [
    "simd parallel processing is a very old idea , with several successful implementations such as illiac - iv@xcite , icl / amt dap , goodyear mpp@xcite , tmc cm-1/2@xcite , and infn / quadrics ape-100 and apemille .",
    "these are large machines made of up to 64k processors , each with its own local memory . except for the ape machines which were designed for lqcd calculation , all of these machines were built before 1990 .",
    "there is another form of simd architecture .",
    "almost all recent microprocessors have some form of simd processing units , with 4 or more arithmetic units .",
    "these include vis of sparc , altivec of powerpc , mvi of alpha , 3dnow ! of amd and mmx , sse , and sse2 of intel x86 .",
    "though both of these two architectures are called `` simd '' , the actual hardware implementation and programming models are completely different . in the former case of large - scale simd parallel machines ,",
    "each processing element has its own memory and the address generation unit for it , and they are connected through some routing network . in the latter case , the instruction set of a single processor is extended to handle a single long word as a vector of multiple short words .",
    "thus , essentially only the arithmetic units are duplicated , and they are connected to single memory unit ( or single l1 cache ) through a single datapath .",
    "the former architecture is for large machines made of a number of processing chips , each with one or few processors .",
    "the latter is for a single processor chip . with present - day technology we could in principle integrate thousands or more of processors used in machines like cm-1 into a single chip , and yet what is currently used is simd extensions with just a few arithmetic units .    in this paper , we describe a new architecture with which we can integrate a very large number of processors into a single chip and achieve reasonable efficiency for a wide range of applications .",
    "figure [ fig : basicsimd ] shows the basic architecture we discuss in this paper .",
    "it consists of a number of processing elements , each of which consists of an fpu and a register file .",
    "they all receive the same instruction from outside the chip , and perform the same operation .",
    "compared to the classic simd architecture such as that of tmc cm-2 , the main difference are the followings .",
    "\\a ) pes do not have large local memories .",
    "\\b ) there is no communication network between pes .",
    "we introduce these two simplifications so that a large number of pes can be integrated into a single chip .",
    "if we want to have a memory with meaningful size connected to each pe , the only economical way is to attach dram chips .",
    "a chip which integrates logics and dram memory can in principle be made , but the price per bit of the dram memory of such a chip is about two orders of magnitude higher compared to that of commercial dram chips .",
    "however , once we decide to use external memory , it becomes very difficult to integrate large number of processors into a chip .",
    "consider an example of a chip with 100 processors , each with single arithmetic unit . if the clock frequency is 1 ghz , the peak speed of the chip is 100 gflops . if we want to add the memory units which can supply one word per clock cycle to these 100 processors , we need the memory bandwidth of 800 gb / s , or around 100 times more than that of the latest microprocessors .",
    "clearly , it is not easy to achieve such a high memory bandwidth . if we eliminate the local memory of processors , we can integrate very large number of processors into a chip .",
    "a communication network , as far as it is limited into a single chip , is not very expensive",
    ". a two - dimensional mesh network would be quite natural , for physically two - dimensional array of pes on a single silicon chip .",
    "however , such a two - dimensional network poses a very hard problem , if we try to extend it to multi - chip systems . with current and near - future vlsi technology ,",
    "it is possible to integrate more than 1000 pes to a single chip , each with fully pipelined fpus .",
    "thus , a 2d array will have the dimensions of 32 by 32 , and the minimum number of external links necessary to construct a 2d network is @xmath0 .",
    "if we want to have , say , 16 wires per link , the total number of pins necessary is 2,048 .",
    "to make such a large number of pins work with reasonable data rate is not impossible , but very costly .",
    "if we eliminate the inter - pe communication network right from the beginning , we have no problem in constructing multi - chip systems , since pes in different chips need not be connected .",
    "thus , this simple architecture has two advantages .",
    "first , we can integrate a very large number of pes into a single chip .",
    "second , a system with multiple chips is easy to construct . as a result",
    ", peak performance of the system can be very high .",
    "important question here is if any real application can actually take advantage of this architecture .",
    "we consider several examples and extend the architecture in the next section .",
    "note that this simd processor works as an attached processor to general - purpose cpu .",
    "since the on - chip memory is limited to just the register files of pes , the simd processor itself can not run any application which requires large amount of memory .",
    "thus , we need to move only the part of the application program which can be efficiently done on the simd processor .",
    "this of course means there will be communication overhead .",
    "before we proceed to the next section , we need a name for the proposed architecture .",
    "since the main difference between the proposed architecture and previous simd architecture is the removal of elements like local memory and inter - pe network , we call this architecture greatly reduced array of processor elements , or grape",
    ". the similarity of this name to the grape for astronomical @xmath1-body simulations@xcite is a pure coincidence .",
    "in many particle - based simulations such as classical md or astrophysical @xmath1-body simulations , the most expensive part of calculation is the evaluation of particle - particle interactions . in general",
    ", it has the form @xmath2 where @xmath3 denotes the variables associated with particle @xmath4 , @xmath5 is some generalized `` force '' from particle @xmath6 to particle @xmath4 , and @xmath7 is the total `` force '' on particle @xmath4 . at least formally , the summation is taken over all particles in the system . therefore the calculation cost is @xmath8 , where @xmath1 is the total number of particles in the system . in some cases the interaction is of short - range nature and",
    "we can apply some cut - off length .",
    "if the interaction is long - ranged , we might be able to use approximate algorithms such as fmm@xcite or barnes - hut tree@xcite .    in these cases , however , the most expensive part is still the direct evaluation of equation ( [ eq : pp ] ) .",
    "the basic simd structure we discussed in the previous section is quite suited to calculations of this type . in the simplest case , we first load data of particles on which we calculate the interaction to the registers of pes . in other words , we first write one @xmath3 to each pe",
    ". then we broadcast one @xmath9 to all pes and let then calculate the force from this particle @xmath6 to their particles .",
    "we repeat sending particles @xmath9 until all particles are sent , and then read the calculated results @xmath7 in each pe .",
    "remaining calculations such as the time integration of particles are done on the host computer which controls the simd processor .    in this way ,",
    "the size of the system is not limited by the size of the simd processor array .",
    "also , since the program , except for that for the evaluation of particle - particle interaction , runs completely on the host computer , it is relatively easy to adopt existing programs to take advantage of the array processor .",
    "if the number of particles is much smaller than the number of pes , the efficiency would become low .",
    "even when the total number of the particles is large , if the interaction is short - ranged , the number of particles with which one particle physically interact can be much smaller than the number of pes .",
    "this problem can be solved in many different ways .",
    "one possibility is to organize the processors into groups , as shown in figure [ fig : ijparallelsimd ] .        in this modified architecture",
    ", pes are organized into groups , each with small buffer memory .",
    "these groups are connected to a reduction network .",
    "the host computer can either write data to individual buffer memories or broadcast the same data to all buffer memories . in this way , pes in different groups can calculate the forces from different particles .",
    "in addition , the reduction network allows multiple pes in different groups to calculate the force on the same particle from different particles .",
    "thus , the efficiency for small-@xmath1 systems or short - range force is greatly improved .",
    "note that the hardware cost of the buffer memory and reduction network is very small , since their cost is proportional to the number of groups , which is a small fraction of the number of pes .",
    "for any dense - matrix algorithms , the basic operation is matrix multiplication @xmath10 .",
    "thus , the key question is if our proposed architecture can achieve reasonable performance for matrix multiplication .",
    "we consider the modified architecture discussed in the previous section . in this architecture , it is easy to implement parallel matrix multiplication . the basic idea is to block - subdivide the matrix @xmath11 into two dimensions in the same way as in the standard canon s algorithm and load them to each pe .",
    "then we take one column of @xmath12 and divide it to @xmath13 pieces , where @xmath13 is the number of groups , and send these pieces to the buffer memories of group .",
    "we then calculate @xmath14 on each pe on each group . by taking summation over groups ,",
    "we obtain one row of @xmath15 .",
    "we have changed the basic simd architecture by grouping pes and adding the buffer memories and the reduction network .",
    "therefore , we call this architecture grape - dr , or greatly reduced array of processor elements with data reduction .",
    "modern gpus and some dsps have the simd architecture very similar to the basic architecture we described in section [ sect : basic ] .",
    "most of them , however , are designed to perform a fixed ( though relatively large ) number of operations per data .",
    "for example , many dsps are designed to perform , say , multiple independent 1k - point fft operations in parallel . thus , the ratio between the calculation speed and the external memory bandwidth must be kept constant , and the peak performance of these systems are generally limited by the available memory bandwidth . in our proposed architecture , we do not intend to run the applications which require large memory bandwidth , and thus the peak performance is not limited by the memory bandwidth .    in this paper",
    ", we describe a modified simd architecture suitable both for a single - chip implementation and for large systems made of a number of such chips .",
    "it consists of a number of extremely simple yet fully programmable processors , connected through hierarchical broadcast / reduction network . in a sense , it is similar to simd parallel processors , but unlike the previous simd machines , the local memory of pe is reduced to small amount , and only the most compute - intensive part of the application will be run on the simd processor array .",
    "a machine base on this architecture is currently under development@xcite .",
    "the author thanks kei hiraki and mary inaba for helpful discussions and useful comments on the manuscript .",
    "he also thanks toshiyuki fukushige , yoko funato , piet hut , toshikazu ebisuzaki , and makoto taiji for discussions related to this work .",
    "this research is partially supported by the special coordination fund for promoting science and technology ( grape - dr project ) , ministry of education , culture , sports , science and technology , japan ."
  ],
  "abstract_text": [
    "<S> we describe a modified simd architecture suitable for single - chip integration of a large number of processing elements , such as 1,000 or more . </S>",
    "<S> important differences from traditional simd designs are :    \\a ) the size of the memory per processing elements is kept small .    </S>",
    "<S> \\b ) the processors are organized into groups , each with a small buffer memory . </S>",
    "<S> reduction operation over the groups is done in hardware .    </S>",
    "<S> the first change allows us to integrate a very large number of processing elements into a single chip . </S>",
    "<S> the second change allows us to achieve a close - to - peak performance for many scientific applications like particle - based simulations and dense - matrix operations . </S>"
  ]
}