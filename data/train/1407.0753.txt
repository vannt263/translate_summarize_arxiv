{
  "article_text": [
    "in this paper , we consider the following optimization problem : @xmath6 where @xmath2 is a linear map from @xmath7 to @xmath8 , @xmath1 is a proper closed function on @xmath8 and @xmath0 is twice continuously differentiable on @xmath7 with a bounded hessian .",
    "we also assume that the proximal ( set - valued ) mappings @xmath9 are well - defined and are simple to compute for all @xmath10 and for any @xmath4 . here",
    ", @xmath11 denotes the set of minimizers , and the simplicity is understood in the sense that _ at least one _ element of the set of minimizers can be obtained efficiently .",
    "concrete examples of such @xmath1 that arise in applications include functions listed in ( * ? ? ?",
    "* table  1 ) , the @xmath5 regularization @xcite , the @xmath12 regularization , and the indicator functions of the set of vectors with cardinality at most @xmath13 @xcite , matrices with rank at most @xmath14 and @xmath13-sparse vectors in simplex @xcite , etc .",
    "moreover , for a large class of nonconvex functions , a general algorithm has been proposed recently in @xcite for computing the proximal mapping .",
    "the model problem with @xmath0 and @xmath1 satisfying the above assumptions encompasses many important applications in engineering and machine learning ; see , for example , @xcite .",
    "in particular , many sparse learning problems are in the form of with @xmath0 being a loss function , @xmath2 being the identity map and @xmath1 being a regularizer ; see , for example , @xcite for the use of the @xmath12 norm as a regularizer , @xcite for the use of the @xmath15 norm , @xcite for the use of the nuclear norm , and @xcite and the references therein for the use of various continuous difference - of - convex functions with simple proximal mappings . for the case when @xmath2 is not the identity map , an application in stochastic realization where @xmath0 is a least squares loss function , @xmath1 is the rank function and @xmath2 is the linear map that takes the variable @xmath16 into a block hankel matrix was discussed in ( * ? ? ?",
    "* section  ii ) .",
    "when @xmath2 is the identity map , the proximal gradient algorithm @xcite ( also known as forward - backward splitting algorithm ) can be applied whose subproblem involves a computation of the proximal mapping of @xmath3 for some @xmath4 .",
    "it is known that when @xmath0 and @xmath1 are convex , the sequence generated from this algorithm is convergent to a globally optimal solution if the step - size is chosen from @xmath17 , where @xmath18 is any number larger than the lipschitz continuity modulus of @xmath19 . for nonconvex @xmath0 and @xmath1",
    ", the step - size can be chosen from @xmath20 so that any cluster point of the sequence generated is stationary ( * ? ? ?",
    "* proposition  2.3 ) ( see section  [ sec2 ] for the definition of stationary points ) , and convergence of the whole sequence is guaranteed if the sequence generated is bounded and @xmath21 satisfies the kurdyka - ojasiewicz ( kl ) property ( * ? ? ? * theorem  5.1 , remark  5.2(a ) ) . on the other hand , when @xmath2 is a general linear map so that the computation of the proximal mapping of @xmath22 , @xmath4 , is not necessarily simple , the proximal gradient algorithm can not be applied efficiently . in the case when @xmath0 and @xmath1 are both convex",
    ", one feasible approach is to apply the alternating direction method of multipliers ( admm ) @xcite .",
    "this has been widely used recently ; see , for example @xcite .",
    "while it is tempting to directly apply the admm to the nonconvex problem , convergence has only been shown under specific assumptions .",
    "in particular , in @xcite , the authors studied an application that can be modeled as with @xmath23 , @xmath1 being some risk measures and @xmath2 typically being an injective linear map coming from data .",
    "they showed that any cluster point gives a stationary point , assuming square summability of the successive changes in the dual iterates .",
    "more recently , in @xcite , the authors considered the case when @xmath0 is a nonconvex quadratic and @xmath1 is the sum of the @xmath24 norm and the indicator function of the euclidean norm ball .",
    "they showed that if the penalty parameter is chosen sufficiently large ( with an explicit lower bound ) and the dual iterates satisfy a particular assumption , then any cluster point gives a stationary point .",
    "in particular , their assumption is satisfied if @xmath2 is surjective .    motivated by the findings in @xcite , in this paper , we focus on the case when @xmath2 is surjective and consider both the admm ( for a general surjective @xmath2 ) and the proximal gradient algorithm ( for @xmath2 being the identity ) .",
    "the contributions of this paper are as follows :    * first , we characterize cluster points of the sequence generated from the admm .",
    "in particular , we show that if the ( fixed ) penalty parameter in the admm is chosen sufficiently large ( with a computable lower bound ) , and a cluster point of the sequence generated exists , then it gives a stationary point of problem .",
    "moreover , our analysis allows replacing @xmath0 in the admm subproblems by its local quadratic approximations so that in each iteration of this variant , the subproblems only involve computing the proximal mapping of @xmath3 for some @xmath4 and solving an unconstrained convex quadratic minimization problem .",
    "furthermore , we also give simple sufficient conditions to guarantee the boundedness of the sequence generated .",
    "these conditions are satisfied in a wide range of applications ; see examples  [ examplenew:3 ] , [ examplenew:1 ] and [ examplenew:2 ] .",
    "* second , under the additional assumption that @xmath0 and @xmath1 are semi - algebraic functions , we show that if a cluster point of the sequence generated from the admm exists , it is actually convergent .",
    "our assumption on semi - algebraicity not only can be easily verified or recognized , but also covers a broad class of optimization problems such as problems involving quadratic functions , polyhedral norms and the cardinality function . *",
    "third , we give a concrete 2-dimensional counterexample in example  [ ex7:nonconverge ] showing that the admm can be divergent when @xmath2 is assumed to be injective ( instead of surjective ) .",
    "* finally , for the particular case when @xmath2 equals the identity map , we show that the proximal gradient algorithm can be applied with a slightly more flexible step - size rule when @xmath0 is nonconvex ( see theorem  [ prop : prox ] for the precise statement ) .",
    "the rest of the paper is organized as follows .",
    "we discuss notation and preliminary materials in the next section .",
    "convergence of the admm is analyzed in section  [ sec : msur ] , and section  [ sec : mi ] is devoted to the analysis of the proximal gradient algorithm .",
    "some numerical results are presented in section  [ sec : num ] to illustrate the algorithms .",
    "we give concluding remarks and discuss future research directions in section  [ sec : con ] .",
    "we denote the @xmath25-dimensional euclidean space as @xmath7 , and use @xmath26 to denote the inner product and @xmath27 to denote the norm induced from the inner product .",
    "linear maps are denoted by scripted letters .",
    "the identity map is denoted by @xmath28 . for a linear map @xmath2",
    ", @xmath29 denotes the adjoint linear map with respect to the inner product and @xmath30 is the induced operator norm of @xmath2 .",
    "a linear self - map @xmath31 is called symmetric if @xmath32 . for a symmetric linear self - map @xmath31",
    ", we use @xmath33 to denote its induced quadratic form given by @xmath34 for all @xmath16 , and use @xmath35 ( resp . , @xmath36 ) to denote the maximum ( resp .",
    ", minimum ) eigenvalue of @xmath31 .",
    "a symmetric linear self - map @xmath31 is called positive semidefinite , denoted by @xmath37 ( resp . ,",
    "positive definite , @xmath38 ) if @xmath39 ( resp .",
    ", @xmath40 ) for all nonzero @xmath16 . for two symmetric linear self - maps @xmath41 and @xmath42",
    ", we use @xmath43 ( resp . , @xmath44 ) to denote @xmath45 ( resp . , @xmath46 ) .",
    "an extended - real - valued function @xmath47 is called proper if it is finite somewhere and never equals @xmath48 .",
    "such a function is called closed if it is lower semicontinuous .",
    "given a proper function @xmath49 $ ] , we use the symbol @xmath50 to indicate @xmath51 and @xmath52 .",
    "the domain of @xmath47 is denoted by @xmath53 and is defined as @xmath54 .",
    "our basic _ subdifferential _ of @xmath47 at @xmath55 ( known also as the limiting subdifferential ) is defined by ( see , for example , ( * ? ? ?",
    "* definition  8.3 ) ) @xmath56 it follows immediately from the above definition that this subdifferential has the following robustness property : @xmath57 for a convex function @xmath47 the subdifferential reduces to the classical subdifferential in convex analysis ( see , for example , ( * ? ? ?",
    "* theorem 1.93 ) ) @xmath58 moreover , for a continuously differentiable function @xmath47 , the subdifferential reduces to the derivative of @xmath47 denoted by @xmath59 . for a function @xmath47 with more than one group of variables",
    ", we use @xmath60 ( resp . , @xmath61 ) to denote the subdifferential ( resp . ,",
    "derivative ) of @xmath47 with respect to the variable @xmath16 .",
    "furthermore , we write @xmath62 . in general , the subdifferential set can be nonconvex ( e.g. , for @xmath63 at @xmath64 ) while @xmath65 enjoys comprehensive calculus rules based on _ variational / extremal principles _ of variational analysis @xcite .",
    "in particular , when @xmath2 is a surjective linear map , using ( * ? ? ? * exercise  8.8(c ) ) and ( * ? ? ? * exercise  10.7 ) , we see that @xmath66 for any @xmath67 .",
    "hence , at an optimal solution @xmath68 , the following necessary optimality condition always holds : @xmath69 throughout this paper , we say that @xmath70 is a stationary point of if @xmath70 satisfies in place of @xmath68 .    for a continuously differentiable function @xmath71 on @xmath7 ,",
    "the bregman distance @xmath72 is defined as @xmath73 for any @xmath74 , @xmath75 . if @xmath71 is twice continuously differentiable and there exists @xmath76 so that the hessian @xmath77 satisfies @xmath78 ^ 2\\preceq \\q$ ] for all @xmath16 , then for any @xmath74 and @xmath79 in @xmath7 , we have @xmath80 dt\\right\\|^2\\\\    & \\le \\left(\\int_0 ^ 1\\left\\|\\nabla^2\\phi(x_2 + t(x_1 - x_2))\\cdot[x_1 - x_2]\\right\\| dt\\right)^2\\\\    & = \\left(\\int_0 ^ 1\\sqrt{\\langle x_1-x_2,[\\nabla^2\\phi(x_2 + t(x_1 - x_2))]^2\\cdot[x_1 - x_2]\\rangle } dt\\right)^2 \\le \\|x_1 - x_2\\|^2_{\\q}. \\end{split}\\ ] ] on the other hand , if there exists @xmath76 so that @xmath81 for all @xmath16 , then @xmath82\\rangle ds \\",
    "\\ge \\frac12 \\|x_1 - x_2\\|^2_{\\q } \\end{split}\\ ] ] for any @xmath74 and @xmath79 in @xmath7 .",
    "a semi - algebraic set @xmath83 is a finite union of sets of the form @xmath84 where @xmath85 and @xmath86 are polynomials with real coefficients in @xmath25 variables .",
    "in other words , @xmath87 is a union of finitely many sets , each defined by finitely many polynomial equalities and strict inequalities .",
    "a map @xmath88 is semi - algebraic if @xmath89 is a semi - algebraic set .",
    "semi - algebraic sets and semi - algebraic mappings enjoy many nice structural properties .",
    "one important property which we will use later on is the kurdyka - ojasiewicz ( kl ) property .",
    "[ def : kl ] * ( kl property & kl function ) * a proper function @xmath47 is said to have the kurdyka - ojasiewicz ( kl ) property at @xmath90 if there exist @xmath91 $ ] , a neighborhood @xmath92 of @xmath93 and a continuous concave function @xmath94 such that :    1 .",
    "@xmath95 and @xmath96 is continuously differentiable on @xmath97 with positive derivatives ; 2 .   for all @xmath98 satisfying @xmath99",
    ", it holds that @xmath100    a proper closed function @xmath47 satisfying the kl property at all points in @xmath101 is called a kl function .",
    "it is known that a proper closed semi - algebraic function is a kl function as such a function satisfies the kl property for all points in @xmath101 with @xmath102 for some @xmath103 and some @xmath104 ( for example , see ( * ? ? ?",
    "* section  4.3 ) ; further discussion can be found in ( * ? ? ?",
    "* corollary  16 ) and ( * ? ? ?",
    "* section  2 ) ) .",
    "in this section , we study the alternating direction method of multipliers for finding a stationary point of . to describe the algorithm , we first reformulate as @xmath105 to decouple the linear map and the nonsmooth part .",
    "recall that the augmented lagrangian function for the above problem is defined , for each @xmath106 , as : @xmath107 our algorithm is then presented as follows : +   + notice that the first subproblem is essentially computing the proximal mapping of @xmath3 for some @xmath4 .",
    "the above algorithm is called the proximal admm since , in the second subproblem , we allow a proximal term @xmath72 and hence a choice of @xmath71 to simplify this subproblem . if @xmath108 , then this algorithm reduces to the usual admm described in , for example , @xcite . for other popular non - trivial choices of @xmath71 ,",
    "see remark  [ rem0 ] below .",
    "we next study global convergence of the above algorithm under suitable assumptions .",
    "specifically , we consider the following assumption .",
    "[ assumption ]    1 .",
    "@xmath109 for some @xmath110 ; and there exist @xmath111 , @xmath112 such that for all @xmath16 , @xmath113 .",
    "2 .   @xmath106 and @xmath71 are chosen so that * there exist @xmath114 so that @xmath115 ^ 2\\succeq \\t^2_2 $ ] for all @xmath16 ; * @xmath116 for some @xmath117 ; * with @xmath118 ^ 2 $ ] for all @xmath16 , there exists @xmath119 so that @xmath120    * ( comments on assumption 1)*[rem0 ] point ( i ) says @xmath2 is surjective . the first and second points in ( ii ) would be satisfied if @xmath121 is chosen to be @xmath122 , where @xmath18 is at least as large as the lipschitz continuity modulus of @xmath123 . in this case , one can pick @xmath124 and @xmath125 .",
    "this choice is of particular interest since it simplifies the @xmath16-update in to a convex quadratic programming problem ; see ( * ? ? ?",
    "* section  2.1 ) . indeed , under this choice , we have @xmath126 and hence the second subproblem becomes @xmath127 finally , point 3 in ( ii )",
    "can always be enforced by picking @xmath128 sufficiently large if @xmath71 , @xmath41 and @xmath42 , are chosen independently of @xmath128 .",
    "in addition , in the case where @xmath129 and hence @xmath125 , it is not hard to show that the requirement that @xmath130 for some @xmath119 is indeed equivalent to imposing @xmath131 .    before stating our convergence results , we note first that from the optimality conditions , the iterates generated satisfy @xmath132 hence , if @xmath133 and if for a cluster point @xmath134 of the sequence @xmath135 , we have @xmath136 along a convergent subsequence @xmath137 that converges to @xmath134 , then @xmath138 is a stationary point of . to see this , notice from and the definition of @xmath139 that @xmath140 passing to the limit in along the subsequence @xmath137 and invoking , and , it follows that @xmath141 in particular , @xmath138 is a stationary point of the model problem .",
    "we now state our global convergence result .",
    "our first conclusion establishes under assumption  [ assumption ] , and so , any cluster point of the sequence generated from the proximal admm produces a stationary point of our model problem such that holds . in the case where @xmath0 is a nonconvex quadratic function with a negative semi - definite hessian matrix and @xmath1 is the sum of the @xmath15 norm and the indicator function of the euclidean norm ball , the convergence of the admm ( i.e. , proximal admm with @xmath142 ) was established in @xcite .",
    "our convergence analysis below follows the recent work in ( * ? ? ?",
    "* section  3.3 ) and @xcite .",
    "specifically , we follow the idea in @xcite to study the behavior of the augmented lagrangian function along the sequence generated from the proximal admm ; we note that this was subsequently also used in ( * ? ? ?",
    "* section  3.3 ) .",
    "we then bound the changes in @xmath143 by those of @xmath144 , following the brilliant observation in ( * ? ? ?",
    "* section  3.3 ) that the changes in the dual iterates can be controlled by the changes in the primal iterates that correspond to the quadratic in their objective .",
    "however , we would like to point out two major modifications : * ( i ) * the proof in ( * ? ? ? * section  3.3 ) can not be directly applied because our subproblem corresponding to the @xmath145-update is not convex due to the possible nonconvexity of @xmath1 .",
    "our analysis is also complicated by the introduction of the proximal term . *",
    "( ii ) * using the special structure of their problem , the authors in ( * ? ? ?",
    "* section  3.3 ) established that the augmented lagrangian for their problem is uniformly bounded below along the sequence generated from their admm .",
    "in contrast , we _ assume _ existence of cluster points in our convergence analysis below and will discuss sufficient conditions for such an assumption in theorem [ thm : boundedness2 ] .",
    "on the other hand , we have to point out that although our sufficient conditions for boundedness of sequence are general enough to cover a wide range of applications , they do _ not _ cover the particular problem studied in @xcite .",
    "our second conclusion , which is new in the literature studying convergence of admm in the nonconvex scenarios , states that if the algorithm is suitably initialized , we can get a strict improvement in the objective values .",
    "in particular , if suitably initialized , one will not end up with a stationary point with a larger objective value .",
    "[ thm : main ] suppose that assumption  [ assumption ] holds .",
    "then we have the following results .    1 .   *",
    "( global subsequential convergence ) * if the sequence @xmath135 generated from the proximal admm has a cluster point @xmath134 , then holds .",
    "moreover , @xmath138 is a stationary point of such that holds .",
    "( strict improvement in objective values ) * suppose that the algorithm is initialized at a non - stationary @xmath146 with @xmath147 , and @xmath148 satisfying @xmath149 .",
    "then for any cluster point @xmath134 of the sequence @xmath135 , if exists , we have @xmath150    [ rem1 ] the proximal admm does not necessarily guarantee that the objective value of is decreasing along the sequence @xmath144 generated",
    ". however , under the assumptions in theorem  [ thm : main ] , any cluster point of the sequence generated from the proximal admm improves the starting ( non - stationary ) objective value .",
    "we now describe one way of choosing the initialization",
    "as suggested in ( ii ) when @xmath1 is nonconvex . in this case , it is common to approximate @xmath1 by a proper closed convex function @xmath151 and obtain a relaxation to , i.e. , @xmath152 then any stationary point @xmath70 of this relaxed problem , if exists , satisfies @xmath153 .",
    "thus , if @xmath154 , then one can initialize the proximal admm by taking @xmath155 and @xmath156 with @xmath157 , so that the conditions in ( ii ) are satisfied .    we start by showing that holds .",
    "first , observe from the second relation in that @xmath158 consequently , we have @xmath159 taking norm on both sides , squaring and making use of ( i ) in assumption  [ assumption ] , we obtain further that @xmath160 where @xmath161 is defined in point 3 in ( ii ) of assumption  [ assumption ] , and we made use of the relation @xmath162 for the first inequality , while the last inequality follows from points 1 and 3 in ( ii ) of assumption  [ assumption ] , and .",
    "on the other hand , from the definition of @xmath139 , we have @xmath163 which implies @xmath164 in view of and , to establish , it suffices to show that @xmath165    we now prove .",
    "we start by noting that @xmath166 next , recall from ( * ? ? ?",
    "* page  553 ,  ex.17 ) that the operation of taking positive square root preserves the positive semidefinite ordering .",
    "thus , point 1 in ( ii ) of assumption  [ assumption ] implies that @xmath167 for all @xmath16 . from this and point 2 in ( ii ) of assumption  [ assumption ] ,",
    "we see further that the function @xmath168 is strongly convex with modulus at least @xmath169 . using this , the definition of @xmath170 ( as a minimizer ) and , we have @xmath171 moreover , using the definition of @xmath172 as a minimizer , we have @xmath173 summing , and",
    ", we obtain that @xmath174 summing the above relation from @xmath175 with @xmath176 , we see that @xmath177 where @xmath178 due to point 3 in ( ii ) of assumption  [ assumption ] ; and the last inequality follows from @xmath179 .    now , suppose that @xmath134 is a cluster point of the sequence @xmath135 and consider a convergent subsequence , i.e. , @xmath180 from lower semicontinuity of @xmath18 , we see that @xmath181 where the last inequality follows from the properness assumption on @xmath1 . on the other hand , putting @xmath182 and @xmath183 in , we see that @xmath184 passing to the limit in and making use of and ( ii ) in assumption  [ assumption ] , we conclude that @xmath185 the desired relation now follows from this and the fact that @xmath186 .",
    "consequently , holds .",
    "we next show that holds along the convergent subsequence in . indeed , from the definition of @xmath187 ( as a minimizer )",
    ", we have @xmath188 taking limit and using , we see that @xmath189 on the other hand , from lower semicontinuity , and , we have @xmath190 the above two relations show that @xmath191 .",
    "this together with and the discussions preceding this theorem shows that @xmath138 is a stationary point of and that holds .",
    "this proves ( i ) .",
    "next , we suppose that the algorithm is initialized at a non - stationary @xmath146 with @xmath147 and @xmath148 chosen with @xmath149 ; we also write @xmath192 .",
    "we first show that @xmath193 . to this end",
    ", we notice that @xmath194 proceeding as in , we have @xmath195 on the other hand , combining the relations @xmath196 and @xmath192 , we see that @xmath197 consequently , if @xmath198 , then it follows from and that @xmath199 and @xmath200 .",
    "this together with implies that @xmath201 i.e. , @xmath146 is a stationary point .",
    "since @xmath146 is non - stationary by assumption , we must have @xmath202 .",
    "we now derive an upper bound on @xmath203 for any @xmath204 . to this end , using the definition of augmented lagrangian function , the @xmath205-update and , we have @xmath206 combining this relation with and , we obtain the following estimate @xmath207 on the other hand , by specializing to @xmath208 and recalling that @xmath209 , we see that @xmath210 combining , and the definition of @xmath211 , we obtain @xmath212 where the strictly inequality follows from the fact that @xmath193 , and the fact that @xmath209 .",
    "the conclusion of the theorem now follows by taking limit in the above inequality along any convergent subsequence , and noting that @xmath192 by assumption , and that @xmath213 .",
    "we illustrate in the following examples how the parameters can be chosen in special cases .",
    "[ example:3 ] suppose that @xmath214 and that @xmath19 is lipschitz continuous with modulus bounded by @xmath18 .",
    "then one can take @xmath215 and @xmath216 .",
    "moreover , assumption  [ assumption](i ) holds with @xmath217 .",
    "furthermore , one can take @xmath218 so that @xmath219 , @xmath125 and @xmath220 .",
    "for the second and third points of assumption  [ assumption](ii ) to hold , one can choose @xmath221 and then @xmath128 can be chosen so that @xmath222 and that @xmath223 these can be achieved by picking @xmath224",
    ".    [ example:4 ] suppose again that @xmath214 and @xmath225 for some linear map @xmath226 and vector @xmath227",
    ". then one can take @xmath142 so that @xmath228 , and @xmath229 , @xmath230 , @xmath220 , where @xmath231 .",
    "observe that assumption  [ assumption](i ) holds with @xmath217 .",
    "for the second and third points of assumption  [ assumption](ii ) to hold , we only need to pick @xmath128 so that @xmath232 , i.e. , @xmath233 , while @xmath234 can be any number chosen from @xmath235 .",
    "[ example:5 ] suppose that @xmath2 is a general surjective linear map and @xmath0 is strongly convex .",
    "specifically , assume that @xmath236 for some @xmath93 so that @xmath237",
    ". then we can take @xmath108 and hence @xmath238 , @xmath239 .",
    "assumption  [ assumption](i ) holds with @xmath240 .",
    "the second point of assumption  [ assumption](ii ) holds with @xmath241 .",
    "for the third point to hold , it suffices to pick @xmath242 , while @xmath234 can be any number chosen from @xmath243 .",
    "we next give some sufficient conditions under which the sequence @xmath135 generated from the proximal admm under assumption  [ assumption ] is bounded .",
    "this would guarantee the existence of cluster point , which is the assumption required in theorem  [ thm : main ] .    *",
    "( boundedness of sequence generated from the proximal admm)*[thm : boundedness2 ] suppose that assumption  [ assumption ] holds , and @xmath128 is further chosen so that there exists @xmath244 with @xmath245 suppose that either    1 .   @xmath2 is invertible and @xmath246 ; or 2 .",
    "@xmath247 and @xmath248 .",
    "then the sequence @xmath135 generated from the proximal admm is bounded .",
    "first , observe from that @xmath249 where the last inequality follows from point 3 in ( ii ) of assumption  [ assumption ] .",
    "in particular , the sequence @xmath250 is decreasing and consequently , we have , for all @xmath251 , that @xmath252 next , recall from that @xmath253 plugging this into , we see further that @xmath254 where @xmath255 , and @xmath256 is chosen so that @xmath257 , i.e. , @xmath258 .    now",
    ", suppose that the conditions in ( i ) hold .",
    "note that @xmath246 implies @xmath248 .",
    "this together with and @xmath257 implies that @xmath259 , @xmath260 , and @xmath261 are bounded .",
    "boundedness of @xmath143 follows from these and .",
    "moreover , the boundedness of @xmath144 follows from the boundedness of @xmath259 , @xmath143 , the invertibility of @xmath2 and the third relation in .",
    "next , consider the conditions in ( ii ) . since @xmath1 is bounded below , and the coerciveness of @xmath262 give the boundedness of @xmath144 .",
    "the boundedness of @xmath143 follows from this and .",
    "finally , the boundedness of @xmath259 follows from these and the third relation in .",
    "this completes the proof .",
    "notice that in order to guarantee boundedness of the sequence generated from the proximal admm , we have to choose @xmath128 to satisfy both assumption  [ assumption ] and .",
    "we illustrate the conditions in theorem  [ thm : boundedness2 ] in the next few examples .",
    "in particular , we shall see that such a choice of @xmath128 does exist in the following examples .",
    "[ examplenew:3 ] consider the problem in example  [ example:3 ] , and suppose in addition that @xmath263 for some linear map @xmath226 and vector @xmath227 , and that @xmath1 is coercive , i.e. , @xmath246 .",
    "this includes the model of @xmath264 regularization considered in @xcite . since @xmath265",
    ", we have @xmath266 where @xmath231 .",
    "thus , holds with @xmath217 and @xmath267 , where @xmath221 .",
    "hence , the sequence generated from the proximal admm is bounded , according to theorem  [ thm : boundedness2 ] ( i ) .",
    "[ examplenew:1 ] consider the problem in example  [ example:4 ] , and suppose in addition that @xmath1 is coercive , i.e. , @xmath246 .",
    "this covers the model of @xmath264 regularization considered in @xcite .",
    "we show that @xmath135 is bounded by verifying the conditions in theorem  [ thm : boundedness2 ] .",
    "indeed , we have from that holds with @xmath217 and @xmath268 ; recall that @xmath231 and @xmath234 can be chosen from @xmath235 in this example .",
    "the conclusion now follows from theorem  [ thm : boundedness2 ] ( i ) .",
    "[ examplenew:2 ] consider the problem in example  [ example:5 ] , and assume in addition that @xmath248 .",
    "we show that @xmath135 is bounded by showing that holds for our choice of @xmath128 .",
    "the conclusion will then follow from theorem  [ thm : boundedness2 ] ( ii ) .    to this end , note that @xmath269 and thus @xmath270 thus , holds with @xmath271 ; recall that @xmath234 can be chosen from @xmath243 in this example .",
    "we further comment on the condition .",
    "in particular , we shall argue that for a fairly large class of twice continuously differentiable function @xmath0 with a bounded hessian , there exists @xmath272 so that @xmath273 actually , let @xmath0 be a twice continuously differentiable function with a bounded hessian and @xmath274 .",
    "then it is well known that @xmath275 where @xmath18 is a lipschitz continuity modulus of @xmath123 .",
    "we include a simple proof for the convenience of the readers .",
    "indeed , @xmath276 where the first inequality follows from the fact that @xmath0 is bounded from below by @xmath277 , and the second inequality follows from the fact that the gradient is lipschitz continuous with modulus @xmath18 .",
    "consequently , for a twice continuously differentiable function @xmath0 with a bounded hessian , the condition holds for some @xmath278 if and only if @xmath0 is bounded below .",
    "we now study convergence of the whole sequence generated by the admm ( i.e. , proximal admm with @xmath142 ) when the objective function is semi - algebraic .",
    "the proof of this theorem relies heavily on the kl property . for recent applications of kl property to convergence analysis of a broad class of optimization methods ,",
    "see @xcite .",
    "we would like to point out that our analysis is adapted from @xcite , and we can not directly apply the results there since some of their assumptions are not satisfied in our settings .",
    "we will further comment on this in remark  [ rem4 ] .    *",
    "( global convergence for the whole sequence)*[th:2 ] suppose that assumption  [ assumption ] holds with @xmath129 ( and hence @xmath142 ) , and that @xmath0 and @xmath1 are semi - algebraic functions .",
    "suppose further that the sequence @xmath135 generated from the admm has a cluster point @xmath134 .",
    "then the sequence @xmath135 converges to @xmath134 and @xmath138 is a stationary point of .",
    "moreover , @xmath279    the conclusion that @xmath138 is a stationary point of follows from theorem  [ thm : main ] . moreover , holds .",
    "we now establish convergence .",
    "first , consider the subdifferential of @xmath280 at @xmath281 .",
    "specifically , we have @xmath282 where the last two equalities follow from the second and third relations in .",
    "similarly , @xmath283 since @xmath284 from . the above relations together with the assumption that @xmath285 and imply the existence of a constant @xmath286 so that @xmath287 moreover , from and @xmath129 ( and hence @xmath125 ) , we see that @xmath288 for some @xmath289 . in particular , @xmath290 is decreasing .",
    "since @xmath280 is also bounded below along the subsequence in , we conclude that @xmath291 exists .",
    "we now show that @xmath292 ; here , we write @xmath293 for notational simplicity . to this end ,",
    "notice from the definition of @xmath172 as a minimizer that @xmath294 using this relation , and the continuity of @xmath280 with respect to the @xmath16 and @xmath205 variables , we have @xmath295 where @xmath296 is a subsequence that converges to @xmath134 .",
    "on the other hand , from , we see that @xmath297 also converges to @xmath134 .",
    "this together with the lower semicontinuity of @xmath280 imply @xmath298 combining , and the existence of @xmath299 , we conclude that @xmath300 as claimed .",
    "furthermore , if @xmath301 for some @xmath251 , since the sequence is decreasing , we must have @xmath302 for all @xmath303 . from , we see that @xmath304 and hence @xmath305 from the fact that @xmath129 and , for all @xmath303 .",
    "consequently , we conclude from that @xmath306 for all @xmath307 , meaning that the algorithm terminates finitely .",
    "since the conclusion of this theorem holds trivially if the algorithm terminates finitely , from now on , we only consider the case where @xmath308 for all @xmath251 .",
    "next , notice that the function @xmath309 is semi - algebraic due to the semi - algebraicity of @xmath0 and @xmath1 .",
    "thus , it is a kl function from ( * ? ? ?",
    "* section  4.3 ) . from the property of kl functions , there exist @xmath310 , a neighborhood @xmath92 of @xmath134 and a continuous concave function @xmath94 as described in definition  [ def : kl ] so that for all @xmath311 satisfying @xmath312 , we have @xmath313 pick @xmath314 so that @xmath315 and set @xmath316 . from the second relation in and , we obtain for any @xmath251 that @xmath317",
    "hence @xmath318 whenever @xmath319 and @xmath251 .",
    "moreover , from the definition of @xmath139 and , we see that whenever @xmath251 , @xmath320 since there exists @xmath321 so that for all @xmath322 , we have @xmath323 ( such an @xmath324 exists due to ) , it follows that @xmath325 whenever @xmath319 and @xmath322 .",
    "thus , if @xmath319 and @xmath322 , we have @xmath326 .",
    "moreover , it is not hard to see that there exists @xmath327 with @xmath328 such that    1 .",
    "@xmath329 ; 2 .",
    "@xmath330 ; 3 .   @xmath331 .",
    "indeed , these properties follow from the fact that @xmath134 is a cluster point , and that @xmath332 for all @xmath251 .",
    "we next show that , if @xmath319 and @xmath333 for some fixed @xmath322 , then @xmath334 .",
    "\\end{split}\\ ] ] to see this , notice that @xmath319 and @xmath322 implies @xmath326 .",
    "hence , holds for @xmath335 . combining , , and the concavity of @xmath71",
    ", we conclude that for all such @xmath336 @xmath337\\\\        & \\ge   { \\rm dist}(0,\\partial l_\\beta(x^t , y^t , z^t))\\cdot[\\varphi(l_\\beta(x^t , y^t , z^t ) - l^ * ) - \\varphi(l_\\beta(x^{t+1},y^{t+1},z^{t+1 } ) - l^*)]\\\\        & \\ge   { \\rm dist}(0,\\partial l_\\beta(x^t , y^t , z^t))\\cdot\\varphi'(l_\\beta(x^t , y^t , z^t ) - l^*)\\cdot[l_\\beta(x^t , y^t , z^t ) - l_\\beta(x^{t+1},y^{t+1},z^{t+1})]\\\\        & \\ge   d \\|x^{t+1}-x^t\\|^2 .",
    "\\end{split}\\ ] ] dividing both sides by @xmath338 , taking square root , using the inequality @xmath339 as in the proof of ( * ? ? ?",
    "* lemma  2.6 ) , and rearranging terms , we conclude that holds .",
    "we now show that @xmath319 whenever @xmath340 .",
    "we establish this claim by induction , and our proof is similar to the proof of ( * ? ? ?",
    "* lemma  2.6 ) .",
    "the claim is true for @xmath341 by construction . for @xmath342",
    ", we have @xmath343 where the first inequality follows from . now",
    ", suppose the claim is true for @xmath344 for some @xmath345 ; i.e. , @xmath346 .",
    "we now consider the case when @xmath347 : @xmath348\\\\        & \\le \\|x^n - x^*\\| + 2\\|x^n - x^{n+1}\\|\\\\        & \\ \\",
    "+ \\frac{c}{d}\\sum_{j=1}^{k-1}[\\varphi(l_\\beta(x^{n+j},y^{n+j},z^{n+j } ) - l^ * ) - \\varphi(l_\\beta(x^{n+j+1},y^{n+j+1},z^{n+j+1 } ) - l^*)]\\\\        & \\le \\|x^n - x^*\\| + 2\\|x^n - x^{n+1}\\| + \\frac{c}{d}\\varphi(l_\\beta(x^{n+1},y^{n+1},z^{n+1 } ) - l^ * ) ,      \\end{split}\\ ] ] where the first inequality follows from , the monotonicity of @xmath290 from , and the induction assumption that @xmath346 .",
    "moreover , in view of and the definition of @xmath349 , we see that the last expression above is less than @xmath349 . hence , @xmath350 as claimed , and we have shown that @xmath319 for @xmath340 by induction .    since @xmath319 for @xmath340 , we can sum from @xmath341 to @xmath351 .",
    "invoking , we arrive at @xmath352 which implies that holds . convergence of @xmath144 follows immediately from this . convergence of @xmath259 follows from the convergence of @xmath144 , the relation @xmath353 from , and .",
    "finally , the convergence of @xmath143 follows from the surjectivity of @xmath2 , and the relation @xmath354 from .",
    "this completes the proof .    *",
    "( comments on theorem [ th:2])*[rem4 ]    * a close inspection of the above proof shows that the conclusion of theorem [ th:2 ] continues to hold as long as the augmented lagrangian @xmath280 is a kl - function .",
    "here , we only state the case where @xmath0 and @xmath1 are semi - algebraic because this simple sufficient condition can be easily verified . *",
    "although a general convergence analysis framework was established in @xcite for a broad class of optimization problems , it is not clear to us whether their results can be applied directly here . indeed , to ensure convergence , three basic properties * h1 * , * h2 * and * h3 * were imposed in ( * ? ? ?",
    "* page  99 ) .",
    "in particular , their property * h1 * ( sufficient descent property ) in our case reads : @xmath355 for some @xmath356 . on the other hand , ( [ eq : smallrelation2 ] ) in our proof only gives us that @xmath357 , which is not sufficient for property * h1 * to hold . * in theorem [ th:2 ] ,",
    "we only discussed the case where @xmath108 . this condition is used to ensure that @xmath290 is a decreasing sequence that is at least as large as @xmath358 .",
    "it would be interesting to see whether the analysis here can be further extended to the case where @xmath359 .    before ending this section , we comment on the behavior of admm in the case where @xmath2 is assumed to be injective ( instead of surjective ) . as suggested by the numerical experiments in @xcite and our preliminary numerical tests ,",
    "it is conceivable that the admm does _ not _ cluster at a stationary point in general when applied to solving problem with an injective @xmath2 .",
    "we hereby give a concrete 2-dimensional example for non - convergence , motivated by the recent counterexample in ( * ? ? ?",
    "* remark  6 ) for the convergence of douglas - rachford splitting method in a nonconvex setting . ;",
    "see @xcite .",
    "moreover , it has been brought to our attention during the revision process of this paper that the known equivalence between the admm and the dr splitting method in the convex case ( see , for example , ( * ? ? ? * remark  3.14 ) ) can be passed through to the nonconvex cases .",
    "thus , the global convergence results in this paper concerning the admm can be specialized to obtain global convergence of the dr splitting method in some nonconvex settings .",
    "we note that the global convergence of the dr splitting method in the nonconvex settings has been studied in @xcite based on a new specially constructed merit function . ]",
    "[ ex7:nonconverge]*(divergence of admm when @xmath2 is injective ) * fix @xmath360 $ ] and set @xmath361 and @xmath362 . then @xmath363 .",
    "consider the optimization problem @xmath364 this problem corresponds to with @xmath365 , @xmath366 where @xmath367 , and @xmath2 is the linear map so that @xmath368 ; the problem can be equivalently reformulated as @xmath369 and the admm can be applied .",
    "let @xmath370 and @xmath371 denote the multipliers corresponding to the first and second equality constraints , respectively .",
    "the iterates in ( with @xmath142 ) now take the form @xmath372 for concreteness , whenever ambiguity arises in updating @xmath373 via the projection onto the nonconvex ( discrete ) set @xmath338 , we choose the element in @xmath338 that is closest to the previous iterate @xmath374 .    for each @xmath375 , consider the initializations @xmath376 , @xmath377 and @xmath378 .",
    "then it is routine to show that the admm described in will exhibit a discrete limit cycle of length @xmath379 .",
    "specifically , @xmath380 for any @xmath381 and @xmath382 .",
    "moreover , @xmath383 in particular , the sequence @xmath144 is not convergent and the successive change of the @xmath205-update does not converge to zero .",
    "in this section , we look at the model problem in the case where @xmath384 . since the objective is the sum of a smooth and a possibly nonsmooth part with a simple proximal mapping , it is natural to consider the proximal gradient algorithm ( also known as the forward - backward splitting algorithm ) . in this approach ,",
    "one considers the update @xmath385 from our assumption on @xmath1 , the update can be performed efficiently via a computation of the proximal mapping of @xmath386 .",
    "when @xmath387 , where @xmath388 , it is not hard to show that any cluster point @xmath138 of the sequence generated above is a stationary point of ; see , for example , @xcite . in what follows",
    ", we analyze the convergence under a slightly more flexible step - size rule .",
    "[ prop : prox ] suppose that there exists a twice continuously differentiable convex function @xmath389 and @xmath390 such that for all @xmath16 , @xmath391 let @xmath144 be generated from with @xmath392 . then the algorithm is a descent algorithm",
    ". moreover , any cluster point @xmath138 of @xmath144 , if exists , is a stationary point .",
    "for the algorithm to converge faster , intuitively , a larger step - size @xmath128 should be chosen ; see also table  [ table3 ] .",
    "condition indicates that the  concave \" part of the smooth objective @xmath0 does not impose any restrictions on the choice of step - size .",
    "this could result in an @xmath393 smaller than the lipschitz continuity modulus of @xmath123 , and hence allow a choice of a larger @xmath128 . on the other hand , since the algorithm is a descent algorithm by theorem  [ prop : prox ] , the sequence generated from would be bounded under standard coerciveness assumptions on the objective function .",
    "notice from assumption that @xmath394 is lipschitz continuous with lipschitz continuity modulus at most @xmath393 .",
    "hence @xmath395 from this we see further that @xmath396 where the first inequality follows from , the last inequality follows from the definition of @xmath170 and the subdifferential inequality applied to the function @xmath389 .",
    "since @xmath397 implies @xmath398 , shows that the algorithm is a descent algorithm .",
    "rearranging terms in and summing from @xmath399 to any @xmath400 , we see further that @xmath401 now , let @xmath138 be a cluster point and take any convergent subsequence @xmath402 that converges to @xmath138 . taking limit on both sides of the above inequality along the convergent subsequence , one can see that @xmath403 .",
    "finally , we wish to show that @xmath404 . to this end ,",
    "note first that since @xmath403 , we also have @xmath405 .",
    "then it follows from lower semicontinuity of @xmath1 that @xmath406 . on the other hand , from",
    ", we have @xmath407 which gives @xmath408 . hence , @xmath409 .",
    "now , using this , @xmath410 , and taking limit along the convergent subsequence in the following relation obtained from @xmath411 we see that the conclusion concerning stationary point holds .",
    "we illustrate the above theorem in the following examples .",
    "suppose that @xmath0 admits an explicit representation as a difference of two convex twice continuously differentiable functions @xmath412 , and that @xmath413 has a lipschitz continuous gradient with modulus at most @xmath414",
    ". then holds with @xmath415 and @xmath416 .",
    "hence , the step - size can be chosen from @xmath417 .",
    "a concrete example of this kind is given by @xmath418 , where @xmath419 is a symmetric indefinite matrix . then holds with @xmath420 , where @xmath421 is the projection of @xmath419 onto the cone of nonpositive semidefinite matrices , and @xmath422 .",
    "the step - size @xmath128 can be chosen within the open interval @xmath423 .    in the case when @xmath262 is a concave quadratic , say , for example , @xmath424 for some linear map @xmath226 , it is easy to see that holds with @xmath425 for _ any _ positive number @xmath393 .",
    "thus , step - size can be chosen to be any positive number .",
    "suppose that @xmath0 has a lipschitz continuous gradient and it is known that all the eigenvalues of @xmath426 , for any @xmath16 , lie in the interval @xmath427 $ ] with @xmath428 . if @xmath429 , it is clear that @xmath19 is lipschitz continuous with modulus bounded by @xmath430 , and hence the step - size for the proximal gradient algorithm can be chosen from @xmath431 . on the other hand , if @xmath432 , then it is easy to see that holds with @xmath433 and @xmath434 .",
    "hence , the step - size can be chosen from @xmath435 .",
    "we next comment on the convergence of the whole sequence .",
    "we consider the conditions * h1 * through * h3 * on ( * ? ? ?",
    "* page  99 ) .",
    "first , it is easy to see from that * h1 * is satisfied with @xmath436 .",
    "next , notice from that if @xmath437 , then @xmath438 .",
    "moreover , from the definition of @xmath439 , we have @xmath440 for any @xmath441 .",
    "this shows that the condition * h2 * is satisfied with @xmath442 .",
    "finally , ( * ? ? ?",
    "* remark  5.2 ) shows that * h3 * is satisfied . thus , we conclude from ( * ? ? ?",
    "* theorem  2.9 ) that if @xmath443 is a kl - function and a cluster point @xmath138 of the sequence @xmath144 exists , then the whole sequence converges to @xmath138 .",
    "a line - search strategy can also be incorporated to possibly speed up the above algorithm ; see @xcite for the case when @xmath1 is a continuous difference - of - convex function .",
    "the convergence analysis there can be directly adapted .",
    "the result of theorem  [ prop : prox ] concerning the interval of viable step - sizes can be used in designing the initial step - size for backtracking in the line - search procedure .",
    "in this section , we perform numerical experiments to illustrate our algorithms .",
    "all codes are written in matlab .",
    "all experiments are performed on a 32-bit desktop machine with an intel@xmath444 i7 - 3770 cpu ( 3.40 ghz ) and a 4.00 gb ram , equipped with matlab 7.13 ( 2011b ) .      [",
    "[ minimizing - constraints - violation . ] ] minimizing constraints violation .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we consider the problem of finding the closest point to a given @xmath445 that violates at most @xmath14 out of @xmath446 equations .",
    "the problem is presented as follows : @xmath447 where @xmath448 has full row rank , @xmath449 , @xmath450 .",
    "this can be seen as a special case of by taking @xmath236 and @xmath451 to be the indicator function of the set @xmath452 , which is a proper closed function ; here , @xmath453 is the @xmath12 norm that counts the number of nonzero entries in the vector @xmath145 .",
    "we apply the admm ( i.e. , proximal admm with @xmath142 ) with parameters specified as in example  [ example:5 ] , and pick @xmath454 so that @xmath242 . from example",
    "[ examplenew:2 ] , the sequence generated from the admm is always bounded and hence convergence of the sequence is guaranteed by theorem  [ th:2 ] .",
    "we compare our model against the standard convex model with the @xmath12 norm replaced by the @xmath15 norm .",
    "this latter model is solved by sdpt3 ( version 4.0 ) , called via cvx ( version 1.22 ) , using default settings .    for the admm",
    ", we consider two initializations : setting all variables at the origin ( @xmath455 init . ) , or setting @xmath146 to be the approximate solution @xmath70 obtained from solving the convex model , @xmath192 and @xmath456 ( @xmath15 init . ) . as discussed in remark  [ rem1 ] , when @xmath70 is feasible for , this latter initialization satisfies the conditions in theorem  [ thm : main](ii ) .",
    "we terminate the admm when the sum of successive changes is small , i.e. , when @xmath457    in our experiments , we consider random instances . in particular , to guarantee that the problem is feasible for a fixed @xmath14 , we generate the matrix @xmath2 and the right hand side @xmath227 using the following matlab codes :    ....    m = randn(m , n ) ;    x_orig = randn(n,1 ) ;    j = randperm(m ) ;    b = randn(m,1 ) ;    b(j(1:m - r ) ) = m(j(1:m - r),:)*x_orig ; % subsystem has a solution ....    we then generate @xmath93 with i.i.d .",
    "standard gaussian entries .",
    "we consider @xmath458 , @xmath459 , @xmath460 , @xmath461 and @xmath462 , @xmath463 , @xmath464 , @xmath465 and @xmath466 .",
    "we generate one random instance for each @xmath467 and solve and the corresponding @xmath15 relaxation .",
    "the computational results are shown in table  [ table2 ] , where we report the number of violated constraints ( vio ) by the approximate solution @xmath16 obtained , defined as @xmath468 , and the distance from @xmath93 ( dist ) defined as @xmath469 .",
    "we also report the number of iterations the admm takes , as well as the cpu time of both the admm initialized at the origin and sdpt3 called using cvx .",
    "we see that the model allows an explicit control on the number of violated constraints .",
    "in addition , comparing with the @xmath15 model , the @xmath12 model solved using the admm always gives a solution closer to @xmath93 .",
    "finally , the solution obtained from the admm initialized from an approximate solution of the @xmath15 model can be slightly closer to @xmath93 than the solution obtained from the zero initialization , depending on the particular problem instance .",
    ".computational results for perturbation with bounded number of violated equalities . [ cols=\"^,^,^,^,^,^,^,^,^,^,^,^,^ \" , ]",
    "in this paper , we study the proximal admm and the proximal gradient algorithm for solving problem with a general surjective @xmath2 and @xmath214 , respectively . we prove that any cluster point of the sequence generated from the algorithms gives a stationary point by assuming merely a specific choice of parameters and the existence of a cluster point .",
    "we also show that if the functions @xmath0 and @xmath1 are in addition semi - algebraic and the sequence generated by the admm ( i.e. , proximal admm with @xmath108 ) clusters , then the sequence is actually convergent .",
    "furthermore , we give simple sufficient conditions for the boundedness of the sequence generated from the proximal admm .",
    "one interesting future research direction would be to adapt other splitting methods for convex problems to solve , especially in the case when @xmath2 is injective , and study their convergence properties .",
    "the second author would like to thank ernie esser and gabriel goh for enlightening discussions .",
    "the authors would also like to thank the anonymous referees for suggestions that help improve the manuscript .",
    "b. p. w. ames and m. hong .",
    "alternating direction method of multipliers for sparse zero - variance discriminant analysis and principal component analysis .",
    "preprint , january 2014 .",
    "available at ` http://arxiv.org/abs/1401.5492 ` .",
    "h. attouch , j. bolte , p. redont and a. soubeyran .",
    "proximal alternating minimization and projection methods for nonconvex problems .",
    "an approach based on the kurdyka - lojasiewicz inequality .",
    "35 , pp . 438457 ( 2010 ) .",
    "h. attouch , j. bolte and b. f. svaiter .",
    "convergence of descent methods for semi - algebraic and tame problems : proximal algorithms , forward - backward splitting , and regularized gauss - seidel methods .",
    "137 , ser .",
    "a , pp . 91129 ( 2013 ) .",
    "m. fortin and r. glowinski .",
    "on decomposition - coordination methods using an augmented lagrangian . in m. fortin and r. glowinski , eds . , _ augmented lagrangion methods : applications to the solution of boundary problems .",
    "_ north - holland , amsterdam , 1983 .",
    "applications of the method of multipliers to variational inequalities . in m. fortin and r.",
    "glowinski , eds .",
    ", _ augmented lagrangion methods : applications to the solution of boundary problems . _ north - holland , amsterdam , 1983 .      p. gong , c. zhang , z. lu , j. huang and j. ye . a general iterative shrinkage and thresholding algorithm for non - convex regularized optimization problems . the 30th international conference on machine learning ( icml 2013 ) ."
  ],
  "abstract_text": [
    "<S> we consider the problem of minimizing the sum of a smooth function @xmath0 with a bounded hessian , and a nonsmooth function . </S>",
    "<S> we assume that the latter function is a composition of a proper closed function @xmath1 and a surjective linear map @xmath2 , with the proximal mappings of @xmath3 , @xmath4 , simple to compute . </S>",
    "<S> this problem is nonconvex in general and encompasses many important applications in engineering and machine learning . in this paper , we examined two types of splitting methods for solving this nonconvex optimization problem : alternating direction method of multipliers and proximal gradient algorithm . for the direct adaptation of the alternating direction method of multipliers , </S>",
    "<S> we show that , if the penalty parameter is chosen sufficiently large and the sequence generated has a cluster point , then it gives a stationary point of the nonconvex problem . </S>",
    "<S> we also establish convergence of the whole sequence under an additional assumption that the functions @xmath0 and @xmath1 are semi - algebraic . </S>",
    "<S> furthermore , we give simple sufficient conditions to guarantee boundedness of the sequence generated . </S>",
    "<S> these conditions can be satisfied for a wide range of applications including the least squares problem with the @xmath5 regularization . finally , </S>",
    "<S> when @xmath2 is the identity so that the proximal gradient algorithm can be efficiently applied , we show that any cluster point is stationary under a slightly more flexible constant step - size rule than what is known in the literature for a nonconvex @xmath0 . </S>"
  ]
}