{
  "article_text": [
    "type - i and type - ii censoring schemes are two most popular censoring schemes which are used in practice .",
    "they can be briefly described as follows .",
    "suppose @xmath0 units are put on a life test .",
    "in type - i censoring , the test is terminated when a pre - determined time , @xmath1 , on test has been reached , and failures after time @xmath1 are not observed . in type - ii",
    "censoring , the test is terminated when a pre - chosen number , @xmath2 , out of @xmath0 items has failed .",
    "it is also assumed that the failed items are not replaced .",
    "so , in type - i censoring scheme , the number of failures is random and in type - ii censoring scheme , the experimental time is random .",
    "a hybrid censoring scheme is a mixture of type - i and type - ii censoring schemes and it can be described as follows .",
    "suppose @xmath0 identical units are put to test .",
    "the test is finished when a pre - selected number @xmath2 out of @xmath0 items are failed , or when a pre - determined time @xmath1 on the test has been obtained . from now on",
    ", we call this type - i hybrid censoring scheme and this scheme has been used as a reliability acceptance test in @xcite .",
    "this censoring scheme was introduced by epstin @xcite , he also studied the life testing data under the assumption of exponential distribution with mean life @xmath3 .",
    "epstein @xcite proposed two - sided confidence intervals for @xmath3 without any formal proof .",
    "fairbanks et al .",
    "@xcite moderated partly the proposition of epstein @xcite and suggested a simple set of confidence intervals .",
    "chen and bhattacharya @xcite earned the exact distribution of the conditional maximum likelihood estimator ( mle ) of @xmath3 and implied a one - sided confidence interval .",
    "childs et al .",
    "@xcite proposed some simplifications of the exact distribution . from the bayesian point of view , drapper and guttmann @xcite studied the same problem , and reached a two - sided credible interval of the mean lifetime based on the gamma prior .",
    "comparison of the different methods using monte carlo simulations , can be found in gupta and kundu @xcite .",
    "for some related work , one may refer to ebrahimi @xcite , jeong et al .",
    "@xcite , childs et al .",
    "@xcite , kundu @xcite , banerjee and kundu @xcite , kundu and pradhan @xcite , dube et al . @xcite and the references cited there .",
    "one of the disadvantages of type - i hybrid censoring scheme is that there may be very few failures occurring up to the pre - fixed time @xmath1 . because of this , childs et al .",
    "@xcite proposed a new hybrid censoring scheme known as type - ii hybrid censoring scheme which can be described as follows .",
    "put @xmath0 identical items on test , and then stop the experiment at the random time @xmath4 , where @xmath2 , and @xmath1 are prefixed numbers and @xmath5 indicates the time of @xmath2th failure in a sample of size @xmath0 . under the type - ii hybrid censoring scheme , we have one of the following three types of observations : + case i : @xmath6",
    "if @xmath7 + case ii : @xmath8 if @xmath9 and @xmath10 + case iii : @xmath11 + where @xmath12 denote the observed ordered failure times of the experimental units . a schematic illustration of the hybrid censoring scheme is presented in figure [ fig1 ] .",
    "( 10,30)(10,15 ) ( 25,25)(1,0)220 ( 35,25)(1,1)20 ( 70,25)(1,1)20 ( 200,25)(1,1)20 ( 25,15)@xmath13 ( 35,50)1-st failure ( 60,15)@xmath14 ( 90,50)2-nd failure ( 190,15)@xmath15 ( 180,50)r - th failure ( experiment stops ) ( 165,15)@xmath1 ( 122,20)@xmath16 ( 300,25)case i ( 35,25 ) ( 70,25 ) ( 170,25 )    ( 10,30)(10,15 ) ( 25,25)(1,0)220 ( 35,25)(1,1)20 ( 70,25)(1,1)20 ( 155,25)(1,1)20 ( 200,25)(1,1)20 ( 25,15)@xmath13 ( 35,50)1-st failure ( 60,15)@xmath14 ( 90,50)2-nd failure ( 110,20)@xmath16 ( 145,15)@xmath17 ( 150,50)d - th failure ( 195,15)@xmath1 ( 215,50)experiment stops ( 300,25)case ii ( 35,25 ) ( 70,25 ) ( 200,25 )    ( 10,30)(10,15 ) ( 25,25)(1,0)220 ( 35,25)(1,1)20 ( 70,25)(1,1)20 ( 155,25)(1,1)20 ( 25,15)@xmath13 ( 35,50)1-st failure ( 60,15)@xmath14 ( 90,50)2-nd failure ( 110,20)@xmath16 ( 145,15)@xmath18 ( 150,50)n - th failure ( experiment stops ) ( 195,15)@xmath1 ( 300,25)case iii ( 35,25 ) ( 70,25 ) ( 200,25 ) ( 10,-5)[fig1]figure 1 : a schematic presentation for type - ii hybrid censored scheme .    in this article , we consider the analysis of type - ii hybrid censored lifetime data when the lifetime of each experimental unit follows a two - parameter weighted exponential ( we ) distribution .",
    "this distribution was originally proposed by gupta and kundu @xcite .",
    "the two - parameter we distribution with the shape and scale parameters @xmath19 and @xmath20 , respectively , has the probability density function ( pdf ) as : @xmath21 we denote a two - parameter we distribution with the pdf ( [ we ] ) by @xmath22 and the corresponding cumulative distribution function ( cdf ) by @xmath23 .",
    "the aim of this article is two fold .",
    "first , we try to earn the mle s of the unknown parameters .",
    "it is observed that the maximum likelihood estimators can be obtained implicitly by solving two nonlinear equations , but they can not be obtained in closed form .",
    "so mle s of parameters are derived numerically .",
    "newton - raphson algorithm is one of the standard methods to determine the mle s of the parameters . to employ the algorithm , second derivatives of the log - likelihood",
    "are required for all iterations .",
    "the em algorithm is a very powerful tool in handling the incomplete data problem see dempster et al .",
    "@xcite and mclachlan and krishnan @xcite",
    ". then we use the em algorithm to compute the mle s .",
    "we also evaluate the observed fisher information matrix using the missing information principle which have been used to obtained asymptotic confidence intervals of the unknown parameters .",
    "the second aim of this article is to provide the bayes inference for the unknown parameters for type - ii hybrid censored data .",
    "it is observed that bayes estimators can not be obtained explicitly , we provide two approximations namely lindley s approximation and gibbs sampling procedure .",
    "so we use the gibbs sampling procedure to compute the bayes estimators , and the hpd confidence intervals .",
    "we compare the performances of the different methods by monte carlo simulations , and for illustrative purposes we have analyzed one real data set .",
    "the rest of the article is arranged as follows . in section 2",
    ", we provide the mle s of the unknown parameters .",
    "fisher information matrix is evaluated in section 3 . using lindley s approximation and gibbs sampling we obtain bayes estimators and hpd confidence intervals for the parametes in section 4 .",
    "simulation results are presented in section 5 .",
    "we verify our theoretical results via analyzing data set in section 6 .",
    "in this section , we study mles of the model parameters @xmath24 and @xmath25 for @xmath26 distribution with density function : @xmath27 for simplicity , we apply a re - parametrization as @xmath24 and @xmath28 . by this",
    ", the @xmath26 distribution can be written as : @xmath29 the likelihood function in case i is given by @xmath30 for case ii , @xmath31 and for case iii , @xmath32 where @xmath33 is presented by ( [ bl ] ) , so @xmath34 we present likelihood functions ( [ a ] ) , ( [ b ] ) and ( [ iii ] ) by : @xmath35 where @xmath36 @xmath37 taking the logarithm of equation [ c ] , we obtain @xmath38 @xmath39 then the normal equations are    @xmath40    maximum likelihood estimators can be secured by solving these equations , but they can not be expressed explicitly .",
    "so we use em algorithm to compute them .",
    "the advantage of this method is that it is convergence for any initial value fast enough .",
    "+      the em algorithm , originally proposed by dempster et al .",
    "@xcite , is a very powerful tool for handling the incomplete data problem .",
    "+ let us symbolize the observed and the censored data by @xmath41 and @xmath42 , respectively . here for a given r , @xmath43 are not observable .",
    "the censored data vector @xmath44 can be thought of as missing data .",
    "the combination of @xmath45 forms the whole data set .",
    "in next we follow the method kundu and pradhan @xcite for missing data introducing .",
    "+ if we denote the log - likelihood function of the uncensored data set by @xmath46 @xmath47 for the e - step of the em algorithm , one needs to compute the pseudo log - likelihood function as @xmath48 therefor , @xmath49 @xmath50 where @xmath51=e(z_i|z_i > c ) ~~\\mbox{and}~~ b(c;\\alpha,\\beta)=e[\\ln(1-e^{-\\beta z_i})|z_i > c],\\ ] ] and they are obtained in appendix a.    now the m - step includes the maximization of the pseudo log - likelihood function [ 2 ] .",
    "therefore , if at the kth stage , the estimation of @xmath52 is @xmath53 , then @xmath54 can be obtained by maximizing @xmath55 @xmath56 note that the maximization of [ 3 ] can be earned quite effectively by the similar method proposed by gupta and kundu @xcite .",
    "first , @xmath57 can be obtain by solving a fixed - point type equation @xmath58 the function @xmath59 is defined @xmath60^{-1}\\ ] ] where @xmath61 and @xmath62 one can follow iteration method .",
    "once @xmath57 is determined , @xmath63 can be evaluated as @xmath64 .    for the estimation of @xmath25 , we can use the invariance property maximum likelihood estimators and obtain @xmath65 as follow : @xmath66",
    "one of the advantages of using em algorithm is that presents a measure of information in censored data through the missing information principle .",
    "louis @xcite improved a procedure for extracting the observed information matrix . in this section ,",
    "we display the observed fisher information matrix by using the missing value principles of louis @xcite . the observed fisher information matrix can be used to build the asymptotic confidence intervals .",
    "+ using the notations : @xmath67 , x = observed data , w = complete data , @xmath68=observed information , @xmath69=complete information and @xmath70=missing information , follow the relation to @xmath71 to evaluate @xmath72 + complete information and the missing information are given respectively as : @xmath73\\ ] ] @xmath74.\\ ] ] as the dimension of @xmath3 is 2 , @xmath68 and @xmath70 are both of the order @xmath75 .",
    "the elements of matrix @xmath69 for complete data set are presented in gupta and kundu @xcite .",
    "they re - parametrized @xmath26 distribution as @xmath25 and @xmath76 .",
    "+ we report @xmath69 which have been evaluated by them here as : @xmath77\\ ] ] where @xmath78 @xmath79 @xmath80 in which @xmath81 + on the other hand , with the above re - parametrization and by using ( [ iwx ] ) , one can easily verify @xmath82,\\ ] ] where @xmath83 @xmath84 @xmath85 in which @xmath86    now , @xmath68 can be computed by ( [ ix ] ) .",
    "the asymptotic variance - covariance matrix of @xmath87 can be obtained by inverting @xmath68 .",
    "we use this matrix to secure the asymptotic confidence intervals for @xmath25 and @xmath88 . to obtain the asymptotic confidence interval for @xmath24",
    ", we use the non - parametric bootstrap method @xcite .",
    "in this section , we study bayes estimators for parameters @xmath24 and @xmath25 under symmetric loss functions .",
    "a very well known symmetric loss function is the squared error which is defined as : @xmath89 with @xmath90 being an estimate of @xmath91 . here",
    "@xmath91 denotes some function of @xmath92 .",
    "bayes estimators , say @xmath93 , is evaluated by the posterior mean of @xmath91 .",
    "let @xmath94 be an observed sample from the hybrid censoring scheme , drawn from a @xmath26 distribution .",
    "we apply re - parametrization as @xmath24 and @xmath76 .",
    "so the likelihood function becomes @xmath95 and @xmath96-likelihood function : @xmath97 @xmath98 it is assumed that @xmath88 and @xmath24 have the following independent gamma priors : @xmath99 @xmath100 so , the joint prior distribution of @xmath24 and @xmath88 is of the form @xmath101 then the posterior distribution @xmath24 and @xmath88 can be written as @xmath102 @xmath103 where @xmath104 @xmath105 now the bayes estimators of @xmath24 and @xmath88 under the squared error loss function l are respectively obtained as : @xmath106=\\frac{1}{k}\\int_0^\\infty\\int_0^\\infty\\alpha^{w_4-n - r}\\beta^{w_2+r-1}(\\alpha+1)^re^{-\\alpha w_3}e^{-\\beta w_1}\\]]@xmath107 and @xmath108=\\frac{1}{k}\\int_0^\\infty\\int_0^\\infty\\alpha^{w_4-n - r-1}\\beta^{w_2+r}(\\alpha+1)^re^{-\\alpha w_3}e^{-\\beta w_1}\\]]@xmath109 since @xmath25 is a function of @xmath24 and @xmath88 , then one can obtain the posterior density function of @xmath25 and so the bayes estimator of @xmath25 under the squared error loss function @xmath110 as : @xmath111=\\frac{1}{k}\\int_0^\\infty\\int_0^\\infty u^{w_4+w_2-n-1}\\lambda^{w_2+r-1}(1+u)^{r}e^{-u(w_3+\\lambda w_1)}\\]]@xmath112 as these estimators can not be evaluated explicitly , so we adopt two different procedures to approximate them :    *   lindley approximation , *   mcmc method .      in previous section , based on type - ii hybrid censored scheme we obtained the bayes estimators of @xmath24 , @xmath88 and @xmath25 against squared error loss function @xmath110 .",
    "it is easily observed that theses estimators have not explicit closed forms . for these evaluation ,",
    "numerical techniques are required .",
    "one of the most numerical techniques is lindley s method ( see @xcite ) , that for these estimators can be describe as follows . in general ,",
    "bayes estimator of @xmath113 as a function of @xmath24 and @xmath88 is identified : @xmath114 where @xmath115 is @xmath96-likelihood function ( defined by [ log ] ) and @xmath116 .",
    "+ by the lindley s method @xmath117 can be approximated as : @xmath118@xmath119+\\frac{1}{2}[(\\hat{u}_{\\alpha}\\hat{\\sigma}_{\\alpha\\alpha}+ \\hat{u}_{\\beta}\\hat{\\sigma}_{\\alpha\\beta})(\\hat{l}_{\\alpha\\alpha\\alpha}\\hat{\\sigma}_{\\alpha\\alpha}+ \\hat{l}_{\\alpha\\beta\\alpha}\\hat{\\sigma}_{\\alpha\\beta}+\\hat{l}_{\\beta\\alpha\\alpha}\\hat{\\sigma}_{\\beta\\alpha}\\]]@xmath120,\\ ] ] where @xmath121 and @xmath122 are the mle s of @xmath24 and @xmath88 respectively .",
    "also , @xmath123 is the second derivative of the function @xmath113 with the respect to @xmath24 and @xmath124 valued of @xmath123 at @xmath125 other expressions can be calculated with following definitions : @xmath126 @xmath127 @xmath128 @xmath129 where @xmath130 @xmath131 @xmath132 @xmath133 @xmath134 @xmath135 @xmath136 and we have : @xmath137 with the above defined expressions , we obtain the approximation bayes estimators .",
    "+ also we have : @xmath138 the bayes estimator of @xmath24 under the squared error loss function @xmath110 becomes @xmath139@xmath140.\\ ] ] proceeding similarly , the bayes estimator of @xmath88 under @xmath110 is given by @xmath141 @xmath142 @xmath143.\\ ] ] finally the bayes estimator of @xmath25 under @xmath110 is given by @xmath144 @xmath145@xmath146+\\frac{1}{2}[(\\hat{u}_{\\alpha}\\hat{\\sigma}_{\\alpha\\alpha}+ \\hat{u}_{\\beta}\\hat{\\sigma}_{\\alpha\\beta})(\\hat{l}_{\\alpha\\alpha\\alpha}\\hat{\\sigma}_{\\alpha\\alpha}+ \\hat{l}_{\\alpha\\beta\\alpha}\\hat{\\sigma}_{\\alpha\\beta}+\\hat{l}_{\\beta\\alpha\\alpha}\\hat{\\sigma}_{\\beta\\alpha}\\]]@xmath147.\\ ] ]    the approximate bayes estimators of @xmath24 , @xmath88 and @xmath25 can be obtained using lindley approximation , but it is not possible to construct highest posterior density ( hpd ) confidence intervals using this method .",
    "therefore , we suggest the following markov chain monte carlo ( mcmc ) method to generate samples from the posterior density function , and in turn to obtain the bayes estimators , and hpd confidence intervals .      here",
    "we study the gibbs sampling method to draw samples from the posterior density function and then compute the bayes estimators and hpd confidence intervals of @xmath24 , @xmath88 and @xmath25 under the squared errors loss function .",
    "let @xmath148 be an observed sample from the hybrid censoring scheme , drawn from a @xmath26 distribution . from ( [ pos ] )",
    ", we can write the joint posterior density function of @xmath24 and @xmath88 given @xmath149 as : @xmath150 @xmath151 by this , the posterior density function of @xmath88 given @xmath24 and @xmath149 is @xmath152 [ the1]the conditional distribution of @xmath88 given @xmath24 and @xmath149 is log - concave .",
    "see appendix , part b. + by ( [ pos2 ] ) , the posterior density function of @xmath24 given @xmath88 and @xmath149 is @xmath153 [ th2]the conditional distribution of @xmath24 given @xmath88 and @xmath149 has a finite maximum point .",
    "see appendix , part c. with the help of the acceptance rejection principle ( see devroye @xcite for details ) and the previous theorem , the generation from ( [ albe ] ) can be performed using the we generator .",
    "+    now we use theorems [ the1 ] and [ th2 ] and pursue the idea of geman and geman @xcite , and suggest the following scheme .",
    "+    step 1 ) take some initial value of @xmath24 and @xmath88 , such as @xmath154 and @xmath155 .",
    "step 2 ) generate @xmath156 and @xmath157 from @xmath158 and @xmath159 .",
    "step 3 ) repeat step 2 , @xmath160 times .    step 4 )",
    "obtain bayes estimators of @xmath24 and @xmath88 with respect to a squared error loss function : @xmath161 where @xmath162 and @xmath163 are the burn - in periods in generating of @xmath164 and @xmath165 respectively .    step 5 )",
    "obtain the hpd confidence interval of @xmath24 : order @xmath166 as @xmath167 and construct all the @xmath168 confidence intervals of @xmath24 , as : @xmath169)}),\\cdots,(\\alpha_{([m_1\\eta])},\\alpha_{(m_1)}),\\ ] ] where @xmath170 $ ] symbolizes the largest integer less than or equal to @xmath171 .",
    "the hpd confidence interval of @xmath24 is the shortest length interval .",
    "similarly , we can construct a @xmath168 hpd confidence interval of @xmath88 .    finally , using the idea of chen and shao @xcite",
    ", we can compute the estimation and hpd confidence interval for @xmath25 .",
    "in this section , we carry out a simulation study to compare the performance of mle s and bayes estimators . in all the cases @xmath172 and @xmath173 are taken .",
    "we estimate the unknown parameters using the mle , bayes estimators obtained by lindley s approximations and also bayes estimators obtained by using mcmc technique .",
    "we compare the performances of different estimators with mse .",
    "we also obtain the average length of the asymptotic confidence intervals and the hpd confidence intervals .    for computing the bayes estimators , it is assumed that @xmath88 and @xmath24 have @xmath174 and @xmath175 priors , respectively .",
    "moreover we use the non - informative priors of both @xmath88 and @xmath24 , by considering @xmath176 .",
    "the bayes estimators are computed under the squared error loss function and with respect to the above non - informative priors .",
    "the simulation is performed for different choices of @xmath177 values .",
    "we replicate the procedure for 1000 times and report the average estimators , the mse s , the average asymptotic confidence intervals length and the average hpd confidence intervals length from the mcmc technique .",
    "the results are reported in table 1 - 4 .",
    "the first and second rows are parameter estimators of @xmath25 and @xmath24 , respectively .    from tables 1 - 4",
    ", it is observed that for fixed n and t as r increases , the mse decrease .",
    "the performances of the mle s and bayes estimators are very similar in all aspects .",
    "the average hpd confidence lengths are smaller than the average asymptotic lengths in all the cases considered .",
    "finally it should be mentioned that bayes estimators are most computationally expensive followed by mle s .",
    "in this section , we demonstrate one data set for illustrative purposes . it has been studied by gupta and kundu @xcite that the @xmath26 distribution can be used quite to analyze them and mle s of @xmath24 and @xmath25 are 1.6232 and 0.0138 respectively .",
    "the data set was studied by bjerkedal @xcite and is given below : + 12 15 22 24 24 32 32 33 34 38 38 43 44 48 52 53 54 54 55 56 57 58 58 59 60 60 60 60 61 62 63 65 65 67 68 70 70 72 73 75 76 76 81 83 84 85 87 91 95 96 98 99 109 110 121 127 129 131 143 146 146 175 175 211 233 258 258 263 297 341 341 376 .",
    "+ we use them and create the following two sampling schemes : @xmath178 @xmath179 now for scheme 1 , mle of @xmath180 are @xmath181 and bayes estimators with assumed non - informative priors , i.e. , @xmath176 with lindley approximation and gibbs sampling method are @xmath182 and @xmath183 respectively .",
    "the @xmath184 confidence intervals based on mle and bayes estimators of @xmath180 are @xmath185 and @xmath186 respectively .",
    "+ for scheme 2 , mle of @xmath180 are @xmath187 bayes estimators with assumed non - informative priors , i.e. , @xmath176 with lindley approximation and gibbs sampling method are @xmath188 and @xmath189 respectively .",
    "the @xmath184 confidence intervals based on mle and bayes estimators of @xmath180 are @xmath190 and @xmath191 respectively .    because we see the effect of the hyper parameters on the bayes estimators and also on confidence intervals , we take the following informative priors @xmath192 + based on this , for scheme 1 , bayes estimators of @xmath180 with lindley approximation and gibbs sampling method are @xmath193 and @xmath194 respectively .",
    "the @xmath184 confidence intervals based on bayes estimators of @xmath180 are @xmath195 for scheme 2 , bayes estimators of @xmath180 with lindley approximation and gibbs sampling method are @xmath196 and @xmath197 respectively .",
    "the @xmath184 confidence intervals based on bayes estimators of @xmath180 are @xmath198    we plot all the different estimated density functions with non - informative priors and informative priors in figure 1 and figure 2 .",
    "[ fig3 ]    [ sch ]    comparing the two schemes with informative and non - informative priors , it is observed that for scheme 1 , estimators have smaller standard errors than scheme 2 , as expected .",
    "also it is clear that the bayes estimators depend on the hyper parameters .",
    "because the hpd confidence intervals based on informative priors are slightly smaller than corresponding length of hpd confidence intervals based on non - informative priors , therefore the prior informative should be used if they are available .",
    "in this article , we have studied the classical and bayes inference procedure for the type - ii hybrid censored @xmath26 distribution .",
    "we provide the maximum likelihood estimators and it is observed that the maximum likelihood estimators of the unknown parameters can not be obtained in the closed form and we suggest the em algorithm to compute them .",
    "we also earn the bayes estimators of the unknown parameters and show that they can not be obtained in explicit forms , and we have proposed two approximation methods to earn them .",
    "we have compared the performance of the different methods by monte carlo simulations , and it is observed that the performance of quite satisfactory .",
    "[ ap ] the proof can be obtained similarly as in ng et al .",
    "+ note that using theorem [ ap ] , we can write @xmath199=\\int_c^\\infty \\frac{\\alpha+1}{\\alpha}\\frac{\\beta}{k}xe^{-\\frac{\\beta}{\\alpha}x}(1-e^{-\\beta x})dx\\;\\;\\;\\;\\left(k = e^{-\\frac{\\beta}{\\alpha}c}(\\alpha+1-e^{-\\beta c})\\right)\\\\ \\\\ \\hspace{1in}=\\frac{(\\alpha+1)\\alpha}{k\\beta}\\int_{\\frac{\\beta}{\\alpha}c}^\\infty ue^{-u}du-\\frac{\\alpha}{k\\beta(\\alpha+1)}\\int_{\\frac{\\beta(\\alpha+1)}{\\alpha}c}^\\infty ve^{-v}dv \\;\\;\\;\\left(put \\;\\;u=\\frac{\\beta}{\\alpha}x\\;\\;\\&\\;\\;v=\\frac{\\beta(\\alpha+1)}{\\alpha}x\\right)\\\\ \\\\",
    "\\hspace{-0.5in}=\\frac{(\\alpha+1)\\alpha}{k\\beta}(-e^{-u})(u+1)]_{\\frac{\\beta}{\\alpha}c}^\\infty-\\frac{\\alpha}{k\\beta(\\alpha+1)}(-e^{-v})(v+1 ) ] _ { \\frac{\\beta(\\alpha+1)}{\\alpha}c}^\\infty\\\\ \\\\",
    "\\hspace{-0.7in}=\\frac{(\\alpha+1)\\alpha}{k\\beta}(\\frac{\\beta}{\\alpha}c+1)e^{-\\frac{\\beta}{\\alpha}c}-\\frac{\\alpha}{k\\beta(\\alpha+1)}(\\frac{\\beta(\\alpha+1)}{\\alpha}c+1)e^ { -\\frac{\\beta(\\alpha+1)}{\\alpha}c}\\\\ \\\\",
    "\\hspace{-1.25in}=\\frac{\\alpha e^{-\\frac{\\beta}{\\alpha}c}}{k\\beta}\\left[(\\alpha+1)(\\frac{\\beta}{\\alpha}c+1)-\\frac{e^{-\\beta c}}{\\alpha+1}(\\frac{\\beta(\\alpha+1)c}{\\alpha}+1)\\right]\\\\ \\\\ \\hspace{-1.1in}=\\frac{e^{-\\frac{\\beta}{\\alpha}c}}{k\\beta}\\left[(\\alpha+1)(\\beta c+\\alpha)-\\frac{e^{-\\beta c}}{\\alpha+1}\\left(\\beta(\\alpha+1)c+\\alpha\\right)\\right ] \\\\ \\\\",
    "c+\\alpha)-\\frac{e^{-\\beta c}(\\beta c(\\alpha+1)+\\alpha)}{\\alpha+1}}{\\beta(\\alpha+1-e^{-\\frac{\\beta}{\\alpha}c } ) } \\end{array}\\ ] ] and about @xmath200 , we have : @xmath201=\\int_c^\\infty\\frac{\\alpha+1}{\\alpha}\\frac{\\beta}{k}\\ln(1-e^{-\\beta x})e^{-\\frac{\\beta}{\\alpha}x}(1-e^{-\\beta x})dx\\]]@xmath202 @xmath203,[4,4],1-e^{-\\beta c})\\right.\\]]@xmath204,[3],1-e^{-\\beta c})\\right.\\]]@xmath205,[3],1-e^{-\\beta c})\\right)(1-e^{-\\beta c})^2\\]]@xmath206,[4,4],1)\\]]@xmath207,[3],1)\\right]\\ ] ] where hypergeom ( . )",
    "is generalized hypergeometric function .",
    "this function is also known as barnes s extended hypergeometric function .",
    "the definition of @xmath208 is : @xmath209 where @xmath210 $ ] , @xmath211 is the number of operands of @xmath212 , @xmath213 $ ] and @xmath214 is the number of operands of @xmath215 .",
    "generalized hypergeometric function is quickly evaluated and readily available in standard software such as maple .",
    "the conditional distribution of @xmath24 given @xmath88 and @xmath149 is @xmath217 in this function , we have @xmath218 and @xmath219 , now it is enough that prove @xmath220 is bounded . with simple calculation",
    "we see that @xmath221 this function is less than the gamma function and the gamma function is a bounded function , so this function is bounded .",
    "therefore @xmath220 has a finite maximum point .",
    "childs.a , chandrasekhar.b , balakrishnan.n , kundu.d ( 2003 ) exact likelihood inference based on type - i and type - ii hybrid censored samples from the exponential distribution,_ann .",
    "_ , 55 ,   319 - 330 ."
  ],
  "abstract_text": [
    "<S> a hybrid censoring scheme is a mixture of type - i and type - ii censoring schemes . </S>",
    "<S> we study the estimation of parameters of weighted exponential distribution based on type - ii hybrid censored data . by applying em algorithm , </S>",
    "<S> maximum likelihood estimators are evaluated . also using fisher infirmation matrix asymptotic confidence intervals </S>",
    "<S> are provided . by applying markov chain monte carlo techniques </S>",
    "<S> bayes estimators , and corresponding highest posterior density confidence intervals of parameters are obtained . </S>",
    "<S> monte carlo simulations to compare the performances of the different methods is performed and one data set is analyzed for illustrative purposes . </S>",
    "<S> +   + _ keywords _ : asymptotic distribution , em algorithm , markov chain monte carlo , hybrid censoring , bayes estimators , type - i censoring , type ii censoring , maximum likelihood estimators +   + _ mathematics subject classification : _ 62f10 , 62f15 , 62n02 </S>"
  ]
}