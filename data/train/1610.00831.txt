{
  "article_text": [
    "the purpose of these notes is to contribute to the theoretical understanding of dataflow matrix machines .",
    "dataflow matrix machines ( dmms ) arise in the context of synchronous dataflow programming with linear streams , i.e. streams equipped with an operation of taking a linear combinations of several streams  @xcite .",
    "this is a new general - purpose programming architecture with interesting properties .",
    "one of these properties is that large classes of programs are parametrized by matrices of numbers . in this aspect dmms",
    "are similar to recurrent neural nets and , in fact , they can be considered to be a very powerful generalization of recurrent neural nets  @xcite .    just like recurrent neural nets ,",
    "dmms are essentially  two - stroke engines \" . on the  up movement \"",
    "the built - in neuron transformations compute the next elements of the streams associated with the neuron outputs from the streams associated with neuron inputs .",
    "this computation is local to the neuron in question and is generally nonlinear . on the  down movement \" , the next elements of the streams associated with all neuron inputs are computed from the streams associated with all neuron outputs using the matrix controlling the dmm .",
    "this computation is linear and is potentially quite global , as any neuron output in the net can contribute to any neuron input in the net .",
    "dmms described in the literature are heavily typed .",
    "one normally defines a finite collection of allowed kinds of linear streams , and a finite collection of allowed types of neurons .",
    "these two collections are called the _",
    "dmm signature_. one considers a particular fixed signature .",
    "then one assumes the address space accommodating a countable number of neurons of each type , and then a dmm is determined by a countable - sized matrix of connectivity weights ( one normally assumes that only a finite number of those weights are non - zero at any given moment of time ) .",
    "in particular , dmms can be equipped with powerful reflection facilities .",
    "include in the signature the kind of streams of matrices shaped in such a fashion as to be capable of describing a dmm over this signature .",
    "then designate a particular neuron , ` self ` , working as an accumulator of matrices of this shape , and agree that the most recent output of this neuron will be used at the  down movement \" of each step as the matrix controlling the calculations of all neuron inputs from all neuron outputs .",
    "dmms seem to be a powerful programming platform .",
    "in particular , it is convenient to manually write general - purpose software as dmms . at the same time",
    "the options to automatically synthesize dmms by synthesizing the matrices in question are available .",
    "however , dmms are a bit too unwieldy for a theoretical investigation .    from the theoretical viewpoint , it is inconvenient that there are many kinds of streams .",
    "it is also inconvenient that one needs to fix a signature , and that the parametrization by matrices is valid only for this fixed signature .",
    "so a question naturally arises : _ what would be the equivalent of untyped lambda - calculus for dataflow matrix machines ? _",
    "one of the principles of untyped lambda - calculus : one data type is enough , namely the type of programs .",
    "all data can be expressed as programs .",
    "the equivalent of this principle for dmms would be to have only one kind of streams : streams of matrices , where a matrix is so shaped as to be able to define a dmm which would be a network of transformers of streams of matrices ( see section  [ address_space ] for details ) .    instead of string rewriting ,",
    "a number of streams of matrices are unfolding in time in this approach .",
    "so all data are to be expressed as countably - infinite matrices of numbers under this approach ( see section  [ as_matrices ] ) , just like all data must be expressed as lambda - terms in the untyped lambda - calculus .",
    "choosing a fixed selection of types of neurons seems too difficult at the moment . for the time being we would like to retain the ability to add arbitrary types of neurons to our dmms .",
    "so instead of selecting a fixed canonical signature , we assume that there is an underlying language allowing to describe countable collection of neuron types in such a fashion that all neuron types of interest can be expressed in that language .",
    "then assume that all neuron types described by all neuron type expressions in the underlying language are in the signature .",
    "assume that our address space is structured in such a way as to accommodate countable number of neurons for each type of neurons ( see section  [ address_space ] ) .",
    "since we have a countable collection of expressions describing neuron types , our overall collection of neurons is still countable , and the matrix describing the rules to recompute neuron inputs from the neuron outputs is still countable .",
    "so , now we have a parametrization by countable matrices of numbers across all dmms , and not just across dmms with a particular fixed signature .",
    "the notion of accumulator plays a key role in a number of dmm constructions including the reflection facility ` self ` .",
    "the most standard version is a neuron performing an identity transform of its vector input , @xmath0 , to its vector output , @xmath1 , of the same kind .",
    "one sets the weight of the recurrent connection from @xmath1 to @xmath0 to 1 , and then the neuron accumulates contributions of other neurons connected to @xmath0 with nonzero weights .",
    "so , at each step the accumulator neuron in effect performs @xmath2 operation .",
    "however , it is somewhat of abuse of the system of kinds of streams to consider @xmath3 and @xmath4 as belonging to the same space , and we ll see evidence that to do so is a suboptimal convention later in the paper .",
    "so , what we do , first of all , is that we equip the accumulator neuron with another input , where @xmath4 is collected .",
    "then the body of the neuron computes the sum of @xmath3 and @xmath4 , instead of just performing the identity transform ( see section  [ accum_revised ] for more details ) .    in the situations , where one has multiple kinds of linear streams",
    ", one would often want to assign different kinds to @xmath3 and to @xmath4 ( although in other situations one would still use the same kind for the both of them , effectively considering @xmath4 to be @xmath5 ) .      in section  [ continuous_models ]",
    "we discuss continuous models of computation and their higher - order aspects . in section  [ higher_order ]",
    "we juxtapose string rewriting with stream - based approaches to higher - order programming . in section  [ address_space ]",
    "we discuss the language of indexes of the network matrix and how to accommodate countable number of neuron types within one signature . in section  [ as_matrices ]",
    "we discuss representation of constants and vectors as matrices .",
    "section  [ accum_revised ] provides two examples where it is natural to split the accumulator input into @xmath3 and @xmath4 .",
    "one such example comes from the neuron ` self ` controlling the network matrix .",
    "another example ( section  [ warmus_numbers ] ) is more involved and requires us to revisit domain theory in the context of linear models of computation .",
    "this is a bitopological setting , more specifically , bi - continuous domains , allowing for both monotonic and anti - monotonic inference , and this is the setting where approximations spaces tend to become embedded into vector spaces , which is where the connection with linear models of computation comes into play .",
    "the history of continuous models of computation is actually quite long . where the progress was more limited was in making higher - order constructions continuous , in particular , in making spaces of programs continuous .",
    "denotationally , the continuous domains representing the meaning of programs are common .",
    "but operationally , we tend to fall back onto discrete schemas .",
    "dataflow matrix machines are seeking to change that and to provide programming facilities using continuous programs and continuous deformations of programs on the level of operational semantics and of implementation .",
    "this can be done both for discrete time and discrete index spaces ( countably - sized matrices of computational elements ) , and , potentially , for continuous time and continuous index spaces for computational elements .",
    "the oldest electronic continuous platform is electronic analog computers .",
    "the analog program itself , however , is very discrete , because this kind of machine has a number of single - contact sockets and for every pair of such sockets there is an option to connect them via a patch cord , or not to connect them .    among dataflow architectures oriented towards handling the streams of continuous data one might mention labview  @xcite and pure data ( e.g.  @xcite ) .",
    "in both cases , the programs themselves are quite discrete .",
    "the computational platform which should be discussed in more details in this context is recurrent neural networks .",
    "turing universality of recurrent neural networks is known for at least 30 years  @xcite .",
    "however , together with many other useful and elegant turing - universal computational systems , recurrent neural networks do not constitute a convenient general - purpose programming platform , but belong to the class of _ esoteric programming languages _",
    "( see  @xcite for detailed discussion of that ) .",
    "interestingly enough , whether recurrent neural networks understood as programs are discrete or continuous depends on how one approaches the representation of network topology .",
    "if one treats the network connectivity as a graph , and thinks about this graph as a discrete data structure , then recurrent neural networks themselves are discrete .    if one states instead that the network connectivity is always the complete graph , and that the topology is defined by some of the weights being zeros , then recurrent neural networks themselves are continuous .",
    "the most frequent case is borderline .",
    "one considers a recurrent neural net to be defined by the matrix of weights , and therefore to be continuous , however there are auxiliary discrete structures , e.g. the matrix of weights is often a sparse matrix , and so a dictionary of nonzero weights comes into play .",
    "also a language used in describing the network or its implementation comes into play as an auxiliary discrete structure .",
    "dataflow matrix machines belong to this borderline case .",
    "in particular , the use of sparse matrices is inevitable , because the matrices in question are countable - sized matrices with finite number of nonzero elements .",
    "there are several approaches to higher - order stream - based programming .",
    "the most popular approach starts with standard higher - order functional programming and focuses on integrating stream - based programming into that standard paradigm .",
    "the theoretical underpinning of this approach is lambda - calculus and string rewriting ( e.g.  @xcite ) .",
    "the dataflow community produced purely stream - based approaches to higher - order programming .",
    "one of those approaches which should be mentioned is an approach based on multidimensional streams  @xcite .",
    "the approach which we adopt in this paper is based on the notion of streams of programs .",
    "an early work which should be mentioned in connection with this approach is  @xcite .",
    "an argument in favor of this approach for programming with linear streams was presented in section 3 of  @xcite .    among recent papers exploring various aspects of the approach based on the notion of streams of programs",
    "are  @xcite .",
    "one of the goals of the present paper is to show that this approach can play the role in synchronous dataflow programming with linear streams comparable to the role played by untyped lambda - calculus in functional programming .",
    "when one has a countable - sized matrix , it is often more convenient to index its rows and columns by finite strings over a fixed finite alphabet than by numbers .",
    "there is no principal difference , but this choice discourages focusing on an arbitrary chosen order , and encourages semantically meaningful names for the indices .    here",
    "we explain how the construction promised in section  [ one_signature ] works .",
    "define the notion of a type of neurons following the outline presented in section 3.1 of  @xcite for multiple kinds of linear streams .",
    "we only have one kind of linear streams in the present paper , so the definition is simplified .",
    "a neuron type consists of a non - negative integer input arity @xmath6 , a positive integer output arity @xmath7 , and a transformation describing how to map @xmath6 input streams of matrices into @xmath7 output streams of matrices .",
    "namely associate with the neuron type in question a transformation @xmath8 taking as inputs @xmath6 streams of length @xmath9 and producing as outputs @xmath7 streams of length @xmath10 for integer time @xmath11 .",
    "require the obvious prefix condition that when @xmath8 is applied to streams of length @xmath11 , the first @xmath10 elements of the output streams of length @xmath12 are the elements which @xmath8 produces when applied to the prefixes of length @xmath9 of the input streams .",
    "the most typical situation is when for @xmath13 the @xmath10 s elements of the output streams are produced solely on the basis of elements number @xmath9 of the input streams , but our definition also allows neurons to accumulate unlimited history , if necessary .      in this section",
    "we are going to use several alphabets .",
    "assume that the following special symbols do nt belong to any of the other alphabets : @xmath15`\\%@`@xmath16 .",
    "assume that there is a language @xmath14 over alphabet @xmath17 , such that finite strings from @xmath18 describe all neuron types of interest .",
    "call a string @xmath10 the name of the neuron type it defines ( we are not worried about uniqueness of names for a type here ) .",
    "assume that the input arity of the type in question is @xmath19 and the output arity of the type in question is @xmath20 .",
    "that for every integer @xmath21 such that @xmath22 associate field name @xmath23 from @xmath14 and for every integer @xmath24 such that @xmath25 associate field name @xmath26 from @xmath14 , so that @xmath27 implies @xmath28 and @xmath29 implies @xmath30 .    also assume that there is an alphabet @xmath31 with more than one letter in it and any finite string @xmath32 over @xmath31 is a valid _ simple name_.      the following convention describes the address space for a countable number of neurons for each of the countable number of neuron types of interest .",
    "the indexes are expressed by strings over the alphabet @xmath33 . for any name of neuron type @xmath34 and for any simple name @xmath32 , the concatenation @xmath35 ` @ ` @xmath36 is a name of a neuron .    for any field name @xmath23 , the concatenation @xmath35 ` @ ` @xmath37 ` \\ `",
    "@xmath36 is the name of the corresponding neuron input . for any field name @xmath26 , the concatenation @xmath35 ` @ ` @xmath38 ` % ` @xmath36 is the name of the corresponding neuron output . for every such pair of indices,@xmath39 ` @ ` @xmath40 ` \\ ` @xmath41 ` @ ` @xmath42 ` % ` @xmath43 , there is a matrix element in our matrices under consideration .    to summarize : in this approach the class of pure dataflow matrix machines is implicitly parametrized by a sufficiently universal language @xmath14 describing all types of neurons taken to be of potential interest together with their associated built - in stream transformations .    for details of dmm functioning see sections 3.3 and 3.4 of  @xcite .",
    "to implement the program outlined in section  [ one_stream ] one needs to express the most important linear streams , such as streams of numbers ( scalars ) , streams of matrix rows and streams of matrix columns , and other frequently used streams of vectors as streams of matrices . as indicated in  @xcite ,",
    "one of the key uses of scalars and also of matrix rows and columns is their use as multiplicative masks .",
    "the ability to use scalars as multiplicative masks needs to be preserved when those scalars are represented by matrices .",
    "for example , if we have a neuron which takes an input stream of scalars @xmath44 and an input stream of matrices @xmath6 , and produces an output stream of matrices @xmath45 , then we still need to be able to reproduce this functionality when scalars @xmath44 are represented by matrices of the same shape as matrix @xmath6 .    the most straightforward way to do this is to have a neuron which takes two input streams of matrices and performs their element - wise multiplication ( hadamard product , sometimes also called the schur product ) .",
    "if we chose the hadamard product as our main bilinear operation on matrices , then the scalar @xmath0 must be represented by the matrix all elements of which are equal to @xmath0 .",
    "one particular feature of this approach is that we can no longer limit ourselves by matrices containing finite number of non - zero elements , but we also need at least some infinite matrices admitting finite descriptions .",
    "this means that one needs a convention of what should be done in case of incorrect operations , such as taking a scalar product of two infinite vectors of all ones ( or adding a matrix consisting of all ones to ` self ` ) .",
    "it seems likely that the technically easiest convention in such cases would be to output zeros ( or to reset the network matrix to all zeros ) .    on the other hand ,",
    "it is of interest to consider and study the limits of sequences of finitely describable matrices , and a network might be computing such a limit when @xmath46 .",
    "streams of matrix rows and streams of matrix columns also play important roles in  @xcite .",
    "represent element @xmath1 of a row by the corresponding matrix column all elements of which equal @xmath1 .",
    "represent element @xmath47 of a column by the corresponding matrix row all elements of which equal @xmath47 .",
    "hence , rows are represented by matrices with equal values along each column , and columns are represented by matrices with equal values along each row .    given matrix row @xmath48 ,",
    "denote by @xmath49 its representation as a matrix .",
    "given matrix column @xmath50 , denote by @xmath51 its representation as a matrix .",
    "given scalar @xmath0 , denote by @xmath52 its representation as a matrix .",
    "respecting the matlab convention to denote the hadamard product by ` . * ` , we denote the hadamard product of two matrices by @xmath53 , while omitting the infix for matrix multiplication , @xmath54 or @xmath55 .    note that because matrix rows correspond to neuron inputs and matrix columns correspond to neuron outputs , one should always think about these matrices as rectangular , and not as square matrices , so the transposition is always needed when performing the standard matrix multiplication on these matrices .    in  @xcite a standard matrix update operation generalized from several natural examples is proposed . given a row @xmath48 , two columns @xmath50 and @xmath56 ( with the constraint that both @xmath50 and @xmath56 have finite number of nonzero elements ) , the matrix is updated by the formula @xmath57 .",
    "in terms of matrix representations what gets added to the network matrix @xmath58 is @xmath59 .    in section 4 of  @xcite matrix rows and columns",
    "are used for subgraph selection . consider a subset of neurons , and",
    "take @xmath48 to be a row with values 1 at the positions corresponding to the neuron outputs of the subset in question and zeros elsewhere , and take @xmath50 to be a column with values 1 at the positions corresponding to the neuron inputs of the subset in question and zeros elsewhere .",
    "denote the element - wise matrix maximum as @xmath60 .",
    "the overall connectivity of the subgraph in question is expressed by the matrix @xmath61 , while the internal connectivity of this subgraph is @xmath62 .",
    "the most straightforward way to represent other finite - dimensional vectors or countable - dimensional vectors with finite number of nonzero elements in this setup is to represent them as matrix rows as well .",
    "this means reserving a finite or countable number of appropriately typed neurons to represent coordinates .",
    "for example , to describe vectors representing characters in the  1-of-@xmath7 \" encoding which is standard in neural nets  @xcite one would need to reserve neurons to represent the letters of the alphabet in question .",
    "here we continue the line of thought started in section  [ accum_revised_small ] .",
    "we give a couple of examples illustrating why it is natural to have separate inputs for @xmath3 and @xmath4 in an accumulator .",
    "the main example is the neuron ` self ` itself , producing the matrix controlling the network on the output , and taking additive updates to that matrix on the input .",
    "this is a countable - sized matrix with finite number of nonzero elements , so it has to be represented as a sparse matrix , e.g. via a dictionary of nonzero elements .",
    "a typical situation is that the additive update on each time step is small compared to the matrix itself ( more specifically , the update is typically small in the sense that the number of affected matrix elements is small compared to the overall number of nonzero matrix elements ) .    so it does not make much sense to actually copy the output of ` self ` to its input of ` self ` and perform the additive update there , which is what should be done if the non - optimized definition of an accumulator with one input is to be taken literally .",
    "what should be done instead is that additive updates should be added together at an input of ` self ` , and then on the  up movement \" the ` self ` should add the sum of those updates to the matrix it accumulates .",
    "so instead of hiding this logic as  implementation details \" , it makes sense to split the inputs of ` self ` into @xmath0 ( with the output of ` self ` connected to @xmath0 with weight 1 , nothing else connected to @xmath0 with non - zero weight , and the copying of the output of ` self ` to @xmath0 being a no - op ) and @xmath63 accumulating the additive updates to ` self ` .",
    "another example of why it is natural to have separate inputs for @xmath3 and @xmath4 in an accumulator comes from considering a scheme of computation with warmus numbers .",
    "we have to explain first what are warmus numbers and why considering them and a particular scheme of computation in question is natural in this context .      _ in the presence of partial inconsistency approximation spaces tends to become embedded into vector spaces .",
    "_ one well - known example of this phenomenon is that if one allows negative values for probabilities , then probabilistic powerdomain is embedded into the space of signed measures which is a natural setting for denotational semantics of probabilistic programs  @xcite .",
    "another example involves algebraic extension of interval numbers with respect to addition .",
    "interval numbers do nt form a group with respect to addition .",
    "however one can extend them with _ pseudosegments _",
    "@xmath64 $ ] with the contradictory property @xmath65 .",
    "for example , [ 3,2 ] is a pseudosegment expressing an interval number with the contradictory constraint that @xmath66 and at the same time @xmath67 .",
    "the so extended space of interval numbers is a group and a 2d vector space over reals .",
    "the first discovery of this construction known to us was made by mieczysaw warmus  @xcite .",
    "since then it was rediscovered many times . for a rather extensive bibliography related to those rediscoveries",
    "see  @xcite .",
    "there are a number of common motives which appear multiple times in various studies of partial inconsistency , in particular , bilattices , bitopology , bicontinuous domains , facilities for non - monotonic and anti - monotonic inference , order - reversing involutions , etc .",
    "together , these motives serve as focal elements of the field of study which has been named the _ partial inconsistency landscape _ in  @xcite .    in particular , the following situation is typical in the context of bitopological groups .",
    "the two topologies , @xmath68 and @xmath69 , are group dual of each other ( that is , the group inverse induces a bijection between the respective systems of open sets ) , and the anti - monotonic group inverse is an order - reversing involution , which is a bicontinuous map from @xmath70 to its bitopological dual , @xmath71  @xcite .    because approximation domains tend to become embedded into vector spaces in this context , the setting of bicontinuous domains  @xcite equipped with two scott topologies which tend to be group dual of each other seems to be natural for semantic studies of computations with linear streams .",
    "section 4 of  @xcite provides a detailed overview of the partial inconsistency landscape , including the bitopological and bilattice properties of warmus numbers .",
    "it turns out that warmus numbers play a fundamental role in mathematics of partial inconsistency .",
    "in particular , section 4.14 of that paper proposes a schema of _ computation via monotonic evolution punctuated by order - reversing involutive steps_.    computations with warmus extension of interval numbers via monotonic evolution punctuated by involutive steps are a good example of why the accumulators should have the asymmetry between @xmath3 and @xmath4 .",
    "if an accumulator neuron is to accumulate a monotonically evolving warmus number by accepting additive updates to that number , then the @xmath63 can not be an arbitrary warmus number , but it must be a pseudosegment @xmath64 $ ] , such that @xmath72 ( the case of @xmath73 is allowed ) . given that there is a constraint of this kind , it is natural to want to accumulate @xmath63 contributions at a separate input on the  down movement \" , and to let the accumulator enforce the constraint on the  up movement \" ( e.g. by ignoring requests for non - monotonic updates ) . yet",
    "another input might be added to trigger involutive steps ( an involutive step in this context transforms @xmath74 $ ] into @xmath75 $ ] ) .",
    "alternatively , requests for non - monotonic updates might trigger the involutions . normally , the involution would be triggered only if the accumulated number is already a pseudosegment , in which case the involution is an anti - monotonic step .      despite impressive progress in studies of bicontinuity and bitopology in the context of partial inconsistency landscape  @xcite",
    ", the issues related to reflexive domains and solutions of recursive domain equation in the context of bicontinuous domains and vector semantics do nt seem to be well understood .    given that dataflow matrix machines equipped with self - referen - tial facilities work directly on the level of vector spaces",
    ", one would hope that the gap between operational and denotational descriptions would be more narrow in this case than for more traditional situations such as untyped lambda - calculus .",
    "dataflow matrix machines work with arbitrary linear streams . in this paper , we focus on the case of _ pure dataflow matrix machines _ , which work with the single kind of linear streams , namely the streams of matrices defining the connectivity patterns and weights in pure dmms themselves .",
    "this allows us to pinpoint the key difference between pure dmms and recurrent neural networks : instead of working with streams of numbers , pure dataflow matrix machines work with streams of programs , with programs being represented as network connectivity matrices .",
    "19 [ 1]#1 [ 1]`#1 ` urlstyle [ 1]doi : # 1    s.  andima , r.  kopperman , and p.  nickolas . an asymmetric ellis theorem . _",
    "topology and its applications _ , 155:0 146160 , 2007 .",
    "m.  bukatin and s.  matthews .",
    "linear models of computation and program learning . in g.",
    "gottlob et  al . ,",
    "editors , _ gcai 2015 _ , easychair proceedings in computing , vol .",
    "36 , pages 6678 , 2015 . + http://easychair.org/publications/download/linear_models_of_computation_and_program_learning .",
    "m.  bukatin , s.  matthews , and a.  radul .",
    "dataflow matrix machines as programmable , dynamically expandable , self - referential generalized recurrent neural networks , 2016 .",
    "http://arxiv.org/abs/1605.05296 .",
    "m.  bukatin , s.  matthews , and a.  radul .",
    "programming patterns in dataflow matrix machines and generalized recurrent neural nets , 2016 .",
    "http://arxiv.org/abs/1606.09470 .",
    "a.  farnell .",
    "_ designing sound_. mit press , 2010 .",
    "fluid : project  fluid \" github repository , + https://github.com/anhinga/fluid    n. goodman , v. mansinghka , d. roy , k.  bonawitz , and j. tenenbaum .",
    "church : a language for generative models . in _ proc . of uncertainty in artificial intelligence _ , 2008 .",
    "+ http://danroy.org/papers/church_goomanroybonten-uai-2008.pdf .",
    "w.  johnston , j.  hanna , and r.  millar .",
    "advances in dataflow programming languages .",
    "_ acm computing surveys _ , 36:0 134 , 2004 .",
    "a.  jung and m.  moshier . on the bitopological nature of stone duality .",
    "technical report csr-06 - 13 , school of computer science , university of birmingham , 2006 .",
    "http://www.cs.bham.ac.uk/@xmath76axj/pub/papers/jung-moshier-2006-on-the-bitopological-nature-of-stone-duality.pdf .",
    "a.  karpathy .",
    "the unreasonable effectiveness of recurrent neural networks , 2015 .",
    "+ http://karpathy.github.io/2015/05/21/rnn - effectiveness/.    k.  keimel .",
    "bicontinuous domains and some old problems in domain theory . _ electronic notes in theoretical computer science _ , 257:0 3554 , 2009 .",
    "d.  kozen .",
    "semantics of probabilistic programs . _ journal of computer and system sciences _ , 220 ( 3):0 328350 , 1981 .",
    "n.  krishnaswami .",
    "higher - order reactive programming without spacetime leaks .",
    "_ acm sigplan notices _ , 480 ( 9):0 221232 , 2013 .    j.  lawson .",
    "stably compact spaces . _ mathematical structures in computer science _ , 210 ( 1):0 125169 , 2011 .    s.  matthews . adding second order functions to kahn data flow .",
    "technical report research report 189 , university of warwick , 1991 .",
    "+ http://www.dcs.warwick.ac.uk/report/pdfs/cs-rr-189.pdf .",
    "j.  pollack .",
    "_ on connectionist models of natural language processing_. phd thesis , university of illinois at urbana - champaign , 1987 .",
    "+ chapter 4 is available at + http://www.demo.cs.brandeis.edu/papers/neuring.pdf .",
    "e.  popova .",
    "the arithmetic on proper & improper intervals ( a repository of literature on interval algebraic extensions ) , 1996 - 2013 .",
    "+ http://www.math.bas.bg/~epopova/directed.html .",
    "h.  siegelmann and e.  sontag . on the computational power of neural nets .",
    "_ journal of computer and system sciences _",
    ", 50:0 132150 , 1995 .",
    "w.  wadge .",
    "higher - order lucid . in r.",
    "jagannathan , editor , _ proceedings of the 4th international symposium on lucid and intensional programming _ , pages 6269 , 1991 .",
    "+ http://www.cse.unsw.edu.au/~plaice/archive/www/1991/u-islip91-lucidhigher.pdf .",
    "m.  warmus .",
    "calculus of approximations .",
    "acad . pol .",
    "iii _ , 40 ( 5):0 253259 , 1956 .",
    "+ http://www.cs.utep.edu/interval-comp/warmus.pdf .",
    "g. zhou , j. wu , c. zhang , and z. zhou .",
    "minimal gated unit for recurrent neural networks , 2016 .",
    "consider a sequence @xmath77 of elements , which are monotonically increasing and are obtained by additive corrections from previous elements of the sequence .",
    "if these are conventional interval numbers , this situation is only possible for the trivial case of @xmath78 , as addition can not reduce the degree of imprecision ( self - distance ) for conventional interval numbers .",
    "it is not possible to perform nontrivial monotonic evolution of conventional interval numbers by adding other interval numbers to previous elements of the sequence in question .    for warmus numbers , monotonic evolution by additive corrections is possible ,",
    "provided that every additive correction summand @xmath79 $ ] is a pseudo - segment anti - approximating zero :    @xmath80 \\sqsubseteq [ a_i , b_i]$ ] , that is @xmath81 .",
    "rectified linear unit ( relu ) is a neuron with the activation function @xmath82 .    in the recent years , relu became the most popular neuron in the context of non - recurrent deep networks .",
    "whether it is equally good for recurrent networks remains to be seen .",
    "the activation function @xmath83 is an integral of the heaviside step function .",
    "lack of smoothness at 0 does not seem to interfere with gradient methods used during neural net training .",
    "interestingly enough , the standard quasi - metrics on reals associated with upper and lower topologies on reals are closely related to relu : @xmath84 .",
    "various schemas of recurrent networks with gates and memory were found to be useful in overcoming the problem of vanishing gradients in the training of recurrent neural networks , starting with lstm in 1997 and now including a variety of other schemas .    for a convenient compact overview of lstm ,",
    "gated recurrent units networks , and related schemas see section 2 of  @xcite .",
    "the standard way to describe lstm and gated recurrent unit networks is to think about them as networks of sigmoid neurons augmented with external memory and gating mechanisms .",
    "however , it is long understood ( and is used in the present paper ) that neurons with linear activation functions can be used as accumulators to implement memory .",
    "it is also known for at least 30 years that bilinear neurons ( such as neurons multiplying two inputs , each of those inputs accumulating linear combinations of output signals of other neurons ) can be used to modulate signals via multiplicative masks ( gates ) and to implement conditional constructions in this fashion  @xcite ( see also section 1.3.2 of  @xcite ) .",
    "looking at the formulas for ltsm and gated recurrent unit networks in table 1 of  @xcite one can observe that instead of thinking about these networks as networks of sigmoid neurons augmented with external memory and gating mechanisms , one can describe them simply as recurrent neural networks built from sigmoid neurons , linear neurons , and bilinear neurons , without any external mechanisms .",
    "when ltsm and gated recurrent unit networks are built as recurrent neural networks from sigmoid neurons , linear neurons , and bilinear neurons , some weights are variable and subject to training , and some weights are fixed as zeros or ones to establish a particular network topology .",
    "pure dataflow matrix machines are countable - sized networks with a finite part of the network being active at any given moment of time . they process streams of countable - sized matrices with finite number of non - zero elements ,",
    "sometimes it is convenient to consider the case of networks of finite size , with fixed number of inputs , @xmath6 , and fixed number of outputs , @xmath7 .",
    "if we still would like those networks to process streams of matrices describing network weights and topology , those matrices would be finite rectangular matrices @xmath85 .",
    "we call the resulting class of networks * lightweight pure dmms*. if we work with reals of limited precision and consider fixed values of @xmath6 and @xmath7 , the resulting class is not turing - universal , as its memory space is finite .",
    "however , it is often useful to consider this class for didactic purposes , as both theoretical constructions and software prototypes tend to be simpler in this case , while many computational effects can already be illustrated in this generality .",
    "so , overall the dimension of space of all possible linear operators from outputs to inputs ( which could potentially be used during the  down movement \" ) is @xmath88 .",
    "however , our model actually uses matrices of the dimension @xmath85 during the  down movement \" , so only a subspace of dimension @xmath85 of the overall space of all possible linear operators of the dimension @xmath88 is allowed .",
    "the matrix is applied not to a vector of numbers , but to a vector of @xmath7 matrices @xmath85 , and yields not a vector of numbers , but a vector of @xmath6 matrices @xmath85 .",
    "this is what accounts for factoring @xmath89 dimension out .",
    "we prototyped lightweight pure dmms in processing 2.2.1 in the lightweight_pure_dmms directory of project fluid , which is our open source project dedicated to experiments with the computational architectures based on linear streams  @xcite .",
    "in particular , we demonstrated during those experiments that it is enough to consider a set of several constant update matrices together with our self - referential network update mechanism described in the present paper to create oscillations of network weights and waves of network connectivity patterns .",
    "assume that the neuron self adds matrices @xmath90 and @xmath91 on the  up movement \" to obtain matrix @xmath92 .",
    "assume that at the starting moment @xmath93 , @xmath94 , @xmath95 for all @xmath96 , @xmath97 , @xmath98 for all @xmath99 .",
    "the network starts with a  down movement \" .",
    "after the first  down movement \" , @xmath90 becomes a copy of @xmath92 , @xmath91 becomes a copy of @xmath100 , and after the first  up movement \" at the time @xmath104 @xmath105 changes sign : @xmath106 .          here instead of @xmath100",
    "we take a collection of constant update matrices , @xmath109 .",
    "just like in the previous example , make sure that the first rows ( indexed by 0 ) of those matrices are 0 .",
    "for the second rows ( indexed by 1 ) , take @xmath110 , and the rest of the elements of the second rows of these matrices are 0 .",
    "start at @xmath93 with @xmath92 matrix having the first row as before , and the second row containing the only non - zero element @xmath111",
    ". then one can easily see ( or verify by downloading and running under processing 2.2.1 the open source software in the lightweight_pure_dmms / aug_27_16_experiment directory of the project fluid  @xcite ) that at the moment @xmath104 the only non - zero element in the second row of @xmath92 is @xmath112 , at the moment @xmath108 the only non - zero element in the second row of @xmath92 is @xmath113 , and so on until at the moment @xmath114 this wave of network connectivity pattern loops back to @xmath111 , and then continues looping indefinitely through these @xmath115 states ."
  ],
  "abstract_text": [
    "<S> dataflow matrix machines are self - referential generalized recurrent neural nets . the self - referential mechanism is provided via a stream of matrices defining the connectivity and weights of the network in question . </S>",
    "<S> a natural question is : what should play the role of untyped lambda - calculus for this programming architecture ? </S>",
    "<S> the proposed answer is a discipline of programming with only one kind of streams , namely the streams of appropriately shaped matrices . </S>",
    "<S> this yields _ pure dataflow matrix machines _ which are networks of transformers of streams of matrices capable of defining a pure dataflow matrix machine .    </S>",
    "<S> [ data - flow languages ]    higher - order programming , dataflow    continuous deformation of software , self - referential software </S>"
  ]
}