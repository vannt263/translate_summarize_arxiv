{
  "article_text": [
    "we consider the semantic pixel labeling problem : given a set of semantic classes , such as tree , cow , etc . , the task is to associate a label with every pixel .",
    "although hotly studied in recent years , semantic labeling remains a critical challenge in the broader image understanding community , for obvious reasons like high intraclass variability , occlusion , etc .",
    "early approaches have relied on texture clustering and segmentation , e.g. , @xcite .",
    "more recently , conditional random fields have become the de facto representation for the problem , e.g. , @xcite .",
    "most such methods learn a strong classifier based on local patches or superpixels and specify some form of a smoothness prior over the field .",
    "although these methods have demonstrated good success on challenging real world datasets @xcite , their performance remains limited for one key reason : they are intrinsically local making it difficult to incorporate any notion of object and even region semantics .",
    "yet , the visual psychophysics literature has demonstrated the clear importance of modeling at the object- and inter - object relational level for full scene understanding @xcite .",
    "although there has been some work on overcoming the challenge of locality ( see next section on related work for a brief survey ) , there has been little work in the direction of incorporating a notion of scene parts nor the inter - relationship among the parts .",
    "in contrast , we present a parts - based approach to full semantic pixel labeling that marries an object - level model of the parts of the scene with a pixel - level representation , rather than a strictly pixel- or region - level model .",
    "our method sits in the broad class of pictorial structures @xcite , which have shown notable success at articulated object modeling in recent years @xcite . however , classical pictorial structures are not well - suited to semantic image labeling : they ( 1 ) parameterize object parts and abstract them completely from the pixel - level , ( 2 ) require all parts to be present in the scene , and ( 3 ) typically adopt simple relational models ( linear springs ) .",
    "these three characteristics of the classical models make them unsuitable for image labeling problems .",
    "our method , called pixel - support parts - sparse pictorial structures , or 3 , overcomes these limitations and takes a step towards a parts - based view of image understanding by proposing a joint global model over image parts  objects in the scene such as trees , cars , the road , etc. which are each nodes in the pictorial structures graph .",
    "it directly ties each part to a set of pixels without any restrictive parameterization , which affords a rich set of object - level measurements , e.g. , global shape .",
    "3 also defines a space of plausible part - graphs and learns complete relation models between the pairwise elements . at inference ,",
    "a suitable part - graph is selected ( in this paper , manually ) and then optimized , which jointly localizes the parts at an object level and performs semantic labeling at the pixel level .",
    "we have tested our method on the msrc and the sift - flow benchmarks and demonstrate better performance with respect to maximum likelihood and markov random field performance in a controlled experimental setting ( exact same appearance models ) .",
    "we also compare our methods to existing semantic pixel labeling approaches , but do so with limited significant due to our assumption of being given the parts - graph for a test image . in the remainder of the paper , we present some related papers",
    ", then describe classical pictorial structures , our extensions including an appropriate inference algorithm , our experimental results , and conclusion and future work .",
    "several other recent papers have similarly demonstrated the significance of moving beyond local methods . @xcite",
    "demonstrate the value of incorporating partial 3d information about the scene during detection .",
    "@xcite take a hierarchical approach to full scene understanding by integrating patch - level , object - level , and textual tags into a generative model .",
    "these examples hold strong promise for scene understanding , but are not directly applicable to labeling .",
    "one promising method applicable to labeling is @xcite , which proposes a relative location prior for each semantic class and model it with a conditional random field over all of the superpixels in an image .",
    "whereas their approach defines a joint distribution over each of the superpixels in an image , which potentially remains too local , our approach defines it essentially in a layer above the superpixels , affording global coverage and the capability to also model the shape of each semantic image part .",
    "another strategy has been to share information among different sub - problems in image understanding .",
    "the cascaded classification models approach @xcite shares information across object detection and geometrical reasoning .",
    "yang et al .",
    "( @xcite ) drive a pixel - labeling process by a bank of parts - based object detectors ; their method demonstrates the power of explicitly modeling objects and their parts within the labeling process .    the layout consistent crf @xcite uses a parts - based representation of object categories to add robustness to partial occlusion and captures different types of local transitions between labels .",
    "other methods look to hierarchies . @xcite",
    "propose an elegant hierarchical extension to the problem that currently performs best on the classic msrc benchmark @xcite .",
    "however , in principle , it remains local and does not incorporate a notion of scene parts nor inter - relationship among the scene parts ; indeed , nor do any of these prior methods .",
    "pictorial structures ( ps ) are a parts - based representation for objects in images .",
    "classical ps models @xcite are in the class of undirected markov graphical models .",
    "concretely , pictorial structures represent an object as a graph @xmath0 in which each vertex @xmath1 , @xmath2 is a _",
    "part _ in the @xmath3-part model and the edges @xmath4 depict those parts that are connected . a _ configuration _",
    "@xmath5 specifies a complete instance of the model , with each @xmath6 specifying the parametric description of each part @xmath1 .",
    "for example , in human pose estimation , each @xmath6 can specifying the location , scale , and in - plane rotation of each body part .",
    "the best configuration for a given image @xmath7 is specified as the one minimizing the following energy : @xmath8 where the @xmath9 and @xmath10 potentials specify the unary and binary potentials , respectively , for parts @xmath6 and @xmath11 , and @xmath12 specify model parameters .",
    "the specific form of these potential functions is arbitrary , but they are most commonly gaussian functions ( elegantly expressed in log - quadratic form in @xcite ) , which gives rise to a spring model interpretation .",
    "such parts - based models have found success in the computer vision community for object recognition problems .",
    "firstly , pictorial structures are a general framework for parts - based modeling .",
    "for example , the constellation and star models @xcite semi - supervisedly learns models for the parts of various object categories under specific topological arrangements .",
    "the framework has been extended to track objects in video @xcite .",
    "more recently , in the context of human pose estimation , adaptive appearance @xcite and adaptive pose prior @xcite were introduced to enhance robustness in the presence of weak localization , appearance or other cues . for tracking objects in which some parts may be missing",
    ", the mixture - of - parts pictorial structure defines a distribution over legal part subsets and a mechanism for retrieving an appropriate structure @xcite . secondly , although the optimization is , in general , np - hard , under certain conditions , such as a tree - structured graph @xcite , the global optimum can be reached efficiently .",
    "thirdly , pictorial structures have a clear statistical interpretation in the form of a gibbs distribution : @xmath13 \\enspace , \\end{aligned}\\ ] ] where @xmath14 is the partition , or normalizing , function and @xmath15 is the energy function defined in ( [ eq : ps1 ] ) .",
    "this statistical view permits principled estimation of the model parameters and globally convergent inference algorithms even in the case of general potentials .    however , classical pictorial structures have significant limitations when applied to more general problems in which ( 1 ) some parts may be missing , ( 2 ) a distribution over structures is present rather than a single one , and ( 3 ) a precise segmentation of each part is required rather than strictly its parametric description .",
    "one such problem is semantic pixel labeling . in most images ,",
    "only a few of the classes are present : e.g. , four to five for the 21 class msrc @xcite .",
    "furthermore , the standard parametric descriptions of the parts @xmath6 do not readily map to pixel labels .",
    "we begin with a concrete problem definition for semantic scene labeling .",
    "let @xmath16 be the pixel lattice and define the basic elements @xmath17 to be either individual pixels , patches , or superpixels , such that @xmath18 and @xmath19 .",
    "let @xmath20 specify the set of semantic class labels , e.g. , car , tree , etc .",
    ", and denote @xmath21 as the label for element @xmath22 . in the maximum a posteriori view , the labeling problem is to associate the best label with each element @xmath23 but we do not directly model the problem at the pixel level .",
    "rather , we model it at the object level @xmath6 as we now explain .",
    "* parts with direct pixel support . *",
    "we take a nonparametric approach and directly represent the part @xmath6 based on its pixel support .",
    "each part @xmath6 comprises a set of basic elements @xmath24 , and induces a binary map , @xmath25 .",
    "a configuration @xmath26 jointly represents a high - level description of the elements in a scene , and also a direct semantic labeling of each pixel in the image .",
    "furthermore , rich , pixel - level descriptions of part - appearance and part - shape are now plausible .",
    "however , it adds significant complexity into the estimation problem : fast inference based on max - product message passing @xcite is no longer a viable option as the parts have a more complex interdependent relation among their supports .",
    "* parts - sparse pictorial structures . * classically , pictorial structures models are defined by a fixed set of @xmath3 parts , and all are expected in the image . in scene labeling , however , most images contain a small subset of the possible labels @xmath20 .",
    "we consider the space @xmath27 containing all plausible pictorial structures for scene labels .",
    "@xmath27 is large , but finite : for an image of size @xmath28 , the upper bound on nodes in a 3 model is @xmath29 , but the typical number is be quite smaller , e.g. , around three to five for the msrc dataset .",
    "each node can be of one class type from @xmath20 . whereas classical pictorial structures model the parameters @xmath12 for a specific structure , in @xmath30 , we model @xmath12 in the unary and binary terms at an individual and pairwise level , independent of the structure .",
    "then , for any plausible layout of parts , we can immediately index into their respective parameters and use them for inference .    in this paper",
    ", we do not define an explicit form on how @xmath27 is distributed .",
    "rather , we enumerate a plausible set of structures and tie one to each image , but in the general case , 3 samples from @xmath27 . in spirit , this notion of parts - sparse pictorial structures has appeared in @xcite .",
    "their mixture - of - parts pictorial structures model has similarly relaxed the assumption that the full set of parts needs to appear .",
    "one can indeed use this mixture distribution on @xmath27 .",
    "we further compare our approach to mopps in section [ sec : results ] .    * standard form of 3 . *",
    "the terms of the energy function underlying 3 operate on functions of the parts @xmath31 and @xmath32 rather than the parts directly .",
    "these functions are arbitrary and depend on how the potentials will be modeled ( we specify exact definitions in the next section ) .",
    "the standard form of a 3 from @xmath27 is @xmath33      * unary term .",
    "* the unary term will capture the appearance , shape and the location of the parts :    m((l_i)| ) = & _ a m_a(l_i| ) + & & + & _ s m_s(l_i| ) + & & + & _ l m_l(_i)| ) & & [ eq : m ]    the @xmath34 are coefficients on each term .",
    "the @xmath35 function maps the pixel support part @xmath6 to the pair @xmath36 , where @xmath37 is the centroid of @xmath6 : @xmath38 .",
    "the terms of ( [ eq : m ] ) are described next .        * appearance . * we model appearance in four - dimensions : lab color - space and texton space .",
    "the texton maps use a 61-channel filter bank @xcite of combined leung - malik @xcite and schmid filters @xcite , followed by a k - means process with 64 cluster centers .",
    "no experimentation was performed to optimize the texton codebook filter and size choice .",
    "the appearance model for a particular class @xmath39 is specified by a set of normalized lab+texton histograms , @xmath40 , in both the foreground @xmath41 and background @xmath42 .",
    "the background histograms are specific to class @xmath39 and are modeled using the narrowband @xmath43 of pixels surrounding the foreground ( see figure [ fig : border ] for an illustration ) . for a histogram distance @xmath44 ( we use intersection ) , which is applied in each dimension independently and summed , the ratio @xmath45 specifies our appearance potential .",
    "the numerator term measures the cross - fitness : how well the foreground histogram matches the background model and vice versa ; and the denominator measures the actual fitness of the part @xmath6 to the class histograms .",
    "the smaller the numerator and larger the denominator the better the overall fit and hence the lower the energy .",
    "* shape . *",
    "the capability to model global part shape is an key feature of the 3 model .",
    "we model it nonparametrically using a kernel density estimator . for a pixel @xmath46 , member of part @xmath6 with centroid @xmath37 and class @xmath47 , define its normalized coordinate with respect to its part , @xmath48 where @xmath28 is a vector specifying the width and height of the image .",
    "the shape probability of the pixel is @xmath49 where @xmath50 is the number of samples for this shape from the training data , which have all been normalized with respect to the centroid of their constituent part and reference frame , and @xmath51 is a windowing function that returns @xmath52 if its argument is less than the size of a pixel in the normalized reference frame and @xmath53 otherwise . in practice , we quantize the density and store a discrete map of @xmath54 normalized pixels ; call this map @xmath55 .",
    "recall , a part @xmath6 induces a binary membership map @xmath56 at the ( normalized ) pixel level .",
    "finally , the shape potential is defined as the mean shape probability over the part s constituent pixels : @xmath57 where @xmath58 is the element - wise product and the frobenius norm is used .",
    "example shape densities are shown in figure [ fig : shapes ] .",
    "there is a clear distinction in the expressiveness of this shape model between the objects ( e.g. , airplane , face ) and the stuff ( e.g. , sky , road ) .",
    "the maps for the stuff classes tend to be diffuse and indiscriminate , whereas the maps for the object classes are mostly recognizable .",
    "section [ sec : results ] shows that this object - level modeling significantly aids in labeling of the object - type classes .    *",
    "* we model the part location with a gaussian distribution on its centroid @xmath37 . for class @xmath39 , denote the mean centroid location @xmath59 and the ( full ) covariance matrix @xmath60 .",
    "the location potential is hence the mahalanobis distance : @xmath61 figure [ fig : location ] gives a few examples of the location potential .",
    "note the contrast , in terms of objects and stuff , to the shape potential : the objects tend to be less informative in terms of location than the stuff .     grouped again in terms of objects and stuff . ]",
    "* binary term .",
    "* the binary term will capture the relative distance and angle of pairwise connected parts :    d((l_i),(l_j)| ) = & _ d d_d(_i,_j| ) + & & + & _ r d_r(_i,_j| ) & & [ eq : d ]    the @xmath34 are again coefficients on each term .",
    "the @xmath62 function maps the pixel support part @xmath6 to the @xmath37 , is the centroid of @xmath6 .",
    "more sophisticated @xmath35 and @xmath62 functions are plausible with the 3 framework , but we do not explore them in this paper .    * distance . * the relative part location is captured simply by the distance between the parts ( classical pictorial structures ) . for parts @xmath6 and @xmath11 , we evaluate the distance @xmath63 and model it by a gaussian parameterized by @xmath64 .",
    "the distance potential is @xmath65    * angle .",
    "* we model the relative angle between the two parts by a von mises distribution , which is a symmetric and unimodal distribution on the circle @xcite .",
    "let @xmath66 denote the angle relating part @xmath67 with respect to part @xmath68 .",
    "the von mises distribution is a function of @xmath69 , the mean direction , and @xmath70 the concentration parameter ( similar to variance ) : @xmath71 } { 2\\pi i_0(\\kappa_{z_iz_j } ) }   \\label{eq : vonmises}\\end{aligned}\\ ] ] where @xmath72 is a bessel function .",
    "finally , the angle potential is the negative log : @xmath73 some examples are presented in figure [ fig : angle ] .",
    "these examples suggest this angle potential is jointly useful for the objects and the shape , especially how they inter - relate .",
    "for example , consider the rightmost plots of tree - given - cow : in the msrc dataset , cows appear in ( or _ on _ ) pasture nearly always and there are often trees on the horizon .          for a training set of images @xmath74 and corresponding configurations @xmath75 , which are essentially pixel - wise image labelings , learning the parameters is cast as a maximum likelihood ( mle ) problem .",
    "@xcite show that the parameters @xmath12 on the unary potentials can be learned independently , and this holds for our pixel - support parts . in our case",
    ", we do not seek to learn a tree - structured graph .",
    "instead , we define a deterministic mapping from a label image , or configuration , @xmath76 to a graph @xmath77 in the following manner : for each connected component in @xmath76 create a part in the graph .",
    "two parts are adjacent in the graph if any pixel in their respective connected components are adjacent in the label image @xmath76 .",
    "it is our assumption that this general structure adds necessary descriptiveness for the labeling problem .",
    "finally , for each pair of adjacent parts , we learn the parameters on the binary potentials via mle .",
    "the last part of learning is to estimate the five @xmath78 weights on the various potentials .",
    "it is widely known that estimating these weights is a significant problem as it requires estimation of the full partition function , which is intractable @xcite . however , in our case , the problem is compounded , even some standard approximations like pseudo - likelihood are intractable because of the pixel - support nature of the parts .",
    "because of these complexities , we simply set the coefficients such that the relative scale of each potential is normalized and finally we ensure the weights specify a convex combination over the potentials .",
    "[ sec : sa ]    inference with the 3 model has two main components : ( 1 ) determine the structure of the 3 for the image at hand , and ( 2 ) determine the optimal configuration @xmath79 given a structure . in this paper",
    ", we study the latter and leave the former for future work .",
    "although this limits the generality of the contributions proposed herein to cases in which a suitable structure for the 3 could be determined or given , we show that even the determination of the optimal configuration alone is a significant problem .",
    "furthermore , direct extensions of the proposed methods present viable options for handling the structure inference , as we will discuss .",
    "the configuration inference problem is posed as @xmath80\\enspace.\\end{aligned}\\ ] ] the corresponding energy minimization problem is @xmath81 . in general , this problem is np - hard , but seems similar to the standard form for which our community has hotly studied approximate solutions over the past decade @xcite .",
    "however , as noted by @xcite , the structure of the graph and the space of possible solutions differ substantially ; in other words , the minimization problem can not be cast as a local labeling problem .",
    "consider the variables in question , @xmath82 .",
    "we already know the class @xmath47 of each part and each @xmath6 has a complex interrelationship to the other parts via its pixel support .",
    "for example , taking one element @xmath22 away from @xmath6 and moving it to @xmath11 has part - global effects on both @xmath6 and @xmath11 in terms of appearance and shape , which differs quite drastically from these prior methods .",
    "one could consider defining the 3 inference as a labeling problem over the elements @xmath83 with each part @xmath6 being a labeling index and associating a label variable , say @xmath84 , with each element @xmath85 .",
    "however , inference would remain outside of the scope of these methods , again because a local change of one label variables @xmath84 would have a far - reaching affect on many other elements @xmath86 .",
    "in addition , classical pictorial structures use parametric representations of @xmath6 , such as part - centroid , and for the typical spring - model case , define a mahalonobis distance to capture the ideal relative location between parts . casting our nonparametric form @xmath6 into this framework would yield an intractable high - dimension problem : even though we rely on parametric functions of @xmath6 for our binary potentials ( [ eq : d ] ) no convenient form of the ideal location is possible since the @xmath6 are tied directly to the pixel support",
    ".    * mcmc sampler .",
    "* we hence adopt a metropolis - hastings ( mh ) approach to handle the general inference @xcite .",
    "the mh sampler is straightforward and yet , guaranteed to ( eventually ) sample from the underlying invariant distribution @xmath87 as it satisfies the detailed balance equation , even when @xmath87 is known only up to a constant .",
    "furthermore , the clique gibbs form of @xmath87 guarantees such an invariant distribution exists @xcite .",
    "mh is an iterative algorithm that walks a markov chain through the state space according to the following acceptance probability @xmath88 q(l^{(t)}|l ' ) } { \\exp \\left [ - h(l^{(t)}|i,\\theta ) \\right ]   q(l'|l^{(t ) } ) } \\biggr\\rbrace \\enspace , \\label{eq : mh}\\end{aligned}\\ ] ] where @xmath89 is the chain configuration at time @xmath90 , @xmath91 is the proposed move , and @xmath92 is the proposal distribution .    we adopt a superpixel specification of the elements @xmath83 , computed via @xcite . each proposed move of the markov chain acts by moving a superpixel from one part , say @xmath6 , to another part , say @xmath11 .",
    "such moves are proposed according to the following proposal distribution : @xmath93 + \\biggr .",
    "\\nonumber \\\\ & \\biggl .",
    "\\bigl(1-\\delta(l_i , l_\\lambda)\\bigr ) \\left [   \\frac{d\\left(h_{l_i},h_\\lambda\\right ) } { d\\left(h_{\\partial l_i},h_\\lambda\\right ) } \\right ] \\biggr ) \\label{eq : proposal } \\enspace,\\end{aligned}\\ ] ] where the @xmath22 is the proposed element to change , @xmath94 is the normal dirac delta , @xmath95 is the histogram from element @xmath22 , @xmath96 is the normalizing term , and we have overloaded the @xmath97 notation to mean the part containing @xmath22 in the current @xmath89 configuration .",
    "the proposal distribution has an intuitive explanation .",
    "first , we uniformly sample from each of the parts .",
    "second , we sample the elements according to how well they would fit their new role with respect to the sampled part based on the ratio of foreground to background appearance fit if the element is currently outside of the part and vice versa . although not represented in the equation for clarity , we only consider those elements touching the boundary of sampled part ( both inside and outside ) .",
    "although the chain is guaranteed to converge regardless of its initialization @xcite , we initialize @xmath98 by assigning each superpixel @xmath22 based on the ratio of its distance and appearance likelihood to each part in the 3 graph .",
    "* data - adaptive simulated annealing . *",
    "we embed the mh sampler into a simulated annealing process @xcite because we seek the maximum a posteriori samples .",
    "simulated annealing adds a temperature parameter @xmath99 into the distribution , @xmath100 $ ] , such that as @xmath101 the @xmath102 distribution approaches the modes of @xmath87 .",
    "however , the theoretical guarantee exists on fairly restrictive bounds on the _ cooling schedule _ , the sequence of temperatures as the process is cooled @xcite .",
    "furthermore , it is not well understood how to set the cooling schedule in practice , especially for very high - dimensional sample spaces , such as the one at hand .",
    "the challenge is that one proposal move @xmath91 will change the density quite little resulting in acceptance probabilities near uniform unless the cooling schedule is tweaked just right .    to resolve this issue",
    ", we propose an principled approach to set the cooling schedule that adapts to each image at hand and requires no manual tweaking .",
    "the basic idea is to directly estimate a map from desired acceptance probabilities to the required temperatures .",
    "denote @xmath103 as shorthand for the acceptance probability . disregarding the proposal distribution ,",
    "consider @xmath103 written directly in terms of the amount @xmath104 of energy the proposed move would make : @xmath105 }       { \\exp\\left[- h\\left(l^{(t)}\\right ) / t \\right ] } = \\frac{\\exp\\left[- \\left ( h\\left(l^{(t)}\\right)+\\rho\\right ) / t \\right ] }       { \\exp\\left[- h\\left(l^{(t)}\\right ) / t \\right ] } \\label{eq : adaptivesa1}\\end{aligned}\\ ] ] for a specific desired @xmath103 value and known @xmath104 , we can solve ( [ eq : adaptivesa1 ] ) for @xmath106 making it possible to adapt the simulated annealing cooling schedule to each image in a principled manner , rather than manually tuning parameters by hand . before beginning the annealing process",
    ", we sample @xmath87 to estimate the @xmath104 for the image . assuming a linear gradient of desired acceptance ratios , the only part that needs to be manually set is the acceptance probability range , @xmath107 , @xmath108 , which we set to @xmath109 and @xmath110 respectively to cover most of the range of acceptance probabilities but never making them guaranteed or impossible .",
    "we use two pixel - labeling benchmark datasets for our experimental analysis : msrc @xcite and sift - flow @xcite . in brief , msrc is a 21-class 596-image dataset and sift - flow is a 33-class 2688-image dataset , both of typical natural photos . the gold standard for these data",
    "is set by manual human labeling and most images have a large percentage of pixels actually labeled in the gold standard . in both cases , we use the posted training - testing splits for learning and evaluation ; we note the split is 55% training for msrc and 90% training for sift - flow .",
    "finally , in the posted split for sift - flow , three classes ( cow , desert , and moon ) do not appear and are hence dropped , yielding a 30-class dataset in actuality .",
    "our primary evaluation goal is to determine and quantify the benefit gained through the global parts - based structure .",
    "hence we make a quantitative comparison of our method against an mle classifier and an mrf , with results shown in figure [ fig : results : quantitative ] . in all cases",
    ", we use the same appearance models and assume full knowledge of the graph structure ( for the mle and mrf methods , this means the subset of possible class labels ) for each test image . in the mle case ,",
    "the basic elements are assumed independent , and in the mrf case , a local potts smoothness model is used between the basic elements .",
    "we note that our proposed 3 model is also an mrf , but over the global scene parts and not over the superpixel basic elements .",
    "we do not make a comparison to other pictorial structures papers as , to the best of our knowledge , no existing pictorial structures method can be directly applied to the pixel labeling problem .",
    "we also do not show a comparison against other methods quantitative scores on these data sets , such as @xcite who currently have the highest score on msrc , and we want to caution the reader against doing so .",
    "the assumption on knowing the graph structure that we have made limits the comparability of our proposed method against these others , and our appearance model is comparatively simpler .",
    "our quantitative scores need to be interpreted as relative among the three methods we have displayed : in all three cases we have used the same appearance model ( discussed earlier ) allowing for a controlled experimental analysis in which the aspect varied is how the basic elements ( superpixels ) are related in the overall model . in this setting",
    ", it is clearly demonstrated that the proposed model outperforms both the superpixel - independent mle classifier and the locally connected mrf model .",
    "our proposed method performs best in global and average per - class labeling accuracy over the pixel - independent mle and local - mrf methods on both datasets .",
    "on msrc we see a gain of 2% in global and 3% in average accuracy .",
    "these are not significant numbers , overall , but we note the significant improvement in two subsets of the classes .",
    "first , in classes with high intraclass variance , such as building , we see a 30+% increase .",
    "second , in classes with strong global object shape , such as airplane , we see a 20% increase .",
    "these exhibit the merits the global modeling of 3 brings to the problem .",
    "the reason why the overall gain is not too much is that the dominant classes , such as sky , grass , and so on , have a strong visual character in the relatively small msrc data set that is already easily modeled at the local level .",
    "we find a different case in the sift - flow dataset , which is much larger and contains more intra - class variance even for these dominant classes . in the sift - flow cases , a larger increase of 9% in global and 12% in average accuracy is observed .",
    "we bring note to the marked improvement in some of the categories corresponding to _ things _ , such as airplane , car , door , and person .",
    "we explain this improvement as being due to the added modeling richness in the parts - based representation : _ things _ in the image benefit from the rich global description through the shape and part - context .",
    "we also note the few categories in sift - flow where 3 was outperformed by the mle and mrf methods ( bridge , crosswalk , fence , and sun ) . in these cases ,",
    "the typical object foreground is _ sparse _ and the global part description is insufficient to accommodate the weak local cues , which get overtaken by the other nearby classes .",
    "examples of this phenomena ( as well as good cases ) are given in figure [ fig : results : visual ] .",
    "we also make a quantitative ( figure [ fig : msrc : quantitative ] ) against a range of papers from the state of the art , textonboost @xcite , mean - shift patches @xcite , graph - shifts @xcite , textonforests @xcite , and hierarchical crf ( h - crf ) @xcite .",
    "nearly all of these papers can be classes within the `` local '' labeling realm .",
    "the state - of - the - art h - crf approach in @xcite makes a clever extension to define a hierarchy of random fields that has shown great potential to overcome the limitations of purely local labeling methods .",
    "however , it still defines the labeling problem based directly on local interactions of label variables rather than on object level interactions , as we do in 3 .",
    "none of the existing pictorial structures papers we are aware of can be directly applied to semantic image labeling and are hence not compared here .",
    "our proposed 3 method performs best in average per - class labeling accuracy ( 78% ) and shows marked improvement in numerous classes , such as flower , bird , chair , etc .",
    "we make careful note that although the table directly compares 3 to the other literature , we assume the graph structure for each testing image is known ; notwithstanding this point , we do feel it is important to demonstrate the comparative performance against the state of the art .",
    "furthermore , we note that our unary potentials are comparatively simpler ( i.e. , color and texton histograms ) to those in many of the other methods .",
    "finally , knowing the appropriate graph for the image does not immediately solve the problem : an mle assignment of superpixels to elements in the graph yields global accuracy of @xmath111 and average accuracy of @xmath112i.e . , the 3 model is indeed adding power to the problem .    * separating objects from stuff . *",
    "the respective merits of the two top performing approaches , namely h - crf and ours , 3 , become immediately evident when inspecting how the methods compare on various classes , as we discuss next . as we mentioned earlier ,",
    "one can group the parts roughly into two types : objects ( cow , sheep , aeroplane , face , car , bicycle , flower , sign , bird , chair , road , cat , dog , body , and boat ) and stuff ( building , grass , tree , sky , water and road )        ; no further processing was performed .",
    "see text for discussion and the list of classes for the two groups . ]",
    "the objects tend to be small and articulated and have high location variance , whereas the stuff tends to be relatively stable in terms of location and appearance distribution .",
    "as we showed in section [ sec : details ] , the shape distributions of the stuff are uninformative , but for the objects , they are quite informative . we have claimed that a key merit of our method is that it allow the modeler to emphasize global object shape and relationship to the scene in general .",
    "this claim is clearly substantiated when looking at the comparative average accuracy of the objects to the stuff in figures [ fig : msrc : quantitative ] and [ fig : msrc : objects - vs - stuff ] .",
    "we explain it via the components of the 3 model as follows : our method performs at about the average performance for the stuff classes , which are comparatively easier to infer using location and appearance .",
    "subsequently , these stuff classes are grounded and drive the object classes during inference allowing them to utilize the objects richer shape and angle potentials .",
    "* comparison to ddmcmc @xcite . * the seminal ddmcmc work laid the groundwork for our approach to inference in this paper , but the underlying problem and model are quite different .",
    "firstly , the ddmcmc work is an approach to the low - level image segmentation problem .",
    "no notion of object class or global object is incorporated into that work , which , as our quantitative results demonstrated is a significant merit of our proposed approach .",
    "secondly , it is primarily seeking samples of image segmentations under all plausible partitions of the image .",
    "we have restricted ours to the set of superpixels , but we can plausibly relax our assumption .",
    "lastly , their work did not seem to seek the modes , whereas we propose a data - adaptive method for mode seeking in the mcmc framework .",
    "* comparison to mixture - of - parts pictorial structures ( mopps ) @xcite . * as far as we know , mopps is the first and only pictorial structures extension to permit part subsets . like our method , they permit a space of plausible pictorial structures . then , the mopps method carefully specifies mixture distribution over parts , a set of legal part configurations and a mechanism for returning a classical pictorial structure given a part subset . in the spirit of sparse - parts pictorial structures ,",
    "3 is similar to mopps .",
    "but mopps remains restricted to the object modeling case ( or in the paper , a nice extention to highly articulated football team players as an object ) . for a given parts subset ,",
    "the mopps structure is classical ( gaussian spring model ) whereas our part potential incorporate a more rich set of relations .",
    "we have presented the pixel - support , parts - sparse pictorial structures , or 3 model .",
    "3 makes a step in scene labeling by moving beyond the de facto local and region based approaches to full semantic scene labeling and into a rich object - level approach that remains directly tied to the pixel level . as such , 3 unifies parts - based object models and scene - based labeling models in one common methodology .",
    "our experimental comparisons demonstrate the merits in moving beyond the restrictive local methods in a number of settings on benchmark data sets ( msrc and sift flow ) .",
    "3 has , perhaps , opened more problems than it has solved , however .",
    "for example , we have assumed that the graph for an image is known during inference . for general applicability",
    ", this assumption needs to be relaxed .",
    "extensions of the proposed mcmc methods into jump - diffusion dynamics @xcite are plausible , but some approximations or other methods to _ marginalize _ the full sample - space are also plausible .",
    "probabilistic ontologies and markov logic present two potential avenues for this problem .",
    "similarly , we have demonstrated that the parameter estimation problem in the 3 is more complex given the global - local part - pixel dependency .",
    "we are not aware of a principled tractable method for estimating these parameters .",
    "finally , we have observed a big disparity in the respective strength of our various model terms for object- and stuff - type classes , but we have not incorporated this distinction into the model itself .",
    "we are grateful for the support in part provided through the following grants : nsf career iis-0845282 , aro yip w911nf-11 - 1 - 0090 , darpa mind s eye w911nf-10 - 2 - 0062 , darpa cssg hr0011 - 09 - 1 - 0022 .",
    "findings are those of the authors and do not reflect the views of the funding agencies ."
  ],
  "abstract_text": [
    "<S> scene understanding remains a significant challenge in the computer vision community . </S>",
    "<S> the visual psychophysics literature has demonstrated the importance of interdependence among parts of the scene . yet , </S>",
    "<S> the majority of methods in computer vision remain local . </S>",
    "<S> pictorial structures have arisen as a fundamental parts - based model for some vision problems , such as articulated object detection . </S>",
    "<S> however , the form of classical pictorial structures limits their applicability for global problems , such as semantic pixel labeling . in this paper </S>",
    "<S> , we propose an extension of the pictorial structures approach , called pixel - support parts - sparse pictorial structures , or 3 , to overcome this limitation . </S>",
    "<S> our model extends the classical form in two ways : first , it defines parts directly based on pixel - support rather than in a parametric form , and second , it specifies a space of plausible parts - based scene models and permits one to be used for inference on any given image . </S>",
    "<S> 3 makes strides toward unifying object - level and pixel - level modeling of scene elements . in this report </S>",
    "<S> , we implement the first half of our model and rely upon external knowledge to provide an initial graph structure for a given image . </S>",
    "<S> our experimental results on benchmark datasets demonstrate the capability of this new parts - based view of scene modeling . </S>"
  ]
}