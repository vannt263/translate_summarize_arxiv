{
  "article_text": [
    "this paper introduces a fast penalized spline method for bivariate smoothing .",
    "it also gives the first local central limit theorem for a bivariate spline smoother .",
    "suppose there is a regression function @xmath0 with @xmath1 ^ 2 $ ] .",
    "initially we assume that @xmath2 , where the @xmath3 s are independent with @xmath4 and @xmath5 , and the design points @xmath6 are deterministic ; thus , the total number of data points is @xmath7 and the data are on a rectangular grid . in section  [ sec :",
    "irregular ] we relax the design assumption to fixed design points not in a regular grid and random design points . with the data on a rectangular grid ,",
    "they can be organized into an @xmath8 matrix @xmath9 .",
    "we propose to smooth across the rows and down the columns of @xmath9 so that the matrix of fitted values @xmath10 satisfies @xmath11 where @xmath12 ( @xmath13 ) is the smoother matrix for @xmath14 ( @xmath15 ) .",
    "so fixing one covariate , we smooth along the other covariate and vice versa , although the two smooths are simultaneous as implied by  ( [ haty ] ) .",
    "estimator is similar in form to the sandwich formula for a covariance matrix , which suggested the name",
    " sandwich smoother \" to a referee .",
    "we have adopted this term .",
    "the tensor product structure of the sandwich smoother allows fast computations , specifically of the generalized cross validation ( gcv ) criterion for selecting smoothing parameters ; see section  [ sec : algorithm ] .",
    "dierckx ( 1982 ) proposed a smoother with the same structure as  ( [ haty ] ) , but our asymptotic analysis and the fast implementation for the sandwich smoother are new",
    ". for smoothing two - dimensional histograms , eilers and goeman ( 2004 ) studied a simplified version of the sandwich smoother with special smoother matrices that lead to non - negative smooth for non - negative data .",
    "the fast method for the sandwich smoother can be applied to their method .    for bivariate spline",
    "smoothing , there are two well known estimators : bivariate p - splines ( eilers and marx , 2003 ; marx and eilers , 2005 ) and thin plate splines , e.g. , the thin plate regression splines ( wood , 2003 ) . for convenience",
    ", the eilers - marx and wood estimators will be denoted by e - m and tprs , respectively .",
    "we use e - m without specification of how the estimator is calculated .",
    "penalized splines have become popular over the years , as they use fewer knots and in higher dimensions require much less computation than smoothing splines or thin plate splines .",
    "see ruppert _",
    "et al . _  ( 2003 ) or wood ( 2006 ) for both methodological development and applications . however , the theoretical study of penalized splines has been challenging . an asymptotic study of univariate penalized splines",
    "was achieved only recently ( opsomer and hall , 2005 ; li and ruppert , 2008 ; claeskens _ et al .",
    "_ , 2009 ; kauermann _ et al . _ , 2009 ; wang _ et al . _ , 2011 ) . the asymptotic convergence rate of smoothing splines , on the other hand , has been well established ; see gu ( 2002 ) for a comprehensive list of references .",
    "the theoretical study of penalized splines in higher dimension is more challenging . to the best of our knowledge",
    ", the literature does not contain central limit theorems or explicit expressions for the asymptotic mean and covariance matrix of @xmath16 for bivariate spline estimators of any kind .",
    "the sandwich smoother has a tensor product structure that simplifies asymptotic analysis , and we show that the sandwich smoother is asymptotically equivalent to a kernel estimator with a product kernel . using this result , we obtain a central limit theorem for the sandwich smoother and simple expressions for the asymptotic bias and variance .    for smoothing of array data , the generalized linear array model ( glam ) by currie _",
    "et al._(2006 ) gives a low storage , high speed algorithm by making use of the array structures of the model matrix and the data .",
    "the e - m estimator can be implemented with a glam algorithm ( denoted by e - m / glam ) .",
    "the sandwich smoother can also be extended to array data of arbitrary dimensions where a glam algorithm can improve the speed of the sandwich smoother ; see section  [ sec : multi ] . because of the fast methods in sections  [ sec : algorithm ] and [ sec : multi : algorithm ] for computing the gcv criterion , a glam algorithn is much faster when used to calculate the sandwich smoother than when used to calculate the e - m estimator . in table",
    "[ time ] in section  [ sec : speed ] , we see that the sandwich smoother is many orders of magnitude faster than the e - m / glam estimator over a wide range of sample sizes and numbers of knots .",
    "the remainder of this paper is organized as follows . in section",
    "[ sec : bivariate ] , we give details about the sandwich smoother . in section  [ sec : asymptotics ] , we establish an asymptotic theory of the sandwich smoother by showing that it is asymptotically equivalent to a bivariate kernel estimator with a product kernel . in section  [ sec : irregular ] , we consider irregularly spaced data . in section  [ sec : simulations ] , we report a simulation study . in section  [ sec : covariance ] , we compare the sandwich smoother with a local linear smoother for estimating covariance functions of functional data .",
    "we find that the sandwich smoother is many orders of magnitude faster than the local linear smoother and they have similar mean integrated squared errors ( mises ) . in section  [ sec : multi ]",
    ", we extend the sandwich smoother to array data of dimension greater than two .",
    "let be the operation that stacks the columns of a matrix into a vector . define @xmath17 and @xmath18 . applying a well - known identity of the tensor product ( seber 2007 , pp .",
    "240 ) to  ( [ haty ] ) gives @xmath19 identity  ( [ haty ] ) shows that the overall smoother matrix is a tensor product of two univariate smoother matrices .",
    "because of this factorization of the smoother matrix , we say our model has a tensor product structure .",
    "we shall use p - splines ( eilers and marx , 1996 ) to construct univariate smoother matrices , i.e. , @xmath20 where @xmath21 and @xmath22 are the model matrix for @xmath14 and @xmath15 using b - spline basis ( defined later ) , and @xmath23 and @xmath24 are differencing matrices of difference orders @xmath25 and @xmath26 , respectively .",
    "then the overall smoother matrix can be written out using identities of the tensor product ( seber 2007 , pp .",
    "235 - 239 ) , @xmath27    the inverse matrix in the second equality of  ( [ hat2 ] ) shows that our model uses tensor - product splines ( defined later ) with penalty @xmath28 on the coefficients matrix .",
    "the tensor - product splines of two variables ( dierckx 1995 , ch .",
    "2 ) is defined by @xmath29 where @xmath30 and @xmath31 are b - spline basis functions for @xmath14 and @xmath15 , respectively , @xmath32 and @xmath33 are the numbers of basis functions for the univariate splines , and @xmath34 is the coefficients matrix .",
    "we use b - splines of degrees @xmath35 ( @xmath36 ) for @xmath14 ( @xmath15 ) , and use @xmath37 ( @xmath38 ) equidistant interior knots .",
    "then @xmath39 , @xmath40 .",
    "it follows that the model is @xmath41 where @xmath42 , @xmath43 , and @xmath44 is an @xmath8 matrix with @xmath45 entry @xmath3 .",
    "let @xmath46(@xmath47 ) .",
    "then an estimate of @xmath48 is given by minimizing @xmath49 where the norm is the frobenius norm and @xmath50 is defined in  ( [ p ] ) .",
    "it follows that the estimate of the coefficient matrix @xmath51 satisfies @xmath52 where for @xmath53 , @xmath54 , or equivalently , @xmath55 satisfies @xmath56 then our penalized estimate is @xmath57 with  ( [ eqnobj ] ) , it is straightforward to show that @xmath58 satisfies  ( [ haty ] ) , which confirms that the proposed method uses tensor - product splines with a particular penalty .      the only difference between the sandwich smoother and the e - m estimator ( marx and eilers , 2003 ; eilers and marx , 2006 ) is the penalty .",
    "let @xmath59 denote the penalty matrix for the e - m estimator , then @xmath60 .",
    "the first and second penalty terms in bivariate p - splines penalize the columns and rows of @xmath47 , respectively , and are thus called column and row penalties .",
    "it can be shown that the first penalty term in  ( [ p ] ) , @xmath61 , like @xmath62 , is a  column \" penalty , but it penalizes the columns of @xmath63 instead of the columns of @xmath47 .",
    "we call this a modified column penalty . the implication of this modified column penalty can be seen from a closer look at model  ( [ model ] ) . by regarding ( [ model ] ) as a model with b - spline base @xmath21 and coefficients @xmath63 , ( [ model ] ) becomes a varying - coefficients model ( hastie and tibshirani , 1993 ) in @xmath14 with coefficients depending on @xmath15 .",
    "so we can interpret the modified column penalty as a penalty for the univariate p - spline smoothing along the @xmath14-axis .",
    "similarly , the penalty term @xmath64 for the sandwich smoother penalizes the rows of @xmath65 and can be interpreted as the penalty for the univariate p - spline smoothing along the @xmath15-axis .",
    "the third penalty in  ( [ hat2 ] ) corresponds to the interaction of the two univariate smoothing .",
    "we derive a fast implementation for the sandwich smoother by showing how the smoothing parameters can be selected via a fast computation of .",
    "gcv requires the computation of @xmath66 and the trace of the overall smoother matrix .",
    "we need some initial computations .",
    "first , we need the singular valued decompositions @xmath67 where @xmath68 is the matrix of eigenvectors and @xmath69 is the vector of eigenvalues . for @xmath70 , let @xmath71 , then @xmath72 and @xmath73 .",
    "it follows that for @xmath53 , @xmath74 with @xmath75 .",
    "we first compute @xmath66 . substituting @xmath76 for @xmath77 in equation  ( [ haty ] )",
    "we obtain @xmath78 where @xmath79 .",
    "let @xmath80 , then @xmath81 we shall use the following operations on vectors : let @xmath82 be a vector containing only positive elements , @xmath83 denotes the element - wise squared root of @xmath82 and @xmath84 denotes the element - wise inverses of @xmath82 .",
    "we can derive that @xmath85 where @xmath86 for @xmath53 and @xmath87 is a vector of @xmath88 s with length @xmath89 .",
    "see appendix  [ sec : derivation ] for the derivation of  ( [ gcv0 ] ) .",
    "the right hand of  ( [ gcv0 ] ) shows that for each pair of smoothing parameters the calculation of @xmath90 is just two inner product of vectors of length @xmath91 and the term @xmath92 just needs one calculation for all smoothing parameters .",
    "next , the trace of the overall smoother matrix can be computed by first using another identity of the tensor product ( seber 2007 , pp .",
    "235 ) @xmath93 and then using a trace identity @xmath94 ( if the dimensions are compatible ) ( seber , 2007 , pp .",
    "55 ) and as well as the fact that @xmath95 , @xmath96 where @xmath97 is the @xmath98th element of @xmath69 .    to summarize , by equations ( [ gcv0 ] ) , ( [ trace ] ) and ( [ trace2 ] ) we obtain a fast implementation for computing gcv that enables us to select the smoothing parameters efficiently .",
    "because of the fast implementation , the sandwich smoother can be much faster than the e - m / glam algorithm ; see section [ sec : speed ] for an empirical comparison . for the e - m / glam estimator",
    ", the inverse of a matrix of dimension @xmath99 is required for every pair of @xmath100 , while for the sandwich smoother , except in the initial computations in ( [ eq : dr01 ] ) , no matrix inversion is required .",
    "in this section , we derive the asymptotic distribution of the sandwich smoother and show that it is asymptotically equivalent to a bivariate kernel regression estimator with a product kernel .",
    "moreover , we show that when the two orders of difference penalties are the same , the sandwich smoother has the optimal rate of convergence .",
    "we shall use the equivalent kernel method first used for studying smoothing splines ( silverman , 1984 ) and also useful in studying the asymptotics of p - splines ( li and ruppert , 2008 ; wang _ et al_. , 2011 ) .",
    "a nonparametric point estimate is usually a weighted average of all data points , with the weights depending on the point and the method being used .",
    "the equivalent kernel method shows that the weights are asymptotically the weights from a kernel regression estimator for some kernel function ( the equivalent kernel ) and some bandwidth ( the equivalent bandwidth ) .",
    "first , we define a univariate kernel function @xmath101 s are the @xmath102 complex roots of @xmath103 that have positive real parts . here",
    "@xmath104 is the equivalent kernel for univariate penalized splines ( wang _ et al .",
    "_ , 2011 ) . by lemma  [ h_m ] in appendix  [ sec : proof ]",
    ", @xmath104 is of order @xmath105 .",
    "note that the order of a kernel determines the convergence rate of the kernel estimator .",
    "see wand and jones ( 1995 ) for more details .",
    "a bivariate kernel regression estimator with the product kernel @xmath106 is of the form @xmath107 , where @xmath108 and @xmath109 are the bandwidths . under appropriate assumptions ,",
    "the sandwich smoother is asymptotically equivalent to the above kernel estimator ( proposition  [ prop1 ] ) . because the asymptotic theory of a kernel regression estimator is well established ( wand and jones , 1995 )",
    ", an asymptotic theory can be similarly established for the sandwich smoother . for notational convenience , @xmath110 implies @xmath111 converges to 1 .",
    "[ prop1 ] assume the following conditions are satisfied .    1 .",
    "there exists a constant @xmath112 such that @xmath113 .",
    "the regression function @xmath0 has continuous @xmath105th order derivatives where @xmath114 .",
    "the variance function @xmath115 is continuous .",
    "the covariates satisfy @xmath116 .",
    "@xmath117 where @xmath118 is a constant .",
    "let @xmath119 , @xmath120 and @xmath121 .",
    "assume @xmath122 and @xmath123 for some constants @xmath124 .",
    "assume also @xmath125 and @xmath126 .",
    "let @xmath127 be the sandwich smoother using @xmath25th ( @xmath26th ) order difference penalty and @xmath128 ( @xmath129 ) degree b - splines on the @xmath14-axis ( @xmath15-axis ) with equally spaced knots .",
    "fix @xmath130 .",
    "+ let @xmath131 .",
    "then @xmath132,\\\\ \\textrm{\\textnormal{var}}\\{\\hat\\mu(x , z)-\\mu^{\\ast}(x , z)\\ } & = o\\{(nh_n)^{-1}\\}.\\end{aligned}\\ ] ]    all proofs are given in appendix  [ sec : proof ] .",
    "[ thm1 ] use the same notation in proposition  [ prop1 ] and assume all conditions and assumptions in proposition  [ prop1 ] are satisfied . to simplify notation ,",
    "let @xmath133 .",
    "furthermore , assume that @xmath134 with @xmath135 @xmath136 , @xmath137 for positive constants @xmath138 and @xmath139 .",
    "then , for any @xmath130 , we have that @xmath140 in distribution as @xmath141 , where @xmath142    the case @xmath143 is important .",
    "the convergence rate of the estimator becomes @xmath144 .",
    "stone ( 1980 ) obtained the optimal rates of convergence for nonparametric estimators . for a bivariate smooth function @xmath0 with continuous @xmath105th derivatives ,",
    "the corresponding optimal rate of convergence for estimating @xmath0 at any inner point of the unit square is @xmath144 .",
    "hence when @xmath143 , the sandwich smoother achieves the optimal rate of convergence . note that the bivariate kernel estimator with the product kernel @xmath145 also has a convergence rate of @xmath144 .    for the univariate case , the convergence rate of p - splines with an @xmath102th order difference penalty is @xmath146 ( see wang _",
    "et al_. , 2011 ) .",
    "so the rate of convergence for the bivariate case is slower which shows the effect of  curse of dimensionality \" .",
    "[ rem3 ] theorem  [ thm1 ] shows that , provided it is fast enough , the divergence rate of the number of knots does not affect the asymptotic distribution . for practical usage ,",
    "we recommend @xmath147 and @xmath148 , so that every bin has at least 4 data points .",
    "note that for univariate p - splines , a number of @xmath149 knots was recommended by ruppert ( 2002 ) .",
    "suppose the design points are random and we use the model @xmath150 , that is @xmath151 , @xmath152 , and @xmath153 now have only a single index rather than @xmath154 as before .",
    "assume the design points @xmath155 are independent and sampled from a distribution @xmath156 in @xmath157 ^ 2 $ ] .",
    "the can not be directly applied to irregularly spaced data .",
    "a solution to this problem is to bin the data first .",
    "we partition @xmath158 ^ 2 $ ] into an @xmath159 grid of equal - size rectangular bins , and let @xmath160 be the mean of all @xmath151 such that @xmath161 is in the @xmath162th bin . if there are no data in the @xmath162th bin , @xmath160 is defined arbitrarily , e.g. , by a nearest neighbor estimator ( see below ) . assuming @xmath163 is a data point at @xmath164 , the center of the @xmath162th bin , we apply the sandwich smoother to the grid data @xmath165 to get @xmath166 where @xmath167 @xmath168 .",
    "then our penalized estimate is defined as @xmath169      for the above estimation procedure to work with the fast implementation in section  [ sec : algorithm ] , we need to handle the problem when there are no data in some bins due to sampling variation . if there are no data in the @xmath170th bin , one solution is to define @xmath160 to be the mean of values in the neighboring bins .",
    "doing this has no effect on asymptotics , since bins will eventually have data . for small samples , filling in empty cells this way allows the sandwich smoother to be calculated , but one might flag the estimates in the vicinity of empty bins as non - reliable .    another solution is to use an algorithm which iterates between the data and the smoothing parameters as follows .",
    "initially , we let @xmath171 if the @xmath162th bin has no data point .",
    "another possibility is to let @xmath160 be , for some @xmath172 , the average of the @xmath173 values of @xmath174 with @xmath175 coordinates located closest to the center of the @xmath162th bin . to determine the smoothing parameters @xmath100 that minimize gcv",
    ", we only calculate the sums of squared errors for the bins with data and ignore the bins with no data .",
    "this gives us an initial pair of smoothing parameters .",
    "then for the bins with no data , we replace the @xmath160 s by the estimated value with this pair of smoothing parameters . now with the updated data , we could obtain another pair of smoothing parameters .",
    "we repeat the above procedure until reaching some convergence .      as",
    "before , we divide the unit interval into an @xmath159 grid and let @xmath176 be the number of bins .",
    "[ thm2 ] assume the following conditions are satisfied .    1 .",
    "there exists a constant @xmath112 such that @xmath177 .",
    "the regression function @xmath0 has continuous @xmath105th order derivatives where @xmath114 .",
    "3 .   the design points @xmath178 are independent and sampled from a distribution @xmath156 with a density function @xmath179 and @xmath179 is positive over @xmath157 ^ 2 $ ] and has continuous first derivatives .",
    "4 .   conditional on @xmath178 , the random errors @xmath180 , are independent with mean 0 and conditional variance @xmath181 .",
    "the variance function @xmath115 is twice continuously differentiable .",
    "@xmath182 and @xmath183 for some constants @xmath184 and @xmath185 .",
    "fix @xmath186 .",
    "then with the same notation and assumptions as in theorem  [ thm1 ] , we have that @xmath187 in distribution as @xmath188 where @xmath189 is defined in  ( [ tilde_mu ] ) and @xmath190 is defined in  ( [ v(x , z ) ] ) .",
    "we assume random design points in theorem  [ thm2 ] .",
    "for the fixed design points , the result in theorem  [ thm2 ] still holds if we replace condition ( c ) with the following : @xmath191 where @xmath192 is the number of data points in the @xmath162th bin and @xmath179 is a continuous and positive function .",
    "this section compares the sandwich smoother , eilers and marx s p - splines implemented with a glam algorithm ( e - m / glam ) and wood s thin - plate regression splines ( tprs ) in terms of mean integrated square errors ( mises ) and computation speed .",
    "section  [ sec : regression ] shows that mises of the sandwich smoother and e - m / glam are roughly comparable and smaller than those of , while section  [ sec : speed ] illustrates the computational advantage of the sandwich smoother over the other smoothers .",
    "two test functions were used in the simulation study : @xmath193 and @xmath194 where @xmath195 . note that @xmath196 was used in wood ( 2003 ) .",
    "the two true surfaces are shown in figure  [ fig0 ] .",
    "performances of the three smoothers were assessed at two sample sizes . in the smaller sample study , each test function was sampled on the @xmath197 regular grid on the unit square , and random errors were iid @xmath198 with @xmath199 equal to 0.1 and 0.5 . in each case , @xmath200 replicate data sets were generated and , for each replicate data , the test function was fitted by the three estimators and the integrated squared error ( ise ) was calculated . for the spline basis and knots",
    "settings , based on the recommendation in remark  [ rem3 ] , @xmath201 and @xmath202 equidistant knots were used for the @xmath14- and @xmath15-axis for the two p - spline estimators .",
    "thus , a total of 150 knots were used to construct the b - spline basis .",
    "cubic b - splines were used with a second order difference penalty . for the thin plate regression estimator ( tprs )",
    ", we implemented the tprs using the function  bam \" in a r package  mgcv \" developed by simon wood . in this study ,",
    "tprs was used with a rank of 150 ( i.e. , the basis dimension is 150 ) .",
    "for all three estimators , the smoothing parameters were chosen by .",
    "the performances of the three estimators were evaluated by the mean ises ( mises ; see table  [ table1 ] ) and also boxplots of the ises ( see figure  [ fig1 ] ) .",
    "[ table1 ]     +   +    from table [ table1 ] we can see that sandwich smoother did better than e - m / glam for estimating @xmath203 while e - m / glam was better for estimating @xmath196 .",
    "the boxplots in figure  [ fig1 ] show that the two p - spline methods are essentially comparable .",
    "compared to the two p - spline methods , tprs gave larger mises except for one case .",
    "one explanation for the relative inferior performance of tprs for estimating @xmath203 is that tprs is isotropic and has only a single smoothing parameter so that the same amount of smoothing is applied in both directions , which might be not appropriate for @xmath203 as @xmath203 is quite smooth in @xmath14 and varies rapidly in @xmath15 ( see figure  [ fig0 ] ) .",
    "a larger sample simulation study with @xmath204 and @xmath205 was also done .",
    "for the two p - spline estimators , the numbers of knots were @xmath206 and @xmath207 .",
    "the rank of the tprs was 1050 , which was the total number of knots used in the two p - spline estimators .",
    "all the other settings were the same as in the smaller sample study .",
    "the resulting mises and boxplots gave the same conclusions as in the smaller sample study . to save space",
    ", we do not show the results here .",
    "the computation speed of the three spline smoothers for smoothing @xmath196 with varying numbers of data points was assessed . for simplicity , we let @xmath208 and considered the case @xmath209 .",
    "we selected the number of knots for the two p - spline smoothers following the recommendation in remark  [ rem3 ] .",
    "we fixed the rank of tprs to the total number of knots used in the p - spline smoothers .",
    "for the two p - spline smoothers , the computation times reported are for the case where the search for optimal smoothing parameters is over a @xmath210 log scale grid in @xmath211 ^ 2 $ ] .",
    "a finer grid with @xmath212 grid points was also used .",
    "the computation was done on 2.83ghz computers running windows with 3 gb of ram .",
    "table  [ time ] summarizes the results and shows that the sandwich smoother is by far the fastest method .",
    "note that the values in parenthesis are the computation time using the finer grid .    to further illustrate its computational capacity , the sandwich smoother was applied to large data with sizes of @xmath213 and @xmath214 . for cubic b - splines coupled with second - order difference penalty , theorem",
    "[ thm1 ] suggested choosing @xmath215 and @xmath216 .",
    "so we let @xmath217 with @xmath218 close to @xmath219 in the simulations .",
    "we also evaluated the speed of e - m / glam . to save time , the e - m / glam",
    "was run for only @xmath220 pairs of smoothing parameters and the computation time was multiplied by 16 ( 64 ) so as to be comparable to that of the sandwich smoother on the coarse ( fine ) grid . the results in table  [ time ] show that the sandwich smoother could process large data quite fast on a personal computer while the e - m / glam is much slower .",
    "the tprs was not applied to these large data as it would require more memory space than the computer could provide .    to summarize ,",
    "the simulation study here and also the fast implementation in section  [ sec : algorithm ] show the advantage of the sandwich smoother over the two other estimators .",
    "so when computation time is of concern , the sandwich smoother might be preferred .",
    "as functional data analysis ( fda ) has become a major research area , estimation of covariance functions has become an important application of bivariate smoothing .",
    "because functional data sets can be quite large , fast calculation of bivariate smooths is essential in fda , especially when the bootstrap is used for inference .",
    "local polynomial smoothing is a popular method in estimating covariance functions ( see e.g. , yao _ et al . _  ( 2005 ) or yao and lee ( 2006 ) ) while other smoothing methods such as kernel ( staniswalis and lee , 1998 ) and penalized splines ( di _ et al .",
    "_ , 2009 ) have also been used . in this section , through a simulation study we compare the performance of the sandwich smoother and local polynomials for estimating a covariance function when the data are observed or measured at a fixed grid .",
    "let @xmath221\\}$ ] be a stochastic process with a continuous covariance function @xmath222 . for simplicity , we assume @xmath223 $ ] .",
    "suppose @xmath224 is a collection of independent realizations of the above stochastic process and we observe the random functions @xmath225 at discrete design points with measurement errors , @xmath226 where @xmath227 is the number of measurements per curve , @xmath228 is the total number of curves , and the @xmath229 are i.i.d .",
    "measurement errors with mean zero and finite variance and they are independent of the random functions @xmath225 .",
    "let @xmath230 . an estimate of the covariance function can be obtained through smoothing the sample covariance matrix @xmath231 by a bivariate smoother .",
    "because we are smoothing a symmetric matrix , for the sandwich smoother we use two identical univariate smoother matrices so there is only one smoothing parameter to select .",
    "we use the commonly used local linear smoother ( yao _ et al .",
    "_ , 2005 , hall _ et al .",
    "_ , 2006 ) for comparison and the bandwidth is selected by the leave - one - curve - out cross validation .",
    "we wrote our own r implementation of the estimator used by yao _",
    "_  ( 2005 ) , since their code is in matlab .",
    "we let @xmath232 where the eigenvalues @xmath233 , and @xmath234 are the eigenfunctions from either of the following @xmath235 the above two sets of eigenfunctions were used in di _ et al . _",
    "( 2009 ) , greven _",
    "et al . _  ( 2010 ) , and zipunnikov _ et al . _  ( 2011 ) .",
    "we let @xmath236 .",
    "we simulate 100 datasets and evaluate the two bivariate smoothers in terms of mean ises ( mises ) .",
    "the results are given in table  [ table2 ] .",
    "from table  [ table2 ] , for case 1 with @xmath237 the local linear smoother is slightly better with smaller mean and standard deviation of ise s and for other cases the two smoothers give close results .",
    "the estimated eigenfunctions by the two smoothers for case 1 with @xmath237 are shown in figure  [ fig3 ] .",
    "the figure shows that both smoothers estimate the eigenfunctions well .",
    "we found similar results for @xmath238 ( results not shown ) .",
    "true and estimated eigenfunctions replicated 100 times with @xmath237 for case 1 .",
    "the variance of noises is @xmath239 .",
    "each box shows the true eigenfunction ( solid black lines ) , the pointwise median estimated eigenfunction ( dashed gray lines ) , the 5th and 95th pointwise percentile curves ( dot - dashed gray lines ) .",
    "the left column is for the sandwich smoother and the right one is for local linear smoother.,width=480 ]    we also compared the computation time of the two smoothers using case 1 for various values of @xmath227 . for the sandwich",
    "smoother , we searched over twenty smoothing parameters . for the local linear",
    "smoother , we fixed the bandwidth . note that selecting the bandwidth by the leave - one - curve - out cross validation means the computation time of the local linear smoother will be multiplied by the number of bandwidths and also the number of curves .",
    "table  [ table3 ] shows that the sandwich smoother is much faster to compute than the local linear smoother for covariance function estimation even when the bandwidth for the latter is fixed .",
    "to summarize , the simulation study suggests that for covariance function estimation when functional data are measured at a fixed grid , the sandwich smoother is comparable to the local linear smoother in terms of mises .",
    "the sandwich smoother is considerably faster to compute than the local linear smoother .",
    "we extend the sandwich smoother to array data of dimensions greater than two .",
    "suppose we have a nonparametric regression model with @xmath240 covariates @xmath241 so the data are collected on a @xmath242-dimensional grid . for simplicity ,",
    "assume the covariates are in @xmath157^d$ ] .",
    "as in the bivariate case , we model the @xmath242-variate function @xmath243 by tensor - product b - splines of @xmath242 variables @xmath244 where @xmath245 are the b - spline basis functions .",
    "we smooth along all covariates simultaneously so that the fitted values and the data satisfy @xmath246 where @xmath77 is the smoother matrix for the @xmath247th covariate using p - splines as in  ( [ s ] ) , @xmath248 is the data vector organized first by @xmath249 , then by @xmath250 , and so on , and @xmath251 is organized the same way as @xmath248 .",
    "similar to equation  ( [ eqnobj ] ) , the estimate of coefficients @xmath55 satisfies @xmath252 and the penalized estimate is @xmath253      two computational issues occur for smoothing data on a multi - dimensional grid .",
    "the first issue is that unless the sizes of @xmath77 s are all small , the storage and computation of @xmath254 will be challenging .",
    "the second issue is selection of smoothing parameters . because of the large number of smoothing parameters involved , finding the smoothing parameters that minimize some model selection criteria such as gcv can be difficult .    the generalized linear array model by currie _",
    "et al._(2006 ) provided an elegant solution to the first issue by making use of the array structures of the model matrix as well as the data .",
    "the smoother matrix @xmath254 in multivariate smoothing has a tensor product structure , hence @xmath251 in  ( [ multi - haty ] ) can be computed efficiently by a sequence of nested operations on @xmath248 by the algorithm .",
    "for instance , consider @xmath255 .",
    "then @xmath251 can be computed efficiently with one line of r code :    .... # the function \" rh \" is the rotated h - transform of an array by a matrix # see currie et al .",
    "( 2006 ) yhat = as.vector(rh(s3,rh(s2,rh(s1,y ) ) ) ) ....    we wrote an r version of the rh function .",
    "the second issue can be easily handled for the multivariate fast p - splines .",
    "because of the tensor product structure of the smoother matrix , the fast implementation in section  [ sec : algorithm ] can be generalized for the multivariate case . as an illustration",
    ", we show how to compute the trace of the smoother matrix .",
    "we first compute the singular value decompositions for all @xmath77 so that  ( [ trace2 ] ) holds for all @xmath256 , then we compute the trace of the smoother matrix by @xmath257 using the identity in  ( [ trace ] ) repeatedly .",
    "note that @xmath258 has a similar expression as in  ( [ trace2 ] ) for all @xmath247 .",
    "the sandwich smoother does not have a glm weight matrix and when it is used for bivariate smoothing , there is no need for rotation of arrays , so we do not consider the bivariate sandwich smoother to be a glam algorithm .",
    "however , our implementation for the bivariate sandwich smoother makes use of tensor product structures to simplify calculations similar to what the glam does .",
    "smoothing simulated image data of size @xmath259 with a @xmath260 grid of smoothing parameters , the sandwich smoother takes about 20 seconds on a 2.4ghz computer running mac software with 4 gb of ram .",
    "we have not found the computation time of other smoothers , but we can give a crude lower bound .",
    "we see in table [ time ] that e - m / glam takes about 1400 seconds ( over 20 minutes ) on a @xmath261 two - dimensional grid where the smoothing parameters are searched over a @xmath210 grid . searching over a @xmath262 grid to select the smoothing parameters ,",
    "the number of times of gcv computation is now 20 times more .",
    "moreover , for each gcv computation , e - m / glam will need much more time for smoothing data of size @xmath259 which is much larger .",
    "therefore , the e - m / glam estimator s computation time for smoothing a @xmath259 will be many hours for an algorithm that does not compute gcv as efficiently as the sandwich smoother does .",
    "this research was partially supported by national science foundation grant dms-0805975 and national institutes of health grant r01-ns060910 .",
    "luo xiao s research was partly supported by the national center for research resources grant ul1-rr024996 .",
    "we thank professor iain currie for his helpful discussion on the glam algorithm .",
    "we thank the two referees and the associate editor for their most helpful comments and suggestions which greatly improve this paper .",
    "we are grateful to one referee who suggested the name  sandwich smoother \" .",
    "first we have @xmath263 it can be shown by  ( [ gcv01 ] ) that @xmath264 in the above derivation , @xmath265 denotes the euclidean norm in the second to last equality ; we used the facts that @xmath266 and that both @xmath267 and @xmath268 are diagonal matrices .",
    "similarly we obtain @xmath269 and hence establishes  ( [ gcv0 ] ) .",
    "[ h_m ] the univariate kernel function @xmath270 defined in  ( [ kernel ] ) satisfies the following : @xmath271 hence @xmath270 is of order @xmath105 .",
    "_ proof of lemma [ h_m ] : _ we need to calculate two types of integrals @xmath272 and @xmath273 .",
    "those indefinite integrals are given by results 3 and 4 in gradshteyn and ryzhik ( 2007 , pp .",
    ". then a routine calculation gives the desired result .",
    "part of the lemma is derived in wang _",
    "et al . _  ( 2011 ) .",
    "details of derivation can be found in xiao _",
    "et al . _  ( 2011 ) .",
    "before proving proposition  [ prop1 ] , we need the following lemma :    [ lem1 ] use the same notation in proposition  [ prop1 ] and assume all conditions and assumptions in proposition  [ prop1 ] are satisfied . for @xmath274 , there exists a constant @xmath275 such that @xmath276,\\ ] ] where @xmath277 $ ] .",
    "_ proof of lemma  [ lem1 ] _ : by  ( [ est ] ) , @xmath278 .",
    "we only need to consider @xmath279 for which @xmath280 and @xmath281 are both non - zero .",
    "hence assume @xmath98 and @xmath282 satisfy @xmath283 , @xmath284 .",
    "let @xmath285 and @xmath286 .",
    "denote by @xmath287 the @xmath288th column of @xmath289 and @xmath290 the @xmath288th column of @xmath291 . as shown in xiao _",
    "et al . _  ( 2011 ) and li and ruppert ( 2008 ) , there exist vectors @xmath292 and a constant @xmath293 so that for @xmath294 , @xmath295 , and for @xmath296 or @xmath297 , @xmath298 $ ] . here",
    "@xmath299 if @xmath300 and @xmath301 otherwise .",
    "similarly , there exist vectors @xmath302 and a constant @xmath303 such that for @xmath304 , @xmath305 , and for @xmath306 or @xmath307 , @xmath308 $ ] .",
    "let @xmath309 and @xmath310 , then @xmath311 where @xmath312 $ ] . by equation  ( [ eqnobj ] )",
    ", @xmath313 letting @xmath314 be the @xmath315th element of @xmath292 and similarly @xmath316 the @xmath317th element of @xmath302 , we express @xmath318 as a double sum @xmath319 with equations  ( [ est ] ) ,  ( [ tilde_theta ] ) and  ( [ tilde_theta2 ] ) , we have @xmath320 , \\end{split}\\ ] ] where @xmath277 $ ] .",
    "_ proof of proposition  [ prop1 ] _ : let @xmath321 and @xmath322 . by proposition 5.1 in xiao _",
    "et al . _  ( 2011 ) , there exists some constants @xmath323 such that @xmath324\\\\ & + \\exp\\left(-\\phi_2 \\frac{|x - x_i|}{h_{n,1}}\\right)\\left[o\\left(\\tilde{\\lambda}_1^{-\\frac{1}{m_1}}\\right)+\\delta_{\\{m_1=1\\}}\\delta_{\\left\\{|x - x_i|\\leq ( p_1 + 1)\\tilde \\lambda_1^{-1/(2m_1)}\\right\\}}o\\left(\\tilde{\\lambda}_1^{-\\frac{1}{2m_1}}\\right)\\right ] .",
    "\\end{split}\\ ] ] here @xmath325 if @xmath326 and 0 otherwise ; the other @xmath327 terms are similarly defined .",
    "similarly , there exist some constants @xmath328 such that @xmath329\\\\ & + \\exp\\left(-\\phi_4\\frac{|z - z_j|}{h_{n,2}}\\right)\\left[o\\left(\\tilde{\\lambda}_2^{-\\frac{1}{m_2}}\\right)+\\delta_{\\{m_2=1\\}}\\delta_{\\left\\{|z - z_j|\\leq ( p_2 + 1)\\tilde \\lambda_2^{-1/(2m_2)}\\right\\}}o\\left(\\tilde{\\lambda}_2^{-\\frac{1}{2m_2}}\\right)\\right ] .",
    "\\end{split}\\ ] ] let @xmath330 it follows from lemma  [ lem1 ] that @xmath331 . hence @xmath332 and @xmath333 .    to simplify notation , denote @xmath334 by @xmath335 .",
    "we prove @xmath336 by showing that @xmath337 is @xmath338 . by lemma  [ lem1 ] ,",
    "@xmath277 $ ] . since @xmath339 and @xmath340 , @xmath341 and",
    "hence @xmath342 for simplicity , we shall only show that @xmath343 and we use the case when @xmath344 as an example . because @xmath345 and @xmath346 , equality  ( [ proof_eqn_1 ] ) is proved .",
    "the case when @xmath347 and the desired results involving @xmath348 can be similarly proved .",
    "next we show that @xmath349 , i.e. , @xmath350 .",
    "note that @xmath351 can be expanded into a sum of individual terms . with similar analysis as before , for each individual term in @xmath351 , the double sum over @xmath154 is either @xmath352 , @xmath353 , or is of smaller order .",
    "_ proof of theorem  [ thm1 ] _ : proposition  [ prop1 ] states that the sandwich smoother is asymptotically equivalent to a kernel regression estimator with a product kernel @xmath354 . to determine the asymptotic bias and variance of the kernel estimator , we conduct a similar analysis of multivariate kernel density estimator as in wand and jones ( 1995 ) . by proposition  [ prop1 ] , @xmath355 where we continue using the notation @xmath356 .",
    "let @xmath357 the first term on the right hand of ( [ mua ] ) is the riemann finite sum of @xmath358 @xmath359 on the grid while the second term is the integral of the same function , and @xmath360 calculates the difference between the two terms .",
    "@xmath360 is not random and lemma  [ lem4 ] shows that @xmath361 .",
    "now ( [ kernelest2 ] ) becomes @xmath362 for the double integral in ( [ mean1 ] ) , we first take the taylor expansion of @xmath363 at @xmath175 until the @xmath364th partial derivative with respect to @xmath14 and the @xmath365th partial derivative with respect to @xmath15 , and then we cancel out those integrals that vanish by lemma [ h_m ] .",
    "it follows that explicit expressions for the asymptotic mean can be attained @xmath366 for any two random variables @xmath367 and @xmath368 , if @xmath369 , then @xmath370 .",
    "hence , by letting @xmath371 and @xmath372 , we can obtain by proposition  [ prop1 ] that @xmath373 to get optimal rates of convergence , let @xmath374 and @xmath375 converge to some constants , repsectively .",
    "then we have @xmath376 for some positive constants @xmath377 and @xmath378 .",
    "( recall that @xmath379 . )",
    "we need to choose @xmath380 so that @xmath381 . hence , @xmath382 for some positive constant @xmath383 and @xmath384",
    "similarly , @xmath385 for some positive constant @xmath386 and @xmath387 .",
    "it is easy to verify that @xmath388 .",
    "let @xmath389 be a real function in @xmath157 $ ] with a continuous second derivative .",
    "let @xmath390 for @xmath391 .",
    "assume @xmath392 as @xmath228 goes to infinity .",
    "then @xmath393 where @xmath270 is defined in  ( [ kernel ] ) .",
    "_ proof of lemma  [ lem3 ] _ : first note that @xmath270 is symmetric and is bounded by 1 .",
    "also @xmath270 is infinitely differentiable over @xmath394 $ ] and all the derivatives are bounded by @xmath102 over @xmath394 $ ] .",
    "let @xmath395 $ ] for @xmath391 .",
    "suppose without loss of generality that @xmath396 } |g(u)| \\leq m$ ] .",
    "we have @xmath397 and @xmath398 in the derivation of  ( [ int2 ] ) , the term @xmath399 follows from @xmath400 and @xmath401 the term @xmath402 follows from @xmath403 since @xmath404 when both @xmath405 and @xmath152 are in @xmath406 .",
    "note that we used the equality @xmath407 in the above derivation and we shall use it later as well . combining  ( [ int1 ] ) and  ( [ int2 ] ) , we have @xmath408 for simplicity , denote by @xmath409 and @xmath410 the first and second derivatives of @xmath270 , respectively .",
    "similarly , denote by @xmath411 and @xmath412 the right derivatives of @xmath270 at 0 .",
    "if @xmath413 , then @xmath414 and hence @xmath415 if @xmath416 , then @xmath417 .",
    "then @xmath419 . we have @xmath420 we can similarly prove that ( [ int5 ] ) holds when @xmath421 .",
    "now with  ( [ int4 ] ) and  ( [ int5 ] ) , @xmath422 which finishes the lemma .",
    "[ lem4 ] the term @xmath360 defined in  ( [ mua ] ) is @xmath423 .",
    "_ proof of lemma  [ lem4 ] _ : to simplify notation , let @xmath424 and @xmath425 .",
    "then @xmath426 is @xmath427 by lemma  [ lem3 ] .",
    "note that @xmath428 is bounded by the sum of @xmath429 and @xmath430 because @xmath426 is @xmath431 , ( [ mua_2 ] ) is also @xmath431 . by theorem 9.1 in the appendix of durrett ( 2005 ) ,",
    "@xmath432 exists and is equal to @xmath433 .",
    "hence @xmath432 is continuous and bounded .",
    "lemma  [ lem3 ] implies  ( [ mua_1 ] ) is @xmath434 which finishes our proof .    _ proof of theorem  [ thm2 ] _ : denote the design points @xmath435 by @xmath436 .",
    "applying lemma  [ lem1 ] and the proof of proposition  [ prop1 ] to the binned data @xmath437 with @xmath438 replaced by @xmath439 , we obtain @xmath440 where @xmath441 and @xmath442 is defined similarly to @xmath443 in the proof of proposition  [ prop1 ] with also @xmath438 replaced by @xmath439 .",
    "let @xmath192 be the number of data points in the @xmath162th bin .",
    "then @xmath444 so @xmath445 is a nadaraya - watson kernel regression estimator of the conditional variance function @xmath115 at @xmath446 .",
    "similarly , we can show @xmath447 is a kernel density estimator of @xmath179 at @xmath448 . by the uniform convergence theory for kernel density estimators and nadaraya - watson kernel regression estimators ( see , for instance , hansen ( 2008 ) ) , @xmath449 and @xmath450 it follows by the above two equalities that @xmath451 by an argument similar to one in the proof of proposition  [ prop1 ] , for any continuous function @xmath452 over @xmath157 ^ 2 $ ] , we can derive that @xmath453 then by equalities  ( [ var0 ] ) and  ( [ var1 ] ) , @xmath454 by letting @xmath455 in  ( [ var2 ] )",
    ", we derive from  ( [ as0 ] ) that @xmath456 where @xmath190 is defined in  ( [ v(x , z ) ] ) .",
    "we can write @xmath457 as @xmath458 equality  ( [ n_k , l ] ) implies each bin is nonempty , so by taking a taylor expansion of @xmath459 at @xmath448 we derive from the above equation that @xmath460 it follows by equality  ( [ mu0 ] ) that @xmath461 it is easy to show that @xmath462 where @xmath189 is defined in  ( [ tilde_mu ] ) . in light of equality  ( [ as2 ] ) and the assumption that @xmath182 with @xmath463 , @xmath464 with  ( [ as1 ] ) and  ( [ as3 ] ) , we can show that @xmath465\\rightarrow n\\left\\{0 , v(x , z)/f(x , z)\\right\\}\\ ] ] in distribution and @xmath466 = \\tilde{\\mu}(x , z ) + o_p(1).\\ ] ] equalities  ( [ as4 ] ) and ( [ as5 ] ) together prove the theorem .",
    ": :    claeskens , g. , krivobokova , t. , and opsomer , j. d. ( 2009 ) ,  asymptotic    properties of penalized spline estimators , \" _ biometrika _ , 96 , 529 - 544 . : :    currie , i.d . ,",
    "durban , m. and eilers , p.h.c .",
    "( 2006 ) , `` generalized    linear array models with applications to multidimensional smoothing , ''    _ j. r. statist . soc .",
    "b _ , 68 , 259 - 280 .",
    ": :    di , c. , crainiceanu , c. m. , caffo , b. s. , and punjabi , n. ( 2009 ) ,    `` multilevel functional principal component analysis , '' _ ann .",
    "_ , 3 , 458 - 488 . : :    dierckx , p. ( 1982 ) , `` a fast algorithm for smoothing data on a    rectangular grid while using spline functions , '' _ siam j. numer .",
    "_ , 19 , 1286 - 1304 . : :    dierckx , p. ( 1995 ) , _ curve and surface fitting with splines _ ,    clarendon press , oxford .",
    ": :    durrett , r. ( 2005 ) , _ probability : theory and examples _ , third edition ,    thomson .",
    ": :    eilers , p.h.c . , currie i.d . and durban m. ( 2006 ) ,  fast and compact    smoothing on large multidimensional grids , \" _ comput .",
    "statist . data    anal .",
    "_ , 50 , 61 - 76 . : :    eilers , p.h.c . and goeman , j.j .",
    "( 2004 ) ,  enhancing scatterplots with    smoothed densities , \" _ bioinformatics _ , 20 , 623 - 628 .",
    ": :    eilers , p.h.c . and marx , b.d .",
    "( 1996 ) ,  flexbile smoothing with    b - splines and penalties ( with discussion ) , \" _ statist .",
    "_ , 11 ,    89 - 121 .",
    ": :    eilers , p.h.c . and marx , b.d .",
    "( 2003 ) ,  mulitvariate calibration with    temperature interaction using two - dimensional penalized signal    regression , \" _ chemometrics and intelligent laboratory systems _ , 66 ,    159 - 174 .",
    ": :    gradshteyn , i.s . and ryzhik , i.m.(2007 ) , _ table of integrals , series ,    and products _ ,",
    "new york : academic press .",
    ": :    greven , s. , crainiceanu , c. , caffo , b. and reich , d. ( 2010 ) ,    `` longitudinal functional principal component , '' _ electronic j.    statist .",
    "_ , 4 , 1022 - 1054 .",
    ": :    gu , c. ( 2002 ) , _ smoothing spline anova models _ , new york : springer .",
    ": :    hall , p. , m \" uller , h.g . , and wang , j.l .",
    "( 2006 ) ,  properties of    principal component methods for functional and longitudinal data    analysis , \" _ ann . statist .",
    "_ , 34 , 1493 - 1517 . : :    hansen , b.e .",
    "( 2008 ) ,  uniform convergence rates for kernel estimation    with dependent data , \" _ econometric theory _",
    ", 24 , 726 - 748 .",
    ": :    hastie , t. and tibshirani , r.(1993 ) ,  varying - coefficients models , \"    _ j. r. statist",
    "b _ , 55 , 757 - 796 . : :    laub , a.j.(2005 ) , _ matrix analysis for scientists and engineers _ ,",
    ": :    kauermann , g. , krivobokova , t. and fahrmeir , l. ( 2009 ) , `` some    asymptotic results on generalized penalized spline smoothing , '' _ j. r.    statist .",
    "b _ , 71 , 487 - 503 . : :    li , y . and ruppert d. ( 2008 ) ,  on the asymptotics of penalized    splines , \" _ biometrika _ , 95 , 415 - 436 . : :    marx , b.d . and eilers , p.h.c .",
    "( 2005 ) ,  multdimensional penalized    signal regression , \" _ technometrics _ , 47 , 13 - 22 .",
    ": :    opsomer , j.d . and hall , p. ( 2005 ) ,  theory for penalised spline    regression , \" _ biometrika _ , 95 , 417 - 436 .",
    ": :    ruppert , d. ( 2002 ) ,  selecting the number of knots for penalized    splines , \" _ j. comput .",
    "_ , 1 , 735 - 757 . : :    ruppert , d. , wand , m.p . and carroll , r.j .",
    "( 2003 ) , _ semiparametric    regression _ , cambridge : cambridge university press .",
    ": :    seber , g.a.f .",
    "( 2007 ) , _ a matrix handbook for statisticians _ , new    jersey : wiley - interscience .",
    ": :    staniswalis , j.g . and lee , j.j .",
    "( 1998 ) ,  nonparametric regression    analysis of longitudinal data , \" _",
    "_ , 93 ,    1403 - 1418 . : :    stone , c.j . ( 1980 ) , `` optimial rates of convergence for nonparametric    estimators , '' _ ann .",
    "_ , 8 , 1348 - 1360 .",
    ": :    wand , m.p . and jones , m.c .",
    "( 1995 ) , _ kernel smoothing _ , london : chapman    & hall .",
    ": :    wang , x. , shen , j. and ruppert , d. ( 2011 ) , `` local asymptotics of    p - spline smoothing , '' _ electronic j. statist .",
    "_ , 4 , 1 - 17 . : :    wood , s.n .",
    "( 2003 ) ,  thin plate regression splines , \" _ j. r. statist .",
    "b _ , 65 , 95 - 114 .",
    ": :    wood , s.n .",
    "( 2006 ) , _ generalized additive models : an introduction with    r _ , london : chapman & hall .",
    ": :    xiao , l. , li , y. , apanasovich , t.v . and ruppert , d. ( 2011 ) ,  local    asymptotics of p - splines , \" available at    http://arxiv.org/abs/1201.0708v3 .",
    ": :    yao , f. and lee c.m .",
    "( 2006 ) , ",
    "penalized spline models for functional    principal component analysis , \" _ j. r. statist .",
    "b _ , 68 , 3 - 25 .",
    ": :    yao , f. , m \" uller , h.g . , and wang , j.l .",
    "( 2005 ) ,  functional data    analysis for sparse longitudinal data , \" _",
    "_ ,    100 , 577 - 590 .",
    ": :    zipunnikov , v. , caffo , b. s. , crainiceanu , c. m. , yousem d.m .",
    ",    davatzikos , c. , and schwartz , b.s .",
    "( 2011 ) , `` multilevel functional    principal component analysis for high - dimensional data , '' _",
    "j. comput .    graph .",
    "_ , 20(4 ) , 852 - 873 ."
  ],
  "abstract_text": [
    "<S> we propose a fast penalized spline method for bivariate smoothing . </S>",
    "<S> univariate p - spline smoothers ( eilers and marx , 1996 ) are applied simultaneously along both coordinates . </S>",
    "<S> the new smoother has a sandwich form which suggested the name `` sandwich smoother '' to a referee . </S>",
    "<S> the sandwich smoother has a tensor product structure that simplifies an asymptotic analysis and it can be fast computed . </S>",
    "<S> we derive a local central limit theorem for the sandwich smoother , with simple expressions for the asymptotic bias and variance , by showing that the sandwich smoother is asymptotically equivalent to a bivariate kernel regression estimator with a product kernel . as far as we are aware , </S>",
    "<S> this is the first central limit theorem for a bivariate spline estimator of any type . </S>",
    "<S> our simulation study shows that the sandwich smoother is orders of magnitude faster to compute than other bivariate spline smoothers , even when the latter are computed using a fast glam ( generalized linear array model ) algorithm , and comparable to them in terms of mean squared integrated errors . </S>",
    "<S> we extend the sandwich smoother to array data of higher dimensions , where a glam algorithm improves the computational speed of the sandwich smoother . </S>",
    "<S> one important application of the sandwich smoother is to estimate covariance functions in functional data analysis . in this application , </S>",
    "<S> our numerical results show that the sandwich smoother is orders of magnitude faster than local linear regression . </S>",
    "<S> the speed of the sandwich formula is important because functional data sets are becoming quite large . </S>",
    "<S> + keywords : asymptotics ; bivariate smoothing ; covariance function ; glam ; nonparametric regression ; penalized splines ; sandwich smoother ; thin plate splines . </S>"
  ]
}