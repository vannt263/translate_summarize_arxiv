{
  "article_text": [
    "future large , wide - field photometric surveys such as the large synoptic survey telescope ( lsst ) will produce a vast amount of data , covering a large fraction of the sky every few nights .",
    "the amount of data produced lends itself to new analysis methods which can learn abstract representations of complex data .",
    "deep learning is a powerful method for gaining multiple levels of abstraction , and has recently produced state - of - the - art results in tasks such as image classification and natural language processing ( see  @xcite for an excellent overview of deep learning and refs . within for more details ) .",
    "there are many applications of deep learning for large photometric surveys , such as : ( 1 ) the measurement of galaxy shapes from images ; ( 2 ) automated strong lens identification from multi - band images ; ( 3 ) automated classification of supernovae ; ( 4 ) galaxy cluster identification . in this paper",
    "we will focus on supernovae classification using deep recurrent neural networks .",
    "the lsst , for example , is expected to find over @xmath3 supernova  @xcite .",
    "however , it is estimated that only 5000 to 10,000 will be spectroscopically confirmed by follow up surveys  @xcite , so classification methods need to be developed for photometry . all previous approaches to automated classification  @xcite",
    "have first extracted features from supernovae light curves before using machine learning algorithms .",
    "one of the advantages of deep learning is replacing this feature extraction .    in this work we will use _ supervised _ deep learning . during training , the machine is given inputs and produces a set of output predictions .",
    "it is also given the correct set of outputs .",
    "an objective loss function then measures the error between the predicted and target outputs , and the machine updates its adjustable parameters to reduce the error .",
    "it can then make predictions for unknown outputs .",
    "bidirectional recurrent neural network for sequence classification .",
    "the input vectors at each sequential step are fed into a pair of bidirectional hidden layers , which can propagate information forwards and backwards .",
    "these are then merged to obtain a consensus view of the network , and finally a softmax layer computes classification probabilities .",
    ", width=313 ]    _ recurrent neural networks _ ( rnns ) are a class of artificial neural network that can learn about sequential data ( for an extremely comprehensive overview see  @xcite ) .",
    "they are commonly used for tasks such speech recognition and language translation , but have several possible applications in astronomy and cosmology for processing temporal or spatial sequential data .",
    "rnns have several properties which makes them suitable for sequential information .",
    "the inputs to the network",
    "are flexible , and they are able to recognise patterns with noisy data ( for example the context of a word in a sentence relative to others can vary , or a time stream can contain instrument noise ) .    the main problem with vanilla rnns is that they are unable to store long - term information , so inputs at the end of a sequence have no knowledge of inputs at the start .",
    "this is a problem if the data has long - term correlations .",
    "several types of rnns have been proposed to solve this problem , including _ long short - term memory _",
    "( lstm ) units  @xcite and _ gated recurrent units _ ( gru )  @xcite .",
    "these are similar in concept , in that information is able to flow through the network via a gating mechanism .",
    "another problem with rnns is that information can only flow in one direction . in _",
    "rnns information is able to pass both forwards and backwards .",
    "bidirectional lstm networks have been shown to be particularly powerful where sequential data is accompanied by a set of discrete labels .",
    "the architecture of a typical bidirectional rnn for sequence labelling is shown in fig .",
    "[ fig : network ] , where the squares represent neurons . in this case the inputs , which are vectors at each sequential step , are connected to two hidden rnn layers , either vanilla rnn or memory units .",
    "each hidden layer contains a number of hidden units ( capable of storing information ) , and in each layer information flows either forwards or backwards , but no information passes between the two directions .",
    "several hidden layers can be stacked to form _ deep _ neural networks .",
    "deep networks are capable of learning higher - level temporal or spatial representations , and complex relationships between the inputs and outputs",
    ".    the output from the final set of hidden layers in each direction is merged at each sequential step , and mean pooled ( averaged ) over all steps to obtain a consensus view of the network .",
    "finally , the mean output is fed to a _",
    "softmax _ layer , taking an input vector @xmath4 and returning normalised , exponentiated outputs for each class label @xmath5 , @xmath6 , i.e. a vector of probabilities .",
    "each neuron is connected to another by a weight matrix , and the optimal weights are found by back - propagating the errors from a _ loss function _ of the output layer . for classification problems ,",
    "this is typically the categorical cross - entropy between predictions and targets , defined as @xmath7 where @xmath8 run over the class labels , @xmath9 are the targets for each class ( either 0 or 1 ) and @xmath10 are the predicted probabilities .",
    "back - propagation takes the derivative of the loss with respect to the weights @xmath11 of the output layer , @xmath12 , and uses the chain rule to update the weights in the network .",
    "in this paper we will consider data from the supernovae photometric classification challenge ( spcc )  @xcite , consisting of 21319 simulated supernova light curves . each supernovae sample consists of a time series of flux measurements , with errors , in the @xmath13 bands ( one band for each timestep ) , along with the position on the sky and dust extinction .",
    "an example set of light curves is shown in fig .  [",
    "fig : lightcurve ] .    due to the format of the input data ,",
    "we first do a small amount of data processing to obtain values of the @xmath13 fluxes and errors at each sequential step . we assume the time sequence begins at day 0 for each supernovae , rather than counting days forwards and backwards from the maxima of the light curve . for observations less than @xmath14 hour apart , we group the @xmath13 values into a single vector , ensuring there is at most one filter - type in each group .",
    "if there is more than one filter - type , we further subdivide the group using a finer time interval .",
    "the group time is the mean of the times of each observation , which is reasonable as the time intervals are small compared to the characteristic time of the light curve .    in fig .",
    "[ fig : violinplot ] we show how the length of the grouped - time data vector is related to the duration of the light - curve .",
    "the bottom left subplot shows that more total number of day since the beginning of observation of the light - curve results in a greater number of grouped time elements in the vector . the upper subplot shows the distribution of observation lengths in the spcc data varies significantly with two distinct peaks .",
    "these are grouped into an average of 40-element data vectors as can be seen in the bottom right subplot .    .",
    "( top ) distribution of the total number of days for each light - curve with the minimum , maximum , mean and median values indicated .",
    "( bottom right ) distribution of the number of elements in the grouped time vector with the minimum , maximum , mean and median values indicated .",
    "( bottom left ) the trend showing that more days in the light - curve result in longer group time vectors . ,",
    "width=317 ]    observations are of the form in table  [ tab : augment ] , where any missing values are denoted by a dash . in order to impute the missing value of @xmath5",
    ", we use _ data augmentation _ and randomly select a value between @xmath15 and @xmath16 .",
    "we make 5 random augmentations of all missing data , thereby increasing the size of the dataset fivefold .",
    "we can test the importance of this by training each augmentation separately and comparing the change in accuracy , which we find is @xmath17 .",
    "training with multiple augmentations at once gives the best performance since the network learns to ignore random - filled values .",
    ".[tab : augment ] data augmentation of missing observations .",
    "the missing data is replaced randomly by a value between @xmath15 and @xmath16 . [ cols=\"^,^,^,^,^\",options=\"header \" , ]     one advantage of our approach is that light curve data can be directly input to a _ pre - trained _ model to give very fast evaluation ( @xmath18s ) of supernovae type . in the lower panel of fig .  [",
    "fig : lightcurve ] we input the light curve , as a function of time , of a type - ia supernovae ( excluded from training ) to the pre - trained 2-layer lstm model discussed above .",
    "the classifier ( type - ia vs. non - type - ia ) is initially unsure of classification , with a type - ia probability of around 0.5 .",
    "the probability then decreases slightly , but rapidly increases near the peak of the light curve .",
    "the classifier has high confidence the supernovae is of type - ia at around 60 days , and the final probability is excess of @xmath19 .",
    "this method could therefore be useful to give early indication of supernovae type in surveys .",
    "we also test the same model using a training fraction of 0.25 ( around 5000 supernovae ) , closer to the lower end of the number likely to be followed up for the lsst .",
    "after 5 randomised runs and training for 200 epochs we obtain an accuracy of @xmath20 , auc of @xmath21 and @xmath22 .",
    "the corresponding type - ia purity and completeness are @xmath23 and @xmath24 respectively .",
    "the @xmath25 metric has degraded by @xmath26 for a reduction in data of @xmath27 .    for @xmath28 of the representative spcc data ,",
    "the training dataset is so small that over - fitting is more severe . using the same 2-layer lstm network with 16 hidden units and dropout of 0.5 we find a notable increase in the test loss after @xmath29 epochs , but the accuracy and other metrics remain relatively constant ( @xmath25 values of 0.35 to 0.4 were obtained )",
    "the reason for this apparent discrepancy is that the accuracy , say , simply takes the maximum value of the softmax output layer .",
    "for example , a 2-class problem with output probabilities [ 0.6 , 0.4 ] and target [ 1 , 0 ] has the same accuracy as one with output probabilities [ 0.8 , 0.2 ] .",
    "the loss in the latter case would be lower however , and represents increased confidence of the network in its predictions .",
    "we therefore reject models with severe over - fitting and an increasing cross - entropy loss at the expense of metrics such as @xmath25 , and decrease the model complexity .    for a training fraction of @xmath28",
    "we find a single - layer lstm network , with 4 hidden units , and dropout of 0.5 satisfies this criteria . for 5",
    "randomised runs , training for 200 epochs , we obtain a classification accuracy of @xmath30% , auc of @xmath31 and @xmath32 .",
    "the corresponding type - ia purity and completeness are @xmath33 and @xmath34 respectively .",
    "it is difficult to directly compare the results from the spcc challenge in  @xcite with this work since the figure of merit is quoted as a function of redshift and a non - representative set of light - curves was originally used . in",
    "@xcite the method of  @xcite had the highest average @xmath25 , with 79% purity and 96% accuracy .",
    "this is a , somewhat , confusing average as @xmath35 at a redshift @xmath36 up to @xmath37 at @xmath38 .",
    "other methods performed similarly .",
    "it is better to consider comparison with other methods using post - spcc data , for we obtain results which are competitive with previous approaches .",
    "the analyses by  @xcite and  @xcite are easier to compare . along with",
    "@xcite these employ a two - step process , where features are first extracted by various methods before machine learning classification .",
    "the results obtained for similar sized training sets are comparable as can be seen in the top section of table  [ tab:05_sn1a ] . when using half the dataset to train on we get a higher @xmath25 value , @xmath1 compared to @xmath2 in  @xcite .",
    "the value in  @xcite is also similar given that the sample size is smaller . for a smaller sample training set of 5.2% of all the data we again perform similarly to  @xcite but under perform compare to  @xcite taking into account the slightly larger sample size in the latter case . in  @xcite using",
    "the salt2 fits provided the best average auc over a range of machine learning techniques . by imposing a purity of 90% a completeness of 85%",
    "was achieved while requiring a completeness of 90% reveals a corresponding purity of 85% .    in the second section of table  [ tab:05_sn1a ] the three class categorisation",
    "there is no available data for comparison of this problem , but compared to classification between type - ia vs. non - type - ia , bidirectional recurrent neural networks do well .",
    "the auc and accuracy remain high , still above 90% when the host-@xmath39 is included using a training fraction of 0.5 . using a smaller training fraction of 0.052 ,",
    "the results are worsened similar to the two class categorisation in the top section of table  [ tab:05_sn1a ] .",
    "the third section of table  [ tab:05_sn1a ] shows the results of the early - epoch challenge from spcc . here",
    "only the data before the night of the sixth observation with @xmath40 for each light - curve can be used - a great reduction from the use of the full light - curve .",
    "we do surprisingly well in this case obtaining an accuracy of @xmath41 , auc of @xmath42 and an @xmath43 with a training fraction of 0.5 and including host-@xmath39 .",
    "these values are not far from those obtained using the whole light - curve and are equivalent to the full results of  @xcite .",
    "the results are not as good with a training fraction of 0.052 , but still comparable to our results using the whole light - curve .",
    "the network trained on the partial light - curves does better than suggested from feeding the early - epoch light - curve through a network trained on the full sequence .",
    "this is due to the later parts of the light - curve influencing the weights of the network whilst training .",
    "training on only the initial part of the light - curve optimises the network weights such that early sequence features have more effect , resulting in better accuracy , auc and @xmath25 values than expected .    finally , the bottom section of table  [ tab:05_sn1a ] has the results of the three class categorisation when using the early - epoch data .",
    "the results are similar to the difference between the full light - curve and early - epoch data sn1a categorisation when comparing with the full light - curve 123 categorisation .",
    "it should be noted that the bidirectional network used for the 123 categorisation using the full light - curve revealed sizeable over - fitting when using the early - epoch data and so a unidirectional network was used instead .",
    "we have presented a new method for performing photometric classification of supernovae . machine learning methodology has previously been applied to spcc classification  @xcite . instead of performing feature extraction before classification",
    ", our approach uses the light - curves directly as inputs to a recurrent neural network , which is able to learn information from the sequence of observations .",
    "although we have trained the network on the cross - entropy loss and not the @xmath25 score , for the same sized dataset of @xmath44 supernovae ( including host galaxy photo-@xmath39 ) , @xcite obtained @xmath25 values of 0.33 ( 0.58 ) , and @xcite values of 0.42 ( 0.57 ) , compared to our 0.31 ( 0.64 ) .",
    "recurrent neural networks therefore compare well with other methods when a larger training set is available .",
    "the performance is nt quite as good with a smaller training set , possibly due to the network having to learn from no prior information about ( noisy ) light curves .",
    "the current state - of - the - art for a small training set ( @xmath45 supernovae ) comes from a combination of salt2 ( spectral adaptive light curve template 2 ) template fits and boosted decision trees  @xcite .",
    "it would be interesting to check how how deep learning compares to this with a larger training set .",
    "as well as finding competitive results for the final metrics , we have shown that it is possible to give fast , early evaluation of supernovae type using pre - trained models .",
    "this is possible since the light curve can be fed to the model directly without needing any feature extraction .",
    "most interestingly , we have found that training a network only on the early epoch light - curve data results in a better early - time predictor than using a network trained on entire light - curve data .",
    "our results using only the early - epoch data are close to those using the entire light - curve data for both sn1a and 123 categorisation with both large and small training fractions .",
    "there are several possibilities for future work .",
    "one of the advantages of recurrent neural networks is that inputs are agnostic , so the impact of any additional inputs could be explored .",
    "it would be possible , for example , to even pass the raw images in each filter though a convolutional network and use those as inputs .",
    "we have considered a representative training sample , but spectroscopic follow up surveys may be biased .",
    "the performance of the network could be measured against selection bias , and the results used to inform the best follow up strategy",
    ". further work could also be performed to optimise the early detection probability of the network . finally , to improve performance in the small data regime",
    "one can use _ transfer learning .",
    "_ here , a more complex network is pre - trained on simulations or existing data from other surveys , then the weights of the network are fine - tuned on the new , smaller dataset .",
    "the simulated spcc data used in this work are based on the des instrument , and we are applying transfer learning to real des data for publication in future work .",
    "we appreciate helpful conversations with steven bamford , simon dye , mark sullivan and michael wood - vasey , and natasha karpenka , richard kessler and michelle lochner for help with data acquisition .",
    "is supported by a stfc studentship , and a.m. is supported by a royal society university research fellowship ."
  ],
  "abstract_text": [
    "<S> we apply deep recurrent neural networks , which are capable of learning complex sequential information , to classify supernovae . the observational time and filter fluxes </S>",
    "<S> are used as inputs to the network , but since the inputs are agnostic additional data such as host galaxy information can also be included . using the supernovae photometric classification challenge ( spcc ) data , we find that deep networks are capable of learning about light curves , however the performance of the network is highly sensitive to the amount of training data . for a training size of 50% of the representational spcc dataset ( around @xmath0 supernovae ) </S>",
    "<S> we obtain a type - ia vs. non - type - ia classification accuracy of 94.7% , an area under the receiver operating characteristic curve auc of 0.986 and a spcc figure - of - merit @xmath1 . </S>",
    "<S> when using only the data for the early - epoch challenge defined by the spcc we achieve a classification accuracy of 93.1% , auc of 0.977 and @xmath2 , results almost as good as with the whole light - curve . by employing bidirectional neural networks </S>",
    "<S> we can acquire impressive classification results between supernovae types -i ,  -ii and  -iii at an accuracy of 90.4% and auc of 0.974 . </S>",
    "<S> we also apply a pre - trained model to obtain classification probabilities as a function of time , and show it can give early indications of supernovae type . </S>",
    "<S> our method is competitive with existing algorithms and has applications for future large - scale photometric surveys . </S>"
  ]
}