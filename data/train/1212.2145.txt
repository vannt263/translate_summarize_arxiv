{
  "article_text": [
    "physical objects in the world appear differently depending on the scale of observation / measurement .",
    "take the tree as an example , meaningful observations range from molecules at the scale of nanometers , to leaves at centimeters , to branches at meters , and to forest at kilometers .",
    "this inherent property is ubiquitous and holds equally true for natural language . on the one hand ,",
    "concepts are meaningful only at the right resolution , for instance , named entities usually range from unigram ( e.g. ,  new \" ) to bigram ( e.g. ,  new york \" ) , to multigram ( e.g. ,  new york times \" ) , and even to a whole long sequence ( e.g. , a song name  another lonely night in new york \" ) . on the other hand",
    ", our understanding of natural language depends critically on the scale at which it is examined , for example , depending on how much detailed we would like to get into a document , our knowledge could range from a collection of  _ keywords _ \" , to a sentence sketch named ",
    "_ title _ \" , to a paragraph summary named  _ abstract _ \" , to a page long  _ introduction _ \" and finally to the _ entire content_.",
    "the notion of scale is fundamental to the understanding of natural language , yet it was largely ignored by existing models for text representation , which include simple bag - of - word ( bow ) or unigram language model ( lm ) , n - gram or higher order lms , and other more advanced text / language models @xcite . one key problem with many of these models is their inflexibility  they capture the semantic structure rather rigidly at only a single resolution ( e.g. , @xmath0-gram with a single fixed value of @xmath0 ) .",
    "however , which scale is appropriate for a specific task is usually unknown a priori and in many cases even not homogeneous ( e.g. , a document may contain named entities of different length ) , making it impossible to capture the right meanings with a fixed single scale .",
    "scale space theory is a well - established and promising framework for multi - resolution representation , developed primarily by the computer vision and signal processing communities with complimentary motivations from physics and bio - vision .",
    "the key idea is to embed a signal into the _ scale space _",
    ", i.e. , to represent it as a family of progressively smoothed signals parameterized by a continuous variable of _ scale _ , where fine - resolution detailed structures are progressively suppressed by the convolution of the original signal with a smoothing kernel ( i.e. , a low pass filter with certain properties ) @xcite .    in this paper",
    ", we adapt the scale - space model from image to text signals , proposing a novel framework that enables multi - resolution representation for documents .",
    "the adaptation poses substantial challenges as the structure of the semantic domain is nontrivially complicated than the spatial domains in traditional image scale space .",
    "we show how this can be made possible with a set of assumptions and simplifications . the scale - space model for text",
    "not only provides new perspectives for how text analysis tasks can be formulated and addressed , but also enables well - established computer vision tools to be adapted and applied to text processing , e.g. , matching , segmentation , description , interests points detection , and classification . to stimulate further investigation in this promising direction",
    ", we initiate a couple of instantiations to demonstrate how this model can be used in a variety of nlp and text analysis tasks to make things easier , better , and most importantly , scale - invariant .",
    "the notion of scale space is applicable to signals of arbitrary dimensions .",
    "let us consider the most common case , where it is applied to 2-dimensional signals such as images .",
    "given an image @xmath1 , its scale - space representation @xmath2 is defined by : @xmath3 where @xmath4 denotes the convolution operator , and @xmath5 is a smoothing kernel ( i.e. , a low pass filter ) with a set of desired properties ( i.e. , the scale - space axioms @xcite ) .",
    "the bandwidth parameter @xmath6 is referred to as scale parameter since as @xmath6 increases , the derived image will become gradually smoother ( i.e. , blurred ) and consequently more and more fine - scale structures will be suppressed .",
    "it has been shown that the gaussian kernel is the unique option that satisfies the conditions for _ linear scale space _ : @xmath7 the resultant linear scale space representation @xmath2 can be obtained equivalently as a solution to the diffusion ( heat ) equation @xmath8 with initial condition @xmath9 , where @xmath10 denotes the laplace operator which in a 2-dimensional spatial space corresponds to @xmath11 .",
    "if we view @xmath12 as a heat distribution , the equation essentially describes how it diffuses from initial value , @xmath13 , in a homogeneous media with uniform conductivity over time @xmath6 .",
    "as we can imagine , the distribution will gradually approach uniform and consequently the fine - scale structure of @xmath13 will be lost .",
    "scale - space theory provides a formal framework for handling the multi - scale nature of both the physical world and the human perception . since its introduction in 1980s",
    ", it has become the foundation of many computer vision techniques and been widely applied to a large variety of vision / image processing tasks . in this paper",
    ", we show how this powerful tool can be adapted and applied to natural language texts .",
    "a straightforward step towards textual sale space would be to represent texts in the way as image signal . in this section ,",
    "we show how this can be made possible .",
    "other alternative signal formulations will be discussed in the followed section .",
    "let @xmath14 be our vocabulary consisting of @xmath15 words , given a document @xmath16 comprised of a finite @xmath17-word sequence @xmath18 , without any information loss , we can characterize @xmath16 as a 2d @xmath19 binary matrix @xmath13 , with the @xmath20-th entry @xmath21 indicates whether or not the @xmath22-th vocabulary word @xmath23 is observed at the @xmath24-th position , i.e. : @xmath25 , where @xmath26 if @xmath27 and 0 otherwise .",
    "hereafter , we will refer to the @xmath24-axis as _ spatial domain _",
    "( i.e. , positions in the document , @xmath28 ) , and @xmath22-axis as the _ semantic axis _ ( i.e. , indices in the vocabulary , @xmath29 ) .",
    "this representation provides an image analogy to text , i.e. , a document @xmath13 is equivalent to a black - and - white image except that here we have _ one spatial _ and _ one semantic _ domains , @xmath20 , instead of two spatial domains , @xmath30 .",
    "interestingly , scale - space representation can also be motivated by this binary model from a slightly different perspective , as a way of robust density estimation .",
    "we have the following definition :    definition 1 . _ a 2d text model _",
    "@xmath31 _ is a probabilistic distribution over the joint spatial - semantic space _ : @xmath32 , @xmath33 , @xmath34 .",
    "this 2d text model defines the probability of observing a semantic word @xmath22 at a spatial position @xmath24 . the binary matrix representation ( after normalization )",
    "can be understood as an estimation of @xmath13 with kernel density estimators : @xmath35 where @xmath36 is the @xmath37-th column vector of an identity matrix , @xmath38 denotes the @xmath24-th row vector and @xmath39 the @xmath22-th column vector .",
    "note that here the dirac impulse kernels @xmath40 is used , i.e. , words are unrelated either spatially or semantically .",
    "this contradicts the common knowledge since neighboring words in text are highly correlated both semantically @xcite and spatially @xcite .",
    "for instance , observing the word  new \" indicates a high likelihood of seeing the other word  york \" at the next position . as a result",
    ", it motivates more reliable estimate of @xmath13 by using smooth kernels such as gaussian @xcite , which , as we will see , leads exactly to the gaussian filtering used in the linear scale - space theory .",
    "the 2d binary matrix described above is not the only option we can work with in scale space . generally speaking",
    ", any vector , matrix or even tensor representation of a document can be used as a signal upon which scale space filtering can be applied . in particular",
    ", we use the following in the current paper :    _ word - level 2d _ signal , @xmath21 , is the binary matrix we described in  [ sec : word2d ] .",
    "it records the spatial position for each word , and is defined on the joint spatial - semantic domains .",
    "_ bag - of - word 1d _ signal is the bow representation @xmath41 , i.e. , the 2d matrix is collapsed to a 1d vector .",
    "since the spatial axis is wiped out , this signal is defined on the semantic domain alone .",
    "_ sentence - level 2d _ signal is a compromise between word - level 2d and the bow signals . instead of collapsing the spatial dimension for the whole document , we do it for each sentence . as a result , this signal , @xmath21 , records the position of each sentence ; for a fixed position @xmath42 , @xmath43 records the bow of the corresponding sentence .",
    "_ topic 1d _ signal , @xmath44 , is composed of the topic embedding of each sentence and defined on the spatial domain only .",
    "assume we have trained a topic model ( e.g. , latent dirichlet allocation ) on a universal corpus in advance , this signal is obtained by applying topic inference to each sentence and recording the topic embedding @xmath45 , where @xmath46 is the dimensionality of the topic space .",
    "topic embedding is beneficial since it endows us the ability to address synonyms and polysemy .",
    "also note that the semantic correlation may have been eliminated and consequently semantic smoothing is no longer necessary .",
    "in other words , although @xmath44 is a matrix , we would rather treat it as a vector - variate 1d signal .",
    "all these textual signals involve either a semantic domain or both semantic and spatial domains . in the following ,",
    "we investigate how scale - space filtering can be applied to these domains respectively .",
    "spatial filtering has long been popularized in signal processing @xcite , and was recently explored in nlp by @xcite .",
    "it can be achieved by convolution of the signal with a low - pass spatial filter , i.e. , @xmath47 . for texts ,",
    "this amounts to borrowing the occurrence of words at one position from its neighboring positions , similar to what was done by a cache - based language model @xcite .    in order not to introduce spurious information , the filter @xmath48 need to satisfy a set of scale - space axioms @xcite .",
    "if we view the positions in a text as a spatial domain , the gaussian kernel @xmath49 or its discrete counterpart @xmath50 are singled out as the unique options that satisfy the set of axioms leading to the linear scale space , where @xmath51 denotes the modified bessel functions of integer order . alternatively ,",
    "if we view the position @xmath24 as a time variable as in the markov language models , a poisson kernel @xmath52 is more appropriate as it retains temporal causality ( i.e. , inaccessibility of future data ) .",
    "semantic filtering attempts to smooth the probabilities of seeing words that are semantically correlated .",
    "in contrast to the spatial domain , the semantic domain has some unique properties .",
    "the first thing we notice is that , as semantic coordinates are nothing but indices to the dictionary , we can permute them without changing the semantic meaning of the representation .",
    "we refer to this property as _",
    "permutation invariance_. semantic smoothing has been extensively explored in natural language processing @xcite .",
    "classical smoothing methods , e.g. , laplacian and dirichlet smoother , usually shrink the original distributions to a predefined reference distribution .",
    "recent advances explored local smoothing where correlated words are smoothed according to their interrelations defined by a semantic network @xcite .    given a semantic graph @xmath53 , where two correlated words @xmath23 and @xmath54 are connected with weight @xmath55 , semantic smoothing can be formulated as solving a graph - based optimization problem : @xmath56 where @xmath57 defines the tradeoff , @xmath58 weights the importance of the node @xmath23 .",
    "interestingly , the solution to eqn.([eq7 ] ) is simply the convolution of the original signal with a specific kernel ) . ] , i.e. , @xmath59 .",
    "compared with spatial filtering , semantic filtering is , however , more challenging .",
    "in particular , the semantic domain is heterogeneous and not shift - invariant  the degree of correlation @xmath55 depends on both coordinates @xmath22 and @xmath60 rather than their difference @xmath61 . as a result , kernels that provably satisfy scale - space axioms are no longer feasible . to this end",
    ", we simply set aside these requirements and define kernels in terms of the dissimilarity @xmath62 between a pair of words @xmath22 and @xmath60 rather than their direct difference @xmath61 , that is , @xmath63 , where we use @xmath64 to denote semantic kernel to distinguish from spatial kernels @xmath65 .",
    "for gaussian , this means @xmath66 .",
    "scale is vital for the understanding of natural language , yet it is nontrivial to determine which scale is appropriate for a specific task at hand in advance . as a matter of fact",
    ", the best choice usually varies from task to task and from document to document . even within one document , it could be heterogeneous , varying from paragraph to paragraph and sentence to sentence .",
    "for the purpose of automatic modeling , there is no way to decide _ a priori _ which scale fits the best .",
    "more importantly , it might be impossible to capture all the right meanings at a single scale .",
    "therefore , the only reasonable way is to simultaneously represent the document at multiple scales , which is exactly the notion of _ scale space_.    scale space representation embeds a textual signal into a _",
    "continuous _ scale - space , i.e. , by a family of progressively smoothed signals parameterized by continuous scale parameters .",
    "in particular , for a 2d textual signal @xmath21 , we have : @xmath67 where the 2d smoothing kernel @xmath48 is separable between spatial and semantic domains , i.e. , @xmath68 note that we have two continuous scale parameters , the spatial scale @xmath69 and the semantic scale @xmath70 . the case for 1d signals are even simpler as they only involve one type of kernels ( spatial or semantic ) . for a 1d spatial signal @xmath71",
    ", we have @xmath72 , and for a semantic signal @xmath73 , @xmath74 . and",
    "if @xmath75 is a vector - variate signal , we just apply smoothing to each of its dimensions independently .",
    "[ [ example . ] ] example .",
    "+ + + + + + + +    as an example , figure  [ fig1 ] shows four samples , @xmath76 , from the scale - space representation @xmath77 of a synthetic short text ",
    "_ new york times offers free iphone 3 g as gifts for new customers in new york _ \" , where @xmath78 , the two scales are set equal @xmath79 for ease of explanation and @xmath12 is obtained based on the word - level 2d signal .",
    "we use a vocabulary containing 12 words ( in order ) :  new \" ,  york \" ,  time \" ,  free \" ,  iphone \" ,  gift \" ,  customer \" ,  apple \" ,  egg \" ,  city \" ,  service \" and  coupon \" , where the last four words are chosen because of their strong correlations with those words that appear in this text .",
    "the semantic graph is constructed based on pairwise mutual information scores estimated on the rcv1-v2 corpus as well as a large set of web search queries .",
    "the ( 0,0)-scale sample , or the original signal , is a @xmath80 binary matrix , recording precisely which word appears at which position .",
    "the smoothed signals at ( 1,1 ) , ( 2,2 ) and ( 8,8)-scales , on the other hand , capture not only short - range spatial correlations such as bi - gram , tri - gram and even higher orders ( e.g. , the named entities  new york \" and  new york times \" ) , but also long - range semantic dependencies as they progressively boost the probability of latent but semantically related topics , e.g. , ",
    "iphone \" @xmath81  apple \" ,  customer \" @xmath81  service \" ,  free \" and  gift \" @xmath81  coupon \" ,  new \" and  iphone \" @xmath81  egg \" ( due to the online electronics store ` newegg.com ` ) .",
    "the scale - space representation creates a new dimension for text analysis . besides providing a multi - scale representation that allows texts to be analyzed in a scale - invariant fashion",
    ", it also enables well - established computer vision tools to be adapted and applied to analyzing texts .",
    "the scale space model can be used in nlp and text mining in a variety of ways . to stimulate further research in this direction",
    ", we initiate a couple of instantiations .      in this section ,",
    "we show how to make text classification scale - invariant by exploring the notion of _ scale - invariant text kernel _ ( sitk ) . given a pair of documents , @xmath16 and @xmath82 , at any fixed scale @xmath6",
    ", the representation @xmath12 induces a single - scale kernel @xmath83 , where @xmath84 denotes any inner product ( e.g. , frobenius product , gaussian rbf similarity , jensen - shannon divergence ) .",
    "this kernel can be made scale - invariant via the expectation : @xmath85=\\int_0^\\infty k_s(d , d^\\prime)q(s)ds,\\label{eq10}\\end{aligned}\\ ] ] where @xmath86 is a probabilistic density over the scale space @xmath87 with @xmath88 and @xmath89 , which in essence characterizes the distribution of the most appropriate scale .",
    "@xmath86 can be learned from data via a em procedure or in a bayesian framework if our belief about the scale can be encoded into a prior distribution @xmath90 . as an example",
    ", we show below one possible formulation .    given a training corpus @xmath91 , where @xmath16 is a document and @xmath22 its label",
    ", our goal in text classification is to minimize the expected classification error . to simplify matters ,",
    "we assume a parametric form for @xmath86 .",
    "particularly , we use the gamma distribution @xmath92 due to its flexibility .",
    "moreover , we propose a formulation that eliminates the dependence on the choice of the classifier , which approximately minimizes the bayes error rate @xcite , i.e. : @xmath93\\\\ \\end{split}\\ ] ] where @xmath94 is a heuristic margin ; @xmath95 , called ",
    "nearest - hit \" , is the nearest neighbor of @xmath96 with the same class label , whereas @xmath97 , the  nearest - miss \" , is the nearest neighbor of @xmath96 with a different label , and the distance @xmath98 .",
    "this above formulation can be solved via a em procedure .",
    "alternatively , we can discretize the scale space ( preferably in log - scale ) , i.e. , @xmath99 , and optimize a discrete distribution @xmath100 directly from the same formulation . in particular , if we regularize the @xmath101-norm of @xmath102 , eq([eq11 ] ) will become a convex optimization with a close - form solution that is extremely efficient to obtain : @xmath103 where @xmath104^\\top$ ] , the average margin vector @xmath105^\\top$ ] with entry @xmath106 , and @xmath107 denotes the positive - part operator .",
    "[ [ experiments . ] ] experiments .",
    "+ + + + + + + + + + + +    we test the scale - invariant text kernels ( sitk ) on the rcv1-v2 corpus with focus on the 161,311 documents from ten leaf - node topics : ` c11 , c24 , c42 , e211 , e512 , gjob , gpro , m12 , m131 ` and ` m142 ` .",
    "each text is stop - worded and stemmed .",
    "the top 20k words with the highest dfs ( document frequencies ) are selected as vocabulary ; all other words are discarded .",
    "the semantic network is constructed based on pairwise mutual information scores estimated on the whole rcv1 corpus as well as a large scale repository of web search queries , and further sparsified with a cut - off threshold .",
    "we implemented the sentence - level 2d , the lda 1d signals and bow 1d for this task .",
    "for the first two , the documents are normalized to the length of the longest one in the corpus via bi - linear interpolation .",
    "we examined the classification performance of the svm classifiers that are trained on the _ one - vs - all _ splits of the training data , where three types of kernels ( i.e. , linear ( frobenius ) , rbf gaussian and jensen - shannon kernels ) were considered .",
    "the average test accuracy ( i.e. , micro - averaged f1 ) scores are reported in table  [ tab1 ] . as a reference , the results by bow representations with tf or tfidf attributes are also included .",
    "for all the three kernel options , the scale - space based sitk models significantly ( according to @xmath108-test at @xmath109 level ) outperform the two bow baselines , while the sentence level sitk performs substantially the best with 7.8% accuracy improvement ( i.e. , 56% error reduction ) .",
    ".text classification test accuracy .",
    "we compared five models : the bag - of - word vector space models with tf or tfidf attributes , and the scale - invariant text kernels with bow 1d ( sitk.bow ) , lda 1d ( sitk.lda ) and sentence - level 2d ( sitk.sentence ) textual signal .",
    "best results are highlighted in * bold*.[tab1 ] [ cols=\"<,^,^,^\",options=\"header \" , ]      the extrema ( i.e. , maxima and minima ) of a signal and its first a few derivatives contain important information for describing the structure of the signal , e.g. , patches of significance , boundaries , corners , ridges and blobs in an image .",
    "scale space model provides a convenient framework to obtain the extrema of a signal at different scales .",
    "in particular , the extrema in the @xmath110-th derivative of a signal is given by the zero - crossing in the @xmath111-the derivative , which can be obtained at any scale in the scale space conveniently via the convolution of the original signal with the derivative of the gaussian kernel , i.e. : @xmath112 since gaussian kernel is infinitely differentiable , the scale - space model makes it possible to obtain local extrema / derivatives of a signal to arbitrary orders even when the signal itself is undifferentiable .",
    "moreover , due to the  non - enhancement of local extrema \" property , local extrema are created _ monotonically _ as we decrease the scale parameter @xmath6 . in this section ,",
    "we show how this can be used to detect keywords from a document in a _ hierarchical _ fashion .",
    "the idea is to work with the word - level 2d signal ( other options are also possible ) and track the extrema ( i.e. , patterns of significance ) of the scale - space model @xmath12 through the zero - crossing of its first derivative @xmath113 to see how extrema progressively emerge as the scale @xmath6 goes from coarse to finer levels .",
    "this process reduces the scale - space representation to a simple ternary tree in the scale space , i.e. , the so - called  _ interval tree _ \" in @xcite . since @xmath13 defines a probability over the spatial - semantic space , it is straightforward to interpret the identified intervals as keywords .",
    "this algorithm therefore yields a _ keyword tree _ that defines topics we could perceive at different levels of granularities from the document .    [",
    "[ experiments.-2 ] ] experiments . + + + + + + + + + + + +    as an illustrative example , we apply the hierarchical keywording algorithm described above to the current paper .",
    "the keywords that emerged in order are as follows :  scale space \" @xmath81  kernel \" ,  signal \" ,  text \" @xmath81  smoothing \" ,  spatial \" ,  semantic \" ,  domains \" ,  gaussian \" ,  filter \" ,  text analysis \" ,  natural language \" ,  word \" @xmath114 .      in the previous section , we show how semantic keywords can be extracted from a text in a hierarchical way by tracking the extrema of its scale space model @xmath12 . in the same spirit , here we show how topic boundaries in a text can be identified by tracking the extrema of the first derivative @xmath115 .",
    "text segmentation is an important topic in nlp and has been extensively investigated previously @xcite .",
    "many existing approaches , however , are only able to identify a flat structure , i.e. , all the boundaries are identified at a flat level .",
    "a more challenging task is to automatically identify a _ hierarchical _",
    "table - of - content style structure for a text , that is , to organize boundaries of different text units in a tree structure according to their topic granularities , e.g. , chapter boundaries at the top - level , followed in order by boundaries of sections , subsections , paragraphs and sentences as the level of depth increases .",
    "this can be achieved conveniently by the _ interval tree _ and _ coarse - to - fine tracking _ idea presented in @xcite . in particular ,",
    "if we keep tracking the extrema of the 1st order derivatives ( i.e. , rate of changes ) by looking at the points satisfying : @xmath116 due to the monotonicity nature of scale space representation , such contours are closed above but open below in the scale space .",
    "they naturally illustrate how topic boundaries appear progressively as scale @xmath6 goes finer . and",
    "the _ exact localization _ of a boundary can be obtained by tracking back to the scale @xmath117 .",
    "also note that this algorithm , unlike many existing ones , does not require any supervision information .",
    "[ [ experiments.-3 ] ] experiments . + + + + + + + + + + + +    as an example , we apply the hierarchical segmentation algorithm to the current paper .",
    "we use the sentence level 2d signal .",
    "let @xmath118 denote the vector @xmath119 , where the semantic scale @xmath120 is fixed to a constant @xmath121 , and the semantic index @xmath22 enumerates through the whole vocabulary @xmath122 .",
    "we identify hierarchical boundaries by tracking the zero contours @xmath123@xmath124 ( where @xmath125 denotes @xmath101-norm ) to the scale @xmath117 , where the length of the projection in scale space ( i.e. , the vertical span ) reflects each contour line s topic granularity , as plotted in figure  [ fig2 ] ( top ) . as a reference , the velocity magnitude curve ( bottom ) @xmath126@xmath127 , and the true boundaries of sections ( red - dashed vertical lines in top figure ) and subsections ( green - dashed )",
    "are also plotted .",
    "as we can see , the predictions match the ground truths with satisfactorily high accuracy .",
    "this paper presented scale - space theory for text , adapting concepts , formulations and algorithms that were originally developed for images to address the unique properties of natural language texts .",
    "we also show how scale - space models can be utilized to facilitate a variety of nlp tasks .",
    "there are a lot of promising topics along this line , for example , algorithms that scale up the scale - space implementations towards massive corpus , structures of the semantic networks that enable efficient or even close - form scale - space kernel / relevance model , and effective scale - invariant descriptors ( e.g. , named entities , topics , semantic trends in text ) for texts similar to the sift feature for images @xcite ."
  ],
  "abstract_text": [
    "<S> scale - space theory has been established primarily by the computer vision and signal processing communities as a well - founded and promising framework for multi - scale processing of signals ( e.g. , images ) . by embedding an original signal into a family of gradually coarsen signals parameterized with a continuous scale parameter , it provides a formal framework to capture the structure of a signal at different scales in a consistent way . in this paper , we present a scale space theory for text by integrating semantic and spatial filters , and demonstrate how natural language documents can be understood , processed and analyzed at multiple resolutions , and how this scale - space representation can be used to facilitate a variety of nlp and text analysis tasks . </S>"
  ]
}