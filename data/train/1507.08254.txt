{
  "article_text": [
    "the problem of _ compressive phase retrieval _ ( cpr ) is generally stated as the problem of estimating a @xmath1-sparse vector @xmath3 from noisy measurements of the form @xmath4 for @xmath5 , where @xmath6 is the sensing vector and @xmath7 denotes the additive noise . in this paper , we study the cpr problem with specific sensing vectors @xmath6 of the form @xmath8 where @xmath9 and @xmath10 are known . in words , the measurement vectors live in a fixed low - dimensional subspace ( i.e , the row space of @xmath11 ) . these types of measurements can be applied in imaging systems that have control over how the scene is illuminated ; examples include systems that use structured illumination with a spatial light modulator or a _ scattering medium _ @xcite . by a standard lifting of the signal @xmath12 to @xmath13 , the quadratic measurements ( [ eq : measurements ] )",
    "can be expressed as @xmath14 with the linear operator @xmath15 and @xmath16 defined as @xmath17_{i=1}^{n } &   & \\text{and } &   & \\mc a:\\mb x\\mapsto\\mc w\\left(\\mb{\\varpsi}\\mb x\\mb{\\varpsi}^{\\msf t}\\right),\\end{aligned}\\ ] ] we can write the measurements compactly as @xmath18 our goal is to estimate the sparse , rank - one , and positive semidefinite matrix @xmath19 from the measurements ( [ eq : lifted - measurements ] ) , which also solves the cpr problem and provides an estimate for the sparse signal @xmath12 up to the inevitable global phase ambiguity .",
    "[ [ assumptions ] ] assumptions + + + + + + + + + + +    we make the following assumptions throughout the paper .    1 .",
    "[ asm : a1 ] the vectors @xmath20 are independent and have the standard gaussian distribution on @xmath21 : @xmath22 2 .",
    "[ asm : a2 ] the matrix @xmath11 is a _ restricted isometry _",
    "matrix for @xmath23-sparse vectors and for a constant @xmath24 $ ] .",
    "namely , it obeys @xmath25 for all @xmath23-sparse vectors @xmath26 .",
    "[ asm : a3 ] the noise vector @xmath27 is bounded as @xmath28    as will be seen in theorem [ thm : master - theorem ] and its proof below , the gaussian distribution imposed by the assumption [ asm : a1 ] will be used merely to guarantee successful estimation of a rank - one matrix through trace norm minimization .",
    "however , other distributions ( e.g. , uniform distribution on the unit sphere ) can also be used to obtain similar guarantees .",
    "furthermore , the restricted isometry condition imposed by the assumption [ asm : a2 ] is not critical and can be replaced by weaker assumptions .",
    "however , the guarantees obtained under these weaker assumptions usually require more intricate derivations , provide weaker noise robustness , and often do not hold uniformly for all potential target signals . therefore , to keep the exposition simple and straightforward we assume ( [ eq : rip ] ) which is known to hold ( with high probability ) for various ensembles of random matrices ( e.g. , gaussian , rademacher , partial fourier , etc ) . because in many scenarios we have the flexibility of selecting @xmath11 , the assumption ( [ eq : rip ] ) is realistic as well .",
    "[ [ notation ] ] notation + + + + + + + +    let us first set the notation used throughout the paper .",
    "matrices and vectors are denoted by bold capital and small letters , respectively .",
    "the set of positive integers less than or equal to @xmath29 is denoted by @xmath30 $ ] .",
    "the notation @xmath31 is used when @xmath32 for some absolute constant @xmath33 . for any matrix @xmath34 , the frobenius norm , the nuclear norm , the entrywise @xmath35-norm , and the largest entrywise absolute value of the entries",
    "are denoted by @xmath36 , @xmath37 , @xmath38 , and @xmath39 , respectively . to indicate that a matrix @xmath34 is positive semidefinite we write @xmath40 .",
    "the main challenge in the cpr problem in its general formulation is to design an accurate estimator that has optimal sample complexity and computationally tractable . in this paper",
    "we address this challenge in the special setting where the sensing vectors can be factored as ( [ eq : nested - sensing ] ) .",
    "namely , we propose an algorithm that    * provably produces an accurate estimate of the lifted target @xmath19 from only @xmath41 measurements , and * can be computed in polynomial time through efficient convex optimization methods .",
    "several papers including @xcite have already studied the application of convex programming for ( non - sparse ) phase retrieval ( pr ) in various settings and have established estimation accuracy through different mathematical techniques .",
    "these phase retrieval methods attain nearly optimal sample complexities that scales with the dimension of the target signal up to a constant factor @xcite or at most a logarithmic factor @xcite . however , to the best of our knowledge , the exiting methods for cpr either lack accuracy and robustness guarantees or have suboptimal sample complexities .",
    "the problem of recovering a sparse signal from the magnitude of its subsampled fourier transforms is cast in @xcite as an @xmath35-minimization with non - convex constraints .",
    "while @xcite shows that a sufficient number of measurements would grow quadratically in @xmath1 ( i.e. , the sparsity of the signal ) , the numerical simulations suggest that the non - convex method successfully estimates the sparse signal with only about @xmath42 measurements .",
    "another non - convex approach to cpr is considered in @xcite which poses the problem as finding a @xmath1-sparse vector that minimizes the residual error that takes a quartic form . a local search algorithm",
    "called gespar @xcite is then applied to ( approximate ) the solution to the formulated sparsity - constrained optimization .",
    "this approach is shown to be effective through simulations , but it also lacks global convergence or statistical accuracy guarantees .",
    "an alternating minimization method for both pr and cpr is studied in @xcite .",
    "this method is appealing in large scale problems because of computationally inexpensive iterations .",
    "more importantly , @xcite proposes a specific initialization using which the alternating minimization method is shown to converge linearly in noise - free pr and cpr .",
    "however , the number of measurements required to establish this convergence is effectively quadratic in @xmath1 . in @xcite and @xcite the @xmath35-regularized form of the trace minimization @xmath43",
    "is proposed for the cpr problem .",
    "the guarantees of @xcite are based on the restricted isometry property of the sensing operator @xmath44_{i=1}^{n}$ ] for sparse matrices . in @xcite , however , the analysis is based on construction of a _ dual certificate _ through an adaptation of the _ golfing scheme",
    "assuming standard gaussian sensing vectors @xmath6 and with appropriate choice of the regularization parameter @xmath45 , it is shown in @xcite that ( [ eq : trace+l1 ] ) solves the cpr when @xmath46 .",
    "furthermore , this method fails to recover the target sparse and rank - one matrix if @xmath29 is dominated by @xmath47 .",
    "estimation of simultaneously structured matrices through convex relaxations similar to ( [ eq : trace+l1 ] ) is also studied in @xcite where it is shown that these methods do not attain optimal sample complexity .",
    "more recently , assuming that the sparse target has a bernoulli - gaussian distribution , a _ generalized approximate message passing _ framework is proposed in @xcite to solve the cpr problem .",
    "performance of this method is evaluated through numerical simulations for standard gaussian sensing matrices which show the _ empirical phase transition _ for successful estimation occurs at @xmath41 and also the algorithms can have a significantly lower runtime compared to some of the competing algorithms including gespar @xcite and cprl @xcite .",
    "the phasecode algorithm is proposed in @xcite to solve the cpr problem with sensing vectors designed using sparse graphs and techniques adapted from coding theory .",
    "although phasecode is shown to achieve the optimal sample complexity , it lacks robustness guarantees .    while preparing the final version of the current paper , we became aware of @xcite which has independently proposed an approach similar to ours to address the cpr problem .",
    "we propose a two - stage algorithm outlined in algorithm [ alg : twostagemethod ] .",
    "each stage of the algorithm is a convex program for which various efficient numerical solvers exists . in the first stage",
    "we solve ( [ eq : pre - estimate ] ) to obtain a low - rank matrix @xmath48 which is an estimator of the matrix @xmath49 then @xmath48 is used in the second stage of the algorithm as the measurements for a sparse estimation expressed by ( [ eq : estimate ] ) .",
    "the constraint of ( [ eq : estimate ] ) depends on an absolute constant @xmath50 that should be sufficiently large .",
    "[ alg : twostagemethod ]    low - rank estimation stage : @xmath51 sparse estimation stage : @xmath52    [ [ post - processing . ] ] post - processing .",
    "+ + + + + + + + + + + + + + + +    the result of the low - rank estimation stage ( [ eq : pre - estimate ] ) is generally not rank - one . similarly , the sparse estimation stage does not necessarily produce a @xmath53 that is @xmath54-sparse ( i.e. , it has at most @xmath1 nonzero rows and columns ) and rank - one .",
    "in fact , since we have not imposed the positive semidefiniteness constraint ( i.e. , @xmath55 ) in ( [ eq : estimate ] ) , the estimate @xmath53 is not even guaranteed to be positive semidefinite ( psd ) .",
    "however , we can enforce the rank - one or the sparsity structure in post - processing steps simply by projecting the produced estimate on the set of rank - one or @xmath54-sparse psd matrices .",
    "the simple but important observation is that projecting @xmath53 onto the desired sets at most doubles the estimation error .",
    "this fact is shown by lemma [ lem : projected - estimator ] in section [ sec : proofs ] in a general setting .",
    "[ [ alternatives . ] ] alternatives .",
    "+ + + + + + + + + + + + +    there are alternative convex relaxations for the low - rank estimation and the sparse estimation stages of algorithm ( [ alg : twostagemethod ] ) .",
    "for example , ( [ eq : pre - estimate ] ) can be replaced by its regularized least squares analog @xmath56 for an appropriate choice of the regularization parameter @xmath45 .",
    "similarly , instead of ( [ eq : estimate ] ) we can use an @xmath35-regularized least squares .",
    "furthermore , to perform the low - rank estimation and the sparse estimation we can use non - convex greedy type algorithms that typically have lower computational costs .",
    "for example , the low - rank estimation stage can be performed via the wirtinger flow method proposed in @xcite .",
    "furthermore , various greedy compressive sensing algorithms such as the iterative hard thresholding @xcite and cosamp @xcite can be used to solve the desired sparse estimation . to guarantee the accuracy of these compressive sensing algorithms , however",
    ", we might need to adjust the assumption [ asm : a2 ] to have the restricted isometry property for @xmath57-sparse vectors with @xmath58 being some small positive integer .",
    "the following theorem shows that any solution of the proposed algorithm is an accurate estimator of @xmath19 .",
    "[ thm : master - theorem]suppose that the assumptions [ asm : a1 ] , [ asm : a2 ] , and [ asm : a3 ] hold with a sufficiently small constant @xmath59 .",
    "then , there exist positive absolute constants @xmath60 , @xmath61 , and @xmath62 such that if @xmath63 then any estimate @xmath53 of the algorithm [ alg : twostagemethod ] obeys @xmath64 for all rank - one and @xmath54-sparse matrices @xmath65 with probability exceeding @xmath66 .",
    "the proof of theorem [ thm : master - theorem ] is straightforward and is provided in section [ sec : proofs ] .",
    "the main idea is first to show the low - rank estimation stage produces an accurate estimate of @xmath67 .",
    "because this stage can be viewed as a standard phase retrieval through lifting , we can simply use accuracy guarantees that are already established in the literature ( e.g. , @xcite ) . in particular , we use ( * ? ? ?",
    "* theorem 2 ) which established an error bound that holds uniformly for all valid @xmath67 .",
    "thus we can ensure that @xmath19 is feasible in the sparse estimation stage .",
    "then the accuracy of the sparse estimation stage can also be established by a simple adaptation of the analyses based on the restricted isometry property such as @xcite .",
    "the dependence of @xmath29 ( i.e. , the number of measurements ) and @xmath1 ( i.e. , the sparsity of the signal ) is not explicit in theorem [ thm : master - theorem ] .",
    "this dependence is absorbed in @xmath68 which must be sufficiently large for assumption [ asm : a2 ] to hold .",
    "considering a gaussian matrix @xmath11 , the following corollary gives a concrete example where the dependence of @xmath29on @xmath1 through @xmath68 is exposed .",
    "[ cor : gaussian - a ] suppose that the assumptions of theorem [ thm : master - theorem ] including ( [ eq : low - rank-|measurements| ] ) hold .",
    "furthermore , suppose that @xmath11 is a gaussian matrix with iid @xmath69 entries and @xmath70 for some absolute constant @xmath71 .",
    "then any estimate @xmath53 produced by algorithm [ alg : twostagemethod ] obeys @xmath64 for all rank - one and @xmath54-sparse matrices @xmath65 with probability exceeding @xmath72 for some constant @xmath73 .",
    "it is well - known that if @xmath11 has iid @xmath69 and we have ( [ eq : sparse-|measurements| ] ) then ( [ eq : rip ] ) holds with high probability .",
    "for example , using a standard covering argument and a union bound @xcite shows that if ( [ eq : sparse-|measurements| ] ) holds for a sufficiently large constant @xmath71 then we have ( [ eq : rip ] ) for a sufficiently small constant @xmath59 with probability exceeding @xmath74 for some constant @xmath33 that depends only on @xmath59 .",
    "therefore , theorem [ thm : master - theorem ] yields the desired result which holds with probability exceeding @xmath75 for some constant @xmath73 depending only on @xmath59 .",
    "we evaluated the performance of algorithm [ alg : twostagemethod ] through some numerical simulations .",
    "the low - rank estimation stage and the sparse estimation stage are implemented using the tfocs package @xcite .",
    "we considered the target @xmath1-sparse signal @xmath12 to be in @xmath76 ( i.e. , @xmath77 ) .",
    "the support set of of the target signal is selected uniformly at random and the entry values on this support are drawn independently from @xmath78 .",
    "the noise vector @xmath27 is also gaussian with independent @xmath79 .",
    "the operator @xmath15 and the matrix @xmath11 are drawn from some gaussian ensembles as described in corollary [ cor : gaussian - a ] .",
    "we measured the relative error @xmath80 of achieved by the compared methods over 100 trials with sparsity level ( i.e. , @xmath1 ) varying in the set @xmath81 .",
    "in the first experiment , for each value of @xmath1 , the pair @xmath82 that determines the size @xmath15 and @xmath11 are selected from @xmath83 .",
    "figure [ fig : revsk ] illustrates the @xmath84 quantiles of the relative error versus @xmath1 for the mentioned choices of @xmath68 .",
    "the empirical 0.9 quantile of the relative estimation error vs. sparsity for various choices of @xmath68 and @xmath29 with @xmath77.,scaledwidth=80.0% ]    in the second experiment we compared the performance of algorithm [ alg : twostagemethod ] to the convex optimization methods that do not exploit the structure of the sensing vectors .",
    "the setup for this experiment is the same as in the first experiment except for the size of @xmath15 and @xmath11 ; we chose @xmath85 and @xmath86 , where @xmath87 denotes the smallest integer greater than @xmath88 .",
    "figure [ fig : comparison ] illustrates the @xmath84 quantiles of the measured relative errors for algorithm [ alg : twostagemethod ] , the semidefinite program ( [ eq : trace+l1 ] ) for @xmath89 and @xmath90 , and the @xmath35-minimization @xmath91 which are denoted by 2-stage , sdp , sdp+@xmath35 , and @xmath35 , respectively .",
    "the sdp - based method did not perform significantly different for other values of @xmath45 in our complementary simulations .",
    "the relative error for each trial is also overlaid in figure [ fig : comparison ] visualize its empirical distribution .",
    "the empirical performance of the algorithms are in agreement with the theoretical results .",
    "namely in a regime where @xmath92 , algorithm [ alg : twostagemethod ] can produce accurate estimates whereas while the other approaches fail in this regime .",
    "the sdp and sdp+@xmath35 show nearly identical performance .",
    "the @xmath35-minimization , however , competes with algorithm [ alg : twostagemethod ] for small values of @xmath1 .",
    "this observation can be explained intuitively by the fact that the @xmath35-minimization succeeds with @xmath93 measurements which for small values of @xmath1 can be sufficiently close to the considered @xmath94 measurements .",
    "the empirical 0.9 quantile of the relative estimation error vs. sparsity for algorithm [ alg : twostagemethod ] and different @xmath95- and/or @xmath35- minimization methods with @xmath77 , @xmath85 , and @xmath86.,scaledwidth=80.0% ]",
    "clearly , @xmath96 is feasible in [ eq : pre - estimate ] because of [ asm : a3 ] .",
    "therefore , we can show that any solution @xmath48 of ( [ eq : pre - estimate ] ) accurately estimates @xmath67 using existing results on nuclear - norm minimization .",
    "in particular , we can invoke ( * ? ? ? * theorem 2 and section 4.3 ) which guarantees that for some positive absolute constants @xmath60 , @xmath97 , and @xmath62 if ( [ eq : low - rank-|measurements| ] ) holds then @xmath98 holds for all valid @xmath67 , thereby for all valid @xmath19 , with probability exceeding @xmath66 .",
    "therefore , with @xmath99 , the target matrix @xmath19 would be feasible in ( [ eq : estimate ] ) .",
    "now , it suffices to show that the sparse estimation stage can produce an accurate estimate of @xmath19 .",
    "recall that by [ asm : a2 ] , the matrix @xmath11 is restricted isometry for @xmath23-sparse vectors .",
    "let @xmath100 be a matrix that is @xmath101-sparse , i.e. , a matrix whose entries except for some @xmath101 submatrix are all zeros . applying ( [ eq : rip ] ) to the columns of @xmath100 and adding the inequalities yield @xmath102 because the columns of @xmath103 are also",
    "@xmath23-sparse we can repeat the same argument and obtain @xmath104 using the facts that @xmath105 and @xmath106 , the inequalities ( [ eq : psix ] ) and ( [ eq : psixpsi ] ) imply that @xmath107 the proof proceeds with an adaptation of the arguments used to prove accuracy of @xmath35-minimization in compressive sensing based on the restricted isometry property ( see , e.g. , @xcite ) .",
    "let @xmath108 .",
    "furthermore , let @xmath109\\times\\left[d\\right]$ ] denote the support set of the @xmath54-sparse target @xmath19 . define @xmath110 to be a @xmath111 matrix that is identical to @xmath112 over the index set @xmath113 and zero elsewhere .",
    "by optimality of @xmath53 and feasibility of @xmath19 in ( [ eq : estimate ] ) we have @xmath114 where the last line follows from the fact that @xmath19 and @xmath115 have disjoint supports .",
    "thus , we have @xmath116 now consider a decomposition of @xmath115 as the sum @xmath117 such that for @xmath118 the @xmath111 matrices @xmath119 have disjoint support sets of size @xmath54 except perhaps for the last few matrices that might have smaller supports . more importantly",
    ", the partitioning matrices @xmath119 are chosen to have a decreasing frobenius norm ( i.e. , @xmath120 ) for @xmath121 .",
    "we have @xmath122 where the chain of inequalities follow from the triangle inequality , the fact that @xmath123 by construction , the fact that the matrices @xmath119 have disjoint support and satisfy ( [ eq : decomposition - delta ] ) , the bound ( [ eq : delta - tail ] ) , and the fact that @xmath110 and @xmath124 are orthogonal .",
    "furthermore , we have @xmath125 where the first term is obtained by the cauchy - schwarz inequality and the summation is obtained by the triangle inequality . because @xmath108 by definition , the triangle inequality and the fact that @xmath19 and @xmath53 are feasible in ( [ eq : estimate ] ) imply that @xmath126 .",
    "furthermore , lemma [ lem : psiinnerproduct ] below which is adapted from ( * ? ? ?",
    "* lemma 2.1 ) guarantees that for @xmath127 and @xmath128 we have @xmath129 .",
    "therefore , we obtain @xmath130 where the chain of inequalities follow from the lower bound in ( [ eq : rip - psipsi ] ) , the bound ( [ eq : psi(e0+e1)psi ] ) , the upper bound in ( [ eq : rip - psipsi ] ) , the bound ( [ eq : tail - less - head ] ) , and the fact that @xmath131 . if @xmath132 , then we have @xmath133 and thus @xmath134 adding the above inequality to ( [ eq : delta - tail ] ) and applying the triangle then yields the desired result .",
    "suppose that @xmath100 and @xmath135 have unit frobenius norm .",
    "using the identity @xmath137 and the fact that @xmath100 and @xmath135 have disjoint supports , it follows from ( [ eq : rip - psipsi ] ) that @xmath138 the general result follows immediately as the desired inequality is homogeneous in the frobenius norms of @xmath100 and @xmath135 .",
    "[ lem : projected - estimator ] let @xmath139 be a closed nonempty subset of a normed vector space @xmath140 .",
    "suppose that for @xmath141 we have an estimator @xmath142 , not necessarily in @xmath139 , that obeys @xmath143 .",
    "if @xmath144 denotes a projection of @xmath145 onto @xmath139 , then we have @xmath146 ."
  ],
  "abstract_text": [
    "<S> we propose a robust and efficient approach to the problem of compressive phase retrieval in which the goal is to reconstruct a sparse vector from the magnitude of a number of its linear measurements . </S>",
    "<S> the proposed framework relies on constrained sensing vectors and a two - stage reconstruction method that consists of two standard convex programs that are solved sequentially .    in recent years </S>",
    "<S> , various methods are proposed for compressive phase retrieval , but they have suboptimal sample complexity or lack robustness guarantees . </S>",
    "<S> the main obstacle has been that there is no straightforward convex relaxations for the type of structure in the target . given a set of underdetermined measurements , there is a standard framework for recovering a sparse matrix , and a standard framework for recovering a low - rank matrix . </S>",
    "<S> however , a general , efficient method for recovering a jointly sparse and low - rank matrix has remained elusive .    deviating from the models with generic measurements , in this paper we show that if the sensing vectors are chosen at random from an incoherent subspace , then the low - rank and sparse structures of the target signal can be effectively decoupled . </S>",
    "<S> we show that a recovery algorithm that consists of a low - rank recovery stage followed by a sparse recovery stage will produce an accurate estimate of the target when the number of measurements is @xmath0 , where @xmath1 and @xmath2 denote the sparsity level and the dimension of the input signal . </S>",
    "<S> we also evaluate the algorithm through numerical simulation .    # 1 # 1 # 1 # 1 # 1 # 1  </S>"
  ]
}