{
  "article_text": [
    "network coding was introduced in @xcite as a means of achieving maximum rate of transmission in wireline networks .",
    "an algebraic formulation of network coding was discussed in @xcite for both instantaneous networks and networks with delays .",
    "convolutional network - error correcting codes(cneccs ) were introduced for acyclic instantaneous networks in @xcite and for unit - delay , memory - free networks in @xcite .    in this work ,",
    "we consider acyclic , single - source networks with delays which have a multicast network code in place .",
    "the set of all code symbols generated at the source at any particular time instant is called a _",
    "generation_. in unit - delay , memory - free networks , the nodes of the network may receive information of different generations on their incoming edges at every time instant and therefore network coding across generations ( _ inter - generation _ ) is unavoidable in general .",
    "however , the sinks have to employ memory to decode the symbols .",
    "if memory is utilized in the internal nodes also , such inter - generation network coding can be avoided thus making the decoding simpler .",
    "we define a _ single - generation network code _ as a network code where all the symbols received at all the sinks are linear combinations of the symbols belonging to the same generation . in @xcite , the technique of adding memory at the nodes to achieve single - generation network coding was discussed .",
    "however this was done only on a per - node basis without considering the entire topology or the network code of the network . on the other hand , we consider the entire network topology and the network code , which govern the addition of memory elements at the nodes and the way in which they are rearranged across the network to reduce the overall memory usage in the network .",
    "the organization and contributions of this work are as follows    * after briefly discussing the network setup and the network code for an acyclic network with delays and memory ( section [ sec2 ] ) , we introduce different methods of adding memory at a node and analyze how each of them affect the local and global encoding kernels of the network code ( section [ sec3 ] ) .",
    "* we also present different memory reduction and distribution techniques ( section [ sec4 ] ) .",
    "* we propose an algorithm which uses the memory at the nodes to achieve single - generation network coding while reducing the overall memory usage in the network ( section [ sec5 ] ) .",
    "* we discuss the advantages of employing memory at the intermediate nodes in tandem with cneccs in terms of their encoding / decoding ( section [ sec6 ] ) .",
    "* we illustrate the the performance benefits by using memory for cneccs for unit - delay networks using simulations on an example unit - delay network under a probabilistic error setting ( section [ sec7 ] ) .",
    "the model for acyclic networks with delays considered in this paper is as in @xcite .",
    "an acyclic network can be represented as an acyclic directed multi - graph ( a graph that can have parallel edges between nodes ) @xmath0 = ( @xmath1 ) where @xmath2 is the set of all vertices and @xmath3 is the set of all edges in the network .",
    "we assume that every edge in the directed multi - graph representing the network has unit _ capacity _ ( can carry utmost one symbol from @xmath4 , the field with @xmath5 elements ) . network links with capacities greater than unit are modeled as parallel edges .",
    "the network has delays , i.e , every edge in the directed graph representing the input has a unit delay associated with it , represented by the parameter  @xmath6 .",
    "such networks are known as _ unit - delay networks_. those network links with delays greater than unit are modeled as serially concatenated edges in the directed multi - graph .",
    "we assume a single - source node @xmath7 and a set of sinks @xmath8 .",
    "let @xmath9 be the unicast capacity for a sink node @xmath10 i.e the maximum number of edge - disjoint paths from @xmath11 to @xmath12",
    ". then @xmath13 is the max - flow min - cut capacity of the multicast connection .",
    "we follow @xcite in describing the network code . for each node @xmath14 , let the set of all incoming edges be denoted by @xmath15 .",
    "then @xmath16 is the in - degree of @xmath17 .",
    "similarly the set of all outgoing edges is defined by @xmath18 , and the out - degree of the node @xmath17 is given by @xmath19 .    for",
    "any @xmath20 and @xmath21 , let @xmath22 , if @xmath17 is such that @xmath23 .",
    "similarly , let @xmath24 , if @xmath17 is such that @xmath25 .",
    "we will assume an ancestral ordering on @xmath26 and @xmath27 of the acyclic graph of the unit - delay , memory - free network .",
    "the network code can be defined by the local kernel matrices of size @xmath28 for each node @xmath29 with entries from @xmath4 .",
    "the global encoding kernels for each edge can be recursively calculated from these local kernels .",
    "the network transfer matrix , which governs the input - output relationship in the network , is defined as given in @xcite for an @xmath30-dimensional ( @xmath31 ) network code . towards this end , the matrices @xmath32,@xmath33,and @xmath34(for every sink @xmath35 )",
    "are defined as follows .",
    "the entries of the @xmath36 matrix @xmath32 are defined as @xmath37 where @xmath38 is the local encoding kernel coefficient at the source coupling input @xmath39 with edge @xmath40 .",
    "the @xmath41 entry of the @xmath42 matrix @xmath33 is @xmath43 which is the local kernel coefficient between @xmath44 and @xmath45 at the node @xmath46 ( if such a node exists ) , and zero if @xmath47 .    for every sink @xmath48 ,",
    "the entries of the @xmath49 matrix @xmath34 are defined as @xmath50 where all @xmath51 .    for unit - delay , memory - free networks , we have @xmath52 where @xmath53 is the @xmath42 identity matrix .",
    "now we have the following definition .",
    "[ nettransmatrix ] _ the network transfer matrix _ , @xmath54 , corresponding to a sink node @xmath55 for a @xmath30-dimensional network code , is a full rank ( over the field of rationals @xmath56 ) @xmath57 matrix defined as @xmath58    with an @xmath30-dimensional network code , the input and the output of the network are @xmath30-tuples of elements from @xmath59 $ ] , the formal power series ring over @xmath60 definition [ nettransmatrix ] implies that if @xmath61 $ ] is the input to the unit - delay , memory - free network , then at any particular sink @xmath62 , we have the output , @xmath63 $ ] , to be @xmath64      we define the _ instantaneous counterpart _ of a unit - delay network as follows .    given a unit - delay network @xmath65 , the network obtained from @xmath0",
    "( having the same node set @xmath26 and the same edge set @xmath27 ) by removing the delays associated with the edges is defined as the _ instantaneous counterpart _ of @xmath66    [ ex ] fig .",
    "[ fig : instantcounter ] illustrates an example .",
    "a modified butterfly unit - delay network ( top ) and its instantaneous counterpart ( bottom ) are shown .",
    "the global kernels of the incoming edges to the sinks @xmath67 and @xmath68 corresponding to a @xmath69 dimensional network code are indicated for both networks .",
    "( a unit - delay network and its instantaneous counterpart ) . , width=268 ]    let @xmath70 be a single - source , acyclic network with every edge of the network having some delay ( a positive integer ) and with memory elements at the nodes available for usage .",
    "if none of the memory elements at the nodes are used , then we can model @xmath71 as a unit - delay , memory - free network @xmath72 let @xmath73 be the instantaneous counterpart of @xmath72 the following lemma ensures the equivalence of a network code between @xmath73 and @xmath72    [ lemma1 ] let @xmath74 be a single - source acyclic , unit - delay , memory - free network , and @xmath75 be the instantaneous counterpart of @xmath76 let @xmath77 be the set of all @xmath78 matrices @xmath79 @xmath80 , i.e , the set of local encoding kernel matrices at each node , describing an @xmath81-dimensional network code ( over @xmath4 ) for @xmath75 ( @xmath82 min - cut of the source - sink connections in @xmath75 ) .",
    "then the network code described by @xmath83 continues to be an @xmath81-dimensional network code ( over @xmath56 ) for the unit - delay , memory - free network @xmath76    if the nodes use memory elements such that inter - generation network coding is prevented at any particular node of the network , then this leads to single - generation network coding in the network .    in section [ sec5 ]",
    "we give an algorithm which uses memory elements at the nodes to achieve single - generation network coding , i.e , the network transfer matrix @xmath84 of every sink @xmath85 in the in @xmath71 becomes @xmath86 where @xmath87 is some positive integer and @xmath88 is the network transfer matrix of the sink @xmath12 in @xmath89 clearly , if @xmath88 is full rank ( over @xmath4 ) , so is @xmath84 ( over @xmath56 ) .",
    "for the source node @xmath11 , let @xmath90 denote the set of @xmath30 virtual incoming edges which denote the @xmath30 inputs .",
    "the global kernels of these edges are therefore the columns of an @xmath91 identity matrix over @xmath4 , the field over which the network code is defined . for every non - source node @xmath80 ,",
    "let @xmath92 for a sink @xmath62 , let @xmath93 denote @xmath30 virtual outgoing edges denoting the @xmath30 outputs at sink @xmath94 the global kernels of these edges are the columns of the network transfer matrix @xmath95 for every non - sink node @xmath80 , let @xmath96 we then define the set @xmath97 as @xmath98    the ancestral ordering on @xmath3 can then be extended to an ancestral ordering on @xmath99 . for any @xmath100 such that @xmath101 , with memory being used at @xmath17 , the local kernel @xmath102 ( the kernel coefficient between @xmath103 and @xmath104 with @xmath105 ) , @xmath106 or @xmath107 ( the kernel coefficient between @xmath44 and @xmath108 for some sink node @xmath17 ) can have elements from @xmath109 we show in section [ sec5 ] that using the memory elements at the nodes according to subsection [ pairmem ] and subsection [ outmem ] is sufficient to guarantee single - generation network coding at each node and therefore in the given network .      for any @xmath100 such that @xmath101 , we define @xmath110 as the number of memory elements utilized at the node @xmath17 to delay the symbols coming from the incoming edge @xmath44 ( before any network coding is performed at node @xmath17 on the symbols from @xmath44 ) such that the local kernel between @xmath44 and @xmath45 is modified in one of the following ways @xmath111 while none of the other local kernels are changed .",
    "the matrix @xmath112 is also correspondingly modified .      for @xmath113 , we define @xmath114 as the number of memory elements added at node @xmath17 to delay the symbols going into the edge @xmath45 after performing network coding at @xmath115 in such a case , the elements of the matrix @xmath33 ( or of the matrix or @xmath32 , or @xmath116 ) are modified according to the following rule . @xmath117",
    "where the set @xmath118 is defined as in the top of the next page .",
    "@xmath119    ' '' ''    the elements of the matrix @xmath120 are also correspondingly modified .",
    "[ ex1 ] fig [ fig : memoryadditions ] illustrates an example of the memory additions at a node .",
    "the memory elements indicated inside the box labeled ` a ' are added at the node for the pair of edges @xmath44 and @xmath45 thereby delaying the symbols on @xmath44 before network coding at the node , i.e , @xmath121 similarly the memory element indicated by ` c ' is added for the pair of edges @xmath44 and @xmath122 , i.e , @xmath123 the memory element indicated by ` b ' is added for the outgoing edge @xmath45 after network coding , i.e , @xmath124     ( adding memory at a node).,width=345 ]",
    "in this section , we look at techniques to reduce the memory used at the nodes of the network and the overall memory used in the network and also to obtain a fairly uniform memory usage distribution throughout the network .",
    "we define the maximum number of memory elements added to delay the symbols coming from an edge @xmath125 into node @xmath126 as @xmath127 where @xmath128 is defined as shown at the top of the next page .",
    "@xmath129    ' '' ''    we define the total number of memory elements used at node @xmath17 as @xmath130      [ nodememreduction ] consider a node @xmath21 in which memory elements have been added to delay symbols coming from an edge @xmath131 then , retaining the @xmath132(as defined in ( [ memax ] ) ) memory elements , all other memory elements placed on @xmath44 can be removed without any change in any local or global kernels by tapping symbols from the @xmath132 memory elements wherever necessary .",
    "doing this for every incoming edge of @xmath17 is equivalent to obtaining a minimal encoder ( one with minimum number of memory elements ) of the transfer function ( input - output relationship ) at node @xmath115    [ ex0 ] fig .",
    "[ fig : memnodereduction ] illustrates a particular example of such a reduction .",
    "the figure on the top ( all @xmath133 ) represents a node @xmath17 before memory reduction with @xmath134 , while the figure on the bottom is the same node after memory reduction with @xmath135     ( memory reduction at a node).,width=288 ]      for a set of edges @xmath136 , let @xmath137 be the set of all nodes defined as follows @xmath138 we now define @xmath139 and @xmath140 as follows .",
    "@xmath141 where @xmath128 is as defined in ( [ eqn26 ] ) .    for a node @xmath142",
    ", we define the set of _ adjacent nodes _ of @xmath17 as the set of nodes @xmath143      for a node @xmath80 , and for some @xmath144 , let @xmath145 be defined as @xmath146 where @xmath147 is as in ( [ eqn27 ] ) , i.e , the global kernels of the edges in @xmath148 are linear combinations of the global kernels of the edges in @xmath149 only and none else . also let @xmath150 and the set @xmath151 of nodes be defined for the set of edges @xmath152 as in ( [ eqn39 ] ) and ( [ eqn40 ] ) respectively .",
    "we define the term @xmath153 as @xmath154 then , if the condition is satisfied , @xmath155 then all of the @xmath156 used at the nodes @xmath157 ( to delay symbols coming from the edges @xmath148 ) can be ` absorbed ' into node @xmath17 by removing all these memory elements and adding @xmath153 memory elements at node @xmath17 for every @xmath158 ( and thereby used for delaying the symbols coming from every @xmath158 ) , without using any additional memory and without changing the global kernels of any outgoing edge of any node in @xmath157 .",
    "this technique of ` absorption ' of the memory elements from a set of nodes which are the ` heads ' of the outgoing edges from a node @xmath17 , to the node @xmath17 itself , is beneficial in terms of reducing the overall memory usage of the network ( to achieve single - generation network coding ) if the condition ( [ eqn29 ] ) is satisfied as a strict inequality .",
    "[ ex2 ] fig .",
    "[ fig : memnodereductionii ] illustrates an example for memory reduction between multiple nodes ( @xmath159 and @xmath160 here ) of a network .",
    "here @xmath161 , and @xmath162 therefore , three memory elements at nodes @xmath163 and @xmath160 are ` absorbed ' into two memory elements at node @xmath164 the boxes indicate the use of memory elements and the node to which the memory elements are attached .",
    "( memory reduction between adjacent nodes).,width=297 ]      for @xmath165 being two sets of edges , we say that they form a pair @xmath166 $ ] if @xmath167 we say that the sets @xmath168 form a pair @xmath169 if @xmath170 for a node @xmath17 , we define the set @xmath171 as follows @xmath172~|~1\\leq i\\leq s_v\\right\\}\\ ] ] such that the following conditions are satisfied @xmath173 where @xmath174 is the maximum number of sets satisfying conditions ( [ eqn36 ] ) and ( [ eqn37 ] ) .",
    "algorithm [ alg : p_v ] shown at the top of the next page obtains the set @xmath171 for some node @xmath115    let @xmath175    ' '' ''    [ example2 ] fig .",
    "[ fig : example2 ] illustrates a node @xmath17 with the local kernel matrix over some field @xmath60 for this node , the set @xmath171 is given as @xmath176 , \\left[\\gamma_{i_2}(v),\\gamma_{o_2}(v)\\right]\\right\\}\\ ] ] where @xmath177     which gives the set @xmath171 of the node @xmath115,width=326 ]    for an pair of edge - sets @xmath178\\in p_v$ ] , we define @xmath179 , a sequence of pairs of edge - sets as @xmath180, ... ,\\left[{\\cal e}_{i_{2}},{\\cal e}_{i_{1}}\\right],\\left[{\\cal e}_{i_{1}},{\\cal e}_{o_{1}}\\right]\\ ] ] where @xmath181=\\left[{\\gamma}_{i_i}(v),{\\gamma}_{o_i}(v)\\right],$ ] and @xmath81 is the maximum length of the sequence , that is possible to be obtained as in ( [ eqn41 ] ) for the edge - set pair @xmath178.$ ]    let @xmath182 be an integer such that @xmath183    for the set @xmath184 let @xmath185 be defined as in ( [ eqn39 ] ) , and the set of nodes @xmath186 be defined as in ( [ eqn40 ] ) .",
    "let the set of nodes @xmath187 be defined as in ( [ eqn40 ] ) for the set @xmath188 also , let @xmath189 be defined as in ( [ eqn43 ] ) for the set @xmath190 and for an edge @xmath191 as in the memory reduction procedure of adjacent nodes , if @xmath192 then the @xmath193 used at the nodes @xmath186 ( to delay symbols coming from the edges @xmath194 ) can be removed without changing the global kernels of the edges of @xmath195 by adding @xmath196 memory elements for each edge @xmath197 at the node @xmath198 this technique will save memory if the condition ( [ eqn42 ] ) is satisfied as a strict inequality .",
    "[ example1 ] figure [ fig : ex5 ] illustrates an example for the memory reduction procedure between non - adjacent nodes .",
    "let @xmath199 in the example , for the node @xmath200 , the set @xmath201 and the sequence @xmath202 corresponding to the only element of @xmath201 are given by ( [ eqn47 ] ) and ( [ eqn48 ] ) at the top of the next page .",
    "@xmath203\\boldsymbol{\\left.\\right\\}}.\\end{aligned}\\ ] ]    ' '' ''    @xmath204,~\\left[\\left\\{e_5,e_6,e_7,e_8\\right\\},\\gamma_{i_1}(v_3)\\right],~\\left[\\gamma_{i_1}(v_3),\\gamma_{o_1}(v_3)\\right]\\end{aligned}\\ ] ]    ' '' ''    now , we have @xmath205 , @xmath206 , @xmath207 and @xmath208 therefore , the @xmath209 memory used for the edges in @xmath210 at the nodes @xmath211 and @xmath212 are ` absorbed ' into a single memory element used at node @xmath213 for edge @xmath214 , thus reducing the memory usage by @xmath215        ' '' ''    the memory reduction procedures of subsubsection [ distributedmemreduction1 ] , and subsubsection [ distributedmemreduction2 ] can sometimes result in exactly the same memory reduction event",
    ". however , there could be instances in which only one of the procedures can achieve memory reduction .",
    "for example , the memory reduction procedure of subsubsection [ distributedmemreduction1 ] can not reduce memory at node @xmath216 in the situation shown in example [ example1 ] because for any @xmath217 , @xmath218 , since @xmath219 however the memory reduction procedure of subsubsection [ distributedmemreduction2 ] does work as shown in fig [ fig : ex5 ] .",
    "similarly , in some cases , at a node , the procedure of subsubsection [ distributedmemreduction1 ] can be used to reduce memory usage , while subsubsection [ distributedmemreduction2 ] can not be applied .",
    "this is because of the fact that , at any node , the procedure of subsubsection [ distributedmemreduction2 ] takes into account only those sets of the form @xmath171 , while the procedure of subsubsection [ distributedmemreduction1 ] takes into account all possible incoming and outgoing edges .",
    "such a case is seen in example [ example3 ] .",
    "[ example3 ] fig .",
    "[ fig : example3 ] shows the node @xmath17 of fig .",
    "[ fig : example2 ] ( example [ example2 ] ) in a particular configuration .",
    "the memory reduction procedure of subsubsection [ distributedmemreduction2 ] can not be applied for the set @xmath220 because @xmath221    but @xmath222 , and therefore @xmath69 memory elements at node @xmath213 and @xmath223 can be absorbed into a single memory element at node @xmath17 , thereby facilitating memory reduction according to subsubsection [ distributedmemreduction1 ] .    .",
    "the box with the incoming edges @xmath224 and @xmath225 represents the node @xmath17 of fig .",
    "[ fig : example2 ] ( example [ example2]).,width=336 ]      the following technique can be used to distribute memory elements throughout the network in a somewhat uniform way .",
    "suppose there exists a node @xmath226 such that for some @xmath227 with @xmath228 and for some integer @xmath229 , @xmath230 then the @xmath81 memory elements at node @xmath231 used to delay symbols coming from edge @xmath45 can be ` absorbed ' into node @xmath17 ( thereby using them to delay symbols going into edge @xmath45 ) without changing the global kernels of any edge in @xmath232    this technique reduces the number of memory elements used at node @xmath231 for delaying its incoming symbols while increasing the number ( @xmath114 ) of memory elements used at node @xmath17 for delaying its outgoing symbols .",
    "[ ex3 ] fig [ fig : memdist ] illustrates an example for memory distribution between two nodes @xmath213 and @xmath233 in the figure on the top , @xmath234 and @xmath235 therefore one memory element from @xmath223 ( used to delay symbols coming from @xmath45 into @xmath223 ) can be ` absorbed ' into node @xmath213 ( and thereby used to delay symbols going into @xmath45 from @xmath213 ) .",
    "the boxes indicate the node to which the memory elements are attached . after distribution , @xmath236 and @xmath237     ( memory distribution).,width=307 ]",
    "this section presents the main contribution of this paper .    for an edge @xmath238 ,",
    "let @xmath239 represent the global kernel of @xmath240 we say that a node @xmath241 is a _ coding node _ if the global kernel of at least one of its outgoing edge is a @xmath56 linear combination of the global kernels of at least two of its incoming edges . otherwise , we call @xmath17 a _ forwarding node_.    let @xmath242 be the set of coding nodes , and @xmath243 be the set of forwarding nodes",
    ". let @xmath244 be the set of all coding nodes such that there exist no path in the network from any other coding node to any node in @xmath245    towards proposing an algorithm to enable single - generation network coding , we make some observations and discuss the addition of memory elements at the coding nodes to achieve single - generation network coding .    for any @xmath246",
    ", the global kernel of any @xmath23 is of the form @xmath247 for some positive integer @xmath248 , with @xmath249 if the network is a unit - delay network and the node @xmath17 uses no memory , the global kernel of any @xmath227 is of the form @xmath250 where @xmath251 is a positive integer signifying accumulated delay from the source to edge @xmath44 , and @xmath252 signifies the local kernel coefficient between @xmath44 and @xmath253 the additional @xmath6 is to account for the delay in the unit delay network .      for every pair of edges @xmath254 ( @xmath147 being as in ( [ eqn27 ] ) ) in ( [ eqn24 ] ) such that @xmath255",
    ", we may add @xmath256 memory elements at node @xmath17 to delay the symbols coming from @xmath44 such that the global kernel of the edge @xmath45 becomes @xmath257 where @xmath258 and @xmath259 once this process of using memory at the node @xmath17 results in the global kernel of every edge in @xmath18 to be a linear combination of symbols from the same generation ( generations between different outgoing edges need not be the same ) , we say that _ single - generation processing _ has been achieved at node @xmath115 for a node @xmath62 , we say that single - generation processing has been achieved at sink @xmath12 if the condition ( [ eqn28 ] ) is satisfied along with condition ( [ eqn44 ] ) for each @xmath260    we iteratively define the set @xmath261 as the set of coding nodes which have path only from @xmath262 where @xmath244 is as defined before . once memory has been used to achieve single - generation processing at all nodes in @xmath263 , it can be observed that the global kernels of the incoming and outgoing edges of any node @xmath264 satisfy the same condition as in ( [ eqn23 ] ) and ( [ eqn24 ] ) .",
    "thus again memory elements can be used at the nodes of @xmath265 to implement single - generation processing , ultimately achieving single - generation processing at each coding node of the network .",
    "algorithm [ alg : singlegen ] shown in the next page is used to achieve single - generation network coding using memory at the nodes of the network , while trying to minimize the total number of memory elements used in the network .",
    "now the global kernel of any edge @xmath266 of any sink @xmath12 is of the form @xmath267 for some positive integer @xmath268 with @xmath269    ' '' ''    [ remark1 ] algorithm [ alg : singlegen ] assumes that every node has unlimited memory to use and then tries to obtain a configuration that reduces the number of memory elements used in the network .",
    "however , if the maximum available memory in the nodes is limited , then the following techniques may be adopted after running algorithm [ alg : singlegen ] .    * in line 27 of the algorithm , instead of checking condition ( [ eqn30 ] ) at every pair of nodes connected by some edge",
    ", the actual memory capability of the nodes must be taken into account and then the distribution procedure of subsection [ memdist ] can be run . * finally , at every node in which the algorithm demands more memory elements than what is available , sufficient memory elements should be removed so that the total memory used at the node is utmost what is available . as the penalty of removing these memory elements",
    "will be compensated by the sinks , the memory elements that will be removed at the nodes should ideally be such that the compensation occurs in the least number of sinks in the least possible quantity .",
    "[ lastex ] fig .",
    "[ fig : ex2a ] , fig .",
    "[ fig : ex3 ] , and fig .",
    "[ fig : ex4 ] represent the network at various stages of the algorithm applied on a modified double - butterfly network as shown in fig .",
    "[ fig : ex1a ] . the modified unit - delay double - butterfly network shown in fig .",
    "[ fig : ex2a ] has the standard network code over @xmath270 @xmath11 is the source node , @xmath271 are the sinks .",
    "the dotted lines represent the virtual input edges at the source and virtual output edges at the sinks .        ' '' ''        ' '' ''        ' '' ''        ' '' ''    table [ tab1 ] shows the network transfer matrices before and after obtaining single - generation processing using algorithm [ alg : singlegen ] .",
    "table [ tab1 ] also shows a comparison between the memory requirements at the sinks ( for decoding ) between inter - generation network coding ( i.e the memory - free case ; the numbers shown are the sum of the row degrees of realizable inverse matrices in the third column ) and single - generation network coding ( as shown in fig .",
    "[ fig : ex4 ] ) . in the memory - free case , assuming that sinks use memory individually to decode , the total number of memory elements used in the network is @xmath272 , and all of them are used at the sinks . in the single - generation network coded network as shown in fig .",
    "[ fig : ex4 ] , it can be seen that the total number of memory elements used in the network is @xmath273 , out of which only @xmath274 are used at the sinks , thereby showing a marked reduction from the memory - free case .",
    "the rest of the memory elements ( numbering @xmath275 ) are distributed across the nodes of the network .    [ cols=\"^,^,^,^,^,^ \" , ]     [ tab1 ]      we can compare the straightforward approach of @xcite and our approach to obtaining a single - generation network coded network for the modified unit - delay double - butterfly network of fig .",
    "[ fig : ex1a ] . according to the technique in @xcite",
    ", the result would be the network as in fig .",
    "[ fig : ex2a ] , thereby resulting in the use of @xmath276 memory elements to obtain single - generation network coding . however , our algorithm utilizes the memory reduction and distribution techniques as given in section [ sec4 ] and results in the output being as in fig [ fig : ex4 ] using @xmath273 memory elements and a more uniform distribution of memory elements across the network than in fig .",
    "[ fig : ex2a ] .",
    "although the overall memory usage is reduced , it still remains to be shown whether algorithm [ alg : singlegen ] actually obtains a configuration of the network with minimal number of memory elements being used to obtain single - generation network coding .",
    "_ construction of a cnecc : _ for details on the basics of convolutional codes , we refer the reader to @xcite . the construction of a cnecc @xcite for a given acyclic , unit - delay , memory - free network which corrects error vectors corresponding to a given set @xmath277 of error patterns ( an error pattern is a subset of @xmath3 indicating the edges in error ) can be summarized as follows    * compute the set @xmath278 of _ error vector reflections _ given by @xmath279 where @xmath280 is an error vector , and @xmath281 means that @xmath282 matches an error pattern @xmath283 @xmath284$](the ring of polynomials ) is some _ processing function _ chosen such that the _ processing matrix _",
    "@xmath285 is a polynomial matrix .",
    "* let @xmath286 choose an input convolutional code @xmath287 with free distance at least @xmath288 as the cnecc for the given network .",
    "the following lemma gives a bound on @xmath289 and therefore the free distance demanded of the cnecc .",
    "[ tdelay ] given an acyclic , unit - delay , memory - free network @xmath65 with a given error pattern set @xmath277 , let @xmath290 be the maximum degree of any polynomial in the @xmath120 matrix .",
    "let @xmath291 indicate the hamming weight over @xmath60 if @xmath292 is the maximum number of non - zero coefficients of the polynomials @xmath293 corresponding to all sinks in @xmath8 , i.e @xmath294 , then we have @xmath295.\\ ] ]    algorithm [ alg : singlegen ] does not increase the value of @xmath296 in the matrix @xmath120 because of the fact that an additional delay would not be introduced on any path between nodes which are at a distance of @xmath296 edges ( the maximum number of edges on any path between any two nodes ) from each other .",
    "also , with memory being introduced in the nodes according to algorithm [ alg : singlegen ] , the network transfer matrices at all the sinks are of the form as given in ( [ eqn28 ] ) .",
    "therefore the processing functions at any sink @xmath12 is of the form @xmath297 , i.e @xmath298    therefore we have that , for the network with delay and memory ( used to achieve single - generation network coding ) , @xmath299.\\ ] ]    thus , it is seen that the bound for @xmath289 and therefore for the free distance demanded of the cnecc may be lower ( if @xmath300 ) for the unit - delay , single - generation network coded network compared to the unit - delay , memory - free counterpart .",
    "however a decrease in the actual value of @xmath289 can not be guaranteed and has to be computed for every network individually in order to decide whether the cnecc designed for the unit - delay , memory - free network will continue to work for the single - generation network coded unit - delay counterpart .",
    "_ decoding of a cnecc : _ let @xmath301 be the generator matrix of the code @xmath287 thus designed",
    ". then we refer to the code @xmath287 as the _ input convolutional code _",
    "@xcite . the effective code seen by a sink @xmath12",
    "is generated by the matrix @xmath302 , which is known as the _ _ output convolutional code__@xcite , @xmath303 , at sink @xmath94 the decoding of the cnecc at any sink @xmath12 can be performed either on the trellis of the code @xmath287 or that of the code @xmath303 at that particular sink according to the free distance of @xmath303 ( @xmath304 ) , the catastrophic / non - catastrophic nature of @xmath305 , and a parameter called @xmath306 , whose definition for a rate @xmath307 code @xmath308 over @xmath4 is given in @xcite as follows .",
    "@xmath309 where @xmath310 @xcite is defined as follows .",
    "@xmath311 where @xmath312\\ ] ] is a truncated codeword sequence with @xmath313 , @xmath314 indicates the content of the delay elements in the encoder at a time @xmath315 , and @xmath291 indicates the hamming weight over @xmath60 the set @xmath310 consisting of all possible truncated codeword sequences @xmath316 of weight less than @xmath317 that start in the zero state .",
    "then , we have the following proposition .",
    "[ minweighttime ] the minimum hamming weight trellis decoding algorithm can correct all error sequences which have the property that the hamming weight of the error sequence in any consecutive @xmath318 segments ( a segment being a collection of @xmath319 output symbols corresponding to every @xmath320 input symbols ) is utmost @xmath321 .    with the cnecc in place in a unit - delay .",
    "memory - free network , under certain conditions ( see subsection iv - d of @xcite ) , a sink has to decode on the trellis of the input convolutional code , in which case the sink has to multiply the incoming @xmath30 output streams with the processing matrix @xmath322 , which may require additional memory elements to implement .",
    "however , with a single - generation network code implemented using memory elements , part of this processing is done in a distributed manner in the other nodes of the network , thereby decreasing the memory requirement at the sinks .    in the forthcoming section , we further observe the advantages that the use of memory in the intermediate nodes offers in the performance of cneccs under a probabilistic error setting .",
    "probabilistic error models have been considered in the context of random network coding in @xcite .",
    "we define a probabilistic error model for a unit delay network @xmath65 by defining the probabilities of any set of @xmath323 edges of the network being in error at any given time instant . across time instants , we assume that the network errors are i.i.d . according to this distribution .",
    "@xmath324 where @xmath325 and @xmath326 are real numbers indicating the probability of any single edge error in the network and the probability of no edges in error respectively , such that @xmath327      fig .",
    "[ fig : butterflydelaymem ] on the top of the next page shows a modified butterfly network before and after running algorithm [ alg : singlegen ] .",
    "this network is clearly a part of the modified double - butterfly network of fig .",
    "[ fig : ex1a ] , and the associated matrices at the sinks @xmath67 and @xmath68 are given in table [ tab1 ] .        ' '' ''    with the probability model as in ( [ eq:1 ] ) and ( [ eq:2 ] ) with @xmath328 for this network , we simulate the performance of @xmath209 input convolutional codes implemented on this network for both the with - memory and memory - free cases as in fig . [",
    "fig : butterflydelaymem ] with the sinks performing hard decision decoding on the trellis of the input convolutional code .    in the following discussion",
    "we refer to sinks @xmath67 and @xmath68 of fig . [",
    "fig : butterflydelaymem ] as sink 1 and sink 2 . the @xmath209 input convolutional codes and the rationality behind choosing them",
    "are given as follows .",
    "* code @xmath329 is generated by the generator matrix @xmath330,\\ ] ] with @xmath331 and @xmath332 this code is chosen only to illustrate the error correcting capability of codes with low values of @xmath317 and @xmath333 * code @xmath334 is generated by the generator matrix @xmath335,\\ ] ] with @xmath336 and @xmath337 this code corrects all double edge errors in the instantaneous version ( with all edge delays and memories being zero ) of fig .",
    "[ fig : butterflydelaymem ] as long as they are separated by @xmath338 network uses .",
    "* code @xmath339 is generated by the generator matrix @xmath340,\\ ] ] with @xmath341 and @xmath342 this code corrects all double edge errors in the unit - delay network given in fig . [",
    "fig : butterflydelaymem ] as long as they are separated by @xmath273 network uses .",
    "we note here that values of @xmath318 of the @xmath209 codes are directly proportional to their free distances , i.e , the code with greater free distance has higher @xmath318 . fig .",
    "[ fig : bersink1mem ] and fig .",
    "[ fig : bersink2mem ] illustrate the bers for these @xmath209 codes for both the with - memory and memory - free case for different values of the parameter @xmath343 ( the probability of a single edge error ) of ( [ eq:1 ] ) .",
    "clearly the ber values fall with decreasing @xmath344        ' '' ''        ' '' ''    the description and explanation of the regions marked ` @xmath345 dominated region ' and ` @xmath346 dominated region ' ( named so according to the dominant parameter in those regions ) are given in @xcite . in the following discussion , we concentrate on the comparison between the performance of every code in the memory - free and the with - memory case . towards that end , we recall from proposition [ minweighttime ] that both the hamming weight of error events and the separation between any two consecutive error events are important to correct them .      1 .   with respect to codes @xmath334 and @xmath339 , we see that there is an improvement in performance when memory is used at the intermediate nodes .",
    "this is because of the fact that the presence of memory elements in the network results in a clumping - together of error bits at the sinks .",
    "for example , assume that in the network of fig .",
    "[ fig : butterflydelaymem ] , an error occurs in edge @xmath347 at time instant @xmath348 we consider the situation at sink 2 . in the memory - free case ,",
    "the effect of this error is felt at different time instants at the two incoming edges of sink 2 , at @xmath349 and at @xmath350 .",
    "however , with memory elements at the intermediate nodes , the effects of the edge error now occur at the same time instant ( @xmath350 ) in both the incoming edges of sink 2 .",
    "the effect of such errors cumulatively result in more error events ( with less hamming weights each ) in the memory - free case ( because of the distribution of errors ) and less error events ( with comparatively more hamming weights each ) in the with - memory case ( as a result of clumped errors ) .",
    "however , because codes @xmath334 and @xmath339 have enough free distance , the number of such error events is what dominates the performance . therefore codes @xmath334 and @xmath339 correct more errors in the with - memory case . the same effect",
    "may be observed at sink 1 also .",
    "2 .   with respect to the code @xmath329",
    ", there is no observable change in performance between the memory - free and with - memory cases .",
    "we note that the same effect is observed with the errors as in the previous case .",
    "but because of @xmath351 being less ( only @xmath69 ) , the clumping together of error bits does not benefit much",
    ". therefore there is no significant improvement in performance .",
    "3 .   there is no significant difference in the performance of any code between the memory - free and the with - memory case in the ` @xmath345 dominated region . '",
    "this is because of the fact that the errors that occur in the network are already sparse .",
    "this work was supported partly by the drdo - iisc program on advanced research in mathematical engineering through a research grant to b.  s.  rajan .",
    "k. prasad and b. sundar rajan , `` convolutional codes for network - error correction '' , arxiv:0902.4177v3 [ cs.it ] , august 2009 , available at : http://arxiv.org/abs/0902.4177 .",
    "a shortened version of this paper is to appear in the proceedings of globecom 2009 , nov .",
    "4 , honolulu , hawaii , usa .    k. prasad and b. sundar rajan , `` network error correction for unit - delay , memory - free networks using convolutional codes '' , arxiv:0903.1967v3[cs.it ] , september 2009 , available at : http://arxiv.org/abs/0903.1967 .        d. silva , f .",
    "r kschischang , and r. koetter , `` capacity of random network coding under a probabilistic error model '' , 24th biennial symposium on communications , kingston , usa , 24 - 26 june 2008 , pp ."
  ],
  "abstract_text": [
    "<S> a single - source network is said to be _ memory - free _ </S>",
    "<S> if all of the internal nodes ( those except the source and the sinks ) do not employ memory but merely send linear combinations of the incoming symbols ( received at their incoming edges ) on their outgoing edges . </S>",
    "<S> memory - free networks with delay using network coding are forced to do inter - generation network coding , as a result of which the problem of some or all sinks requiring a large amount of memory for decoding is faced . in this work , we address this problem by utilizing memory elements at the internal nodes of the network also , which results in the reduction of the number of memory elements used at the sinks . </S>",
    "<S> we give an algorithm which employs memory at the nodes to achieve single - generation network coding . for fixed latency </S>",
    "<S> , our algorithm reduces the total number of memory elements used in the network to achieve single - generation network coding . we also discuss the advantages of employing single - generation network coding together with convolutional network - error correction codes ( cneccs ) for networks with unit - delay and </S>",
    "<S> illustrate the performance gain of cneccs by using memory at the intermediate nodes using simulations on an example network under a probabilistic network error model . </S>"
  ]
}