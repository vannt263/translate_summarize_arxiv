{
  "article_text": [
    "in various fields , discovering hidden homogeneous classes from high - dimension , low - sample size ( hdlss ) data is of significant importance . many clustering methods for high - dimensional data have been proposed ( e.g. , ahn et al . , 2013 ; liu et al . ,",
    "2008;witte and tibsirani , 2010 ) .",
    "one prevalent clustering method operates via variable selection ( e.g. , witte and tibsirani , 2010 ) .",
    "conversely , ahn et al .",
    "( 2013 ) focused on how to measure the distance between clusters and proposed an efficient clustering method . in this study , as in the case of ahn et al .",
    "( 2013 ) , we focus on the means of measuring the distance between clusters .",
    "hall et al .",
    "( 2005 ) prove the significant fact regarding the geometric representation of data points in hdlss contexts .",
    "based on this fact , the closeness of the euclidean distance depends on the mean and variance structure , and does not always contain hidden cluster information .",
    "thus , a classical clustering method does not always work well for high dimensional data . for more details about the asymptotic behaviors of the classical hierarchical method for high - dimensional data ,",
    "see borysov et al .",
    "thus , we need a distance measure between clusters for hdlss data that is more appropriate than the euclidean distance . the maximal data piling ( mdp ) distance ( ahn and marron , 2010 ) is one possible choice for measuring the difference between clusters .",
    "the mdp distance was proposed in the context of supervised learning , but we can also apply this distance measure to the case of unsupervised learning .",
    "ahn et al .",
    "( 2013 ) proposed a hierarchical clustering method based on the mdp distance , called mdp clustering . in ahn",
    "et al . ( 2013 ) study , under certain conditions , mdp clustering can detect the difference between mean vectors of  two \" clusters when the dimension tends to infinity with the sample size fixed .",
    "in addition , ahn et al .",
    "( 2013 ) showed that we can approximate mdp clustering by a simple algorithm based on singular value decomposition .",
    "these properties have proven to be quite useful for hdlss data .    however , the sufficient condition for the label consistency of mdp clustering depends on the sample sizes and variances of the two clusters , while mdp clustering only focuses on the difference between the mean vectors of two clusters .",
    "moreover , we can not detect the differences between the variances of clusters . in hdlss contexts , there is some possibility that the difference between the variances of each cluster contains the cluster information .",
    "in this study , we point out the important fact that it is not the closeness , but the  _ values _ \" of the euclidean distance that contain information regarding the cluster structure in high - dimensional space .",
    "based on this fact , we propose an efficient and simple clustering approach , called distance vector clustering , for hdlss data . by the proposed approach , we can detect not only the differences between mean vectors of clusters but also the differences between the variances of clusters .",
    "moreover , the computational cost of the proposal approach increases linearly with the number of dimensions of data . under the assumption given in the work of hall et al .",
    "( 2005 ) , we show that the proposed approach also gives the true cluster label under milder conditions when the dimension tends to infinity with the sample size fixed .",
    "this paper is organized as follows . in section @xmath0 ,",
    "some notation and preliminaries are described .",
    "then , the difficulty of clustering hdlss data by the usual method is presented , and the sufficient condition for label consistency of mdp clustering is discussed . in section @xmath1 , the main idea of the proposed method is described , and the algorithm of the distance vector clustering is proposed . in section @xmath2 ,",
    "sufficient conditions for the asymptotic label consistency of the proposed approach are described . in sections",
    "@xmath3 and @xmath4 , the effectiveness of the proposed approach is illustrated through a numerical experiment and real data analysis , respectively .",
    "let @xmath5 be the number of clusters , @xmath6 be the sample size , and @xmath7 be the sample size of the @xmath8-th cluster @xmath9 .",
    "that is , @xmath10 .",
    "@xmath11 denotes the @xmath12-dimensional random vector for the @xmath8-th cluster @xmath9 . for @xmath13 ,",
    "let the independent and identically distributed ( i.i.d . )",
    "sample points of the @xmath8-th cluster be denoted by @xmath14 . as was done by ahn et al .",
    "( 2013 ) , we also assume the following conditions in hall et al .",
    "( 2005 ) :    @xmath15 ^ 2\\rightarrow \\mu_{k}^2\\quad$ ] as @xmath16 ,    @xmath17 ^ 2\\rightarrow \\sigma_{k}^2\\quad$ ] as @xmath16 ,    @xmath18 ^ 2 -\\mathbb{e}[x_{ls}]^2 \\}\\rightarrow \\delta_{kl}^2\\quad$ ] as @xmath16 ,    there exists a permutation of variables , which is @xmath19-mixing for functions that are dominated by quadratics",
    ".    moreover , let @xmath20\\mathbb{e}[x_{ls}]$ ] . under these assumptions ,",
    "hall et al .",
    "( 2005 ) provides the following important facts .",
    "[ prop:1 ] ( hall et al . , 2005 )",
    "let @xmath21 and @xmath22 be sample points independently drawn from the distribution of @xmath23 . as @xmath12 goes to infinity , @xmath24 where @xmath25 and @xmath26 are the euclidean norm and the inner product , respectively .",
    "based on this fact , we obtain the sufficient condition for the label consistency of the classical hierarchical clustering method .",
    "let @xmath27 and @xmath28 be independent random vectors with the distribution of @xmath23 .",
    "let @xmath29 be a random vector with the distribution of @xmath30 . here",
    ", we assume that @xmath31 and @xmath29 are mutually independent .",
    "[ fig:1 ]     and @xmath29 in the hdlss contexts . ]    ( a )     and @xmath29 in the hdlss contexts . ]",
    "( b )    figure @xmath32 shows geometric representations of these objects in the hdlss context . if @xmath33 , then two objects in the same cluster , @xmath34 and @xmath35 , may be combined first in the classical hierarchical clustering . on the other hand , if @xmath36 , then two objects in different clusters , @xmath34 and @xmath37 , may be combined first in classical hierarchical clustering .",
    "thus , the sufficient condition for the label consistency of classical hierarchical clustering is given by @xmath38    these facts indicate that in hdlss contexts , the closeness of two objects may not reflect the hidden true cluster structure . thus , ahn et al .",
    "@xmath39 proposed a clustering method using the maximal data piling distance , called mdp clustering .",
    "the mdp distance between two clusters is defined as the orthogonal distance between the affine subspaces generated by the sample points in each cluster .",
    "mdp clustering finds successive binary splits , each of which creates two clusters in such a way that the mdp distance between them is as large as possible . in ahn",
    "et al . ( 2013 ) , the sufficient condition for label consistency of mdp clustering is given by @xmath40 where @xmath41 .",
    "since it is difficult to understand this condition directly , we consider two specific cases under the conditions @xmath42 .",
    "first , we consider the case where there is no clear difference between two mean vectors , that is , @xmath43 . in this case , the sufficient condition is given by @xmath44 but we have @xmath45 for all @xmath46 and the sufficient condition can not hold .",
    "thus , we can not detect the difference between variances of two clusters by mdp clustering .",
    "next , we consider the case that there is a clear difference between two mean vectors , that is , @xmath47 . in this case , the sufficient condition is given by @xmath48 for simplicity , we assume that @xmath49 . if @xmath50 , the sufficient condition holds .",
    "thus , when the difference between mean vectors of two clusters is sufficiently large , we can discover the true cluster structure .",
    "consequently , mdp clustering focuses on the difference between the mean vectors of two clusters .",
    "in hdlss data , there is some possibility that the differences between variances of each cluster contain the cluster information .",
    "moreover , the sufficient condition of mdp clustering depends on the variances and the sample sizes of two clusters , whereas mdp clustering focuses on the difference between the mean vectors of two clusters .    in this section",
    ", we propose a simple and efficient clustering approach based on the usual distance ( or inner product ) matrix . here",
    ", we first describe the main idea of our approach .",
    "[ example:2 ] let @xmath23 be a sample point drawn from the standard @xmath12-dimensional normal distribution @xmath51 . for fixed @xmath52 ,",
    "let @xmath30 be a sample point drawn from @xmath53 .",
    "let @xmath54 be i.i.d .",
    "copies of @xmath23 and @xmath55 be i.i.d .",
    "copies of @xmath30 .",
    "write @xmath56 . in this",
    "setting , the condition of mdp clustering does not hold .",
    "we compute the distance matrix for the data matrix @xmath57 .",
    "figure @xmath58 shows heatmaps of the distance matrices for various numbers of dimensions . from this figure",
    ", we can see that the contrast of the distance matrix between two clusters becomes apparent with increasing number of dimensions .",
    "[ fig:2 ]    ) . ]",
    "example @xmath59 indicates that _ in hdlss contexts the closeness between data points may not be meaningful , but `` values '' of distance contain the true cluster information . _",
    "based on this fact , we propose the following clustering algorithm :    step @xmath60 .",
    ": :    compute the usual euclidean distance matrix    @xmath61 ( or the inner product    matrix @xmath62 ) from the centered data matrix    @xmath63 .",
    "step @xmath64 .",
    ": :    compute the following distance matrix    @xmath65 from the matrix    @xmath66 ( or @xmath67 ) .",
    "@xmath68 step @xmath69 .",
    ": :    for the matrix @xmath70 , apply a usual clustering method    ( e.g. , ward s method ) .",
    "figure @xmath71 shows the flow of this algorithm using the inner product matrix @xmath62 and ward s method .     and",
    "ward s method . ]      in this section , we prove the label consistency of this algorithm with a conventional clustering method under the assumption given by hall et al .",
    "first , we prove the label consistency for the @xmath8-means type algorithm .",
    "the objective function of the @xmath8-means type distance vector clustering method is given by @xmath72 where @xmath73 is a partition of objects , @xmath74 we can optimize this function by the usual @xmath8-means algorithm ( e.g. , lloyd s algorithm ) .    from proposition @xmath75 , we can obtain the following result .",
    "[ lemma:1 ] let @xmath5 be the true number of clusters . under the general assumptions",
    "a ) - d ) , for an arbitrary @xmath76 , @xmath77    the proof is straightforward .",
    "based on lemma @xmath78 , we obtain the sufficient condition for the label consistency of the distance ( or inner product ) vector clustering approach .",
    "[ prop:2 ] we assume the general assumptions a ) - d ) and and also assume that @xmath79 .",
    "suppose that the true number of clusters @xmath5 is given .",
    "a ) : :    if @xmath80 or    @xmath81 , then the estimated cluster label vector    with the @xmath8-means type distance vector clustering method    based on the distance matrix @xmath66 converges to the true    label vector in probability as @xmath82 .",
    "b ) : :    moreover , if @xmath83 ,    then then the estimated cluster label vector with the    @xmath8-means type distance vector clustering method based on    the inner product matrix @xmath67 converges to the true label    vector in probability as @xmath82 .    let @xmath84 be the true cluster partition .",
    "from proposition [ prop:1 ] , for @xmath85 we have @xmath86 as @xmath82 . for @xmath87 and @xmath88 , as @xmath82 , @xmath89 conversely , for @xmath90 and @xmath91 , as @xmath82 , @xmath92 for @xmath93 and @xmath94 , if @xmath95 as @xmath82 , then we have @xmath96 which contradicts the assumption @xmath97 or @xmath81 .",
    "thus , we obtain the condition where @xmath98 converges in probability to some positive constant for @xmath93 and @xmath94 . from lemma @xmath78 and this fact ,",
    "we obtain the label consistency of the @xmath8-means type distance vector clustering method based on the distance matrix @xmath66 .",
    "next , we consider the @xmath8-means type clustering based on the inner product matrix @xmath67 . from proposition",
    "[ prop:1 ] , for @xmath85 we also have @xmath86 as @xmath82 . for @xmath87 and @xmath88 , as @xmath82 , @xmath99 conversely , for @xmath90 and @xmath91 , as @xmath82 , @xmath100 for @xmath93 and @xmath94 , if @xmath95 as @xmath82 , then we obtain @xmath101 which contradicts the assumption @xmath81 .",
    "thus , for the inner product matrix @xmath67 , we also obtain the condition where @xmath98 converges in probability to some positive constant for @xmath93 and @xmath94 . from lemma @xmath78 and this fact , we obtain the label consistency of the @xmath8-means type distance vector clustering method based on the inner product matrix @xmath67 .      for hierarchical clustering with the matrix @xmath70",
    ", the label consistency also holds under the same conditions as that of the @xmath8-means type method .",
    "the following theorem provides sufficient conditions of the label consistency for the distance vector clustering approach using classical hierarchical clustering methods ( e.g. , the single linkage and ward s method ) .",
    "we assume the general assumptions a ) - d ) and also assume that @xmath79 .",
    "let @xmath84 be the true cluster partition .",
    "a ) : :    if @xmath80 or    @xmath81 , then    @xmath102 where    @xmath103 .",
    "b ) : :    moreover , if @xmath83 ,    then    @xmath102 where    @xmath104 .",
    "the proof of this proposition is equivalent to the proof of proposition @xmath105 .    to compare the sufficient condition of mdp clustering",
    ", we consider the case where the number of clusters is two . in this case , the sufficient condition of proposed approach using the distance matrix is given by @xmath106 moreover , the sufficient condition of the proposed approach using the inner product matrix is given by @xmath107 thus , if we use the distance matrix , we can detect the differences between variances or mean vectors .",
    "alternatively , if we use the inner product matrix , we only focus on the differences between mean vectors . moreover , the sufficient conditions of our approach do not depend on the sample size",
    ". the sufficient condition of our approach using the inner product matrix dose not depend on variances .",
    "in fact , the following example shows that the proposal clusterings with @xmath67 and @xmath66 works well , but mdp clustering does not .",
    "[ example:3 ] let @xmath108 and @xmath23 be a sample point drawn from the standard @xmath12-dimensional normal distribution @xmath109 .",
    "let @xmath30 be a sample point independently drawn from @xmath110 .",
    "let @xmath111 be i.i.d .",
    "copies of @xmath23 and @xmath112 be i.i.d .",
    "copies of @xmath30 . here , we set @xmath113 .",
    "write @xmath114 . in this setting ,",
    "the conditions for the label consistency and the approximation algorithm for mdp clustering does not hold while the conditions for the consistency of the proposed approach using @xmath67 and @xmath66 hold . figure [ fig:4 ] shows the results of these methods .",
    "figure [ fig:4 ] , we can see that the proposed approach using @xmath67 and @xmath66 works well , but mdp clustering does not .    , the colors of the points represent the cluster label ) and the proposed approach using ( b ) the inner product matrix @xmath67 and ( c ) the distance matrix @xmath66 . ]    ( a )    , the colors of the points represent the cluster label ) and the proposed approach using ( b ) the inner product matrix @xmath67 and ( c ) the distance matrix @xmath66 . ]",
    "( b )    , the colors of the points represent the cluster label ) and the proposed approach using ( b ) the inner product matrix @xmath67 and ( c ) the distance matrix @xmath66 . ]    ( c )",
    "in this section , we illustrate the performance of the proposed approach via numerical experiments . here , to compare the proposed approach , we choose ward s method , @xmath8-means clustering , sparse @xmath8-means ( sk - means ) clustering ( witten and tibshirani , 2010 ) , and mdp clustering . for the proposed approach ,",
    "we use the ward type and @xmath8-means type distance vector clustering methods with the inner product matrix @xmath67 and the distance matrix @xmath66 . here , we refer to the @xmath8-means type distance vector clustering methods using @xmath67 and @xmath66 as dskm and ddkm , respectively .",
    "similarly , we refer to the ward type distance vector clustering methods using @xmath67 and @xmath66 as dsw and ddw , respectively .    in these experiments",
    ", we set the true number of clusters @xmath115 , the number of variables @xmath116 and @xmath117 , and the sample size @xmath118 . for @xmath119",
    ", we set @xmath120 as the centers of the three clusters .",
    "observations @xmath121^t\\;(i=1,\\dots , n)$ ] are generated as @xmath122 where @xmath123 and @xmath124 are independently generated from the multinomial distribution for three trials with probabilities @xmath125 and the @xmath12-dimensional normal distribution @xmath126 , respectively . in this experiment , we use the following four settings :    * setting i. * let @xmath127 and @xmath128 , where @xmath129 is the @xmath130 identity matrix . set @xmath131 and @xmath132 .",
    "* setting ii .",
    "* let @xmath127 and @xmath133 where @xmath134 and @xmath135 are generated from the uniform distribution on the interval @xmath136 $ ] . set @xmath131 and @xmath132 .    * setting iii .",
    "* let @xmath137 and @xmath138 and @xmath139 , where @xmath140 .",
    "set @xmath131 and @xmath132 .    * setting iv .",
    "* let @xmath137 and @xmath133 where @xmath141 , @xmath142 @xmath143 and @xmath135 are generated from the uniform distribution on the interval @xmath136 $ ] . set @xmath131 and @xmath132 .",
    "we constructed @xmath144 datasets for each setting and applied the eight methods to each standardized dataset with zero means and unit variances . to compare the results of the eight clustering methods",
    ", we used the adjusted rand index ( ari ) ( hubert and arabie , 1985 ) . note that we applied to mdp clustering with the turning parameters @xmath145 and @xmath146 , and employed the best ari score as the ari score listed for the mdp clustering results .",
    "table @xmath147 shows the average ari scores and their standard errors for each method in setting i. in setting i , there are only differences between mean vectors of the three clusters . even if @xmath148 , mdp clustering and the distance vector clustering approach using the inner product matrix work well .",
    "since ddw and ddkm focus on differences between both mean vectors and variances , these methods do not work well in this setting .",
    "table @xmath149 shows the average ari scores and their standard errors for each method in setting ii . in setting ii ,",
    "there are also only differences between mean vectors of the three clusters but informative variables are correlated .",
    "the data for setting ii show a similar tendency to that of setting i , although the ari scores of setting ii are overall lower than those of setting i.    .average ari scores and their standard errors for each method in setting i. [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]     [ table:1 ]    we mention here that there are some differences between our table and the results in ahn et al .",
    "for example , in ahn et al .",
    "( 2013 ) , the number of errors for the mdp clustering method for colon data is @xmath150 .",
    "however , in this work , the number of errors for the mdp clustering method for colon data is @xmath151 . these differences may be because of differences in data preprocessing . for the preprocessed data used in this study , the mdp distance for the split induced by the largest gap of the discarded first eigenvector is @xmath152 while the mdp distance with the discarded second eigenvector is @xmath153 . according to the algorithm of mdp clustering",
    ", we choose the split which has the largest mdp distance .",
    "thus , we must choose the split induced by the first eigenvector , while the number of errors of the second eigenvector is @xmath150 .",
    "in this study , we pointed out the important fact that it is not the closeness , but the  _ values _ \" of distance that contain information of the cluster structure in high - dimensional space .",
    "we proposed an efficient and simple clustering approach , called distance vector clustering , for hdlss data based on that fact . under the assumption of hall et al .",
    "( 2005 ) , we showed that the proposed approach provides the true cluster label under milder conditions when the dimension tends to infinity with the sample size fixed .",
    "the effectiveness of the distance vector clustering approach was illustrated through numerical experiments and real data analysis . under some regularity conditions , in hdlss data , we can detect the cluster structure , which consists of differences not only between mean vectors but also variances .",
    "moreover , we also showed that the distance vector clustering approach using the inner product matrix is less susceptible to the variances of hidden clusters than in the mdp clustering method . only the distance or the inner product matrix and the usual clustering algorithm",
    "are needed for the distance vector clustering approach .",
    "thus , the distance vector clustering approach is easily implementable and understandable .",
    "it can be considered from the present results that this approach is another possible choice for clustering hdlss data .    in future work , we intend to provide an efficient selection method for the determination of the number of clusters by this method .",
    "the author wishes to express his thanks to dr .",
    "shota katayama for his helpful discussions .",
    "this work was supported by grant - in - aid for jsps fellows number @xmath154 ."
  ],
  "abstract_text": [
    "<S> in high - dimension , low - sample size ( hdlss ) data , it is not always true that closeness of two objects reflects a hidden cluster structure . </S>",
    "<S> we point out the important fact that it is not the closeness , but the  _ values _ \" of distance that contain information of the cluster structure in high - dimensional space . based on this fact </S>",
    "<S> , we propose an efficient and simple clustering approach , called distance vector clustering , for hdlss data . under the assumptions given in the work of hall et al . </S>",
    "<S> ( 2005 ) , we show the proposed approach provides a true cluster label under milder conditions when the dimension tends to infinity with the sample size fixed . </S>",
    "<S> the effectiveness of the distance vector clustering approach is illustrated through a numerical experiment and real data analysis . </S>"
  ]
}