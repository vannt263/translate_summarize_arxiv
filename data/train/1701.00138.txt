{
  "article_text": [
    "the rnn - based encoder - decoder ( encdec ) approach has recently been providing significant progress in various natural language generation ( nlg ) tasks , _",
    "i.e. _ , machine translation ( mt )  @xcite and abstractive summarization ( abs )  @xcite .",
    "since a scheme in this approach can be interpreted as a conditional language model , it is suitable for nlg tasks .",
    "however , one potential weakness is that it sometimes repeatedly generates the same phrase ( or word ) .",
    "this issue has been discussed in the neural mt ( nmt ) literature as a part of a _ coverage problem _",
    "such repeating generation behavior can become more severe in some nlg tasks than in mt .",
    "the _ very short _ abs task in duc-2003 and 2004  @xcite is a typical example because it requires the generation of a summary in a pre - defined limited output space , such as ten words or 75 bytes .",
    "thus , the repeated output consumes precious limited output space .",
    "unfortunately , the coverage approach can not be directly applied to abs tasks since they require us to optimally find salient ideas from the input in a _ lossy compression _ manner , and thus the summary ( output ) length hardly depends on the input length ; an mt task is mainly _ loss - less _ generation and nearly one - to - one correspondence between input and output  @xcite .    from this background ,",
    "this paper tackles this issue and proposes a method to overcome it in abs tasks .",
    "the basic idea of our method is to jointly estimate the upper - bound frequency of each target vocabulary that can occur in a summary during the encoding process and exploit the estimation to control the output words in each decoding step .",
    "we refer to our additional component as a * word - frequency estimation ( wfe ) sub - model*. the wfe sub - model explicitly manages how many times each word has been generated so far and might be generated in the future during the decoding process .",
    "thus , we expect to decisively prohibit excessive generation .",
    "finally , we evaluate the effectiveness of our method on well - studied abs benchmark data provided by rush et al .  , and evaluated in  @xcite .",
    "the baseline of our proposal is an rnn - based encdec model with an attention mechanism  @xcite .",
    "in fact , this model has already been used as a strong baseline for abs tasks  @xcite as well as in the nmt literature . more specifically , as a case study we employ a _",
    "2-layer bidirectional lstm _ encoder and a _",
    "2-layer lstm _ decoder with a global attention  @xcite .",
    "we omit a detailed review of the descriptions due to space limitations .",
    "the following are the necessary parts for explaining our proposed method .",
    "let @xmath0 and @xmath1 be input and output sequences , respectively , where @xmath2 and @xmath3 are one - hot vectors , which correspond to the @xmath4-th word in the input and the @xmath5-th word in the output .",
    "let @xmath6 denote the vocabulary ( set of words ) of output . for simplification",
    ", this paper uses the following four notation rules :    1 .",
    "@xmath7 is a short notation for representing a list of ( column ) vectors , _",
    "i.e. _ , @xmath8 .",
    "2 .   @xmath9 represents a @xmath10-dimensional ( column ) vector whose elements are all @xmath11 , _",
    "i.e. _ , @xmath12 .",
    "3 .   @xmath13 $ ] represents the @xmath4-th element of @xmath14 , _",
    "i.e. _ , @xmath15 , then @xmath16=0.2 $ ] .",
    "4 .   @xmath17 and , @xmath18 always denotes the index of output vocabulary , namely , @xmath19 , and @xmath20 $ ] represents the score of the @xmath18-th word in @xmath21 , where @xmath22 .    * encoder * : let @xmath23 denote the overall process of our 2-layer bidirectional lstm encoder .",
    "the encoder receives input @xmath24 and returns a list of final hidden states @xmath25 : @xmath26    [ cols= \" > , < \" , ]",
    "we investigated the effectiveness of our method on abs experiments , which were first performed by rush et al . ,  .",
    "the data consist of approximately 3.8 million training , 400,000 validation and 400,000 test data , respectively .",
    "generally , 1951 test data , randomly extracted from the test data section , are used for evaluation .",
    "additionally , duc-2004 evaluation data  @xcite were also evaluated by the identical models trained on the above gigaword data .",
    "we strictly followed the instructions of the evaluation setting used in previous studies for a fair comparison .",
    "table  [ table : configuration ] summarizes the model configuration and the parameter estimation setting in our experiments .",
    "table  [ table : result_duc2004 ] shows the results of the baseline encdec and our proposed encdec+wfe .",
    "note that the duc-2004 data was evaluated by recall - based rouge scores , while the gigaword data was evaluated by f - score - based rouge , respectively . for a validity confirmation of our encdec baseline , we also performed opennmt tool .",
    "the results on gigaword data with @xmath27 were , 33.65 , 16.12 , and 31.37 for rouge-1(f ) , rouge-2(f ) and rouge - l(f ) , respectively , which were almost similar results ( but slightly lower ) with our implementation .",
    "this supports that our baseline worked well as a strong baseline .",
    "clearly , encdec+wfe significantly outperformed the strong encdec baseline by a wide margin on the rouge scores .",
    "thus , we conclude that the wfe sub - model has a positive impact to gain the abs performance since performance gains were derived only by the effect of incorporating our wfe sub - model .",
    "table  [ table : topsystem ] lists the current top system results .",
    "our method encdec+wfe successfully achieved the current best scores on most evaluations .",
    "this result also supports the effectiveness of incorporating our wfe sub - model .",
    "mrt  @xcite previously provided the best results .",
    "note that its model structure is nearly identical to our baseline . on the contrary , mrt trained a model with a sequence - wise minimum risk estimation , while we trained all the models in our experiments with standard ( point - wise ) log - likelihood maximization .",
    "mrt essentially complements our method .",
    "we expect to further improve its performance by applying mrt for its training since recent progress of nmt has suggested leveraging a sequence - wise optimization technique for improving performance  @xcite .",
    "we leave this as our future work .",
    "figure  [ fig : raw_generation ] shows actual generation examples . based on our motivation , we specifically selected the redundant repeating output that occurred in the baseline encdec .",
    "it is clear that encdec+wfe successfully reduced them .",
    "this observation offers further evidence of the effectiveness of our method in quality .      to evaluate the wfe sub - model alone , table  [ table : result_wordset ]",
    "shows the confusion matrix of the frequency estimation .",
    "we quantized @xmath28 by @xmath29 + 0.5\\rfloor$ ] for all @xmath18 , where 0.5 was derived from the margin in @xmath30 . unfortunately , the result looks not so well .",
    "there seems to exist an enough room to improve the estimation .",
    "however , we emphasize that it already has an enough power to improve the overall quality as shown in table  [ table : result_duc2004 ] and figure  [ fig : raw_generation ]",
    ". we can expect to further gain the overall performance by improving the performance of the wfe sub - model .",
    "this paper discussed the behavior of redundant repeating generation often observed in neural encdec approaches .",
    "we proposed a method for reducing such redundancy by incorporating a sub - model that directly estimates and manages the frequency of each target vocabulary in the output .",
    "experiments on abs benchmark data showed the effectiveness of our method , encdec+wfe , for both improving automatic evaluation performance and reducing the actual redundancy .",
    "our method is suitable for _ lossy compression _ tasks such as image caption generation tasks .",
    "kyunghyun cho , bart van merrienboer , caglar gulcehre , dzmitry bahdanau , fethi bougares , holger schwenk , and yoshua bengio . .",
    "in _ proceedings of the 2014 conference on empirical methods in natural language processing ( emnlp 2014 ) _ , pages 17241734 , 2014 .",
    "zhaopeng tu , zhengdong lu , yang liu , xiaohua liu , and hang li . .",
    "in _ proceedings of the 54th annual meeting of the association for computational linguistics ( volume 1 : long papers ) _ , pages 7685 , berlin , germany , august 2016 .",
    "association for computational linguistics .",
    "url http://www.aclweb.org/anthology/p16-1008 .",
    "haitao mi , baskaran sankaran , zhiguo wang , and abe ittycheriah .",
    "coverage embedding models for neural machine translation . in _ proceedings of the 2016 conference on empirical methods in natural language processing _",
    ", pages 955960 , austin , texas , november 2016 .",
    "association for computational linguistics .",
    "url https://aclweb.org/anthology/d16-1096 .",
    "sumit chopra , michael auli , and alexander  m. rush . . in _ proceedings of the 2016 conference of the north american chapter of the association for computational linguistics :",
    "human language technologies _ , pages 9398 , san diego , california , june 2016 .",
    "association for computational linguistics .",
    "url http://www.aclweb.org/anthology/n16-1012 .",
    "ramesh nallapati , bowen zhou , cicero dos santos , caglar gulcehre , and bing xiang . . in _ proceedings of the 20th signll conference on computational natural language learning _ , pages 280290 , berlin , germany , august 2016 .",
    "association for computational linguistics .",
    "url http://www.aclweb.org/anthology/k16-1028 .",
    "yuta kikuchi , graham neubig , ryohei sasano , hiroya takamura , and manabu okumura .",
    "controlling output length in neural encoder - decoders . in _ proceedings of the 2016 conference on empirical methods in natural language processing _ , pages 13281338 , austin , texas ,",
    "november 2016 .",
    "association for computational linguistics .",
    "url https://aclweb.org/anthology/d16-1140 .",
    "sho takase , jun suzuki , naoaki okazaki , tsutomu hirao , and masaaki nagata .",
    "neural headline generation on abstract meaning representation . in _ proceedings of the 2016 conference on empirical methods in natural language processing _",
    ", pages 10541059 , austin , texas , november 2016 .",
    "association for computational linguistics .",
    "url https://aclweb.org/anthology/d16-1112 .",
    "caglar gulcehre , sungjin ahn , ramesh nallapati , bowen zhou , and yoshua bengio . .",
    "in _ proceedings of the 54th annual meeting of the association for computational linguistics ( volume 1 : long papers ) _ , pages 140149 , berlin , germany , august 2016 .",
    "association for computational linguistics .",
    "url http://www.aclweb.org/anthology/p16-1014 .",
    "thang luong , hieu pham , and christopher  d. manning . . in _ proceedings of the 2015 conference on empirical methods in natural language processing _ , pages 14121421 ,",
    "lisbon , portugal , september 2015 .",
    "association for computational linguistics .",
    "url http://aclweb.org/anthology/d15-1166 .",
    "xavier glorot , antoine bordes , and yoshua bengio . . in geoffrey  j. gordon and david  b. dunson , editors , _ proceedings of the fourteenth international conference on artificial intelligence and statistics ( aistats-11 ) _ , volume  15 , pages 315323",
    "journal of machine learning research - workshop and conference proceedings , 2011 .",
    "url http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf .",
    "yonghui wu , mike schuster , zhifeng chen , quoc  v. le , mohammad norouzi , wolfgang macherey , maxim krikun , yuan cao , qin gao , klaus macherey , jeff klingner , apurva shah , melvin johnson , xiaobing liu , lukasz kaiser , stephan gouws , yoshikiyo kato , taku kudo , hideto kazawa , keith stevens , george kurian , nishant patil , wei wang , cliff young , jason smith , jason riesa , alex rudnick , oriol vinyals , greg corrado , macduff hughes , and jeffrey dean .",
    "google s neural machine translation system : bridging the gap between human and machine translation .",
    "_ corr _ , abs/1609.08144 , 2016 .",
    "url http://arxiv.org/abs/1609.08144 .",
    "ian  j. goodfellow , david warde - farley , mehdi mirza , aaron  c. courville , and yoshua bengio . .",
    "in _ proceedings of the 30th international conference on machine learning , icml 2013 , atlanta , ga , usa , 16 - 21 june 2013 _ , pages 13191327 , 2013 .",
    "url http://jmlr.org/proceedings/papers/v28/goodfellow13.html .",
    "sam wiseman and alexander  m. rush .",
    "sequence - to - sequence learning as beam - search optimization . in _ proceedings of the 2016 conference on empirical methods in natural language processing _",
    ", pages 12961306 , austin , texas , november 2016 .",
    "association for computational linguistics .",
    "url https://aclweb.org/anthology/d16-1137 .",
    "shiqi shen , yong cheng , zhongjun he , wei he , hua wu , maosong sun , and yang liu",
    ". minimum risk training for neural machine translation . in _ proceedings of the 54th annual meeting of the association for computational linguistics ( volume 1 : long papers ) _ , pages 16831692 , berlin , germany , august 2016 .",
    "association for computational linguistics .",
    "url http://www.aclweb.org/anthology/p16-1159 ."
  ],
  "abstract_text": [
    "<S> this paper tackles the reduction of redundant repeating generation that is often observed in rnn - based encoder - decoder models . </S>",
    "<S> our basic idea is to jointly estimate the upper - bound frequency of each target vocabulary in the encoder and control the output words based on the estimation in the decoder . </S>",
    "<S> our method shows significant improvement over a strong rnn - based encoder - decoder baseline and achieved its best results on an abstractive summarization benchmark . </S>"
  ]
}