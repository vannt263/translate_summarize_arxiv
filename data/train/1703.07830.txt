{
  "article_text": [
    "kernel methods play an important role in solving various machine learning problems , such as non - linear regression and classification tasks @xcite .",
    "the kernel methods rely on the so called `` kernel trick '' , which transforms a given non - linear problem into a linear one by using a kernel function @xmath0 , which is a similarity function defined over pairs of input data points @xmath1 .",
    "this way , the input data @xmath2 is mapped into a high dimensional ( or even infinite - dimensional ) feature space @xmath3 , where the inner product @xmath4 can be calculated with a positive definite kernel function ( that is satisfying mercer s condition)@xcite : @xmath5 therefore , the mapping into the high dimensional feature space is done implicitly , without the need to explicitly map the data points @xmath3 . also , assuming that @xmath6 is the training data , then using the representer theorem any non - linear function @xmath7 can be expressed as a linear combination of kernel products evaluated on the training data points @xcite : @xmath8    the main methods for the kernel classification problems are the support vector machines ( svm)@xcite and the least - squares support vector machines ( ls - svm)@xcite . in this paper",
    "we focus on the ls - svm classifier , where the main difficulty is the @xmath9 training complexity , where @xmath10 is the size of the training data set . because of this high complexity",
    ", the ls - svm method is not a suitable candidate for applications with large data sets . here , we discuss several approximation methods using randomized block kernel matrices , that significantly reduce the complexity of the problem .",
    "the proposed methods are based on the nystrm@xcite , kaczmarz@xcite and matching pursuit@xcite algorithms , and we show that they provide good accuracy and reliable scaling to relatively large multi - class classification problems .",
    "in the binary classification setting the kernel svm method constructs an optimal separating hyperplane ( with the maximal margin ) between the two classes in the feature space @xmath3 .",
    "the training problem is represented as a ( convex ) quadratic programming problem involving inequality constraints , which has a global and unique solution @xcite .",
    "the kernel ls - svm method simplifies the optimization problem by considering equality constraints only , such that the solution is obtained by solving a system of linear equations @xcite . with these modifications ,",
    "the problem is equivalent to a ridge regression formulation with binary targets @xmath11 .",
    "also , the kernel ls - svm allows us to treat the multi - class classification task in a much simple and compact way .",
    "more exactly , we assume that @xmath12 classes are encoded using the standard basis in the @xmath13 space . therefore ,",
    "if @xmath14 is an input in class @xmath15 , then the output @xmath16 is encoded by a binary row vector with 1 in the @xmath17th position and 0 in all other positions : @xmath18    thus , for the input data @xmath19 and the feature mapping function @xmath3 we consider the following optimization problem:@xcite @xmath20 such that : @xmath21 where @xmath22 is the bias coefficient , @xmath23 is the vector of weights corresponding to the class @xmath24 , and @xmath25 is the approximation error .",
    "the objective function @xmath26 is a sum of the least - squares error and a regularization term , and therefore it corresponds to a multi - dimensional ridge regression problem with the regularization parameter @xmath27 .    in the primal weight space",
    "the multi - class classifier takes the form : @xmath28 where @xmath29 is the nonlinear softmax function :    @xmath30    because @xmath3 may be infinite dimensional we seek a solution in the dual space @xmath3 by introducing the lagrangian : @xmath31,\\ ] ] with the lagrange multipliers @xmath32 .",
    "the optimality conditions for the minimization problem are : @xmath33 by eliminating @xmath23 and @xmath25 we obtain : @xmath34 a_{nj } + b_j = y_{ij } , \\quad i=1,\\dots , n,\\ ; j=1,\\dots , k,\\ ] ] where we applied the condition @xmath35 , and @xmath36 is kronecker s delta : @xmath37 if @xmath38 , and @xmath39 otherwise .    therefore , in the dual space the multi - class classifier takes the form : @xmath40    one can see that the above system of equations ( 13 ) is equivalent to @xmath12 independent systems of equations with binary targets @xmath41 .",
    "each system can be written in the following equivalent form : @xmath42 where @xmath43 is the @xmath44 identity matrix , @xmath45^t$ ] is an @xmath10 dimensional vector with all the components equal to 1 , @xmath46^t$ ] and @xmath47^t$ ] are the weight and respectively the target column vectors for the class @xmath48 .",
    "each system has @xmath49 linear equations with @xmath49 unknowns , and requires the inversion of the same matrix : @xmath50 the above @xmath12 systems can be written in a compact matrix form as following : @xmath51 where @xmath52 and @xmath53 are @xmath54 matrices with the columns : @xmath55 despite these nice mathematical properties , the complexity of the problem is @xmath56 , and it is implied by the matrix inversion .",
    "our goal is to reduce the complexity of the kernel ls - svm classification method by using iterative approximations based on randomized block kernel matrices .",
    "more exactly we use the well known nystrm method,@xcite and we introduce new methods based on the kaczmarz@xcite and the matching pursuit@xcite algorithms .      the nystrm method is probably the most popular low rank approximation of the matrix @xmath57.@xcite since the kernel matrix is a positive definite symmetric matrix we can find its eigenvalue decomposition : @xmath58 where @xmath59 is the diagonal matrix of the eigenvalues , and @xmath60 is the corresponding matrix of the eigenvectors . using the sherman - morrison formula@xcite one can show that the solution of the linear system of equations can be written as : @xmath61\\ ] ] to reduce the computation one can consider only a small selection @xmath62 , @xmath63 , based on a random sample of the training data .",
    "the eigenvalue decomposition of @xmath64 is therefore : @xmath65 one can show that the following relations exist between the eigenvalues and eigenvectors of the two matrices @xmath57 and respectively @xmath66 : @xmath67 such that : @xmath68 where @xmath69 and @xmath70 are the @xmath71 and respectively the @xmath72 block submatrices taken from @xmath57 , corresponding to the randomly selected @xmath73 columns . therefore , in this case only a matrix of size @xmath74 needs to be inverted , which simplifies the computation .",
    "while this method seems to scale as @xmath75 it has the difficulty of optimally choosing the @xmath73 support vectors from the training data set .",
    "it has been shown that an optimal method for choosing the @xmath73 support vectors is based on the maximization of the renyi entropy of the the matrix @xmath66.@xcite therefore , one can use a heuristic greedy algorithm to search for the best set of @xmath73 support vectors , that maximize the renyi entropy of @xmath66 , however this approach becomes difficult in the case of large data sets .",
    "here we consider a much simpler approach , based on a committee machine made of @xmath76 classifiers .",
    "let us assume that the classifiers are characterized by the following randomly selected matrices : @xmath77 , @xmath78 , @xmath79 , @xmath80 .",
    "the random sampling can be done without replacement , such that after @xmath81 selections all the training data is used .",
    "an efficient randomization strategy would be to use a random shuffling function which generates a random permutation @xmath82 of the set @xmath83 , and then to select contiguous subsets with size @xmath73 from @xmath84 .",
    "after the algorithm consumes all the @xmath81 subsets one can re - shuffle @xmath84 in order to create another randomized list .    for each classifier we solve the system : @xmath85 the solution is given by : @xmath86 where @xmath87 and @xmath88 are the moore - penrose pseudo - inverse matrices of the random block matrices @xmath89 and @xmath90 , which can be calculated at a lower cost of @xmath91 . in the end we aggregate the weights of the classifiers as following : @xmath92 and the classification process is performed using the equations ( 14 ) and ( 15 ) .",
    "the kaczmarz method@xcite is a popular solver for overdetermined linear systems of equations of the form : @xmath93 with @xmath94 .",
    "the method has numerous applications in computer tomography and image reconstruction from projections .",
    "assuming that @xmath95 is an initial estimation of the solution , the algorithm proceeds cyclically and at each step it projects the estimate @xmath96 onto a subspace normal to the row @xmath97 of @xmath98 , such that : @xmath99    because of its inherent cyclical definition , the convergence of the method depends on the order of the rows . a simple method to overcome",
    "this problem is to select the rows of @xmath98 in a random order :    @xmath100    where @xmath101 is a random function .",
    "it has been shown that if the rows are selected with a probability : @xmath102 where @xmath103 if the frobenius norm , then the algorithm converges in expectation exponentially , with a rate independent on the number of equations : @xmath104 where @xmath105 is the condition number of @xmath98 , with @xmath106 and @xmath107 the maximal and minimal singular values of @xmath98 respectively.@xcite this remarkable results shows that the estimate @xmath96 converges exponentially fast to the solution , in just @xmath108 iterations . also ,",
    "since each iteration requires @xmath108 time steps , the overall complexity of the method is @xmath109 , which is much smaller than @xmath110 required by the standard approach .",
    "another big advantage is the low memory requirement , since the method only needs the randomly chosen row at a given time , and it does nt operate with the whole matrix @xmath98 . this makes the method appealing for very large systems of equations .",
    "it is interesting to note that the matrix @xmath98 can be `` standardized '' , such that each of its rows has unit euclidean norm : @xmath111 where @xmath112 therefore , by using this simple standardization method , the random row selection can be done with uniform probability , since all the rows have an identical norm @xmath113 , @xmath114 . in this case",
    "the iterative equation takes the form : @xmath115 \\hat{a}_{r(i)},\\ ] ] and @xmath101 is a uniformly distributed random function .    in the case of our multi - class classification problem , we prefer a randomized block version of the kaczmarz method , in order to use more efficiently the computing resources .",
    "also we assume that the system of equations is standardized using the above described procedure . in this case at each iteration we randomly select a subset @xmath116 of the row indexes of @xmath98 , with a size @xmath117 , and we project the current estimate @xmath96 onto a subspace normal to these rows : @xmath118,\\ ] ] where :    @xmath119 ^{-1}\\ ] ]    is the moore - penrose pseudo - inverse of the matrix @xmath120 , and @xmath121 is the subvector of @xmath122 with the components from @xmath116 .",
    "the above equation is equivalent to : @xmath123,\\ ] ] where @xmath2 is the exact solution of the system .",
    "therefore we have : @xmath124.\\ ] ] this can be rewritten as : @xmath125.\\ ] ] where @xmath126 . since @xmath127 is an orthogonal projection we have : @xmath128 \\vert \\leq   \\vert",
    "x(t ) - x \\vert^2,\\ ] ] and the algorithm is guaranteed to converge .",
    "in fact , for @xmath129 , the randomized block algorithm is equivalent to the simple ( one row at a time ) randomized algorithm , which guarantees exponential convergence.@xcite    in our multi - class classification setting we solve simultaneously @xmath12 systems of equations , corresponding to the @xmath12 different classes : @xmath130 therefore , the iterative equations are : @xmath131 , \\quad j=1,\\dots , k,\\ ] ] where @xmath132 and @xmath133 are the standardized versions of the randomly selected block matrix @xmath134 and the corresponding target subvector @xmath135 .",
    "these equations can be written compactly in a matrix form as following : @xmath136.\\ ] ] the iteration stops when no significant improvement for @xmath52 is made , or the number of iterations reach a maximum accepted value .",
    "once the matrix @xmath52 containing the weights @xmath137 and the bias @xmath138 is calculated , one can use the multi - class classifier defined by the equations ( 14)-(15 ) , in order to classify any new @xmath139 data sample .",
    "the matching pursuit ( mp ) method is frequently used to decompose a given signal into a linear expansion of functions selected from a redundant dictionary of functions.@xcite thus , given a signal @xmath140 , we seek a linear expansion approximation : @xmath141 where @xmath142 are the columns of the redundant dictionary @xmath143 , with @xmath144 . here , @xmath145 is a selection function which returns the index of the column from the dictionary .",
    "thus , we solve the minimization problem : @xmath146 starting with an initial approximation @xmath147 , at each new step the algorithm iteratively selects a new column @xmath148 from the dictionary , in order to reduce the future residual @xmath149 .",
    "therefore , from @xmath150 one can build a new approximation : @xmath151 by finding @xmath152 and @xmath148 that minimizes the residual error : @xmath153 the minimization condition : @xmath154 gives : @xmath155 and therefore we have : @xmath156 ^ 2 \\leq \\vert r_{m-1 } \\vert^2.\\ ] ] the index of the best column that minimizes the residual is given by : @xmath157 ^ 2.\\ ] ]    the mp algorithm can be easily extended for kernel functions.@xcite however , for large data sets this approach is not quite efficient due to the increasing time required by the search for the best column .",
    "we notice that according to ( 54 ) the algorithm converges even if the selection of the column is randomly done .",
    "therefore , here we prefer this weaker form , where the functions from the redundant dictionary are simply selected randomly .    in our case",
    ", we consider that the dictionary corresponds to the matrix @xmath57 , and as before we consider a block matrix formulation of the weak matching pursuit algorithm .",
    "initially the matrix @xmath52 is empty , @xmath158 , and the residual is set to @xmath159 . at each iteration step",
    "we select randomly ( without replacement ) @xmath73 columns from @xmath57 , which we denote as the @xmath71 matrix @xmath134 , where @xmath116 is the random list of columns .",
    "thus , at each iteration step we solve the following system : @xmath160 from here we find the solution at time @xmath161 :    @xmath162    the weights are updated as : @xmath163 here , @xmath164 is the block of @xmath52 containing only the rows from the random list @xmath116 .",
    "also , the new residual is updated as following : @xmath165 due to the orthogonality of these quantities we have : @xmath166 and the algorithm is guaranteed to converge .",
    "once the matrix @xmath52 is calculated , one can use the multi - class classifier defined by the equations ( 14)-(15 ) , in order to classify any new @xmath139 data sample .",
    "interestingly , one can easily combine the kaczmarz and the mp methods into a hybrid kaczmarz - mp kernel method . in both methods we have to select a random list @xmath116 of @xmath73 indexes ,",
    "therefore we can use the same list to perform a kaczmarz step followed by an mp step ( or vice versa ) .",
    "the cost of this approach is similar to the randomized nystrm method , since it requires two matrix pseudo - inverses per iteration step .",
    "in this section we provide several new numerical results obtained for the mnist@xcite and the cifar-10@xcite data sets , that illustrate the practical applicability and the performance of the proposed methods .",
    "the mnist data set of hand - written digits is a a widely - used benchmark for testing multi - class classification algorithms .",
    "the data consists of @xmath167 training samples and 10,000 test samples , corresponding to @xmath168 different classes .",
    "all the images @xmath169 , @xmath114 , are of size @xmath170 pixels , with @xmath171 gray levels , and the intra - class variability consists in local translations and deformations of the hand written digits . some typical examples of images from the mnist data set are shown in figure 1 .    while the mnist data set is not very large , it still imposes a challenge for the standard kernel classification approach based on ls - svm , because the kernel matrix has @xmath172 elements , which in double precision ( 64bit floating point numbers ) requires about 28.8 gb . in order to simulate even more drastic conditions we used a pc with only 8 gb of ram , and 4 cpu cores .",
    "also , all the simulation programs were written using the julia language.@xcite obviously , with such a limited machine , the attempt to solve the problem directly ( for example using the gaussian elimination or the cholesky decomposition ) is not feasible .",
    "however , we can easily use the proposed randomized approximation methods , and in order to do so we perform all the computations in single precision ( 32 bit floating point numbers ) , which provides a double storage capability , comparing to the case of double precision .",
    "thus , we trade a potential precision loss for extra memory storage space , in order to adapt to the data size .    in our numerical experiments",
    "we have used only the raw data , without any augmentation or distortion .",
    "it is well known that better results can be obtained by using more complex unsupervised learning of image features , data augmentation and image distortion methods at the pre - processing level .",
    "however , our goal here is only to evaluate the performance of the discussed methods , and therefore we prefer to limit our investigation to the raw data .    to our knowledge ,",
    "the best reported results in the literature for the kernel svm classification of the mnist raw data have a classification error of @xmath173,@xcite and respectively @xmath174,@xcite and they have been obtained by combining ten kernel svm classifiers .",
    "we will use these values for comparison with our results .",
    "in our approach we used a simple pre - processing method , consisting in a two step normalization of each image , as following : @xmath175 where @xmath176 denotes the average .",
    "also , the images are `` vectorized '' by concatenating the columns , such that : @xmath177 . therefore , after pre - processing all the images are unit norm vectors : @xmath178 , @xmath114 .",
    "this normalization is useful because it makes all the inner products equal to the cosine of the angle between the vectors , which is a good similarity measure : @xmath179 , \\quad \\forall i , j=1,\\dots , n.\\ ] ]    we have experimented with several kernel types ( gaussian , polynomial ) , and the best results have been obtained with a polynomial kernel of degree 4 : @xmath180 therefore all the results reported here are for this particular kernel function .",
    "also , the regularization parameter was always set to @xmath181 , and the classification error @xmath182 was simply measured as the percentage of the test images which have been incorrectly classified .    the full kernel matrix : @xmath183 would still require 14.4 gb , which is not feasible with the imposed constraints , and",
    "therefore we must calculate the kernel elements on the fly at each step . here",
    ", the exponent @xmath184 means that the power is calculated element - wise .",
    "also we assume that the vectorized images correspond to the rows of the matrix @xmath185 .",
    "the memory left is still enough to hold the matrix : @xmath186 which is required for the classification of the testing data .",
    "this matrix is not really necessary to store in memory since the classification can be done separately for each test image , once the weights and the biases have been computed .",
    "this matrix requires about 2.4 gb and it is convenient to store it just to be able to perform a fast measurement of the classification error after each iteration .     for the iterative randomized kernel methods ( mnist data set ) : ( a ) nystrm method ; ( b ) kaczmarz method ; ( c ) mp method ;",
    "( d ) kaczmarz - mp method .",
    "here @xmath161 is the iteration time step and @xmath73 is the size of the random block submatrices . , width=566 ]     for the randomized mp method with the fourier pre - processing ( mnist data set ) . here",
    "@xmath161 is the iteration time step and @xmath73 is the size of the random block submatrices.,width=377 ]    in figure 2(a ) we give the results obtained for the randomized nystrm method .",
    "here we have represented the classification error as a function of the random block size @xmath73 and the number of aggregated classifiers @xmath161 .",
    "one can see that the method converges very fast , and only few classifiers are needed to reach a plateau for @xmath182 .",
    "unfortunately , the error @xmath182 is dependent on @xmath73 , and this result suggests that the method performs better for larger values of @xmath73 .",
    "this is the fastest method considered here , since it requires the aggregation of only a few classifiers , such that for @xmath187 the total number of necessary classifiers is only @xmath188 .",
    "the randomized kaczmarz method shows a different behavior , figure 2(b ) . in this case",
    "we ran the algorithm for @xmath189 iterations using random data blocks of size @xmath190 .",
    "this method shows a much slower convergence , and the whole computation process needed almost 5 hours to complete ( @xmath191 seconds per iteration time step ) , including the time for the error evaluation at each step .",
    "after the first iteration step the classification error drops abruptly below @xmath192 , then the classification error decreases slowly , and fluctuates closer to @xmath193 , which is in very good agreement with the previously reported values .",
    "better results for the kaczmarz method can be obtained by simply averaging the weights and biases of several separately ( parallel ) trained classifiers , with different random seeds for the random number generator , and/or different block sizes @xmath73 .",
    "this form of aggregation is possible because the kernel matrix is the same for all classifiers , and the systems are linear in the dual space . assuming that we have @xmath26 classifiers , @xmath194 , each of them having the output : @xmath195 then the output of the averaged classifiers is : @xmath196 therefore , in the end one can store only the average values of the weights and the biases .",
    "the advantage of averaging is the decrease of the amplitude of the fluctuations in the classification error , which gives better and more confident results , and also increases the generalization capabilities .    in figure 2(c ) we give the results for the randomized mp method .",
    "this method shows similar results to the randomized kaczmarz method , but it s convergence is faster and the fluctuations have a lower amplitude . also , the method is about twice faster , with @xmath197 seconds per iteration time step , for a random block of size of @xmath190 .",
    "again , the obtained result @xmath198 is in very good agreement with the previously reported results .",
    "the results for the hybrid kaczmarz - mp method are given in figure 2(d ) , and not surprisingly it shows inbetween convergence speed , and a classification error of @xmath199 , which confirms the previous results .    in our last experiment",
    "we have decided to modify the data pre - processing , in order to see if better results can be obtained .",
    "we only did a simple modification by concatenating the images with the square root of the absolute value of their fast fourier transform ( @xmath200 ) .",
    "since the fft is symmetrical , only the first half of the values were used , each image becoming a vector of 1176 elements .",
    "the new image pre - processing consists in the following steps : @xmath201^t , \\quad i=1,\\dots , n.\\end{aligned}\\ ] ] with this new pre - processing we used the randomized mp method and the results are shown in figure 3 .",
    "the classification error drops to @xmath202 , which means an improvement of @xmath203 over the previously reported results .",
    "the 85 images incorrectly recognized are shown in figure 4 .",
    "the cifar-10 dataset consists of 60,000 images .",
    "each image has @xmath204 pixels and 3 colour ( rgb ) channels with 256 levels .",
    "the data set contains 10 classes of equal size ( 6,000 ) , corresponding to objects ( airplane , automobile , ship , truck ) and animals ( bird , cat , deer , dog , frog , horse ) .",
    "the training set contains 50,000 images , while the test set contains the rest of 10,000 images .        in figure 5",
    "we show the first 10 images from each class .",
    "obviously this data set is much more challenging than the mnist data set , showing a very high variation in each class .",
    "again , we only use the raw data without any augmentation and distortion , and without employing any other technique for advanced feature extraction and learning . in figure 6",
    "we give the results of the numerical experiments for the randomized mp method , with the random data block size @xmath205 .    in the first experiment ( @xmath206 ) we used the simple two - step normalization described in ( 61)-(62 ) .",
    "thus , after the normalization steps each image becomes a unit vector with @xmath207 pixels , obtained by concatenating the columns from the three colour channels .",
    "we ran the randomized mp algorithm for @xmath208 steps , such that the classification error settled at @xmath209 .",
    "this is a good improvement over the random guessing `` method '' , which has an error @xmath210 .    in the second experiment ( @xmath211 ) , before the two - step normalization we have applied a gaussian filter on each channel .",
    "the filter is centred in the middle of the image and it is given by : @xmath212 \\right ) , \\",
    "; i , j=1,\\dots , l,\\ ] ] where @xmath213 is the size of the image .",
    "the filter is applied element - wise to the pixels and it is used to enhance the center of the image , where supposedly the important information resides , and to attenuate the information at the periphery of the image .",
    "the best results have been obtained for a constant @xmath214 , and the classification error dropped to @xmath215 .    in the third experiment ( @xmath216 ) we have used the gaussian filtering ( 76 ) and the fft normalization described by the equations ( 69)-(75 ) .",
    "this pre - processing method improved the results again and we have obtained an error of only @xmath217 . the resulted confusion matrix is given in table 1 .",
    "not surprisingly , one can see that main `` culprits '' are the cats and dogs .",
    "for @xmath218 the cats are mistaken for dogs , while for @xmath219 the dogs are mistaken for cats .     for the randomized mp method ( cifar-10 data set ) . here",
    "@xmath161 is the iteration time step and @xmath73 is the size of the random block submatrices .",
    "the following pre - processing steps have been used : @xmath206 - two - step data normalization ( 61)-(62 ) ; @xmath211 - gaussian filtering ( 76 ) and two - step data normalization ( 61)-(62 ) ; @xmath216 - gaussian filtering ( 76 ) and fft normalization ( 69)-(75 ) .",
    ", width=340 ]",
    "in this paper we have discussed several approximation methods for the kernel ls - svm multi - class classification problem .",
    "these methods use randomized block kernel matrices , and their main advantages are the low memory requirements and the relatively simple iterative implementation , significantly reducing the complexity of the problem .",
    "another advantage is that these methods do not require complex parameter tuning mechanisms .",
    "the only parameters needed are the regularization parameter and the size of the random block matrices . despite of their simplicity",
    ", these methods provide very good accuracy and reliable scaling to relatively large multi - class classification problems . also , we have reported new numerical results for the mnist and cifar-10 data sets , and we have shown that better results can be obtained by using a simple fourier pre - processing step of the raw data .",
    "the results reported here for the mnist raw data set are in very good agreement with the previously reported results using the kernel svm approach , while the results for the cifar-10 data are surprisingly good for the small amount of pre - processing used .",
    "t. hofmann , b. schlkopf , a. j. smola , _ kernel methods in machine learning , _ the annals of statistics 36(3 ) , 1171 ( 2008 ) .",
    "j. mercer , _ functions of positive and negative type and their connection with the theory of integral equations , _ philos .",
    "ser . a math .",
    "a 209 , 415 ( 1909 ) . c. cortes , v. vapnik , _ support vector networks _ , machine learning 20 , 273 ( 1995 ) .",
    "suykens , j. vandewalle , _ least squares support vector machine classifiers _ , neural processing letters 9(3 ) , 293 ( 1999 ) .",
    "e. j. nystrm , _ ber die praktische auflsung von integralgleichungen mit anwendungen auf randwertaufgaben \" , _ acta mathematica 54(1 ) , 185 ( 1930 ) .",
    "s. kaczmarz , _ angenherte auflsung von systemen linearer gleichungen , _ bulletin international de lacadmie polonaise des sciences et des lettres .",
    "classe des sciences mathmatiques et naturelles .",
    "srie a , sciences mathmatiques , 35 , 355 ( 1937 ) . s. g. mallat , z. zhang , _ matching pursuits with time - frequency dictionaries , _ ieee transactions on signal processing 41(12 ) , 3397 ( 1993 ) .",
    "c.k.i williams , m. seeger , _ using the nystrm method to speed up kernel machines , _",
    "proceedings neural information processing systems 13 , 682 ( 2001 ) .",
    "j. sherman , w.j .",
    "morrison , _ adjustment of an inverse matrix corresponding to a change in one element of a given matrix , _ annals of mathematical statistics 21 , 124 ( 1950 ) .",
    "m. girolami , _",
    "orthogonal series density estimation and the kernel eigenvalue problem , _ neural computation 14 , 669 ( 2002 ) .",
    "t. strohmer , r. vershynin , _ a randomized kaczmarz algorithm for linear systems with exponential convergence _ ,",
    "journal of fourier analysis and applications 15 , 262 ( 2009 ) .",
    "p. vincent , y. bengio , _ kernel matching pursuit , _ machine learning 48 , 169 ( 2002 ) .",
    "y. lecun , l. bottou , y. bengio , p. haffner , _ gradient - based learning applied to document recognition , _ proceedings of the ieee 86(11 ) , 2278 ( 1998 ) .",
    "a. krizhevsky , g. hinton .",
    "_ learning multiple layers of features from tiny images , _ computer science department , university of toronto , tech . rep . , 2009 .",
    "j. bezanson , s. karpinski , v.b .",
    "shah , a. edelman , _ julia : a fast dynamic language for technical computing _ , arxiv:1209.5145 ( 2012 ) .",
    "burges , b. schlkopf , _ improving the accuracy and speed of support vector machines _ , advances in neural information processing systems 9 , 375 ( 1997 ) ."
  ],
  "abstract_text": [
    "<S> the least - squares support vector machine is a frequently used kernel method for non - linear regression and classification tasks . </S>",
    "<S> here we discuss several approximation algorithms for the least - squares support vector machine classifier . </S>",
    "<S> the proposed methods are based on randomized block kernel matrices , and we show that they provide good accuracy and reliable scaling for multi - class classification problems with relatively large data sets . also , we present several numerical experiments that illustrate the practical applicability of the proposed methods .    keywords : kernel methods ; multiclass classification .    </S>",
    "<S> pacs : 07.05.mh , 02.10.yn ; 02.30.mv </S>"
  ]
}