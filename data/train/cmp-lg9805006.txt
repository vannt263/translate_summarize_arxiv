{
  "article_text": [
    "the idea of a computer system for translating from one language to another is almost as old as the idea of computer systems .",
    "the earliest written record of this idea is a 1949 memorandum by warren weaver .",
    "more recently , @xcite have proposed methods for constructing machine translation systems automatically . instead of codifying the human translation process from introspection",
    ", @xcite appealed to machine learning techniques to induce models of the process from examples of its input and output .",
    "the proposal generated much excitement , because it held the promise of automating a task that fifty years of research have proven extremely labor - intensive and error - prone .",
    "yet , very few other researchers have taken up the cause , partly because @xcite s approach was quite a departure from the paradigm in vogue at the time .",
    "formally , @xcite built statistical models of translational equivalence ( or * translation models * , for short ) .",
    "* translational equivalence * is a relation that holds between two expressions with the same meaning , where the two expressions are in different languages . as with all statistical models , the best translation models are those whose parameters correspond best with the sources of variance in the data .",
    "translation models whose parameters reflect existing knowledge about particular languages and language pairs and/or universal properties of translational equivalence benefit from the best of both the empiricist and rationalist traditions .",
    "this article presents three such models , along with methods for efficiently estimating their parameters .",
    "more specifically , in this article , i introduce methods for modeling three universal properties of translational equivalence in parallel texts ( * bitexts * ) :    1 .",
    "most word tokens translate to only one word token .",
    "i capture this tendency in a one - to - one assumption .",
    "most text segments are not translated word - for - word .",
    "i build an explicit noise model .",
    "3 .   different linguistic objects have statistically different behavior in translation .",
    "i show a way to condition translation models on different word classes to help account for the variety .",
    "quantitative evaluation with respect to a gold standard has shown that each of the three methods effects a significant improvement in translation model accuracy .",
    "a review of previously published translation models follows an introduction to the different kinds of possible translation models .",
    "the core of the article is a presentation of the model estimation biases described above and an analysis of their expected behavior in the face of sparse data .",
    "the last section reports the results of a variety of experiments designed to evaluate these innovations .    throughout this article ,",
    "i shall use @xmath0 letters to denote entire text corpora and other sets of sets , capital letters to denote collections , including strings and bags , and _ italics _ for scalar variables .",
    "i shall also distinguish between * types * and tokens by using * bold font * for the former and plain font for the latter .",
    "there are two kinds of applications of translation models : those where word order plays a crucial role and those where it does nt .",
    "empirically estimated models of translational equivalence among word types can play a central role in both kinds of applications .",
    "applications where word order is not important ( or at least not essential ) include    * cross - language information retrieval (   * ? ? ?",
    "* ) , * computer - assisted language learning @xcite , * certain machine - assisted translation tools (   * ? ? ? * ; * ? ? ?",
    "* ) , * concordancing for bilingual lexicography @xcite , * corpus linguistics (   * ? ? ?",
    "* ) , * `` crummy '' machine translation on the internet @xcite .    for these applications ,",
    "empirical models have a number of advantages over hand - crafted models such as on - line versions of printed bilingual dictionaries .",
    "two of the advantages are the possibility of better coverage and the possibility of frequent updates by non - expert users to keep up with rapidly evolving vocabularies .",
    "a third advantage is that empirical models can provide more accurate information about the relative importance of different translations .",
    "such information is crucial for applications such as cross - language information retrieval ( clir ) . in clir ,",
    "the query vector  @xmath1 is in a different language ( a different vector space ) from the document vectors  @xmath2 .",
    "matrix multiplication by a word - to - word translation model  @xmath3 can map  @xmath1 into a vector  @xmath4 in the vector space of  @xmath2 : @xmath5 . in order for the mapping to be accurate",
    ", @xmath3 must be able to encode many levels of relative importance among the possible translations of each element of  @xmath1 .",
    "a typical machine - readable bilingual dictionary says only what the possible translations are , which is equivalent to positing a uniform translational distribution .",
    "the performance of cross - language information retrieval with a uniform  @xmath3 is likely to be limited in the same way as the performance of conventional information retrieval without term frequency information , _",
    "i.e. _  where the system knows which terms occur in which documents , but not how often @xcite .",
    "fully automatic high - quality machine translation is the prototypical application where word order is crucial .",
    "in such an application , a word - to - word translation model can serve as an independent module in a more complex string - to - string translation model .",
    "the independence of such a module is desirable for two reasons , one practical and one philosophical .",
    "the practical reason is illustrated in this article : order - independent translation models can be accurately estimated more efficiently in isolation .",
    "the philosophical reason is that words are an important epistemological category in our naive mental representations of language .",
    "we have many intuitions ( and even some testable theories ) about what words are and how they behave .",
    "we can bring these intuitions to bear on our translation models without being distracted by other facets of language , such as phrase structure .",
    "for example , chapter  9 of my dissertation is based on the intuition that words can have multiple senses @xcite ; @xcite s model  3 and my work on non - compositional compounds @xcite are based on the intuition that spaces in text do not necessarily delimit words .",
    "the independence of a word - to - word translation module in a string - to - string translation model can be effected by a two - stage decomposition .",
    "the first stage is based on the observation that every string * s * is just an ordered bag , and that the bag * b * can be modeled independently of its order * o*. for example , the string @xmath6 consists of the bag @xmath7 and the ordering relation @xmath8 .",
    "if we represent each string * s * as a pair * ( b ,  o ) * , then @xmath9 now , let @xmath10 and @xmath11 be two strings and let * a * be a one - to - one mapping between the elements of @xmath10 and the elements of @xmath11 . borrowing a term from the operations research literature , i shall refer to such mappings as * assignments*. let @xmath12 be the the set of all possible assignments between @xmath10 and @xmath13 . using assignments , we can decompose conditional and joint probabilities over strings : @xmath14 where @xmath15    the second stage of decomposition takes us from bags of words to the words that they contain .",
    "the following bag - pair generation process illustrates how a word - to - word translation model can be embedded in a bag - to - bag translation model for languages @xmath16 and @xmath17 :    1 .   generate a bag size @xmath18 with probability @xmath19 ( mnemonic :",
    "@xmath20 is the @xmath21 distribution ) .",
    "@xmath18 is also the assignment size .",
    "2 .   generate @xmath18 language - independent concepts @xmath22 .",
    "3 .   from each concept",
    "@xmath23 , @xmath24 , generate a pair of word strings @xmath25 from , according to the distribution @xmath26 , to lexicalize the concept in the two languages .",
    "some concepts are not lexicalized in some languages , so one of @xmath27 and @xmath28 may be empty .",
    "a pair of bags containing @xmath29 and @xmath30 non - empty word strings can be generated by a process where @xmath18 is anywhere between 1 and @xmath31 .    without loss of generality",
    ", we can assume that each different pair of word string types @xmath32 is deterministically generated from a different concept .",
    "thus , a bag - to - bag translation model can be fully specified by the distributions @xmath20 and @xmath33 .",
    "for notational convenience , the elements of the two bags can be labeled so that @xmath34 and @xmath35 , where some of the @xmath36 s and @xmath37 s may be empty .",
    "the elements of an assignment , then , are pairs of bag element labels : @xmath38 , where each @xmath39 ranges over @xmath40 , each @xmath41 ranges over @xmath42 , each * @xmath39 * is distinct and each * @xmath41 * is distinct .",
    "the label pairs in a given assignment can be generated in any order , so there are @xmath43 ways to generate an assignment of size @xmath18 .",
    "it follows that the probability of generating a pair of bags @xmath44 with a particular assignment * a * of size @xmath18 is @xmath45 the joint probability distribution @xmath46 is a * word - to - word translation model*.",
    "the most general word - to - word translation model @xmath47 , where @xmath48 and @xmath49 range over the strings of @xmath16 and @xmath17 , has an infinite number of parameters .",
    "this model can be constrained in various ways to make it more practical .",
    "the models presented in this article are based on the * one - to - one  assumption * : each word is translated to at most one other word . in these models ,",
    "@xmath48 and @xmath49 may consist of at most one word each .",
    "as before , one of the two strings ( but not both ) may be empty .",
    "i shall describe empty strings as consisting of a special * null * word , so that each word string will contain exactly one word and can be treated as a scalar . henceforth , i shall write @xmath50 and @xmath51 instead of @xmath48 and @xmath52 . under the one - to - one assumption ,",
    "a pair of bags containing @xmath29 and @xmath30 non - empty words can be generated by a process where the bag size @xmath18 is anywhere between @xmath53 and @xmath31 .",
    "the one - to - one assumption is not as restrictive as it may appear : the explanatory power of a model based on this assumption may be raised to an arbitrary level by redefining what words are .",
    "for example , i have shown elsewhere how to efficiently estimate word - to - word translation models where a word can be a non - compositional compound consisting of several space - delimited tokens @xcite .",
    "for the purposes of this article , however , * words * are the tokens generated by my tokenizers and stemmers for the languages in question .",
    "therefore , the models in this article are only a first approximation to the vast complexities of translational equivalence .",
    "they are intended mainly as stepping stones towards better models .",
    "most methods for estimating translation models from bitexts start with the following intuition : words that are translations of each other are more likely to appear in corresponding bitext regions than other pairs of words . following this intuition ,",
    "most authors begin by counting the number of times that word types in one half of the bitext co - occur with word types in the other half .",
    "different co - occurrence counting methods stem from different models of co - occurrence .",
    "a * model of co - occurrence * is a boolean predicate , which indicates whether a given pair of word _ tokens _ co - occur in corresponding regions of the bitext space .",
    "different models of co - occurrence are possible , depending on the kind of bitext map that is available , the language - specific information that is available , and the assumptions made about the nature of translational equivalence .",
    "all the translation models reviewed and introduced in this article can be based on any of the co - occurrence models described by @xcite . for expository purposes ,",
    "however , i shall assume a boundary - based model of co - occurrence throughout this article . a boundary - based model of co",
    "- occurrence assumes that both halves of the bitext have been segmented into @xmath54 segments , so that segment @xmath55 in one half of the bitext and segment @xmath56 in the other half are mutual translations , @xmath57 . under this model of co - occurrence , the co - occurrence count @xmath58 for word types * u * and * v * is the number of times that @xmath59 and @xmath60 in some aligned segment pair @xmath39 .",
    "many researchers have proposed greedy algorithms for estimating non - probabilistic word - to - word translation models , also known as translation lexicons @xcite .",
    "most of these algorithms can be summarized as follows :    1 .",
    "choose a similarity function  @xmath61 between word types in @xmath16 and word types in @xmath17 .",
    "2 .   compute association scores @xmath62 for a set of word type pairs @xmath63 that occur in training data .",
    "3 .   sort the word pairs in descending order of their association scores .",
    "discard all word pairs for which @xmath62 is less than a chosen threshold @xmath64 .",
    "the remaining word pairs become the entries in the translation lexicon .",
    "the various proposals differ mainly in their choice of similarity function .",
    "almost all the similarity functions in the literature are based on a model of co - occurrence with some linguistically - motivated filtering ( see * ? ? ? * for a notable exception ) .    given a reasonable similarity function , the greedy algorithm works remarkably well , considering how simple it is .",
    "however , the association scores in step  2 are typically computed independently of each other .",
    "the problem with this independence assumption is illustrated in figure  [ dep ] .",
    "the two strings represent corresponding regions of a bitext .",
    "if @xmath65 and @xmath51 co - occur much more often than expected by chance , then any reasonable similarity metric will deem them likely to be mutual translations .",
    "if @xmath66 and @xmath51 are indeed mutual translations , then their tendency to co - occur is called a * direct association*. now , suppose that @xmath66 and @xmath67 often co - occur within their language .",
    "then @xmath51 and @xmath67 will also co - occur more often than expected by chance .",
    "the arrow between @xmath68 and @xmath69 in figure  [ dep ] represents an * indirect association * , since the association between @xmath68 and @xmath69 arises only by virtue of the association between each of them and @xmath70 .",
    "models of translational equivalence that are ignorant of indirect associations have `` a tendency  to be confused by collocates '' @xcite .    paradoxically , the irregularities ( noise ) in text and in translation mitigate the problem .",
    "if noise in the data reduces the strength of a direct association , then the same noise will reduce the strengths of any indirect associations that are based on this direct association .",
    "on the other hand , noise can reduce the strength of an indirect association without affecting any direct associations .",
    "therefore , direct associations are usually stronger than indirect associations .",
    "if all the entries in a translation lexicon are sorted by their association scores , the direct associations will be very dense near the top of the list , and sparser towards the bottom .",
    "@xcite have shown that entries at the very top of the list can be over 98% correct .",
    "their algorithm gleaned lexicon entries for about 61% of the word tokens in a sample of 800 english sentences . to obtain 98% precision ,",
    "their algorithm selected only entries for which it had high confidence that the association score was high .",
    "these would be the word pairs that co - occur most frequently .",
    "a random sample of 800 sentences from the same corpus showed that 61% of the word tokens , where the tokens are of the most frequent types , represent 4.5% of all the word types .",
    "a similar strategy was employed by @xcite and by @xcite .",
    "fung skimmed off the top 23.8% of the noun - noun entries in her lexicon to achieve a precision of 71.6% .",
    "wu  &  xia have reported automatic acquisition of 6517 lexicon entries from a 3.3-million - word corpus , with a precision of 86% .",
    "the first 3.3 million word tokens in an english corpus from a similar genre contained 33490 different word types , suggesting a recall of roughly 19% .",
    "note , however , that wu  &  xia chose to weight their precision estimates by the probabilities attached to each entry :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ for example , if the translation set for english word _",
    "detect _ has the two correct chinese candidates with 0.533 probability and with 0.277 probability , and the incorrect translation with 0.190 probability , then we count this as 0.810 correct translations and 0.190 incorrect translations .",
    "@xcite _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    this is a reasonable evaluation method , but it is not comparable to methods that simply count each lexicon entry as either right or wrong (   * ? ? ?",
    "a weighted precision estimate pays more attention to entries that are more frequent and hence easier to estimate .",
    "therefore , weighted precision estimates are generally higher than unweighted ones .",
    "most translation model re - estimation algorithms published to date are variations on the theme proposed by @xcite .",
    "these models involve conditional probabilities , but they can be compared to symmetric models based on joint probabilities if the latter are normalized by the appropriate marginal distribution .",
    "i shall review these models using the notation in table  [ tmnotation ] .",
    "[ cols=\"<,^ , < \" , ]     table  [ cats ] reports the distribution of correct lexicon entries among the types v , p and  i. figure  [ prec - bytype ] graphs the precision of the method against recall , with 95% confidence intervals .",
    "the upper curve represents precision when incomplete links are considered correct , and the lower when they are considered incorrect . on the former metric ,",
    "the method can generate translation lexicons with precision and recall both exceeding 90% , as well as dictionary - sized translation lexicons that are over 99% correct .",
    "there are many ways to model translational equivalence and many ways to estimate translation models .",
    "`` the mathematics of statistical machine translation '' proposed by @xcite are just one kind of mathematics for one kind of statistical translation . in this article ,",
    "i have proposed and evaluated new kinds of translation model biases , alternative parameter estimation strategies , and general techniques for exploiting pre - existing knowledge that may be available about particular languages and language pairs . on a variety of evaluation metrics , each infusion of knowledge",
    "about the problem domain resulted in better translation models .",
    "each innovation presented here opens the way for more research .",
    "model biases can be mixed and matched with each other , with previously published biases like the word order correlation bias , and with other biases yet to be invented .",
    "the competitive linking algorithm can be generalized in various ways .",
    "new kinds of pre - existing knowledge can be exploited to effect significant accuracy improvements for particular language pairs or even just for particular bitexts .",
    "it is difficult to say where the greatest advances will come from . yet",
    ", one thing is clear from our current vantage point : research on empirical methods for modeling translational equivalence has not run out of steam , as some have claimed , but has only just begun .",
    "many of the ideas in this paper came from enlightening correspondence with ken church , mike collins , ido dagan , jason eisner , steven finch , george foster , djoerd hiemstra , adwait ratnaparkhi and lyle ungar .",
    "this research was supported by darpa grant n6600194c-6043 .",
    "p. f. brown , j. cocke , s. a. della pietra , v. j. della pietra , f. jelinek , r. l. mercer , & p. roossin .",
    "( 1988 ) `` a statistical approach to language translation , '' _ proceedings of the 12th international conference on computational linguistics_. budapest , hungary .",
    "p. f. brown , s. a. della pietra , v. j. della pietra , m. j. goldsmith , j. hajic , r. l. mercer & s. mohanty .",
    "( 1994 ) `` but dictionaries are data too , '' _ proceedings of the arpa hlt workshop_. princeton , nj .                    w. e. demming & f. f. stephan .",
    "( 1940 ) `` on a least squares adjustment of a sampled frequency table when the expected marginal totals are known , '' _ the annals of mathematical statistics 11 _ , pp .  427 - 444",
    ".          p. fung .",
    "( 1995 ) `` a pattern matching method for finding noun and proper noun translations from noisy parallel corpora , '' _ proceedings of the 33rd annual meeting of the association for computational linguistics_. boston , ma .",
    "a. kumano & h. hirakawa .",
    "`` building an mt dictionary from parallel texts based on linguistic and statistical information , '' _ proceedings of the 15th international conference on computational linguistics_. kyoto , japan .    e. macklovitch .",
    "( 1994 ) `` using bi - textual alignment for translation validation : the transcheck system , '' _ proceedings of the 1st conference of the association for machine translation in the americas_. columbia , md .",
    "i. d. melamed .",
    "( 1998d ) `` manual annotation of translational equivalence : the blinker project , '' institute for research in cognitive science technical report # 98 - 07 .",
    "university of pennsylvania , philadelphia , pa .    j. nerbonne , l. karttunen , e. paskaleva , g. proszeky & t. roosmaa .",
    "( 1997 ) `` reading more into foreign languages , '' _ proceedings of the 5th acl conference on applied natural language processing_. washington , dc .",
    "p. resnik & d. yarowsky .",
    "( 1997 ) `` a perspective on word sense disambiguation methods and their evaluation , '' _ proceedings of the siglex workshop on tagging text with lexical semantics_. washington , dc ."
  ],
  "abstract_text": [
    "<S> parallel texts ( bitexts ) have properties that distinguish them from other kinds of parallel data . </S>",
    "<S> first , most words translate to only one other word . </S>",
    "<S> second , bitext correspondence is noisy . </S>",
    "<S> this article presents methods for biasing statistical translation models to reflect these properties . </S>",
    "<S> analysis of the expected behavior of these biases in the presence of sparse data predicts that they will result in more accurate models . </S>",
    "<S> the prediction is confirmed by evaluation with respect to a gold standard  translation models that are biased in this fashion are significantly more accurate than a baseline knowledge - poor model . </S>",
    "<S> this article also shows how a statistical translation model can take advantage of various kinds of pre - existing knowledge that might be available about particular language pairs . </S>",
    "<S> even the simplest kinds of language - specific knowledge , such as the distinction between content words and function words , is shown to reliably boost translation model performance on some tasks . </S>",
    "<S> statistical models that are informed by pre - existing knowledge about the model domain combine the best of both the rationalist and empiricist traditions . </S>"
  ]
}