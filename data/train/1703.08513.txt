{
  "article_text": [
    "the human brain is seen as one of the most complex and sophisticated dynamic systems .",
    "humans can build precise instruments and write essays about higher purpose of life because they have reached a state of specialisation and knowledge by externalising information and by interaction with each other .",
    "we not only utter short sounds to indicate an intention , but also describe complex procedural activity , share abstract declarative knowledge , and may even completely think in language  @xcite . for us , it is extremely easy as well as important to share information about matter , space , and time in complex interactions through natural language . often it is claimed that language is the cognitive capability that differentiates humans most from other beings in the animal kingdom .",
    "however , humans natural language processing perhaps is the least well understood cognitive capability .",
    "the main reason for this may be the complexity of human language and our inability to observe and study this capability in less complex related species .",
    "another reason is that the neural wiring in the human brain perhaps is not the only component , which is necessary for language to develop .",
    "it seems that socio - cultural principles are as well important , and only the inclusion of all factors may allow us to understand language processing .",
    "nevertheless , it is our brain that enables humans to acquire perception capabilities , motor skills , language , and social cognition .",
    "the capability for _ language acquisition _ thus may result from the concurrence of general mechanisms on information processing in the brain s architecture .",
    "in particular , in recent studies in neuroscience it was found that the brain indeed includes both hemispheres and all modalities in language processing , and the embodied development of representations might be key in language acquisition  @xcite . furthermore , hierarchical dependencies in connectivity were identified , including different but specific delays in information processing . in linguistic accounts and behavioural studies a number of important principles , such as compositional _ and _ holistic properties in entities , body - rationality , and social interaction ,",
    "have been found that might ease  or actually enable  the acquisition of a language competence  @xcite . in light of the mechanistic conditions of the brain",
    "as well as enabling factors of how we learn language _ and _ other higher cognitive functions , the key objective is to understand the _ characteristics _ of a brain - inspired _ appropriate _ neural architecture that _ facilitates _ language acquisition .    in this paper , we propose a novel embodied multi - modal model for language acquisition to study these important characteristics . as a significant innovation ,",
    "this model grounds spoken language into the temporal dynamic processing of somatosensory and visual perception and explores a mechanism that abstracts latent representations from these dynamics in a self - organising fashion .",
    "our contribution to knowledge is adding to the understanding of whether connectivity and plasticity attributes of the human brain allow for emergence and development of languages .",
    "results from analytical as well as empirical studies with computer simulations and an interactive humanoid robot will reveal the importance of this self - organisation as well of specific timing in processing speech and multi - modal sensory information .      in the past , researchers have suggested valuable models to explain grounding of language in embodied perception and action , based on neuroscientific data and hypotheses ( compare @xcite for an overview ) .",
    "this includes early work on symbol grounding e.g. @xcite , studies on language evolution and symbol emergence , e.g. @xcite , and research sentence comprehension and role filler assignment e.g. @xcite .",
    "however , due to the tremendous complexity , models are rare which consider the dynamics in full scale and avoid assumptions on predefined word representations ( short cutting language processing ) or on static or categorically predefined observations ( short cutting dynamics in grounding ) . from studies that approach this complexity",
    ", we can adopt important insights .",
    "models for grounding in _ dynamic vision _ are supposed to represent language in the alteration of e.g. perceived objects .",
    "objects can , for example , be altered in terms of changing morphology or motion by self - induced manipulation . due to complexity",
    ", models were often based on a certain decoupling and simplification of the visual stream to achieve a feasible level of coherence in visually perceived features .",
    "for example , @xcite developed a model that coupled lexical acquisition with object categorisation . here",
    ", the learning processes of visual categorisation and lexical acquisition were modelled in a close loop .",
    "this led to the emergence of the most important associations , but also to the development of links between words and categories and thus to linking similar fillers for a role .",
    "the visual features reflect little morphology over time since perception in the visual stream stemmed from unchanging preprocessed shapes in front of a plain background . with changing morphology , @xcite modelled grounding of language in visual object properties .",
    "this model is designed for a micro - language that stems from a small context - sensitive grammar and includes two input streams for scene and auditory information and an input - output stream for related prompts and responses . in between the input and input - output layer ,",
    "several layers of lstm blocks are employed to find statistical regularities in the data .",
    "this includes the overall meaning of a particular scene in terms of finding a latent symbol system that is inherent in the used grammar and dictionary . yet ,",
    "fed in object properties are  in principle  present as given prompts for the desired output responses . this way the emerging symbols in internal memory layers can be determined or shaped by the prompt and response data and are perhaps less latent .",
    "thus it remains unclear how we can relate the emergence of pre - defined or latent symbols to the problem of grounding natural language in dynamic sensory information to eventually understand how noisy perceived information contributes .",
    "overall , the studies show that dynamic vision can be integrated as embodied sensation if the dynamics of perception can be reasonably abstracted . for a novel model , however , it is crucial to control complexity in perception to attempt explaining the emerging internal representation .",
    "integrating multiple modalities into language acquisition is particularly difficult because the linked processes in the brain are extraordinary complex   and in fact   in large parts not yet understood .",
    "for this reason , to the best of the authors knowledge , there is no model available that describes language processing integrated into multi - modal perception with full spatial and temporal resolution for the cortex without making difficult assumptions or explicit limitations .",
    "however , frameworks were studied that included temporally dynamic perception that forms the basis for grounding . @xcite defined a controller for a simulated _ cognitive universal body _ ( icub ) robot based on _ recurrent neural networks _ ( rnns ) .",
    "the icub s neural architecture was trained to receive linguistic input ( bit - strings representing pseudo - words ) before the robot started to push an object ( ball , cube , or cylinder ) and observe the reaction in a sensorimotor way .",
    "experiments showed that the robot was not only able to distinguish between objects via correct `` linguistic '' tags , but could reproduce a linguistic tag via observing the dynamics without receiving linguistic input and a correct object description .",
    "thus , the authors claimed that the meaning of labels is not associated with a static representation of the object , but with its dynamical properties .",
    "@xcite modelled the grounding of words in both , object - directed actions and visual object sensations . in the model ,",
    "motor sequences were learned by a continuous actor - critic learning that integrated the joint positions with linguistic input ( processed in an _ echo state network _ ( esn ) ) and a visually perceived position of an object ( learned a priori in a _",
    "feed forward network _",
    "( ffn ) ) .",
    "a specific strength of the approach is that the model , embedded into a simulated icub , can adapt well to different motor constellations and can generalise to new permutations of actions and objects .",
    "however , the action , shape and colour descriptions ( in binary form ) are already present in the input of motor and vision networks .",
    "thus , this information is inherently included in the filtered representations that are fed into the model part for a linguistic description .",
    "in addition , the linguistic network was designed as a fixed - point classifier that outputs two active neurons per input : one ` word ' for an object and one for an action .",
    "accordingly , the output is assuming a word representation and omits the sequential order .",
    "in a framework for multi - modal integration , @xcite suggested integrating visual and sensorimotor features in a deep auto - encoder .",
    "the employed time delay neural network can capture features on varying timespan by time - shifts and hence can abstract from higher level features to some degree .",
    "in their study , both modalities of features stem from the perception of interactions with some toys and form reasonable complex representations in the sequence of 30 frames .",
    "although language grounding was not pursued , the shared multi - modal representation in the central layer of the network formed an abstraction of perceived scenes with a certain internal structuring and provided certain noise - robustness .",
    "nevertheless , all in all , we can obtain the insight that forming representation for language can perhaps get facilitated by the shared multi - modal representations and combinations of mechanisms of the brain that filter features on multiple levels .",
    "this paper is structured as follows : with the related work in mind from the introduction , in sec .  2",
    "we will introduce important principles and mechanisms that have been found underlying language acquisition . in sec .",
    "3 we will develop a novel computational model by integrating these principles and mechanisms into a general recurrent architecture with aims at neurocognitive plausibility with respect to representation and temporal dynamic processing .",
    "we include a complete formalisation to ease re - implementation and will introduce a novel mechanism for unsupervised learning based on gradient descent .",
    "then , in sec .  4 , we follow up with our evaluation and the analysis .",
    "we will specify the scenario for a language learning robot as well as a complete description of the utilised neurocognitively - inspired representations for verbal utterances and embodied multi - modal perception .",
    "in addition , we report experiments for generalisation and self - organisation . finally , in sec .  5",
    "we will discuss our findings , conclusions , and future prospects .",
    "research on language acquisition is approached in different disciplines by complementary methods and research questions .",
    "research in linguistics investigates different aspects of language in general and complexity of formal languages in particular .",
    "ongoing debates about nature versus nurture and symbol grounding led to valuable knowledge of _ principles _ of learning and _ mechanisms _ of information fusion in the brain that facilitate language competence ( compare @xcite and @xcite for a roadmap ) .",
    "recent research suggests that the well - known principles of statistical frequency and of compositionality in language acquisition are particularly important for forming representation by means of structuring multi - sensory data .",
    "researchers in different fields related to behavioural psychology study top - down both , the development of language competence in growing humans and the reciprocal effects of interaction with their environment , and have identified important socio - cultural principles . in computational neuroscience , many researchers look bottom - up into the _ where _ and _ when _ of language processing and refined the map of activity across the brain for language comprehension and production .",
    "new imaging methods allow for much more detailed studies on both , temporal and spatial level , and led to a major paradigm shift in our understanding of language acquisition and underlying mechanisms .      for language acquisition ,",
    "the first year after birth of a human infant is most crucial .",
    "in contrast to other mammals , the infant is not born mobile and matured , but develops capabilities and competencies postnatal @xcite .",
    "development of linguistic competence occurs in parallel  and highly interwoven  with cognitive development of other capabilities such as multi - modal perception , attention , motion control , and reasoning , while the brain matures and wires various regions  @xcite . in this process of individual learning",
    "the infant undergoes several phases of linguistic comprehension and production competence , ranging from simple phonetic discrimination up to complex narrative skills  @xcite .    during this development",
    "the infant s cognitive system makes use the following crucial principles among others  @xcite :    * * preposition for reference . *",
    "the temporally coherent perception of a physical entity in the environment and a describing stream of spoken natural language leads to the association of both  @xcite .",
    "* * body - rationality .",
    "* representations , which an infant might form , develop through sensorimotor - level environmental interactions accompanied by goal - directed actions @xcite .",
    "in addition , the embodiment is suggested as a necessary precondition for building up higher thoughts  @xcite . *",
    "* social cognition . * the development of language is seen only possible by interaction of a child with a caregiver that provides digestible amounts of spoken language @xcite . in particular , mothers provide an age - dependent simplification of grammar and focus on more common words first @xcite .",
    "overall this means that postnatal development of the processes of thought together with an appropriate interaction of a teacher enables acquisition of language .",
    "based on new imaging methods , several hypotheses have been introduced stating that many cortical areas are involved in language processing . in particular , it was claimed that several pathways between superior temporal gyrus and inferior frontal gyrus are involved in both language production and comprehension  @xcite .",
    "these pathways are suggested to include dorsal streams for sensorimotor integration and ventral streams for processing syntax and semantics .",
    "an important mechanism found is the activation of conceptual networks that are distributed over sensory areas during processing of words related to body parts ( somatosensory areas ) or object shapes ( visual areas )  @xcite .",
    "other seemingly important mechanisms found are :    * * cell assemblies . * in higher stages of the spatial or temporal hierarchy",
    ", neurons are organised in cell assemblies  @xcite .",
    "those might be distributed over different cortical areas or even across hemispheres and the activation of large and highly distributed cell assemblies can form higher - level concepts .",
    "other cell assemblies exist that represent specific semantics like morphemes and lemmas in language processing or are mediators between different levels  @xcite .",
    "the aforementioned conceptual networks can be seen as cell assemblies on word ( morpheme ) level . *",
    "* phonological and lexical priming . *",
    "the structure of brain connectivity and timing leads to priming , for example in cohort activation of most relevant sounds or lemmas  @xcite . *",
    "* spatial and temporal hierarchical abstraction . * strongly varying _ timescales _ take place in the brain .",
    "for example in the frontal lobe on caudal - rostral axis , processing of information occurs on much greater timescales from the pre - motor area up to mid - dorsolateral pre - frontal cortex , suggesting that these timings might be relevant for the processing of a plan for motor movement , over sequentialisation , and execution of motor primitives  @xcite .",
    "similar temporal hierarchies have been found in lower auditory processing  @xcite and higher vision  @xcite .",
    "overall this indicates the tight involvement of _ general _ processes in the brain for reducing and representing complexity in language processing .",
    "based on aforementioned principles and mechanistic characteristics we can build up a model , which is a neurocognitively plausible _ constraint _ of a general nonlinear neural architecture . as a starting point",
    "we adopt the _ continuous time recurrent neural network _ ( ctrnn ) as a valid abstraction for cortex - level processing  @xcite : @xmath0 where the activity @xmath1 of a neuron @xmath2 is derived over time @xmath3 as an accumulation of previous activity and a function over presynaptic input @xmath4 ( can be sensory input @xmath5 , recurrent input @xmath6 , or both ) , plastic connections @xmath7 and a bias @xmath8",
    ". the derivation is governed by a time constant @xmath9 that describes how fast the firing rate approaches the steady state value . although we can deduce the ctrnn from the _ leaky integrate - and - fire _ ( lif ) model and thus from a simplification of the hodgkin - huxley model from 1952 , the network architecture was suggested independently by @xcite as a nonlinear graded - response neural network and by @xcite as an adaptive neural oscillator .",
    "the ctrnn is thus the most general computational network model as it allows us to define arbitrary input , output , or recurrence characteristics within one ( horizontal ) layer . because of the recurrent connections , the network is arbitrarily deep and nonlinear , based on continuous information that are processed over time .      to explore the mechanism of _ timescales _ as a constraint of the ctrnn , tani et al .",
    "replicated the learning of mammal body motions in an experimental setup along the developmental robotic approach  @xcite . these _ multiple timescale recurrent neural networks _",
    "( mtrnns ) were specified by three layers ( called _ input - output _ ( io ) layer , _ context - fast _ ( cf ) layer , and _ context - slow _ ( cs ) layer ) with variable timescales and have been trained with a gradient descent method for sequences .",
    "the analysis revealed that for a trained network , which could reproduce sequences best ( merely indicated by converging to the smallest training error ) for io , @xmath10  for cf , and @xmath11  for cs layers  @xcite .",
    "] , the patterns in different layers self - organised towards a decomposition of the body movements .",
    "the researchers were able to interpret from the neural activity that the cf layer is always coding for the same short primitive , while the cs layer patterns are unique per sequence and consist of slow changing values , which function as triggering points for primitives .      in those original experiments , the researchers were able to train an mtrnn for reasonably diverse and long sequences by initialising the network s neural activity at first time step with specific values of the experimenter s choice  @xcite .",
    "these _ initial states _ were kept for the training of each specific sequence and represented the ( nonlinear ) association of a constant ( starting ) value and its dynamic pattern . in later experiments , nishide et al . adapted and integrated the idea of _ parametric bias _ ( pb ) units into the mtrnn  @xcite . therein , bias units are part of the cs layer and parametrise the motion sequence with a certain property ( e.g. which tool is used in a certain action ) , while another initial neural activity is not specified . however , for these bias or _ context - controlling _ ( csc ) units only an initialisation before training is necessary , while the values of these units can self - organise during training . similar to the _ recurrent neural network with parametric bias _ ( rnnpb ) , these initial states can be seen as a general context of a sequence . by modulating these internal states , differing other sequences can be generated .",
    "overall , for the conducted experiments on motor primitives , the slow context codes for the general _ concept _ of a certain body motion .    by combining the characteristics of the various experiments on ctrnns with multiple timescales and context bias properties ( similar to parametric bias but also changing over time ) , we arrive at a general description of the mtrnn as illustrated in fig .",
    "[ fig_cog_mtrnn ] . for certain contexts , provided as initial states to some of the neurons with the highest timescale @xmath12 ( slowest neurons ) , the network is processing certain sequences over time .",
    "the constraints on connectivity and relative timescale setting are inspired by the brain  @xcite and have been challenged in developmental robotics studies to confirm a hierarchical compositionality e.g. in body motion . for further models",
    ", we can process dynamic sequences in terms of discretised time steps ( e.g. for linguistic processing of smallest graphemic or phonetic units , or visual and sensorimotor processing with a certain sampling rate ) , but can regard any task as continuous by means of the absolute variability of the timescales .    , where the cs layer includes some _ context - controlling _ ( csc ) units . while the io layer processes dynamic patterns over time",
    ", the csc units at first time step ( @xmath13 ) contain the _ context _ of the sequence , where a certain concept can trigger the generation of the sequence . ]      by defining the time constant as a neuron- or unit - dependent variable @xmath14 and solving the equation with respect to a time step @xmath3 , we can also describe this special ctrnn in detail : in the mtrnn information is processed continuously with a unit - specific firing rate as a sequence of @xmath15 discrete time steps .",
    "such a sequence @xmath16 is represented as a flow of activations of neurons in the io layer ( @xmath17 ) .",
    "the input activation @xmath4 of a neuron @xmath18 at time step @xmath3 is calculated as : @xmath19 where we can either project desired ( sensory ) input @xmath20 to the io layer ( @xmath21 ) or read out the desired output @xmath22 of the io layer ( @xmath23 ) , depending on how the architecture is employed in a task .",
    "the input activation for neurons @xmath24 is initialised with   @xmath25 at the beginning of the sequence .",
    "the internal state @xmath26 of a neuron @xmath2 at time step @xmath3 is determined by : @xmath27 where @xmath28 is the initial internal state of the csc units @xmath29 ( at time step  @xmath25 ) , @xmath30 are the weights from @xmath31th to @xmath2th neuron , and @xmath32 is the bias of neuron  @xmath2 .",
    "the output ( activation value ) @xmath1 of a neuron @xmath2 at time step @xmath3 is defined by an arbitrary differentiable activation function : @xmath33 depending on the representation for neurons in io and on the desired shape of the activation for postsynaptic neurons , e.g. decisive normalisation ( softmax ) or sigmoidal .      during learning the mtrnn",
    "can be trained with sequences , and self - organises the weights and also the internal state values of the csc units .",
    "the overall method can be a variant of _ backpropagation through time _ ( bptt ) , sped up with appropriate measures based on the task characteristics .",
    "for instance , if the mtrnn produces continuous activity ( io ) we can modify the input activation with a prorated _ teacher forcing _ ( tf ) signal @xmath340,1[$ ] of the desired output @xmath22 together with the generated output @xmath1 of the last time step : @xmath35 in the forward pass , an appropriate error function @xmath36 is accumulating the error between activation values ( @xmath1 ) and desired activation values ( @xmath22 ) of io neurons at every time step based on the utilised activation function . in the second step ,",
    "the partial derivatives of calculated activation ( @xmath1 ) and desired activation ( @xmath22 ) are derived in a backward pass . in the case of e.g. a decisive normalisation function ( softmax ) in io and a sigmoidal function @xmath37 in all other layers",
    ", we can specify the error on the internal states of all neurons as follows : @xmath38 where the gradients are @xmath25 for the time step @xmath39 .",
    "for the error function @xmath36 of the decisive normalisation the _ kullback - leibler divergence _ ( kld ) is used , where the cross - entropy is generalised to @xmath40 classes  @xcite .",
    "importantly , the error propagated back from future time steps is particularly dependent on the ( different ) timescales . finally , in every epoch @xmath41 the weights @xmath7 but",
    "also biases @xmath8 are updated : @xmath42 where the partial derivatives for @xmath7 and @xmath8 are respectively the accumulated sums of weight and bias changes over the whole sequence , and @xmath43 and @xmath44 denote the learning rates for weight and bias changes .",
    "to facilitate the application of different methods for speeding up training , we can use individual learning rates for all weights and biases to allow for individual modifications of the weight and bias updates respectively .",
    "the initial internal states @xmath28 of the csc units define the behaviour of the network and are also updated as follows : @xmath45 where @xmath46 denotes the learning rates for the initial internal state changes .      for speeding up training",
    "we employ an adaptation of the _ resilient propagation _ ( rprop )",
    "algorithm that makes use of different individual learning rates @xmath43 and @xmath44 and adapt the learning rates @xmath47 for the update of the initial internal states",
    "@xmath28 as well  @xcite .",
    "in particular , the learning rates @xmath47 are adapted proportionally to the average of all learning rates @xmath43 over all weights that are connected with unit @xmath2 and neurons of the same ( cs ) and adjacent ( cf ) layer : @xmath48 since the update of the @xmath28 depends on the same partial derivatives ( time  step @xmath13 ) as the weights , we do not need additional parameters in this adaptive mechanism .      in the mtrnn with context bias we found that the timescale characteristic is crucial for a hierarchical compositionality of temporal dynamic output sequences .",
    "the increasingly slower information processing in the context led to generation of a sequence from an abstract concept . in order to design an architecture that can abstract a context from temporal dynamic input sequences ,",
    "we can reverse the notion of the context bias and thus reverse the processing from the context to the io layer .",
    "the structure of such a novel mtrnn with context abstraction is visualised in fig .",
    "[ fig_mod_mtrnncontextabstraction ] . for certain sequential input ,",
    "provided as a dynamic pattern to the fastest neurons ( lowest timescale ) @xmath49 , the network is accumulating a common _ concept _ in the slowest neurons ( highest timescale ) @xmath50 .",
    "since the timescale characteristics yield a slow adaptation of these so - called csc units , information in the units will accumulate abstract pattern from the input sequence ( filtered by neurons in a potential intermediate layer ) .",
    "the accumulation of information is characterised by a logarithmic skew to the near past and a reach - out to the long past depending on timescale values @xmath51 ( and @xmath52 ) .    , where the cs layer includes some _ context - controlling _ ( csc ) units .",
    "while the io layer processes dynamic patterns over time , the csc units abstract the context of the sequence at _ last _ time step ( @xmath53 ) . the crucial difference to the mtrnn with context bias",
    "is an inversion of the direction of procession and an accumulation of abstract context instead of production from a given abstract context . ]",
    "the mtrnn with context abstraction can be trained in supervised fashion to capture a certain concept from the temporal dynamic pattern .",
    "this is directly analogue to fixed - point classification with _ elman recurrent neural networks _ ( ernns )",
    "@xcite or ctrnns : we can determine the error between a desired temporal static concept pattern and the activity in the csc units at final time step ( @xmath53 ) .",
    "with a gradient descent method we can propagate the error backwards through time over the whole temporal dynamic pattern from which the concept was abstracted . however , for an architecture that is supposed to model the processing of a certain cognitive function in the brain , we are also interested in removing the necessity of providing a _ desired _ target concept a priori . instead , the representation of the concept should _ self - organise _ based on regularities latent in the stimuli .    for the mtrnn with parametric bias ,",
    "this was realised in terms of modifying the csc units activity in the initial time step ( @xmath13 ) backwards by the partial derivatives for weights connecting from those units .",
    "thus the internal states of the initial csc units self - organised in csc space towards values that were suited best for generating the sequences of the data set  @xcite . to foster a similar self - organisation of the csc units at final time step of the mtrnn with context abstraction ,",
    "a semi - supervised mechanism is developed that allows us to modify the desired concept pattern based on the derived error .",
    "since we aim at an abstraction from perception input to the overall concept , the _ least mean square _",
    "( lms ) error function is modified for the internal state  @xmath26 at time step  @xmath3 of neurons   introducing a _ self - organisation forcing constant _ @xmath54 as follows : @xmath55 where @xmath56 are internal states at the _ final _ time step @xmath15 ( indicating the last time step of a sequence ) of the csc units .    the particularly small self - organisation forcing constant allows the final internal states  @xmath56 of the csc units to adapt upon the data , although they actually serve as a target for shaping the weights of the network .",
    "accordingly , the final internal states @xmath56 of the csc units define the abstraction of the input data and are also updated as follows : @xmath57 where @xmath46 denotes the learning rates for the final internal state changes .",
    "thereby the learning error @xmath36 is used in one part ( @xmath54 ) to modify the final internal states and in another part ( @xmath58 ) to modify the weights .",
    "thus , similarly to the parametric bias units , the final internal states @xmath56 of the csc units self - organise during training in conjunction with the weights ( and biases ) towards the highest entropy . we can observe that the self - organisation forcing constant and the learning rate are dependent , since changing @xmath47 would also shift the self - organisation  for arbitrary but fixed @xmath54 .",
    "however , this is an useful mechanism to self - organise towards concepts that are most appropriate with respect to the structure of the data .      to test in a preliminary experiment how the abstracted concepts form for different sequences using this unsupervised learning mechanism , the architecture was trained for abstracting two contrary cosine waves into context patterns . in particular",
    ", for a sequence two cosines waves were presented to two input neurons and discretised to 33 time step . by differently phase - shifting the cosines",
    ", four different sequences were prepared .",
    "the key aspect of this task is to learn abstract the different phase - shifts in the otherwise identical sequences . in particular because of the ambiguous nature of saddle points , the network can not simply learn to predict the next time step , but must capture the whole sequence .",
    "processing such a sequence by the mtrnn with context abstraction is supposed to result in a specific pattern of the final csc units activity as the abstracted concept .    for determining how those patterns self - organise , the architecture was trained with predefined unchanging patterns ( chosen randomly : @xmath59}$ ] ) as well as with randomly initialised patterns that adapt during training by means of the varied self - organisation forcing parameter @xmath54 .",
    "to measure the result of the self - organisation , two distance measures @xmath60 , and @xmath61 are used : @xmath62 where @xmath63 describes the number of sequences and @xmath64 denotes the final csc units of sequence @xmath65 . with @xmath60 , which uses the standard _ lebesgue _",
    "@xmath66 or _",
    "euclidean _ distance , we can estimate the average distance of all patterns , while with @xmath61 we can describe the relative difference of distances .",
    "for example , in case the distances between all patterns are exactly the same , this measure would yield the best possible result .",
    "for example , it is not possible to arrange four points in a 2d - plane with equal distance @xmath67 . in this case , when representing four sequences with two csc units , we can derive a theoretical optimal @xmath68 .",
    "this example can be visualised as having four points in a the 2d - plane arranged optimally on the edges of a square .",
    "] of  @xmath69 . comparing both measures for varied settings of @xmath54",
    "provides an insight on how well the internal representation is distributed after self - organisation .",
    "the results for the experiment are presented in fig .",
    "[ fig_mod_mtrnnca_eval ] . from the plots",
    ", we can obtain that patterns of the abstracted context show a fair distribution for no self - organisation ( the random initialisation ) up to especially small values of about @xmath70 , a good distribution for values around @xmath71 and a degrading distribution for larger @xmath54 .",
    "the scatter plots for arbitrary but representative runs in fig .",
    "[ fig_mod_mtrnnca_eval]c  f visualise the resulting patterns for no ( @xmath72 ) , too small ( @xmath70 ) , good ( @xmath71 ) , and too large self - organisation forcing ( @xmath73 ) . from inspecting the csc units",
    ", we can learn that a `` good '' value for @xmath54 leads to a marginal self - organisation towards an ideal distribution of the concepts over the csc space during the training of the weights .",
    "furthermore , a larger @xmath54 is driving a stronger adaptation of the csc patterns than of the weights , thus leading to a convergence to similar patterns for all sequences .     + ( a ) training effort .",
    "+ ( b ) mean and relative @xmath66 distances .     + ( c ) @xmath72 .     + ( d ) @xmath70 .",
    "+ ( e ) @xmath71 .",
    "+ ( f ) @xmath73 .",
    "the task in this preliminary experiment is quite simple , thus a random initialisation within a feasible range of values ( @xmath74 $ ] ) of the csc units often already provides a fair representation of the context and allows for convergence to very small error values .",
    "however , for larger numbers of sequences , which potentially share some primitives , the random distribution of respective concept abstraction values is unlikely to provide a good distribution , thus self - organisation forcing mechanism can drive the learning .      in previous studies , the mtrnn with context bias was tested for modelling language processing due to the mechanism of spatial and temporal hierarchical compositionality . in particular ,",
    "@xcite utilised the architecture in a model to learn language from continuous input of sentences composed of words and graphemes that stem from a small grammar . for the model ,",
    "no implicit information is provided on word segmentation and on roles or categories for words . instead ,",
    "the input is modelled as streams of spike - like activities on graphemic level . during training , the architecture self - organises to the decomposition of the sentences",
    "hierarchically , based on the explicit structure of the inputs and the specific characteristics of some layers .",
    "the authors found that the characteristics of information processing on different timescales indeed leads to a hierarchical decomposition of the sentences in a way that certain character orders form words and certain word orders form the sentences .",
    "although the model was reproducing learned symbolic sentences quite well in their experiments , generalisation was not possible to test , because the generation of sentences was initiated by the internal state of the csc units , which had to be trained individually for every sentence in the model .",
    "@xcite extended this model to process the language embodied in a way that visual input will trigger the model to produce a meaningful verbal utterance that appropriately represents the input .",
    "the architecture , called model , consists of similar mtrnn layers for the language network , where a verbal utterance is processed as a sequence on phoneme level based on initial activity on an overall concept level .",
    "the overall concept is associated with raw feature input over merged shape and colour information of a visually perceived object .",
    "thereby the model incorporates the following hypotheses : a ) speech is processed on a multiple - time resolution , and b ) semantic circuits are involved in the processing of language .",
    "experiments revealed that the model can generalise to new situations , e.g. describe an object with a novel combination of shape and colour with the correct corresponding utterance due to the appropriate hierarchical component structure . yet , in this model the multi - modal complexity of real - world scenarios has not yet been tackled exhaustively .",
    "the temporal dynamic nature of visual observations or sensations from another modality was not included and especially not processed on multiple - time resolution .",
    "previous models of language processing ( compare in sec .",
    "[ sec_mod_prev ] ) provided insight into the architectural characteristics of language production , grounded in _ some _ perception . in recent neuroscientific studies , we learned about the importance of _ conceptual networks _ that are activated in processing speech and that most of the involved processes operate in producing speech as well ( compare @xcite ) .",
    "central findings include that the sensorimotor system is involved in these conceptual networks in general and in action and language comprehension in particular .    for the action comprehension phenomenon",
    ", these networks supposedly seem to involve multiple senses . as an example , for actions perceived from visual stimuli , @xcite found that there is a tight connection between perceiving the form and the motion of an action .",
    "a sequence of body poses is perceived as an action if the frames are integrated within 120 milliseconds .",
    "additionally , they found that the visual sequence is represented best as an action if both cues are present , but that in such a case the representation is mostly based on form information . since _",
    "body - rational _ motion information is hierarchically processed in proprioception as well , an integration of visual form and somatosensory motion seems more important .",
    "these multi - modal contributions  visual and somatosensory  are suggested to be strictly hierarchically organised ( compare  @xcite ) .",
    "the structure of integration in a conceptual network seems to derive from spatial conditions of the areas on the cortex that have been identified for higher abstraction from the sensory stimuli .",
    "these areas , for example the _ superior temporal gyrus _ ( stg ) , but also the _ inferior frontal gyrus _ ( ifg ) , are connected more densely , compared to the sensory regions , but they also show a high interconnectivity with other areas of higher abstraction . from the studies on _ cell assemblies _ ( cas )",
    "we deduced that such a particularly dense connectivity , on the one hand , can form general concepts ( for example about a certain situated action ) and , on the other hand , may invoke activation first  @xcite .",
    "from these recent findings , hypotheses , and the previous related work , we can adopt that the computational neural model for natural language production should be embedded in an architecture that integrates multiple modalities of contributing perceptual ( sensory ) information .",
    "the perceptual input should also be processed horizontally from sensation encoding over primitive identification ( if compositional ) up to the conceptual level .",
    "highly interconnected neurons between higher conceptual areas should form cas and thus share the representations for the made experiences .",
    "importantly the representations should form based on the structure in the perceptual input without a specific target .    in line with the developmental robotics approach  @xcite",
    ", the multi - modal perception should be based on real world data .",
    "both , the perceptual sensation as well as the auditory production , should be represented neurocognitively plausible . by employing this approach , an embodied and situated agent",
    "should be created that acquires a language by interaction with its environment as well as a verbally describing teacher . in this case , the interaction is experienced in terms of the temporal dynamic manipulation of different shaped and coloured objects .    with these requirements , the model implements the principles and mechanistic characteristics described in sec .",
    "[ sec_app_psy ] .",
    "properties of the model supposedly are generalisation despite dynamic embodied perception and disambiguation of inherently focused but limited uni - modal sensation by multi - modal integration . all in all ,",
    "goals of this model are a ) to refine connectivity characteristics that foster language acquisition and b ) to investigate merged conceptual representation .      in order to meet the requirements of such a multi - modal model",
    ", the following hypotheses are added to the previous model ( compare sec .  [ sec_mod_prev ] ) into a novel model named : a ) somatosensation and visual sensation are processed hierarchically by means of a multiple - time resolution , and b ) higher levels of abstractions are encoded in cas that are distributed over the sensory and motor areas . as a specific refinement of the previous model ,",
    "the neural circuits for processing the perceptions are modelled each as an mtrnn with context abstraction . the first one , called mtrnn@xmath75 , processes somatosensation , specifically proprioceptive perception , while the second one , named mtrnn@xmath76 , processes visual perception .",
    "the csc units of all mtrnns ( within the layers with the highest timescale cs ) are linked as fully connected associator neurons that constitute the cas for representing the concepts of the information .",
    "based on the abstract concepts the mtrnn with context bias , here called mtrnn@xmath77 , processes the verbal utterance , again as a sequence on phoneme level .",
    "all recurrent neural structures are specifications of a ctrnn to maintain neurocognitive plausibility .",
    "the notation of the io , cf , and cs layers in the novel perception components of the model , stand for input , fusion , and context of both modalities , somatosensory and vision , respectively .",
    "an overview of the architecture is presented in fig .",
    "[ fig_mod_multimodalmtrnn ] .",
    "an arising hypothesis for the computational model is that during learning a composition of a general feature emerges , which is invariant to the length of the respective sensory input .",
    "a second hypothesis is that features are ambiguous if uni - modal sensations are ambiguous for a number of overall _ different _ observations , but that the association can provide a distinct representation for the production of a verbal utterance .          for every scene",
    ", verbal utterances are presented together with sequences of proprioceptive and visual stimuli of an action sequence . during training of the system , the somatosensory mtrnn@xmath75 and the visual mtrnn@xmath76 self - organise weights and also the internal states of the csc units in parallel , for processing of an incoming perception . for the production of utterances , the auditory mtrnn@xmath77 self - organises weights and also the internal states of csc units .",
    "the important difference is that the mtrnn@xmath75 and the mtrnn@xmath76 self - organise towards the _ final _ internal states of the csc ( end of perception ) , while the mtrnn@xmath77 self - organises towards the _ initial _ internal states of the csc ( start of utterance ) . finally , the activity of the csc units of all mtrnns gets associated in the cas .",
    "the output layers of the mtrnn@xmath77 are specified by the decisive normalisation , while all other neurons are set up with a sigmoidal function ( using a logistic function with @xmath78 for range , and @xmath79 for slope as suggested in  @xcite .",
    "this particularly includes the neurons in the io layers of the mtrnn@xmath75 and mtrnn@xmath77 as well .    for training of the auditory mtrnn@xmath77 the procedure and mechanisms are kept identical to the training in all previous models : the adaptive bptt variant is utilised by specifying the kld and the lms as the respective error functions .",
    "the training of the mtrnn@xmath75 and mtrnn@xmath76 is conducted similarly , but it includes for both the suggested self - organisation forcing mechanism as described in eq .",
    "[ eqn_mod_mtrnnso ] ( sec .",
    "[ sec_mod_unifyingmtrnn_selforganisation ] ) . for these",
    "mtrnn with context abstraction , again the error is measured on randomly initialised ( desired ) activities of the csc units at the final time step and is used for self - organising both , the weights and the desired internal csc states . for the cas , associations between the csc units of the mtrnn@xmath75 , mtrnn@xmath76 , and",
    "mtrnn@xmath77 are trained with the lms rule on the activity of the csc units : @xmath80 where @xmath81 , @xmath82 and @xmath83 denote the internal states of the csc units for the mtrnn@xmath77 , mtrnn@xmath75 and mtrnn@xmath76 respectively .    with a trained network the generation of novel verbal utterances from proprioception and visual input can be tested .",
    "the final csc values of the mtrnn@xmath75 and mtrnn@xmath76 are abstracted from the input sequences respectively and associated with initial csc values of the auditory mtrnn@xmath77 .",
    "these values , in turn , initiate the generation of a phoneme sequence .",
    "generating novel utterances from a trained system by presenting new interactions only depends on the calculation time needed for the pre - processing and encoding , and can be done in real time .",
    "no additional training is needed .",
    "in order to analyse the proposed model s characteristics , we are first of all interested in identifying a parameter setting for the best ( relative ) generalisation capabilities . particularly , this enables to analyse the information patterns that emerges for different parts of the architecture .",
    "inspired by infant learning such an analysis will be embedded in a real world scenario , where a robot learns language from interaction with a teacher and its environment  @xcite . as a prelude for such an analysis the self - organisation forcing mechanisms need to be inspected further for the impact on the developed internal representation of the abstracted proprioception .",
    "premised on the principle of social cognition ( compare sec .",
    "[ sec_app_psy ] ) , the scenario is also based in the interaction of a human teacher with a robotic learner to acquire and ground language in embodied and situated experience . for testing the refined model ,",
    "a _ nao humanoid robot _ ( nao ) is supposed to learn to describe the manipulation of objects with various characteristics to be able to novel manipulation actions with correct novel verbal utterances .",
    "manipulations are to be done by the nao s effectors and thus to be observed by its motor feedback ( proprioception ) and visual perception ( see fig .",
    "[ fig_ana_scenario]a for an overview ) . in this study , for the developmental robotics approach , it is particularly important to include the influence of natural variances in interaction , which originate in varying affordances of different objects , but also in unforeseen natural noise .        0.979lp0.720 & move the arm behind the object and drag it towards the torso + & move the arm in front of the object and push it away from the torso + & point with the hand to the object + & move the arm to the right of the object and slide it horizontally to the left +     + ( b ) instructions for manipulation action teaching .",
    "example : + `` slide the red apple . ` '    s @xmath84 act the col obj .",
    "+ act @xmath84 pull @xmath85 push +     @xmath85 show me @xmath85 slide + col @xmath84 blue @xmath85 green +     @xmath85 red @xmath85 yellow + obj @xmath84 apple @xmath85 banana +     @xmath85 dice @xmath85 phone + ( c ) grammar .",
    "( d ) action teaching over time ( bottom : learner s view ) .    for a given scene in this scenario ,",
    "the human teacher guides the robot s arm in an interaction with a coloured object and verbally describes the manipulation action , e.g. `` slide the red apple ` ' .",
    "later , the robot should be able to describe a new interaction composed of motor movements ( proprioception ) and visual experience that it may have seen before with a verbal utterance , e.g. `` show me the yellow apple ` ' .",
    "the scenario should be controllable in terms of combinatorial complexity and mechanical feasibility for the robot , but at the same time allow for analysing how the permutation is handled .",
    "for this reason , the corpus is limited to a set of verbal utterances , which are generated from the small grammar as summarised in fig .",
    "[ fig_ana_scenario]c . for every single object of the same four distinct shapes ( apple , banana , phone , or dice ) and four colours ( blue , green , red , or yellow ) , four different manipulations are feasible with the arm of the nao : pull , push , show me , and slide .",
    "the grammar is overall unambiguous , meaning that a specific scene can only be described by one specific utterance .",
    "nevertheless , all objects have a similar mass and similar surface conditions ( friction ) .",
    "this way the proprioceptive sensation alone is mostly ambiguous for a certain manipulation action on objects with differing colours , but also with different shapes .    in order to collect data for this study , the @xmath86 different possible interactions were recorded four times , each with the same verbal utterance and arm - starting position but with slightly varying movements and object placements .",
    "this was done by asking different subjects ( colleagues from the computer science department ) to perform the teaching of such interactions in order to minimise the experimenter s bias ( instructions listed in fig .",
    "[ fig_ana_scenario]b ) .      to encode an utterance into a sequence @xmath87 of neural activation over time ,",
    "a phoneme - based adaptation of the encoding scheme suggested by @xcite is used : all verbal utterances for the descriptions are taken from the symbolic grammar , but are transformed into phonetic utterances based on phonemes from the arpabet and four additional signs to express pauses and intonations in propositions , exclamations , and questions : , with size @xmath88 .",
    "the occurrence of a phoneme  @xmath89 is represented by a spike - like neural activity of a specific neuron at relative time  step  @xmath90 .",
    "in addition , some activity is spread backwards in time ( rising phase ) and forwards in time ( falling phase ) , represented as a gauian function @xmath91 over the interval @xmath92 $ ] .",
    "all activities of spike - like peaks are normalised by a decisive normalisation function for every absolute time step @xmath3 over the set of input neurons .",
    "on the absolute course of time @xmath3 the peaks mimic priming effects in articulatory phonetic processing .",
    "for example , the previous occurrence of the phoneme `` p ` ' could be related to the occurrence of the phoneme `` ah ` ' leading to an excitation of the respective neuron for `` ah ` ' , when the neuron for `` p ` ' was activated .",
    "a sketch of the utterance encoding is shown in fig .  [ fig_emb_procutteranceencoding ] .",
    "neurons times @xmath93 time steps . ]",
    "the gauian @xmath91 for @xmath89 is defined by : @xmath94 where @xmath95 is the mean and the variance @xmath96 represents the filter sharpness factor .",
    "a peak occurs for the neuron @xmath17 with @xmath97 , if the phoneme @xmath89 is equal to the @xmath2th phoneme in the phoneme alphabet @xmath98 . from the spike - like activities the internal state @xmath26 of a neuron  @xmath2 at time step @xmath3",
    "is determined by : @xmath99 where @xmath100 is the filter width , @xmath101 is a head margin to put some noise to the start of the sequence , @xmath102 is the interval between two phonemes , and @xmath103 is a scaling factor for the neuron s activity @xmath22 of maximal values for possibly overlapping spikes .",
    "the scaling factor depends on the number of io neurons and scales the activity to @xmath1040,0.9]$ ] for the specified decisive normalisation ( softmax ) function : @xmath105 for the scenario , the constants are set to @xmath106 , @xmath107 , @xmath108 , and @xmath109 .",
    "the ideal neural activation for an encoded sample utterance is visualised in fig .",
    "[ fig_ana_representations]a .",
    "the utterance encoding is neurocognitively plausible because it reflects both , the neural priming mechanism as well as the fluent activation on a spatially distinct phonetic map  @xcite .",
    "although research on neural spatial organisation of phoneme coding is in its infancy , there is evidence for an early organisation of the _ primary auditory cortex _ ( a1 ) and the _ superior temporal sulcus _ ( sts ) forming a map for speech related and speech unrelated sounds  @xcite .",
    "the input representation is also in line with an ideal input normalisation to the mean of the activation function , as suggested in @xcite .    to gather and encode the proprioception of a corresponding manipulation action , the right arm of the nao is guided by the human teacher . from this steered arm movement ,",
    "joint angles of the five joints are directly measured with a sampling rate of @xmath110 _ frames per second _ ( fps ) .",
    "the resulting values are scaled to @xmath111 $ ] , based on the minimal and maximal joint positions ( see fig .",
    "[ fig_ana_representations]b for an example of the proprioceptive features @xmath112 ) . in a data recording conducted via this scheme ,",
    "the human teachers are instructed about the four different movements as listed in fig .",
    "[ fig_ana_scenario]b .",
    "having an encoding on the joint angle level is neurocognitively plausible because the ( human ) brain merges information from joint receptors , muscle spindles , and tendon organs into a similar proprioception representation in the s1 area @xcite .",
    "[ fig_ana_representations]c shows the encoded proprioception for the exemplary manipulation action .    for the visual perception , we aim at capturing a representation that is neurocognitively plausible but on a level of abstraction of shapes and colours and make use of conventional visual perception methods as shown in fig .",
    "[ fig_hri_procvisualencoding ] .",
    "neurons , with @xmath113 being the sum over . ]    at first , the mean shift algorithm is employed for segmentation on an image taken by the robotic learner  @xcite .",
    "the algorithm finds good segmentation parameters by determining modes that describe best the clusters in a transformed 3-d feature space by estimating best matching probability density functions .",
    "secondly , the _ canny edge detection _ as well as the opencv contour finder are applied for object discrimination  @xcite .",
    "the first algorithm basically applies a number of filters to find strong edges and their direction , while the second determines a complete contour by finding the best match of contour components .",
    "thirdly , the centre of mass and 16 distances to salient points around the contour are calculated . here ,",
    "salient means , for example , the largest or shortest distance between the centre of mass and the contour within intervals of @xmath114 . finally , the distances are scaled by the square root of the object s area and ordered clockwise , starting with the largest .",
    "the resulting encoding of 16 values in @xmath111 $ ] represents the characteristic shape , which is invariant to scaling and rotation .",
    "encoding of the perceived colour is realised by averaging the three r , g , and  b values of the area within the shape .",
    "other colour spaces e.g. based on only _ hue _ and _ saturation _ could be used as well , but they are in this step mainly a technical choice .",
    "additionally , the perceived relative position of the object is encoded by measuring the two values of the centroid coordinate in the field of view to allow for tests on interrelations between multiple objects later .",
    "the resulting encoding is plausible because in the brain is representing visual information in the process of recognising objects similarly by primarily integrating shape and colour features received from the _ visual cortex four _ ( v4 ) area  @xcite .",
    "the shape representation codes the discrimination of objects by combining a number of contour fragments described as the curvature - angular position relative to the objects centre of mass  @xcite .",
    "the colour representation codes hue and saturation information of the object invariant to luminance changes  @xcite . for an overview ,",
    "[ fig_ana_representations]c provides two prototypical example results of the perception process , fig .",
    "[ fig_ana_representations]d provides a sketch of the visual shape perception encoding , and fig .",
    "[ fig_ana_representations]e shows some of the used objects .",
    "the objects have been designed via 3d - print to possess similar masses despite different shapes and comparable colour characteristics across the shapes to provide for robustly and controllably perceivable characteristics .",
    "( a ) encoded utterance .",
    "( b ) enc .",
    "proprioception .",
    "( d ) enc . visual shapes .",
    "( c ) perceived shapes .",
    "( e ) some objects of interest .    capturing motion features also in visual perception",
    "is deliberately avoided for several reasons .",
    "first of all , from a conceptual perspective , it is desired to keep the visual sensation ambiguous on its own as well as to study the multi - model integration on a conceptual level .",
    "secondly , an agent could experience the movement of an entity in the field of view simply by tracking the said entity with its head or the eyes .",
    "this would shift the perception to the somatosensory level and would introduce a redundancy with respect to the arm sensation , which could be difficult to preclude in an analysis .      for evaluation ,",
    "the data was divided 50:50 into training and test sets ( all variants of a specific interaction are either in the training or in the test set only ) and used to train ten randomly initialised systems . in addition , this whole process was repeated 10 times as well ( 10-fold cross - validation ) to obtain @xmath115 runs for analysis .",
    "the mtrnns were parametrised as follows ( all parameters are given in tab .",
    "[ tab_ana_multi_expsp ] ) .",
    "the auditory mtrnn@xmath77 and the visual mtrnn@xmath76 were specified in size based on the previous studies for the model  @xcite .",
    "the somatosensory mtrnn@xmath75 was shaped similarly with @xmath116 and @xmath117 , based on the experience acquired as well as on other work  @xcite .",
    "the number of io neurons in all three mtrnns were based on the representations for utterances , proprioception , and visual perception and set to @xmath118 ,  @xmath119 ,  and  @xmath120 respectively .",
    "also based on previous experience and independent of the data set the number of csc units were set to @xmath121 .",
    "all weights were initialised similarly within the interval  @xmath122 $ ] , while the initial csc units ( auditory mtrnn@xmath77 ) were randomly taken from interval @xmath123 $ ] and the final csc units ( somatosensory mtrnn@xmath75 and visual mtrnn@xmath76 ) from interval @xmath74 $ ] .",
    "the learning mechanisms and parameters were identically chosen as in previous studies  @xcite .",
    "likewise , the timescales for the mtrnn@xmath77 and the mtrnn@xmath76 were based on the resulting values for the related models ( @xmath124 , @xmath125 , and @xmath126 )  @xcite . a good starting point for the timescale setting of the mtrnn@xmath75",
    "were the parameters suggested in original studies ( @xmath127 , @xmath128 , and @xmath129 ) to provide a progressive abstraction  @xcite .",
    "for this scenario , the timescales for the somatosensory modality seem not particularly crucial , since the manipulation actions are not strongly dependent on shared motion primitives .",
    "a preliminary parameter search ( not shown ) confirmed these suggestions and revealed good settings for the vision modality in similar ranges ( @xmath130 , @xmath131 , and @xmath132 ) .",
    "[ tab_ana_multi_expsp ]    for the self - organisation forcing parameter of the visual mtrnn@xmath76 , a parameter exploration was conducted similarly and is excluded here for brevity .",
    "this search revealed that the self - organisation is more crucial for this data set , but that a setting of @xmath133 again is good will be presented within this section ] .",
    "based on good parameters for dimensions , timescales , and learning , a variation of the self - organisation forcing parameter @xmath134 of the somatosensory mtrnn@xmath75 was conducted to test the overall performance of the model .",
    "the results of the experiment show that the system is able to generalise well : a high f@xmath135-score and a low edit distance ( insertion=1 , deletion=1 , substitution=2 ) of @xmath136 and @xmath137 on the training as well as @xmath138 and @xmath139 on the test set was determined for the best network . on average over",
    "all runs an f@xmath135-score and an edit distance of @xmath140 and @xmath141 for the training as well as @xmath142 and @xmath143 for the test have been measured ( @xmath144 , @xmath145 ) .",
    "note , due to the rigid training scheme there is a high chance that the system had to describe scenes , for which not all aspect ( shape , colour , or manipulation action ) have been learned before ( intended to keep the scenario challenging ) . for a parameter variation of the self - organisation forcing @xmath134 over @xmath146 , @xmath147 ,",
    "all results are provided in fig .",
    "[ fig_ana_multimotor_evalsocsc]a and c. notably , the best results originated from setting @xmath148 .",
    "training is challenging and rarely perfect yet not over - fitted systems were obtained on the training data .",
    "nevertheless , a high precision ( small number of false positives ) with a lower up to medium recall ( not the exact production of desired positives ) was observed on the test data .",
    "the errors made in production were mostly minor substitution errors ( single wrong phonemes ) and only rarely word errors .    using a self - organisation mechanism on the final initial csc values for the somatosensory and visual mtrnns caused good abstraction from the perception for the described scenario and",
    "the chosen @xmath134 and @xmath149 values . in this scenario , in fact , the mechanism is very crucial . for both sensory modalities",
    "the performance was significantly worse ( threshold for @xmath150 ) when using static random values for the final internal states of the csc units in abstracting the sensation @xmath72 .",
    "in particular for proprioception the rate of successfully described novel scenes nearly doubled when using self - organisation forcing with @xmath148 compared to random patterns .",
    "based on the experience acquired in the preliminary test ( compare sec .  [ sec_mod_unifyingmtrnn_evaluation ] ) , the obvious hypothesis is that the mtrnn@xmath75 self - organised a better distribution of the csc patterns in the csc space .",
    "however , measuring the csc space by using the @xmath66 distance metrics revealed that the patterns are not spreading out , but rather shrink towards small context values , regardless @xmath134 is set too large ( see fig .",
    "[ fig_ana_multimotor_evalsocsc]b ) : for smaller @xmath134 the shrinking develops similar but less strong .",
    "( 1,0.9007 ) ( 0,0)-score  ( a ) and mixed edit distance  ( b ) ",
    "`` mixed '' measures indicate a combination of training and test results with equal weight , mean of average and relative pattern distances  ( c ) , and intra- and intra - cluster distances  ( d ) with interval of the standard error , each over 100 runs and over varied @xmath149 respectively ; representative developed csc patterns  ( e  g ) reduced from @xmath151 to two dimensions ( pc1 and pc2 ) for selected parameter settings of no , `` good '' , and large self - organisation forcing respectively .",
    "different words for shapes and colours are shown with different coloured markers ( black depicts ` position ' utterance).,title=\"fig : \" ] ( 0.08298478,0.29383967)(0,0)[lb ] ( 0.08298478,0.43599475)(0,0)[lb ] ( 0.08298478,0.57814983)(0,0)[lb ] ( 0.08298478,0.72030491)(0,0)[lb ] ( 0.08298478,0.86245998)(0,0)[lb ] ( 0.25768995,0.2874 ) ( 0.33315478,0.2874 ) ( 0.40361961,0.2874 ) ( 0.47408444,0.2874 ) ( 0.54454927,0.2874 ) ( 0.61501410,0.2874 ) ( 0.68547893,0.2874 ) ( 0.75594376,0.2874 ) ( 0.82640859,0.2874 ) ( 0.89687342,0.2874 ) ( 0.96733829,0.2874 ) ( 0.05,0.586214983 ) ( 0.61601410,0.01175)(0,0)[cb ]     + ( a ) mean mixed f@xmath135-score .     + ( b ) mean and relative @xmath66 distances .",
    "+ ( c ) mean mixed edit distance .",
    "+ ( d ) inter- and intra - cluster @xmath66 distances .",
    "+ ( e ) @xmath152 .",
    "+ ( f ) @xmath153 .     + ( g ) @xmath154 .    to find an alternative hypothesis , the patterns were inspected again in detail .",
    "they showed some regularity for scenes including the same manipulation action .",
    "thus , a good performance might correlate with a self - organisation towards similar patterns for similar manipulations . to quantify this effect ,",
    "two additional measures are used to describe the difference between patterns for scenes with the same or with different manipulations @xmath155 : @xmath156{$(|m|-1)\\cdot(|m|/2)$}}\\sum_{k=1}^{|m|-1 } \\sum_{l = k+1}^{|m|}d\\left(\\text{centroid}\\scalebox{1.0}[1.0]{$(c_{m_k})$},\\text{centroid}\\scalebox{1.0}[1.0]{$(c_{m_l})$}\\right ) \\ , \\end{aligned}\\ ] ] where the inter - cluster distance @xmath157 is the average of all unweighted pair distances of patterns over the scenes that include the same manipulation ( e.g. pull , push , show me , and slide )  subsequently averaged over all manipulations .",
    "the intra - cluster distance @xmath158 provides the mean of all distances of centroids for the clusters @xmath159 that contain patterns of the same manipulation .",
    "the measurements of the inter- and intra - cluster distances over the varied @xmath134 are presented in fig .",
    "[ fig_ana_multimotor_evalsocsc]c .",
    "the plots are compared on the same absolute scale and show that the inter - distance is decreasing rapidly with increased @xmath134 , but the intra - distance decreases much slower . at some point , in fact ( e.g. for @xmath148 ) ,",
    "the inter - distance is smaller than the intra - distance .",
    "this means that the patterns are indeed clustered best for certain @xmath134 values before the shrinkage for the csc patterns is too strong and the distances vanish . in fig .",
    "[ fig_ana_multimotor_evalsocsc]e  g we can visually confirm this measured clustering on a representative example ( `` good '' in [ fig_ana_multimotor_evalsocsc]f ) .      throughout all tests of the model , diverse patterns of the internal states of the csc units developed across the modalities . nonetheless , frequently similar patterns emerged in the respective modality for similar utterances or perceptions .",
    "this is particularly the case for the csc units of the sensory modalities ( mtrnn@xmath75 and mtrnn@xmath76 ) , as shown in the last experiment ( where a clustering towards patterns for similar perceptions emerged ) , but also for csc units of the auditory production subsequently to the activation within the cas . during training",
    ", the csc units in the auditory mtrnn@xmath77 also self - organised for the presented sequences ( utterances ) .",
    "however , within the formation of the cas by means of the associations , patterns emerged that are able to cover the whole space of scenes in training and test data .    to inspect how these patterns self - organise",
    ", we can look into the generated csc patterns after the whole model is activated by the perception on somatosensory and visual modalities from the training _ and _ the test data .",
    "an example for such csc activations is presented in fig .",
    "[ fig_ana_cscanalysis ] for well - converged architectures with a low - score : low generalisation rate  @xmath160 , high generalisation rate  @xmath138 . ] generalisation rate ( a , c , and  e ) and a high generalisation rate ( b , d , and  f ) .",
    "the visualisation is provided by reducing the activity of the csc units to two dimensions using again _ principle component analysis _ ( pca ) and normalising the values , low / visual : @xmath161 , low / auditory : @xmath162 , high / proprioceptive : @xmath163 , high / visual : @xmath164 , high / auditory : @xmath165 . ] .",
    "the results confirm that the patterns form dense and sparse clusters for the visual csc ( the patterns , in fact , overlap each other for different manipulations on the _ same _ coloured and shaped object ) .",
    "for the somatosensory csc , the clusters are again reasonable distinct for the same manipulations , although there is a notable mixing between some manipulations on certain objects . for the auditory csc in case of high generalisation , the patterns are also distinctly clustered . in the example , presented in fig .",
    "[ fig_ana_cscanalysis]f , we can discover clustering by colour ( prominently on pc2 ) , by manipulation ( notably on pc1 ) and by shape ( in between and on lower components ) .",
    "the low generalisation example of fig .",
    "[ fig_ana_cscanalysis]e shows the clusters less clear with more patterns scattered across the pc1 and pc2 .     + ( a ) csc somatosensory  low .",
    "+ ( c ) csc visual  low .     + ( e ) csc auditory  low .     to two dimensions ( pc1 and pc2 ) and normalised , each .",
    "visualisation a , c , e are shown for an representative example for low and b , d , f for high generalisation . ]",
    "+ ( b ) csc somatosensory  high .",
    "+ ( d ) csc visual  high .",
    "+ ( f ) csc auditory  high .    inspecting the sensory data revealed that visual shape and colour sequences are strikingly similar for different manipulation on the same objects , while the proprioception sequences show some differences for some objects .",
    "for example , the slide manipulation on banana - shaped objects was notably different than on the other objects .",
    "apart from that , the proprioception sensation is mostly ambiguous with respect to the specific scene ( which object of which shape was manipulated )  which was intended in the scenario design .",
    "thus it seems that in the cas there is a tendency of restructuring the characteristics ( shape , colour , or proprioception ) , which were overlapping for the single modalities , into a representation where all characteristics are distributed .      in sum",
    ", embedding mtrnns with context abstraction and an mtrnn with context bias into one coherent architecture allows for a composition of temporal dynamic multi - modal perception into overall concepts and for the decomposition into meaningful sequential actuation , e.g. in terms of a verbal description . from the results",
    ", we can deduce that the self - organisation forcing indeed is facilitating the clustering of concepts for similar perceptions by self - organising the space of the internal states of the csc units upon the structure of the data .",
    "self - organising the patterns in the cas towards well - distributed clusters highly correlated with the ability to generalise well .    in the novel model ,",
    "good clustering self - organised for the abstracted context patterns of visual perception and also for somatosensation . for vision",
    ", this clustering occurs in particularly dense clusters that are sparsely distributed over the csc space . for models that generalise",
    "well , we found that in the cas associations emerged that projected the csc space of the multi - modal sensation ( shape , colour , proprioception ) into a well - distributed csc space of auditory production .",
    "this distribution self - organised again towards sparsely - distributed dense clusters .",
    "models that are able to successfully describe all training data , but can not generalise , showed a less well - distributed auditory csc space .    for the generalisation",
    "this means that a well - distributed ( sparse ) but well - structured ( conceptual clusters ) auditory csc space facilitates the grounding of language acquisition into the temporal dynamic features .",
    "such a csc space allows _ modulating _ which motor sequence needs to be selected to describe the perception . a good overall abstraction of the respective perceptual features into the cas thus fosters a correct ( good ) decomposition into a chain of words and then into phonemes .",
    "as a consequence , the cas fuse but more importantly disambiguate single modalities , which are ambiguous on their own , into an overall coherent representation . since in the model",
    "this happens temporally concurrent , it seems sufficient that different aspects of an observation , just need to co - occur to form a rich but latent overall representation for all modalities .",
    "for the brain , it has been shown that spatial characteristics of neural connectivity and temporal characteristics of neural activation lead to a hierarchical processing of sensation and actuation ( compare sec .  [ sec_app ] ) . in previous studies",
    "researchers have adopted these natural conditions of the cortex in order to model similar hierarchical processing in motor movements and speech production aspects ( compare sec .",
    "[ sec_mod_mtrnn ] ) .",
    "in particular , these conditions were utilised to constrain ctrnns with timescales and to integrate a context bias .",
    "such a so - called _ mtrnn with context bias _ model can decompose an initial context into a sequence of primitives . in this paper , this concept is developed further and reversed to allow for composing a sequence of primitives into an abstracted context .",
    "a mechanism is proposed to force an entropy - based self - organisation of such a context , which supposedly serves as a key component of an overall model for grounding language in embodied multi - modal sensation .",
    "the self - organisation forcing mechanism provides the development of a latent representation for the respective abstracted context of a sequential perception , without the need of an a priori definition . in the model ,",
    "the self - organisation forcing parameter is quite sensitive as too small values hinder a self - organisation , while too large values lead to a fast premature convergence of the architecture .",
    "a cause for the latter case is that both , the forward activity from small weights as well as a too strong adaptation towards this activity , lead to small errors .",
    "thus , the internal states of the final csc values are self - organised to match the activity from the network , before the network is self - organised to cover the regularities of the data .",
    "this issue could be further approached by using a regularisation for the self - organisation or by using weight initialisations based on the eigenvalue of the weight matrix . for the first option",
    ", it would be important to consider methods that are independent of the direction of the gradient .",
    "for example , a simple normalisation of the internal states of the final csc units would only skew the distribution and hence could lead to a convergence towards similar csc patterns .",
    "for the second option , a divergence could occur because the randomly initialised csc pattern could by chance be all similarly small or similarly large .",
    "utilising weight initialisation and normalisation techniques , used in learning deep ffns @xcite , might be interesting but can lead to additional instability during rnn training .    for our model",
    ", however , this means that for forming a compositional representation it perhaps is sufficient that the data contains regularities as well as irregularities .",
    "it seems that a compositional representation is formed solely by minimising differing activity for similar temporal dynamic patterns ( in production and sensation ) , thus by the entropy of different versus similar patterns . for the concepts of the whole temporal dynamic sequences ,",
    "this entropy - based descent , which is inherent in the self - organisation forcing mechanism , leads to a restructuring of the concept space to represent similar sequences with similar temporally static concept patterns .",
    "all in all , the regularities in the data , which that are also rich in our natural environment @xcite , also seem sufficient for an architecture with different timescales .      in the novel model , the density of the formed clusters of certain observations was observed to be closely related to the similarity of the abstracted sequences .",
    "this observation is logical since the data for the somatosensory and the visual modalities was not compositional and thus the patterns in the csc formed as a compression of the temporal dynamic observations . as a consequence ,",
    "the clustering of sequences is limited by the variability of the sequences , since there is no mapping required to a category within the single modalities . by associating the ( clustered ) multi - modal sensory representations with the auditory production representations , the cell assemblies form as a direct link of the active patterns .",
    "the resulting mappings show a close relation to the action - perception circuits measured in the brain  @xcite : the csc space is re - organised to form specific conceptual webs for co - occurring multi - modal patterns .",
    "since this effect was not built in but emerged from the entropy - based learning , it seems that the conceptual webs are the obvious consequence of the self - organisation .",
    "regarding our model , this means that the contexts for the single modalities indeed restructure towards a clustering of similar up to identical patterns for similar perceptions . in this way , the model self - organises towards capturing the features that are different in the otherwise ambiguous sequences . by associating the abstracted temporally static context representations of multiple perception modalities with the speech production modality , concept - level cell assemblies emerge that provide a well distributed unambiguous context space .",
    "thereby the context space is modulated to produce novel but correct speech productions . with regard to the brain",
    "this relates to the finding of synchronous firing between individual neurons , which react to the same stimulus but scaled - up to cortex level  @xcite .",
    "again , both , the uni - modal representations and the associations , self - organise themselves , driven by the regularities in the data .",
    "however , the structuring in the single modalities seems less complex and is easier to re - organise .",
    "hence , the hierarchical abstractions seem to operate like a filter on _ some _ features from the rich perception .",
    "summarising , this means , that the multi - modal context is an abstraction for important aspects of the perception on various pathways , to cope with the inherently varying temporal resolutions and information densities of the different modalities .",
    "overall , in this paper , we present a neurocognitively plausible model for embodied multi - modal language grounding and demonstrate it in a natural interaction of a robotic agent with its environment .",
    "the model is an extension of a previous model on embodied grounding , which showed that the spatial and temporal abstraction is an important characteristic for language in the brain  @xcite , and includes the processing of temporal dynamic somatosensory and visual perception .",
    "the characteristics of a neural architecture that facilitating language acquisition that we obtained from the novel model are : a ) shared representations of abstracted multi - modal sensory stimuli and motor actions can integrate novel experience and modulate novel production and b ) self - organisation might occur naturally because of the structure in the sensorimotor data and both , the spatial and temporal nesting that has evolved in the human brain .",
    "future research must address the demanding training , a scaling up to larger , and more natural language corpora to cover wider ranges of sensorimotor contingencies .",
    "perhaps we can ease the training by fuzzy characteristics of the neurons , e.g. a stochastic variance in the neurons firing rate , or the recruitment of new connections without changing the architecture s dimension  @xcite . in conceptually similar tasks in application , namely sequence to sequence mapping such as video annotation , the machine learning community made tremendous progress recently , e.g.  @xcite .",
    "although many utilised architectures employ computational mechanisms that are not neurocognitively plausible , some aspects like pooling , drop - out , and normalisation can be utilised to short - cut parts of the training that are conceptually not crucial for the model , such as preprocessing for visual feature extraction . for scaling - up , the complexity might get reduced by employing the principle of scaffolding in learning a language corpus  @xcite : words and holo - phrases first , and then more complex utterances without altering the weights from a `` word''-layer ( e.g. cf ) to the phonetic output . with respect to modelling further phenomena in the brain",
    ", it has been suggested that the same conceptual networks may be involved in speech processing , motor action as well as somatosensation  @xcite .",
    "further refinements can embed hierarchical abstraction and decomposition in utterance comprehension and motor action as well , and test how such a model can replicate an action for verbal descriptions , which were passively learned before or in co - occurrence with the production of an utterance .    with the outcome from our novel model and further refinements",
    ", we can design novel neuroscientific experiments on discovering multi - modal integration as well as hierarchical dependencies particularly in language processing and perhaps construct future robotic companions that participate in fascinating discourses .",
    "we would like to thank sascha griffiths , sven magg , wolfgang menzel , and cornelius weber for fruitful discussions on model characteristics and experimental design as well as for valuable comments on earlier versions of this manuscript .",
    "also , we want to thank erik strahl and carolin mnter for important support with the robotic hardware and experimental data collections .",
    "finally , we gratefully acknowledge partial support from the german research foundation ( dfg ) under project crossmodal learning , trr-169 .",
    "alho , j. , lin , f .- h . , sato , m. , tiitinen , h. , sams , m. , and jskelinen , i.  p. ( 2014 ) . enhanced neural synchrony between left auditory and premotor cortex is associated with successful phonetic categorization . , 5(394):110 .",
    "awano , h. , ogata , t. , nishide , s. , takahashi , t. , komatani , k. , and okuno , h.  g. ( 2010 ) .",
    "human - robot cooperation in arrangement of objects using confidence measure of neuro - dynamical system . in _ proc .",
    "2010 ieee int . conf . on systems man and cybernetics ( smc ) _ , p. 25332538 .",
    "ieee .",
    "dominey , p.  f. , inui , t. , and hoen , m. ( 2009 ) . neural network processing of natural language : ii . towards a unified model of corticostriatal",
    "function in learning sentence comprehension and non - linguistic sequencing .",
    ", 109(2):8092 .",
    "donahue , j. , hendricks , l.  a. , guadarrama , s. , rohrbach , m. , venugopalan , s. , saenko , k. , and darrell , t. ( 2015 ) .",
    "long - term recurrent convolutional networks for visual recognition and description . in _ proc .",
    "2015 ieee conf . on computer vision and pattern recognition ( cvpr 2015 ) _ , p. 26252634 .",
    "garagnani , m. and pulvermller , f. ( 2016 ) .",
    "conceptual grounding of language in action and perception : a neurocomputational model of the emergence of category specificity and semantic hubs .",
    ", 43(6):721737 .",
    "heinrich , s. , magg , s. , and wermter , s. ( 2015 ) .",
    "analysing the multiple timescale recurrent neural network for embodied language understanding . in koprinkova - hristova ,",
    ", eds . , _ artificial neural networks  methods and applications in bio-/neuroinformatics _ , vol .  4 of _ ssbn _ , ch .  8 , p. 149174 .",
    "springer int . pub .",
    "switzerland .",
    "heinrich , s. , weber , c. , and wermter , s. ( 2012 ) .",
    "adaptive learning of linguistic hierarchy in a multiple timescale recurrent neural network . in _ proc .",
    "22nd int . conf . on artificial neural networks ( icann 2012 ) _ ,",
    "7552 of _ lncs _ , p. 555562 .",
    "springer ber .",
    "heinrich , s. and wermter , s. ( 2014 ) .",
    "interactive language understanding with multiple timescale recurrent neural networks . in _ proc .",
    "24th int . conf . on artificial neural networks ( icann 2014 ) _ ,",
    "8681 of _ lncs _ , p. 193200 .",
    "springer int . pub .",
    "switzerland .",
    "krger , n. , janssen , p. , kalkan , s. , lappe , m. , leonardis , a. , piater , j. , rodrguez - snchez , a.  j. , and wiskott , l. ( 2013 ) .",
    "deep hierarchies in the primate visual cortex : what can we learn for computer vision ?",
    ", 35(8):18471871 .",
    "murata , s. , namikawa , j. , arie , h. , sugano , s. , and tani , j. ( 2013 ) . learning to reproduce fluctuating time series by inferring their time - dependent stochastic properties : application in robot learning via tutoring . , 5(4):298310 .",
    "nishide , s. , nakagawa , t. , ogata , t. , tani , j. , takahashi , t. , and okuno , h.  g. ( 2009 ) . modeling tool - body assimilation using second - order recurrent neural network . in _ proc",
    ". 2009 ieee / rsj int . conf . on intelligent robots and systems ( iros 2009 ) _ , p. 53765381 .",
    "ieee .",
    "riedmiller , m. and braun , h. ( 1993 ) . a direct adaptive method for faster backpropagation learning : the rprop algorithm . in _ proc .",
    "on neural networks ( icnn93 ) _ , vol .  1 ,",
    "p. 586591 .",
    "ieee .",
    "schulz , r. , glover , a. , milford , m.  j. , wyeth , g. , and wiles , j. ( 2011 ) .",
    "lingodroids : studies in spatial cognition and language . in _ proc .",
    "conf . on robotics and automation ( icra2011 )",
    "_ , p. 178183 .",
    "ieee .",
    "steels , l. , spranger , m. , van trijp , r. , hfer , s. , and hild , m. ( 2012 ) .",
    "emergent action language on real robots . in steels , l. and hild , m. , eds .",
    ", _ language grounding in robots _ , ch .  13 , p. 255276 .",
    "springer science+business media llc new york .",
    "sutskever , i. , vinyals , o. , and le , q. v.  v. ( 2014 ) .",
    "sequence to sequence learning with neural networks . in _ proc .",
    "28th annual conf . on neural information processing systems ( nips2014 )",
    "_ , vol .",
    "27 of _ adv . in nips _ , p. 31043112 .",
    "curran assoc . ,"
  ],
  "abstract_text": [
    "<S> the human brain is one of the most complex dynamic systems that enables us to communicate in natural language . </S>",
    "<S> we have a good understanding of some principles underlying natural languages and language processing , some knowledge about socio - cultural conditions framing acquisition , and some insights about _ where _ activity is occurring in the brain . </S>",
    "<S> however , we were not yet able to understand the behavioural and mechanistic characteristics for natural language and _ how _ mechanisms in the brain allow to acquire and process language .    in an effort to bridge the gap between insights from behavioural psychology and neuroscience , </S>",
    "<S> the goal of this paper is to contribute a computational understanding of the appropriate characteristics that favour language acquisition , in a brain - inspired neural architecture . </S>",
    "<S> accordingly , we provide concepts and refinements in cognitive modelling regarding principles and mechanisms in the brain  such as the hierarchical abstraction of context  in a plausible recurrent architecture . on this basis </S>",
    "<S> , we propose neurocognitively plausible model for embodied language acquisition from real world interaction of a humanoid robot with its environment . </S>",
    "<S> the model is capable of learning language production grounded in both , temporal dynamic somatosensation and vision . in particular , </S>",
    "<S> the architecture consists of a continuous time recurrent neural network , where parts have different leakage characteristics and thus operate on multiple timescales for every modality and the association of the higher level nodes of all modalities into cell assemblies . </S>",
    "<S> thus , this model features hierarchical concept abstraction in sensation as well as concept decomposition in production , multi - modal integration , and self - organisation of latent representations .    </S>",
    "<S> language acquisition ; recurrent neural networks ; embodied cognition ; multi - modal integration ; developmental robotics </S>"
  ]
}