{
  "article_text": [
    "this paper investigates identification and estimation of a multivariate linear regression model with measurement error in all the regressors and arbitrarily dependent unobserved regressors using characteristic functions .",
    "we show that the coefficients are identified if the unobserved regressors are not jointly normal and are independent of the errors .",
    "further , if any of the errors and regressors are dependent then the model is no longer identified .",
    "the literature on errors - in - variables in linear models is vast .",
    "three excellent reviews are cheng and van ness ( 1999 ) , fuller ( 1987 ) , and gillard ( 2010 ) . in the single regressor errors - in - variables model with independent regressor and errors ,",
    "the coefficient is identified if and only if ( a ) the regressor is not normal or ( b ) neither of the errors are normal ( reiersl , 1950 ; schennach & hu , 2013 ) . in the multivariate errors - in - variables model with regressors that are independent of errors , it is known that the coefficients are identified if ( a ) the errors are normal and the regressors are not jointly normal ( willassen , 1979 ) or ( b ) the unobserved regressors have nonzero and finite third or higher order cumulants , which usually means that identification comes from skewness and / or kurtosis ( geary , 1949 ; lewbel , 1997 ) .",
    "we show that the coefficients are identified when the unobserved regressors are not jointly normal , allowing the unobserved regressors to be arbitrarily dependent , the errors to have arbitrary distributions , and the third and higher order cumulants of all the random variables to be zero or infinite .",
    "identification is based on the property that the second - order partial derivatives of the log characteristic function ( lcf ) of the unobserved regressors are not all equal to a constant if and only if they are not jointly normal .",
    "we will use this variation to show that the coefficients uniquely minimize a distance between second - order partial derivatives of lcfs and covariances of observables .",
    "further , since the jointly normal distribution is the only distribution that does not have variation in all the second - order partial derivatives of its lcf , this provides a testable condition for nonnormality .",
    "the identification strategy extends ben - moshe ( 2013 ) to allow dependent unobservables .",
    "estimation of the coefficients without any additional information is usually based on higher - order moments ( cragg , 1997 ; dagenais & dagenais , 1997 ; erickson , jiang , & whited , 2013 ; pal , 1980 ) , which can have high variance and bias and are sensitive to outliers and data transformations .",
    "our estimator is an extremum estimator based on second - order partial derivatives of the lcf of the observed variables , which contains all the information from the higher - order moments and is root-@xmath0 consistent . in monte - carlo simulations",
    ", we find that our estimator performs well relative to estimators based on third and fourth order moments and is robust to various distributions including symmetric ones .",
    "plots of the second - order partial derivatives of the lcf of the observables provide evidence for or against variation and appropriateness of using second - order partial derivatives for estimation , and evidence of not jointly normal unobserved regressors .",
    "this section presents the model , assumptions , identification , and estimation .",
    "our tool of analysis is the lcf , which is denoted by @xmath1 $ ] where @xmath2 is a random vector and @xmath3 .",
    "consider the classic errors - in - variables model , @xmath4 where @xmath5 is an observed outcome , @xmath6 are observed measurements , @xmath2 are unobserved regressors , @xmath7 are unobserved measurement errors , @xmath8 is an unobserved error , and @xmath9 are the unknown coefficients of interest . instead of intercepts we allow @xmath10 to have nonzero means .",
    "the identification strategy is invariant to these intercepts and means .",
    "the following assumption describes the dependence structure , which allows the unobserved regressors to be arbitrarily dependent but independent of errors .",
    "[ as : ind_eiv ] the unobserved errors @xmath11 ,  , @xmath12 , and @xmath8 are mutually independent and independent of @xmath13 .",
    "the vector of unobserved regressors @xmath14 is dependent .    no other unobserved variable can be dependent on @xmath13 for @xmath15 to be identified .",
    "[ le : dep ] if @xmath16 or @xmath17 is dependent then @xmath15 is not identified .    for simplicity",
    "assume that @xmath13 can not be divided into subsets of mutually independent vectors .",
    "if @xmath13 can be divided into mutually independent vectors then the following assumption is modified from @xmath13 is not jointly normal to each subset is not jointly normal . even with the modification , the formula in theorem [ th : eiv ] remains the same .",
    "[ as : gau_eiv ] one of the following equivalent conditions hold :    a.   the unobserved vector of regressors @xmath13 is not jointly normal , [ as : gau_eiv1 ] b.   on all neighborhoods of the origin @xmath18 for some @xmath19,[as : gau_eiv2 ] c.   on all neighborhoods of the origin @xmath20 for some @xmath19.[as : gau_eiv3 ]    in the single regressor errors - in - variables model , reiersl ( 1950 ) shows that a nonnormal unobserved regressor is sufficient for identification and schennach and hu ( 2013 ) prove that the model is identified if and only if the unobserved regressor or both of the errors are nonnormal . in the multivariate errors - in - variables model , willassen ( 1979 ) shows that if @xmath21 is jointly normal then @xmath22 is identified if and only if @xmath13 is not jointly normal .",
    "the intuition for requiring some nonnormality is that normal distributions are completely characterized by their means and covariances and klepper and leamer ( 1984 ) prove that higher - order moments @xmath23 are necessary for identification .",
    "while other papers identify @xmath15 with restrictions on ( and existence of ) higher - order moments like nonzero skewness or kurtosis , we use all the information from higher - order moments and only require nonnormality of the unobserved regressors ( assumption [ as : gau_eiv ] ) .",
    "identification comes from variation in the second - order partial derivatives of the lcf of @xmath13 ( assumption [ as : gau_eiv ] ) , which is a characteristic of a random vector if and only if it is not jointly normal .",
    "assumption [ as : gau_eiv ] is a testable restriction that can be checked in data and conceivably provides a test for normality .      assuming that the unobserved regressors are not jointly normal and independent of the errors",
    ", the following theorem proves that the coefficients in the multivariate errors - in - variables model are identified .",
    "[ th : eiv ] consider the multivariate errors - in - variables model from equation .",
    "let @xmath24 .",
    "if @xmath25 and @xmath26 for @xmath27 , then @xmath22 is identified when assumptions [ as : ind_eiv ] and [ as : gau_eiv ] hold and is the unique solution to @xmath28 where @xmath29 is the absolute value of @xmath30 and the weight functions @xmath31 satisfy @xmath32 and @xmath33 for all @xmath34 .",
    "identification uses all cross partial derivatives and all the arguments of @xmath35 .",
    "it thus seems hard to see how any assumption can be weakened and the coefficients still identified .",
    "we sketch the proof with details in the appendix .",
    "the proof converts the problem to uniqueness of the solution of a functional equation .",
    "the lcf of the observed variables is , @xmath36 where the equality follows from the dependence structure in assumption [ as : gau_eiv ] .",
    "identification comes from variation in the second - order partial derivatives of @xmath35 through choices of @xmath37 .",
    "let assumption [ as : gau_eiv ] hold for some fixed @xmath38 and @xmath39 with @xmath40 .",
    "then , @xmath41 substituting in @xmath42 and @xmath43 , @xmath44 only jointly normal random variables do not have variation in all second - order partial derivatives so that @xmath45 for all @xmath46 and all @xmath38 and @xmath39 . by assumption",
    "[ as : gau_eiv ] , equation equals zero if and only if @xmath47 .",
    "formula follows by minimizing the distance between the partial derivatives of @xmath35 evaluated at @xmath48 and @xmath49 , which by assumption [ as : gau_eiv ] is uniquely minimized when @xmath50 .      for a given sample @xmath51 of iid observations ,",
    "estimation replaces the population quantities in equation with sample analogs .",
    "the extremum estimator is @xmath52 where @xmath53    we repeat the following standard conditions and theorems needed for consistency and asymptotic normality of an extremum estimator ( eg .",
    "newey & mcfadden , 1994 ) .",
    "[ as : ext ] ( i ) @xmath54 is uniquely maximized at @xmath15 ; ( ii ) @xmath55 and @xmath56 is compact ; ( iii ) @xmath57 is continuous ; ( iv ) @xmath58 converges uniformly in probability to @xmath54 ; ( v ) @xmath58 is twice continuously differentiable in a neighborhood of @xmath15 ; ( vi ) @xmath59 ; ( vii ) there is an @xmath60 continuous at @xmath15 such that @xmath61 converges uniformly in probability to @xmath60 in a neighborhood of @xmath15 ; ( viii ) @xmath62 is nonsingular .    [",
    "prop : conan ] if assumptions [ as : ext](i)-(iv ) hold then @xmath63 . if assumptions [ as : ext](i)-(viii ) hold then @xmath64 .",
    "assumption [ as : ext ] is satisfied when @xmath15 is identified i.e. theorem [ th : eiv ] holds .",
    "continuity and uniform convergence use the uniform continuity of a cf , assumptions that the cf is nonzero on an interval around the origin , and that moments are bounded .",
    "the proofs use taylor expansions of sample analogs around population quantities .",
    "the exact expressions are lengthy but only require taking derivatives of the expressions above .",
    "this section analyzes the finite sample performance of the extremum estimator based on the second - order partial derivatives of the lcf from the previous section ( labeled pd ) and compares it to a third - order cumulant estimator ( c3 ) , a fourth - order cumulant estimator ( c4 ) , and the ordinary least squares estimator ( ols ) using data generated from monte - carlo simulations .",
    "consider the errors - in - variables model ,    @xmath65    let @xmath66 , @xmath67 , and @xmath68 .",
    "table 1 displays the means and standard errors ( in parentheses ) of the monte carlo distributions of the estimates of @xmath69 ( the estimates of @xmath70 are similar ) obtained from @xmath71 simulations of sample size @xmath72 without measurement error ( @xmath73 ) and with measurement error ( @xmath11 and @xmath74 iid standard normal ) . in the first two columns ( design 1 ) , @xmath75 are generated by drawing @xmath76 iid samples from a beta distribution with parameters @xmath77 and adjusting the regressors to each have variance @xmath78 and covariance @xmath79 ( by multiplying the generated variables by a cholesky decomposition of the covariance matrix ) . in the third and fourth columns ( design 2 ) ,",
    "@xmath75 are generated in the same way as the first two columns except that they are drawn from a @xmath80-distribution with @xmath81 degrees of freedom .",
    "similarly , in the fifth and sixth columns ( design 3 ) , @xmath75 are generated from a t - distribution with @xmath81 degrees of freedom .    in all three designs ,",
    "the ols estimator has the tightest confidence bands around @xmath69 when there is no measurement error and is badly biased when there is measurement error . with measurement error , the pd estimator has the tightest confidence bands around @xmath69 .",
    "the t - distribution is symmetric so that there is no identifying information from the skewness , which manifests itself in biased c3 estimates with the largest standard errors .",
    "l : c : c : c : c : c : c & + & & &    ' '' ''     + estimator & @xmath82 & @xmath83 & @xmath82 & @xmath83 & @xmath82 & @xmath83    ' '' ''     + pd & 1.01 ( 0.07 ) & 1.02 ( 0.16 ) & 1.02 ( 0.06 ) & 1.00 ( 0.10 ) & 1.00 ( 0.13 ) & 1.01 ( 0.12 ) + c3 & 1.02 ( 0.07 ) & 1.02 ( 0.20 ) & 1.01 ( 0.06 ) & 1.01 ( 0.11 ) & 1.05 ( 0.78 ) & 0.83 ( 1.47 ) + c4 & 0.98 ( 0.13 ) & 1.04 ( 0.30 ) & 0.97 ( 0.15 ) & 0.98 ( 0.21 ) & 1.04 ( 0.16 ) & 0.98 ( 0.19 ) + ols & 1.00 ( 0.03 ) & 0.82 ( 0.04 ) & 1.00 ( 0.03 ) & 0.78 ( 0.03 ) & 1.00 ( 0.03 ) & 0.75 ( 0.03 ) +     estimates are based on the errors - in - variables model in equations - . for each design ,",
    "we generate @xmath71 simulations with @xmath84 observations .",
    "the unobserved regressors are drawn from a @xmath85 distribution ( design 1 ) , @xmath86 distribution ( design 2 ) or @xmath87 distribution ( design 3 ) with @xmath88 and @xmath89 .",
    "the first , third , and fifth columns have no measurement errors while the second , fourth , and sixth columns have measurement errors drawn from a standard normal distribution . the numbers without parentheses are means and the numbers in parentheses are standard errors",
    ".    figures 1 , 2 , and 3 plot for designs 1 , 2 , and 3 respectively the 5% , 50% , and 95% of the monte - carlo distributions of the estimates of the second - order partial derivatives , @xmath90 no constant function lies between the 5% and 95% confidence bands in all but one of the graphs in figures 1 , 2 , and 3 so we can be confident of identifying variation in all but one of the pds ( the exception is @xmath91 when the regressors are generated from the @xmath92-distribution ) .",
    "the t - distribution approaches the normal distribution as the degrees of freedom approach infinity .",
    "figure 4 plots the second - order partial derivatives of a t - distribution with @xmath93 degrees of freedom and although the median has variation , a constant function does lie between the 5% and 95% confidence bands so we are less confident about identification and our estimator",
    ".    figures 1 , 2 , and 3 show a tradeoff between values of @xmath94 close to the origin and values of @xmath94 far from the origin ; when @xmath94 is close to the origin then the second - order partial derivatives are more accurately estimated ( narrow confidence bands ) but there is less variation in the second - order partial derivatives from their value at the origin while when @xmath94 is far from the origin the second - order partial derivatives are less accurately estimated ( wide confidence bands ) but there is more variation in the second - order partial derivatives from their value at the origin .",
    "we performed other simulations using the model in equations - using various distributions , variances , and covariances , and choices of @xmath15 ( and using different ways to construct the dependence between @xmath95 and @xmath96 and different distributions of errors ) .",
    "in most the simulations , the pd estimator had the tightest confidence bands around @xmath69 .",
    "further , we plotted the second - order partial derivatives of @xmath97 to check for variation and the appropriateness of using the pd estimator .",
    "this paper considers identification and estimation of coefficients in a multivariate linear regression with measurement errors in all the variables . assuming that the unobserved regressors are not jointly normal and independent of errors , we identify and estimate the coefficients using variation in the second - order partial derivatives of the lcf of the unobserved regressors , which is only possible if the unobserved regressors are not jointly normal .    in our simulations , we find that the set on which @xmath98 is important but not the choice of function @xmath99 .",
    "this is similar to choosing bandwidth and could be an interesting topic for future research .",
    "the following relationship will be used in the proofs below .",
    "@xmath100 \\nonumber \\\\ & = \\ln e\\left [ \\exp    \\left(i\\sum_{k=1}^k ( x^*_k+u_k)s_k + i(\\sum_{k=1}^k \\beta_k x^*_k+\\varepsilon)s_{y}\\right ) \\right ]   \\nonumber \\\\ & = \\ln e\\left[\\exp   \\left(i\\sum_{k=1}^k ( s_k+\\beta_k s_{y})x^*_k + i\\sum_{k=1}^k s_k u_k + is_{y}\\varepsilon \\right )   \\right ]                                   \\label{eq : lcf}\\end{aligned}\\ ] ] where the second equality follows by substituting in @xmath101 and @xmath102 .",
    "assume @xmath103 is dependent .",
    "let @xmath104 where @xmath105 , @xmath106 , and @xmath107 . then using , @xmath108 \\\\ & = \\ln e\\left[\\exp   \\left(i\\sum_{k=1}^k ( s_k+ c_k\\beta_k s_{y})(x^*_k+u_k )   + is_{y}\\left(\\varepsilon + \\sum_{k=1}^k \\beta_k\\left(x^*_k ( 1-c_k)- c_k u_k \\right )",
    "\\right ) \\right )   \\right]\\\\ & = \\ln e\\left[\\exp   \\left(i\\sum_{k=1}^k ( s_k+\\widetilde{\\beta}_k s_{y})\\widetilde{x}^*_k + is_{y}\\widetilde{\\varepsilon } \\right )   \\right]\\\\ & = \\varphi _ { \\widetilde { \\bm   x}^*,\\widetilde \\varepsilon}\\left ( s_1 + \\widetilde \\beta_1 s_{y } , \\ldots , s_k + \\widetilde \\beta_k s_{y},s_y   \\right ) + \\sum_{k=1}^{k}\\varphi_{\\widetilde u_k }   \\left(s_k \\right ) \\end{aligned}\\ ] ] hence , @xmath109 is observationally equivalent to @xmath15 .",
    "assume @xmath110 is dependent .",
    "let @xmath104 where @xmath105 , @xmath111 , and @xmath112 . then using , @xmath113 \\\\ & = \\ln e\\left[\\exp   \\left(i\\sum_{k=1}^k ( s_k+\\frac{\\beta_k}{c_k } s_{y})c_k x^*_k + i\\sum_{k=1}^k s_k \\left ( u_k + x^*_k(1-c_k ) \\right ) + is_y   \\varepsilon \\right )   \\right]\\\\ & = \\ln e\\left[\\exp   \\left(i\\sum_{k=1}^k ( s_k+\\widetilde{\\beta}_k s_{y})\\widetilde{x}^*_k + i\\sum_{k=1}^k s_k \\widetilde u_k   + is_y \\widetilde \\varepsilon \\right )   \\right]\\\\ & = \\varphi_{\\widetilde{\\bm x}^*,\\widetilde { \\bm u}}\\left ( s_1 + \\widetilde \\beta_1 s_{y } , \\ldots , s_k + \\widetilde \\beta_k s_{y},s_1,\\ldots ,",
    "s_k   \\right ) + \\varphi_{\\widetilde \\varepsilon }   \\left(s_y \\right ) \\end{aligned}\\ ] ] hence , @xmath109 is observationally equivalent to @xmath15 .",
    "we prove the contrapositive . the unobserved vector of regressors @xmath13 is jointly normal with covariance matrix @xmath115 if and",
    "only if @xmath116 by analytic continuity ( e.g. lukacs , 1970 ) , the lcf of @xmath13 is given by equation if and only if the equation holds for some neighborhood of the origin .",
    "if holds on a neighborhood around the origin then the second - order partial derivatives on this neighborhood are @xmath117 where @xmath118 are the elements of @xmath115 . by the fundamental theorem of calculus",
    "if holds then holds on this neighborhood ( with possibly different mean @xmath119 ) and so @xmath13 is normal .",
    "@xmath120 . using equation and assumption [ as : ind_eiv ] , @xmath121 the second - order partial derivatives are @xmath122 by , @xmath123 if and only if @xmath20 where @xmath40 . if @xmath124 ( and @xmath125 ) then by , @xmath126 if and only if @xmath127 .",
    "using equation and assumption [ as : ind_eiv ] , @xmath36 the second - order pds are @xmath128 define @xmath129 and define @xmath130 where and follow by substituting in and .",
    "if @xmath131 then @xmath132 for all @xmath133 .",
    "we now show that @xmath134 is the only solution to @xmath135 .",
    "if @xmath135 then @xmath136 where the first equality follows by equation and the second equality follows by using the first equality and equation .",
    "using assumption [ as : gau_eiv ] , the only solution is @xmath134 .",
    "hence , @xmath137 .",
    "schennach , s. m. and hu , y. ( 2013 ) : `` nonparametric identification and semiparametric estimation of classical measurement error models without side information , '' _ journal of the american statistical association _ , 108 , 177 - 186 ."
  ],
  "abstract_text": [
    "<S> this paper identifies and estimates the coefficients in a multivariate errors - in - variables linear model when the unobserved arbitrarily dependent regressors are not jointly normal and independent of errors . to identify the coefficients , </S>",
    "<S> we use variation in the second - order partial derivatives of the log characteristic function of the unobserved regressors ; a property of only not jointly normal distributions . </S>",
    "<S> a root-@xmath0 consistent and asymptotically normal extremum estimator performs well in simulations relative to third and fourth order moment estimators .    </S>",
    "<S> 1.2    * keywords : * _ errors - in - variables , measurement errors , dependent regressors , nonnormality , second - order partial derivatives of the log characteristic function _ </S>"
  ]
}