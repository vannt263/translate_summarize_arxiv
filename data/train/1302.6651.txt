{
  "article_text": [
    "consider the following class of regression models , with response variable denoted by @xmath0 and @xmath1-dimensional covariate vector by @xmath2 , @xmath3 where @xmath4 is the unknown parameter vector , @xmath5 is the unobserved error term that is independent of @xmath2 with a completely unspecified distribution , and @xmath6 is a monotone increasing , but otherwise unspecified function .",
    "it is easily seen that this class of models contains many commonly used regression models as its submodels that are especially important in the econometrics and survival analysis literature .",
    "for example , with @xmath7 , becomes the standard regression model with an unspecified error distribution ; with @xmath8 ( @xmath9 ) , the box - cox transformation model ( box and cox , 1964 ) ; with @xmath10 $ ] , the binary choice model ( maddala , 1983 ; mcfadden , 1984 ) ; with @xmath11 $ ] , a censored regression model ( tobin , 1958 ; powell , 1984 ) ; with @xmath12 , the accelerated failure times ( aft ) model ( cox and oakes , 1984 ; kalbfleisch and prentice , 2002 ) ; with @xmath5 having an extreme value density @xmath13 , the cox proportional hazards regression ( cox , 1972 ) ; with @xmath5 having the standard logistic distribution , the proportional odds regression ( bennett , 1983 ) .    a basic tool for handling model is the maximum rank correlation ( mrc ) estimator proposed in the econometrics literature by han ( 1987 ) .",
    "because both the transformation function @xmath6 and the error distribution are unspecified , not all components of @xmath4 are identifiable . without loss of generality",
    ", we shall assume henceforth that the last component , @xmath14 .",
    "let @xmath15 be a random sample from .",
    "han s mrc estimator , denoted by @xmath16 , is the maximizer of following objective function @xmath17}i[{{{\\bf x}}_i'{\\mbox{\\boldmath$\\beta$}}({\\mbox{\\boldmath$\\theta$}})>{{\\bf x}}_j'{\\mbox{\\boldmath$\\beta$}}({\\mbox{\\boldmath$\\theta$}})}],\\ ] ] where @xmath18 $ ] denotes the indicator function , @xmath19 the transpose of @xmath2 , and @xmath20 the first @xmath21 components of @xmath4 , i.e. @xmath22 . han ( 1987 ) proved that the mrc estimator @xmath16 is strongly consistent under certain regularity conditions .",
    "an important subsequent development is due to sherman ( 1993 ) , who made use of the empirical process theory and hoeffding s decomposition to approximate the objective function , viewed as a u - process .",
    "he showed that @xmath16 is , in fact , asymptotically normal under additional regularity conditions .",
    "estimation of the transformation function @xmath6 was studied by chen ( 2002 ) , who constructed a rank - based estimator and established its consistency and asymptotic normality .",
    "in addition to the econometrics , model also encompasses the main semiparametric models in survival analysis , where right censoring is a major feature . under the right censorship , there is a censoring variable @xmath23 and one observes @xmath24 and @xmath25 .",
    "khan and tamer ( 2007 ) constructed the following partial rank correlation function as an extension of the rank correlation objective function , @xmath26}i[{{{\\bf x}}_i'{\\mbox{\\boldmath$\\beta$}}({\\mbox{\\boldmath$\\theta$}})>{{\\bf x}}_j'{\\mbox{\\boldmath$\\beta$}}({\\mbox{\\boldmath$\\theta$}})}].\\ ] ] they showed that the resulting maximum partial rank correlation estimate ( prce ) @xmath27 , as the maximizer of @xmath28 , is consistent and asymptotically normal .",
    "crucial for the statistical inference of based on @xmath29 is the consistent variance estimation . in standard objective ( loss ) function derived estimation ,",
    "the asymptotic variance is usually estimated by a sandwich - type estimator of form @xmath30 with @xmath31 being the second derivative of the objective function and @xmath32 an estimator of the variance of the first derivative ( score ) .",
    "the challenge here , however , is that @xmath33 itself is a ( discontinuous ) step function that precludes automatic use of differentiation to obtain @xmath31 .",
    "furthermore , @xmath32 is also difficult to obtain since the score function can not be derived directly from @xmath33 via differentiation .",
    "sherman(1993 ) suggested using numerical derivatives of first and second orders to construct @xmath31 and @xmath32 .",
    "his approach requires bandwidth selection for the derivative functions .",
    "it is unclear how stable the resulting variance estimator is .",
    "alternatively , one may resort to bootstrap ( efron , 1979 ) or other resampling methods ( e.g. jin et al .",
    "these approaches require repeatedly solving the maximization of , which is discontinuous and often multidimensional when @xmath34 .",
    "the computational cost could therefore be prohibitive .    in this paper",
    ", we develop a self - induced smoothing method for rank correlation criterion function so that the differentiation can be performed , while bypassing the bandwidth selection .",
    "both point and variance estimators can be obtained simultaneously in a straightforward way that is typically used for smooth objective functions .",
    "the new method is motivated by a novel approach proposed in brown and wang ( 2005 , 2007 ) , where an elegant self - induced smoothing method was introduced for non - smooth estimating functions . although our approach bears similarity with that of brown and wang ( 2005 ) , it is far from clear why such self - induced smoothing is suitable for the discrete objective function ( rank correlation ) .",
    "in fact , undersmoothing would make the hessian ( second derivative ) unstable while oversmoothing would introduce significant bias . through highly technical and tedious derivations , we will show that the proposed method does strike a right balance in terms of asymptotic unbiasedness and enough smoothness for differentiation ( twice ) .",
    "the rest of the paper is organized as follows . in section 2 ,",
    "the new methods are described and related large sample properties are developed . in particular , we give construction for simultaneous point and variance estimation and show that the resulting point estimator is asymptotically normal and the variance estimator is consistent . in section 3 , the approach , along with the algorithm and large sample properties , is extended to handle survival data with right censoring .",
    "simulation results are reported in section 4 , where application to a real data set is also given .",
    "section 5 contains some concluding remarks .",
    "additional technical proofs can be found in the appendix .",
    "in this section we develop a self - induced smoothing method for the rank correlation criterion function defined by .",
    "it is divided into three subsections , with the first introducing the method and the algorithm , the second establishing large sample properties and the third covering proofs .",
    "since mrc estimator @xmath29 is asymptotically normal ( sherman , 1993 ) , its difference with the true parameter value , @xmath35 , should approximately be a gaussian noise @xmath36 , where @xmath37 is a @xmath21-dimensional normal random vector with mean @xmath38 and covariance matrix @xmath39 .",
    "assume that @xmath40 is independent of data and let @xmath41 denote the expectation with respect to @xmath40 given data .",
    "a self - induced smoothing for @xmath33 is @xmath42 .",
    "the self - induced smoothing using the limiting gaussian distribution was originally proposed by brown and wang ( 2005 ) for certain non - smooth estimating functions .    to get an explicit form for @xmath43 ,",
    "let @xmath44 be the standard normal distribution function , @xmath45 , @xmath46 where @xmath47 denotes the first @xmath21 components of @xmath48 .",
    "then , it is easy to see that @xmath49\\phi\\left(\\sqrt{n}{{\\bf x}}_{ij}'{\\mbox{\\boldmath$\\beta$}}({\\mbox{\\boldmath$\\theta$}})/\\sigma_{ij}\\right).\\ ] ] we shall use @xmath50 to denote the corresponding estimator , which will be called the smoothed maximum rank correlation estimator ( smrce ) . here and in the sequel , @xmath51 denotes the parameter space for @xmath20 .",
    "smoothing is an appealing way for a simple solution to the inference problem associated with the mrce .",
    "if @xmath43 were a usual smooth objective function , then its first derivative would become the score function and its second derivative could be used for variance estimation .",
    "speficically , if we use @xmath52 to denote the limiting variance of the score scaled by @xmath53 and @xmath54 the limit of the second derivative , then the asymptotic variance of the resulting estimator , scaled by @xmath55 , should be of form @xmath56 . a consistent estimator could then be obtained by the plug - in method , i.e. replacing unknown parameters by their corresponding empirical estimators .",
    "it is unclear , however , whether or not the self - induced smooth will provide a right amount of smoothing , even in view of the results given in brown and wang ( 2005 ) . with over - smoothing",
    ", @xmath57 may be asymptotically biased , i.e. the bias is not of order @xmath58 ; with under - smoothing , the `` score '' function ( first derivative of @xmath43 ) may have multiple `` spikes '' and thus the second derivative matrix ( hessian ) of @xmath43 may not behave properly and certainly can not be expected to provide a consistent variance estimator .    in subsection 2.2",
    ", we show that the self - induced smoothing here does result in a right amount of smoothing in the sense that the bias is asymptotically negligible and the hessian matrix behave properly . before starting the theoretic developments ,",
    "we first describe our method .",
    "we first differentiate the smoothed objective function @xmath43 to get score @xmath59 where @xmath60 .",
    "this is a u - process of order 2 with kernel @xmath61 where @xmath62 denotes the pair @xmath63 .    by hoeffding s decomposition ,",
    "the asymptotic variance of @xmath64 is approximated by @xmath65\\right\\}^{\\otimes2},\\\\ \\end{split}\\end{aligned}\\ ] ] where , for a vector @xmath66 , @xmath67 .",
    "thus , @xmath68 is used to estimate @xmath52 , the middle part of the `` sandwich '' variance formula discussed in remark 1 .",
    "as for @xmath54 , we differentiate @xmath69 to get @xmath70^{\\otimes2}\\right\\},\\ ] ] where @xmath71 is the derivative of @xmath72 .",
    "although the self - induced smoothing was motivated earlier with @xmath39 being the limiting covariance matrix of the estimator , we will show later that for any positive definite matrix @xmath39 , @xmath73 converges to @xmath54 .",
    "note that the above discussions about @xmath54 and @xmath52 are not mathematically rigorous .",
    "this is because the kernel function for the score process is sample size @xmath53-dependent .",
    "the usual asymptotic theory for the u - process is not applicable",
    ". indeed , our rigorous derivations , to be given in subsection 2.3 , are quite tedious , involving many approximations that are quite delicate .",
    "let @xmath74 if @xmath20 is the true parameter value , then @xmath75 converges to the limiting covariance matrix , which is the desired choice for @xmath39 in the self - induced smoothing .",
    "therefore , leads to an iterative algorithm of form @xmath76 ; see also brown and wang ( 2005 ) . specifically , we propose the following algorithm :    ( smrce )    1 .",
    "compute the mrc estimator @xmath16 and set @xmath77 to be the identity matrix .",
    "2 .   update variance - covariance matrix @xmath76 .",
    "smooth the rank correlation @xmath78 using covariance matrix @xmath79 .",
    "maximize the resulting smoothed rank correlation to get an estimator @xmath80 .",
    "repeat step 2 until @xmath80 converge .",
    "this subsection is devoted to the large sample theory .",
    "the main results are : 1 .",
    "the smoothed mrc estimator ( smrce ) is asymptotically equivalent to the mrc estimator ; 2 .",
    "the proposed method leads to a consistent variance estimator ; and 3 .",
    "the iterative algorithm for point and variance estimation converges numerically .",
    "we first introduce notation as well as assumptions , which are similar to those in sherman ( 1993 ) for the mrc estimator .",
    "let @xmath81}i_{[({{\\bf x}}-{{\\bf x}})'{\\mbox{\\boldmath$\\beta$}}({\\mbox{\\boldmath$\\theta$}})>0]}+ i_{[y < y]}i_{[({{\\bf x}}-{{\\bf x}})'{\\mbox{\\boldmath$\\beta$}}({\\mbox{\\boldmath$\\theta$}})<0]}\\right],\\ ] ] which is the projection of the kernel of u - process @xmath78 .",
    "the expectation is taken for @xmath82 .",
    "also let @xmath83    the following assumptions 1 and 2 are used in han ( 1987 ) ( see also sherman , 1993 ) to establish consistency for the mrc estimator . for asymptotic normality ,",
    "we need an additional regularity condition ( assumption 3 ) given in sherman ( 1993 ) .",
    "the true parameter value @xmath84 is an interior point of @xmath51 , which is a compact subset of the @xmath21-dimensional euclidean space @xmath85 .",
    "the support of @xmath2 is not contained in any linear subspace of @xmath86 .",
    "conditional on the first @xmath21 components of @xmath2 , the last component of @xmath2 has a density function with respect to the lebesgue measure .",
    "there exists a neighborhood , @xmath87 , of @xmath84 such that for each pair @xmath88 of possible values of @xmath89 ,    \\(i ) the second derivatives of @xmath90 with respect to @xmath20 exist in @xmath91 .",
    "\\(ii ) there is an integrable function @xmath92 such that for all @xmath20 in @xmath91 , @xmath93    \\(iii ) @xmath94    \\(iv ) @xmath95    \\(v ) the matrix @xmath96 is strictly negative definite .",
    "( sherman , 1993 ) assume that assumptions 1 - 3 hold .",
    "we have , uniformly over any @xmath97 neighborhood of @xmath84 , @xmath98 where @xmath99 , @xmath100 and @xmath101 . consequently , for the mrc estimator @xmath16 , @xmath102 where @xmath103 , @xmath104')$ ] and @xmath105 .    because of the standardization , the rank correlation criterion function @xmath33 is bounded by 1 .",
    "it is not difficult to establish a uniform law of large numbers @xmath106 where @xmath107 is the expectation of @xmath78 ; cf .",
    "han ( 1987 ) and sherman ( 1993 ) .",
    "likewise , we can show that such uniform convergence also holds for @xmath43 , i.e. @xmath108 note that the limit @xmath109 remains the same .    in the following theorem",
    ", we claim that the estimate obtained from maximizing the smoothed rank correlation function is also asymptotically normal with the same asymptotic covariance matrix as han s mrce .",
    "[ thy : normality ] for any given positive definite matrix @xmath39 , let @xmath110 be defined as in and @xmath111 . then , under assumptions 1 - 3 , @xmath112 is consistent , @xmath113 a.s . and asymptotically normal , @xmath114 where @xmath115 is defined as in proposition 1 .",
    "in addition , @xmath112 is asymptotically equivalent to @xmath16 in the sense that @xmath116 .",
    "recall that defines the sandwich - type variance estimator by pretending that @xmath117 is a standard smooth objective function .",
    "theorem [ thy : var formula ] below shows that is consistent .",
    "[ thy : var formula ] let @xmath118 be the mrc estimator and @xmath119 be defined by . then , for any fixed positive definite matrix @xmath39 , @xmath120 converges in probability to @xmath115 , the limiting variance - covariance matrix of @xmath121 .",
    "the self - induced smoothing uses the limiting covariance matrix @xmath115 as @xmath39 . in practice",
    ", we may initially choose the identity matrix for @xmath39 , which is the same way as the initial step in algorithm i. by theorem 2.1 , we know that the one - step estimator @xmath122 in algorithm i converges in probability to the true covariance . however , this one - step estimator depends on the initial choice of @xmath39 .",
    "algorithm 1 is an iterative algorithm with the variance - covariance estimator converging to the fixed point of @xmath123 .",
    "convergence of algorithm 1 is ensured by the following theorem . for notational simplicity , we let @xmath124 be the vectorization of matrix @xmath125 . for any function @xmath126 of @xmath39 , @xmath127 where @xmath128 denotes the @xmath129 entry of @xmath39 .",
    "[ thy : alg converg ] let @xmath130 be defined as in algorithm 1 .",
    "suppose that assumptions 1 - 3 hold .",
    "then there exist @xmath131 , @xmath132 , such that for any @xmath133 , there exists @xmath134 , such that for all @xmath135 , @xmath136    for a fixed @xmath53 , @xmath137 represents the fixed point matrix in the iterative algorithm .",
    "the above theorem shows that with probability approaching 1 , the iterative algorithm converges to a limit , as @xmath138 , and the limit converges in probability to the limiting covariance matrix @xmath115 .",
    "the speed of convergence of @xmath130 to @xmath137 is faster than any exponential rate in the sense that @xmath139 for any @xmath140 .",
    "this can be seen from step 2 of algorithm 1 in subsection 2.1 and below , @xmath141_{r , s}=o_p(1),\\ ] ] which will be proved in the appendix . here",
    "@xmath142 is a small neighborhood of @xmath115 and @xmath39 is a positive definite matrix .      in this section",
    ", we provide proofs for ( 1 ) asymptotic equivalence of smrce to mrce , ( 2 ) consistency of the induced variance estimator and ( 3 ) convergence of algorithm 1 . some of the technical developments used in the proofs will be given in the appendix .",
    "[ proof of theorem [ thy : normality ] ] without loss of generality , we assume @xmath143 as in subsection 2.1 , let @xmath40 be a @xmath21-variate normal random vector with mean @xmath144 and covariance matrix @xmath39 .",
    "define @xmath145 let @xmath146 and @xmath147 . define @xmath148 = { \\rm argmax}_{{\\mbox{\\boldmath$\\theta$ } } } { \\widetilde\\gamma_n}({\\mbox{\\boldmath$\\theta$}}).\\ ] ] let @xmath149 $ ] , where @xmath150 then @xmath151 due to the gaussian tail of @xmath40 .",
    "since @xmath152 and @xmath153 , @xmath154\\}|\\leq   p({\\mbox{\\boldmath$\\omega$}}_n)=o(n^{-2}).\\ ] ]    by the cauchy - schwarz inequality , @xmath155\\}=o(n^{-2 } ) \\mbox { and } e_{{{\\bf z } } } \\{|{{\\bf z}}|^2 i[{\\mbox{\\boldmath$\\omega$}}_n]\\}=o(n^{-2}).\\ ] ]    by , uniformly over @xmath156 neighborhoods of @xmath144 , @xmath157\\ } = ( 1/2)e_{{{\\bf z } } } \\{({\\mbox{\\boldmath$\\theta$}}+{{\\bf z}}/\\sqrt{n})'{{\\bf a}}_0({\\mbox{\\boldmath$\\theta$}}+{{\\bf z}}/\\sqrt{n})i[{\\mbox{\\boldmath$\\omega$}}_n^c]\\ } \\\\",
    "& + ( 1/\\sqrt{n})e_{{{\\bf z } } } \\{({\\mbox{\\boldmath$\\theta$}}+{{\\bf z}}/\\sqrt{n})'{\\bf w}_ni[{\\mbox{\\boldmath$\\omega$}}_n^c]\\ } + o_p(e_{{{\\bf z } } } \\{|{\\mbox{\\boldmath$\\theta$}}+{{\\bf z}}/\\sqrt{n}|^2i[{\\mbox{\\boldmath$\\omega$}}_n^c]\\}+\\frac{1}{n}).\\end{aligned}\\ ] ]    note that @xmath158\\ } \\leq 2 ( e_{{{\\bf z}}}|{\\mbox{\\boldmath$\\theta$}}|^2 + e_{{{\\bf z}}}|{{\\bf z}}|^2/n)=o(|{\\mbox{\\boldmath$\\theta$}}|^2 + 1/n).\\ ] ]    therefore , uniformly over @xmath156 neighborhoods of @xmath144 , we have @xmath159 replacing @xmath20 in with @xmath160 and subtracting it from @xmath161 , we have @xmath162 combining ( 15 ) with lemma 1 in the appendix , we get , @xmath163 therefore , from and , we have @xmath164 finally , strong consistency of @xmath112 follows the uniform almost sure convergence of @xmath165 as stated in .",
    "this completes the proof .    for notational simplicity ,",
    "we assume throughout the proof that @xmath39 is the identity matrix .",
    "the same argument with modifications to include constants for up and lower bound may be applied to deal with a general covariance matrix @xmath39 .",
    "we first show @xmath166 by definition , @xmath167_{r , s}=\\partial^2\\tilde q_n({\\mbox{\\boldmath$\\theta$}})/(\\partial\\theta_r\\partial\\theta_s)$ ] .",
    "as defined in @xmath168 has the following integral representation , @xmath169 by change of variable @xmath170 , @xmath171    @xmath172 . from , @xmath173 and @xmath174 where @xmath175 and @xmath176 .",
    "in view of , to show , it suffices to prove @xmath177_{r , s } + o_p(1)\\ ] ] uniformly over @xmath178 . to show",
    ", we define @xmath179    by lemma 2(i ) and the boundedness of @xmath180 , we have , @xmath181 where @xmath182 for set @xmath183 denotes its complement .",
    "therefore , reduces to @xmath184_{r , s } + o_p(1).\\ ] ]    to show , we establish a quadratic expansion of @xmath180 for @xmath185 . since @xmath186 for @xmath185 and @xmath187 , it follows that @xmath188 .",
    "therefore , by , @xmath189 therefore , the left hand side of equals @xmath190 , where @xmath191\\ddot{k}_{n , r , s}({{\\bf t}},{\\mbox{\\boldmath$\\theta$}})d{{\\bf t}},\\\\ \\bf{ii}&=q_n({\\mbox{\\boldmath$\\theta$}}_0)\\times\\int_{{\\mbox{\\boldmath$\\omega$}}_{n , r}\\cap{\\mbox{\\boldmath$\\omega$}}_{n , s}}\\ddot{k}_{n , r , s}({{\\bf t}},{\\mbox{\\boldmath$\\theta$}})d{{\\bf t}},\\\\ \\bf{iii}&=\\frac{{\\bf w}_n'}{\\sqrt{n}}\\times\\int_{{\\mbox{\\boldmath$\\omega$}}_{n , r}\\cap{\\mbox{\\boldmath$\\omega$}}_{n , s}}({{\\bf t}}-{\\mbox{\\boldmath$\\theta$}}_0)\\ddot{k}_{n , r , s}({{\\bf t}},{\\mbox{\\boldmath$\\theta$}})d{{\\bf t}},\\\\ \\bf{iv}&=\\frac{1}{2}\\int_{{\\mbox{\\boldmath$\\omega$}}_{n , r}\\cap{\\mbox{\\boldmath$\\omega$}}_{n , s}}({{\\bf t}}-{\\mbox{\\boldmath$\\theta$}}_0)'{{\\bf a}}({\\mbox{\\boldmath$\\theta$}}_0)({{\\bf t}}-{\\mbox{\\boldmath$\\theta$}}_0)\\ddot{k}_{n , r , s}({{\\bf t}},{\\mbox{\\boldmath$\\theta$}})d{{\\bf t}}. \\end{split}\\end{aligned}\\ ] ]    by the definition of @xmath192 , @xmath193 by lemma 2(ii ) , @xmath194 .",
    "furthermore , @xmath195 due to lemma 2(iii ) .",
    "note that @xmath196 where the last equality follows from the fact that @xmath192 and @xmath197 are symmetric at @xmath20 and @xmath198 is an odd function of @xmath199_{r}$ ] for @xmath200 . combining this with lemma 2(i ) , we have @xmath201 .",
    "again by symmetry , @xmath202 by lemma 2 ( i ) and ( iv ) , @xmath203_{r , s } + o(n^{-1/2})$ ] . combining the approximations for @xmath204",
    ", we get .",
    "next we prove @xmath205 by showing , componentwise , @xmath206_{r , s } = [ { { \\bf v}}({\\mbox{\\boldmath$\\theta$}}_0)]_{r , s } + o_p(1)\\ ] ] uniformly over @xmath178 for @xmath207 .",
    "define    @xmath208}i_{[({{\\bf x}}-\\tilde { { \\bf x}})'{\\mbox{\\boldmath$\\beta$}}>0 ] } + i_{[y<\\tilde y]}i_{[({{\\bf x}}-\\tilde { { \\bf x}})'{\\mbox{\\boldmath$\\beta$}}<0]},$ ]    where @xmath209 and @xmath210 .",
    "in addition , let @xmath211 where @xmath212 is the empirical distribution for @xmath213 s . by definition , @xmath214_{r , s } & = \\frac{1}{n}\\sum_{i=1}^n \\left[\\frac{\\partial}{\\partial\\theta_r } \\int \\tau_n({{\\bf u}}_i,{\\mbox{\\boldmath$\\theta$}}+\\frac{{{\\bf z}}}{\\sqrt{n } } ) ( 2\\pi)^{-\\frac{d}{2}}e^{-\\frac{\\|{{\\bf z}}\\|_2 ^ 2}{2}}d{{\\bf z}}\\right]\\\\ & \\quad \\quad \\times \\left[\\frac{\\partial}{\\partial\\theta_s } \\int \\tau_n({{\\bf u}}_i,{\\mbox{\\boldmath$\\theta$}}+\\frac{\\tilde { { \\bf z}}}{\\sqrt{n } } ) ( 2\\pi)^{-\\frac{d}{2}}e^{-\\frac{\\|\\tilde { { \\bf z}}\\|_2 ^ 2}{2}}{d\\tilde{{\\bf z}}}\\right ] .",
    "\\end{split}\\end{aligned}\\ ] ]    letting @xmath170 and @xmath215 , we have @xmath214_{r , s } & = \\frac{1}{n}\\sum_{i=1}^n \\frac{\\partial}{\\partial\\theta_r } \\int \\tau_n({{\\bf u}}_i,{{\\bf t}})k_n({{\\bf t}},{\\mbox{\\boldmath$\\theta$}})d{{\\bf t}}\\times   \\frac{\\partial}{\\partial\\theta_r } \\int \\tau_n({{\\bf u}}_i,{\\mbox{\\boldmath$\\omega$ } } ) k_n({\\mbox{\\boldmath$\\omega$}},\\theta)d{\\mbox{\\boldmath$\\omega$}}\\\\ & = \\frac{1}{n}\\sum_{i=1}^n \\int \\tau_n({{\\bf u}}_i,{{\\bf t}})\\dot{k}_{n , r}({{\\bf t}},{\\mbox{\\boldmath$\\theta$}})d{{\\bf t}}\\times      \\int \\tau_n({{\\bf u}}_i,{\\mbox{\\boldmath$\\omega$}})\\dot{k}_{n , s}({\\mbox{\\boldmath$\\omega$}},{\\mbox{\\boldmath$\\theta$}})d{\\mbox{\\boldmath$\\omega$}}\\\\ & = \\int g_n({{\\bf t}},{\\mbox{\\boldmath$\\omega$ } } ) \\dot{k}_{n , r}({{\\bf t}},{\\mbox{\\boldmath$\\theta$}})\\dot{k}_{n , s}({\\mbox{\\boldmath$\\omega$}},{\\mbox{\\boldmath$\\theta$}})d{{\\bf t}}d{\\mbox{\\boldmath$\\omega$ } } , \\end{split}\\end{aligned}\\ ] ] where @xmath216 , which is bounded by 0 and 1 . by lemma 2 ( vii ) ,",
    "@xmath217_{r , s}&=o(n^{-\\frac{1}{2}})\\\\ & + \\int_{{\\mbox{\\boldmath$\\omega$}}_{n , r}\\times{\\mbox{\\boldmath$\\omega$}}_{n , s } } g_n({{\\bf t}},{\\mbox{\\boldmath$\\omega$ } } ) \\dot{k}_{n , r}({{\\bf t}},{\\mbox{\\boldmath$\\theta$}})\\dot{k}_{n , s}({\\mbox{\\boldmath$\\omega$}},{\\mbox{\\boldmath$\\theta$}})d{{\\bf t}}d{\\mbox{\\boldmath$\\omega$}}\\end{split}\\end{aligned}\\ ] ] uniformly over @xmath218 .",
    "let @xmath219 and @xmath220 , the symmetrized @xmath221 . by definition , @xmath222 clearly @xmath223 is a third - order u - statistics and @xmath224 is a second - order u - statistics . applying hoeffding s decomposition ( van der vaart , 1998 , section 12.3 ) , @xmath225 where @xmath226 is a u - statistics of order @xmath227 ( @xmath228 ) and defined as @xmath229.\\ ] ] here , adopting the notations from van der vaart ( 1998 , section 11.4 ) , we define @xmath230 $ ] as a projection of @xmath231 such that @xmath232 - e f^ * , \\\\",
    "p_{\\{i , j\\ } } f^ * & = e [ f^ * | { { \\bf u}}_i , { { \\bf u}}_j ] - e [ f^ * | { { \\bf u}}_i ] - e [ f^ * | { { \\bf u}}_j ] + e f^*,\\\\ p_{\\{1,2,3\\ } } f^ * & = e [ f^ * | { { \\bf u}}_1 , { { \\bf u}}_2 , { { \\bf u}}_3 ] - \\sum_{i\\neq j } e [ f^ * | { { \\bf u}}_i , { { \\bf u}}_j ] + \\sum_{i=1,2,3}e [ f^ * | { { \\bf u}}_i ] - e f^*. \\end{split}\\end{aligned}\\ ] ] we know from hoeffding s decomposition that @xmath233 and @xmath234 are second- and third - order degenerated u - statistics with bounded kernels and thus of order @xmath235 and @xmath236 ; see sherman ( 1992 , corollary 8) . therefore , by lemma 2(vi ) , @xmath237 replacing @xmath226 by @xmath238 in also results in @xmath97 .",
    "then combining this and with and , reduces to @xmath239_{r , s } & = 3 \\times\\int_{{\\mbox{\\boldmath$\\omega$}}_{n , r}\\cap{\\mbox{\\boldmath$\\omega$}}_{n , s } }      u_{n,1}\\times\\dot{k}_{n , r}({{\\bf t}},{\\mbox{\\boldmath$\\theta$}})\\dot{k}_{n , s}({\\mbox{\\boldmath$\\omega$ } } , { \\mbox{\\boldmath$\\theta$}})d{{\\bf t}}d{\\mbox{\\boldmath$\\omega$}}\\\\ & + \\int_{{\\mbox{\\boldmath$\\omega$}}_{n , r}\\cap{\\mbox{\\boldmath$\\omega$}}_{n , s } } ef\\times\\dot{k}_{n , r}({{\\bf t}},{\\mbox{\\boldmath$\\theta$}})\\dot{k}_{n ,",
    "s}({\\mbox{\\boldmath$\\omega$}},{\\mbox{\\boldmath$\\theta$}})d{{\\bf t}}d{\\mbox{\\boldmath$\\omega$}}+ o_p(1 ) . \\end{split}\\ ] ] let @xmath240 $ ] , @xmath241 $ ] and @xmath242 $ ] .",
    "we define @xmath243 . by the definitions of @xmath244 and @xmath245",
    ", we have @xmath246 . by lemma 3 and applying integration by parts twice , @xmath247 where @xmath248 . by lemma 3 ,    @xmath249_{r , s}+o_p(1)\\ ] ] uniformly over @xmath250",
    ". therefore , @xmath251_{r , s}+o_p(1).\\ ] ]    similarly , applying integration by parts and by lemma 3 and 2(vi ) , we have @xmath252_{r , s}+o_p(1),\\ ] ] @xmath253_{r , s}+o_p(1),\\ ] ] @xmath254\\times \\dot{k}_{n , r}({{\\bf t}},{\\mbox{\\boldmath$\\theta$}})\\dot{k}_{n , s}({\\mbox{\\boldmath$\\omega$}},{\\mbox{\\boldmath$\\theta$}})d{{\\bf t}}d{\\mbox{\\boldmath$\\omega$}}= [ { { \\bf v}}({\\mbox{\\boldmath$\\theta$}}_0)]_{r , s}+o_p(1).\\ ] ] hence the right hand side of is @xmath255_{r , s}+o_p(1)$ ] , which gives . from and ,",
    "@xmath256 .",
    "from theorem , we know that @xmath257 and @xmath258 . by the mean value theorem , @xmath259_{r , s } & = [ \\hat{\\mbox{\\boldmath$\\sigma$}}_n(\\hat{\\mbox{\\boldmath$\\theta$}}_n , \\hat{{\\mbox{\\boldmath$\\sigma$}}}_n^{(1 ) } ) - \\hat{\\mbox{\\boldmath$\\sigma$}}_n(\\hat{\\mbox{\\boldmath$\\theta$}}_n , { { \\bf d}}_0)]_{r , s }      + [ \\hat{\\mbox{\\boldmath$\\sigma$}}_n(\\hat{\\mbox{\\boldmath$\\theta$}}_n , { { \\bf d}}_0 ) - { { \\bf d}}_0]_{r , s}\\\\ & = \\left[\\frac{\\partial}{\\partial{\\mbox{\\boldmath$\\sigma$}}}[\\hat{{{\\bf d}}}_n]_{r , s}\\bigg|_{{\\mbox{\\boldmath$\\sigma$}}={\\mbox{\\boldmath$\\sigma$}}^*}\\right ] '      \\times vech(\\hat{{\\mbox{\\boldmath$\\sigma$}}}_n^{(1)}-{{\\bf d}}_0 ) + [ \\hat{\\mbox{\\boldmath$\\sigma$}}_n(\\hat{\\mbox{\\boldmath$\\theta$}}_n , { { \\bf d}}_0 ) - { { \\bf d}}_0]_{r , s } , \\end{split}\\end{aligned}\\ ] ] where @xmath260 and thus @xmath261 . in view of lemma 4 and @xmath258 , @xmath262 . again by the mean value theorem , @xmath263_{r , s}= \\left[\\frac{\\partial}{\\partial { \\mbox{\\boldmath$\\sigma$}}}[\\hat { { { \\bf d}}}_n]_{r , s}\\bigg|_{{\\mbox{\\boldmath$\\sigma$}}={\\mbox{\\boldmath$\\sigma$}}^*}\\right ] ' \\times vech(\\hat{{\\mbox{\\boldmath$\\sigma$}}}_n^{(k)}-{{\\bf d}}_0),\\ ] ] where @xmath264 . then by lemma 4 and mathematical induction ,",
    "we know that for any @xmath133 and @xmath140 , there exist @xmath265 and @xmath134 , such that for any @xmath135 and @xmath266 , @xmath267_{r , s}\\big| \\leq \\eta \\times \\big|[\\hat{\\mbox{\\boldmath$\\sigma$}}_n^{(k)}-\\hat{\\mbox{\\boldmath$\\sigma$}}_n^{(k-1)}]_{r , s}\\big| , \\mbox { for all } k > k\\right)>1-\\epsilon,\\ ] ] where @xmath268 .",
    "note that the inequality inside the above probability implies that @xmath79 converges as @xmath269 and the limit @xmath137 satisfies @xmath270 and @xmath271 .",
    "in this section , we extend the approach to the partial rank correlation ( prc ) criterion function @xmath272 , defined by ( 3 ) , of khan and tamer ( 2007 ) for censored data . under the usual conditional independence between failure and censoring times given covariates and additional regularity conditions , khan and tamer ( 2007 ) developed asymptotic properties for prce that are parallel to those by sherman ( 1993 ) .",
    "the same self - induced smoothing can be applied to partial rank correlation criteria function to get    @xmath273\\phi\\left(\\sqrt{n}{{\\bf x}}_{ij}'{\\mbox{\\boldmath$\\beta$}}({\\mbox{\\boldmath$\\theta$}})/\\sigma_{ij}\\right ) .",
    "\\end{split}\\end{aligned}\\ ] ]    we define its maximizer , @xmath274 , as the smoothed partial rank correlation estimator ( sprce ) .",
    "let @xmath275^{\\otimes2}\\right\\},\\ ] ] @xmath276\\right\\}^{\\otimes2 } , \\\\",
    "\\end{split}\\end{aligned}\\ ] ] @xmath277^{-1}\\times \\hat { { \\bf v}}_n^*({\\mbox{\\boldmath$\\theta$ } } , { \\mbox{\\boldmath$\\sigma$}})\\times [ \\hat { { \\bf a}}^*_n({\\mbox{\\boldmath$\\theta$ } } , { \\mbox{\\boldmath$\\sigma$}})]^{-1},\\ ] ] where @xmath278-\\delta_i \\times i[\\tilde y_j > \\tilde y_i]$ ] .    based on @xmath279 , we have the following iterative algorithm to compute the sprce and variance estimate simultaneously .",
    "( sprce )    1 .",
    "compute the prc estimator @xmath280 and set @xmath77 to be the identity matrix .",
    "2 .   update variance - covariance matrix @xmath281 .",
    "smooth the partial rank correlation @xmath282 using covariance matrix @xmath283 . maximize the resulting smoothed partial rank correlation to get an estimator @xmath284 .",
    "repeat step 2 until @xmath284 converge .",
    "in addition to assumptions 1 - 3 , khan and tamer ( 2007 ) added the following assumption for the consistency of prce .",
    "let @xmath285 be the support of @xmath286 , and @xmath287 be the set @xmath288 then @xmath289 .",
    "similar to the rank correlation function , it can be shown that under assumptions 1 - 4 , and still hold for partial rank correlation function @xmath290 .",
    "therefore , theorems 1 - 3 in section 2 continue to hold when replacing the point and variance estimators for smoothed rank correlation by the corresponding ones for the smoothed partial rank correlation .",
    "specifically , for any positive definite matrix @xmath39 , under assumptions 1 - 4 , we have    1 .",
    "the sprce @xmath291 is asymptotically equivalent to the prce @xmath27 in the sense that @xmath292 , and , therefore , @xmath293 where @xmath294 is the limiting variance - covariance matrix of @xmath27 .",
    "variance estimator is consistent : @xmath295 .",
    "algorithm 2 converges numerically in the sense that there exist @xmath131 , @xmath132 , such that for any @xmath133 , there exists @xmath134 , such that for all @xmath135 , @xmath296    the proofs are similar to those of theorems 1 - 3 in section 2 , and are , therefore , omitted .",
    "in this section , we first apply the proposed self - induced smoothing method to analyze the primary biliary cirrhosis ( pbc ) data ( fleming and harrington , 1990 , appendix d ) and compare the result with that using the cox regression .",
    "we then report results from several simulation studies we conducted using the method .",
    "we applied smoothed prce to the survival times of the first 312 subjects with no missing covariates in the pbc data .",
    "we included two covariates albumin and age50 ( age divided by 50 ) .",
    "we reparameterized the transformation model ( 1 ) by setting @xmath297 as 1 , and estimated @xmath298 by sprce .",
    "we also calculated prce for @xmath298 and fitted the standard cox model .",
    "for the cox regression , the ratio @xmath299 is the estimate of @xmath298 .",
    "the results are summarized in table 1 .",
    "table 1 + regression analysis of pbc data    [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]      we conducted simulation studies for a number of cases . in the first case ( design i )",
    ", we generated @xmath2 from a bivariate normal distribution with mean @xmath300'$ ] and a covariance matrix diag@xmath301 .",
    "we set @xmath302 $ ] and generated @xmath303 from the probability density function @xmath304 .",
    "we set the transformation @xmath305 as @xmath306 .",
    "this is indeed a weibull proportional hazard model .",
    "the sample sizes were @xmath307 and the numbers of replications were @xmath308 .",
    "the smrce , mrce and cox model were used to estimate @xmath309 , and the standard error of smrce was computed by algorithm 1 .",
    "the mean(mean ) , bias(bias ) and root mean square error(rmse ) for each method as well as mean of standard error(se ) and coverage of @xmath310 confidence interval for the smrce are reported in table 2 .",
    "the second case ( design ii ) is similar to the first one except that @xmath0 is censored by a random variable @xmath23 , which is independent of @xmath2 and normally distributed with mean @xmath311 and variance @xmath312 .",
    "the sample sizes were @xmath313 and the numbers of replications were @xmath308 .",
    "this design is similar to that in grgens and horowitz ( 1999 ) .",
    "the sprce , prce and cox model were used to estimate @xmath309 , and the standard error of sprce was computed by algorithm 2 .",
    "the resulting estimates are summarized in table 3 where we also report bias(bias ) , root mean square error(rmse ) , mean of standard error(se ) , and coverage of @xmath310 confidence interval .    in the third case ( design iii ) , we generated @xmath314'$ ] by two steps .",
    "we first generated @xmath315'$ ] from a bivariate normal distribution with mean @xmath316'$ ] and an identity covariance matrix .",
    "we then generated @xmath317 as 0 or 2 with equal probability .",
    "we set @xmath318 $ ] and generated @xmath303 from a normal distribution with @xmath319 and @xmath312 .",
    "we set the transformation @xmath320 .",
    "the sample sizes were @xmath321 and the numbers of replications were @xmath308 .",
    "the smrce , mrce and least squared method were applied to estimate @xmath322 and @xmath323 , and the standard error of smrce was computed by algorithm 1 .",
    "table 4 reports the mean ( mean ) , bias ( bias ) and root mean square error ( rmse ) for each method as well as mean of standard error ( se ) and coverage of @xmath310 confidence interval for the smrce .    from tables 2 , 3 and 4",
    ", we find that ( 1 ) the root mean squared error is close to the mean standard error for the smrce ( sprce ) ; ( 2 ) as the sample size increases , the bias reduces and the coverage of @xmath310 confidence interval converges to the nominal level . these show that the proposed variance estimator is accurate and algorithms 1 and 2 work well",
    "this paper provides a simple yet general recipe for smoothing the discontinuous rank correlation criteria function .",
    "the smoothing is self - induced in the sense that the implied bandwidth is essentially the asymptotic standard deviation of the regression parameter estimator .",
    "it is shown that such smoothing does not introduce any significant bias in that the resulting estimator is asymptotically equivalent to the original maximum rank correlation estimator , which is asymptotically normal .",
    "the smoothed rank correlation can be used as if it were a regular smooth criterion function in the usual m - estimation problem , in the sense that the standard sandwich - type plug - in variance - covariance estimator is consistent .",
    "simulation and real data analysis provide additional evidence that the proposed method gives the right amount of smoothing .",
    "because of the family of transformation models contains both the proportional hazards and accelerated failure time models as its submodels , the new approach may be used for model selection .",
    "the specification test commonly used in the econometrics literature ( hausman , 1978 ) may also be used for testing a specific semiparametric model assumption .",
    "in addition , the smoothed objective function also makes it possible to fit a penalized regression by introducing a lasso - type penalty .",
    "the method and theory developed herein can easily be extended to other problems of similar nature , i.e. discontinous objective functions with associated estimators being asymptotically normal .",
    "in particular , we can apply the self - induced smoothing to the estimator introduced by chen ( 2002 ) for the transformation function @xmath6 to obtain a consistent variance estimator .",
    "lemma 1 below is due to sherman ( 1993 , theorem 2 ) .",
    "we denote @xmath324 as general objective functions which are centered and satisfies the same regularity conditions as in sherman ( 1993 ) .",
    "suppose @xmath325 is consistent for @xmath84 , an interior point of @xmath51 .",
    "suppose also that uniformly over @xmath97 neighborhoods of @xmath84 ,    @xmath326 where @xmath54 is a negative definite matrix , and @xmath327 converges in distribution to a @xmath328 random vector .",
    "then @xmath329    recall in theorem 2 , we define @xmath330 and its first and second partial derivatives with respect to @xmath20 as @xmath331 also recall @xmath332 then we have the following lemma .    uniformly over @xmath333 ,    \\(i ) @xmath334 + ( ii)@xmath335 . + ( iii)@xmath336",
    ". + ( iv ) @xmath337_{r , s}+o(n^{-1}).$ ] + ( v)@xmath338 .",
    "+ ( vi)@xmath339 .",
    "+ ( vii)@xmath340 + @xmath341 + @xmath342    let @xmath343 , and divide its complement into @xmath344 and @xmath345 . we prove ( i)-(iv ) for @xmath346 . for @xmath347 ,",
    "the proofs are similar and omitted .    for ( i ) , note that @xmath348    since @xmath349 and @xmath350\\geq0 $ ] , @xmath351 where the last equality follows from @xmath352 .",
    "similarly , @xmath353    for ( ii ) , by definition , @xmath354 \\bigg|_{t_r=1/\\sqrt{n}}^{2\\sqrt{\\frac{\\log n}{n } } } + \\left[n t_r e^{-\\frac{nt_r^2}{2}}\\right ] \\bigg|_{t_r=1/\\sqrt{n}}^{0}\\right ) = o(1 ) , \\end{split}\\end{aligned}\\ ] ] where the inequality follows from @xmath355 .    for ( iii ) , by definition , @xmath356 + @xmath357 , where the last equality follows from @xmath352 .",
    "iv ) , by definition and applying integration by parts twice , @xmath358 where @xmath359 with @xmath360 entry being 1 , and the last equality follows from the gaussian tail probability .    for ( v ) , we know , by definition , @xmath361 . by symmetry , @xmath362 where the inequality follows from @xmath352",
    "similarly , @xmath363    for ( vi ) , by definition , @xmath364 where the second equality is due to symmetry and the third equality follows from @xmath352 .    to prove ( vii ) , without loss of generality , we assume @xmath365 . we denote @xmath366 as @xmath192 and @xmath367 its complement .",
    "then , @xmath368 where @xmath369 and @xmath370 are chosen from @xmath371 . then by ( v ) and ( vi ) , @xmath372    uniformly over @xmath373 such that @xmath374 and @xmath375 , we have @xmath376 = e\\left[\\frac{\\partial\\tau({{\\bf u}},{\\mbox{\\boldmath$\\theta$}}_0)}{\\partial\\theta_r } \\tau({{\\bf u}},{\\mbox{\\boldmath$\\theta$}}_0)\\right]+o_p(1),\\ ] ] @xmath377 = e\\left[\\frac{\\partial\\tau({{\\bf u}},{\\mbox{\\boldmath$\\theta$}}_0)}{\\partial\\theta_r } \\frac{\\partial\\tau({{\\bf u}},{\\mbox{\\boldmath$\\theta$}}_0)}{\\partial\\theta_s}\\right]+o_p(1).\\ ] ]    we sketch the main steps of the proof below .",
    "first of all , observe that @xmath378 & -\\frac{1}{n}\\sum_{i=1}^n \\left[\\frac{\\partial\\tau({{\\bf u}}_i,{\\mbox{\\boldmath$\\theta$}}_0)}{\\partial\\theta_r}\\tau({{\\bf u}}_i,{\\mbox{\\boldmath$\\omega$}})\\right]\\bigg|           \\\\ & \\leq\\frac{1}{n}\\sum_{i=1}^n \\left[\\bigg|\\frac{\\partial\\tau({{\\bf u}}_i,{{\\bf t}})}{\\partial\\theta_r } -\\frac{\\partial\\tau({{\\bf u}}_i,{\\mbox{\\boldmath$\\theta$}}_0)}{\\partial\\theta_r}\\bigg| \\times",
    "\\big|\\tau({{\\bf u}}_i,{\\mbox{\\boldmath$\\omega$}})\\big|\\right ]     \\\\ & \\leq \\frac{1}{n}\\sum_{i=1}^n m_2({{\\bf u}}_i)\\times|{{\\bf t}}-{\\mbox{\\boldmath$\\theta$}}_0| , \\end{split}\\end{aligned}\\ ] ] where @xmath379 is an integrable function .",
    "the last inequality is due to assumption 3 and @xmath380 .",
    "since @xmath379 is integrable , by the law of large numbers , the left hand side of above inequality is thus @xmath97 . by a similar argument",
    ", we can show that + @xmath381= \\frac{1}{n}\\sum_{i=1}^n \\left[\\frac{\\partial\\tau({{\\bf u}}_i,{\\mbox{\\boldmath$\\theta$}}_0)}{\\partial\\theta_r } \\tau({{\\bf u}}_i,{\\mbox{\\boldmath$\\theta$}}_0)\\right]+o_p(1).\\ ] ] by the law of large numbers , we get .",
    "the proof of is similar .",
    "let @xmath382 and @xmath383 be the same as those in .",
    "then , for @xmath384 , we have @xmath385_{r , s}=o_p(1),\\ ] ] @xmath386_{r , s}=o_p(1),\\ ] ] where @xmath142 is a small neighborhood of @xmath115 and @xmath39 is a positive definite matrix .",
    "we now extend the definition of kernels in lemma 2 for any covariance matrix @xmath39 as follows , @xmath387 where @xmath388 is the determinant of @xmath39 .",
    "then the first and second derivatives of @xmath389 with respect to @xmath20 become @xmath390{\\bf e_s}\\\\ & \\qquad\\qquad\\qquad\\qquad\\qquad\\times e^{-\\frac{n}{2}({{\\bf",
    "t}}-\\theta)'{\\mbox{\\boldmath$\\sigma$}}^{-1}({{\\bf t}}-\\theta)}. \\end{split}\\end{aligned}\\ ] ]    we partition @xmath85 into @xmath192 and its complement @xmath391 , where @xmath392",
    ". furthermore , we define @xmath393    note that @xmath394 is bounded for @xmath395 .",
    "similar to the proofs of theorem 2 and lemma 2 , we can get @xmath396_{r , s}}{\\partial { \\mbox{\\boldmath$\\sigma$}}}= [ { { \\bf a}}({\\mbox{\\boldmath$\\theta$}}_0)]_{r , s}\\int_{\\tilde{\\mbox{\\boldmath$\\omega$}}_{n , r } } \\frac{\\partial k_n({{\\bf t}},{\\textbf 0},{\\mbox{\\boldmath$\\sigma$}})}{\\partial { \\mbox{\\boldmath$\\sigma$}}}d{{\\bf t}}+o_p(1),\\ ] ] @xmath397_{r , s}}{\\partial { \\mbox{\\boldmath$\\sigma$}}}= [ { { \\bf v}}({\\mbox{\\boldmath$\\theta$}}_0)]_{r , s}\\int_{\\tilde{\\mbox{\\boldmath$\\omega$}}_{n , r } } \\frac{\\partial k_n({{\\bf t}},{\\textbf 0},{\\mbox{\\boldmath$\\sigma$}})}{\\partial { \\mbox{\\boldmath$\\sigma$}}}d{{\\bf t}}+o_p(1),\\ ] ] uniformly over @xmath398 such that @xmath399 and @xmath400 .    likewise , we have @xmath401 , which , combined with @xmath402 , implies @xmath403 this completes the proof .    for @xmath384",
    ", we have +    @xmath404_{r , s}=o_p(1)$ ] .    first , by theorem 2 , lemma 4 and the mean value theorem",
    ", we can show that @xmath405_{r , s } = [ \\hat{{{\\bf a}}}_n({\\mbox{\\boldmath$\\theta$ } } , { \\mbox{\\boldmath$\\sigma$}})-\\hat{{{\\bf a}}}_n({\\mbox{\\boldmath$\\theta$ } } ,   { { \\bf d}}_0 ) + \\hat{{{\\bf a}}}_n({\\mbox{\\boldmath$\\theta$ } } ,   { { \\bf d}}_0)]_{r , s}=[{{\\bf a}}({\\mbox{\\boldmath$\\theta$}}_0)]_{r , s}+o_p(1)$ ] . by matrix differentiation , @xmath406 .",
    "thus @xmath407 , where @xmath101 . the rest of the proof is straightforward and thus omitted .",
    "[ thy : fast convergence ] for @xmath408 , we have @xmath409_{r , s}=o_p(1).\\ ] ]    the result follows immediate from lemma 4 and corollary 1 .",
    "suppose @xmath221 is the joint density for @xmath410 and @xmath411 is the conditional density of @xmath412 given @xmath413 and @xmath414 .",
    "suppose @xmath415 is the conditional density of @xmath416 given @xmath414 and @xmath417 is the conditional density of @xmath416 given @xmath413 and @xmath414 .",
    "by change of variable , @xmath418 . therefore ,          where @xmath423 is the marginal distribution of @xmath0 .",
    "therefore if the conditional density @xmath424 has bounded derivatives up to order three for each @xmath425 in the support of space @xmath426 , it is not difficult to show that assumption 3 is satisfied .",
    "the sufficient condition can be easily verified in certain common situations such as when the conditional density @xmath427 is normal ."
  ],
  "abstract_text": [
    "<S> this paper deals with a general class of transformation models that contains many important semiparametric regression models as special cases . </S>",
    "<S> it develops a self - induced smoothing for the maximum rank correlation estimator , resulting in simultaneous point and variance estimation . </S>",
    "<S> the self - induced smoothing does not require bandwidth selection , yet provides the right amount of smoothness so that the estimator is asymptotically normal with mean zero ( unbiased ) and variance - covariance matrix consistently estimated by the usual sandwich - type estimator . </S>",
    "<S> an iterative algorithm is given for the variance estimation and shown to numerically converge to a consistent limiting variance estimator . </S>",
    "<S> the approach is applied to a data set involving survival times of primary biliary cirrhosis patients . </S>",
    "<S> simulations results are reported , showing that the new method performs well under a variety of scenarios .    ,    , + </S>"
  ]
}