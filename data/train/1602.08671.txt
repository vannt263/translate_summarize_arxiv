{
  "article_text": [
    "recurrent neural networks ( rnns ) are powerful devices that , unlike conventional neural networks , are able to keep state across time .",
    "they achieved great results in diverse fields like machine translation @xcite , speech recognition @xcite , image captioning @xcite , and many others .",
    "however , despite such advances , traditional rnns still have trouble maintaining memory for long periods of time , presenting an obstacle to attaining human - like general intelligence . following the pioneering work of graves et al .",
    "@xcite and weston et al .",
    "@xcite , researchers have studied many variations of external memories equipped to rnns or explicit memory structures which ameliorate the problem discussed above and obtained great results in applications like question answering @xcite , algorithm learning @xcite , machine translation @xcite , and others . in this paper",
    "we propose a new variation of external memory . in a conventional ram used in personal computers ,",
    "memory is stored at integer addresses , and access is either random or sequential . here",
    "we replace the integers with @xmath0 , and to retrieve memory , the controller can either issue a brand new address or `` drag '' the previous address in some chosen `` direction '' ( formally , apply a lie group action to the previous address ) .",
    "the former is the analog of random access , and the latter is the analog of sequential access .",
    "we call the latter `` lie access , '' with the meaning parametrized by a lie group @xmath4 which specifies how this `` dragging '' is to be done .",
    "we call a model built around this concept of `` lie access '' a lie access neural turing machine , or lantm .",
    "we give two specific implementations in section [ lieaccessmemory ] and explore them in section [ experiments ] with several experiments .",
    "while we will refer to these implementations also as lantms , we want to stress they are certainly not the only ways of instantiating the `` lie access '' concept .",
    "we assume the reader has a basic knowledge of groups and group actions and the passing notion that lie groups are just groups with `` differentiable '' operations .",
    "such a background should enable one to understand the rest of this paper other than section [ generalization ] .",
    "we defer readers who need slightly more exposition on these topics to appendix [ liegroups ] .",
    "unlike the conventional feedforward neural network , a recurrent neural network ( rnn ) has self - connections .",
    "mathematically , an rnn is a function @xmath5 , where @xmath6 is the input space , @xmath7 the output space , and @xmath8 the space of internal states . on input @xmath9 and with initial state @xmath10 , the rnn transitions into states @xmath11 ( internally ) and",
    "returns a sequence @xmath12 ( externally ) defined recursively by @xmath13    in this work , we use a particular variant of rnn called the long short term memory ( lstm ) @xcite .",
    "lstm s hidden state consists of two variables @xmath14 , where @xmath15 is also the output to the external world ( i.e. it fills the role of @xmath16 in the above description ) . the @xmath17 is the `` memory '' of the machine , designed to be maintained for a long time when necessary .",
    "there are many variants of lstm . in this paper",
    "we define the function @xmath18 as follows : @xmath19 where @xmath20 is the logistic function .",
    "@xmath21 are called the input , forget , and output gates , respectively , which modulate multiplicatively different quantities in the computation . the weights @xmath22 are trainable through backpropagation through time ( bptt ) @xcite .",
    "the undashed parts of figure [ lstm ] show a schematic of the equations above .",
    "gate indicates concatenating inputs and applying a linear transformation given by the weights of the network .",
    "the @xmath23 gate indicates the splitting of a vector .",
    "@xmath24 is any processing of @xmath15 to produce the final output @xmath16 , e.g. a softmax to produce a distribution over vocabulary . ]    in models with external memories , lstm often serves as the controller @xcite .",
    "this means that 1 ) the entire system carries state over time from both the lstm and the external memory , 2 ) the lstm controller collects reading from and computes additional instructions to the external memory , and 3 ) the lstm possibly performs extra processing @xmath24 to return the desired output at each time point .",
    "the dashed parts of figure [ lstm ] demonstrate a typical such arrangement , in which @xmath25 represents the state of the memory , @xmath26 represents the reading from the memory , @xmath27 represents a subroutine used for reading from and writing to the memory .",
    "the entire system is now described by the recurrence @xmath28 defined by @xmath29 where @xmath30 is a set of instructions to read from and write to the memory , as illustrated in figure [ lstm ] .",
    "@xmath24 is usually a softmax layer that produces a distribution over all possible symbols in a language task such as those explored in this paper , and this is indeed the case with lantm . in the next section , we show how lantm implements @xmath27 .",
    "we here review basic concepts of ( lie ) group theory .    a * group * is a set @xmath131 with operations @xmath132 ( multiplication ) , @xmath133 ( inverse ) , and @xmath134 ( unit ) of arity respectively 2 , 1 , 0 , such that    * ( associativity ) for all @xmath135 , @xmath136 * ( inverse ) for all @xmath137 , @xmath138 * ( identity ) for all @xmath137 , @xmath139    the classical examples are @xmath140 , @xmath141 , matrix groups like @xmath142 , and cyclic groups @xmath143 .",
    "a group often `` acts on '' another object or set , like a hand twists a rubik s cube .",
    "for example , imagine an equilateral triangle with its vertices colored differently .",
    "rotating the triangle by 120 degrees permutes the vertex color but leaves the overall shape unchanged .",
    "if we let @xmath144 correspond respectively to rotations of the equilateral triangle by 0 , 120 , or 240 degrees , and addition in @xmath145 corresponds to applying two such rotations consecutively , then @xmath145 is said to act on the set of color permutations of the triangle , because it maps one such permutation to another by a rotation . or , consider @xmath146 as a set of vectors and @xmath147 as a set of points .",
    "one may drag an element of @xmath148 by a vector from @xmath149 , thus mapping it to another element of @xmath148 .",
    "then we say @xmath149 acts on @xmath148 by vector addition .",
    "as this example illustrates , a group @xmath4 always acts on itself by the group multiplication ( in the example , this is addition of @xmath79 vectors ) .",
    "so in fact , every group acts on another set .",
    "formally , a _ group action _ of group @xmath4 on set @xmath6 is defined as a mapping @xmath150 such that    * @xmath151 for all @xmath152 * @xmath153 for all @xmath154 .",
    "it is the ubiquity of group action that explains the ubiquity of groups in mathematics . in this paper",
    ", we only borrow the language of groups and group actions to the extent it neatly expresses many ideas central to our design .",
    "no advanced ideas from mathematics are used .",
    "a _ lie group _ is a group with a smooth manifold structure such that multiplication and inverse operations are smooth maps .",
    "similarly , a _",
    "smooth group action _ of a lie group @xmath4 on smooth manifold @xmath155 is just a group action @xmath156 that is smooth . in the context of smooth lie group action",
    ", we also call elements of @xmath4 _ lie actions_.    the reader who has had no experience with smooth topology need not worry too much about the precise meaning of these definitions beyond the intuition that `` lie group is a group such that most things you do to it are differentiable '' and `` smooth lie group action is a differentiable group action '' . indeed , the only reason we require a lie group rather than a group is so that its group action yields to gradient descent .",
    "( to that end , it is not strictly necessary for the groups to be infinitely differentiable , but as all common differentiable groups are lie groups and all groups explored in this paper are lie group , this distinction is not needed . ) the reader hoping to learn the basics of smooth manifolds and lie groups can consult john lee s excellent _ introduction to smooth manifolds _ @xcite .",
    "the lie access neural turing machine ( lantm ) is inspired by the external memory architecture of neural turing machine ( ntm ) : a neural network controller reads from and writes to a memory structure via specially designed , differentiable functions called `` heads '' .",
    "the heads themselves do not have any trainable parameters , so the only learning done is by the controller , and the entire network can be trained by gradient descent .    in a lantm ,",
    "the memory structure is a dictionary , with keys in an euclidean space @xmath0 for a fixed @xmath31 , called the _ key space _ or _ address space _ ; and with values ( called _ memory vectors _ ) in another euclidean space @xmath32 for a fixed @xmath33 ( @xmath33 is called the _ memory width _ ) . at time step @xmath34 , each read head converts instructions from the controller to a read address @xmath35 that retrieves a reading @xmath26 from the memory by a weighted inverse squared law , to be elaborated below .",
    "each write head converts instructions from the controller to a new memory vector @xmath36 and a new address @xmath37 , along with a scalar @xmath38 $ ] , called the _ memory strength _ of the vector .",
    "such a triple @xmath39 is essentially appended to the memory .",
    "the most important hyperparameter of a lantm is its choice of lie group @xmath4 that acts on @xmath0 . at time @xmath40",
    ", the controller may emit new addresses for each head ( random access ) or issue lie actions @xmath41 that change the old addresses ( lie access ) .",
    "one may imagine the key space to be a piece of paper , and the read and write heads to be stones placed on this paper .",
    "the controller is a hand that moves the stones from turn to turn .",
    "sometimes it may lift a stone up and place it somewhere completely unrelated to its original position ( random access ) ; other times it may drag a stone along a chosen direction ( lie access ) .",
    "thus lie access generalizes sequential access in a conventional memory array to a continuous setting . in the design discussed in this paper",
    ", there is no explicit erasure .",
    "however , the machine can theoretically store the exact negation of a memory vector at the same location to cancel out that memory , albeit the required precision to do so would probably be overwhelming .",
    "what follows are details of the overview given above .",
    "let @xmath42 denote the set of memory vectors stored in the key space by time @xmath34 .",
    "we choose a canonical ordering on this set , for example by time added , and write @xmath43 for the @xmath44th vector in this order",
    ". denote by @xmath45 the corresponding addresses of @xmath43 and by @xmath46 the corresponding memory strength of @xmath43 . in this section",
    "we introduce two _ weight schemes _ for retrieving a value from the memory via an address .",
    "the main idea of both is summarized by figure [ retrieval ] .    .",
    "]    the read key @xmath47 produces weightings @xmath48 over all memory vectors @xmath49 , each with address @xmath50 , by normalizing their inverse squared distances and multiplying by their strengths @xmath51 : @xmath52 with the convention that it takes the limit value when @xmath53 for some @xmath44 .",
    "can induce numerical instability as @xmath53 for some @xmath44 , we adjust the formula with a small @xmath54 , e.g. @xmath55 , so that @xmath56 ]    the reading is then defined as @xmath57    we call this method of converting a read key to a set of weighting via a polynomial law _ invnormalize _ , or _",
    "invnorm _ for short , in contrast with the use of exponential law in the case of softmax weight scheme , which computes the weights @xmath48 as @xmath58 where @xmath59 is a _ temperature _ emitted by the controller at time @xmath34 that represent the certainty of its reading .",
    "the higher @xmath59 is , the more @xmath60 tends to be uniform .",
    "given the ubiquity of softmax in the machine learning literature , one may consider it a natural choice for the weight scheme .",
    "but as will be seen in the experiments , invnorm is crucial in making the euclidean space work as an address space .",
    "there is no extra ingredient to writing other than adding the produced memory vector @xmath61 , its strength @xmath62 , and its address @xmath63 to the collection of memory vectors , strengths , and addresses .",
    "to ensure that memory selection by weighted average works well , we squash the values of @xmath61 to @xmath64 $ ] by @xmath65 , but squashing by the logistic sigmoid function is also conceivable . without such squashing , a memory vector @xmath49 with large values can dominate the output of a weight method despite having low weight @xmath66 .",
    "here we describe how the keys @xmath67 and @xmath68 are produced .",
    "the procedure is the same for both read and write keys , so we assume that we are to compute a single key @xmath69 .",
    "we describe the abstraction of the process over a fixed lie group @xmath4 acting smoothly on the key space @xmath0 .",
    "the controller emits 3 things : a _ candidate key _",
    "@xmath70 , a _ mixing coefficient _ , or _ gate _ , @xmath71 $ ] ( via the sigmoid function ) , and an action @xmath72 that we also call _ step_. the gate @xmath73 mixes the previous key @xmath74 with the candidate key to produce a _",
    "pre - action key _",
    "@xmath75 , which is transformed by @xmath76 to produce the final key @xmath69 : ( here @xmath77 denotes group action ) @xmath78 figure ( [ address_mech ] ) summarizes the addressing procedure .    ]    in our experiments , the lie group is @xmath79 acting additively on @xmath79 .",
    "this means that the controller outputs 2 numbers @xmath80 , so that @xmath81 acts upon a key @xmath82 by @xmath83 section [ examplereps ] in the appendix gives example implementations for the scaling rotation @xmath84 and the rotation groups @xmath85 acting on @xmath79 .      for readers unfamiliar with the lie group examples mentioned below , we recommend a visit to section [ examplereps ] in the appendix .    for groups like @xmath86",
    ", there is a well - defined convex interpolation between two elements that stays in the group .",
    "for some others like @xmath84 , the straight - line interpolation @xmath87 for @xmath88 $ ] , @xmath89 sometimes produce elements outside the group ( in this case sometimes the elements cancel out and get 0 ) , but does so with probability zero in a suitable sense",
    ".    then , as for keys , we can let the controller output a candidate action @xmath90 and a mixing coefficient @xmath91 to smoothly mix with the previous action @xmath92 to produce a final action @xmath93    this allows the controller to `` move in a straight line within the group of actions '' by merely left saturating ( i.e. squash to 0 ) the gates @xmath94 and @xmath91 for all @xmath34 , so that @xmath95 .",
    "of course , the `` straight line '' can be actually curved depending on the group .",
    "for example , when @xmath96 , a typical `` straight line '' will be a spiral tending exponentially toward the origin or growing exponentially unbounded .",
    "even if a group does nt have a natural straight - line interpolation , there may be another way to mix two actions . in the case of @xmath97",
    ", we can just project a straight - line interpolation onto the circle ( barring a measure zero chance of intepolating into @xmath98 ) .",
    "the final addressing mechanism is shown in figure [ address_mech ] . all together , the interaction of the controller with the external memory is shown in figure [ controller_mem ] .    .",
    "note that all input , output and the states of the lstm other than @xmath26 have been omitted .",
    "[ controller_mem ] ]",
    "in our experiments , the lie group for both types of lantm is the translation group @xmath79 acting on @xmath79 , , which produced acceptable results when input lengths were small but encountered numerical problems when input lengths were large due to exponentiating scale . ] and we used lie action interpolation as specified above .",
    "we outline the most important experimental setup in the main text below but defer other details to the appendix section [ exp - details ] .",
    "we tested the two variations of lantm along with a baseline lstm in an encoder - decoder setup ( cf .",
    "@xcite ) on the copy , reverse , and bigram flip tasks as done in @xcite , as well as the double and addition tasks designed in a similar vein .",
    "table [ permutation_inputoutput ] shows input / output templates for each permutation task .",
    "[ permutation_inputoutput ]    each arithmetic tasks have all numbers , input or output , formatted with the least significant digits _ on the left _ and with zero padding .",
    "the double task takes an integer @xmath99 padded to @xmath1 digits and outputs @xmath100 in @xmath101 digits , zero padded to @xmath101 digits .",
    "the addition task takes two integers @xmath102 padded to @xmath1 digits and _ interleaved _ , forming a length @xmath103 input sequence and outputs @xmath104 zero padded to @xmath101 digits . table [ arithmetic_inputoutput ] show example input / outputs for each task with @xmath105 .",
    "the machines are first fed a learnable initial state and then provided with the input sequence , flanked by a start - of - input ( soi ) symbol @xmath106 and a repetition of an end - of - input ( eoi ) symbol @xmath107 .",
    "the machines are to output the correct sequence during the _ response phase _ , which starts when they receive the first @xmath108 .",
    "the repetition of @xmath108 effectively means that the correct symbols are not shown to the machines during answering , i.e. we do not use teacher forcing .",
    "the machine also must correctly emit an end - of - output ( eoo ) symbol @xmath109 to terminate their answers .",
    "figure ( [ example_inout ] ) is an example of inputs and correct outputs during a copy task .    ]    as usual , prediction is performed via argmax but training is done by minimizing negative log likelihood . to evaluate the performance of the models , we compute the fraction of characters correctly predicted and the fraction of all answers completely correctly predicted , respectively called `` fine score '' and `` coarse score '' following @xcite .    * task parameters and hyperparameters .",
    "* we trained the models on the above tasks for input sizes summarized by table [ input_sizes ] .",
    "for all tasks , the lantm has a single - layer , 50-cell or 100-cell lstm controller .",
    "the memory width ( i.e. the size of each memory vector ) is 20 .",
    "for all tasks , the lstm baseline has 4 layers , each with 256 cells . in the appendix ,",
    "the exact parameters for each model in each task are listed in table [ param_sizes ] , and other experimental details are given in section [ exp - details ] .",
    "notice that the lstm has 2 orders of magnitude more parameters than the lantm models .",
    "* results .",
    "* lantm - invnorm was able to master all tasks and generalize nearly perfectly to 2x the training sizes , as shown in table [ exp - results ] .",
    "lantm - softmax did as well on the copy and double tasks but failed at all the others , having performed worse than the lstm baseline .",
    "the baseline itself learned tasks with smaller training input sizes ( bigramflip , double , addition ) almost flawlessly , but generalization to 2x training size was inadequate on all tasks , with coarse score not exceeding 6% .",
    "[ exp - results ]    we tested the learned invnorm model on larger , arbitrarily selected input sizes . the results are summarized by table [ explore - gen ] .",
    "on permutation tasks , it generalized quite well when challenged by 4 times the training size , able to get more than 90% of test problems correct .",
    "on the double task , its extrapolation performance was similar , with 86% coarse score on 4x training size .",
    "notice that lantm - invnorm on several of the tasks ( 8x bigramflip , 8x double , 4x addition ) achieved high fine scores when extrapolating to large input sizes despite having low coarse scores .",
    "this suggests that the extrapolation errors systematically occur at the end of each output on those tasks .",
    "[ explore - gen ]    we have created videos of the read and write locations of lantm - invnorm and lantm - softmax while learning each of the 5 tasks , tracking their progress over time .",
    "they are available in the supplementary materials , with details explained in appendix [ gifs ] . in appendix",
    "[ close_anal ] , we look at the behaviors of trained lantm - invnorm through their read and write locations , gate values , and example input / output to analyze what exactly they learned and where their extrapolation errors come from when challenged by extreme input lengths .      the above problem setting is highly structured and favors the design of lantm .",
    "in this task we trained the models on generated python programs , following @xcite , that is more natural .",
    "the dataset comprises of 6 types of programs of integers : addition / subtraction , identity , multiplication with one small operand , small for loops , variable substitution , and ternary `` @xmath110 if @xmath111 else @xmath112 '' statements , as illustrated in table [ python_input_output ] .",
    "the models are required to read the input program , which terminates with a `` print '' statement , and output the correct integer response , _ in reverse sequence _ , without being fed the correct answer ( same as in our last experiment , but different from @xcite , which used teacher forcing ) .",
    "we performed curriculum learning , using the `` mixed '' strategy of @xcite , starting from 2 digits operands up to 4 digits operands .",
    "we evaluated the models on their coarse and fine scores on randomly sampled 4 digit programs .",
    "training was done by rmsprop with learning rate 0.002 , which was multiplied by 0.8 whenever the validation accuracy became lower than the highest of the last four .",
    "here the lstm baseline is a single layer of 128 cells , and the lantm models also have controllers who have the same size .",
    "in addition , each lantm model has memory size 128 .",
    "the results are summarized by table [ l2xresults ] .",
    "we noted that the small loop programs were the most difficult program type , for which all models predicted less than half of the characters correctly , so we trained them in a separate experiment only on small loop programs .",
    "the results are given in table [ smallloopresults ]    here the advantage of lantm over lstm is not as dramatic .",
    "the memory access of lantm were not nearly as orderly and neat as in the previous experiment , but rather erratic looking .",
    "an interactive plot of example read and write locations and other state data of lantm - invnorm while learning small loops can be found in the supplementary materials .",
    "finally , we tested the models on the penn treebank corpus . to train and predict continuously , whenever the external memories of lantms were fill up to 100 memory vectors , the oldest 60 vectors were discarded . as in the last experiment , the lstm baseline is a single layer of 128 cells , and the lantm models also have controllers with the same size . in addition , each lantm model has memory size 128 .",
    "we unrolled bptt to 20 steps , and trained with adagrad with learning rate 0.05 , which was halved each time the validation perplexity exceeded that of the previous epoch .",
    "we observed that lantm - invnorm had its read and write locations at two distant clusters , so that its read weights were all diffuse across the entire memory .",
    "this may be due to the repeated application of a ( approximately ) single lie action over the long course of training , blowing up the magnitude of keys , which degrades random access , as the typical squashing functions of the controller limits the range of keys it can produce .",
    "this means that , rather than storing useful information at particular locations , the machine stored _ deltas _ at each time step , so that the whole memory averaged together gave the desired information .",
    "lantm - softmax also exhibited the same behavior , but because high fidelity access only required the read key to be closer to the desired key @xmath1 much more than to other keys ( rather than that its distance to @xmath1 be absolutely small as with invnorm ) , we can not immediately infer that it also only stored deltas .",
    "zaremba et al .  @xcite taught lstm to evaluate simple python programs via curriculum learning , which formed the basis of one of our experiments .",
    "kalchbrenner et al .",
    "@xcite arranged lstm cells in a multidimensional grid to form the _ grid long short term memory _ , and learned copy and addition tasks as well .",
    "graves et al .",
    "@xcite created ntm which has inspired much of the design in our work .",
    "zhang et al .",
    "@xcite found several tweaks to ntm to improve its convergence and performance .",
    "grefenstette et al .",
    "@xcite designed smooth versions of stack , queue , and deque as external memories to an lstm controller .",
    "their unbounded memory and experimental setups were direct influences on this paper .",
    "zaremba et al .",
    "@xcite used reinforcement learning to absolve the need of the ntm to involve the entire memory during memory retrieval .",
    "weston et al .",
    "@xcite came upon similar ideas in the _ memory network _ as the ntm at around the same time , but with less focus on sequence learning and more on question answering tasks ( qa ) .",
    "sukhbaater et al .",
    "@xcite improved on their results to give a memory network trainable via gradient descent end - to - end and allowing multiple adaptive memory queries ( `` multiple hops '' ) which help in complex relational reasoning . _",
    "dynamic memory network _ of kumar et al .",
    "@xcite added an episodic memory module similar to the multiple hops feature of sukhbaatar et al.s model , but which dynamically chose when to stop accessing memory rather than after a fixed number of times .",
    "they achieved state of art results in several tasks such as qa and sequence modelling .",
    "danihelka et al .",
    "@xcite designed an external memory based on holographic reduced representations , which can store unlimited memory but the larger the size the more noisy the retrieval .",
    "kaiser et al .",
    "@xcite created the _ neural gpu _ based on convolutional kernels , which learned long multiplication of binary numbers up to 20 bits but were able to generalize to 2000 bits .",
    "kurach et al .",
    "@xcite generalized tha random access of conventional rams to create the _ neural random access machine _ , which learned simple algorithm and was able to generalize to larger lengths , and memory access during inference can be done in constant time .",
    "neelakantan et al .",
    "@xcite investigated adding gradient noise to training , and found that in many of the models mentioned above , this method improved the performance or allowed a greater percentage of random initializations to converge to the optimum .",
    "we want to stress that the model explained [ lieaccessmemory ] is but one way to implement lie access memory .",
    "indeed , the euclidean key space could be generalized to any riemannian manifold equipped with a subgroup of its isometry group , as 1 ) a notion of metric is required in lie access memory ( hence the riemannian part ) , and 2 ) one wants the ability to store and retrieve information in a `` straight line '' which suggests that the lie action be invariant with respect to the metric ( hence the isometry part ) .    a potentially useful riemannian manifold other than @xmath0 is the hyperbolic space , specifically the poincare disk model @xcite .",
    "as seen in the language modelling task , repeated application of lie action on @xmath0 may blow up the magnitude of keys , degrading random access .",
    "the poincare disk model has its points in the ( open ) unit ball that prevents this problem from occurring .",
    "the other standard riemannian model , the sphere , is not quite as desirable in this setting , because it `` wraps around '' ( i.e. is not acyclic , in homological / homotopic terms ) , which can confuse gradient descent .",
    "in this paper we introduced lie access memory and explored two different implementations in the experiments .",
    "the lantm model with the invnorm weight scheme in all tasks performed better than the baseline , and spectacularly so in sequence and addition tasks where it learned to generalize to extraordinary lengths , whereas that with the softmax weight scheme failed to outperform the baseline in the reverse , bigramflip , addition , and language modelling tasks .",
    "lantm - invnorm held its largest advantage over lstms in case of long , structured tasks .",
    "the python program experiment shows that in less structured environments or environments with redundant or useless information , our lantm designs could not utilize their memory as impressively as in more structure environments .",
    "thus further work needs to be done toward combining logical reasoning with natural language processing .",
    "we adopted a simple way to turn the episodic nature of our unbounded memory to continuous use , but it was far from perfect . in the language modelling experiment , the lantm models did not seem to use the memory in a remarkable way . future work should explore different options for adapting lie access memory to continuous tasks , for example , by bounding the memory or by using the poincare disk model as the underlying manifold as suggested in section [ generalization ] .",
    "the baselines of our experiments are lstms in an encoder - decoder setup as described in @xcite .",
    "we tested 2 variations of lantm with an invnorm and a softmax address mechanism , along with the lstm baseline , on the permutation and arithmetic tasks to be described .",
    "the lie group for both types of lantm is the translation group @xmath79 acting on @xmath79 . , which produced acceptable results when input lengths were small but encountered numerical problems when input lengths were large due to exponentiating scale .",
    "] for both lantms and lstm , we embed the input vocabulary continuously via a real embedding matrix into an euclidean space before feeding into the models ; we also pass the outputs through a softmax layer to arrive at probability distributions over the vocabulary set ( this is the @xmath24 box in figure [ lstm ] ) . as usual , prediction is performed via argmax but training is done by minimizing negative log likelihood .",
    "the machines are first fed a learnable initial state and then provided with the input sequence , flanked by a start - of - input ( soi ) symbol @xmath106 and a repetition of an end - of - input ( eoi ) symbol @xmath107 .",
    "the machines are to output the correct sequence during the _ response phase _ , which starts when they receive the first @xmath108 .",
    "the repetition of @xmath108 effectively ensures that the correct symbols are not shown to the machines during answering .",
    "the machine also must correctly emit an end - of - output ( eoo ) symbol @xmath109 to terminate their answers .",
    "the lantm models are not allowed to write to the memory during the response phase , so that there is more emphasis on collecting the right information during the input phase .",
    "figure ( [ example_inout ] ) is an example of inputs and correct outputs during a copy task .",
    "* tasks . *",
    "each task has a length parameter @xmath1 .",
    "the permutation tasks include    1 .",
    "copy @xmath113 2 .   reverse @xmath114 3 .",
    "bigramflip @xmath115    the arithmetic tasks include the following .",
    "note that all numbers , input or output , are formatted with the least significant digits * on the left * and with zero padding .    1",
    ".   double .",
    "let @xmath116 be an integer in the range @xmath117 $ ] , with zero padding in front ( on the right ) to make up @xmath1 digits .",
    "@xmath118 2 .   addition .",
    "let @xmath116 and @xmath119 be integers in the range @xmath117 $ ] , with zero padding in front ( on the right ) to make up @xmath1 digits . if they have digits @xmath120 and @xmath121 , respectively , with the _ least _ significant digits on the left , then @xmath122 + in other words , we interleave the inputs . thus this is a different encoding of the addition problem from previous works like @xcite and @xcite .    * task parameters and hyperparameters .",
    "* we trained the models on the above tasks for input sizes summarized by table [ input_sizes ] .",
    "for all tasks , the lantm has a single - layer , 50-cell or 100-cell lstm controller .",
    "the lie group for all lantms is the translation group @xmath79 acting on the key space @xmath79 .",
    "the memory width ( i.e. the size of each memory vector ) is 20 .",
    "for all tasks , the lstm baseline has 4 layers , each with 256 cells .",
    "the exact setting of parameters for each model in each task is listed in table [ param_sizes ] .",
    "`` vocab '' is the size of the vocabulary ( i.e. the total number of possible characters of each input sequence ) .",
    "`` embed '' is the dimension of the embedding space .",
    "for example , if `` embed '' is 7 , then each character is mapped to a vector in @xmath123 .",
    "width '' is the size of each memory vector . ``",
    "lr '' is the learning rate .",
    "`` # param '' gives the total number of trainable parameters .",
    "[ param_sizes ]    * training and testing . *",
    "we seek to minimize the negative log likelihood of the individual output characters given the input .",
    "all models are trained through rmsprop with momentum .95 .",
    "every epoch has 10 batches , and every batch has 32 instances of the task . for the lantm models , after 100 epochs",
    ", we half the learning rate if the best error so far is not improved in 30 epochs .",
    "the lstms are trained with learning rate 0.0002 , with no learning rate adjustments during training .    since the training sets are large and separate from the test sets , we train until convergence , testing the models periodically  every 20 epochs for the lantm models , and every 200 epochs for the lstm baseline .",
    "after training is complete , the best test scores are tabulated .",
    "we tested the models by drawing 100 batches of random problems and computing fine and coarse scores as in @xcite .",
    "fine score refers to the percentage of digit or characters ( including the eoo marker ) that the model correctly outputs .",
    "coarse score refers to the percentage of total problems that the model answers completely correctly .",
    "* tweaks to the lantm model . *",
    "we applied two tweaks to the lantm model : 1 ) we initialized the mix coefficients for write address and action to strong negative values .",
    "this means that the lantm would tend to write in a straight line .",
    "2 ) we normalized the step sizes to approximately 1 but did not normalize the initial state step sizes .",
    "we found that these two tweaks improved convergence speed and consistency .. ] note that with the second tweak , the `` group '' of actions is no longer a group .",
    "this is akin to restricting the head shifts of an ntm to @xmath124 and @xmath125 @xcite .",
    "there are 6 types of programs of integers : addition / subtraction , identity , multiplication with one small operand , small for loops , variable substitution , and ternary `` @xmath110 if @xmath111 else @xmath112 '' statements , as illustrated in table [ python_input_output ] .",
    "the models were required to read the input program , which terminates with a `` print '' statement , and output the correct integer response , _ in reverse sequence _ , without being fed the correct answer ( same as in sequence and arithmetic tasks , but different from @xcite , which used teacher forcing ) .",
    "the lantm models were prohibited from writing during the answer phase , as above .",
    "all input symbols were embedded into @xmath126 before being fed to the machines .",
    "we performed curriculum learning , using the `` mixed '' strategy of @xcite , starting from 2 digits operands up to 4 digits operands .",
    "we evaluated the models on their coarse and fine scores on randomly sampled 4 digit programs .",
    "training was done by rmsprop with learning rate 0.002 , which was multiplied by 0.8 whenever the validation accuracy became lower than the highest of the last four .",
    "bptt was always performed over the entire input and response phase .",
    "the lstm baseline had a single layer of 128 cells , as did the controllers of lantm - invnorm and lantm - softmax , which also had memory width of 128 .",
    "this comes out to be 127,890 parameters for the lstm baseline and 212,149 parameters for the lantm models .",
    "the lstm was initialized to have weights uniformly in @xmath127 $ ] except that the forget gates are set to 1 .",
    "the controllers of the lantm models have weights initialized uniformly in @xmath128 $ ] and the forget gates set to 1 as well .",
    "there were no write biases or normalization of step sizes .",
    "the penn tree - bank corpus consists of 929k/73k/82k train / validation / test words , with a total vocabulary of 10k words .",
    "we followed @xcite for preprocessing the corpus .",
    "we used batch size of 32 .",
    "we embed the words into @xmath129 before feeding into the models .",
    "the lstm baseline is 1 layer of 128 cells , and the lantm models have controllers of the same size , along with memory vectors in @xmath126 .",
    "this translates to 4,047,632 parameters for lstm and 4,323,329 parameters for the lantm models .",
    "the lstm was initialized to have weights uniformly in @xmath127 $ ] except that the forget gates are set to 1 .",
    "the controllers of the lantm models have weights initialized uniformly in @xmath130 $ ] and the forget gates set to 1 as well .",
    "the write biases were set to -10 as in the sequence and arithmetic tasks , but there is no normalization of step sizes . whenever the external memories filled up to 100 , the oldest 60 memory vectors were discarded .",
    "the number of bptt steps is 20 , and we used adagrad with learning rate 0.05 , which was halved each time the validation perplexity exceeded that of the previous epoch .",
    "the scaling rotation group @xmath84 is the group of linear transformations of @xmath79 that decomposes into a rotation followed by a dilation ( or contraction ) .    in the specific case of @xmath96",
    ", the controller would produce 2 numbers @xmath80 , which represents the element @xmath157 of the group .",
    "the matrix acts on a key @xmath158 by left matrix multiplication    @xmath159    this is the same as scaling by the scalar @xmath160 and then rotating ( i.e. left multiplication ) by the orthogonal matrix @xmath161    another viewpoint is to treat @xmath162 as the complex number @xmath163",
    ". then one can view the action @xmath164 for @xmath158 as the complex multiplication @xmath165 .",
    "the rotation , or special orthogonal , group @xmath85 is as its name suggests , the group of all linear transformations of @xmath79 expressable as a rotation .    when @xmath166 , we can just modify the scheme from the last example by scaling @xmath167 to unit norm , @xmath168 .",
    "the rest will follow just the same .",
    "for each task and each of lantm - invnorm and lantm - softmax , we created a video of sample read and writes over the course of learning ; the entire album is available in the supplementary materials .",
    "each video was created as follows :    1 .   at the end of each epoch , we randomly selected an input of the maximium training length specific to that task ( for example , in the case of addition task , two 16-digit numbers interleaved ) .",
    "2 .   we ran the model , with all weights set as trained so far , on this input and record the read and write locations in the key space , along with the strength of each memory vector .",
    "when training is complete , we plot the recording of each epoch in a separate frame , and string them together into a video file .",
    "the write locations are marked by red circles , and filled so that a darker fill color means higher memory strength .",
    "the read locations are marked by blue disks and connected together by a blue line chronologically ( the read line ) .",
    "even though we did not explicitly indicate the directionality of the read line , one may infer the directionality of the write sequence by noting that a red circle with white filling marks the beginning of the writes .",
    "then the read sequence will follow this directionality in all tasks other than the reverse task .",
    "_ analysis .",
    "_ one sees clearly that lantm - invnorm learned to write in a straight line ( which is not surprising given our tweaks to the model ) and then read along that same line . on the other hand , lantm - softmax tended to quarantine its read locations to one end of the write line in the reverse , bigramflip , and addition tasks . in the copy and double tasks",
    ", the read line does nt stick to the write line as closely with lantm - softmax as with lantm - invnorm .",
    "this is expected since softmax assigns a memory vector with high value just if its location @xmath110 is closer to the read location @xmath1 than any other memory vector , whereas invnorm requires @xmath1 to be very close to @xmath110 .",
    "in this section , we discuss the performance of latnm - invnorm through various statistics and example input / outputs .",
    "figure [ copy320keys ] shows the read and write locations of such a lantm - invnorm , trained on length 1 to 64 input , running on a typical length 320 input .",
    "as one might expect , the reads and writes proceed along straight lines in the key space .",
    "the actual read locations keep close to the corresponding write locations . in this execution ,",
    "the lantm made no errors ( figure [ copy320anspred ] ) .",
    "figure [ copy320gates ] shows the values of the 4 gates governing the computation of read and write keys .",
    "a value of 0 means the gate takes the previous step or key location , while a value of 1 means the gate takes the newly computed step or key location . while the write location gates during the input phase and the read location gates during the response phase were as expected pushed to 0 , the write step and read step gates were unexpectedly pushed to 1 .",
    "thus the lantm must have memorized a fixed step size and used it for both reads and writes .",
    "the counterparts of these graphs for the reverse task are exhibited in figure [ reversegraphs ] .",
    "on the left we have data for length 128 input , demonstrating a correct execution , while on the right we have data for length 300 input , demonstrating what goes on when extrapolating to higher input sizes .",
    "we see that lantm trained on the reverse task functions much like that trained on the copy task , with read and write heads traversing on straight lines , except now the directionalities are opposed . however , when running on length 300 input , the read line , i.e. the curve connecting the read locations in sequence , bends suddenly toward the end , causing almost all reads at the end to diverge from the writes and making almost all outputs at the end to be incorrect .",
    "this is somewhat surprising , for one might have expected error to come in the form of the accumulation of a small difference between the slopes of the read and write lines .",
    "along with the sudden dip in read step gate value at the end ( blue line in figure [ reverse300gates ] ) , the bending of the read line suggests that the lstm controller started to forget its learned program as the answering phase drew toward a conclusion .",
    "the same phenomena appear with the bigramflip task , where reads and writes happen along 2 closely aligned lines , but when tested by a long input , the reads will abruptly fall out of order : while in the reverse task , the read line visibly bends away from the write line , here the lines stay straight but each step in the read line is elongated , starting around the 187th read ( figure [ bigram256keys ] ) .",
    "1 .   lantm stores representations of the inputs in input order .",
    "meanwhile it memorizes the first two input characters and outputs them in the reverse order after reading the first two eoi symbols . 3 .",
    "when it sees the first eoi symbols , it starts reading the second bigram , i.e. it reads characters 3 and 4 ( or their representations in memory ; this corresponds to the 5th and 6th memory vectors ) after seeing the first and second eoi symbols .",
    "this effectively allows it to `` look ahead '' and have each bigram on hand before having to output the flipped image of it .",
    "the lstm flips each `` look ahead '' bigram and outputs it in order .",
    "repeat for each bigram .",
    "unique to the lantm trained on bigramflip is the oscillation of the read step gate between 0 and 1 ( figure [ bigram128gates ] and [ bigram256gates ] ) .",
    "this seems like more an artifact of the learning process than a feature of the learned computation , as it would also imply that the controller memorized a single fixed read step , and that the error that occurs with extrapolation seems to stem from the adulteration of this memory .",
    "is emitted when the lantm reads the @xmath44th eoi symbol ( so that the actual value read is fed back into the lantm at time @xmath169 ) . *",
    "( b ) gate values during the execution * the vertical dashed line in the middle ( slightly hard to see due to overlap with the green trace ) marks the time when the lantm reads the eoi symbol . *",
    "( c ) correct answer , lantm s prediction , and the difference*. each different color represents one of the 128 possible values of the vocabulary .",
    "the first row is the correct answer , the second row lantm s prediction , and the third the difference between the two . in the third row , at most two colors are present : blue means lantm s response is correct ; red means incorrect . in this case",
    "there are no red bars because the lantm was able to perfectly copy the input . ]",
    "is emitted when the lantm reads the @xmath44th eoi symbol ( so that the actual value read is fed back into the lantm at time @xmath169 ) . *",
    "( b ) gate values during the execution * the vertical dashed line in the middle ( slightly hard to see due to overlap with the green trace ) marks the time when the lantm reads the eoi symbol . * ( c ) correct answer , lantm s prediction , and the difference*. each different color represents one of the 128 possible values of the vocabulary .",
    "the first row is the correct answer , the second row lantm s prediction , and the third the difference between the two . in the third row , at most two colors are present : blue means lantm s response is correct ; red means incorrect .",
    "in this case there are no red bars because the lantm was able to perfectly copy the input . ]",
    "is emitted when the lantm reads the @xmath44th eoi symbol ( so that the actual value read is fed back into the lantm at time @xmath169 ) . *",
    "( b ) gate values during the execution * the vertical dashed line in the middle ( slightly hard to see due to overlap with the green trace ) marks the time when the lantm reads the eoi symbol . * ( c ) correct answer , lantm s prediction , and the difference*. each different color represents one of the 128 possible values of the vocabulary .",
    "the first row is the correct answer , the second row lantm s prediction , and the third the difference between the two . in the third row , at most two colors are present : blue means lantm s response is correct ; red means incorrect . in this case",
    "there are no red bars because the lantm was able to perfectly copy the input . ]                .37 , except that in subfigure [ bigram256keys ] , there is now a fourth plot showing keys around where the reads start to diverge from the writes . in the gate value plots , the write step lines ( orange )",
    "are almost constant at 0 , hidden behind the red lines ( write location).,title=\"fig : \" ]    .5 , except that in subfigure [ bigram256keys ] , there is now a fourth plot showing keys around where the reads start to diverge from the writes . in the gate value plots , the write step lines ( orange )",
    "are almost constant at 0 , hidden behind the red lines ( write location).,title=\"fig : \" ]    .36 , except that in subfigure [ bigram256keys ] , there is now a fourth plot showing keys around where the reads start to diverge from the writes . in the gate value plots , the write step lines ( orange ) are almost constant at 0 , hidden behind the red lines ( write location).,title=\"fig : \" ]    .4 , except that",
    "in subfigure [ bigram256keys ] , there is now a fourth plot showing keys around where the reads start to diverge from the writes . in the gate value plots , the write step lines ( orange )",
    "are almost constant at 0 , hidden behind the red lines ( write location).,title=\"fig : \" ]    .36 , except that in subfigure [ bigram256keys ] , there is now a fourth plot showing keys around where the reads start to diverge from the writes . in the gate value plots , the write step lines ( orange ) are almost constant at 0 , hidden behind the red lines ( write location).,title=\"fig : \" ]    .4 , except that in subfigure [ bigram256keys ]",
    ", there is now a fourth plot showing keys around where the reads start to diverge from the writes . in the gate value plots , the write step lines ( orange )",
    "are almost constant at 0 , hidden behind the red lines ( write location).,title=\"fig : \" ]    .37 , except that in the first row , the overview plots are magnified horizontally to accentuate the divergence of the read and write lines , demonstrating that the divergence is a gradual build up of difference in slope . in the gate value plots , the write step lines ( orange ) are almost constant at 0 , hidden behind the red lines ( write location ) . *",
    "[ double80anspred ] and [ double320anspred ] .",
    "* we show the doubling problems given to the lantm and its response .",
    "dollar signs ( $ ) represent the end symbol",
    ". asterisks ( * ) mark points where lantm s answers are incorrect . in e ,",
    "only the first ( most significant ) 130 digits are shown , as there are no errors in the remaining digits.,title=\"fig : \" ]    .4 , except that in the first row , the overview plots are magnified horizontally to accentuate the divergence of the read and write lines , demonstrating that the divergence is a gradual build up of difference in slope . in the gate value plots , the write step lines ( orange )",
    "are almost constant at 0 , hidden behind the red lines ( write location ) . *",
    "[ double80anspred ] and [ double320anspred ] . *",
    "we show the doubling problems given to the lantm and its response .",
    "dollar signs ( $ ) represent the end symbol",
    ". asterisks ( * ) mark points where lantm s answers are incorrect . in e ,",
    "only the first ( most significant ) 130 digits are shown , as there are no errors in the remaining digits.,title=\"fig : \" ]    .36 , except that in the first row , the overview plots are magnified horizontally to accentuate the divergence of the read and write lines , demonstrating that the divergence is a gradual build up of difference in slope . in the gate value plots , the write step lines ( orange ) are almost constant at 0 , hidden behind the red lines ( write location ) . * [ double80anspred ] and [ double320anspred ] .",
    "* we show the doubling problems given to the lantm and its response .",
    "dollar signs ( $ ) represent the end symbol",
    ". asterisks ( * ) mark points where lantm s answers are incorrect . in e ,",
    "only the first ( most significant ) 130 digits are shown , as there are no errors in the remaining digits.,title=\"fig : \" ]    .4 , except that in the first row , the overview plots are magnified horizontally to accentuate the divergence of the read and write lines , demonstrating that the divergence is a gradual build up of difference in slope . in the gate value plots , the write step lines ( orange ) are almost constant at 0 , hidden behind the red lines ( write location ) . *",
    "[ double80anspred ] and [ double320anspred ] . *",
    "we show the doubling problems given to the lantm and its response .",
    "dollar signs ( $ ) represent the end symbol",
    ". asterisks ( * ) mark points where lantm s answers are incorrect . in e ,",
    "only the first ( most significant ) 130 digits are shown , as there are no errors in the remaining digits.,title=\"fig : \" ]      .... problem 2 x     19405944699084738804681122807647896060125309533161058781560471151464278983184417 = $ 038811889398169477609362245615295792120250619066322117563120942302928557966368834 lantm prediction $ 038811889398169477609362245615295792120250619066322117563120942302928557966368834 diff _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ....      .... problem 2 x     1168008529246365494001163765851968108385917130977679634159993953145055040607051735753912407298503487088205933788143138139895600272 ... = $ 02336017058492730988002327531703936216771834261955359268319987906290110081214103471507824814597006974176411867576286276279791200544 ... lantm prediction $ $ 2233601615442732988800322531703937236677034261955359368319987996290110081214103471507824814597006974176411866576286276279791200544 ... diff _ * _ * _ * * * * * * * _ * _ _ _ * _ _ _ * _ * _ _ * _ _ _ _ _ _ _ _ * _ * _ * _ * * _ _ _ _ _ _ _ _ _ _ _ * _ _ _ _ _ _ _ _ _ * _ _ _ _ _ _ _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ... ....    .4 .",
    "in the gate value plots , the write step lines ( orange ) are almost constant at 0 , hidden behind the red lines ( write location ) . * [ addition64anspred ] and [ addition128anspred ] .",
    "* we show the addition problems given to the lantm and its responses .",
    "dollar signs ( $ ) represent the end symbol",
    ". asterisks ( * ) mark points where lantm s answers are incorrect.,title=\"fig : \" ]    .4 . in the gate value plots , the write step lines ( orange )",
    "are almost constant at 0 , hidden behind the red lines ( write location ) . *",
    "[ addition64anspred ] and [ addition128anspred ] . *",
    "we show the addition problems given to the lantm and its responses .",
    "dollar signs ( $ ) represent the end symbol",
    ". asterisks ( * ) mark points where lantm s answers are incorrect.,title=\"fig : \" ]    .36 . in the gate value plots , the write step lines ( orange ) are almost constant at 0 , hidden behind the red lines ( write location ) .",
    "* [ addition64anspred ] and [ addition128anspred ] . *",
    "we show the addition problems given to the lantm and its responses .",
    "dollar signs ( $ ) represent the end symbol",
    ". asterisks ( * ) mark points where lantm s answers are incorrect.,title=\"fig : \" ]    .4 . in the gate value plots , the write step lines ( orange )",
    "are almost constant at 0 , hidden behind the red lines ( write location ) . * [ addition64anspred ] and [ addition128anspred ] .",
    "* we show the addition problems given to the lantm and its responses .",
    "dollar signs ( $ ) represent the end symbol",
    ". asterisks ( * ) mark points where lantm s answers are incorrect.,title=\"fig : \" ]      .... problem    12073190535916485602949518287285 + 86090378999878149582784619496855 = $ 098163569535794635185734137784140 lantm prediction $ 098163569535794635185734137784140 diff _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ....      .... problem    7551725121767466617753259624470709413611531758233121189170215537 + 6546773181603442580771303866863379302959060107624784946180377555 = $ 14098498303370909198524563491334088716570591865857906135350593092 lantm prediction $ $ $ $ 18498303370909198524563491334088716570591865857906135350593092 diff _ * * * * _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ....        in the addition task , the lantm learned to compress each pair of digits of the input numbers ( which , as mentioned above , are interleaved ) and store them in the odd write locations ; the even write locations had vanishing memory strength ( figure [ addition64keys ] and [ addition128keys ] ) .",
    "the lantm then read off the information by skipping through the odd memory locations .",
    "as with copy and reverse tasks , the read step gate values during the response phase were all close to 1 , meaning that the lantm kept the read step in the lstm controller memory .",
    "this suggests that the read step gate might be an unnecessary design ."
  ],
  "abstract_text": [
    "<S> following the recent trend in explicit neural memory structures , we present a new design of an external memory , wherein memories are stored in an euclidean key space @xmath0 . </S>",
    "<S> an lstm controller performs read and write via specialized read and write heads . </S>",
    "<S> it can move a head by either providing a new address in the key space ( aka random access ) or moving from its previous position via a lie group action ( aka lie access ) . in this way , </S>",
    "<S> the `` l '' and `` r '' instructions of a traditional turing machine are generalized to arbitrary elements of a fixed lie group action . for this reason , we name this new model the lie access neural turing machine , or lantm .    </S>",
    "<S> we tested two different configurations of lantm against an lstm baseline in several basic experiments . </S>",
    "<S> we found the right configuration of lantm to outperform the baseline in all of our experiments . </S>",
    "<S> in particular , we trained lantm on addition of @xmath1-digit numbers for @xmath2 , but it was able to generalize almost perfectly to @xmath3 , all with the number of parameters 2 orders of magnitude below the lstm baseline .    </S>",
    "<S> author title </S>"
  ]
}