{
  "article_text": [
    "0.32        0.32        0.32     many large streaming datasets are generated by systems that continually record the location of spatially - centric events .",
    "examples include the address where ambulances are requested through an emergency dispatch center , location names detected in a query through a web search engine , and the gps coordinates recorded by a taxicab s routing software .",
    "when spatial measurements come from a relatively small and discrete set , they can often be handled just like any other covariate .",
    "spatial data recorded from a very large set , such as addresses , or as a continuous measurement of latitude and longitude , may require more specialized treatment .",
    "predicting the location of the next observation , for example , is often accomplished by modeling historical data as a spatial point process and estimating a density function with techniques such as kernel density estimators or gaussian mixtures .    with enough data",
    ", it may be both possible and desirable to generate alternative density estimates for different classes of events .",
    "when crossing several variables , densities can quickly become quite granular ; consider , for example , the density of ambulance requests ` related to traffic accidents from 7am-8am on tuesdays ' .",
    "an increase in granularity comes with a corresponding increase in the number of density estimates that must be stored . on its own ,",
    "this increase is not a particular point of concern .",
    "the estimates can be stored in an off - the - shelf database solution , a small subset relevant to a given application can be queried , and the results used in a standard fashion .",
    "computational issues do arise when there is a need to simultaneously work with a large set of densities .",
    "inverse problems , where an event is detected in a location and the type of event must be predicted , present a common examples of such an application .",
    "performing set operations , such as intersections and unions , to construct even more complex density estimates is another example .",
    "a large set of non - parametric density estimates , represented by predictions over a fine - grained lattice , can lead to a substantial amount of memory consumption .",
    "parametric estimates , perhaps compactly represented by gaussian mixtures , may be safely loaded into memory but eventually cause problems as set operations between densities yield increasingly complex results .",
    "these make inference and visualization intractable as the scale of the data increases .    as a solution to these problems , we present a method that combines the benefits of parametric and non - parametric density estimates for large spatial datasets .",
    "it represents densities as a sum of uniform densities over a small set of differently sized tiles , thus yielding a sparse representation of the estimated model . at the same time , the space of all possible tiles is a relatively small and fixed set ; this allows multiple densities to be aggregated , joined , and intersected in a natural way . by using a geometry motivated by quadtrees we are also able to support constant time set operations between density estimates .",
    "the estimation of general density estimation has a long history , with many spatial density routines being a simple application of generic techniques to the two - dimensional case .",
    "mixture models were studied as early as 1894 by karl pearson @xcite , with non - parametric techniques , such as kernel density estimation , being well known by at least the 1950s @xcite , @xcite .",
    "further techniques have also been developed in the intervening years , such as smoothed histograms @xcite , splines @xcite , poisson regression @xcite , and hierarchical bayes @xcite .",
    "a variant on mixture models that allow for densities to depend on additional covariates was proposed by schellhase and kauermann @xcite , though their work focuses only on one - dimensional models .",
    "computational issues regarding density estimation have also been studied in the recent literature .",
    "some attention has focused on calculating the kernel density estimates given by a large underlying training dataset @xcite . as an active area of computer vision , there as been a particular interest on two - dimensional problems @xcite .",
    "these often utilize some variation of the fast gauss transform of greengard and strain @xcite . to the best of our knowledge ,",
    "no prior work has focused on the computational strains of working simultaneously with a large set of density estimates .",
    "a significant amount of literature and software also exists from the perspective of manipulating generic spatial data within a database .",
    "these store spatial data in a format optimized for some set of spatial queries , such as k - nearest neighbors or spatial joins @xcite , and can be made to handle and query fairly large sets of data .",
    "recent developments have even made spatial queries possible over data distributed across horizontally scalable networks @xcite , and integrated for fast real - time visualizations @xcite .",
    "the elements in such systems are typically points , lines , or polygons .",
    "density functions can be encoded into such a system by storing the centroid of the density modes within mixture models , or saving predictions over a fine grid for non - parametric models .",
    "the problem in using this solution for our goal is that operations on the parametric models do not translate into fast queries on the database , and the size of the grid required to store non - parametric models quickly grows prohibitively large .",
    "our solution builds a density estimate that can be queried within a database system , but stored in a significantly smaller space .",
    "for the remainder of this article , we focus on density estimation over a rectangular grid of points .",
    "we assume that there is some observed sample density @xmath0 over the grid , which generally corresponds to assigning observations to the nearest grid point , and an initial estimate of the density given by @xmath1 .",
    "the latter typically comes from a kernel density estimate , but may be generated by any appropriate mechanism .",
    "our goal is to calculate predictions @xmath2 such that the following are all true :    1 .",
    "the new estimates are expected to be nearly equally as predictive for a new set of observed data @xmath3 as the estimates @xmath1 , @xmath4 2 .",
    "the estimates @xmath2 can be represented as a sparse vector over some fixed dictionary @xmath5 , which has a total dimension of size @xmath6 , the number of data points in the grid .",
    "queries of the form @xmath7 can typically be calculated faster than @xmath8 for @xmath9 corresponding to any subset of points over the original grid .    to satisfy these requirements , we construct an over - complete , hierarchical dictionary and calculate an estimate of @xmath10 using an @xmath11-penalized regression model .      in order to describe our target dictionary",
    ", it is best to consider the case where the grid of points is a square with @xmath12 points on each side .",
    "we otherwise embed the observed grid in the smallest such square and proceed as usual knowing that we can throw away any empty elements in the final result .    for any integer @xmath13 between @xmath14 and @xmath15 , we define the following sets : @xmath16 for a fixed @xmath13 , these sets produce a disjoint , equally sized , partition of the grid of points , with @xmath17 and @xmath18 giving the horizontal and vertical coordinates of the superimposed grid .",
    "we refer to each @xmath19 as a _ tile _ , as it represents a square subset of the original lattice . for a visualization of the partitioning scheme over a small grid ,",
    "see figure  [ tilepart ] .",
    "it will be helpful to have a way of referencing the set of all tiles that share a given @xmath13 parameter , often referred to as a zoom level : @xmath20 combining all of the zoom levels , we can construct the set of all tiles from which we will define the final dictionary : @xmath21 we will assume that there is some fixed order of this set , so that we may refer unambiguously to the @xmath22-th tile @xmath23 in the set @xmath24 .",
    "the elements @xmath25 , @xmath26 , and @xmath27 refer to the corresponding indices of that @xmath22-th tile .",
    "the set @xmath24 consists of all tiles corresponding to a fixed depth quadtree over the grid of all points .",
    "they mirror the structure of tiles used in tilemap servers , such as slippy map generated by openstreetmap @xcite .",
    "when the grid is defined over the entire globe , our tiles directly coincide with the slippy tiles .",
    "finally , we define the dictionary @xmath28 as a sparse matrix in @xmath29 , with @xmath30 .",
    "it is defined such that : @xmath31 for @xmath32 equal to @xmath33 , element @xmath34 encodes the proportion of the tile @xmath23 that is covered by the point @xmath2 .",
    "the element @xmath35 is instead a simple indicator for whether a the point @xmath2 is in tile @xmath23 .",
    "other values of @xmath32 provide a continuous scale of weightings to the tiles that moves between these two extremes , and will be useful in the penalized estimation routine .",
    "notice that the dictionary is an over - representation of the space of all possible estimators , and for @xmath32 equal to @xmath14 , contains a copy of the identity matrix as a permuted subset of its columns .      in order to learn a sparse representation of @xmath10 given our dictionary @xmath36",
    ", we first use an @xmath11-penalized estimator .",
    "this well - known technique produces a parsimonious estimator via convex optimization .",
    "the parameter @xmath32 is set to a number between @xmath14 and @xmath33 to control the degree to which the penalty should be proportional to tile size . a large parameter will yield a model with many small tiles , whereas a small parameter gives a smaller model with more large tiles .",
    "we find that a value near @xmath37 typically works well .",
    "@xmath38 in order to solve equation  [ ell1 ] , we calculate the regularization path for a sequence of @xmath39 values using software that provides a customized application of coordinate decent @xcite . choosing the final tuning parameter",
    "can be done by a number of methods ; we have found the one standard deviation rule , originally suggested by breiman , with 5-fold cross validation provides good predictability without overfitting the model @xcite .    to increase interpretability and reduce prediction bias , we calculate the non - negative least squares estimator over the support of the @xmath11-penalized estimator . with a large training dataset , this should be refit on a holdout set .",
    "empirically , we observe good performance even when refitting on the same training data . without the @xmath11-penalty",
    "the choice of @xmath32 does not effect the predicted values , so here we use @xmath40 to facilitate the interpretability and normalization of the results .",
    "@xmath41 we then hard threshold the non - negative least squares solution by a value of @xmath42 .",
    "@xmath43 and finally , the density estimator is normalized to have a sum of @xmath33 : @xmath44 the predicted values , @xmath45 , can be calculated by projecting @xmath46 by the dictionary @xmath47 .",
    "@xmath48 clearly we do not want to save the predicted values explicitly .",
    "otherwise , we would have simply saved the raw predicted values @xmath10 .",
    "we instead show , in the next subsection , that predictions @xmath45 can be generated quickly from the sparse vector @xmath46 .        [",
    "cols=\"^,^,^,^,^,^,^,^,^,^,^ \" , ]",
    "we sampled a set of two hundred thousand points from a gaussian mixture model with @xmath49 modes .",
    "using @xmath50 , we calculated the sparse representation of the density over the quadtree based dictionary .",
    "a scatterplot of the sampled points superimposed over the predicted densities are shown in figure  [ sim1image ] for a particular grid size and value of @xmath32 .",
    "table  [ tvsim ] displays the total variation distance between the estimated density and the true gaussian distribution over a grid of tile sizes and values of @xmath32 .",
    "these are compared to the total variation distances gained by directly using the observed densities at each point .",
    "we see that , with the exception of the smallest grid size , the total variation of both estimators performs more poorly with larger grid sizes .",
    "this seems reasonable as larger grid sizes provide a harder estimation problem with many more degrees of freedom .",
    "the total variation as a function of @xmath32 shows that values too close to @xmath33 perform poorly regardless of the grid size .",
    "values near @xmath14 function okay , but the optimal value tends to be from @xmath51 to @xmath52 .    in table",
    "[ sizesim ] , we show the number of non - zero elements in the prediction vector by grid size and the @xmath32 parameter .",
    "the number of non - zero elements generally increases with the grid size .",
    "given the exponentially growing dictionary size this is not surprising , however it is interesting that the sizes do not grow exponentially .",
    "this shows the ability of our method to represent complex , granular densities in a relatively compact way regardless of the grid size .",
    "when @xmath32 is equal to @xmath33 ( or very close to it ) the predicted model size is very small .",
    "this is the result of a poor model fit due to overfitting on small tile sizes ; the cross - validation procedure eliminates the overfitting and leaves a relatively constant model .",
    "we recommend setting @xmath32 to anything between @xmath14 and the value that maximized the model size . in this simulation notice that maximizing the model size with respect to @xmath32 would generate the lowest total variation of the resulting model .",
    "this is a useful heuristic because while table  [ tvsim ] can not be generated without knowing the true generating distribution , table  [ sizesim ] can be created without an external information .",
    "the city of chicago releases incident level data for all reported crimes that occur within the city @xcite .",
    "we fit sparse density estimates to six classes of crimes ; these estimated densities are shown in figure  [ chiimage ] .",
    "we have picked classes that exhibit very different spatial densities over the city .",
    "for example , deceptive practice crimes occur predominantly in the center of the city whereas narcotics violations are concentrated in the western and southern edges of chicago .",
    "table  [ chisize ] shows the number of non - zero terms in the predicted density vectors of pairwise unions and intersections .",
    "of particular importance , note that the complexity of the unions and intersections are not significantly larger than the complexity of the original estimates ( given by diagonal terms of the table of unions ) .",
    "this property would not hold for most other density estimation algorithms .      in response to a freedom of information request , the new york city government released a dataset showing the requested pickup locations from @xmath53 million rides commissioned by the transit company uber @xcite .",
    "we used this data to construct two - hour density buckets , three of which are shown in figure  [ uberimage ] .",
    "notice that , unlike the chicago crime data , the non - zero tiles are relatively consistent from image to image , roughly following the population density of the city .",
    "temporal differences do exist : for example the heavily neighborhood of the upper west side has a particularly high density only during the morning commute .",
    "the ability to detect localized spikes at the airports exhibit the adaptive nature of the sparse learning algorithm .",
    "table  [ unionsize ] shows the model sizes when computing the union of densities from any continuous time interval during the day . due to the truncating of small densities by @xmath42 and the fact that the non - zero tiles generally line up across time periods , the overall size of the unions never grows much larger than the original estimates .",
    "table  [ intersectsize ] shows the same information over arbitrary unions .",
    "these intersections would be useful , for example , when trying to determine where taxicab waiting spots should be constructed as they indicate areas of high density throughout periods of the day .",
    "overall , we are able to quickly calculate @xmath54 complex densities by only estimating and storing @xmath55 of them ( or @xmath56 , when considering non - contiguous time periods ) .",
    "we have presented an algorithm for calculating sparse representations of spatial densities .",
    "this method has been shown to be able to compute fast density estimations over arbitrary regions and to support union and intersections over a large set of independent density estimates .",
    "these claims have been illustrated theoretically , by controlled simulations , and over two real datasets .",
    "we are now looking to generalize the quadtree approach to work with alternative hierarchical partitions of space .",
    "in particular , this could create a finer grained dictionary near places of high density ( i.e. , roads and city centers ) allowing for a smaller dictionary and further decreasing the estimation error .",
    "jan  kristof nidzwetzki and ralf  hartmut gting .",
    "distributed secondo : a highly available and scalable system for spatial data processing . in _ advances in spatial and temporal databases _ ,",
    "pages 491496 .",
    "springer , 2015 .",
    "daniel  am villela , claudia  t codeo , felipe figueiredo , gabriela  a garcia , rafael maciel - de freitas , and claudio  j struchiner . a bayesian hierarchical model for estimation of abundance and spatial density of aedes aegypti . 2015 .    changjiang yang , ramani duraiswami , nail gumerov , larry davis , et  al . improved",
    "fast gauss transform and efficient kernel density estimation . in",
    "_ computer vision , 2003 .",
    "ninth ieee international conference on _ , pages 664671 .",
    "ieee , 2003 .",
    "yan zheng , jeffrey jestes , jeff  m phillips , and feifei li . quality and efficiency for kernel density estimates in large data . in _ proceedings of the 2013 acm",
    "sigmod international conference on management of data _ , pages 433444 .",
    "acm , 2013 .",
    "yan zheng and jeff  m phillips . l-@xmath57 error and bandwidth selection for kernel density estimates of large data . in _ proceedings of the 21th acm sigkdd international conference on knowledge discovery and data mining _ , pages 15331542 .",
    "acm , 2015 ."
  ],
  "abstract_text": [
    "<S> large spatial datasets often represent a number of spatial point processes generated by distinct entities or classes of events . when crossed with covariates , such as discrete time buckets </S>",
    "<S> , this can quickly result in a data set with millions of individual density estimates . </S>",
    "<S> applications that require simultaneous access to a substantial subset of these estimates become resource constrained when densities are stored in complex and incompatible formats . </S>",
    "<S> we present a method for representing spatial densities along the nodes of sparsely populated trees . </S>",
    "<S> fast algorithms are provided for performing set operations and queries on the resulting compact tree structures . the speed and simplicity of the approach </S>",
    "<S> is demonstrated on both real and simulated spatial data . </S>"
  ]
}