{
  "article_text": [
    "the most powerful computers for lattice gauge theory are industrial supercomputers or special purpose parallel computers ( see e.g. @xcite ) .",
    "it is more and more accepted that e.g. off - the - shelf pc systems can be used to build parallel computers for lattice qcd simulations @xcite .",
    "single pc hardware has excellent price / performance ratios . for recent review papers",
    "see @xcite .",
    "we present our experiences and benchmark results on a scalable system , which uses nearest - neighbor communication through gigabit ethernet ( gige ) cards .",
    "the communication is fast enough ( consuming 40% of the total time in typical applications ) and cheap enough ( 30% of the total price is spent on the communication ) .",
    "the system can sustain @xmath0100 gflops on today s medium / large lattices .",
    "its price / performance ratio is below $ 1/mflops for 32-bit applications",
    "( twice as much for 64-bit applications ) .    already in 1999",
    "a report was presented @xcite on the pc - based parallel computer project at the etvs university , budapest , hungary .",
    "a machine was constructed with 32 pcs arranged in a three - dimensional 2@xmath14@xmath14 mesh .",
    "each node had two special , purpose designed communication cards providing communication to the six neighbors .",
    "we used the `` multimedia extension '' instruction set of the processors to increase the performance . the communication bandwidth",
    "( 16 mbit / s ) at that time was only enough for bosonic simulations .",
    "here we report on a system which needs no hardware development and has two orders of magnitude larger communication bandwidth through gige cards .",
    "each node consists of an intel kd850 gb motherboard , intel - p4 - 1.7ghz processor , 512  mb rdram , 100  mbit ethernet card , 20.4  gb ide hdd and four smc9452 gige cards for the two dimensional inter - node communication . the 100  mbit ethernet cards with switches are used for job management only .",
    "we have 137 pc nodes .",
    "a single machine is used for controlling .",
    "two smaller clusters with 4 nodes are used for development .",
    "the remaining 128 machines can be used as one cluster with 128 nodes ( or two clusters with 64 nodes or four clusters with 32 nodes ) for mass production .",
    "the best number of connected nodes for a given lattice can be determined by optimizing the surface to volume ratio of the local sub - lattice .",
    "changing the node topology needs reconnecting cables , which can be done easily in a few minutes .    in april , 2002",
    "the price of one node including the 100  mbit ethernet switches is $ 380 ( see @xcite ) .",
    "the four gige cards with cables cost additional $ 160 for each node .",
    "the power consumption of the nodes ( 140w each ) requires a cooling system which costs around $ 13 per node .",
    "thus , the total node ( pc+communication+cooling ) price is about $ 553 .    the key element of a cost effective design is an appropriate balance between communication and the performance of the nodes .",
    "we spent more than twice as much on the bare pc ( $ 393 including cooling ) than for the gigabit communication ( $ 160 ) .",
    "these numbers are in strong contrast with myrinet based pc systems , for which the high price of the myrinet card exceeds the price of such a pc by a factor of two .",
    "the lattice sizes are chosen to be @xmath2 .",
    "the dotted line represents the $ 1/mflops value .",
    "wilson and staggered quarks are both indicated .",
    "the machine size ( the number of communicating nodes ) for a given lattice volume is shown by an inserted scale .",
    "[ perf ] ]    the main operating system of the cluster is suse linux 7.1 , being installed on each node .",
    "job management is done by the `` main '' computer through the 100  mbit ethernet network .",
    "we developed a simple job - management utility to distribute jobs on the nodes and collect the results .    to take advantage of the gigabit communication from applications ( e.g. c , c++ or fortran ) ,",
    "a simple c library was written using the standard linux network interface .",
    "currently , we are using a standard socket based communication with transmission control protocol ( tcp ) widely used in network applications .",
    "the typical bandwidth that we can reach in qcd applications is around 400  mbit / s in contrast to the theoretical 1000  mbit / s .",
    "writing a lower level driver for the gigabit cards which would increase the bandwidth is in progress .",
    "the functions of the library can be used to open and close the communcation channels and transfer data between the neighbors .",
    "2 ) system as a function of the local lattice volume v=@xmath3 ( lower panel ) .",
    "squares show the performance of a myrinet , triangles that of a gigabit sytem .",
    "the one - node performance is also shown by stars .",
    "communication is of no use if the one - node performance is better than the 4-node one .",
    "thus , the useful region is given by local lattices @xmath4 or larger .",
    "in this useful region the relative difference between the performances of the myrinet and gigabit systems ( upper panel ) is always less than 12% .",
    "[ myrinet],width=230 ]",
    "figure [ perf ] gives a summary of our benchmark runs .",
    "single precision is used for local variables ( gauge links and vectors , see @xcite ) and dot products are accumulated in double precision . using double precision gauge links and vectors reduces the performance by approximately 50% .    for benchmarking we started from the milc code  @xcite .",
    "to increase the performance of the code we modified it by three different techniques .",
    "first of all , we used the `` multimedia extension '' ( sse ) instructions of the intel - p4 processors as we pointed out it in 1999 @xcite this capability can accelerate the processor by a large factor .",
    "we rewrote almost the whole conjugate gradient part of the program to assembly ( including also loops over the lattice and not only elementary matrix operations ) .",
    "this way we obtained a speedup factor of @xmath02 .",
    "( see similar results of e.g. ref .",
    "@xcite ) .",
    "secondly , an important speedup was obtained by changing the data structure as suggested by s. gottlieb @xcite . in the original `` site",
    "major '' concept all the physical variables of a given site are stored in one structure and the lattice is an array of these structures . instead of this concept",
    "one should use `` field major '' variables .",
    "the set of a given type of variable of the different sites are collected and stored sequentially .",
    "this increases the number of cache hits .",
    "similarly to ref .",
    "@xcite this change leads to a speedup factor of @xmath02 .",
    "the third ingredient of our improvement was a better cache management by extensive use of the `` prefetch '' instruction .",
    "one of the most obvious features of figure [ perf ] is the sharp drop of the performance when one turns on the communication .",
    "the most economic solution is to turn on the communication only if the memory is insufficient for the single - node mode .",
    "smaller local lattices can be also studied by using the communication ( for instance for thermalization or parameter tuning ) ; however , in these cases the communication overhead increases somewhat .",
    "the performance as a function of the local lattice volume is shown on fig .",
    "[ myrinet ] ( we also made a direct comparison between our architecture and a myrinet system at desy , hamburg ) .",
    "two orders of magnitude smaller local lattice means factor of @xmath02 drop in the performance .",
    "thus , for local lattices as small as @xmath4 the performance is still acceptable .",
    "this indicates clearly the scalability of this architecture . note",
    "that the performance of a myrinet based system is only 5 - 10% better than that of the gige based system .",
    "we reportde on the status of our pc - based parallel computer project for lattice qcd ( for more details see ref .",
    "nearest - neighbor communication is implemented in a two - dimensional mesh using gigabit ethernet cards .",
    "this architecture presents a good compromise between computation and communication .",
    "the saturated sustained performances on large lattices are around 0.5(0.8 ) gflops / node for staggered ( wilson ) fermions , which gives a price / performance ratio better than $ 1.0(0.7)/mflop .    *",
    "* we thank f.  csikor and z.  horvth for their continuous help .",
    "this work was supported by hungarian science foundation grants under contract nos .",
    "otka - t37615/t34980/t29803/m37071/omfb1548/ommu-708 .",
    "99 r. alfieri et al . , hep - lat/0102011 .",
    "http://phys.columbia.edu/  cqft .",
    "n. eicker et al .",
    "( proc . suppl . ) 83 ( 2000 ) 798 [ hep - lat/9909146 ] .",
    "f. csikor et al . , comp .",
    "phys . comm . 134",
    "( 2001 ) 139 [ hep - lat/9912059 ] .",
    "luo et al . , cs.dc/0109004 .",
    "n. christ , nucl .",
    "( proc . suppl . ) 83 ( 2000 ) 111 ; s. gottlieb , nucl .",
    "( proc . suppl . ) 94 ( 2001 ) 833 .",
    "m. lscher , nucl .",
    "* 106 * ( 2002 ) 21 .",
    "sg / milc.html",
    ". m. di pierro , hep - lat/0110116 .",
    "s. gottlieb , hep - lat/0112026 , hep - lat/0112038 , hep - lat/0112039 .",
    "z.  fodor , s.  d.  katz and g.  papp , hep - lat/0202030 ."
  ],
  "abstract_text": [
    "<S> a pc - based parallel computer for medium / large scale lattice qcd simulations is suggested . the etvs univ . </S>",
    "<S> , inst . theor . </S>",
    "<S> phys . cluster consists of 137 intel p4 - 1.7ghz nodes . </S>",
    "<S> gigabit ethernet cards are used for nearest neighbor communication in a two - dimensional mesh . </S>",
    "<S> the sustained performance for dynamical staggered(wilson ) quarks on large lattices is around 70(110 ) gflops . </S>",
    "<S> the exceptional price / performance ratio is below $ 1/mflop . </S>"
  ]
}