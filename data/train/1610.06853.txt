{
  "article_text": [
    "compressed sensing is a problem that arises very naturally in signal processing applications .",
    "a sparse signal @xmath9 ( a vector consisting of only @xmath10 non - zero entries ) is detected by a sensing matrix @xmath11 with @xmath12 .",
    "the goal is to recover the exact sparse vector @xmath6 from the clearly under - determined / compressed measurements @xmath13 . in applications ,",
    "the sensing matrix @xmath1 is treated as a physical device taking linear measurements of our signal @xmath6 , and we then think of @xmath14 as an undersampled measurement of @xmath6 .",
    "representative works include , for instance , @xcite and @xcite .",
    "one may also find , e.g. , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , and @xcite for comprehensive descriptions .",
    "for some visualizable applications of compressed sensing theory , there are among others , @xcite , @xcite , @xcite on magnetic resonance imaging ( mri ) , and @xcite , @xcite , @xcite , @xcite , etc . on radar imaging .",
    "the task of recovering @xmath6 can be recast as a minimization problem : @xmath15 this is an np - hard combinatorial problem ( c.f .",
    "@xcite ) , and so the convex relaxation , @xmath16 is typically solved instead . here",
    "@xmath17 and @xmath18 .",
    "we will refer to as the _ basis pursuit _",
    "problem , @xcite .",
    "although numerical experiment suggests that the solution to and are equivalent , the first theoretical justification was given by candes et al who proved that solutions to and are equivalent with high probability for random matrices such as gaussian random matrices as long as @xmath19 ( @xcite @xcite ) , for some constant @xmath20 . for a more general discussion about the relationship between @xmath21 and @xmath22 , reader can refer to @xcite , @xcite , @xcite , @xcite or @xcite , @xcite and @xcite , and the references therein .",
    "we will not be discussing methods other than basis pursuit for signal recovery in this paper , but they do exist : @xcite , @xcite , @xcite , @xcite , @xcite , among others .    naturally , the problem of compressed sensing makes sense only if a unique solution to the ( @xmath23 ) problem exists and we wish to recover it by different algorithms . in general , the greater the number of non - zero entries of @xmath6 is , the more difficult the signal recovery it would be despite the uniqueness of the solution to ( @xmath23 ) .",
    "furthermore , it is well - known that we need to require @xmath24 , or @xmath0 less than half of the spark of @xmath1 , for the unique @xmath0-sparse solution to exist ( see for example @xcite , @xcite ) .",
    "we discover however that the uniqueness of @xmath21 is possible with full lebesgue measure even when @xmath25 .",
    "surprisingly , we further demonstrate that a tail minimization procedures works well in this regime .",
    "our investigation was originally motivated from a more involved problem of compressed sensing with sparse frame representations .",
    "recall that a frame is a set of vectors @xmath26 in an inner product space @xmath27 such that there exists @xmath28 with the property that @xmath29 for a general reference on frames , see , e.g. , @xcite .",
    "compressed sensing with frames is explored in e.g. @xcite , @xcite , @xcite , @xcite , @xcite , @xcite , and @xcite . in particular , @xcite establishes a condition under which signal recovery with frames succeeds .",
    "let @xmath30 be the matrix whose column vectors are the frame vectors @xmath26 .",
    "it is known that there are signals @xmath31 in practice that are naturally sparse in a frame representation @xcite , namely @xmath32 and @xmath6 is sparse . when coupled with compressed sensing methodologies , the under - determined matrix @xmath1 measures the signal @xmath31 by @xmath33 .",
    "the task is to recover @xmath31 from the known @xmath14 , @xmath1 and @xmath30 .",
    "the two typical approaches are the @xmath2-synthesis problem , see , e.g. , @xcite , @xmath34 or the @xmath2-analysis problem , e.g. , @xcite , @xmath35 note that when @xmath30 is actually a basis these methods are equivalent .",
    "an important object of study when solving these signal recovery problems is the error bound on a given recovered signal @xmath36 .",
    "the @xmath2-analysis approach has an error bound given by , under appropriate d - rip condition of @xmath1 , see , @xcite , @xcite : @xmath37 where @xmath38 is a dual frame of @xmath30 .",
    "notice in particular that the error bound depends linearly on the _ tail _ ( smallest @xmath39 entries ) of the signal : @xmath40 .",
    "since the error bound is directly proportional to the tail coefficients , minimizing the tail directly is a worthy topic of study , namely , @xmath41 one immediately sees , however , the above minimization problem is non - convex .",
    "our next natural choice is to work with an iterative approach , where at each step , we identify an estimated support set @xmath42 and and solve the following _ tail minimization problem _ :",
    "@xmath43 where @xmath42 is the estimated support of @xmath44 , and @xmath45 is the complement of @xmath42 .",
    "the procedure can have a number of variations .",
    "the simplest case is similar to the iterative hard thresholding approach , @xcite .",
    "the hard thresholding step is to find an estimated support index @xmath42 , and then solve the `` tail - min '' problem ( [ tail - min ] ) , followed by hard thresholding again for the next @xmath42 , etc .",
    "such a test was also performed over the traditional compressed sensing problem in a similar tail - minimization principle : @xmath46 to be precise , we do the following :    * tail minimization algorithm *    1 .",
    "inputs : a matrix @xmath11 and measurement vector @xmath47 . starting at iteration one , let the support @xmath48 .",
    "at the first iteration , we solve the basis pursuit problem [ p_0 ] to obtain an initial approximation @xmath49 for our signal .",
    "2 .   find the index set of the @xmath0-largest elements of @xmath49 , call it @xmath50 . solve the tail minimization problem @xmath51 or @xmath52 3 .",
    "continue for k steps : find the index set of the @xmath0-largest elements @xmath53 of @xmath54 . solve the tail minimization problem ( [ tm1 ] ) or ( [ tm2 ] ) with @xmath53 .",
    "call the solution at this iteration @xmath55 .",
    "4 .   the algorithm terminates when successive iterations differ by a small enough constant : @xmath56 .",
    "figures [ tmplot ] and [ fig2 ] below demonstrate extensive random tests and a statistics of successful recovery rate versus the sparsity level are presented for the `` tail - min '' procedure .",
    "also tested and plotted are the ( @xmath2 ) basis pursuit results , for both conventional compressed sensing , and that with sparse frame representations .",
    "for the conventional compressed sensing problem , we let @xmath1 be a gaussian random matrix . we apply both procedures , basis pursuit and tail minimization , to @xmath0-sparse signals",
    "@xmath6 , and let @xmath0 increase until both procedures fail with certainty . below are the results for @xmath57 trials .",
    "[ tmplot ]   be a gaussian random matrix , @xmath58 .",
    "we plot the sparsity @xmath0 of the signal @xmath6 against the fraction of successful signal recovery for @xmath57 trials .",
    "the dotted line is the traditional basis pursuit ( [ p_0 ] ) and the solid line is the tail minimization procedure ( [ tmin ] ) .",
    "signal recovery is considered a success if the relative error is less than a given tolerance , @xmath59.,title=\"fig : \" ]     be a gaussian random matrix , @xmath60 a fourier frame , and @xmath61 .",
    "we plot the sparsity @xmath0 of the signal @xmath6 against the fraction of successful signal recovery for @xmath57 trials .",
    "the dotted line is the @xmath2-analysis problem ( [ analysis ] ) and the solid line is the tail minimization procedure ( [ tail - min ] ) .",
    "signal recovery is considered a success if the relative error is less than a given tolerance , @xmath59 . ]",
    "it turns out that the procedure is not only doing well , but also greatly exceeding our expectation .",
    "in fact , for vectors @xmath6 whose number of non - zeros @xmath0 greatly exceeding @xmath62 for a full rank matrix @xmath1 with @xmath4 rows , the `` tail - min '' procedure still recovers them all well , uniquely , in a massive amount of random testing .",
    "it is widely known that the recovery of signals with @xmath63 is problematic since the ( @xmath23 ) problem does not have unique solution .",
    "specifically , consider a full - rank sensing matrix @xmath1  that is , rank@xmath64 .",
    "the following is a well - known result ( see e.g. @xcite or ( * ? ? ?",
    "* theorem 2.13 ) ) :    every @xmath0-sparse vector is a unique solution of ( 1.2 ) if and only if every @xmath65-columns of @xmath1 are linearly independent .    consequently , if @xmath66 , we can not distinguish all @xmath0-sparse vectors in general .",
    "for example , if @xmath67 , then any @xmath68 columns must be linearly dependent .",
    "hence , there exists @xmath69 such that @xmath70 and the support of @xmath71 has at most 65 non - zero entries .",
    "decompose @xmath72 where @xmath73 , @xmath74 , and @xmath75 .",
    "then @xmath76 , which shows that there is not enough information to distinguish a 50-sparse vector @xmath77 and a 15-sparse vector @xmath78 .",
    "the statistically 100% recovery with @xmath5 for the `` tail - min '' procedure in _ figures [ tmplot ] _ and _ [ fig2 ] _ above initiates us to investigate the reason why it happens .",
    "it turns out , a measure theoretical uniqueness solution exists for the ( @xmath23 ) problem for @xmath5 with full spark matrices @xmath1 ( see section 2 for a definition of full spark ) .",
    "we will prove that , given any @xmath0-sparse plane with @xmath79 , the solution to @xmath80 is unique for @xmath6 with @xmath7 up to a set of measure 0 in the @xmath0-sparse coordinate plane ( theorem [ maintheorem1 ] ) .    on the other hand , we show that , when @xmath81 , the traditional basis pursuit fails on a set of infinite measures in some @xmath0-sparse plane ( see section 3 ) .",
    "it comes to our attention during the investigation that @xcite presents a similar tail minimization procedure , though the analysis of the algorithm is quite different than what we go about .",
    "our recoverability studies and the convergence analysis of the tail minimization algorithm will be presented in forthcoming articles .    for the rest of the paper",
    ", we will prove our measure theoretic uniqueness theorem in section 2 and prove the failure of the basis pursuit in section 3 , both for the near spark - level sparsity .",
    "the _ spark _ of a matrix @xmath1 is the smallest number of linearly dependent columns of @xmath1 . the mathematical definition of spark can be written as @xmath82 we say that @xmath11 is _ full - spark _ if any @xmath4 columns of @xmath1 are linearly independent , i.e. spark@xmath83 .",
    "full - spark frames exist almost everywhere under many probability models and it is a dense and open set in the zariski topology in the sense that its complement is a finite union of zero sets of polynomials @xcite .",
    "given @xmath11 , for @xmath84 we define @xmath85 be the submatrix in @xmath86 formed by taking columns indexed by @xmath42 . the coordinate plane indexed by @xmath42 is defined as @xmath87 we collect all possible @xmath88 of sparsity @xmath0 as follows : @xmath89    we know that a full spark matrix can recover all @xmath0-sparse signals by @xmath21 if @xmath90 .",
    "the following theorem suggests that we can in principle recover almost all @xmath0 sparsity signals as long as @xmath79 .",
    "[ maintheorem1 ] let @xmath63 and @xmath11 be full - spark .",
    "let @xmath91 be any hyperplane in @xmath92 .",
    "then for almost everywhere @xmath93 ( with respect to the @xmath0-dimensional lebesgue measure on @xmath91 ) , @xmath6 is the unique solution of @xmath21 .",
    "that is , for @xmath94 , @xmath71 @xmath0-sparse , we have @xmath95 for almost everywhere @xmath93 .    in order to prove the theorem",
    ", we require some lemmas :    [ lem3.1 ] let @xmath6 be @xmath0-sparse . then @xmath6 is the unique solution of @xmath21 if and only if @xmath96    assume first that ( [ eq3.1 ] ) holds .",
    "suppose that @xmath94 and @xmath71 is @xmath0-sparse . then @xmath97 .",
    "if @xmath98 , then @xmath99 .",
    "but @xmath100 , so @xmath101 , which is a contradiction to ( [ eq3.1 ] ) .",
    "conversely , suppose that @xmath6 belongs to the set in ( [ eq3.1 ] ) .",
    "then @xmath102 , @xmath103 , @xmath100 , @xmath104 .",
    "we have that @xmath105 .",
    "this implies that @xmath6 can not be the unique solution of @xmath21 .",
    "this completes the proof .",
    "[ lem3.2 ] let @xmath106 .",
    "then @xmath107 for any @xmath108 such that @xmath109 .",
    "in particular , if @xmath1 is full spark , then all @xmath107 for any @xmath108 and @xmath110 .    since @xmath111 , columns in @xmath1 indexed by @xmath42 are linearly independent",
    ". thus if @xmath112 and @xmath113 , we would have @xmath114 , as any @xmath0 columns of @xmath1 are linearly independent .",
    "this implies that @xmath115    because of the previous lemma , when @xmath1 is full - spark , we have a direct sum between the subspace @xmath88 and @xmath116 , we shall denote it as @xmath117 . hence , for every @xmath118 , we have that @xmath119 for unique @xmath120 and unique @xmath121 .",
    "the following lemma will be needed in the proof .",
    "[ lem3.3 ] given @xmath91 in the theorem [ maintheorem1 ] and any @xmath122 , suppose that @xmath123 .",
    "define a map @xmath124 then @xmath125 , @xmath126 is linear and @xmath126 is one - to - one in @xmath127 .    as @xmath128 and @xmath129 , @xmath130 . the fact that @xmath126 is linear follows by the uniqueness of the representation .",
    "indeed , @xmath131 to see that it is one - to - one on @xmath132 , we suppose that @xmath133 for @xmath134 , we have @xmath135 .",
    "let @xmath136 .",
    "then we have @xmath137 .",
    "thus @xmath138 since @xmath139 and @xmath140 .",
    "but @xmath141 , so we have @xmath142 .",
    "thus @xmath143 .",
    "now we move on to the proof of the theorem :    let @xmath144 be a plane . using lemma [ lem3.1 ] , @xmath93 is _ not _ the unique solution of @xmath21 if and only if @xmath6 belongs to the union defined in ( [ eq3.1 ] ) .",
    "we now decompose the the union in ( [ eq3.1 ] ) into two sets :    @xmath145 , \\",
    "x_2 : =    h_{t_0 } \\cap \\bigg [ \\underset{|t| \\leq s \\atop \\#(t_0 \\cup t ) > m}\\bigcup h_t + ( \\ker{a } \\setminus \\left\\{0\\right\\})\\bigg].\\ ] ] we first claim that @xmath146 . indeed ,",
    "if @xmath128 and @xmath147 for some @xmath148 , then @xmath149 , where the sparsity of @xmath150 . but @xmath1 is full - spark and so these vectors indexed by @xmath151 are linearly independent , so @xmath152 .",
    "this forces @xmath153 is empty .",
    "hence , the union in ( [ eq3.1 ] ) is just @xmath154 .",
    "suppose that this union has positive lebesgue measure in @xmath91 .",
    "we enlarge the set by considering the zero vector in .",
    "@xmath155 consider the set @xmath156 then @xmath157 has positive measure in @xmath91 .",
    "there exists some @xmath42 such that @xmath104 and @xmath158 with the property that @xmath159 in @xmath91 , where @xmath160 denotes the lebesgue measure .",
    "since @xmath161 is a subspace of @xmath91 , positive measure implies that @xmath162 .",
    "thus @xmath163 .    by lemma [ lem3.3 ]",
    ", @xmath164 is one - to - one .",
    "we now compute the dimensions of the following subspaces .",
    "since @xmath127 is a hyperplane , we have @xmath165 note that @xmath166 . because @xmath1 is full - spark",
    ", we have @xmath167 but @xmath168 , @xmath169 then we have that @xmath170 note that @xmath171 and @xmath172 , @xmath173 this forces @xmath174 .",
    "this is a contradiction to our assumption of @xmath0 .",
    "hence , we must have @xmath175 , completing the proof .    indeed",
    ", we can replace @xmath4 by spark@xmath176 .",
    "we have the same conclusion as in theorem [ maintheorem1 ]    [ maintheorem2 ] let @xmath11 and let @xmath0 be such that @xmath177 let @xmath91 be any hyperplane in @xmath92 . then for almost everywhere @xmath93 ( with respect to the @xmath0-dimensional lebesgue measure on @xmath88 ) , @xmath6 is the unique solution of @xmath21 .",
    "following the same proof of theorem [ maintheorem1 ] until ( [ eq3.3 ] ) , we note that @xmath178 otherwise , if @xmath179 , then @xmath180 will contain rank@xmath181 + 1 linearly independent vectors , which contradicts to the definition of the rank . hence , @xmath182 continuing the same proof and we finally arrives at @xmath183 , contradicting our initial assumption .",
    "as we have indicated in the introduction , uniqueness solution is possible over the traditional regime of compressed sensing . from the numerical result through a random choice of initial @xmath0-sparse @xmath6 , the measure theoretical conclusion shows that the probability of choosing @xmath6 as a non - unique solution of @xmath21 is 0 .",
    "therefore , recovery of @xmath6 in @xmath184 is possible .    comparing the basis pursuit which fails far before @xmath62 ,",
    "our @xmath2 tail minimization approach is not only capable of recovery vectors of near spark - level sparsity , but also going over to recover signals in @xmath185 regime .",
    "in this section , we provide a justification that the basis pursuit problem ( 1.2 ) has no unique solution on a significantly large set when @xmath63 and @xmath1 is a sensing matrix with real - valued entries .",
    "we first recall that the null space property is a necessary and sufficient condition for recovery of signals via basis pursuit .",
    "when the nsp of order @xmath0 holds , then every @xmath0-sparse vector is the unique solution to @xmath21 . because if @xmath189 is a solution to @xmath190 and @xmath6 is the solution to @xmath22 , then @xmath191 .",
    "but every @xmath0-sparse vector is the unique solution to @xmath22 , @xmath192 .",
    "since @xmath5 , by proposition [ prop_larges_nspfails ] , the nsp for some @xmath198 fails .",
    "there exists some @xmath200 such that @xmath201 . by proposition [ prop_singlevecrecovery ] , @xmath93 is recovered as a unique solution of @xmath22 if and only if @xmath202 now , @xmath201 for some @xmath203 . if we consider @xmath93 such that sgn(@xmath204 , then the left hand side of ( [ eqn_singlevecrecovery ] ) becomes @xmath205 .",
    "this shows all such @xmath6 are not recovered by @xmath22 .",
    "let @xmath206 be the sign such that @xmath207 .",
    "then the failure set of @xmath22 contains @xmath208 .",
    "this contains at least a quadrant in @xmath91 , which is of infinite measure .",
    "thus the statement holds .",
    "the necessity of the _ proposition [ prop_singlevecrecovery ] _ is not true over the complex field ( * ? ? ?",
    "* remark 4.29 ) .",
    "it is not known for now whether theorem [ th3.1 ] holds for @xmath209 and complex matrices @xmath1 .",
    "nonetheless , the theorem shows that in the real case , @xmath21 and @xmath22 are not equivalent on a significant set of vectors when @xmath185 .",
    "we conclude the article by stating again that the @xmath2 tail minimization can still recover sparse signals for @xmath210 .",
    "r.  giryes and m.  elad .",
    "can we allow linear dependencies in the dictionary in the sparse synthesis framework ?",
    "in _ acoustics , speech and signal processing ( icassp ) , 2013 ieee international conference on _ , pages 54595463 .",
    "ieee , 2013 ."
  ],
  "abstract_text": [
    "<S> solving compressed sensing problems relies on the properties of sparse signals . </S>",
    "<S> it is commonly assumed that the sparsity @xmath0 needs to be less than one half of the spark of the sensing matrix @xmath1 , and then the unique sparsest solution exists , and recoverable by @xmath2-minimization or related procedures . </S>",
    "<S> we discover , however , a measure theoretical uniqueness exists for nearly spark - level sparsity from compressed measurements @xmath3 . specifically , suppose @xmath1 is of full spark with @xmath4 rows , and suppose @xmath5 . then the solution to @xmath3 is unique for @xmath6 with @xmath7 up to a set of measure 0 in every @xmath0-sparse plane . </S>",
    "<S> this phenomenon is observed and confirmed by an @xmath2-tail minimization procedure , which recovers sparse signals uniquely with @xmath8 in thousands and thousands of random tests . </S>",
    "<S> we further show instead that the mere @xmath2-minimization would actually fail if @xmath8 even from the same measure theoretical point of view . </S>"
  ]
}