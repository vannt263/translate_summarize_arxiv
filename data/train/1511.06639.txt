{
  "article_text": [
    "motivating application for the ideas presented in this paper was the use of sensor networks for structural health monitoring .",
    "an example is seen in the monitoring of concrete - based structures .",
    "sensors can be randomly embedded in concrete during the building phase of the structure , or placed on the surface of the structure .",
    "several tasks can be performed by such a network .",
    "one such task is auto - localization , which allows the tracking of the geometry of the network , and hence the detection of changes in the geometry provoked by modifications to the medium .",
    "another task is output - only modal identification . in these applications ,",
    "estimation of correlation functions can be required ( _ e.g. _ , for time - delay estimation , for power spectrum estimation ) .",
    "the context of this study is _ passive _ structural health monitoring .",
    "sensors can be , for example , micro electro - mechanical system accelerometers embedded in the propagation medium or positioned on the surface of the structure .",
    "the signals of interest are elastic waves propagated in the medium that are related to uncontrolled sources , such as microseismic waves , human - activity - induced vibrations , and others @xcite .",
    "typical distances between neighboring sensors range from metric to decametric distances .",
    "thus , relying on electromagnetic - wave - based active techniques for autolocalization is barely possible @xcite .",
    "indeed , the relative time - delay resolution would remain very poor , and the precision ( using , _ e.g. _ , received signal strength indicator - based solutions ) would not discriminate enough .",
    "electromagnetic waves are consequently used only for transmission purposes .    in the passive framework considered here",
    ", sensors can carry out some calculations and must communicate with neighbors .",
    "however , even if some sensors in the network are highlighted as anchors ( _ i.e. _ , typically wired ) , many of them are autonomous : their energy supply is finite ( _ i.e. _ , a battery ) , and calculation and communication devices need to be as economical as possible . _ the aim of this study was to design correlation function estimators using as minimal resources as possible , in terms of both calculation and communication_. note , however , that nowadays communication is more energy demanding than calculation and storage .",
    "the solutions we explore here rely on modern ideas , such as random projections , as well as old ideas , such as polarity - coincidence detectors . indeed , we combine these ideas through the design of compressed and one - bit - quantized estimators .",
    "one - bit - quantized correlation estimators date back to the 1940s , with the military research for radar .",
    "a report published in 1966 was indeed almost entirely written during world war 2 , as mentioned in its foreword @xcite .",
    "polarity - coincidence estimators work on the sign of signals instead of correlating the continuous waveforms .",
    "they perform one - bit quantization prior to any estimation . in the present context of inter - sensor correlation estimation ,",
    "one - bit - quantization allows the bit rate to be lowered for the transmission between sensors and to keep the energy consumption relatively low .",
    "as an additional means to lower data transmission requirements , we also consider compressive acquisition @xcite .",
    "this has been described over the last 20 years , whereby compressive sensing states that a small number of random linear combinations of a signal sample maintains the full information on the signal provided the signal is sparse in some dictionaries .",
    "more precisely , if the signal is sparse in a dictionary , it can be reconstructed exactly from compressed measurements .",
    "the reconstruction is based on optimization techniques that in general demand a lot of calculation resources .",
    "even if there are efficient optimization procedures nowadays , such optimization techniques are too demanding in terms of resources to be considered here .",
    "however , it was realized in recent years that if information is present in compressed samples , it is often not necessary to reconstruct the signal if a particular task is needed , such as a classification or estimation @xcite .",
    "the ideas behind these developments come from the johnson - lindenstrauss ( jl ) lemma or transform @xcite . depending on the context , the jl lemma or transform can be stated as the controlled approximate conservation of norms or inner products after random projection of vectors in lower dimensional spaces . as correlation is merely an inner product",
    ", we can expect that correlation can be correctly estimated after random projection .",
    "more precisely , following the details in @xcite , a random matrix @xmath2 is a jl transform with parameters @xmath3 , if with probability at least @xmath4 and any @xmath5-element subset @xmath6 , @xmath7 for any @xmath8 .",
    "when a matrix is a jl transform , it can in general be turned into a @xmath9-embedding of a subspace , which means that it approximately conserves the norms of all of the vectors of the subspace .",
    "for example , a matrix of size @xmath10 with independent and identically distributed ( i.i.d . )",
    "gaussian entries @xmath11 is a jl transform with parameters @xmath3 , provided that @xmath12 ( where @xmath13 is a constant ) . such a matrix can be shown to be a @xmath9-embedding with probability @xmath14 for the column space of any @xmath15 matrix @xmath16 , provided that @xmath17 @xcite . then for any @xmath18 , @xmath19 . from this last property , it can then be shown that if @xmath2 is a @xmath9-embedding for two @xmath15 matrices @xmath16 and @xmath20 , then with probability of at least @xmath14 , @xmath21 for any @xmath22 , provided of course that @xmath23 satisfies the bounds given above .",
    "this ( almost ) preservation of the inner product is at the root of what might be called compressed ( linear ) processing .",
    "this was noted in @xcite , although it appeared even earlier ; _",
    "e.g. _ , in @xcite . in the following , we make use of this property , and we define and study the correlation as the inner product of compressed vectors .",
    "a similar problem was studied by @xcite from a different perspective .",
    "this study developed here is different mainly in two points .",
    "first , the aim of the first part of our paper is the estimation of the statistics of some vectors , and secondly , the asymptotic analysis we provide here is given at a fixed compression rate , which is not the case in @xcite .",
    "finally , the study of @xcite was restricted to a particular class of random matrices , whereas the analysis developed here concerns any random matrices with i.i.d .",
    "interestingly , however , as in @xcite , we insist on the importance of the fourth - order cumulant of the entries of the random matrix .",
    "the approach taken here is different from correlation - matching approaches , as developed _",
    "e.g. _ in @xcite ( and references therein ) . in correlation",
    "matching , a correlation matrix is searched for as a linear combination of known matrices .",
    "the parameters are estimated using an optimization procedure , and the condition under which the parameters can still be estimated from compressed measurements was studied in @xcite .",
    "coarse quantization has already been explored in the context of compressive sensing .",
    "early references included @xcite .",
    "these studies then led to several developments , such as in @xcite , to cite but a few . in almost all of these studies , the problem of reconstructing a signal from one - bit - quantized compressed samples was addressed .",
    "a notable exception was @xcite , in which classification is addressed as an application .",
    "the main results that are shown in this paper are described in the following .",
    "the correlation between two signals @xmath24 and @xmath25 is evaluated from @xmath26 dimensional vectors @xmath27 and @xmath28 that collect successive samples of the signals .",
    "the usual correlation estimate is the inner product @xmath29 , where @xmath30 represents a transposition .",
    "the compressed estimator using the random matrix @xmath31 is defined as @xmath32 .",
    "it is assumed that the entries of @xmath2 , denoted as @xmath33 or @xmath34 generically , are i.i.d .. this allows for a large choice of matrices , and even includes subsampling with replacement as a particular case .",
    "however , matrices @xmath2 used for subsampling without replacement do not satisfy the i.i.d .",
    "condition , and sampling without replacement is studied separately .    for @xmath35 to be unbiased , @xmath34 must have zero mean and variance of @xmath36 . for any @xmath2",
    ", we generalize for the bounds shown in @xcite .",
    "these results define the bounds for the loss in variance provided by compression , and they read as @xmath37   -   { \\mbox{var}}[c_n ] & \\leq &   \\frac{2}{m n^2 }   { \\mbox{e}}\\big [ \\| { \\mbox{\\boldmath$x$}}\\|^2_2\\|{\\mbox{\\boldmath$y$}}\\|^2_2\\big ]   \\nonumber \\\\&+&m    { \\mbox{cum}}_4[\\varphi ] { \\mbox{e}}\\big [ \\| { \\mbox{\\boldmath$x$}}\\circ { \\mbox{\\boldmath$y$}}\\|^2_2\\big ] \\end{aligned}\\ ] ] where @xmath38 is the hadamard product ( the entry - wise product of vectors ) . in @xcite , these bounds are given for a particular sparse matrix for which the fourth - order cumulant is negative ( and is thus omitted ) .",
    "interestingly , these bounds indicate that compression leads to a loss that is at most of the same order @xmath39 as the variance of the usual estimator , using @xmath0 consecutive samples .",
    "this holds also for very sparse matrices .",
    "we then quantify the loss ( or gain ) obtained by compression compared to the usual estimator calculated on @xmath0 consecutive samples .",
    "indeed , compression from @xmath26 down to @xmath0 samples is interesting not only if the quality of @xmath35 is not degraded too much compared to the quality of @xmath40 , but also if the quality of @xmath35 is better than the quality of the usual estimate on @xmath0 consecutive samples .",
    "we provide some arguments that show that the further away from white noise the signals are , the greater the advantage of compression .",
    "this means that sparsity in the spectral domain is an important hypothesis for good behavior of compressed estimates .",
    "the results are shown by studying the asymptotics @xmath41 at a fixed compression rate @xmath42 .",
    "the choice of the compression matrix is important . in our context of limited calculation resources , using very sparse matrices or subsampling strategies is very interesting .",
    "compared to full matrices , such as gaussian or bernoulli matrices , the loss in variance is larger , although it remains reasonable .    for the compressed and quantized estimates ,",
    "the definitions are the same as before , although they are calculated using the sign of the signals .",
    "they are given by @xmath43 and @xmath44 , where the sign function applies entry wise .",
    "even in the gaussian case , the bias of the compressed and quantized estimator is out of reach analytically .",
    "this is because there is no simple closed form for the probability mass of high dimensional gaussian vectors in an orthant .",
    "the problem is even more difficult for the variance .",
    "however , we argue that in many situations , some hints on the behavior of these estimators can be given . indeed , the compression matrix is useful , as it mixes random variables : @xmath45 can practically be considered as gaussian due to the central limit theorem .",
    "this is valid when the matrix is full and @xmath27 is arbitrary ( the dependence structure between its components must be soft ) , or when the matrix is sparse and we restrict the signals to gaussian . in these situations ,",
    "the mean of the quantized estimators is proportional to the arcsin of the correlation function targeted ( _ i.e. _ , the arcsin law ) .",
    "the results are presented as follows . in section [",
    "estimation : sec ] , we first develop and study the different compressed estimators .",
    "the statistics for finite sample size are given , and then they are studied in the asymptotic regime at a fixed compression rate .",
    "the influence of the compression matrix is highlighted . as subsampling without replacement",
    "can not be studied within the random projection framework , special treatment is devoted to it . in this section ,",
    "we illustrate all of the findings by studying the ar(1 ) case in detail . in section [ quantized : sec ] , we turn to the one - bit quantized version of the compressed estimates .",
    "the analysis of the quality of the estimates is essentially empirical .",
    "we conclude this section with an illustration of real data that consists of vibrations recorded in a tall building .",
    "for these measurements , we show the interest of the approach for sensor networks in structural health monitoring .",
    "all of the calculations that were developed to show the results of these studies are detailed in a separate final section .",
    "let @xmath46 and @xmath47 be two jointly stationary zero - mean processes .",
    "the correlation function is @xmath48 $ ] , and @xmath49 $ ] is a fourth - order cumulant based correlation function .",
    "a basic assumption is the absolute summability of these functions @xcite , @xmath50    two sensors labelled @xmath24 and @xmath25 deliver @xmath26 consecutive samples from each of the signals .",
    "the samples are stored in vectors @xmath51 and @xmath52 , where the dependence in @xmath53 is omitted in the notation for the sake of clarity . in the medium where the sensors are located ,",
    "the signals @xmath24 and @xmath25 are carried by some physical waves ( _ e.g. _ , acoustic , elastic ) .",
    "the delay @xmath54 is the delay of propagation of the waves between sensors @xmath24 and @xmath25 .",
    "the usual empirical estimate of the correlation function is @xmath55 .    to obtain a compressed estimator ,",
    "the vectors @xmath27 and @xmath28 from @xmath56 are embedded into @xmath57 , with @xmath58 using a random matrix .",
    "let @xmath2 be this random matrix of dimension @xmath59 .",
    "we assume the entries @xmath33 of @xmath2 are identically and independently distributed with zero mean .",
    "the distribution of the entries is not yet specified ( @xmath34 without indices stands for a variable independent of @xmath33 , and is distributed as @xmath33 ) .",
    "we then form the compressed estimator as @xmath60 .",
    "we also consider the usual estimator evaluated on @xmath0 successive samples and denoted as @xmath61 .",
    "finally , we will also consider later a compressed estimator @xmath62 based on subsampling without replacement .",
    "this compressed estimator however does not fit into the general framework based on random embedding , and will be studied separately .      the statistics of @xmath63 are well documented and can be found in any classical statistical signal - processing textbook ( _ e.g. _ , @xcite ) . to sum these up , the first - order and second - order statistics are @xmath64 & = & \\gamma_{xy}(\\tau ) \\nonumber \\\\ { \\mbox{var}}[c_{n , xy}(\\tau ) ] & = &   \\frac{1}{n^2 } \\sum_{k =- n}^{n } \\left ( n-|k|\\right ) f_{xy}^\\tau(k ) \\label{statcor : eq}\\end{aligned}\\ ] ] where @xmath65 .",
    "as is well known , the usual empirical estimate is unbiased and its variance has the usual @xmath66 rate , provided the processes are mixing in some sense ( the correlation functions rapidly decrease to zero at infinity ) .",
    "this condition is provided by the assumption of absolute summability made earlier , which ensures that @xmath67 is summable .",
    "the evaluation of the same statistics for the compressed estimator is not difficult , but it requires some care .",
    "evaluation of the mean leads to @xmath68=mn .   { \\mbox{e}}[\\varphi^2 ] \\gamma_{xy}(\\tau)$ ] , which implies the unbiasedness condition @xmath69 = 1 \\label{unbiasedness : eq}\\end{aligned}\\ ] ] for the variance , the calculations detailed in section [ proof : sec ] lead to @xmath37   & =   & { \\mbox{var}}[c_n ] + m { \\mbox{cum}}_4[\\varphi ] \\sum_\\alpha { \\mbox{e}}[x_\\alpha^2 y_\\alpha^2 ] \\nonumber \\\\ & + & \\frac{1}{m n^2 } \\big (      { \\mbox{e}}\\big [ \\| { \\mbox{\\boldmath$x$}}\\|^2_2\\|{\\mbox{\\boldmath$y$}}\\|^2_2\\big ] + { \\mbox{e}}\\big[({\\mbox{\\boldmath$x$}}^\\top { \\mbox{\\boldmath$y$}})^2\\big]\\big ) \\label{variancecompresseddefinter : eq}\\end{aligned}\\ ] ] where @xmath70 $ ] is the fourth - order cumulant of @xmath34 .",
    "some comments can be made at this point :    * the variance @xmath71 $ ] is ( hopefully ) greater than @xmath72 $ ] ; the increment is shown to be @xmath73 $ ] in section [ proof : sec ] . *",
    "the variance depends on @xmath2 explicitly only through the fourth - order cumulant of its entries .",
    "* we can simply bound the difference of the variance : @xmath37   -   { \\mbox{var}}[c_n ] & \\leq & m { \\mbox{cum}}_4[\\varphi ] \\sum_\\alpha { \\mbox{e}}[x_\\alpha^2 y_\\alpha^2 ] \\nonumber \\\\&+ & \\frac{2}{m n^2 }   { \\mbox{e}}\\big [ \\| { \\mbox{\\boldmath$x$}}\\|^2_2\\|{\\mbox{\\boldmath$y$}}\\|^2_2\\big ] \\big ] \\\\ & \\leq&\\frac{2}{m n^2 }   { \\mbox{e}}\\big [ \\| { \\mbox{\\boldmath$x$}}\\|^2_2\\|{\\mbox{\\boldmath$y$}}\\|^2_2\\big ]   \\ ] ]    where the first inequality follows from the cauchy - schwartz inequality , and the second inequality is valid only when @xmath70 \\leq 0 $ ] ; this is the case for matrices with gaussian entries , uniform entries , bernoulli entries , and several bounded random entries .",
    "the inequality shows in these cases that the loss incurred by compression is no more than a @xmath39 order term that is an interesting guarantee ( indeed , it is shown in section [ proof : sec ] that @xmath74 = o(1)$ ] ) .",
    "this @xmath39 order term is of the same order as the variance of @xmath61 , the usual estimator using @xmath0 consecutive samples .",
    "therefore , in the worst situations , the compressed estimator will perform as well as @xmath61 , the correlation estimator evaluated on @xmath0 consecutive samples .",
    "we will see later , however , that it can be much more efficient than @xmath61 .    to obtain the behavior of the variance as a function of @xmath26 and @xmath0 ,",
    "the expectations in equation ( [ variancecompresseddefinter : eq ] ) must be further developed . using the developments made in section [ proof : sec ] , we get @xmath75   & = & ( 1+\\frac{1}{m } )    { \\mbox{var}}[c_{n }   ] \\nonumber   \\\\ & + & mn   { \\mbox{cum}}_4 [ \\varphi ]   \\left(g_{xy}^\\tau(0)+\\gamma_{xx}(0)\\gamma_{yy}(0)\\right)\\nonumber \\\\ & + & \\frac{1}{m } \\big ( \\gamma_{xx}(0)\\gamma_{yy}(0)+\\gamma_{xy}(\\tau)^2\\nonumber   \\\\ & & + \\frac{1}{n^2 } \\sum_{k =- n}^{n } \\left ( n-|k|\\right )",
    "g_{xy}^\\tau(k )   \\big ) \\label{variancecompresseddef : eq}\\end{aligned}\\ ] ] where @xmath76 . in the following ,",
    "we detail some of the consequences of these results .",
    "we study the estimates when @xmath26 and @xmath0 go to infinity for a fixed compression rate @xmath77 .",
    "the absolute summability of the second - order and fourth - order correlation function implies absolute summability of @xmath67 and @xmath78 ( defined respectively in eqs .",
    "( [ statcor : eq ] ) and ( [ variancecompresseddef : eq ] ) ) . invoking",
    "the lebesgue dominated - convergence theorem leads to @xmath79&=&\\sum_{k =- n}^{n } \\left ( 1-\\frac{|k|}{n}\\right ) f_{xy}^\\tau(k ) \\\\ &",
    "\\stackrel{n\\rightarrow + \\infty}{\\longrightarrow } & \\sum_{k\\in { \\mathbb{z } } }   f_{xy}^\\tau(k )   : = v(\\tau)\\end{aligned}\\ ] ] likewise , we have @xmath80   & = & \\frac{n}{m }    \\sum_{k =- m}^{m } \\left ( 1-\\frac{|k|}{m}\\right ) f_{xy}^\\tau(k)\\\\ & \\stackrel{n\\rightarrow + \\infty}{\\longrightarrow}&\\alpha v(\\tau)\\end{aligned}\\ ] ] and @xmath81 & = & v(\\tau)+\\alpha   \\left ( \\gamma_{xx}(0)\\gamma_{yy}(0)+\\gamma_{xy}(\\tau)^2\\right ) \\\\",
    "& + & \\left ( \\gamma_{xx}(0)\\gamma_{yy}(0)+g_{xy}^\\tau(0)\\right ) c_{4,\\varphi}\\end{aligned}\\ ] ] where @xmath82 $ ] , assuming it exists .    to compare the estimators ,",
    "it is interesting to evaluate what the variance loss is between @xmath83 and @xmath84 , and also between @xmath84 and @xmath85 . indeed , compression is interesting not only if @xmath86 -{\\mbox{var}}[c_{n , xy}(\\tau ) ] )   \\\\ & = & \\alpha   \\left ( \\gamma_{xx}(0)\\gamma_{yy}(0)+\\gamma_{xy}(\\tau)^2\\right ) \\\\ & + &   \\left ( \\gamma_{xx}(0)\\gamma_{yy}(0)+g_{xy}^\\tau(0)\\right ) c_{4,\\varphi}\\end{aligned}\\ ] ] is small , but also if @xmath87 compares favorably to @xmath88 .",
    "thus , we evaluate @xmath89 -{\\mbox{var}}[c_{m , xy}(\\tau ) ] ) \\nonumber \\\\   & = & \\delta ( c_n , c_n )   \\nonumber \\\\   & + & \\lim_{n\\rightarrow + \\infty } n({\\mbox{var}}[c_{n , xy}(\\tau ) ] -{\\mbox{var}}[c_{m , xy}(\\tau ) ] ) \\nonumber \\\\ & = & ( 1-\\alpha ) v(\\tau )   + \\alpha   \\left ( \\gamma_{xx}(0)\\gamma_{yy}(0)+\\gamma_{xy}(\\tau)^2\\right)\\nonumber   \\\\ & + & \\left ( \\gamma_{xx}(0)\\gamma_{yy}(0)+g_{xy}^\\tau(0)\\right ) c_{4,\\varphi }     \\label{deltac : eq}\\end{aligned}\\ ] ] hence , if @xmath90 , the compressed estimator is better than the usual estimator evaluated on @xmath0 points .    if the signals are jointly gaussian and if we denote @xmath91 as the normalized correlation function , the preceding equation ( [ deltac : eq ] ) implies that @xmath92 if and only if @xmath93 in many applications , we are interested in estimating the auto - correlation function . in this case , @xmath94 , and the loss in variance then reads as @xmath95 therefore , for a given @xmath54 , the lower @xmath96 the greater @xmath97 , and the worse the compressed estimator . for @xmath98 ,",
    "this expression reduces to @xmath99 , which is always greater than or equal to 2 ( because @xmath100 ) .",
    "equality occurs when @xmath101 ; _ i.e. _ , when @xmath24 is white noise .    when @xmath102 , @xmath103 is the convolution @xmath104 evaluated at @xmath105 .",
    "if @xmath106 has a very rapid decay to zero ( much more rapid than @xmath54 ) , the convolution at @xmath105 is very small , @xmath107 is large , and the compressed estimator behaves poorly .    in conclusion , if the process is close to white noise , the compressed estimator behaves poorly , whereas the conclusion is reversed if the correlation function of the process is far from a dirac function. we study these arguments more precisely using an ar model in subsection [ arstudy : ssec ] .",
    "as seen in the variance expression , the distribution of the entries of the random matrix enters through its fourth - order cumulant and is constrained to have zero mean and a second - order moment of @xmath108 ( unbiasedness condition of eq .",
    "[ unbiasedness : eq ] ) .",
    "furthermore , one of the goals in this study is to minimize local calculations as much as possible .",
    "in light of these constraints , we discuss some different distributions .    * gaussian entries * @xmath109 : the advantage of this choice is to eliminate the term in @xmath70 $ ] in the variance of the compressed estimator .",
    "a drawback when it comes to implement this choice on some chips is the high complexity required , both as storage capacity and calculation requirements .",
    "a gaussian matrix is full , and obtaining the compressed vector requires @xmath110 multiplications .",
    "* bernoulli entries * @xmath111 equiprobably : the fourth - order cumulant is @xmath112 , and therefore the term @xmath113 $ ] behaves as @xmath114 , which leads to @xmath115 . using",
    "this matrix is easy , as no multiplication is required to obtain the compressed vectors .",
    "however , the matrix is full and it requires high - capacity storage .    * sparse matrices * : if the ternary distribution @xmath116 with probability @xmath114 , and @xmath117 with probability @xmath118 , are used , @xmath119=1/2 - 3m^{-1}$ ] and @xmath120 : this increases the variance loss . however , the resulting matrix is very sparse , and the calculations are easy ( there are a mean of @xmath121 nonzero elements among the @xmath122 entries of the matrix ) .",
    "this class of matrices was studied in a similar context in @xcite .",
    "let us note that an equivalent matrix can be used that contains exactly @xmath26 nonzero elements , and has the same statistical characteristics .",
    "this matrix is defined as follows .",
    "let @xmath123 be a stochastic process defined on @xmath124 with values in @xmath125 .",
    "the @xmath123 are supposed to be i.i.d . , and uniformly distributed .",
    "let @xmath126 be another i.i.d .",
    "stochastic process on @xmath124 , but taking values @xmath127 equiprobably . @xmath128 and @xmath129 are assumed to be independent .",
    "let a matrix @xmath2 have entries @xmath130 : each column has only one nonzero element , chosen equiprobably in @xmath131 , the value being @xmath127 equiprobably .",
    "this matrix has zero mean entries , @xmath132= 0 $ ] , as @xmath128 and @xmath129 are independent , and @xmath128 has zero mean . @xmath133=",
    "1/m$ ] as @xmath128 and @xmath129 are independent , and @xmath128 has variance 1 , and @xmath134=1/m$ ] .",
    "thus , to obtain the unbiasedness condition , @xmath135 must be used .",
    "likewise , @xmath136= 1/m$ ] and then @xmath70= ( 1 - 3/m)/m>0 $ ] .",
    "indeed this @xmath135 has the same statistics as the ternary @xmath137 , with probability law @xmath138 .",
    "the only difference is that the number of nonzero elements is @xmath26 almost surely . for this choice",
    ", @xmath119=1 - 3m^{-1}$ ] leads to an additional term in the variance loss , as @xmath139 .    in the simulations that follow ,",
    "the first choice of ternary distribution @xmath140 leading to @xmath120 is considered . however , as a practical implementation , the second choice @xmath137 is preferable due to its ease of implementation using tables , and its gain of @xmath26 zero term ( in the mean ) .",
    "* subsampling with replacement * : another simple way to compress is to randomly subsample the vectors by sampling without or with replacement @xmath0 samples out of the @xmath26 samples of the vectors",
    ". sampling without replacement is treated separately in section [ subsampwor : ssec ] . for sampling with replacement",
    ", it suffices to consider the same construction made above for a very sparse matrix .",
    "let @xmath141 be a stochastic process defined on @xmath125 with values of @xmath124 .",
    "the @xmath123 are supposed to be i.i.d .",
    "and uniformly distributed .",
    "let @xmath142 be another i.i.d .",
    "stochastic process on @xmath125 , but taking values @xmath127 equiprobably . @xmath143 and @xmath144 are assumed to be independent .",
    "let a matrix @xmath2 have entries @xmath145 : each row has only one nonzero element ( chosen equiprobably in @xmath146 ) , the value being @xmath127 equiprobably .",
    "this matrix has zero mean entries , @xmath132= 0 $ ] , as @xmath143 and @xmath144 are independent , and @xmath143 has zero mean .",
    "@xmath133= 1/n$ ] as @xmath128 and @xmath129 are independent , and @xmath128 has variance 1 , and @xmath134=1/n$ ] .",
    "thus , to obtain the unbiasedness condition , @xmath147 must be used . in this situation , @xmath119=nm^{-1}-3m^{-1}=\\alpha   -3m^{-1}$ ] , and therefore this choice gives an additional variance loss with @xmath148 , which can be relatively high for high compression loss ( recall however that it is a @xmath149 term ) .",
    "subsampling without replacement can be written as an embedding with a particular random matrix , although this matrix does not fulfill the hypotheses required in the framework adopted above : in a matrix @xmath2 under sampling without replacement , each row contains exactly one nonzero element , although no two rows can have the same nonzero element .",
    "thus , such a random matrix can not have independent rows , and its elements can not be i.i.d .. a separate analysis must be made , which is detailed in section [ proofssubsamp : ssec ] .",
    "the dataset is composed of @xmath150 and @xmath151 .",
    "we form the hadamard product ( the entry - wise product ) of the two vectors @xmath152 .",
    "the subsample @xmath153 is obtained uniformly at random , without replacement from @xmath154 .",
    "we form the estimator @xmath155 .",
    "we show in section [ proofssubsamp : ssec ] that @xmath156 is unbiased , and that its variance reads as @xmath157 = { \\mbox{var } } [ c_n ]   + \\frac{\\alpha-1}{n-1 } \\big ( f^\\tau_{xy}(0)-{\\mbox{var}}[c_n ] \\big)\\end{aligned}\\ ] ] where we recall that @xmath42 is the compression rate , and @xmath158 .",
    "therefore , asymptotically , the loss for sampling without replacement reads as @xmath159 - { \\mbox{var } } [ c_n ]   ) \\\\&= & ( \\alpha-1)f^\\tau_{xy}(0)\\end{aligned}\\ ] ] for the case of sampling with replacement , we have seen that @xmath160 , and we can write @xmath161 because @xmath162 . we thus see that sampling with replacement has an asymptotic variance loss of @xmath163 with respect to sampling without replacement .",
    "the analysis developed so far shows that sparsity in the frequency domain is required to obtain good behavior of the compressed estimator . to illustrate this further",
    ", we consider the case of the autoregressive process of order 1 . for this process",
    ", one parameter allows the modulation of the correlation decay rate , and hence the sparsity in the frequency domain .",
    "we consider the estimation of the correlation function of a gaussian process that follows an ar(1 ) model , @xmath164 , where @xmath165 is a sequence of i.i.d .- normalized , zero - mean gaussian variables . here , @xmath166 .",
    "the case @xmath167 corresponds to the white gaussian noise case .",
    "we estimate the autocorrelation function of @xmath24 . for this particular case , as @xmath168 , we obtain the asymptotic variance for the different estimators @xmath169 & = & 1 + ( 2\\tau+1 ) a^{2|\\tau| } + 2a^2 \\frac{1+a^{2|\\tau|}}{1-a^2 } \\\\&:= & v(\\tau ) \\\\ \\lim_{n\\rightarrow + \\infty }   n { \\mbox{var } } [ c_m ] & = & \\alpha v(\\tau )   \\\\ \\lim_{n\\rightarrow + \\infty }   n { \\mbox{var}}[c_n ] & = & v(\\tau ) + \\alpha ( 1 + a^{2|\\tau| } ) + c_{4,\\varphi}(1 + 2 a^{2|\\tau| } ) \\\\ \\lim_{n\\rightarrow + \\infty }   n { \\mbox{var}}[{{\\cal c}}_m ] & = & v(\\tau ) + ( \\alpha-1 ) ( 1 + a^{2|\\tau| } ) \\end{aligned}\\ ] ] figure ( [ cor_compressed_a : fig ] ) illustrates these asymptotic variances for @xmath170 and 0.7 , which corresponds to an increase in correlation time , and for a compression rate @xmath171 .",
    "the matrix @xmath2 that is chosen satisfies @xmath172=0 $ ] . as seen in figure ( [ cor_compressed_a : fig ] ) , increasing @xmath173 results in an improvement in @xmath35 and @xmath156 compared to @xmath61 .",
    "as discussed earlier , the larger @xmath174 the less white the process is , and the better the compressed estimators behave .",
    "this is confirmed in figure ( [ deltam : fig ] ) , which illustrates @xmath175 for @xmath98 , the difference between the variance of the compressed estimator and the usual estimator based on @xmath0 samples .",
    "the plot shows @xmath176 for three compression rates @xmath177 and @xmath178 , for two values of @xmath179 .",
    "thus , the singularity in each curve corresponds to a change of sign in @xmath180 : for each curve , @xmath181 is to the left of the singularity , and it is negative to the right .",
    "when @xmath182 increases , the singularity shifts to the left . finally , for @xmath183 only , the nonasymptotic result is superposed , obtained here for @xmath184 .",
    "there is good agreement between the asymptotic and nonasymptotic .",
    "note that @xmath180 always decreases as a function of @xmath173 in the asymptotic regime .",
    "this is not the case in the nonasymptotic analysis ; indeed , when @xmath185 , the process is a constant , and random compression can not be better than the usual average estimation .",
    "the position of the singularity can be easily found in this example . for @xmath98",
    ", a rapid calculation leads to @xmath186 the @xmath187 zone where the compression is interesting is displayed in gray in the left plot of figure ( [ deltam : fig ] ) . for @xmath188 and 50",
    ", the compressed estimator outperforms the usual estimator based on @xmath0 samples as soon as @xmath173 is large enough .",
    "note that in these plots the asymptotic curves are given for @xmath183 ( gaussian or bernoulli random matrix ) as well as for @xmath120 ( ternary random matrix ) .",
    "this illustrates the weak influence of the matrix choice on the improvement , and opens the way to a dramatic decrease in the computational needs for a given performance .    , for @xmath40 , @xmath61 , and @xmath35 with a gaussian random matrix , and for @xmath156 , the estimator based on sampling without replacement .",
    "the compressed estimator of the @xmath26 to @xmath0 samples can outperform the usual estimator based on @xmath0 samples if the signal is sufficiently correlated .",
    "the compression rate chosen is @xmath171 . ]     as a function of @xmath173 for three possible compression rates of @xmath189 and 50 ( right to left ) , and for @xmath98 .",
    "the thin dashed line corresponds to the evaluation of @xmath190 for finite @xmath191 calculations ( here @xmath184 ) .",
    "the thick lines correspond to the asymptotic limits ( dashed for the ternary @xmath192 , continuous for the matrices with @xmath183 ) . for each curve ,",
    "the part to the left of the singularity corresponds to @xmath193 , whereas the right part corresponds to @xmath194 , for which the compressed estimator of the @xmath26 to @xmath0 samples is better than the usual estimator on @xmath0 samples .",
    "right plot : curves @xmath195 delimiting the zones of @xmath196 in gray , and the zones of @xmath193 in white .",
    "the curves plotted are easily shown to satisfy @xmath197 . ]",
    "the effects reported here have a simple interpretation .",
    "if the signals are white noise , the evaluation of the correlation function using @xmath0 consecutive samples or @xmath0 randomly chosen samples over a window of length @xmath26 will be equivalent .",
    "furthermore , using linear combinations of these @xmath0 randomly chosen samples will degrade the quality of the estimation a little .",
    "in contrast , if the signals are highly correlated ( in time ) , @xmath0 consecutive samples provide less information than @xmath0 samples chosen irregularly from @xmath26 consecutive samples .",
    "the interesting point used here is that this remains true if we use @xmath0 random linear combinations of @xmath26 samples .",
    "furthermore , as illustrated previously , the longer the correlation time the higher the gain of compression .",
    "this can be viewed as an illustration of sparsity in the frequency domain . in the example developed , the longer the correlation time",
    ", the less frequency bands occupied .",
    "it is interesting that this can also be linked to compressibility in a coding sense , as a high correlation time corresponds to a low information content and leads to a high rate coding .",
    "it is well known that the statistical information content of zero crossings of a stochastic process is very close to the information content of the process itself .",
    "this led studies in the 1950s to implement correlation estimates of a process using one - bit quantized measurements .",
    "this was done at that time for ease of computation using analog devices , although this methodology has now been replaced by the usual correlation estimates due to the increase in digital computational resources .",
    "however , in the era of sensor networks that demand high resources in communication , this one - bit quantization signal processing methodology has a lot to offer . here",
    ", we empirically demonstrate that joining these old ideas to the new ideas of compressive measurements can dramatically decrease the need for computation and communication resources for correlation estimation and time - delay estimation in sensor networks .",
    "we consider the same setting as in the previous section , except that all of the data are now one - bit quantized . for any variable @xmath198 ,",
    "we denote @xmath199 , the variable that is + 1 if @xmath200 and @xmath201 if @xmath202 .",
    "the same notation is adopted for vectors , knowing that the operation is applied element - wise .",
    "then , we consider the estimators @xmath203 , @xmath204 , and @xmath205 for the subsampling without replacement .",
    "the analysis of these estimators is more tedious than before .",
    "however , in the gaussian case , some elements can be put forward to justify the use of these estimators .",
    "therefore , in what follows in this section , we assume that the processes under study are jointly gaussian .",
    "it is well known ( _ e.g. _ , @xcite ) that for two normalized ( _ i.e. _ , zero mean and unit variance ) jointly gaussian random variables @xmath173 and @xmath206 , the correlation between their signed versions is given by @xmath207=(2/\\pi)\\arcsin({\\mbox{e}}[ab])$ ] . using this result",
    ", we can evaluate the mean of the usual estimators using one - bit quantized measurements , and we get @xmath208 & = & \\frac{2}{\\pi } \\arcsin \\frac{\\gamma_{xy}(\\tau)}{\\sqrt{\\gamma_{xx}(0 ) } \\sqrt{\\gamma_{yy}(0 ) } }    \\end{aligned}\\ ] ] the mean of @xmath205 can be easily obtained ( arguments for this will be detailed shortly ) , and it turns out to be equal to @xmath209 $ ] .",
    "however , for the compressed estimator @xmath210 , even if @xmath27 and @xmath28 are jointly gaussian , the embedded vectors @xmath45 and @xmath211 are not jointly gaussian , and we can not apply the @xmath212 law .    however , we can make some comments here :    * the compressed and quantized estimator reads @xmath213 .",
    "when the matrix @xmath2 is full , which is the case for a gaussian matrix or a bernoulli matrix , we can expect that the variates @xmath214 and @xmath215 obey jointly a central limit theorem . indeed , under the hypothesis made , the signals @xmath24 and @xmath25 are mixing , which means that the correlation decays sufficiently fast in time , and the random matrix is independent of the signals .",
    "thus , it is expected than when @xmath26 is large , we can apply the @xmath212 law , even if the signals are not gaussian . *",
    "if sparse matrices are used , the preceding comment is likely to fail . in this case , when the signals are gaussian , it is likely that @xmath216 and @xmath217 will remain gaussian ( conditional to the matrix ) .",
    "this is truly the case with sampling without replacement .",
    "hence in this case again , we can apply the @xmath212 law . * in any other case",
    ", we do not control the statistics of the quantized estimates . *",
    "the one - bit quantized estimators are insensitive to the power of the signals analyzed .",
    "this is reflected in the fact that they provide an estimate of the correlation function in place of the covariance function , as seen in the expression of the mean of the estimates .    based on the previous comments ,",
    "the estimators are modified to take into account the distorsion .",
    "we define here @xmath218 we know from the delta method @xcite that the modified estimators behave correctly if the unmodified estimators do so ; _ i.e. _ , it statisfies a usual central limit theorem .",
    "this is obtained if the random variables @xmath219 form a sufficiently mixing sequence . in this case , assuming that @xmath220 converges in law to @xmath221 , and the undistorted estimator @xmath222 converges to @xmath223 .",
    "this last form is a consequence of the delta method , and @xmath224 .",
    "the same result holds for @xmath225 and @xmath226 if we know the variance of @xmath227 and @xmath228 .",
    "unfortunately , the variance of the one - bit quantized estimators can not be evaluated in closed form , as there is no ( known ) closed form equation for the probability mass of a four - dimensional gaussian in a positive orthant @xcite , except evidently in some particular cases .",
    "we are thus not able to give an analytic form for @xmath229\\end{aligned}\\ ] ] except in very special cases . for more information on this particular point and its application to correlation estimation using clipping or quantization ,",
    "see for example @xcite and section [ clippingex : ssec ] , where we illustrate the difficulty .",
    "we show that evaluation of the variance requires either numerical integration or monte - carlo simulation .",
    "we chose the latter .",
    "for the compressed estimator @xmath210 using a random matrix , the difficulty is the same , and we can not evaluate the variance . for @xmath230 , however , it is possible to evaluate the loss due to compression with respect to @xmath231 : the calculation of @xmath232 $ ] follows the same lines as the calculation of @xmath233 $ ] , as if we replace @xmath234 with @xmath235 .",
    "we show in section [ proofssubsamp : ssec ] that for the gaussian case considered here , @xmath236 & = & { \\mbox{e}}[c^q_n ] = \\frac{2}{\\pi } \\arcsin \\frac{\\gamma_{xy}(\\tau)}{\\sqrt{\\gamma_{xx}(0 ) } \\sqrt{\\gamma_{yy}(0)}}\\\\ { \\mbox{var}}[{{\\cal c}}^q_m ] & = & { \\mbox{var } } [ c^q_n ]   + \\frac{\\alpha-1}{n-1 } \\big (   1 -{\\mbox{e}}[c^q_n]^2 -{\\mbox{var}}[c^q_n ] \\big)\\end{aligned}\\ ] ] so that @xmath237 -{\\mbox{var } } [ c^q_{n , xy}(\\tau ) ] )   =   ( \\alpha-1 ) \\big (   1 - { \\mbox{e}}[c^q_n]^2   \\big)\\end{aligned}\\ ] ]      we apply to the ar case the same methodology ( _ i.e. _ , the gaussian random matrix ) as in the preceding section , but add to the results the one - bit estimators .",
    "we illustrate the behavior of the different estimators in figure ( [ arcompquantized : fig ] ) . for these plots",
    ", we chose @xmath184 and @xmath238 , for which a compression factor of 10 is obtained .",
    "the correlation function of the ar process is evaluated over the first 20 lags , and the variance of the estimators is estimated by averaging over 1,000 independent snapshots of the process .    as seen in figure ( [ arcompquantized : fig],top ) , we recover the elements discussed above .",
    "the larger the ar parameter , the greater the advantage of the compression .",
    "however , quantizing the signal over one bit introduces an additional distortion . for high values of the compression factor , this distortion is high , and it can double the variance ( _ e.g. _ , see @xmath239 ) .",
    "however , interestingly , when the process is sufficiently correlated or compressible , the loss incurred by high quantization is still compensated for by the random acquisition of @xmath0 samples over a horizon of @xmath26 samples .",
    "we note however that quantization has a large impact at high compression rates : when comparing compressed estimators and their quantized version ( _ e.g. _ , comparing figs .",
    "( [ cor_compressed_a : fig ] ) and ( [ arcompquantized : fig],top ) for @xmath240 ) , we see that the gain obtained for the quantized version is not as large as the gain obtained using compression only .",
    "note , however , that by construction , quantized estimates have zero variance at the maximum of the correlation function ; this is important , especially for time - delay estimation .",
    "the gain in variance can appear not to be that important .",
    "however , we must stress that we want to transmit as little as possible . thus if we constraint the number of transmitted bits to m bits per correlation evaluation .",
    "if the processor used represents floats on @xmath241 bits , a fair comparison would be to compare the variance of @xmath35 for a compression rate of @xmath242 to the variance of @xmath210 for a compression rate of @xmath243 .",
    "for the example of ar(1 ) signals considered here , figure [ arcompquantized : fig],bottom ) shows these variances for @xmath244 and @xmath245 , when @xmath26 has been set to 1,024 samples . as seen in figure [ arcompquantized : fig],bottom ) , the gain is dramatic , and for example , reaches a factor of four to eight .",
    "therefore , for a fixed number of transmitted bits and a given required quality , using the one - bit - quantized compressed estimator is preferable to using the compressed estimator .     and @xmath238 ( compression factor of 10 ) .",
    "the one - bit quantization has a negligible effect for the usual estimator ( dotted line _ versus _",
    "continuous line ) . for the compressed estimators ( dashed and dashed - dotted lines )",
    ", quantization has more impact .",
    "however , when the process is sufficiently correlated ( @xmath246 , in the example ) , the quantized compressed estimator remains better than the usual estimator calculated over @xmath0 successive samples .",
    "bottom : for the ar(1 ) signal with @xmath240 , the variance of the compressed and quantized estimator for @xmath171 compared to the variance of the compressed estimator for @xmath247 and @xmath248 .",
    "this shows that for a fixed number @xmath0 of transmitted bits , using the one - bit - quantized compressed estimate is preferable to using the compressed estimate . ]     and @xmath238 ( compression factor of 10 ) .",
    "the one - bit quantization has a negligible effect for the usual estimator ( dotted line _ versus _",
    "continuous line ) . for the compressed estimators ( dashed and dashed - dotted lines )",
    ", quantization has more impact .",
    "however , when the process is sufficiently correlated ( @xmath246 , in the example ) , the quantized compressed estimator remains better than the usual estimator calculated over @xmath0 successive samples .",
    "bottom : for the ar(1 ) signal with @xmath240 , the variance of the compressed and quantized estimator for @xmath171 compared to the variance of the compressed estimator for @xmath247 and @xmath248 .",
    "this shows that for a fixed number @xmath0 of transmitted bits , using the one - bit - quantized compressed estimate is preferable to using the compressed estimate . ]      in this section the methodologies developed so far are applied to real data .",
    "we take the opportunity in this application to first discuss some technological issues regarding the methods proposed .",
    "we have proposed to use random projections or subsampling to compress in the sample space , and to use one - bit quantization to compress in the amplitude space .",
    "this allows considerable gain to be obtained in terms of the calculation and transmission loads . for evaluation of @xmath249 at sensor @xmath24 , in the theoretical analysis we used @xmath250 and @xmath251 . for the compressed/",
    "quantized estimator at sensor @xmath24 , this @xmath27 is required , as well as the vectors @xmath252 for all of the values of @xmath54 .",
    "this requires that sensor @xmath25 transmits all of the vectors @xmath252 to sensor @xmath24 .",
    "this can be expensive .",
    "an alternative is to allow the sensors to have a buffer . in this case , to evaluate @xmath249 at sensor @xmath24 , this sensor will buffer vectors @xmath253 for the values of @xmath54 required , and sensor @xmath25 will transmit only @xmath254 , where @xmath255 .",
    "thus in this set - up , if @xmath256 is the total number of lags required , the sensors should have a buffer of at least @xmath257 bits , and they have to transmit only @xmath0 bits to their neighbors .",
    "this set - up should be adopted whenever possible .",
    "indeed , in present - day technology , the more costly part in terms of energy in sensors is the transmission .",
    "calculation and storage capabilities are not very expensive .",
    "we consider here the real data recorded by accelerometers ( sf3000l ; colibrys company , www.colibrys.com ) set on the ground of the 20th floor of a tall building in grenoble , france .",
    "for the sake of illustration , we omit any comment on the units used .",
    "time , frequency , and amplitude are in arbitrary units .",
    "the signal consists of 12,500 samples .",
    "the signal , its power spectrum , and a spectrogram are shown in figure ( [ spectroarpej : fig ] ) .",
    "three well - localized harmonics show up at low frequencies . the spectrogram is illustrated to show that for the window of observation , the signal can reasonably be considered as stationary .",
    "note that the signal is far from white noise , as shown by its power spectrum .",
    "therefore , the compressed estimators are intended to behave well .",
    "evaluating @xmath258 using the full dataset allows a good reference to be obtained for the correlation function .",
    "this also allows an estimate to be produced for the mean square error @xmath259 $ ] for any estimator @xmath260 .",
    "to study compressed and quantized estimates , the signal was cut into six blocks .",
    "the different estimators are then evaluated for each of these blocks , for @xmath261 samples , and @xmath262 samples for a compression rate of @xmath182 .",
    "the different estimators are plotted in figure ( [ corarpej : fig ] ) for @xmath171 , when the matrix @xmath2 is chosen to be very sparse ( @xmath34 is distributed according to a ternary distribution ) .",
    "specifically , the estimate @xmath258 using the full dataset is depicted in the top plot of figure ( [ corarpej : fig ] ) , with no error bars as it is used as the ground truth . then displayed from top to bottom",
    "there are @xmath263 and @xmath264 : for each , the mean over the six blocks is plotted ( continuous lines ) , plus / minus twice the standard deviation ( gray shading around the mean ) evaluated for the six blocks .",
    "this allows the mean behavior to be studied , as well as the variability over the blocks .    using the six blocks we also evaluate an estimate of the root mean square error ( rmse ) integrated over @xmath54 , taking @xmath258 as the reference .",
    "the results are shown in table [ tableeqm : tab ] for @xmath265 , for @xmath177 and 20 .",
    "the loss in integrated rmse for the compressed and compressed - quantized estimators is much lower than the loss of the estimators over @xmath0 consecutive points .",
    "it is remarkable that the compressed - quantized estimator for @xmath266 provides a good estimate of the correlation .",
    "if the performance degrades at large time lags , examination of the first 50 lags shows that in this range the estimation is very good .",
    "furthermore , at low compression rates ( @xmath267 ) , the integrated rmse for the compressed quantized estimator is the same as that of the quantized estimate .",
    ".root mean square error ( with respect to the best estimates over the whole dataset ) of the estimators quantized or not using @xmath184 samples , their compressed version for @xmath188 and @xmath268 , and the usual estimator in @xmath269 samples . [",
    "cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]     [ tableeqm : tab ]     samples @xmath40 , compressed estimator @xmath35 , quantized estimator @xmath270 , compressed and quantized estimate @xmath225 using a ternary matrix and @xmath171 , subsampling without replacement and quantization @xmath264 . for these last five plots ,",
    "the full dataset was cut into six blocks over which all of the estimators were applied .",
    "the functions displayed are the mean ( black line ) over the six blocks , @xmath271 twice the standard deviation evaluated on these six blocks ( gray shading ) .",
    "the scales are the same for all of the plots . ]",
    "in this study , old ideas are married to recent ideas on dimension reduction using random projections .",
    "we provide estimates of the correlation function between two signals by correlation of the quantized compressed acquisition of the signals .",
    "the theoretical study for the compressed part shows that compression is good when the correlation under study is far from the correlation function of white noise . in this respect ,",
    "we recover the idea underlying compressed sensing , which states that the compressed measurements carry all of the information about a signal whenever the signal is sparse on some basis . we give a full second - order analysis of the compressed estimators . however , we only have empirical arguments for studying the compressed and quantized estimates .",
    "the nonlinearity makes the analysis very difficult , and out of reach when thinking of closed - form equations .",
    "however , simulations and a real case study confirm that the estimates proposed can be interesting within severe energy - constrained frameworks . indeed , quantizing over one bit in amplitude , and compression at a rate of around 10 in terms of samples , leads to results that are very good when the signal has sparse spectral content .",
    "the theoretical study has to be pursued .",
    "the main question remains to qualify the estimators when quantization is applied .",
    "for the compressed part , we did not impose any particular model for the correlation functions . to go further in the analysis , studying the behavior of the compressed estimators for",
    "particular classes of correlations might provide more guarantees .",
    "for example , supposing that the correlation is sparse in the strict sense in the fourier domain might be of interest .",
    "a second tasks is currently being developed .",
    "this consists of the evaluation of the performance of time - delay estimation based on compressed and quantized estimates .",
    "the signals that lead to good performance of the compressed estimator should be far from white noise , a property that is in contradiction with the properties required for good time - delay estimation .",
    "however , as the context here is passive monitoring , the signals used can not be controlled , and the sources of opportunity are the only sources of information to estimate delays in the propagation .",
    "@xmath272 and @xmath273 . recall for later use that @xmath274 and @xmath275 . recall also that the generic entry of @xmath2 is denoted as @xmath34 , which can be indexed if necessary",
    "we write @xmath276 we first evaluate the conditional mean and variance .",
    "we have @xmath277 & = & { \\mbox{e}}\\big[\\sum_{\\alpha,\\beta } x_{\\alpha}y_\\beta \\varphi_{k\\alpha}\\varphi_{k\\beta } \\big ] \\\\ & = & { \\mbox{e}}[\\varphi^2 ] { \\mbox{\\boldmath$x$}}^\\top { \\mbox{\\boldmath$y$}}\\end{aligned}\\ ] ] because the entries @xmath33 are i.i.d . and",
    "zero mean .",
    "thus @xmath278=m { \\mbox{e}}[\\varphi^2 ] { \\mbox{\\boldmath$x$}}^\\top { \\mbox{\\boldmath$y$}}$ ] .",
    "next the conditional variance of @xmath279 reads @xmath280 & = & \\sum_{\\alpha,\\beta,\\gamma,\\delta } x_\\alpha y_\\beta x_\\gamma y_\\delta   { \\mbox{cov } } [ \\varphi_{k\\alpha}\\varphi_{k\\beta},\\varphi_{k\\gamma}\\varphi_{k\\delta } ] \\\\   & = & \\big({\\mbox{e}}[\\varphi^4]-{\\mbox{e}}[\\varphi^2]\\big )   \\sum_\\alpha x_\\alpha^2 y_\\alpha^2   \\\\&+ & { \\mbox{e}}[\\varphi^2]^2 \\sum_{\\alpha\\not=\\beta }   \\big (   x_\\alpha^2 y_\\beta^2 + x_\\alpha y_\\alpha",
    "x_\\beta y_\\beta \\big ) \\\\   & = & { \\mbox{cum}}_4[\\varphi ] \\sum_\\alpha",
    "x_\\alpha^2 y_\\alpha^2 \\\\&+ & { \\mbox{e}}[\\varphi^2]^2 \\big (    \\| { \\mbox{\\boldmath$x$}}\\|^2_2\\|{\\mbox{\\boldmath$y$}}\\|^2_2 + ( { \\mbox{\\boldmath$x$}}^\\top { \\mbox{\\boldmath$y$}})^2\\big)\\end{aligned}\\ ] ] the second line is obtained because the @xmath33 are i.i.d . and",
    "zero mean .",
    "the sum over the four indices is then cut into four cases @xmath281 and the three circular permutations of @xmath282 .",
    "noting that @xmath279 and @xmath283 for @xmath284 are independent conditionally to @xmath285 , we obtain @xmath286 & = & m { \\mbox{cum}}_4[\\varphi ] \\sum_\\alpha x_\\alpha^2 y_\\alpha^2 \\\\&+&m { \\mbox{e}}[\\varphi^2]^2 \\big (    \\| { \\mbox{\\boldmath$x$}}\\|^2_2\\|{\\mbox{\\boldmath$y$}}\\|^2_2 + ( { \\mbox{\\boldmath$x$}}^\\top { \\mbox{\\boldmath$y$}})^2\\big)\\end{aligned}\\ ] ]    finally , we get @xmath287&=&e_{xy}\\big[{\\mbox{e}}[c|xy]\\big ]   \\nonumber \\\\ & = & m { \\mbox{e}}[\\varphi^2 ] { \\mbox{e}}[{\\mbox{\\boldmath$x$}}^\\top { \\mbox{\\boldmath$y$ } } ]   \\label{meanofc : eq}\\\\ { \\mbox{var}}[c ] & = &    e_{xy}\\left [ { \\mbox{var } } [ c | xy ] \\right ] + { \\mbox{var}}_{xy}\\left [ { \\mbox{e}}[c   | xy ] \\right ]   \\nonumber \\\\ & = & m^2{\\mbox{e}}[\\varphi^2]^2 { \\mbox{var}}[{\\mbox{\\boldmath$x$}}^\\top { \\mbox{\\boldmath$y$ } } ] + m { \\mbox{cum}}_4[\\varphi ] \\sum_\\alpha { \\mbox{e}}[x_\\alpha^2 y_\\alpha^2 ] \\nonumber \\\\&+&m { \\mbox{e}}[\\varphi^2]^2 \\big (    { \\mbox{e}}\\big [ \\| { \\mbox{\\boldmath$x$}}\\|^2_2\\|{\\mbox{\\boldmath$y$}}\\|^2_2\\big ] + { \\mbox{e}}\\big[({\\mbox{\\boldmath$x$}}^\\top { \\mbox{\\boldmath$y$}})^2\\big]\\big ) \\label{varianceofc : eq}\\end{aligned}\\ ] ]    the empirical estimate based on @xmath26 samples is given by @xmath288 and is unbiased .",
    "thus , for @xmath13 to be unbiased , examining equation ( [ meanofc : eq ] ) shows that the variance of @xmath34 must satisfy @xmath289=1/(mn)$ ] . imposing this unbiasedness condition in equation ( [ varianceofc : eq ] ) leads to @xmath290   & = & { \\mbox{var}}[c ] + m { \\mbox{cum}}_4[\\varphi ] \\sum_\\alpha { \\mbox{e}}[x_\\alpha^2 y_\\alpha^2 ] \\nonumber\\\\&+&\\frac{1}{m n^2 } \\big (    { \\mbox{e}}\\big [ \\| { \\mbox{\\boldmath$x$}}\\|^2_2\\|{\\mbox{\\boldmath$y$}}\\|^2_2\\big ] + { \\mbox{e}}\\big[({\\mbox{\\boldmath$x$}}^\\top { \\mbox{\\boldmath$y$}})^2\\big]\\big ) \\label{varianceinterm : eq}\\end{aligned}\\ ] ]    to obtain equation ( [ variancecompresseddef : eq ] ) , recall that @xmath274 and @xmath275 , define @xmath49 $ ] and @xmath76 to evaluate @xmath291   & = & n \\big ( c_{xyxy}(\\tau,0,\\tau ) + 2\\gamma_{xy}(\\tau)^2 + \\gamma_{xx}(0)\\gamma_{yy}(0 ) \\big ) \\\\   & = & n \\big ( g_{xy}^\\tau(0)+ \\gamma_{xx}(0)\\gamma_{yy}(0)\\big ) \\\\",
    "{ \\mbox{e}}\\big [ \\| { \\mbox{\\boldmath$x$}}\\|^2_2\\|{\\mbox{\\boldmath$y$}}\\|^2_2\\big ] & = & \\sum_{\\alpha,\\beta } { \\mbox{e } } [   x_\\alpha^2 y_\\beta^2 ]   \\\\   & = & \\sum_{\\alpha,\\beta }   \\big ( c_{xyxy}(\\tau+\\alpha-\\beta,0,\\tau+\\alpha-\\beta ) \\\\&+ & 2\\gamma_{xy}(\\tau+\\alpha-\\beta)^2 + \\gamma_{xx}(0)\\gamma_{yy}(0)\\big)\\\\   & = & n^2 \\gamma_{xx}(0)\\gamma_{yy}(0 ) +   \\sum_{k =- n}^n ( n-|k| ) g_{xy}^\\tau(k )   \\\\ { \\mbox{e}}\\big[({\\mbox{\\boldmath$x$}}^\\top { \\mbox{\\boldmath$y$}})^2\\big]&=&{\\mbox{var}}\\big[({\\mbox{\\boldmath$x$}}^\\top { \\mbox{\\boldmath$y$}})\\big ] + { \\mbox{e}}\\big[({\\mbox{\\boldmath$x$}}^\\top { \\mbox{\\boldmath$y$}})\\big]^2 \\\\    & = & n^2 { \\mbox{var}}\\big[c\\big]+ n^2 \\gamma_{xy}(\\tau)^2\\end{aligned}\\ ] ] where the last expression holds since the empirical estimator is an unbiased estimate of the correlation function . plugging the last three expressions in equation ( [ varianceinterm : eq ] ) leads to equation ( [ variancecompresseddef : eq ] ) .",
    "we sample uniformly at random without replacement @xmath0 elements from @xmath124 .",
    "successive samples are obtained independently .",
    "let @xmath294 be the subsample obtained .",
    "then , @xmath295 with probability @xmath296 , and for @xmath297 with probability @xmath298 .",
    "the estimator can be written as @xmath299 or equivalently @xmath300 where @xmath301 is series of bernoulli variables of parameter @xmath302 .",
    "these variables are correlated , and their correlation is given by @xmath303 = \\pi_{2}$ ] .",
    "they are supposed to be independent from the @xmath304 .",
    "the calculation of the variance of @xmath156 makes use of the law of total covariance , written for any random elements @xmath308 as @xmath309=e_z[{\\mbox{cov}}[x , y|z]]+{\\mbox{cov}}[e[x|z],e[y|z]]$ ] .",
    "we have @xmath157&=&\\frac{1}{m^2 } \\sum_{i , j } { \\mbox{cov}}[z_i\\varepsilon_i , z_j\\varepsilon_j ] \\\\ & = & \\frac{1}{m^2 } \\sum_{i , j }   e\\big[z_iz_j { \\mbox{cov}}[\\varepsilon_i,\\varepsilon_i]\\big ]   + \\frac{1}{m^2 } \\sum_{i , j }   \\pi_1 ^ 2{\\mbox{cov } } [ z_i , z_j ] \\end{aligned}\\ ] ] where the law of total covariance has been applied , and where we have used the independence between the @xmath310s and the @xmath304s . since @xmath296 , the second sum in the last expression is equal to @xmath72 $ ] .",
    "then , cutting the first sum into two parts we get @xmath157&=&{\\mbox{var}}[c_n ] + \\frac{1}{m^2}\\sum_i \\pi_1(1-\\pi_1 ) e[z_i^2 ] \\\\&+ & \\frac{1}{m^2}\\sum_{i\\not = j } ( \\pi_{2 } -\\pi_1 ^ 2 ) e[z_iz_j ] \\end{aligned}\\ ] ] replacing @xmath302 by @xmath311 and @xmath312 by @xmath313 , using the rate of compression @xmath42 then leads to @xmath157&=&{\\mbox{var}}[c_n ] + \\frac{\\alpha-1}{n-1}\\big ( \\frac{n-1}{n^2}\\sum_i   e[z_i^2 ] - \\frac{1}{n^2}\\sum_{i\\not = j }    e[z_iz_j ]    \\big)\\\\ & = & { \\mbox{var}}[c_n ] + \\frac{\\alpha-1}{n-1}\\big ( \\frac{1}{n}\\sum_i   e[z_i^2 ] - \\frac{1}{n^2}\\sum_{i , j }    e[z_iz_j ]    \\big)\\\\ & = & { \\mbox{var}}[c_n ] + \\frac{\\alpha-1}{n-1}\\big ( { \\mbox{var}}[x_1y_1 ] - { \\mbox{var}}[c_n ]   \\big)\\\\\\end{aligned}\\ ] ] where stationarity of the @xmath304s has been used . if the sequence @xmath304 is i.i.d . , then @xmath314 = { \\mbox{var } } [ c_n ]   + \\frac{\\alpha-1}{n-1 } \\frac{n-1}{n } { \\mbox{var}}[z ] $ ] and we recover the simple expression @xmath315= \\alpha { \\mbox{var}}[c_n ] $ ] . indeed , selecting @xmath0 samples out of @xmath26 i.i.d .",
    "samples leads to this result immediately .",
    "furthermore , back to the estimation of @xmath316 , we have @xmath157 = { \\mbox{var } } [ c_n ]   + \\frac{\\alpha-1}{n-1 } \\big ( f^\\tau_{xy}(0 ) -{\\mbox{var}}[c_n ] \\big)\\end{aligned}\\ ] ] where we can recall that @xmath65 .",
    "note that nowhere do we use the distribution of @xmath24 and @xmath25 .",
    "therefore , the calculation remains valid if we work on the quantized signals .",
    "let @xmath317 , @xmath318 , @xmath319 , @xmath320 and @xmath321 , where the sample @xmath294 is taken uniformly at random without replacement from @xmath124 .",
    "then , from the results above , we have @xmath236 & = & { \\mbox{e}}[c^q_n ] = \\frac{2}{\\pi } \\arcsin \\frac{\\gamma_{xy}(\\tau)}{\\sqrt{\\gamma_{xx}(0 ) } \\sqrt{\\gamma_{yy}(0)}}\\\\ { \\mbox{var}}[{{\\cal c}}^q_m ] & = & { \\mbox{var } } [ c^q_n ]   + \\frac{\\alpha-1}{n-1 } \\big ( { \\mbox{var } } [ \\bar{x_1 } \\bar{y_1 } ] -{\\mbox{var}}[c^q_n ] \\big)\\end{aligned}\\ ] ] however , we can easily evaluate @xmath322 $ ] as @xmath323   & = & { \\mbox{e}}[\\bar{x_1 } \\bar{y_1}\\bar{x_1 } \\bar{y_1}]-{\\mbox{e}}[\\bar{x_1 } \\bar{y_1}]^2\\\\ & = & 1 - \\frac{4}{\\pi^2 } \\arcsin^2 \\frac{\\gamma_{xy}(\\tau)}{\\sqrt{\\gamma_{xx}(0 ) } \\sqrt{\\gamma_{yy}(0)}}\\end{aligned}\\ ] ] the last line of which is valid under the gaussian assumption .      in @xcite ,",
    "the variance of @xmath325 is detailed in a particular case .",
    "the existence of closed form solutions that might be of interest here are obtained only in simple cases . to illustrate this , in the particular case @xmath326",
    "for which we denote @xmath327 as the normalized correlation function , the results obtained in @xcite lead to @xmath328&=&\\frac{1}{n } \\sum_{k =- n}^n ( 1-\\frac{|k|}{n } ) ( 2i^\\tau_1(k)+i^\\tau_2(k)+i^\\tau_3(k ) ) \\\\ & - & { \\mbox{e}}[c^q_{n , xy}(\\tau)]^2\\end{aligned}\\ ] ] where the @xmath329 are defined as follows .",
    "let @xmath330 be the entries of a four - dimensional correlation matrix @xmath331 ( normalized ) , and let @xmath332 be the entries of the partial correlation matrix ( normalized ) ( _ i.e. _ @xmath333 , where @xmath334 is the diagonal matrix extracted from @xmath335 ) .",
    "when @xmath324 , the matrix @xmath331 is given by @xmath336 then the three terms @xmath329 read @xmath337 and @xmath338 . in these equations , the coefficient @xmath339 ( resp .",
    "@xmath340 ) is a nonlinear function of the @xmath330 defined in equation ( [ matrixlambda : eq ] ) , except obviously for @xmath341 ( resp .",
    "@xmath342 ) , which is the dummy variable of integration .",
    "we thus see that even for @xmath324 the evaluation of the variance requires numerical integration or monte - carlo simulation .",
    "we chose the latter .",
    "r.vincent , m. carmona , o.j.j .",
    "michel , j.l .",
    "lacoume :  passive acoustic sensor network localization : application to structure geometry monitoring.\",in proc . of 7th european workshop on structural health monitoring , nantes , france , 2014 ."
  ],
  "abstract_text": [
    "<S> in passive monitoring using sensor networks , low energy supplies drastically constrain sensors in terms of calculation and communication abilities . designing processing algorithms at the sensor level that take into account these constraints </S>",
    "<S> is an important problem in this context . </S>",
    "<S> we study here the estimation of correlation functions between sensors using compressed acquisition and one - bit - quantization . </S>",
    "<S> the estimation is achieved directly using compressed samples , without considering any reconstruction of the signals . </S>",
    "<S> we show that if the signals of interest are far from white noise , estimation of the correlation using @xmath0 compressed samples out of @xmath1 can be more advantageous than estimation of the correlation using @xmath0 consecutive samples . </S>",
    "<S> the analysis consists of studying the asymptotic performance of the estimators at a fixed compression rate . </S>",
    "<S> we provide the analysis when the compression is realized by a random projection matrix composed of independent and identically distributed entries . </S>",
    "<S> the framework includes widely used random projection matrices , such as gaussian and bernoulli matrices , and it also includes very sparse matrices . however </S>",
    "<S> , it does not include subsampling without replacement , for which a separate analysis is provided . when considering one - bit - quantization as well , the theoretical analysis is not tractable . </S>",
    "<S> however , empirical evidence allows the conclusion that in practical situations , compressed and quantized estimators behave sufficiently correctly to be useful in , for example , time - delay estimation and model estimation .    </S>",
    "<S> compressed acquisition , random projection , sampling without replacement , one - bit quantization , correlation function estimation </S>"
  ]
}