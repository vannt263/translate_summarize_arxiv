{
  "article_text": [
    "quarks and gluons are the building blocks of a large number of elementary particles , collectively called hadrons , that include well - known particles such as protons and neutrons .",
    "a remarkable property of quarks and gluons is confinement , that is while there is solid evidence that they exist within hadrons , they have never been observed in isolation in experiments . the theoretical principle governing the physical dynamics of quarks and gluons",
    "is described by a gauge field theory called quantum chromodynamics ( qcd ) .",
    "qcd is a highly non - linear quantum mechanical system in which the basic quark and gluon field degrees of freedom are defined at each point of four - dimensional space - time .",
    "while the fluctuations of the fields with short wave length are weakly coupled , the coupling becomes stronger for longer wave lengths .",
    "these features render an analytical solution of qcd an impossible arduous task .",
    "instead progress over the past two decades came from numerical simulations using a formulation of qcd on a four - dimensional space - time lattice , known as lattice qcd@xcite .    approximating continuous space - time with a sufficiently fine lattice",
    "necessarily requires a large lattice size @xmath0 , with the consequence that the number of degrees of freedom increases as @xmath1 .",
    "when we increase @xmath0 we usually reduce the light quark mass such that it becomes closer to the physical value ; this requires additional computations due to the critical slowing down . taking these two factors into account , the amount of computing actually needed@xcite is considered to increase , at least , as fast as @xmath2 .",
    "the numerical simulation of lattice qcd therefore requires significant computing power . on the other hand ,",
    "quark and gluon fields interact only locally in space - time in qcd .",
    "thus lattice qcd simulations are ideally suited for parallelism in the space - time coordinates .",
    "exploiting this feature , a number of dedicated parallel computers has been developed since the 1980 s aiming to advance lattice qcd simulations@xcite .",
    "the cp - pacs parallel computer is one of the latest efforts in this direction@xcite .",
    "it is worth emphasizing , however , that the parallelism inherent in lattice qcd is shared by a large number of physics problems in which space - time or space fields are the basic dynamical variable .",
    "thus the overall objective of the cp - pacs project is broader , encompassing astrophysics and condensed matter applications in computational physics .",
    "this is reflected in the name of the computer , which is an acronym for _ computational physics by parallel array computer system_. the cp - pacs has been developed in collaboration with hitachi ltd .    the cp - pacs started to operate for physics computations in april 1996 with 1024 processing nodes . the upgrade to the final 2048 processor system with a peak speed of 614 gflops",
    "was completed in late september 1996 .",
    "so far most of the cpu time has been devoted to simulations of lattice qcd . in this article",
    "we report the performance of cp - pacs for this problem based on the measurements recorded in the actual production runs .",
    "we first performed a large scale simulation of qcd in the `` quenched '' approximation where the effects of quark - antiquark pair creation / annihilation are neglected in the intermediate processes .",
    "quenched qcd calculations require a large memory size and were performed using the entire system of 2048 nodes .",
    "physics results of the quenched simulation have been presented elsewhere@xcite .",
    "we then started a systematic study of `` full qcd '' , progressively eliminating the quenched approximation .",
    "full qcd simulations demand much more computer time than quenched simulations . preliminary physics results of our full qcd simulations have been presented in @xcite . for a short summary of physics results from the cp - pacs ,",
    "see @xcite .    summarizing the results for the performance of the entire cp - pacs system ,",
    "our optimized code has achieved a sustained speed of 237.5 g  for the heat - bath update of gluon variables , 264.6 g  for the over - relaxation update , and 325.3 g  for quark matrix inversion with the even - odd preconditioned minimal residual algorithm .",
    "the cp - pacs is a mimd parallel computer with distributed memory consisting of 2048 processing nodes ( pu ) and 128 i / o nodes ( iou ) .",
    "the nodes are interconnected into an @xmath3 array by a 3-dimensional _ hyper - crossbar _ network of crossbar switches as shown schematically in figure  [ fig : hxb ] .",
    "each pu has a newly made risc processor _",
    "harp1-e _ with the peak speed of 300 mflops for 64 bit data and 64256 mbyte of main memory .",
    "for intermediate storage a raid-5 disk of 8.3 gbyte is attached to each iou .",
    "thus cp - pacs as a whole has the peak speed of 614 g , 320 gbyte of main memory and 1060 gbyte of distributed disk space .",
    "we list further specifications in table  [ specification ] .",
    ".specification of the cp - pacs computer [ cols= \" < , < \" , ]      in quenched qcd simulations , the performance of a gluon update program is often compared in terms of the `` link update time '' , _",
    "i.e. , _ the execution time needed to update a single gluon field @xmath4 on a link @xmath5",
    ".    the link update time may be translated into gflops using the number of floating operations per link . for the heat bath update ,",
    "this number can not be fixed uniquely since the heat bath is a stochastic process and elementary functions are called in the program .",
    "we adopt the number 5700 which is widely accepted in the lattice community and has been used in previous estimations@xcite .",
    "the previous best performance among published data was obtained by the nal - yamagata collaboration@xcite . employing the numerical wind tunnel ( nwt ) with 128 nodes at national aerospace laboratory ( nal ) , they reported a link update time of 0.0317 @xmath6sec or 179.8 g , which won the gordon bell prize of 1995 .",
    "a better performance is listed on the web page of nal@xcite : 215.8 g  obtained on the enhanced nwt with 160 nodes , corresponding to the link - update time 0.0264 @xmath6sec .",
    "performance on other computers is summarized in @xcite .",
    "the link update time on the cp - pacs is 0.0240 @xmath6sec .",
    "this is equivalent to 237.5 g , which exceeds the best value on nwt by 10% .    for the over - relaxation update ,",
    "our link - update time is 0.0112 @xmath6sec .",
    "the number of floating point operations per link can be counted precisely for this case , which equals 3050 using the algorithm we adopted @xcite .",
    "therefore , our over - relaxation code achieves 264.6 g.    c|cccccc + machine & vpp500 & cm-5 & paragon & t3d & cm-5 & acpmaps + & ( fujitsu ) & ( tmc ) & ( intel ) & ( cray ) & ( tmc ) & + location & kek@xmath7 & lanl@xmath8 &  & psc@xmath9 &  & fermilab@xmath10 + # node & 64 &  & 64 & 64 & 64 & 128 + problem & ks & wilson & ks & ks & ks & wilson + m / node & 1105 & 35 & 23.2 & 22.2 & 20.0 & 8.5 + g & 70.7 &  & 1.5 & 1.4 & 1.3 & 1.1 + comment & ( 1 ) & ( 2 ) & ( 3 ) & ( 3 ) & ( 3 ) & ( 3 ) + reference & @xcite&@xcite & @xcite & @xcite & @xcite & @xcite +   + machine & nwt & cm-5 & paragon & t3d & cm-5 & acpmaps + location & nal@xmath11 & lanl@xmath8 & snl@xmath12 & us gov . &",
    "lanl@xmath8 & fermilab@xmath10 + # node & 167 & 1056 & 3680 & 1024 & 1056 & 612 + g & 196.1 & 37.0 & 85.4 & 22.7 & 21.1 & 5.2 +   +   +   +   +   +   +   +   +   +      the calculation of performance for the quark propagator solver is made as follows .",
    "the computations in the main loop consist of four subroutines ( see table  [ tab : solvercppacs ] ) . for each subroutine",
    ", we measure the execution time per one subroutine call and the ratio of time spent for the subroutine in the total execution time . counting the number of floating point operations in each subroutine , which can be done precisely ,",
    "we convert the time data into g. the raw data for each subroutine are summarized in table  [ tab : solvercppacs ] .",
    "the average of the performances with weight of the time ratio leads to 158.8 m / pu for the solver as a whole , or equivalently 325.3 g  for the entire cp - pacs system .    for comparison ,",
    "we reproduce in the top half of table  [ tab : solverother ] the performance data for several machines reported in the literature .",
    "they refer to either the wilson quark action used in our run or the kogut - susskind quark action which is another form of lattice quark action often used in lattice qcd simulations .",
    "the algorithm for the solver also differs as noted in the comments .",
    "we should remark in particular that these results are mostly calculated by employing a smaller number of nodes than maximally available .",
    "therefore , we have tried to estimate `` possible performance '' by assuming that the measured performance scales linearly with the number of nodes up to the maximal configuration with the same architecture which exists now or has existed in the past .",
    "this estimate is shown on the bottom half of table  [ tab : solverother ] .",
    "the best performance for a quark matrix solver recorded has been obtained on the vpp500 with 64 nodes for a ks quark propagator solver . if the reported value is translated to the possible performance on the nwt with 167 nodes , we obtain 196.1 g , although nwt has not been used for hadron spectroscopy calculations .",
    "the measured performance of 325.3 g  on the cp - pacs is 66% larger than this .",
    "in full qcd simulations , the most time - consuming part is mult both in update and solver .",
    "the performance of mult has already been discussed in sec .",
    "[ sec : solver ] . here",
    ", however , several additional remarks are in order .    first , full qcd simulations are extremely computer - time consuming compared to those of quenched qcd .",
    "simple scaling estimates place a hundred - fold or more increase in the amount of computations for full qcd compared to that of quenched qcd with current algorithms .",
    "therefore , the use of a large lattice comparable to the quenched case is difficult ; we are forced to employ coarse lattices .    in order to keep a reasonable sublattice size for each pu",
    ", we perform calculations on partitions of the cp - pacs .",
    "the largest partition we have used in our full qcd simulation consists of 512 pu s with a lattice size of @xmath13 .",
    "we report the performance measured on this lattice . in this case",
    ", the vector length is 24 for major loops .",
    "furthermore , in order to suppress discretization errors caused by the coarse lattice used in full qcd calculations , we apply action improvement which has been widely pursued in the last few years .",
    "based on a comparative study of various combinations of improved and unimproved actions in full qcd @xcite , we adopt an improved gluon action proposed by iwasaki @xcite , combined with an improved quark action suggested by sheikholeslami and wohlert @xcite .",
    "although the basic structure of the computations can be maintained , improvement of the action implies several additional computations which are coded in fortran in our programs .",
    "together with the short vector length , the relative weight of the fortran parts over the parts coded in the assembly language ( such as mult ) is much larger than that in the quenched simulations .    in the hybrid monte carlo algorithm",
    "@xcite adopted in our full qcd update , one unit of update calculation is called `` trajectory '' .",
    "several trajectories are required to suppress autocorrelation among succeeding configurations . in our simulations , we separated the measurement steps by five trajectories .",
    "major parts of the computer time for one trajectory of full qcd update can be assembled as + c , with negligible @xmath14 .",
    "here , @xmath15 is the number of iteration steps required in the computation of @xmath16 , and @xmath17 is the evolution step size for the hybrid monte carlo algorithm . in our simulations ,",
    "@xmath18500 and @xmath19150 depending on the values of physical parameters such as quark mass and lattice volume .",
    "@xmath20 and @xmath21 are contributions from the computations in each pu , whose most time - consuming part is mult . for the case of the algorithm",
    "we adopt , we find that the total number of floating point operations for @xmath20 and @xmath21 are @xmath22 and @xmath23 , respectively , where @xmath24 is the lattice volume",
    ". relatively large number of operations for @xmath20 and @xmath21 is due to the improved action and the improved algorithm we use .",
    "the speed of @xmath20 and @xmath21 are measured to be 94 and 113 m / pu , respectively , which correspond to 31 and 38% of the peak performance .",
    "@xmath25 and @xmath26 are from internode communications . in our simulations ,",
    "the computer time for @xmath25 is negligible and that for @xmath26 is about 810% of @xmath21 .",
    "measured throughput for the @xmath26 part was 157 mb / sec on a @xmath13 lattice simulated with a 512 pu partition .",
    "an extrapolation to the limit of large @xmath27 leads to 192 mb / sec .",
    "we have presented the performance data of the cp - pacs computer measured during recent production runs for quantum chromodynamics simulations of quarks and gluons . in a run with the quenched approximation",
    ", we used the full 2048 processing nodes of the cp - pacs and obtained a sustained speed of 237.5 g  for the heat - bath update of gluon variables , 264.6 g  for the over - relaxation update , and 325.3 g  for quark matrix inversion with an even - odd preconditioned minimal residual algorithm .",
    "these performances correspond to 4353% of the theoretical peak speed of the cp - pacs . in more recent full qcd simulations",
    "in which the quenched approximation is removed , we used sub - partitions of the cp - pacs up to 512 processing nodes .",
    "we found 113 m / pu for the kernel part of the simulation , which corresponds to 38% of the peak performance .",
    "we are grateful to members of the cp - pacs project for discussions and encouragements .",
    "we thank y.  iwasaki , a.  ukawa and h.p .",
    "shanahan for reading the manuscript .",
    "this work is supported by the grant - in - aid of ministry of education , science and culture ( no .  08np0101 and no .",
    "10640248 ) .",
    "for introduction to lattice gauge theories , see , m.  creutz , _ quarks , gluons and lattices _ ( cambridge university press , cambridge , 1988 ) ; i.  montvay and g.  mnster , _ field theory on the lattice _ ( cambridge university press , cambridge , 1993 ) .",
    "the cp - pacs project has been described in y.  iwasaki , nucl .",
    "b ( proc.suppl . ) 60a ( 1998 ) 246 .",
    "see also y.  iwasaki , _ ibid .",
    "_ 34 ( 1994 ) 78 ; a.  ukawa , _ ibid .",
    "_ 42 ( 1995 ) 194 ; y.  iwasaki , _ ibid .",
    "_ 53 ( 1997 ) 1007",
    ". for further details , see , http://www.rccp.tsukuba.ac.jp .",
    "the present members of the cp - pacs project are : s.  aoki , r.  burkhalter , t.  boku , m.  fukugita , s.  gunji , t.  hoshino , s.  ichii , m.  imada , n.  ishizuka , y.  iwasaki , k.  kanaya , h.  kawai , t.  kawai , m.  miyama , s.  miyashita , m.  mori , y.  nakamoto , h.  nakamura , t.  nakamura , i.  nakata , k.  nakazawa , k.  nemoto , m.  okawa , a.  oshiyama , y.  oyanagi , s.  sakai , t.  shirakawa , a.  ukawa , m.  umemura , k.  wada , y.  watase , y.  yamashita , m.  yasunaga , and t.  yoshi .",
    "cp - pacs collaboration , s.  aoki _ et al .",
    "_ , nucl .",
    "b ( proc .  suppl . ) 60a ( 1998 ) 14 ; _ ibid .",
    "_ 63 ( 1998 ) 161 ; hep - lat/9809146 , 9810043 , to be published in proceedings of international workshop on lattice field theories ( lattice 98 ) , boulder , co , july 13 - 18 , 1998 [ nucl",
    ".  phys .",
    "b ( proc .  suppl . )",
    "( 1999 ) ] .",
    "cp - pacs collaboration , s.  aoki _ et al .",
    "_ , hep - lat/9809118 , 9809120 , 9809185 , 9810043 , to be published in proceedings of international workshop on lattice field theories ( lattice 98 ) , boulder , co , july 13 - 18 , 1998 [ nucl .",
    "b ( proc .  suppl . )",
    "( 1999 ) ] .",
    "t. boku , k. itakura , h. nakamura , and k. nakazawa , `` cp - pacs : a massively parallel processor for large scale scientific calculations '' , in proc .",
    "international conference on supercomputing 97 , pp.108 - 115 ."
  ],
  "abstract_text": [
    "<S> the cp - pacs is a massively parallel mimd computer with the theoretical peak speed of 614 gflops which has been developed for computational physics applications at the university of tsukuba , japan . </S>",
    "<S> we report on the performance of the cp - pacs computer measured during recent production runs using our quantum chromodynamics code for the simulation of quarks and gluons in particle physics . with the full 2048 processing nodes , </S>",
    "<S> our code shows a sustained speed of 237.5 g  for the heat - bath update of gluon variables , 264.6 g  for the over - relaxation update , and 325.3 g  for quark matrix inversion with an even - odd preconditioned minimal residual algorithm .    </S>",
    "<S> # 1    utccp - p-62 + march 1999 +    ,    ,    ,    ,    , </S>"
  ]
}