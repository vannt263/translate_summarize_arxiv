{
  "article_text": [
    "in the past several decades , pulsar timing has been successfully used to study a wide range of science .",
    "past successes include the confirmation of gravitational waves @xcite , and very accurate tests of general relativity @xcite .",
    "the interesting science of these examples stems from the fact that accurate measurements of the times of arrival ( toas ) of the radio pulses allow for a precise determination of the trajectory of the pulsar relative to the earth .",
    "this is possible because the toas can be accurately accounted for by current models of the pulsar trajectory , pulse propagation , and pulsar spin evolution in relativistic gravity .    among on - going pulsar timing projects",
    "are pulsar timing arrays ( ptas ) , which are programmes designed to detect low - frequency ( @xmath0@xmath1hz ) extragalactic gravitational - waves ( gws ) directly , by using a set of galactic millisecond pulsars as nearly - perfect einstein clocks @xcite .",
    "gws perturb space - time between the pulsars and the earth , and this creates detectable deviations from the strict periodicity in the toas @xcite .",
    "one of the main source candidates for ptas is an isotropic stochastic background of gravitational waves ( gwb ) , thought to be generated by a large number of massive black - hole binaries located at the centres of galaxies @xcite , by relic gravitational - waves @xcite , or , more speculatively , by oscillating cosmic - string loops @xcite .",
    "the analysis of pulsar timing data , and even more so pta data , can become prohibitively time - consuming for large datasets .",
    "this is especially true for bayesian data analysis methods , like the analysis of pta data ( * ? ? ?",
    "* hereafter vhlml ) , and the correction for dispersion measure variations ( lee et al .",
    ", in prep . )",
    "typically , the computational cost scales as @xmath2 or @xmath3 , with @xmath4 the total number of observations ; the computational difficulties will increase sharply over time .    in this work ,",
    "one possible solution for the computational difficulties is explored in the case the signal of interest is a time - correlated stochastic signal : the abc - method ( acceleration by compression ) .",
    "the abc - method is based on lossy linear data compression . by significantly reducing the dimensionality of the problem , the evaluation of computationally",
    "expensive quantities can be greatly accelerated .",
    "we specifically focus on the international pta ( ipta ) mock data challenge ( released by m.  keith , k.  j.  lee , and f.  a.  jenet ) , in which the gwb is a good example of a compressible stochastic signal .",
    "the outline of the paper is as follows . in section  [ sec : ptaan ]",
    "we briefly review the relevant theory of pulsar timing observations , with a special attention to the likelihood in the presence of time - correlated stochastic signals .",
    "we introduce the abc - method , and the compressibility of datasets , in section  [ sec : lossycompression ] . in section  [ sec :",
    "practice ] we look into some of the practicalities concerned with compression of pta data , and investigate the computational demand of different terms in the evaluation of the likelihood . in that section ,",
    "we provide and test a method based on cubic spline interpolation to estimate the compressed covariance matrix .",
    "this causes an extra speedup of a few orders of magnitude .",
    "finally we present our conclusions in section  [ sec : conclusions ] .",
    "the typical data processing pipeline for pulsar timing observations processes the raw baseband data in several data reduction steps , where at each data reduction step the data volume is drastically reduced .",
    "the data reduction steps condense the scientifically interesting information into a significantly smaller number of data points , sometimes mitigating noise in the process . at the end of the pipeline we are left with toas .",
    "this work proposes a method to compress the toa data even further to what we call generalised residuals .",
    "the data compression is based on the likelihood for the toas and the fisher information , with information preserved only for a specific stochastic signal . to this end",
    ", we review the theory of toas , the likelihood , and inclusion of the timing - model in this section .",
    "we consider @xmath5 pulsars , with @xmath6 toas for the @xmath7-th pulsar , where the @xmath8 toas are described as an addition of a deterministic and a stochastic part . in the observations this distinction is blurred because we can not fully separate the stochastic contributions from the deterministic contributions . in practice",
    "we therefore work with timing residuals that are produced using first estimates @xmath9 of the @xmath10 timing - model parameters @xmath11 ( @xmath12 between @xmath13 and @xmath10 ) ; this initial guess is usually assumed to be accurate enough to use a linear approximation of the timing - model @xcite .",
    "here @xmath14 is the sum of the number of timing - model parameters of all the individual pulsars . in this linear approximation ,",
    "the timing - residuals depend on @xmath15 as : @xmath16 where @xmath17 are the timing - residuals in the linear approximation to the timing - model , @xmath18 is the vector of pre - fit timing - residuals , @xmath19 is the vector with timing - model parameters for all @xmath5 pulsars , and the @xmath20 matrix @xmath21 is the so - called design matrix ( see e.g.  @xmath22 of * ? ? ?",
    "* vhlml ) , which describes how the timing - residuals depend on the model parameters . as an example , for a simple timing model which only contains quadratic spindown , the matrix @xmath21 is a @xmath23 matrix , with the @xmath24-th column describing a @xmath25-th order polynomial .",
    "the elements of @xmath21 are then : @xmath26 , with @xmath27 the @xmath12-th toa .",
    "identical to vhlml and ( * ? ? ?",
    "* hereafter vhl ) , we model the stochastic contributions to the toas as a time - correlated stochastic signal , described by a random gaussian process . the corresponding likelihood is equal to : @xmath28      } {        \\sqrt{(2\\pi)^{n^{\\prime}}\\det c^{\\prime } }      } , \\ ] ] where @xmath29 is the vector describing all the stochastic model parameters , and @xmath30 is the covariance matrix of the sum of all stochastic signals .",
    "this includes the measurement uncertainties , the timing noise ( red spin noise ) , and a possible gwb .",
    "using equation  ( [ eq : likelihood ] ) is computationally not very efficient because of the large number of timing model parameters .",
    "however , in the case of uniform priors ( vhlml ) and gaussian priors ( vhl ) it is possible to analytically marginalise the posterior distribution over the timing model parameters . in the remainder of this work",
    "we assume no prior information about the timing model parameters , and use uniform priors .    in their search for a simplified representation of the analytic marginalisation procedure , vhl decomposed the design matrix into an orthogonal basis based on the singular value decomposition @xmath31 , where @xmath32 and @xmath33 are @xmath34 and @xmath35 orthogonal matrices , and",
    "@xmath36 is an @xmath20 diagonal matrix .",
    "the first @xmath10 columns of @xmath32 span the column space of @xmath21 , and the last @xmath37 columns of @xmath32 span the complement .",
    "we denote these two subspace bases as @xmath38 and @xmath39 respectively : @xmath40 . in section  [ sec : marislos ] we show that @xmath39 is actually a lossless data compression matrix .",
    "now , integrating over @xmath19 , our marginalised likelihood becomes ( vhlml ) : @xmath41 with : @xmath42 where the singular matrix @xmath43 is the inverse of @xmath44 in the non - singular subspace of its basis .",
    "the singular matrix matrix @xmath44 is the post - fit covariance matrix of the timing - residuals ( vhl ; * ? ? ?",
    "* hereafter d12 ) . d12 use a pseudo - inverse based on a singular value decomposition of @xmath45 to evaluate @xmath43 in their evaluation of a gwb detection statistic ; this is equivalent to marginalising over the timing model parameters ( vhl ) .",
    "data compression is the encoding of information in a smaller data volume than the original information data volume .",
    "this can be done without losing information ( lossless ) , or with losing information ( lossy ) @xcite .",
    "we would like to use data compression to reduce our data volume , with the aim of speeding up the computations that are necessary for the analysis of pta data . in this work",
    "we compress the data in such a way to retain the sensitivity to one stochastic signal ( e.g. the isotropic background of gravitational waves ) : the `` abc - method '' ( acceleration by compression ) .    in section  [ sec : marislos ]",
    "we show that marginalisation over the timing model parameters is equivalent to lossless data compression . in section  [ sec : lincom ]",
    "we expand the data compression formalism , and show how to construct a basis in which sensitivity to a particular signal is retained .",
    "we define the corresponding compression fidelity in section  [ sec : comfid ] . finally , in section  [ sec : interpret ] and section  [ sec : compressibility ] , we discuss how to interpret the compressed basis of generalised residuals , and how far a dataset can be compressed without significant loss of information .",
    "vhl showed that equation  ( [ eq : marginalisedlikelihood ] ) can be rewritten as : @xmath46      } { \\sqrt{(2\\pi)^{n}\\det      \\left(g^{t}c^{\\prime}g\\right)}},\\ ] ] with notation as in section  [ sec : martm ] .",
    "this is an unmarginalised likelihood of a random gaussian process in @xmath4 dimensions , with data @xmath47 and covariance matrix @xmath48 .",
    "the dimensionality of the data is reduced from @xmath49 to @xmath4 due to the marginalisation process . from here",
    "onwards , we start the convention that a prime superscript denotes that a vector or a covariance matrix lies in in the larger unmarginalised space , whereas no prime denotes that either of them lies in the marginalised space .",
    "the vector @xmath50 contains all the information about all stochastic signals : marginalisation over the timing model parameters is the same as lossless linear data compression in this formalism .",
    "the matrix @xmath39 is our linear data compression matrix , and @xmath50 is our vector of reduced data .",
    "we would like to compress the reduced data @xmath50 even further , without losing too much information about the stochastic signal of our interest .",
    "we expect this to be possible , since usually the signal and the noise differ in power spectral density .",
    "only some parts of the spectrum are dominated by the signal ; other parts are dominated by the noise .",
    "the data compression scheme in this work is based on throwing away the parts of the data that are dominated by the noise by using linear data compression : @xmath51 , with @xmath52 the compressed data , or `` generalised residuals '' as we will call them , and @xmath53 the _ compression matrix_. here the number of columns of @xmath53 is less than the number of rows , where we define the compression to be the total number of timing residuals divided by the number of compressed generalised timing residuals .",
    "we derive one possible scheme to construct a suitable @xmath53 in this section .    in order to determine how much information about our signal of interest",
    "is in our data , we use the fisher information .",
    "we acknowledge that formally the fisher information does not completely quantify how well a parameter can be confined with a specific dataset , especially in the case of a low signal - to - noise ratio ( e.g. * ? ? ?",
    "* ) , but in this exploratory work we consider the fisher information as a sufficient first attempt . denoting the log - likelihood of equation  ( [ eq : marginalisedlikelihoodnew ] ) as @xmath54 , we find for the fisher information : @xmath55 where @xmath56 is the fisher information , and @xmath57 and @xmath58 are model parameters that affect the signal power spectral density .",
    "suppose that the stochastic processes in the reduced data @xmath50 are described by the covariance matrix @xmath59 , where @xmath36 is the covariance matrix of the noise , and @xmath60 is the covariance matrix of the signal of interest with amplitude @xmath61 .",
    "we would like to know which basis vectors have the largest contribution to the fisher information , which would be easiest to determine if we could completely diagonalise the matrices in the trace of equation  [ eq : fisherinformation ] .",
    "this is possible with a non - orthogonal transformation .",
    "even though the inner product is not preserved in such a transformation , the trace remains invariant .",
    "we use a square root of the noise matrix , @xmath62 , to do this . for the moment",
    "we assume that this estimate of @xmath63 is indeed correct , but in section  [ sec : interpret ] we argue that an inaccurate noise estimate still results in a usable compression . in this new basis ,",
    "the whitened data and covariance become @xmath64 and @xmath65 .",
    "the maximum sensitivity based on the fisher information now has a simple form : @xmath66 where @xmath67 is the @xmath12-th eigenvalue of @xmath68 . the @xmath69 should be interpreted as signal to noise ratios .",
    "we can only evaluate equation  ( [ eq : sensitivity ] ) if we have complete knowledge of the signal @xmath60 , the signal amplitude @xmath7 , and the noise @xmath36 .",
    "however , the @xmath67 and the corresponding basis vectors do not depend on @xmath7 , which means we can examine the sensitivity to @xmath7 as a function of the number of @xmath67 we include . here",
    ", we do assume knowledge of @xmath36 and @xmath60 .    in the limit where @xmath7 is large , the strong signal limit",
    ", we can neglect the one in the denominator of the sum of equation  ( [ eq : sensitivity ] ) , which makes all terms in the sum equal .",
    "this means that all generalised residuals carry equal information as is expected in such a case : the noise is negligible compared to the signal , so no parts of the signal are buried under the noise . in the strong signal limit ,",
    "data compression is therefore not possible .",
    "note that the sensitivity is then proportional to the number of generalised residuals , as it should .    in the limit that @xmath7 is small , the low - signal limit",
    ", we can neglect all terms @xmath69 in the denominator of the sum , making the sensitivity equal to the sum of all @xmath70 .",
    "the distribution of values of the @xmath67 eigenvalues is determined by the power spectral density of the signal compared to the noise .",
    "if the signal spectrum is the same as the noise spectrum , all the @xmath67 will be identical .",
    "however , if the signal has a different spectrum than the noise , the @xmath67 can span a wide range of values , where the large @xmath67 correspond to basis vectors where the signal is relatively large compared to the noise . in this case",
    "there are nearly - redundant data points , and compression is possible .",
    "we define the fidelity @xmath71 $ ] to be the fraction of the total sensitivity we retain in our compressed data .",
    "we choose the number of generalised residuals that we keep , @xmath72 , to be the smallest number such that : @xmath73 where we have ordered the @xmath67 to have the largest values for the lowest indices .",
    "we typically work with @xmath74 , which in favourable cases like the ipta mock data challenge allows for compressions greater than @xmath75 : less than @xmath76 of the original data volume is kept .",
    "computationally , we suggest to use a singular value decomposition to produce the eigenvalues and eigenvectors of @xmath68 , where our fidelity criterion , equation  ( [ eq : fidelity ] ) , keeps only @xmath72 of the @xmath4 generalised residuals @xmath77 .",
    "we construct the @xmath78 matrix @xmath79 as consisting of the columns of the @xmath72 eigenvectors that belong to the selected eigenvalues .",
    "the data compression matrix is now : @xmath80 . using equation  ( [ eq : marginalisedlikelihoodnew ] )",
    ", we now find for the likelihood of the compressed data @xmath81 : @xmath82      } { \\sqrt{(2\\pi)^{l}\\det\\left(\\sigma_w \\right)\\det      \\left(h^{t}ch\\right)}},\\ ] ] where the extra determinant of @xmath83 comes from the whitening , and can be ignored in practice as it is absorbed in the overall normalisation constant .",
    "this equation is the basis of the abc - method , as the computationally expensive inversion has been replaced with a lower - dimensional one .",
    "equation  ( [ eq : compressedlikelihood ] ) is completely general , and can readily be applied to realistic datasets . as in @xcite ,",
    "all timing - model parameters and jumps can be included in the likelihood , and are therefore by design part of the data compression scheme .",
    "we therefore expect not to encounter any difficulties in applying data compression to realistic data sets , even though in this work we only test the effectiveness on the mock data challenge",
    ".      as we have discussed in section  [ sec : marislos ] , marginalising over the timing model is the same as linear data compressing to the subspace of the original data @xmath17 orthogonal to the columns of the design matrix @xmath21 .",
    "similarly , the data compression we suggest in section  [ sec : lincom ] is equivalent to marginalising over vectors that lie in the subspace orthogonal to the column space of @xmath53 with uniform priors . by considering the data in the basis orthogonal to the column space of @xmath53 to be nuisance parameters with uniform priors ,",
    "the resulting likelihood of the compressed generalised residuals becomes independent of the value of the data in the orthogonal complement ( we have not found another prior with the same property ) .",
    "this interpretation of data compression in terms of marginalisation assures us that we are not introducing any biases or unwanted systematics in our analysis .",
    "the difference with the marginalisation over the timing model is that we do not marginalise over physical nuisance parameters ; we are throwing away information .",
    "the data compression matrix @xmath53 as constructed in section  [ sec : lincom ] guarantees that we throw away as little information about the signal amplitude @xmath7 as possible .",
    "this is not true for the other parameters this signal may also depend on : optimal sensitivity to those parameters possibly requires a different basis , construction of which is subject of ongoing follow - up research .",
    "we ignore this issue in the rest of this exploratory work , and assume that sensitivity to the signal amplitude is sufficient for our purposes .",
    "the interpretation of data compression in terms of marginalising over non - physical parameters assures us that the likelihood of the compressed data in equation  ( [ eq : compressedlikelihood ] ) is also valid if we do not provide good estimators for @xmath83 and @xmath60 .",
    "we may be throwing away more information than we thought if our estimators are not accurate , but we do not introduce any bias or systematics in our likelihood .",
    "it is therefore not imperative to be thorough in the estimates of the signal and the noise ; a reasonable guess may be sufficient for practical purposes .",
    "it is instructive to inspect the compressed basis vectors @xmath84 for highly compressible signals .",
    "we choose the ipta mock data challenge as an example , since the gwb signal strongly dominates the noise at the lowest frequencies in these datasets .",
    "in figure  [ fig : compressedbases ] we present the first three compressed basis vectors for j0030 + 0451 and j0437 - 4715 of mock data challenge open @xmath13 .",
    "we observe that , roughly , the first basis vector corresponds to a third - order polynomial : start negative , then ascend to a maximum , descend to a minimum , and finally end positive .",
    "the other two basis vectors display a similar behaviour with the order of the polynomial equal to the order of the basis vector @xmath85 .",
    "note that the zeroth , first , and second order are missing due to the removal of quadratics in our marginalisation over the timing model parameters .    .",
    "these basis vectors are the the first three columns of the matrix @xmath84 .",
    "the basis vectors are normalised , so we have ignored the scaling on the y - axis .",
    "the basis vectors of j0437 - 4715 have more high - frequency structure due to the fact that j0437 - 4715 is in a binary.,scaledwidth=50.0% ]    we note that the compressed basis vectors for both pulsars in figure  [ fig : compressedbases ] are similar , except that those of j0437 - 4715 display more high - frequency behaviour .",
    "this is because j0437 - 4715 resides in a binary , and the timing - model therefore includes parameters for binary motion .",
    "a natural question that arises in data compression is how much we can compress the data without losing a significant amount of information . to answer this question , we consider the fidelity as a function of the number of generalised residuals in the dataset . for compressible datasets",
    "we expect the fidelity to stay close to one , only to drop for high compression rates",
    ". one possible measure of compressibility , which we use in our application of the abc - method , is the maximum compression for which the fidelity stays above @xmath86 .",
    "this maximum compression depends on the signal amplitude and power spectral density compared to that of the noise .    as an example , we plot the fidelity of the mock data of j0030 + 0451 from the open mock data challenge versus the compression in figure  [ fig : fidelityopen ] , where the signal of interest is the gravitational - wave background . in these datasets",
    "the noise is white .",
    "open dataset 3 does formally contain some extra ( mildly ) red noise which we do include in these plots , but the level of red noise is so low that it is negligible in practice . because the signal is of such a different spectral shape than the noise , data compression is very efficient . in such a case ,",
    "the higher the noise level compared to the signal , the more compressible the dataset is .     because the errorbars for j0030 + 0451 observations were set higher in open dataset @xmath87 than in open dataset @xmath13 .",
    "we have also plotted the fidelity of the low - signal approximation ( open @xmath88 lsa ) for open dataset @xmath88 , and the fidelity of an incompressible signal ( incompressible).,scaledwidth=50.0% ]    in figure  [ fig : fidelityopen ] we plot the fidelity for open dataset @xmath88 in the low - signal limit ( lsa ) .",
    "data compression is most efficient in the low - signal limit . as a comparison",
    "we show the fidelity for an incompressible signal in figure  [ fig : fidelityopen ] as well .",
    "this corresponds to the high - signal limit .",
    "we see that an increase in the compression results in an equal decrease in the fidelity .      for realistic datasets",
    "we generally do not know the details of the signal and the noise .",
    "the noise typically has to be characterised from the data , and we may not even be certain of the presence of a signal of interest .",
    "since the fidelity depends on estimates of the signal and the noise , it is not clear how far exactly we can compress the dataset without losing information from the signal . here",
    ", we therefore recommend a conservative approach when preparing the abc - method .",
    "the @xmath69 in the denominator of equation  ( [ eq : fidelity ] ) represents the signal relative to the noise .",
    "the larger it is relative to @xmath13 , the less likely we will discard that generalised residual . therefore , if we are sure not to overestimate the noise , and if we are sure not to underestimate the signal , the compression fidelity will not be overestimated .",
    "specifically , we recommend to calculate the fidelity as follows : 1 ) construct the noise covariance estimate @xmath83 such that it only consists of the toa uncertainties .",
    "2 ) choose a suitable spectral form for the signal of interest .",
    "for example : this consists of fixing the spectral index @xmath89 for the gwb .",
    "3 ) use the estimates of vhl ( equation ( 22 ) & ( 24 ) of * ? ? ?",
    "* ) to estimate the signal amplitude .",
    "for a gwb signal , this is : @xmath90 where @xmath91 is the duration of the experiment , @xmath92 is the dimensionless gwb amplitude , and @xmath93 is the rms residual due to the gwb in the data . for other power",
    "spectral densities a similar calculation to vhl is required .    by completely ignoring other effects like red spin noise in these estimates",
    ", we are ensured that we do not throw away more information than we should .",
    "indeed , more noise in this calculation would mean a higher compression .",
    "this conservative approach is therefore also guaranteed to work in the presence of ( strong ) red noise .",
    "we note that this approach can overestimate the fidelity if the toa uncertainties have been overestimated , or when the shape of the signal power spectral density has been estimated incorrectly with , for instance , an incorrect spectral index .",
    "the toa uncertainties depend on complex details of the data reduction pipeline prior to the formation of the toas and of the cross - correlation of the pulse profile with a template @xcite . however , underestimation of the toa uncertainty is uncommon in practice . how to choose a suitable basis to be sensitive to the spectral index is a subject of ongoing follow - up research . here",
    "we assume we know the spectral index of the signal of interest .    in the open mock data challenge , shown in figure  [ fig : fidelityopen ]",
    ", high compressions of over @xmath94 still yield a fidelity close to @xmath95 in the low - signal limit .",
    "since realistic datasets are expected to be in the low - signal limit - we have not detected a gwb yet - we expect high compressions in realistic datasets to be possible as well . however , realistic datasets can have far more toas per pulsar than the @xmath96 toas per pulsar in the mock data challenge . since in the low - signal limit",
    "only a few generalised residuals per pulsar is enough to reach @xmath74 , we expect very high compressions , possibly up to @xmath97 depending on the size of the dataset , to be realistic for initial pta applications .",
    "although the raw data of pulsar observations can be quite voluminous , the pulsar time of arrival data files are typically several kilobytes in size . because it seems quite unlikely that data volume at this stage of the analysis is ever going to be a problem , the only reason to resort to data compression is because it can greatly accelerate the analysis of pulsar timing data . in this section ,",
    "we discuss the computational costs of evaluating the likelihood function with the abc - method , and we present some computational shortcuts . a straightforward application is a bayesian analysis ( e.g. vhlml ) , but other analysis methods described in the time domain are expected to see an equally large acceleration ( e.g. d12 ) .",
    "special attention is given to power - law signals , for which we present a convenient approximation of the compressed covariance matrix , thereby maximising the effectiveness of data compression .",
    "the computational demand of equation  ( [ eq : marginalisedlikelihoodnew ] ) scales as @xmath2 ( vhl ) due to the inversion operation of an @xmath98 matrix . with linear data compression",
    ", we have decreased the size of the inversion matrix , which will therefore also decrease the computational demands .",
    "the computational demand of the inversion in equation  ( [ eq : compressedlikelihood ] ) scales as @xmath99 . depending on the compression , this @xmath99 operation may or may not be the computational bottleneck . for large enough compression factors , the computational bottleneck will either be the computation of @xmath100 ( @xmath3 operation ) , or the multiplication @xmath101 ( @xmath102 operation ) . in the case of an array of pulsars",
    ", the matrix @xmath53 will be block - diagonal if the data compression has been done per individual pulsar .",
    "then , the computation of @xmath101 can be accelerated with a factor of the number of pulsars by block - wise multiplication ( vhl ) .    in this assessment of computational demand",
    ", we have neglected the construction of the data compression matrix @xmath53 . a computationally expensive singular value decomposition of a full covariance matrix is required for this . however , this only needs to be done once : we do not change the compressed basis during subsequent likelihood evaluations , even if we vary the noise / signal parameters during a markov chain monte carlo simulation .",
    "since the compressed basis can be calculated for each pulsar individually , we therefore do not expect the construction the data compression matrix @xmath53 to be a computational bottleneck in the foreseeable future .",
    "we test the performance of the abc - method on the ipta mock data challenge : all challenges consist of @xmath96 observations per pulsar , with @xmath103 pulsars .",
    "our likelihood contains the following deterministic and stochastic signal contributions : 1 ) the tempo2 @xcite timing - model parameters 2 ) error bars for every toa 3 ) power - law red timing noise for every pulsar 4 ) a correlated gwb evaluation of the likelihood of equation  ( [ eq : marginalisedlikelihood ] ) took on average @xmath104 seconds , where most of that time comes from inverting the full covariance matrix .",
    "we compare the efficiency of equation  ( [ eq : marginalisedlikelihood ] ) to that of the data compression likelihood of equation  ( [ eq : compressedlikelihood ] ) , where the latter equation becomes equation  ( [ eq : marginalisedlikelihoodnew ] ) when the compression is @xmath13 .",
    "in the evaluation of the compressed likelihood , three terms take up the majority of the computational cost : 1 ) @xmath105 , the evaluation of the @xmath106 elements of the covariance matrix of the gwb .",
    "2 ) @xmath107 , the matrix multiplication to obtain the compressed covariance matrix .",
    "3 ) @xmath108 , inversion of the compressed covariance matrix . all other operations are negligible compared to these three . in figure",
    "[ fig : profiling ] we present the computational cost of these three terms , together with the sum of the three , in the bottom panel .",
    "the uncompressed likelihood is given as a single point .",
    "we see that the inversion of the compressed covariance matrix is the dominant term for low compression factors : if roughly @xmath109 or more generalised residuals per pulsar are kept . for higher compression factors ,",
    "the evaluation of @xmath105 is the most time - consuming part of the evaluation of the likelihood . because this is an @xmath110 operation that does not depend on the compression , compressing the data to less than @xmath94 generalised residuals per pulsar",
    "does not gain us any computational efficiency in this configuration .",
    "pulsars was used , with @xmath96 observations per pulsar .",
    "the dominating terms are : 1 ) @xmath105 , ( dashed line ) , the evaluation of the @xmath106 elements of the covariance matrix of the gwb . only present in the lower panel .",
    "2 ) @xmath107 , ( dotted line ) , the matrix multiplication to obtain the compressed covariance matrix . only present in the lower panel .",
    "3 ) interpolation , ( gray solid line ) , the construction of the @xmath111 compressed covariance matrix @xmath101 by cubic spline interpolation . only present in the upper panel .",
    "4 ) @xmath108 , ( dash - dotted line ) , inversion of the compressed covariance matrix .",
    "the total computational cost is shown as a solid line , and the uncompressed likelihood of equation  ( [ eq : marginalisedlikelihood ] ) is shown as an upper limit at @xmath96 generalised residuals per pulsar . in the lower panel , these terms are evaluated for the compressed likelihood of equation  ( [ eq : compressedlikelihood ] ) , without any computational shortcuts . for high compression factors ( low number of compressed generalised residuals ) ,",
    "the evaluation of @xmath105 is dominant , which means that further compression does not buy one more computational time . in the upper panel",
    "the compressed likelihood is evaluated , where the cubic spline interpolation method of section  [ sec : cubicspline ] is used to evaluate @xmath105 . in this case , the inversion @xmath108 is always the dominant term , and data compression is most efficient .",
    "note how the line for @xmath108 is ( nearly ) identical in both panels.,scaledwidth=50.0% ]      as explained in the previous section , in the case where a dataset is highly compressible , the computational bottleneck becomes evaluating @xmath100 , which contains @xmath105 in the example of section  [ sec : profiling ] , at each step of the likelihood function .",
    "if we label the contributions to the compressed covariance matrix as @xmath112 , then in some cases it is possible to greatly accelerate the evaluation of @xmath112 .",
    "the simplest type of stochastic signal is the type where the power spectral density shape is known completely , but the amplitude @xmath113 is an unknown model parameter .",
    "examples of signals of this type include the stochastic behaviour due to toa uncertainties ( with an unknown scaling , or `` efac '' , parameter ) , pulse phase jitter ( e.g. * ? ? ?",
    "* ) , or a gwb with a known spectral index . for these types of signal",
    "we can evaluate @xmath112 just once for unit amplitude , and store this in memory . then , each time we need to evaluate the likelihood function , we can multiply this stored matrix with the amplitude @xmath113 to obtain the compressed covariance matrix without having to re - calculate such matrices every time . especially when @xmath114 , this greatly reduces the time necessary to evaluate @xmath112 .",
    "most stochastic signal models have more free parameters than only an amplitude , and the acceleration method of section  [ sec : unamp ] is not applicable . in this section",
    "we present a practical solution for signals with two free parameters : an amplitude , and some other parameter .",
    "we focus only on signals with a power - law power spectral density , but we expect that the method is also appropriate for other signals with a parametrised power spectral density .",
    "power - law signals are used in various ways in pulsar timing , both as a model for noise sources ( i.e. red spin noise * ? ? ?",
    "* ; * ? ? ?",
    "* ) , and as signal sources ( i.e. the istotropic background of gravitational waves * ? ? ?",
    "* ; * ? ? ?",
    "we use the following definition for the power spectral density of a power - law signal : @xmath115 where @xmath116 is the signal frequency , @xmath7 is the signal amplitude , and @xmath117 is the spectral index that describes the steepness of the spectrum .",
    "the rms in the timing residuals of such a signal is given by : @xmath118 .",
    "because this is an unphysical power spectrum that diverges at the low frequencies , in practice a third parameter is used to describe a power - law signal that represents a lowest frequency @xmath119 below which the signal is assumed to be zero .",
    "the reduced data @xmath50 and therefore also the compressed data @xmath52 are not affected by @xmath119 ( vhl ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) .    for highly compressed data ,",
    "the compressed covariance matrix @xmath101 contains far less elements than @xmath100 : the number of unique elements for this matrix is @xmath120 .",
    "for a single pulsar power - law noise covariance matrix this is typically only of the order of a hundred elements , depending on the number of observations and the compression .",
    "we propose to use an interpolation approximation for each element of the matrix @xmath101 as a function of @xmath117 , with @xmath121 .",
    "the elements of the covariance matrix diverge at both ends of the interval . in the case of a single pulsar , this means we have @xmath120 functions on the interval @xmath121 that we want to write an interpolation approximation for .",
    "we choose a cubic spline interpolation method for this , where the domain of the function is divided in sub - intervals in which the function is approximated by a third - order polynomial .",
    "we construct all polynomials such that their values and derivatives match at the edges .",
    "the only free parameter in this approach is the number of cubics used in total .",
    "this number needs to be tuned for performance .     as a function of @xmath117 . for the pulsar j0030",
    "+ 0451 , with data as in ipta mock data challenge open @xmath87 , we used the interpolation technique of section  [ sec : cubicspline ] to approximate the elements of the compressed covariance matrix @xmath101 . in the upper panel , we have plotted @xmath122 of element @xmath123 ( row 1 , column 4 ) of the compressed covariance matrix as a function of the spectral index @xmath117 . here",
    "@xmath123 is the true value of the element of @xmath101 , and @xmath124 is the difference between the true value of @xmath123 , and the interpolated value .",
    "this plot looked similar for all elements . in the bottom panel",
    "we have plotted the corresponding quantity for the log - likelihood : @xmath125 , with @xmath54 the log - likelihood , and @xmath126 the difference between the true and interpolated value .",
    "we initialised the cubic spline interpolation with @xmath127 points , evenly distributed on the interval @xmath128 .",
    "we see that the discrepancy between the interpolated and the true values grows steeply near the boundaries of the interval . at the boundaries , the elements of the compressed covariance matrix diverge.,scaledwidth=50.0% ]    in figure",
    "[ fig : interpolelements ] we show the difference between the true value and the interpolated value of an arbitrary element of @xmath101 as a function of @xmath117 for j0030 + 0451 of mock data challenge open @xmath87 .",
    "these results are typical ; we find a similar plot for every element , where the difference between the true value and the interpolated value always inflates near the boundaries of the interval .",
    "we also show the difference between the accompanying log - likelihood @xmath54 as a function of @xmath117 for the same dataset . here",
    "we also see that the difference inflates near the boundaries .",
    "the precision of the interpolation depends on the number of cubic splines used in the interpolation . for lower numbers of splines in the approximation",
    ", we saw the accuracy quickly decrease near the boundaries .",
    "this caused the compressed covariance matrix to become non - positive definite or singular close to the boundaries . in our simulations , @xmath127 equally spaced cubic splines was enough on a slightly reduced interval @xmath129 to not run into numerical issues .",
    "the cubic spline interpolation removes the necessity to calculate the total covariance matrix @xmath100 . in the top panel of figure",
    "[ fig : profiling ] we present the computational cost of the computationally dominant terms in the compressed likelihood , in the case where we use cubic spline interpolation for the elements of @xmath101 .",
    "the computationally dominant term is the inversion @xmath108 for the whole range of possible compressions , which means that data compression is maximally efficient .",
    "we almost reached full capacity of random access memory of our workstation for very low compressions . for large datasets with an incompressible signal ,",
    "this may cause problems for the cubic spline interpolation method .",
    "however , for current applications , we do nt believe this to be an issue .",
    "for the mock data challenge , the total typical speedup at @xmath130 fidelity is almost three orders of magnitude .",
    "we test the abc - method with the cubic spline interpolation technique on the open mock data challenge .",
    "we present the results here of mock data challenge open @xmath13 because the noise level was the same for all pulsars in that challenge .",
    "that makes it easier to compare the results we see here with the fidelity levels of figure  [ fig : fidelityopen ] : they are approximately the same for all pulsars . in figure",
    "[ fig : fidelityopen ] we see that for a compression of @xmath131 , we start to approach @xmath132 .",
    "this corresponds to @xmath133 compressed generalised residuals per pulsar . in figure",
    "[ fig : gammasensitivity ] we present the likelihood credible regions for mock data challenge open @xmath13 both for the full array of pulsars and for pulsar j0030 + 0451 , with different compression levels .",
    "we see that with @xmath133 generalised residuals per pulsar , the compressed likelihood is practically equal to the uncompressed likelihood , as predicted by figure  [ fig : fidelityopen ] . with less than @xmath133",
    "generalised residuals per pulsar , the likelihood credible regions are broader , with significant covariance between the gwb amplitude and the spectral index .",
    "this covariance may partially be a result of the compressed basis being optimal only for the injected value of the spectral index @xmath134 ; this dependence is the subject of follow up work .     for ipta mock data challenge open one .",
    "no parameters are numerically marginalised over .",
    "the timing model parameters are analytically marginalised over as part of the data compression . on the left panel",
    "the likelihood is plotted for only pulsar j0030 + 0451 , with compression to @xmath75 generalised residuals ( top ) , and compression to @xmath133 generalised residuals ( bottom ) . on the right",
    "the likelihood is plotted for the full array of pulsars , with compression to @xmath131 generalised residuals per pulsar ( top ) , and compression to @xmath133 generalised residuals per pulsar ( bottom ) . in each panel ,",
    "the blue lines represent the credible regions of the compressed likelihood , the red lines , labelled `` ref '' , represent the reference credible regions of the uncompressed likelihood of equation  ( [ eq : marginalisedlikelihoodnew ] ) .",
    "the contours represent the @xmath135 ( @xmath136 ) , @xmath137 ( @xmath138 ) , and @xmath139 ( @xmath140 ) credible regions .",
    "the injected values are marked with an x.,scaledwidth=50.0% ]    the results of this section hold for all three of the open mock data challenge datasets : when the fidelity @xmath74 , the likelihood credible regions where almost indistinguishable from the uncompressed likelihood credible regions . with a compression such that the fidelity is significantly less than that ,",
    "the credible regions were broader , with a covariance between the amplitude and spectral index .",
    "we investigate the acceleration of the analysis of pulsar timing data by compressing the data with a linear transformation , without losing a significant amount of information of a particular stochastic signal of interest : the abc - method . in this formalism ,",
    "marginalisation over the timing - model parameters is equivalent to lossless linear data compression .",
    "we show that when the stochastic signal of interest has a significantly different spectrum than the noise , the data is highly compressible .",
    "the abc - method is most efficient in the low - signal limit , where the signal is buried under the noise over most of the frequency range .",
    "data compression is not possible in the strong - signal limit , where the signal dominates the noise in the whole frequency range .",
    "the likelihood function of the compressed signal is computationally more efficient , and unbiased .",
    "we introduce the concepts of compression and compression fidelity , where the compression is the total number of timing residuals divided by the number of generalised timing residuals that are kept in the compression , and the fidelity is a measure of the amount of information about the signal of interest that is kept in the compression . for the ipta mock data challenge , we show that the compression is of the order of @xmath75 , at a fidelity @xmath141 , if one is interested in the isotropic stochastic background of gravitational waves .",
    "when applied to highly compressible datasets , computational shortcuts are required to optimally accelerate the evaluation of the compressed likelihood .",
    "we present an practical method based on cubic spline interpolation of the compressed covariance matrix . when this interpolation approximation is used , the total acceleration of the evaluation of the compressed likelihood is @xmath142 , with @xmath143 the compression .",
    "we test the cubic spline interpolation method , and conclude that it works well for the purposes of the ipta mock data challenge .",
    "the total acceleration is about three orders of magnitude for a compression of @xmath75 , with results almost identical to an analysis without the abc - method .",
    "the abc - method can be readily applied to realistic datasets , without any adjustments .",
    "realistic datasets of current pulsar timing arrays are expected to reside in the low - signal approximation : no stochastic gravitational - wave background has been detected as of yet .",
    "therefore , a high compression factor of several hundred is realistic for such datasets , which yields a total acceleration of over six orders of magnitude .",
    "we expect linear data compression to become one of the key solutions for the issues related to computational cost in pulsar timing array data analysis .",
    "the author thanks k.j . lee for some insightful discussions , and alberto sesana for providing the inspiration for this work .",
    "yuri levin and michele vallisneri are thanked for their extensive comments on this manuscript .",
    "the research in this paper is conducted as part of the efforts of the european pulsar timing array ( epta ) ."
  ],
  "abstract_text": [
    "<S> the analysis of pulsar timing data , especially in pulsar timing array ( pta ) projects , has encountered practical difficulties : evaluating the likelihood and/or correlation - based statistics can become prohibitively computationally expensive for large datasets . in situations where a stochastic signal of interest has a power spectral density that dominates the noise in a limited bandwidth of the total frequency domain ( e.g. the isotropic background of gravitational waves ) , a linear transformation exists that transforms the timing residuals to a basis in which virtually all the information about the stochastic signal of interest is contained in a small fraction of basis vectors . by only considering such a small subset of these `` generalised residuals '' , </S>",
    "<S> the dimensionality of the data analysis problem is greatly reduced , which can cause a large speedup in the evaluation of the likelihood : the abc - method ( acceleration by compression ) . </S>",
    "<S> the compression fidelity , calculable with crude estimates of the signal and noise , can be used to determine how far a dataset can be compressed without significant loss of information . </S>",
    "<S> both direct tests on the likelihood , and bayesian analysis of mock data , show that the signal can be recovered as well as with an analysis of uncompressed data . in the analysis of ipta mock data challenge datasets , speedups of a factor of three orders of magnitude </S>",
    "<S> are demonstrated . </S>",
    "<S> for realistic pta datasets the acceleration may become greater than six orders of magnitude due to the low signal to noise ratio .    gravitational waves  </S>",
    "<S> pulsars : general  methods : data analysis </S>"
  ]
}