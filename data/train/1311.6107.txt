{
  "article_text": [
    "learning ( rl ) is a machine learning technique that has been widely studied from the computational intelligence and machine learning scope in the artificial intelligence community @xcite .",
    "rl technique refers to an actor or agent that interacts with its environment and aims to learn the optimal actions , or control policies , by observing their responses from the environment . in @xcite , sutton and",
    "barto suggested a definition of rl method , i.e. , any method that is well suited for solving rl problem can be regarded as a rl method , where the rl problem is defined in terms of optimal control of markov decision processes .",
    "this obviously established the relationship between the rl method and control community .",
    "moreover , rl methods have the ability to find an optimal control policy from unknown environment , which makes rl a promising method for control design of real systems .    in the past few years ,",
    "many rl approaches @xcite have been introduced for solving the optimal control problems .",
    "especially , some extremely important results were reported by using rl for solving the optimal control problem of discrete - time systems @xcite .",
    "such as , liu and wei suggested a finite - approximation - error based iterative adaptive dynamic programming approach @xcite , and a novel policy iteration ( pi ) method @xcite for discrete - time nonlinear systems . for continuous - time systems ,",
    "murray et al .",
    "@xcite presented two pi algorithms that avoid the necessity of knowing the internal system dynamics .",
    "vrabie et al .",
    "@xcite introduced the thought of pi and proposed an important framework of integral reinforcement learning ( irl ) .",
    "modares et al .",
    "@xcite developed an experience replay based irl algorithm for nonlinear partially unknown constrained - input systems . in @xcite , an online neural network ( nn ) based decentralized control strategy",
    "was developed for stabilizing a class of continuous - time nonlinear interconnected large - scale systems .",
    "in addition , it worth mentioning that the thought of rl methods have also been introduced to solve the optimal control problem of partial differential equation systems @xcite .",
    "however , for most of practical real systems , the existence of external disturbances is usually unavoidable .    to reduce the effects of disturbance",
    ", robust controller is required for disturbance rejection .",
    "one effective solution is the @xmath1 control method , which achieves disturbance attenuation in the @xmath2-gain setting @xcite , that is , to design a controller such that the ratio of the objective output energy to the disturbance energy is less than a prescribed level . over the past few decades , a large number of theoretical results on nonlinear @xmath1 control",
    "have been reported @xcite , where the @xmath1 control problem can be transformed into how to solve the so - called hamilton - jacobi - isaacs ( hji ) equation .",
    "however , the hji equation is a nonlinear partial differential equation ( pde ) , which is difficult or impossible to solve , and may not have global analytic solutions even in simple cases .",
    "thus , some works have been reported to solve the hji equation approximately @xcite . in @xcite",
    ", it was shown that there exists a sequence of policy iterations on the control input such that the hji equation is successively approximated with a sequence of hamilton - jacobi - bellman ( hjb)-like equations .",
    "then , the methods for solving hjb equation can be extended for the hji equation . in @xcite ,",
    "the hjb equation was successively approximated by a sequence of linear pdes , which were solved with galerkin approximation in @xcite . in @xcite ,",
    "the successive approximation method was extended to solve the discrete - time hji equation .",
    "similar to @xcite , a policy iteration scheme was developed in @xcite for the constrained input system . for implementation purpose of this scheme ,",
    "a neuro - dynamic programming approach was introduced in @xcite and an online adaptive method was given in @xcite .",
    "this approach suits for the case that the saddle point exists , thus a situation that the smooth saddle point does not exist was considered in @xcite . in @xcite , a synchronous policy iteration method was developed , which is the extension of the work @xcite . to improve the efficiency for computing the solution of hji equation , luo and",
    "wu @xcite proposed a computationally efficient simultaneous policy update algorithm ( spua ) .",
    "in addition , in @xcite the solution of the hji equation was approximated by the taylor series expansion , and an efficient algorithm was furnished to generate the coefficients of the taylor series .",
    "it is observed that most of these methods @xcite are model - based , where the full system model is required .",
    "however , the accurate system model is usually unavailable or costly to obtain for many practical systems .",
    "thus , some rl approaches have been proposed for @xmath1 control design of linear systems @xcite and nonlinear systems @xcite with unknown internal system model . but these methods are on - policy learning approaches @xcite , where the cost function should be evaluated by using system data generated with policies being evaluated .",
    "it is found that there are several drawbacks ( to be discussed in section [ sec_3 ] ) to apply the on - policy learning to solve real @xmath1 control problem .    to overcome this problem",
    ", this paper introduces an off - policy rl method to solve the nonlinear continuous - time @xmath1 control problem with unknown internal system model .",
    "the rest of the paper is rearranged as follows .",
    "sections [ sec_2 ] and [ sec_3 ] present the problem description and the motivation .",
    "the off - policy learning methods for nonlinear systems and linear systems are developed in [ sec_4 ] and [ sec_5 ] respectively .",
    "the simulation studies are conducted in section [ sec_6 ] and a brief conclusion is given in section [ sec_7 ]",
    ".    _ notations _ : @xmath3 and @xmath4 are the set of real numbers , the @xmath5-dimensional euclidean space and the set of all real matrices , respectively .",
    "@xmath6 denotes the vector norm or matrix norm in @xmath7 or @xmath4 , respectively .",
    "the superscript @xmath8 is used for the transpose and @xmath9 denotes the identify matrix of appropriate dimension .",
    "@xmath10 denotes a gradient operator notation . for a symmetric matrix @xmath11",
    "means that it is a positive ( semi - positive ) definite matrix .",
    "@xmath12 for some real vector @xmath13 and symmetric matrix @xmath14 with appropriate dimensions .",
    "@xmath15 is function space on @xmath16 with first derivatives are continuous .",
    "@xmath17 is a banach space , for @xmath18 .",
    "let @xmath19 and @xmath20 be compact sets , denote @xmath21 . for column vector functions @xmath22 and @xmath23 , where @xmath24  define inner product @xmath25 and norm @xmath26 @xmath27 .",
    "@xmath28 is a sobolev space that consists of functions in space @xmath29 such that their derivatives of order at least @xmath30 are also in @xmath29 .",
    "let us consider the following affine nonlinear continuous - time dynamical system :    \\(t ) & = f(x(t ) ) + g(x(t))u(t ) + k(x(t))w(t ) [ eq_sys ] + z(t ) & = h(x ) [ eq_h(x ) ]    where @xmath31^t \\in \\mathcal{x } \\subset \\mathbb{r}^n$ ] is the state , @xmath32^t \\in \\mathcal{u } \\subset \\mathbb{r}^m$ ] is the control input and @xmath33 , @xmath34^t \\in \\mathcal{w } \\subset \\mathbb{r}^q$ ] is the external disturbance and @xmath35 , @xmath36^t \\in \\mathbb{r}^p$ ] is the objective output .",
    "@xmath37 is lipschitz continuous on a set @xmath38 that contains the origin , @xmath39 .",
    "@xmath37 represents the internal system model which is assumed to be _ unknown _ in this paper .",
    "@xmath40 and @xmath41 are known continuous vector or matrix functions of appropriate dimensions .",
    "the @xmath1 control problem under consideration is to find a state feedback control law @xmath42 such that the system is closed - loop asymptotically stable , and has @xmath2-gain less than or equal to @xmath43 , that is , @xmath44 for all @xmath45 and @xmath46 is some prescribed level of disturbance attenuation . from @xcite ,",
    "this problem can be transformed to solve the so - called hji equation , which is summarized in lemma [ lemma_1 ] .",
    "[ lemma_1 ] assume the system and is zero - state observable .",
    "for @xmath46 , suppose there exists a solution @xmath47 to the hji equation    [ hji ] & [ v^*(x)]^t f(x ) + h^t(x)h(x ) + & - [ v^*(x)]^t g(x ) r^-1 g^t(x ) v^*(x ) + & + [ v^*(x)]^t k(x ) k^t(x ) v^*(x ) = 0 .    where @xmath48 and @xmath49 .",
    "then , the closed - loop system with the state feedback control @xmath50 has @xmath2-gain less than or equal to @xmath43 , and the closed - loop system , and ( when @xmath51 ) is locally asymptotically stable .",
    "from lemma [ lemma_1 ] , it is noted that the @xmath0 control relies on the solution of the hji equation .",
    "therefore , a model - based iterative method was proposed in @xcite , where the hji equation is successively approximated by a sequence of linear pdes : @xmath53^t ( f + gu^{(i ) } + kw^{(i , j ) } ) + h^th + \\vert u^{(i ) } \\vert _ r^2 \\nonumber \\\\ - \\gamma^2 \\vert w^{(i , j ) } \\vert ^2=0\\end{aligned}\\ ] ] and then update control and disturbance policies with    w^(i , j+1 ) & ^-2 k^t v^(i , j+1)[eq_3.2 ] + u^(i+1 ) & - r^-1 g^t v^(i+1)[eq_3.3 ]    with @xmath54 . from @xcite , it was indicated that the @xmath55 can converge to the solution of the hji equation , i.e. , @xmath56 .",
    "[ remark1 ] _ note that the key point of the iterative scheme - depends on the solution of the linear pde .",
    "thus , several related methods were developed , such as , galerkin approximation @xcite , synchronous policy iteration @xcite , neuro - dynamic programming approach @xcite and online adaptive control method @xcite for constrained input systems , and galerkin approximation method for discrete - time systems @xcite .",
    "obviously , the iteration - will generate two iterative loops since the control and disturbance policies are updated at the different iterative steps , i.e. , the inner loop for updating disturbance policy @xmath57 with index @xmath58 , and the outer iterative loop for updating control policy @xmath59 with index @xmath60 . the outer loop will not be activated until the inner loop is convergent , which results in low efficiency .",
    "therefore , luo and wu @xcite proposed a simultaneous policy update algorithm ( spua ) , where the control and disturbance policies are updated at the same iterative step , and thus only one iterative loop is required .",
    "it worth noting that the word  simultaneous \" in @xcite and the word  synchronous / simultaneous \" in @xcite represent different meanings .",
    "the former emphasizes the same  iterative step, while the latter emphasizes the same  time instant \" . in other words ,",
    "the spua in @xcite updates control and disturbance policies at the  same \" iterative step , while the algorithms in @xcite update control and disturbance policies at the  different \" iterative steps .",
    "@xmath61 _    the procedure of model - based spua is given in algorithm [ algorithm_1 ] .    ' '' ''    [ algorithm_1 ]    ' '' ''    it worth noting that algorithm [ algorithm_1 ] is an infinite iterative procedure , which is used for theoretical analysis rather than for implementation purpose .",
    "that is to say , algorithm [ algorithm_1 ] will converge to the solution of the hji equation when the iteration goes to infinity . by constructing a fixed point equation ,",
    "the convergence of algorithm [ algorithm_1 ] is established in @xcite by proving that it is essentially a newtons iteration method for finding the fixed point . with the increase of index @xmath60 , the sequence @xmath62 obtained by the spua with equations - can converge to the solution of hji equation , i.e. , @xmath63 .",
    "[ remark2a ] _ it is necessary to explain the rationale of using equations and for control and disturbance policies update .",
    "the @xmath1 control problem - can be viewed as a two - players zero - sum differential game problem @xcite .",
    "the game problem is a minimax problem , where the control policy @xmath64 acts as the minimizing player and the disturbance policy @xmath65 is the maximizing player .",
    "the game problem aims at finding the saddle point @xmath66 , where @xmath67 is given by expression and @xmath68 is given by @xmath69 .",
    "correspondingly , for the @xmath1 control problem - , @xmath67 and @xmath68 are the associated @xmath1 control policy and the worst disturbance signal @xcite , respectively .",
    "thus , it is reasonable using expressions and ( that are consistent with @xmath67 and @xmath68 in form ) for control and disturbance policies update . similar control and disturbance policy update method",
    "could be found in references @xcite .",
    "@xmath61 _    observe that both iterative equations and require the full system model .",
    "for the @xmath0 control problem that the internal system dynamic @xmath37 is unknown , data based methods @xcite were suggested to solve the hji equation online .",
    "however , most of related existing online methods are on - policy learning approaches @xcite . from the definition of on - policy learning @xcite , the cost function",
    "should be evaluated with the data generated from the evaluating policies .",
    "for example , @xmath70 in equation is the cost function of the policies @xmath71 and @xmath72 , which means that @xmath70 should be evaluated with system data by using evaluating policies @xmath71 and @xmath72 .",
    "it is observed that these on - policy learning approaches for solving the @xmath0 control problem have several drawbacks :    * \\1 ) for real implementation of on - policy learning methods @xcite , the approximate evaluating control and disturbance policies ( rather than the actual policies ) are used to generate data for learning their cost function . in other words , the on - policy learning methods using the  inaccurate \" data to learn their cost function , which will increase the accumulated error .",
    "for example , to learn the cost function @xmath70 in equation , some approximate policies @xmath73 and @xmath74 ( rather than its actual policies @xmath71 and @xmath72 , which are usually unknown because of estimate error ) are employed to generate data ; * \\2 ) the evaluating control and disturbance policies are required to generate data for on - policy learning , thus disturbance signal should be adjustable , which is usually impractical for most of real systems ; * \\3 ) it is known @xcite that the issue of  exploration \" is extremely important in rl for learning the optimal control policy , and the lack of exploration during the learning process may lead to divergency .",
    "nevertheless , for on - policy learning , exploration is restricted because only the evaluating policies can be used to generate data . from the literature investigation , it is found that the  exploration \" issue is rarely discussed in existing work that using rl techniques for control design ; * \\4 ) the implementation structure is complicated , such as in @xcite , three nns are required for approximating cost function , control and disturbance policies , respectively ; * \\5 ) most of existing approaches @xcite are implemented online , thus they are difficult for real - time control because the learning process is often time - consuming .",
    "furthermore , online control design approaches just use current data while discard past data , which implies that the measured system data is used only once and thus results in low utilization efficiency .    to overcome the drawbacks mentioned above , we propose an off - policy rl approach to solve the @xmath0 control problem with unknown internal system dynamic @xmath37 .",
    "in this section , an off - policy rl method for @xmath0 control design is derived and its convergence is proved .",
    "then , a nn - based critic - actor structure is developed for implementation purpose .      to derive the off - policy rl method , we rewrite the system as : @xmath75 + k [ w - w^{(i ) } ] .\\ ] ] for @xmath76 .",
    "let @xmath77 be the solution of the linear pde , then taking derivative along the state of system yields ,    [ eq_4_2 ] & = [ v^(i+1)]^t ( f + gu^(i ) + kw^(i ) ) + & + [ v^(i+1)]^t g[u - u^(i ) ] + [ v^(i+1)]^t k [ w - w^(i ) ] .    with the linear pde ,",
    "conducting integral on both sides of equation in time interval @xmath78 $ ] and rearranging terms yield ,    & _ t^t+t [ v^(i+1)(x())]^t g(x())[u ( ) - u^(i)(x ( ) ) ] d + & + _ t^t+t [ v^(i+1)(x())]^t k(x ( ) ) [ w()-w^(i)(x ( ) ) ] d + & + v^(i+1)(x(t ) ) - v^(i+1)(x(t+t ) )    [ eq_4_3 ] & = _ t^t+t ( h^t(x())h(x ( ) ) + u^(i ) ( x ( ) ) _ r^2 .",
    "- ^2 w^(i ) ( x ( ) )",
    "^2 ) d    it is observed from the equation that the cost function @xmath79 can be learned by using arbitrary input signals @xmath64 and @xmath65 , rather than the evaluating policies @xmath72 and @xmath80 .",
    "then , replacing linear pde in algorithm [ algorithm_1 ] with results in the off - policy rl method . to show its convergence , theorem [ theorem_4.1 ] establishes the equivalence between iterative equations and .",
    "[ theorem_4.1 ] let @xmath81 and @xmath82 .",
    "@xmath77 is the solution of equation iff @xmath83if and only if @xmath84 it is the solution of the linear pde , i.e. , equation is equivalent to the linear pde .    *",
    "* from the derivation of equation , it is concluded that if @xmath77 is the solution of the linear pde , then @xmath77 also satisfies equation . to complete the proof",
    ", we have to show that @xmath77 is the unique solution of equation .",
    "the proof is by contradiction .    before starting the contradiction proof , we derive a simple fact",
    ". consider    [ eq_4_4 ] & _ t 0 _ t^t+t ( ) d + & = _ t 0 ( _ 0^t+t ( ) d - _ 0^t ( ) d ) + & = _ 0^t ( ) d + & = ( t ) .    from , we have    [ eq_4_5 ] & = _ t 0   + & = _ t 0 _ t^t+t [ v^(i+1)(x())]^t g(x ( ) ) + & d + & + _ t 0 _ t^t+t [ v^(i+1)(x())]^t k(x ( ) ) + & d + & - _ t 0 _ t^t+t d.    by using the fact , the equation is rewritten as    [ eq_4_6 ] & = [ v^(i+1)(x(t))]^t g(x(t))[u(t ) - u^(i)(x(t ) ) ] + & + [ v^(i+1)(x(t))]^t k(x(t ) ) [ w(t)-w^(i)(x(t ) ) ] + & - .",
    "suppose that @xmath85 is another solution of equation with boundary condition @xmath86 .",
    "thus , @xmath87 also satisfies equation , i.e. ,    [ eq_4_7 ] & = [ w(x(t))]^t g(x(t))[u(t ) - u^(i)(x(t ) ) ] + & + [ w(x(t))]^t k(x(t ) ) [ w(t)-w^(i)(x(t ) ) ] + & - .    substituting equation from yields ,    [ eq_4_7a ] & ( v^(i+1)(x )",
    "-w(x ) ) + & = ^t g(x)[u - u^(i)(x ) ] + & + ^t k(x ) [ w -w^(i)(x ) ] .",
    "this means that equation holds for @xmath76 .",
    "if letting @xmath88 , we have @xmath89 = 0.\\ ] ] then , @xmath90 for @xmath91 , where @xmath92 is a real constant , and @xmath93 .",
    "thus,@xmath94 , i.e. , @xmath95 for @xmath91 .",
    "this completes the proof .",
    "@xmath61    [ remark3 ]",
    "_ it follows from theorem [ theorem_4.1 ] that the solution of equation is equivalent to equation , and thus the convergence of the off - policy rl is guaranteed , i.e. , the solution of the iterative equation will converge to the solution of hji equation as iteration step @xmath60 increases .",
    "different from the equation in algorithm [ algorithm_1 ] , the off - policy rl with equation uses system data instead of the internal system dynamic @xmath37 .",
    "hence , the off - policy rl can be regarded as a direct learning method for @xmath0 control design , which avoids the identification of @xmath37 .",
    "in fact , the information of @xmath37 is embedded in the measurement of system data .",
    "that is to say , the lack of knowledge about @xmath37 does not have any impact on the off - policy rl to obtain the solution of hji equation and the @xmath0 control policy .",
    "it worth pointing out that the equation is similar with the form of the irl @xcite , which is an important framework for control design of continuous - time systems .",
    "the irl in @xcite is an online optimal control learning algorithm for partially unknown systems . @xmath61",
    "_      to solve equation for the unknown function @xmath77 based on system data , we develop a nn based actor - critic structure . from the well known high - order weierstrass approximation theorem @xcite , a continuous function can be represented by an infinite - dimensional linearly independent basis function set . for real practical application , it is usually required to approximate the function in a compact set with a finite - dimensional function set .",
    "we consider the critic nn for approximating the cost function on a compact set @xmath38 .",
    "let @xmath96^t$ ] be the vector of linearly independent activation functions for critic nn , where @xmath97 is the number of critic nn hidden layer neurons . then , the output of critic nn is given by @xmath98 for @xmath99 , where @xmath100^t$ ] is the critic nn weight vector .",
    "it follows from , and that the disturbance and control policies are given by :    ^(i ) ( x ) & = - r^-1 g^t ( x ) ^t(x ) ^(i)[eq_4_10 ] + ^(i ) ( x ) & = ^-2 k^t ( x ) ^t(x ) ^(i ) [ eq_4_9 ]    for @xmath99 , and @xmath101^t$ ] is the jacobian of @xmath102 .",
    "expressions and can be viewed as actor nns for the disturbance and control policies respectively , where @xmath103 and @xmath104 are the activation function vectors and @xmath105 is the actor nn weight vector .    due to estimation errors of the critic and actor",
    "nns - , the replacement of @xmath106 and @xmath107 in the iterative equation with @xmath108 and @xmath109 respectively , yields the following residual error :    [ eq_4_11 ] & ^(i ) ( x(t ) , u(t ) , w(t ) ) + & _ t^t+t [ u ( ) - ^(i)(x())]^t g^t(x ( ) ) ^t(x ( ) ) ^(i+1 ) d + & + _ t^t+t [ w ( ) -^(i)(x ( ) ) ] ^t k^t(x ( ) ) ^t(x ( ) ) ^(i+1 ) d + & + [ ( x(t ) ) - ( x(t+t))]^t ^(i+1 ) + & - _ t^t+t d + & = _",
    "t^t+t u^t ( ) g^t(x ( ) ) ^t(x ( ) ) ^(i+1 ) d + & + _ t^t+t ( ^(i ) ) ^t ( x ( ) ) g(x ( ) ) r^-1 g^t(x ( ) ) + & ^t(x ( ) ) ^(i+1 ) d + & + _ t^t+t w^t ( ) k^t(x ( ) ) ^t(x ( ) ) ^(i+1 ) d + & - ^-2 _ t^t+t ( ^(i ) ) ^t ( x ( ) ) k(x ( ) ) k^t(x ( ) ) + & ^t(x ( ) ) ^(i+1 ) d + [ ( x(t ) ) - ( x(t+t))]^t ^(i+1 ) + & - _ t^t+t ( ^(i ) ) ^t ( x ( ) ) g(x ( ) ) r^-1 g^t(x ( ) ) + & ^t(x ( ) ) ^(i ) d + & + ^-2 _ t^t+t ( ^(i ) ) ^t ( x ( ) ) k(x ( ) ) k^t(x ( ) ) + & ^t(x ( ) ) ^(i ) d - _",
    "t^t+t h^t(x())h(x ( ) ) d    for notation simplicity , define    _ ( x(t ) ) & ^t + _ g ( x(t ) ) & _ t^t+t ( x ( ) ) g(x ( ) ) r^-1 g^t(x ( ) ) + & ^t(x ( ) ) d + _ k ( x(t ) ) & _ t^t+t ( x ( ) ) k(x ( ) ) k^t(x ( ) ) + & ^t(x ( ) ) d + _ u ( x(t),u(t ) ) & _",
    "t^t+t u^t ( ) g^t(x ( ) ) ^t(x ( ) ) d + _ w ( x(t),w(t ) ) & _",
    "t^t+t w^t ( ) k^t(x ( ) ) ^t(x ( ) ) d + _ h ( x(t ) ) & _",
    "t^t+t h^t(x())h(x ( ) ) d    then , expression is rewritten as    [ eq_4_12 ] & ^(i ) ( x(t ) , u(t ) , w(t ) ) + & = _ u ( x(t),u(t ) ) ^(i+1 ) + ( ^(i ) ) ^t _ g ( x(t))^(i+1 ) + & + _ w ( x(t),w(t ) ) ^(i+1 ) - ^-2 ( ^(i ) ) ^t _ k ( x(t ) ) ^(i+1 ) + & + _ ^(i+1 ) - ( ^(i ) ) ^t _ g ( x(t))^(i ) + & + ^-2 ( ^(i ) ) ^t _ k ( x(t ) ) ^(i ) - _ h ( x(t ) ) .    for description convenience",
    ", expression is represented as a compact form @xmath110 where    & ^(i)(x(t ) , u(t ) , w(t ) ) _ u ( x(t),u(t ) ) + ( ^(i ) ) ^t _ g ( x(t ) ) + & + _ w ( x(t),w(t ) ) - ^-2 ( ^(i ) ) ^t _ k ( x(t ) ) + _ + & ^(i)(x(t ) ) ( ^(i ) ) ^t _ g ( x(t))^(i ) + & - ^-2 ( ^(i ) ) ^t _ k ( x(t ) ) ^(i ) + _ h ( x(t ) )",
    ".    for description simplicity , denote @xmath111^t$ ] .",
    "based on the method of weighted residuals @xcite , the unknown critic nn weight vector @xmath112 can be computed in such a way that the residual error @xmath113 ( for @xmath114 ) of is forced to be zero in some average sense .",
    "thus , projecting the residual error @xmath113 onto @xmath115 and setting the result to zero on domain @xmath116 using the inner product , @xmath117 , i.e. , @xmath118 then , the substitution of into yields , @xmath119 where the notations @xmath120 and @xmath121 are given by + @xmath122 $ ] and @xmath123^t $ ] .",
    "+ thus , @xmath124 can be obtained with    [ eq_4_15 ] ^(i+1 ) = & ^-1 _   + & _ .",
    "the computation of inner products @xmath125 and @xmath126 involve many numerical integrals on domain @xmath116 , which are computationally expensive .",
    "thus , the monte - carlo integration method @xcite is introduced , which is especially competitive on multi - dimensional domain .",
    "we now illustrate the monte - carlo integration for computing @xmath125 .",
    "let @xmath127 , and @xmath128 be the set that sampled on domain @xmath116 , where @xmath129 is size of sample set @xmath130 .",
    "then , @xmath125 is approximately computed with    [ eq_4_16 ] & _ + & = _ ( ^(i)(x , u , w ) ) ^t ^(i)(x , u , w ) d(x , u , w ) + & = _ m=1^m ( ^(i)(x_m , u_m , w_m ) ) ^t ^(i)(x_m , u_m , w_m ) + & = ( z^(i ) ) ^t z^(i )    where @xmath131^t $ ] .",
    "similarly ,    [ eq_4_17 ] & _ + & = _ m=1^m ( ^(i)(x_m , u_m , w_m ) ) ^t ^(i)(x_m ) + & = ( z^(i ) ) ^t ^(i )    where @xmath132^t $ ] .",
    "then , the substitution of and into yields , @xmath133^{-1 } \\left ( z^{(i ) } \\right)^t \\eta^{(i)}.\\ ] ]    it is noted that the critic nn weight update rule is a least - square scheme . based on the update rule , the procedure for @xmath0 control design with nn - based off - policy rl is presented in algorithm [ algorithm_2 ] .",
    "[ remark4a ] _ in the least - square scheme , it is required to compute the inverse of matrix @xmath134 .",
    "this means that the matrix @xmath135 should be full column rank , which depends on the richness of the sampling data set @xmath136 and its size @xmath137 . to attain this goal in real implementation",
    ", it would be useful by increasing the size @xmath137 , starting from different initial states , and using rich input signals , such as random noises , sinusoidal function noises with enough frequencies . of course",
    ", it would be nice , if possible but is not a necessity , to use the persistent exciting input signals , while it is still a difficult issue @xcite that requires further investigation . in a word ,",
    "the choices of rich input signals and the size @xmath137 are generally experience - based .",
    "@xmath61 _    ' '' ''    [ algorithm_2 ]    ' '' ''    note that algorithm [ algorithm_2 ] has two parts : the first part is step 1 for data processing , i.e. , measure system data @xmath138 for computing @xmath139 and @xmath140 ; the second part is steps 2 - 4 for offline iterative learning the solution of the hji equation .",
    "[ remark4 ] _ from theorem [ theorem_4.1 ] , the proposed off - policy rl is mathematically equivalent to the model - based spua ( i.e. , algorithm [ algorithm_2 ] ) , which is proved to be a newton s method @xcite .",
    "hence , the off - policy rl have the same advantages and disadvantages as the newton s method .",
    "that is to say , the off - policy rl is a local optimization method , and thus there exists a problem that an initial critic nn weight vecotr @xmath141 should be given such that the initial solution @xmath142 locates in a neighbourhood @xmath143 of the hji equation .",
    "in fact , this problem also widely arises in many existing works for solving optimal and @xmath0 control problems of either linear or nonlinear systems through the observations from computer simulation , such as @xcite . till present",
    ", it is still a difficult issue for finding proper initializations or developing global approaches .",
    "there is no exception for the proposed off - policy rl algorithm , where the selection of initial weight vector @xmath144 is still experience - based and requires further investigation .",
    "@xmath61 _    algorithm [ algorithm_2 ] can be viewed as an off - policy learning method according to references @xcite , which overcomes the drawbacks mentioned in section [ sec_3 ] , i.e. ,    * \\1 ) in the off - policy rl algorithm ( i.e. , algorithm [ algorithm_2 ] ) , the control @xmath64 and disturbance @xmath65 can be arbitrarily on @xmath145 and @xmath20 , where no error occurs during the process of generating data , and thus the accumulated error ( exists in the on - policy learning methods mentioned in section [ sec_3 ] ) can be reduced ; * \\2 ) in the algorithm [ algorithm_2 ] , the control @xmath64 and disturbance @xmath65 can be arbitrarily on @xmath145 and @xmath20 , and thus disturbance @xmath65 does not required to be adjustable ; * \\3 ) in the algorithm [ algorithm_2 ] , the cost function @xmath79 of control and disturbance policies @xmath146 can be evaluated by using system data generated with other different control and disturbance signals @xmath147 .",
    "thus , the obvious advantage of the developed off - policy rl method is that it can learn the cost function and control policy from system data that are generated according to a more exploratory or even random policies ; * \\4 ) the implementation of algorithm [ algorithm_2 ] is very simple , in fact _ only one _ nn is required , i.e. , critic nn .",
    "this means that once the critic nn weight vector @xmath124 is computed via , the action nns for control and disturbance policies can be obtained based on and accordingly ; * \\5 ) the developed off - policy rl method learns the @xmath1 control policy offline , which is then used for real - time control .",
    "thus , it is much more practical than online control design methods since less computational load will generate during real - time application .",
    "meanwhile , note that in algorithm [ algorithm_2 ] , once the terms @xmath139 and @xmath140 are computed with sample set @xmath130 ( i.e. , step 1 is finished ) , no extra data is required for learning the @xmath1 control policy ( in steps 2 - 4 ) .",
    "this means that the collected data set can be utilized repeatedly , and thus the utilization efficiency is improved compared to the online control design methods .",
    "[ remark5a ] _ observe that the experience replay based irl method @xcite can be viewed as an off - policy method based on its definition @xcite .",
    "there are three obvious differences between the method and the work of this paper .",
    "firstly , the method in @xcite is for solving the optimal control problem without external disturbance , while the off - policy rl algorithm in this paper is for solving the @xmath1 control problem with external disturbance .",
    "secondly , the method in @xcite is online adaptive control approach .",
    "the off - policy rl algorithm in this paper uses real system information , and learns the @xmath1 control policy by using an offline process .",
    "after the learning process is finished , the convergent control policy is employed for real system control .",
    "thirdly , the method in @xcite involves two nns ( i.e. , one critic nn and one actor nn ) for adaptive optimal control realization , while only one nn ( i.e. , critic nn ) is required in the algorithm of this paper.@xmath61 _      it is necessary to analyze the convergence of the nn - based off - policy rl algorithm . from theorem [ theorem_4.1 ] ,",
    "the equation in the off - policy rl is equivalent to the linear pde , which means that the derived least - square scheme is essentially for solving the linear pde . in @xcite ,",
    "a similar least - square method was suggested to solve the first order linear pde directly , wherein some theoretical results are useful for analyzing the convergence of the proposed nn - based off - policy rl algorithm . the following theorem [ theorem_4.2 ]",
    "is given to show the convergence of critic nn and actor nns .",
    "[ theorem_4.2 ] for @xmath148 , assume that @xmath149 is the solution of , the critic nn activation functions @xmath150 , @xmath151 are selected such that they are complete when @xmath152 , @xmath79 and @xmath153 can be uniformly approximated , and the set @xmath154 is linearly independent and complete for @xmath155 .",
    "then ,    & _ x |^(i+1)(x ) - v^(i+1)(x)| 0 [ eq_4_3.1 ] + & _ x |^(i+1)(x ) - v^(i+1)(x)| 0 [ eq_4_3.2 ] + & _ x |^(i+1)(x ) - u^(i+1)(x)| 0 [ eq_4_3.3 ] + & _ x |^(i+1)(x ) - w^(i+1)(x)| 0 [ eq_4_3.4 ] .    * proof . *",
    "the proof procedure of the above results is very similar with that in reference @xcite , and thus some similar proof steps will be omitted for avoidance of repetition . to use the theoretical results in @xcite",
    ", we firstly prove the @xmath156 is linear independent by contradiction .",
    "assume this is not true , then there exists a vector @xmath157^t \\neq 0 $ ] such that @xmath158 which means that for @xmath91 ,    & _ t^t+t _ l=1^l _ l _ l ( f+gu^(i)+kw^(i ) ) d + & = _",
    "l d + & = _ l=1^l _",
    "l [ _ l(x(t+t ) ) - _ l(x(t ) ) ] + & = _ l=1^l _",
    "l ( x(t+t),x(t ) ) + & = 0 .",
    "this contradicts the fact that the set @xmath159 is linearly independent , which implies that the set @xmath156 is linear independent .",
    "+ from theorem [ theorem_4.1 ] , @xmath79 is the solution of the linear pde . then , with the same procedure used in theorem 2 and corollary 2 of the reference @xcite , the results - can be proven . and",
    "the result can be proven in a similar way for .",
    "@xmath61 + the results - in theorem [ theorem_4.2 ] imply that the critic nn and actor nns are convergent . in the following theorem [ theorem_4.3 ] , we prove that the nn - based off - policy rl algorithm converges uniformly to the solution of the hji equation and the @xmath0 control policy",
    ".    [ theorem_4.3 ] if the conditions in theorem [ theorem_4.2 ] hold , then , for @xmath160 , @xmath161 , when @xmath162 and @xmath163 , we have    & _ x |^(i)(x ) - v^*(x)| < [ eq_4_3.5 ] + & _ x |^(i)(x ) - u^*(x)| < [ eq_4_3.6 ] + & _ x |^(i)(x ) - w^*(x)| < [ eq_4_3.7 ] .    * proof . * by following the same proof procedures in theorems 3 and 4 in @xcite , the results - can be proven directly . similar to , the result can also be proven .",
    "[ remark7 ] _ the proposed off - policy rl method is to learn the solution of the hji equation and the @xmath1 control policy .",
    "it follows from theorem [ theorem_4.3 ] that the control policy @xmath74 designed by the off - policy rl will uniformly converge to the @xmath1 control policy @xmath67 . with the @xmath1 control policy ,",
    "it is noted from lemma [ lemma_1 ] that the closed - loop system with @xmath51 is locally asymptotically stable .",
    "furthermore , it is observed from that for the closed - loop system with disturbance @xmath35 , the output @xmath164 is in @xmath17 @xcite , i.e. , the closed - loop system is ( bounded - input bounded - output ) stable.@xmath61 _",
    "in this section , the developed nn - based off - policy rl method ( i.e. , algorithm [ algorithm_2 ] ) is simplified for linear @xmath1 control design .",
    "consider the linear system :    \\(t ) = & ax(t ) + b_2u(t ) + b_1w(t ) [ eq_5.1 ] + z(t ) = & cx [ eq_5.2 ]    where @xmath165 , @xmath166 , @xmath167 and @xmath168 .",
    "then , the hji equation of the linear system and results in an algebraic riccati equation ( are ) @xcite : @xmath169 where @xmath170 .",
    "if are has a stabilizing solution @xmath171 , the solution of the hji equation of the linear system and is @xmath172 , and then the linear @xmath1 control policy is accordingly given by @xmath173    consequently , @xmath174 , then the iterative equations - in algorithm [ algorithm_1 ] are respectively represented with    u^(i ) = & -r^-1 b_2^t p^(i ) x [ eq_5.3 ] + w^(i ) = &",
    "^-2 b_1^t p^(i ) x [ eq_5.4 ]    @xmath175    where @xmath176 and @xmath177 @xmath178 @xmath179 .",
    "similar to the derivation of the off - policy rl method for nonlinear @xmath1 control design in section [ sec_4 ] , rewrite the linear system as @xmath180 + b_1 [ w - w^{(i ) } ] .\\ ] ] based on equations - , the equation is given by    [ eq_5.7 ] & _",
    "t^t+t x^t ( ) p^(i+1 ) b_2 d + & + _ t^t+t x^t ( ) p^(i+1 ) b_1 d + & + [ x(t ) -",
    "x(t+t)]^t p^(i+1 ) [ x(t ) - x(t+t ) ] + & = _",
    "t^t+t x^t ( ) ^(i ) x ( ) d    where @xmath181 is a @xmath182 unknown matrix to be learned . for notation simplicity ,",
    "define    _",
    "x(x(t ) ) & x(t ) - x(t+t ) + _ xx ( x(t ) ) & _ t^t+t x ( ) x()d + _ ux ( x(t),u(t ) ) & _ t^t+t u ( ) x()d + _ wx ( x(t),w(t ) ) & _ t^t+t",
    "w ( ) x()d    where @xmath183 denotes kronecker product .",
    "each term of equation can be written as :    & _",
    "t^t+t x^t ( ) p^(i+1 ) b_2 u ( ) d + & = ^t_ux ( x(t),u(t ) ) ( b^t_2 i ) vec ( p^(i+1 ) ) + & _",
    "t^t+t x^t ( ) p^(i+1 ) b_2 r^-1 b_2^t p^(i ) x ( ) d + & = ^t_xx ( x(t ) ) ( p^(i ) b_2 r^-1b^t_2 i ) vec ( p^(i+1 ) ) + & _",
    "t^t+t x^t ( ) p^(i+1 ) b_1 w ( ) d + & = ^t_wx ( x(t),w(t ) ) ( b^t_1 i ) vec ( p^(i+1 ) ) + & ^-2 _",
    "t^t+t x^t ( ) p^(i+1 ) b_1 b_1^t p^(i ) x ( ) d + & = ^-2 ^t_xx ( x(t ) ) ( p^(i ) b_1 b^t_1 i ) vec ( p^(i+1 ) ) + & [ x(t ) - x(t+t)]^t p^(i+1 ) [ x(t ) - x(t+t ) ] + & = ^t_x(x(t ) ) vec ( p^(i+1 ) ) + & _",
    "t^t+t x^t ( ) ^(i ) x ( ) d= ^t_xx ( x(t ) ) vec ( ^(i ) )    where @xmath184 denotes the vectorization of the matrix @xmath185 formed by stacking the columns of @xmath185 into a single column vector .",
    "then , equation can be rewritten as @xmath186 with    & ^(i)(x(t ) , u(t ) , w(t ) ) = ^t_ux ( x(t),u(t ) ) ( b^t_2 i ) + & + ^t_wx ( x(t),w(t ) ) ( b^t_1 i ) + ^t_x(x(t ) ) + & - ^-2 ( p^(i ) b_1 b^t_1 i ) ] + & ^(i)(x(t ) ) = ^t_xx ( x(t ) ) vec ( ^(i ) ) .",
    "it is noted that equation is equivalent to the equation with residual error @xmath187 .",
    "this is because no cost function approximation is required for linear systems .",
    "then , by collecting sample set @xmath130 for computing @xmath188 and @xmath189 , a more simpler least - square scheme can be derived to obtain the unknown parameter vector @xmath190 accordingly .     at each iteration.,width=163 ]     at each iteration.,width=163 ]",
    "in this section , the efficiency of the developed nn - based off - policy rl method is tested on a f16 aircraft plant .",
    "then , it is applied to the rotational / translational actuator ( rtac ) nonlinear benchmark problem .",
    "consider a f16 aircraft plant that used in @xcite , where the system dynamics is described by a linear continuous - time model :    = & x + & + u+ w [ eq_6.1 ] + z = & x.[eq_6.2 ]    where the system state vector is @xmath191^t $ ] , @xmath192 denotes the angle of attack , @xmath193 is the pitch rate and @xmath194 is the elevator deflection angle .",
    "the control input @xmath64 is the elevator actuator voltage and the disturbance @xmath65 is wind gusts on angle of attack .",
    "select @xmath195 and @xmath196 for the @xmath2-gain performance . then , solve the associated are with the matlab command care , we obtain @xmath197 .",
    "\\nonumber\\ ] ]     weights at each iteration.,width=163 ]     weights at each iteration.,width=163 ]    for linear systems , the solution of the hji equation is @xmath172 , thus the complete activation function vector for critic nn is @xmath198^t$ ] of size @xmath199 .",
    "then , the idea critic nn weight vector is @xmath200^t $ ] @xmath201 @xmath202^t$ ] . letting initial critic nn weight @xmath203 , iterative",
    "stop criterion @xmath204 and integral time interval @xmath205 , algorithm [ algorithm_2 ] is applied to learn the solution of the are . to generate sample set @xmath130 ,",
    "let sample size @xmath206 and generate random noise in interval [ 0,0.1 ] as input signals .",
    "figures [ fig1]-[fig2 ] give the critic nn weight @xmath144 at each iteration , in which the dash lines represent idea values of @xmath207 .",
    "it is observed from the figures that the critic nn weight vector converges to the idea values of @xmath207 at @xmath208 iteration .",
    "then , the efficiency of the developed off - policy rl method is verified .",
    "in addition , to test the influence of the parameter @xmath209 to algorithm [ algorithm_2 ] , we re - conduct simulation with different parameter cases : @xmath210 , and the results show that the critic nn weight vector @xmath144 still converges to the idea values of @xmath207 at @xmath208 iteration for all cases .",
    "this implies that the developed off - policy rl algorithm [ algorithm_2 ] is insensitive to the parameter @xmath209 .",
    "the rtac nonlinear benchmark problem @xcite has been widely used to test the abilities of control methods .",
    "the dynamics of this nonlinear plant poses challenges because the rotational and translation motions are coupled .",
    "the rtac system is given as follows :    = & + u + & + w [ eq_simulation_11 ] + z = & i x[eq_simulation_12 ]    where @xmath211 . for the @xmath2-gain performance ,",
    "let @xmath195 and @xmath212 .     of the closed - loop rtac system.,width=163 ]     of the closed - loop rtac system.,width=163 ]    then , the developed off - policy rl method is used to solve the nonlinear @xmath1 control problem of system and .",
    "select the critic nn activation function vector as @xmath213^t \\nonumber\\end{aligned}\\ ] ] of size @xmath214 . with",
    "the initial critic nn weight @xmath215 , iterative stop criterion @xmath204 and integral time interval @xmath216 , algorithm [ algorithm_2 ] is applied to learn the solution of the hji equation . to generate sample set @xmath130 ,",
    "let sample size @xmath217 and generate random noise in interval [ 0,0.5 ] as input signals .",
    "it is found that the critic nn weight vector converges fast to @xmath218^t \\nonumber\\end{aligned}\\ ] ] at @xmath219 iteration .",
    "figures [ fig3]-[fig4 ] show first 10 critic nn weights ( i.e. , @xmath220 ) at each iteration . with the convergent critic",
    "nn weight vector @xmath221 , the @xmath1 control policy can be computed with . under the disturbance",
    "signal @xmath222 $ ] is a random number ) , closed - loop simulation is conducted with the @xmath1 control policy .",
    "figures [ fig5]-[fig7 ] give the trajectories of state and control policy . to show the relationship between @xmath2-gain and time ,",
    "define the following ratio of disturbance attenuation as @xmath223 figure [ fig8 ] shows the curve of @xmath224 , where it converges to 3.7024@xmath225 as time increases , which implies that the designed @xmath1 control law can achieve a prescribed @xmath2-gain performance level @xmath43 for the closed - loop system .     of the closed - loop rtac system.,width=163 ]     of the closed - loop rtac system.,width=163 ]",
    "a nn - based off - policy rl method has been developed to solve the @xmath0 control problem of continuous - time systems with unknown internal system model . based on the model - based spua ,",
    "an off - policy rl method is derived , which can learn the solution of hji equation from the system data generated by arbitrary control and disturbance signals .",
    "the implementation of the off - policy rl method is based on an actor - critic structure , where only one nn is required for approximating the cost function , and then a least - square scheme is derived for nn weights update .",
    "the effectiveness of the proposed nn - based off - policy rl method is tested on a linear f16 aircraft plant and a nonlinear rtac problem .",
    "j.  j. murray , c.  j. cox , g.  g. lendaris , and r.  saeks , `` adaptive dynamic programming , '' _ ieee transactions on systems , man , and cybernetics , part c : applications and reviews _ , vol .",
    "32 , no .  2 , pp .",
    "140153 , 2002 .",
    "v.  yadav , r.  padhi , and s.  balakrishnan , `` robust / optimal temperature profile control of a high - speed aerospace vehicle using neural networks , '' _ ieee transactions on neural networks _ , vol .",
    "18 , no .  4 , pp .",
    "11151128 , 2007 .",
    "h.  zhang , q.  wei , and y.  luo , `` a novel infinite - time optimal tracking control scheme for a class of discrete - time nonlinear systems via the greedy hdp iteration algorithm , '' _ ieee transactions on systems , man , and cybernetics , part b : cybernetics _ , vol .",
    "38 , no .  4 , pp .",
    "937942 , 2008 .",
    "h.  zhang , y.  luo , and d.  liu , `` neural - network - based near - optimal control for a class of discrete - time affine nonlinear systems with control constraints , '' _ ieee transactions on neural networks _",
    "20 , no .  9 , pp .  14901503 , 2009 .",
    "h.  zhang , l.  cui , x.  zhang , and y.  luo , `` data - driven robust approximate optimal tracking control for unknown general nonlinear systems using adaptive dynamic programming method , '' _ ieee transactions on neural networks _ , vol .  22 , no .  12 , pp .  22262236 , 2011 .",
    "d.  liu , d.  wang , d.  zhao , q.  wei , and n.  jin , `` neural - network - based optimal control for a class of unknown discrete - time nonlinear systems using globalized dual heuristic programming , '' _ ieee transactions on automation science and engineering _ , vol .  9 , no .  3 , pp .",
    "628634 , 2012 .",
    "b.  luo and h .- n .",
    "wu , `` approximate optimal control design for nonlinear one - dimensional parabolic pde systems using empirical eigenfunctions and neural network , '' _ ieee transactions on systems , man , and cybernetics , part b : cybernetics _ , vol .",
    "42 , no .  6 , pp .",
    "15381549 , 2012 .",
    "h .- n . wu and b.  luo , `` heuristic dynamic programming algorithm for optimal control design of linear continuous - time hyperbolic pde systems , '' _ industrial & engineering chemistry research _",
    "51 , no .",
    "27 , pp .  93109319 , 2012 .",
    "q.  wei and d.  liu , `` a novel iterative @xmath226-adaptive dynamic programming for discrete - time nonlinear systems , '' _ ieee transactions on automation science and engineering _ , doi : 10.1109/tase.2013.2280974 , online available , 2013 .",
    "d.  liu , d.  wang , and h.  li , `` decentralized stabilization for a class of continuous - time nonlinear interconnected systems using online learning optimal control approach , '' _ ieee transactions on neural networks and learning systems _ , vol .  25 , no .  2 , pp .  418428 , 2014 .",
    "h.  modares , f.  l. lewis , and m .- b .",
    "naghibi - sistani , `` integral reinforcement learning and experience replay for adaptive optimal control of partially - unknown constrained - input continuous - time systems , '' _ automatica _ , vol .",
    "50 , no .  1 ,",
    "193202 , 2014 .",
    "d.  liu and q.  wei , `` policy iteration adaptive dynamic programming algorithm for discrete - time nonlinear systems , '' _ ieee transactions on neural networks and learning systems _ , vol .  25 , no .  3 , pp .",
    "621634 , 2014 .",
    "b.  luo , h .-",
    "wu , and h .- x .",
    "li , `` data - based suboptimal neuro - control design with reinforcement learning for dissipative spatially distributed processes , '' _ industrial & engineering chemistry research _",
    ", doi : 10.1021/ie4031743 , online available , 2014 .",
    "m.  abu - khalaf , f.  l. lewis , and j.  huang , `` policy iterations on the hamilton ",
    "isaacs equation for @xmath0 state feedback control with input saturation , '' _ ieee transactions on automatic control _ , vol .  51 , no .  12 , pp .  19891995 , 2006 .",
    "k.  g. vamvoudakis and f.  l. lewis , `` online solution of nonlinear two - player zero - sum games using synchronous policy iteration , '' _ international journal of robust and nonlinear control _ , vol .",
    "22 , no .  13 , pp .  14601483 , 2012 .",
    "y.  feng , b.  anderson , and m.  rotkowitz , `` a game theoretic algorithm to compute local stabilizing solutions to hjbi equations in nonlinear @xmath0 control , '' _ automatica _ , vol .",
    "45 , no .  4 , pp .",
    "881888 , 2009 .",
    "d.  liu , h.  li , and d.  wang , `` neural - network - based zero - sum game for discrete - time nonlinear systems via iterative adaptive dynamic programming algorithm , '' _ neurocomputing _ , vol .",
    "110 , no .",
    "13 , pp .  92100 , 2013 .",
    "n.  sakamoto and a.  v.  d. schaft , `` analytical approximation methods for the stabilizing solution of the hamilton ",
    "jacobi equation , '' _ ieee transactions on automatic control _ , vol .  53 , no .  10 , pp .  23352350 , 2008 .",
    "r.  beard , g.  saridis , and j.  wen , `` approximate solutions to the time - invariant hamilton ",
    "bellman equation , '' _ journal of optimization theory and applications _ , vol .",
    "96 , no .  3 , pp .",
    "589626 , 1998 .",
    "s.  mehraeen , t.  dierks , s.  jagannathan , and m.  l. crow , `` zero - sum two - player game theoretic formulation of affine nonlinear discrete - time systems using neural networks , '' _ ieee transactions on cybernetics _ , vol .",
    "43 , no .  6 , pp .",
    "16411655 , 2013 .",
    "h.  modares , f.  l. lewis , and m .- b .",
    "n. sistani , `` online solution of nonquadratic two - player zero - sum games arising in the @xmath0 control of constrained input systems , '' _ international journal of adaptive control and signal processing _ , p.  in press , 2014 .",
    "b.  luo and h .- n .",
    "wu , `` computationally efficient simultaneous policy update algorithm for nonlinear @xmath1 state feedback control with galerkin s method , '' _ international journal of robust and nonlinear control _",
    "23 , no .  9 , pp .  9911012 , 2013 .",
    "h .- n . wu and b.  luo , `` neural network based online simultaneous policy update algorithm for solving the hji equation in nonlinear @xmath1 control , '' _ ieee transactions on neural networks and learning systems _ , vol .",
    "23 , no .  12 , pp .  18841895 , 2012 .",
    "h.  zhang , l.  cui , and y.  luo , `` near - optimal control for nonzero - sum differential games of continuous - time nonlinear systems using single - network adp , '' _ ieee transactions on cybernetics _ , vol .",
    "43 , no .  1 ,",
    "pp .  206216 , 2013 .",
    "z.  chen and s.  jagannathan , `` generalized hamilton ",
    "bellman formulation - based neural network control of affine nonlinear discrete - time systems , '' _ ieee transactions on neural networks _ ,",
    "19 , no .  1 ,",
    "pp .  90106 , 2008 .",
    "g.  escobar , r.  ortega , and h.  sira - ramirez , `` an @xmath227 disturbance attenuation solution to the nonlinear benchmark problem , '' _ international journal of robust and nonlinear control _",
    ", vol .  8 , no .  4 - 5 , pp .",
    "311330 , 1999 .",
    "biao luo received the b.e .",
    "degree in measuring and control technology and instrumentations and the m. e. degree in control theory and control engineering from xiangtan university , xiangtan , china , in 2006 and 2009 , respectively .",
    "he is now working for the ph .",
    "d. degree in control science and engineering with beihang university ( beijing university of aeronautics and astronautics ) , beijing , china .    from february 2013 to august 2013 , he was a research assistant with the department of system engineering and engineering management ( seem ) , city university of hong kong , kowloon , hong kong . from september 2013 to december 2013 , he was a research assistant with department of mathematics and science , texas a&m university at qatar , doha , qatar .",
    "his current research interests include distributed parameter systems , optimal control , data - based control , fuzzy / neural modeling and control , hypersonic entry / reentry guidance , learning and control from big data , reinforcement learning , approximate dynamic programming , and evolutionary computation .",
    "huai - ning wu was born in anhui , china , on november 15 , 1972 .",
    "he received the b.e .",
    "degree in automation from shandong institute of building materials industry , jinan , china and the ph.d .",
    "degree in control theory and control engineering from xian jiaotong university , xian , china , in 1992 and 1997 , respectively .    from august 1997 to july 1999 , he was a postdoctoral researcher in the department of electronic engineering at beijing institute of technology , beijing , china . in august 1999 , he joined the school of automation science and electrical engineering , beihang university ( formerly beijing university of aeronautics and astronautics ) , beijing . from december 2005",
    "to may 2006 , he was a senior research associate with the department of manufacturing engineering and engineering management ( meem ) , city university of hong kong , kowloon , hong kong . from october to december during 2006 - 2008 and from july to august in 2010 , he was a research fellow with the department of meem , city university of hong kong . from july to august in 2011 and 2013",
    ", he was a research fellow with the department of systems engineering and engineering management , city university of hong kong .",
    "he is currently a professor with beihang university .",
    "his current research interests include robust control , fault - tolerant control , distributed parameter systems , and fuzzy / neural modeling and control .",
    "wu serves as associate editor of the ieee transactions on systems , man & cybernetics : systems .",
    "he is a member of the committee of technical process failure diagnosis and safety , chinese association of automation .",
    "tingwen huang is a professor at texas a & m university at qatar .",
    "he received his b.s .",
    "degree from southwest normal university ( now southwest university ) , china , 1990 , his m.s .",
    "degree from sichuan university , china , 1993 , and his ph.d .",
    "degree from texas a & m university , college station , texas , 2002 .",
    "after graduated from texas a&m university , he worked as a visiting assistant professor there .",
    "then he joined texas a & m university at qatar ( tamuq ) as an assistant professor in august 2003 , then he was promoted to professor in 2013 .",
    "huang s focus areas for research interests include neural networks , chaotic dynamical systems , complex networks , optimization and control . he has authored and co - authored more than 100 refereed journal papers ."
  ],
  "abstract_text": [
    "<S> the @xmath0 control design problem is considered for nonlinear systems with unknown internal system model . </S>",
    "<S> it is known that the nonlinear @xmath1 control problem can be transformed into solving the so - called hamilton - jacobi - isaacs ( hji ) equation , which is a nonlinear partial differential equation that is generally impossible to be solved analytically . </S>",
    "<S> even worse , model - based approaches can not be used for approximately solving hji equation , when the accurate system model is unavailable or costly to obtain in practice . to overcome these difficulties , an off - policy reinforcement leaning ( rl ) method is introduced to learn the solution of hji equation from real system data instead of mathematical system model , and its convergence is proved . in the off - policy rl method , </S>",
    "<S> the system data can be generated with arbitrary policies rather than the evaluating policy , which is extremely important and promising for practical systems . for implementation purpose , a neural network ( nn ) based actor - critic structure </S>",
    "<S> is employed and a least - square nn weight update algorithm is derived based on the method of weighted residuals . finally , the developed nn - based off - policy rl method is tested on a linear f16 aircraft plant , and further applied to a rotational / translational actuator system .    </S>",
    "<S> shell : bare demo of ieeetran.cls for journals    @xmath1 control design ; reinforcement learning ; off - policy learning ; neural network ; hamilton - jacobi - isaacs equation . </S>"
  ]
}