{
  "article_text": [
    "we will consider in this paper _ las vegas algorithms _ , introduced a few decades ago by  @xcite , i.e. randomized algorithms whose runtime might vary from one execution to another , even with the same input . an important class of las vegas algorithms is the family of _ stochastic local search _ methods  @xcite . they have been used in combinatorial optimization for finding optimal or near - optimal solutions for several decades @xcite , stemming from the pioneering work of lin on the traveling salesman problem @xcite .",
    "theses methods are now widely used in combinatorial optimization to solve real - life problems when the search space is too large to be explored by complete search algorithm , such as mixed integer programming or constraint solving , c.f . @xcite .    in the last years , several proposal for implementing local search algorithms on parallel computer have been proposed , the most popular being to run several competing instances of the algorithms on different cores , with different initial conditions or parameters , and let the fastest process win over others .",
    "we thus have an algorithm with the minimal execution time among the launched processes .",
    "this lead to so - called independent multi - walk algorithms in the local search community  @xcite and portfolio algorithms in the sat community ( satisfiability of boolean formula )  @xcite .",
    "this parallelization scheme can of course be generalized to any las vegas algorithm .",
    "the goal of this paper is to study the parallel performances of las vegas algorithms under this independent multi - walk scheme , and to predict the performances of the parallel execution from the runtime distribution of the sequential runs of a given algorithm .",
    "we will confront these predictions with actual speedups obtained for a parallel implementation of a local search algorithm and show that the prediction can be quite accurate , matching the actual speedup very well up to 100 parallel cores and then with a deviation limited to about 20% up to 256 cores .",
    "the paper is organized as follows .",
    "section 2 is devoted to present the definition of las vegas algorithms , their parallel multi - walk execution scheme , and the main idea for predicting the parallel speedups .",
    "section  [ probabilistic - model ] will detail the probabilistic model of las vegas algorithms and their parallel execution scheme .",
    "section  [ local - search ] will present the example of local search algorithms for combinatorial optimization , while section  [ benchmarks ] will detail the benchmark problems and the sequential performances . then",
    ", section  [ prediction ] will apply the general probabilistic model to the benchmark results and thus predict their parallel speedup , which will be compared to actual speedups of a parallel implementation in section  [ analysis ] . a short conclusion and future work end will the paper .",
    "we borrow the following definition from  @xcite , chapter 4 .",
    "an algorithm a for a problem class @xmath0 is a ( generalized ) las vegas algorithm if and only if it has the following properties :    1 .",
    "if for a given problem instance @xmath1 , algorithm a terminates returning a solution @xmath2 , @xmath2 is guaranteed to be a correct solution of @xmath3 .",
    "2 .   for any given instance @xmath4 ,",
    "the run - time of a applied to @xmath3 is a random variable .",
    "this is a slight generalization of the classical definition , as it includes algorithms which are not guaranteed to return a solution .",
    "a large class of las vegas algorithms is the so - called family of _ metaheuristics _ , such as simulated annealing , genetic algorithms , tabu search , swarm optimization , ant - colony optimization , etc , which have been applied to different sets of problems ranging from resource allocation , scheduling , packing , layout design , frequency allocation , etc .",
    "parallel implementation of local search metaheuristics  @xcite has been studied since the early 1990s , when parallel machines started to become widely available  @xcite . with the increasing availability of pc clusters in the early 2000s ,",
    "this domain became active again  @xcite .",
    "apart from domain - decomposition methods and population - based method ( such as genetic algorithms ) ,  @xcite distinguishes between single - walk and multi - walk methods for local search .",
    "single - walk methods consist in using parallelism inside a single search process , _ e.g.,_for parallelizing the exploration of the neighborhood ( see for instance  @xcite for such a method making use of gpus for the parallel phase ) .",
    "multi - walk methods ( parallel execution of multi - start methods ) consist in developing concurrent explorations of the search space , either independently or cooperatively with some communication between concurrent processes .",
    "sophisticated cooperative strategies for multi - walk methods can be devised by using solution pools  @xcite , but require shared - memory or emulation of central memory in distributed clusters , thus impacting on performance .",
    "a key point is that a multi - walk scheme is easier to implement on parallel computers without shared memory and can lead , in theory at least , to linear speedups  @xcite .",
    "however this is only true under certain assumptions and we will see that we need to develop a more realistic model in order to cope with the performance actually observed in parallel executions .",
    "let us now formally define a parallel multi - walk las vegas algorithm .",
    "an algorithm a for a problem class @xmath0 is a ( parallel ) multi - walk las vegas algorithm if and only if it has the following properties :    1 .",
    "it consists of @xmath5 instances of a sequential las vegas algorithm a for @xmath0 , say @xmath6 .",
    "if , for a given problem instance @xmath7 , there exists at least one @xmath8 $ ] such that @xmath9 terminates , then let @xmath10 $ ] , be the instance of a terminating with the minimal runtime and let @xmath2 be the solution returned by @xmath11 .",
    "then algorithm a terminates in the same time as @xmath11 and returns solution @xmath2 .",
    "3 .   if , for a given problem instance @xmath7 , all @xmath12 $ ] , do not terminate then a does not terminate .",
    "the multi - walk parallel scheme is rather simple , yet it provides an interesting test - case to study how las vegas algorithms can scale - up in parallel .",
    "indeed runtime will vary among the processes launched in parallel and the overall runtime will be that of the instance with minimal execution time ( i.e. `` long '' runs are killed by `` shorter '' ones ) .",
    "the question is thus to quantify the ( relative ) notion of short and long runs and their probability distribution .",
    "this might gives us a key to quantify the expected parallel speed - up .",
    "obviously , this can be observed from the sequential behavior of the algorithm , and more precisely from the proportion of long and short runs in the sequential runtime distribution .    in the following ,",
    "we propose a probabilistic model to quantify the expected speed - up of multi - walk las vegas algorithms .",
    "this makes it possible to give a general formula for the speed - up , depending on the sequential behavior of the algorithm .",
    "our model is related to _ order statistics _ , a rather new domain of statistics  @xcite , which is the statistics of sorted random draws .",
    "indeed , explicit formulas have been given for several well - known distributions .",
    "relying on an approximation of the sequential distribution , we compute the average speed - up for the multi - walk extension .",
    "experiments show that the prediction is quite good and opens the way for defining more accurate models and apply them to larger classes of algorithms .",
    "previous works @xcite studied the case of a particular distribution for the sequential algorithm , the exponential distribution .",
    "this case is ideal and the best possible , as it yields a linear speed - up .",
    "our model makes it possible to approximate las vegas algorithms by other types of distribution , such as a shifted exponential distribution or a lognormal distribution . in the last two cases the speed - up is",
    "no longer linear , but admits a finite limit when the number of processors tends toward infinity .",
    "we will see that it fits experimental data for some problems .",
    "local search algorithms are stochastic processes .",
    "they include several random components : choice of an initial configuration , choice of a move among several candidates , plateau mechanism , random restart , etc . in the following",
    ", we will consider the _ computation time _ of an algorithm ( whatever it is ) as a random variable , and use elements of probability theory to study its multi - walk parallel version .",
    "notice that the computation time is not necessarily the cpu - time ; it can also be the number of iterations performed during the execution of the algorithm .",
    "consider a given algorithm on a given problem of a given size , say , the magic - square@xmath13 .",
    "depending on the result of some random components inside the algorithm , it may find a solution after @xmath14 iterations , @xmath15 iterations , or @xmath16 iterations .",
    "the number of iterations of the algorithm is thus a discrete random variable , let s call it @xmath17 , with values in @xmath18 .",
    "@xmath17 can be studied through its cumulative distribution , which is by definition , the function @xmath19 s.t .",
    "@xmath20}}$ ] , or by its distribution , which is by definition the derivative of @xmath19 : @xmath21 .",
    "it is often more convenient to consider distributions with values in @xmath22  because it makes calculations easier .",
    "for the same reason , although @xmath23 is defined in @xmath18 , we will use its natural extension to @xmath22 .",
    "the expectation of the computation is then defined as @xmath24}}=\\int_0^\\infty t { \\ensuremath{f_{y}}}(t ) dt$ ]    assume that the base algorithm is concurrently run in parallel on @xmath5 cores . in other words , over each core the running process is a fork of the algorithm . the first process which finds a solution then kills all others .",
    "and the algorithm terminates .",
    "the @xmath25-th process corresponds to a draw of a random variable @xmath26 , following distribution @xmath23 .",
    "the variables @xmath26 are thus independently and identically distributed ( i.i.d . ) .",
    "the computation time of the whole parallel process is also a random variable , let @xmath27 , with a distribution @xmath28 that depends both on @xmath5 and on @xmath29 .",
    "since all the @xmath26 are i.i.d . , the cumulative distribution @xmath30 can be computed as follows : @xmath31 } } \\\\   & = & { \\ensuremath{\\mathbb{p}[\\exists i \\in \\{1 ... n\\ } , x_i \\leq x]}}\\\\ & = & 1 - { \\ensuremath{\\mathbb{p}[\\forall i \\in \\{1 ... n\\ } , x_i > x]}}\\\\ & = & 1 - \\prod_{i=1}^{n } { \\ensuremath{\\mathbb{p}[x_i > x]}}\\\\ & = & 1 - \\left ( 1 - { \\ensuremath{\\mathcal{f}_{y}}}(x)\\right)^n \\\\\\end{aligned}\\ ] ] which leads to : @xmath32 thus , knowing the distribution for the base algorithm @xmath17 , one can calculate the distribution for @xmath33 . in the general case",
    ", the formula shows that the parallel algorithm favors short runs , by killing the slower processes .",
    "thus , we can expect that the distribution of @xmath33 moves toward the origin , and is more peaked . as an example , figure [ min - distribution ] shows this phenomenon when the base algorithm admits a gaussian distribution .",
    "distribution of @xmath27 , in the case where @xmath17 admits a gaussian distribution ( cut on @xmath34 and renormalized ) .",
    "the blue curve is @xmath17 .",
    "the distributions of @xmath33 are in pink for @xmath35 , in yellow for @xmath36 and in green for @xmath37 .",
    ", scaledwidth=40.0% ]      the model described above gives the probability distribution of a parallelized version of any random algorithm .",
    "we can now calculate the expectation for the parallel process with the following relation :    @xmath38 } } & = & \\int_0^\\infty t { \\ensuremath{f_{{\\ensuremath{z^{(n)}}}}}}(t ) dt\\\\ & = & n \\int_0^\\infty t { \\ensuremath{f_{y}}}(t ) ( 1-{\\ensuremath{\\mathcal{f}_{y}}}(t))^{n-1}dt \\\\\\end{aligned}\\ ] ]    unfortunately , this does not lead to a general formula for @xmath39 $ ] . in the following , we will study it for different specific distributions .    to measure the gain obtained by parallelizing the algorithm on @xmath5 core , we will study the speed - up @xmath40 defined as : @xmath41}}/{\\ensuremath{\\mathbb{e}[{\\ensuremath{z^{(n)}}}]}}\\ ] ]    again , no general formula can be computed and the expression of the speed - up will depend on the distribution of @xmath17 .",
    "however , it is worth noting that our computation of the speed - up is related to order statistics , see @xcite for a detailed presentation .",
    "for instance , the first order statistics of a distribution is its minimal value , and the @xmath42 order statistic is its @xmath42-smallest value . for predicting the speedup , we are indeed interested in computing the expectation of the distribution of the minimum draw . as the above formula suggests , this may lead to heavy calculations , but recent studies such as @xcite give explicit formulas for this quantity for several classical probability distributions .",
    "assume that @xmath17 has a shifted exponential distribution , as it has been suggested by  @xcite .",
    "@xmath43}}=&x_0 + 1/\\lambda\\end{aligned}\\ ] ] then the above formula can be symbolically computed by hand : @xmath44    the intuitive observation of section [ min - distribution ] is easily seen on the expression of the parallel distribution , which has an initial value multiplied by @xmath5 but an exponential factor decreasing @xmath5-times faster , as shown on the curves of figure  [ lois - exponentielle ] .    for an exponential distribution , here in blue with @xmath45 and @xmath46 , simulations of the distribution of @xmath27for @xmath47 ( pink ) , @xmath48 ( yellow ) and @xmath49 ( green).,scaledwidth=45.0% ]    and in this case",
    ", one can symbolically compute both the expectation and speed - up for @xmath27 : @xmath38}}&= & n",
    "\\lambda \\int_{x_0}^\\infty t e^{- n \\lambda ( t - x_0 ) } dt \\\\ & = & x_0+\\frac{1}{n\\lambda } \\\\    { \\ensuremath{\\mathcal{g}_{n}}}&= & \\frac{x_0+\\frac{1}{\\lambda } } { x_0+\\frac{1}{n\\lambda } } \\\\\\end{aligned}\\ ] ]    figure  [ speedup - exponentielle ] shows the evolution of the speed - up when the number of cores increases .",
    "with such a rather simple formula for the speed - up , it is worth studying what happens when the number of cores @xmath5 tends to infinity .",
    "depending on the chosen algorithm , @xmath50 may be null or not .",
    "if @xmath51 , then the expectation tends to @xmath14 and the speed - up is equal to @xmath5 .",
    "this case has already been studied by  @xcite . for @xmath52 ,",
    "the speed - up admits a finite limit which is @xmath53 .",
    "yet , this limit may be reached slowly , but depends on the value of @xmath50 and @xmath54 : obviously , the closest @xmath50 is to zero and the higher it will be . another interesting value is the coefficient of the tangent at the origin , which approximates the speed - up for a small number of cores . in case of an exponential ,",
    "it is @xmath55 .",
    "the higher @xmath50 and @xmath54 , the bigger is the speed - up at the beginning . in the following",
    ", we will see that , depending on the combinations of @xmath50 and @xmath54 , different behaviors can be observed",
    ".    predicted speed - up in case of an exponential distribution , with @xmath45 and @xmath46 , w.r.t .",
    "the number of cores.,scaledwidth=40.0% ]      other distributions can be considered , depending on the behavior of the base algorithm .",
    "we will study the case of a lognormal distribution , which is the log of a gaussian distribution , because it will be shown in section  [ ms200 ] that it fits one experiment .",
    "it has two parameters , the mean @xmath56 and the standard deviation @xmath57 . in the same way as the shifted exponential",
    ", we shift the distribution so that it starts at a given parameter @xmath50 .",
    "formally , a ( shifted ) lognormal distribution is defined as :    @xmath58    where erfc is the complementary error function defined by @xmath59 .",
    "figure  [ loi - lognormale ] depicts lognormal distributions of @xmath27 , for several @xmath5 .",
    "the computations for the distribution of @xmath27 , its expectation and the theoretical speed - up are quite complicated formulas . but @xcite gives an explicit formula for all the moments of lognormal order statistics with only a numerical integration step , from which we can derive a computation of the speed - up ( since the expectation of @xmath27is the first order moment for the first order statistics ) .",
    "this allows us to draw the general shape of the speed - up , an example being given on figure  [ speedup - lognormale ] .",
    "due to the numerical integration step , which requires numerical values for the number of cores @xmath5 , we restrict the computation to integer values of @xmath5 .",
    "this is a reasonable limitation as the number of cores is indeed an integer .    for a lognormal distribution , here in blue with @xmath51 , @xmath60 and @xmath61 ,",
    "simulations of the distribution of @xmath27for @xmath47 ( pink ) , @xmath48 ( yellow ) and @xmath49 ( green).,scaledwidth=40.0% ]    predicted speed - up in case of a lognormal distribution , with @xmath51 , @xmath60 and @xmath61 , depending on the number of cores ( on the abscissa).,scaledwidth=40.0% ]",
    "since about a decade , the interest for the family of local search methods and metaheuristics for solving large combinatorial problems has been growing and has attracted much attention from both the operations research and the artificial intelligence communities for solving real - life problems  @xcite .",
    "local search starts from a random configuration and tries to improve this configuration , little by little , through small changes in the values of the problem variables .",
    "hence the term `` local search '' as , at each time step , only new configurations that are `` neighbors '' of the current configuration are explored",
    ". the definition of what constitutes a neighborhood will of course be problem - dependent , but basically it consists in changing the value of a few variables only ( usually one or two ) .",
    "the advantage of local search methods is that they will usually quickly converge towards a solution ( if the optimality criterion and the notion of neighborhood are defined correctly ... ) and not exhaustively explore the entire search space .    applying local search to constraint satisfaction problems ( csp )",
    "has been attracting some interest since about a decade @xcite , as it can tackle csps instances far beyond the reach of classical propagation - based constraint solvers @xcite .",
    "a generic , domain - independent constraint - based local search method , named adaptive search , has been proposed by @xcite .",
    "this meta - heuristic takes advantage of the structure of the problem in terms of constraints and variables and can guide the search more precisely than a single global cost function to optimize , such as for instance the number of violated constraints",
    ". the algorithm also uses a short - term adaptive memory in the spirit of tabu search in order to prevent stagnation in local minima and loops .",
    "an implementation of adaptive search ( as ) has been developed in c language as a framework library and is available as a freeware at the url : +    we used this reference implementation for our experiments .",
    "the adaptive search method can be applied to a large class of constraints ( _ e.g. _ linear and non - linear arithmetic constraints , symbolic constraints , etc . ) and naturally copes with over - constrained problems @xcite .",
    "the input of the method is a constraint satisfaction problem ( csp for short ) , which is defined as a triple ( x;d;c ) , where x is a set of variables , d is a set of domains , i.e. , finite sets of possible values ( one domain for each variable ) , and c a set of constraints restricting the values that the variables can simultaneously take . for each constraint , an _ error function _ needs to be defined ; it gives , for each tuple of variable values , an indication of how much the constraint is violated .",
    "this idea has also been proposed independently by @xcite , where it is called `` penalty functions '' , and then reused by the comet system @xcite , where it is called `` violations '' . for example , the error function associated with an arithmetic constraint @xmath62 , for a given constant @xmath63 , can be @xmath64 .",
    "adaptive search relies on iterative repair , based on variable and constraint error information , seeking to reduce the error on the worst variable so far .",
    "the basic idea is to compute the error function for each constraint , then combine for each variable the errors of all constraints in which it appears , thereby projecting constraint errors onto the relevant variables .",
    "this combination of errors is problem - dependent , see @xcite for details and examples , but it is usually a simple sum or a sum of absolute values , although it might also be a weighted sum if constraints are given different priorities . finally , the variable with the highest error is designated as the `` culprit '' and its value is modified . in this second step ,",
    "the well known min - conflict heuristic @xcite is used to select the value in the variable domain which is the most promising , that is , the value for which the total error in the next configuration is minimal .    in order to prevent being trapped in local minima",
    ", the adaptive search method also includes a short - term memory mechanism to store configurations to avoid ( variables can be marked tabu and `` frozen '' for a number of iterations ) .",
    "it also integrates reset transitions to escape stagnation around local minima .",
    "a reset consists in assigning fresh random values to some variables ( also randomly chosen ) .",
    "a reset is guided by the number of variables being marked tabu .",
    "it is also possible to restart from scratch when the number of iterations becomes too large ( this can be viewed as a reset of all variables but it is guided by the number of iterations ) .",
    "the core ideas of adaptive search can be summarized as follow :    * to consider for each constraint a heuristic function that is able to compute an approximated degree of satisfaction of the goals ( the current _ error _ on the constraint ) ; * to aggregate constraints on each variable and project the error on variables thus trying to repair the _",
    "worst _ variable with the most promising value ; * to keep a short - term memory of bad configurations to avoid looping ( _ i.e. _ some sort of _ tabu list _ ) together with a reset mechanism .",
    "we have chosen to test this method on two problems from the csplib benchmark library  @xcite , and on a hard combinatorial problem abstracted from radar and sonar applications .",
    "after briefly introducing the classical benchmarks , we detail the latter problem , called costas array . then we show the performance and the speed - ups obtained with both sequential and a multi - walk adaptive search algorithm on these problems .",
    "we use two classical benchmarks from csplib consisting of :    * the all - intervalseries problem ( prob007 in csplib ) , * the magic - squareproblem ( prob019 in csplib ) .    although these benchmarks are academic , they are abstractions of real - world problems and could involve very large combinatorial search spaces , _ e.g.,_the",
    "200@xmath65200 magic - squareproblem requires 40,000 variables whose domains range over 40,000 values . indeed",
    "the search space in the adaptive search model ( using permutations ) is @xmath66 , _",
    "i.e.,_more than @xmath67 configurations .",
    "classical propagation - based constraint solvers can not solve this problem for instances higher than 20x20 . also note that we are tackling constraint _ satisfaction _",
    "problems as optimization problems , that is , we want to minimize the global error ( representing the violation of constraints ) to value zero , therefore finding a solution means that we actually reach the bound ( zero ) of the objective function to minimize",
    ".          this problem is described as ` prob007 ` in the csplib .",
    "this benchmark is in fact a well - known exercise in music  @xcite where the goal is to compose a sequence of @xmath68 notes such that all are different and tonal intervals between consecutive notes are also distinct .",
    "this problem is equivalent to finding a permutation of the @xmath68 first integers such that the absolute difference between two consecutive pairs of numbers are all different .",
    "this amounts to finding a permutation @xmath69 of @xmath70 such that the list @xmath71 is a permutation of @xmath72 .",
    "a possible solution for @xmath73 is @xmath74 since all consecutive distances are different :    3 @xmath75 6 @xmath76 0 @xmath77 7 @xmath78 2 @xmath79 4 @xmath80 5 @xmath81 1      the magic - squareproblem is catalogued as ` prob019 ` in csplib and consists in placing the numbers @xmath82 on an @xmath83 square such that each row , column and main diagonal equal the same sum ( the constant @xmath84 ) .",
    "for instance , this figure shows a well - known solution for @xmath85 ( depicted by albrecht drer in his engraving _ melancholia i _ , 1514 ) .",
    "a costas array is an @xmath86 grid containing @xmath68 marks such that there is exactly one mark per row and per column and the @xmath87 vectors joining the marks are all different .",
    "we give here an example of costas array of size 5 .",
    "it is convenient to see the costas arrayproblem ( cap ) as a permutation problem by considering an array of @xmath68 variables @xmath88 which forms a permutation of @xmath89 .",
    "the above costas array can thus be represented by the array @xmath90 $ ] .",
    "historically these arrays have been developed in the 1960 s to compute a set of sonar and radar frequencies avoiding noise @xcite . a very complete survey on costas arrays",
    "can be found in @xcite .",
    "the problem of finding a costas array of size @xmath68 is very complex since the required time grows exponentially with @xmath68 . in the 1980 s ,",
    "several algorithms have been proposed to build a costas array given @xmath68 ( methods to produce costas arrays of order 24 to 29 can be found in  @xcite ) , such as the welch construction @xcite and the golomb construction @xcite , but these methods can not built costas arrays of size @xmath91 and some higher non - prime sizes .",
    "nowadays , after many decades of research , it remains unknown if there exist any costas arrays of size @xmath91 or @xmath92 .",
    "another difficult problem is to enumerate all costas arrays for a given size . using the golomb and welch constructions , drakakis _ et .",
    "al _ present in @xcite all costas arrays for @xmath93 .",
    "they show that among the @xmath94 permutations , there are only 164 costas arrays , and 23 unique costas arrays up to rotation and reflection .",
    "we run our benchmarks in a sequential manner in order to have about 650 runtimes for each .",
    "sequential experiments , as well as parallel experiments , have been done on the _ griffon _ cluster of the grid5000 platform .",
    "the following tables  [ tab : seqtime ] and  [ tab : seqiter ] shows the minimum , mean , median and maximum respectively among the runtimes and the number of iterations of our benchmarks .",
    ".sequential execution times ( in seconds ) [ cols=\"^ , > , > , > , > \" , ]     it is worth noticing that our model approximates the behaviors of experimental results very closely , as shown by the predicted speed - ups matching closely the real ones .",
    "moreover we can see that on the three benchmark programs , we needed to use three different types of distribution ( exponential , shifted exponential and lognormal ) , in order to approximate the experimental data most closely .",
    "this shows that our model is quite general and can accommodate different types of parallel behaviors .",
    "a quite interesting behavior is exhibited by the costas 21 problem .",
    "our model predicts a linear speedup , up to 10,000 cores and beyond , and the experimental data gathered for this paper confirms this linear speed - up up to 256 cores .",
    "would it scale up with a larger number of cores ?",
    "indeed such an experiment has been done up to 8,192 cores on the jugene ibm bluegene / p at the jlich supercomputing center in germany ( with a total 294,912 cores ) , and reported in  @xcite , of which figure  [ fig : speedup - jugene ] is adapted .",
    "we can see that the speed - up is indeed linear up to 8,192 cores , thus showing the adequation of the prediction model with the real data .",
    "[ fig : speedup - jugene ]    finally , let us note that our method exhibits an interesting phenomenon . for the three problems considered , the probability of returning a solution in _ no _ iterations is non - null :",
    "since they start by a uniform random draw on the search space , there is a very small , but not null , probability that this random initialization directly returns the solution .",
    "hence , in theory , @xmath51 and the speed - up should be linear , with an infinite limit when the number of cores tends to infinity .",
    "intuitively , if the number of cores tends to infinity , at some point it will be large compared to the size of the search space and one of the cores is likely to immediately find the solution .",
    "yet , in practice , observations shows that the experimental curves may be better approximated by a shifted exponential with @xmath52 , as it is the case for ai  700 . with an exponential distribution ,",
    "this leads to non - linear speed - up with a finite limit .",
    "indeed , the experimental speed - up for ai  700 is far from linear . on the contrary ,",
    "costas  21 has a linear speed - up due to its @xmath95 , which makes the statistical test succeed for @xmath96 .",
    "firstly , this suggests that the comparison between @xmath50 and @xmath97 on a number of observations is a key element for the parallel behavior .",
    "it also means that the number of observations needed to properly approximate the sequential distribution probably depends on the problem .",
    "we have proposed a theoretical model for predicting and analyzing the speed - ups of las vegas algorithms . it is worth noticing that our model mimics the behaviors of the experimental results very closely , as shown by the predicted speedups matching closely the real ones .",
    "our practical experiments consisted in testing the accuracy of the model with respect to three instances of a local search algorithm for combinatorial optimization problems .",
    "we showed that the parallel speed - ups predicted by our statistical model are accurate , matching the actual speed - ups very well up to 64 parallel cores and then with a deviation of about 10% , 15% or 30% ( depending on the benchmark problem ) up to 256 cores .",
    "however , one limitation of our approach is that , in practice , we need to be able to compute the expectation of the minimum distribution . nevertheless , apart from the exponential distribution for which this computation is easy , recent results in the field of order statistics gives explicit formulas for a number of useful distributions : gaussian , lognormal , gamma , beta .",
    "this provides a wide range of tools to analyze different behaviors . in this paper",
    "we validated our approach on classical combinatorial optimization and csp benchmarks , but further research will consider a larger class of problems and algorithms , such as sat solvers and other randomized algorithms ( e.g. quick sort ) .",
    "another interesting extension of this work would be to devise a method for predicting the speed - up from scratch , that is , without any knowledge on the algorithm distribution .",
    "preliminary observation suggests that , given a problem and an algorithm , the general shape of the distribution is the same when the size of the instances varies .",
    "for example , the different instances of all - intervalthat we tested all admit a shifted exponential distribution .",
    "if this property is valid on a wide range of problems / algorithms , then we can develop a method for predicting the speed - up for large instances by learning the distribution shape on small instances ( which are easier to solve ) , and then estimating the parallel speed - up for larger instances with our model .",
    "j.  beard , j.  russo , k.  erickson , m.  monteleone , and m.  wright .",
    "costas array generation and search methodology . _ aerospace and electronic systems , ieee transactions on _ , 430 ( 2):0 522 538 , april 2007 .",
    "issn 0018 - 9251 .",
    "doi : 10.1109/taes.2007.4285351 .",
    "y.  caniou , d.  diaz , f.  richoux , p.  codognet , and s.  abreu .",
    "performance analysis of parallel constraint - based local search . in _",
    "ppopp 2012 , 17th acm sigplan symposium on principles and practice of parallel programming _ , new orleans , la , usa , 2012 .",
    "acm press .",
    "poster paper .",
    "d.  diaz , f.  richoux , y.  caniou , p.  codognet , and s.  abreu .",
    "parallel local search for the costas array problem . in _ ieee workshop on new trends in parallel computing and optimization ( pc012 ) , in conjunction with ipdps 2012 _ , shanghai , china , may 2012 . ieee press .",
    "s.  minton , m.  d. johnston , a.  b. philips , and p.  laird .",
    "minimizing conflicts : a heuristic repair method for constraint satisfaction and scheduling problems . _ artificial intelligence _",
    ", 580 ( 1 - 3):0 161205 , 1992 .      p.  m. pardalos , l.  s. pitsoulis , t.  d. mavridou , and m.  g.  c. resende .",
    "parallel search for combinatorial optimization : genetic algorithms , simulated annealing , tabu search and grasp . in _ proceedings of irregular _ , pages 317331 , 1995 .",
    "t.  van  luong , n.  melab , and e .-",
    ". local search algorithms on graphics processing units . in _",
    "evolutionary computation in combinatorial optimization _ , pages 264275 .",
    "lncs 6022 , springer verlag , 2010 ."
  ],
  "abstract_text": [
    "<S> we propose a probabilistic model for the parallel execution of _ las vegas algorithms _ , _ </S>",
    "<S> i.e.,_randomized algorithms whose runtime might vary from one execution to another , even with the same input . </S>",
    "<S> this model aims at predicting the parallel performances ( _ i.e.,_speedups ) by analysis the runtime distribution of the sequential runs of the algorithm . </S>",
    "<S> then , we study in practice the case of a particular las vegas algorithm for combinatorial optimization , on three classical problems , and compare with an actual parallel implementation up to 256 cores . </S>",
    "<S> we show that the prediction can be quite accurate , matching the actual speedups very well up to 100 parallel cores and then with a deviation of about 20% up to 256 cores .    </S>",
    "<S> theory , algorithms , performance .    </S>",
    "<S> las vegas algorithms , prediction , parallel speed - ups , local search , statistical modeling , runtime distributions . </S>"
  ]
}