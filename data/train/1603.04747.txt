{
  "article_text": [
    "understanding an individual s key topics of interest to meet their customized needs is an important challenge for personalization and information filtering applications , such as recommender systems . with the proliferation of diverse types of applications and smart devices",
    ", users are generating a large amount of text through their emails , chats , and on social media , such as twitter and facebook .",
    "this data typically contains many words that reflect their interests : for example , if we consider the work emails of someone who works on datacenter servers , they will likely have several semantically - similar words such as processors , cloud computing , virtualization etc .",
    "individual emails , however , tend to be more about the context in which these words are used , for instance , meetings , status updates , follow - ups , and so on .",
    "the topics themselves are diffused across all the emails  our goal in this paper is to build an unsupervised algorithm that automatically infers these key topics of interest .",
    "state - of - the - art topic modeling algorithms , such as _ latent dirichlet allocation _",
    "( lda , @xcite ) , have been successfully applied to discover the main topics across a large collection of documents for some time now , and are a natural candidate for solving the problem at hand .",
    "the typical goal of these methods has been to organize this collection according to the discovered topics  they work well for news articles , papers in a journal , etc . , where the document is about the key topics , and the same words show up several times .",
    "user - generated content , however , is usually about the context : actual keywords show up infrequently , and are surrounded by several contextual words such as meetings , status , email , thanks , etc . because of this difference in structure of corpus , lda tends to cluster topics with similar contexts , and for each topic tends to capture the most frequently used words .",
    "it , however , fails to capture the key topics across the entire corpus .    in this paper , we propose a new technique of topic modeling , _",
    "vec2topic _ , that is aimed toward capturing the key topics across user - generated content .",
    "our algorithm consists of two main ideas .",
    "the first idea is that a key topic in a corpus should contain a large number of semantically - similar words that relate to that topic .",
    "we capture this notion using a _ depth measure _ of a word , that helps identify clusters with the highest density of semantically - similar words .",
    "we leverage word semantics as captured by high - dimensional distributed word embeddings . specifically , we perform agglomerative clustering on these distributed word vectors over the vocabulary , and we use the resulting dendrogram to compute the depth score for a word as the number of links between the word and the root node .",
    "clusters that contain words with the highest depth scores reflect the user s key topics of interest .",
    "our second idea is aimed at deriving good labels that best describe these key topics  the idea is that such keywords not only have high depth , but also show up in the context of a large number of different words .",
    "we capture this using a _ degree measure _ of a word , which we define as the count of the number of unique words that co - occur with the word within a context window in the corpus .",
    "we then combine the depth and degree measures into a single .",
    "we use this score to rank all the topics in the corpus .",
    ".former enron ceo kenneth lay s emails : top-10 words from the top-3 scoring topics in _",
    "vec2topic _ compared with three lda - based topics [ cols=\"^,^,^,^,^,^ \" , ]     [ tab : apple ]      [ [ sec : normalization ] ] normalization parameters , @xmath0 and @xmath1 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the word score computed in uses the normalization parameters @xmath0 and @xmath1 , which were fixed as @xmath2 in the experiments .",
    "we briefly discuss how these may be modified to improve the performance of the algorithm in some cases .",
    "the score combines two different types of measures : depth and degree , which are distributed quite differently over the vocabulary .",
    "we need to ensure that the role both these measures play in identifying the core topics is `` balanced . '' in most documents , setting @xmath2 works because degree is normalized by the logarithmic transformation . however , we find that a more robust score can be obtained by choosing @xmath0 and @xmath1 so that after this power transformation the depth and degree measures both have median values equal to 1/2 .",
    "formally , we choose @xmath0 so that we have @xmath3 the value for @xmath1 can be computed in an analogous fashion . for datasets in this paper ,",
    "the values of @xmath0 and @xmath1 so computed are quite close to one ; incorporating this into the algorithm leaves all the results in the paper qualitatively unchanged .",
    "however , in our extensive experiments we did find some datasets for which using this normalization method improved performance .",
    "[ [ performance ] ] performance + + + + + + + + + + +    in this paper , we have demonstrated that _",
    "vec2topic _ works well both for user - generated and non - user generated datasets and even for single documents .",
    "the ability of the algorithm to capture core topics depends on how well - defined these topics are . for documents with a clear core topic , the algorithm can extract it even when the size of the document is quite small . in our extensive experiments",
    ", we found it to perform well even for documents with a couple of hundred tokens in total , such as a news article .",
    "however , in that case , we did not build local word embeddings and instead used the knowledge - base word vectors alone .",
    "the reason this works is because a document such as a news article is written around a central theme or topic , and hence the context of usage is not that consequential for identifying the topic .",
    "we would like to point out that though _ vec2topic _ does not explicitly use a probabilistic approach , its output is not deterministic because it depends on building local word embeddings . in our experiments , we used the skip - gram approach , which results in slightly different word embeddings each time it is run .",
    "this effect is more pronounced with smaller datasets , so while the degree score remains constant across runs , the depth score may vary slightly . nevertheless , the overall results and the top core topics are qualitatively quite robust to this effect .",
    "the performance of the algorithm is also limited by the quality of the knowledge - base vectors . in our experiments",
    ", we used the english wikipedia for our knowledge - base .",
    "one of the limitations therein was with respect to foreign words .",
    "this knowledge - base contains many foreign words ( for instance , movie names ) without sufficient context .",
    "this implies that the learned word representations of these words are not semantically accurate . to alleviate this issue , we removed such words ( this issue came up with mr .",
    "kaminski s emails ) .",
    "another related issue is that of people names . the word vectors for these tend to be quite close to each other , and if there are a large number of names , then this affects the depth measure .",
    "clearly , people names would not make for key topics .",
    "so , to resolve this issue , in the email corpora , we remove names of people listed in the address fields . in the nips",
    "dataset , we remove the references section of the paper .",
    "another way to resolve this issue is to run the dataset through a named entity recognition algorithm .",
    "in this paper we propose a novel technique for topic modeling that leverages understanding of word semantics using high - dimensional word vectors .",
    "we showed that _",
    "vec2topic _ works well to extract a user s key topics of interest across their own generated content  it also ranks these topics , and identifies keywords that best describe it .",
    "we contrasted it with the state - of - the - art topic modeling algorithm lda , and observed that it works much better when the topic keywords are spread across the various documents , and are surrounded by several contextual words that are generic in nature .",
    "further we observe that the technique is not limited to user generated content ; it works equally well on more structured documents such as scientific papers , news articles , blogs , web pages , etc .",
    "it is also fairly robust to the corpus size  it can scale from a single document to a large collection .",
    "one of our ongoing efforts is focused on extending the algorithm to identify phrases  sequences of keywords that together capture the user s key interests .",
    "for example , turning to the example of a professional who works on datacenter servers , the phrase `` cloud server virtualization '' conveys a lot more topical context than cloud , server and virtualization individually .",
    "each of these words may show up in several contexts in the user s data  we therefore need a way to rank such phrases . also , these phrases differ from bigram / trigram phrases in the sense that their words may not show up consecutively in the user s data .",
    "we believe extending topic modeling in this direction is an interesting avenue for future study .",
    "the authors would like to thank achal bassamboo for many useful discussions .",
    "r.  collobert and j.  weston . a unified architecture for natural language processing : deep neural networks with multitask learning . in _ proceedings of the 25th international conference on machine learning _ , icml 08 , pages 160167 , new york , ny , usa , 2008 .",
    "e.  h. huang , r.  socher , c.  d. manning , and a.  y. ng .",
    "improving word representations via global context and multiple word prototypes . in _ proceedings of the 50th annual meeting of the association for computational linguistics : long papers - volume 1 _ , acl 12 , pages 873882 , stroudsburg , pa , usa , 2012 .",
    "association for computational linguistics .",
    "t.  mikolov , i.  sutskever , k.  chen , g.  s. corrado , and j.  dean . distributed representations of words and phrases and their compositionality . in c.",
    "burges , l.  bottou , m.  welling , z.  ghahramani , and k.  weinberger , editors , _ advances in neural information processing systems 26 _ , pages 31113119 .",
    "curran associates , inc . , 2013 .",
    "r.  socher , e.  h. huang , j.  pennin , c.  d. manning , and a.  y. ng .",
    "dynamic pooling and unfolding recursive auto - encoders for paraphrase detection . in _ advances in neural information processing systems _ , pages 801809 , 2011 .",
    "r.  socher , c.  c. lin , c.  manning , and a.  y. ng .",
    "parsing natural scenes and natural language with recursive neural networks . in _ proceedings of the 28th international conference on machine learning ( icml-11 ) _ , pages 129136 , 2011 .",
    "r.  socher , j.  pennington , e.  h. huang , a.  y. ng , and c.  d. manning .",
    "semi - supervised recursive auto - encoders for predicting sentiment distributions . in _ proceedings of the conference on empirical methods in natural language processing _ , pages 151161 .",
    "association for computational linguistics , 2011 .",
    "r.  socher , a.  perelygin , j.  y. wu , j.  chuang , c.  d. manning , a.  y. ng , and c.  potts .",
    "recursive deep models for semantic compositionality over a sentiment treebank . in _ proceedings of the conference on empirical methods in natural language processing ( emnlp )",
    "_ , volume 1631 , page 1642 .",
    "citeseer , 2013 .",
    "j.  turian , l.  ratinov , and y.  bengio .",
    "word representations : a simple and general method for semi - supervised learning . in _ proceedings of the 48th annual meeting of the association for computational linguistics _ , pages 384394 .",
    "association for computational linguistics , 2010 ."
  ],
  "abstract_text": [
    "<S> we propose a new algorithm for topic modeling , _ vec2topic _ , that identifies the main topics in a corpus using semantic information captured via high - dimensional distributed word embeddings . </S>",
    "<S> our technique is unsupervised and generates a list of topics ranked with respect to importance . </S>",
    "<S> we find that it works better than existing topic modeling techniques such as latent dirichlet allocation for identifying key topics in user - generated content , such as emails , chats , etc . , where topics are diffused across the corpus . </S>",
    "<S> we also find that _ vec2topic _ works equally well for non - user generated content , such as papers , reports , etc . , and for small corpora such as a single - document . </S>"
  ]
}