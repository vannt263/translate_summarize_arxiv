{
  "article_text": [
    "the design of fast exponential time algorithms for finding exact solutions to np - hard problems such as independent set and dominating set has been a topic for research for over 30 years , see e.g. , the results on independent set in the 1970s by tarjan and trojanowski @xcite .",
    "a number of different techniques have been used for these and other exponential time algorithms @xcite .",
    "an important paradigm for the design of exact algorithms is _ branch and reduce _ , pioneered in 1960 by davis and putnam @xcite .",
    "typically , in a branch and reduce algorithm , a collection of reduction rules and branching rules are given .",
    "each reduction rule simplifies the instance to an equivalent , simpler instance .",
    "if no rule applies , the branching rules generate a collection of two or more instances , on which the algorithm recurses .    an important recent development in the analysis of branch and",
    "reduce algorithms is _ measure and conquer _ , which has been introduced by fomin , grandoni and kratsch @xcite .",
    "the measure and conquer approach helps to obtain good upper bounds on the running time of branch and reduce algorithms , often improving upon the currently best known bounds for exact algorithms .",
    "it has been used successfully on dominating set @xcite , independent set @xcite , dominating clique @xcite , the number of minimal dominating sets @xcite , connected dominating set @xcite , minimum independent dominating set @xcite , and possibly others .    in this paper , we show that the measure and conquer approach can not only be used for the _ analysis _ of exact algorithms , but also for the _ design _ of such algorithms . more specifically ,",
    "measure and conquer uses a non - standard size measure for instances .",
    "this measure is based on weight vectors , which are computed by solving a quasiconvex programming problem .",
    "analysis of the solution of this quasiconvex program yields not only an upper bound to the running time of the algorithm , but also shows where we should improve the algorithm",
    ". this can lead to a new rule , which we add to the branch and reduce algorithm .",
    "we apply this _",
    "design by measure and conquer _ methodology to a set cover modelling of the dominating set problem , identical to the setting in which measure and conquer was first introduced .",
    "if we start with the trivial algorithm , then , we can obtain in a number of steps the original algorithm of fomin et al .",
    "@xcite , but with additional steps , we obtain a faster algorithm , using @xmath0 time and polynomial space , with a variant that uses exponential memory and @xmath1 time .",
    "we also show that at this point we can not improve this measure and conquer computed running time , unless we choose a different measure or change the branching rules .",
    "while for several classic combinatorial problems , the first non - trivial exact algorithms date many years ago , for the dominating set problem , the first algorithms with running time @xmath2 with @xmath3 are from 2004 , with three independent papers : by fomin et al .",
    "@xcite , by randerath and schiermeyer @xcite , and by grandoni @xcite .",
    "the so far fastest algorithm for dominating set was found in 2005 by fomin , grandoni , and kratsch @xcite : this algorithm uses @xmath4 time and polynomial space , or @xmath5 time and exponential space .",
    "given a collection of non - empty sets @xmath6 , a _ set cover _ of @xmath6 is a subset @xmath7 such that every element in any of the sets in @xmath6 occurs in some set in @xmath8 . in the set cover problem",
    "we are given a collection @xmath6 and are asked to compute a set cover of minimum cardinality .",
    "the universe @xmath9 of a set cover problem instance is the set of all elements in any set in @xmath6 ; @xmath10 .",
    "the frequency @xmath11 of an element @xmath12 is the number of sets in @xmath6 in which this element occurs .",
    "let @xmath13 be the set of sets in @xmath6 in which the element @xmath14 occurs .",
    "we define a connected component @xmath8 in a set cover problem instance @xmath6 in a natural way : a minimal non - empty subset @xmath15 for which all elements in the sets in @xmath8 occur only in sets contained in @xmath8 .",
    "the dimension @xmath16 of a set cover problem instance is the sum of the number of sets in @xmath6 and the number of elements in @xmath9 ; @xmath17 .",
    "let @xmath18 be an undirected graph .",
    "a subset @xmath19 of nodes is called a _ dominating set _ if every node @xmath20 is either in @xmath21 or adjacent to some node in @xmath21 .",
    "the dominating set problem is to compute for a given graph @xmath22 a dominating set of minimum cardinality .",
    "we can reduce the minimum dominating set problem to the set cover problem by introducing a set for each node of @xmath22 which contains the node itself and its neighbours ; @xmath23 \\;|\\ ; v \\in v \\}$ ] .",
    "thus we can solve a dominating set problem on a graph of @xmath24 nodes by a minimum set cover algorithm running on an instance of dimension @xmath25 .",
    "both problems are long known to be np - complete @xcite , which motivates the search for fast exponential time algorithms .",
    "in this section , we give our new algorithm for dominating set .",
    "the algorithm is an improvement to the algorithm by fomin et al .",
    "@xcite ; it is obtained from this algorithm by adding some additional reduction rules .",
    "these rules were derived using the design by measure and conquer approach , see section  [ section : design ] . after introducing our algorithm , we recall the necessary background of the measure and conquer method @xcite .",
    "our algorithm for the dominating set problem uses the set cover modelling of dominating set shown in section [ sec : definitions ] .",
    "it is a branch and reduce algorithm on this modelling consisting of four simple reduction rules , one base case for the recursion and a branching rule .",
    "see algorithm [ algorithm ] .    for a given problem instance",
    "we first apply the following reduction rules :    1 .",
    "_ base case_. [ rule : matching ] if all sets in the instance are of size at most two then finding a minimum set cover is equivalent to finding a maximum matching in a graph .",
    "introduce a node for each element and an edge for each set of size two .",
    "now the maximum matching joined with some sets containing the unmatched nodes form a minimum set cover .",
    "this matching can be computed in polynomial time @xcite .",
    "_ splitting connected components_. [ rule : component ] if the instance contains multiple connected components , compute the minimum set cover in each connected component separately .",
    "3 .   _ subset rule_. [ rule : subset ] if the instance contains sets @xmath26 , @xmath27 with @xmath28 , then we remove @xmath26 from the instance .",
    "namely , in each minimum set cover that contains @xmath26 , we can replace @xmath26 by @xmath27 and obtain a minimum set cover without @xmath27 .",
    "subsumption rule_. [ rule : subsumption ] if the set of sets @xmath29 in which an element @xmath30 occurs is a subset of the set of sets @xmath31 in which another element @xmath14 occurs , we remove the element @xmath14 . for any set cover , covering @xmath30",
    "also covers @xmath14 .",
    "5 .   _ unique element or singleton rule_. if any set of size one remains in the instance after application of the previous rules , we add this set to the set cover . for the element in this set must occur uniquely in this set ,",
    "otherwise it would have been a subset of another set and have been removed by rule [ rule : subset ] .",
    "_ avoiding unnecessary branchings based on frequency two elements_. [ rule : special ] for any set @xmath32 in the problem instance let @xmath33 be the number of frequency two elements it contains .",
    "let @xmath34 be the number of elements in the union of sets containing the other occurrences of these frequency two elements , excluding any element already in @xmath32 .",
    "if for any set @xmath32 : @xmath35 then include @xmath32 in the set cover . + this rule is correct since if we would branch on @xmath32 and include it in the set cover we would cover @xmath36 elements with one set .",
    "if the set cover does not use @xmath32 , it must use all sets containing the other occurrence of the frequency two elements , since they have become unique elements now . notice that by rule [ rule : subsumption ] all other occurrences of the frequency two elements must be in different sets and thus we would cover @xmath37 elements with @xmath33 sets .",
    "so if @xmath35 the first case can be preferred over the second : we can just add @xmath32 to the cover and have @xmath38 sets left to cover at least this much elements .    for the branching rule",
    ", we select a set of maximum cardinality and create two subproblems by either including it in the minimum set cover and removing all newly covered elements from the problem instance or removing it .",
    "msc(@xmath6 ) = \\ { minimum set cover of @xmath6 by computing a matching msc(@xmath8 ) + msc(@xmath39 ) msc(@xmath40 ) @xmath41 @xmath42 @xmath43 ) @xmath44 let @xmath45 @xmath46 @xmath47 }      the basic idea of _ measure and conquer _ is the usage of a non - standard measure for the complexity of a problem instance in combination with an extensive subcase analysis . in the case of set cover , we give weights to set sizes and element frequencies , and sum these weights over all items and sets .",
    "we enumerate many subcases in which the algorithm can branch and derive recurrence relations for each of these cases in terms of these weights .",
    "finally we obtain a large numerical optimisation problem which computes the weights corresponding to the smallest solution to all recurrence relations , giving an upper bound on the running time of our algorithm .",
    "this analysis is similar to @xcite .",
    "we let @xmath48 $ ] be the weight of an element of frequency @xmath49 and a set of size @xmath49 respectively , and set our _ variable measure of complexity _",
    "@xmath50 to : @xmath51 sets of different sizes and elements of different frequencies contribute equally to the dimension of the instance , but now larger sets and higher frequency elements can contribute more to the measured complexity of the instance .",
    "furthermore we set @xmath52 since all frequency one elements and size one sets are removed by the reduction rules . for later use",
    "we introduce quantities representing the reduction in problem complexity when the size of a set or the frequency of an element is reduced by one . for technical reasons",
    ", these quantities must be non - negative .",
    "@xmath53    the next step will be to derive recurrence relations representing problem instances the algorithm branches on .",
    "let @xmath54 be the number of subproblems generated in order to solve a problem of measured complexity @xmath55 . and let @xmath56 ( include @xmath32 ) and @xmath57 ( discard @xmath32 ) be the difference in measured complexity of both subproblems compared to the problem instance we branch on .",
    "finally let @xmath58 , where @xmath59 the number of elements in @xmath32 of frequency  @xmath49 .",
    "if we add @xmath32 to the set cover , @xmath32 is removed together with all its elements .",
    "this results in a reduction in size of @xmath60 .",
    "because of the removal of these elements , other sets are reduced in size ; this leads to an additional complexity reduction of at least @xmath61 . to keep the formula ( and the next ) linear , we set @xmath62 and keep the formula correctly modelling the algorithm by introducing the following constraints on the weights :",
    "@xmath63 one can show that including these in the numerical optimisation problem does not change the solution , as it gives the same weights .",
    "the constraints help to considerably speed up this optimisation process .",
    "if we discard @xmath32 , we also remove it from the problem instance , and hence all its elements are reduced in frequency by one .",
    "so we have a complexity reduction of @xmath64 . besides this reduction ,",
    "the sets which contain the second occurrences of any frequency two element are included in the set cover .",
    "notice that these must be different sets due to reduction rule [ rule : subsumption ] .",
    "because of rule [ rule : special ] we know that at least @xmath33 other elements must be in these sets as well , and these also must occur somewhere else in the instance , hence even more sets are reduced in size .",
    "summation leads to an additional size reduction of @xmath65 .",
    "here we also use rule [ rule : component ] to make sure that not all these frequency two elements occur in the same set , because in that case all considered sets form a connected component of at most five sets which thus can be solved in @xmath66 time .",
    "this leads to the following set of recurrence relations : @xmath67 : @xmath68 where @xmath69    we make the problem finite by setting for some large enough @xmath70 all @xmath71 for @xmath72 , and only consider the subcases @xmath73 , where @xmath74 .",
    "now we have a finite set of recurrences which model our algorithm since the recurrences for the cases where @xmath75 are dominated by those where @xmath76 .",
    "the best value for @xmath70 follows from the optimisation , for if chosen too small the now constant recurrences ( weights equal  1 ) will dominate all others in the optimum , and if chosen too large the extra @xmath77 and @xmath78 are optimised to 1 ( and the optimisation problem was unnecessarily hard ) . here",
    "@xmath70 equals  @xmath79 .",
    "a solution to this set of recurrence relations will be of the form @xmath80 , where @xmath81 is the smallest solution of the set of inequalities : @xmath82 since @xmath83 where @xmath84 the dimension of the problem , we know that the algorithm will have a running time of @xmath85 , for any @xmath86 : @xmath87 from here on we let @xmath88 be the error in the upward decimal rounding of @xmath81 .",
    "so for any given vector @xmath89 and @xmath90 we can now compute the running time measured with these weights . as a result",
    "we have obtained a numerical optimisation problem : choose the best weights so that the upper bound on the running time is minimal .    the numerical solution to this problem",
    "can be found in the last cell of table [ tab : steps ] , resulting in an upper bound on the running time of the algorithm of @xmath91 :      the sort of numerical optimisation problems arising from measure and conquer analyses are _ quasiconvex programs _ , named after the kind of function we are optimising : a _ quasiconvex function _ , which is a function with convex level sets @xmath92 .    to our knowledge",
    "there are currently two different techniques in use to solve these quasiconvex programs : randomised search , and eppstein s _ smooth quasiconvex programming _",
    "algorithm @xcite .",
    "we have implemented a variant of the second technique ; for details see @xcite .",
    "as discussed , we have now obtained the following result .",
    "algorithm [ algorithm ] solves a set cover problem instance of dimension @xmath84 in @xmath91 time and polynomial space .    using the minimum set cover modelling of dominating set this results in :    there exists an algorithm that solves the dominating set problem in @xmath0 time and polynomial space .",
    "we can further reduce the time complexity of the algorithm at the cost of exponential space .",
    "this can be done by dynamic programming ; the algorithm keeps track of all solutions to subproblems solved and if the same subproblem turns up more than ones it is looked up . notice that querying and storing the subproblems can be implemented in polynomial time .",
    "we compute the new time complexity based on @xcite and obtain :    [ thrm : exptime ] algorithm [ algorithm ] , modified as above , solves a set cover problem of dimension @xmath84 in @xmath93 time and space .",
    "there exists an algorithm that solves the dominating set problem in @xmath1 time and space .",
    "the beauty of our algorithm lies in the fact that it has been designed using a form of _ computer aided algorithm design _ which we call _ design by measure and conquer_. given a variable measure of complexity as in the analysis in section [ sec : m&c ] and a set of branching rules , all polynomial time computable reduction rules relative to this measure and branching rules follow by the method .",
    "we start with a trivial branch and reduce algorithm , i.e. one without any reduction rules and only consisting of the branching rule and a trivial base case ( if the problem is empty , return @xmath94 ) .",
    "next we exhaustively apply an improvement step , which comes up with a new reduction rule and hence a possibly faster algorithm .",
    "this changes the algorithm analysis technique measure and conquer into a technique for algorithm design .",
    "thus , this gives a very nice process , where a human invents additional reduction rules , and the computational power of our computer does the extensive measure and conquer analysis and points to all possible points of direct improvement .",
    "this combination has proven to be successful as we see from the results of section [ sec : m&calgorithm ] . while constructing our algorithm , the previously fastest algorithm for dominating set by fomin et al .",
    "@xcite has been obtained as an intermediate step .",
    "it has now been improved up to a point where we need to either change the branching rule ( or add new branching rules ) or modify the measure and conquer analysis , i.e. use a different variable measure or perform a more elaborate subcase analysis .",
    "see table [ tab : steps ] for information on the analysis and added rules for all algorithms , from the starting trivial algorithm without any reduction rules till we obtain algorithm [ algorithm ] .",
    "we now demonstrate how the improvement step works , by giving one such improvement as elaborate example , namely an improvement we can make when we start with the algorithm by fomin et al .  @xcite .",
    "this step is marked with a star in table [ tab : steps ] .",
    "first we perform a measure and conquer analysis on the current algorithm giving us the optimal instantiation of the variable measure , and an upper bound on the running time of the algorithm .",
    "next we examine the quasiconvex function we have just optimised .",
    "the quasiconvex function has the following form : @xmath95 where @xmath8 is the set of all possible instances the algorithm can branch on and @xmath96 are the differences in measured complexity between the generated subproblems and branching subcase @xmath97 .",
    "each one of the functions @xmath98 is quasiconvex ( see @xcite ) , i.e. it has convex level sets .",
    "the situation is very similar to finding the point @xmath99 of minimum maximum distance to a set of points @xmath100 in @xmath101 dimensional space : only a few points in @xmath100 have distance to @xmath99 tight to this maximum , and moving away from @xmath99 always results in at least one of these distances to increase .",
    "if one such tight point is moved or removed , this directly influences the optimum @xmath99 and the minimum maximum distance .",
    "we now consider the eight cases that are tight to the value of the quasiconvex function @xmath102 in the optimum .",
    "these are : latexmath:[\\[\\begin{aligned }     now , if we can formulate a reduction rule that either further reduces the size of any subproblem generated in these cases , or removes any of these cases completely , then we lower the value of the corresponding @xmath98 , or remove this @xmath98 respectively , resulting in a new optimum corresponding to a faster running time .",
    "we take the simplest case for improvement ; @xmath36 as small as possible , and with as low frequency elements as possible .",
    "this corresponds to an instance with : @xmath104 we emphasize that this is not an entire instance , but just a fragment of an instance containing the set @xmath32 used for branching and the collection of sets in which the elements from @xmath32 also occur . in an instance corresponding to this subcase the element 4 can be of frequency two or higher , but all sets are of size three or smaller .",
    "we note that we do not need to branch on this particular subcase : elements 1 and 2 occur in exactly the same sets , and thus if a set cover covers one of these , the other is covered as well .",
    "we generalise this and formulate the subsumption rule ( rule [ rule : subsumption ] of algorithm [ algorithm ] ) .",
    "now we have a new algorithm , for which we can adjust the measure and conquer analysis , and repeat this process .",
    "current formula for @xmath57 & + subcases considered & instance part of the simplest worst case ; + weights vectors @xmath105 and @xmath106 & @xmath107 other occurrences of elements of @xmath32 +   + * trivial algorithm * & @xmath108 +   + @xmath109 & @xmath110 + @xmath111 & @xmath112 + * stop when all sets of size one * & @xmath113 +   + @xmath114 & @xmath115 + @xmath116 & @xmath117 + * include all frequency one elements * & @xmath118 +   + @xmath119 & @xmath120 + @xmath121 & @xmath122 + * subset rule * & @xmath123 +   + @xmath124 & @xmath125 + @xmath126 & @xmath127 + * compute matching for size two sets@xmath128 * & @xmath129 +   + @xmath130 & @xmath131 + @xmath132 & @xmath133 + * subsumption rule * & @xmath134 +   + @xmath130 & @xmath135 + @xmath136 & @xmath137 + * avoid unnecessary branchings * & @xmath138 +   + @xmath139 & @xmath140 +   +   + * connected components ( final ) * & @xmath141 +   + @xmath139 & @xmath140 +   +   +      above , we discussed how to perform one step of the design by measure and conquer process . for a complete overview of the construction of algorithm [ algorithm ] see table [ tab : steps ] , with the relevant data for each improvement step .",
    "note from table [ tab : steps ] that after each new step , the example worst case instance part is no longer a valid worst case for the next step . as a result , at each step either some subcases are removed by using a larger smallest set @xmath32 or by removing small sets or elements ( setting @xmath142 or @xmath143 ) , or the size reduction in the formula for @xmath57 is increased . after each step we refactored the reduction rules and removed possible superfluous ones .",
    "we have not included the formula for @xmath56 in this table , since it does not change except that @xmath144 in early stages .",
    "it appears that we must use a different approach to obtain a faster algorithm .",
    "considered the following problem :    [ npcproblem ] given a set cover instance @xmath6 and a set @xmath145 with the properties :    1 .",
    "non of the reduction rules of algorithm [ algorithm ] apply to @xmath6 .",
    "all sets in @xmath6 have cardinality at most three ; @xmath146 .",
    "every element @xmath147 has frequency two .",
    "question : does there exist a minimum set cover of @xmath6 containing @xmath32 ?    [ prop : npc ] problem [ npcproblem ] is np - complete .",
    "proposition [ prop : npc ] implies that we can not formulate a polynomial time reduction rule that removes the current simplest worst case of our algorithm by deciding on whether @xmath32 is in a minimum set cover or not , unless @xmath148 .",
    "we can construct similar np - complete problems for all other worst cases of algorithm [ algorithm ] .",
    "therefore algorithm [ algorithm ] is optimal in some sense : we can not straightforwardly improve it by performing another iteration . in order to obtain a faster branch and reduce algorithm using polynomial time reduction rules with a smaller measure and",
    "conquer proved time bound , it is necessary to either change the variable measure , the branching rule(s ) , or perform a more extensive subcase analysis .",
    "very recently , we pursued the last option with little result .",
    "we tried to further subdivide the frequency two elements in the branch set depending on the size of the set containing their second occurence ( two or larger ) and if this is a set of size two , on the frequency of the other element in this set .",
    "this resulted in a set of very technical reduction rules and a small speedup for the case where we use only polynomial space .",
    "this speedup , however , was almost completely lost when using exponential space because some of the weights involved became almost zero .",
    "in this paper , we have given the currently fastest exact algorithm for the dominating set problem .",
    "besides setting the current record for this central graph theoretic problem , we also have shown that measure and conquer can be used as a tool for the design of algorithms .",
    "we have shown that there exists a strong relation between the chosen variable measure , the branching rule(s ) and the reduction rules of a measure and conquer based algorithm .",
    "we intend to further investigate this relation and examine to what point we can deduce not only reduction rules , but also branching rules from the given measure .",
    "we plan to apply the _ design by measure and conquer _ method to a number of other combinatorial problems , and hope and expect that in a number of cases , such a computer aided algorithm design will give further improvements to the best known exact algorithms for these problems .    in this paper",
    ", we observe that measure and conquer can be used as a form of _ computer aided algorithm design_. another intriguing question is whether we can automate some additional steps in the design process , e.g. , can we automatically obtain reduction rules from the solution of the quasiconvex program ?",
    "f.  v. fomin , f.  grandoni , and d.  kratsch . measure and conquer : domination  a case study . in _ proceedings of the 32nd international colloquium on automata , languages and programming , icalp 2005 _ , pages 191203 .",
    "springer verlag , lecture notes in computer science , vol .  3580 , 2005 .",
    "f.  v. fomin , f.  grandoni , and d.  kratsch .",
    "measure and conquer : a simple @xmath149 independent set algorithm . in _ proceedings of the 16th annual acm - siam symposium on discrete algorithms , soda 2006 _ , pages 1825 , 2006 .",
    "f.  v. fomin , f.  grandoni , and d.  kratsch . solving connected dominating set faster than @xmath150 . in s.  arun - kumar and n.  garg , editors , _",
    "proceedings 26th international comference on foundations of software technology and theoretical computer science , fsttcs 2006 _ , pages 152163 .",
    "springer verlag , lecture notes in computer science , vol .",
    "4337 , 2006 .",
    "f.  v. fomin , f.  grandoni , and d.  kratsch .",
    "a measure & conquer approach for the analysis of exact algorithms . technical report 359 , department of informatics , university of bergen , bergen , norway , 2007 .",
    "f.  v. fomin , f.  grandoni , a.  pyatkin , and a.  a. stepanov .",
    "bounding the number of minimal dominating sets : a measure and conquer approach . in _",
    "proceedings 16th international symposium on algorithms and computation , isaac 2005 _ , pages 192203 .",
    "springer verlag , lecture notes in computer science , vol .  3827 , 2005 .",
    "f.  v. fomin , d.  kratsch , and g.  j. woeginger .",
    "exact ( exponential ) algorithms for the dominating set problem . in j.",
    "hromkovic , m.  nagl , and b.  westfechtel , editors , _",
    "proceedings 30th international workshop on graph - theoretic concepts in computer science wg04 _ , page 245=256 .",
    "springer verlag , lecture notes in computer science , vol .  3353 , 2004 .",
    "s.  gaspers and m.  liedloff . a branch - and - reduce algorithm for finding a minimum independent dominating set in graphs . in f.",
    "v. fomin , editor , _",
    "proceedings 32nd international workshop on graph - theoretic concepts in computer science wg06 _ , pages 7889 .",
    "springer verlag , lecture notes in computer science , vol .",
    "4271 , 2006 .",
    "d.  kratsch and m.  liedloff .",
    "an exact algorithm for the minimum dominating clique problem . in h.",
    "l. bodlaender and m.  a. langston , editors , _",
    "proceedings 2nd international workshop on parameterized and exact computation , iwpec 2006 _ , pages 128139 .",
    "springer verlag , lecture notes in computer science , vol .",
    "4169 , 2006 .            j.  m.  m. van rooij .",
    "design by measure and conquer : an @xmath151 algorithm for minimum dominating set and similar problems .",
    "master s thesis , institute for information and computing sciences , utrecht university , 2006 .",
    "g.  j. woeginger .",
    "exact algorithms for np - hard problems : a survey . in _",
    "combinatorial optimization :  eureka , you shrink  _ , pages 185207 , berlin , 2003 .",
    "springer lecture notes in computer science , vol .",
    "2570 .    g.  j. woeginger .",
    "space and time complexity of exact algorithms : some open problems ( invited talk ) . in r.",
    "g. downey and m.  r. fellows , editors , _",
    "proceedings 1st international workshop on parameterized and exact computation , iwpec 2004 _ , pages 281290 , berlin , 2004 .",
    "springer lecture notes in computer science , vol .  3162 ."
  ],
  "abstract_text": [
    "<S> the _ measure and conquer _ approach has proven to be a powerful tool to _ analyse _ exact algorithms for combinatorial problems , like dominating set and independent set . in this paper </S>",
    "<S> , we propose to use measure and conquer also as a tool in the _ design _ of algorithms . in an iterative process </S>",
    "<S> , we can obtain a series of _ branch and reduce _ algorithms . a mathematical analysis of an algorithm in the series with measure and conquer results in a quasiconvex programming problem . </S>",
    "<S> the solution by computer to this problem not only gives a bound on the running time , but also can give a new reduction rule , thus giving a new , possibly faster algorithm . </S>",
    "<S> this makes _ </S>",
    "<S> design by measure and conquer _ a form of _ computer aided algorithm design_.    when we apply the methodology to a set cover modelling of the dominating set problem , we obtain the currently fastest known exact algorithms for dominating set : an algorithm that uses @xmath0 time and polynomial space , and an algorithm that uses @xmath1 time .    </S>",
    "<S> johan m. m. van rooij    hans l. bodlaender </S>"
  ]
}