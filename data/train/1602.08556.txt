{
  "article_text": [
    "the advancements in semiconductor technology have led to renewed interest in efficient implementations of complex neuromorphic systems . artificial neural networks ( anns ) that consist of fully connected layers of neurons , as illustrated in fig . 1 ,",
    "have been widely used in classification and recognition applications .",
    "it is primarily due to their inherent ability to learn sophisticated nonlinear mapping between large unstructured input and output data [ 1 , 2 ] .",
    "deep architectures , essentially inspired by the hierarchical organization of the human brain , are the cornerstone of contemporary neuromorphic systems . the superior performance of such multilayered neural networks can be attributed to hierarchical feature extraction , wherein the low level features uncovered by the initial layers are subsequently used to discern abstract patterns [ 3 ] .",
    "this has resulted in their widespread utility in a diverse suite of applications , including visual object classification [ 4 ] , speech recognition [ 5 ] , and dimensionality reduction [ 6 ] .        in our work ,",
    "we focus on multilayered anns , which possess intrinsic error resiliency [ 7 ] .",
    "this can be attributed to the inherent redundancy in feature representation , which stems from the observation that information is stored in a distributed manner among a group of neurons .",
    "hence information is not completely lost due to errors introduced in a few of the neurons , or modest perturbations in the stored synaptic weights .",
    "significant energy benefits have been demonstrated in previous literature , using approximate neural processing elements , and scaling the precision of the inputs and synaptic weights while paying a negligible performance penalty [ 8 ] .",
    "we note that the number of synapses is orders of magnitude larger than the number of neurons .",
    "hence , the on - chip synaptic memory contributes substantially to the power consumption of a typical digital cmos implementation of anns .",
    "we propose scaling the supply voltage of the system in order to achieve energy efficiency .",
    "the digital logic comprising the neural processing elements and the associated controllers could be operated reliably at scaled voltages by clocking them at a lower frequency .",
    "however , the 6 t sram that has been the workhorse of on - chip memories is prone to bitcell failures at scaled voltages .",
    "the process parameter variations could have a detrimental impact on the relative strength of the transistors , resulting in an imbalance in the bitcell that was engineered to have symmetric operation [ 9 ] .",
    "supply voltage scaling exacerbates the impact of parameter variations , which could potentially cause a bitcell to experience read access , read disturb , or write failures [ 10 ] .",
    "we analyzed the impact of voltage scaling on the performance of anns on mnist [ 11 ] , which is a widely used handwritten digit recognition dataset .",
    "our analysis indicates that the inherent error resiliency of anns enables the supply voltage to be moderately scaled for a negligible degradation in the classification accuracy",
    ". however , aggressive scaling causes a substantial deterioration in the accuracy due to increased probability of failures in the msbs of the synaptic weights .",
    "we , therefore propose a significance driven hybrid 8t-6 t sram , wherein the sensitive msbs of the synaptic weights are stored in 8 t bitcells while the relatively resilient lsbs are stored in 6 t bitcells .",
    "the 8 t bitcells provide stable operation under scaled supply voltage conditions , primarily due to the presence of independently optimized read and write paths .",
    "this enables aggressive voltage scaling of the hybrid array for a minimal degradation in the classification accuracy , albeit with an area penalty .",
    "the area overhead can be further minimized by judiciously selecting the number of sensitive msbs for the synaptic weights interconnecting different layers of an ann .",
    "we propose a synaptic - sensitivity driven hybrid memory architecture consisting of multiple 8t-6 t sram banks , each of which stores synapses carrying a definite significance .",
    "this is motivated by the intuition that the synapses fanning out of a layer of significant neurons need to be protected from large perturbations , over those fanning out of resilient neurons .",
    "it provides improvements in memory access power at reduced area costs .",
    "the key contributions of our work are :    1 .",
    "we show that the supply voltage of a typical digital cmos implementation of anns can be scaled to achieve energy efficiency ; despite the susceptibility of a 6 t sram based synaptic storage to bitcell failures .",
    "2 .   in an effort to scale the voltage",
    "even further , we propose a significance driven hybrid 8t-6 t sram that stores the sensitive msbs in robust 8 t bitcells .",
    "3 .   we further present a synaptic - sensitivity driven hybrid memory architecture , wherein the number of msbs of the synaptic weights that need to be stored in 8 t bitcells are chosen based on their sensitivity so as to gain power benefits with minimal area overheads .",
    "the rest of the paper is organized as follows .",
    "section ii provides a brief introduction on anns .",
    "section iii describes the proposed memory architecture and the significance driven synaptic memory design .",
    "section iv analyzes the bitcell failures and power consumption of both 6 t and 8 t sram arrays at scaled supply voltages .",
    "section v explains the simulation methodology .",
    "section vi presents the results while section vii concludes the paper .",
    "anns are composed of layers of artificial neurons interconnected by synapses , and approximately mimic certain types of computations performed by the human brain .",
    "the neurons are arranged in different layers _ viz .",
    "_ input , one or more hidden , and output layers .",
    "every layer in the ann saving the output layer is fully connected to the layer immediately following it , and such a network with acyclic synaptic connectivity from the input to the output layer is known as a feedforward ann .",
    "every neuron in the ann with the exception of input neurons sums the product of the incoming inputs and connecting weights .",
    "subsequently , a nonlinear function , for instance , the sigmoid function is applied to the summed result in order to obtain the activation output .",
    "an ann is trained in a _ supervised _ manner using the backpropagation algorithm [ 12 ] on a designated training dataset .",
    "the key idea is to iteratively update the synaptic weights in a manner that minimizes their contribution to the output error . a trained neural network stores the learned features in a distributed manner .",
    "it is this property that enables an ann to tolerate errors in neuronal computations and synaptic weight perturbations .",
    "we exploit the error resiliency to architect a power - efficient on - chip synaptic memory as will be described in the following section .",
    "this section describes the proposed synaptic memory designs for a typical neuromorphic system shown in fig .",
    "2 . it consists of neural processing elements ( npes ) that mimic the core computations of the artificial neurons , and a conventional 6 t sram for on - chip synaptic storage .",
    "it additionally requires a controller to coordinate the sequence of operations between the npes and the synaptic memory .",
    "digital cmos implementations of anns are innately power hungry due to the heavy computational demands placed on the npes , and the synaptic memory access and leakage power consumption .",
    "supply voltage scaling can be used to achieve significant energy savings .",
    "the npes and the associated control logic can be reliably operated at scaled voltages by lowering the clock frequency . however , a 6 t sram is susceptible to bitcell failures under scaled voltage conditions .",
    "the failures are aggravated by process parameter variations in scaled technology nodes .",
    "on the other hand , anns being error resilient applications can tolerate moderate perturbations in their constituent synaptic weights .",
    "nevertheless , the classification accuracy might deteriorate substantially if the msbs of a large fraction of the synapses are corrupted .",
    "hence , the stability of bitcells that store the msbs is of paramount importance for aggressive voltage scaling .",
    "this intuition is central to the significance and sensitivity based memory designs that will be described subsequently .      in a significance driven hybrid array , few msbs of all the synaptic weights",
    "are stored in 8 t bitcells as illustrated in fig .",
    "an 8 t bitcell has read and write paths that can be independently optimized for the respective operations as opposed to a shared path in 6 t bitcells .",
    "this enables the voltage of the hybrid array to be scaled aggressively .",
    "this leads to improvements in memory access and leakage power consumption , however at the cost of an increase in area .",
    "we use additional application - level insights , and present an improved memory architecture to minimize the area penalty , as described in the following sub - section .      the basic significance driven architecture protected an equal number of msbs in all the synaptic weights .",
    "we note that additional power benefits can be obtained with minimal area overheads by reducing the number of 8 t bitcells .",
    "3(c ) shows a block diagram of the proposed synaptic - sensitivity driven hybrid memory architecture .",
    "it consists of multiple 8t-6 t sram banks , each of which stores the synapses fanning out of neurons in the corresponding layer of an ann .",
    "the number of msbs of the synaptic weights that needs to be stored in 8 t bitcells is chosen based on their sensitivity .    in a deep ann ,",
    "the first hidden layer extracts the low level features from the input dataset .",
    "furthermore , a reasonable fraction of the synapses are concentrated in the input and the initial hidden layers , since the number of neurons per layer decreases progressively from the input to the output layer [ 14 ] . in general ,",
    "large synaptic weight perturbations in the input and the first hidden layer could have a detrimental effect on the classification accuracy , and are hence deemed significant .",
    "it has been shown that the fraction of resilient neurons decreases while moving towards the output layer [ 8 ] . intuitively , injecting substantial errors in the synapses fanning into the output layer",
    "would directly impact the classifier performance .",
    "hence the output layer is more sensitive than the central hidden layers .",
    "the resilient synapses have relatively fewer msbs stored in 8 t bitcells in comparison to the ones that were deemed significant .",
    "the proposed memory architecture thus exploits the varying significance of the synapses connecting different layers in order to provide power savings at reduced area costs .",
    "a rigorous analysis of 8 t and 6 t bitcells is required at the circuit level in order to quantify the power benefits and area overhead associated with each of the configurations described in this section and shown in fig .",
    "3 . the following section presents a comprehensive failure analysis of both the 8 t and 6 t bitcell topologies .",
    "failures in srams arise due to random variations in the process parameters [ 1517 ] . among the different sources of random intra - die variations ,",
    "the most prominent one is the threshold voltage variation that is caused by random dopant fluctuations [ 10 ] .",
    "we have therefore considered only the failures caused due to on - die variations in the threshold voltage without loss of generality .",
    "the different types of failures in an sram are :    1 .   read access failure , which is caused by an inability to complete a successful read operation before the end of the read cycle .",
    "2 .   write failure , which is caused by an inability to successfully flip a bit within the stipulated write time .",
    "disturb failure , which is caused by unintended flipping of the stored data during a read operation .",
    "we designed a 6 t sram bitcell shown in fig .",
    "4(a ) in 22 nm technology using predictive models [ 18 ] .",
    "it is sized to have a nominal static read noise margin of 195 mv , which is a measure of the robustness of an sram bitcell against flipping . the write margin , which is a measure of the easiness to write into an sram bitcell is 250 mv .",
    "a 6 t sram has conflicting read and write sizing requirements [ 10 ] making it susceptible to failures at scaled voltages .",
    "on the other hand , an 8 t sram shown in fig .",
    "4(b ) can be optimized separately for read and write operations .",
    "the 6 t and 8 t bitcells were designed for equal read access and write times , which were determined by considering the delay incurred in charging / discharging the bitline capacitance associated with a 256x256 sram sub - array .",
    "the sram bitcells were then subjected to threshold voltage ( @xmath0 ) fluctuations .",
    "the @xmath0 fluctuations ( @xmath1 ) in the transistors in an sram bitcell are considered as independent gaussian random variables with zero mean [ 19 ] .",
    "the standard deviation of the @xmath0 fluctuations ( @xmath2 ) is a strong function of the transistor sizes [ 10 , 20 ] , and the dependency is given by    @xmath3    where @xmath4 is the standard deviation of a minimum sized transistor , @xmath5 and @xmath6 are the minimum allowed length ( @xmath7 ) and width ( @xmath8 ) of the technology node respectively .",
    "finally , monte carlo simulations were run on a 256x256 sram sub - array to estimate the read access , read disturb , and write failure rates at different operating voltages .",
    "it can be seen from fig .",
    "5 that read access failures dominate over write failures in a 6 t sram at scaled voltages .",
    "the corresponding failures for an 8 t sram are negligible in the voltage range of interest .",
    "similarly , we found that the read disturb failures were small enough to be neglected for a 6 t sram while an 8 t sram is free from disturb failures [ 21 ] .",
    "6 shows the variation of memory access and leakage power with supply voltage scaling .",
    "it can be seen that an 8 t bitcell consumes roughly 20% more read and write power , and 47% more leakage power than a 6 t bitcell under iso - voltage conditions .",
    "our layout analysis indicates that the 8 t bitcell incurs a 37% area overhead .",
    "we further note that the hybrid 8t-6 t arrays can effectively be laid out in a single row [ 13 ] , and hence does not incur any other overhead aside from the obvious area and power penalty owing to an increase in the transistor count .",
    "the 8 t and 6 t bitcell characteristics thus obtained would be used to evaluate the proposed synaptic memory designs .",
    "the simulation methodology is described in the following section .",
    "a circuit to system - level simulation framework was developed to analyze the impact of voltage scaling on the proposed synaptic memory designs . at the circuit level ,",
    "the 6 t and 8 t bitcells were designed , and subjected to spice simulations to estimate the area , power , and failure rates .",
    "the failure probabilities and the different synaptic memory configurations _ viz . _",
    "6 t sram , hybrid 8t-6 t sram are fed to an ann functional simulator .    at the system level ,",
    "the deep learning toolbox [ 22 ] , which is an open source neural network simulator , was used to train and evaluate the performance of the ann under consideration . the read access and write failures",
    "are modeled by introducing bit flips while accessing and updating the synaptic weights in the functional simulator .",
    "the distribution of bit failures depends on the synaptic memory configuration , for instance , the failures are distributed uniformly for a 6 t sram while only the lsbs are affected in a hybrid 8t-6 t sram .",
    "the failure analysis of 8 t bitcell conclusively proves that it is virtually unaffected by supply scaling within the voltage range of interest .",
    "it was additionally assumed that a 6 t bitcell can not simultaneously have read access and write failures since they necessitate conflicting requirements .",
    "the simulator computes the degradation in classification accuracy owing to synaptic weight perturbations .",
    "the bitcell characteristics determined from spice simulations together with the synaptic memory configuration are used to calculate the area , memory access and leakage power consumption .",
    "the benefits of the proposed synaptic memory designs are evaluated on a multilayered feedforward ann that is trained to classify handwritten digits .",
    "the essential parameters of the benchmark ann are shown in table i.",
    "in this section , we present the results that demonstrate the trade - offs between classification accuracy , power and area for the proposed synaptic memory designs .",
    "as noted earlier , we primarily focus on the on - chip synaptic memory storage , since the digital neurons and the required control logic can be operated reliably at scaled voltages .",
    "we use a synaptic precision of 8 bits since the observed degradation in accuracy is less than 0.5% from the nominal value , which corresponds to a precision of 32 bits .",
    "7(a ) shows the impact of voltage scaling on the classification accuracy of an ann , when a 6 t sram is used for on - chip synaptic weight storage .",
    "the results indicate that the intrinsic error resiliency of the ann allows the voltage to be scaled up to 200 mv from the nominal operating voltage ( 950 mv ) for almost no loss ( less than 0.5% ) in the classification accuracy .",
    "7(b ) illustrates the savings in memory access and leakage power as a consequence of supply scaling .",
    "our analysis further indicates that aggressive scaling results in a degradation of more than 30% in the accuracy owing to substantial errors in the msbs of the synaptic weights .",
    "8 illustrates that a hybrid 8t-6 t sram , wherein a few msbs of all the synaptic weights are stored in 8 t bitcells allows the voltage to be scaled by another 100 mv .",
    "the aggressive voltage scaling is made possible because of the robust operation of 8 t bitcells at reduced voltages .",
    "an iso - stability analysis was carried out to demonstrate the power benefits .",
    "a 6 t sram operating at 0.75 v was used as the baseline synaptic memory configuration in order to evaluate the improvement in power consumption , and the corresponding area overheads .",
    "the results show that protecting three msbs of all the synapses provides a 29% improvement in memory access and leakage power consumption , albeit at a 13.75% area penalty .",
    "the error resiliency of the ann to lsbs of the synaptic weights precludes the need for an all 8 t sram .",
    "8(a ) conclusively proves that protecting three or four msbs in 8 t bitcells is sufficient to achieve close to nominal accuracy .",
    "this memory architecture consists of five 8t-6 t sram banks , each of which store the synapses fanning out of neurons in the corresponding layer of the benchmark ann .",
    "the following intuitions were used to determine the sensitivity of synapses , which are corroborated by the results shown in fig . 9 .    1 .",
    "the synapses fanning out of the input and first hidden layer are significant in comparison to those interconnecting the central hidden layers",
    "the synapses fanning into the output layer are important , since any errors directly impact the classifier output .",
    "our analysis further indicates that the input layer is resilient relative to the first hidden layer .",
    "intuitively , this can be attributed to the fact that the input images typically contain a slew of insignificant pixels along with the features of interest .",
    "for instance , the digits in the minist dataset are concentrated in the center .",
    "thus , the pixels at the image boundaries do not contain useful information . the ability of the input layer to tolerate synaptic errors better than the first hidden layer yields power savings while further reducing the area costs .",
    "the results show that a 30.91% reduction in memory access power can be obtained with a 10.41% area overhead for less than 1% loss in the classification accuracy .",
    "additional power savings of 7.38% could be achieved at a further 40.25% reduction in the area cost , if a degradation of less than 4% in the classification accuracy could be tolerated .",
    "in this work , we explored scaling the supply voltage of large - scale neuromorphic systems to achieve energy efficiency . in scaled technologies , under supply voltage scaling , an on - chip synaptic memory designed using a standard",
    "6 t sram is susceptible to bitcell failures .",
    "we took advantage of the ability of anns to tolerate modest errors in the synaptic weights to minimize the power consumption .",
    "nevertheless , aggressive scaling resulted in substantial performance degradation due to errors in msbs of the synaptic weights . to this end , we proposed a significance driven hybrid 8t-6 t sram , wherein a few msbs of the synaptic weights are stored in robust 8 t bitcells .",
    "the hybrid array yielded substantial power savings , since it allowed the voltage to be scaled lower than that which could be achieved using a 6 t sram .",
    "we finally presented a synaptic - sensitivity driven hybrid memory architecture .",
    "we availed the varying significance of synapses connecting different layers of the ann to gain power benefits with minimal area overheads .",
    "thus , we harnessed the significance driven computing methodology and error resiliency of anns to architect an efficient on - chip synaptic storage .",
    "k. hornik et al .",
    "`` universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks '' neural networks 3 , no .",
    "551 - 560 , 1990 .",
    "g. hinton et al . `` improving neural networks by preventing co - adaptation of feature detectors . ''",
    "arxiv preprint arxiv:1207.0580 , 2012 .",
    "y. bengio `` learning deep architectures for ai . ''",
    "foundations and trends in machine learning 2 , no .",
    "1 , pp . 1 - 127 , 2009 .",
    "a. krizhevsky et al .",
    "`` imagenet classification with deep convolutional neural networks .",
    "'' advances in neural information processing systems , pp .",
    "1097 - 1105 , 2012 .",
    "g. hinton et al .",
    "`` deep neural networks for acoustic modeling in speech recognition : the shared views of four research groups . ''",
    "signal processing magazine , ieee 29 , no .",
    "82 - 97 , 2012 .",
    "g. hinton et al . `` reducing the dimensionality of data with neural networks . ''",
    "science 313 , no .",
    "5786 , pp .",
    "504 - 507 , 2006 .",
    "k. mehrotra et al .",
    "`` fault tolerance of neural networks . ''",
    "technical report rl - tr-94 - 93 , july 1994 .",
    "s. venkataramani et al .",
    "`` axnn : energy - efficient neuromorphic systems using approximate computing . '' in proc .",
    "islped , pp .",
    "27 - 32 , acm , 2014 .",
    "j. kulkarni et al .",
    "`` ultralow - voltage process - variation - tolerant schmitt - trigger - based sram design . '' very large scale integration ( vlsi ) systems , ieee transactions on 20 , no .",
    "319 - 332 , 2012 .",
    "s. mukhopadhyay et al .",
    "`` modeling of failure probability and statistical design of sram array for yield enhancement in nanoscaled cmos . ''",
    "computer - aided design of integrated circuits and systems , ieee transactions on 24 , no .",
    "12 , pp . 1859 - 1880 , 2005 .",
    "y. lecun et al .",
    "`` gradient - based learning applied to document recognition . '' in proc .",
    "ieee 86 , no .",
    "2278 - 2324 , 1998 .",
    "d. rumelhart et al .",
    "`` learning representations by back - propagating errors . ''",
    "cognitive modeling 5 , pp . 3 , 1988 .",
    "i. chang et al . `` a priority - based 6t/8 t hybrid sram architecture for aggressive voltage scaling in video applications . ''",
    "circuits and systems for video technology , ieee transactions on 21 , no .",
    "101 - 112 , 2011 .",
    "d. ciresan et al .",
    "`` deep , big , simple neural nets for handwritten digit recognition . ''",
    "neural computation 22 , no .",
    "3207 - 3220 , 2010",
    ". s. r. nassif , `` modeling and analysis of manufacturing variations . '' in proc .",
    "custom integrated circuits , pp . 223228 , 2001 . c. visweswariah , `` death , taxes and failing chips . '' in proc .",
    "dac , pp . 343347 , 2003 . s. borkar et al",
    ". `` parameter variation and impact on circuits and microarchitecture , '' in proc .",
    "dac , pp . 338342 , 2003 .",
    "a. bhavnagarwala et al .",
    "`` the impact of intrinsic device fluctuations on cmos sram cell stability . '' solid - state circuits , ieee journal of 36 , no .",
    "658665 , april 2001 . y. taur and th",
    "ning `` fundamentals of modern vlsi devices . ''",
    "cambridge univ . press , 1998 .",
    "c. leland et al .",
    "`` an 8t - sram for variability tolerance and low - voltage operation in high - performance caches .",
    "'' solid - state circuits , ieee journal of 43 , no .",
    "956 - 963 , april 2008 .",
    "r. palm `` prediction as a candidate for learning deep hierarchical models of data . ''",
    "technical university of denmark , 2012 ."
  ],
  "abstract_text": [
    "<S> multilayered artificial neural networks have found widespread utility in classification and recognition applications . the scale and complexity of such networks together with the inadequacies of general purpose computing platforms have led to a significant interest in the development of efficient hardware implementations . in this work , </S>",
    "<S> we focus on designing energy - efficient on - chip storage for the synaptic weights , motivated primarily by the observation that the number of synapses is orders of magnitude larger than the number of neurons . </S>",
    "<S> typical digital cmos implementations of such large - scale networks are power hungry . in order to minimize the power consumption , </S>",
    "<S> the digital neurons could be operated reliably at scaled voltages by reducing the clock frequency . on the contrary , </S>",
    "<S> the on - chip synaptic storage designed using a conventional 6 t sram is susceptible to bitcell failures at reduced voltages . </S>",
    "<S> however , the intrinsic error resiliency of neural networks to small synaptic weight perturbations enables us to scale the operating voltage of the 6 t sram . </S>",
    "<S> our analysis on a widely used digit recognition dataset indicates that the voltage can be scaled by 200 mv from the nominal operating voltage ( 950 mv ) for practically no loss ( less than 0.5% ) in accuracy ( 22 nm predictive technology ) . scaling beyond that causes substantial performance degradation owing to increased probability of failures in the msbs of the synaptic weights . </S>",
    "<S> we , therefore propose a significance driven hybrid 8t-6 t sram , wherein the sensitive msbs are stored in 8 t bitcells that are robust at scaled voltages due to decoupled read and write paths . in an effort to further minimize the area penalty </S>",
    "<S> , we present a synaptic - sensitivity driven hybrid memory architecture consisting of multiple 8t-6 t sram banks . </S>",
    "<S> our circuit to system - level simulation framework shows that the proposed synaptic - sensitivity driven architecture provides a 30.91% reduction in the memory access power with a 10.41% area overhead , for less than 1% loss in the classification accuracy .    </S>",
    "<S> artificial neural networks , hybrid 8t-6 t sram , significance driven computation , error resilient design . </S>"
  ]
}