{
  "article_text": [
    "consider a non - linear , non - gaussian state space model for an observed sequence @xmath0 .",
    "this model , with parameters @xmath1 , assumes that the @xmath2 are drawn from an observation density @xmath3 , where @xmath4 is an unobserved markov process with initial density @xmath5 and transition density @xmath6 . here",
    ", the @xmath7 might be either continuous or discrete",
    ". we may be interested in inferring both the realized values of the markov process @xmath8 and the model parameters @xmath1 . in a bayesian approach to this problem , this can be done by drawing a sample of values for @xmath9 and @xmath1 using a markov chain that alternately samples from the conditional posterior distributions @xmath10 and @xmath11 . in this paper",
    ", we will only consider inference for @xmath9 by sampling from @xmath10 , taking the parameters @xmath1 to be known . as a result",
    ", we will omit @xmath1 in model densities for the rest of the paper . except for linear gaussian models and models with a finite state space",
    ", this sampling problem has no exact solution and hence approximate methods such as mcmc must be used .",
    "one method for sampling state sequences in non - linear , non - gaussian state space models is the embedded hmm method ( neal , 2003 ; neal , beal and roweis , 2004 ) .",
    "an embedded hmm update proceeds as follows .",
    "first , at each time @xmath12 , a set of @xmath13 `` pool states '' in the latent space is constructed . in this",
    "set , @xmath14 of the pool states are drawn from a chosen pool state density and one is the current value of @xmath7",
    ". this step can be thought of as temporarily reducing the state space model to an hmm with a finite set of @xmath13 states , hence the name of the method .",
    "then , using efficient forward - backward computations , which take time proportional to @xmath15 , a new sequence @xmath16 is selected from the `` ensemble '' of @xmath17 sequences passing through the set of pool states , with the probability of choosing each sequence proportional to its posterior density divided by the probability of the sequence under the pool state density . at the next iteration of the sampler , a new set of pool states is constructed , so that the chain can sample all possible @xmath7 , even when the set of possible values is infinite .",
    "another method is the particle gibbs with backward sampling ( pgbs ) method .",
    "the particle gibbs ( pg ) method was first introduced in andrieu , doucet and holenstein ( 2010 ) ; whiteley suggested the backward sampling modification in the discussion following this paper .",
    "lindsten and schon ( 2012 ) implemented backward sampling and showed that it improves the efficiency of pg .",
    "starting with a current sequence @xmath9 , pgbs first uses conditional sequential monte carlo ( smc ) to construct a set of candidate sequences and then uses backward sampling to select a new sequence from the set of candidate ones . here , conditional smc works in the same way as ordinary smc when generating a set of particles , except that one of the particles at time @xmath12 is always set to the current @xmath7 , similar to what is done in the embedded hmm method , which allows the sampler to remain at @xmath7 if @xmath7 lies in a high - density region .",
    "while this method works well for problems with low - dimensional state spaces , the reliance of the smc procedure on choosing an appropriate importance density can make it challenging to make the method work in high dimensions .",
    "an important advantage of particle gibbs , however , is that each iteration takes time that is only linear in the number of particles .",
    "both the pgbs and embedded hmm methods can facilitate sampling of a latent state sequence , @xmath9 , when there are strong temporal dependencies amongst the @xmath7 . in this case , using a method that samples @xmath7 conditional on fixed values of @xmath18 and @xmath19 can be an inefficient way of producing a sample from @xmath20 , because the conditional density of @xmath7 given @xmath18 and @xmath19 can be highly concentrated relative to the marginal density of @xmath7 .",
    "in contrast , with the embedded hmm and pgbs methods it is possible to make changes to blocks of @xmath7 s at once .",
    "this allows larger changes to the state in each iteration of the sampler , making updates more efficient .",
    "however , good performance of the embedded hmm and pgbs methods relies on appropriately choosing the set of pool states or particles at each time @xmath12 .    in this paper",
    ", our focus will be on techniques for choosing pool states for the embedded hmm method .",
    "when the latent state space is one - dimensional , embedded hmms work well when choosing pool states in a variety of ways .",
    "for example , in shestopaloff and neal ( 2013 ) , we choose pool states at each time @xmath12 by constructing a `` pseudo - posterior '' for each latent variable by taking the product of a `` pseudo - prior '' and the observation density , the latter treated as a `` pseudo - likelihood '' for the latent variable . in shestopaloff and neal ( 2014 )",
    ", we choose pool states at each time @xmath12 by sampling from the marginal prior density of the latent process .",
    "ways of choosing pool states that work well in one dimension begin to exhibit problems when applied to models with higher - dimensional state spaces .",
    "this is true even for dimensions as small as three .",
    "since these schemes are global , designed to produce sets of pool states without reference to the current point , as the dimension of the latent space grows , a higher proportion of the sequences in the ensemble ends up having low posterior density . ensuring that performance does nt degrade in higher dimensions",
    "thus requires a significant increase in the number of pool states . as a result",
    ", computation time may grow so large that any advantage that comes from using embedded hmms is eliminated .",
    "one advantage of the embedded hmm method over pgbs is that the embedded hmm construction allows placing pool states locally near the current value of @xmath7 , potentially allowing the method to scale better with the dimensionality of the state space .",
    "switching to such a local scheme fixes the problem to some extent .",
    "however , local pool state schemes come with their own problems , such as making it difficult to handle models with multiple posterior modes that are well - separated  the pool states might end up being placed near only some of the modes .    in this paper , we propose an embedded hmm sampler suitable for models where the state space is high dimensional .",
    "this sampler uses a sequential approximation to the density @xmath21 or to the density @xmath22 as the pool state density .",
    "we show that by using this pool state density , together with an efficient mcmc scheme for sampling from it , we can reduce the cost per iteration of the embedded hmm sampler to be proportional to @xmath23 , as with pgbs . at the same time",
    ", we retain the ability to generate pool states locally , allowing better scaling for high - dimensional state spaces .",
    "our proposed scheme can thus be thought of as combining the best features of the pgbs and the embedded hmm methods , while overcoming the deficiencies of both .",
    "we use two sample state space models as examples . both have gaussian latent processes and poisson observations , with one model having a unimodal posterior and the second a multimodal one . for the multimodal example",
    ", we introduce a `` mirroring '' technique that allows efficient movement between the different posterior modes . for these models , we show how our proposed embedded hmm method compares to a simple metropolis sampler , a pgbs sampler , as well as a sampler that combines pgbs and simple metropolis updates .",
    "further details on ensemble methods are available in the phd thesis of shestopaloff ( 2016 ) .",
    "we review the embedded hmm method ( neal , 2003 ; neal , beal and roweis , 2004 ) here .",
    "we take the model parameters , @xmath1 , to be fixed , so we do not write them explicitly",
    ". let @xmath24 be the density from which the state at time @xmath25 is drawn , let @xmath26 be the transition density between states at times @xmath12 and @xmath27 , and let @xmath28 be the density of the observation @xmath29 given @xmath7 .",
    "suppose our current sequence is @xmath8 .",
    "the embedded hmm sampler updates @xmath9 to @xmath16 as follows .",
    "first , at each time @xmath30 , we generate a set of @xmath13 pool states , denoted by @xmath31 } , \\ldots , x_{i}^{[l]}\\}$ ] .",
    "the pool states are sampled independently across the different times @xmath12 .",
    "we choose @xmath32 uniformly at random and set @xmath33}$ ] to @xmath7 .",
    "we sample the remaining @xmath14 pool states @xmath34 } , \\ldots , x_{i}^{[l_{i}-1 ] } , x_{i}^{[l_{i}+1 ] } , \\ldots , x_{n}^{[l]}$ ] using a markov chain that leaves a pool density @xmath35 invariant , as follows .",
    "let @xmath36 be the transitions of this markov chain with @xmath37 the transitions for this markov chain reversed ( i.e.  @xmath38 ) , so that @xmath39 for all @xmath9 and @xmath16 . then , starting at @xmath40 , use reverse transitions @xmath41}|x_{i}^{[j+1]})$ ] to generate @xmath42 } , \\ldots , x_{i}^{[1]}$ ] and starting at @xmath43 use forward transitions @xmath44}|x_{i}^{[j-1]})$ ] to generate @xmath45 } , \\ldots , x_{n}^{[l]}$ ] .    at each @xmath30 , we then compute the forward probabilities @xmath46 , with @xmath9 taking values in @xmath47 . at time @xmath48",
    ", we have @xmath49 and at times @xmath50 , we have @xmath51})\\alpha_{i-1}(x_{i-1}^{[l ] } ) \\label{eq : alphaall}\\end{aligned}\\ ] ]    finally , we sample a new state sequence @xmath16 using a stochastic backwards pass . this is done by selecting @xmath52 amongst the set , @xmath53 , of pool states at time @xmath54 , with probabilities proportional to @xmath55 , and then going backwards , sampling @xmath56 from the set @xmath57 , with probabilities proportional to @xmath58 .",
    "note that only the relative values of the @xmath46 will be required , so the @xmath59 may be computed up to some constant factor .",
    "alternatively , given a set of pool states , embedded hmm updates can be done by first computing the backward probabilities .",
    "we will see later on that the backward probability formulation of the embedded hmm method allows us to introduce a variation of our proposed pool state selection scheme .",
    "setting @xmath60 for all @xmath61 , we compute for @xmath62 @xmath63})p(x_{i+1}^{[l]}|x)\\beta_{i+1}(x_{i+1}^{[l ] } ) \\label{eq : beta}\\end{aligned}\\ ] ]    a new state sequence is then sampled using a stochastic forward pass , setting @xmath64 to one of the @xmath9 in the pool @xmath65 with probabilities proportional to @xmath66 and then choosing subsequent states @xmath67 from the pools @xmath47 with probabilities proportional to @xmath68 .",
    "computing the @xmath59 or @xmath69 at each time @xmath70 takes time proportional to @xmath71 , since for each of the @xmath13 pool states it takes time proportional to @xmath13 to compute the sums in ( [ eq : alphaall ] ) or ( [ eq : beta ] ) .",
    "hence each iteration of the embedded hmm sampler takes time proportional to @xmath15 .",
    "we review the particle gibbs with backward sampling ( pgbs ) sampler here . for full details , see the articles by andrieu , doucet and holenstein ( 2010 ) and lindsten and schon ( 2012 ) .",
    "let @xmath72 be the importance density from which we sample particles at time @xmath25 , and let @xmath73 be the importance density for sampling particles at times @xmath70 .",
    "these may depend on the current value of the parameters , @xmath1 , which we suppressed in this notation .",
    "suppose we start with a current sequence @xmath9 .",
    "we set the first particle @xmath74}$ ] to the current state @xmath75 .",
    "we then sample @xmath14 particles @xmath76 } , \\ldots , x_{1}^{[l]}$ ] from @xmath77 and compute and normalize the weights of the particles : @xmath78 } & = & \\frac{p(x_{1}^{[l]})p(y_{1}|x_{1}^{[l]})}{q_{1}(x_{1}^{[l]}|y_{1 } ) } \\\\ w_{1}^{[l ] } & = & \\frac{w_{1}^{[l]}}{\\sum_{m=1}^{l}w_{1}^{[m]}}\\end{aligned}\\ ] ] for @xmath79 .    for @xmath70",
    ", we proceed sequentially .",
    "we first set @xmath34 } = x_{i}$ ] .",
    "we then sample a set of @xmath14 ancestor indices for particles at time @xmath12 , defined by @xmath80 } \\in \\{1 , \\ldots , l\\}$ ] , for @xmath81 , with probabilities proportional to @xmath82}$ ] .",
    "the ancestor index for the first state , @xmath83}$ ] , is @xmath25 .",
    "we then sample each of the @xmath14 particles , @xmath84}$ ] , at time @xmath12 , for @xmath81 , from @xmath85}]})$ ] and compute and normalize the weights at time @xmath12 @xmath86 } & = & \\frac{p(x_{i}^{[l]}|x_{i-1}^{[a_{i-1}^{[l]}]})p(y_{i}|x_{i}^{[l]})}{q_{i}(x_{i}^{[l]}|y_{i},x_{i-1}^{[a_{i-1}^{[l ] } ] } ) } \\\\ w_{i}^{[l ] } & = & \\frac{w_{i}^{[l]}}{\\sum_{m=1}^{l}w_{i}^{[m]}}\\end{aligned}\\ ] ] a new sequence taking values in the set of particles at each time is then selected using a backwards sampling pass .",
    "this is done by first selecting @xmath52 from the set of particles at time @xmath54 with probabilities @xmath87}$ ] and then selecting the rest of the sequence going backward in time to time @xmath25 , setting @xmath67 to @xmath84}$ ] with probability @xmath88}p(x_{i+1}'|x_{i}^{[l]})}{\\sum_{m=1}^{l}w_{i}^{[m]}p(x_{i+1}'|x_{i}^{[m]})}\\end{aligned}\\ ] ] a common choice for @xmath89 is the model s transition density , which is what is compared to in this paper .",
    "note that each iteration of the pgbs sampler takes time proportional to @xmath90 , since it takes time proportional to @xmath13 to create the set of particles at each time @xmath12 , and to do one step of backward sampling .",
    "we propose two new ways , denoted @xmath91 and @xmath92 , of generating pool states for the embedded hmm sampler . unlike previously - used pool state selection schemes , where pool states are selected independently at each time , our new schemes select pool states sequentially , with pool states at time @xmath12 selected conditional on pool states at time @xmath27 , or alternatively at time @xmath93 .",
    "the first way to generate pool states is to use a forward pool state selection scheme , with a sequential approximation to @xmath94 as the pool state density . in particular , at time @xmath25 , we set the pool state distribution of our proposed embedded hmm sampler to @xmath95 as a result of equation ( [ eq : alphainit ] ) , @xmath96 is constant . at time @xmath70",
    ", we set the pool state distribution to @xmath97})\\end{aligned}\\ ] ] which makes @xmath46 constant for @xmath70 as well ( see equation ( [ eq : alphaall ] ) ) .",
    "we then draw a sequence composed of these pool states with the forward probability implementation of the embedded hmm method , with the @xmath46 s all set to @xmath25 .",
    "the second way is to instead use a backward pool state selection scheme , with a sequential approximation of @xmath22 as the pool state density .",
    "we begin by creating the pool @xmath53 , consisting of the current state @xmath98 and the remaining @xmath14 pool states sampled from @xmath99 , the marginal density at time @xmath54 , which is the same as @xmath24 if the latent process is stationary .",
    "the backward probabilities @xmath100 , for @xmath9 in @xmath53 , are then set to @xmath25 . at time",
    "@xmath62 we set the pool state densities to @xmath101})p(x_{i+1}^{[\\ell]}|x)\\end{aligned}\\ ] ] so that @xmath102 is constant for all @xmath30 ( see equation [ eq : beta ] ) .",
    "we then draw a sequence composed of these pool states as in the backward probability implementation of the embedded hmm method , with the @xmath102 s all set to @xmath25 .    if the latent process is gaussian , and the latent state at time @xmath25 is sampled from the stationary distribution of the latent process , it is possible to update the latent variables by applying the forward scheme to the reversed sequence @xmath103 by making use of time reversibility , since @xmath104 is also sampled from the stationary distribution , and the latent process evolves backward in time according to the same transition density as it would going forward .",
    "we then use the forward pool state selection scheme along with a stochastic backward pass to sample a sequence @xmath105 , starting with @xmath75 and going to @xmath98 .",
    "it can sometimes be advantageous to alternate between using forward and backward ( or , alternatively , forward applied to the reversed sequence ) embedded hmm updates , since this can improve sampling of certain @xmath7 .",
    "the sequential pool state selection schemes use only part of the observed sequence in generating the pool states . by alternating update directions ,",
    "the pool states can depend on different parts of the observed data , potentially allowing us to better cover the region where @xmath7 has high posterior density .",
    "for example , at time @xmath25 , the pool state density may disperse the pool states too widely , leading to poor sampling for @xmath75 , but sampling @xmath75 using a backwards scheme can be much better , since we are now using all of the data in the sequence when sampling pool states at time @xmath25 .      to sample from @xmath106 or @xmath107 , we can use any markov transitions @xmath108 that leave this distribution invariant .",
    "the validity of the method does not depend on the markov transitions for sampling from @xmath106 or @xmath107 reaching equilibrium or even on them being ergodic .    directly using these pool state densities in an mcmc",
    "routine leads to a computational cost per iteration that is proportional to @xmath15 , like in the original embedded hmm method , since at times @xmath70 we need at least @xmath13 updates to produce @xmath13 pool states , and the cost of computing an acceptance probability is proportional to @xmath13 .",
    "however , it is possible to reduce the cost per iteration of the embedded hmm method to be proportional to @xmath23 when we use @xmath106 or @xmath107 as the pool state densities . to do this ,",
    "we start by thinking of the pool state densities at each time @xmath70 as marginal densities summing over the variable @xmath109 that indexes a pool state at the previous time .",
    "specifically , @xmath106 can be viewed as a marginal of the density @xmath110})\\end{aligned}\\ ] ] while @xmath107 is a marginal of the density @xmath111})p(x_{i+1}^{[\\ell]}|x)\\end{aligned}\\ ] ] both of these densities are defined given a pool @xmath112 at time @xmath27 or pool @xmath113 at time @xmath93 .",
    "this technique is reminiscent of the auxiliary particle filter of pitt and shephard ( 1999 ) .",
    "we then use markov transitions , @xmath108 , to sample a set of values of @xmath9 and @xmath114 , with probabilities proportional to @xmath115 for the forward scheme , or probabilities proportional to @xmath116 for the backward scheme .",
    "the chain is started at @xmath9 set to the current @xmath7 , and the initial value of @xmath114 is chosen randomly with probabilities proportional to @xmath117})$ ] for the forward scheme or @xmath118})p(x_{i+1}^{[\\ell]}|x_{i})$ ] for the backward scheme . this stochastic initialization of @xmath114 is needed to make the algorithm valid when we use @xmath115 or @xmath116 to generate the pool states .",
    "sampling values of @xmath9 and @xmath114 from @xmath115 or @xmath116 can be done by updating each of @xmath9 and @xmath114 separately , alternately sampling values of @xmath9 conditional on @xmath114 , and values of @xmath114 conditional on @xmath9 , or by updating @xmath9 and @xmath114 jointly , or by a combination of these .",
    "updating @xmath9 given @xmath114 can be done with any appropriate sampler , such as metropolis , or for a gaussian latent process we can use autoregressive updates , which we describe below . to update @xmath114 given @xmath9 , we can also use metropolis updates , proposing @xmath119 , with @xmath120 drawn from some proposal distribution on @xmath121 .",
    "alternatively , we can simply propose @xmath122 uniformly at random from @xmath123 .    to jointly update @xmath9 and @xmath114 ,",
    "we propose two novel updates , a `` shift '' update and a `` flip '' update .",
    "since these are also metropolis updates , using them together with metropolis or autoregressive updates , for each of @xmath9 and @xmath114 separately , allows embedded hmm updates to be performed in time proportional to @xmath23 .      for sampling pool states in our embedded hmm mcmc schemes , as well as for comparison mcmc schemes",
    ", we will make use of neal s ( 1998 ) `` autoregressive '' metropolis - hastings update , which we review here .",
    "this update is designed to draw samples from a distribution of the form @xmath124 where @xmath125 is multivariate gaussian with mean @xmath126 and covariance @xmath127 and @xmath128 is typically a density for some observed data .",
    "this autoregressive update proceeds as follows .",
    "let @xmath13 be the lower triangular cholesky decomposition of @xmath127 , so @xmath129 , and @xmath54 be a vector of i.i.d .",
    "normal random variables with zero mean and identity covariance .",
    "let @xmath130 $ ] be a tuning parameter that determines the scale of the proposal .",
    "starting at @xmath131 , we propose @xmath132 because these autoregressive proposals are reversible with respect to @xmath125 , the proposal density and @xmath125 cancel in the metropolis - hastings acceptance ratio .",
    "this update is therefore accepted with probability @xmath133 note that for this update , the same value of @xmath134 is used for scaling along every dimension .",
    "it would be of independent interest to develop a version of this update where @xmath134 can be different for each dimension of @xmath131 .",
    "we can simultaneously update @xmath114 and @xmath9 at time @xmath70 by proposing to update @xmath135 to @xmath136 where @xmath122 is proposed in any valid way while @xmath16 is chosen in a way such that @xmath16 and @xmath137}$ ] are linked in the same way as @xmath9 and @xmath138}$ ] .",
    "the shift update makes it easier to generate a set of pool states at time @xmath12 with different predecessor states at time @xmath27 , helping to ensure that the pool states are well - dispersed .",
    "this update is accepted with the usual metropolis probability .    for a concrete example we use",
    "later , suppose that the latent process is an autoregressive gaussian process of order @xmath25 , with the model being that @xmath139 . in this case , given @xmath122 , we propose @xmath140 } - x_{i-1}^{[\\ell]})$ ] .",
    "this update is accepted with probability @xmath141 as a result of the transition densities in the acceptance ratio cancelling out , since @xmath142 } & = & x_{i } + \\phi ( x_{i-1}^{[\\ell ' ] } - x_{i-1}^{[\\ell ] } ) - \\phi x_{i-1}^{[\\ell ' ] } \\\\ & = & x_{i } - \\phi x_{i-1}^{[\\ell]}\\end{aligned}\\ ] ]    to be useful , shift updates normally need to be combined with other updates for generating pool states .",
    "when combining shift updates with other updates , tuning of acceptance rates for both updates needs to be done carefully in order to ensure that the shift updates actually improve sampling performance .",
    "in particular , if the pool states at time @xmath27 are spread out too widely , then the shift updates may have a low acceptance rate and not be very useful .",
    "therefore , jointly optimizing proposals for @xmath9 and for @xmath9 and @xmath114 may lead to a relatively high acceptance rate on updates of @xmath9 , in order to ensure that the acceptance rate for the shift updates is nt low .",
    "generating pool states locally can be helpful when applying embedded hmms to models with high - dimensional state spaces but it also makes sampling difficult if the posterior is multimodal .",
    "consider the case when the observation probability depends on @xmath143 instead of @xmath7 , so that many modes with different signs for some @xmath7 exist .",
    "we propose to handle this problem by adding an additional flip update that creates a `` mirror '' set of pool states , in which @xmath144 will be in the pool if @xmath7 is . by having a mirror set of pool states , we are able to flip large segments of the sequence in a single update , allowing efficient exploration of different posterior modes .    to generate a mirror set of pool states , we must correctly use the flip updates when sampling the pool states .",
    "since we want each pool state to have a negative counterpart , we choose the number of pool states @xmath13 to be even .",
    "the chain used to sample pool states then alternates two types of updates , a usual update to generate a pool state and a flip update to generate its negated version .",
    "the usual update can be a combination of any updates , such as those we consider above . so that each state will have a flipped version , we start with a flip transition between @xmath145}$ ] and @xmath146}$ ] , a usual transition between @xmath146}$ ] and @xmath147}$ ] , and so on up to a flip transition between @xmath148}$ ] to @xmath149}$ ] .    at time @xmath25 , we start with the current state @xmath75 and randomly assign it to some index @xmath150 in the chain used to generate pool states .",
    "then , starting at @xmath75 we generate pool states by reversing the markov chain transitions back to @xmath25 and going forward up to @xmath13 .",
    "each flip update is then a metropolis update proposing to generate a pool state @xmath151 given that the chain is at some pool state @xmath75 .",
    "note that if the observation probability depends on @xmath75 only through @xmath152 and @xmath24 is symmetric around zero then this update is always accepted .    at time @xmath70 , a flip update proposes to update a pool state @xmath135 to @xmath153 such that @xmath137 } = -x_{i-1}^{[\\ell]}$ ] . here ,",
    "since the pool states at each time are generated by alternating flip and usual updates , starting with a flip update to @xmath34}$ ] , the proposal to move from @xmath114 to @xmath122 can be viewed as follows .",
    "suppose that instead of labelling our pool states from @xmath25 to @xmath13 we instead label them @xmath154 to @xmath14 .",
    "the pool states at times @xmath154 and @xmath25 , then @xmath155 and @xmath156 , and so on will then be flipped pairs , and the proposal to change @xmath114 to @xmath122 can be seen as proposing to flip the lower order bit in a binary representation of @xmath122 .",
    "for example , a proposal to move from @xmath157 to @xmath158 can be seen as proposing to change @xmath114 from @xmath159 to @xmath160 ( in binary ) .",
    "such a proposal will always be accepted assuming a transition density for which @xmath161 and an observation probability which depends on @xmath7 only via @xmath143 .",
    "the forward pool state selection scheme can be used to construct a sampler with properties similar to pgbs .",
    "this is done by using independence metropolis to sample values of @xmath9 and @xmath114 from @xmath115 .    at time @xmath25",
    ", we propose our pool states from @xmath24 . at times",
    "@xmath162 , we propose @xmath122 by selecting it uniformly at random from @xmath123 and we propose @xmath16 by sampling from @xmath163})$ ] .",
    "the proposals at all times @xmath12 are accepted with probability @xmath141 this sampler has computational cost proportional to @xmath90 per iteration , like pgbs .",
    "it is analogous to a pgbs sampler with importance densities @xmath164 and @xmath165 with the key difference between these two samplers being that pgbs uses importance weights @xmath28 on each particle , instead of an independence metropolis accept - reject step .",
    "we modify the original proof of neal ( 2003 ) , which assumes that the sets of pool states @xmath166 are selected independently at each time , to show the validity of our new sequential pool state selection scheme .",
    "another change in the proof is to account for generating the pool states by sampling them from @xmath115 or @xmath116 instead of @xmath106 or @xmath107 .",
    "this proof shows that the probability of starting at @xmath9 and moving to @xmath16 with given sets of pool states @xmath47 ( consisting of values of @xmath9 at each time @xmath12 ) , pool indices @xmath167 of @xmath7 , and pool indices @xmath168 of @xmath67 is the same as the probability of starting at @xmath16 and moving to @xmath9 with the same set of pool states @xmath47 , pool indices @xmath168 of @xmath67 , and pool indices @xmath167 of @xmath7 .",
    "this in turn implies , by summing / integrating over @xmath47 and @xmath167 , that the embedded hmm method with the sequential pool state scheme satisfies detailed balance with respect to @xmath169 , and hence leaves @xmath169 invariant .",
    "suppose we use the sequential forward scheme .",
    "the probability of starting at @xmath9 and moving to @xmath16 decomposes into the product of the probability of starting at @xmath9 , which is @xmath169 , the probability of choosing a set of pool state indices @xmath167 , which is @xmath170 , the probability of selecting the initial values of @xmath171 for the stochastic initialization step , the probability of selecting the sets of pool states @xmath47 , @xmath172 , and finally the probability of choosing @xmath16 .",
    "the probability of selecting given initial values for the links to previous states @xmath173 is @xmath174})}{\\sum_{m=1}^{l}p(x_{i}|x_{i-1}^{[m]})}\\end{aligned}\\ ] ] the probability of choosing a given set of pool states is @xmath175 at time @xmath25 , we use a markov chain with invariant density @xmath176 to select pool states in @xmath65 . therefore",
    "@xmath177}|x_{1}^{[j-1]})\\prod_{j = l_{1}-1}^{1}\\tilde{r}_{1}(x_{1}^{[j]}|x_{1}^{[j+1 ] } ) \\notag \\\\ & = & \\prod_{j = l_{1}+1}^{l}r_{1}(x_{1}^{[j]}|x_{1}^{[j-1]})\\prod_{j = l_{1}-1}^{1}r_{1}(x_{1}^{[j+1]}|x_{1}^{[j]})\\frac{\\kappa_{1}(x_{1}^{[j]})}{\\kappa_{1}(x_{1}^{[j+1 ] } ) } \\notag \\\\ & = & \\prod_{j = l_{1}}^{l-1}r_{1}(x_{1}^{[j+1]}|x_{1}^{[j]})\\prod_{j = l_{1}-1}^{1}r_{1}(x_{1}^{[j+1]}|x_{1}^{[j]})\\frac{\\kappa_{1}(x_{1}^{[j]})}{\\kappa_{1}(x_{1}^{[j+1 ] } ) } \\notag \\\\ & = & \\frac{\\kappa_{1}(x_{1}^{[1]})}{\\kappa_{1}(x_{1}^{[l_{1}]})}\\prod_{j=1}^{l-1}r_{1}(x_{1}^{[j+1]}|x_{1}^{[j]})\\end{aligned}\\ ] ] for times @xmath70 we use a markov chain with invariant density @xmath115 to sample a set of pool states , given @xmath112 .",
    "the chain is started at @xmath33 } = x_{i}$ ] and @xmath178 } = \\ell_{i}$ ] .",
    "therefore @xmath179 } , \\ell_{i}^{[j]}|x_{i}^{[j-1 ] } , \\ell_{i}^{[j-1]})\\prod_{j = l_{i}-1}^{1}\\tilde{r}_{i}(x_{i}^{[j ] } , \\ell_{i}^{[j]}|x_{i}^{[j+1 ] } , \\ell_{i}^{[j+1 ] } ) \\notag \\\\ & = & \\prod_{j = l_{i}+1}^{l}r_{i}(x_{i}^{[j ] } , \\ell_{i}^{[j]}|x_{i}^{[j-1 ] } , \\ell_{i}^{[j-1]})\\prod_{j = l_{i}-1}^{1}r_{i}(x_{1}^{[j+1 ] } , \\ell_{i}^{[j+1]}|x_{i}^{[j ] } , \\ell_{i}^{[j]})\\frac{\\lambda_{i}(x_{i}^{[j ] } , \\ell_{i}^{[j]})}{\\lambda_{i}(x_{1}^{[j+1 ] } , \\ell_{i}^{[j+1 ] } ) } \\notag \\\\ & = & \\prod_{j = l_{i}}^{l-1}r_{i}(x_{i}^{[j+1 ] } , \\ell_{i}^{[j+1]}|x_{i}^{[j ] } , \\ell_{i}^{[j]})\\prod_{j = l_{i}-1}^{1}r_{i}(x_{i}^{[j+1 ] } , \\ell_{i}^{[j+1]}|x_{i}^{[j ] } , \\ell_{i}^{[j]})\\frac{\\lambda_{i}(x_{i}^{[j ] } , \\ell_{i}^{[j]})}{\\lambda_{i}(x_{i}^{[j+1 ] } , \\ell_{i}^{[j+1 ] } ) } \\notag \\\\ & = & \\frac{\\lambda_{i}(x_{i}^{[1 ] } , \\ell_{i}^{[1]})}{\\lambda_{i}(x_{i}^{[l_{i } ] } , \\ell_{i}^{[l_{i}]})}\\prod_{j=1}^{l-1}r_{i}(x_{i}^{[j+1 ] } , \\ell_{i}^{[j+1]}|x_{i}^{[j ] } , \\ell_{i}^{[j]})\\end{aligned}\\ ] ]    finally , we choose a new sequence @xmath16 amongst the collection of sequences consisting of the pool states with a backward pass .",
    "this is done by first choosing a pool state @xmath52 uniformly at random from @xmath53 .",
    "we then select the remaining states @xmath180}$ ] by selecting @xmath181 with probability @xmath182})}{\\sum_{m=1}^{l}p(x_{i}'|x_{i-1}^{[m]})}\\end{aligned}\\ ] ]    thus , the probability of starting at @xmath9 and going to @xmath16 , with given @xmath166 , @xmath183 and @xmath184 is @xmath185})}{\\sum_{m=1}^{l}p(x_{i}|x_{i-1}^{[m ] } ) } \\times \\frac{\\kappa_{1}(x_{1}^{[1]})}{\\kappa_{1}(x_{1}^{[l_{1}]})}\\prod_{j=1}^{l-1}r_{1}(x_{1}^{[j+1]}|x_{1}^{[j ] } )   \\label{eq : proba } \\\\ & & \\ \\times \\ \\prod_{i=2}^{n}\\biggl[\\frac{\\lambda_{i}(x_{i}^{[1 ] } , \\ell_{i}^{[1]})}{\\lambda_{i}(x_{i}^{[l_{i } ] } , \\ell_{i}^{[l_{i}]})}\\prod_{j=1}^{l-1}r_{i}(x_{i}^{[j+1 ] } , \\ell_{i}^{[j+1]}|x_{i}^{[j ] } , \\ell_{i}^{[j]})\\biggr]\\times \\frac{1}{l } \\times \\prod_{i=2}^{n}\\frac{p(x_{i}'|x_{i-1}^{[l_{i-1}']})}{\\sum_{m=1}^{l}p(x_{i}'|x_{i-1}^{[m ] } ) } \\notag \\\\ & & = \\",
    "\\kappa_{1}(x_{1}^{[1]})\\prod_{j=1}^{l-1}r_{1}(x_{1}^{[j+1]}|x_{1}^{[j ] } ) \\times \\ \\prod_{i=2}^{n}\\biggl[\\lambda_{i}(x_{i}^{[1 ] } , \\ell_{i}^{[1]})\\prod_{j=1}^{l-1}r_{i}(x_{i}^{[j+1 ] } , \\ell_{i}^{[j+1]}|x_{i}^{[j ] } , \\ell_{i}^{[j]})\\biggr]\\notag \\\\ & & \\ \\times \\ \\frac{1}{l^{n+1 } } \\times \\frac{p(x|y)}{\\kappa_{1}(x_{1})\\prod_{i=2}^{n}\\lambda_{i}(x_{i } , \\ell_{i } ) } \\times \\prod_{i=2}^{n}\\frac{p(x_{i}|x_{i-1}^{[\\ell_{i}]})}{\\sum_{m=1}^{l}p(x_{i}|x_{i-1}^{[m]})}\\prod_{i=2}^{n}\\frac{p(x_{i}'|x_{i-1}^{[l_{i-1}']})}{\\sum_{m=1}^{l}p(x_{i}'|x_{i-1}^{[m ] } ) } \\notag\\end{aligned}\\ ] ] here , we have @xmath186 } = x_{i-1}'$ ] .",
    "also @xmath187 and @xmath188})}{\\sum_{x_{i } \\in \\mathcal{p}_{i } } \\sum_{m=1}^{l}p(y_{i}|x_{i})p(x_{i}|x_{i-1}^{[m]})}\\end{aligned}\\ ] ] and @xmath189 therefore ( [ eq : proba ] ) can be simplified to @xmath190})\\prod_{j=1}^{l-1}r_{1}(x_{1}^{[j+1]}|x_{1}^{[j ] } ) \\times \\ \\prod_{i=2}^{n}\\biggl[\\lambda_{i}(x_{i}^{[1 ] } , \\ell_{i}^{[1]})\\prod_{j=1}^{l-1}r_{i}(x_{i}^{[j+1 ] } , \\ell_{i}^{[j+1]}|x_{i}^{[j ] } , \\ell_{i}^{[j]})\\biggr ] \\times \\frac{1}{l^{n+1 } } \\notag \\\\ & & \\times \\ \\prod_{i=2}^{n}p(x_{i}|x_{i-1 } ) \\times \\prod_{i=2}^{n}p(x_{i}'|x_{i-1 } ' ) \\times \\prod_{i=2}^{n}\\frac{1}{\\sum_{m=1}^{l}p(x_{i}|x_{i-1}^{[m ] } ) } \\times \\prod_{i=2}^{n}\\frac{1}{\\sum_{m=1}^{l}p(x_{i}'|x_{i-1}^{[m ] } ) } \\notag \\\\ & & \\times \\ \\sum_{x_{1 } \\in \\mathcal{p}_{1}}p(x_{1})p(y_{1}|x_{1})\\prod_{i=2}^{n}\\sum_{x_{i } \\in \\mathcal{p}_{i } } \\sum_{m=1}^{l}p(y_{i}|x_{i})p(x_{i}|x_{i-1}^{[m]})\\end{aligned}\\ ] ] the last factor in the product only depends on the selected set of pool states . by exchanging @xmath9 and @xmath16",
    "we see that the probability of starting at @xmath16 and then going to @xmath9 , with given sets of pool states @xmath47 , pool indices @xmath167 of @xmath7 and pool indices @xmath168 of @xmath67 is the same .",
    "to demonstrate the performance of our new pool state scheme , we use two different state space models .",
    "the latent process for both models is a vector autoregressive process , with @xmath191 where @xmath192 and @xmath193 note that @xmath194 is the covariance of the stationary distribution for this process .    for model 1 ,",
    "the observations are given by @xmath195    for model 2 , the observations are given by @xmath196    for model 1 , we use a @xmath160-dimensional latent state and a sequence length of @xmath197 , setting parameter values to @xmath198 , and @xmath199 , @xmath200 , @xmath201 for @xmath202 , with @xmath203 .    for model 2",
    ", we increase the dimensionality of the latent space to @xmath204 and the sequence length to @xmath205 .",
    "we set @xmath198 and @xmath200 , @xmath206 for @xmath202 , with @xmath207 .",
    "we generated one random sequence from each model to test our samplers on .",
    "these observations from model 1 and model 2 are shown in figure [ fig : data ] .",
    "note that we are testing only sampling of the latent variables , with the parameters set to their true values .",
    "the code for all experiments in this paper is available at http://arxiv.org/abs/1602.06030 .",
    "0.45 .,title=\"fig : \" ]       0.45 .,title=\"fig : \" ]      a simple scheme for sampling the latent state sequence is to use metropolis - hastings updates that sample each @xmath7 in sequence , conditional on @xmath208 and the data , starting at time @xmath25 and going to time @xmath54 .",
    "we sample all dimensions of @xmath7 at once using autoregressive updates ( see section 5.4.3 ) .",
    "the conditional densities of the @xmath4 are @xmath209    the densities @xmath210 , and @xmath211 are all gaussian .",
    "the means and covariances for these densities can be derived by viewing @xmath212 or @xmath26 as a gaussian prior for @xmath7 and @xmath213 as a gaussian likelihood for @xmath7 . in particular",
    ", we have @xmath214 where @xmath215^{-1}[(\\phi^{-1}\\sigma\\phi^{-1})^{-1}\\phi^{-1}x_{2 } ] \\\\ & = & [ ( \\phi^{-1}\\sigma\\phi^{-1})^{-1 } + \\sigma_{\\textnormal{init}}^{-1}]^{-1}[\\sigma^{-1}(\\phi x_{2 } ) ] \\\\ & = & [ \\phi^{2 } + \\sigma_{\\textnormal{init}}^{-1}\\sigma]^{-1}\\phi x_{2 } \\\\",
    "\\sigma_{1 } & = & [ ( \\phi^{-1}\\sigma\\phi^{-1})^{-1 } + \\sigma_{\\textnormal{init}}^{-1}]^{-1}\\\\   & = & [ \\phi(\\sigma^{-1}\\phi ) + \\sigma_{\\textnormal{init}}^{-1}]^{-1 } \\\\ \\\\",
    "\\mu_{i } & = & [ ( \\phi^{-1}\\sigma\\phi^{-1})^{-1 } + \\sigma^{-1}]^{-1}[\\sigma^{-1}\\phi x_{i-1 } + ( \\phi^{-1}\\sigma\\phi^{-1})^{-1}\\phi^{-1}x_{i+1 } ] \\\\ & = & ( \\phi^{-1}\\sigma\\phi^{-1})^{-1 } + \\sigma^{-1}]^{-1}[\\sigma^{-1}(\\phi ( x_{i-1 } + x_{i+1 } ) ) ] \\\\ & = & [ \\phi^{2 } + i]^{-1}\\phi ( x_{i-1 } + x_{i+1 } ) \\\\ \\sigma_{i } & = & [ ( \\phi^{-1}\\sigma\\phi^{-1})^{-1 } + \\sigma^{-1}]^{-1 } \\\\ & = & [ \\phi(\\sigma^{-1}\\phi ) + \\sigma^{-1}]^{-1 } \\\\ \\\\ \\mu_{n } & = & \\phi x_{n-1 } \\\\",
    "\\sigma_{n } & = & \\sigma\\end{aligned}\\ ] ] to speed up the metropolis updates , we precompute and store the matrices @xmath216^{-1}\\phi$ ] , @xmath217^{-1}\\phi$ ] as well as the cholesky decompositions of the posterior covariances .    in both of our test models ,",
    "the posterior standard deviation of the latent variables @xmath218 varies depending on the value of the observed @xmath219 . to address this",
    ", we alternately use a larger or a smaller proposal scaling , @xmath134 , in the autoregressive update when performing an iteration of the metropolis sampler .",
    "we implement the pgbs method as described in section 5.3 , using the initial density @xmath24 and the transition densities @xmath26 as importance densities to generate particles .",
    "we combine pgbs updates with single - state metropolis updates from section 5.6.2 .",
    "this way , we combine the strengths of the two samplers in targeting different parts of the posterior distribution .",
    "in particular , we expect the metropolis updates to do better for the @xmath7 with highly informative @xmath29 , and the pgbs updates to do better for the @xmath7 where @xmath29 is not as informative .      for model @xmath25 , we compared the embedded hmm sampler to the simple single - state metropolis sampler , to the pgbs sampler , and to the combination of pgbs with metropolis . for model 2 , we compared the embedded hmm sampler to the pgbs with metropolis sampler . for both models and all samplers , we ran the sampler five times using five different random number generator seeds .",
    "we implemented the samplers in matlab on a linux system with a 2.60 ghz intel i7 - 3720qm cpu .",
    "for the single - state metropolis sampler , we initialized all @xmath218 to @xmath154 .",
    "every iteration alternately used a scaling factor , @xmath134 , of either @xmath220 or @xmath221 , which resulted in an average acceptance rate of between @xmath222 and @xmath223 for the different @xmath7 over the sampler run .",
    "we ran the sampler for @xmath224 iterations , and prior to analysis , the resulting sample was thinned by a factor of @xmath160 , to @xmath225 .",
    "the thinning was done due to the difficulty of working with all samples at once , and after thinning the samples still possessed autocorrelation times significantly greater than @xmath25 .",
    "each of the @xmath226 samples took about @xmath227 seconds to draw .    for the pgbs sampler and the sampler combining pgbs and metropolis updates , we also initialized all @xmath218 to @xmath154 .",
    "we used @xmath228 particles for the pgbs updates . for",
    "the metropolis updates , we alternated between scaling factors of @xmath220 and @xmath221 , which also gave acceptance rates between @xmath222 and @xmath223 . for the standalone pgbs sampler , we performed a total of @xmath229 iterations .",
    "each iteration produced two samples for a total of @xmath230 samples and consisted of a pgbs update using the forward sequence and a pgbs update using the reversed sequence .",
    "each sample took about @xmath231 seconds to draw . for the pgbs with metropolis sampler , we performed a total of @xmath232 iterations of the sampler .",
    "each iteration was used to produce four samples , for a total of @xmath233 samples , and consisted of a pgbs update using the forward sequence , ten metropolis updates ( of which only the value after the tenth update was retained ) , a pgbs update using the reversed sequence , and another ten metropolis updates , again only keeping the value after the tenth update .",
    "the average time to draw each of the @xmath233 samples was about @xmath234 seconds .      for model 2",
    ", we were unable to make the single - state metropolis sampler converge to anything resembling the actual posterior in a reasonable amount of time .",
    "in particular , we found that for @xmath218 sufficiently far from @xmath154 , the metropolis sampler tended to be stuck in a single mode , never visiting values with the opposite sign .",
    "for the pgbs with metropolis sampler , we set the initial values of @xmath235 to @xmath25 .",
    "we set the number of particles for pgbs to @xmath236 , which was nearly the maximum possible for the memory capacity of the computer we used . for the metropolis sampler",
    ", we alternated between scaling factors of @xmath237 and @xmath25 , which resulted in acceptance rates ranging between @xmath238 and @xmath239 .",
    "we performed a total of @xmath228 iterations of the sampler .",
    "as for model @xmath25 , each iteration produced four samples , for a total of @xmath240 samples , and consisted of a pgbs update with the forward sequence , fifty metropolis updates ( of which we only keep the value after the last one ) , a pgbs update using the reversed sequence , and another fifty metropolis updates ( again only keeping the last value ) .",
    "it took about @xmath241 seconds to draw each sample .      for both model 1 and model 2",
    ", we implemented the proposed embedded hmm method using the forward pool state selection scheme , alternating between updates that use the original and the reversed sequence . as for the baseline samplers , we ran the embedded hmm samplers five times for both models , using five different random number generator seeds .",
    "we generate pool states at time @xmath25 using autoregressive updates to sample from @xmath242 . at times",
    "@xmath243 , we sample each pool state from @xmath244 by combining an autoregressive and shift update .",
    "the autoregressive update proposes to only change @xmath9 , keeping the current @xmath245 fixed .",
    "the shift update samples both @xmath9 and @xmath245 , with a new @xmath245 proposed from a uniform@xmath123 distribution . for model 2",
    ", we also add a flip update to generate a negated version of each pool state .",
    "note that the chain used to produce the pool states now uses a sequence of updates .",
    "therefore , if our forward transition first does an autoregressive update and then a shift update , the reverse transitions must first do a shift update and then an autoregressive update .    as for",
    "the single - state metropolis updates , it is beneficial to use a different proposal scaling , @xmath134 , when generating each pool state at each time @xmath12 .",
    "this allows generation of sets of pool states which are more concentrated when @xmath29 is informative and more dispersed when @xmath29 holds little information .      for model 1 , we initialized all @xmath218 to @xmath154 .",
    "we used @xmath246 pool states for the embedded hmm updates . for each metropolis update to sample a pool state , we used a different scaling @xmath134 , chosen at random from a @xmath247 distribution .",
    "the acceptance rates ranged between @xmath248 and @xmath249 for the metropolis updates and between @xmath250 and @xmath251 for the shift updates .",
    "we performed a total of @xmath252 iterations of the sampler , with each iteration consisting of an embedded hmm update using the forward sequence and an embedded hmm update using the reversed sequence , for a total of @xmath253 samples .",
    "each sample took about @xmath254 seconds to draw .      for model 2 , we initialized the @xmath218 to @xmath25 .",
    "we used a total of @xmath255 pool states for the embedded hmm sampler ( i.e.  @xmath256 positive - negative pairs due to flip updates ) .",
    "each metropolis update used to sample a pool state used a scaling , @xmath134 , randomly drawn from the @xmath257 distribution .",
    "the acceptance rates ranged between @xmath258 and @xmath223 for the metropolis updates and between @xmath250 and @xmath259 for the shift updates .",
    "we performed a total of @xmath252 iterations of the sampler , producing two samples per iteration with an embedded hmm update using the forward sequence and an embedded hmm update using the reversed sequence .",
    "each of the @xmath253 samples took about @xmath260 seconds to draw .      as a way of comparing the performance of the two methods , we use an estimate of autocorrelation time for each of the latent variables @xmath235 .",
    "autocorrelation time is a measure of how many draws need to be made using the sampling chain to produce the equivalent of one independent sample . the autocorrelation time is defined as @xmath261 , where @xmath262 is the autocorrelation at lag @xmath120",
    "it is commonly estimated as @xmath263 where @xmath264 are estimates of lag-@xmath120 autocorrelations and the cutoff point @xmath265 is chosen so that @xmath264 is negligibly different from @xmath154 for @xmath266 . here",
    "@xmath267 where @xmath268 is an estimate of the lag-@xmath120 autocovariance @xmath269 when estimating autocorrelation time , we remove the first @xmath270 of the sample as burn - in . then , to estimate @xmath268 , we first estimate autocovariances for each of the five runs , taking @xmath271 to be the overall mean over the five runs .",
    "we then average these five autocovariance estimates to produce @xmath268 . to speed up autocovariance computations",
    ", we use the fast fourier transform .",
    "the autocorrelation estimates are then adjusted for computation time , by multiplying the estimated autocorrelation time by the time it takes to draw a sample , to ensure that the samplers are compared fairly .",
    "0.45        0.45     0.45        0.45     the computation time - adjusted autocorrelation estimates for model 1 , for all the latent variables , plotted over time , are presented in figure [ fig : acf ] .",
    "we found that the combination of single - state metropolis and pgbs works best for the unimodal model .",
    "the other samplers work reasonably well too .",
    "we note that the spike in autocorrelation time for the pgbs and to a lesser extent for the pgbs with metropolis sampler occurs at the point where the data is very informative .",
    "this in turn makes the use of the diffuse transition distribution the particles are drawn from inefficient and much of the sampling in that region is due to the metropolis updates . here",
    ", we also note that the computation time adjustment is sensitive to the particularities of the implementation , in this case done in matlab , where performance depends a lot on how well vectorization can be exploited .",
    "implementing the samplers in a different language might change the relative comparisons .",
    "we now look at how the samplers perform on the more challenging model 2 .",
    "we first did a preliminary check of whether the samplers do indeed explore the different modes of the distribution by looking at variables far apart in the sequence , where we expect to see four modes ( with all possible combinations of signs ) .",
    "this is indeed the case for both the pgbs with metropolis and embedded hmm samplers .    0.45 , title=\"fig : \" ]       0.45 , title=\"fig : \" ]    next , we look at how efficiently the latent variables are sampled . of particular interest",
    "are the latent variables with well - separated modes , since sampling performance for such variables is illustrative of how well the samplers explore the different posterior modes . consider the variable @xmath272 , which has true value @xmath273 .",
    "figure [ fig : trace ] shows how the different samplers explore the two modes for this variable , with equal computation times used to produced the samples for the trace plots .",
    "we can see that the embedded hmm sampler with flip updates performs significantly better for sampling a variable with well - separated modes .",
    "experiments showed that the performance of the embedded hmm sampler on model 2 without flip updates is much worse .",
    "0.45 , title=\"fig : \" ]       0.45 , title=\"fig : \" ]    we can also look at the product of the two variables @xmath274 , with true value @xmath275 .",
    "the trace plot is given in figure [ fig : trace2 ] .",
    "in this case , we can see that the pgbs with metropolis sampler performs better .",
    "since the flip updates change the signs of all dimensions of @xmath7 at once , we do not expect them to be as useful for improving sampling of this function of state .",
    "the vastly greater number of particles used by pgbs , @xmath236 , versus @xmath255 for the embedded hmm method , works to the advantage of pgbs , and explains the performance difference .",
    "0.45        0.45     looking at these results , we might expect that we can get a good sampler for both @xmath272 and @xmath274 by alternating embedded hmm and pgbs with metropolis updates .",
    "this is indeed the case , which can be seen in figure [ fig : trace3 ] . for producing these plots , we used an embedded hmm sampler with the same settings as in the experiment for model @xmath155 and a pgbs with metropolis sampler with @xmath276 particles and metropolis updates using the same settings as in the experiment for model @xmath155 .",
    "this example of model 2 demonstrates another advantage of the embedded hmm viewpoint , which is that it allows us to design updates for sampling pool states to handle certain properties of the density .",
    "this is arguably easier than designing importance densities in high dimensions .",
    "we have demonstrated that it is possible to use embedded hmm s to efficiently sample state sequences in models with higher dimensional state spaces .",
    "we have also shown how embedded hmms can improve sampling efficiency in an example model with a multimodal posterior , by introducing a new pool state selection scheme .",
    "there are several directions in which this research can be further developed .",
    "the most obvious extension is to treat the model parameters as unknown and add a step to sample parameters given a value of the latent state sequence . in the unknown parameter context , it would also be interesting to see how the proposed sequential pool state selection schemes can be used together with ensemble mcmc updates of shestopaloff and neal ( 2013 ) .",
    "for example , one approach is to have the pool state distribution depend on the average of the current and proposed parameter values in an ensemble metropolis update , as in shestopaloff and neal ( 2014 )",
    ".    one might also wonder whether it is possible to use the entire current state of @xmath9 in constructing the pool state density at a given time .",
    "it is not obvious how ( or if it is possible ) to overcome this limitation .",
    "for example , for the forward scheme , using the current value of the state sequence at some time @xmath277 to construct pool states at time @xmath12 means that the pool states at time @xmath120 will end up depending on the current value of @xmath278 , which would lead to an invalid sampler .    at each time",
    "@xmath62 , the pool state generation procedure does not depend on the data after time @xmath12 , which may cause some difficulties in scaling this method further . on one hand",
    ", this allows for greater dispersion in the pool states than if we were to impose a constraint from the other direction as with the single - state metropolis method , potentially allowing us to make larger moves . on the other hand",
    ", the removal of this constraint also means that the pool states can become too dispersed . in higher dimensions , one way in which this can be controlled",
    "is by using a markov chain that samples pool states close to the current @xmath7  that is , a markov chain that is deliberately slowed down in order not to overdisperse the pool states , which could lead to a collection of sequences with low posterior density .",
    "we thank arnaud doucet for helpful comments .",
    "this research was supported by the natural sciences and engineering research council of canada .",
    "a.  s.  is in part funded by an nserc postgraduate scholarship .",
    "r.  n.  holds a canada research chair in statistics and machine learning .",
    "andrieu , c. , doucet , a. and holenstein , r. ( 2010 ) `` particle markov chain monte carlo methods '' , _ journal of the royal statistical society b _ , vol .  72 , pp .  269 - 342 .",
    "lindsten , f. ; schon , t.b .",
    "( 2012 ) `` on the use of backward simulation in the particle gibbs sampler '' , in _ acoustics , speech and signal processing ( icassp ) , 2012 ieee international conference on _ , pp .",
    "3845 - 3848 .",
    "neal , r. m. ( 2003 ) `` markov chain sampling for non - linear state space models using embedded hidden markov models '' , technical report no .",
    "0304 , department of statistics , university of toronto , http://arxiv.org/abs/math/0305039 .",
    "neal , r. m. , beal , m. j. , and roweis , s. t. ( 2004 ) `` inferring state sequences for non - linear systems with embedded hidden markov models '' , in s. thrun , et al ( editors ) , _ advances in neural information processing systems 16 _ , mit press ."
  ],
  "abstract_text": [
    "<S> we propose a new scheme for selecting pool states for the embedded hidden markov model ( hmm ) markov chain monte carlo ( mcmc ) method . </S>",
    "<S> this new scheme allows the embedded hmm method to be used for efficient sampling in state space models where the state can be high - dimensional . </S>",
    "<S> previously , embedded hmm methods were only applied to models with a one - dimensional state space . </S>",
    "<S> we demonstrate that using our proposed pool state selection scheme , an embedded hmm sampler can have similar performance to a well - tuned sampler that uses a combination of particle gibbs with backward sampling ( pgbs ) and metropolis updates . </S>",
    "<S> the scaling to higher dimensions is made possible by selecting pool states locally near the current value of the state sequence . </S>",
    "<S> the proposed pool state selection scheme also allows each iteration of the embedded hmm sampler to take time linear in the number of the pool states , as opposed to quadratic as in the original embedded hmm sampler . </S>",
    "<S> we also consider a model with a multimodal posterior , and show how a technique we term `` mirroring '' can be used to efficiently move between the modes . </S>"
  ]
}