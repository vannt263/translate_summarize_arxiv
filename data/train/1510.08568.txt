{
  "article_text": [
    "heuristic search methods such as local search , simulated annealing , evolutionary algorithms and ant colony optimization have been shown to be very successful for various combinatorial optimization problems .",
    "although they usually do nt come with performance guarantees on their runtime and/or approximation behaviour , they often perform very well in several situations . understanding the conditions under which optimization algorithms perform well is essential for automatic algorithm selection , configuration and effective algorithm design . in both the artificial intelligence ( ai )  @xcite and operational research communities  @xcite",
    ", this topic has become a major point of interest .",
    "the feature - based analysis of heuristic search algorithms has become an important part in understanding such type of algorithms  @xcite .",
    "this approach characterizes algorithms and their performance for a given problem based on features of problem instances .",
    "thereby , it provides an important tool for bridging the gap between pure experimental investigations and mathematical methods for analysing the performance of search algorithms  @xcite .",
    "current methods for the feature - based analysis are based on constructing hard and easy instances for an investigated search heuristic and a given optimization problem by evolving instances using an evolutionary algorithm  @xcite .",
    "this evolutionary algorithm constructs problem instances where the examined algorithm either shows a bad ( good ) approximation behaviour and/or requires a large ( small ) computational effort to come up with good or optimal solutions .",
    "although the evolutionary algorithm for constructing such instances is usually run several times to obtain a large set of hard ( easy ) instances , the question arises whether the results in terms of features of the instances obtained give a good characterization of problem difficulty .    in the paper",
    ", we propose a new approach of constructing hard and easy instances .",
    "following some recent work on using evolutionary algorithms for generating diverse sets of instances that are all of high quality  @xcite , we introduce an evolutionary algorithm which maximizes diversity of the obtained instances in terms of a given feature .",
    "our approach allows to generate a set of instances that is much more diverse with respect to the problem feature at hand .",
    "carrying out this process for several features of the considered problem and algorithm gives a much better classification of instances according to their difficulty of being solved by the considered algorithm .    to show the benefit of our approach",
    "compared to previous methods , we consider the classical @xmath0-optalgorithm for the tsp .",
    "previous feature - based analyses have already considered hard and easy instances in terms of approximation ratio and analyzed the features of such hard ( easy ) instances obtained by an evolutionary algorithm .",
    "the experimental results of our new approach show that diversity optimization of the features results in an improved coverage of the feature space over classical instance generation methods .",
    "in particular , the results show that for some combinations of two features it is possible to classify hard and easy instances into two clusters with a wider coverage of the feature space compared to the classical methods .",
    "moreover , the three - feature combinations further improve the classification of hard and easy instances for most of the feature combinations .",
    "furthermore , a classification model is built using these diverse instances that can classify tsp instances based on hardness for @xmath0-opt .",
    "the remainder of this paper is organized as follows .",
    "firstly , we introduce the euclidean tsp and the background on feature based analysis .",
    "afterwards , we state our diversity optimization approach for evolving instances according to feature values and report on the impact of diversity optimization in terms of the range of feature values .",
    "as features value can be very diverse both for easy and hard instances , we consider the combinations of several features for instance classification afterwards .",
    "we then build a classification model that can classify instances based on hardness and finally finish with some conclusions .",
    "we consider the classical np - hard euclidean traveling salesperson problem ( tsp ) as the example problem for evolving hard and easy instances which have a diverse set of features .",
    "our methodology can be applied to any optimization problem , but using the tsp in our study has the advantage that it has already been investigated extensively from different perspectives including the area of feature - based analysis .",
    "the input of the problem is given by a set @xmath1 of @xmath2 cities in the euclidean plane and euclidean distances @xmath3 between the cities .",
    "the goal is to find a hamiltonian cycle whose sum of distances is minimal . a candidate solution for the tsp is often represented by a permutation @xmath4 ) of the @xmath2 cities and the goal is to find a permutation @xmath5 which minimizes the tour length given by @xmath6    for our investigations cities are always in the normalized plane @xmath7 ^ 2 $ ]",
    ", i.e.each city has an @xmath8- and @xmath9-coordinate in the interval @xmath7 $ ] . in following",
    ", a tsp instance always consists of a set of @xmath2 points in @xmath7 ^ 2 $ ] and the euclidean distances between them .",
    "local search heuristics have been shown to be very successful when dealing with the tsp and the most prominent local search operator is the @xmath0-optoperator  @xcite .",
    "the resulting local search algorithm starts with a random permutation of the cities and repeatedly checks whether removing two edges and reconnecting the two resulting paths by two other edges leads to a shorter tour .",
    "if no improvement can be found by carrying out any @xmath0-optoperation , the tour is called locally optimal and the algorithm terminates .",
    "the key factor in the area of feature - based analysis is to identify the problem features and their contribution to the problem hardness for a particular algorithm and problem combination .",
    "this can be achieved through investigating hard and easy instances of the problem .    using an evolutionary algorithm ,",
    "it is possible to evolve sets of hard and easy instances by maximizing or minimizing the fitness ( tour length in the case of the tsp ) of each instance  @xcite .",
    "however , none of these approaches have considered the diversity of the instances explicitly . within this study",
    "we expect to improve the evolutionary algorithm based instance generation approach by introducing diversity optimization .",
    "the structural features are dependent on the underlying problem . in @xcite , there are 47 features in @xmath10 groups used to provide an understanding of algorithm performance for the tsp .",
    "the different feature classes established are distance features , mode features , cluster features , centroid features , mst features , angle features and convex hull features .",
    "the feature values are regarded as indicators which allow to predict the performance of a given algorithm on a given instance .",
    "initialize the population @xmath11 with @xmath12 tsp instances of approximation ratio at least @xmath13 .",
    "+ let @xmath14 where @xmath15 .",
    "+ for each @xmath16 , produce an offspring @xmath17 of @xmath18 by mutation . if @xmath19 , add @xmath17 to @xmath11 .",
    "+ while @xmath20 , remove an individual @xmath21 uniformly at random .",
    "+ repeat step 2 to 4 until termination criterion is reached .",
    "+    in this section , we introduce our approach of evolving a diverse set of easy or hard instances which are diverse with respect to important problem features . as in previous studies , we measure hardness of a given instance by the ratio of the solution quality obtained by the considered algorithm and the value of an optimal solution .    the approximation ratio of an algorithm @xmath22 for a given instance @xmath18 is defined as @xmath23 where @xmath24 is value of the solution produced by algorithm @xmath22 for the given instance @xmath18 , and @xmath25 is value of an optimal solution for instance @xmath18 . within this study , @xmath24 is the tour length obtained by @xmath0-optfor a given tsp instance @xmath18 and @xmath25 is the optimal tour length which we obtain in our experiments by using the exact tsp solver concorde  @xcite .",
    "we propose to use an evolutionary algorithm to construct sets of instances of the tsp that are quantified as either easy or hard in terms of approximation and are diverse with respect to underlying features of the produced problem instances .",
    "our evolutionary algorithm ( shown in algorithm  [ ea ] ) evolves instances which are diverse with respect to given features and meet given approximation ratio thresholds .",
    "the algorithm is initialized with a population @xmath11 consisting of @xmath12 tsp instances which have an approximation ratio at least @xmath13 in the case of generating a diverse set of hard instances . in the case of easy instances , we start with a population where all instances have an approximation ratio of at most @xmath26 and only instances of approximation ratio at most @xmath26 can be accepted for the next iteration . in each iteration , @xmath27 offspring are produced by selecting @xmath28 parents and applying mutation to the selected individuals .",
    "offsprings that do nt meet the approximation threshold are rejected immediately .",
    "the new parent population is formed by reducing the set consisting of parents and offsprings satisfying the approximation threshold until a set of @xmath12 solutions is achieved .",
    "this is done by removing instances one by one based on their contribution to the diversity according to the considered feature .",
    "the core of our algorithm is the selection among individuals meeting the threshold values for the approximation quality according to feature values .",
    "let @xmath29 be the elements of @xmath11 and @xmath30 be their features values .",
    "furthermore , assume that @xmath31 $ ] , i.e. feature values are non - negative and upper bounded by @xmath32 .",
    "we assume that @xmath33 holds .",
    "the diversity contribution of an instance @xmath18 to a population of instances @xmath11 is defined as @xmath34 where @xmath35 is a contribution based on other individuals in the population    let @xmath36 be an individual for which @xmath37 and @xmath38 .",
    "we set    @xmath39 which assigns the diversity contribution of an individual based on the next smaller and next larger feature values . if @xmath40 or @xmath41 , we set @xmath42 if there is no other individual @xmath43 in @xmath11 with @xmath44 and @xmath45 otherwise .",
    "this implies an individual @xmath36 with feature value equal to any other instances in the population gains @xmath46 .",
    "furthermore , an individual with the unique smallest and largest feature value always stays in the population when working with @xmath47 .    [ features ]    in @xcite , @xmath48 features of tsp instances for characterizing easy and hard tsp instances have been studied .",
    "we consider @xmath49 features coming from different feature classes which have shown to be well suited for classification and prediction .",
    "these features are :    * _ angle_mean _ : - mean value of the angles made by each point with its two nearest neighbor points * _ centroid_mean_distance_to_centroid _ : - mean value of the distances from the points to the centroid * _ chull_area _ : - area covered by the convex hull * _ cluster_10pct_mean_distance_to_centroid _ : - mean value of the distances to cluster centroids at @xmath50 levels of reachability * _ mst_depth_mean _ : - mean depth of the minimum spanning tree * _",
    "nnds_mean _ : - mean distance between nearest neighbours * _ mst_dists_mean _ : - mean distance of the minimum spanning tree    we refer the reader to @xcite for a detailed explanation for each feature .",
    "we carry out our diversity optimization approach for these features and use the evolutionary algorithm to evolve for each feature a diverse population of instances that meets the approximation criteria for hard / easy instances given by the approximation ratio thresholds .",
    "all programs in our experiments are written in r and run in r environment  @xcite .",
    "we use the functions in tspmeta package to compute the feature values  @xcite .",
    "the setting of the evolutionary algorithm for diversity optimization used in our experiments is as follows .",
    "we use @xmath51 and @xmath52 for the parent and offspring population size , respectively .",
    "the @xmath0-optalgorithm is executed on each instance @xmath18 five times with different initial solutions and we set @xmath24 to the average tour length obtained .",
    "the examined instance sizes @xmath2 are @xmath53 , @xmath54 and @xmath55 , which are denoted by the number of cities in one instance .",
    "based on previous investigations in @xcite and initial experimental investigations , we set @xmath56 for instances of size @xmath53 and @xmath54 , and @xmath57 for instances of size @xmath55 .",
    "evolving hard instances , we use @xmath58 for instances of size @xmath59 , respectively .",
    "the mutation operator picks in each step one city for the given parent uniformly at random and changes its @xmath8- and @xmath9-coordinator by choosing an offset according to the normal - distribution with standard deviation @xmath60 .",
    "coordinates that are out of the interval are reset to the value of the parent . based on initial experiments we use two mutation operators with different values of @xmath60 .",
    "we use @xmath61 with probability @xmath62 and @xmath63 with probability @xmath64 in a mutation step .",
    "the evolutionary algorithm terminates after @xmath65 generations which allows to obtain a good diversity for the considered features . for each @xmath59 and",
    "each of the @xmath49 features , a set of easy and hard instances are generated , which results in @xmath66 independent runs of the ( @xmath12+@xmath28)-ea~d~.",
    "we first evaluate our diversity optimization approach in terms of the diversity that is obtained with respect to a single feature .",
    "focusing on a single feature in each run provides the insight of the possible range of a certain feature value for hard or easy instances .",
    "the previous study  @xcite , suggests that there are some differences in the possible range of feature values for easy and hard instances .",
    "we study the effect of the diversity optimization on the range of features by comparing the instances generated by diversity optimization to the instances generated by the conventional approach in  @xcite . evolving hard instances based on the conventional evolutionary algorithm ,",
    "the obtained instances have mean approximation ratios of @xmath67 for @xmath68 , @xmath69 for @xmath70 , and @xmath71 for @xmath72 .",
    "for easy instances , the mean approximation ratios are @xmath73 for @xmath74 and @xmath75 for @xmath72 .",
    "figure  [ fig : feature - range ] ( left ) presents the variation of the mean distance of the distances between points and the centroid feature ( _ centroid_mean_distance_to_centroid _ ) for hard and easy instances of the three considered sizes @xmath53 , @xmath54 and @xmath55 .",
    "each set consists of 100 instances generated by independent runs  @xcite .",
    "as shown in figure  [ fig : feature - range ] ( left ) the hard instances have higher feature values than for easy instances for all instance sizes .",
    "for example , for instance size 100 and for the hard instances the median value ( indicated by the red line ) is @xmath76 while its only @xmath77 for the easy instances .",
    "the respective range of the feature value is @xmath78 for the hard instances and @xmath79 for the easy instances . for the instances generated by diversity optimization",
    "( easy and hard instances are indicated by e ( b ) and h ( b ) respectively ) , there is a difference in the median feature values for the hard and easy instances similar to the instances generated by the conventional approach .",
    "additionally , the range of the feature values for both the hard and easy instances has significantly increased .",
    "for example , for the instance size @xmath55 , the median value for easy instances is @xmath80 and the range is @xmath81 .",
    "for the hard instances of the same size , the median is @xmath82 while the range is @xmath83 ( see figure  [ fig : feature - range ] ( left ) ) .",
    "similarly , figure  [ fig : feature - range ] ( right ) presents the variation of cluster @xmath50 distance to centroid ( _ cluster_10pct_distance_to_centroid _ ) feature for the hard and easy instances generated by the conventional approach ( indicated by ( e ( a ) and h ( a ) ) and for the hard and easy instances generated by diversity optimization ( indicated by ( e ( b ) and h ( b ) ) ) .",
    "the general observations from these box plots are quite similar to the observations from the _",
    "mst_dist_mean _ shown in figure  [ fig : feature - range ] ( left ) . for the easy instances of size @xmath55 ,",
    "the range of the feature value is @xmath84 for conventional instances and @xmath85 for the instances generated by diversity optimization .",
    "similarly , for the hard instances the range of the feature values has increased from @xmath78 to @xmath86 by the diversity optimization approach .",
    "as shown in figure  [ fig : feature - range ] ( right ) , there is a significant increase in the range for other instance sizes as well .",
    "improved ranges in feature values are observed for all considered features however , due to space limitations these are not included in the paper .",
    "the above results suggest that the diversity optimization approach has resulted in a significant increase in the coverage over the feature space . having the threshold for approximation ratios ( @xmath26 and",
    "@xmath13 ) our method guarantees the hardness of the instances .",
    "these approximation thresholds are more extreme than the mean approximation values obtained by the conventional method .",
    "furthermore , starting with initial population of duplicated instances and a hard coded threshold , the modified @xmath87-ea is able to achieve hard instances with approximation ratio @xmath88 , @xmath89 and @xmath90 , respectively for instance size @xmath53 , @xmath54 and @xmath55 .",
    "the majority of the instances are clustered in a small region in the feature space while some other points are dispersed across the whole space .",
    "this is evident in the median values similar to the values for the instances with respect to conventional approach and with significantly larger range in feature value .",
    "the conventional approach has failed to explore certain regions in the feature space and missed some instances existing in those regions .",
    "being able to discover all these instances spread in the whole feature space , our approach provides a strong basis for more effective feature based prediction .    as a result of the increased ranges and the similar gap in median feature values for hard and easy instances compared to the conventional instances",
    ", there is a strong overlap in the ranges of the features for easy and hard instances generated by the diversity optimization .",
    "this is observed in the results for _ mst_dist_mean _ and _ cluster_10pct_distance_to_centroid _ shown in figure  [ fig : feature - range ] .",
    "similar pattern holds for the other features as well .",
    "this prevents a good classification of problem instances based on single feature value .",
    "as a single feature is not capable in clearly classifying the hard / easy instances , combinations of two or three different features are examined in the following .",
    "our analysis mainly focuses on combinations of the @xmath49 previously introduced features .",
    "firstly , we represent the instances according to the combination of two different features in the @xmath0-dimensional feature value space ( see figure  [ fig:2d_good ] for an example ) .    according to the observation and discussion in  @xcite",
    ", the two features _ distance _ max _ and _ angle_mean _ can be considered together to provide an accurate classification of the hard and easy instances . whereas after increasing the diversity over the seven different feature values and a wider coverage of the @xmath0d space is achieved , the separation of easy and hard instances is not so obvious .",
    "the clusters of dots representing hard and easy instances have some overlapping as shown in the left graphs of figure  [ fig:2d_good ] .",
    "there are large overlapping areas lying between the two groups of instances .",
    "another example of some separation given by two - feature combination is _",
    "mst_dists_mean _ and _ chull_area _ which measure the mean distance of the minimum spanning tree and the area of the convex hull .",
    "however , as the number of cities in an instance increases , the overlapping area becomes larger .",
    "it is hard to do classification based on this .",
    "after examining the @xmath91 different combinations of two features out of the seven features , we found out that some combinations of two features provide a fair separation between hard and easy instances after increasing the diversity over different feature values .",
    "as shown in figure  [ fig:2d_good ] , taking both _",
    "mst_dists_mean _ and _ chull_area _ features into consideration , some separations can be spotted between hard and easy instances . however , most combinations are not able to give a clear classification between hard and easy instances , for example in figure  [ fig:2d_bad ] , neither the combination of features _",
    "nnds_mean _ and _ centroid_mean_distance_to_centroid _ nor features _",
    "mst_depth_mean _ and _ chull_area _ shows clear classification between instances of different hardness",
    ". moreover , along with the instance size increment , the overlapping area of the dots standing for hard and easy instances grows .",
    "since the majority of two - feature combinations are not capable of classifying easy and hard instances , the idea of combining three different feature is put forward . as in the analysis of two - feature combination ,",
    "the values of the three selected features are plotted in @xmath92d space .    by considering a third feature in the combination , in the @xmath93 different combinations",
    ", it is clear that there are some separations between the two groups of @xmath94 instances from the @xmath92d - plots . a good selection of features results in an accurate classification of the instances",
    ". the three - feature combinations with the features measuring statistics about minimum spanning tree always provide good separation between hard and easy instances as shown in figure  [ fig:3d_good ] and figure  [ fig:3d_good2 ] .",
    "although there is an overlapping in the area between the two clusters of hard and easy instances , from the @xmath92d - plots , we can spot some areas where there are only dots for instances of certain hardness .    taken another feature value into consideration , the two - feature combination that is not able to provide good separation can give some clear classification in hard and easy instances .",
    "an example illustrating this is included as figure  [ fig:2d-3d ] , where together with an additional feature _",
    "mst_dists_mean _ , the two - feature combination of features _ mst_depth_mean _ and _ chull_area _ shows a clear separation between easy and hard instances comparing to the results shown in the left graph in figure  [ fig:2d_bad ] .    from the investigation of both the two - feature combination and three - feature combination",
    ", we found out that the range of feature values for larger tsp instances is smaller .",
    "some of the good combinations for classifying the hardness of smaller instances may not work for larger instances , such as centroid features which performs well when combining with another feature in classifying the hardness of instances of @xmath53 cities while does not show a clear separation with instance size @xmath54 and @xmath55 in our study .",
    "however , there exist some three - feature combinations that give good classification of easy and hard instances without regarding to the instance size , for example _",
    "mst_dists_mean _ , _ chull_area _ and _ nnds_mean _ , and _",
    "mst_dists_mean _ , _ chull_area _ and _",
    "mst_depth_mean_.                            in order to examine the relationship between feature combination and hardness of the instances , a weighted population diversity based on multiple features is introduced .",
    "the weighted population diversity for a certain set of features @xmath95 is defined as the weighted sum of the normalised population diversity over these @xmath96 features .",
    "the contribution of an instance @xmath18 to the weighted population diversity is defined as @xmath97 where @xmath98 denotes the normalised contribution to the population diversity @xmath99 over certain feature @xmath100 and @xmath101 represents the weight of feature @xmath100 .",
    "the contribution of an individual to the population diversity on certain feature is normalised based on the maximum population diversity on the feature , in order to reduce the bias among different features .",
    "this weighted population diversity is used in algorithm  [ ea ] to gain some insight of the relationship between features combination and instance quality .",
    "the same parent and offspring population sizes are used for these experiments , which are @xmath51 and @xmath52 .",
    "the instance sizes examined are still @xmath53 , @xmath54 and @xmath55 .",
    "the @xmath0-optalgorithm is executed five times to obtain the approximation quality . in the experiments , ( @xmath12+@xmath28)-ea~d~execute for @xmath65 generation as previous . since it is shown in section  [ sec : range ] that a combination of three features is able to provide a good separation between hard and easy instances , some of the good three - feature combinations",
    "are chosen for exploration .",
    "the weight distributions for @xmath102 considered in the experiments are @xmath103 , @xmath104 , @xmath105 , @xmath106 , @xmath107 , @xmath108 , @xmath109 . the same hardness thresholds are used in these experiments as previous .",
    "after the seven independent runs for easy and hard instances , the final solution sets are put together .",
    "therefore the results set has @xmath94 instances for each instance size and hardness type , which is the same as previous experiments .",
    "the results are plotted in @xmath92d space and compared to the previous experiments on single feature discussed in section  [ sec : approach ] and  [ sec : range ] .",
    "the weighted population diversity offers a way to examine the overlapping area of hard and easy instances . with the weighting technique",
    ", it takes consideration about the relationship between the different features examined .",
    "since most of these features are not independent from each others and the weighted population diversity considers multiple features at the same time , it is predictable that with the weighted population diversity the extreme value for each single feature may not reach .",
    "an example is shown in figure  [ fig : weighted ] focusing on maximizing the weighted population diversity over the combination of features _",
    "mst_dists_mean _ , _",
    "nnds_mean _ and _ chull_area _ , which is shown to be a good combination for separating the hard and easy instances . from the comparison between figure  [ fig:3d_good ] and",
    "figure  [ fig : weighted ] , we can see that although the results from maximizing weighted population diversity does not cover a wider search space , it provides a detailed insight into the intersection between the hard and easy instances .",
    "the @xmath92d plots of different instance sizes show that the combination of these three certain features provide a clear separation between hard and easy instances .",
    "there are some overlapping areas in the search space , but it is clear that this combination of features provide some hints for predicting of hard or easy instances .",
    "support vector machines ( svms ) are well - known supervised learning models in machine learning which can be used for classification , regression and outliers detection  @xcite . in order to quantify the separation between instances of different hardness based on the feature values ,",
    "svm models are constructed for each combination of features .",
    "the linear classifier is the first model tried in classifying the dataset . in svm",
    "the linear classifiers that can separate the data with maximum margin is termed as the optimal separating hyper - plane . from the plots in figure  [ fig:2d_good ] ,  [ fig:2d_bad ] ,  [ fig:3d_good ] and  [ fig:3d_good2 ] ,",
    "it is clear that none of the datasets are linearly separable . taken the trade - off between maximizing the margin and minimizing the number of misclassified data points into consideration , the soft - margin svm is used for classification .",
    "let @xmath110 be the training accuracy of a feature combination in separating the hard and easy instances of size @xmath2 .",
    "we define @xmath110 as the ratio of number of instances which are correctly classified by the model to the total number of instances in the dataset .",
    "all classification experiments are done in r with library@xmath111  @xcite .",
    "the training data of the svm models are the population of 420 instances generated as in section  [ sec : approach ] and the training accuracy is regarded as a quantified measurement of the separation between hard and easy instances .",
    "the feature combinations used for classification are the @xmath91 two - feature combinations and @xmath93 three - feature combinations discussed in section  [ sec : exp ] .    from experiment results , @xmath112 for two - feature combinations",
    "lie in the range of @xmath113 to @xmath114 with an average accuracy of @xmath115 , while the @xmath112 for three - feature combination lie between @xmath116 to @xmath117 with average value @xmath118 . in the case of instances with city number of @xmath54 , two - feature combination results in @xmath119 lying in the range of @xmath120 to @xmath121 with an average of @xmath122 while @xmath119 of three - feature combinations are from @xmath123 to @xmath124 with average accuracy equal to @xmath125 . for larger instance",
    "size , @xmath126 are in the range between @xmath127 and @xmath128 with average @xmath129 for two - feature combination , whereas those for three - feature combination lie in the scope of @xmath130 to @xmath131 with average @xmath132 .",
    "although three - feature combinations show better accuracy in separation of hard and easy instances than those two - feature combinations , there is no significant difference in @xmath133 for two - feature combinations and three - feature combinations .",
    "moreover , the general low accuracy implies the high possibility that the linear models are not suitable for separating the hard and easy instances based on most of the feature combinations .",
    "we then move to applying kernel function for non - linear mapping of the feature combination .",
    "the linearly non - separable features can become linearly separable after mapped to a higher dimension feature space .",
    "the radial basis function ( rbf ) kernel is one of the well - known kernel function used in svm classification .",
    "there are two parameters need to be selected when applying rbf , which are @xmath134(cost ) and @xmath135 .",
    "the parameter setting for rbf is crucial , since increasing @xmath134 and @xmath135 leads to accurate separation of the training data but at the same time causes over - fitting .",
    "the svms here are generated for quantifying the separation rate between hard and easy instances rather than classifying other instances .",
    "after some initial trials , @xmath136 is set to @xmath137 in all the tests to avoid over - fitting .",
    "this parameter setting may not be the best parameters for the certain feature combination in svm classifying , but it helps us to gain some understanding of the separation of hard and easy instances generated from previous experiments based on the same condition .",
    "table  [ tb : acc_2d ] and  [ tb : acc_3d ] show the accuracy of different two features or three features combination in hard and easy instances separation .",
    "with rbf kernel , svm with certain parameter setting can generate a model separating the dataset with average accuracy of @xmath138 , @xmath139 and @xmath140 in @xmath0d feature space for instance size @xmath53 , @xmath54 and @xmath55 respectively . whereas with three features , svm with the same parameter setting provides a separation with average accuracy of @xmath141 , @xmath142 and @xmath143 for instance size @xmath53 , @xmath54 and @xmath55 respectively .    from the results",
    ", it can be concluded that there are better separations between hard and easy instances in the @xmath92d feature space .",
    "[ cols=\"<,<,^,^,^\",options=\"header \" , ]",
    "with this paper , we have introduced a new methodology of evolving easy / hard instances which are diverse with respect to feature sets of the optimization problem at hand . using our diversity optimization approach",
    "we have shown that the easy and hard instances obtained by our approach covers a much wider range in the feature space than previous methods .",
    "the diversity optimization approach provides instances which are diverse with respect to the investigated features .",
    "the proposed population diversity measurements provide good evaluation of the diverse over single or multiple feature values .",
    "our experimental investigations for @xmath0-optand tsp have shown that our large set of diverse instances can be classified quite well into easy and hard instances when considering a suitable combination of multiple features which provide some guidance for predication as the next step .",
    "in particular , the svm classification model built with the diverse instances that can classify tsp instances based on problem hardness provides a strong basis for future performance prediction models that lead to automatic algorithm selection and configuration . building such models",
    "would require further experimentation to determine the minimal set of strong features that can predict performance accurately .",
    "o.  mersmann , b.  bischl , h.  trautmann , m.  wagner , j.  bossek , and f.  neumann . a novel feature - based approach",
    "to characterize algorithm performance for the traveling salesperson problem .",
    ", 69(2):151182 , 2013 .",
    "s.  nallaperuma , m.  wagner , and f.  neumann .",
    "parameter prediction based on features of evolved instances for ant colony optimization and the traveling salesperson problem . in _",
    "ppsn xiii - 13th international conference , ljubljana , slovenia , september 13 - 17 , 2014 .",
    "proceedings _ , pages 100109 , 2014 .",
    "s.  nallaperuma , m.  wagner , f.  neumann , b.  bischl , o.  mersmann , and h.  trautmann . a feature - based comparison of local search and the christofides algorithm for the travelling salesperson problem . in _ foga 13 _ ,",
    "pages 147160 , 2013 .",
    "k.  smith - miles , j.  van hemert , and x.  y. lim .",
    "nderstanding tsp difficulty by learning from evolved instances . in _",
    "4th international conference on learning and intelligent optimization ( lion ) _",
    ", lion10 , pages 266280 .",
    "springer , 2010 .",
    "t.  ulrich , j.  bader , and l.  thiele .",
    "defining and optimizing indicator - based diversity measures in multiobjective search . in r.",
    "schaefer , c.  cotta , j.  kolodziej , and g.  rudolph , editors , _ ppsn ( 1 ) _ , volume 6238 of _ lecture notes in computer science _ , pages 707717 .",
    "springer , 2010 ."
  ],
  "abstract_text": [
    "<S> understanding the behaviour of heuristic search methods is a challenge . </S>",
    "<S> this even holds for simple local search methods such as @xmath0-opt for the traveling salesperson problem . in this paper , we present a general framework that is able to construct a diverse set of instances that are hard or easy for a given search heuristic . </S>",
    "<S> such a diverse set is obtained by using an evolutionary algorithm for constructing hard or easy instances that are diverse with respect to different features of the underlying problem . </S>",
    "<S> examining the constructed instance sets , we show that many combinations of two or three features give a good classification of the tsp instances in terms of whether they are hard to be solved by @xmath0-opt . </S>"
  ]
}