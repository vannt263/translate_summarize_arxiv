{
  "article_text": [
    "the fast increase of computing resources needed to analyse the data collected in modern hadron - collider experiments , and the lower cost of processing units with respect to storage , pushes high - energy physics ( hep ) experiments to explore new techniques and technologies to move as much as possible of the data analysis at the time of the data acquisition ( _ online _ ) in order to select candidates to be stored on disk , with maximal , reasonably achievable , background rejection . besides ,",
    "research on multivariate algorithms , active both within and outside of the hep community , is approaching the challenge of operating in distributed computing environments , which represents a further motivation for studies of new classes of algorithms .",
    "statistical inference of probability density functions underlying experimental datasets is common in _ high energy physics_. _ fitting _ is an example of _ parametric _ density estimation .",
    "when possible , defining a parametric form of the underlying distribution and choose the values for the parameters maximizing the likelihood is usually the best approach . in multivariate problems with a large number of variables and important correlation , however , fitting may become unpractical , and _ non - parametric density estimation _ becomes a valid , largely employed , solution .    in hep , the most common non - parametric density estimation , beyond the histogram ,",
    "is probably _ kernel density estimation _",
    "@xcite , based on the sum of normalized kernel functions centered on each data - entry .    in this",
    "write - up , i discuss _ density estimation trees _",
    ", algorithms based on a _ multivariate , binary tree _ structure , oriented to _ non - parametric density estimation_. density estimation trees are less accurate than kernel density estimation , but much faster .",
    "integrating density estimation trees is also trivial and fast , making iterative search algorithms convenient .",
    "finally , storing a density estimation trees and propagate it through the computing nodes of a distributed system is relatively cheap , offering a reasonable solutions for compressing the statistical information of large datasets .",
    "an implementation of the algorithm in root / roofit is available through cern gitlab ] .",
    "the idea of iteratively splitting a data sample , making the density estimation to coincide with the average density in each portion of the data space is not new .",
    "however , the technique had little room for applications in analyses of datasets up to a few thousands of entries described by small sets of correlated variables .",
    "recently , @xmath0-trees @xcite have been used to split large samples into sub - sets consisting of equal fractions of the data entries .",
    "the idea underlying @xmath0-trees is the iterative splitting of the data - sample using as threshold the median of a given projection .",
    "while powerful to solve a vast range of problems , including notably nearest - neighbour searches , the lack of appropriateness of @xmath0-trees to estimate probability densities is evident considering samples including multiple data - entries .    ideally , the density estimation @xmath1 approximating the underlying density function @xmath2 should minimize the quantity @xmath3 it can be shown @xcite that , exploiting the monte  carlo approximation @xmath4 the minimization of @xmath5 is equivalent to growing a density estimation tree iteratively splitting the node @xmath6 , into the two sub nodes @xmath7 and @xmath8 minimizing the _ gini _ index , @xmath9 . here , @xmath10 represents the _ replacement error _ , defined by @xmath11 where @xmath12 is the hyper - volume of the portion of the data - space associated to the node @xmath6 , and @xmath13 the number of data entries it includes ; @xmath14 is the number of data entries in the whole dataset .",
    "figure [ fig : training ] shows an example of how the training is performed .",
    "top , an example of an overtrained density estimation tree .",
    "the random alignment of data - entries with respect to one of the variables describing the problem is misinterpreted as a spike in the density estimation , as evident in the projection onto the vertical axis shown on the bottom . ]",
    "top , an example of an overtrained density estimation tree .",
    "the random alignment of data - entries with respect to one of the variables describing the problem is misinterpreted as a spike in the density estimation , as evident in the projection onto the vertical axis shown on the bottom .",
    ", title=\"fig : \" ] +   top , an example of an overtrained density estimation tree .",
    "the random alignment of data - entries with respect to one of the variables describing the problem is misinterpreted as a spike in the density estimation , as evident in the projection onto the vertical axis shown on the bottom .",
    ", title=\"fig : \" ]      as in the case of classification algorithms , overtraining is the misinterpretation of statistical fluctuations of the dataset as relevant features to be reproduced by the model .",
    "an example of overtraining of density estimation trees is presented in figure [ fig : overtraining ] . in the presented dataset",
    ", the alignment of data - entries in one of the input variables is interpreted as narrow spikes . to compensate spikes , in terms of absolute normalization",
    ", the density is underestimated in all of the other points of the parameter space .",
    "overtraining in decision trees is controlled through an iterative approach consisting in _ pruning _ and _ cross - validation _ : finding and removing the branches increasing the complexity of the tree without enhancing the statistical agreement with a set of test samples .",
    "cross - validation is very expensive in terms of computing power and often fails to identify problems of over - training arising close to the root of the decision tree .",
    "overtraining in density estimation is fought , instead , by defining _ a priori _ an expected resolution width , neglecting fluctuations under that resolution while building the statistical model .",
    "for example , kernel density estimation algorithms require a parameter , named _ bandwidth _ as an input .",
    "the bandwidth is related to the width of the kernel function .",
    "abundant literature exists on techniques to optimise the bandwidth for a certain dataset , most of them represent a preliminary step of the density estimation algorithm . growing a density estimation tree with a minimal leaf width is fast , does nt require post - processing and it is found to result in better - quality estimations with respect to cross - validation procedures . the same techniques used to compute the optimal bandwidth parameter for kernel density estimation algorithms can be used to define the optimal minimal leaf width of the density estimation tree .      as mentioned in the introduction , fast integration of the statistical model built",
    "using density estimation trees is one of the strengths of the algorithm .",
    "integration usually responds to two different needs : _ normalization _ and _ slicing _ ( or _ projecting _ , or _ marginalizing _ ) .",
    "integrals to compute the overall normalization of the density estimation , or the contribution in a large fraction of the data - space , gain little from exploiting the tree structure of the density estimator .",
    "a sum over the contributions of each leaf represents the best strategy .",
    "instead , integrals over a narrow subset of the data - space should profit of the tree structure of the density estimation to exclude from the integration domain as many leaves as possible , as early as possible . exploiting the tree structure when performing integrals of slices",
    "can drastically reduce the computing time in large density estimation trees .",
    "combining weighted density estimation trees can be useful to model data samples composed of two or more components .",
    "combination is achieved implementing both scalar and binary operations .",
    "there is not much to discuss about scalar operations , where the scalar operation is applied to each leaf independently .",
    "instead , binary operations require the combination of two different density estimation trees , which is not trivial because the boundaries are _ a priori _ different .",
    "the algorithm to combine two density estimation trees consists of the iterative splitting of the terminal nodes of the first tree , following the boundaries of the terminal nodes of the second one .",
    "once the combination is done , the first tree is compatible with the second one and the binary operation can be performed node per node .",
    "the resulting tree may have several additional layers with respect to the originating trees , therefore a final step removing division between negligibly different nodes is advisable .",
    "density estimation trees are useful to approach problems defined by many variables and for which huge statistical samples are available . to give a context to the following examples of applications , i consider the calibration samples for the particle identification ( pid ) algorithms at the lhcb experiment .",
    "pid calibration samples are sets of decay candidates reconstructed and selected relying on kinematic variables only , to distinguish between different types of long - lived particles : electrons , muons , pions , kaons , and protons .",
    "the pid strategy of the lhcb detector relies on the combined response of several detectors : two ring cherenkov detectors , an electromagnetic calorimeter , a hadronic calorimeter and a muon system @xcite .",
    "the response of the single detectors are combined into likelihoods used at analysis level to define the tightness of the pid requirements .",
    "calibration samples count millions of background - subtracted candidates , each candidate is defined by a set of kinematic variables , for example momentum and pseudorapidity , and a set of pid likelihoods , one per particle type .",
    "the correlation between all variables is important and not always linear .",
    "the first application considered is the construction of tables defining the probability that the pid likelihood of a candidate , defined by a set of kinematic variables , satisfies a particular requirement .",
    "building two density estimation trees with the kinematic variables defining the data - space , one with the full data sample ( tree @xmath15 ) , and one with the portion of data sample passing the pid criteria ( tree @xmath16 ) , allows to compute the efficiency for each combination of the kinematic variables by evaluating the density estimation tree obtained taking the ratio @xmath17 .    for frequently - changing criteria a dynamic determination of the efficiency can be envisaged . for simplicity , consider the generic univariate pid criterion @xmath18 . in this case",
    "a single density estimation tree @xmath19 defined by the kinematic variables @xmath20 , and one pid variable @xmath21 , has to be trained on the calibration sample . the dynamic representation of the efficiency for a candidate having kinematic variables @xmath22 is the ratio @xmath23    thanks to the fast slice - integration algorithm , the computation of this ratio can be included in an iterative optimization procedure aiming at an optimization of the threshold on @xmath21 .",
    "another important application is related to fast simulation of hep events .",
    "full simulation , including interaction of the particles with matter , is becoming so expensive to be expected exceeding the experiments budgets in the next few years .",
    "parametric simulation is seen as a viable solution , as proved by the great interest raised by the delphes project @xcite . however , parametrizing a simulation presents the same pitfalls as parametrizing a density estimation : when correlation among different variables becomes relevant , the mathematical form of the parametrization increases in complexity up to the point it becomes unmanageable .",
    "density estimation trees are an interesting candidate for non - parametric fast simulation .",
    "let @xmath24 be a density estimation tree trained on a set of candidates defined by _",
    "variables @xmath25 , and by variables @xmath26 obtained through full simulation .",
    "for example , @xmath27 could represent the kinematic variables of a track and @xmath28 the pid likelihoods .",
    "the aim of fast non - parametric simulation is , given a new set of values @xmath29 for @xmath27 , to compute a set of values for @xmath28 distributed according to the conditional probability density function @xmath30 .",
    "once the det is trained , the tree structure of the density estimator is used to compute for each leaf @xmath6 the hyper - volume @xmath31 of the intersection between @xmath6 and hyper - plane defined by @xmath32 .",
    "a random leaf @xmath33 is then chosen with probability proportional to @xmath34 , and variables @xmath28 are generated following a flat distribution bounded within @xmath35 .",
    "a set of _ generator _",
    "variables @xmath36 can then be completed by the corresponding @xmath37 variables without full simulation , but relying on the joint multivariate distribution learnt by the density estimation tree .",
    "i discussed density estimation tree algorithms as fast modelling tools for high statistics problems characterized by a large number of correlated variables and for which an approximated model is acceptable .",
    "the fast training and integration capabilities make these algorithms of interest for the high - demanding future of the high - energy physics experiments .",
    "the examples discussed , which benefited from an active discussion within the particle identification group of the lhcb collaboration , explore cases where the statistical features of huge samples have to be assessed in a time shorter than what standard estimators would require . in future , density estimation trees",
    "could be sampled to train regression multivariate algorithms , such as neural networks , in order to smooth the response and further speed up the query time , at the cost of loosing its fast - integration properties .",
    "i thank alberto cassese , anton poluektov , and marco cattaneo for the encouragements in developing this work and for the useful discussions we had .",
    "9 k.  s.  cranmer , comput .",
    "phys .  commun .",
    "* 136 * ( 2001 ) 198 doi:10.1016/s0010 - 4655(00)00243 - 5 [ hep - ex/0011057 ] .",
    "j.  l.  bentley , communications of the acm * 18 * ( 1975 ) 9 doi:10.1016/s0010 - 4655(00)00243 - 5"
  ],
  "abstract_text": [
    "<S> a density estimation tree ( det ) is a decision trees trained on a multivariate dataset to estimate the underlying probability density function . while not competitive with kernel techniques in terms of accuracy , dets are incredibly fast , embarrassingly parallel and relatively small when stored to disk . </S>",
    "<S> these properties make dets appealing in the resource - expensive horizon of the lhc data analysis . </S>",
    "<S> possible applications may include selection optimization , fast simulation and fast detector calibration . in this contribution </S>",
    "<S> i describe the algorithm and its implementation made available to the hep community as a roofit object . </S>",
    "<S> a set of applications under discussion within the lhcb collaboration are also briefly illustrated . </S>"
  ]
}