{
  "article_text": [
    "in this paper we consider optimization in matrices with orthogonality constraints , @xmath0 where @xmath1 is a differentiable function and @xmath2 is the identity matrix .",
    "the feasible set @xmath3 is known in the literature as the `` stiefel manifold '' which is reduced to the unit sphere when @xmath4 and in the case @xmath5 it is known as `` orthogonal group '' ( see @xcite ) .",
    "it is known that the dimension of the stiefel manifold is @xmath6 @xcite , and it can be seen as an embedded sub - manifold of @xmath7 .",
    "problem ( [ problem ] ) encompasses many applications such as nearest low - rank correlation matrix problem @xcite , linear eigenvalue problem @xcite , kohn - sham total energy minimization @xcite , orthogonal procrustes problem @xcite , weighted orthogonal procrustes problem @xcite , sparse principal component analysis @xcite , joint diagonalization  ( blind source separation ) @xcite , among others .",
    "in addition , many problems such as pca , lda , multidimensional scaling , orthogonal neighborhood preserving projection can be formulated as problem ( [ problem ] ) @xcite . on the other hand , it is known that stiefel manifold is a compact set which guarantees a global optimum , nevertheless it s not a convex set which makes problem - solving ( [ problem ] ) very hard .",
    "in particular , the _ quadratic assignment problem _ ( qap ) , which can be formulated as minimization over a permutation matrix in @xmath8 , that is , @xmath9 , @xmath10 and `` leakage interference minimization '' @xcite are np - hard .",
    "+ the problem ( [ problem ] ) can be treated as a regular ( vector ) optimization problem , which conduces to several difficulties because orthogonal constraints can lead to many local minimizers . on the other hand ,",
    "generating sequences of feasible points is not easy because preserving orthogonality constraints is numerically expensive .",
    "most existing algorithms that generate feasible points either use routines to matrix reorthogonalization or generate points along geodesics on the manifold .",
    "the first one requires singular value decomposition or qr - decomposition , and the second one computes the matrix exponential or solves partial differential equations .",
    "we shall use the first approach . some effort to avoid these approaches have been done , calculating inverse matrices instead of svd s at each step @xcite .",
    "+ by exploiting properties of the stiefel manifold , we propose a non - monotone line search constraint preserving algorithm to solve problem ( [ problem ] ) with a mixed gradient based search direction .",
    "we analyze the resulting algorithm , and compare it to other state - of - the - art procedures , obtaining promising numerical results .",
    "we say that a matrix @xmath11 is skew - symmetric if @xmath12 .",
    "the trace of @xmath13 is defined as the sum of the diagonal elements which we will denote by @xmath14 $ ] .",
    "the euclidean inner product of two matrices @xmath15 is defined as @xmath16 $ ] .",
    "the frobenius norm is defined by the above inner product , that is @xmath17 .",
    "let @xmath18 be a differentiable function , we denote by @xmath19 the matrix of partial derivatives of @xmath20 .",
    "when evaluated at the current iterate @xmath21 of the algorithm , we denote the gradient by @xmath22 .",
    "other objects depending on @xmath21 are also denoted with subindex @xmath23 .",
    "additionally , the directional derivative of @xmath20 along a given matrix @xmath24 at a given point @xmath13 is defined by : @xmath25 : = \\lim_{t\\rightarrow 0}\\frac{\\mathcal{f}(x+tz ) - \\mathcal{f}(x)}{t } = \\langle \\mathcal{df}(x),z \\rangle . \\label{derivdireccional}\\end{aligned}\\ ] ]      the rest of the paper is organized as follows . in section 2",
    "we propose and analyze our mixed constraint preserving updating scheme .",
    "subsubsection 2.1 establishes the optimality conditions , while subsection 2.2 is devoted to the projection operator on the stiefel manifold .",
    "the proposed updating scheme is introduced and analyzed in subsection 2.3 .",
    "two algorithms are presented in section 3 .",
    "the first one uses armijo line search , and the second one barzilai - borwein stepsize scheme .",
    "finally , numerical experiments are carried out for comparing our algorithm with other state - of - the - art procedures by solving instances of weighted orthogonal procrustes problem ( wopp ) , total energy minimization , and linear eigenvalue problems , are presented in section 4 .",
    "the classical constrained optimization theory for continuous functions in @xmath26 involve finding minimizers of the lagrangean function applied to problem ( [ problem ] ) , given by : @xmath27 , \\label{funlagrange } \\nonumber\\end{aligned}\\ ] ] where @xmath28 is the lagrange multipliers matrix , which is a symmetric matrix because the matrix @xmath29 is also symmetric .",
    "the lagrangean function leads to the first order optimality conditions for problem ( [ problem ] ) :    @xmath30    @xmath31    by differentiating both sides of ( [ kkt2 ] ) , we obtain the tangent space of stiefel manifold at @xmath13 : @xmath32    the following technical result , demonstrated by wen - yin in @xcite , provides a tool to calculate roots for eqs .",
    "( [ kkt1])-([kkt2 ] ) .",
    "[ lema1 ] suppose that @xmath13 is a local minimizer of problem ( [ problem ] )",
    ". then @xmath13 satisfies the first order optimality conditions with the associated lagrangian multiplier @xmath33 .",
    "define @xmath34    then @xmath35 .",
    "moreover , @xmath36 if and only if @xmath37 .",
    "the lemma [ lema1 ] establishes an equivalence to the eq .",
    "( [ kkt1 ] ) condition , i.e. @xmath38 satisfies @xmath39 if and only if @xmath13 satisfies equation ( [ kkt1 ] ) , so we can use this result as a stopping criterion for our algorithm .",
    "+      the following lemma establishes an important property for matrices on stiefel manifold .",
    "[ lema_normfro ] if @xmath40 then @xmath41 for all @xmath42 matrix .    in fact ,",
    "let @xmath40 and @xmath42 , and let @xmath43 , @xmath44 be the singular value decomposition of @xmath45 and @xmath46 respectively , where @xmath47 and @xmath48 are diagonal matrices .",
    "we denote by @xmath49 . since @xmath50 is a orthogonal matrix , i.e. , @xmath51 then ,    @xmath52    we denote by @xmath53 the matrix obtained from @xmath54 by extracting the last @xmath55 rows , consequently , @xmath56 .",
    "now , we have ,    @xmath57 \\label{prop2_eq7 } \\nonumber \\\\                            & \\leq & \\sum_{j=1}^{p}(\\sigma_j^{m})^2(\\sum_{i=1}^{n}q_{ij}^2 ) \\label{prop2_eq8 } \\\\                            & = & \\sum_{j=1}^{p}(\\sigma_j^{m})^2 \\label{prop2_eq9 } \\\\                            & = & ||m||_f^{2}. \\label{prop2_eq10 } \\nonumber\\end{aligned}\\ ] ]    the second line of ( [ prop2_eq5 ] ) is obtained by using the fact that the frobenius norm is invariant under orthogonal transformations , the third line ( [ prop2_eq6 ] ) is obtained by using that @xmath58 because @xmath9 and the sixth line ( [ prop2_eq9 ] ) is followed by using ( [ prop2_eq3 ] ) in ( [ prop2_eq8 ] ) .",
    "thus , we conclude that    @xmath59    another tool we shall employ is the projector operator on stiefel manifold",
    ". first we define it and then provide a characterization , shown in @xcite , in terms of the singular value decomposition of the underlined matrix .",
    "let @xmath60 be a rank @xmath61 matrix .",
    "the projection operator @xmath62 is defined to be @xmath63    the following proposition , demonstrated in @xcite , provides us an explicit expression of the projection operator @xmath64 .",
    "let @xmath60 be a rank @xmath61 matrix .",
    "then , @xmath65 is well defined . moreover , if the svd of @xmath13 is @xmath66 , then @xmath67 .",
    "in @xcite it is presented an algorithm which adapts the well known steepest descent algorithm to solve problem ( [ problem ] ) .",
    "the ingredients of the updating formula are the derivative of the lagrangean function , the projection operator ( [ projection ] ) and the step size choice by means of armijo .",
    "the direction in ( [ direction ] ) is in the tangent space of the manifold at the current point . with the intuition of perturbing the steepest descent direction , we propose a modification of manton s procedure by a mixture of tangent directions which incorporates @xmath68 .",
    "note that if @xmath69 is a local minimizer of the problem ( [ problem ] ) then @xmath70 .",
    "since both of the directions belong to @xmath71 , then the obtained mixture is also in this space .",
    "this mean that the obtained algorithm preserve the matrix structure of the problem , instead of using optimization in @xmath26 .",
    "+ in this paper we focus on a modification of the projected gradient - type methods : given a feasible point @xmath13 , we compute the new iteration @xmath72 as a point on the curve : @xmath73 where the term @xmath74 represents the step size . the direction @xmath75 is defined by : @xmath76 where @xmath77 , @xmath78 , @xmath79 is defined as in lemma [ lema1 ] and @xmath80 is given by : @xmath81    note that @xmath82 , and the new trial point @xmath72 satisfies the orthogonality constraints of the problem ( [ problem ] ) because @xmath83 is a curve on the manifold .",
    "proposition [ prop1 ] below shows that direction @xmath75 belongs to the tangent space of the manifold at the point @xmath13 .",
    "[ prop1 ] the direction matrix defined in ( [ direction ] ) belongs to the tangent space of @xmath84 at @xmath13 .",
    "we must prove that : @xmath85 .",
    "for this , we prove that : @xmath86 and @xmath87 .",
    "in fact , by using @xmath88 , we have , @xmath89 and due to the feasibility of @xmath13 ( @xmath90 ) we obtain @xmath91 consequently @xmath92 and @xmath93 belongs to @xmath71 . since @xmath94 is a vector space , we have the linear combination of @xmath92 , and @xmath93 also belongs to @xmath94 , concluding that @xmath85 .",
    "the following proposition shows that the curve @xmath83 defines a descent direction for @xmath77 .",
    "@xmath95 values next to zero provide bad mixed directions .",
    "[ prop2 ] if @xmath77 and @xmath96 then @xmath83 is a descent curve at @xmath97 , that is @xmath98 = \\frac{\\partial \\mathcal{f}(x+\\tau \\dot{z}(0))}{\\partial \\tau } \\big|_{\\tau = 0 } \\leq   - \\frac{\\alpha}{2}||a||_f^{2 }   < 0 . \\nonumber\\ ] ]    we begin by calculating the derivative of the curve @xmath83 at @xmath99 . from taylor s second order approximation in stiefel manifold ( prop . 12 in @xcite ) , if @xmath40 and @xmath100 then : @xmath101 so , deriving ( [ prop12 ] ) with respect to @xmath102 , and evaluating at @xmath97 , @xmath103 it follows from this fact , and our update formula @xmath83 that , @xmath104 now , from the definition ( [ derivdireccional ] ) and using trace properties we have , @xmath105 & = & -tr[g^{\\top}h ] \\nonumber \\\\ & = & - \\alpha tr[g^{\\top}a x ] - \\beta tr[g^{\\top}(g - xx^{\\top}g ) ] \\nonumber \\\\ & = &   - \\frac{\\alpha}{2}||a||_f^{2 } - \\beta||g||_f^{2 } + \\beta tr([(x^{\\top}g)^{\\top}x^{\\top}g ] \\nonumber \\\\ & = & - \\frac{\\alpha}{2}||a||_f^{2 } - \\beta||g||_f^{2 } + \\beta ||x^{\\top}g||_f^{2 } , \\label{prop2_eq2 } \\nonumber\\end{aligned}\\ ] ]    since @xmath77 and using lemma [ lema_normfro ] we arrive at : @xmath105 & \\leq & - \\frac{\\alpha}{2}||a||_f^{2 }   \\nonumber \\\\                                & < & 0 , \\nonumber\\end{aligned}\\ ] ] which completes the proof .    from proposition",
    "[ prop1 ] , we obtain that the mapping @xmath83 defined in ( [ update_formulae ] ) is a retraction on the stiefel manifold , see @xcite .",
    "hence , the convergence results in @xcite regarding retractions apply directly to algorithm [ alg1 ] .",
    "it is well known that the steepest descent method with a fixed step size may not converge . however , by choosing the step size wisely , convergence can be guaranteed and its speed can be accelerated without significantly increasing of the cost at each iteration . at iteration @xmath23 , we can choose a step size by minimizing @xmath106 along the curve @xmath107 with respect to @xmath102 .",
    "since finding its global minimizer is computationally expensive , it is usually sufficient to obtain an approximated minimum , in order to deal with this , we use the armijo condition @xcite to select a step size : @xmath108 \\label{armijo1 } \\nonumber\\ ] ] with @xmath109 + our approximated monotone procedure is resumed in algorithm [ alg1 ] .",
    "@xmath110 , @xmath111 , @xmath112 , @xmath113 , @xmath114.[lin : linearara ] @xmath115 a local minimizer .",
    "@xmath116 , @xmath117 , @xmath118 , @xmath119 , @xmath120      we propose a variant of algorithm [ alg1 ] , instead of using armijo condition , we acoplate barzilai borwein ( bb - step ) step size , see @xcite , which sometimes improve the performance of linear search algorithms such as the steepest descent method without adding a lot of extra computational cost .",
    "this technique considers the classic updating of the line search methods : @xmath121 where @xmath122 is the gradient of the objective function , and @xmath95 is the step size .",
    "this approach ( barzilai borwein step size ) proposes as the step size , the value @xmath95 that satisfies the secant equation : @xmath123 or well , @xmath124 where @xmath125 , @xmath126 and the matrix @xmath127 , is considered an approximation of the hessian of the objective function , so the step size @xmath95 is obtained by forcing a quasi - newton property .",
    "it follows from eqs .",
    "( [ bb1])-([bb2 ] ) that , @xmath128}\\quad   \\textrm{and } \\quad    \\alpha_k^{bb2 } = \\frac{tr[s_k^{\\top}r_k]}{||r_k||_f^2}. \\label{bb - steps}\\ ] ] since the quantities @xmath129 and @xmath130 can be negatives , it is usually taken the absolute value of any of these step sizes .",
    "on the other hand , the bb steps do not necessarily guarantee the descent of the objective function at each iteration , this may imply that the method does not converge . in order to solve this problem",
    "we will use a globalization technique which guarantees global convergence on certain conditions @xcite , that is , we use a non - monotone linear search described as in @xcite . from the above considerations we arrive at the following algorithm :    @xmath110 , @xmath74 , @xmath131 , @xmath132 , @xmath133 , k=0 .",
    "[ lin : linearara ] @xmath115 a local minimizer .",
    "@xmath117 , @xmath118 , @xmath134and @xmath135 , choose @xmath136 or @xmath137 with @xmath138 defined as in ( [ bb - steps ] ) @xmath139 , @xmath119 , @xmath120    note that when @xmath140 the algorithm [ alg2 ] becomes algorithm [ alg1 ] with bb - step .",
    "moreover , when we select the parameters @xmath141 and @xmath142 , we obtain a procedure very similar to the `` modified steepest descent method '' ( _ msdstifel _ ) proposed by manton in @xcite , however , in this case , our algorithm [ alg2 ] is an accelerated version of _ msdstifel _ , since it incorporates a non - monotone search combined with the bb - step , which usually accelerates the gradient - type methods , whereas the algorithm presented in @xcite , uses a double backtracking strategy to estimate the step size , that in practice is very slow since requires more computing .",
    "+ in addition , in our implementation of the algorithm [ alg2 ] , we update @xmath143 by the approximation ( [ prop12 ] ) , specifically , we calculate @xmath144 , with @xmath145 as in ( [ direction ] ) , and in case the feasibility error is sufficiently small , that is , @xmath146 1e-13 this point is accepted and we update the new point by @xmath147 ; otherwise , we update the new trial point by @xmath148 where @xmath149 = \\verb\"svd\"(x_k - \\tau_k h_k , 0)$ ] using matlab notation .",
    "note that if the step size @xmath150 is small , or if the sequence @xmath151 approaches a stationary point of the lagrangian function ( [ funlagrange ] ) then ( [ prop12 ] ) closely approximates the projection operator , in view of this , in several iterations our algorithm may saves the svd s computation . + all these details",
    "make our algorithm [ alg2 ] into a quicker and improved version of the _ msdstifel _ algorithm , for the case when the parameters @xmath141 and @xmath142 ; and in the case when we take another different selection of these parameters our method incorporates a mixture of descent directions that has shown to be a better direction that the one used by manton , in some cases . in the section of experiments ( see table [ tab:2 ] ) , we compare our algorithm [ alg2 ] ( with @xmath152 and @xmath142 ) against _ msdstifel _ , in this experiment it is clearly shown that our algorithm is much faster and efficient than the one proposed by manton .",
    "in this section , we will study the performance of our approaches to different optimization problems with orthogonality constraints , and we show the efficiency and effectiveness of our proposals on these problems .",
    "we implemented both algorithms 1 and 2 in matlab . since algorithm 2 appears to be more efficient in most test sets , we compare both algorithms on the first test set in subsection [ subsec:7 ] and compare only algorithm 2 with other two state - of - the - art algorithms on problems in the remaining test . in all experiments presented in upcoming subsections , we used the default parameters given by the author s implementations solvers for the abovementioned algorithms .",
    "we specify in each case the values of @xmath95 and @xmath153 for the mixed direction .",
    "when @xmath154 and @xmath155 , our direction coincide with manton s direction , and for this case , the difference in the procedures is established by the line search .",
    "the experiments were ran using matlab 7.10 on a intel(r ) core(tm ) i3 , cpu 2.53 ghz with 500 gb of hd and 4 gb of ram .      in our implementation",
    ", we are checking the norm of the gradient , and monitoring the relative changes of two consecutive iterates and their corresponding objective function values : @xmath156    then , given a maximum number k of iterations , our algorithm will stop at iteration @xmath23 if @xmath157 or @xmath158 , or @xmath159 and @xmath160 , or @xmath161 ) \\leq 10tolx \\quad and \\quad \\textrm{mean } ( [ \\textrm{rel}_{1+k - min(k , t)}^{f},\\ldots,\\textrm{rel}_{k}^{f } ] ) \\leq 10tolf \\nonumber\\ ] ]",
    "the default values for the parameters in our algorithms are : @xmath162 1e-4 , @xmath163 1e-6 , @xmath164 1e-12 , @xmath165 , @xmath166 , @xmath167 , @xmath168 1e-4 , @xmath169 1e-20 , @xmath170 1e+20 and @xmath171 . + in the rest of this section we will denote by : `` nitr '' to the number of iterations , `` nfe '' to the number of evaluations of the objective function , `` time '' to cpu time in seconds , `` fval '' to the value of the evaluated objective function in the optimum estimated , `` nrmg '' to the norm of gradient of the lagrangean function evaluated in the optimum estimate ( @xmath172 ) and `` feasi '' to the feasibility error ( @xmath173 ) , obtained by the algorithms .      in this subsection , we consider the weighted orthogonal procrustes problem ( wopp ) @xcite , which is formulated as follows : @xmath174 where @xmath175 , @xmath176 and @xmath177 are given matrices . + for the numerical experiments we consider @xmath178 , @xmath179 , @xmath180 and @xmath181 , where @xmath182 and @xmath183 are random orthogonal matrices , @xmath184 is a householder matrix , @xmath185 is a diagonal matrix with elements uniformly distributed in the interval @xmath186 $ ] and @xmath187 is a diagonal matrix defined for each type of problem . as a starting point @xmath188 we randomly generate the starting points by the built - in functions @xmath189 and @xmath190 : @xmath191 = \\verb\"svd\"(\\overline{x},0 ) , \\quad x_{0 } = uv^{\\top}. \\nonumber\\ ] ] when it is not specified how the data were generated , it was understood that they were generated following a normal standard distribution . + against the exact solution of the problem , we create a known solution @xmath192 by taking @xmath193 .",
    "the tested problems were taken from @xcite and are described below .",
    "+ * problem 1 * : the diagonal elements of @xmath187 are generated by a normal truncated distribution in the interval [ 10,12 ] . + * problem 2 * : the diagonal of @xmath187 is given by @xmath194 , where @xmath195 are random numbers uniformly distributed in the interval [ 0,1 ] . + * problem 3 * : each diagonal element of @xmath187 is generated as follows : @xmath196 , with @xmath195 uniformly distributed in the interval [ 0,1 ] .",
    "+    note that when the matrix @xmath187 is generated following problem 1 then the matrix @xmath197 has a small condition number , and when @xmath187 is generated following problems 2 and 3 , the matrix @xmath197 has a big condition number , which becomes bigger as @xmath198 grows . in view of this , in the remainder of this subsection , we will refer to * problem 2 * and * problem 3 * as ill conditioned wopp s problems and to * problem 1 * as well conditioned problems .",
    "+ first of all , we perform an experiment in order to calibrate the parameters ( @xmath95,@xmath153 ) . to make the study more tractable",
    ", we consider the direction @xmath145 as a convex combination of @xmath199 and @xmath200 , that is , we put @xmath201 with @xmath202 $ ] .",
    "more specifically , we randomly generated one problem of each type , as explained above ( * problem 1 * , * problem 2 * and * problem 3 * are generated ) selecting @xmath203 and @xmath204 , and each one is solved for the following alpha values : @xmath205.@xmath206 ( using matlab notation ) .",
    "figure [ fig:0 ] shows the curves of the iterations versus each alpha value , and for each type of problem . in this figure",
    "we see that for well - conditioned problems ( * problem 1 * ) our algorithm obtains the best result when it uses @xmath207 , converging in a time of 0.1388 seconds and with gradient norm of @xmath208 . in this plot",
    "it seems that @xmath209 also produces good results because it performs very few iterations , however the algorithm gets a bad result in terms of nrmg . on the other hand",
    ", we see that for ill - conditioned problems ( * problem 2,3 * ) the algorithm [ alg2 ] obtains better results when @xmath95 approaches 1 , in particular in the plot we notice that with @xmath141 less iterations are done .",
    "+ from this experiment and our experience testifying our algorithm for different values of @xmath210 , we note that for well - conditioned wopp problems , our algorithm tends to perform better with values of @xmath210 close to 0.5 ; whereas for wopp ill - conditioned problems our procedure shows best results when @xmath210 is close to 1 .",
    "however , it will remain as future work to study the performance of our method for the case when the direction @xmath145 is taken as a linear combination instead of a convex combination of @xmath211 and @xmath212 .",
    "+ it is worth mentioning that deciding which set of parameters to use to obtain a good performance of our algorithm is not an easy task to predict , since in general these parameters will depend to a great extent on the objective function and the good or ill conditioning of the problem .",
    "a strategy to select these parameters could be to use some metaheuristic that optimize these parameters for a specific problem or application .",
    "however , based on these experiments and in our numerical experience running our algorithm , we will use for the following subsections @xmath213 and we will take alpha in the set @xmath214.@xmath215 or some number close to 1 , because this choice usually reach good results .",
    "+ in the second experiment , we will test the efficiency of the non - monotone line search , by making a comparison between the * algorithm 1 * and the * algorithm 2*. in the table  [ tab:1 ] we present the results of this comparison , in this experiment we choose @xmath216 in both algorithms .",
    "we show the minimum ( min ) , maximum ( max ) and the average ( mean ) of the results achieved from @xmath217 simulations . according to the number of iterations a small difference in favor to algorithm 1 can be observed .",
    "however , in terms of cpu time , the dominance of algorithm 2 is overwhelming .",
    "this conclusion is also appreciated in figure [ fig:1 ] . in view of this , for the remaining experiments we only compare our algorithm 2 , which we hereafter refer as * grad - retract * , to other state of the art procedures , namely edelman , manton , wen - yin @xcite .",
    "+    in the table  [ tab:2 ] , we compare our * grad - retrac * versus the modified steepest descent method ( msdstiefel ) proposed in @xcite . more specifically ,",
    "we compare the performance of these methods solving wopp s .",
    "we create the matrices @xmath197 , @xmath218 and @xmath219 as explained at the beginning of this subsection and generate the matrix @xmath220 randomly with their entries uniformly distributed in the range [ 0,1 ] .",
    "a total of 30 simulations were run .",
    "we see in table [ tab:2 ] that the performance of our * grad - retrac * is consistently better than the performance of the algorithm proposed by manton @xcite , in terms of number of iterations , and cpu time .",
    "both of the procedures solve all proposed test problems .",
    "+ for all experiments presented in tables [ tab:3]-[tab:5 ] , we compare our algorithm * grad - retrac * with the methods * pgst * , * optstiefel * proposed in @xcite , @xcitewotaoyin / papers / feasible_method_matrix_manifold.html ] respectively , and we use as tolerance @xmath162 1e-5 and as maximum number of iterations @xmath221 for all methods .",
    "in addition , in the experiments shown in table [ tab:3 ] we choose @xmath222 ( see eq .  [ direction ] ) , while for the experiments shown in tables [ tab:4]-[tab:5 ] , we select : @xmath141 , and @xmath142 in the descent direction of our method . for each value to compare in these tables",
    ", we show the minimal ( min ) , maximum ( max ) end the average ( mean ) of the results achieved from 30 simulations .",
    "+ table [ tab:3 ] include numerical results for well conditioned wopp s of different sizes .",
    "we observe from this table that all methods converge to good solutions , while our method ( * grad - retrac * ) always performs much better than * pgst * and * optstiefel * in terms of the cpu time , except on orthogonal group problems ( see table ex.3 and ex.4 in table [ tab:3 ] ) , where * optstiefel * method shows better performance .",
    "tables [ tab:4]-[tab:5 ] presents the results achieved for all methods solving wopp s in presence of ill conditioning . in these tables",
    ", we see that * grad - retrac * and * optstiefel * algorithms show similar performance in most experiments , while the pgst method exhibit the worst performance in all cases examined here , however all methods achieve good solutions .",
    "+    in figure [ fig:2 ] , we give the convergence history of each method in the wopp s problems shown in the tables [ tab:3]-[tab:5 ] .",
    "more specifically , we depict the average assessments of objective function as well as the average gradient norm for ex.5 , ex.9 and ex.15 experiments . in these charts",
    ", we observe that in well conditioned problems ( see figure [ fig:2](a)-(b ) ) the method * pgst * converge faster than the other , but in the presence of ill conditioning , our method is faster . in addition , in all charts , * optstiefel * an our method showed similar performance .",
    "for next experiments we consider the following total energy minimization problem : @xmath223 + \\frac{\\mu}{4}\\rho(x)^{\\top}l^{\\dag}\\rho(x ) \\quad s.t .",
    "\\quad x^{\\top}x = i_k \\label{te_problem } \\nonumber\\ ] ] where @xmath224 is a discrete laplacian operator , @xmath225 is a given constant , @xmath226 is the vector containing the diagonal elements of the matrix @xmath227 and @xmath228 is the moore - penrose generalized inverse of matrix @xmath224 .",
    "the total energy minimization problem ( [ te_problem ] ) is a simplified version of the hartreefock ( hf ) total energy minimization problem and the kohn - sham ( ks ) total energy minimization problem in electronic structure calculations ( see for instance @xcite ) .",
    "the first order necessary conditions for the total energy minimization problem ( [ te_problem ] ) are given by : @xmath229 where the diagonal matrix @xmath28 contains the @xmath23 smallest eigenvalues of the symmetric matrix @xmath230 . the symbol @xmath231 is a diagonal matrix with a vector @xmath232 on its diagonal .",
    "+ for examples 6.1 - 6.4 below taken from @xcite , we repeat our experiments over 100 different starting points , moreover , we use a tolerance of @xmath162 1e-4 and a maximum number of iterations @xmath166 . to show the effectiveness of our method over the problem ( [ te_problem ] ) , we report the numerical results for examples 6.1 - 6.4 with different choices of @xmath233 , @xmath23 , and @xmath234 , and we compare our method with the steepest descent method ( * steep - dest * ) , trust - region method ( * trust - reg * ) and conjugate gradient method ( * conj - grad * ) from `` _ _ manopt _ _ '' toolbox .",
    "+ : we consider the nonlinear eigenvalue problem for different choices of @xmath233 ; @xmath23 ; @xmath234 : ( a ) @xmath235 ; @xmath236 ; @xmath237 ; ( b ) @xmath238 ; @xmath239 ; @xmath240 ; ( c ) @xmath241 ; @xmath242 ; @xmath243 ; ( d ) @xmath241 ; @xmath244 ; @xmath245 .",
    "+ : we consider the nonlinear eigenvalue problem for different choices of @xmath233 ; @xmath23 ; @xmath234 : ( a ) @xmath235 ; @xmath236 ; @xmath246 ; ( b ) @xmath238 ; @xmath239 ; @xmath237 ; ( c ) @xmath241 ; @xmath242 ; @xmath247 ; ( d ) @xmath241 ; @xmath244 ; @xmath248 .",
    "+ : we consider the nonlinear eigenvalue problem for @xmath242 ; @xmath247 , and varying @xmath249 .",
    "+ : we consider the nonlinear eigenvalue problem for @xmath241 and @xmath250 and varying @xmath234 .",
    "+ in all these experiments , the @xmath224 matrix in the problem ( [ te_problem ] ) is constructed as the one - dimensional discrete laplacian with @xmath251 on the diagonal and @xmath252 on the sub- and sup - diagonals . furthermore ,",
    "for all these experiments we select : @xmath253 , and @xmath254 in the descent direction of our method .",
    "+ the results for the example 6.1 and example 6.2 are shown in tables [ tab:6]-[tab:7 ] .",
    "we see from these tables that our method ( * grad - retrac * ) is more efficient than the other methods from `` _ _ manopt _ _ ''",
    "toolbox in terms of cpu time .",
    "table [ tab:8 ] gives numerical results for example 6.3 .",
    "we see from table [ tab:8 ] that in almost all cases , our method is more efficient than the other in terms of cpu time . only in experiments ex.11 and ex.12 ( see table [ tab:8 ] ) , the conjugate gradient method from `` _ _ manopt _ _ '' library gets better performance than our method slightly .",
    "in addition , all algorithms take longer to converge as it increases the value of @xmath233 , nevertheless , when the problems are larger size , the increase in the convergence time of the * conj - grad * and * grad - retrac * methods is much less than the of the other methods .",
    "+    table [ tab:9 ] lists numerical results for example 6.4 . in this table",
    ", we note that our method gets its best performance when it used the smaller size of @xmath234 , we also observe that our method is more efficient than the other in all of the experiments list in this table .",
    "given a symmetric matrix @xmath255 and let @xmath256 be the eigenvalues of @xmath197 .",
    "the @xmath61-largest eigenvalue problem can be formulated as : @xmath257 \\quad s.t .",
    "\\quad x^{\\top}x = i \\nonumber\\ ] ]    in this subsection , we compared algorithm 2 with the * sgmin * algorithm proposed in @xciteripper / www / sgmin.html ] and the * optstiefel * proposed in @xcite . in this case , @xmath258 , so , our direction coincide with manton s direction . in all experiments presented in this subsection , we generate the matrix @xmath197 as follows : @xmath259 , where @xmath260 is a matrix whose elements are sampled from the standard gaussian distribution . the tables [ tab:10]-[tab:11 ] shows the average ( mean ) of the results achieved for each value to compare from 100 simulations , in addition , we used 1000 by the maximum number of iterations and @xmath162 1e-5 as tolerance for each algorithm .",
    "for all methods to compare , we use the four stop criteria presented in subsection [ subsec:6 ] in this tables , _ error _ denotes the relative error between the objective values given by _",
    "eigs _ function of matlab and the objective values obtain by each algorithm , i.e. @xmath261 |}{|tr[x_{est}^{\\top}ax_{est}]| } \\nonumber\\ ] ] where @xmath262 is the @xmath263-largest eigenvalue of @xmath197 calculated using the _ eig _ function of matlab , and @xmath264 denotes the estimated local optima for each algorithm . + the results corresponding to varying @xmath61 but fixed @xmath265 are presented in table [ tab:10 ] , from these results we can observe that * optstiefel * and our * grad - retrac * are much more efficient methods that * sgmin*. moreover , we observe that * optstiefel * and * grad - retrac * show almost the same performance , we also see that when @xmath61 grows , our method converges faster * optstiefel * in terms of cpu time ( see table [ tab:10 ] with @xmath266 and @xmath267 ) . the second test compares the algorithms to a fixed value of @xmath61 and varying @xmath233 , the numerical results of this test are presented in table [ tab:11 ] , in this table we note that * optstiefel * and * grad - retrac * algorithms showed similar performance , while the method * sgmin * shows poor performance .",
    "in this article we study a feasible approach to deal with orthogonally constrained optimization problems .",
    "our algorithm implements the non - monotone barzilai - borwein line search on a mixed gradient based search direction .",
    "the feasibility at each iteration is guaranteed by projecting each updated point , which is in the current tangent space , onto the stiefel manifold , through svd s decomposition .",
    "the mixture is controlled by the coefficients @xmath95 and @xmath153 , associated to @xmath268 and @xmath269 respectively .",
    "when @xmath154 and @xmath155 , the obtained direction coincides with manton s direction , and the difference to these method is just the implementation of bb - line search instead of armijo s .",
    "+ our _ grad - retract _ algorithm is numerically compared to other state - of - the art algorithms , in a variety of test problems , achieving clear advantages in many cases .",
    "this work was supported in part by conacyt ( mexico ) , grant 258033 and the brazilian government , through the excellence fellowship program capes / impa of the second author while visiting the department of mathematics at ufpr .",
    "absil , p. a. , mahony , r. and sepulchre , r. ( 2009 ) optimization algorithms on matrix manifolds .",
    "princeton university press .",
    "absil , p. a. and malick , j. ( 2012 ) projection - like retractions on matrix manifolds .",
    "siam journal on optimization , * 22*(1 ) , 135 - 158 .",
    "barzilai , j. and borwein , j. m. ( 1988 ) two - point step size gradient methods .",
    "i m a journal of numerical analysis , * 8*(1 ) , 141 - 148 .",
    "dai , y.h . and",
    "fletcher r. ( 2005 ) projected barzilai - borwein methods for large - scale box - constrained quadratic programming .",
    "numerische mathematik , * 100*(1 ) , 21 - 47 .",
    "daspremont , a. , ei ghaoui , l. , jordan , m.i . and lanckriet , g.r .",
    "( 2007 ) a direct formulation for sparse pca using semidefinite programming .",
    "siam review * 49*(3 ) , 434 - 448 .",
    "edelman , a. , arias , t.a . and smith , s.t . ( 1998 ) the geometry of algorithms with orthogonality constraints .",
    "siam journal on matrix analysis and applications * 20*(2 ) , 303 - 353 .",
    "eldn , l. and park , h. ( 1999 ) a procrustes problem on the stiefel manifold .",
    "numerische mathematik * 82*(4 ) , 599 - 619 .",
    "francisco , j.b . and martini , t. ( 2014 ) spectral projected gradient method for the procrustes problem .",
    "tema ( s@xmath270o carlos ) , * 15*(1 ) , 83 - 96 .",
    "golub , g.h . and van loan , c.f .",
    "( 2012 ) matrix computations .",
    "3 ) , jhu press .",
    "grubi@xmath271i i. and pietersz , r. ( 2007 ) efficient rank reduction of correlation matrices .",
    "linear algebra and its applications , * 422*(2 ) , 629 - 653 .",
    "joho , m. and mathis , h. ( 2002 ) joint diagonalization of correlation matrices by using gradient methods with application to blind signal separation . in : sensor array and multichannel signal processing workshop proceedings , pp .",
    "273 - 277 .",
    "journe , m. , nesterov , y. , richtrik , p. and sepulchre , r. ( 2010 ) generalized power method for sparse principal component analysis .",
    "journal of machine learning research , * 11*(feb ) , 517 - 553 .",
    "kokiopoulou , e. , chen , j. and saad , y. ( 2011 ) trace optimization and eigenproblems in dimension reduction methods . numerical linear algebra with applications , volume 18 ,",
    "issue 3 , pages 565 - 602 .",
    "lai , r. and osher , s. ( 2014 ) a splitting method for orthogonality constrained problems .",
    "journal of scientific computing , * 58*(2 ) , 431 - 449 .",
    "liu , y.f .",
    ", dai , y.h . and luo , z.q .",
    "( 2011 ) on the complexity of leakage interference minimization for interference alignment .",
    "ieee 12th international workshop on signal processing advances in wireless communications , pp .",
    "471 - 475 .",
    "manton , j. h. ( 2002 ) optimization algorithms exploiting unitary constraints .",
    "ieee transactions on signal processing , * 50*(3 ) , 635 - 650 .",
    "martin , r. m. ( 2004 ) electronic structure : basic theory and practical methods .",
    "cambridge university press .",
    "nocedal , j. and wright , s. j. ( 2006 ) numerical optimization , springer series in operations research and financial engineering .",
    "springer , new york , second ed .",
    "pietersz , r. and groenen , p.j .",
    "( 2004 ) rank reduction of correlation matrices by majorization .",
    "quantitative finance * 4*(6 ) , 649 - 662 .",
    "raydan m. ( 1997 ) the barzilai and borwein gradient method for the large scale unconstrained minimization problem .",
    "siam journal on optimization , * 7*(1 ) , 26 - 33 .",
    "rebonato , r. and j@xmath272ckel , p. ( 1999 ) the most general methodology to creating a valid correlation matrix for risk manage - ment and option pricing purposes .",
    "journal of risk * 2 * , 17 - 27 .",
    "saad , y. ( 1992 ) numerical methods for large eigenvalue problems .",
    "158 ) , manchester : manchester university press .",
    "sch@xmath273nemann , p.h . , a generalized solution of the orthogonal procrustes problem",
    ". psychometrika *",
    "31*(1 ) , 1 - 10 . szabo , a. and ostlund , n. s. ( 1966 ) modern quantum chemistry : introduction to advanced electronic structure theory .",
    "courier corporation .",
    "theis , f. , cason , t. and absil , p.a .",
    "( 2009 ) soft dimension reduction for ica by joint diagonalization on the stiefel manifold . in : international conference on independent component analysis and signal separation ( pp .",
    "354 - 361 ) .",
    "springer berlin heidelberg .",
    "wen , z. , yang , c. , liu , x. and zhang , y. ( 2016 ) trace - penalty minimization for large - scale eigenspace computation .",
    "journal of scientific computing , * 66*(3 ) , 1175 - 1203 .",
    "wen , z.w . and",
    "yin , w.t . (",
    "2013 ) a feasible method for optimization with orthogonality constraints .",
    "mathematical programming , * 142*(1 - 2 ) , 397 - 434 .",
    "yang , c. , meza , j. c. and wang , l. w. ( 2006 ) a constrained optimization algorithm for total energy minimization in electronic structure calculations . journal of computational physics , vol .",
    "217 , no 2 , p. 709",
    "yang , c. , meza , j. c. and wang , l. w. ( 2007 ) a trust region direct constrained minimization algorithm for the kohn - sham equation .",
    "siam journal on scientific computing , vol .",
    "29 , no 5 , p. 1854 - 1875 .",
    "yang , c. , meza , j. c. , lee , b. and wang , l.w .",
    "( 2009 ) kssolv - a matlab toolbox for solving the kohn - sham equations .",
    "acm transactions on mathematical software * 36 * , 1 - 35 .",
    "zhang , h. and hager , w. ( 2004 ) a nonmonotone line search technique and its application to unconstrained optimization .",
    "siam journal on optimization , * 14*(4 ) , 1043 - 1056 .",
    "zhao , z. , bai , z. j. and jin , x. q. ( 2015 ) a riemannian newton algorithm for nonlinear eigenvalue problems .",
    "siam journal on matrix analysis and applications , * 36*(2 ) , 752 - 774 .",
    "zou , h. , hastie , t. , and tibshirani , r. ( 2006 ) sparse principal component analysis .",
    "journal of computational and graphical statistics * 15*(2 ) , 265 - 286 ."
  ],
  "abstract_text": [
    "<S> in this paper , we propose a non - monotone line search method for solving optimization problems on stiefel manifold . </S>",
    "<S> our method uses as a search direction a mixed gradient based on a descent direction , and a barzilai - borwein line search . </S>",
    "<S> feasibility is guaranteed by projecting each iterate on the stiefel manifold , through svd factorizations . </S>",
    "<S> some theoretical results for analyzing the algorithm are presented . </S>",
    "<S> finally , we provide numerical experiments comparing our algorithm with other state - of - the - art procedures . </S>"
  ]
}