{
  "article_text": [
    "-5 mm    the most common theoretical approach to understanding the behavior of algorithms is worst - case analysis . in worst - case analysis ,",
    "one proves a bound on the worst possible performance an algorithm can have .",
    "a triumph of the algorithms community has been the proof that many algorithms have good worst - case performance  a strong guarantee that is desirable in many applications .",
    "however , there are many algorithms that work exceedingly well in practice , but which are known to perform poorly in the worst - case or lack good worst - case analyses . in an attempt to rectify this discrepancy between theoretical analysis and observed performance , researchers introduced the average - case analysis of algorithms . in average - case analysis ,",
    "one bounds the expected performance of an algorithm on random inputs .",
    "while a proof of good average - case performance provides evidence that an algorithm may perform well in practice , it can rarely be understood to explain the good behavior of an algorithm in practice .",
    "a bound on the performance of an algorithm under one distribution says little about its performance under another distribution , and may say little about the inputs that occur in practice .",
    "smoothed analysis is a hybrid of worst - case and average - case analyses that inherits advantages of both .    in the formulation of smoothed analysis used in  @xcite",
    ", we measure the maximum over inputs of the expected running time of a simplex algorithm under slight random perturbations of those inputs . to see how this measure compares with worst - case and average - case analysis ,",
    "let @xmath0 denote the space of linear - programming problems of length @xmath1 and let @xmath2 denote the running time of the simplex algorithm on input @xmath3 .",
    "then , the worst - case complexity of the simplex algorithm is the function @xmath4 and the average - case complexity of the algorithm is @xmath5 under some suitable distribution on @xmath0 .",
    "in contrast , the smoothed complexity of the simplex algorithm is the function @xmath6 where @xmath7 is chosen according to some distribution , such as a gaussian . in this case",
    "@xmath8 is a gaussian random vector of standard deviation @xmath9 .",
    "we multiply by @xmath10 so that we can relate the magnitude of the perturbation to the magnitude of that which it perturbs .    in the smoothed analysis of algorithms ,",
    "we measure the expected performance of algorithms under slight random perturbations of worst - case inputs . more formally ,",
    "we consider the maximum over inputs of the expected performance of algorithms under slight random perturbations of those inputs .",
    "we then express this expectation as a function of the input size and the magnitude of the perturbation . while an algorithm with a good worst - case analysis will perform well on all inputs ,",
    "an algorithm with a good smoothed analysis will perform well on almost all inputs in every small neighborhood of inputs . smoothed analysis makes sense for algorithms whose inputs are subject to slight amounts of noise in their low - order digits , which is typically the case if they are derived from measurements of real - world phenomena .",
    "if an algorithm takes such inputs and has a good smoothed analysis , then it is unlikely that it will encounter an input on which it performs poorly .",
    "the name `` smoothed analysis '' comes from the observation that if one considers the running time of an algorithm as a function from inputs to time , then the smoothed complexity of the algorithm is the highest peak in the plot of this function after it is convolved with a small gaussian .",
    "in our paper introducing smoothed analysis , we proved that the simplex method has polynomial smoothed complexity  @xcite .",
    "the simplex method , which has been the most popular method of solving linear programs since the late 1940 s , is the canonical example of a practically useful algorithm that could not be understood theoretically .",
    "while it was known to work very well in practice , contrived examples on which it performed poorly proved that it had horrible worst - case complexity@xcite .",
    "the average - case complexity of the simplex method was proved to be polynomial  @xcite , but this result was not considered to explain the performance of the algorithm in practice .",
    "-5 mm    we recall that a linear programming problem can be written in the form @xmath11 where @xmath12 , @xmath13 and @xmath14 , for @xmath15 in  @xcite , we bound the smoothed complexity of a particular two - phase simplex method that uses the shadow - vertex pivot rule to solve linear programs in this form .",
    "we recall that the constraints of the linear program , that @xmath16 , confine @xmath17 to a ( possibly open ) polytope , and that the solution to the linear program is a vertex of this polytope .",
    "simplex methods work by first finding some vertex of the polytope , and then walking along the 1-faces of the polytope from vertex to vertex , improving the objective function at each step .",
    "the pivot rule of a simplex algorithm dictates which vertex the algorithm should walk to when it has many to choose from .",
    "the shadow - vertex method is inspired by the simplicity of the simplex method in two - dimensions : in two - dimensions , the polytope is a polygon and the choice of next vertex is always unique . to lift this simplicity to higher dimensions",
    ", the shadow - vertex simplex method considers the orthogonal projection of the polytope defined by the constraints onto a two - dimensional space .",
    "the method then walks along the vertices of the polytope that are the pre - images of the vertices of the shadow polygon . by taking the appropriate shadow ,",
    "it is possible to guarantee that the vertex optimizing the objective function will be encountered during this walk .",
    "thus , the running time of the algorithm may be bounded by the number of vertices lying on the shadow polygon .",
    "our first step in proving a bound on this number is a smoothed analysis of the number of vertices in a shadow .",
    "for example , we prove the bound :    * theorem 2.1 * ( shadow size ) _ let @xmath18 and @xmath19 .",
    "let @xmath20 and @xmath21 be independent vectors in @xmath22 , and let @xmath23 be gaussian random vectors in @xmath22 of variance @xmath24 centered at points each of norm at most @xmath25 .",
    "then , the expected number of vertices of the shadow polygon formed by the projection of @xmath26 onto @xmath27 is at most @xmath28 _",
    "this bound does not immediately lead to a bound on the running time of a shadow - vertex method as it assumes that @xmath21 and @xmath20 are fixed before the @xmath29s are chosen , while in a simplex method the plane on which the shadow is followed depends upon the @xmath29s .",
    "however , we are able to use the shadow size bound as a black - box to prove for a particular randomized two - phase shadow vertex simplex method :    * theorem 2.2 * ( simplex method ) _ let @xmath18 and @xmath30 .",
    "let @xmath12 and @xmath31 .",
    "let @xmath23 be gaussian random vectors in @xmath22 of variance @xmath24 centered at points each of norm at most @xmath25 .",
    "then the expected number of simplex steps taken by the two - phase shadow - vertex simplex algorithm to solve the program specified by @xmath32 , @xmath20 , and @xmath33 is at most @xmath34 where the expectation is over the choice of @xmath33 and the random choices made by the algorithm . _",
    "while the proofs of theorems  2.1 and  2.2 are quite involved , we can provide the reader with this intuition for theorem  2.1 : _",
    "after perturbation , most of the vertices of the polytope defined by the linear program have an angle bounded away from flat_. this statement is not completely precise because `` most '' should be interpreted under a measure related to the chance a vertex appears in the shadow , as opposed to counting the number of vertices .",
    "also , there are many ways of measuring high - dimensional angles , and different approaches are used in different parts of the proof",
    ". however , this intuitive statement tells us that most vertices on the shadow polygon should have angle bounded away from flat , which means that there can not be too many of them .",
    "one way in which angles of vertices are measured is by the condition number of their defining equations .",
    "a vertex of the polytope is given by a set of equations of the form @xmath35 the condition number of @xmath36 is defined to be @xmath37 where we recall that @xmath38 and that @xmath39 the condition number is a measure of the sensitivity of @xmath3 to changes in @xmath36 and @xmath40 , and is also a normalized measure of the distance of @xmath36 to the set of singular matrices . for more information on the condition number of a matrix , we refer the reader to one of  @xcite .",
    "condition numbers play a fundamental role in numerical analysis , which we will now discuss .",
    "-5 mm    the condition number of a problem instance is generally defined to be the sensitivity of the output to slight perturbations of the problem instance . in numerical analysis ,",
    "one often bounds the running time of an iterative algorithm in terms of the condition number of its input .",
    "classical examples of algorithms subject to such analyses include newton s method for root finding and the conjugate gradient method of solving systems of linear equations .",
    "for example , the number of iterations taken by the method of conjugate gradients is proportional to the square root of the condition number .",
    "similarly , the running times of interior - point methods have been bounded in terms of condition numbers  @xcite .",
    "blum  @xcite suggested that a complexity theory of numerical algorithms should be parameterized by the condition number of an input in addition to the input size .",
    "smale  @xcite proposed a complexity theory of numerical algorithms in which one :    1 .",
    "_ proves a bound on the running time of an algorithm solving a problem in terms of its condition number , and then _ 2 .",
    "_ proves that it is unlikely that a random problem instance has large condition number .",
    "_    this program is analogous to the average - case complexity of theoretical computer science and hence shares the same shortcoming in modeling practical performance of numerical algorithms .    to better model the inputs that occur in practice",
    ", we propose replacing step 2 of smale s program with    1 .   _",
    "prove that for every input instance it is unlikely that a slight random perturbation of that instance has large condition number .",
    "_    that is , we propose to bound the smoothed value of the condition number .",
    "in contrast with the average - case analysis of condition numbers , our analysis can be interpreted as demonstrating that if there is a little bit of imprecision or noise in the input , then it is unlikely it is ill - conditioned .",
    "the combination of step 2 with step 1 of smale s program provides a simple framework for performing smoothed analysis of numerical algorithms whose running time can be bounded by the condition number of the input .",
    "-5 mm    one of the most fundamental condition numbers is the condition number of matrices defined at the end of section  2 . in his paper , `` the probability that a numerical analysis problem is difficult '' , demmel  @xcite proved that it is unlikely that a gaussian random matrix centered at the origin has large condition number .",
    "demmel s bounds on the condition number were improved by edelman  @xcite . as bounds on the norm of a random matrix",
    "are standard , we focus on the norm of the inverse , for which edelman proved :    * theorem 4.1 * ( edelman ) _ let @xmath41 be a @xmath42-by-@xmath42 matrix of independent gaussian random variables of variance @xmath25 and mean @xmath43",
    ". then , @xmath44 _",
    "we obtain a smoothed analogue of this bound in work with sankar  @xcite .",
    "that is , we show that for every matrix it is unlikely that the slight perturbation of that matrix has large condition number .",
    "the key technical statement is :    * theorem 4.2 * ( sankar - spielman - teng ) _ let  @xmath45 be an arbitrary @xmath42-by-@xmath42 real matrix and  @xmath46 a matrix of independent gaussian random variables centered at  @xmath45 , each of variance  @xmath47 . then @xmath48 _    in contrast with the techniques used by demmel and edelman , the techniques used in the proof of theorem  4.2 are geometric and completely elementary .",
    "we now give the reader a taste of these techniques by proving the simpler :    * theorem 4.3 * _ let  @xmath45 be an arbitrary @xmath42-by-@xmath42 real matrix and  @xmath46 a matrix of independent gaussian random variables centered at  @xmath45 , each of variance  @xmath47 .",
    "then , @xmath49 _    the first step of the proof is to relate @xmath50 to a geometric quantity of the vectors in the matrix @xmath46 .",
    "the second step is to bound the probability of a configuration under which this geometric quantity is small .",
    "the geometric quantity is given by :    * definition * _ for @xmath42 vectors in @xmath22 , @xmath51 , define @xmath52 _    * lemma 4.5 * _ for @xmath42 vectors in @xmath22 , @xmath53 , @xmath54 _    * proof .",
    "* let @xmath55 be a unit vector such that @xmath56 without loss of generality , let @xmath57 be the largest entry of @xmath21 in absolute value , so @xmath58 .",
    "then , we have @xmath59    * proof of theorem  4.3 . * let @xmath53 denote the columns of @xmath46 .",
    "lemma  4.5 tells us that if @xmath60 , then @xmath61 is less than @xmath62 . for each @xmath63 , the probability that the height of @xmath64 above @xmath65 is less than @xmath66 is at most @xmath67 thus , @xmath68 so , @xmath69    * conjecture 1 * _ let  @xmath45 be an arbitrary @xmath42-by-@xmath42 real matrix and  @xmath46 a matrix of independent gaussian random variables centered at  @xmath45 , each of variance  @xmath47 .",
    "then @xmath70 _",
    "-5 mm    the perceptron algorithm solves linear programs of the following simple form :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ given a set of points @xmath33 , find a vector @xmath71 such that @xmath72 for all @xmath63 , if one exists .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    one can define the condition number of the perceptron problem to be the reciprocal of the `` wiggle room '' of the input",
    ". that is , let @xmath73 and @xmath74 then , the condition number of perceptron problem is defined to be @xmath75 .",
    "the perceptron algorithm works as follows : ( 1 ) initialize @xmath76 ; ( 2 ) select any @xmath77 such that @xmath78 and set @xmath79 ; ( 3 ) while @xmath80 , go back to step ( 2 ) .    using the following two lemmas ,",
    "blum and dunagan  @xcite obtained a smoothed analysis of the perceptron algorithm .",
    "* theorem 5.1 * ( block - novikoff ) _ on input @xmath33 , the perceptron algorithm terminates in at most @xmath81 iterations .",
    "_    * theorem 5.2 * ( blum - dunagan ) _ let @xmath33 be gaussian random vectors in @xmath22 of variance @xmath82 centered at points each of norm at most @xmath25 .",
    "then , @xmath83 _    setting @xmath84 , blum and dunagan concluded    * theorem 5.3 * ( blum - dunagan ) _ let @xmath33 be gaussian random vectors in @xmath22 of variance @xmath82 centered at points each of norm at most @xmath25 .",
    "then , there exists a constant @xmath85 such that the probability that the perceptron takes more than @xmath86 iterations is at most @xmath87 . _    in his seminal work , renegar  @xcite defines the condition of a linear program to be the normalized reciprocal of its distance to the set of ill - posed linear programs , where an ill - posed program is one that can be made both feasible and infeasible or bounded and unbounded by arbitrarily small changes to its constraints .",
    "renegar proved the following theorem .",
    "* theorem 5.4 * ( renegar ) _ there is an interior point method such that , on input a linear program specified by @xmath88 and an @xmath89 , it will terminate in @xmath90 iterations and return either a solution within @xmath91 of the optimal or a certificate that the linear program is infeasible or unbounded .",
    "_    with dunagan , we recently proved the following smoothed bound on the condition number of a linear program  @xcite :    * theorem 5.5 * ( dunagan - spielman - teng ) _ for any @xmath92 , let @xmath93 be a set of gaussian random vectors in @xmath22 of variance @xmath94 centered at points @xmath95 , let @xmath96 be a gaussian random vector in @xmath22 of variance @xmath94 centered at @xmath97 and let @xmath98 be a gaussian random vector in @xmath99 of variance @xmath94 centered at @xmath100 such that @xmath101 . then @xmath102 and hence @xmath103 _    combining these two theorem ,",
    "we have    * theorem 5.6 * ( smoothed complexity of interior point methods ) _ let @xmath104 and @xmath105 be as given in theorem  5.5 , then , renegar s interior point method solves the linear program specified by @xmath105 to within precision @xmath106 in expected @xmath107 iterations . _",
    "-5 mm    as the norm of the inverse of matrix is such a fundamental quantity , it is natural to ask how the norms of the inverses of the @xmath108 @xmath42-by-@xmath42 square sub - matrices of a @xmath42-by-@xmath1 matrix behave .",
    "moreover , a crude bound on the probability that many of these are large is a dominant term in the analysis of complexity of the simplex method in  @xcite .",
    "the bound obtained in that paper is :    * lemma 6.1 * _ let @xmath33 be gaussian random vectors in @xmath22 of variance @xmath109 centered at points of norm at most @xmath25 . for @xmath110}{d}$ ] a @xmath42-set ,",
    "let @xmath111 denote the indicator random variable that is 1 if @xmath112^{-1 } } \\geq       \\frac{\\sigma^{2 } }       { 8 d^{3/2 } n^{7}}.\\ ] ] then , @xmath113 _                nina amenta and gunter ziegler .",
    "deformed products and maximal shadows of polytopes . in b.",
    "chazelle , j.e .",
    "goodman , and r.  pollack , editors , _ advances in discrete and computational geometry _ , number 223 in contemporary mathematics , 5790 .",
    "soc . , 1999 .",
    "lenore blum .",
    "lectures on a theory of computation and complexity over the reals ( or an arbitrary ring ) . in erica jen , editor , _ the proceedings of the 1989 complex systems summer school , santa fe , new mexico _ , volume  2 , 147 , june 1989 .",
    "arvind sankar , daniel  a. spielman , and shang - hua teng . smoothed analysis of the condition numbers and growth factors of matrices .",
    "available at ` http://math.mit.edu/\\simspielman/smoothedanalysis ` , 2002 .",
    "daniel spielman and shang - hua teng . smoothed analysis of algorithms : why the simplex algorithm usually takes polynomial time . in _ proceedings of the 33rd annual acm symposium on the theory of computing ( stoc 01 ) _ , 296305 , 2001 .",
    "full version available at ` http://math.mit.edu/\\simspielman/smoothedanalysis ` ."
  ],
  "abstract_text": [
    "<S> spielman and teng  [ stoc 01 ] introduced the smoothed analysis of algorithms to provide a framework in which one could explain the success in practice of algorithms and heuristics that could not be understood through the traditional worst - case and average - case analyses . in this talk </S>",
    "<S> , we survey some of the smoothed analyses that have been performed .    </S>",
    "<S> 4.5 mm    * 2000 mathematics subject classification : * 65y20 , 68q17 , 68q25 , 90c05 .    * keywords and phrases : * smoothed analysis , simplex method , condition number , interior point method , perceptron algorithm . </S>"
  ]
}