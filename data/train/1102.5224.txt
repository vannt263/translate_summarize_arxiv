{
  "article_text": [
    "a change - point model for a sequence of independent random variables @xmath0 is a model in which there exist unknown change points @xmath1 , @xmath2 , such that , for each @xmath3 , @xmath4 are identically distributed with a distribution that depends on @xmath5 . here , we consider parametric change - point models in which the distribution of @xmath6 is parametric ; however , the form of the distribution can be different for each @xmath5 .",
    "change - point models are used in many fields .",
    "for example , broemeling and tsurumi  ( @xcite ) uses a multiple change - point model for the us demand for money ; lombard  ( @xcite ) uses a multiple change - point model to model the effect of sudden changes in wind direction on the flight of a projectile ; reed ( @xcite ) uses a multiple change - point model in the analysis of forest fire data .",
    "a  number of authors have used multiple change - point models in the analysis of dna sequences ; see , for example , braun and muller ( @xcite ) , fu and curnow ( @xcite ) and halpern  ( @xcite ) .",
    "many further examples are provided in the monographs chen and gupta ( @xcite ) and csrg and horvth ( @xcite ) .",
    "the goal of this paper is to establish the asymptotic properties of maximum likelihood estimators of the parameters of a multiple change - point model , under easily verifiable conditions .",
    "these results are based on the following model .",
    "assume that the vectors in the data set @xmath7 are independently drawn from the parametric model @xmath8 where @xmath9 is a probability density function of a continuous distribution with unknown common parameter @xmath10 for all @xmath11 and unknown within - segment parameters @xmath12 for each @xmath11 ; @xmath9 may have the same functional form for some or all of @xmath11 ; @xmath10 may be a vector ; @xmath12 may be a different vector parameter of different dimensions for each @xmath11 .",
    "in this model , there are @xmath13 unknown change points  @xmath14 , where the number of change points @xmath13 is assumed to be known .",
    "the parameter @xmath15 is common to all segments .",
    "there are a number of results available on the asymptotic properties of parameter estimators in change - point models .",
    "see , for example , hinkley  ( @xcite ) , hinkley and hinkley  ( @xcite ) , battacharya ( @xcite ) , fu and curnow ( @xcite ) , jandhyala and fotopoulos ( @xcite ) and hawkins ( @xcite ) ; the two monographs chen and gupta ( @xcite ) and csrgo and horvth ( @xcite ) have detailed bibliographies on this topic .    in particular",
    ", hinkley ( @xcite ) considers likelihood - based inference for a single change - point model , obtaining the asymptotic distribution of the maximum likelihood estimator of the change point under the assumption that the other parameters in the model are known .",
    "hinkley ( @xcite ) and hinkley ( @xcite ) argue that this asymptotic distribution is also valid when the parameters are unknown .",
    "unfortunately , there are problems in extending the approach used in hinkley ( @xcite ) to the setting considered here .",
    "the method used in hinkley ( @xcite ) is based on considering the relative locations of a candidate change point and the true change point .",
    "when there is only a single change point , there are only three possibilities : the candidate change point is either greater than , less than or equal to the true change point . however , in models with @xmath13 change points , the relative positions of the candidate change points and the true change points can become quite complicated and the simplicity and elegance of the single change point argument is lost .",
    "a second problem arises when extending the argument for the case in which the change points are the only parameters in the model to the case in which there are unknown within - segment parameters .",
    "the consistency argument used in the former case is extended to the latter case using a `` consistency assumption '' ( hinkley ( @xcite ) , section 4.1 ) ; this condition is discussed in appendix and examples are given which show that this assumption is a strong one that is not generally satisfied in the class of models considered here .",
    "there are relatively few results available on the asymptotic properties of maximum likelihood estimators in multiple change - point models .",
    "thus , the present paper has done several things . in the general model described above ,",
    "in which there is a fixed , but arbitrary , number of change points , we show that the maximum likelihood estimators of the change points are consistent and converge to the true change points at the rate @xmath16 , under relatively weak regularity conditions . as noted above , a simple extension of the approach used in single change - point models is not available ; thus , the second thing achieved by this paper is the introduction of the tools necessary for analyzing the likelihood function in a multiple change - point model . finally , the asymptotic distribution of the maximum likelihood estimators of the parameters of the within - segment distributions is derived for the general case described above , in which the form of the distribution can change from segment to segment and in which , possibly , there are parameters that are common to all segments .",
    "the paper is organized as follows . the asymptotic theory of maximum likelihood estimators of a multiple change - point model",
    "is described in section [ sec2 ] .",
    "section [ sec3 ] contains a numerical example illustrating these results and section [ sec4 ] contains some discussion of future research which builds on the results given in this paper .",
    "appendix discusses the `` consistency assumption '' used in hinkley ( @xcite ) ; all technical proofs are given in appendix .",
    "consider estimation of the multiple change - point model introduced in section [ sec1 ] .",
    "for any change point configuration @xmath17 , the log - likelihood function is given by @xmath18 estimators of all change points , all within - segment parameters and the common parameter are given by @xmath19 where @xmath20 , and @xmath21 are the parameter spaces of @xmath22 , @xmath23 , and @xmath24 , respectively .",
    "let @xmath25 note that @xmath26 is taken to be a constant vector as @xmath27 goes to infinity .",
    "define @xmath28    the expected information matrix is given by @xmath29 = \\pmatrix { e[-\\ell^0_{\\psi\\psi}(\\psi,\\theta);\\phi ] & e[-\\ell^0_{\\psi\\theta}(\\psi , \\theta);\\phi]\\vspace*{3pt}\\cr e[-\\ell^0_{\\psi\\theta}(\\psi,\\theta);\\phi]^t&e[-\\ell^0_{\\theta\\theta } ( \\psi,\\theta);\\phi]},\\ ] ] @xmath30\\\\ & & \\quad=\\bigl(e\\bigl[-\\ell^{(1)}_{\\psi\\theta_1}(\\psi,\\theta_1);\\phi\\bigr],e\\bigl[-\\ell ^{(2)}_{\\psi\\theta_2}(\\psi,\\theta_2);\\phi\\bigr],\\ldots , e\\bigl[-\\ell^{(k+1)}_{\\psi \\theta_{k+1}}(\\psi,\\theta_{k+1});\\phi\\bigr]\\bigr ) , \\\\ & & e[-\\ell^0_{\\theta\\theta}(\\psi,\\theta);\\phi]\\\\ & & \\quad = \\operatorname{diag}\\bigl ( e\\bigl[-\\ell^{(1)}_{\\theta_1\\theta_1}(\\psi,\\theta_1);\\phi\\bigr ] , e\\bigl[-\\ell^{(2)}_{\\theta_2\\theta_2}(\\psi,\\theta_2);\\phi\\bigr ] , \\ldots , e\\bigl[-\\ell^{(k+1)}_{\\theta_{k+1}\\theta_{k+1}}(\\psi,\\theta_{k+1});\\phi\\bigr]\\bigr),\\end{aligned}\\ ] ] where @xmath31 denotes a diagonal block matrix whose diagonal blocks are in the bracket , other elements are zeros and the average expected information matrix is given by @xmath32    the asymptotic properties of these estimators are based on the following regularity conditions .",
    "other than the parts concerning change points , these conditions are typically similar to those required for the consistency and asymptotic normality of maximum likelihood estimators of parameters in models without change points ; see , for example , wald ( @xcite ) .",
    "particularly , compactness of parameter spaces is a common assumption in the classical likelihood literature .",
    "these conditions are different from those required by ferger ( @xcite ) and dring ( @xcite ) , who consider estimation of change points in a nonparametric setting in which nothing is assumed about the within - segment distributions , using a type of nonparametric m - estimator based on empirical processes .",
    "thus , these authors do not require conditions on the within - segment likelihood functions ; on the other hand , their method does not provide estimators of within - segment parameters .",
    "[ a20 ] it is assumed that for @xmath33 , @xmath34 on a  set of non - zero measure .",
    "this assumption guarantees that the distributions in two neighboring segments are different ; clearly , this is required for the change points to be well defined .",
    "[ a21 ] it is assumed that :    for @xmath11 , @xmath22 and @xmath12 are contained in @xmath35 , where @xmath35 is a compact subset of  @xmath36 ; @xmath24 and @xmath10 are contained in @xmath21 where @xmath21 is a compact subset of @xmath37 ; here , @xmath38 are non - negative integers ;    @xmath39 is third - order continuously differentiable with respect to @xmath40 ;    the expectations of the first and second order derivatives of @xmath41 with respect to @xmath42 exist for @xmath42 in its parameter space .",
    "compactness of the parameter space is used to establish the consistency of the maximum likelihood estimators of @xmath43 ; see , for example , bahadur ( @xcite ) for further discussion of this condition and its necessity in general models .",
    "if we assume further conditions on models , the compactness of the parameter space may be avoided . but this appears to be a substantial task for future work .",
    "differentiability of the log - likelihood function is used to justify certain taylor series expansions .",
    "both parts of assumption [ a21 ] are relatively weak and are essentially the same as conditions used in parametric models without change points ; see , for example , schervish ( @xcite ) , section 7.3 .",
    "part 3 is very weak and is used in the proof of theorem  [ thm3 ] .",
    "[ a22]it is assumed that :    for any @xmath11 and any integers @xmath44 satisfying @xmath45 , @xmath46\\}\\biggr)^2\\biggr\\}\\leq c(t - s)^r , \\ ] ] where @xmath47 and @xmath48 is a constant ;    for any @xmath11 and any integers @xmath44 satisfying @xmath49 , @xmath50 -v(\\psi,\\theta_j;\\psi^0,\\theta^0_j)\\}\\biggr)^2\\biggr\\}\\\\ & & \\quad\\leq d(t - s)^r , \\end{aligned}\\ ] ] where @xmath51 is introduced in equation , @xmath47 and @xmath52 is a constant .",
    "parts 1 and 2 of assumption [ a22 ] are technical requirements on the behavior of the log - likelihood function between and within segments , respectively .",
    "this condition is used to ensure that the information regarding the within- and between - segment parameters grows quickly enough to establish consistency and asymptotic normality of the parameter estimators .",
    "these conditions are relatively weak ; it is easy to check that they are satisfied by at least all distributions in the exponential family .",
    "consider a probability density function of exponential family form : @xmath53 it is then straightforward that the schwarz inequality gives @xmath54\\}\\biggr)^2 \\\\ & & \\quad \\leq\\biggl[1+\\sum_{q=1}^{m}w_{q}(\\eta)^2\\biggr]\\\\ & & \\qquad{}\\times\\biggl\\ { \\biggl[\\sum_{i = s+1}^{t}\\bigl(\\log h(x_i)-e(\\log h(x_i))\\bigr)\\biggr]^2+\\sum_{q=1}^{m}\\biggl[\\sum _ { i = s+1}^{t}\\bigl(t_{q}(x_i)-e(t_{q}(x_i))\\bigr)\\biggr]^2\\biggr\\}.\\end{aligned}\\ ] ] therefore , part 1 of assumption [ a22 ] is satisfied with @xmath55 because the function @xmath56 assumed to be continuous can achieve its maximum on the compact parameter space . similarly , part 2 of assumption [ a22 ] is also satisfied with @xmath55 .",
    "the main results of this paper are given in the following three theorems .",
    "[ thm1 ] under assumption , part 1 of assumption and part 1 of assumption , @xmath57 and @xmath58 as @xmath59 , that is , @xmath60 and @xmath61 , where @xmath62 for @xmath63 and @xmath11 .",
    "note that @xmath64 , are not consistent ( hinkley ( @xcite ) ) ; it is the estimators of the change - point fractions @xmath65 , that are consistent .",
    "the consistency of @xmath66 , and @xmath67 is the same as the corresponding result in classical likelihood theory for independent , identically distributed data .",
    "[ thm2 ] under assumptions , we have @xmath68 where @xmath69 @xmath70 that is , @xmath71 for @xmath72 .    we now consider the asymptotic distribution of @xmath73 , where @xmath74 .    [ thm3 ] under assumptions , @xmath75 where @xmath76 is the @xmath77-dimensional multivariate normal distribution with mean vector zero and covariance matrix @xmath78 .",
    "the proofs of theorems [ thm1][thm3 ] are based on the following approach .",
    "define a function @xmath79 by @xmath80f_i(\\psi^0,\\theta_i^0;x)\\,dx\\biggr\\}\\nonumber\\\\ & & { } + \\frac{1}{n}\\sum_{j=1}^{k+1}\\sum_{i = n_{j-1}+1}^{n_j}\\ { \\log f_j(\\psi,\\theta_j;x_i)-e[\\log f_j(\\psi,\\theta_j;x_i)]\\}\\\\ & & { } -\\frac{1}{n}\\sum_{j=1}^{k+1}\\sum_{i = n_{j-1}^0 + 1}^{n_j^0}\\ { \\log f_j(\\psi^0,\\theta_j^0;x_i)-e[\\log f_j(\\psi^0,\\theta_j^0;x_i)]\\ } , \\nonumber\\end{aligned}\\ ] ] where @xmath81 is the number of observations in the set @xmath82\\cap[n_{i-1}^0 + 1,n_i^0]$ ] for @xmath83 .",
    "we obviously have that @xmath84 thus , the maximum likelihood estimators may be defined as the maximizers of @xmath79 rather than as the maximizers of @xmath85 .",
    "let @xmath86 be defined by @xmath87f_i(\\psi^0,\\theta _",
    "i^0,x)\\,dx \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & \\quad\\mbox{for } i , j=1,2,\\ldots , k+1.\\end{aligned}\\ ] ] note that @xmath79 may be written @xmath88 , where @xmath89 and @xmath90\\ } \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & { } -\\frac{1}{n}\\sum_{j=1}^{k+1}\\sum _ { i = n_{j-1}^0 + 1}^{n_j^0}\\{\\log f_j(\\psi^0,\\theta_j^0;x_i)-e[\\log f_j(\\psi^0,\\theta_j^0;x_i)]\\}.\\end{aligned}\\ ] ] alternatively , we may write @xmath91 \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & { } \\hspace*{44pt}-\\sum_{t\\in\\tilde{n}_{ji}}[\\log f_i(\\psi^0,\\theta",
    "_ i^0;x_t)-e(\\log f_i(\\psi^0,\\theta_i^0,x_t))]\\biggr\\},\\end{aligned}\\ ] ] where @xmath92\\cap[n_{i-1}^0 + 1,n_i^0]$ ] .",
    "note that @xmath93 is a weighted sum of the negative kullback ",
    "leibler distances ; it will be shown that @xmath94 approaches @xmath95 as @xmath96 .",
    "also , @xmath86 @xmath97 with equality if and only if @xmath98 almost everywhere ( kullback and leibler ( @xcite ) ) .",
    "lemma [ lem1 ] gives a bound for @xmath93 .",
    "[ lem1 ] under assumption and part 1 of assumption , there exist two positive constants @xmath99 and @xmath100 such that , for any @xmath101 and @xmath42 , @xmath102 where @xmath103 and @xmath104 .",
    "lemma [ lem2 ] describes between - segment properties and within - segment properties of this model .",
    "[ lem2 ] under part 1 of assumption [ a21 ] , the following two results follow from parts 1 and  2 of assumption [ a22 ] respectively :    for any @xmath11 , any @xmath105 and any positive number @xmath106 , there exist a constant @xmath107 , independent of @xmath108 , and a constant @xmath47 , such that @xmath109\\}\\biggr|>\\varepsilon\\biggr ) \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & \\quad \\leq a_j\\frac { ( m_2-m_1)^r}{\\varepsilon^2}.\\end{aligned}\\ ] ]    for any @xmath11 and any positive number @xmath106 , there exist a constant @xmath110 , independent of @xmath108 , and a constant @xmath47 , such that @xmath111 \\nonumber \\\\[-8pt ] \\\\[-8pt ] \\nonumber & & { } \\hspace*{134pt}-v(\\psi,\\theta_j;\\psi^0,\\theta^0_j)\\}>\\varepsilon\\biggr)\\leq b_j\\frac { ( n^0_j - n^0_{j-1})^r}{\\varepsilon^2}.\\end{aligned}\\ ] ]    in practical applications , it is useful to have an estimator of @xmath112",
    ". let @xmath113 & \\hat{e}[-\\hat{\\ell}_{\\psi\\theta}(\\hat{\\psi},\\hat{\\theta});\\hat{\\phi}]\\vspace*{3pt}\\cr \\hat{e}[-\\hat{\\ell}_{\\psi\\theta}(\\hat{\\psi},\\hat{\\theta});\\hat{\\phi } ] ^t&\\hat{e}[-\\hat{\\ell}_{\\theta\\theta}(\\hat{\\psi},\\hat{\\theta});\\hat { \\phi}]},\\\\ \\hat{e}[-\\hat{\\ell}_{\\psi\\psi}(\\hat{\\psi},\\hat{\\theta});\\hat{\\phi } ] & = & \\sum_{j=1}^{k+1}\\sum_{i=\\hat{n}_{j-1}+1}^{\\hat{n}_j}\\frac { 1}{f_j^2(\\hat{\\psi},\\hat{\\theta}_j;x_i)}{f_j}_{\\psi}(\\hat{\\psi},\\hat { \\theta}_j;x_i){f_j}_{\\psi}^t(\\hat{\\psi},\\hat{\\theta}_j;x_i),\\\\ \\hat{e}[-\\hat{\\ell}_{\\psi\\theta_j}(\\hat{\\psi},\\hat{\\theta});\\hat{\\phi } ] & = & \\sum_{i=\\hat{n}_{j-1}+1}^{\\hat{n}_j}\\frac{1}{f_j^2(\\hat{\\psi},\\hat { \\theta}_j;x_i)}{f_j}_{\\psi}(\\hat{\\psi},\\hat{\\theta}_j;x_i){f_j}_{\\theta _ j}^t(\\hat{\\psi},\\hat{\\theta}_j;x_i),\\\\",
    "\\hat{e}[-\\hat{\\ell}_{\\theta_j\\theta_j}(\\hat{\\psi},\\hat{\\theta});\\hat { \\phi}]&=&\\sum_{i=\\hat{n}_{j-1}+1}^{\\hat{n}_j}\\frac{1}{f_j^2(\\hat{\\psi } , \\hat{\\theta}_j;x_i)}{f_j}_{\\theta_j}(\\hat{\\psi},\\hat{\\theta } _ j;x_i){f_j}_{\\theta_j}^t(\\hat{\\psi},\\hat{\\theta}_j;x_i)\\end{aligned}\\ ] ] for @xmath11 .",
    "then @xmath114 is a consistent estimator of @xmath115 .",
    "consider the problem of analyzing the mineral content of a core sample , which is extensively studied in chen and gupta ( @xcite ) , chernoff ( @xcite ) and srivastava and worsley ( @xcite ) .",
    "in particular , we consider the data in chernoff ( @xcite ) on the mineral content of @xmath116 minerals in a core sample measured at @xmath117 equally spaced points . since some of the minerals have a very low assay , we follow chen and gupta ( @xcite ) and srivastava and worsley ( @xcite ) in analyzing only the @xmath118 variables @xmath119 and @xmath120 with the highest assays .",
    "thus , we assume that @xmath121 has a @xmath122-variate normal distribution with a within - segment mean parameter vector and a variance - covariance matrix that is common to all segments .",
    "the analyses of chen and gupta ( @xcite ) , chernoff ( @xcite ) and srivastava and worsley ( @xcite ) suggest that there are @xmath122 change points of the mean vector and , hence , we make that assumption here .",
    "the estimates of @xmath122 change points , within - segment parameters of mean vectors and common parameter of variance - covariance matrix were computed using maximum likelihood .",
    "the estimated change points are @xmath123 and @xmath124 , which are different from those estimated change points by chen and gupta ( @xcite ) , chernoff ( @xcite ) and srivastava and worsley  ( @xcite ) , and are more reasonable .",
    "this is because chen and gupta ( @xcite ) , chernoff ( @xcite ) and srivastava and worsley  ( @xcite ) use the binary segmentation procedures which detect multiple change points one by one , not simultaneously , whereas the method in this paper simultaneously estimates multiple change points .",
    "the estimated six within - segment mean vectors are in the following .",
    "they are arranged according to the order of from left to right .",
    "for example , the two vectors on the first line are , respectively , the first and second within - segment mean vectors . @xmath125",
    "the estimated common variance - covariance matrix is @xmath126",
    "this paper establishes the consistency of maximum likelihood estimators of the parameters of a general class of multiple change - point models and gives the asymptotic distribution of the parameters of the within - segment distributions .",
    "the required regularity conditions are relatively weak and are generally satisfied by exponential family distributions .",
    "some important problems in the analysis of multiple change - point models were not considered here .",
    "one is that the asymptotic distribution of the maximum likelihood estimator of the vector of change points was not considered .",
    "the reason for this is that the methods used to determine this asymptotic distribution are quite different from the methods used to establish the consistency of the maximum likelihood estimator ; see , for example , hinkley ( @xcite ) for a treatment of this problem in a single change - point model .",
    "thus , this is essentially a separate research topic .",
    "however , the asymptotic properties obtained in this paper are necessary for the establishment of the asymptotic distribution of the maximum likelihood estimator of the vector of change points in this model",
    ". this will be a subject of future work .",
    "another important problem is to extend the results of this paper to the case in which the number of change points is not known and must be determined from the data .",
    "clearly , a likelihood - based approach to this problem will require an understanding of the properties of maximum likelihood estimators in the model in which the number of change points is known .",
    "thus , the results of the present paper can be considered as a first step toward the development of a likelihood - based methodology that can be used to determine simultaneously the number and location of the change points .",
    "this is also a topic of future research .",
    "consider a change - point model with a single change point , @xmath127 , and suppose that there are no common parameters in the model . in hinkley ( @xcite )",
    ", it is shown that @xmath128 , the maximum likelihood estimator of @xmath127 , satisfies @xmath129 under the condition @xmath130 with probability @xmath131 as @xmath132 , which was described as a `` consistency assumption '' .",
    "note that the random variables in the sum @xmath133 are drawn from the distribution with density @xmath134 .",
    "suppose that @xmath135 converges to @xmath136 as @xmath137 , uniformly in @xmath138 , where @xmath139 is distributed according to the distribution with density @xmath140 .",
    "equation ( [ eqa1 ] ) then holds , provided that @xmath141 note that , by properties of the kullback  leibler distance and assumption [ a20 ] , @xmath142 for each @xmath138 .",
    "thus , condition ( [ eqa1 ] ) fails whenever the distribution corresponding to the density @xmath140 is in the closure of the set of distributions corresponding to densities of the form @xmath143 , in a certain sense .",
    "one such case occurs if @xmath144 and @xmath134 have the same parametric form with parameters @xmath145 , respectively , satisfying @xmath146 .",
    "for instance , suppose that the random variables in the first segment are normally distributed with mean @xmath147 and standard deviation @xmath131 and the random variables in the second segment are normally distributed with mean @xmath148 and standard deviation @xmath131 .",
    "then @xmath149 where @xmath150 is normally distributed with mean @xmath148 and variance @xmath151 .",
    "clearly , ( [ eqa1 ] ) does not hold in this case .",
    "a similar situation occurs when the distribution with density @xmath152 can be viewed as a  limit of the distributions with densities @xmath143 .",
    "for instance , suppose that @xmath144 is the density of a  weibull distribution with rate parameter @xmath153 and shape parameter @xmath154 , @xmath155 , @xmath156 , and @xmath134 is the density of an exponential distribution with rate parameter @xmath157 .    in this appendix",
    ", we show that this is a strong assumption that is not generally satisfied by otherwise well - behaved models .",
    "for instance , suppose that @xmath144 and @xmath134 have the same functional form and that the difference between the two distributions is due to the fact that @xmath158 .",
    "again , ( [ eqa1 ] ) will not hold .",
    "thus , the consistency condition used in hinkley ( @xcite ) is too strong for the general model considered here .",
    "proof of lemma [ lem1 ] we first need to prepare some results which are to be used in this proof . for @xmath63 ,",
    "let us define @xmath159,\\ ] ] where @xmath160 .",
    "we then have that @xmath161 for @xmath63 .",
    "it is straightforward to show that @xmath162 is a convex function with respect to @xmath154 for any @xmath63 .",
    "let @xmath163 . because @xmath164 for @xmath165 , convexity of @xmath166 gives that @xmath167 noting that @xmath168,\\ ] ] it follows from assumption [ a20 ] that @xmath169 .",
    "if we let @xmath170 , then @xmath171 .",
    "let @xmath172 .",
    "consider a change - point fraction configuration @xmath101 such that @xmath173 .",
    "for any @xmath33 , there are two cases : a candidate change - point fraction @xmath174 may be on the left or the right of the true change - point fraction @xmath175 .    for any @xmath5 with @xmath174 on the right of @xmath175 , we have that @xmath176 .",
    "then @xmath177 if we define @xmath178 , then the case @xmath173 gives that @xmath179 and @xmath180\\\\ & \\leq&\\frac{n_{j , j+1}}{n}g_j(\\phi^0)\\leq(\\lambda_j-\\lambda_j^0)\\bar { g}(\\phi^0).\\end{aligned}\\ ] ]    for any @xmath5 with @xmath174 on the left of @xmath175 , we have that @xmath181 .",
    "similarly , we define @xmath182 . using the fact that @xmath183 , it similarly gives that @xmath184 .",
    "therefore , if @xmath173 , then we obtain that @xmath185 .",
    "on the other hand , @xmath186 we have @xmath187 for any @xmath5 , so @xmath188    now , consider the other case of a change - point fraction configuration @xmath101 , where @xmath189 .",
    "it is clear that there exists a pair of integers @xmath190 such that @xmath191 , @xmath192 and @xmath193 .",
    "let @xmath194 .",
    "for any @xmath42 , we have that @xmath195\\\\ & \\leq&\\frac{n_{i , j+1}+n_{ij}}{n}\\min(\\alpha_{i , j+1},1-\\alpha _ { i , j+1})\\bar{g}(\\phi^0)\\\\ & \\leq&\\frac{\\delta_{\\lambda}^0}{2}\\min\\biggl(\\frac { n_{i , j+1}}{n},\\frac{n_{ij}}{n}\\biggr)\\bar{g}(\\phi^0)\\\\ & \\leq&\\frac{1}{2}\\biggl(\\frac{{\\delta_{\\lambda}^0}}{2}\\biggr)^2\\bar { g}(\\phi^0).\\end{aligned}\\ ] ]    combining the results from the two cases of @xmath196 and @xmath197 , it follows that @xmath198 and    @xmath199 \\leq-\\frac{\\delta_{\\lambda}^0}{2}\\min\\biggl[\\rho(\\phi,\\phi^0),-\\frac{\\delta _ { \\lambda}^0}{4}\\bar{g}(\\phi^0)\\\\\\biggr ] .\\ ] ]    note that ( [ eqb1 ] ) can be simplified .",
    "if we define @xmath200 then we have that @xmath201 .",
    "it follows from inequality ( [ eqb1 ] ) that @xmath202.\\ ] ] if @xmath203 , then we have that @xmath204 if @xmath205 , then @xmath206 .",
    "letting @xmath207 inequality ( [ eqb1 ] ) gives that @xmath208 .",
    "setting @xmath209 , we finally have that @xmath210 which concludes the proof .",
    "proof of lemma [ lem2 ] with part 1 of assumption [ a22 ] in mind , equation ( [ eq6 ] ) can be achieved by induction with respect to @xmath211 .",
    "the induction method is similar to the one used in mricz , serfling and stout  ( @xcite ) , so its proof is omitted here . using part 2 of assumption [ a22 ] , equation ( [ eq7 ] )",
    "can be proven similarly by the same induction method .",
    "proof of theorem [ thm1 ] let @xmath212 then , for any @xmath213 , it follows from lemma [ lem1 ] that @xmath214 therefore , we obtain that @xmath215\\}\\biggr|>\\frac{c_1}{2}\\delta\\biggr ) \\\\ & & \\qquad{}+p_r\\biggl(\\sum_{j=1}^{k+1}\\frac{1}{n}\\biggl|\\sum _ { i = n_{j-1}^0 + 1}^{n_j^0}\\{\\log f_j(\\psi^0,\\theta_j^0;x_i)-e[\\log f_j(\\psi^0,\\theta_j^0;x_i)]\\biggr|>\\frac{c_1}{2}\\delta\\biggr)\\\\ & & \\quad\\leq\\sum_{j=1}^{k+1}p_r\\biggl(\\max_{0\\leq n_{j-1}<n_j\\leq n,\\theta_j\\in \\theta_j,\\psi\\in\\psi}\\frac{1}{n}\\biggl|\\sum_{i = n_{j-1}+1}^{n_j}\\{\\log f_j(\\psi , \\theta_j;x_i)-e[\\log f_j(\\psi,\\theta_j;x_i)]\\}\\biggr|\\\\ & & \\quad\\hspace*{43pt}>\\frac{c_1\\delta } { 2(k+1)}\\biggr)\\\\ & & \\qquad{}+\\sum_{j=1}^{k+1}p_r\\biggl(\\frac{1}{n}\\biggl|\\sum _ { i = n_{j-1}^0 + 1}^{n_j^0}\\{\\log f_j(\\psi^0,\\theta_j^0;x_i)-e[\\log f_j(\\psi^0,\\theta_j^0;x_i)]\\}\\biggr|>\\frac{c_1\\delta}{2(k+1)}\\biggr ) .\\end{aligned}\\ ] ]    it follows from lemma [ lem2 ] that @xmath216 ^ 2\\biggl(\\sum_{j=1}^{k+1}a_j\\biggr)n^{r-2}\\rightarrow0\\qquad \\mbox { as } n\\rightarrow+\\infty,\\ ] ] noting that @xmath47 .    for @xmath73 , we similarly obtain that @xmath217\\}\\biggr|\\\\ & & { } \\hspace*{51pt}>\\frac{c_2\\delta } { 2(k+1)}\\biggr)\\\\ & & \\qquad{}+\\sum_{j=1}^{k+1}p_r\\biggl(\\frac{1}{n}\\biggl|\\sum _ { i = n_{j-1}^0 + 1}^{n_j^0}\\{\\log f_j(\\psi^0,\\theta_j^0;x_i)-e[\\log f_j(\\psi^0,\\theta_j^0;x_i)]\\}\\biggr|>\\frac{c_2\\delta}{2(k+1)}\\biggr).\\end{aligned}\\ ] ] similarly , lemma [ lem2 ] shows that @xmath218 .",
    "noting the fact that @xmath219 if and only if @xmath220 and @xmath221 , it follows that @xmath58 and @xmath222 for @xmath11 , which completes the proof .",
    "proof of theorem [ thm2 ] let us first define @xmath223 for any @xmath213 .",
    "because of the consistency of @xmath224 , we need to consider only those terms whose observations are in @xmath225 , @xmath226 and @xmath227 for all @xmath5 in equation ( [ eqn2.5 ] ) .",
    "therefore , we have @xmath228\\\\ & & \\qquad{}\\hspace*{78pt}-\\frac{1}{n}\\sum_{t\\in\\tilde{n}_{jj}}[\\log f_j(\\psi ^0,\\theta^0_j;x_t)-e(\\log f_j(\\psi^0,\\theta^0_j;x_t))]\\\\ & & \\qquad{}\\hspace*{78pt}+\\frac{1}{3(k+1)}j_1\\biggr\\}>0\\biggr)\\\\ & & \\qquad{}+\\sum_{j=2}^{k+1}p_r\\biggl(\\max_{\\lambda\\in\\lambda_{\\delta , n},\\phi\\in\\phi}\\biggl\\{\\frac{1}{n}\\sum_{t\\in\\tilde{n}_{j , j-1}}[\\log f_j(\\psi , \\theta_j;x_t)-e(\\log f_j(\\psi,\\theta_j;x_t))]\\\\ & & \\qquad{}\\hspace*{92pt}-\\frac{1}{n}\\sum_{t\\in\\tilde{n}_{j , j-1}}[\\log f_{j-1}(\\psi ^0,\\theta^0_{j-1};x_t)-e(\\log f_{j-1}(\\psi^0,\\theta^0_{j-1};x_t))]\\\\ & & \\qquad{}\\hspace*{92pt}+\\frac { 1}{3k}j_1\\biggr\\}>0\\biggr)\\\\ & & \\qquad{}+\\sum_{j=1}^{k}p_r\\biggl(\\max_{\\lambda\\in\\lambda_{\\delta , n},\\phi \\in\\phi}\\biggl\\{\\frac{1}{n}\\sum_{t\\in\\tilde{n}_{j , j+1}}[\\log f_j(\\psi,\\theta _ j;x_t)-e(\\log f_j(\\psi,\\theta_j;x_t))]\\\\ & & \\qquad{}\\hspace*{91pt}-\\frac{1}{n}\\sum_{t\\in\\tilde{n}_{j , j+1}}[\\log f_{j+1}(\\psi ^0,\\theta^0_{j+1};x_t)-e(\\log f_{j+1}(\\psi^0,\\theta^0_{j+1};x_t))]\\\\ & & \\qquad{}\\hspace*{92pt}+\\frac { 1}{3k}j_1\\biggr\\}>0\\biggr)\\\\ & & \\quad \\equiv\\sum_{j=1}^{k+1}i_{1j}+\\sum_{j=2}^{k+1}i_{2j}+\\sum _ { j=1}^{k}i_{3j}.\\end{aligned}\\ ] ]    first , consider the probability formulas @xmath229 in the above equation for any @xmath11 .",
    "the consistency of @xmath224 allows us to restrict our attention to the case @xmath230 . for this case",
    ", we have that @xmath231    therefore , we obtain that @xmath232\\\\ & & \\qquad{}\\hspace*{16pt}-v(\\psi^*,\\theta^*_j;\\psi ^0,\\theta^0_j)\\}>\\frac{n^0_j - n^0_{j-1}}{6(k+1 ) } |v(\\psi^*,\\theta _ j^*;\\psi^0,\\theta^0_j)|\\biggr)\\\\ & \\leq & p_r\\biggl(\\max_{n_{j-1}^0\\leq s <",
    "t\\leq n_j^0,\\psi\\in\\psi , \\theta_j\\in\\theta_j}\\sum_{i = s+1}^t\\{[\\log f_j(\\psi,\\theta _ j;x_t)-\\log f_j(\\psi^0,\\theta^0_j;x_t)]\\\\ & & \\qquad{}\\hspace*{112pt}-v(\\psi,\\theta_j;\\psi^0,\\theta^0_j)\\}>\\frac { e}{6(k+1)}(n^0_j - n^0_{j-1})\\biggr),\\end{aligned}\\ ] ] where @xmath233 , @xmath234 , @xmath235 and @xmath236 are , respectively , the maximizing values of @xmath237 , @xmath24 , @xmath238 and @xmath101 obtained through the maximization .",
    "equation ( [ eq7 ] ) of lemma [ lem2 ] can then be applied to show that @xmath239 as @xmath240 .",
    "next , consider the probability formula @xmath241 for any @xmath242 . in this case , @xmath243 .",
    "we have that @xmath244+\\frac{1}{6k}j_1\\biggr\\}>0\\biggr)\\\\ & & { } \\hspace*{18pt}+p_r\\biggl(\\max_{\\lambda\\in\\lambda_{\\delta , n},\\phi\\in\\phi}\\biggl\\ { -\\frac{1}{n}\\sum_{t\\in\\tilde{n}_{j , j-1}}[\\log f_{j-1}(\\psi,\\theta _ { j-1};x_t)-e(\\log f_{j-1}(\\psi,\\theta_{j-1};x_t))]\\\\ & & \\qquad{}\\hspace*{74pt}+\\frac{1}{6k}j_1\\biggr\\}>0\\biggr)\\\\ & \\equiv & i_{2j}^{(1)}+i_{2j}^{(2)}.\\end{aligned}\\ ] ]    @xmath245 and @xmath246 can be handled in the same way , so we just show how to handle @xmath245 .",
    "only two cases have to be considered .    if @xmath247 , then @xmath248\\biggr|\\\\ & & { } \\quad\\hspace*{32pt}>\\frac{c_1\\delta}{6k}\\biggr).\\end{aligned}\\ ] ] equation ( [ eq6 ] ) of lemma [ lem2 ] gives that @xmath249 as @xmath250 .",
    "if @xmath251 for the other case , then @xmath252 . therefore , we obtain that @xmath253\\\\ & & \\qquad{}\\hspace*{113pt}-\\frac{c_1}{6k}\\biggr)>0\\biggr)\\\\ & \\leq & p_r\\biggl(\\max_{n_{j-1}\\leq s < t\\leq n^0_{j-1},\\theta _ j\\in\\theta_j,\\psi\\in\\psi}\\biggl|\\sum_{i = s+1}^{t}[\\log f_j(\\psi,\\theta _",
    "j;x_i)-e(\\log f_j(\\psi,\\theta_j;x_i))]\\biggr|\\\\ & & \\quad{}\\hspace*{2pt}>\\frac { c_1}{6k}(n^0_{j-1}-n_{j-1})\\biggr),\\end{aligned}\\ ] ] which converges to zero as @xmath254 , by equation ( [ eq6 ] ) of lemma [ lem2 ] .",
    "@xmath255 can be handled in the same way as @xmath241 .",
    "therefore , theorem [ thm2 ] is proved .",
    "proof of theorem [ thm3 ] we first have the expansion @xmath256(\\hat{\\phi } -\\phi^0).\\ ] ] the fact that @xmath257 then gives that @xmath258^{-1}\\frac{\\hat{\\ell}_{\\phi}(\\psi^0,\\theta^0)}{\\sqrt{n}}.\\ ] ]    now , consider the limit of @xmath259 .",
    "we have that @xmath260 + \\frac{1}{\\sqrt{n}}\\ell^0_{\\phi}(\\psi^0,\\theta^0).\\nonumber\\end{aligned}\\ ] ]    because of the consistency of @xmath224 , we can assume that @xmath261 for @xmath33 .",
    "it is then straightforward to obtain that @xmath262\\\\ & & \\quad=\\frac{1}{\\sqrt{n}}\\sum_{j=1}^{k+1}\\bigl[\\hat{\\ell}_{\\phi } ^{(j)}(\\psi^0,\\theta_j^0)-\\ell_{\\phi}^{(j)}(\\psi^0,\\theta _ j^0)\\bigr]\\nonumber\\\\ & & \\quad = \\frac{1}{\\sqrt{n}}\\sum_{j=1}^{k+1}\\biggl\\ { i(\\hat{n}_j\\geq n^0_j,\\hat{n}_{j-1}\\geq n^0_{j-1})\\\\ & & { } \\hspace*{56pt}\\times\\biggl[\\sum_{i = n^0_j+1}^{\\hat{n}_j}\\frac{\\partial}{\\partial\\phi } \\log f_j(\\psi^0,\\theta^0_j;x_i ) -\\sum_{i = n^0_{j-1}+1}^{\\hat{n}_{j-1}}\\frac{\\partial}{\\partial\\phi}\\log f_j(\\psi^0,\\theta^0_j;x_i)\\biggr]\\\\ & & { } \\hspace*{58pt}+i(\\hat{n}_j\\geq n^0_j,\\hat{n}_{j-1 } < n^0_{j-1})\\\\ & & { } \\hspace*{67pt}\\times\\biggl[\\sum_{i = n^0_j+1}^{\\hat{n}_j}\\frac{\\partial}{\\partial\\phi } \\log f_j(\\psi^0,\\theta^0_j;x_i)+\\sum_{i=\\hat { n}_{j-1}+1}^{n^0_{j-1}}\\frac{\\partial}{\\partial\\phi}\\log f_j(\\psi ^0,\\theta^0_j;x_i)\\biggr]\\\\ & & { } \\hspace*{58pt}+i(\\hat{n}_j < n^0_j,\\hat{n}_{j-1}\\geq n^0_{j-1})\\\\ & & { } \\hspace*{66pt}\\times\\biggl[-\\sum_{i=\\hat{n}_j+1}^{n^0_j}\\frac{\\partial}{\\partial\\phi } \\log f_j(\\psi^0,\\theta^0_j;x_i)-\\sum_{i = n^0_{j-1}+1}^{\\hat { n}_{j-1}}\\frac{\\partial}{\\partial\\phi}\\log f_j(\\psi^0,\\theta ^0_j;x_i)\\biggr]\\\\ & & { } \\hspace*{58pt}+i(\\hat{n}_j < n^0_j,\\hat{n}_{j-1 } < n^0_{j-1})\\\\ & & { } \\hspace*{68pt}\\times\\biggl[-\\sum_{i=\\hat{n}_j+1}^{n^0_j}\\frac{\\partial}{\\partial\\phi } \\log f_j(\\psi^0,\\theta^0_j;x_i)+\\sum_{i=\\hat { n}_{j-1}+1}^{n^0_{j-1}}\\frac{\\partial}{\\partial\\phi}\\log f_j(\\psi ^0,\\theta^0_j;x_i)\\biggr]\\biggr\\}.\\end{aligned}\\ ] ]    it follows from theorem [ thm2 ] that @xmath263=\\frac{1}{\\sqrt{n}}\\mathrm{o}_p(1),\\ ] ] which converges to zero in probability as @xmath264 .",
    "since @xmath265 it follows that @xmath266    in a similar way , we easily obtain that @xmath267 therefore , we have that @xmath268 proving the result .",
    "h. he thanks professor peter hall for his support of this research .",
    "the research of h. he was financially supported by a mascos grant from the australian research council .",
    "the research of t.a .",
    "severini was supported by the u.s",
    ". national science foundation ."
  ],
  "abstract_text": [
    "<S> models with multiple change points are used in many fields ; however , the theoretical properties of maximum likelihood estimators of such models have received relatively little attention . </S>",
    "<S> the goal of this paper is to establish the asymptotic properties of maximum likelihood estimators of the parameters of a multiple change - point model for a general class of models in which the form of the distribution can change from segment to segment and in which , possibly , there are parameters that are common to all segments . </S>",
    "<S> consistency of the maximum likelihood estimators of the change points is established and the rate of convergence is determined ; the asymptotic distribution of the maximum likelihood estimators of the parameters of the within - segment distributions is also derived . </S>",
    "<S> since the approach used in single change - point models is not easily extended to multiple change - point models , these results require the introduction of those tools for analyzing the likelihood function in a multiple change - point model . </S>"
  ]
}