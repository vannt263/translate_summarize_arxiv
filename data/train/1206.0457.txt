{
  "article_text": [
    "in recent years , independent component analysis ( ica ) has seen an explosion in its popularity in diverse fields such as signal processing , machine learning , and medical imaging , to name a few . for a wide - ranging list of algorithms and applications of ica ,",
    "see the monograph by @xcite . in the ica paradigm",
    ", one observes a random vector @xmath4 that can be expressed as a non - singular linear transformation of @xmath5 mutually independent latent factors @xmath6 ; thus @xmath7 where @xmath8 and @xmath1 is a @xmath9 full rank matrix often referred to as the mixing matrix .",
    "as such , ica postulates the following model for the probability distribution @xmath10 of @xmath11 : for any borel set @xmath12 in @xmath13 , @xmath14 where @xmath15 is the so - called unmixing matrix , and @xmath16 are the univariate probability distributions of the latent factors @xmath17 respectively .    the goal of ica , as in other blind source separation problems , is to infer from a sample @xmath18 of independent observations of @xmath11 , the independent factors @xmath19 , or equivalently the unmixing matrix @xmath20 .",
    "this task is typically accomplished by first postulating a certain parametric family for the marginal probability distributions @xmath16 , and then optimising a contrast function involving @xmath21 .",
    "the contrast functions are often chosen to represent the mutual information as measured by kullback ",
    "leibler divergence or maximum entropy ; or non - gaussianity as measured by kurtosis or negentropy .",
    "alternatively , in recent years , methods for ica have also been developed which assume @xmath22 have smooth ( log ) densities , e.g. @xcite , @xcite , @xcite and @xcite .",
    "although more flexible than their aforementioned parametric peers , there remain unsettling questions about what happens if the smoothness assumptions on the marginal densities are violated , which may occur , in particular , when some of the marginal probability distributions @xmath22 have atoms .",
    "another issue is that , in common with most other smoothing methods , a choice of tuning parameters is required to balance the fidelity to the observed data and the smoothness of the estimated marginal densities , and it is notoriously difficult to select these tuning parameters appropriately in practice .    in this paper , we argue that these assumptions and tuning parameters are unnecessary , and propose a new paradigm for ica , based on the notion of nonparametric maximum likelihood , that is free of these burdens .",
    "in fact , we show that the usual nonparametric ( empirical ) likelihood approach does not work in this context , and instead we proceed under the working assumption that the marginal distributions of @xmath6 are log - concave . more specifically , we propose to estimate @xmath20 by maximising @xmath23 over all @xmath9 non - singular matrices @xmath24 , and univariate log - concave densities @xmath25 .",
    "remarkably , from the point of view of estimating the unmixing matrix @xmath20 , it turns out that it makes no difference whether or not this hypothesis of log - concavity is correctly specified .",
    "the key to understanding how our approach works is to study what we call the log - concave ica projection of a distribution on @xmath26 onto the set of densities that satisfy the ica model with log - concave marginals . in section  [ sec : notation ] below , we define this projection carefully , and give necessary and sufficient conditions for it to make sense . in section  [ sec : pdica ] , we prove that the log - concave projection of a distribution from the ica model preserves both the ica structure and the unmixing matrix",
    ". finally , in section  [ sec : pd ] , we derive a continuity property of log - concave ica projections , which turns out to be important for understanding the theoretical properties of our ica procedure .    our ica estimating procedure uses the log - concave ica projection of the empirical distribution of the data , and is studied in section  [ sec : estproc ] . after explaining why the usual empirical likelihood approach can not be used , we prove the consistency of our method .",
    "we also present an iterative algorithm for the computation of our estimator .",
    "our simulation studies in section  [ sec : sim ] confirm our theoretical results and show that the proposed method compares favourably with existing methods .",
    "our proposed nonparametric maximum likelihood estimator can be viewed as the projection of the empirical distribution of @xmath18 onto the space of ica distributions with log - concave densities . to understand its behavior , it is useful to study the properties of such projections in general .",
    "let @xmath27 be the set of probability distributions @xmath10 on @xmath28 satisfying @xmath29 and @xmath30 for all hyperplanes @xmath31 , i.e. the probability measures in @xmath32 that have finite mean and are not supported in a translate of a lower dimensional linear subspace of @xmath32 . here and throughout",
    ", @xmath33 denotes the euclidean norm on @xmath28 , and we will be interested in the cases @xmath34 and @xmath35 .",
    "further , let @xmath36 denote the set of non - singular @xmath37 real matrices .",
    "we use upper case letters to denote matrices in @xmath36 , and the corresponding lower case letters with subscripts to denote rows : thus @xmath38 is the @xmath39th row of @xmath40 .",
    "let @xmath41 denote the class of borel sets on @xmath28 .",
    "then the ica model @xmath42 is defined to be the set of @xmath43 of the form @xmath44 for some @xmath40 and @xmath45 . as shown by (",
    "* theorem  2.2 ) , the condition @xmath43 is necessary and sufficient for the existence of a unique upper semi - continuous and log - concave density that is the closest to @xmath10 in the kullback  leibler sense .",
    "more precisely , let @xmath46 denote the class of all upper semi - continuous , log - concave densities with respect to lebesgue measure on @xmath28 .",
    "then the projection @xmath47 given by @xmath48 is well - defined and surjective . in what follows",
    ", we refer to @xmath49 as the log - concave projection operator and @xmath50 as the log - concave projection of @xmath10 . by a slight abuse of notation",
    ", we also use @xmath51 to denote the log - concave projection from @xmath52 to @xmath53 .",
    "although the log - concave projection operator does play a role in this paper , our main interest is in a different projection , onto the subset of @xmath54 consisting of those densities that satisfy the ica model .",
    "this class is given by @xmath55 note that , in this representation , if @xmath11 has density @xmath56 , then @xmath57 has density @xmath58 .",
    "the corresponding log - concave ica projection operator @xmath59 is defined for any distribution @xmath10 on @xmath26 by @xmath60 we also write @xmath61 .    [ prop : cases ]    1 .   if @xmath62 , then @xmath63 and @xmath64 .",
    "2 .   if @xmath65 , but @xmath66 for some hyperplane @xmath31 , then @xmath67 and @xmath68 .",
    "3 .   if @xmath43 , then @xmath69 and @xmath70 defines a non - empty , proper subset of @xmath71 .    in view of proposition",
    "[ prop : cases ] , and to avoid lengthy discussion of trivial exceptional cases , we henceforth consider @xmath59 as being defined on @xmath72 .",
    "in contrast to @xmath73 , which defines a unique element of @xmath54 , the log - concave ica projection operator @xmath70 may not define a unique element of @xmath71 , even for @xmath43 .",
    "for instance , consider the situation where @xmath10 is the uniform distribution on the closed unit disk in @xmath74 equipped with the euclidean norm . here",
    ", the spherical symmetry means that the choice of @xmath40 is arbitrary .",
    "in fact , after a straightforward calculation , it can be shown that @xmath70 consists of those @xmath56 where , in the representation  ( [ eq : fdica ] ) , @xmath40 is arbitrary and @xmath75 are given by @xmath76\\}}$ ] .",
    "it is certainly possible to make different choices of @xmath20 that yield different elements of @xmath77 .",
    "this example shows that , in general , we must think of @xmath70 as defining a subset of @xmath77 .",
    "the relationship between the spaces introduced above and the projection operators is illustrated in the diagram below : @xmath78 our next subsection studies the restriction of @xmath79 to @xmath42 , denoted @xmath80 ; section  [ sec : pdica ] examines @xmath79 more generally as a map on @xmath72 .",
    "our first result in this subsection characterises @xmath80 .",
    "[ thm : pdica ] if @xmath81 , then @xmath70 defines a unique element of @xmath71 .",
    "the map @xmath80 is surjective , and coincides with @xmath82 . moreover , suppose that @xmath81 , so that @xmath83 for some @xmath40 and @xmath45",
    ". then @xmath84 can be written as @xmath85 where @xmath86 .",
    "it is interesting to observe that log - concave projection operator @xmath51 preserves the ica structure .",
    "but perhaps the most important aspect of this result is the fact that the same unmixing matrix @xmath20 can be used to represent both the original ica model and its log - concave projection .",
    "this observation lies at the heart of the rationale for our approach to ica .",
    "a remaining concern is that the unmixing matrix may not be identifiable .",
    "for instance , applying the same permutation to the rows of @xmath20 and the vector of marginal distributions @xmath87 leaves the distribution unchanged ; similarly , the same effect occurs if we multiply any of the rows of @xmath20 by a scaling factor and applying the corresponding scaling factor to the relevant marginal distribution .",
    "the question of identifiability for ica models was first addressed by @xcite , who assumed that @xmath20 is orthogonal , and was settled in the general case by @xcite .",
    "one way to state their result is as follows : suppose that a probability measure @xmath10 on @xmath26 has two representations as @xmath88 where @xmath20 , @xmath89 and @xmath90 are probability measures on @xmath91 .",
    "then the pair of conditions that @xmath22 are not dirac point masses and not more than one of @xmath22 is gaussian is necessary and sufficient for the existence of a permutation @xmath92 of @xmath93 and scaling vector @xmath94 such that @xmath95 for all @xmath96 , and @xmath97 .",
    "when such a permutation and scaling factor exist for any two ica representations of @xmath10 , we say that _ the ica representation of @xmath10 is identifiable _ , or simply that _",
    "p is identifiable_. by analogy , we define @xmath56 to be identifiable if not more than one of @xmath25 in the representation  ( [ eq : fdica ] ) is gaussian .",
    "our next result shows that @xmath79 preserves the identifiability of the ica model .",
    "together with theorem  [ thm : pdica ] , we see that if @xmath81 is identifiable , then the unmixing matrices of @xmath81 and @xmath70 are identical up to the permutation and scaling transformations described above .",
    "[ thm : identifiability ] let @xmath98 .",
    "then @xmath70 is identifiable if and only if @xmath10 is identifiable .",
    "we now consider the general log - concave ica projection @xmath79 defined on @xmath72 .",
    "define the mallows distance @xmath5 ( also known as the wasserstein distance ) between probability measures @xmath10 and @xmath99 on @xmath26 with finite mean by @xmath100 where the infimum is taken over all pairs @xmath101 of random vectors @xmath102 and @xmath103 on a common probability space . recall that @xmath104 if and only if both @xmath105 and @xmath106 .",
    "we are interested in the continuity of @xmath79 .",
    "[ prop : pd ] let @xmath107 be probability measures in @xmath72 with @xmath104 as @xmath108",
    ". then @xmath109 .",
    "moreover , @xmath110 as @xmath108 .",
    "the second part of this proposition says that any element of @xmath111 is arbitrarily close in total variation distance to some element of @xmath70 once @xmath112 is sufficiently large . in the special case",
    "where @xmath70 consists of only a single element , we can say more .",
    "it is convenient to let @xmath113 denote the set of permutations of @xmath93 , and write @xmath114 if @xmath40 and @xmath115 can be used to give an ica representation of @xmath56 in ( [ eq : fdica ] ) .",
    "similarly , we write @xmath116 if @xmath40 and @xmath45 represent @xmath81 in ( [ eq : pdica ] ) .",
    "[ thm : conv ] suppose that @xmath81 , and write @xmath84",
    ". if @xmath117 are such that @xmath104 , then @xmath118 suppose further that @xmath10 is identifiable and that @xmath116",
    ". then @xmath119 for each @xmath120 , where @xmath86 . as a consequence , for sufficiently large @xmath112",
    ", every @xmath121 is identifiable .",
    "the first part of theorem  [ thm : conv ] show that if @xmath122 and @xmath123 are close in mallows distance , then every @xmath124 is close to the corresponding ( unique ) log - concave ica projection @xmath125 in total variation distance .",
    "the second part shows further that if @xmath10 is identifiable , then up to permutation and scaling , every @xmath124 and every choice of unmixing matrix @xmath126 and marginal densities @xmath127 in the ica representation of @xmath128 is close to the unmixing matrix @xmath20 and marginal densities @xmath25 in the ica representation of @xmath129 .    to conclude this subsection",
    ", we remark that , by analogy with the situation when @xmath81 described in theorem  [ thm : pdica ] , if @xmath43 and @xmath102 , any @xmath130 can be written as @xmath85 for some @xmath40 , where @xmath86 , and @xmath131 is the marginal distribution of @xmath57 .",
    "this observation reduces the maximisation problem involved in computing @xmath70 to a finite - dimensional one ( over @xmath40 ) , and follows because @xmath132",
    "we are now in position to study the proposed nonparametric maximum likelihood estimator .",
    "now assume @xmath133 are independent copies of a random vector @xmath134 satisfying the ica model .",
    "thus @xmath0 , where @xmath135 and @xmath136 has independent components . in this section ,",
    "we study a nonparametric maximum likelihood estimator of @xmath20 and the marginal distributions @xmath22 of @xmath6 based on @xmath18 , where @xmath137 .",
    "we start by noting that the usual nonparametric maximum likelihood estimate does not work . indeed , in the spirit of empirical likelihood @xcite , it would suffice to consider , for a given @xmath138 , estimates @xmath139 of the marginal distribution @xmath131 , supported on @xmath140 .",
    "this leads to the nonparametric likelihood @xmath141 where @xmath142 .",
    "let @xmath143 denote a subset of @xmath144 distinct indices in @xmath145 , and let @xmath146 denote the @xmath147 matrix obtained by extracting the columns of @xmath148 with indices in @xmath143 .",
    "now let @xmath149 denote the @xmath37 matrix obtained by removing the @xmath39th column of @xmath146 .",
    "let @xmath150 have @xmath39th row @xmath151 , for @xmath120 , where @xmath152 is a @xmath5-vector of ones .",
    "our next result shows that every @xmath153 corresponds to a maximiser of the nonparametric likelihood  ( [ eq : nonlike ] ) .",
    "[ prop : emplike ] suppose that @xmath18 are in general position .",
    "then for any choice @xmath143 of @xmath144 distinct indices in @xmath145 , there exist @xmath154 such that @xmath155 maximises @xmath156 .",
    "if @xmath11 has a density with respect to lebesgue measure on @xmath26 , then with probability 1 , every subset of @xmath18 of size @xmath144 is in general position . on the other hand",
    ", there is no reason for different choices of @xmath143 to yield similar estimates @xmath153 , so we can not hope for such an empirical likelihood - based procedure to be consistent .    as a remedy",
    ", we propose to estimate @xmath157 by @xmath158 , where @xmath159 denotes the empirical distribution of @xmath160 .",
    "more explicitly , we estimate the unmixing matrix and the marginals by maximising the log - likelihood @xmath161 over @xmath40 and @xmath115 .",
    "note from proposition  [ prop : cases ] that @xmath158 exists as a proper subset of @xmath77 once the convex hull of @xmath18 is @xmath5-dimensional , which happens with probability 1 for sufficiently large @xmath112 . as a direct consequence of theorem  [ thm : conv ] and",
    "the fact that @xmath162 , we have the following consistency result .",
    "suppose that @xmath157 is identifiable and is represented by @xmath163 and @xmath164 .",
    "then for any maximiser @xmath165 of @xmath166 over @xmath40 and @xmath167 , there exist a permutation @xmath168 of @xmath93 and scaling factors @xmath169 such that @xmath170 for @xmath120 , where @xmath171 .",
    "pre - whitening is a standard pre - processing technique in the ica literature ; see @xcite or @xcite . in this subsection",
    ", we explain the rationale for pre - whitening and the simplifications it provides .",
    "assume for now that @xmath81 and @xmath172 , and let @xmath173 denote the ( positive - definite ) covariance matrix corresponding to @xmath10 . consider the ica model @xmath0 , where @xmath102 , the mixing matrix @xmath1 is non - singular and @xmath174 has independent components with @xmath175 .",
    "assuming without loss of generality that each component of @xmath2 has unit variance , we can write @xmath176 , say , where @xmath177 belongs to the set @xmath178 of orthogonal @xmath37 matrices . thus the unmixing matrix @xmath20 belongs to the set @xmath179 .",
    "it follows that , if @xmath173 were known , we could maximise @xmath180 with the restriction that @xmath181 .",
    "in practice , @xmath173 is typically unknown , but we can estimate it using the sample covariance matrix @xmath182 . for @xmath112 large enough that the convex hull of @xmath18 is @xmath5-dimensional , we can therefore consider maximising @xmath183 over @xmath184 and @xmath115 .",
    "denote such a maximiser by @xmath185 .",
    "the corollary below shows that , under a second moment condition , @xmath186 and @xmath187 have the same asymptotic properties as the original estimators @xmath188 and @xmath189 .",
    "[ cor : hats ] suppose that @xmath190 is identifiable , is represented by @xmath163 and @xmath164 and that @xmath191 . then with probability 1 for sufficiently large @xmath112",
    ", a maximiser @xmath185 of @xmath166 over @xmath184 and @xmath167 exists .",
    "moreover , for any such maximiser , there exist a permutation @xmath192 of @xmath93 and scaling factors @xmath193 such that @xmath194 where @xmath171 .    an alternative , equivalent way of computing @xmath185 is to _ pre - whiten _ the data by replacing @xmath18 with @xmath195 , and then maximise @xmath196 over @xmath197 and @xmath198 . if @xmath199 is such a maximiser , we can then set @xmath200 and @xmath201 .",
    "note that pre - whitening breaks down the estimation of the @xmath202 parameters in @xmath20 into two stages : first , we use @xmath182 to estimate the @xmath203 free parameters of the symmetric , positive definite matrix @xmath173 , leaving only the maximisation over the @xmath204 free parameters of @xmath197 at the second stage .",
    "the advantage of this approach is that it facilitates more stable maximisation algorithms , such as the one described in the next subsection .      in this subsection , we address the challenge of maximising @xmath205 over @xmath206 and @xmath198 . as a starting point , we choose @xmath20 to be randomly distributed according to haar measure on the set @xmath178 of @xmath37 orthogonal matrices .",
    "a simple way of generating @xmath20 with this distribution is to generate a @xmath37 matrix @xmath207 whose entries are independent @xmath208 random variables , compute the @xmath209-factorisation @xmath210 , and let @xmath211 .",
    "our proposed algorithm then alternates between maximising the log - likelihood over @xmath25 for fixed @xmath20 , and then over @xmath20 for fixed @xmath25 .",
    "the first of these steps is straightforward given theorem  [ thm : pdica ] and the recent work on log - concave density estimation : we set @xmath58 to be the log - concave maximum likelihood estimator of the data @xmath140 .",
    "this can be computed using the ` r ` package ` logcondens ` @xcite .",
    "this leaves the challenge of updating @xmath206 . in order to describe our proposal ,",
    "we recall some basic facts from differential geometry .",
    "the set @xmath178 is a @xmath204-dimensional submanifold of @xmath212 .",
    "the tangent space at @xmath206 is @xmath213 .",
    "in fact , if we define the natural inner product @xmath214 on @xmath215 by @xmath216 , then @xmath178 becomes a riemannian manifold .",
    "( note that if we think of @xmath217 and @xmath218 as vectors in @xmath212 , then this inner product is simply the euclidean inner product . )",
    "there is no loss of generality in assuming @xmath20 belongs to the riemannian manifold @xmath219 , the set of special orthogonal matrices having determinant 1 .",
    "we can now define geodesics on @xmath219 , recalling that the matrix exponential is given by @xmath220 the unique geodesic passing through @xmath221 with tangent vector @xmath222 ( where @xmath223 ) is the map @xmath224 \\rightarrow so(d)$ ] given by @xmath225 .",
    "we update @xmath20 by moving along a geodesic in @xmath219 , but need to choose an appropriate skew - symmetric matrix @xmath226 , which ideally should ( at least locally ) give a large increase in the log - likelihood .",
    "the key to finding such a direction is proposition  [ prop : diff ] below . to set the scene for this result , observe that for @xmath227 $ ]",
    ", we can write @xmath228 for some @xmath229 ( e.g. * ? ? ? * ) .",
    "since we may assume that @xmath230 are strictly decreasing , the minimum in   is attained in either one or two indices .",
    "it is convenient to let @xmath231 .",
    "[ prop : diff ] consider the map @xmath232 given by @xmath233 let @xmath226 be a skew - symmetric matrix and let @xmath234 denote the @xmath39th row of @xmath222 . if @xmath235 , let @xmath236 denote the unique element of @xmath237 . if @xmath238 , write @xmath239 .",
    "if @xmath240 , let @xmath241 , where @xmath242 ; if @xmath243 , let @xmath241 , where @xmath244 .",
    "then the one - sided directional derivative of @xmath245 at @xmath20 in the direction @xmath222 is @xmath246    for @xmath247 , let @xmath248 denote the @xmath37 matrix with @xmath249 , @xmath250 and all other entries equal to zero .",
    "then @xmath251 forms an orthonormal basis for the set of skew - symmetric matrices .",
    "let @xmath252 .",
    "we choose @xmath253 to maximise @xmath254 .",
    "we therefore update @xmath20 with @xmath255 , and it remains to select @xmath256 .",
    "this we propose to choose by means of a backtracking line search .",
    "specifically , we fix @xmath257 and @xmath258 , and if @xmath259 we accept a move from @xmath20 to @xmath255 .",
    "otherwise , we successively reduce @xmath256 by a factor of @xmath260 until is satisfied , and then move to @xmath255 . in our implementation , we used @xmath261 and @xmath262",
    ".    our algorithm produces a sequence @xmath263 .",
    "we terminate the algorithm once @xmath264 where , in our implementation , we chose @xmath265 .",
    "as with other ica algorithms , global convergence is not guaranteed , so we used 10 random starting points and took the solution with the highest log - likelihood .",
    "to illustrate the practical merits of our proposed nonparametric maximum likelihood estimation method for ica models , we conducted several sets of numerical experiments . to fix ideas",
    ", we focus on two - dimensional signals , that is @xmath266 . the components of the signal were generated independently , and then rotated by @xmath267 , so the mixing matrix is @xmath268 our goal is to reconstruct the signal and estimate @xmath1 , or equivalently @xmath269 , based on @xmath270 observations of the rotated input .",
    "we first consider a typical example in the ica literature where the density of each component of the true signal is uniform on the interval @xmath271 $ ] .",
    "the top left panel of figure  [ fig : uniform ] plots the @xmath272 simulated signal pairs , while the top right panel gives the rotated observations .",
    "the bottom left panel plots the recovered signal using the proposed nonparametric maximum likelihood method . also included in the bottom right panel of the figure",
    "are the estimated marginal densities of the two sources of signal .",
    "figure  [ fig : exp ] gives corresponding plots when the marginals have an @xmath273 distribution .",
    "we note that both uniform and exponential distributions have log - concave densities and therefore our method not only recovers the mixing matrix but also accurately estimates the marginal densities , as can be seen in figures  [ fig : uniform ] and  [ fig : exp ] .        to investigate the robustness of the proposed method when the marginal components do not have log - concave densities ,",
    "we repeated the simulation in two other cases , with the true signal simulated firstly from a @xmath274-distribution with two degrees of freedom scaled by a factor of @xmath275 and secondly from a mixture of normals distribution @xmath276 . figures  [ fig : t2 ] and  [ fig : mix ] show that , in both cases , the misspecification of the marginals does not affect the recovery of the signal .",
    "also , the estimated marginals represent estimates of the log - concave projection of the true marginals ( a standard laplace density in this case ) , as correctly predicted by our theoretical results .",
    "signal : top left panel , top right panel and bottom left panel give the true signal , rotated observations and the reconstructed signal respectively .",
    "the bottom right panel gives the estimated marginal densities along with the true marginal ( grey line).,scaledwidth=50.0% ]        as discussed before , one of the unique advantages of the proposed method over existing ones is its general applicability .",
    "for example , the method can be used even when the marginal distributions of the true signal do not have densities . to demonstrate this property",
    ", we now consider simulating signals from a @xmath277 distribution . to the best of our knowledge ,",
    "none of the existing ica methods are applicable for these types of problems .",
    "the simulation results presented in figure  [ fig : bin ] suggest that the method works very well in this case .        to further conduct a comparative study , we repeated each of the previous simulations 200 times and computed our estimate along with those produced by the fastica and prodenica methods .",
    "fastica is a popular parametric ica method ; prodenica is a nonparametric ica method proposed by @xcite , and has been shown to enjoy the best performance among a large collection of existing ica methods @xcite .",
    "both the fastica and prodenica methods were implemented using the ` r ` package ` prodenica ` @xcite . to compare the performance of these methods ,",
    "we follow convention @xcite and compute the amari metric between the true unmixing matrix @xmath20 and its estimates .",
    "the amari metric between two @xmath9 matrices is defined as @xmath278 where @xmath279 .",
    "boxplots of the amari metric for all three methods are given in figure  [ fig : comp ] .",
    "it is clear that both nonparametric methods outperform the parametric method .",
    "several further observations can also be made on the comparison between the two nonparametric methods .",
    "for both uniform and exponential marginals , the proposed method improves upon prodenica .",
    "this might be expected since both distributions have log - concave densities .",
    "it is , however , interesting to note the robustness of the proposed method on the marginals as it still outperforms prodenica for @xmath280 marginals , and remains competitive for the mixture of normal marginals .",
    "the most significant advantage of the proposed method , however , is displayed when the marginals are binomial .",
    "recall that prodenica , and perhaps all existing nonparametric methods , assume that the log density ( or density itself ) is smooth .",
    "this assumption is not satisfied with the binomial distribution and as a result , prodenica performs rather poorly .",
    "in contrast , our proposed method works fairly well in this setting even though the true marginal does not have a log - concave density with respect to lebesgue measure .",
    "all these observations confirm our earlier theoretical development .",
    "of proposition  [ prop : cases ] 1 .",
    "suppose that @xmath62 .",
    "fix an arbitrary @xmath56 , and find @xmath281 and @xmath282 such that @xmath283 .",
    "then @xmath284 thus @xmath63 and @xmath285 .",
    "now suppose that @xmath65 , but @xmath66 for some hyperplane @xmath286 , where @xmath287 is a unit vector in @xmath26 and @xmath288 .",
    "find @xmath289 such that @xmath290 is an orthonormal basis for @xmath26 .",
    "define the family of density functions @xmath291 then @xmath292 , and @xmath293 as @xmath294 .",
    "now suppose that @xmath43 .",
    "notice that the density @xmath295 belongs to @xmath77 and satisfies @xmath296 moreover , @xmath297 where the second inequality follows from the proof of theorem  2.2 of @xcite .",
    "we may therefore take a sequence @xmath298 such that @xmath299 let @xmath300 denote the convex support of @xmath10 ; that is , the intersection of all closed , convex sets having @xmath10-measure 1 . following the arguments in the proof of theorem  2.2 of @xcite",
    ", there exist @xmath281 and @xmath282 such that @xmath301 for all @xmath302 .",
    "moreover , these arguments ( see also the proof of theorem  4 of @xcite ) yield the existence of a closed , convex set @xmath303 , a log - concave density @xmath304 with @xmath305 and a subsequence @xmath306 such that @xmath307 since the boundary of @xmath308 has zero lebesgue measure , we deduce from fatou s lemma applied to the non - negative functions @xmath309 that @xmath310 it remains to show that @xmath311 .",
    "we can write @xmath312 where @xmath313 and @xmath314 for each @xmath315 and @xmath120 .",
    "let @xmath316 be a random vector with density @xmath317 , and let @xmath11 be a random vector with density @xmath304 .",
    "we know that @xmath318 as @xmath319 , and that @xmath320 are independent for each @xmath321 .",
    "let @xmath322 and @xmath323 .",
    "then we have @xmath324 where the matrix @xmath325 has @xmath39th row @xmath326 .",
    "moreover , @xmath327 and @xmath328 , so  ( [ eq : secondrep ] ) provides an alternative , equivalent representation of the density @xmath329 , in which each row of the unmixing matrix has unit euclidean length . by reducing to a further subsequence if necessary",
    ", we may assume that for each @xmath120 , there exists @xmath330 such that @xmath331 as @xmath319 . by slutsky s theorem",
    ", it then follows that @xmath332 thus , for any @xmath333 , @xmath334 we conclude that @xmath335 are independent . since @xmath336 for all @xmath39",
    ", we deduce further that @xmath337 is non - singular .",
    "moreover , each of @xmath335 has a log - concave density , by theorem  6 of @xcite .",
    "this shows that @xmath311 , as required .",
    "@xmath338    of theorem  [ thm : pdica ] suppose that @xmath81 satisfies @xmath339 for some @xmath40 and @xmath45 .",
    "consider maximising @xmath340 over @xmath341 .",
    "letting @xmath342 and @xmath343 , where @xmath344 , we can equivalently maximise @xmath345 over @xmath346 .",
    "but , by theorem  4 of @xcite , the unique solution to this maximisation problem is to choose @xmath347 , where @xmath86 .",
    "this shows that @xmath348 can be written as @xmath349 since @xmath350 also , we deduce that @xmath351 is also the unique maximiser of @xmath352 over @xmath353 , so @xmath354 .",
    "@xmath338    of theorem",
    "[ thm : identifiability ] suppose that @xmath81 .",
    "let @xmath102 , so there exists @xmath40 such that @xmath355 has independent components .",
    "writing @xmath131 for the marginal distribution of @xmath356 , note that @xmath45 . by theorem  [ thm : pdica ] and",
    "the identifiability result of @xcite , it therefore suffices to show that @xmath357 has a gaussian density if and only if @xmath358 is a gaussian density .",
    "if @xmath131 has a gaussian density @xmath359 , then since @xmath359 is log - concave , we have @xmath86 .",
    "conversely , suppose that @xmath131 does not have a gaussian density .",
    "since @xmath86 satisfies @xmath360 ( * ? ? ?",
    "* remark  2.3 ) , we may assume without loss of generality that @xmath131 and @xmath359 have mean zero .",
    "we consider maximising @xmath361 over all mean zero gaussian densities @xmath129 .",
    "writing @xmath362 for the mean zero gaussian density with variance @xmath363 , we have @xmath364 this expression is maximised uniquely in @xmath363 at @xmath365 . but",
    "@xcite show that the only way a distribution @xmath131 and its log - concave projection @xmath358 can have the same second moment is if @xmath131 has a log - concave density , in which case @xmath131 has density @xmath358 .",
    "we therefore conclude that the only way @xmath358 can be a gaussian density is if @xmath131 has a gaussian density , a contradiction . @xmath338    of proposition  [ prop : pd ] the proof of this proposition is very similar to the proof of theorem  4.5 of @xcite , so we only sketch the argument here .",
    "for each @xmath366 , let @xmath121 , and consider an arbitrary subsequence @xmath306 . by reducing to a further subsequence if necessary",
    ", we may assume that @xmath367 $ ] .",
    "observe that @xmath368 arguments from convex analysis can be used to show that the sequence @xmath306 is uniformly bounded above , and @xmath369 for all @xmath370 .",
    "from this it follows that there exist @xmath371 and @xmath372 such that @xmath373 .",
    "thus , by reducing to a further subsequence if necessary , we may assume there exists @xmath304 such that @xmath374 note from this that @xmath375 in fact , we can use the argument from the proof of proposition  [ prop : cases ] to deduce that @xmath311 .",
    "skorokhod s representation theorem and fatou s lemma can then be used to show that @xmath376 .",
    "we can obtain the other bound @xmath377 by taking any element of @xmath70 , approximating it from above using lipschitz continuous functions , as in the proof of theorem  4.5 of @xcite , and using monotone convergence . from these arguments , we conclude that @xmath109 and @xmath130 .",
    "we can see from  ( [ eq : aeconv ] ) that @xmath378 , so @xmath379 , by scheff s theorem .",
    "thus , given any @xmath121 and any subsequence @xmath306 , we can find @xmath130 and a further subsequence of @xmath306 which converges to @xmath380 in total variation distance .",
    "this yields the second part of the proposition . @xmath338    of theorem",
    "[ thm : conv ] the first part of the theorem is a special case of proposition  [ prop : pd ] .",
    "now suppose @xmath81 is identifiable and is represented by @xmath40 and @xmath45 .",
    "suppose without loss of generality that @xmath381 for all @xmath120 and let @xmath84 .",
    "recall from theorem  [ thm : pdica ] that if @xmath11 has density @xmath380 , then @xmath356 has density @xmath86 .",
    "suppose for a contradiction that we can find @xmath382 , integers @xmath383 , @xmath384 and @xmath385 such that @xmath386 we can find a subsequence @xmath387 such that @xmath388 , say , as @xmath389 , for all @xmath120 .",
    "the argument towards the end of the proof of case 3 of proposition  [ prop : cases ] shows that @xmath126 can be used to represent the unmixing matrix of @xmath380 , so by the identifiability result of @xcite and the fact that @xmath336 , there exist @xmath390 and a permutation @xmath92 of @xmath93 such that @xmath391 . setting @xmath392 and @xmath393",
    ", we deduce that @xmath394 for @xmath120 . now observe that if @xmath395 has density @xmath396 , then by slutsky s theorem , @xmath397 .",
    "it therefore follows from proposition  2(c ) of @xcite that @xmath398 for @xmath120 .",
    "this contradiction establishes that @xmath399 for each @xmath120 .",
    "it remains to prove that for sufficiently large @xmath112 , every @xmath121 is identifiable .",
    "recall from the identifiability result of @xcite and theorem  [ thm : identifiability ] that not more than one of @xmath400 is gaussian .",
    "let @xmath401 denote the univariate normal density with mean @xmath402 and variance @xmath363 .",
    "let @xmath143 denote the index set of the non - gaussian densities among @xmath400 , so the cardinality of @xmath143 is at least @xmath403 , and consider , for each @xmath404 , the problem of minimising @xmath405 over @xmath406 and @xmath407 .",
    "observe that @xmath245 is continuous with @xmath408 for all @xmath402 and @xmath409 , that @xmath410 as @xmath411 and @xmath412 as @xmath413 .",
    "it follows that @xmath245 attains its infimum , and there exists @xmath414 such that @xmath415 comparing  ( [ eq : supinf ] ) and  ( [ eq : inf ] ) , we see that , for sufficiently large @xmath112 , whenever @xmath121 and @xmath416 , at most one of the densities @xmath417 can be gaussian .",
    "it follows that when @xmath112 is large , every @xmath121 is identifiable .",
    "@xmath338    of proposition  [ prop : emplike ] it is well - known that for fixed @xmath40 , the nonparametric likelihood @xmath156 defined in  ( [ eq : nonlike ] ) is maximised by choosing @xmath418 for @xmath419 , @xmath40 and @xmath120 , let @xmath420 the binary relation @xmath421 if @xmath422 defines an equivalence relation on @xmath145 , so we can let @xmath423 denote a set of indices obtained by choosing one element from each equivalence class .",
    "then @xmath424 since @xmath18 are in general position by hypothesis , we have that @xmath425 .",
    "it follows that @xmath426 .",
    "moreover , for any choice @xmath143 of distinct indices in @xmath145 if we construct the matrix @xmath150 as described just before the statement of proposition  [ prop : emplike ] , then @xmath427 .",
    "@xmath338    of corollary  [ cor : hats ] let @xmath428 denote the empirical distribution of @xmath195 . writing @xmath429 and @xmath430 , note that the covariance matrix corresponding to @xmath428 is @xmath431 observe further that there is a bijection between the set of maximisers @xmath185 of @xmath432 over @xmath184 and @xmath167 , and the set of maximisers @xmath199 of @xmath433 over @xmath197 and @xmath434 via the correspondence @xmath200 and @xmath201 .",
    "it follows from the discussion in section  [ sec : pre - whiten ] that maximising @xmath433 over @xmath197 and @xmath434 amounts to computing the log - concave ica projection of @xmath428 .",
    "existence of a maximiser therefore follows from proposition  [ prop : cases ] and the fact that the convex hull of @xmath435 is @xmath5-dimensional with probability 1 for sufficiently large  @xmath112 .",
    "now suppose @xmath436 and @xmath437 represent the log - concave ica projection @xmath438 .",
    "further , let @xmath439 denote the distribution of @xmath440 , so @xmath441 has identity covariance matrix and suppose @xmath442 .",
    "then @xmath443 as @xmath108 , so by theorem  [ thm : conv ] , there exist a permutation @xmath192 of @xmath93 and scaling factors @xmath193 such that @xmath444 where @xmath445 . writing @xmath446 , @xmath200 , @xmath201 and noting that @xmath447 , the conclusion of the corollary follows immediately .",
    "@xmath338    of proposition  [ prop : diff ] for @xmath382 , let @xmath448 , and let @xmath449 denote the @xmath39th row of @xmath450 .",
    "notice that @xmath451 as @xmath452 .",
    "it follows that for sufficiently small @xmath382 , @xmath453 as @xmath452 .",
    "@xmath338                          hastie , t. and tibshirani , r. ( 2003 ) independent component analysis through product density estimation . in _ advances in neural information processing systems 15 ( becker , s. and obermayer , k. , eds )",
    "_ , mit press , cambridge , ma .",
    "pp 649 - 656 .",
    "hastie , t. and tibshirani , r. ( 2003 ) ` prodenica ` : product density estimation for ica using tilted gaussian density estimates ` r ` package version 1.0 ` http://cran.r-project.org/web/packages/prodenica/ ` ."
  ],
  "abstract_text": [
    "<S> independent component analysis ( ica ) models are very popular semiparametric models in which we observe independent copies of a random vector @xmath0 , where @xmath1 is a non - singular matrix and @xmath2 has independent components . </S>",
    "<S> we propose a new way of estimating the unmixing matrix @xmath3 and the marginal distributions of the components of @xmath2 using nonparametric maximum likelihood . </S>",
    "<S> specifically , we study the projection of the empirical distribution onto the subset of ica distributions having log - concave marginals . </S>",
    "<S> we show that , from the point of view of estimating the unmixing matrix , it makes no difference whether or not the log - concavity is correctly specified . </S>",
    "<S> the approach is further justified by both theoretical results and a simulation study .    * keywords * : blind source separation , density estimation , independent component analysis , log - concave projection , nonparametric maximum likelihood estimator . </S>"
  ]
}