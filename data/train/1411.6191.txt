{
  "article_text": [
    "the discovery of error backpropagation was hailed as a breakthrough because it solved the main problem of distributed learning  the spatial credit assignment problem @xcite .",
    "decades later , backprop is the workhorse underlying most deep learning algorithms , and a major component of the state - of - the - art in supervised learning .    since backprop s introduction",
    ", there has been tremendous progress improving the performance of neural networks .",
    "an enormous amount of effort has been expended exploring the effects of : the activation functions of nodes ; network architectures ( e.g. number of layers and number of nodes ) ; regularizers such as dropout @xcite ; modifications to accelerate gradient descent ; and unsupervised methods for pre - training to find better local optima .",
    "however , it was known from the start that backprop is not biologically plausible @xcite .",
    "implementing backprop requires that neurons produce two distinct signals  outputs and errors  whereas only one has been observed in cortex @xcite .",
    "it is therefore remarkable that almost no attempts have been made to rethink the core algorithm ",
    "backpropagation  and the problem that it solves  credit assignment .",
    "this paper revisits the credit assignment problem and takes a fresh look at the signaling architecture that underlies backprop .",
    "[ [ outline . ] ] outline .",
    "+ + + + + + + +    our starting point is to decompose backprop into local learning algorithms , theorem  [ t : backprop ] .",
    "nodes under backprop are modeled as agents that minimize their losses .",
    "backprop ensures that nodes cooperate , towards the shared goal of minimizing the output layer s error , by gluing together their loss functions using recursively computed error signals .",
    "reformulating backprop as local learners immediately suggests modifying the signaling architecture ( the glue ) whilst keeping the learners . in this paper , we aim to simplify backprop s error signals .",
    "theorem  [ t : regret ] lays the groundwork , by providing a regret bound for local learners that holds for any scalar feedback  and not just the error signals used by backprop .",
    "the next step is to show that , when a neural network has 1-dimensional outputs ( e.g. nonparametric regression ) , backprop s error signals factorize into two components , theorem  [ t : factorization ] .",
    "the first component is a scalar error computed at the output layer that is analogous to a neuromodulatory signal ; the second is a complicated sum over paths to the output layer that has no biological analog .    our proposed algorithm , _ kickback _ , modifies backprop by truncating the second component .",
    "kickback is _ not _ gradient descent on the output error .",
    "nevertheless , theorem  [ t : coherence ] provides a simple sufficient condition , coherence , for kickback to follow the error gradient .",
    "it turns out that many of the components of kickback have close neurophysiological analogs .",
    "we discuss kickback s biological significance by relating it to a recently developed , discrete - time model neuron @xcite .    finally , we present experiments demonstrating that kickback matches backprop s performance on standard benchmark datasets .    [",
    "[ synopsis . ] ] synopsis .",
    "+ + + + + + + + +    our contribution is twofold .",
    "firstly , we provide a series of simple , fundamental theorems on backprop , one of the most heavily used learning algorithms .",
    "in particular , theorem  [ t : backprop ] suggests that ideas from multi - agent learning and mechanism design have a role to play in deep learning .",
    "secondly , we propose kickback , a stripped - down variant of backprop that simultaneously performs well and ties in nicely with the signaling architecture of cortical neurons .",
    "[ [ s : related ] ] related work .",
    "+ + + + + + + + + + + + +    the idea of building learning algorithms out of individual learning agents dates back to at least @xcite .",
    "more recent approaches include reinforce @xcite , the hedonistic neurons in @xcite , and the neurons modeled using online learning in @xcite .",
    "none of these approaches have led to algorithms that are competitive on benchmarks .",
    "the algorithm closest to kickback is attention - gated reinforcement learning ( agrel ) , which also eliminates the error signals from backprop @xcite . agrel and kickback are analogous at a high level , however the details differ markedly . in terms of results ,",
    "the main differences are as follows .",
    "firstly , we implement kickback for networks with 2 and 3 hidden layers ; whereas agrel was only implemented for 1 hidden layer .",
    "indeed , as discussed in @xcite , extending agrel to multiple hidden layers is problematic .",
    "secondly , agrel achieved comparable performance to backprop on toy datasets : xor , counting , and a mine detection dataset containing @xmath0 inputs ; whereas kickback matches backprop on much larger , real - world nonparametric regression problems .",
    "finally , agrel converges @xmath1 to @xmath2 times slower than backprop , whereas kickback s convergence is essentially identical to backprop .",
    "recent work has shown that using rectilinear functions instead of sigmoids can significantly improve the performance of neural networks .",
    "we restrict to rectifiers because they perform well empirically @xcite , are more realistic models of cortical neurons than sigmoid units @xcite , and are universal function approximators @xcite .    denote the positive and negative rectifiers by @xmath3 and @xmath4 respectively .",
    "rectifiers are continuous everywhere and differentiable everywhere except at 0 .",
    "the subgradients are : @xmath5    let @xmath6 denote either @xmath7 or @xmath8 ; the notation is useful when discussing positive and negative rectifiers simultaneously .",
    "similarly , let @xmath9 denote either subgradient .",
    "the subgradient @xmath9 acts as a _ signed _ indicator function .",
    "the output of node @xmath10 is @xmath11 .",
    "we say that node @xmath10 fires if @xmath12 ; the firing rate is @xmath13 .    [ [ from - global - to - local - learning . ] ] from global to local learning .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    under backprop the entire neural network optimizes a single objective function using gradient descent on the network s error .",
    "the partial derivatives with respect to weights are computed via the chain rule .    in more detail ,",
    "suppose a neural network has error function @xmath14 that depends on the output layer @xmath15 and labels @xmath16 .",
    "backprop recursively updates weight vectors using the chain rule . for nodes in the output layer , @xmath17 . for hidden node @xmath10 ,",
    "the error signal is derived via @xmath18    our first result is that , when the hidden nodes are rectilinear , backprop decomposes into many interacting learning algorithms that maximize local objective functions .",
    "consider the following setup .",
    "[ d : rl ] a node with a rectilinear activation function @xmath19 receives input @xmath20 and incurs * rectilinear loss * @xmath21 that depends on an externally provided scalar @xmath22 .",
    "if the node fires then the rectilinear loss is the linear loss @xmath23 , which has been extensively analyzed in online learning @xcite . if the node does not fire then the rectilinear loss is zero .",
    "[ t : backprop ]    the weight updates induced by backprop on rectilinear hidden node @xmath10 are the same as gradient descent on the rectilinear loss : @xmath24    the rectilinear loss resembles the hinge loss .",
    "however , it is not convex since , even if the node has a positive rectifier , @xmath22 is not necessarily positive .",
    "let @xmath25 and @xmath26 .",
    "weight updates under backprop are @xmath27 weight updates for gradient descent on the rectilinear loss are @xmath28 substituting @xmath29 yields the theorem .",
    "backprop is thus a collection of local optimizations glued together by the recursively computed error signals .",
    "[ [ a - regret - bound . ] ] a regret bound .",
    "+ + + + + + + + + + + + + + +    since the rectilinear loss has not been previously studied , our second result is a guarantee on the predictive performance of the local learners .",
    "[ t : regret ] suppose that weights are projected into a compact convex set @xmath30 at each time step .",
    "let @xmath31 denote the time - points when the node fired .",
    "the following guarantee holds for any sequence of inputs and scalar feedback when @xmath32 : @xmath33          \\\\",
    "\\leq \\sqrt{\\frac{8 d e}{|f| } }      \\end{gathered}\\ ] ] where @xmath34 and @xmath35 .",
    "theorem  [ t : regret ] shows that the loss incurred by rectifiers on _ the inputs that cause them to fire _ converges towards the loss of the best weight - vector in hindsight .",
    "the theorem is shown for hard constraints ( i.e. projecting into @xmath30 ) ; similar results hold for convex regularizers .",
    "the result holds for arbitrary sequences of inputs and feedbacks , including adversarial .",
    "it is therefore more realistic than the standard _",
    "_ assumption . indeed , even if a network s inputs are _ i.i.d .",
    "_ , the inputs to nodes in deeper layers are not  due to weight - updates within the network .",
    "standard results on online learning do not directly apply , since the rectilinear loss is not convex . to adapt these results , observe that , by , nodes only learn from the inputs that cause them to fire .    clearly , @xmath36 for all @xmath37 .",
    "that is , a node s output is linear on the inputs for which it fires .",
    "further , the rectilinear loss is linear on @xmath38 .",
    "the theorem follows from a well - known result on gradient descent for the linear loss , see @xcite .",
    "theorem  [ t : regret ] is not restricted to backprop s error signals ; it holds for any sequence of scalars @xmath39 .",
    "this suggests exploring alternate ways of gluing together local learners .",
    "[ f : bp ]    backprop has two unfortunate properties .",
    "firstly , the error signals @xmath40 are computationally expensive : they depend on the activity and weights of all downstream nodes in the network .",
    "secondly , nodes produce two distinct signals : outputs that are fed forward and errors that are fed back . in contrast",
    ", cortical neurons communicate with only one signal type , spikes , which are sent in all directions .",
    "this suggests that it may be possible to make do with less .",
    "viewed from a distance , backprop is a single distributed optimization , performing gradient descent on the network s error . zooming in , via theorem  [ t : backprop ] ,",
    "reveals that backprop is a collection of local learners _ glued together _ by recursively computed error signals .",
    "we thus have a framework for experimenting with alternate feedback signals @xcite .",
    "_ kickback _ takes the same local learners as backprop but weakens the glue that binds them , thereby reducing communication complexity and increasing biological plausibility .",
    "[ [ factorizing - backprops - error - signals . ] ] factorizing backprop s error signals .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    it is necessary to distinguish between global and local error signals .",
    "local errors signals are the recursively computed signals @xmath40 .",
    "the global error is the derivative of the network s error function with respect to the activity of the output layer .",
    "[ d : influence ] the influence of node @xmath10 on node @xmath41 is @xmath42 .",
    "the * influence * of node @xmath10 on the next layer is @xmath43 .",
    "the * total influence * of node @xmath10 on downstream nodes is @xmath44 the sum over all paths from @xmath10 to the output layer .",
    "our third result is that backprop s error signals factorize whenever a neural network has 1-dimensional outputs .",
    "[ t : factorization ] suppose neural network @xmath45 has scalar output and let @xmath46 be the global error .",
    "then , the error signal of a hidden node @xmath10 factorizes as @xmath47    the theorem holds in the setting of nonparametric regression . multi - label classification is excluded .",
    "backprop recursively updates weight vectors using the chain rule , recall .",
    "when the output is one - dimensional , @xmath48 contributes @xmath49 to the recursive computation of @xmath50 over hidden nodes .",
    "[ [ kickback . ] ] kickback .",
    "+ + + + + + + + +    we are now ready to introduce kickback .",
    "[ a : kickback ] the * truncated feedback * @xmath51 at node @xmath10 is @xmath52 under * kickback * , hidden nodes perform gradient descent on the rectilinear loss with truncated feedback : @xmath53    kickback and backprop are contrasted in figure  1 and in equations versus .",
    "importantly , kickback eliminates the need for nodes to communicate error signals  as distinct from their outputs .",
    "[ [ kickback - as - time - averaged - backprop . ] ] kickback as time - averaged backprop .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    truncating the feedback signal , from to , preserves more information than appears at first glance .",
    "the truncated signal received by node @xmath10 _ explicitly _ depends on @xmath10 s influence on the next layer .",
    "however , kickback _ implicitly _ incorporates information about the influence of multiple layers .",
    "for simplicity , suppose there is no regularizer and that the learning rate @xmath54 is constant .",
    "then , summing over the updates in , a weight at time @xmath55 is @xmath56 . in the specific case of kickback , the weight is @xmath57 the weight @xmath58 thus implicitly incorporates the effect of interactions @xmath59 in the next layer down , and so on recursively .",
    "[ [ coherence . ] ] coherence .",
    "+ + + + + + + + + +    with a small enough learning rate , gradient descent will tend towards a local minimum .",
    "kickback does not perform gradient descent on the error function since it uses modified feedback signals .",
    "thus , without further assumptions , it is not guaranteed to improve performance .",
    "our fourth result is to provide a sufficient condition .",
    "[ d : coherence ] node @xmath10 is * coherent * when @xmath60 .",
    "a network is coherent when all its nodes are coherent .",
    "[ eg : signed ] an easy way to guarantee coherence for every node is to impose the purely _ local _ condition that all connections targeting positive nodes have positive weights , and similarly that all connections targeting negative nodes have negative weights .    if a network is coherent , then increasing a positive node s firing rate increases the average ( signed ) activity in the next layer and _ all downstream layers_. increasing the activity of negative nodes has the opposite effect .    on the other hand ,",
    "if a network is not coherent , then nothing can be said in general about how the activity of nodes in one layer affects other layers .",
    "coherence thus enforces _ interpretability _ : it ensures that a node s influence on the next layer is indicative of its _ total _ influence on all downstream layers .",
    "[ t : coherence ] if a network is coherent then weight updates under kickback , with a sufficiently small learning rate , improve performance .",
    "it suffices to show that the feedback has the same sign under backprop , @xmath61 , and kickback , @xmath62 for an arbitrary hidden node @xmath10 .",
    "if @xmath10 is coherent then @xmath60 . if , furthermore , all downstream nodes are coherent , then unraveling obtains that @xmath63 .",
    "the result follows .    under backprop ,",
    "each node s total influence is computed explicitly .",
    "kickback makes do with less information : a node `` knows '' its influence on the next layer , but does not `` know '' its total influence .",
    "there is a direct link from kickback to neurobiology provided by the _ selectron _ : a simplified model neuron @xcite .",
    "the selectron is derived from standard models of neural dynamics and learning  the spike response model ( srm ) and spike - timing dependent plasticity ( stdp )  by taking the so - called `` fast - time constant limit '' to go from continuous to discrete time .",
    "[ t : stdp ] the fast time - constant limit of the srm @xcite is a node that outputs @xmath64 if @xmath65 and @xmath66 otherwise .",
    "weight updates in the fast time - constant limit of neuromodulated stdp @xcite are @xmath67 where @xmath68 is a global , scalar - valued neuromodulatory signal .",
    "the weight updates in are gradient _ ascent _ on @xmath69    setting @xmath70 in @xmath71 recovers the rectilinear loss in definition  [ d : rl ] .",
    "the selectron thus maximizes a _ rectilinear reward _ via the same weight updates used to minimize the rectilinear loss .",
    "the difference between the two models is that the selectron has 0/1-valued outputs ( spikes ) , whereas nodes have real - valued outputs ( firing rates ) .",
    "@xcite .",
    "kickback s weight updates are @xmath72 .",
    "each factor has a biological analog .",
    "the global error , @xmath49 , corresponds to neuromodulators , such as dopamine , that have been experimentally observed to signal prediction errors for future rewards @xcite .",
    "the _ kickback _ term , @xmath73 , corresponds to nmda backconnections that have a multiplicative effect on synaptic updates , proportional to the weighted sum of downstream activity @xcite .",
    "the feedforward term , @xmath74 , corresponds to pre - synaptic spiking activity @xcite .",
    "finally , the signed indicator function @xmath75 , ensures that only active nodes update their weights  thereby playing the role of post - synaptic activity in stdp .",
    "the regret bound in theorem  [ t : regret ] is also biologically significant .",
    "synapses incur a significant metabolic cost @xcite .",
    "regularizing synaptic weights provides a way to quantify metabolic costs .",
    "indeed , limits on the physical size and metabolic budget of synapses suggest that synaptic weights may be constrained to an @xmath76-ball @xcite .    to the best of our knowledge , theorem  [ t : regret ] is the first _ adversarial _ generalization bound for a biologically derived model .",
    "the generalization bound for the selectron in @xcite assumes that inputs are _",
    "i.i.d_. moving beyond the _ i.i.d .",
    "_ assumption is important because biological organisms face adversarial environments .",
    "the final ingredient is coherence .",
    "investigating biologically plausible mechanisms that ensure coherence ( or some other sufficient condition ) is deferred to future work .",
    "[ [ goals . ] ] goals .",
    "+ + + + + +    our primary aim is to compare kickback s performance to backprop .",
    "we present results on two robotics datasets , sarcos and barrett wam .",
    "kickback s performance across multiple hidden layers is of particular interest , since it truncates errors .",
    "results for 3 hidden layers are reported ; results for 1 and 2 hidden layers were similar .",
    "a secondary aim is to investigate the effect of coherence .",
    "competing on the datasets tackled by deep learning algorithms is not yet feasible .",
    "further work is required to adapt kickback to multiclass learning .",
    "[ [ architecture . ] ] architecture .",
    "+ + + + + + + + + + + + +    experiments were performed on a 5-layer network with 2 output nodes , 10 , 100 and 200 nodes in three hidden layers , and with the input layer directly drawn from the data .",
    "experiments were implemented in theano @xcite .",
    "all nodes are rectifiers .",
    "we set half of nodes as positive and half as negative .",
    "output nodes perform rectilinear regression , see below , whereas hidden nodes minimize the rectilinear loss on feedback implementing either kickback or backprop .",
    "training was performed in batch sizes of 20 .",
    "lower batch - sizes yield better performance at the cost of longer training times .",
    "we chose 20 as a reasonable compromise .",
    "[ [ rectilinear - regression . ] ] rectilinear regression .",
    "+ + + + + + + + + + + + + + + + + + + + + + +    recently , @xcite introduced an @xmath76 penalty on firing rates , which encourages sparsity and can improve performance .",
    "here , we consider an @xmath77-penalty : @xmath78 .",
    "weight updates under gradient descent are @xmath79 notice that the penalty @xmath80 in is the firing rate .",
    "comparing with the gradient @xmath81 of the mean - squared error @xmath82 shows that the @xmath77-activation penalty leads nodes to _ perform linear regression on the inputs that cause them to fire _",
    "a regret bound analogous to theorem  [ t : regret ] holds for rectilinear regression , with a faster convergence rate of @xmath83 .",
    "training error is the mse of the output node with the correct sign ; test error is the sum of the output nodes mses .",
    "[ [ initialization - and - coherence . ] ] initialization and coherence .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + +    no pretraining was used .",
    "we consider two network initializations .",
    "the first is * uniform * : draw weights uniformly at random from an interval symmetric about 0 , without regard to coherence .",
    "the second initialization , * signed * is taken from example  [ eg : signed ] : draw weights uniformly , then change their signs so that connections targeting positive nodes have positive weights and conversely for negative nodes .",
    "* signed * guarantees coherence at initialization . although it is possible to impose coherence during training , we found that doing so was unnecessary in practice .",
    "results are plotted under both initializations for kickback  excepting panel ( e ) , where * uniform * failed to converge . for backprop , the initialization that yielded the _",
    "better _ performance is reported .",
    "[ [ results . ] ] results .",
    "+ + + + + + + +    we report normalized mean - squared errors . to directly compare the behavior of the algorithms , we report individual runs . performance was robust to significant changes in tuning parameters :",
    "e.g. changing parameters by @xmath84 increased the mse on sarcos 3 from .6% to .8% .",
    "each sarcos dataset consists of 44,484 training and 4,449 test points ; barrett split as 12,000 and 3,000 .",
    "parameters were tuned via grid - search with 5-fold cross - validation .",
    "backprop s only parameter is the learning rate .",
    "kickback was implemented with a learning rate tuned for backprop .",
    "kickback has two additional parameters that rescale the feedback to hidden layers 1 & 2 . we observed that tuning via cross - validation typically set the rescaling factors such that the truncated errors are rescaled to about same magnitude , on average , as backprop s feedback .",
    "kickback and backprop are competitive with non - parametric methods such as kernel regression , e.g. @xcite .",
    "kickback performs best with * signed * initialization , as expected from theorem  [ t : coherence ] . with *",
    "signed * initialization , kickback almost exactly matches backprop in all 6 datasets .",
    "importantly , kickback continues to reduce the mse after 100s of epochs ; following the correct gradient even when the error is small .",
    "the comparison between backprop and kickback is not completely fair : kickback s additional parameters cause it to outperform backprop in panel  ( b ) .",
    "we have endeavored to keep the comparison as level as possible .",
    "[ [ the - effect - of - coherence . ] ] the effect of coherence .",
    "+ + + + + + + + + + + + + + + + + + + + + + + +    kickback s performance was better than expected : coherence was not imposed after initialization under * signed * ; and no guarantees are applicable to * uniform*. a possible explanation is that kickback preserves or increases coherence .    to test this hypothesis , we quantified the coherence of layer @xmath85 as @xmath86 , which lies in @xmath87 $ ] . with * signed * initialization , coherence consistently remained above 0.9 under kickback ; but exhibited considerable variability under backprop . with *",
    "uniform * initialization , kickback increased the coherence of hidden layers 2 & 3 , from 0 to @xmath88 , with the exception of panel  ( c ) .",
    "backprop did not alter coherence in any consistent way .",
    "barrett 4 is the only dataset where nodes become incoherent @xmath89 on average .",
    "the oscillations in panel  ( c ) for * uniform * arise because kickback is not guaranteed to follow the training error gradient in the absence of coherence .",
    "it is surprising the network learns at all .",
    "note that oscillations do not occur when networks are given a * signed * initialization .",
    "a necessary step towards understanding how the brain assigns credit is to develop a minimal working model that fits basic constraints .",
    "backprop solves the credit assignment problem .",
    "it is one of the simplest and most effective methods for learning representations . in combination with various tricks and optimizations , it continues to yield state - of - the - art performance .",
    "however , it flouts a basic constraint imposed by neurobiology : it requires that nodes produce error signals that are distinct from their outputs .",
    "kickback is a stripped - down version of backprop motivated by theoretical ( theorems  [ t : backprop][t : coherence ] ) and biological ( fig .  1 and theorem  [ t : stdp ] ) considerations . under kickback ,",
    "nodes perform gradient descent , or ascent , on the representation  that is , the kicked back activity  produced by the next layer .",
    "the sign of the global error determines whether nodes follow the gradient downwards , or upwards .",
    "kickback is the first competitive algorithm with biologically plausible credit - assignment .",
    "earlier proposals were not competitive and only worked for one hidden - layer ( kickback works well for @xmath90 hidden - layers ; we have not tested @xmath91 ) .",
    "kickback s simplified signaling is suited to hardware implementations @xcite .",
    "an important outcome of the paper is a new formulation of backprop in terms of interacting local learners , that may connect deep learning to recent developments in multi - agent systems @xcite and mechanism design @xcite .",
    "perhaps the most important direction is to extend kickback to multiclass learning . for this , it is necessary to consider multidimensional outputs , in which case the derivative of the energy function with respect to the output layer is not a scalar .",
    "a natural approach to tackle this setting is to use more sophisticated global error signals . indeed , modeling the neuromodulatory system as producing scalar outputs is a vast oversimplification @xcite .",
    "finally , reinforcement learning is a better model of how an agent adapts to its environment than supervised learning @xcite .",
    "a natural avenue to explore is how kickback , suitably modified , performs in this setting .",
    "bergstra , j. ; breuleux , o. ; bastien , f. ; lamblin , p. ; pascanu , r. ; desjardins , g. ; turian , j. ; warde - farley , d. ; and bengio , y. 2010 .",
    "theano : a cpu and gpu math expression compiler . in _ proc .",
    "python for scientific comp .",
    "( scipy)_.        dahl , g.  e. ; sainath , t.  n. ; and hinton , g. 2013 . improving deep neural networks for lvcsr using rectified linear units and dropout . in _",
    "ieee international conference on acoustics , speech and signal processing ( icassp)_.              indiveri , g. ; linares - barranco , b. ; hamilton , t.  j. ; van schaik , a. ; etienne - cummings , r. ; delbruck , t. ; liu , s .- c . ; dudek , p. ; hfliger , p. ; and _ et al_. 2011 .",
    "neuromorphic silicon neuron circuits .",
    "5(73 ) .",
    "sutton , r. ; modayil , j. ; delp , m. ; degris , t. ; pilarski , p.  m. ; white , a. ; and precup , d. 2011 .",
    "horde : a scalable real - time architecture for learning knowledge from unsupervised motor interaction . in _ proc .",
    "10th int . conf . on autonomous agents and multiagent systems ( aamas)_.              zeiler , m.  d. ; ranzato , m. ; monga , r. ; mao , m. ; yang , k. ; le , q.  v. ; nguyen , p. ; senior , a. ; vanhoucke , v. ; dean , j. ; and hinton , g. 2013 . on rectified linear units for speech processing .",
    "in _ ieee int conf on acoustics , speech and signal proc ( icassp)_."
  ],
  "abstract_text": [
    "<S> error backpropagation is an extremely effective algorithm for assigning credit in artificial neural networks . </S>",
    "<S> however , weight updates under backprop depend on lengthy recursive computations and require separate output and error messages  features not shared by biological neurons , that are perhaps unnecessary . in this paper , we revisit backprop and the credit assignment problem .    </S>",
    "<S> we first decompose backprop into a collection of interacting learning algorithms ; provide regret bounds on the performance of these sub - algorithms ; and factorize backprop s error signals . using these results , we derive a new credit assignment algorithm for nonparametric regression , kickback , that is significantly simpler than backprop . </S>",
    "<S> finally , we provide a sufficient condition for kickback to follow error gradients , and show that kickback matches backprop s performance on real - world regression benchmarks . </S>"
  ]
}