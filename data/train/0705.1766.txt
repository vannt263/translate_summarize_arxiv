{
  "article_text": [
    "let @xmath0 be independent identically distributed ( i.i.d . )",
    "random variables ( r.v.s ) with a common distribution function @xmath1 with a real unknown parameter @xmath2 .",
    "an @xmath3-estimator of @xmath2 is defined as a statistic @xmath4 which is a solution w.r.t .",
    "@xmath5 of the estimating equation @xmath6 where @xmath7 is a suitably chosen function .",
    "for example , if @xmath2 is a location parameter in the normal family of distribution functions , the choice @xmath8 gives the mle ( maximum likelihood estimator ) .",
    "for the same problem , if @xmath9 the solution of reduces to the median of @xmath10 .",
    "in general , if @xmath11 is the probability density function ( or probability function ) of @xmath12 ( w.r.t . a @xmath13-finite measure",
    "@xmath14 ) then the choice @xmath15 yields the mle .",
    "suppose now that @xmath0 are not necessarily independent or identically distributed r.vs , with a joint distribution depending on a real parameter @xmath2 .",
    "then an @xmath3-estimator of @xmath2 is defined as a solution of the estimating equation @xmath16 where @xmath17 with @xmath18 .",
    "so , the @xmath7-functions may now depend on the past observations as well .",
    "for instance , if @xmath19 s are observations from a discrete time markov process , then one can assume that @xmath20 .",
    "in general , if no restrictions are placed on the dependence structure of the process @xmath19 , one may need to consider @xmath7-functions depending on the vector of all past and present observations of the process ( that is , @xmath21 ) .",
    "if the conditional probability density function ( or probability function ) of the observation @xmath22 given @xmath23 is @xmath24 , then one can obtain the mle on choosing @xmath25 besides mles , the class of @xmath3-estimators includes estimators with special properties such as robustness . under certain regularity and ergodicity conditions",
    "it can be proved that there exists a consistent sequence of solutions of which has the property of local asymptotic linearity .",
    "( see e.g. , serfling @xcite , huber @xcite , lehman @xcite . a comprehensive bibliography can be found in launer and wilkinson @xcite , hampel at al @xcite , rieder @xcite , and jure@xmath26kov@xmath27 and sen @xcite . )",
    "if @xmath7-functions are nonlinear , it is rather difficult to work with the corresponding estimating equations , especially if for every sample size @xmath28 ( when new data are acquired ) , an estimator has to be computed afresh . in this paper",
    "we consider estimation procedures which are recursive in the sense that each successive estimator is obtained from the previous one by a simple adjustment .",
    "note that for a linear estimator , e.g. , for the sample mean , @xmath29 we have @xmath30 , that is @xmath31 , indicating that the estimator @xmath32 at each step @xmath28 can be obtained recursively using the estimator at the previous step @xmath33 and the new information @xmath34 .",
    "such an exact recursive relation may not hold for nonlinear estimators ( see , e.g. , the case of the median ) .    in general",
    ", the following heuristic argument can be used to establish a possible form of an approximate recursive relation ( see also jure@xmath26kov@xmath27 and sen @xcite , khasminskii and nevelson @xcite , lazrieva and toronjadze @xcite ) . since @xmath32 is defined as a root of the estimating equation , denoting the left hand side of by @xmath35 we have @xmath36 and @xmath37 . assuming that the difference @xmath38 is `` small '' we can write @xmath39 @xmath40 @xmath41 therefore , @xmath42 where @xmath43 . now , depending on the nature of the underlying model , @xmath44 can be replaced by a simpler expression .",
    "for instance , in i.i.d . models with @xmath15 ( the mle case ) , by the strong law of large numbers , @xmath45   = -i(\\theta)\\ ] ] for large @xmath28 s , where @xmath46 is the one - step fisher information .",
    "so , in this case , one can use the recursion    @xmath47    to construct an estimator which is `` asymptotically equivalent '' to the mle .",
    "motivated by the above argument , we consider a class of estimators @xmath48 where @xmath49 is a suitably chosen vector process , @xmath50 is a ( possibly random ) normalizing matrix process and @xmath51 is some initial value",
    ". note that while the main goal is to study recursive procedures with non - linear @xmath49 functions , it is worth mentioning that any linear estimator can be written in the form with linear , w.r.t .",
    "@xmath2 , @xmath49 functions . indeed , if @xmath52 where @xmath53 and @xmath54 are matrix and vector processes of suitable dimensions , then ( see section 4.2 for details ) @xmath55 which is obviously of the form with @xmath56    it should be noted that at first glance , recursions and resemble the newton - raphson iterative procedure of numerical optimisation . in the i.i.d .",
    "case , the newton - raphson iteration for the likelihood equation is @xmath57 where @xmath58 is minus the second logarithmic derivative of the log - likelihood function , that is , @xmath59 or its expectation , that is , the information matrix @xmath60 in the latter case , the iterative scheme is often called the method of scoring , see e.g. , harvey @xcite .",
    "( we do not consider the so called one - step newton - raphson method since it requires an auxiliary consistent estimator ) .",
    "the main feature of the scheme is that @xmath61 , at each step @xmath62 is @xmath63 - measurable ( where @xmath63 is the @xmath13-field generated by the random variables @xmath64 ) . in other words , is a deterministic procedure to find a root , say @xmath65 , of the likelihood equation + @xmath66 on the other hand the random variable @xmath67 derived from is an estimator of @xmath2 for each n=1,2, ",
    "(is @xmath68- measurable at each @xmath28 ) .",
    "note also that in the iid case , can be regarded as a stochastic iterative scheme , i.e. , a classical stochastic approximation procedure , to detect the root of an unknown function when the latter can only be observed with random errors ( see remark 3.1 ) .",
    "a theoretical implication of this is that by studying the procedures , or in general , we study asymptotic behaviour of the estimator of the unknown parameter . as far as applications are concerned , there are several advantages in using .",
    "firstly , these procedures are easy to use since each successive estimator is obtained from the previous one by a simple adjustment and without storing all the data unnecessarily .",
    "this is especially convenient when the data come sequentially .",
    "another potential benefit of using is that it allows one to monitor and detect certain changes in probabilistic characteristics of the underlying process such as change of the value of the unknown parameter .",
    "so , there may be a benefit in using these procedures in linear cases as well .    in i.i.d .",
    "models , estimating procedures similar to have been studied by a number of authors using methods of stochastic approximation theory ( see , e.g. , khasminskii and nevelson @xcite , fabian @xcite , ljung and soderstrom @xcite , ljung et al @xcite , and references therein ) . some work has been done for non i.i.d .",
    "models as well . in particular ,",
    "englund et al @xcite give an asymptotic representation results for certain type of @xmath34 processes .",
    "in sharia @xcite theoretical results on convergence , rate of convergence and the asymptotic representation are given under certain regularity and ergodicity assumptions on the model , in the one - dimensional case with @xmath69 ( see also campbell @xcite , sharia @xcite and lazrieva et al @xcite ) .",
    "in the present paper , we study multidimensional estimation procedures of type for the general statistical model .",
    "section 2 introduces the basic model , objects and notation . in section 3 , imposing `` global '' restrictions on the processes @xmath7 and @xmath70 , we study `` global '' convergence of the recursive estimators , that is the convergence for an arbitrary starting point @xmath71 . in section 4 ,",
    "we demonstrate the use of these results on some examples .",
    "( results on rate of convergence , asymptotic linearity and efficiency , and numerical simulations will appear in subsequent publications , see sharia @xcite , @xcite . )",
    "let @xmath72 be observations taking values in a measurable space @xmath73 equipped with a @xmath13-finite measure @xmath74 suppose that the distribution of the process @xmath75 depends on an unknown parameter @xmath76 where @xmath77 is an open subset of the @xmath78-dimensional euclidean space @xmath79 .",
    "suppose also that for each @xmath80 , there exists a regular conditional probability density of @xmath75 given values of past observations of @xmath81 , which will be denoted by @xmath82 where @xmath83 is the probability density of the random variable @xmath84 without loss of generality we assume that all random variables are defined on a probability space @xmath85 and denote by @xmath86 the family of the corresponding distributions on @xmath87    let @xmath88 be the @xmath13-field generated by the random variables @xmath89 by @xmath90 we denote the @xmath78-dimensional euclidean space with the borel @xmath13-algebra @xmath91 .",
    "transposition of matrices and vectors is denoted by @xmath92 . by @xmath93",
    "we denote the standard scalar product of @xmath94 that is , @xmath95    suppose that @xmath96 is a real valued function defined on @xmath97 .",
    "we denote by @xmath98 the row - vector of partial derivatives of @xmath99 with respect to the components of @xmath2 , that is , @xmath100 also we denote by h@xmath101 the matrix of second partial derivatives .",
    "the @xmath102 identity matrix is denoted by @xmath103 .    if for each @xmath80 , the derivative @xmath104 w.r.t .",
    "@xmath2 exists , then we can define the function @xmath105 with the convention @xmath106 .",
    "the _ one step conditional fisher information matrix _ for @xmath80 is defined as @xmath107 we shall use the notation @xmath108 @xmath109 note that the process @xmath110 is _ `` predictable '' _ , that is , the random variable @xmath111 is @xmath112 measurable for each @xmath113    note also that by definition , @xmath110 is a version of the conditional expectation w.r.t . @xmath114",
    "that is , @xmath115 everywhere in the present work conditional expectations are meant to be calculated as integrals w.r.t . the conditional probability densities .",
    "the _ conditional fisher information _ at time @xmath116 is @xmath117 if the @xmath75 s are independent random variables , @xmath118 reduces to the standard fisher information matrix . sometimes @xmath118 is referred as the incremental expected fisher information .",
    "detailed discussion of this concept and related work appears in barndorff - nielsen and sorensen @xcite , prakasa - rao @xcite ch.3 , and hall and heyde @xcite .",
    "we say that @xmath119 is a sequence of estimating functions and write @xmath120 , if for each @xmath121 @xmath122 is a borel function .",
    "let @xmath123 and denote @xmath124 we write @xmath125 if   @xmath126 is a martingale - difference process for each @xmath76   i.e. ,   if @xmath127 for each @xmath128 ( we assume that the conditional expectations above are well - defined and @xmath129 is the trivial @xmath13-algebra ) .",
    "note that if differentiation of the equation @xmath130 is allowed under the integral sign , then @xmath131 .",
    "+ 0.5 cm    * convention * _ everywhere in the present work @xmath132 is an arbitrary but fixed value of the parameter .",
    "convergence and all relations between random variables are meant with probability one w.r.t . the measure @xmath133 unless specified otherwise . a sequence of random variables @xmath134 has some property eventually if for every @xmath135 in a set @xmath136 of @xmath133 probability 1",
    ", @xmath137 has this property for all @xmath116 greater than some @xmath138 . _",
    "suppose that @xmath120 and @xmath139 , for each @xmath140 , is a predictable @xmath102 matrix process with @xmath141 , @xmath142 .",
    "consider the estimator @xmath143 defined by @xmath144 where @xmath145 is an arbitrary initial point .",
    "let @xmath146 be an arbitrary but fixed value of the parameter and for any @xmath147 define @xmath148    suppose that    ( c1 ) : :    @xmath149 ( c2 ) : :    @xmath150 for each @xmath151    @xmath152 ( c3 ) : :    there exists a predictable scalar process    @xmath153 such that    @xmath154 for each    @xmath155 @xmath133-a.s . , and    @xmath156    then @xmath157 is strongly consistent ( i.e. , @xmath158-a.s . ) for any initial value @xmath159",
    ".    we will derive this theorem from a more general result ( see the end of the section ) .",
    "let us first comment on the conditions used here .",
    "conditions ( c1 ) , ( c2 ) , and ( c3 ) are natural analogues of the corresponding assumptions in theory of stochastic approximation .",
    "indeed , let us consider the i.i.d .",
    "case with @xmath160 where @xmath161 and @xmath162 for some invertible non - random matrix @xmath163 .",
    "then @xmath164 implying that @xmath165 .",
    "denote @xmath166 and rewrite in the form @xmath167 where @xmath168 equation defines a robbins - monro stochastic approximation procedure that converges to the solution of the equation @xmath169 when the values of the function @xmath170 can only be observed with zero expectation errors @xmath171 .",
    "note that in general , recursion can not be considered in the framework of classical stochastic approximation theory ( see lazrieva et al @xcite , @xcite for the generalized robbins - monro stochastic approximations procedures ) .",
    "for the i.i.d .",
    "case , conditions ( c1 ) , ( c2 ) and ( c3 ) can be written as * ( i ) * and * ( ii ) * in section 4 , which are standard assumptions for stochastic approximation procedures of type ( see , e.g. , robbins and monro @xcite , gladyshev @xcite , khasminskii and nevelson @xcite , ljung and soderstrom @xcite , ljung et al @xcite ) .    to understand how the procedure works , consider the one - dimensional case , denote @xmath172 and rewrite in the form @xmath173 then , @xmath174 suppose now that at time  @xmath175   @xmath176 that is , @xmath177 then , by ( c1 ) , @xmath178 implying that @xmath179 so , the next step @xmath157 will be in the direction of @xmath2 .",
    "if at time  @xmath175   @xmath180 by the same reason , @xmath181 so , the condition ( c1 ) ensures that , on average , at each step the procedure moves towards @xmath2 .",
    "however , the magnitude of the jumps @xmath182 should decrease , for otherwise , @xmath157 may oscillate around @xmath2 without approaching it .",
    "this is guaranteed by ( c3 ) . on the other hand , ( c2 )",
    "ensures that the jumps do not decrease too rapidly to avoid failure of @xmath157 to reach @xmath183    + 0.2 cm now , let us consider a maximum likelihood type recursive estimator @xmath184 where @xmath185 and @xmath186 is the conditional fisher information with @xmath187 ( see also for the i.i.d .",
    "case ) . by theorem 3.1 , @xmath188 is strongly consistent if conditions ( c1 ) , ( c2 ) and ( c3 ) are satisfied with @xmath189 and @xmath118 replacing @xmath190 and @xmath139 respectively . on the other hand ,",
    "if e.g. , in the one - dimensional case , @xmath191 is differentiable at @xmath192 and the differentiation is allowed under the integral sign , then @xmath193 so , if the differentiation w.r.t . @xmath2   of @xmath194",
    "is allowed under the integral sign , @xmath195 implying that ( c1 ) always holds for small values of @xmath196    + 0.2 cm condition ( c2 ) in the i.i.d .",
    "case is a requirement that the function @xmath197 is separated from zero on each finite interval that does not contain @xmath198 . for the i.i.d .",
    "case with continuous w.r.t @xmath199 functions @xmath200 and @xmath201 condition ( c2 ) is an easy consequence of ( c1 ) .",
    "+ 0.2 cm condition ( c3 ) is a boundedness type assumption which restricts the growth of @xmath202 w.r.t .",
    "@xmath2 with certain uniformity w.r.t .",
    "@xmath116 .",
    "+ 0.2 cm    we denote by @xmath203 ( respectively @xmath204 ) the positive ( respectively negative ) part of @xmath205 .",
    "suppose that for @xmath206 there exists a real valued nonnegative function @xmath207 having continuous and bounded partial second derivatives and    ( g1 ) : :    @xmath208 and for each    @xmath151    @xmath209 ( g2 ) : :    there exists a set @xmath210 with    @xmath211 such that for each    @xmath151    @xmath212 ^ -=\\infty\\ ] ]    on @xmath213 , where @xmath214    ( g3 ) : :    @xmath150 for @xmath215    @xmath216^+",
    "< \\infty , \\qquad     p^\\theta\\mbox{-a.s.}.\\ ] ]    then @xmath217-a.s . ) for any initial value @xmath159 .    * proof . * as always ( see the convention in section 2 ) , convergence and all relations between random variables are meant with probability one w.r.t . the measure @xmath133 unless specified otherwise .",
    "rewrite in the form @xmath173 by the taylor expansion , @xmath218^t { \\mbox{\\\"{v}}}_\\theta(\\tilde \\dl_{t } ) \\gm_t^{-1}(\\theta+\\dl_{t-1 } ) \\p_t(\\theta+\\dl_{t-1 } ) , \\nonumber\\end{aligned}\\ ] ] where @xmath219 . taking the conditional expectation w.r.t .",
    "@xmath220 yields @xmath221 using the obvious decomposition @xmath222}^+ - { [ { \\cal n}_t(\\dl_{t-1})]}^- , $ ] the previous inequality can be rewritten as @xmath223 ^ -,\\ ] ] where @xmath224^+.\\ ] ] by condition ( g3 ) , @xmath225 according to lemma a1 in appendix a ( with @xmath226 , @xmath227 and @xmath228}^-$ ] ) , inequalities and imply that the processes @xmath229 and @xmath230 ^ -\\ ] ] converge to some finite limits .",
    "it therefore follows that @xmath231 suppose that @xmath232 then there exists @xmath233 such that @xmath234 eventually . because of ( g2 ) , this implies that for some ( possibly random ) @xmath235 @xmath236 ^ - \\ge \\sum_{s = t_0}^{\\infty } \\inf_{\\ve \\le v_\\theta(u ) \\le { 1/\\ve } } \\left[{\\cal n}_s(u)\\right]^-=\\infty\\ ] ] on the set @xmath213 with @xmath237 which contradicts the existence of a finite limit of @xmath238 hence , @xmath239 and so , @xmath240 .",
    "now , @xmath241 follows from ( g1 ) ( otherwise there would exist a sequence @xmath242 such that @xmath243 for some @xmath244 and ( g1 ) would imply that @xmath245 ) . @xmath246    + 0.2 cm *",
    "proof of theorem 3.1 . * as always ( see the convention in section 2 ) , convergence and all relations between random variables are meant with probability one w.r.t . the measure @xmath133 unless specified otherwise .",
    "let us show that the conditions of theorem 3.1 imply those in theorem 3.2 with @xmath247 condition ( g1 ) trivially holds .",
    "since @xmath248 and v@xmath249 it follows that @xmath250 then , by ( c1 ) and ( c3 ) , @xmath251^+ \\nonumber\\\\ & & \\le    \\sum_{t=1}^\\infty ( 1+\\|\\dl_{t-1}\\|^2)^{-1 } e_\\theta \\left\\{\\|\\gm_t^{-1}(\\theta+\\dl_{t-1 } ) \\p_t(\\theta+\\dl_{t-1}\\|^2 \\mid { \\cf}_{t-1}\\right\\ } \\nonumber\\\\ & & \\le \\sum_{t=1}^\\infty   b_t < \\infty.\\end{aligned}\\ ] ] so , ( g3 ) holds . to derive ( g2 ) , using the obvious inequality @xmath252 ^ - \\ge -a$ ] and ( c1 ) , we write @xmath253 ^ - & \\ge & \\inf \\left",
    "[ -2u^t \\gm_t^{-1}(\\theta+u )   b_t(\\theta , u ) \\right . \\\\ & & -e_\\theta \\left .",
    "\\left\\{\\|\\gm_t^{-1}(\\theta+u ) \\p_t(\\theta+u)\\|^2 \\mid { \\cf}_{t-1}\\right\\ } \\right]\\\\ \\nonumber & \\ge & \\inf \\left| 2u^t\\gm_t^{-1}(\\theta+u)b_t(\\theta , u)\\right| \\\\ & & -\\sup\\left[e_\\theta \\left\\{\\|\\gm_t^{-1}(\\theta+u ) \\p_t(\\theta+u)\\|^2 \\mid { \\cf}_{t-1}\\right\\ } \\right],\\end{aligned}\\ ] ] where @xmath254 s and @xmath255 s are taken over @xmath256 . from ( c3 ) , @xmath257 \\le b_t(1 + 1/{\\ve^2})\\ ] ] and @xmath258 now , using ( c2 ) , we finally obtain @xmath259 ^ - \\ge \\sum_{t=1}^\\infty \\inf \\left| 2u^t\\gm_t^{-1}(\\theta+u)b_t(\\theta , u)\\right| - ( 1 + 1/{\\ve^2})\\sum_{t=1}^\\infty   b_t=\\infty,\\ ] ] which implies ( g2 ) .",
    "so , theorem 3.1 follows on application of theorem 3.2 .",
    "@xmath246    it follows from the proof of theorem 3.2 that if conditions ( g1 ) and ( g3 ) are satisfied then @xmath260 converges ( @xmath133-a.s . ) to a finite limit , for any initial value @xmath159 .",
    "in particular , to guarantee this convergence , it suffices to require conditions ( c1 ) and ( c3 ) of theorem 3.1 ( this can be seen by taking @xmath261 and ) .",
    "consider the classical scheme of i.i.d .",
    "observations @xmath262 with a common probability density / mass function @xmath263 suppose that @xmath264 is an estimating function with @xmath265 let us define the recursive estimator @xmath143 by @xmath266 where @xmath163 is a non - random matrix such that @xmath267 exists for any @xmath140 and @xmath51 is any initial value .",
    "+ 0.5 cm similar results ( for i.i.d .",
    "schemes ) were obtained by khasminskii and nevelson @xcite ch.8 , @xmath2784 , and fabian @xcite .",
    "note that conditions ( i ) and ( ii ) are derived from theorem 3.1 and are sufficient conditions for the convergence of . applying theorem 3.2 to",
    ", one can obtain various alternative sufficient conditions analogous to those given in fabian ( 1978 ) .",
    "note also that , in ( 4.1 ) , the normalising sequence is @xmath276 but theorems 3.1 and 4.1 allow to consider procedures with arbitrary predictable @xmath279 + 0.5 cm      consider the recursion @xmath280 where the @xmath281 and @xmath282 are predictable processes , @xmath283 is an adapted process ( i.e. , @xmath283 is @xmath284-measurable for @xmath142 ) and all three are independent of @xmath183 the following result gives a sets of sufficient conditions for the convergence of in the case when the linear @xmath285 is a martingale - difference .",
    "( a ) : :    @xmath287   for    @xmath288  @xmath133-a.s . , ( b ) : :    @xmath289 eventually for some    @xmath290   and    @xmath291 on a    set @xmath213 of positive probability @xmath133 .",
    "( c ) : :    @xmath292      * proof .",
    "* we need to check that the conditions of theorem 3.2 hold for for @xmath295 .",
    "using ( a ) we obtain @xmath296 and @xmath297 @xmath298 where @xmath299 now , using , @xmath300 @xmath301 to derive ( g2 ) , we use the obvious inequality @xmath252 ^ - \\ge -a$ ] ( for any @xmath302 ) , conditions ( b ) and ( c ) , and write @xmath303 ^ -\\ge \\sum_{t=1}^\\infty \\inf_{\\ve \\le u^2 \\le { 1/\\ve } } \\left(\\delta u^2\\gamma_t\\gamma_t^{-1 } -\\gamma_t^{-2}\\mathcal{p}_t^\\theta\\right)=\\infty\\ ] ] on @xmath213 . to check ( g3 )",
    "we write @xmath304^+ \\le \\sum_{t=1}^\\infty \\left[{\\cal n}_t(\\dl_{t-1})\\right]^+\\le \\sum_{t=1}^\\infty \\gamma_t^{-2}\\mathcal{p}_t^\\theta<\\infty\\ ] ] ( @xmath133-a.s . ) , which completes the proof .",
    "@xmath246    suppose that @xmath305 then @xmath306 this can be easily seen by inspecting the difference @xmath182 for the sequence , to check that holds .",
    "it is also interesting to observe that since in this case , @xmath307 @xmath308 where , @xmath309 is a @xmath133 martingale .",
    "now , if @xmath310 , a necessary and sufficient condition for the convergence to @xmath2 is the convergence to zero of the sequence @xmath311 condition ( c ) in corollary 4.2 is a standard sufficient condition in martingale theory to guarantee @xmath312 ( see e.g. , shiryayev @xcite , ch.vii , @xmath313 theorem 4 ) .",
    "the first part of ( b ) will trivially hold if @xmath314 .",
    "also , in this case , @xmath310 implies @xmath315 ( see proposition a3 in appendix a ) .      as a particular example , consider the process @xmath316 where , @xmath137 is a @xmath133 martingale - difference with @xmath317 the choice @xmath318 and @xmath319 , in yields the least square estimator of @xmath183 it is easy to verify that ( a ) holds . also , since @xmath320 it follows that ( c ) in corollary 4.2 is equivalent to @xmath321 this , as well as ( b ) hold if @xmath310 ( see proposition a3 in appendix a ) .",
    "so , if @xmath310 the least square procedure is strongly consistent . if , e.g. , @xmath137 are i.i.d .",
    "r.v.s , then @xmath310 for all values of @xmath322 ( see , e.g , shiryayev @xcite , ch.vii , 5.5 ) .",
    "a reasonable class of procedures in this model should have a form @xmath327 where @xmath328 and @xmath329 ( @xmath330 ) are respectively vector and matrix processes meeting conditions of the previous section .",
    "suppose that the probability density function of @xmath331 w.r.t .",
    "lebesgue s measure is @xmath332 .",
    "then the conditional probability density function is @xmath333 so , denoting @xmath334 it is easy to see that @xmath335 and becomes a likelihood recursive procedure",
    ". a possible choice of @xmath336 in this case would be the conditional fisher information matrix @xmath337 where @xmath338 an interesting class of recursive estimators for strongly stationary ar(m ) processes is studied in campbell @xcite .",
    "these estimators are recursive versions of robust modifications of the least squares method and are defined as @xmath339 where @xmath340 is a sequence of a positive numbers with @xmath341 @xmath342 is a bounded scalar function and @xmath343 is a vector function of the form @xmath344 for some non - negative function @xmath96 of @xmath199 ( see also leonov @xcite ) .",
    "the class of procedures of type is clearly a subclass of that defined by and therefore can be studies using the results of the previous section .",
    "suppose that @xmath345 are i.i.d .",
    "random variables with a bell - shaped , symmetric about zero probability density function @xmath346 ( that is , @xmath347 and @xmath348 on @xmath349 ) .",
    "suppose also that @xmath350 is an odd , continuous in zero function .",
    "let us write conditions of theorem 3.1 for @xmath351 we have @xmath352 it follows from lemma a2 in appendix a that if @xmath353 @xmath354 therefore , @xmath355 @xmath356 also , since @xmath342 is a bounded function , @xmath357 for some positive constant @xmath358 .",
    "therefore , conditions of theorem 3.1 hold if ( @xmath133-a.s . ) , @xmath359 and @xmath360 if @xmath75 is a stationary process , these conditions can be verified using limit theorems for stationary processes .",
    "suppose , e.g. , that @xmath361 , @xmath362 for any @xmath363 , and @xmath346 is continuous .",
    "then @xmath364 for any @xmath365 ( see appendix a , lemma a2 ) .",
    "therefore , it follows from an ergodic theorem for stationary processes that in probability @xmath366 @xmath367          as a particular example of ( 4.4 ) , consider the process @xmath316 where , @xmath368 are independent student random variables with degrees of freedom @xmath369 .",
    "so , the probability density functions of @xmath137 is @xmath370 where @xmath371    since @xmath372 ( see also ) , @xmath373 and the conditional fisher information is @xmath374 where @xmath375 therefore , a likelihood recursive procedure is @xmath376 where @xmath159 is any starting point .",
    "note that @xmath377 can also be derived recursively by @xmath378 clearly , is a recursive procedure of type but with a stochastic normalizing sequence @xmath379 .",
    "now , @xmath380 is of a form of with @xmath381 and @xmath382 and @xmath346 is a bell - shaped and symmetric about zero .",
    "therefore , to show convergence to @xmath383 it suffices to check conditions and , which , in this case can be written as @xmath384 and @xmath385 ( @xmath133-a.s . ) .",
    "we have , @xmath386 for any @xmath387 ( see , e.g , shiryayev @xcite , ch.vii , 5.5 ) .",
    "since @xmath388 we obtain that follows from proposition a3 in appendix a. let us assume now that @xmath389 by lemma a2 in appendix a , @xmath390 for any @xmath391 then if we assume that the the process is strongly stationary , it follows from the ergodic theorem that in probability @xmath133 , @xmath392 ( it can be proved that these hold without assumption of strong stationarity . ) therefore , in probability @xmath366 @xmath393 and now follows on application of proposition a4 in appendix a.    we have shown above that the recursive estimator is strongly consistent , i.e. , converges to @xmath2 a.s .",
    ", if @xmath394 it is worth mentioning that , and therefore , holds for any @xmath322 , which guarantees ( c3 ) of theorem 3.1 .",
    "also , implies that ( c1 ) of theorem 3.1 holds as well .",
    "therefore , according to remark 3.3 , we obtain that @xmath395 converges ( @xmath133-a.s . ) to a finite limit for any @xmath322 .",
    "note that conditions and will still hold if we replace @xmath377 by @xmath396 where @xmath397 is a sequence of non - negative r.v.s such that @xmath398 eventually .",
    "so , the procedure will remain consistent if @xmath377 is replaced by @xmath399 i.e. , if tuning constants are introduced .",
    "we have shown that the procedure is consistent , i.e. , the recursive estimator is close to the value of the unknown parameter for the large @xmath116 s .",
    "but in practice , the tuning constants may be useful to control the behaviour of a recursion at the `` beginning '' of the procedure .",
    "fig.1 shows realisations of for @xmath400 and @xmath401 for three different starting values .",
    "the number of observations is 40 . as we can see from these graphs , the recursive procedure , at each step moves in the direction of the parameter ( see also remark 3.2 ) , but oscillates quite violently for the first ten steps and then settles down nicely after another ten steps .",
    "this oscillation is due to the small values of the normalising sequence for the first several steps and can be dealt with by introducing tuning constants . on other occasions",
    ", it may be desirable to lower the value of the normalising sequence for the first several steps .",
    "this happens when a procedure settles down too quickly without any , or little oscillation ( before reaching the actual value of the parameter ) .",
    "the detailed discussion of these and related topics will appear elsewhere .",
    "* lemma a1 * _ let @xmath402 be a non - decreasing sequence of @xmath13-algebras and @xmath403 are nonnegative r.v.s such that @xmath404 eventually . then _",
    "@xmath405 where @xmath406 denotes the set where @xmath407 exists and is finite .",
    "+ 0.2 cm * remark * proof can be found in robbins and siegmund @xcite .",
    "note also that this lemma is a special case of the theorem on the convergence sets nonnegative semimartingales ( see , e.g. , lazrieva et al @xcite ) .",
    "* lemma a2 * _ suppose that @xmath408 is a nonnegative even function on @xmath409 and @xmath348 on @xmath410 .",
    "suppose also that @xmath342 is a measurable odd function on @xmath409 such that @xmath411 for @xmath412 and @xmath413 for all @xmath414 .",
    "then @xmath415 for any @xmath416 furthermore , if @xmath346 is continuous , then for any _",
    "@xmath417 @xmath418 * proof * denote @xmath419 using the change of variable @xmath420 in the integral over @xmath421 and the equalities @xmath422 and @xmath423 , we obtain @xmath424 suppose now that @xmath425 . then @xmath426 is closer to @xmath198 than @xmath427 , and the properties of @xmath428 imply that @xmath429 . since @xmath411 for @xmath412 , @xmath430 .",
    "the equality @xmath431 would imply that @xmath432 for _ all _ @xmath433 since , being monotone , @xmath428 has right and left limits at each point of @xmath434 .",
    "the last equality , however , contradicts the restrictions on @xmath428 .",
    "therefore , ( a1 ) holds .",
    "similarly , if @xmath435 , then @xmath427 is closer to @xmath198 than @xmath426 , and @xmath436 .",
    "hence @xmath437 , which yields ( a1 ) as before .    to prove ( a2 ) note that the continuity of @xmath428 implies that @xmath438 is a continuous functions of @xmath439 and ( a2 ) will follow from ( a1 ) if one proves that @xmath440 is also continuous in @xmath439",
    "so , it is sufficient to show that the integral in ( a3 ) is uniformly convergent for @xmath441 .",
    "it follows from the restrictions we have placed on @xmath428 that there exists @xmath442 such that @xmath443 in a neighbourhood of 0 .",
    "then the condition @xmath444 implies that @xmath342 is locally integrable on @xmath409 .",
    "it is easy to see that , for any @xmath417 , @xmath445 where @xmath446 is the indicator function of the interval @xmath447 $ ] .",
    "since the function @xmath448 is integrable on @xmath434 and does not depend on @xmath439 , we conclude that the integral in ( a3 ) is indeed uniformly convergent for @xmath441 . @xmath449    + 0.3 cm * proposition a3 * _ if @xmath450 is a nondecreasing sequence of positive numbers such that @xmath451 , then @xmath452 and @xmath453 _ + 0.2 cm * proof * the first claim is easily obtained by contradiction from the kronecker lemma ( see , e.g. , lemma 2 , @xmath2783 , ch .",
    "iv in shiryayev @xcite ) .",
    "the second one is proved by the following argument @xmath454 @xmath449      + 0.3 cm * proof * denote @xmath461 . since @xmath462 in probability , it follows that there exists a subsequence @xmath463 of @xmath464 with the property that @xmath465 with probability 1 .",
    "now , assume that @xmath466 on a set @xmath213 of positive probability .",
    "then , it follows from the kronecker lemma , ( see , e.g. , lemma 2 , @xmath2783 , ch .",
    "iv in shiryayev @xcite ) that @xmath467 on @xmath213 .",
    "then it follows that @xmath468 on @xmath213 as well , implying that @xmath469 on @xmath213 which contradicts the assumptions that @xmath470 with probability 1 .",
    "@xmath449    1 o.e . and sorensen , m. : a review of some aspects of asymptotic likelihood theory for stochastic processes , _ international statistical review . , _",
    "* 62 , * 1 ( 1994 ) , 133 - 165 .",
    "k. : recursive computation of m - estimates for the parameters of a finite autoregressive process , _ ann .",
    "_ , * 10 * ( 1982 ) , 442 - 453 .",
    "e . , holst , u. , and ruppert , d. : recursive estimators for stationary , strong mixing processes  a representation theorem and asymptotic distributions , _",
    "stochastic processes appl .",
    "_ , * 31 * ( 1989 ) , 203222 .",
    "v. : on asymptotically efficient recursive estimation , _ ann .",
    "_ , * 6 * ( 1978 ) , 854 - 867 .",
    "e.g. : on stochastic approximation , _ theory probab",
    "_ , * 10 * ( 1965 ) , 297300 .    p. and heyde , c.c . : _ martingale limit theory and its application _ , academic press , new york , 1980 .",
    "f.r . , ronchetti , e.m . ,",
    "rousseeuw , p.j . , and stahel , w. : _ robust statistics - the approach based on influence functions _ , wiley , new york , 1986 . a.c .",
    ": _ time series models _ , harvester wheatsheaf , london , 1993 .",
    "_ robust statistics _ , wiley , new york , 1981",
    ". j. and sen , p.k . : _ robust statistical procedures - asymptotics and interrelations , _ wiley , new york , 1996 .",
    "r.z . and nevelson , m.b .",
    ": _ stochastic approximation and recursive estimation _ ,",
    "nauka , moscow , 1972 .",
    "r.l . and wilkinson , g.n . :",
    "_ robustness in statistics _ , academic press , new york , 1979 .",
    "n. , sharia , t. and toronjadze , t. : the robbins - monro type stochastic differential equations . i. convergence of solutions , _ stochastics and stochastic reports , _ * 61 * ( 1997 ) , 6787 . n. , sharia , t. and toronjadze , t. : the robbins - monro type stochastic differential equations .",
    "asymptotic behaviour of solutions , _ stochastics and stochastic reports , _ * 75*(2003 ) , 153180",
    ". n. and toronjadze , t. : .",
    "ito - ventzel s formula for semimartingales , asymptotic properties of mle and recursive estimation , _ lect .",
    "notes in control and inform .",
    "sciences , 96 , stochast .",
    "systems , h.j , engelbert , w. schmidt ( eds . ) , 1987 , springer , _ 346355 . e.l .",
    ": _ theory of point estimation , _ wiley , new york , 1983 .",
    "s.l . : on recurrent estimation of autoregression parameters , _ avtomatika",
    "i telemekhanika , _ * 5 * ( 1988 ) , 105 - 116 .",
    "l. pflug , g. and walk , h. : _ stochastic approximation and optimization of random systems , _ birkhuser , basel , 1992 .",
    "l. and soderstrom , t. : _ theory and practice of recursive identification , _ mit press , 1987 .",
    "b.l.s . : _",
    "semimartingales and their statistical inference , _ chapman @xmath471 hall , new york , 1999 .",
    "h. : _ robust asymptotic statistics , _ springer ",
    "verlag , new york , 1994 .",
    "h. and monro , s. : a stochastic approximation method , _ ann .",
    "statist . _",
    "* 22 * ( 1951 ) , 400407 .",
    "h. and siegmund , d. : a convergence theorem for nonnegative almost supermartingales and some applications , _ optimizing methods in statistics _",
    "rustagi academic press , new york , 1971 , 233257 .",
    "approximation theorems of mathematical statistics , _ wiley , new york , 1980 .",
    "t. : on the recursive parameter estimation for the general discrete time statistical model , _",
    "stochastic processes appl .",
    "_ * 73 * , * 2 * ( 1998 ) , 151172 .",
    "t. : truncated recursive estimation procedures , _ proc .",
    "a. razmadze math .",
    "* 115 * ( 1997 ) , 149159 .",
    "t. : rate of convergence in recursive parameter estimation procedures , ( submitted ) ( _ http://personal.rhul.ac.uk/ukah/113/gmja.pdf_ ) .",
    "t. : recursive parameter estimation : asymptotic expansion , ( submitted ) ( _ http://personal.rhul.ac.uk/ukah/113/tshar.pdf_ ) .",
    "probability , _ springer - verlag , new york , 1984 ."
  ],
  "abstract_text": [
    "<S> we consider estimation procedures which are recursive in the sense that each successive estimator is obtained from the previous one by a simple adjustment . we propose a wide class of recursive estimation procedures for the general statistical model and study convergence .    </S>",
    "<S> h    f    p    [ section ] [ section ] [ section ] [ section ]    _ department of mathematics + royal holloway , university of london + egham , surrey tw20 0ex + e - mail : t.sharia@rhul.ac.uk_    keywords : recursive estimation , estimating equations , stochastic approximation .    </S>",
    "<S> subject classifications : 62m99 , 62l12 , 62l20 , 62f10 , 62f12 , 62f35 </S>"
  ]
}