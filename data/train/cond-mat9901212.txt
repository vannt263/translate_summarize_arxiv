{
  "article_text": [
    "one of the most popular forms of neural network training is on - line learning , in which training examples ( input - output pairs ) are presented sequentially and independently at each learning iteration ( for an overview of on - line learning in neural networks , see  @xcite ) .",
    "natural gradient descent ( ngd ) was recently proposed by amari as a principled alternative to standard on - line gradient descent ( gd )  @xcite . when learning the parameters of a statistical model , in our case a feedforward neural network , this algorithm has the desirable properties of asymptotic optimality , given a realizable learning problem and differentiable model , and invariance to reparametrizations of our model distribution .",
    "ngd is already established as a popular on - line algorithm for independent component analysis  @xcite and shows much promise for other statistical learning problems .",
    "yang and amari recently introduced an ngd algorithm for training a multilayer perceptron  @xcite . in this paper",
    "we provide an analysis of ngd for this problem using a statistical mechanics formalism .",
    "our results indicate that ngd provides significantly improved performance over gd and we quantify these gains for both the transient and asymptotic stages of learning ( preliminary results from this work have been reported in  @xcite ) .",
    "the intuition behind ngd comes from viewing the parameter space of a statistical model as a riemannian space .",
    "a natural measure of infinitesimal distance between probability distributions is given by the kullback - leibler divergence  @xcite .",
    "in this case the fisher information matrix can be shown to be the appropriate riemannian metric .",
    "the natural gradient direction is defined as the direction of steepest descent under this metric and is obtained by pre - multiplying the standard euclidean error gradient with the inverse of the fisher information matrix .",
    "since this metric is derived from a divergence between neighboring model distributions , the algorithm is clearly independent of model parametrization .",
    "an additional beneficial feature of using this matrix pre - multiplier is that it remains positive - definite and therefore ensures convergence to a minimum of the generalization error ( assuming the learning rate is annealed appropriately ) .",
    "this is to be contrasted with other variable - metric algorithms which utilize the inverse averaged hessian matrix .",
    "pre - multiplying the error gradient with the inverse hessian may make other fixed points stable , so that the algorithm could converge to maxima or saddle points on the mean error surface .",
    "although such methods can be adapted to ensure a positive - definite matrix pre - multiplier , such adaptations are rather ad - hoc in nature and are not theoretically well motivated outside of the asymptotic regime .",
    "variable - metric methods are often difficult to implement as on - line algorithms since they require the averaging and inversion of a large matrix . in the case of ngd",
    "we require knowledge of the input distribution in order to calculate the fisher information matrix .",
    "yang and amari discuss methods for pre - processing training examples in order to obtain a whitened gaussian process for the inputs  @xcite .",
    "if this is possible then , when the input dimension @xmath0 is large compared to the number of hidden units @xmath1 , inversion of the fisher information for two - layer feedforward networks requires only @xmath2 operations , providing an efficient and practical algorithm in many cases .",
    "such a simplification is not possible for hessian based methods , because the hessian involves an average over input - output pairs . in general",
    "it will not be possible to apply this pre - processing because the input distribution may be far from gaussian and difficult to estimate . in this case",
    "other on - line methods will be required in order to approximate the ngd algorithm .",
    "we have recently proposed a method based on a matrix momentum algorithm  @xcite which allows efficient on - line inversion and averaging of the fisher information matrix .",
    "this algorithm can be shown to approximate ngd closely and also provides optimal asymptotic performance , although at the cost of introducing an extra variable parameter  @xcite .    here",
    ", we will consider the idealized situation in which we have the fisher information matrix at our disposal .",
    "we solve the averaged dynamics of ngd using a statistical mechanics framework which becomes exact as @xmath3 for finite @xmath1 ( see , for example , @xcite ) .",
    "this allows us to compare performance with standard gd in both the transient and asymptotic phases of learning , so that we can quantify the advantage that ngd can be expected to provide .",
    "numerical results for a small network provide evidence of improved performance . in order to obtain more generic results",
    "we introduce a site - symmetric ansatz for the special case of a realizable learning scenario , so that we can efficiently explore a broad range of task complexity and non - linearity .",
    "we show that trapping time in an unstable fixed point which dominates the training time , the symmetric phase , is significantly reduced by using ngd and exhibits a slower power law increase as task complexity grows .",
    "we also find that asymptotic performance is greatly improved , with the generalization performance of ngd equalling the known universal asymptotics for batch learning  @xcite .",
    "we consider a probabilistic model @xmath4 for the distribution of a scalar output @xmath5 given a vector of inputs @xmath6 which is parameterized by @xmath7 .",
    "the kullback - leibler divergence provides an appropriate measure for the distance between distributions  @xcite and for two nearby points in parameter space we find , @xmath8 where @xmath9 is the fisher information matrix , @xmath10 this matrix provides a riemannian metric within the space of model parameters .",
    "we choose the training error @xmath11 .",
    "the direction of steepest descent within this riemannian space in terms of expected error is obtained by pre - multiplying the mean euclidean error gradient with @xmath12  @xcite .    in an on - line learning scheme",
    "we draw inputs sequentially @xmath13 from some distribution @xmath14 each labelled according to some stochastic rule @xmath15 .",
    "the ngd algorithm is defined by a corresponding sequence of weight updates , @xmath16 where the learning rate is scaled by the input dimension for convenience .",
    "this algorithm therefore utilizes an unbiased estimate of the steepest descent direction in our riemannian parameter space .",
    "if the rule can be realized by the model and exemplars are corrupted by output noise then annealing the learning rate as @xmath17 at late times ( where @xmath18 ) results in optimal asymptotic performance in terms of the quadratic estimation error , saturating the cramer - rao lower bound and equalling in performance even the best batch algorithms  @xcite .",
    "consider the deterministic mapping @xmath19 which defines a soft committee machine ( we call this the student network ) , where @xmath20 is some sigmoid activation function for the hidden units , @xmath21 is the set of input to hidden weights and the hidden to output weights are set to one .",
    "we choose the following gaussian noise model , @xmath22 the fisher information matrix for this model distribution is given by @xmath23 with @xmath24 in block form , @xmath25 a particularly convenient choice for activation function is @xmath26 as this allows the average over inputs to be carried out analytically for an isotropic gaussian input distribution @xmath27 , @xmath28 \\ , \\ ] ] where @xmath29 and @xmath30 .",
    "in order to analyse ngd beyond the asymptotic regime we use a statistical mechanics description of the learning process which is exact in the limit of large input dimension @xmath0 and provides an accurate model of mean behaviour for realistic values of @xmath0  @xcite .",
    "we consider the case where outputs are generated by a teacher network corrupted by gaussian noise , @xmath31 where @xmath32 . due to the flexibility of this mapping",
    "@xcite we can represent a variety of learning scenarios within this framework .",
    "the weight update at each iteration of ngd is then given by , @xmath33 where @xmath34 $ ] and @xmath35 is zero - mean gaussian noise of variance @xmath36 .",
    "notice that knowledge of the noise variance is not required to execute this algorithm since the contributions from the fisher information matrix and log - likelihood cancel ( recall eq .",
    "( [ j_update ] ) ) .",
    "the model noise variance is therefore not included as a variable parameter and the algorithm is well defined even in the deterministic case where @xmath37 .",
    "the fisher information matrix can be inverted using the partitioning method described in  @xcite ( see appendix  [ app_inv ] ) ; each block is some additive combination of the identity matrix and outer products of the student weight vectors , @xmath38 where @xmath39 are scalars while @xmath40 are @xmath1 dimensional square matrices . using the methods described in  @xcite it is then straightforward to derive equations of motion for a set of order parameters @xmath41 , @xmath42 and @xmath43 , measuring the various overlaps between student and teacher vectors .",
    "these order parameters are necessary and sufficient to determine the generalization error @xmath44 , which we defined to be the expected error in the absence of noise  @xcite",
    ". the equations of motion are in the form of coupled first order differential equations for the order parameters with respect to the normalized number of examples , @xmath45 where @xmath46 $ ] , @xmath47 $ ] and @xmath48 $ ] .",
    "the explicit expressions are given in appendix  [ app_eom ] .",
    "these equations can be integrated numerically in order to determine the evolution of the generalization error .",
    "in fig .  [ fig_dynamics ]",
    "we show an example of the ngd dynamics for a realizable and noiseless learning scenario ( @xmath49 , @xmath50 ) . fig .",
    "[ fig_dynamics](a ) shows the evolution of the generalization error while figs .",
    "[ fig_dynamics](b ) and ( c ) show the student - teacher and student - student overlaps respectively ( the indices have been re - ordered _ a posteriori _ ) .",
    "we have used initial conditions corresponding to an input dimension of about @xmath51 , although we expect the dynamical equations to describe mean behavior accurately for much smaller systems as was found to be the case for gd  @xcite . the dashed line in fig .",
    "[ fig_dynamics](a ) shows the effect of reducing the initial conditions for each @xmath52 and @xmath53 by a factor of @xmath54 , which corresponds to an input dimension of about @xmath55 .",
    "the symmetric phase seems to grow logarithmically as @xmath0 increases , as was also found to be the case for gd  @xcite .",
    "as is the case for gd  @xcite the dynamics for this example can be characterized by two major phases of learning , the symmetric phase and asymptotic convergence . following a short initial transient",
    "the order parameters are trapped in a subspace characterized by a lack of differentiation between the activities of different teacher nodes .",
    "after an initial reduction , the generalization error remains at a constant non - zero value and the student - teacher overlaps are virtually indistinguishable .",
    "this symmetric phase is an unstable fixed point of the dynamics and eventually small perturbations due to the random initial conditions lead to escape and convergence towards zero generalization error .",
    "if the teacher is deterministic , as in this example , then the generalization error converges to zero exponentially unless the learning rate is chosen too large ( see the inset to fig .",
    "[ fig_dynamics](a ) ) .",
    "if the teacher s output is corrupted by noise then the learning rate must be annealed in order for the generalization error to decay asymptotically ( we will consider this regime in more detail in section  [ sec_asymptotic ] ) .",
    "the dynamics differs from the gd result in that the symmetric phase is typically less pronounced , although the dashed line in fig .",
    "[ fig_dynamics](a ) shows how the symmetric phase increases in duration as @xmath0 increases ( because of a reduced asymmetry in the initial conditions ) .",
    "the dynamics for gd and ngd are qualitatively different for small learning rates , where fluctuations in the gradient are completely suppressed and the @xmath56 terms in eq .",
    "( [ eq_dynamics ] ) can be neglected . in this limit",
    "the symmetric phase disappears completely for ngd , while it still dominates the learning time for gd .",
    "the symmetric phase is a fluctuation driven phenomena for ngd , rather than a perturbation around the deterministic result .",
    "as described in the next section , this makes analysis of the symmetric phase more difficult than for gd since a small learning rate expansion is no longer meaningful .",
    "a quantitative comparison of gd and ngd is difficult because both algorithms have a free parameter , the learning rate @xmath57 , which can be chosen arbitrarily and which will be critical to performance . in order to make a principled comparison we choose to compare the algorithms when",
    "their learning rates are chosen to be optimal .",
    "this can be achieved by using a variational method which allows us to determine the globally optimal time - dependent learning rate for each algorithm  @xcite .",
    "the resulting learning rate optimizes the total change in generalization error over a fixed time - window and is found by extremitization of the following functional ( see  @xcite for details and results for optimized gd ) : @xmath58 \\ = \\",
    "\\int_{\\alpha_0}^{\\alpha_1 } \\frac{\\rd      \\mbox{\\large$\\epsilon$}_{g}}{\\rd\\alpha } \\",
    "\\rd\\alpha \\ = \\",
    "\\int_{\\alpha_0}^{\\alpha_1 } \\cal{l}[\\eta(\\alpha),\\alpha ] \\",
    "\\rd\\alpha \\ . \\label{eq_opt}\\ ] ] numerical results suggest that the optimal learning rates determined here are close to the critical learning rate within the symmetric phase for both methods , above which the student weight vector norms increase without bound .    in fig .",
    "[ fig_opt ] we compare the performance of optimized gd and optimized ngd for a two - node student learning from a noiseless , isotropic teacher starting from the same initial conditions ( @xmath59 , @xmath50 ) . figs .",
    "[ fig_opt](a ) , ( b ) and ( c ) show results for teachers with @xmath60 , @xmath61 and @xmath62 hidden nodes respectively . in each case the optimal learning rate schedule for ngd is shown by the inset .",
    "it should be noted that although there is a significant temporal variation in the optimized @xmath57 , very similar performance would be achieved by choosing @xmath57 to be fixed at its average value .",
    "we see that ngd significantly outperforms gd in each example . for the over - realizable example shown in fig .",
    "[ fig_opt](a ) the difference is most significant , with ngd displaying no obvious symmetric plateau .",
    "performance of the ngd algorithm seems to reflect the difficulty inherent in the task , while gd displays very similar performance in each case .",
    "it is interesting to compare our results with those found using a locally optimal rule derived by variational arguments  @xcite .",
    "the variational approach requires rather detailed information about the teacher s structure and would be difficult to approximate with a practical algorithm .",
    "however , we find rather similar performance with ngd , especially for the @xmath49 example shown both here and in  @xcite .",
    "the performance bottleneck for gd is due to an inherent symmetry in the student parametrization , while for ngd the task complexity seems to be more important .",
    "also notice that the generalization error is significantly lower during the symmetric plateau for ngd in each case , which is due to reduced weight vector norms ( this is also true for the locally optimal algorithm ) .",
    "it is the growth of these norms which limits increases in the learning rate for gd and it appears that ngd is much more effective in controlling this effect .",
    "another interesting difference between the ngd and gd dynamics is in the short transient prior to the symmetric phase .",
    "the ngd dynamic seems to converge much slower to the symmetric fixed point , as shown in fig .",
    "[ fig_opt ] , reflecting the fact that the strong eigenvalues , related to eigenvectors which lead the dynamic to the symmetric pixed point , are effectively reweighed and suppressed by the ngd rule .",
    "although our equations of motion are sufficient to describe learning for arbitrary system size , the number of order parameters is @xmath63 so that the numerical integration soon becomes rather cumbersome as @xmath1 and @xmath64 grow and analysis becomes difficult . to obtain generic results in terms of system size we therefore exploit symmetries which appear in the dynamics for isotropic tasks and structurally matched student and teacher ( @xmath65 and @xmath66 ) .",
    "this site - symmetric ansatz is only rigorously justified for the special case of symmetric initial conditions and further investigations are required to determine the validity of this approximation in general for large values of @xmath1 ( fixed points other than those considered here have been reported for gd  @xcite and it is unclear whether or not their basins of attraction are negligible ) .",
    "simulations of the gd dynamics for @xmath1 up to @xmath67 , with random initial conditions , show good correspondence with the symmetric system . in this case",
    "we define a four dimensional system via @xmath68 and @xmath69 which can be used to study the dynamics for arbitrary @xmath1 and @xmath70 ( here , @xmath71 denotes the kronecker delta ) . in appendix  [ app_inv ]",
    "we show how the fisher information matrix can be inverted for the reduced dimensionality system and the resulting equations of motion are given in appendix  [ app_eom_red ]",
    ".    analytical study of the symmetric phase for gd is only feasible for small learning rates , since in this case the symmetric fixed point is easily determined and a linear expansion around this fixed point is possible  @xcite .",
    "such an analysis is not feasible for ngd because the dynamics never approaches this fixed point ( the fisher information matrix becomes singular when @xmath72 ) . in any case , a small @xmath57 analysis will be of limited value since it is the fluctuation driven terms in the dynamics ( terms proportional to @xmath56 in eq .",
    "( [ eq_dynamics ] ) ) which set the learning time - scale and determine the optimal and maximal learning rate during the symmetric phase . in order to study the performance of both methods for larger learning rates we will therefore apply a the optimal learning rate framework described in the preceding section  @xcite .",
    "the impact of output noise on the symmetric phase dynamics is not considered explicitly here . for low noise levels",
    "there is no noticeable effect on the length of the symmetric phase , or on the order parameters and generalization error within this phase . for larger noise levels",
    "the symmetric phase increases in length and the student norms increase , resulting in a larger generalization error .",
    "we expect that these are secondary effects and that most essential features of this phase are captured by the noiseless dynamics .",
    "this is not true for later stages of learning , where the inclusion of noise completely alters qualitative features of the dynamics .",
    "these asymptotic effects are considered in section  [ sec_asymptotic ] below .",
    "the optimal learning rate is determined as describe before eq .",
    "( [ eq_opt ] ) in section  [ sec_num ] . in the following examples we use a brief initial learning phase with gd ( until @xmath73 ) as this results in faster entry into the symmetric phase and also leads to quicker convergence of the learning rate optimization .",
    "the effect on learning time will be negligible as @xmath1 becomes very large , but this procedure might be used to improve performance in practice for realistically sized networks .",
    "[ fig_symphase ] summarises our results for transient learning in the absence of noise . in fig .",
    "[ fig_symphase](a ) we compare optimal performance for @xmath74 and @xmath75 , which indicates a significant shortening of the symmetric phase for ngd ( the inset shows the optimal learning rate for ngd ) .",
    "[ fig_symphase](b ) shows the time required for ngd to reach a generalization error of @xmath76 as a function of @xmath1 ( for @xmath75 ) .",
    "the learning time is dominated by the symmetric phase , so that these results provide a scaling law for the length of the symmetric phase in terms of task complexity .",
    "we find that the escape time for ngd scales as @xmath77 , while the inset shows that the learning rate within the symmetric phase approaches a @xmath78 decay .",
    "scaling laws for gd were determined in  @xcite ( also using a site - symmetric ansatz ) , showing a @xmath79 law for escape time and a learning rate scaling of @xmath80 within the symmetric phase . the escape time for the adaptive learning rule studied in  @xcite scales as @xmath81 , which is also worse than ngd .",
    "after the symmetric phase , the order parameters begin convergence towards their asymptotic values ( @xmath82 , @xmath83 ) and for the realizable scenario considered here the generalization error converges towards zero ( recall that we have defined the generalization error to be the expected error in the absence of noise ) . in the absence of output noise this convergence is exponential for a fixed learning rate so long as we do not choose the learning rate too high .",
    "however , in the presence of output noise the learning rate must be annealed in order to achieve zero generalization error asymptotically .",
    "it is known that ngd is asymptotically optimal , in terms of the covariances of the student - teacher weight deviations ( the quadratic estimation error ) , with @xmath17 , saturating the cramer - rao bound and equalling in performance even the best batch methods  @xcite",
    ". however , the quadratic estimation error has no direct interpretation in terms of generalization ability . in fig .",
    "[ fig_noise ] we show results for optimized ngd dynamics with @xmath84 and @xmath75 .",
    "[ fig_noise](a ) shows the generalization error and fig .",
    "[ fig_noise](b ) shows the corresponding optimal learning rate schedules for three noise levels ( @xmath85 , @xmath86 and @xmath87 ) .",
    "the graphs are on log - log scales and show that the optimized learning rates indeed converge to a @xmath88 decay after leaving the symmetric phase .",
    "the generalization error decays at the same rate , but with a prefactor which depends on the noise level .",
    "in order to determine the asymptotic generalization error decay analytically we apply recent results for the annealing dynamics of gd  @xcite .",
    "this allows a comparison between the asymptotic generalization error for ngd and the result for gd . in appendix  [ app_eom_asy ]",
    "we solve the asymptotic dynamics for annealed learning .",
    "as expected , the optimal annealing schedule for ngd is found by setting @xmath17 at late times .",
    "by contrast , although the optimal learning rate for gd is also inversely proportional to @xmath89 , the optimal prefactor depends on @xmath1 and @xmath70 in a non - trivial manner  @xcite . for both optimized gd and ngd",
    "the generalization error decays according to an inverse power law : @xmath90 the exact result for ngd takes a very simple form @xmath91 independent of the value of @xmath70 .",
    "this equals the universal asymptotics for optimal maximum likelihood and bayes estimators which depend only on the learning machine s number of degrees of freedom  @xcite .",
    "ngd is therefore asymptotically optimal in terms of both generalization error and quadratic estimation error .    in fig .",
    "[ fig_asympt ] we compare the prefactor of the generalization error decay for ngd and optimal gd .",
    "[ fig_asympt](a ) shows the result for @xmath75 as a function of @xmath1 , indicating an approximately linear scaling law for gd ( the result above shows that the ngd scaling is linear in @xmath1 ) . in fig .",
    "[ fig_asympt](b ) we compare the decay prefactors for each method as a function of @xmath70 , showing how the difference diverges as @xmath70 is reduced ( the gd results are for large @xmath1 ) .",
    "this can be explained by examining the asymptotic expression for the fisher information matrix , shown in eq .",
    "( [ asy_a ] ) . for large @xmath70",
    "the diagonals of this matrix are @xmath92 and equal ( for large @xmath0 ) while all other terms are at most @xmath93 , so that the fisher information is effectively proportional to the identity matrix in this limit and ngd is asymptotically equivalent to gd . however , for small @xmath70 the diagonals are @xmath94 while the off - diagonals remain finite , so that the fisher information is dominated by off - diagonals in this limit .",
    "we have used a statistical mechanics formalism to solve and analyse the dynamics of natural gradient descent ( ngd ) for learning in a two - layer feedforward neural network . in order to quantify the comparative performance of ngd and gradient descent ( gd ) we compared the optimized performance of each algorithm by determining the optimal learning rate in each case .",
    "we found that ngd provided significant gains in performance over gd in every case examined , both in the transient and asymptotic stages of learning .",
    "a site - symmetric ansatz was applied in order to simplify the dynamical equations for a realizable and isotropic task .",
    "this allowed the dynamics of large networks to be integrated efficiently so that we could determine generic behaviour for large networks .",
    "we found that the learning time scaled as @xmath77 where @xmath1 is the number of hidden nodes , compared to a scaling of @xmath79 for gd  @xcite .",
    "asymptotically ngd is known to provide optimal performance with @xmath17 in terms of the quadratic estimation error .",
    "an asymptotic solution to the annealed learning rate dynamics showed this schedule to also be optimal in terms of generalization error , with the error decay saturating the universal asymptotics for optimal maximum likelihood and bayes estimators  @xcite .",
    "we compared this result with the optimized schedule for gd and plotted the relative performance for various values of task - nonlinearity @xmath70 . the difference in performance",
    "was found to be largest for small values of @xmath70 .",
    "however , in the case of ngd the optimal annealing schedule at late times is known , while for gd it is a complex function of @xmath1 and @xmath70 which will be difficult to estimate in general .",
    "one possible drawback for ngd is the rather complex transient behavior of the optimal learning rate .",
    "for example , in the realizable isotropic case the optimal learning rate scales as @xmath78 in the symmetric phase and @xmath95 asymptotically in the absence of noise .",
    "it is also unclear where learning rate annealing should begin in the presence of output noise .",
    "asymptotically the optimal annealing schedule is known , so the situation is better than for gd , but the problem of setting a good learning rate in the transient remains . in practical applications there will also be an increased cost required in estimating and inverting the fisher information matrix  @xcite . here , we have only considered the idealized situation in which the fisher information matrix is exactly known . in",
    "@xcite we adapt a matrix momentum algorithm due to orr and leen  @xcite in order to obtain efficient averaging and inversion of the fisher information matrix on - line .",
    "this algorithm is shown to provide a good approximation to ngd , although this is at the cost of including an extra parameter .",
    "we would like to thank shun - ichi amari for useful discussions .",
    "this work was supported by the epsrc grant gr / l19232 .",
    "in general the fisher information matrix should be inverted using the block inversion method described in  @xcite . the parameters in eq .",
    "( [ gen_ainv ] ) are then complicated functions of @xmath96 which must be determined iteratively ( see  @xcite for a similar method applied to the hessian matrix for @xmath97 ) . below we consider the simpler situation of a site - symmetric system , in which case the inversion can be carried out in closed form for arbitrary @xmath1 .",
    "asymptotically the result is shown to be further simplified .",
    "exploiting symmetries in the dynamics of realizable isotropic learning ( @xmath65 and @xmath98 ) we consider a reduced dimensionality system with @xmath99 .",
    "we can then write block @xmath100 of the fisher information matrix as ( see eq .",
    "( [ def_a ] ) ) , @xmath101 where we have defined , @xmath102 block @xmath100 in the inverse of @xmath24 is then given by , @xmath103 and symmetries suggests the following general form for @xmath104 , @xmath105 we therefore have to set @xmath106 free parameters in order to fully specify @xmath104 .",
    "this is achieved by substituting eqs .",
    "( [ def_asym ] ) and ( [ def_ainv ] ) into the definition of the inverse , @xmath107 equating like terms leads to a set of @xmath108 equations and we can choose any linearly independent subset of @xmath106 equations in order to determine @xmath109 .",
    "for one particular choice we find , @xmath110 where the non - zero terms in @xmath111 $ ] and @xmath112 $ ] are defined below , @xmath113 \\ , \\\\ & &      m_{1,3 } = m_{2,8 } = m_{4,10 } = ( q - c)(dk+c ) \\ , \\\\ & & m_{1,4 }      = m_{1,6 } = c\\,(q - c ) \\",
    ", \\quad m_{2,4 } = m_{4,6 } = c\\,c \\\\ & &      m_{2,6 } = m_{4,4 } = m_{5,4 } = m_{5,6 } = d(q - c ) \\ , \\\\ & &      m_{3,2 } = m_{6,4 } = m_{8,6 } = m_{11,8 } = a \\ , \\\\ & & m_{3,3 }      = m_{6,6 } = m_{7,4 } = m_{7,6 } = m_{8,4 } = m_{11,10 } = e(q - c ) \\      , \\\\ & & m_{5,1 } = m_{9,5 } = b + dq + ec \\ , \\\\ & & m_{5,2 } =      m_{7,2 } = m_{10,9 } = ( d+e)(q+c(k-1 ) ) \\ , \\\\ & & m_{5,3 } =      d(q - c ) + kb + a \\ , \\quad m_{7,1 } = m_{10,2 } = m_{10,3 } =      eq+dc \\ , \\\\ & & m_{7,3 } = m_{10,10 } = e(q - c ) + c\\,c + ( d+e)kc      \\ , \\\\ & & m_{7,5 } = m_{10,7 } = ( q - c)(d+c+e)+a+c\\,c+k(qe+cd ) \\      , \\\\ & & m_{7,7 } = m_{10,11 } = ( q - c)(k(d+e)+c ) + c\\,ck +      ( d+e)ck^2 \\ , \\\\ & & m_{9,2 } = b \\ , \\quad m_{9,3 } = m_{10,4 }      = m_{10,6 } = ( d+e)c \\ , \\\\ & & m_{9,7 } = kb+a+(d+e)(q+c(k-1 ) )      \\ , \\\\ & & m_{10,8 } = ( q - c)(d+2e)+c\\,c+2kc(d+e ) \\ , \\\\ \\\\ & &      \\beta_1 = -\\frac{c}{a } \\ , \\quad \\beta_4 =      \\frac{b(c+dk)}{a(a+bk ) } - \\frac{d}{a}\\ , \\quad \\beta_5 =      -\\frac{d}{a } \\\\ & & \\beta_7 = \\beta_8 = -\\frac{e}{a } \\ , \\quad      \\beta_{10 } = \\beta_{11 } = \\frac{eb}{a(a+bk ) } \\ .\\end{aligned}\\ ] ]      for realizable rules the asymptotic form for each block of * a * is ( to leading order ) , @xmath114 where , @xmath115 block @xmath100 in the inverse of @xmath24 is then given by , @xmath116 substituting these expressions into eq .",
    "( [ def_inva ] ) and using the orthogonality of the teacher weight vectors ( @xmath98 ) we obtain a matrix equation for @xmath117 $ ] , @xmath118 where , @xmath119 here , we have defined @xmath120 to be a @xmath1-dimensional row vector with a one in the @xmath121th element and zeros everywhere else , while @xmath122 is a row vector of ones . solving for @xmath123",
    "we find , @xmath124 where , @xmath125",
    "using the definition of @xmath126 given in eq .",
    "( [ gen_ainv ] ) we find , @xmath127 here @xmath128 , @xmath129 and @xmath130 where @xmath131 and @xmath132 are activations of the @xmath133th student and @xmath121th teacher hidden nodes respectively and .",
    "the brackets denote averages over inputs which can be written as averages over the multivariate gaussian distribution of student and teacher activations .",
    "the explicit expressions for @xmath134 , @xmath135 , @xmath136 depend exclusively on the weight overlaps ( the covariances of the activation distribution ) and are given in  @xcite .",
    "we substitute the definition of the inverse fisher information for a symmetric system from eq .",
    "( [ def_ainv ] ) into eq .",
    "( [ j_update ] ) to get the weight update equation : @xmath137 \\ , \\ ] ] where @xmath138 and @xmath139 .",
    "differential equations for the order parameters can then be derived by the methods described in  @xcite and for the reduced dimensionality system we find , @xmath140 \\ ,      \\nonumber \\\\ \\frac{\\rd s}{\\rd \\alpha } & = & \\eta\\bigl [      s\\,\\phi_a + t\\,(\\phi_s+(k-1)\\phi_a ) + w_r + z_r s\\bigr ] \\ ,      \\nonumber",
    "\\\\ \\frac{\\rd q}{\\rd \\alpha } & = & \\eta\\bigl [      s\\,\\psi_s + t\\,(\\psi_s+(k-1)\\psi_a ) + 2(v_q + w_q + z_q      q)\\bigr ] \\nonumber \\\\ & & + \\eta^2\\bigl [ s^2\\upsilon_s +      ( 2s\\,t+t^2k)(\\upsilon_s+(k-1)\\upsilon_a)\\bigl ] \\ , \\nonumber\\\\      \\frac{\\rd c}{\\rd \\alpha } & = & \\eta\\bigl [ s\\,\\psi_a +      t\\,(\\psi_s+(k-1)\\psi_a ) + 2(w_q + z_q c)\\bigr ] \\nonumber \\\\ &      & + \\eta^2\\bigl [ s^2\\upsilon_a +      ( 2s\\,t+t^2k)(\\upsilon_s+(k-1)\\upsilon_a)\\bigl ] \\ , \\label{def_eom}\\end{aligned}\\ ] ] where , @xmath141",
    "\\ + \\nonumber \\\\ & & \\hspace{-2 cm } ( r+(k-1)s)\\bigl [      ( \\gamma_2+\\gamma_3+k\\gamma_7)\\psi_s +      ( 2\\gamma_8+\\gamma_9+\\gamma_{10}+k\\gamma_{11})(\\psi_s+(k-1)\\psi_a)\\bigr]\\      , \\\\",
    "z_r & = & ( \\gamma_1+\\gamma_5k)\\psi_s +      ( \\gamma_2+\\gamma_3+k\\gamma_7)(\\psi_s+(k-1)\\psi_a ) \\ , \\end{aligned}\\ ] ] and @xmath142 , @xmath143 and @xmath144 are the same except that @xmath145 and @xmath146 are replaced by @xmath96 and @xmath147 respectively everywhere they appear explicitly . here , @xmath148 $ ] is defined in eq .",
    "( [ def_gam ] ) and we have defined , @xmath149 where @xmath71 with two indices denotes the kronecker delta and brackets denote averages over the inputs .",
    "these averages can again be calculated in closed form  @xcite .",
    "the asymptotic dynamics for gd with an annealed learning rate have recently been solved under the statistical mechanics formalism and the optimal generalization error decay is known in this case  @xcite . here",
    "we extend those results to ngd .    following the notation in  @xcite",
    "we define @xmath150 to be the deviation from the asymptotic fixed point .",
    "if the learning rate decays according to some power law then the linearized equations of motion around this fixed point are given by , @xmath151 where @xmath152 is the jacobian of the equations of motion to first order in @xmath57 while the only non - vanishing second order terms are proportional to the noise variance .",
    "for @xmath75 we find , @xmath153 and the two non - zero entries in @xmath154 are , @xmath155 the solution to eq .",
    "( [ eom_asy ] ) with @xmath156 is , @xmath157 where @xmath158 is a diagonal matrix whose entries @xmath159 are eigenvalues of @xmath160 .",
    "we have defined the diagonal matrix * x * to be , @xmath161      \\ , \\label{u_soln}\\ ] ] where annealing begins when @xmath162 . for natural gradient learning",
    "we find two degenerate eigenvalues @xmath163 , @xmath164 and by substituting eq .",
    "( [ u_soln ] ) into a first order expansion of the generalization error it is straightforward to show @xmath165 to be optimal . in this case",
    "the modes corresponding to @xmath166 do not contribute to the asymptotic generalization error and for all values of @xmath70 we find , @xmath167                                  t.  k.  leen , b.  schottky , and d.  saad , in _ advances in neural information processing systems _ , edited by m.  i.  jordan , m.  j.  kearns and s.  a.  solla ( mit press , cambridge , ma , 1998 ) vol .",
    "10 , p  301 ; phys .",
    "e * 59 * , in press ( 1999 ) ."
  ],
  "abstract_text": [
    "<S> natural gradient descent is a principled method for adapting the parameters of a statistical model on - line using an underlying riemannian parameter space to redefine the direction of steepest descent . </S>",
    "<S> the algorithm is examined via methods of statistical physics which accurately characterize both transient and asymptotic behavior . </S>",
    "<S> a solution of the learning dynamics is obtained for the case of multilayer neural network training in the limit of large input dimension . </S>",
    "<S> we find that natural gradient learning leads to optimal asymptotic performance and outperforms gradient descent in the transient , significantly shortening or even removing plateaus in the transient generalization performance which typically hamper gradient descent training . </S>"
  ]
}