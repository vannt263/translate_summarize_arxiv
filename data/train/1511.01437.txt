{
  "article_text": [
    "let @xmath2 and @xmath0 be two probability measures on a set @xmath5 equipped with some sigma - algebra .",
    "suppose that @xmath0 is absolutely continuous with respect to @xmath2 .",
    "let @xmath6 be the probability density of @xmath0 with respect to @xmath2 .",
    "let @xmath7 be a sequence of @xmath5-valued random variables with law @xmath2 .",
    "let @xmath8 be a measurable function .",
    "suppose that our goal is to evaluate the integral @xmath9 the _ importance sampling estimate _ of this quantity based on the sample @xmath10 is given by @xmath11 sometimes , when the probability density @xmath6 is known only up to a normalizing constant  that is , @xmath12 where @xmath13 is explicit but @xmath14 is hard to calculate  the following alternative estimate is used : @xmath15 it is easy to see that @xmath16 therefore , the expected value of @xmath17 is the quantity @xmath18 that we are trying to estimate .",
    "however , @xmath17 may have large fluctuations .",
    "the main problem in importance sampling is to determine the sample size required for getting a reliable estimate .",
    "a straightforward approach is to compute the variance of @xmath17 . indeed , this is easy to compute : @xmath19 the formula for the variance can be used , at least in theory , to calculate a sample size that is sufficient for guaranteeing any desired degree of accuracy for the importance sampling estimate . in practice , however , this number is often much larger than what is actually required for good performance .",
    "a simple instance of this is provided by the following example .",
    "suppose that @xmath20 , @xmath2 is the exponential distribution with mean @xmath21 and @xmath0 is the exponential distribution with mean @xmath22 . let @xmath23",
    ". then @xmath24 and @xmath25 .",
    "the variance formula shows that for any @xmath1 , the variance of @xmath17 is infinity . on the other hand , it is seen in simulations that a sample of size a few thousand suffices to guarantee that @xmath17 is fairly close to @xmath18 .",
    "we will see later ( example  [ ex1 ] ) how to mathematically understand this convergence .",
    "sometimes the variance formula is estimated using the simulated data @xmath10 .",
    "this estimate is known as the empirical variance .",
    "there is an inherent unreliability in using the empirical variance to determine convergence of importance sampling .",
    "we will elaborate on this in section [ problem ] .",
    "we begin by stating our main theorems .",
    "proofs are collected together in section [ proofs ] .",
    "a literature review on importance sampling is at the end of this introduction .",
    "the main result of this article , stated below , identifies the ` correct ' sample size required for good performance of the importance sampling estimate .",
    "[ impthm ] let @xmath5 , @xmath2 , @xmath0 , @xmath6 , @xmath26 , @xmath18 and @xmath17 be as above .",
    "let @xmath27 be an @xmath5-valued random variable with law @xmath0 .",
    "let @xmath28 be the kullback ",
    "leibler divergence of @xmath2 from @xmath0 , that is , @xmath29 let @xmath30 . if @xmath31 for some @xmath32 , then @xmath33 conversely , let @xmath21 denote the function from @xmath5 into @xmath34 that is identically equal to @xmath21 . if @xmath35 for some @xmath32 , then for any @xmath36 , @xmath37    the theorem says that the sample size @xmath1 required for @xmath17 to be close to @xmath18 with high probability is roughly @xmath3 .",
    "more precisely , it says that if @xmath38 is the typical order of fluctuations of @xmath39 around its expected value , then a sample of size @xmath40 is sufficient and a sample of size @xmath41 is necessary for importance sampling to perform well .",
    "note that theorem [ impthm ] does not just give the sample size required to ensure that @xmath17 is close to @xmath18 in the @xmath42 sense ; the second part of the theorem implies that if we are below the sample size prescribed by theorem [ impthm ] , then for @xmath43 , there is a substantial chance that @xmath17 is actually _ not close _ to @xmath18 .",
    "such lower bounds can not be given merely by moment estimates .",
    "for example , lower bounds on moments like @xmath44 and @xmath45 imply nothing ; @xmath17 may be close to @xmath18 with high probability and yet @xmath44 and @xmath45 may be large .",
    "the second part of theorem [ impthm ] gives an actual lower bound on the sample size required to ensure that @xmath17 is close to @xmath18 with high probability , and the first part shows that this lower bound matches a corresponding upper bound .",
    "it is interesting that the sample size required for small @xmath42 error turns out to be the actual correct sample size for good performance .    as shown later in this section , it is fairly common that @xmath39 is concentrated around its expected value in large systems . in this situation ,",
    "a sample of size roughly @xmath3 is both necessary and sufficient .",
    "it is important to note that the required sample size prescribed by theorem [ impthm ] is derived for the worst - case scenario .",
    "it is possible that for some @xmath26 , the convergence of @xmath17 to @xmath18 happens much faster .",
    "a trivial example is the function @xmath26 that is identically equal to zero , where convergence happens instantaneously . in this context",
    "it should also be noted that theorem [ impthm ] actually says what a worst - case function is , in an asymptotic sense : it is the function that is identically equal to  @xmath21 .",
    "the next theorem gives the analogous result for the estimate @xmath46 .",
    "the conclusion is essentially the same .",
    "[ selfimpthm ] let all notation be as in theorem [ impthm ] and let @xmath46 be the estimate defined in .",
    "suppose that @xmath47 for some @xmath32 .",
    "let @xmath48 then @xmath49 conversely , suppose that @xmath35 for some @xmath32 .",
    "let @xmath50 denote the function from @xmath5 into @xmath34 that is equal to @xmath21 when @xmath51 and @xmath52 otherwise .",
    "then @xmath53 and @xmath54 .",
    "sometimes , importance sampling is used to estimate the probabilities of rare events under the target measure @xmath0 . typically , the quantity of interest is @xmath55 , where @xmath56 is a rare event under @xmath0 but is not a rare event under @xmath2 .",
    "the method of estimation is the same as before , that is , let @xmath57 be the function that is @xmath21 if @xmath58 and @xmath52 otherwise , and let @xmath59 be the importance sampling estimate of @xmath55 .",
    "the difference with the previous setting is that when estimating @xmath55 , we are not satisfied if @xmath60 is small because @xmath55 itself is a small number .",
    "rather , it is satisfactory if the ratio @xmath61 is close to @xmath21 .",
    "it turns out that the sample size that is necessary and sufficient for this purpose is not @xmath3 , but @xmath62 , where @xmath63 is the probability measure @xmath0 conditioned on the event @xmath56 .",
    "this is quantified by the following theorem .",
    "[ rarethm ] let all notation be as in theorem [ impthm ] .",
    "let @xmath56 be any event such that @xmath64 and let @xmath65 be the indicator function of @xmath56 , defined above .",
    "let @xmath63 be the measure @xmath0 conditioned on the event @xmath56 , that is , for any event @xmath66 , @xmath67 let @xmath68 be the probability density function of @xmath63 with respect to @xmath2 .",
    "let @xmath69 .",
    "if @xmath70 for some @xmath32 , then @xmath71 conversely , suppose that @xmath72 for some @xmath32 .",
    "then for any @xmath36 , @xmath73    let us now investigate the numerical performance of the error bound given in theorem [ impthm ] in a few simple examples .",
    "[ ex1 ] as a first example , consider the one discussed earlier in the introduction : @xmath20 , @xmath74 the exponential distribution with mean @xmath21 , @xmath75 the exponential distribution with mean @xmath22 , and @xmath76 . we have observed earlier that for this problem , the variance of the importance sampling estimate is infinity . on the other hand , @xmath77 in this example .",
    "the mean absolute deviation @xmath44 , however , is finite and decreasing as @xmath1 grows .",
    "it is easy to estimate the mean absolute deviation using simulations .",
    "figure  [ fig1 ] compares the estimated true value of the mean absolute deviation with the upper bound given in theorem  [ impthm ] .",
    "the figure shows that the upper bound performs quite well for this problem .",
    "interestingly , even for this simple problem , a relatively large sample size ( approximately @xmath78 ) is required to get reasonable accuracy ( within @xmath79 of the true value ) .",
    "[ ex2 ] let @xmath80 .",
    "let @xmath2 be the binomial@xmath81 distribution and @xmath0 be the binomial@xmath82 distribution .",
    "here again @xmath83 can be computed exactly ; its value is approximately @xmath84 .",
    "let @xmath8 be the function that is identically equal to @xmath21 .",
    "figure [ fig2 ] shows the estimated true value of @xmath44 and the theoretical upper bound given by theorem [ impthm ] , plotted against the logarithm of the sample size . from the plot we see that the importance sampling estimate converges somewhere around a sample size of @xmath85 . as mentioned above , the value of @xmath83 in this problem is approximately @xmath86 , and the standard deviation of @xmath39 is approximately @xmath21 .",
    "this is why the mean absolute error does not exhibit a sharp ` cut - off ' .",
    "the cut - off phenomenon for importance sampling requires the system to be much larger .",
    "the next example demonstrates a situation where this happens .",
    "[ hypothetical ] let us now consider a hypothetical example for demonstrating the concentration of the required sample size .",
    "let @xmath87 .",
    "let @xmath2 be the binomial@xmath88 distribution and let @xmath0 be the binomial@xmath89 distribution",
    ". then @xmath90 let @xmath27 be a random variable with distribution @xmath0 .",
    "then @xmath27 is approximately normally distributed with mean @xmath91 and standard deviation @xmath92 .",
    "consequently , @xmath39 is approximately normally distributed with mean @xmath93 and standard deviation @xmath94 .",
    "a simple computation using these values in theorem [ impthm ] shows , for example , that if @xmath95 , then for any @xmath96 $ ] , @xmath97 on the other hand , if @xmath98 and @xmath21 denotes the function that is identically equal to @xmath21 , then theorem [ impthm ] gives @xmath99 this shows that a sample of size @xmath100 is inadequate in this example , whereas a sample of size @xmath101 suffices to guarantee good performance .",
    "these sample sizes are , of course , enormous . but this computation serves to demonstrate how the sample size required for good performance of importance sampling estimates may concentrate in the logarithmic scale .",
    "we had to choose this extreme example because the concentration phenomenon for the required sample size seems to manifest itself only at these severely large scales . incidentally",
    ", the variance formula   shows that in this example , @xmath102 for this variance to be small , one needs @xmath103 , which is far larger than the actual sample size required ( that was computed above ) to guarantee that @xmath104 is close to @xmath105 with high probability .",
    "let @xmath5 be the set of all monotone paths from @xmath106 to @xmath107 in the two dimensional lattice . here",
    ", paths are only allowed to go up and to the right .",
    "the target measure is the uniform distribution on all such paths .",
    "clearly , @xmath108 .",
    "the sampling measure @xmath2 in this example constructs a random path @xmath109 as follows ( this is known as sequential importance sampling ) : choose one of the two directions ` up ' or ` right ' with probability @xmath110 until the walk hits the top or right side of the @xmath111 ` box ' , when the remainder of the walk is forced . if @xmath112 is the first time the path hits the top or right side then @xmath113 .",
    "both the uniform distribution @xmath114 and @xmath115 have the property that , conditional on @xmath116 , the paths are uniformly distributed .",
    "thus distributional questions are determined by the distribution of @xmath112 .",
    "the following proposition from @xcite shows that under the sampling distribution @xmath2 , @xmath112 is usually about @xmath117 from the maximum possible @xmath118 , but under the uniform distribution @xmath0 , @xmath112 is usually about @xmath119 away from @xmath118 .    with the notation above ,    1 .   under the importance sampling distribution @xmath2 , @xmath120 2",
    "for @xmath1 large and fixed positive @xmath121 , @xmath122 3 .   under the uniform distribution @xmath0 ,",
    "@xmath123 further @xmath124 .",
    "4 .   for @xmath1 large and any fixed @xmath125 , @xmath126    the quantity @xmath83 of theorem [ impthm ]",
    "is determined from @xmath127 as @xmath128 thus , @xmath129 , and moreover , @xmath130 has fluctuations of order @xmath21 around its mean .",
    "thus , a sample size of order @xmath131 is necessary and sufficient for accuracy of importance sampling in this example .",
    "the sufficiency was already observed using variance computations in @xcite ; the necessity is a new result .",
    "similar computations can be carried out for paths allowed to go left or right or up ( staying self avoiding ) using results of @xcite .    [ thm3ex ] as an example for theorem [ rarethm ] , fix @xmath132 and @xmath133 and let @xmath2 be the binomial@xmath134 distribution .",
    "let @xmath135 .",
    "take @xmath0 to be the binomial@xmath136 distribution .",
    "let @xmath137 be the probability of @xmath56 under @xmath0 .",
    "estimating @xmath138 by simple sampling from @xmath0 would be a crazy task ; for example when @xmath139 and @xmath140 , @xmath141 . a standard importance sampling approach ( @xcite )",
    "is to sample @xmath142 from @xmath2 and use @xmath143 theorem [ rarethm ] shows that this will be accurate in ratio for @xmath1 of order @xmath144 .",
    "the calculations below show that @xmath145 . when @xmath139 and @xmath140 , @xmath146 ( still an impossible sample size ) .",
    "the proof below allows @xmath2 to be binomial@xmath147 and shows that @xmath148 minimizes @xmath144 , agreeing with the variance minimization in @xcite .",
    "[ rareprop ] fix @xmath132 and @xmath133 with @xmath149 integral .",
    "let @xmath2 be the @xmath150 distribution , @xmath0 be the @xmath151 distribution and @xmath152 .",
    "then @xmath153 of theorem  [ rarethm ] satisfies , as @xmath132 tends to infinity , @xmath154    the proof of this proposition is given in section [ proofs ] . incidentally , truncating on @xmath56 changes the sample size , when compared to usual importance sampling for @xmath2 and @xmath0 .",
    "for example , when @xmath2 is binomial@xmath150 and @xmath0 is binomial@xmath136 , the @xmath83 of theorem [ impthm ] can be easily computed as @xmath155 .",
    "when @xmath139 and @xmath140 , @xmath156 .",
    ".2 in * review of the literature .",
    "* our interest in this topic started with a question from our colleague don knuth in @xcite .",
    "he used sequential importance sampling to generate random self - avoiding paths starting at @xmath106 and ending at @xmath157 in a two dimensional @xmath158 grid .",
    "for @xmath159 he calculated the number of paths ( about @xmath160 ) , the average path length ( @xmath161 ) and the proportion of paths passing through @xmath162 ( @xmath163 ) .",
    "he noticed huge fluctuations along the way and wanted to know about the accuracy of his estimates . in the follow up",
    "work @xcite , exact computation showed surprising accuracy for his example . @xcite and",
    "@xcite studied toy versions of knuth s problem where exact calculations can be done ; they confirm the extreme variability and make the accuracy observed mysterious .    in our work ,",
    "the choice of the proposal measure @xmath2 is considered fixed .",
    "a good deal of the art of successful implementation of importance sampling consists in a careful choice of @xmath2 , adapted to the problem under study .",
    "this is often done to minimize the variance of the resulting estimate .",
    "our work , especially the main result of section [ problem ] , suggests that the variance is a poor measure of accuracy for these long tailed problems . thus , there is work to be done , exploring ways of adapting the many good ideas below , based on the variance , to minimizing the kullback ",
    "leibler divergence .",
    "any book on simulation will treat importance sampling .",
    "we recommend  @xcite , @xcite , @xcite and @xcite . to begin our review of the research literature , a classical choice of the sampling measure @xmath2 for estimating @xmath164 is to take @xmath165 proportional to @xmath166 ( @xcite ) .",
    "@xcite suggests using a mixture of measures for @xmath2 with one component proportional to @xmath167 near its maximum .",
    "this is closely related to the widely used method of umbrella sampling ( @xcite ; nicely developed in @xcite ) .",
    "@xcite combine hesterberg s idea with control variates to give an attractive , practical approach .",
    "in later work , @xcite suggest an adaptive version , attempting to improve the proposed @xmath2 using previous sampling .",
    "this is based on the empirical variance which means that our laments in section [ problem ] apply .",
    "the idea of using @xmath42 distance to measure performance of importance sampling has appeared in a few prior instances .",
    "two notable examples are @xcite and @xcite , where @xmath42 error was used to compare the monte carlo and quasi - monte carlo approaches to estimating singular integrands via importance sampling .",
    "importance sampling is often used to do rare event simulation .",
    "then , it is natural to tilt the sampling distribution @xmath2 towards to region of interest .",
    "@xcite gives an asymptotically principled approach to doing this , which has given rise to much follow - up work , some of it quite deep mathematically .",
    "a unifying account of a variety of importance sampling algorithms for simulating the maxima of gaussian fields appears in @xcite . a host of novel ways of building importance sampling estimates for problems such as estimating the size of the union of a collection of sets when the size of each is known is in @xcite .",
    "the work of paul dupuis with many coauthors is notable here . @xcite and @xcite are representative papers with useful pointers to an extensive literature . @xcite give a textbook account of this part of the subject .",
    "an important part of the literature adapts importance sampling from the case of independent proposals considered here to use with a markov chain generating proposals . @xcite",
    "give a clear development as do the textbook accounts of @xcite or @xcite .",
    "an important class of techniques for building proposal distributions is known as sequential importance sampling .",
    "an early appearance of this to sampling self - avoiding paths occurs in @xcite . for contingency table examples",
    "see @xcite . for degree sequences of graphs ,",
    "see @xcite . for time",
    "series and a general review see the textbook by @xcite or the survey of @xcite .",
    "one large related topic is the connection between importance sampling and particle filters .",
    "roughly , when building a proposal @xmath2 sequentially , one begins with a number @xmath132 of starts . as the proposals are independently built up",
    ", some weights may be much larger than others .",
    "one can generate @xmath132 new proposals from the present ones ( say with probability proportional to weights ) .",
    "this will replicate some proposals and kill of those with smaller weights .",
    "this resampling can be repeated several times .",
    "the final weighted samples are used , in the usual way , to form importance sampling estimates .",
    "this large enterprise can be surveyed in the textbooks of @xcite and @xcite .",
    "work of @xcite harnesses martingale central limit theorems to get the limiting distribution of these importance sampling methods in a variety of complex stochastic models .",
    "the web page of arnaud doucet is extremely useful .",
    "a very clear recent paper is : @xcite .    besides the broad classifications outlined above",
    ", importance sampling has a variety of other applications that are harder to categorize .",
    "a recent example is the paper by @xcite that suggests the use of importance sampling for generating from bayesian posterior distributions . in this context",
    ", an interesting note is that simulating from a bayesian posterior by rejection sampling was investigated by @xcite , who found a connection with the kullback ",
    "leibler divergence that bears some similarities with the results of this paper .",
    "the theory developed in section [ theory ] , while theoretically interesting , is possibly not very useful from a practical point of view .",
    "determining @xmath4 requires in - depth knowledge of not only the measure @xmath2 , but also the usually much more complicated measure @xmath0 .",
    "it is precisely the lack of understanding about @xmath0 that motivates importance sampling , so it seems pointless to ask a practitioner to compute the required sample size by using properties of @xmath0 .    to determine",
    "whether the importance sampling estimate has converged , a common practice is to estimate @xmath45 by estimating the variance formula   using the data from @xmath2 .",
    "explicitly , this estimate  is @xmath168 the importance sampling estimate is declared to have converged if for some @xmath1 , @xmath169 turns out to be smaller than some pre - specified tolerance threshold @xmath170 ( see @xcite ) .",
    "the following theorem shows that the traditional approach of testing for convergence using the estimated variance of the importance sampling estimate has a flaw : for any given tolerance level @xmath170 , there is high probability that the test declares convergence at or before a sample size that depends only on @xmath170 and not on the two distributions @xmath2 and @xmath0 .",
    "this is absurd , since convergence may take arbitrarily long , depending on the problem ( for instance , recall example  [ hypothetical ] ) .",
    "[ flawthm ] consider the setting of theorem [ impthm ] , and let @xmath169 be as above .",
    "take any @xmath8 such that @xmath171 .",
    "then , given any @xmath172 , there exists @xmath173 such that @xmath174 .",
    "this result holds for all @xmath2 and @xmath0 .",
    "although the upper bound on @xmath1 is very large  for example , for @xmath175 the upper bound is roughly @xmath176  theorem [ flawthm ] gives a conceptual proof that the method of estimated variance for testing convergence of importance sampling is fundamentally problematic . as the measures @xmath2 and @xmath0 get more and more singular with respect to each other ( which often happens as system size gets larger ) , importance sampling should take longer to converge",
    "a test that does not respect this feature can not be a plausible test for convergence .",
    "incidentally , it is open whether the upper bound on @xmath1 in theorem [ flawthm ] can be improved to something more reasonable .",
    "the ineffectiveness of the variance diagnostic is not hard to demonstrate in examples .",
    "two such examples are given below .",
    "in the large @xmath132 binomial example [ hypothetical ] , @xmath169 stays extremely close to zero for any realistic value of @xmath1 because @xmath177 is very close to zero with high probability .",
    "but here we know that the actual convergence takes place at an astronomical sample size .",
    "[ binex1 ] consider @xmath74 binomial@xmath81 and @xmath75 binomial@xmath178 .",
    "let @xmath26 be the function that is identically equal to @xmath21 .",
    "figure [ fig3 ] shows the plot of the estimated standard deviation @xmath179 against @xmath1 , as @xmath1 ranges from @xmath21 to @xmath180 .",
    "the estimated standard deviation remains fairly small throughout .",
    "however , since we know the actual value of @xmath18 in this case ( which is @xmath21 ) , it is easy to compute the actual error @xmath181 and check that the variance diagnostic is giving a false conclusion .",
    "there are results in the literature that claim to show that the variance estimation method gives a valid criterion for the convergence of importance sampling .",
    "however , what these results actually show is that if @xmath1 is so large that the importance sampling estimates are accurate , then @xmath169 is small . in other words ,",
    "the smallness of @xmath169 is a _ necessary _ condition for convergence of importance sampling , but not a _ sufficient _ condition . for a diagnostic criterion to be useful",
    ", it needs to be a sufficient condition for convergence , not a necessary condition .",
    "we suggest the following alternative diagnostic . as usual",
    ", let @xmath2 be the sampling measure , @xmath0 be the target measure , and @xmath182 .",
    "let @xmath183 be i.i.d .",
    "random variables with law @xmath2 .",
    "define @xmath184 , where @xmath185 the size of @xmath186 is our criterion for diagnosing convergence of importance sampling .",
    "the general prescription is that if for some value of @xmath1 the quantity @xmath186 is smaller than some pre - specified threshold ( say , @xmath187 ) , declare that @xmath1 is large enough for importance sampling to work .",
    "note that the random variable @xmath188 always lies between @xmath52 and @xmath21 , and therefore @xmath189 $ ] .",
    "moreover , given any @xmath1 , it is possible to estimate @xmath186 up to any desired degree of accuracy by repeatedly simulating @xmath188 and taking an average , since @xmath190 and @xmath188 always lies between @xmath52 and @xmath21 .",
    "lastly , note that for estimating @xmath186 using simulations in the above manner , it suffices to know the density @xmath6 up to an unspecified normalizing constant .",
    "why should one expect the smallness of the quantity @xmath186 to be a valid diagnostic criterion for convergence of importance sampling ?",
    "first , let us hasten to add the caveat that one can produce examples where it does not work .",
    "one such example is the following : take a large number @xmath132 .",
    "let @xmath2 be the uniform distribution on @xmath191 .",
    "let @xmath0 be the distribution that puts mass @xmath192 on the points @xmath193 , and mass @xmath194 on the point @xmath132 .",
    "then @xmath195 for @xmath196 and @xmath197 . under the sampling measure @xmath2 , @xmath198 with probability @xmath199 .",
    "therefore when @xmath200 , the quantity @xmath186 will be small ; but convergence of importance sampling will not happen until @xmath201 .    in spite of the above counterexample , we expect that @xmath186 is a valid diagnostic for many natural examples .",
    "this is made precise to a certain extent in the setting of gibbs measures by theorem  [ newthm ] in the next section . a general heuristic argument for the effectiveness of the @xmath186 diagnostic , on which the proof of theorem [ newthm ] is based ,",
    "can be described as follows .",
    "suppose that @xmath202 is concentrated under @xmath0 , so that theorem [ impthm ] applies , and the sample size required for convergence of importance sampling is roughly @xmath203 , where @xmath204 .",
    "take any @xmath1 below this threshold .",
    "let @xmath205 . since @xmath206 are i.i.d .",
    "random variables , it is easy to see that under mild conditions , @xmath207 with high probability , where @xmath208 solves @xmath209 next , let @xmath210 .",
    "since @xmath207 , therefore @xmath211 therefore @xmath212 now , @xmath213 .",
    "thus , @xmath214 is a large deviation probability .",
    "therefore under mild conditions , one may expect that @xmath215 plugging this into , we get @xmath216 using the equation to evaluate the last term , we get @xmath217 , and therefore @xmath218 by markov s inequality . since @xmath207 , this shows that @xmath219 the above heuristic shows that if @xmath220 and some appropriate conditions hold , then @xmath221 .",
    "in other words , smallness of @xmath186 should be a sufficient condition for convergence of importance sampling .",
    "this sketch can be made rigorous under certain circumstances .",
    "an instance of this is illustrated by theorem [ newthm ] in the next section .",
    "the smallness of @xmath186 is also a necessary condition for convergence of importance sampling . unlike sufficiency",
    ", the necessity can be rigorously proved in full generality .",
    "[ qthm ] let all notation be as in theorem [ impthm ] .",
    "let @xmath186 be defined as above .",
    "let @xmath222 .",
    "then @xmath223 where @xmath14 is a universal constant .",
    "as mentioned above , this theorem shows that the smallness of @xmath186 is a necessary condition for convergence of importance sampling ( recalling that by theorem [ impthm ] , convergence in @xmath42 is equivalent to actual good performance ) ; if @xmath224 is small , then @xmath186 is forced to be small .",
    "the performance of @xmath186 in example [ binex1 ] is depicted in figure [ fig4 ] .",
    "the figure plots the estimated standard deviation @xmath179 and the statistic @xmath225 , against @xmath226 as @xmath1 ranges from @xmath21 to @xmath180 . as in figure",
    "[ fig3 ] , we see that the estimated standard error is generally quite misleading and unstable . on the other hand",
    "the statistic @xmath225 detects the non - convergence in small samples and is very stable .",
    "the estimation of @xmath225 was based on a sample of size @xmath227 for each @xmath1 .",
    "another illustration is given in figure [ fig5 ] , which investigates the performance of @xmath186 for knuth s self - avoiding walks on a @xmath228 grid , that was described in the literature review part of section  [ theory ] .",
    "the plot shows the behavior of @xmath186 as @xmath1 ranges from @xmath21 to @xmath229 .",
    "we see that @xmath186 is not too small ( greater than @xmath230 ) when @xmath231 , but starts getting appreciably small around @xmath232 . when @xmath233 , @xmath186 is minuscule .",
    "the random quantity @xmath188 is closely related to some existing diagnostics in the literature on sequential monte carlo ( particle filters ) .",
    "it has the same form as the @xmath234-ess statistic proposed by @xcite in the context of sequential monte carlo .",
    "here ess stands for ` effective sample size ' , a familiar concept in the sequential monte carlo literature .",
    "there is a substantial body of work on the efficacy of the effective sample size as a diagnostic tool , possibly beginning with @xcite and @xcite .",
    "see @xcite for some latest results .",
    "@xcite established similar properties for the @xmath234-ess",
    ". it would be interesting to see whether analogs of these results can be proved for the @xmath188 and @xmath186 statistics proposed in this section .",
    "as in section [ theory ] , let @xmath5 be a set equipped with some sigma - algebra .",
    "let @xmath235 be a finite measure on @xmath5 that we shall call the ` base measure ' .",
    "let @xmath236 be a measurable function , called the hamiltonian , and let @xmath237 be a parameter , called the inverse temperature .",
    "the exponential family distribution ( gibbs measure ) @xmath238 on @xmath5 defined by the sufficient statistic ( hamiltonian ) @xmath239 at a parameter value ( inverse temperature ) @xmath240 is the probability measure on @xmath5 that has probability density @xmath241 with respect to the base measure @xmath235 , where @xmath242 is the normalizing constant , which is assumed to be finite .",
    "let @xmath243 in physics parlance , the quantity @xmath244 is known as the free energy of the system at inverse temperature  @xmath240 .    often , the normalizing constant @xmath245 is hard to calculate theoretically .",
    "importance sampling is used to estimate @xmath245 in a variety of ways .",
    "see @xcite for a useful review .",
    "@xcite show the breadth of this problem .",
    "one simple technique : let @xmath246 be an inverse temperature at which we know how to generate a sample from the gibbs measure .",
    "for example @xmath247 is often a good choice , because @xmath248 is nothing but the base measure @xmath235 normalized to have total mass one .",
    "the goal is to estimate @xmath245 using a sample from @xmath249 .",
    "let @xmath10 be an i.i.d .",
    "sample of size @xmath1 from @xmath249 .",
    "the importance sampling estimate of @xmath245 based on this sample is the following : @xmath250 it is easy to see that @xmath251 .",
    "the question is , how large does @xmath1 need to be , so that the ratio @xmath252 is close to @xmath21 with high probability ?",
    "the following theorem shows that under favorable conditions , a sample of size approximately @xmath253 is necessary and sufficient .",
    "this theorem is a result for finite systems .",
    "a more general version of this result that applies in the thermodynamic limit is given later in this section .",
    "[ gibbsthm ] let all notation be as above .",
    "suppose that the hamiltonian @xmath239 satisfies the condition that for some @xmath254 , @xmath255 then @xmath256 is infinitely differentiable at @xmath240 .",
    "let @xmath257 and @xmath258 if @xmath259 for some @xmath260 , then @xmath261 conversely , if @xmath262 for some @xmath260 , then for any @xmath36 , @xmath263    it is not difficult to verify by direct calculation that @xmath264 is always nonnegative .",
    "this implies , in particular , that @xmath256 is convex . as a consequence of this feature , @xmath83 and @xmath265",
    "are also nonnegative .    in standard examples , @xmath256 , @xmath266 and @xmath264",
    "are all of the same order of magnitude , and the magnitudes are large . therefore @xmath83 is large and @xmath267 , which implies that the required sample size is concentrated in the logarithmic scale at @xmath268 .",
    "the situation is illustrated through the following examples .",
    "[ gibbs1 ] take some @xmath269 and let @xmath270 .",
    "let @xmath235 be the counting measure on this set , and for @xmath271 , let @xmath272 the @xmath238 is nothing but the joint law of @xmath132 i.i.d .",
    "random variables that take value @xmath21 with probability @xmath273 and @xmath274 with probability @xmath275 .",
    "a simple computation gives @xmath276 .",
    "therefore @xmath277 thus , for any given @xmath246 and @xmath240 , @xmath278 and @xmath279 therefore , @xmath83 is typically of order @xmath132 and @xmath265 is typically of order @xmath280 .",
    "[ gibbs2 ] as in the previous example , let @xmath270 and let @xmath235 be the counting measure on this set .",
    "for @xmath281 , let @xmath282 where @xmath283 , @xmath284 , and @xmath285 in the first sum stands for @xmath286 .",
    "this is the hamiltonian for the one dimensional ising model for a system of @xmath132 spins with periodic boundary .",
    "the parameters @xmath287 and @xmath288 are traditionally known as the coupling constant and the strength of the external magnetic field .",
    "the partition function of this model is easily computed by the transfer matrix method ( see @xcite ) : @xmath289 , where @xmath290 is the @xmath291 matrix @xmath292\\,.\\ ] ] in other words , if @xmath293 and @xmath294 are the two eigenvalues of this matrix ( arranged such that @xmath295 ) , then @xmath296 consequently , @xmath297 it is not hard to verify that @xmath298 and @xmath299 using these formulas it is easy to write down explicit formulas for @xmath83 and @xmath265 for any given @xmath240 and @xmath246 , and compute @xmath300 and @xmath301 such that as @xmath302 , @xmath303 and @xmath304 .",
    "examples [ gibbs1 ] and [ gibbs2 ] demonstrate how theorem [ gibbsthm ] can be applied to calculate the sample size required for importance sampling in statistical mechanical models",
    ". however , these examples required exact computations in finite systems , which is rarely possible in complex models .",
    "our next theorem deals with a generic sequence of models that converge to a limit .",
    "exact computations are assumed to be possible only in the limit .",
    "let @xmath305 be a sequence of spaces equipped with sigma - algebras and finite measures @xmath306 .",
    "for each @xmath132 let @xmath307 be a measurable function , and for each @xmath237 let @xmath308 be the probability measure that has probability density proportional to @xmath309 with respect to  @xmath310 .",
    "let @xmath311 be the normalizing constant of @xmath312 , and assume that these quantities are finite .",
    "let @xmath313 let @xmath314 be a sequence of numbers tending to infinity , and let @xmath315 whenever the limit exists and is finite . for a suitable choice of @xmath316 depending on the situation ,",
    "the function @xmath317 is sometimes referred to as the thermodynamic limit ( or the thermodynamic free energy ) of the sequence of systems described above .",
    "the thermodynamic limit is said to have a @xmath318 order phase transition at an inverse temperature @xmath240 if the first @xmath319 derivatives of @xmath320 are continuous at @xmath240 but the @xmath318 derivative is discontinuous at @xmath240 .",
    "fix two inverse temperatures @xmath246 and @xmath240 such that @xmath321 .",
    "the goal is to estimate @xmath322 using importance sampling with a sample of size @xmath1 from the gibbs measure @xmath323 , and determine how fast @xmath1 needs to grow with @xmath132 such that the ratio of this estimate and the true value tends to one as @xmath302 .",
    "recall that the importance sampling estimate of @xmath324 is @xmath325 where @xmath10 are i.i.d .",
    "draws from @xmath326 .",
    "the following theorem identifies the sample size required for good performance of the above estimate as long as the system does not exhibit a first - order phase transition at @xmath240 in the thermodynamic limit .",
    "[ statmechthm ] let all notation be as above .",
    "let @xmath314 be a sequence of constants such that the thermodynamic free energy @xmath320 exists and is differentiable in a neighborhood of @xmath240 , and exists at @xmath246 .",
    "assume that the derivative @xmath327 is continuous at @xmath240 , and that there exists a finite constant @xmath14 such that for all @xmath132 and all @xmath328 , @xmath329 .",
    "suppose that the sample size @xmath330 grows with @xmath132 in such a way that @xmath331 converges to a limit @xmath332 $ ] , and let @xmath333 then the following conclusions hold :    1 .   if @xmath334 , then @xmath335 in probability as @xmath302 .",
    "2 .   if @xmath336 , then @xmath337 in probability as @xmath302 .",
    "3 .   if @xmath338 and @xmath327 is not constant in any neighborhood of @xmath240 , then @xmath339 in probability as @xmath302 . note that this is a weaker version of the conclusion of part .",
    "theorem [ statmechthm ] has potentially much wider applicability than theorem [ gibbsthm ] , since thermodynamic limits are known in many important statistical mechanical systems .",
    "classical examples from statistical physics include the 2d ising model , the six and eight vertex models , and many others ( see @xcite and @xcite ) .",
    "recently , a variety of exponential random graph models have been explicitly ` solved ' ( see @xcite , @xcite , @xcite and @xcite ) .",
    "similar progress has been made for non - uniform distributions on permutations ( see @xcite , @xcite and @xcite ) .",
    "all of these models provide examples for our theory .",
    "the main strength of theorem [ statmechthm ] is also its main weakness : while it gives a definitive answer for exactly solvable models , the theorem is not useful for systems that are not exactly solvable in a thermodynamic limit . as discussed in section [ problem ] , what a practitioner really wants is a diagnostic test that will confirm whether importance sampling has converged .",
    "interestingly , it turns out that the use of the alternative diagnostic test proposed in section [ problem ] can be partially justified in the setting of theorem [ statmechthm ] , under one additional assumption .",
    "the extra assumption is that the system has no first - order phase transition at any point between @xmath246 and @xmath240 , strengthening the assumption made in theorem [ statmechthm ] that there is no first - order phase transition at @xmath240 .",
    "take @xmath246 and @xmath240 such that @xmath340 .",
    "recall the quantities @xmath188 and @xmath186 defined in section [ problem ] .",
    "since there are two parameters @xmath1 and @xmath132 involved here , we will write @xmath341 and @xmath342 instead of @xmath186 and @xmath188 . then note that @xmath343 and @xmath344 .",
    "( note that @xmath341 has nothing to do with @xmath345 . )",
    "the following theorem shows that if @xmath1 is large enough ( depending on @xmath132 ) for the importance sampling to work , then @xmath341 is exponentially small in @xmath316 .",
    "otherwise , it is not exponentially small .",
    "[ newthm ] let all notation and assumptions be as in theorem [ statmechthm ] .",
    "additionally , assume that there is an open interval @xmath346 $ ] such that the thermodynamic free energy @xmath320 is well - defined and continuously differentiable in @xmath347 , and that @xmath327 is not constant in any nonempty open subinterval of @xmath347 .",
    "then :    1 .   if @xmath348 , then @xmath349 moreover , @xmath350 in probability as @xmath302 .",
    "if @xmath351 , then @xmath352 moreover , there exists @xmath353 such that @xmath354 as @xmath302 .",
    "in particular , if @xmath1 grows with @xmath132 so fast that @xmath341 decays to zero like a negative power of @xmath316 , then the estimated free energy @xmath355 converges to the correct limit @xmath317 in probability .",
    "incidentally , the binomial distribution , as well as more complicated systems like knuth s self - avoiding paths , can be put into the framework of theorem [ newthm ] by an appropriate choice of the hamiltonian and the inverse temperatures @xmath246 and @xmath240 , so that the system at inverse temperature @xmath246 gives the sampling distribution and the system at inverse temperature @xmath240 gives the target distribution .",
    "the main theoretical question would be to prove the absence of a phase transition between @xmath246 and @xmath240 .",
    "suppose that @xmath356 and let @xmath357 .",
    "let @xmath358 if @xmath359 and @xmath52 otherwise .",
    "then @xmath360 first , note that by the cauchy ",
    "schwarz inequality , @xmath361 finally , note that @xmath362 combining the upper bounds obtained above , we get the first inequality in the statement of the theorem .",
    "suppose that @xmath356 and let @xmath368 .",
    "let @xmath369 then by theorem [ impthm ] , for any @xmath370 , @xmath371 and @xmath372 now , if @xmath373 and @xmath374 , then @xmath375 taking @xmath376 and @xmath377 completes the proof of the first inequality in the statement of the theorem .",
    "note that if @xmath170 turns out to be bigger than @xmath21 , then the bound is true anyway .",
    "let @xmath383 suppose that @xmath384 and let @xmath368 .",
    "applying theorem [ impthm ] with @xmath6 replaced by @xmath385 , this gives @xmath386 which is the first assertion of the theorem .",
    "the second claim follows similarly .",
    "let @xmath387 so that @xmath388 to explore the choice of sampling distribution let @xmath2 be the binomial@xmath147 distribution for fixed @xmath389 .",
    "then @xmath390 an identity of de moivre ( see @xcite ) shows that for any @xmath125 , @xmath391 , @xmath392 thus , since @xmath149 is integral , @xmath393 to approximate @xmath394 , use an inequality of @xcite , specialized here : let @xmath395 then @xmath396 where @xmath397 for large @xmath132 and @xmath320 fixed , this gives @xmath398 stirling s formula gives @xmath399 putting these approximations into @xmath153 , we get @xmath400 the right side , as a function of @xmath401 , is minimized when @xmath402 . plugging this in gives the claim .",
    "let @xmath403 be a random variable with law @xmath2 . then note that @xmath404 therefore , for any @xmath405 , @xmath406 thus , there exists @xmath407 such that @xmath408 fixing @xmath409 , take any such @xmath125 .",
    "the above inequality implies that there exists @xmath410 $ ] such that @xmath411 now take any @xmath412 .",
    "let @xmath413 $ ] , where @xmath414 $ ] is the integer part of @xmath415 .",
    "then there exists @xmath407 and @xmath410 $ ] such that the above inequality is satisfied .",
    "let @xmath416 + 1 $ ]",
    ". then @xmath417 consequently , for this @xmath1 , @xmath418 to complete the proof , note that @xmath419 .",
    "since @xmath420 , therefore @xmath421 suppose that @xmath422 then by the previous display , @xmath423 let @xmath424 $ ] and @xmath425 $ ] . for @xmath426 , define @xmath427 then for any @xmath428 , @xmath429 since @xmath430 and @xmath431 , this gives @xmath432 suppose that @xmath433 then the previous equation gives @xmath434 taking @xmath435 , assuming and , and using , gives @xmath436 now note that @xmath437 is asymptotic to @xmath438 as @xmath439 , and is positive everywhere in the interval @xmath440 .",
    "therefore there is a positive constant @xmath441 such that @xmath442 for all @xmath443 $ ] .",
    "using this in the above inequality gives @xmath444 where @xmath445 is a universal constant . by markov s inequality , @xmath446 .",
    "therefore @xmath447 this shows that as @xmath448 , @xmath186 must also tend to zero . using this and the monotonicity of the map @xmath449 for @xmath450 , it is easy to show that @xmath451 where @xmath452 is a universal constant .",
    "note that this holds under and .",
    "the maximum in the statement of the theorem accounts for these constraints .    by the integrability condition on @xmath239 and the dominated convergence theorem",
    ", it is easy to see that @xmath256 is infinitely differentiable .",
    "moreover , if @xmath27 is a random variable with law @xmath238 , then @xmath453 and @xmath454 the probability density of @xmath238 with respect to @xmath249 is @xmath455 therefore @xmath456 in the notation of theorem [ impthm ] , this is nothing but @xmath104 . now note that if @xmath457 , then by and , @xmath458 and @xmath459 the proof is now easily completed by an application of theorem [ impthm ] , together with chebychev s inequality for bounding the tail probabilities occurring in the statement of theorem [ impthm ] .",
    "let @xmath460 be the probability density of @xmath312 with respect to @xmath326 .",
    "as in the proof of theorem [ gibbsthm ] , we have @xmath461 for each @xmath109 , let @xmath462 be a random variable with law @xmath463 .",
    "a simple computation shows that for any bounded measurable function @xmath464 , @xmath465 it is an easy fact that if @xmath403 is a real - valued random variable and @xmath26 and @xmath466 are two increasing functions , then @xmath467 . from this and the above identity",
    ", it follows that for any bounded increasing function @xmath468 , @xmath469 in particular , for any @xmath470 , @xmath471 is an increasing function of @xmath109 .",
    "this is an important observation that will be used below .",
    "take any @xmath109 such that @xmath320 is well - defined and differentiable in an open neighborhood of @xmath109 .",
    "note that @xmath472 is a convex function , since @xmath473 is nonnegative by .",
    "therefore for any @xmath474 , @xmath475 consequently , if @xmath288 is small enough , then @xmath476 taking @xmath477 , we get @xmath478 similarly , @xmath479 this proves that for all @xmath109 in an open neighborhood of @xmath240 , @xmath480 using the monotonicity of @xmath481 and @xmath327 and the continuity of @xmath327 at @xmath240 , it is easy to conclude from the above identity that for any sequence @xmath482 , @xmath483 by , note that for any @xmath109 @xmath484 therefore @xmath485 thus , there exists @xmath486 $ ] such that @xmath487 since @xmath488 , therefore @xmath489 . hence by , , and , @xmath490 and @xmath491 this implies that @xmath492 in probability .",
    "therefore , by our previous observation about the monotonicity of tail probabilities , @xmath493 for any @xmath494 . in a similar manner , one can show that @xmath495 thus , @xmath496 in probability .",
    "consequently , @xmath497 in probability .",
    "the proofs of parts ( i ) and ( ii ) are now easily completed by applying theorem  [ impthm ] . to prove part ( iii ) , take any @xmath498 . since @xmath327 is nonconstant in any neighborhood of @xmath240 and @xmath327 is an increasing function due to the convexity of @xmath320 , therefore @xmath499 .",
    "thus , by the convexity of @xmath320 , @xmath500 by part ( i ) of the theorem , this implies that if @xmath338 , then @xmath501 in probability , and therefore @xmath502 in probability .",
    "now note that for any @xmath503 , @xmath504 therefore latexmath:[\\[\\label{zbd }      suppose that @xmath507 is a sequence of real - valued random variables and @xmath508 is a real number . in this proof , we will use the notation @xmath509 to mean that for any @xmath172 , @xmath510 .",
    "similarly , @xmath511 means that for any @xmath172 , @xmath512 , and @xmath513 means that both of these hold , that is , @xmath514 in probability .",
    "first , suppose that @xmath348 .",
    "since @xmath320 has no interval of linear behavior in the interval @xmath347 , therefore the convexity of @xmath320 implies that @xmath327 is strictly increasing in @xmath347 . from this and a variant of it",
    "is easy to see that in the interval @xmath515 , @xmath516 is continuous and strictly increasing .",
    "moreover , @xmath517 .",
    "it follows that for any @xmath518 $ ] , there exists @xmath519 $ ] such that @xmath520 .",
    "therefore , since @xmath521 , therefore @xmath522 for some @xmath523 $ ] .",
    "suppose that @xmath524 .",
    "then by part ( i ) and part ( iii ) of theorem [ statmechthm ] , @xmath525 let @xmath526 denote the left - hand side of , without the limit . using the positivity of the second derivative , it is easy to see that @xmath527 is a convex function of @xmath109 .",
    "take any @xmath528 .",
    "then by the convexity of @xmath527 , we have @xmath529 now let @xmath302 on both sides and apply , and then let @xmath530 on the right .",
    "this gives @xmath531 next , note that @xmath532 by and , this implies that @xmath533 note that this inequality was proved under the assumption that @xmath524 .",
    "next , suppose that @xmath534 .",
    "observe the easy inequality @xmath535 from this and the fact that @xmath536 , it follows that holds even if @xmath534 .",
    "next , note that we trivially have @xmath537 which is same as @xmath538 equations and prove that if @xmath348 , then @xmath539 in probability . next ,",
    "note that @xmath540 $ ] , which implies that @xmath541 $ ] and hence @xmath542 on the other hand , jensen s inequality gives @xmath543 it is not difficult to see that since @xmath544 and @xmath545 , therefore the random variable @xmath546 is bounded by a non - random constant that does not vary with @xmath132 .",
    "since we already know that @xmath350 in probability , this shows that @xmath547 combining this with and , we get @xmath548 this completes the proof of part ( i ) of the theorem .",
    "next , suppose that @xmath334 . then note that by theorem  [ statmechthm ] , @xmath549 next , let @xmath550 since @xmath320 is continuously differentiable in the interval @xmath347 and @xmath327 is strictly increasing , therefore there exists @xmath551 such that @xmath552 . if @xmath553 , then @xmath554 therefore @xmath555 define @xmath556 note that if @xmath27 is a random variable with law @xmath312 , then for any @xmath557 , @xmath558 let @xmath559 and choose @xmath560 .",
    "then by the strict convexity of @xmath320 in @xmath347 , @xmath561 by and markov s inequality , @xmath562 taking logarithm on both sides , dividing by @xmath316 and sending @xmath302 , we get @xmath563 in particular , @xmath564 next , note that @xmath565 from , and we get @xmath566 by combining , , and the fact that @xmath567 , this shows that there exists @xmath568 such that @xmath569 as @xmath302 .    next , let @xmath570 then @xmath571 is nothing but the importance sampling estimate @xmath104 when the sampling measure is @xmath323 and the target measure is @xmath312 . in this setting , we have already seen in the proof of theorem  [ statmechthm ] that the quantity @xmath83 of theorem  [ impthm ] is asymptotic to @xmath572 ( to see this , simply combine the equations and ) .",
    "combined with the fact that @xmath573 , this implies that the quantity @xmath574 of theorem  [ impthm ] is asymptotic to @xmath575 in the present setting .",
    "next , let @xmath576 and @xmath460 be the probability density of @xmath312 with respect to @xmath323 .",
    "the formula   implies that @xmath577 is asymptotic to @xmath578 . combining all of these observations and applying theorem [ impthm ]",
    ", it follows that there is a positive constant @xmath508 ( which may depend on @xmath240 , @xmath246 and @xmath579 ) such that for all large enough @xmath132 , @xmath580 take any @xmath581 .",
    "then @xmath582 it is easy to see that @xmath583 is asymptotic to @xmath584 .",
    "thus , the logarithm of the right - hand side in the above display is asymptotic to @xmath585 . since @xmath327 is continuous in a neighborhood of @xmath240",
    ", we can choose a @xmath401 small enough so that @xmath586 .",
    "therefore , there exists @xmath587 such that @xmath588 for all large enough @xmath132 . combining these steps",
    "we see that there is a positive constant @xmath445 such that @xmath589 for all large @xmath132 , and hence @xmath590 now note that by , @xmath591 where @xmath592 is defined in and @xmath593 recall that by , there are positive constants @xmath452 and @xmath594 such that for all large enough @xmath132 , @xmath595 since @xmath596 $ ] , , and imply that @xmath597 however , we have already seen in that there is a constant @xmath598 such that @xmath599 for all large enough @xmath132 .",
    "thus , @xmath600 .",
    "this completes the proof of the theorem .                                                  and roy , d.  m. ( 2010 ) .",
    "when are probabilistic programs probably computationally tractable",
    "? presented at the _ nips workshop on monte carlo methods for modern applications , 2010 .",
    "_ downloadable at http://danroy.org/papers/freermanroy-nipsmc-2010.pdf"
  ],
  "abstract_text": [
    "<S> the goal of importance sampling is to estimate the expected value of a given function with respect to a probability measure @xmath0 using a random sample of size @xmath1 drawn from a different probability measure @xmath2 . </S>",
    "<S> if the two measures @xmath2 and @xmath0 are nearly singular with respect to each other , which is often the case in practice , the sample size required for accurate estimation is large . in this article </S>",
    "<S> it is shown that in a fairly general setting , a sample of size approximately @xmath3 is necessary and sufficient for accurate estimation by importance sampling , where @xmath4 is the kullback  </S>",
    "<S> leibler divergence of @xmath2 from @xmath0 . </S>",
    "<S> in particular , the required sample size exhibits a kind of cut - off in the logarithmic scale . </S>",
    "<S> the theory is applied to obtain a fairly general formula for the sample size required in importance sampling for exponential families ( gibbs measures ) . </S>",
    "<S> we also show that the standard variance - based diagnostic for convergence of importance sampling is fundamentally problematic . </S>",
    "<S> an alternative diagnostic that provably works in certain situations is suggested . </S>"
  ]
}