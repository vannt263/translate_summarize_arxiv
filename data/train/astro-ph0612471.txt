{
  "article_text": [
    "photometric redshifts , both from empirical training sets and template seds , are important for the application of objects to the study of cosmology , as they enable the exploration of large regions of space that are otherwise inaccessible .",
    "this is achieved both in cosmological volume through a higher number density of objects and in parameter space through finer binning .",
    "after the early work of @xcite , @xcite , and @xcite , a variety of techniques were developed extensively @xcite on galaxies in the deep , but narrow , hubble deep field north , ( hdf - n ; * ? ? ?",
    "these different methods were shown to be mutually consistent and relatively accurate in blind - testing @xcite .",
    "more recently , wide - field surveys with multicolor photometry and fiber - based spectroscopy have generated large , uniform samples that enable photometric redshifts to be estimated for both galaxies and quasars .    for galaxies in these surveys at redshifts of @xmath13 , ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) a number of results have converged to an rms dispersion of @xmath14 ( i.e. , @xmath15 ) between spectroscopic and photometric redshifts , with no serious systematic effects .",
    "it should be emphasized , however , that galaxy photometry in these previous analyses has been very good , typically a few percent or better .",
    "@xcite show similar results when combining the sdss dr2 @xcite , galex gr1 @xcite and the extended source catalog of the 2 micron all sky survey @xcite .",
    "the results at moderate redshifts have also been successful , with luminous red galaxies @xcite in the sdss trained with redshifts in the 2slaq survey @xcite having an rms of @xmath16 @xcite for a sample at @xmath17 ( see also * ? ? ?",
    "* ) .    at high redshifts ,",
    "the number of spectra available is smaller and , in addition to the hdf - n , there have been analyses of other deep fields such as the hdf - south @xcite and the hubble ultra deep field @xcite . in the latter , @xcite",
    "show an accuracy of @xmath18 for @xmath19 .",
    "in contrast to galaxies , which show small numbers of outliers but no significant groups of outlying objects , all wide - field quasar photometric redshift results to date @xcite suffer from regions of ` catastrophic ' failure , in which groups of objects are assigned a redshift very different from the true value .",
    "the first four use sdss data , while the latter uses the elais n1 and n2 fields and the chandra deep field north .",
    "* hereafter w04 ) implement an empirical method based on color - redshift relations , which we use as our baseline .",
    "catastrophic failures severely hamper cosmological investigations that use photometrically selected quasar samples ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , particularly by assigning objects at @xmath20 to @xmath21 and vice - versa , thus eliminating these regions is important .",
    "reasons for the failures , depending on the details of the way a particular dataset is chosen , include quasar reddening , degeneracy in the color - redshift relation , and superimposition of emission from another object , for example , an extended host galaxy .",
    "results using a more restricted parameter space @xcite , defined by @xmath22 and @xmath23 in the 17 filter set of the combo-17 survey ( e.g. , * ? ? ?",
    "* ) , have met with more success .",
    "however the sample size , 192 quasars , is small , and limited in angular extent , and therefore is of limited cosmological applicability .    in this paper",
    "we utilize optical data from the fifth data release of the sdss , and near- and far - uv data from the second data release of the galaxy evolution explorer ( galex ; * ? ? ?",
    "* ) to assign photometric redshifts to quasars .",
    "our results improve upon previous wide - field techniques , by eliminating regions of catastrophic failure , resulting in a distribution of quasar photometric redshifts comparable to those obtained for galaxies .",
    "we do not address the application of the photometric redshifts to any parameter space beyond that represented by the training and blind test sets .",
    "we utilize data from the fifth data release ( dr5 , sdss collaboration , in preparation ) of the sloan digital sky survey ( sdss , * ? ? ?",
    "* ) and the second data release ( gr2 ) of the galaxy evolution explorer @xcite .",
    "we select primary non - repeat observations of objects classified as quasars ( specclass = qso or hiz_qso ) in the specobj view of the sdss dr5 catalog archive server database .",
    "the hiz_qso objects are at redshifts of @xmath24 and trigger the use of the lyman @xmath25 finding code in the sdss spectroscopic pipelines ( frieman et al . , schlegel et al . , in preparation ) .",
    "we also require that the spectroscopic flags zwarning = 0 and zstatus @xmath26 2 , and that all input magnitudes are not at clearly unphysical extreme values , being in the range 040 .",
    "the resulting sample contains 55,746 quasars .",
    "in addition to the sdss sample , the sdss objects are cross - matched to the primary photometric objects in the photoobjall view of the galex gr2 database .",
    "we find 8,174 matches within an ra+dec tolerance of 4 arcsec .",
    "532 of these have more than one match and are rejected , leaving an sdss+galex sample of 7,642 unique matches . for the galex objects , we require primary_flag = 1 , a detection in both near and far - uv bands , magnitudes again in the range 040 , and the flags fuv_artifact and nuv_artifact to be 0 .    throughout , the sdss magnitudes are corrected for galactic extinction using the dust maps of @xcite and the galex magnitudes using the @xmath27 ( e_bv ) term inferred from these maps using the standard formula of @xcite .    the resulting samples of 55,746 and 7,642 objects form training sets used as input for the learning algorithms .",
    "the full set of object attributes for the sdss sample consists of 16 training features .",
    "these are the colors @xmath28 , @xmath29 , @xmath30 , and @xmath31 , where the sdss bands @xmath32 , @xmath33 , @xmath34 , @xmath35 , and @xmath36 are given for each of the four magnitude types psf , fiber , petrosian , and model @xcite .",
    "for sdss+galex , we add the colors @xmath37 and @xmath38 , where @xmath32 is given in each of the four sdss magnitude types , resulting in 21 training features .",
    "in addition to the sdss and sdss+galex datasets , we also analyze the sdss+galex sample of objects , but using only sdss features .",
    "this dataset , referred to as _ galex - sdss - only _ , enables us to quantify the level of improvement in sdss+galex seen from the addition of the galex uv features , as opposed to possible improvement due to the sample only containing quasars that appear in both sdss and galex .",
    "we implement instance - based learning on the sdss , sdss+galex and galex - sdss - only datasets .",
    "the results are compared to those on the same data for an empirical color - redshift relation containing full probability density functions ( strand , in preparation ) .",
    "we also study the utility of subsets of the full set of training features using genetic algorithms .",
    "the machine learning is implemented in the java environment data - to - knowledge @xcite .",
    "it is optimized through use of nationally peer - reviewed allocated time on the xeon linux cluster tungsten at the national center for supercomputing applications .",
    "this enables an extensive exploration of the parameter space describing the training features of the objects and the settings of the learning algorithms .",
    "instance - based learning ( ib , e.g. * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) , is a powerful class of empirical machine learning methods that to date has not been extensively utilized on large astronomical datasets due to its computational intensity .",
    "two examples where the method has been used are @xcite and @xcite , who both use the method on the sdss early data release ( edr * ? ? ?",
    "* ) . however , they only utilize single nearest neighbors , and in addition the dr5 dataset analyzed herein is approximately 15 times the size of the edr . here , through the use of tungsten (  [ sec : algorithms ] , above ) , we are able to realize the full potential of the algorithm , via the use of the @xmath0-nearest neighbor method ( e.g. , * ? ? ?",
    "* ) .    in its simplest form ,",
    "the ` training ' of the algorithm is trivial , and involves simply memorizing the positions of each of the examples in the training set . for each object in the testing set , the nearest training example is then found , and the predicted value , either a classification or a continuous value , is taken to be that of the training example .",
    "thus the computational expense is incurred at the time of classification , as a large number of distance calculations must be performed .",
    "however , the method is powerful because it uses all of the information available in the training set , rather than a model of the training set as is typically used by most other learning algorithms .",
    "there are a number of simple refinements to this method , which in practice result in large improvements in performance : ( 1 ) instead of the nearest neighbor to the testing example , the @xmath0 nearest neighbors can be found , and the distances weighted using a predictive integration function to produce a weighted output .",
    "this function , @xmath39 , takes the form @xmath40 where the @xmath41 are the euclidean distances to the neighbors , and the exponent @xmath42 can take on any positive value , typically but not necessarily an integer .",
    "( 2 ) the input features can be _ standardized _ such that the mean and variance of each are 0 and 1 respectively .",
    "this stops the training being dominated by features with larger numerical values or spreads .",
    "alternatively , one could also normalize the range of features to be 01 . ( 3 ) objects in the training set can be allocated to collective regions of parameter space , which can considerably reduce the required number of distance calculations .    of the methods described , we implement ( 1 ) and ( 2 ) , but not ( 3 ) as we wish to use the full information available in the training data .",
    "we optimize the values of @xmath0 and @xmath42 and standardize all training features .",
    "further refinements can also be made for objects which have non - continuous values such as a classification or missing data .",
    "however , in this paper all values are considered , i.e. , the training features and the spectroscopic and photometric redshifts , are continuous .      we have implemented the color - redshift relation ( czr ) method of @xcite on the same data as the ib .",
    "this enables a direct comparison of the performance of the two methods .",
    "the czr establishes an empirical relation between the spectroscopic redshifts and the colors of the training set . the maximum likelihood redshift probability density function ( pdf ) is then found for each object in the test set .",
    "the methods above select and optimize a learning algorithm for a given set of training features .",
    "however , it is possible that different subsets of the features available will produce better results .",
    "in particular , the results for instance - based learning can be made worse by noise in the training set or by irrelevant training features . to explore this possibility",
    ", we implement a binary genetic algorithm on the training feature sets .    a genetic algorithm ( ga : e.g. , * ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) mimics evolution , in the sense that the most successful individuals are those that are best adapted for the task at hand .",
    "we implement the binary genetic algorithm , in which each individual is a string of 0s and 1s which represents whether or not to use a particular input feature ( in our case the 16 colors ) .",
    "an initial population of random individuals is created and the ib is run using the features selected .",
    "the result , in this case the variance between photometric and spectroscopic redshift , is the _ fitness _ of that individual .",
    "the individuals and their fitnesses are then combined to produce new individuals , and those with higher fitnesses are favored . in principle , a good approximation to the best set of features to use as the training set should be selected with this approach .",
    "the combination involves identifying the best individuals to breed via tournament selection , in which a specified number of individuals from the population are selected and the best is put in the mating pool to be combined with other individuals .",
    "two individuals are combined using one point crossover , in which a segment of one is swapped with that of the other .",
    "to more fully explore the parameter space and prevent the algorithm from converging too rapidly on a local minimum , a probability of mutation is introduced on the newly created individuals before they are processed .",
    "this is simply the probability that a 0 becomes a 1 , or vice - versa .",
    "an approximate number of individuals to use is given by @xmath43 where @xmath44 is the number of features . here , for the sdss and galex - sdss - only , @xmath45 , and for sdss+galex , @xmath46 .",
    "hence , @xmath47 respectively for these two values of @xmath44 .",
    "the algorithm converges , i.e. , finds the best individual and hence the best training set , in @xmath48 iterations , where @xmath25 is a problem - dependent constant .",
    "generally @xmath49 , giving an expected value for our data of @xmath50 for @xmath45 , and @xmath51 for @xmath46 .",
    "we employ this number of iterations with larger numbers of individuals to be sure that the algorithm has converged .",
    "further information on genetic algorithm design can be found in , e.g. , @xcite .",
    "our ga is implemented on the ib for each of the sdss , sdss+galex and galex - sdss - only datasets .",
    "the settings of these algorithms are fixed for the duration of the ga iteration .",
    "it is possible in principle to combine the optimization of the learning algorithm and the feature set ; however , we defer this analysis to a later paper .",
    "the ib and czr are supervised learning algorithms  they are given a training set of objects and attempt to minimize a cost function which describes the quality of the predictions on a separate testing set .    for ib , the cost function",
    "is given by the variance between the photometric and spectroscopic redshifts for objects with spectra : @xmath52 where @xmath53 , @xmath54 is the spectroscopic redshift value , and @xmath55 is the photometric redshift prediction made by the learning algorithm .",
    "the second term in the variance equation is small .",
    "the value of the variance is dominated by the outliers . however , in our case , this is a desirable property , because it is these objects which we wish to pull in the most toward the correct values .",
    "the dominance of the outliers renders the variance susceptible to variations in this population .",
    "we therefore quote errors on all of our blind test variances , derived from splitting the population using multiple random seeds ( see below ) . for the czr ,",
    "the cost function is the likelihood of the pdf .",
    "instance - based learning , like any supervised machine learning algorithm , is susceptible to incompleteness and noise in the training set . at the present time , the sdss dr5 is by far the largest and most homogeneous quasar dataset available , and it has a high completeness ( e.g. , * ? ? ?",
    "other available datasets are either not as deep , smaller ( e.g. , * ? ? ?",
    "* ) , or deeper but orders of magnitude smaller ( e.g. , * ? ? ?",
    "one could prune noisy exemplars , however , it is difficult to meaningfully define what is a noisy or sparsely populated region of parameter space , and pruning particular regions could introduce new and poorly defined biases .",
    "the use of multiple nearest neighbors smoothes the noise , and the blind test results address both incompleteness and noise by presenting realistic results on unseen data .",
    "the distance measure parameters of a number of nearest neighbors and the distance weighting assume that the input training features are uncorrelated , however , given that we repeat the same four colors in four magnitude types , and in addition that a set of features is always derived from a particular object , the input features will always be correlated , both in magnitude type ( e.g. , psf @xmath28 is correlated to fiber @xmath28 , and so on ) , and in color ( e.g. , psf @xmath28 is correlated to psf @xmath29 , and so on . )",
    "correlated input features are therefore unavoidable ; we feel , however , that our algorithmic approach is acceptable because we select the parameters to produce the optimal blind test result .",
    "different splits of the training set are investigated at various points in the learning process , giving four adjustable ratios : ( 1 ) @xmath56 is the ratio between the data used as the training set and for testing the algorithm s performance according to the cost function to adjust the final model settings ( for ib there is no adjustment so the ratio just affects the performance through the information available ) . ( 2 ) @xmath57 is the ratio of the whole set of data used in training and testing to that unseen by the algorithm until it is applied , as it would be to new data from another survey ; this is the pseudo - blind test . (",
    "3 ) @xmath58 is the ratio of the data used in each bagged model to the rest of the training data , where the training data is @xmath56 of the whole dataset . ( 4 ) @xmath59 is similar , but for cross - validation .",
    "the latter is distinguished from bagging because it takes different random subsamples of the whole @xmath56 training and @xmath60 testing set , whereas bagging subsamples @xmath56 .",
    "the value for which we quote results for all of these ratios is 80:20 . for application to new data",
    "not used here , the value of @xmath56 would be 100% , to maximize the information available . this is the standard @xmath61 reported in the literature for czr techniques , but its value would be meaningless for instance - based approaches .    for ib ,",
    "the variances obtained are quoted from the pseudo - blind test , as this represents the most realistic standard of performance available from within the sdss and galex datasets to be expected on new data .",
    "@xmath57 is always such that the training data is representative of the full dataset .",
    "we quote the mean and standard deviation of the best variance from ten training runs with differing random seeds for @xmath57 .",
    "each run produces a grid of models with the range @xmath62 and @xmath63 , where @xmath0 is the number of nearest neighbors and @xmath42 the exponent in the distance - weighting function (  [ subsec : ib ] ) .",
    "integral values of @xmath0 and @xmath42 were used , although this is not a requirement .",
    "we use positive values of @xmath42 as negative values would result in objects other than the nearest neighbor being given the highest weighting , which would be unphysical as increasingly large values of @xmath0 would be given an ever higher weight .",
    "we investigated bagging and cross - validation using values of @xmath58 and @xmath59 of 80:20 and 50:50 but these were not found to be necessary for ib . other measures , such as @xmath64 , and the percentage of objects within @xmath7 are also given for comparison to other work .",
    "we do not quote any results in which there is any overlap between the training and testing data .",
    "the comparative czr results were obtained by using a 10-fold bootstrapped pseudo - blind test , again in the ratio @xmath57 = 80:20 .",
    "we now describe results for the full sdss dr5 , sdss dr5 + galex gr2 , and galex - sdss - only datasets , all of which are summarized in table [ table : photoz ] .",
    "we found that the ideal parameters are @xmath65 nearest neighbors ( nn ) and a distance weighting ( dw ) of @xmath66 . in the pseudo - blind test on the unseen 20% of the data , the best variance between the photometric and spectroscopic redshifts is @xmath67 .",
    "a comparison between the photometric and spectroscopic redshifts is shown in figure [ fig : sdss photoz ] , and the effect of varying the nn and the dw for the pseudo - blind test is shown in figure [ fig : sdss grid ] .",
    "we find that @xmath4 , @xmath5 , and @xmath68 of the objects are within @xmath7 , respectively .",
    "the variance weighted by redshift is @xmath69 and the mean @xmath70",
    ".    because the values of nn and dw used here are discrete ( in principle they can be continuous , but that was not attempted ) , the results presented in figure [ fig : sdss photoz ] were obtained with the values of nn , dw and the blind test set random seed that gave the best variance in its grid that was closest to the mean . here , these values are @xmath71 , @xmath72 and a random seed of 8 ( for the seeds we used the integers 0 to 9 ) .",
    "the variance is 0.1240 , which is consistent with the mean variance quoted .",
    "our key result , shown in figure [ fig : sdss photoz ] is the absence of regions of catastrophic failure  there is no upturn in a histogram of @xmath73 values at large @xmath73 , just a smooth decline such that few objects are outliers .",
    "this is in contrast to previous results for quasar photometric redshifts , which , while showing a comparable spread of objects with low @xmath73 , show outlying regions of objects with high @xmath73 .",
    "the scattering of outliers obtained by the ib is similar in form to that seen in other studies for normal galaxies at redshifts of @xmath1 ( see , for example , figure 3 of @xcite for sdss main sample galaxies , which have a mean redshift of @xmath74 ) , although there is still structure seen in figure [ fig : sdss photoz ] , especially at @xmath75 .",
    "we have also implemented the methods of w04 on the sdss dr3 , without removing the reddened quasars ( strand , in preparation ) . here",
    "we apply that method to the sdss dr5 dataset as a direct comparison between the empirical czr and the ib .",
    "we find that the czr has slightly narrower dispersion than the ib , with @xmath73 percentages of @xmath76 , @xmath77 and @xmath78 within @xmath7 .",
    "however , as shown in figure [ fig : czr photoz ] , it still shows regions of catastrophic failure .",
    "the variance is therefore significantly higher , at @xmath79 .",
    "we again plot the run from the ten with the closest variance to the mean . in this case",
    "this was the final run of the ten , with @xmath80 .",
    "previous results using empirical czrs show a similar pattern .",
    "for example , figure 4 of w04 shows regions of quasars at @xmath81 and @xmath82 over the spectroscopic redshift range @xmath83 .",
    "similar results are seen in @xcite , @xcite , and @xcite .      adding the galex data",
    "significantly improves the results , as shown in figures [ fig : galex photoz ] and [ fig : galex grid ] .",
    "here we obtain a variance of @xmath84 for the pseudo - blind test , @xmath9 , @xmath10 and @xmath85 of objects within @xmath7 , @xmath86 , and the mean @xmath87 .",
    "the number of nearest neighbors and distance weighting are @xmath88 and @xmath89 respectively .",
    "a higher distance weighting is expected due to the greater dimensionality of the training feature space ( 21 colors instead of 16 ) compared to the sdss dataset .    the exact values of nn and dw that are plotted in figure [ fig : galex photoz ] are chosen in the same manner as for the sdss , and are @xmath90 , @xmath91 and a random seed of 3 . the variance is 0.0521 .    to show that the improvement is not simply due to the smaller set of objects which appear in both surveys ( for example , these objects may be brighter quasars in the sdss with better photometry ) , we also applied the sdss training procedure to the cross - matched sample .",
    "this gives better results than the sdss sample , but they are still significantly worse than sdss+galex .",
    "the variance is @xmath92 , and the other results are as seen in table [ table : photoz ] .",
    "the sdss results extend deeper than those matched with galex , to @xmath19 rather than @xmath93 .",
    "the lack of quasars in the ` redshift desert ' at @xmath94 is seen in figure [ fig : galex photoz ] , caused by the lyman break in the spectrum at a restframe wavelength of @xmath95 being shifted out of the uv .    the czr results for sdss+galex also improve over those from the full sdss dataset .",
    "@xmath96 , @xmath97 , and @xmath98 of the objects are within @xmath7 .",
    "this is still slightly better than ib for @xmath99 and @xmath100 , but is the same for @xmath101 .",
    "the application of the genetic algorithms on the sdss , sdss+galex and galex - sdss - only datasets converged on the use of approximately half of the training parameters , but the variance was not significantly different from that from using the full set of training features .",
    "the full sets were therefore used throughout .",
    "the result indicates that there is some redundancy in the training features , which is expected given that they are measuring the four colors four different times , just through different apertures .",
    "although the results here represent an important step in the sense that there are no regions of catastrophic failure , further improvement is still possible . in particular : ( 1 ) the input object parameter distributions may be generalized into the form of a pdf for each object , which can be propagated through the learning process , to make more explicit those objects for which the redshift is less certain , to take into account the error on each parameter , and to output a pdf for each object instead of a scalar value .",
    "( 2 ) the no - catastrophics of the instance - based and the lower low-@xmath73 dispersion of the czr can be combined into a new learning algorithm .",
    "the ib is in fact able to obtain similar results to the czr ( i.e. , an approximately 5% narrower dispersion and regions of catastrophic failure instead of a spread of objects ) , by using the single nearest neighbor instead of @xmath0 nearest neighbors .",
    "( 3 ) the addition of other multiwavelength training data , such as infrared data from ukidss @xcite and spitzer @xcite , can be included in the training process .",
    "we also obtained quasar photometric redshifts using decision trees , as used in @xcite for star - galaxy separation .",
    "the variances obtained were generally comparable to , but slightly worse than , those for instance - based , and are , therefore , not reported here .",
    "we apply instance - based machine learning to 55,746 objects spectroscopically classified as quasars in the fifth data release of the sloan digital sky survey ( sdss ) , and to 7,642 objects cross - matched from this sample to the second data release of the galaxy evolution explorer legacy data ( sdss+galex ) .",
    "the algorithm is able to assign photometric redshifts to quasars without regions of catastrophic failure , unlike previously published results .",
    "this will enable samples of quasars to be constructed for cosmological studies with minimal contamination from objects at severely incorrect redshifts .      for sdss , we find a photometric - to - spectroscopic variance of @xmath67 for a sample of the data not used in the training . for sdss+galex",
    ", this improves to @xmath84 . using purely sdss on the latter dataset ( galex - sdss - only )",
    ", the variance is @xmath102 .",
    "hence the improvement results from the extra uv information provided by galex and not the reduced sample size , better photometry , or lower redshifts .",
    "the percentages of objects within @xmath99 are @xmath4 , @xmath9 , and @xmath103 for sdss , sdss+galex , and galex - sdss - only , respectively .",
    "cccccccc sdss & ib & @xmath67 & @xmath104 & @xmath105 & @xmath106 & @xmath107 & @xmath108 + sdss+galex & ib & @xmath84 & @xmath109 & @xmath110 & @xmath111 & @xmath112 & @xmath113 + galex - sdss - only & ib & @xmath102 & @xmath114 & @xmath115 & @xmath116 & @xmath117 & @xmath118 + sdss & czr & @xmath119 & @xmath120 & @xmath121 & @xmath122 & @xmath123 & @xmath124 + sdss+galex & czr & @xmath125 & @xmath126 & @xmath127 & @xmath128 & @xmath129 & @xmath130 + galex - sdss - only & czr & @xmath131 & @xmath132 & @xmath133 & @xmath134 & @xmath135 & @xmath136 +    we thank the referee for a prompt and useful report which improved the paper , and kumara sastry of the illinois genetic algorithms laboratory for a clarification on our use of specific genetic algorithms .",
    "the authors acknowledge support from nasa through grants nn6066h156 and 05-galex05 - 0036 , from microsoft research , and from the university of illinois .",
    "the authors made extensive use of the storage and computing facilities at the national center for supercomputing applications and thank the technical staff for their assistance in enabling this work .",
    "funding for the sdss and sdss - ii has been provided by the alfred p. sloan foundation , the participating institutions , the national science foundation , the u.s .",
    "department of energy , the national aeronautics and space administration , the japanese monbukagakusho , the max planck society , and the higher education funding council for england .",
    "the sdss web site is http://www.sdss.org/.    the sdss is managed by the astrophysical research consortium for the participating institutions .",
    "the participating institutions are the american museum of natural history , astrophysical institute potsdam , university of basel , cambridge university , case western reserve university , university of chicago , drexel university , fermilab , the institute for advanced study , the japan participation group , johns hopkins university , the joint institute for nuclear astrophysics , the kavli institute for particle astrophysics and cosmology , the korean scientist group , the chinese academy of sciences ( lamost ) , los alamos national laboratory , the max - planck - institute for astronomy ( mpa ) , the max - planck - institute for astrophysics ( mpia ) , new mexico state university , ohio state university , university of pittsburgh , university of portsmouth , princeton university , the united states naval observatory , and the university of washington . based on observations made with the nasa galaxy evolution explorer .",
    "galex is operated for nasa by the california institute of technology under nasa contract nas5 - 98034 .",
    "data to knowledge ( d2k ) software , d2k modules , and/or d2k itineraries , used by us , were developed at the national center for supercomputing applications ( ncsa ) at the university of illinois at urbana - champaign ."
  ],
  "abstract_text": [
    "<S> we apply instance - based machine learning in the form of a @xmath0-nearest neighbor algorithm to the task of estimating photometric redshifts for 55,746 objects spectroscopically classified as quasars in the fifth data release of the sloan digital sky survey . </S>",
    "<S> we compare the results obtained to those from an empirical color - redshift relation ( czr ) . </S>",
    "<S> in contrast to previously published results using czrs , we find that the instance - based photometric redshifts are assigned with no regions of catastrophic failure . </S>",
    "<S> remaining outliers are simply scattered about the ideal relation , in a similar manner to the pattern seen in the optical for normal galaxies at redshifts @xmath1 . </S>",
    "<S> the instance - based algorithm is trained on a representative sample of the data and pseudo - blind - tested on the remaining unseen data . </S>",
    "<S> the variance between the photometric and spectroscopic redshifts is @xmath2 ( compared to @xmath3 for the czr ) , and @xmath4 , @xmath5 , and @xmath6 of the objects are within @xmath7 respectively . </S>",
    "<S> we also match our sample to the second data release of the galaxy evolution explorer legacy data and the resulting 7,642 objects show a further improvement , giving a variance of @xmath8 , and @xmath9 , @xmath10 , and @xmath11 of objects within @xmath7 . </S>",
    "<S> we show that the improvement is indeed due to the extra information provided by galex , by training on the same dataset using purely sdss photometry , which has a variance of @xmath12 . </S>",
    "<S> each set of results represents a realistic standard for application to further datasets for which the spectra are representative . </S>"
  ]
}