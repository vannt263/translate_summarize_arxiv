{
  "article_text": [
    "the problem of estimating the marginal likelihood has received considerable attention during the last two decades .",
    "the topic is of importance in bayesian statistics as it is associated with the evaluation of competing hypotheses or models via bayes factors and posterior model odds .",
    "consider , briefly , two competing models @xmath0 and @xmath1 with corresponding prior probabilities @xmath2 and @xmath3 .",
    "after observing a data vector @xmath4 , the evidence in favour of @xmath0 ( or against @xmath1 ) is evaluated through the odds of the posterior model probabilities @xmath5 and @xmath6 , that is , @xmath7 the quantity @xmath8 is the ratio of the marginal likelihoods or prior predictive distributions of @xmath0 and @xmath1 and is called the bayes factor of @xmath0 versus @xmath1 .",
    "the bayes factor can also be interpreted as the ratio of the posterior odds to the prior odds . when @xmath0 and @xmath1 are assumed to be equally probable a - priori , the bayes factor is equal to the posterior odds .",
    "the marginal likelihood of a given model @xmath9 associated with a parameter vector @xmath10 is essentially the normalizing constant of the posterior @xmath11 , obtained by integrating the likelihood function @xmath12 with respect to the prior density @xmath13 , i.e. @xmath14 the integration in ( 1 ) may be evaluated analytically for some elementary cases .",
    "most often , it is intractable thus giving rise to the marginal likelihood estimation problem .",
    "numerical integration methods can be used as an approach to the problem , but such techniques are of limited use when sample sizes are moderate to large or when vector @xmath10 is of large dimensionality .",
    "in addition , the simplest monte carlo ( mc ) estimate , which is given by @xmath15 using draws @xmath16 from the prior distribution is extremely unstable when the posterior is concentrated in relation to the prior .",
    "this scenario is frequently met in practice when flat , low - information prior distributions are used to express prior ignorance . a detailed discussion regarding bayes factors and marginal likelihood estimation is provided by kass and raftery ( 1995 ) .",
    "it is worth noting that the problem of estimating ( 1 ) can be bypassed by considering model indicators as unknown parameters .",
    "this option has been investigated by several authors ( e.g. green , 1995 , carlin and chib , 1995 , dellaportas et al . ,",
    "2002 ) who introduce mcmc algorithms which sample simultaneously over parameter and model space and deliver directly posterior model probabilities",
    ". however , implementation of these methods can get quite complex since they require enumeration of all competing models and specification of tuning constants or `` pseudopriors '' ( depending upon approach ) in order to ensure successful mixing in model space .",
    "moreover , since these methods focus on the estimation of posterior model probabilities , accurate estimation of the marginal likelihoods and/or bayes factors will not be feasible in the cases where a dominating model exists in the set of models under consideration ; tuning , following the lines of ntzoufras et al .",
    "( 2005 ) , might be possible but is typically inefficient and time consuming .",
    "in contrast , `` direct '' methods provide marginal likelihood estimates by utilizing the posterior samples of separate models .",
    "these methods are usually simpler to implement and are preferable in practice when the number of models under consideration is not large , namely when it is practically feasible to obtain a posterior sample for each of the competing models .",
    "work along these lines includes the laplace - metropolis method ( lewis and raftery , 1997 ) , the harmonic - mean and the prior / posterior mixture importance sampling estimators ( newton and raftery , 1994 ) , bridge - sampling methods ( meng and wong , 1994 ) , candidate s estimators for gibbs sampling ( chib , 1995 ) and metropolis - hastings sampling ( chib and jeliazkov , 2001 ) , annealed importance sampling ( neal , 2001 ) , importance - weighted marginal density estimators ( chen , 2005 ) and nested sampling approaches ( skilling , 2006 ) . more recently , raftery et al . (",
    "2007 ) presented a stabilized version of the harmonic - mean estimator , while friel and pettitt ( 2008 ) and weinberger ( 2012 ) proposed new approaches based on power posteriors and lebesgue integration theory , respectively .",
    "it is worth mentioning that bayesian evidence evaluation is also of particular interest in the astronomy literature where nested sampling is commonly used for marginal likelihood estimation ( e.g. feroz et al . , 2009 ; feroz et al . ,",
    "recent reviews comparing popular methods based on mcmc sampling can be found in friel and wyse ( 2012 ) as well as in ardia et al .",
    "alternative approaches for marginal likelihood estimation include sequential monte carlo ( del moral et al . ,",
    "2006 ) and variational bayes ( parise and welling , 2007 ) methods .    in this paper",
    "we propose using the marginal posterior distributions on importance sampling estimators of the marginal likelihood .",
    "the proposed approach is particularly suited for the gibbs sampler , but it is also feasible to use for other types of mcmc algorithms . the estimator can be implemented in a straightforward manner and it can be extended to multi - block parameter settings without requiring additional mcmc sampling apart from the one used to obtain the posterior sample .    the remainder of the paper is organized as follows .",
    "the proposed estimator and its variants are discussed in section 2 . in section 3",
    "the method is applied to normal regression models , to finite normal mixtures and also to hierarchical longitudinal poisson models .",
    "concluding remarks are provided in section 4 .",
    "in the following we first introduce the proposed estimator in a two block setting .",
    "the more general multi - block case is considered next , explaining why the estimator will be useful in such cases .",
    "we further present details concerning the implementation of the proposed approach when the model formulation includes latent variables or nuisance parameters that are not of prime interest for model inference .",
    "the section continues with a description of the different estimation approaches of the posterior marginal distributions used as importance functions .",
    "we conclude with a note on a convenient implementation of the estimator for models where the posterior distribution becomes invariant under competing diffuse priors and brief remarks about the calculation of numerical standard errors . in the remaining of the paper , the dependence to the model indicator @xmath17 ( introduced in the previous section )",
    "is eliminated for notational simplicity .",
    "let us consider initially the 2-block setting where @xmath18 is the likelihood of the data conditional on parameter vectors @xmath19 and @xmath20 , which can be either independent , i.e. @xmath21 , or dependent , e.g. @xmath22 , a - priori . in general , one can improve the estimator in ( 2 ) by introducing a proper importance sampling density @xmath23 and then calculate the marginal likelihood as an expectation with respect to @xmath23 instead of the prior , i.e. @xmath24.\\ ] ] this quantity can be easily estimated as @xmath25 where @xmath26 and @xmath27 , for @xmath28 , are draws from @xmath23 .",
    "theoretically , an ideal importance sampling density is proportional to the posterior . in practice , we seek densities which are similar to the posterior and easy to sample from .    given this consideration , we propose to use the product of the marginal posterior distributions as importance sampling density , i.e. @xmath29 . under this approach",
    "@xmath30 which yields the estimator @xmath31 note that the only twist in ( 3 ) is that the draws @xmath26 and @xmath27 , for @xmath28 , are draws from the marginal posteriors @xmath32 and @xmath33 and not from the joint posterior @xmath34 . in most cases the marginal posterior distributions will not be known , nevertheless , this does not constitute a major obstacle neither for sampling from the marginal posteriors nor for calculating the marginal probabilities which appear in the denominator of ( 3 ) ; the former issue is discussed here , the latter is handled in section 2.4 .",
    "it is straightforward to see that the product marginal posterior is the optimal importance sampling density when @xmath35 and @xmath36 are independent a - posteriori , since in this case @xmath37 leading to the zero - variance estimator .",
    "although posterior independence is not frequently met in practice , the product marginal posterior can serve as a good approximation to the joint posterior even if @xmath38 and @xmath39 are not completely independent a - posteriori .",
    "first , it has exactly the same support as the joint posterior .",
    "second , the blocking of the parameters can be such that the parameter blocks are close to orthogonal regardless whether the elements within @xmath40 and @xmath41 are strongly correlated .",
    "furthermore , appropriate reparameterizations can be used in order to form parameter blocks which are orthogonal or close to orthogonal ( see e.g. gilks and roberts , 1996 ) . moreover , in generalized linear models the augmentation scheme of ghosh and clyde ( 2011 ) can be used to obtain orthogonal parameters .",
    "it is worth noting that the estimator in ( 3 ) is similar to the markov chain importance sampling approach described in botev et al .",
    "( 2012 ) and the marginal likelihood estimator proposed in chan and eisenstat ( 2013 ) based on the cross - entropy method .",
    "botev et al .",
    "( 2012 ) show that the product of the marginals is the best importance sampling density  in the sense of minimizing the kullback - leibler divergence with respect to the zero - variance importance sampling density  among all product form",
    "importance sampling densities , given that the zero - variance density is also decomposable in product form .",
    "similarly , chan and eisenstat ( 2013 ) locate the importance sampling density minimizing kullback - leibler divergence with respect to block - independent factorizations of the joint posterior from distributions belonging to the same parametric families as the priors .",
    "the approach presented here differentiates from the above estimators since we consider directly the marginal posteriors and `` manipulate '' the joint mcmc sample in order to construct marginal samples , thus avoiding further importance sampling and leaving estimation of marginal densities as the main issue to deal with .    in general , marginal posterior samples can be obtained from any mcmc algorithm .",
    "the only problem is that a single mcmc chain corresponds to a sample from the joint distribution , with non - zero covariance between parameter blocks .",
    "one option is to use a different mcmc run for each block of parameters . in this case",
    ", one can calculate the estimator in ( 3 ) by using draws @xmath26 , @xmath27 coming from two independent mcmc samples of equal size @xmath42 .",
    "nevertheless , this approach can considerably increase the number of mcmc iterations , especially for a large number of parameter blocks .",
    "in addition , the approach is not economical in the sense that only @xmath42 posterior draws are used from a total sample of @xmath43 draws .",
    "a simple and more efficient solution is to re - order a single mcmc chain in such a way that it does not correspond to a sample from the joint posterior distribution .",
    "this can be easily implemented by systematically permuting either the sampled values of @xmath38 or those of @xmath39 .",
    "for instance , consider one mcmc chain where the initial mcmc draws are indexed as @xmath44 where @xmath45 .",
    "then , one can simply re - order the sample of @xmath39 as @xmath46 and join the set of draws @xmath47 , thus forming a sample of paired realizations from the distribution @xmath48 .",
    "obviously , when the paired sample has been formed , the distinction between the two sets of indices becomes irrelevant and a common index may be adopted as presented in equation ( 3 ) .",
    "reordering is trivial in implementation regardless of the number of parameter blocks ; the only initial requirement is that the size of the final mcmc sample @xmath42 must be dividable with the number of blocks , say @xmath49 , so that @xmath49 independent reorderings of the mcmc chain can be formed .",
    "this line of reasoning also holds for cases of multiple - chain mcmc sampling , which is a frequent mcmc strategy favoured mainly on the basis of mcmc convergence checks ( e.g. gelman and rubin , 1992 ) . for such cases",
    ", one can re - order the within - chain posterior samples in a similar manner to that previously described , and then form a joined sample from the multiple re - ordered chains .",
    "another , even simpler , alternative for removing correlations between marginal samples is to randomly permute each sample .",
    "results using this approach will be similar to the above systematic re - ordering , except in extreme , unlikely cases where randomly permuted samples with non - zero sample correlation are generated by chance .",
    "irrespective of the reorderding scheme used , for the remainder of this paper we use a common index @xmath50 across parameter blocks , referring to joined independent block samples .      generalization to multi - block hierarchical settings",
    "is straightforward .",
    "consider @xmath49 blocks @xmath51 ; in this case the product of the @xmath49 marginal posteriors is used as importance sampling density and @xmath52 which yields the following estimator @xmath53    note that the estimator in ( 4 ) may also refer to multiple unidimensional blocks where each parameter forms one block .",
    "the advantage of such an estimator raises from the fact that it is easy to construct good approximations of univariate marginal posterior distributions . on the other hand , any possible gain in the efficiency earned from the construction of good approximating densities for the marginal posteriors",
    "might be moderated by the use of an importance function which assumes overall independency .",
    "therefore , the most efficient strategy is to choose blocks of minimal size constituted only by highly correlated parameters , which have at the same time weak between - block corrrelations . of course , quite often the model design is such that this condition is already met , e.g. for reasons of efficient mcmc mixing .",
    "in addition , for cases of gibbs sampling the natural blocking is the most convenient to use , since in such cases marginal posterior densities can be estimated accurately even for high - dimensional parameter blocks ( see section 2.4 for more details ) .",
    "many hierarchical models include a block component @xmath54 , usually not of main inferential interest , which is associated with a hyperparameter vector @xmath55 through the relationship @xmath56 .",
    "for instance , @xmath54 may be a random effect vector or a latent vector used to facilitate posterior simulation through gibbs sampling as in the data augmentation setting introduced in tanner and wong ( 1987 ) .",
    "in such cases inference usually focuses on the marginal sampling likelihood by integrating out @xmath54 .",
    "for example , when there is only one parameter block @xmath38 , the marginal sampling likelihood is @xmath57 . in this case , a marginal likelihood estimate is obtained through equation ( 3 ) , where @xmath39 is simply replaced by @xmath55 .",
    "the extension to the multi - block setting is essentially the same as in ( 4 ) , only with the addition of @xmath55 , specifically @xmath58 alternatively , there is also the option of working with the hierarchical likelihood and including @xmath54 in the estimation process , i.e. @xmath59 the latter approach is less practical to implement as it requires evaluation of @xmath60 .",
    "in addition , marginalization over @xmath61 will in general lead to more precise marginal likelihood estimates due to scaling down the parameter space ; see vitoratou _ et al . _ ( 2013 ) for further details .",
    "therefore , estimator ( 5 ) is overall preferable to estimator ( 6 ) , except perhaps in cases where the likelihood in ( 5 ) is not available analytically and also estimation of @xmath60 is easy to handle based on the methods discussed next .",
    "as seen so far , the proposed approach is fairly simple to implement .",
    "the only remaining issue is the evaluation of the marginal posterior probabilities appearing in the denominators of estimators ( 3)(6 ) . here",
    "we discuss some different approaches that can be adopted .",
    "a first simple approach is to assume normality either directly or indirectly .",
    "let us consider , for instance , the 2-block setting of section 2.1 with parameter blocks @xmath38 and @xmath39 .",
    "suppose , for instance , that @xmath38 relates to a vector of means or a vector of regression parameters .",
    "then , for moderate to large sample sizes , a reasonable option is to assume that @xmath62 , where @xmath63 and @xmath64 are the estimated posterior mean vector and variance - covariance matrix from the mcmc output , respectively .",
    "vector @xmath39 , on the hand , may refer to a vector of disperion parameters , where the assumption of normality may not be suitable .",
    "one stategy , often sufficient in many cases , is to assume that a transformation @xmath65 is approximately normal , i.e. @xmath66 , and consequently @xmath67 for an appropriate invertible function @xmath68 .",
    "an alternative is to mimic the marginal posteriors by adopting appropriate distributional assumptions and matching parameters to posterior moments .",
    "this option is more suitable when the assumption of normality is not particularly supported and appropriate transformation functions are hard to find . in such cases , one can also consider a wide range of options based on multivariate kernel methods ( e.g. scott , 1992 ) as an efficient alternative .",
    "moreover , when implementing gibbs sampling where the normalizing constants of the full conditional distributions are known , marginal posterior densities can be estimated through an efficient , simulation - consistent technique referred as rao  blackwellization by gelfand and smith ( 1990 ) .",
    "consider , for instance , the @xmath49 parameter block setting of section 2.2 ; the rao - blackwell estimates in this case are @xmath69 note that not all @xmath42 posterior draws need to be used ; usually a sufficiently large subsample of @xmath70 posterior draws is adequate .",
    "for instance , in the examples presented next we find that samples between 200 to 500 draws are sufficient , which significantly reduces computational expense .",
    "it should also be noted that rao - blackwell estimates must be based on draws from the joint posterior distribution , that is draws from the initial non - permuted mcmc sample .    finally , for cases of hybrid gibbs sampling where only some full conditionals are known , one can use a combination of the methods discussed here .",
    "rao - blackwellization may be used for parameter blocks with known full conditional distributions , whereas for the remaining blocks one can choose among distributional approximations based on moment - fitting and kernel methods .",
    "estimators ( 3)(6 ) will not be unbiased when approximating the marginal posterior densities using moment - matching strategies .",
    "nevertheless , in practice such `` proxies '' can be very accurate for univariate as well as multivariate distributions .",
    "for instance , as illustrated in section 3.3 , high - dimensional marginal posteriors are approximated efficiently through multivariate normal distributions .",
    "in addition , the degree of bias can be empirically checked by comparing such estimates to the corresponding ones using importance samples from the moment - matched approximating distributions .",
    "the latter procedure yields an unbiased estimator of the marginal likelihood and , therefore , small observed differences will imply that the bias introduced is negligible .",
    "as known , the marginal likelihood is very sensitive to changes in the prior distribution , whereas the posterior distribution ( after a point ) is insensitive to the prior as the latter becomes more and more diffuse .",
    "therefore , a usual drawback of marginal likelihood estimators that are based solely on draws from the posterior distribution is that they are typically not reliable for evaluating the marginal likelihoods of different models when considering diffuse priors ( see e.g. friel and wyse , 2012 ) .    nevertheless , this is not the case for the proposed estimator as it incorporates the prior in the estimation of the marginal likelihood .",
    "in fact , we can easily adopt estimator ( 3 ) in order to estimate the marginal likelihood under different diffuse priors ( that have no essential effect on the posterior distribution ) using a sample from a single mcmc run . to illustrate this , consider the 2-block setting of section 2.1 and two diffuse priors @xmath71 under which the posterior distribution remains unchanged , i.e. @xmath72 .",
    "let us assume that draws @xmath73 are available from an initial mcmc run and that the marginal likelihood @xmath74 under @xmath75 has already been estimated through ( 3 ) .",
    "then , the marginal likelihood under @xmath76 can be accurately estimated by @xmath77 the estimator in ( 7 ) does not require additional mcmc sampling , likelihood evaluations or evaluations of the marginal posterior densities since the posterior distributions @xmath78 and @xmath79 are the same under @xmath76 and @xmath75 , respectively ; the only extra effort involved is calculation of the prior probabilities @xmath80 , for @xmath28 .",
    "the method of batch means provides a straightforward way for calculating the numerical or mc error of the estimator .",
    "consider for instance the 2-block setting of section 2.1 ; in this case the block - independent posterior sample @xmath81 is divided into @xmath82 batches @xmath83 of size @xmath84 , i.e. @xmath85 and @xmath86 , and one calculates @xmath87 for @xmath88 .",
    "then , an estimate of the standard error is given by @xmath89 ^ 2 } , \\ ] ] where @xmath90 is the average batch mean estimate .",
    "note that @xmath82 must be large enough to ensure proper estimation of the variance ( the usual choice is @xmath91 ) and @xmath84 must also be sufficiently large so that the @xmath92 s are roughly independent ( see e.g. carlin and louis , 1996 ) .    alternatively , we can consider the variance estimators of newey and west ( 1987 ) and geyer ( 1992 ) for dependent mcmc draws .",
    "such estimators are suited when systematic re - ordering is used to form the block - independent posterior sample , since , in this case , the posterior dependency patterns will be the same as those of the initial mcmc sample .",
    "this is due to the fact that the number of parameter blocks @xmath49 will be usually much smaller than the size of the posterior sample ( @xmath93 ) .",
    "therefore , serial auto - correlations for lags greater than @xmath94 are expected to be negligible ( for converged mcmc runs ) , while auto - correlations of lower order are not affected by the re - ordering .    finally , checking whether the variance is finite or not can be investigated empirically ;",
    "if the variance is finite then one should expect that increasing the mcmc sample by a factor of @xmath95 should lead to a decrease of the standard error estimate by a factor approximately equal to @xmath96 . as illustrated in section 3.4",
    ", the variance of the proposed estimator is finite for the examples presented in section 3 .",
    "in this section we apply our method to three common classes of models .",
    "first , we consider normal linear regression where the true marginal likelihood can be calculated analytically , and compare the proposed estimator to other estimators commonly used in practice .",
    "the second example concerns finite normal mixture models where marginal likelihood estimation has proven particularly problematic due to non - identifiability . in the third example",
    ", we apply the proposed methods to an hierarchical longitudinal poisson model where the integrated sampling likelihood is analytically unavailable and , furthermore , standard gibbs sampling can not be implemented .",
    "the section closes with an empirical diagnostic for checking the assumption of finite variance by comparing the corresponding errors from samples of size @xmath42 and @xmath43 .    in all illustrations ,",
    "we denote the likelihood functions with @xmath97 , prior densities with @xmath98 and posterior or full conditional distributions with @xmath99 . concerning specific distributional notation , the inverse - gamma density defined in terms of shape @xmath100 and rate @xmath101",
    "is denoted by @xmath102 , the dirichlet distribution with @xmath103 concentration parameters by @xmath104 and the @xmath105-dimensional inverse - wishart distribution with @xmath106 degrees of freedom and scale matrix @xmath107 by @xmath108 .",
    "here we consider the data set presented in montgomery et al .",
    "( 2001 , p.128 ) concerning 25 direct current ( dc ) electric charge measurements ( volts ) and wind velocity measurements ( miles / hour ) .",
    "the goal is to infer about the effect of wind velocity on the production of electricity from a water mill .",
    "the models under consideration are +    * @xmath109 : the null model with the intercept , + * @xmath0 : intercept@xmath110 , + * @xmath1 : intercept@xmath111 and + * @xmath112 : intercept@xmath113@xmath114 , +    where @xmath115 is wind velocity and @xmath116 is the logarithm of wind velocity .",
    "let @xmath117 denote the model indicator , i.e. @xmath118 . the likelihood and prior assumptions are the following @xmath119 where @xmath120 and @xmath121 correspond to the regression vector and design matrix of model @xmath117 , respectively , and @xmath122 with @xmath123 .",
    "in relation to the context of section 2 this is a 2-block setting where @xmath124 and @xmath125 . under this conjugate prior",
    "design the distributions @xmath126 and @xmath127 are all of known form .",
    "we treat the posterior distribution as unknown and implement a gibbs sampler in r. specifically , one gibbs chain is iterated 10,000 times and the first 1000 iterations are discarded as burn - in , resulting in a final posterior sample of 9,000 draws for each model .",
    "we calculate two variations of estimator ( 3 ) , considering : i ) the true marginals @xmath128 and @xmath127 , and ii ) rao - blackwell estimates of @xmath128 and @xmath127 based on reduced samples of 200 posterior draws .",
    "the two variants are denoted by @xmath129 and @xmath130 , respectively .    for comparison reasons",
    ", we also consider the following commonly used marginal likelihood estimators : the laplace - metropolis estimator ( lewis and raftery , 1997 ) , the importance - weighted marginal density estimator of chen ( 2005 ) , the candidate s estimator from gibbs sampling ( chib , 1995 ) and the optimal bridge - sampling estimator ( meng and wong , 1996 ) . for the laplace - metropolis we require only the mcmc estimated posterior mean vector and posterior covariance matrix .",
    "for the second estimator , which requires specification of approximating densities , we use normal distributions for the @xmath120 s and inverse gamma distributions for the @xmath131 s which mimic the respective component - wise marginal posteriors through moment - fitting . in addition , the points which maximize the unnormalized posterior density of each model are used as posterior ordinates . in order to apply chib s estimator in a realistic context ( using reduced gibbs sampling )",
    "the posterior ordinates ( the points maximazing the unnormalized posterior ) are decomposed according to the univariate densities .",
    "the reduced posterior ordinates are calculated via rao - blackwellization based on 9,000 draws from further gibbs updating ( additional sampling is not needed for the simple intercept - mopel ) .",
    "finally , for the optimal bridge - sampling estimator , which is calculated iteratively , we utilize the same approximating densities as in the implementation of chen s estimator and iterate 1000 times using the geometric bridge - sampling estimates , also presented in meng and wong ( 1996 ) , as starting values .",
    "the three additional estimators are denoted by @xmath132 , @xmath133 , @xmath134 and @xmath135 , respectively .",
    "l@  c@  cccc & & & & @xmath136 & @xmath137 & @xmath138 & @xmath139 laplace - metropolis & @xmath140 & -35.1381 & -12.3676 & -0.4044 & -0.9044&&(0.0092 ) & ( 0.0124 ) & ( 0.0092 ) & ( 0.0112 ) + importance - weighted & @xmath141&-34.8815 & -13.1407 & -1.5979 & -2.2277&&(0.0029 ) & ( 0.0039 ) & ( 0.0031 ) & ( 0.0068 ) + candidate s & @xmath142&-34.8789 & -13.1420 & -1.5962 & -2.2337&&(0.0020 ) & ( 0.0028 ) & ( 0.0023 ) & ( 0.0067 ) + optimal bridge - sampling & @xmath143 & -34.8807 & -13.1412 & -1.5979 & -2.2294 & & ( 0.0011 ) & ( 0.0019 ) & ( 0.0022 ) & ( 0.0030 ) +   + exact marginals & @xmath144 & -34.8786 & -13.1420 & -1.5932 & -2.2302 & & ( 0.0023 ) & ( 0.0035 ) & ( 0.0030 ) & ( 0.0030 ) + rao - blackwellization & @xmath145 & -34.8782 & -13.1405 & -1.5919 & -2.2280 + & & ( 0.0023 ) & ( 0.0030 ) & ( 0.0030 ) & ( 0.0033 ) + target value & @xmath146 & -34.8797 & -13.1429 & -1.5953 & -2.2270    in order to calculate mc errors the posterior samples are divided into 30 batches of 300 draws .",
    "batch mean estimates , mc errors and the true marginal log - likelihoods are presented in table [ tab1 ] . in practical terms",
    ", we found @xmath132 being the easiest to compute . on the other hand , this estimator performs poorly in comparison to the others , as seen in table 1 .",
    "variations of @xmath132 based on multivariate medians ( @xmath147 centers ) and maximum density points of the unnormalized posteriors ( not presented here ) did not yield substantially different estimates .",
    "in contrast , estimators @xmath133 , @xmath134 and @xmath135 perform substantially better .",
    "implementation for @xmath135 is in general somewhat more complicated in comparison to @xmath133 as it requires an iterative solution in addition to specification of approximating densities , whereas @xmath134 requires additional gibbs sampling for models @xmath137 , @xmath138 and @xmath139 . the estimators proposed here , @xmath129 and @xmath130 , only require as input the posterior marginal samples and yield comparable batched mean estimates , with mc errors lower than those of @xmath133 and just slightly higher than those of estimator @xmath135 . also , note that @xmath133 and @xmath134 yield higher mc errors for @xmath139 . in addition , the estimates derived through rao - blackwellization are very similar to the estimates obtained from the true marginal posteriors , while the mc errors are similar across models .",
    "we proceed by testing the ability of the proposed estimator to capture the sensitivity of the marginal likelihood over different diffuse prior distributions which have minimal effect on the posterior distributions of the regression coefficients @xmath148 .",
    "the prior used , with @xmath149 , corresponds to a zellner @xmath23-prior ( zellner , 1986 ) with @xmath23 set equal to @xmath150 . for this particular data set",
    "the posterior distribution of @xmath148 is sensitive to the prior when setting @xmath23 equal to @xmath151 and @xmath50 , which are among the commonly used options ( see fernndez et al . , 2001 ) .",
    "therefore , we assume more diffuse priors and use the values of 1000 , 1500 and 2000 for @xmath23 ; for these choices the posterior distributions are essentially equivalent with posterior expectations ( means , standard deviations etc . ) being exact up to the 3@xmath152 decimal place .",
    "we adopt the approach discussed in section 2.5 using estimator ( 7 ) and the model with @xmath153 as the base model from which we sample from the posterior 10,000 draws discarding the first 1000 as burn - in .",
    "batch mean estimates from the rao - blackwell estimator and mc errors , based on 30 batches 300 draws , along with the true marginal log - likelihoods are presented in table [ tab2 ] .",
    ".[tab2]estimated marginal log - likelihood rao - blackwell estimates compared with the true values for example 1 for the initial @xmath23-prior and three diffuse @xmath23-priors for the regression vector ; average batch mean estimates ( mc errors in parentheses ) are presented using 30 batches of size 300 .",
    "the estimates for @xmath154 and @xmath155 are based on posterior samples from the models with @xmath153 . [ cols=\"<,<,^,^,^,^ \" , ]     as seen in table [ tab2 ] , the estimates are accurate despite the fact that the posterior distributions remain the same . in addition , using estimator ( 7 ) based on draws from the model with @xmath153 required only calculation of prior probabilities for the models with @xmath23 equal to 1500 and 2000 , and led to consistent marginal likelihood estimates .",
    "the estimates based on the exact marginal posteriors ( not presented here ) are equivalent .      in this example",
    "we consider the well - known galaxy data which where initially presented by postman et al .",
    "the data are velocities ( km s per second ) of 82 galaxies from six separated conic sections of the corona borealis region .",
    "the data set is taken from ` mass ` library in r which contains a `` typo '' ; the value of the 78^th^ observation was corrected to 26960 .",
    "the goal is to investigate whether the galaxies can be classified into different clusters according to their velocities , as suggested in astronomical theories .",
    "gaussian finite mixture models are used in the related literature with the purpose of finding the most plausible number of clusters or components . under this modeling assumption ,",
    "the likelihood of the velocity data @xmath156 for a model with @xmath103 components @xmath157 , such that @xmath158 for @xmath159 , is given by @xmath160 where @xmath161 , @xmath162 , @xmath163 and @xmath164 is the p.d.f .",
    "of the normal distribution .",
    "vectors @xmath165 and @xmath166 consist of the component - specific means and variances , respectively . as originally shown in dempster et al .",
    "( 1977 ) , any mixture model can be expressed in terms of missing or latent data ; if @xmath167 represents a latent indicator variable associated with observation @xmath168 , so that @xmath169 and @xmath170 , then we have that @xmath171 summation over the components @xmath172 results in the complete marginalized data likelihood presented in ( 10 ) .",
    "as illustrated in west ( 1992 ) and diebolt and robert ( 1994 ) , data - augmentation facilitates posterior simulation via gibbs sampling from the full conditional densities of @xmath173 and @xmath174 .",
    "the conjugate priors are @xmath175 , @xmath176 and @xmath177 .",
    "the prior for @xmath174 is fixed by model design , since @xmath169 .",
    "gibbs sampling is straightforward to implement , given these prior assumptions ; let @xmath178 be the set of observation indices for those @xmath168 classified into the @xmath117-th cluster and let @xmath179 denote the number of observations falling into the @xmath117-th cluster .",
    "then , we sample sequentially @xmath180 where @xmath181 , @xmath182 and @xmath183 we are also interested in models which have a common variance term in ( 10 ) ; in this case the full conditional of @xmath184 is @xmath185 with @xmath186 .",
    "a central point in the discussion that follows is the identifiability problem which is present in mixture models , known as `` label - switching '' .",
    "non - identifiability arises from the fact that relabelling the mixture components @xmath172 does not change the likelihood in ( 10 ) .",
    "therefore , when the priors are also invariant to label permutations , the posterior distribution has @xmath187 symmetrical modes . in terms of posterior sampling",
    "this implies that common mcmc samplers will most probably fail to explore adequately all @xmath187 modes as it is very likely that an mcmc chain will get `` trapped '' in one particular mode thus leaving the remaining @xmath188 modes unvisited .",
    "a first suggestion proposed in the early literature is to impose prior ordering constraints , e.g. @xmath189 or @xmath190 , which translate to truncated priors that restrict inference to constrained unimodal posteriors .",
    "robert and mengersen ( 1999 ) further extended this strategy to the use of improper priors through reparameterization .",
    "nevertheless , other authors object to the use of prior identifiability constraints and recommend sampling from the unconstrained posterior . among them , celeux et al .",
    "( 2000 ) propose tempered transition algorithms and appropriate loss functions for permutation invariant posteriors , while marin et al .",
    "( 2005 ) suggest ex - post reordering schemes .",
    "chib ( 1995 ) was the first who estimated directly the marginal likelihoods of these data for two and three component models via the candidate s formula and gibbs updating for the estimation of reduced posterior ordinates .",
    "nevertheless , as pointed out in neal ( 1998 ) , chib s use of the gibbs sampler for mixture models results in biased marginal likelihood estimates due to lack of label - switching within the gibbs sampler .",
    "a simple approach to correct for bias is to multiply the marginal likelihood estimates with a factor of @xmath187 , but as neal remarked the bias correction will only be valid when the symmetrical modes are well - separated ( i.e. when label - switching is not likely to occur ) . therefore , neal ( 1998 ) suggests either to introduce special relabelling transitions into the gibbs sampler or to enforce constrained priors during gibbs updating which will be @xmath187 times larger than the unconstrained priors , as general but computationally demanding solutions .",
    "motivated by the practical bias - correction approach , berkhof et al .",
    "( 2003 ) present simulation consistent marginal likelihood estimators based on a stratification principle and ex - post randomly permuted samples .",
    "frhwirth - schnatter ( 2004 ) , on the other hand , recommends to use mcmc samplers which adequately explore all @xmath187 labeling posterior subspaces and presents bridge - sampling estimators based on draws from the unconstrained random permutation sampler introduced in frhwirth - schnatter ( 2001 ) .",
    "we consider the same models as chib ( 1995 ) and show that the estimator proposed here can accurately estimate the marginal likelihoods either by taking into account the bias - correction of neal ( 1998 ) or through the use of mcmc samplers which explore effectively the unconstrained posterior space .",
    "specifically , interest lies in the 2-component equal - variance model and 3-component models with equal and unequal variances ( i.e. @xmath191 ) , under the prior assumptions @xmath192 , @xmath193 , @xmath194 , @xmath195 and @xmath196 for @xmath159 .",
    "we further take into account a 4-component equal variance model ( @xmath197 ) with the same prior assumptions .",
    "models with more than four clusters are not considered due to the fact that there is not enough information in the data to support @xmath198 , which gives rise to serious convergence problems due to non - identifiability of parameters for more than four clusters ; see carlin and chib ( 1995 ) .",
    "the gibbs sampler for these models can be easily implemented through package ` bayesmix ` ( grn , 2011 ) in r , which also allows for ex - post reordering and random permutation sampling .",
    "we iterate the gibbs sampler 13,000 times and discard the first 1000 iterations as burn - in . in the context of section 2 this is a multi - block problem @xmath199 , including a latent vector @xmath200 which is integrated out .",
    "we divide the reordered product marginal posterior sample into @xmath201 batches of @xmath202 draws and calculate the marginal likelihood for each batch as @xmath203 marginal posterior densities are estimated through rao - blackwellization based on reduced samples of size @xmath204 , i.e. @xmath205=@xmath206 $ ] , @xmath207 $ ] and @xmath208 , for @xmath209 . for the equal - variance models we have that @xmath210 .",
    "table [ tab3 ] shows batch mean estimates on log scale and the corresponding mc errors for the simple estimator @xmath211 , the bias - corrected estimator @xmath212 ( obtained by adding the constant @xmath213 to @xmath214 ) and the estimator based on ex - post random permutation sampling @xmath215 .",
    "lcccc & & 2 clusters & 3 clusters & 3 clusters & 4 clusters & equal & equal & unequal & equal & variance & variance & variance & variance & -240.458 & -228.597 & -228.595 & -229.027 & ( 0.002 ) & ( 0.003 ) & ( 0.029 ) & ( 0.045 ) & -239.765 & -226.805 & -226.803 & -225.849 & ( 0.002 ) & ( 0.003 ) & ( 0.029 ) & ( 0.045 ) & -239.762 & -226.778 & -226.771 & -225.922 & ( 0.010 ) & ( 0.018 ) & ( 0.051 ) & ( 0.060 ) neal ( 1998 ) estimates & -239.764 & -226.803 & -226.791 & ",
    "+    the benchmark results reported by neal ( 1998 ) , based on @xmath216 draws from the prior distributions , are also included in table [ tab3 ] ; the corresponding standard errors are 0.005 for the 2 component equal - variance model , 0.040 for the three component equal - variance model and 0.089 for the three component unequal - variance model .",
    "it is obvious , that the simple estimator @xmath211 results in biased estimates , which are very similar to the ones presented in chib ( 1995 ) ; see neal ( 1998 ) for the `` typo - corrected '' estimate of chib for the @xmath217 model . on the other hand , as reflected in the bias - corrected estimator @xmath212 , simply adding the term @xmath213 results in accurate marginal likelihood estimates which are in agreement with the estimates of neal .",
    "interestingly , the mc errors of @xmath212 are similar to the `` coefficients of variation '' in steele et al .",
    "( 2006 ) who handle marginal likelihood estimation through an incremental mixture importance sampling approach based on marginalization",
    ". nevertheless , steele et al .",
    "( 2006 ) adopt different prior assumptions and , therefore , their marginal likelihood estimates are not comparable to the ones in table 3 .",
    "histograms of posterior means for the three models from gibbs sampling.,height=302 ]    histograms of posterior means for the three models from random permutation sampling.,height=302 ]    histograms of posterior means from gibbs sampling are presented in figure [ fig1 ] . as seen , the gibbs sampler remains in one particular mode for the 2-component and 3-component equal variance models .",
    "this is not the case for the 3-component unequal variance model , where label - switching does actually occur for parameters @xmath218 and @xmath219 . for the 4-component model",
    "label - switching is noticeable for all posterior means . despite that fact",
    ", the bias - corrected estimator still performs well for these models .",
    "nevertheless , we would not warrant to guarantee that the bias - corrected estimator will always perform well , especially as the number of clusters gets larger and the posterior modes are not well separated .",
    "alternatively , one can simply use random permutation sampling and estimate the marginal likelihoods without the need to account for bias - correction .",
    "in addition , random permutation sampling will probably prove to be a more reliable solution for models with many components , since the marginal posteriors from random permutation sampling capture all possible modes .",
    "the estimates from @xmath215 are indeed very similar to neal s estimates and to the bias - corrected estimates .",
    "the mc errors are slightly higher for the random permutation estimates , nevertheless , this is understandable since ex - post random permutation artificially increases mcmc variability .",
    "histograms of posterior means from random permutation sampling are presented in figure [ fig2 ] ; the symmetries in the posterior distributions due to non - identifiability are now apparent . in accordance to the discussion in carlin and chib ( 1995 ) ,",
    "the histograms for the 4-component model show that only three modes are estimated efficiently as there is a significant overlap between the 2^nd^ and 3^rd^ mode .    in conclusion , both @xmath212 and @xmath215 yield satisfactory results . for models with a small number of components ( i.e. when label - switching is not likely to occur ) the bias - corrected estimator will most probably be sufficient . for more complicated models and",
    "when the two estimators result in estimates which are in disagreement , we would recommend to use either the correction for the candidate estimator proposed by marin and robert ( 2008 ) or the estimator based on alternative mcmc strategies ( e.g. frhwirth - schnatter , 2001 ; geweke , 2007 ) .      as a last example",
    ", we consider a data set taken from diggle et al .",
    "( 1995 ) , consisting of seizure counts @xmath220 from a group of epilepticts @xmath221 which is monitored initially over an 8-week baseline period @xmath222 and then over four subsequent 2-week periods @xmath223 . each patient is randomly assigned either a placebo or the drug progabide after the baseline period .",
    "this example is chosen mainly because standard gibbs sampling is not possible to implement for the model presented next .",
    "in addition , the epilepsy data is also considered by chib et al .",
    "( 1998 ) and chib and jeliazkov ( 2001 ) who present marginal likelihood estimates based on the candidate s formula and metropolis - hastings sampling .",
    "reduced posterior ordinates are calculated through kernel density estimation in chib et al .",
    "( 1998 ) , whereas chib and jeliazkov ( 2001 ) employ metropolis - hastings updating . for the sake of comparison",
    ", we adopt exactly the same modeling assumptions .",
    "the main model under consideration is @xmath224 where @xmath225 is the offset which equals 8 when @xmath226 and 2 otherwise , @xmath227 is an indicator of treatment ( 0 for placebo , 1 for progabide treatment ) , @xmath228 is an indicator of time period ( 0 for baseline , 1 otherwise ) and @xmath229 are latent random effects for @xmath230 ( subject 49 is removed from the analysis due to unusually high pre - and post - randomization seizure counts ) .",
    "the prior assumptions are bivariate normal distributions for @xmath231 and @xmath232 and a bivariate inverse - wishart for @xmath233 , namely @xmath234 , @xmath235 and @xmath236 , where @xmath237 is the @xmath238 identity matrix .",
    "the full conditionals of @xmath232 and @xmath233 are known , specifically we have that @xmath239 where @xmath240 and @xmath241 . the full conditionals for @xmath231 and the @xmath242 s",
    "are not known distributions and thus standard gibbs sampling is not feasible .",
    "another complication is that the integrated sampling likelihood @xmath243 , with @xmath244 , is also not available analytically .",
    "therefore , evaluating @xmath245 requires either numerical integration or some other efficient technique , such as importance sampling for instance .",
    "we utilize winbugs software ( spiegelhalter et al . , 2003 ) to sample from the posterior . specifically , one chain is iterated 31,000 times and the first 1000 iterations are discarded as burn - in , resulting in a final sample of 30,000 draws .",
    "posterior means and standard deviations , for the parameters of scientific interest , are presented in table [ tab4 ] .",
    "the estimates for the main model are comparable to the metropolis - hastings estimates presented in chib et al .",
    "table [ tab4 ] also includes the estimates for the simpler model without the random effects related to time . for this model",
    "we assume a - priori that @xmath246 , for @xmath230 . in order to keep equivalent prior assumptions to the main model",
    ", we define the priors as @xmath247 and @xmath248 .",
    "as discussed in section 2.3 , there are two approaches for estimating the marginal likelihood of this model .",
    "the first is to treat it as a 3-block setting , i.e. consider the product of the marginal posteriors of @xmath249 as importance sampling density . in this case",
    "one needs to estimate the integrated likelihood which is unknown .",
    "the second approach is to treat the problem as a 4-block setting , i.e. also include the joint marginal posterior of the @xmath242 s in the importance sampling density .",
    "the advantage with this approach is that we can work directly with the hierarchical poisson likelihood .",
    "initially , let us consider the first approach which corresponds to estimator ( 5 ) of section 2.3 ; in this case the parameters of scientific interest are @xmath250 , while @xmath251 is used only for rao - blackwellization .",
    "first , we appropriately re - order the posterior sample in order to correspond to a sample from the product marginal posterior and then we split the sample into @xmath201 batches of @xmath252 draws .",
    "the marginal likelihood estimate for each batch is calculated as @xmath253 marginal posterior probabilities for @xmath232 and @xmath233 are estimated via rao - blackwellization based on reduced posterior samples of @xmath254 draws which are randomly re - sampled from the initial mcmc sample , i.e. @xmath255=@xmath256 and @xmath257 . for the marginal posterior of @xmath231",
    "we assume that @xmath258 , where @xmath259 and @xmath260 are estimated from the mcmc output . similarly to chib et al .",
    "( 1998 ) and chib and jeliazkov ( 2001 ) , we employ further importance sampling to evaluate the likelihood @xmath245 .",
    "we employ multivariate normals as importance sampling functions , namely @xmath261 for the main model and @xmath262 for the reduced model , where @xmath263 are the vector of means and the complete covariance matrix of the random effects estimated from the mcmc output .",
    "likelihood estimation is based on 100 importance sampling draws .    based on the alternative 4-block approach ,",
    "corresponding to estimator ( 6 ) of section 2.3 , the batched marginal likelihood estimates are calculated as @xmath264.\\ ] ] with this approach , there is no need to implement further importance sampling for evaluating the likelihood function since the data conditional on the random effects parameters follow the poisson distribution . despite this convenient aspect ,",
    "the downside of the 4-block estimator is that it requires estimation of the high - dimensional joint marginal @xmath265 . given that the rao - blackwell device can not be used , we adopt a simple assumption and namely use the importance sampling functions used for likelihood evaluation in estimator @xmath266 , i.e. we assume that @xmath267 for the model with time effects and @xmath268 for the model not including time effects .",
    "lcccc & & & & & mean & st.dev .",
    "& mean & st.dev .",
    "constant @xmath269 & 1.065 & 0.146 & 1.095 & 0.138treatment @xmath270 & -0.0003 & 0.209 & -0.071 & 0.190time @xmath271 & 0.005 & 0.111 & - & -interaction @xmath272 & -0.349 & 0.156 & -0.191 & 0.052@xmath273 & 0.474 & 0.100 & 0.531 & 0.105@xmath274 & 0.017 & 0.057 & - & -@xmath275 & 0.243 & 0.063 & - & - +   +   + @xmath276 & & @xmath277 & & chib et al .",
    "( 1998 ) & & + chib & jeliazkov ( 2001 ) & & & +    average batch mean marginal likelihood estimates and mc errors for the two models in question are presented in table [ tab4 ] .",
    "the 3-block approach provides accurate estimates with mc errors being very low in comparison to the magnitude of the batched means .",
    "the 4-block estimates are in agreement with the 3-block estimates , but have higher monte carlo errors ; approximately four times higher than the mc errors of @xmath266 . nevertheless , this is expected due to the much larger augmented parameter space and the use of the normal approximation of the high - dimensional joint posterior of the random effects .",
    "overall , estimator @xmath278 is computationally less demanding than @xmath266 and the resulting mc errors , although higher than those of @xmath266 , are still relatively low in comparison to the batched mean marginal likelihood values .",
    "table [ tab4 ] also includes the estimates presented in chib et al .",
    "( 1998 ) and chib and jeliazkov ( 2001 ) from 10,000 posterior draws .",
    "both latter studies report standard errors , based on the variance estimator of newey and west ( 1987 ) , of approximately 0.1 for the main model . the reduced model is briefly considered in chib et al .",
    "( 1998 ) without reporting a standard error .",
    "our 3-block and 4-block estimates are comparable to those of chib et al .",
    "( 1998 ) and chib and jeliazkov ( 2001 ) but not in strict agreement .",
    "our experience from this particular example is that a long mcmc chain is needed in order to obtain accurate posterior estimates ; initial 3-block estimates based on 10,000 posterior draws were actually closer to the estimates of chib et al .",
    "( 1998 ) and chib and jeliazkov ( 2001 ) , namely -915.566 for the main model and -969.580 for the reduced model , but with considerably higher mc errors .",
    "l@lcc & & * mcmc * & * mc * + & & * length * & * error * + & + & @xmath279 ( @xmath280 ) & & + & & @xmath42 & 0.0023 + & & @xmath43 & 0.0018 + & & & ( 0.0016 ) + & & @xmath42 & 0.0030 + & & @xmath43 & 0.0021 + & & & ( 0.0021 ) + & & @xmath42 & 0.0030 + & & @xmath43 & 0.0023 + & & & ( 0.0021 ) + & & @xmath42 & 0.0033 + & & @xmath43 & 0.0026 + & & & ( 0.0023 ) + & + & @xmath281 ( @xmath282 ) & & + & & @xmath42 & 0.010 + & & @xmath43 & 0.007 + & & & ( 0.007 ) + & & @xmath42 & 0.018 + & & @xmath43 & 0.015 + & & & ( 0.013 ) + & & @xmath42 & 0.051 + & & @xmath43 & 0.039 + & & & ( 0.036 ) + & & @xmath42 & 0.060 + & & @xmath43 & 0.047 + & & & ( 0.042 ) + & + & @xmath283 ( @xmath284 ) & & + & & @xmath42 & 0.137 + & & @xmath43 & 0.105 + & & & ( 0.097 ) + & & @xmath42 & 0.063 + & & @xmath43 & 0.040 + & & & ( 0.045 ) +    here we briefly investigate the issue of finite variance for the illustrated examples presented in this section . following the discussion in section 2.6 , an informal empirical diagnostic for checking",
    "whether the variance is finite can be performed by comparing the mc errors from mcmc samples of different sizes . in general , increasing the mcmc sample by a factor of @xmath95 should lead to a decrease of mc errors by a factor of @xmath96 for estimators which have finite variance .",
    "table [ tab5 ] depicts the estimated mc errors for the rao - blackwell estimator for example 1 , the estimator based on random - permutation sampling for example 2 and the 4-block estimator for example 3 . in all cases , the mc errors from the original posterior samples of size @xmath42 are compared with the corresponding errors from samples of size @xmath43 and with the errors from the original samples scaled down by a factor of @xmath285 , which are the expected mc errors under the assumption of finite variance . from this table , it is evident that the mc errors from the mcmc runs with length equal to @xmath43 are roughly equal to the mc errors from the chains with length @xmath42 divided by @xmath285 , for all models , indicating that the variance of the corresponding estimators is finite .",
    "in this paper we have presented a method of marginal likelihood estimation based on utilizing the product marginal posterior as importance sampling density .",
    "the approach is in general straightforward to implement even for multi - block parameter settings as it is non - iterative and does not require adaptations in mcmc sampling . as illustrated , the estimator is accurate in capturing changes in the marginal likelihood due to different diffuse prior setups that do not affect the posterior distribution . for such cases ,",
    "the computational demands for estimating marginal likelihoods of competing models under diffuse priors are reduced significantly , since only one mcmc run is required . in general , the overall performance of the estimator depends on ; i ) the efficiency of approximating the joint posterior through independent univariate or multivariate marginals and ii ) the accuracy in estimating marginal posterior densities .",
    "arguably , the method can fail when the product of marginal posteriors is a poor approximation to the joint posterior .",
    "nevertheless , appropriate parameter blocking and reparameterizations can always improve the performance of the method , so that it will be feasible to work with a few parameter blocks that are close to orthogonal regardless whether the elements within the blocks are highly correlated . in the three , relatively diverse , examples handled in this paper the natural blocking of the parameters proved to be sufficient in delivering accurate estimates .",
    "it is worth noting that similar estimators based on importance sampling from independent posterior factorizations have shown to perform well ( botev et al . , 2012 ;",
    "chan and eisenstat , 2013 ) . moreover ,",
    "independent posterior factorization is also extensively used for the variational bayes ( bishop , 2006 ) and expectation - propagation ( minka , 2001 ) approaches in the maching - learning literature . as a last remark concerning this topic , ghosh and clyde ( 2011 )",
    "present a methodology for linear and binary regression models that augments non - orthogonal designs to obtain orthogonal designs based on gibbs sampling for the `` missing '' response variables . with some additional effort one could consider this orthogonalization approach which would guarantee an optimal importance sampling density , thus leaving estimation of univariate marginal posterior densities as the only remaining source of error .",
    "with respect to estimating marginal probabilities , the approach proposed here is particularly suited for gibbs sampling settings where rao - blackwellization can be used to obtain simulation - consistent marginal posterior density estimates .",
    "practically , the proposed estimator can get computationally demanding when using rao - blackwellization for the entire posterior sample .",
    "nevertheless , the related coding work basically requires averaging and is straighforward , without requiring any special effort in implementation or fine - tuning of parameters in trial and error runs .",
    "in addition , as illustrated in the examples , the sample needed for rao - blackwellization is substantially smaller than the total mcmc sample and is obtained once as a random sub - sample of the mcmc chain .",
    "the approach can also be applied under other types of mcmc schemes by adopting other strategies for estimating the marginal posterior densities such as normal approximations , fitting posterior moments , kernel methods and so forth . in strict theory , the method will not yield unbiased estimates when using such approximating strategies , nevertheless , in practical terms such approaches can often be sufficient and can lead to accurate estimates even for high dimensional multivariate approximations , as demonstrated in section 3.3 .",
    "in addition , the degree of bias can be checked indirectly by using the approximating densities as importance sampling densities .",
    "it is worth noting , that more elaborated strategies can also be considered , for instance the methods discussed in oh ( 1999 ) based on importance - weighted marginal density estimation ( chen , 1994 ) or the integrated nested laplace approximations ( inla s ) presented in rue et al .",
    "( 2009 ) .",
    "the advantage of not depending on the type of mcmc scheme used to sample from the posterior becomes obvious for classes of models like the finite normal mixtures , considered here , where conventional gibbs sampling fails to explore multi - modal posterior surfaces .",
    "this implies that the proposed method will also work well for models with similar posterior symmetries , based on alternative samplers ( e.g. frhwirth - schnatter , 2001 ; geweke , 2007 ) , without increased complexity in estimation .",
    "a possibly interesting extension of the idea presented here is to incorporate it within bridge - sampling estimation by using the product marginal posterior as approximating density .",
    "the authors would like to thank two anonymous referees for their interesting comments and suggestions .",
    "ardia , d. , batrk , n. , hoogerheide , l. and van dijk , h.k .",
    "( 2012 ) . a comparative study of monte carlo methods for efficient evaluation of marginal likelihood . _ computational statistics and data analysis _ , * 56 * , 33983414 .",
    "feroz , f. , hobson , m.p . and bridges , m. ( 2009 ) .",
    "multinest : an efficient and robust bayesian inference tool for cosmology and particle physics .",
    "_ monthly notices of the royal astronomical society _ , * 398 * , 16011614 .",
    "ghosh , j. and clyde , m.a .",
    "rao - blackwellization for bayesian variable selection and model averaging in linear and binary regression : a novel data augmentation approach .",
    "_ journal of the american statistical association _ , * 106 * , 10411052",
    ".    gilks , w.r . and roberts , g.o .",
    "( 1996 ) . strategies for improving mcmc . in",
    "_ markov chain monte carlo in practice _",
    ", 6 , eds .",
    "gilks , s. richardson and d.j .",
    "spiegelhalter , london : chapman & hall\\crc , pp .",
    "89114 .",
    "marin , j .-",
    "m . , mengersen , k. and robert , c. ( 2005 ) .",
    "bayesian modelling and inference on mixtures of distributions . in _",
    "handbook of statistics _",
    "25 , eds . c. rao and d. dey , new york",
    ": elsevier , pp . 459507 .",
    "parise , s. and welling , m. ( 2007 ) .",
    "bayesian model scoring in markov random fields . in _ advances in neural information processing systems",
    "_ , 19 , eds .",
    "b. schlkopf , j. platt and t. hoffman , cambridge , ma : mit press , pp .",
    "10731080 .",
    "raftery , a.e . ,",
    "newton , m.a . ,",
    "satagopan , j.m . and krivitsky , p.n .",
    "( 2007 ) . estimating the integrated likelihood via posterior simulation using the harmonic mean identity . in _",
    "bayesian statistics _ , 8 ,",
    "bernando , m.j .",
    "bayarri , j.o .",
    "berger , a.p .",
    "dawid , d. heckerman , a.f.m .",
    "smith and m. west , oxford , u.k .",
    ": oxford university press , pp . 145 .",
    "rue , h. , martino , s. and chopin , n. ( 2009 ) .",
    "approximate bayesian inference for latent gaussian models by using integrated nested laplace approximations ( with discussion ) . _ journal of the royal statistical society b _ , * 71 * , 31992 .",
    "spiegelhalter , d. , thomas , a. , best , n. and lunn , d. ( 2003 ) .",
    "_ winbugs user manual , version 1.4_. uk : mrc biostatistics unit , institute of public health and department of epidemiology and public health , imperial college school of medicine .",
    "available at http://www.mrc-bsu.cam.ac.uk/bugs/winbugs/manual14.pdf    steele , r.j . ,",
    "raftery , a.e . and",
    "emond , m.j .",
    "( 2006 ) . computing normalizing constants for finite mixture models via incremental mixture importance sampling .",
    "_ journal of computational and graphical statistics _ ,",
    "* 15 * , 712734 .          west , m. ( 1992 ) .",
    "modelling with mixtures ( with discussion ) . in _ bayesian statistics _ , 4 , eds .",
    "bernando , j.o .",
    "berger , a.p .",
    "dawid and a.f.m .",
    "smith , oxford , u.k .",
    ": oxford university press , pp . 503524 .",
    "zellner , a. ( 1986 ) . on assessing prior distributions and bayesian regression analysis with g - prior distributions .",
    "bayesian inference and decision techniques : essays in honour of bruno de finetti _ , eds .",
    "p. goel and a. zellner , amsterdam : north - holland , pp"
  ],
  "abstract_text": [
    "<S> we investigate the efficiency of a marginal likelihood estimator where the product of the marginal posterior distributions is used as an importance sampling function . </S>",
    "<S> the approach is generally applicable to multi - block parameter vector settings , does not require additional markov chain monte carlo ( mcmc ) sampling and is not dependent on the type of mcmc scheme used to sample from the posterior . </S>",
    "<S> the proposed approach is applied to normal regression models , finite normal mixtures and longitudinal poisson models , and leads to accurate marginal likelihood estimates .    _ </S>",
    "<S> keywords : _ finite normal mixtures , importance sampling , marginal posterior , marginal likelihood estimation , random effect models , rao - blackwellization </S>"
  ]
}