{
  "article_text": [
    "a concept that underlies the many recent developments in the area of compressed sensing is _",
    "sparsity_. much earlier , sparsity has been used in a similar way ( but often implicitly ) in theoretical mathematics , and most notably in geometric functional analysis .",
    "recently the understanding of the role of sparsity led to formalizing some of the connections between the statements in those areas , leading to a new interplay between `` pure '' and `` applied '' mathematics .    while applications of `` pure '' mathematics to compressed sensing are expected and indeed quite common , the reverse direction  from compressed sensing to mathematics  is still rarely seen .",
    "this paper discusses one such application to the problem of _ invertibility of random matrices _ , which has been addressed in particular in the papers of rudelson and the author @xcite , @xcite and @xcite .",
    "since we would like to focus here on techniques rather than results , we will often make oversimplifying assumptions and state weaker forms of the results .",
    "for the same reason , we discuss very little of history of these results and related work .",
    "the interested reader is encouraged to look at the original papers cited above for the statements of complete results , and for bibliography",
    ".    we will denote positive absolute constants by @xmath2 ; their values may change from line to line .",
    "one normally thinks of sparsity as a way to represent objects ( vectors or functions ) in a certain basis in an economical way  such that only a small number of basis elements can be used to accurately represent each object . in this discussion",
    ", we shall identify the basis with the canonical basis of @xmath3 , and our objects will be vectors in @xmath3 .",
    "we then say that a vector in @xmath3 is _",
    "@xmath4-sparse _ if it has few non - zero coordinates : @xmath5 we shall denote the set of all such vectors in @xmath3 by @xmath6 .",
    "this set clearly consists of the union of all @xmath4-dimensional subspaces of @xmath3 .",
    "an efficient way to use sparsity is through control of the metric entropy of the space @xmath6 .",
    "recall that , given a subset @xmath7 of a metric space and a number @xmath8 , the _ covering number _",
    "@xmath9 is the smallest cardinality of an @xmath10-net of @xmath7 , i.e. the smallest number of @xmath10-balls centered at points in @xmath7 needed to cover @xmath7 .",
    "the logarithm of the covering number is often called _",
    "metric entropy_.    a simple argument based on comparison of volumes leads to an exponential bound of the metric entropy of many natural subsets of @xmath3 , and in particular of the euclidean sphere @xmath11 , see e.g. lemma  9.5 in @xcite .",
    "this bound for the interesting range @xmath12 reads as @xmath13 this bound improves significantly for the set of sparse vectors .",
    "since there are @xmath14 ways to choose the support of a sparse vector , we have @xmath15 using along with the bound @xmath16 valid for @xmath17 , which follows from stirling s formula , we conclude with @xmath18 comparing this with , we see that sparse vectors enjoy significantly smaller entropy than the whole sphere ",
    "the covering number is essentially exponential in the sparsity @xmath4 rather than the dimension @xmath19 .",
    "this advantage is crucially used in many arguments , such as in the following one .    in compressed sensing , a basic quality of matrices that guarantees",
    "their good performance as measurement operators is the _ restricted isometry condition_. an @xmath20 matrix @xmath21 with @xmath22 is said to satisfy the restricted isometry condition ( ric ) if @xmath21 acts as an approximate isometry when restricted to the set of sparse vectors . formally , for every integer @xmath23 we define the ric constant @xmath24 of the matrix @xmath21 as the minimal number that satisfies the two - sided inequality @xmath25 for all @xmath26 .",
    "candes and tao @xcite have shown ( with constant improved in @xcite ) that , given a matrix @xmath21 with @xmath27 , one can exactly recover every @xmath4-sparse vector @xmath28 from its `` measurement vector '' @xmath29 by solving the convex optimization problem @xmath30    while it is difficult to explicitly construct matrices with good dimensions and ric constants , random constructions are abundant in the literature ( see section v in @xcite ) . here",
    "we sketch the known argument for gaussian matrices , which will highlight sparsity as entropy control , and will lead to our discussion of more difficult questions in random matrix theory .",
    "[ gaussian ric ] let @xmath31 be an @xmath20 matrix whose entries are independent standard normal random variables .",
    "let @xmath32 and @xmath33 . if @xmath34 then , with high probability , the matrix @xmath35 satisfies ric with constant @xmath36 . here",
    "@xmath37 only depends on @xmath38 .",
    "( sketch ) an approximation argument shows that it is enough to check for all @xmath28 in any fixed @xmath38-net of @xmath39 .",
    "so we choose such a net @xmath40 of cardinality controlled as in , and we fix a vector @xmath41 . due to independence of the rows of @xmath21 and the rotation invariance of the normal distribution , the random variable @xmath42 is distributed identically with @xmath43 , where @xmath44 are independent standard normal random variables . by the known concentration properties of the @xmath45 distribution , or alternatively by the standard exponential concentration inequalities",
    ", one has @xmath46 with probability at least @xmath47 . in other words , with this probability , the restricted isometry condition holds for a fixed vector @xmath41 . taking the union bound and using the bound on the cardinality of the net , we see that holds for _ all _ vectors @xmath41 with probability at least @xmath48 by the condition we made on the dimensions , the proof is complete",
    ".    the argument above can be easily generalized to distributions other than normal by using standard exponential concentration inequalities ; suitable moment bounds ( subgaussian ) are sufficient for this purpose .",
    "one can view the restricted isometry condition as the condition that all submatrices of @xmath21 with a given number of columns are well conditioned .",
    "the question of how well conditioned random matrices are goes back to at least von neumann and his collaborators , in connection with their work on large matrix inversion .",
    "some history of the work on this problem is described in @xcite and @xcite , and some new results appeared since then , see @xcite . here",
    "we shall focus on the original prediction going back to von neumann and his group  that the smallest singular value @xmath49 of an @xmath1 matrix with random independent centered entries is typically of order @xmath50 .",
    "coupled with the known estimate on the largest singular value @xmath51 ( valid under suitable moment assumptions ) , the prediction implies that the condition number @xmath52 , i.e. is typically linear in the dimension .",
    "this prediction was verified for gaussian matrices in @xcite using the explicit formula for the joint density of their eigenvalues , and was first proved for general random matrices in @xcite under some mild moment assumptions .",
    "ideas based on sparsity play an important role in @xcite .",
    "we will discuss this role the in the rest of the paper , and sketch the proof of the prediction above :    [ square ] let @xmath21 be an @xmath1 matrix whose entries are independent identically distributed random variables with mean zero , unit variance , and fourth moment bounded by a constant .",
    "then the median of @xmath49 is bounded below by @xmath53 .",
    "note that the result is sharp  it was proved in @xcite that the median of @xmath49 is bounded above by @xmath54 .",
    "our plan is to first prove theorem  [ square ] for gaussian matrices @xmath21 ( whose all entries are standard normal random variables ) , and then to indicate how to modify the proof for general distributions .",
    "the smallest singular value has the following convenient expression : @xmath55 our goal is then to bound @xmath56 below uniformly for all unit vectors @xmath28 .",
    "we already know how to achieve this goal for all _ sparse _",
    "vectors @xmath28 .",
    "indeed , by proposition  [ gaussian ric ] the gaussian matrices satisfy the restricted isometry property .",
    "if we choose @xmath57 and @xmath58 with sufficiently small absolute constant @xmath59 , proposition  [ gaussian ric ] shows that , with high probability , @xmath60 note that this bound is much better than we need in theorem  [ square ]  we would be happy with @xmath53 in the right hand side .",
    "now we need to handle the non - sparse vectors .",
    "our success with sparse vectors is due the fact that there are `` not too many '' of them .",
    "as we have seen by comparing to , the metric entropy of the set of sparse vectors is much smaller than that of all vectors .",
    "such a nice entropy control allowed us to handle all sparse vectors by taking a union bound ( in the proof of proposition  [ gaussian ric ] ) without paying too much price in the probability estimates .",
    "repeating a similar argument for non - sparse vectors is hopeless , as they lack a nice entropy control .",
    "instead , we could first try to identify the class of vectors which is entirely opposite to the sparse vectors , and try to handle this class . these are _ spread vectors _  those vectors in @xmath11 whose all coordinates have the same order @xmath50 .",
    "an advantage of spread vectors over sparse ones is that we know the magnitude of all their coefficients .",
    "so we develop the following _ geometric _ argument to prove the invertibility on the set of spread vectors .",
    "let us begin with a qualitative argument .",
    "suppose the matrix @xmath21 performs extremely poor , and we have @xmath61 ; in other words , @xmath21 is a singular matrix .",
    "therefore one of its columns @xmath62 of @xmath21 lies in the span @xmath63 of the others .",
    "this simple observation can be made into a quantitative argument , which will work very well with the spread vectors .",
    "suppose @xmath64 is a spread vector .",
    "then , for every @xmath65 , we have @xmath66 since the left hand side does not depend on @xmath28 , we have proved in particular ( for @xmath67 ) that @xmath68    it remains to estimate the distance between the random vector @xmath69 and the independent random hyperplane @xmath70 . since @xmath69 is a gaussian vector , it is easy to check ( using the rotation invariance of the gaussian disstibution ) that @xmath71 is distributed identically with the absolute value of a standard normal random variable @xmath72 .",
    "but the density of @xmath72 is bounded by the absolute constant @xmath73 , which makes @xmath74 with arbitrarily high constant probability ( say , @xmath75 ) .",
    "we have thus shown that , with arbitrarily high constant probability , @xmath76 this is a desired uniform bound for the spread vectors .",
    "there are of course many vectors that are neither sparse nor spread , but it will now be relatively easy to bridge these two classes .    consider all vectors in @xmath11 that are within a small absolute constant distance @xmath77 from the set of sparse vectors @xmath78 .",
    "we shall call such vectors _ compressible _ , and the rest of the vectors on the sphere are _ incompressible_. the intuition , which again is coming from sparse recovery , suggests that compressible vectors should behave similarly to sparse vectors , while incompressible vectors should be similar to spread vectors .",
    "indeed , a trivial approximation argument extends our invertibility bound from sparse to compressible vectors ( one just need to approximate a compressible vector by a sparse one and use that the error of this approximation @xmath79 can only blow up by a factor @xmath80 ) .",
    "so we have the desired bound @xmath81    for incompressible vectors , instead of an approximation argument ( which wo nt work ) one makes the following simple observation : every incompressible vector has @xmath82 coordinates of magnitude @xmath0 .",
    "this is a way how incompressible vectors are similar to spread ones .    to complete the proof for incompressible vectors",
    ", we again use the geometric argument , but stop just before the last estimate in : @xmath83 as we already know , for each @xmath65 , the distance satisfies @xmath84 with arbitrarily large probability .",
    "therefore , still with high probability , most of these distances ( arbitrarily high constant proportion of them ) are @xmath85 . on the other hand , we also know that some fixed proportion of the coordinates @xmath86 are of magnitude @xmath0 .",
    "therefore , intersecting these two events , we see that for any incompressible vector @xmath28 there exists a coordinate @xmath87 that satisfies both bounds .",
    "this implies that , with high probability , @xmath88 this is a desired bound which , along with the already proved estimate for compressible vectors , implies the final result : @xmath89",
    "the above argument generalizes from gaussian to general distributions .",
    "there are two places where we used rotation invariance of the gaussian distribution .",
    "one such place was the use of proposition  [ gaussian ric ] in the treatment of sparse vectors .",
    "as we already mentioned , proposition  [ gaussian ric ] can be easily extended to more general distributions using the standard large deviation inequalities .",
    "the other place where gaussian distribution was used was in the argument for spread vectors .",
    "we argued there that the distance @xmath71 between a random vector and a random independent hyperplane is @xmath85 with arbitrarily high probability . for gaussian distribution of the entries ,",
    "this followed by a direct and easy computation . for more general distributions ,",
    "this estimate is still true , but it requires more work .",
    "let us condition on a realization of the hyperplane @xmath70 , and let @xmath90 be a normal vector of @xmath70 .",
    "then clearly @xmath91 writing this is coordinates for @xmath92 and @xmath93 we see that @xmath94 where @xmath7 is clearly a sum of independent random variables .",
    "our goal is to show that @xmath95 with high probability .",
    "one way to do this is to use a central limit theorem ( in the form of berry - esseen ) to approximate @xmath7 by a standard normal random variable @xmath72 , for which we already have the desired result . for the the central limit theorem to work",
    ", one obviously needs that many coordinates of @xmath96 are not too small ( for example , it will clearly not work if @xmath96 is @xmath97-sparse , as the sum @xmath7 will consist of just one term ) .",
    "however , sparsity ideas can be again of help here . running an argument similar to the one above for compressible vectors",
    ", one can show that , with high probability , the normal @xmath96 to the random hyperplane @xmath70 is incompressible .",
    "we then condition on such @xmath70 , and the central limit theorem works well : @xmath98 this proves the desired bound @xmath99 with high probability @xmath100 .    in @xcite ,",
    "theorem  [ square ] was extended to _ rectangular matrices _",
    "@xmath101 , where @xmath102 . under the same assumptions , the median of the smallest singular value @xmath103 of such random matrices",
    "is bounded below by @xmath104 , which is asymptotically optimal .",
    "note that for square matrices , where @xmath105 , this bound equals @xmath106 , which agrees with theorem  [ square ] .    under stronger moment assumptions on the entries ( subgaussian ) , not only the median of the smallest singular value can be estimated , but also strong probability inequalities can be proved .",
    "for example , square matrices satisfy @xmath107 and rectangular matrices satisfy @xmath108 for all @xmath109 .",
    "proving such exponential inequalities is more difficult , because one can not afford a polynomial error in probability @xmath110 which one necessarily obtains when applying central limit theorem in . instead of using central limit theorems , one develops a littlewood - offord theory , whose probability estimates are fine - tuned to the additive structure of the coefficients of @xmath96 . since the sparsity does not play a key role in these arguments , we will not discuss this direction here .",
    "the interested reader is encouraged to consult the papers @xcite .",
    "this research was partially supported by nsf frg grant dms 0918623 ."
  ],
  "abstract_text": [
    "<S> we discuss applications of some concepts of compressed sensing in the recent work on invertibility of random matrices due to rudelson and the author . </S>",
    "<S> we sketch an argument leading to the optimal bound @xmath0 on the median of the smallest singular value of an @xmath1 matrix with random independent entries . </S>",
    "<S> we highlight the parts of the argument where sparsity ideas played a key role . </S>"
  ]
}