{
  "article_text": [
    "information - theoretic arguments often involve random coding techniques to prove the existence of good codes .",
    "usually , the codebook is constructed by randomly choosing the components of each codeword independently and identically from a judiciously chosen probability distribution . while this technique is powerful , the resulting codebooks do not exhibit any structure that may be of practical interest .",
    "one such desirable structure is linearity , which allows complexity reductions at the encoder and decoder by utilizing efficient algebraic processing techniques .",
    "further , in certain communication scenarios linear codes achieve higher rates than random code ensembles , as was shown by krner and marton  @xcite for a distributed source coding problem .",
    "structured coding schemes have been widely studied in the literature , especially for communications in the presence of side information and in multi - terminal networks . for an overview of structured coding schemes we refer the reader to  @xcite and the references therein .    for communication in the wireless domain ,",
    "structured codes can be obtained by choosing finite subsets of points from lattices  @xcite .",
    "a lattice is an infinite discrete set of points in the euclidean space that are regularly arranged and are closed under addition .",
    "codes based on lattices , known as _ ( nested ) lattice codes _ or _ voronoi codes _ , are the analogues of linear codes in wireless communications .",
    "efficient lattice based strategies are known for a variety of communication scenarios , such as for achieving the capacity of the point - to - point additive white gaussian noise ( awgn ) channel  @xcite , for dirty - paper coding  @xcite , the wyner  ziv problem  @xcite and communication in relay networks  @xcite , to name only a few .",
    "in this paper we present capacity - achieving lattice strategies for communication in gaussian broadcast channels where receivers have prior side information about the messages being transmitted . in particular , we assume that the transmitter is broadcasting @xmath4 message symbols @xmath5 from a finite field @xmath6 , of prime size @xmath0 , to all the receivers , and each receiver may have _ coded side information _ about the messages : the prior knowledge of the values of ( possibly multiple ) @xmath6-linear combinations of @xmath5",
    ". the number of linear combinations available as side information and the coefficients of these linear combinations can differ from one receiver to the next .",
    "the capacity of this channel follows from the results of tuncel  @xcite , where the achievability part requires the use of random gaussian codebooks .",
    "the broadcast channel with coded side information is an instance of a noisy wireless version of _ index coding _",
    "index coding considers a noiseless broadcast link where each receiver demands a subset of the source messages and knows the values of some other subset as side information .",
    "the modification of the ( noiseless ) index coding problem in which the receivers have access to linear combinations of messages was studied recently in  @xcite .",
    "the case of gaussian broadcast channel with coded side information is motivated by applications to multi - terminal communication networks .",
    "it is known that signal interference in wireless channels can be harnessed by decoding linear combinations of transmit messages instead of either treating interference as noise or decoding interference along with the intended message  @xcite .",
    "when such a technique is used in a mutli - hop communication protocol , one encounters receivers that have coded side information obtained from transmissions in the previous phases .",
    "similarly , in a network that consists of both wired and wireless channels , the symbols received from wired links can be utilized as side information for decoding the wireless signals . if a linear network code is used in the wired part of the network , then the side information is in the form of linear combinations of the source messages .",
    "[ ex : relay_network ]    consider a wireless network with two base stations @xmath7 and @xmath8 , that hold message symbols @xmath9 and @xmath10 , respectively .",
    "the base stations are required to broadcast @xmath9 and @xmath10 to four user nodes @xmath11 through the relay node @xmath12 , see fig .",
    "[ fig : relay_network ] . in the first phase of the protocol , @xmath7 and @xmath8",
    "encode the data symbols @xmath9 and @xmath10 , and transmit the resulting codewords simultaneously . by using the decoding technique of compute - and - forward  @xcite",
    ", @xmath13 reliably decodes some linear combination @xmath14 , @xmath15 , from the received noisy superposition of the two transmit signals . on the other hand , @xmath12 has a higher signal - to - noise ratio and successfully decodes both @xmath9 and @xmath10 by behaving as a multiple - access receiver .",
    "further , there is no signal interference at @xmath16 and @xmath17 , and these two nodes reliably decode @xmath9 and @xmath10 , respectively .",
    "we observe that the second phase of the protocol is a broadcast channel with coded side information at the receivers : the relay needs to jointly broadcast @xmath18 to four user nodes , the first three users @xmath19 have prior knowledge of the linear combinations @xmath20 , @xmath21 and @xmath14 , respectively , while the fourth user has no such side information .",
    "[ ex : overlay ]     encodes @xmath5 and broadcasts wirelessly to the destination nodes @xmath22 in order to supplement the communication through the wired network .",
    "each destination receives ( possibly multiple ) @xmath6-linear combinations of the source symbols through a linear ( wireline ) network code and uses this information to decode the wireless signal broadcast by @xmath23.,width=288 ]    assume a network of noiseless wired links in the form of a directed acyclic graph , where the source node @xmath23 desires to multicast @xmath4 independent messages @xmath24 to a set of destination nodes @xmath25 .",
    "the wireline network employs a traditional _ ( scalar ) linear network code _",
    "@xcite , i.e. , the symbol transmitted on each outgoing edge of a node is an element of @xmath6 generated as a linear combination of the symbols received on its incoming edges . at every destination node @xmath26 ,",
    "the decoder attempts to recover the @xmath4 message symbols from their @xmath6-linear combinations received on its incoming edges .",
    "recovery is possible if and only if the number of linearly independent equations available at @xmath27 is @xmath4 .",
    "it is known that the maximum number of linearly - independent equations that can be made available at @xmath27 is @xmath28 , where @xmath29 is the maximum number of edge - disjoint paths from @xmath23 to @xmath27 , see  @xcite .",
    "it follows that multicasting is possible if and only if @xmath30 for every @xmath26 .",
    "now suppose there exist destination nodes with @xmath31 less than @xmath4 , i.e. , the communication demands are beyond the wireline network s capacity .",
    "a solution to meet the demands is to broadcast a wireless signal from the source to fill the capacity deficiency of the wired network , see fig .",
    "[ fig : overlay ] . at each destination , the @xmath6-linear combinations obtained from the wireline network serve as side information to decode the wireless broadcast signal .",
    "a special case of coded side information is the gaussian broadcast channel where each receiver has prior knowledge of the values of some subset of the @xmath4 messages .",
    "the known capacity - achieving coding schemes for this special case are based on random coding using i.i.d .",
    "( independent and identically distributed ) codewords  @xcite .",
    "constructions of binary codes for this channel were proposed in  @xcite , explicit codes based on lattices were proposed in  @xcite and codes based on quadrature amplitude modulation were constructed in  @xcite .",
    "the objective of this paper is to prove that lattice codes achieve the capacity of gaussian broadcast channels with coded side information .",
    "we use the information - theoretic framework set by erez and zamir  @xcite to this end .",
    "the proposed coding scheme uses an ensemble of construction  a lattices and the decoding scheme involves _ algebraic binning _",
    "@xcite where the receiver side information is used to expurgate the channel code and obtain a lower rate subcode .",
    "the set of linear equations available as side information may differ from one receiver to another , and hence , each receiver must employ a different binning scheme for the same channel code .",
    "the coding scheme ensures that the binning performed at each receiver produces a good lattice subcode of the transmitted code .",
    "following expurgation , each receiver decodes the channel output by minimum mean square error ( mmse ) scaling and quantization to an infinite lattice .",
    "the algebraic structure of the coding scheme facilitates the performance analysis by decomposing the original channel into multiple independent point - to - point awgn channels  one corresponding to each receiver  where each of the point - to - point awgn channels uses a lattice code for communication . unlike  @xcite , where achievability in a point - to - point awgn channel was proved using error exponent analysis",
    ", we provide a direct proof based only on simple counting arguments .    as a corollary to the main result",
    ", we obtain an alternative proof of the goodness of lattice codes in achieving the capacity of the point - to - point awgn channel .",
    "previous proofs of this result presented in  @xcite also use ensembles of lattices obtained by applying construction  a to random linear codes over a prime field @xmath6 ; see also  @xcite .",
    "while  @xcite used primes @xmath0 that were exponential in the code length @xmath1 ,  @xcite and  @xcite improved this result to let @xmath0 grow as @xmath32 and @xmath33 , respectively .",
    "the corollary presented in this paper further improves these results by enabling a rate of growth of @xmath2 , for any choice of @xmath3 .",
    "the organization of this paper is as follows .",
    "we introduce the channel model in section  [ sec : channel_model ] and review the relevant background on lattices and lattice codes in section  [ sec : lattice_prelim ] . in section  [ sec:3 ] , we state the main theorem , and describe the lattice code ensemble and encoding and decoding procedures .",
    "we prove the main theorem and state a few corollaries in section  [ sec : proof_of_main ] , and finally , we discuss some concluding remarks in section  [ sec : conclusion ] .    _",
    "notation : _ matrices and column vectors are denoted by bold upper and lower case letters , respectively .",
    "the symbol @xmath34 denotes the euclidean norm of a vector , and @xmath35 is the transpose of a matrix or a vector .",
    "the kronecker product of two matrices @xmath36 and @xmath37 is @xmath38 , @xmath39 is the @xmath40 identity matrix , and @xmath41 is the all zero matrix of appropriate dimension .",
    "the symbol @xmath42 denotes logarithm to the base @xmath43 and @xmath44 denotes logarithm to the base @xmath45 .",
    "the expectation operator is denoted by @xmath46 .",
    "the symbol @xmath47 denotes the elements in the set @xmath48 that do not belong to the set @xmath49 .",
    "we consider a ( non - fading ) gaussian broadcast channel with a single transmitter and finitely many receivers , where all terminals are equipped with single antennas .",
    "the transmitter operates under an average power constraint and the receivers are affected by additive white gaussian noise with possibly different noise powers .",
    "there are @xmath4 independent messages @xmath5 at the transmitter that assume values with a uniform probability distribution from a prime finite field @xmath6 .",
    "each receiver desires to decode all the @xmath4 messages while having prior knowledge of the values of some @xmath6-linear combinations of the messages @xmath5 .",
    "consider a generic receiver that has access to the values @xmath50 , @xmath51 , of the following set of @xmath52 linear equations @xmath53 we will denote this side information configuration using the matrix @xmath54 \\in { \\mathbb{f}_p}^{m \\times k}$ ] , where each row of @xmath55 represents one linear equation .",
    "any row of @xmath55 that is linearly dependent on the other rows represents redundant information and can be discarded with no loss to the receiver side information , and hence , with no loss to system performance . hence , without loss in generality , we will assume that the rows of @xmath55 are linearly independent over @xmath6 , i.e. , @xmath56 , and @xmath57 .",
    "note that the values of @xmath55 and @xmath52 can be different across the receivers .",
    "a receiver with no side information is represented with an empty matrix for @xmath55 ( with @xmath58 ) .    a receiver in the broadcast channel is completely characterized by its _ ( coded ) side information matrix _ @xmath55 and the variance @xmath59 of the additive noise .",
    "if we assume that the average transmit power at the source is @xmath60 , then the signal - to - noise ratio at this receiver is @xmath61 .",
    "we will denote a receiver by the pair @xmath62 , where @xmath55 is any matrix over @xmath6 with @xmath4 columns and linearly independent rows , and @xmath63 .",
    "note that _ uncoded _ side information , i.e. , the prior knowledge of the values of a size @xmath52 subset of @xmath5 , is a special case , and hence , is contained within the definition of our channel model .    consider a source transmitting @xmath64 symbols , @xmath65 , from the finite field @xmath66 .",
    "a receiver that has prior knowledge of the value of @xmath10 has side information matrix @xmath67 this corresponds to the equation @xmath68 , and the number of linearly independent equations at this receiver is @xmath69 .",
    "now consider another receiver that has the knowledge of the values of the following three equations : @xmath70 , @xmath71 and @xmath72 . in matrix form , this side information is represented by @xmath73 where the three rows represent the three equations , in that order .",
    "the first row of this matrix is equal to the sum ( over @xmath74 ) of the second and third rows , and hence , the side information from the first equation is redundant and can be discarded .",
    "since the remaining two rows are linearly independent , the side information at this receiver can be represented by the following matrix that consists of these two rows , @xmath75 the number of linearly independent equations at this receiver is @xmath76 .    from elementary linear algebra we know that if the values @xmath50 of @xmath52 linearly independent combinations of the variables @xmath5 are given , then the set of all possible solutions of @xmath77 is a coset of a @xmath78 dimensional linear subspace of @xmath79 . since the apriori probability distribution of @xmath5 is uniform ,",
    "we conclude that , given the side information values @xmath50 , @xmath51 , the probability distribution of @xmath77 is uniform over this coset .",
    "using the fact that the number of elements in the coset is @xmath80 , we observe that the conditional entropy of @xmath77 given the side information is @xmath81 suppose we want to transmit , on the average , one realization of @xmath77 in every @xmath82 uses of the broadcast channel .",
    "the transmission rate of each message is @xmath83  b / dim ( bits per real dimension or bits per channel use ) .",
    "the information - theoretic result of  @xcite , which is based on the average performance of random gaussian codebooks , shows that a transmission rate of @xmath84 is achievable if and only if the following condition is satisfied at every receiver @xmath62 of the broadcast channel , @xmath85 or equivalently , @xmath86 where @xmath87 . for the simplicity of exposition",
    ", we consider only the symmetric case where all the @xmath4 messages are required to be transmitted at the same rate @xmath84 .",
    "the general scenario , where the messages are of different rates , can be reduced to the symmetric case through _ rate - splitting _",
    ": if there are @xmath88 messages with transmission rates @xmath89 , respectively , then by splitting each of these original sources into multiple virtual sources , one can generate a set of @xmath4 sources ( @xmath90 ) such that their rates @xmath91 are as close to each other as required .",
    "we will assume that the encoding at the transmitter is performed on a block of @xmath92 independent realizations of the @xmath4 message symbols , i.e. , the source jointly encodes @xmath4 message vectors @xmath93 .",
    "the transmitter uses an @xmath1-dimensional channel code @xmath94 together with a function @xmath95 to jointly encode the @xmath4 message vectors .",
    "the number of codewords in @xmath96 is @xmath97 , and we will assume that the codebook @xmath96 satisfies the per - codeword power constraint @xmath98 the average number of channel uses to transmit each realization of @xmath77 is @xmath99 .",
    "the resulting rate of transmission of each of the @xmath4 messages is @xmath100 the sum rate of all the messages is @xmath101  b / dim",
    ".     of the broadcast channel uses its own side information @xmath102 to expurgate the channel code @xmath96 and obtain a subcode @xmath103 .",
    "note that the resulting subcodes can be different across the receivers . in order to achieve the capacity of the broadcast channel , we require that each of these expurgated codes be good for channel coding over the point - to - point awgn channel .",
    ", width=480 ]    the side information at @xmath62 over a block of @xmath92 realizations of the @xmath4 message symbols is of the form @xmath104 , @xmath51 , where @xmath87 and @xmath105 .",
    "this side information allows the receiver to conclude that the transmitted codeword must belong to the following subcode of @xmath96 , @xmath106 the optimal decoder at @xmath62 decodes the channel output vector to the nearest codeword @xmath107 of this subcode , and the error probability at this receiver is the probability that the estimated message tuple @xmath108 is not equal to the transmit message @xmath109 . in order to achieve the optimal performance at a given receiver @xmath62",
    ", we thus require that the expurgated code @xmath103 be a good channel code for the point - to - point awgn channel . in a broadcast channel with multiple receivers ,",
    "the side information matrix @xmath55 can vary from one receiver to the next , and hence , the expurgated codes can be different at each receiver , see fig .",
    "[ fig : expurg ] .",
    "hence , a capacity - achieving broadcast channel code @xmath96 is such that the resulting expurgated code at every receiver is a good channel code for the awgn channel .",
    "we now briefly review the necessary background on lattices and lattice codes , and establish our notation and terminology .",
    "the material presented in this section consists of standard ingredients used in the literature , and is mainly based on  @xcite .      a _ lattice _ @xmath110 in @xmath1 dimensions is a discrete additive subgroup of @xmath111 .",
    "we will only consider lattices that have full - rank , i.e. , @xmath1-dimensional lattices of the form @xmath112 , where @xmath113 is known as the _ generator matrix _ of the lattice @xmath110 , and @xmath37 is of rank @xmath1 .",
    "the _ lattice quantizer _ corresponding to @xmath110 is the function @xmath114 given by @xmath115 where ties between competing lattice points @xmath116 are broken in a systematic way .",
    "the _ voronoi region _",
    "@xmath117 of the lattice @xmath110 is the set of all points in @xmath111 that are mapped to @xmath118 by the function @xmath119 .",
    "the volume of the voronoi region , i.e. , @xmath120 , is denoted by @xmath121 . for any @xmath122",
    ", @xmath123 is the set of all points in @xmath111 that are mapped to @xmath116 under @xmath119 , and it has the same volume @xmath121 as @xmath117 .",
    "for any two distinct lattice points @xmath124 , the sets @xmath125 and @xmath126 are disjoint .",
    "the _ modulo_-@xmath110 operation on @xmath127 is defined as @xmath128 { { \\rm~mod~}}{\\lambda}=\\pmb{x}-q_{{\\lambda}}(\\pmb{x})$ ] , and denotes the error when @xmath129 is quantized to the nearest point in @xmath110 .",
    "the following identities are relevant @xmath130 { { \\rm~mod~}}{\\lambda}&\\in { \\mathcal{v}}({\\lambda } ) \\text { for all } { \\pmb}{x } \\in { \\mathbb{r}}^n , \\nonumber \\\\ [ { \\pmb}{x}_1 + { \\pmb}{x}_2 ] { { \\rm~mod~}}{\\lambda}&= \\big [   \\ , [ { \\pmb}{x}_1 ] { { \\rm~mod~}}{\\lambda}+ { \\pmb}{x}_2 \\big ] { { \\rm~mod~}}{\\lambda}\\text { for all } { \\pmb}{x}_1,{\\pmb}{x}_2 \\in { \\mathbb{r}}^n , \\text { and } \\label{eq : mod_is_distributive } \\\\ [ { \\pmb}{x } ] { { \\rm~mod~}}{\\lambda}&= { \\pmb}{0 } \\text { if and only if } { \\pmb}{x } \\in { \\lambda}. \\label{eq : when_is_mod_zero}\\end{aligned}\\ ] ]    we will denote the @xmath1-dimensional ball of radius @xmath131 with center @xmath132 as @xmath133 , i.e. @xmath134 we will denote the volume of a unit - radius ball in @xmath1 dimensions by @xmath135 .",
    "it follows that the volume of @xmath133 equals @xmath136 .",
    "the _ covering radius _",
    "@xmath137 of the lattice @xmath110 is the radius of the smallest @xmath1-dimensional ball that contains @xmath117 , and it satisfies @xmath138 the _ effective radius _",
    "@xmath139 of @xmath110 is the radius of the @xmath1-dimensional ball whose volume equals @xmath121 , and is given by @xmath140 evidently , @xmath141 for any lattice .",
    "rogers  @xcite showed that for every dimension @xmath1 there exists a lattice @xmath110 such that @xmath142 where @xmath143 is a constant .",
    "note that the right hand side of the above inequality converges to @xmath60 as @xmath144 . a sequence of lattices of increasing dimension @xmath1",
    "is said to be _",
    "rogers - good _ if @xmath145 .",
    "rogers result   shows that such a sequence exists ( see also  @xcite ) .",
    "let @xmath146 and @xmath110 be two lattices such that @xmath147 . then @xmath146 is a _ sub - lattice _ of @xmath110 and forms a additive subgroup of @xmath110 .",
    "the index of @xmath146 in @xmath110 , i.e. , the size of the quotient group @xmath148 is given by @xmath149 each coset of @xmath146 in @xmath110 is represented by its unique coset leader in @xmath150 .",
    "hence , the additive group @xmath148 can be identified with the set @xmath151 , where the addition operation is performed modulo-@xmath146 .",
    "let @xmath152 be a fixed vector .",
    "a _ ( nested ) lattice code _ or a _ voronoi code _",
    "@xmath153 is the set @xmath154 obtained by applying the @xmath155 operation on the points of the lattice translate @xmath156 .",
    "the code consists of all the points in @xmath156 that lie within the voronoi region of @xmath146 , i.e. , @xmath157 .",
    "the lattice @xmath146 is called the _ coarse lattice _ or the _ shaping lattice _ , @xmath110 is called the _ fine lattice _ or the _ coding lattice _ , and @xmath158 is the _ dither _ vector .",
    "the cardinality of this code is @xmath159 , and every codeword point @xmath160 satisfies @xmath161 .",
    "note that @xmath148 is a lattice code with zero dither .      in this subsection",
    "we briefly describe the method proposed in  @xcite to construct a pair @xmath147 of nested lattices , and recall its relevant properties .",
    "this construction uses a coarse lattice @xmath146 and a linear code @xmath162 to generate a fine lattice @xmath110 such that @xmath163 .",
    "let @xmath164 denote the natural map that embeds @xmath165 into @xmath166 . when applied to vectors , @xmath164 acts independently on each component of a vector .",
    "let @xmath167 be a linear code of rank @xmath168 , @xmath169 , @xmath170 where @xmath171 is the @xmath172 generator matrix with full column rank , and @xmath173 is the message encoded to @xmath162 .",
    "the set @xmath174 obtained by tiling copies of @xmath175 at every vector of @xmath176 is a lattice in @xmath111 and is known as the _ construction  a _ lattice of the linear code @xmath162  @xcite .",
    "note that the number of points in @xmath177 contained in the voronoi region of the lattice @xmath176 is @xmath178 .",
    "we obtain @xmath110 by scaling down the construction  a lattice by @xmath179 and transforming it by the generator matrix @xmath180 of @xmath146 @xmath181 since @xmath162 contains the all zero codeword , it follows that @xmath182 . we observe that applying the transformation @xmath183 to the lattice @xmath176 ( instead of the lattice @xmath177 ) generates @xmath146 ( instead of @xmath110 ) .",
    "hence , @xmath148 has the same algebraic structure as that of @xmath184 , which in turn , is equivalent to the linear code @xmath162 .",
    "in particular , @xmath185 encoded by @xmath162 and the points in the lattice code @xmath148 .",
    "this result , which is originally from  ( * ? ? ?",
    "* lemma  5 ) , is proved below for completeness .",
    "[ lem : w_to_t ] the map @xmath186 { { \\rm~mod~}}{\\lambda_{\\rm c}}$ ] is a bijection between @xmath187 and @xmath148 .    from  , we know that @xmath188 . hence , it only remains to show that no two distinct messages @xmath189 and @xmath190 are mapped to the same point in @xmath148 .",
    "assuming the contrary , we have @xmath191 { { \\rm~mod~}}{\\lambda_{\\rm c}}= \\left[{{\\pmb}{b}_{\\rm c}}p^{-1 } g({\\pmb}{gw}_b)\\right ] { { \\rm~mod~}}{\\lambda_{\\rm c}}$ ] . using   and  ,",
    "we obtain @xmath192 multiplying both sides by @xmath193 , we obtain @xmath194 . reducing this result modulo-@xmath0 ,",
    "we have @xmath195 over @xmath6 .",
    "since this implies @xmath196 over @xmath6 while @xmath197 and @xmath171 has full column rank , we have arrived at a contradiction .    in order to prove capacity achievability ,",
    "we will rely on random coding arguments to show the existence of a good choice of @xmath171 . as in  @xcite",
    ", we will assume that @xmath171 is a random matrix chosen with uniform probability distribution on @xmath198 .",
    "the following result is useful in upper bounding the decoding error probability over the ensemble of random codes .",
    "[ lem : t_is_uniform ] let @xmath199 be a given non - zero vector , and let @xmath171 be uniformly distributed in @xmath198 . then @xmath200 \\ ! { { \\rm~mod~}}{\\lambda_{\\rm c}}$ ] is uniformly distributed over @xmath201 , i.e. , over the lattice code @xmath202 .",
    "we will assume that the number of messages @xmath4 and a design rate @xmath84 are given , and show that there exist good lattice codes of sufficiently large dimension @xmath1 that encode @xmath4 messages over an appropriately chosen prime field @xmath6 at rates close to @xmath84  b / dim . in order to rigorously state the main result",
    ", we consider a fixed non - zero tolerance @xmath203 , and a free parameter @xmath3 that determines the value of the prime @xmath0 as a function of @xmath1 .",
    "[ thm : main ] let the number of messages @xmath4 , design rate @xmath84 , parameter @xmath3 and tolerance @xmath203 be given .",
    "for every sufficiently large dimension @xmath1 and any prime @xmath0 in the interval @xmath204 $ ] , there exists an @xmath1-dimensional lattice code @xmath205 that encodes @xmath4 message vectors over @xmath6 such that the rate of transmission of each message is at least @xmath206  b / dim and the probability of error at any receiver @xmath62 is less than @xmath207 if @xmath208    to prove theorem  [ thm : main ] , we utilize the lattice code ensemble introduced in  @xcite ( see section  [ sec : prelim : lattices_from_codes ] of this paper ) . in the rest of this section",
    "we describe the construction of random lattice codes , and the encoding and decoding operations .",
    "we provide the proof of the theorem  [ thm : main ] in section  [ sec : proof_of_main ] .",
    "we require that the prime @xmath0 increase with the code dimension @xmath1 as @xmath209 for some fixed @xmath3 . from bertrand",
    "s postulate  @xcite we know that for every @xmath210 there exists a prime in the interval @xmath211 $ ]",
    ". we will choose @xmath0 to be any prime in this interval , i.e. , @xmath212 .      once @xmath0 is fixed , we choose @xmath92 as the largest integer that satisfies @xmath213 the left - hand side in the above inequality is the actual rate at which the lattice code encodes each message , while @xmath84 is the desired rate .",
    "the difference between the two is at the most @xmath214 which converges to @xmath215 as @xmath144 .",
    "hence , for a given @xmath216 and for all sufficiently large @xmath1 , the code rate is at least @xmath206  b / dim .",
    "from   in section  [ sec : prelim : lattices ] , we know that for a given @xmath203 and for all sufficiently large @xmath1 , there exists an @xmath1-dimensional lattice @xmath146 such that @xmath217 we will choose such a rogers - good lattice as @xmath146 , and scale it so that @xmath218 it follows that @xmath219 . using the definition of the effective radius  , we arrive at the following lower bound on the volume of the voronoi region of @xmath146 @xmath220      the fine lattice is obtained by the construction of  @xcite described in  section  [ sec : prelim : lattices_from_codes ] .",
    "the length of the linear code @xmath162 is @xmath1 , and its rank @xmath221 is the number of message symbols to be encoded by the lattice code .",
    "note that this requires that @xmath222 be true . using   and the property @xmath223 , we have @xmath224 rearranging the terms in the above inequality",
    ", we obtain @xmath225 where the right - hand side converges to @xmath215 as @xmath144 .",
    "hence , for all sufficiently large @xmath1 , @xmath222 .",
    "if @xmath226 is the generator matrix of @xmath162 , then @xmath227",
    ". we will choose @xmath171 uniformly random over @xmath228 matrices , resulting in a random ensemble of fine lattices @xmath110 .",
    "we will rely on random coding arguments to prove the existence of a translate @xmath158 such that the code @xmath205 performs close to capacity .",
    "we will assume that @xmath158 is distributed uniformly in @xmath150 and is chosen independently of @xmath171 .",
    "this random dither @xmath158 is usually viewed as a common randomness available at the transmitter and the receivers  @xcite .",
    "note that @xmath229 .",
    "to a point in @xmath205.,width=480 ]    we will now describe the encoding operation @xmath230 at the transmitter that maps the message vectors @xmath231 to a codeword @xmath232 .",
    "the encoder first concatenates the @xmath4 messages into the vector @xmath233 , encodes @xmath173 to a codeword in the linear code @xmath162 , and maps it to a point @xmath234 using construction  a as follows @xmath235 { { \\rm~mod~}}{\\lambda_{\\rm c}}.\\ ] ] from the discussion in section  [ sec : sub : choice_of_parameters ] , we know that @xmath236 , and hence , @xmath237 .",
    "finally , the transmit codeword @xmath238 is generated by dithering @xmath239 , @xmath240 { { \\rm~mod~}}{\\lambda_{\\rm c}}= \\left [ { { \\pmb}{b}_{\\rm c}}\\ , p^{-1 } g\\left ( { \\pmb}{gw } \\right ) - { \\pmb}{d } \\right ] { { \\rm~mod~}}{\\lambda_{\\rm c}}.\\ ] ] this sequence of operations is illustrated in fig .",
    "[ fig : encoder ] .",
    "note that since @xmath241 , each codeword @xmath238 satisfies @xmath242 , and hence , the power constraint @xmath243 it is straightforward to show that the dithering operation   is a one - to - one correspondence between @xmath237 and @xmath232 .",
    "further , from lemma  [ lem : w_to_t ] we know that   is a bijection between the message space @xmath244 and the undithered codewords @xmath148 if @xmath171 is full rank . hence , to ensure that no two messages are mapped to the same codeword , we only require that the random matrix @xmath171 be full rank .",
    "it can be shown that ( see  @xcite ) @xmath245 we will only require a relaxation based on the above inequality . from  , for all sufficiently large @xmath1 , @xmath246 .",
    "similarly , for all large enough @xmath1 , we have @xmath247 , and hence , @xmath248      the receiver employs a two stage decoder : in the first stage the receiver identifies the subcode of @xmath205 corresponding to the available side information , and in the second stage it decodes the channel output to a point in this subcode .",
    "the side information at @xmath62 over a block of @xmath92 realizations of the @xmath4 messages is of the form @xmath249 the receiver desires to identify the set of all possible values of the message vector @xmath233 that satisfy  . using the notation @xmath250 , the side information   can be rewritten compactly in terms of @xmath173 and @xmath251 as @xmath252 where @xmath253 denotes the kronecker product of matrices and @xmath254 is the @xmath40 identity matrix over @xmath6 . observe that   is an under - determined system of linear equations , and the set of solutions is a coset of the null space of @xmath255 . let @xmath256 be a rank @xmath257 matrix such that @xmath258 , i.e. , the columns of @xmath259 form a basis of the null space of @xmath260 .",
    "then the set of all solutions to   is @xmath261 where @xmath262 is the coset leader . from  , we conclude that the undithered codeword must be of the form @xmath263 \\ !",
    "{ { \\rm~mod~}}{\\lambda_{\\rm c}}\\text { for some }   { \\pmb}{\\tilde{w } } \\in { \\mathbb{f}_p}^{(k - m)\\ell}.\\ ] ] we will now use the property of @xmath164 that for any @xmath264 , @xmath265 .",
    "therefore , @xmath266 for some @xmath267 . using this in",
    ", we obtain @xmath268 \\ !",
    "{ { \\rm~mod~}}{\\lambda_{\\rm c}}\\nonumber \\\\ = & \\left[\\ , { { \\pmb}{b}_{\\rm c}}p^{-1 } g\\left ( { \\pmb}{gv } \\right ) + \\left[{{\\pmb}{b}_{\\rm c}}p^{-1 } g\\left({\\pmb}{ga_s\\tilde{w } } \\right)\\right ] \\ !",
    "{ { \\rm~mod~}}{\\lambda_{\\rm c}}+ \\left[\\ , { { \\pmb}{b}_{\\rm c}}{\\pmb}{c } \\ , \\right]\\ ! \\ ! { { \\rm~mod~}}{\\lambda_{\\rm c}}\\ , \\right ] { { \\rm~mod~}}{\\lambda_{\\rm c}}\\nonumber \\\\ = & \\left[\\ , { { \\pmb}{b}_{\\rm c}}p^{-1 } g\\left ( { \\pmb}{gv } \\right ) + \\left[{{\\pmb}{b}_{\\rm c}}p^{-1 } g\\left({\\pmb}{ga_s\\tilde{w } } \\right)\\right ] \\ ! { { \\rm~mod~}}{\\lambda_{\\rm c}}\\ , \\right ] { { \\rm~mod~}}{\\lambda_{\\rm c } } ,   \\label{eq : subcode_of_valid_codewords}\\end{aligned}\\ ] ] where we have used  ,   and the fact that @xmath269 .",
    "since the receiver knows @xmath262 , the component of @xmath239 unavailable from the side information is @xmath270 \\ !",
    "{ { \\rm~mod~}}{\\lambda_{\\rm c}}.\\ ] ] let @xmath271 be the subcode of @xmath162 with generator matrix @xmath272 , and @xmath273 be the lattice obtained by applying construction  a to @xmath274 and transforming it by @xmath183 , i.e. , @xmath275 using @xmath272 instead of @xmath171 in lemma  [ lem : w_to_t ] , we see that @xmath276 and that   is a one - to - one correspondence between @xmath277 and @xmath276 as long as @xmath272 is full rank . together with  ,  , and  , we conclude that the transmit vector @xmath238 belongs to the following lattice subcode of @xmath205 , @xmath278 \\ !",
    "{ { \\rm~mod~}}{\\lambda_{\\rm c}}~\\big\\vert~ { \\pmb}{\\tilde{t } } \\in { \\lambda_{{\\pmb}{s}}}/{\\lambda_{\\rm c}}\\right\\}.\\ ] ] the decoding problem at the second stage is to estimate @xmath279 , or equivalently @xmath280 , from the channel output .      .",
    "the vector @xmath262 and the lattice @xmath273 are determined using the side information available at the receiver.,width=528 ]    let the channel output at the receiver @xmath62 be @xmath281 , where @xmath282 is a gaussian vector with zero mean and variance @xmath59 per dimension .",
    "the received vector is scaled by the coefficient @xmath283 , resulting in @xmath284 this mmse pre - processing improves the effective signal - to - noise ratio of the system beyond the channel signal - to - noise ratio @xmath285 and allows the lattice decoder to perform close to capacity  @xcite .",
    "let @xmath286 be the effective noise term in  . using the facts that @xmath238 and @xmath282 are independent , @xmath287 , and @xmath282 has zero mean",
    ", we have @xmath288 where @xmath46 is the expectation operator .",
    "the choice of @xmath289 minimizes this upper bound and yields @xmath290 which is less than the gaussian noise power @xmath291 .",
    "in the rest of the paper we will assume that @xmath292 and use the notation @xmath293 the lower bound   on signal - to - noise ratio can be rewritten in terms of @xmath294 as @xmath295    from  , we know that @xmath296 for some @xmath297 .",
    "after mmse scaling , the decoder removes the contributions of the dither @xmath158 and the offset @xmath298 from @xmath299 to obtain @xmath300 the decoder proceeds by quantizing @xmath301 to the lattice @xmath273 and reducing the result modulo @xmath146 .",
    "if the noise @xmath302 is sufficiently ` small ' , then this sequence of operations will yield @xmath303 { { \\rm~mod~}}{\\lambda_{\\rm c}}= \\left [ q_{{\\lambda_{{\\pmb}{s}}}}({\\pmb}{\\tilde{t } } + { { \\pmb}{\\lambda}_{\\rm c}}+ { \\pmb}{z } ) \\right ] { { \\rm~mod~}}{\\lambda_{\\rm c}}= \\left [ { \\pmb}{\\tilde{t } } + { { \\pmb}{\\lambda}_{\\rm c}}\\right ] { { \\rm~mod~}}{\\lambda_{\\rm c}}= { \\pmb}{\\tilde{t}}.\\end{aligned}\\ ] ] given @xmath279 , the receiver uses   to obtain the undithered codeword @xmath239 , and hence the message vector @xmath304 , as follows @xmath305 { { \\rm~mod~}}{\\lambda_{\\rm c}}.\\ ] ] to conclude , the decoder obtains the estimate @xmath306 of the undithered codeword @xmath239 from the received vector @xmath307 as @xmath308 { { \\rm~mod~}}{\\lambda_{\\rm c}}+ { { \\pmb}{b}_{\\rm c}}p^{-1 } g({\\pmb}{gv } )   \\ , \\big ] { { \\rm~mod~}}{\\lambda_{\\rm c}}\\\\              & = \\left [ \\ , q_{{\\lambda_{{\\pmb}{s}}}}({\\pmb}{y ' } ) + { { \\pmb}{b}_{\\rm c}}p^{-1 } g({\\pmb}{gv } )   \\ , \\right ] { { \\rm~mod~}}{\\lambda_{\\rm c}}\\\\              & = \\left [ \\ , q_{{\\lambda_{{\\pmb}{s}}}}\\left(\\alpha { \\pmb}{y } - { { \\pmb}{b}_{\\rm c}}p^{-1 } g({\\pmb}{gv } ) + { \\pmb}{d}\\right ) + { { \\pmb}{b}_{\\rm c}}p^{-1 } g({\\pmb}{gv } )   \\ , \\right ] { { \\rm~mod~}}{\\lambda_{\\rm c}},\\end{aligned}\\ ] ] which shows that the @xmath155 operation arising from   can be ignored .",
    "the steps involved in the decoding operation are illustrated in fig .",
    "[ fig : decoder ] .",
    "note that the effective information vector @xmath280 is not encoded in the point @xmath309 , but is encoded in the coset @xmath310 .",
    "the error event for this decoder is @xmath311 , i.e. , @xmath312 , which is equivalent to @xmath313 .",
    "hence , a decoding error occurs if and only if @xmath302 is closer to a point in @xmath314 than any vector in the coarse lattice @xmath146 , i.e. , if and only if @xmath315",
    "in this section we first state and prove two technical lemmas ( section  [ sec : technical_lemma ] ) , use these lemmas to show that the error probability at a given fixed receiver @xmath62 is small ( section  [ sec : error_at_single_receiver ] ) , and then complete the proof of the main theorem by showing that the error probability at every receiver of the broadcast channel is simultaneously small ( section  [ sec : complete_the_proof ] ) .",
    "finally , we state some important corollaries of the main theorem ( section  [ sec : corollaries ] ) .      the first result , which is a direct generalization of  ( * ?",
    "* lemma  1 ) and  ( * ? ? ?",
    "* lemma  2.3 ) , gives an upper bound on the number of lattice points lying inside a ball .",
    "[ lem : counting ] for any @xmath132 , @xmath316 and any @xmath1-dimensional lattice @xmath146 , @xmath317 where @xmath135 is the volume of a unit ball in @xmath111 .",
    "let @xmath318 be the set of all points in @xmath111 that are mapped to one of the points in @xmath319 by the lattice quantizer @xmath320 .",
    "since @xmath321 is a union of the pairwise disjoint sets @xmath322 , @xmath323 , and since each of these sets has volume @xmath324 , we have @xmath325 using the fact that @xmath326 , we have the following @xmath327 where the last step follows from the triangle inequality .",
    "consequently , we have an upper bound on the volume of @xmath321 , @xmath328 . using this result with",
    "proves the lemma .    as in  @xcite , we will rely on the fact that , with very high probability , the norm of the noise @xmath302 is not much larger than @xmath329 . but unlike these previous works , we need to accommodate multiple receivers in the broadcast channel , and hence , we will require a slightly stronger result : the probability that the effective noise is large is exponentially small in @xmath1 .",
    "the proof of this result is given below .",
    "[ lem : noise_is_small ] let @xmath238 be uniformly distributed in @xmath150 and @xmath330 be any positive number .",
    "then @xmath331    we will prove   for every fixed realization of @xmath238 in @xmath150 , which shows that the statement of the lemma is true for any distribution of @xmath238 on @xmath150 .",
    "in the rest of the proof we will assume that @xmath332 is an arbitrary fixed vector and @xmath282 is gaussian distributed . using @xmath333 , we have @xmath334 hence",
    ", we have the following upper bound @xmath335 from the definition   of @xmath294 , we have @xmath336 .",
    "hence , the above upper bound corresponds to the event @xmath337 the event   occurs _ only if _ at least one of the following two events occur @xmath338 therefore , @xmath339 .",
    "we will now individually upper bound @xmath340 and @xmath341 , and thereby complete the proof .",
    "a rearrangement of terms in   yields @xmath342 .",
    "this is the probability that a gaussian vector with unit variance per dimension lies outside the sphere of squared radius @xmath343 .",
    "the following is a well known upper bound on this probability ( see  @xcite ) @xmath344 the event @xmath345 is equivalent to @xmath346 . using @xmath289 , we can show that this is same as @xmath347 . since @xmath348 is a zero mean gaussian random variable with variance",
    "@xmath349 , we have @xmath350 where @xmath351 is the gaussian tail function . using @xmath352 and",
    "the chernoff bound @xmath353 , we arrive at @xmath354 this completes the proof .      in this subsection",
    "we derive an upper bound on the decoding error probability @xmath355 at a receiver @xmath62 when averaged over the ensemble of lattice codes generated by choosing @xmath171 uniformly over @xmath356 and @xmath158 uniformly over @xmath150 .    the following result from  @xcite , known as the _ crypto lemma _ , captures an important characteristic of random dithering .",
    "[ lem : crypto ] let @xmath357 be any random vector . if @xmath158 is independent of @xmath239 and is uniformly distributed over @xmath150 , then @xmath358 \\ ! { { \\rm~mod~}}{\\lambda_{\\rm c}}$ ] is independent of @xmath239 and uniformly distributed over @xmath150 .",
    "the property that the transmit vector @xmath238 is statistically independent of @xmath239 implies that the effective noise @xmath359 is independent of the transmit message .",
    "this facilitates the error probability analysis through the observation that the error event   is statistically independent of @xmath279 .    for distinct messages @xmath280 to be mapped to distinct points @xmath279 , we require that @xmath272 be full rank .",
    "since @xmath360 is full rank , this is same as requiring that @xmath171 be full rank . apart from the event @xmath361 , we assume that the decoder declares an error whenever the event @xmath362 occurs .",
    "hence , the error probability @xmath355 at the receiver @xmath62 satisfies @xmath363 from  , we already know that @xmath364 is exponentially small in @xmath1 . using the given design tolerance @xmath207 , we set @xmath365 , which is positive if @xmath203 .",
    "let @xmath366 be the radius of the typical noise vector and @xmath367 .",
    "then , @xmath368 lemma  [ lem : noise_is_small ] provides an exponential upper bound on @xmath369 . in the following theorem",
    "we show that @xmath370 is also exponentially small in @xmath1 .",
    "the proof of this result uses the technique of  @xcite to bound the number of lattice points lying in an @xmath1-dimensional ball .",
    "let @xmath371 be the least noise standard deviation @xmath372 among all the receivers .",
    "the knowledge of @xmath373 enables us to derive an upper bound on error probability which is independent of the side information matrix @xmath55 and the noise standard deviation @xmath372 as long as the condition   on the minimum required @xmath374 is satisfied .",
    "the following result , which follows immediately from  , will be used in the proof of theorem  [ thm : pe_one_receiver ] , @xmath375    [ thm : pe_one_receiver ] for any receiver @xmath62 with @xmath376 , and for all large enough @xmath1 , @xmath377 when averaged over the ensemble of random lattice codes .    from",
    ", we note that the decoder is in error when @xmath302 is closer to some coset @xmath378 , with @xmath379 and @xmath380 , than any point in @xmath146 .",
    "the number of competing cosets is @xmath381 , and we index them using the non - zero vectors @xmath382 . to each @xmath383",
    ", we associate the coset corresponding to the coset leader @xmath384 \\ !",
    "{ { \\rm~mod~}}{\\lambda_{\\rm c}}.\\ ] ] since @xmath171 is random , the coset leader @xmath385 associated with a given @xmath383 is a random vector .",
    "given that @xmath386 and @xmath387 , the euclidean distance between @xmath302 and @xmath146 is at the most @xmath388 .",
    "hence , for an error event to occur , there must exist a coset @xmath378 at a distance less than @xmath389 from @xmath302 , i.e. , @xmath390 .",
    "indexing the cosets by @xmath383 , we have @xmath391 where the second inequality follows from union bound , and the last inequality follows from the observation @xmath392 where @xmath393 is the indicator function .",
    "note that the expectation operation in   is with respect to the random vector @xmath385 as well as the effective noise @xmath302 .",
    "the matrix @xmath360 has full column rank , and hence , @xmath394 for every @xmath395 .",
    "using   and applying lemma  [ lem : t_is_uniform ] , we see that @xmath385 is uniformly distributed in @xmath396 .",
    "further , from lemma  [ lem : crypto ] the distribution of @xmath385 is independent of @xmath302 .",
    "hence , the probability mass function of @xmath385 equals @xmath397 over every element of the set @xmath398 . using this result , we obtain @xmath399 where the last equality follows from the fact that the set of cosets @xmath400 form a partition of @xmath401 . since the number of competing @xmath383 is less than @xmath402 , and @xmath403 , @xmath404 using lemma  [ lem : counting ] , we bound the number of lattice points inside the ball @xmath405 , and obtain @xmath406    using the bounds @xmath407 , from  ; @xmath408 , from  ; @xmath409 , from  ; and the relations @xmath241 , @xmath366 , and @xmath410",
    ", we obtain @xmath411 since @xmath412 , we have @xmath413    to complete the analysis , we use the bound @xmath223 and the property that @xmath414 from below as @xmath415 .",
    "hence , for all @xmath3 , @xmath416 we want an upper bound on @xmath370 which is exponentially small in @xmath1 , and hence , we require an upper bound on @xmath417 which is exponentially smaller than @xmath418 , say , for instance , @xmath419 . using  ,",
    "we observe that @xmath420 for @xmath3 , @xmath203 and for all sufficiently large @xmath1 .",
    "hence , @xmath421 , and consequently , @xmath422 for all large enough @xmath1 .",
    "we will now combine the result of theorem  [ thm : pe_one_receiver ] with   and  , and upper bound the error probability @xmath355 at the receiver @xmath62 as @xmath423 using theorem  [ thm : pe_one_receiver ] , lemma  [ lem : noise_is_small ] and  , we obtain @xmath424 for sufficiently large @xmath1 . note that @xmath425 , and @xmath426 as long as @xmath365 is positive .",
    "consequently , the parameter @xmath427 is positive , and the value of each of the terms on the right - hand side of   is at the most @xmath428 .",
    "hence the error probability at the receiver @xmath62 can be upper bounded as @xmath429 for all sufficiently large @xmath1 .",
    "we remark that the minimum required value of @xmath1 for this upper bound to hold is a function of only @xmath207 , @xmath430 and @xmath373 , and is independent of the side information matrix @xmath55 .",
    "the bound   shows that the error probability for a fixed side information matrix @xmath55 , averaged over the random code ensemble , tends to @xmath215 as the code dimension increases .",
    "hence , there exists a choice of lattice code ( which is chosen for the given side information matrix @xmath55 ) with a small error probability at this receiver .",
    "we want to prove a slightly stronger result , viz . , there exists a lattice code such that the decoding error probability for every possible side information matrix @xmath55 is small as long as the receiver @xmath374 is large enough . in order to prove this result",
    ", we consider a hypothetical broadcast network that consists of one receiver for each possible choice of the matrix @xmath55 .",
    "note that two distinct values of the matrix @xmath55 that have identical row space constitute equivalent receiver side information configurations .",
    "hence , it is enough to consider a broadcast channel that consists of one receiver corresponding to each possible subspace of @xmath79 , where the dimension of the subspace can be between @xmath215 and @xmath431 .",
    "a subspace of dimension @xmath52 , @xmath432 , can be mapped to a @xmath433 matrix whose rows form a basis of the subspace .",
    "this map embeds the set @xmath434 of all non - equivalent choices of side information matrix @xmath55 into @xmath435 , which is the set of all matrices over @xmath6 with @xmath4 columns and at the most @xmath431 rows .",
    "hence , the number of receivers @xmath436 can be upper bounded as latexmath:[\\[\\begin{aligned }   \\label{eq : set_s }    note that the number of receivers in this hypothetical broadcast channel grows polynomially in @xmath1 , while the probability of error at each receiver   decays exponentially .",
    "this observation lets us complete the proof of theorem  [ thm : main ] by strengthening the result of the previous subsection .",
    "we assume that each receiver @xmath62 , @xmath438 , satisfies the lower bound   on @xmath374 and outputs an estimated message vector @xmath439 using its own channel observation .",
    "we say that the broadcast network is in error if any of the receivers commits a decoding error . using a union bound argument and the upper bounds   and",
    ", we see that the network error probability @xmath440 averaged over the random ensemble of lattice codes satisfies @xmath441 which tends to @xmath215 as @xmath1 becomes arbitrarily large .",
    "hence , for a sufficiently large @xmath1 , there exists a lattice code such that the network error probability is as small as desired .",
    "in particular , this implies that there exists a choice of lattice code such that the decoding error probability at every receiver @xmath62 , @xmath438 , is simultaneously small .",
    "this completes the proof of the main theorem .        using standard arguments based on markov inequality  @xcite",
    ", we show that almost all codes from the random lattice code ensemble yield a small error probability . in order to prove this , it is sufficient to show that for almost all lattice codes the network error probability is small over the hypothetical broadcast channel that consists of one receiver for each possible side information matrix .    for a given dimension @xmath1 ,",
    "all the lattice codes in the random code ensemble use the same coarse lattice @xmath146 , but differ in the choice of the fine lattice @xmath110 and/or the dither vector @xmath158 .",
    "let @xmath442 denote the network error probability for a given choice of @xmath443 in the hypothetical broadcast channel . if @xmath110 and @xmath158 are chosen randomly , then @xmath444 is a random variable . from  , we know that the expected value of @xmath444 , which is equal to the average network error rate @xmath440 , is small .",
    "suppose we want a lower bound on the fraction of random codes with error probability at the most @xmath445 .",
    "using markov inequality , we have @xmath446 it follows that , asymptotically , for almost all choices of the fine lattice @xmath110 and dither vector @xmath158 , the resulting lattice code @xmath205 provides an exponentially small error probability in the broadcast channel , i.e. , @xmath447      our model of broadcast channel includes as a special case the receiver with no side information , i.e. , @xmath55 is the empty matrix and @xmath448 .",
    "the decoder for such a receiver uses the @xmath449 identity matrix for @xmath360 and the all zero vector for @xmath262 , see  . specializing the main theorem for a single receiver with @xmath58 , we immediately deduce that the ensemble of random lattice codes achieves the capacity of the single - user awgn channel .",
    "it is well known that ( nested ) lattice codes , and lattice constellations in general , can achieve the capacity of the point - to - point awgn channel  @xcite .",
    "our corollary to the main theorem provides an alternate proof of this result which is based only on simple counting arguments .",
    "the proof technique presented in this paper relies on lattices obtained by applying construction  a to random linear codes over a large enough prime field @xmath6 .",
    "this technique was introduced by loeliger in  @xcite and used in  @xcite to prove the goodness of lattice codes in awgn channel .",
    "each of these results requires a different rate of growth for the prime @xmath0 and places different requirements on the characteristics of the coarse lattice @xmath146 .",
    "the following are some of the properties that have been used in the literature :    * _ rogers - good : _ the ratio of covering radius @xmath450 to the effective radius @xmath451 of the lattice must be close to @xmath60 , see  .",
    "such a lattice is also said to be _",
    "good for covering_. * _ mse - good : _ the value of the lattice parameter @xmath452 , known as the _ normalized second moment _",
    ", is close to @xmath453 , see  @xcite .",
    "every rogers - good lattice is also mse - good , and hence , this is a weaker requirement . * _ poltyrev - good : _ such a lattice , when used as an infinite constellation , achieves the capacity of an awgn channel in which the transmitter has no power constraints  @xcite .",
    "these lattices are resilient against additive gaussian noise .",
    "the achievability result of  @xcite requires @xmath146 to be simultaneously rogers - good and poltyrev - good , and uses @xmath454 ( exponential in @xmath1 ) .",
    "the random code ensemble of  @xcite uses an mmse - good lattice for @xmath146 , lets @xmath0 grow as @xmath32 , and can accommodate a wide class of channel noise statistics , including white gaussian noise .",
    "the code construction of  @xcite requires @xmath0 to be at least @xmath33 , needs no dithering operation , i.e. , uses @xmath455 , but is known to achieve capacity only if @xmath456 . in comparison , our proof method allows @xmath0 to grow as @xmath2 , for any fixed @xmath3 , and holds for all @xmath457 , while requiring that @xmath146 be rogers - good .",
    "we have showed that lattice codes are optimal for common - message broadcast in gaussian channels where receivers have side information .",
    "we used random lattice ensembles obtained by applying construction  a to linear codes over appropriately large prime fields @xmath6 .",
    "the growth of @xmath0 as @xmath2 does not necessarily pose a limitation in communication applications .",
    "for instance , in the relay network of example  [ ex : relay_network ] , the first phase of the protocol , namely compute - and - forward  @xcite , only requires that @xmath458 as @xmath144 , which can be met by our scheme by choosing @xmath459 .",
    "similarly , with example  [ ex : overlay ] , where the broadcast signal supplements a wired multicast network , it is known that wireline network codes meeting the @xmath31 bound exist over every large enough finite field  @xcite . hence , we can choose @xmath1 to be sufficiently large to simultaneously optimize both the wired and wireless parts of the hybrid network .",
    "on the other hand , designing lattice strategies for a fixed size of the finite field , especially sizes that are powers of two , may have greater practical significance .",
    "the capacity of the gaussian broadcast channel with receiver side information under general message demands , such as with private message requests , is known only for some special cases  @xcite .",
    "these known achievability schemes utilize random gaussian codebooks together with dirty - paper and superposition coding .",
    "it will be interesting to examine if the lattice structure of the codes proposed in this paper can be exploited to derive new capacity results beyond the known cases .",
    "o.  ordentlich and u.  erez , `` a simple proof for the existence of `` good '' pairs of nested lattices , '' in _ electrical electronics engineers in israel ( ieeei ) , 2012 ieee 27th convention of _ , nov .",
    "2012 , pp . 112 .",
    "n.  di  pietro , `` on infinite and finite lattice constellations for the additive white gaussian noise channel , '' theses , universit de bordeaux , jan . 2014 .",
    "[ online ] .",
    "available : https://tel.archives-ouvertes.fr/tel-01135575          m.  wilson , k.  narayanan , h.  pfister , and a.  sprintson , `` joint physical layer coding and network coding for bidirectional relaying , '' _ ieee trans .",
    "inf . theory _ ,",
    "56 , no .  11 , pp .",
    "56415654 , nov . 2010 .",
    "s.  el  rouayheb , a.  sprintson , and c.  georghiades , `` on the index coding problem and its relation to network coding and matroid theory , '' _ ieee trans .",
    "inf . theory _",
    "56 , no .  7 , pp . 31873195 , jul .",
    "k.  shum , m.  dai , and c.  w. sung , `` broadcasting with coded side information , '' in _ personal indoor and mobile radio communications ( pimrc ) , 2012 ieee 23rd international symposium on _ , sep .",
    "2012 , pp . 8994 .",
    "y.  ma , z.  lin , h.  chen , and b.  vucetic , `` multiple interpretations for multi - source multi - destination wireless relay network coded systems , '' in _ proc .",
    "ieee 23rd int .",
    "personal indoor and mobile radio communications ( pimrc ) _ , sep .",
    "2012 , pp . 22532258 .",
    "l.  natarajan , y.  hong , and e.  viterbo , `` lattice index coding for the broadcast channel , '' in _ information theory workshop ( itw ) , 2015 ieee _ , apr .",
    "2015 , pp . 15 , extended version available online , arxiv:1410.6569 .",
    "b.  asadi , l.  ong , and s.  johnson , `` the capacity of three - receiver awgn broadcast channels with receiver message side information , '' in _ proc .",
    "information theory ( isit ) _ , jun .",
    "2014 , pp . 28992903 .",
    "j.  sima and w.  chen , `` joint network and gelfand - pinsker coding for 3-receiver gaussian broadcast channels with receiver message side information , '' in _ proc .",
    "information theory ( isit ) _ , jun .",
    "2014 , pp . 8185 ."
  ],
  "abstract_text": [
    "<S> lattices possess elegant mathematical properties which have been previously used in the literature to show that structured codes can be efficient in a variety of communication scenarios , including coding for the additive white gaussian noise ( awgn ) channel , dirty - paper channel , wyner - ziv coding , coding for relay networks and so forth . following the approach introduced by erez and zamir , </S>",
    "<S> we show that lattice codes are optimal for the family of gaussian broadcast channels where the source transmits a set of common messages to all receivers and each receiver has _ coded side information _ , </S>",
    "<S> i.e. , prior information in the form of linear combinations of the messages . </S>",
    "<S> this channel model , which is an instance of the gaussian version of index coding , is motivated by applications to multi - terminal networks where the nodes may have access to coded versions of the messages from previous signal hops or through orthogonal channels . </S>",
    "<S> the known results on the capacity of this channel are based on random gaussian codebooks . </S>",
    "<S> the structured coding scheme proposed in this paper utilizes construction  a lattices designed over prime finite fields , and _ algebraic binning _ at the decoders to expurgate the channel code and obtain good lattice subcodes , for every possible set of linear combinations available as side information . as a corollary </S>",
    "<S> , we show that lattice codes based on construction  a can achieve the capacity of single - user awgn channels with the size @xmath0 of the prime field growing as a function of the code length @xmath1 as @xmath2 , for any fixed @xmath3 , which is the slowest yet reported in the literature .    </S>",
    "<S> additive white gaussian noise ( awgn ) , capacity , construction  a , gaussian broadcast channel , index coding , lattice , side information , structured codes . </S>"
  ]
}