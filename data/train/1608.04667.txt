{
  "article_text": [
    "medical imaging including x - rays , magnetic resonance imaging ( mri ) , computer tomography ( ct ) , ultrasound etc .",
    "are susceptible to noise @xcite .",
    "reasons vary from use of different image acquisition techniques to attempts at decreasing patients exposure to radiation . as the amount of radiation is decreased , noise increases @xcite .",
    "denoising is often required for proper image analysis , both by humans and machines .",
    "image denoising , being a classical problem in computer vision has been studied in detail .",
    "various methods exist , ranging from models based on partial differential equations ( pdes ) @xcite , domain transformations such as wavelets @xcite , dct @xcite , bls - gsm @xcite etc . , non local techniques including nl - means @xcite , combination of non local means and domain transformations such as bm3d @xcite and a family of models exploiting sparse coding techniques @xcite .",
    "all methods share a common goal , expressed as    @xmath0    where @xmath1 is the noisy image produced as a sum of original image @xmath2 and some noise @xmath3 .",
    "most methods try to approximate @xmath2 using @xmath1 as close as possible . in most cases ,",
    "@xmath3 is assumed to be generated from a well defined process .    with recent developments in deep learning @xcite ,",
    "results from models based on deep architectures have been promising .",
    "autoencoders have been used for image denoising @xcite .",
    "they easily outperform conventional denoising methods and are less restrictive for specification of noise generative processes . denoising autoencoders constructed using convolutional layers have better image denoising performance for their ability to exploit strong spatial correlations .    in this paper we present empirical evidence that stacked denoising autoencoders built using convolutional layers work well for small sample sizes , typical of medical image databases . which is in contrary to the belief that for optimal performance , very large training datasets are needed for models based on deep architectures",
    "we also show that these methods can recover signal even when noise levels are very high , at the point where most other denoising methods would fail .",
    "rest of this paper is organized as following , next section discusses related work in image denoising using deep architectures .",
    "section iii introduces autoencoders and their variants .",
    "section iv explains our experimental set - up and details our empirical evaluation and section v presents our conclusions and directions for future work .",
    "although bm3d @xcite is considered state - of - the - art in image denoising and is a very well engineered method , burger et al .",
    "@xcite showed that a plain multi layer perceptron ( mlp ) can achieve similar denoising performance .",
    "denoising autoencoders are a recent addition to image denoising literature .",
    "used as a building block for deep networks , they were introduced by vincent et al .",
    "@xcite as an extension to classic autoencoders .",
    "it was shown that denoising autoencoders can be stacked @xcite to form a deep network by feeding the output of one denoising autoencoder to the one below it .",
    "jain et al .",
    "@xcite proposed image denoising using convolutional neural networks . it was observed that using a small sample of training images , performance at par or better than state - of - the - art based on wavelets and markov random fields can be achieved .",
    "xie et al .",
    "@xcite used stacked sparse autoencoders for image denoising and inpainting , it performed at par with k - svd .",
    "agostenelli et al .",
    "@xcite experimented with adaptive multi column deep neural networks for image denoising , built using combination of stacked sparse autoencoders .",
    "this system was shown to be robust for different noise types .",
    "an autoencoder is a type of neural network that tries to learn an approximation to identity function using backpropagation , i.e. given a set of unlabeled training inputs @xmath4 , it uses    @xmath5    an autoencoder first takes an input @xmath6^d$ ] and maps(encode ) it to a hidden representation @xmath7^{d'}$ ] using deterministic mapping , such as    @xmath8    where @xmath9 can be any non linear function .",
    "latent representation @xmath3 is then mapped back(decode ) into a reconstruction @xmath1 , which is of same shape as @xmath2 using similar mapping .",
    "@xmath10    in , prime symbol is not a matrix transpose .",
    "model parameters ( @xmath11 ) are optimized to minimize reconstruction error , which can be assessed using different loss functions such as squared error or cross - entropy .",
    "basic architecture of an autoencoder is shown in fig .",
    "[ autoencoder_fig ] @xcite        here layer @xmath12 is input layer which is encoded in layer @xmath13 using latent representation and input is reconstructed at @xmath14 .    using number of hidden units lower than inputs forces",
    "autoencoder to learn a compressed approximation .",
    "mostly an autoencoder learns low dimensional representation very similar to principal component analysis ( pca ) .",
    "having hidden units larger than number of inputs can still discover useful insights by imposing certain sparsity constraints .",
    "denoising autoencoder is a stochastic extension to classic autoencoder @xcite , that is we force the model to learn reconstruction of input given its noisy version .",
    "a stochastic corruption process randomly sets some of the inputs to zero , forcing denoising autoencoder to predict missing(corrupted ) values for randomly selected subsets of missing patterns .",
    "basic architecture of a denoising autoencoder is shown in fig .",
    "[ denautoencoder_fig ]        denoising autoencoders can be stacked to create a deep network ( stacked denoising autoencoder ) @xcite shown in fig .",
    "[ sautoencoder_fig ] @xcite .",
    "output from the layer below is fed to the current layer and training is done layer wise .",
    "convolutional autoencoders @xcite are based on standard autoencoder architecture with convolutional _ encoding _ and _ decoding _ layers .",
    "compared to classic autoencoders , convolutional autoencoders are better suited for image processing as they utilize full capability of convolutional neural networks to exploit image structure .    in convolutional autoencoders ,",
    "weights are shared among all input locations which helps preserve local spatiality .",
    "representation of @xmath15th feature map is given as    @xmath16    where bias is broadcasted to whole map , @xmath17 denotes convolution ( 2d ) and @xmath9 is an activation . single bias per latent map is used and reconstruction is obtained as    @xmath18    where @xmath19 is bias per input channel , @xmath20 is group of latent feature maps , @xmath21 is flip operation over both weight dimensions .",
    "backpropogation is used for computation of gradient of the error function with respect to the parameters .",
    "we used two datasets , mini - mias database of mammograms(mmm ) @xcite and a dental radiography database(dx ) @xcite .",
    "mmm has 322 images of 1024 @xmath22 1024 resolution and dx has 400 cephalometric x - ray images collected from 400 patients with a resolution of 1935 @xmath22 2400 .",
    "random images from both datasets are shown in fig .",
    "[ random_real ] .",
    "all images were processed prior to modelling .",
    "pre - processing consisted of resizing all images to 64 @xmath22 64 for computational resource reasons .",
    "different parameters detailed in table [ datasets ] were used for corruption .",
    ".dataset perturbations [ cols=\"<,<\",options=\"header \" , ]      + @xmath23 represents 50% corrupted images with @xmath24 , @xmath25 are images corrupted with @xmath26 , @xmath27 are corrupted with @xmath28 and @xmath29 are corrupted with a poisson noise using @xmath30    also , as the noise level is increased the network has trouble converging .",
    "[ troublecon ] shows the loss curves for gaussian noise with @xmath31 . even using 100 epochs ,",
    "model has not converged .",
    "we have shown that denoising autoencoder constructed using convolutional layers can be used for efficient denoising of medical images . in contrary to the belief , we have shown that good denoising performance can be achieved using small training datasets , training samples as few as 300 are enough for good performance .",
    "our future work would focus on finding an optimal architecture for small sample denoising .",
    "we would like to investigate similar architectures on high resolution images and the use of other image denoising methods such as singular value decomposition ( svd ) and median filters for image pre - processing before using cnn dae , in hope of boosting denoising performance .",
    "it would also be of interest , if given only a few images can we combine them with other readily available images from datasets such as imagenet @xcite for better denoising performance by increasing training sample size .",
    "agostinelli , forest , michael r. anderson , and honglak lee .",
    "`` adaptive multi - column deep neural networks with application to robust image denoising . '' _ advances in neural information processing systems_. 2013 .        burger , harold c. , christian j. schuler , and stefan harmeling .",
    "`` image denoising : can plain neural networks compete with bm3d ? . '' _ computer vision and pattern recognition ( cvpr ) _ ,",
    "2012 ieee conference on .",
    "ieee , 2012 .",
    "rudin , leonid i. , and stanley osher .",
    "`` total variation based image restoration with free local constraints . '' image processing , 1994 . proceedings .",
    "_ icip-94_. , ieee international conference .",
    "1 . ieee , 1994 .",
    "vincent , pascal , et al .",
    "`` stacked denoising autoencoders : learning useful representations in a deep network with a local denoising criterion . ''",
    "_ journal of machine learning research _ 11.dec ( 2010 ) : 3371 - 3408 .",
    "yaroslavsky , leonid p. , karen o. egiazarian , and jaakko t. astola .",
    "`` transform domain image restoration methods : review , comparison , and interpretation . ''",
    "_ photonics west 2001-electronic imaging_. international society for optics and photonics , 2001 ."
  ],
  "abstract_text": [
    "<S> image denoising is an important pre - processing step in medical image analysis . </S>",
    "<S> different algorithms have been proposed in past three decades with varying denoising performances . more recently , </S>",
    "<S> having outperformed all conventional methods , deep learning based models have shown a great promise . </S>",
    "<S> these methods are however limited for requirement of large training sample size and high computational costs . in this paper </S>",
    "<S> we show that using small sample size , denoising autoencoders constructed using convolutional layers can be used for efficient denoising of medical images . </S>",
    "<S> heterogeneous images can be combined to boost sample size for increased denoising performance . </S>",
    "<S> simplest of networks can reconstruct images with corruption levels so high that noise and signal are not differentiable to human eye .    </S>",
    "<S> image denoising , denoising autoencoder , convolutional autoencoder </S>"
  ]
}