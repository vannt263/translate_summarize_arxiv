{
  "article_text": [
    "biological systems store and process information at many different scales @xcite . organisms or cells react to changes in the external environment by gathering information and making the right decisions -once such information is properly interpreted . in a way , we can identify the external changes as input signals to be coded and decoded by the cellullar machinery or information processing of neural networks , and include the exchange of signals between individuals or abstract agents sharing a given communication system @xcite .",
    "the ability to store information to interpret the surroundings beyond pure noise is thus an important property of biological systems .",
    "an organism or abstract agent can make use of this feature to react to the environment in a selectively advantageous way .",
    "this is possible provided that , in biological systems , a communicative signal must be necessarily linked to a referential value , that is , it must have a meaningful content . as pointed out by john hopfield :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ meaningful content , as distinct from noise entropy , can be distinguished by the fact that a change in a meaningful bit will have an effect on the macroscopic behavior of a system _ @xcite .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the meaningful content of information can be understood as something additional to classical information which is preserved through generations ( or by the members of a given population in a given communicative exchange ) resulting in a _ consistent _ response to the environment @xcite .",
    "the explicit incorporation of the referential value in the information content is , in some sense , external to classical information theory , since , roughly speaking , the standard measure of mutual information only accounts for the relevance of correlations among sets of random variables .",
    "indeed , one can establish configurations among coder and decoder by which mutual information is maximal but the referentiality value of the signal is lost during the communicative exchange .",
    "let us consider the following example : suppose a system where the event _ fire _ is coded as the signal @xmath0 , and that such a signal @xmath0 is always decoded as the event _",
    "water_. suppose , also , that the event _ water _ is coded as the signal @xmath4 and it is always decoded as _",
    "fire_. in this system , both the coder and the decoder depict a one - to - one mapping between input and output , and the mutual information between the set of events shared by coder and decoder would be maximum .",
    "however , if we take the system as a whole , the non - preservation of any referential value renders the communication code useless .",
    "not surprisingly , evolutionary experiments involving artificial agents ( such as robots ) include , as part of the selective pressures , the consistency of signals and referents .",
    "if survival or higher scores depend on a fitness measure which requires a proper sharing of information , the final outcome of the dynamics is a set of agents using common signals to refer to the same object @xcite .",
    "formally , we say that the communicative sign has a dual nature : a sign would involve a pair @xmath5 composed of a _ signal _ , @xmath6 , and a _ referent _ , @xmath7 .",
    "such pair must be conserved in a consistent communicative interchange .",
    "the problem of consistency of the communicative process was early addressed in @xcite , through a formalism consisting in signal / referent matrices .",
    "further works showed the suitability of such formalism , and enabled the study of the emergence of consensus driven by selective forces @xcite .",
    "these studies showed that an evolutionary process could result in a shared code by a population of interacting agents . under this framework ,",
    "the existence of optimal solutions has been studied @xcite , as well as the problem of the information catastrophe or _ linguistic error limit _",
    "@xcite , using evolutionary game theory involving a payoff function accounting for the average number of well - referentiated signals .",
    "it is the purpose of this theoretical work to rigorously identify the amount of information which conserves the dual structure of a sign , i.e. , the amount of _ consistent information _ , and to explore some of its consequences .",
    "specifically , we evaluate the relevance of the consistent input / output pairs , assuming that the input set and the output set are equal .",
    "the study of the behaviour of the consistent information displays interesting differences with classical shannon s mutual information .    we should properly differentiate the problem of consistency from the problem of _ absolute information content _ of a given signal -or , in general , mathematical object .",
    "the latter arises from the fact that , in shannon s information theory , the information content of a given signal is computed from the relative abundance of such a signal against the occurrences of the whole set of signals .",
    "the information content of an isolated signal is not defined ( or equal to zero ) .",
    "this is solved by the definition of the kolmogorov complexity @xcite , which can be understood as the absolute information content of a given signal -or mathematical object .",
    "our purpose can be embedded in shannon s framework . accepting the relative nature of the information content , we attack the problem of the consistency of input / output pairs .    the paper is written in a self - contained way .",
    "thus , beyond basics of probability theory we properly introduce the concepts and the required mathematical apparatus . at the end of the paper , a case study ( the classical binary symmetric channel ) is described in detail .",
    "in this section we define the minimal system composed of two agents able to both code and decode a set of external events .",
    "consider a set of ( at least , two ) interacting agents _ living _ in a shared world @xcite .",
    "agents communicatively interact through noisy channels .",
    "the description of this system is based on the probability transition matrices defining the coding and decoding processes , the probability transition matrix for the channel and the random variables associated to the inputs and outputs , which account for the successive information processing through the system formed by two agents and the noisy channel -see fig.1 .",
    "the qualitative difference with respect to the classical communication scheme is that we take into account the particular value of the input and the output therby capturing the referential value of the communicative exchange .",
    "an _ agent _ , @xmath2 , is defined as a pair of computing devices , @xmath8 where @xmath9 is the coder module and @xmath10 is the decoder module .",
    "the shared world is defined by a random variable @xmath11 which takes values on the set of events , @xmath12 : @xmath13 being the ( always non - zero ) probability associated to any event @xmath14 defined by @xmath15 .",
    "the coder module , @xmath9 , is described by a mapping from @xmath12 to the set @xmath16 : @xmath17 to be identified as the set of signals . for simplicity , here we assume @xmath18 .",
    "this mapping is realized according to the following matrix of transition probabilities : @xmath19 which satisfies the following condition : @xmath20 the output of the coding process is described by the random variable @xmath21 , taking values on @xmath22 according to the probability distribution @xmath23 : @xmath24    the channel , @xmath25 , is characterized by the @xmath26 matrix of conditional probabilities @xmath27 , i.e. , @xmath28 the output of the composite system coder@xmath29channel , @xmath30 , is described by the random variable @xmath31 , which takes values on the set @xmath16 following the probability distribution @xmath32 , defined as : @xmath33 where @xmath34 finally , the decoder module is a computational device described by a mapping from @xmath22 to @xmath35 , i.e it receives @xmath16 as the input set , emitted by another agent through the channel , and yields as output elements of the set @xmath12 .",
    "@xmath10 is completely defined by its transition probabilities , i.e. : @xmath36 which satisfies the following condition : @xmath37 aditionally , we can impose another condition : @xmath38 which is necessary for @xmath2 to reconstruct @xmath12 , i.e. , if the population of interacting agents share the world . by imposing condition ( [ eq1 ] )",
    "we avoid configurations in which some @xmath39 can not be referentiated by the decoder agent .",
    "we notice that it is consistent with the fact that no element from @xmath12 has zero probability to occur .",
    "furthermore , we emphasize the assumption that , in a given agent @xmath2 , following @xcite but not @xcite there is a priori no correlation between @xmath9 and @xmath10 . finally , under the presence of another agent @xmath3 , we can define the output of @xmath10 as the random variable @xmath40 , taking values on the set @xmath12 and following the probability distribution @xmath41 , which takes the form : @xmath42 where @xmath43 consistently , @xmath44    once we have the description of the different pieces of the problem , we proceed to study the couplings among them in order to obtain a suitable measure of the consistency of the communicative process .",
    "the first natural quantitative observable to account for the degree of consistency is the fraction of events @xmath45 which are consistently decoded . from eq .",
    "( [ joint ] ) it is straigtforward to conclude that such a fraction ( @xmath46 ) is given by : @xmath47 and if we take into account that the communicative exchange takes place in both directions , we have : @xmath48 putting aside slight variations , eq .",
    "( [ payoff2 ] ) has been widely used as a payoff function to study the emergence of consistent codes -in terms of duality preservation- through an evolutionary process involving several agents in every generation @xcite .",
    "such an evolutionary dynamics yielded important results which help understanding how selective pressures push a population of communicating agents to reach a consensus in their internal codes .",
    "now we proceed to compute the mutual information among relevant variables of the system .",
    "we stress that it does not account for the referentiality of the sent signals .",
    "instead , it quantifies , in bits , the relevance of the correlations among two random variables , as a potential message conveyer system , never specifying the referential value of any sequence or signal .",
    "let us briefly review some fundamental definitions and concepts of information theory .",
    "we know that , given two random variables @xmath49 , with associated probability functions @xmath50 , conditional probabilities @xmath51 and joint probabilities @xmath52 , its mutual information @xmath53 is defined as @xcite : @xmath54 or equivalently : @xmath55 being @xmath56 the _ shannon entropy _ or _ uncertainty _ associated to the random variable @xmath57 : @xmath58 and @xmath59 the _ conditional entropy _ or _ conditional uncertainty _ associated to the random variable @xmath57 with respect to the random variable @xmath60 : @xmath61 we can also define the _ joint entropy _ among two random variables @xmath62 , written as @xmath63 : @xmath64 a key concept of information theory is the so - called _ channel capacity _ , @xmath65 , which , roughly speaking , is the maximum amount of bits that can be reliably processed by the system , namely : @xmath66 as usual , in our minimal system of two interacting agents we explicitely introduced the channel , @xmath25 , as a matrix of transition probabilities between the two agents .",
    "channel capacity is an intrinsic feature of the channel ; as the fundamental theorem of information theory @xcite states , it is possible to send any message of @xmath67 bits through the channel with an arbitrary small probability of error if : @xmath68 otherwise , the probability of errors in transmission is no longer negligible . one should not confuse the statements concerning the capacity of the channel with the fact that given a random variable with associated probability distribution @xmath69",
    ", we have : @xmath70 ( provided that @xmath71 ) . in those cases , we refer to the channel as _",
    "noiseless_.    let us now return to our system .",
    "( [ infokl ] ) and the joint probabilities derived in eq .",
    "( [ joint ] ) , we can compute the mutual information among @xmath11 and @xmath40 when @xmath2 is the coder and @xmath3 the decoder , to be noted @xmath72 , as follows : @xmath73 notice that , since the coding and decoding modules of a given agent are depicted by different , a priori non - related matrices , in general @xmath74 the average of shared information among agent @xmath2 and @xmath3 will be : @xmath75 clearly , since the channel is the same in both directions of the communicative exchange , the following inequality holds : @xmath76    in the next section we investigate the role of the _ well_- correlated pairs and its impact in the overall quantity of information .",
    "to obtain the amount of consistent information shared among @xmath3 and @xmath2 , we must find a special type of correlations among @xmath11 and @xmath40 . specifically , we are concerned with the observations of both coder and decoder such that the input and the output are the same element , i.e. , the fraction of information that can be extracted from the observation of all consistent pairs @xmath77 .",
    "this fraction is captured by the so - called _ referential parameter _ , and its derivation is the objective of the next subsection .",
    "the mutual information among two random variables is obtained by exploring the behavior of input / output pairs , averaging the logarithm of the relation among the actual probability to find a given pair and the one expected by chance .",
    "consistently , the referential parameter is thus obtained by averaging the fraction of information that can be extracted by observing consistent pairs against the whole information we can obtain by looking at all possible ones .",
    "following the standard definitions of the information conveyed by a signal @xcite , the information we extract from the observation of a pair input - output @xmath79 is : @xmath80 following eq .",
    "( [ jointentrop ] ) , the average of information obtained from the observation of pairs will be precisely the joint entropy between @xmath11 and @xmath81 , @xmath82 : @xmath83 let us simplify the notation by defining a matrix @xmath84 .",
    "the elements of such a matrix are the joint probabilities , namely : @xmath85 from the above matrix , we can identify the contributions of the consistent pairs by looking at the elements of the diagonal . the relative impact of consistent pairs on the overall measure of information will define the _ referential parameter _ associated to the communicative exchange @xmath86 , to be indicated as @xmath87 .",
    "this is our key definition , and its explicit form will be : @xmath88 where @xmath89 is the _ trace _ of the matrix @xmath90 , i.e. : @xmath91 by dividing @xmath92 by @xmath93 we capture the fraction of bits obtained from the observation of consistent pairs against all possible pairs @xmath94 , which captures the degree of mixture of a given quantum state and its associated uncertainty in measuring @xcite . in this way",
    ", we observe that @xmath95 can be , roughly speaking , identified with an indicator of the consistency of the quantum state .",
    "however , it is worth noting that these measures are conceptually and formally different . ] .",
    "the amount of _ consistent information _ , @xmath96 , is obtained by weighting the overall mutual information with the referential parameter : @xmath97 the average of consistent information among two agents , @xmath98 will be , consistently : @xmath99 since @xmath100 $ ] , from the defintion of channel capacity and the symmetry properties of the mutual information , it is straightforward to show that : @xmath101    eqs .",
    "( [ saussureant ] , [ dualinfo ] ) and ( [ infop ] ) are the central equations of this paper .",
    "let us focus on eq .",
    "( [ dualinfo ] ) . in this equation",
    ", we derive the average of consistent bits in a minimal system consisting of two agents ( coder / decoder ) .",
    "consistent information has been obtained by mathematically inserting the dual nature of the communicative sign -which forces the explicit presence of coder , channel and decoder modules- and subsequently selecting the subset of correlations by which the input symbol ( the specific realization of @xmath11 ) is equal to the output symbol ( i.e. , the specific realization of @xmath40 ) .",
    "( [ infop ] ) accounts for the ( possibly ) symmetrical nature of the communicative exchange among agents : a priori , all agents can be both coder and decoder , and we have to evaluate and average the two possible configurations .",
    "the information - theoretic flavour of @xmath102 enables us to study the conservation of referentiality from the well - grounded framework of information theory .",
    "so far we have been concerned with the derivation of the amount of information which is consistently decoded , taking into account the dual nature of the communicative sign -equations ( [ saussureant ] ) , ( [ dualinfo ] ) and ( [ infop ] ) .",
    "now we explore some of its properties , and we highlight the conceptual and quantitative differences between @xmath103 and @xmath104 .    to study the behavior of @xmath103 and its relation to @xmath105 , we will isolate the first three most salient features .",
    "specifically , we shall concern ourselves with the following logical implications : @xmath106 the first @xmath107 implication refers to the perfect conservation of referentiality , which , in turn , implies maximum mutual information .",
    "however , the inverse , @xmath108 , is not generally true , since , as we shall see , there are many situations by which the mutual information is maximum although there is no conservation of referentiality .",
    "furthermore , we consider a third case , the noisy channel ( which implies that @xmath109 ) . in this case : @xmath110    we begin with the implications @xmath107 and @xmath108 . in both cases ,",
    "the whole process is noiseless , since from eq .",
    "( [ i(x : y)=h(x ) ] ) @xmath111 . to address the first logical implication , @xmath107",
    ", we obtain the typology of configurations of @xmath112 leading to @xmath113 .",
    "we observe that the condition ( [ sigma1 ] ) is achieved if @xmath114 , i.e. , the identity matrix : @xmath115 such a condition only holds if @xmath116 since given a square matrix @xmath117 , @xmath118 -provided that @xmath119 exists . from the conditions imposed over the transition matrices provided in eqs .",
    "( [ normap],[normaq],[p(|)=n ] ) , the above relation is fullfilled if and only if all the matrices @xmath120 are _ permutation matrices_. let us briefly revise this concept , which will be useful in the following lines .",
    "permutation matrix _ is a square matrix which has exactly one entry equal to 1 in each row and each column and 0 s elsewhere .",
    "for example , if @xmath121 , we have 6 permutation matrices , namely : @xmath122 @xmath123 the set of @xmath124 permutation matrices is idicated as @xmath125 and it can be shown that , if @xmath126 , @xmath127 and , if @xmath128 , the product @xmath129 .",
    "furthermore , it is clear that @xmath130 . if we translate the above facts of permutation matrices to our problem , we find that @xmath131 is achieved if : @xmath132 leading to the following chain of equalitites , which only holds in this special case : @xmath133    case @xmath108 is easily demonstrated by observing that , if @xmath134 , then @xmath135 and thus @xmath136 leading to : @xmath137 which is achieved only imposing that @xmath138 however , as we saw above , only a special configuration of permutation matrices leads to @xmath139 .",
    "thus , for the majority of cases where @xmath140 , the conservation of the referentiality fails , leading to @xmath141 unless condition ( [ pq^t ] ) is satisfied .",
    "let us notice that there are limit cases where , although @xmath140 , @xmath142 , since it is possible to find a configuration of @xmath143 such that @xmath144 is a permutation matrix with all zeros in the main diagonal , leading to @xmath145 .",
    "case @xmath146 is by far the most interesting , since natural systems are noisy , and the conclusion could invalidate some results concerning the information measures related to systems where referentiality is important .",
    "the first inequality trivially derives from equation ( [ info1 ] ) , from which we conclude that @xmath147 .",
    "the argument to demonstrate the second inequality lies on the following implication : @xmath148 indeed , let us proceed by contradiction : let us suppose that @xmath149 . then ,",
    "as discussed above , @xmath150 . but",
    "this should imply that @xmath151 , thus contradicting the premise that @xmath152 .",
    "this has a direct consequence . since such conditional probabilities satisfy eq .",
    "( [ p(|)=n ] ) , then , more than @xmath153 matrix elements of @xmath154 must be different from zero .",
    "the same applies to the matrix of joint probabilities @xmath84 and thus it also applies to @xmath155 . since the trace is a sum of @xmath153 elements , it should be clear that , under noise : @xmath156 leading to : @xmath157 thus recovering the chain of inequalities provided in eq . ( [ dissip ] ) : @xmath158 if we expand the reasoning to the symmetrical consistent information @xmath159 defined in ( [ infop ] ) : @xmath160    we see that referentiality conservation introduces an extra source of dissipation of information . in those scenarios where referentiality conservation is an important advantage ,",
    "the dissipation of information , @xmath161 , among two agents has two components : @xmath162 being the amount of useful information provided by consistent information , namely : @xmath163",
    "as an illustration of our general formalism , let us consider the standard example of a binary symmetric channel where we have two agents , @xmath164 , sharing a world with two events , namely @xmath165 such that @xmath166 .",
    "@xmath167    * case 1 : non - preservation of referentiality*. we will consider a case where @xmath168 but @xmath169 .",
    "the transition matrices of agents @xmath2 and @xmath3 are identical and defined as : @xmath170 the channel between such agents , @xmath25 , is noiseless : @xmath171    we begin by identifying the different elements involved in the process .",
    "first , from eq .",
    "( [ mu ] ) we obtain : @xmath172 the matrix of joint probabilities , @xmath84 , is -see eq . ( [ j ] ) : @xmath173 thus , rearranging terms , the mutual information from @xmath3 to @xmath2 -see ( eq . [ info2kull])- will be : @xmath174 we observe that , for a communication system consisting of two possible signals , @xmath175 thus the mutual information is maximum .",
    "however , it is evident that such a system does not preserve referentiality , since , if @xmath176 , then @xmath177 , and viceversa .",
    "indeed , let us first obtain the matrix @xmath178 , which will be : @xmath179 and , thus , by its definition , the referential term will be ( eq .",
    "[ saussureant ] ) : @xmath180 ( notice that @xmath181 , although we keep the logarithm for the sake of clarity ) being the amount of consistent information : @xmath182 this extreme case dramatically illustrates the non - trivial relation between @xmath183 and @xmath105 , proposing a situation where the communication system is completely useless , although the mutual information between the random variables depicting the input and the output is maximum .",
    "@xmath167    * case 2 : preservation of the referentiality*. in this configuration , the referentiality is conserved .",
    "let us suppose a different configuration of the agents .",
    "now the transition matrices of agents @xmath2 and @xmath3 are identical and defined as : @xmath184 the channel between such agents , @xmath25 , is the two - dimensional noiseless channel defined in eq .",
    "( [ noiselessch ] ) .",
    "it is straightforward to check that the mutual information is maximal ( @xmath185 ) , as above .",
    "the matrix @xmath178 will be , now , @xmath186 this leads to @xmath113 , and , consequently : @xmath187 the above configuration is the only one which leads to @xmath188 .",
    "furthermore -as shown in section iii.b- it can only be achieved when @xmath105 is maximum , i.e. , in a noiseless scenario . in the last example we will deal with a noisy situation .",
    "@xmath167    * case 3 : noisy channel*. we finally explore the case where the matrix configuration of agents is the same as in the above example ( eq . [ exemple2 ] ) but",
    "the channel is noisy , namely : @xmath189 we first derive the matrix of joint probabilities , @xmath190which takes the following form : @xmath191 we now proceed by observing that @xmath192 .",
    "thus , the mutual information will be : @xmath193 to evaluate the degree of consistency of the communicative system , we firstly compute the matrix @xmath178 : @xmath194 since @xmath195 bits , the referential parameter is : @xmath196 ( where the last  bit \" refers to  bit obtained from the observation of input - output pairs \" ) .",
    "the consistent information is , thus : @xmath197 due to the symmetry of the problem , the average among the two agents is : @xmath198 the amount of dissipated information is , thus : @xmath199 we want to stress the following point : the matrix configuration is consistent with the framework proposed in _ case 2 _ , where the amount of consistent information is maximum , but now the channel is noisy .",
    "the noisy channel has a double effect : first , it destroys information in the standard sense , since the noise parameter @xmath109 , but it also has an impact on the consistency of the process , introducing an amount of referential _ noise _ due to the lack of consistency derived from it .",
    "thus , as derived in section iii.b , eq .",
    "( [ dissip ] ) , in the presence of noise , we have shown that the inequalities @xmath200 hold , being , in our special case : @xmath201",
    "the accurate definition of the amount of information carried by consistent input / output pairs is an important component of information transfer in biological or artificial communicating systems . in this paper",
    "we explore the central role of information exchanges in selective scenarios , highlighting the importance of the referential value of the communicative sign .",
    "the conceptual novelty surrounding the paper can be easily understood from the role we attribute to _ noise_. physical information considers a source of @xmath56 bits and a _ dissipation _ of @xmath59 bits due to , for example , thermal fluctuations .",
    "we add another source of information dissipation : the non - consistency of the pair signal / referent , putting aside the degree of correlation among random variables ( see eq .",
    "[ dissipation ] ) .",
    "indeed , in many physical processes no referentiality is at work , perhaps because , it is not relevant to wonder about the consistency of the communicative process .",
    "moreover , if the whole system is _ designed _ , consistency problems are apriori ruled out , unless the engineer wants to explicitly introduce disturbances in the system . what makes biology different",
    ", however , is that biological systems are not designed but instead , are the outcomes of an evolutionary process where the nature of the response to a given stimulus is important , which makes the problem of consistency relevant for evolutionary scenarios .",
    "this problem needs an explicit formulation , being what we called _ consistent information _ the theoretical object that links raw information and function , or environmental response .    are information processing mechanisms of living systems optimal regarding referentiality conservation ?",
    "as we discussed above , it seems reasonable to assume that the conservation of referentiality must be at the core of any communicative system with some selective advantage . the general problem to find the optimal code",
    ", however , resembles the problem of finding the channel capacity , for which is well known that no general procedure exists @xcite .",
    "thus , how autonomous systems deal with such a huge mathematical problem ?",
    "one may consider the possibility that the co - evolution of the abstract coding and decoding entities ; this would avoid the system to face a great amount of configurations per generation , thereby being all options highly limited at each generation where selection is at work .",
    "we finally emphasize that the unavoidable dissipation of mutual information points to a reinterpretation of information - transfer phenomena in biological or self - organized systems , due to the important consequences that can be derived from it .",
    "further work should explore the relevance of this limitation on more realistic scenarios , together with other implications that can be derived by placing equation ( [ dualinfo ] ) at the center of information transfer in biology .",
    "we thank the members of the complex systems lab for useful discussions .",
    "this work has been supported by a juan de la cierva grant from te ministerio de ciencia y tecnologa ( jf ) , the james s. mcdonnell foundation ( bcm ) and by santa fe institute ( rs ) ."
  ],
  "abstract_text": [
    "<S> one of the most basic properties of the communicative sign is its dual nature . </S>",
    "<S> that is , a sign is a twofold entity composed of a formal component , which we call _ signal _ , and a referential component , namely a _ </S>",
    "<S> reference_. based on this conception , we say that a referent is coded in a particular sign , or that a sign is decoded in a particular referent . in selective scenarios </S>",
    "<S> it is crucial for the success of any adaptive innovation or communicative exchange that , if a particular referent @xmath0 is coded in a particular signal @xmath1 during the coding process , then the referent @xmath0 is decoded from the sign @xmath1 during the decoding process . in other words </S>",
    "<S> the _ referentiality of a signal _ must be preserved after being decoded , due to a selective pressure . despite the information - theoretic flavour of this requirement , an inquiry into classical concepts of information theory such as entropy or mutual information </S>",
    "<S> will lead us to the conclusion that information theory as usually stated does not account for this very important requirement that natural communication systems must satisfy . </S>",
    "<S> motivated by the relevance of the preservation of referentiality in evolution , we will fill this gap from a theoretical viewpoint , by deriving the consistent information conveyed from an arbitrary coding agent @xmath2 to an arbitrary decoding agent @xmath3 and discussing several of its interesting properties .     </S>"
  ]
}