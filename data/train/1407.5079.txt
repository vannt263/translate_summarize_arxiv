{
  "article_text": [
    "an equivalence test is a statistical hypothesis test whose inferential goal is to establish practical equivalence rather than a statistically significant difference [ @xcite ] .",
    "these tests arise from the fact that within the frequentist paradigm , failing to reject a null hypothesis of no difference is not logically equivalent to accepting said null .",
    "examples of scenarios requiring equivalence tests include the assessment of a  generic drug s performance relative to a  brand name drug and method comparison studies , in which the agreement of a  new device with the `` gold - standard '' for measuring a particular phenomenon must be assured before the new device can replace the old one .",
    "equivalence tests for scalar data typically involve the establishment of upper and lower equivalence thresholds dependent on the metric of equivalence being used .",
    "the inferential aim is to establish that the metric falls within the upper and lower equivalence thresholds with a prespecified type i error rate .",
    "see @xcite for a comprehensive overview of commonly used procedures .",
    "oftentimes the use of scalar data is adequate , but in some instances the question of practical equivalence can not be reduced to a hypothesis regarding scalar data .    the motivation for this research arose from a method comparison study between a new device for assessing pulmonary function , structured light plethysmography ( slp ) , and the industry standard for such assessments , a spirometer .",
    "slp holds many advantages over spirometry : it is noninvasive , it can be used to diagnose patients of a wider range of age and health levels , and it provides detailed information regarding specific regions of the lung that may be malfunctioning . before slp may be used extensively for diagnostic purposes it must be assured beyond a reasonable doubt that the measurements obtained by slp are practically equivalent to those produced by a spirometer .",
    "doctors rely on a host of information that can be produced both by slp and by spirometry .",
    "some of these measurements are scalar and , hence , their equivalence can be addressed using available scalar methods ; however , not all diagnostic tools utilized are scalar .",
    "for example , the `` flow - volume loop '' is a phase plot of flow of air into and out of the lungs versus volume of air within the lungs over time for each breath .",
    "this plot allows doctors to investigate the relationship between flow and volume at various points in time during a given breath , which can indicate whether one has normally functioning lungs , suffers from an obstructive airway disease ( such as asthma ) , suffers from a restrictive lung disease ( such as certain types of pneumonia ) , or rather has another condition altogether .",
    "in fact , certain pulmonary ailments are associated with certain shapes of these loops .",
    "figure  [ figdisease ] shows flow - volume loops for healthy patients and for patients with varying pulmonary ailments [ @xcite ] .    ]",
    "@xcite discuss a frequentist approach for comparing two functions through the use of a fourier basis expansion .",
    "behseta and kass ( @xcite ) propose a bayesian method for assessing the equality of two functions using a nonparametric regression method known as bayesian adaptive regression splines ( bars ) .",
    "neither of these approaches uses the idea of establishing _ practical _ equivalence ; rather , both papers test strict equality between the functions of interest , and in fact set strict equality as the null hypothesis and lack thereof as the alternative . in this paper",
    ", we propose a framework for functional equivalence testing that is analogous to its univariate counterpart .",
    "this involves an extension of scalar techniques to the functional realm and a modification of said techniques when a simple extension is not possible . in so doing",
    ", the inferential objective becomes to establish that a _ functional _",
    "metric of equivalence lies within a tolerance region with a prespecified type i error rate .",
    "we then discuss methods for equivalence testing within the frequentist and bayesian paradigms , and illustrate these techniques with data from the method comparison study between slp and spirometry .",
    "we further introduce a bayesian model for heteroscedastic functional data inspired by the work of @xcite that separately places priors on the correlation structure and the underlying variance functions .",
    "in the scalar case , equivalence testing begins by defining a metric whose value can be used to assess equivalence between the two populations of interest , say , @xmath0 .",
    "common choices include the difference between group means , @xmath1 , and the difference of logarithms of group means , @xmath2 ( provided one s data are strictly positive ) .",
    "one then chooses lower and upper thresholds , @xmath3 and @xmath4 , such that we can reject or fail to reject nonequivalence depending on whether or not @xmath0 falls between @xmath3 and @xmath4 .",
    "the null hypothesis is nonequivalence and the alternative is equivalence : @xmath5 a common approach for conducting this hypothesis test within the frequentist paradigm is known as a two one - sided test ( tost ) [ @xcite ] . as the name suggests ,",
    "this is a two step procedure . in no particular order ,",
    "one separately tests for the alternatives that @xmath6 and @xmath7 with each test being conducted with size @xmath8 . if one successfully rejects for both tests , practical equivalence",
    "may then be suggested at size @xmath8 ; otherwise , one fails to suggest practical equivalence .",
    "the lack of compensation in the significance level of the individual tests ( say , to @xmath9 ) follows immediately from the theory of intersection - union tests ( or iuts ) , which are tests for which the null parameter space can be described as the union of disjoint sets , and the alternative as the intersection of the complements of those sets .",
    "one can see that an equivalence test is an iut [ @xcite ] , as its null region is @xmath10 \\cup[\\kappa_u,\\infty)\\}$ ] and its alternative region is @xmath11 .",
    "the tost testing procedure can suffer from a lack of power . @xcite and",
    "@xcite propose procedures which are uniformly more powerful for the scalar case , however , these methods are themselves quite complicated even when dealing with univariate data , to such an extent that tost continues to be the method of choice in the vast majority of applications .",
    "we proceed within the tost framework , which not only has intuitive appeal but can also be naturally extended to a test of equivalence for functional data within the frequentist paradigm .",
    "the most common goal of equivalence testing is to prove equivalence of means , but this may not be sufficient . @xcite and",
    "@xcite both suggest that in addition to comparing mean responses , the variance of the two responses should also be compared , as a device or drug with smaller variability may be preferred .",
    "we will thus include a test for equivalence of variance in our testing procedure .",
    "we now extend the equivalence testing framework to the functional regime .",
    "let @xmath12 denote a functional measurement of similarity between the location parameters of two functions .",
    "one potential choice for @xmath12 is the difference between overall mean functions .",
    "@xmath13 , but the choice of @xmath12 should depend on the nature of the inference being conducted .",
    "let @xmath14 and @xmath15 denote lower and upper equivalence _ bands _ , which again vary over the same continuum as do the functional data .",
    "these bands are chosen such that practical equivalence can be suggested or refuted depending on whether or not @xmath12 falls entirely within @xmath14 and @xmath15 .    for testing the equivalence of variability of the functional data ,",
    "let @xmath16 be a  measurement of similarity between spreads of the populations .",
    "choices may include @xmath17 , the ratio between the variance functions of the two populations , or @xmath18 , the difference between the two variances .",
    "we again establish upper and lower bands , @xmath19 and @xmath20 , within which we can suggest practical equivalence of variance functions .",
    "the null and alternative hypotheses for the tests of location and spread can then be stated as follows : @xmath21 note that the above test , in aggregate , is an iut ; the alternative space is @xmath22 . in order to test these hypotheses within the frequentist paradigm , we propose conducting two tost procedures , one each for the location and spread parameters .",
    "since this is an iut , each of the four total hypothesis tests can be conducted at significance level @xmath8 to arrive at an overall size of @xmath8 .",
    "details of our frequentist testing procedure can be found in section  [ secfrequentist ] . sections  [ sec8 ] and  [ sec9 ] also discuss conducting this test as a bayesian .",
    "falling outside of the equivalence region for variability need not be a condemnation ; to the contrary , whichever population has markedly smaller variability could be favored on those grounds .",
    "if one were comparing a gold standard to a new device and the new device had markedly lower variation , that would strengthen the case for the introduction of the new device into the market .",
    "hence , in the case of method comparison studies , a simple one - sided test of noninferiority may be sufficient for comparing residual variability .",
    "note that , in practice , functional data are measured along a finite grid of values .",
    "thus , the grid must be fine enough such that areas of potential dissimilarity along the domain are not ignored .",
    "as was explained in section  [ secintro ] , we are interested in whether or not the flow - volume loops produced by spirometry are practically equivalent to those produced by slp in terms of location and variability . measurements for volume over time and flow over time were recorded in 2009 for 16 individuals , with the devices set up such that each breath was simultaneously recorded by slp and spirometry .",
    "these data were not the result of a clinical trial and , hence , our use of the data serves exposition of our methodology rather than an argument for the equivalence of slp and spirometry .",
    "our analysis herein focuses on using the 453 pairs of volume over time curves measured by both devices on these 16 patients to assess the equivalence of slp and spirometry .",
    "figure  [ figoverlay ] shows the visual correspondence between these volume over time plots for slp and spirometry from an individual .",
    "the data require preprocessing before our analysis can proceed , as we must break our recordings into individual breaths that are aligned between devices and that are comparable in terms of their domains and scale ; see the supplementary materials [ @xcite ] for details .",
    "this results in 453 pairs of breaths , where each breath is measured at 25 equispaced time points , time is scaled to the interval @xmath23 $ ] , and time @xmath24 for slp corresponds with time @xmath24 for spirometry within each pair to the best of our ability .",
    "we use a functional analysis of variance model with cross - covariance between pairs of functions for our data",
    ". functional analysis of variance models are appropriate when one s data are comprised of functional responses that are believed to differ from one another solely due to certain categorical variables [ @xcite ] .",
    "our model states that we can express the measured volume in the lungs of person @xmath25 using both devices ( denoting slp by @xmath26 and spirometry by @xmath27 ) in the @xmath28th breath at time @xmath29 as follows : @xmath30 & = & \\left [ \\begin{array}{c } \\alpha_{i,1}(t ) \\\\",
    "\\alpha_{i,2}(t ) \\end{array } \\right ] + \\left [ \\begin{array}{c } \\epsilon_{i,1,k}(t ) \\\\",
    "\\epsilon_{i,2,k}(t ) \\end{array } \\right ] , \\\\ \\left [ \\begin{array}{c } \\alpha_{i,1}(t ) \\\\",
    "\\alpha_{i,2}(t ) \\end{array } \\right ] & = & \\left [ \\begin{array}{c } \\mu_{1}(t ) \\\\",
    "\\mu_{2}(t ) \\end{array } \\right ] + \\left [ \\begin{array}{c } \\varepsilon_{i,1}(t ) \\\\",
    "\\varepsilon_{i,2}(t ) \\end{array } \\right].\\end{aligned}\\ ] ] in this model @xmath31 $ ] represent the overall mean volume over time trajectory for each device .",
    "we model the pairs @xmath32\\}$ ] as random effects , as we think of the individuals as draws from a larger population .",
    "the terms @xmath33\\}$ ] are the mean zero error functions for the realized volume over time trajectory of each pair of devices , assumed to be independent between breaths while allowing for both strong autocorrelation along the domain of a given breath and cross - correlation between two breaths in a given pair .",
    "this means that not only is there correlation between the value of the functions at times @xmath24 and @xmath34 for each breath from a specific device , but there will also be a correlation between the observation at time @xmath24 from slp and the observation at time @xmath34 from the spirometer .",
    "denote the variance functions of these errors by @xmath35 $ ] .",
    "the terms @xmath36\\}$ ] are the mean zero error functions for each patient s pair of random effects , assumed to be independent between patients while allowing for both strong autocorrelation along the domain of a given breath and cross - correlation between random effects in a given pair .",
    "denote the variance functions of these random effects by @xmath37 $ ] .      for our analysis ,",
    "we define @xmath38 , @xmath39 . in addition",
    ", we want to assure ourselves that the variabilities of the random effect functions are similar between the two populations ; otherwise , there may be evidence of a systematic bias .",
    "as such , we define a third metric of equivalence as @xmath40 .",
    "research is currently being conducted to ascertain proper values for upper and lower equivalence bands for our measures of equivalence of location and spread .",
    "these equivalence bands must be established via consultation of field experts ( in our case , with pulmonary specialists ) . for the purpose of illustrating the methodology outlined herein , however , we set reasonable equivalence bands based on the fact that the time points immediately before , during , and immediately after maximal volume is attained are critical for diagnostic purposes : @xmath41 ; @xmath42 ; @xmath43 ; @xmath44 .",
    "we use the same sets of equivalence bands for the error variances and the random effect variances , although in practice these should be chosen separately .",
    "the class of equivalence bands need not be symmetric , as this assumption may be unrealistic ; we have merely done so for simplicity .",
    "figure  [ figlocthreshold ] shows the locational discrepancy between volume curves if the true differences between devices truly were at the upper and lower thresholds of equivalence we have specified .     and @xmath15 applied . ]",
    "we propose using the nonparametric bootstrap [ @xcite ] for assessing equivalence by constructing pointwise confidence intervals for each metric of equivalence , and then using the duality between confidence intervals and pointwise hypothesis tests to conduct our inference .",
    "we begin with a testing procedure for i.i.d",
    ". data , as we imagine many situations encountered in practice will be of this form .",
    "we then discuss a procedure for testing within a random effects model .",
    "allowing for random effects is useful for repeated measures data such as our pulmonary device data . through our exposition",
    ", we illustrate why pointwise coverage of our confidence intervals is actually sufficient for guaranteeing that the resultant inference is of the desired size .",
    "we use the difference in mean functions , @xmath45 and the ratio of variance functions @xmath46 , as metrics for equivalence .",
    "let @xmath47 and @xmath48 denote the @xmath49 and @xmath50 observations from groups 1 and 2 , respectively , and let @xmath51 denote the sample mean functions .",
    "we use @xmath52 and @xmath53 as our test statistics for the hypothesis test , and use the nonparametric bootstrap to derive pointwise confidence intervals for the corresponding parameters .",
    "we then use the duality between one - sided confidence intervals and one - sided tests to reject or fail to reject nonequivalence .    in each bootstrap simulation",
    ", we do the following :    sample @xmath49 curves _ with replacement _ from the curves in group 1 , and sample @xmath50 curves _ with replacement _ from the modified curves in group  2 .",
    "compute the pointwise mean curve from these samples and the pointwise variance curves for each population .",
    "denote these as @xmath54 and @xmath55 .",
    "compute @xmath56 and @xmath57 .    store this value .",
    "next , we find upper and lower one - sided pointwise @xmath58 confidence intervals",
    ". let @xmath59 $ ] denote the @xmath60-quantile for the distribution of @xmath61 evaluated at time @xmath24 .",
    "then , we define our upper and lower pointwise confidence intervals for @xmath62 using a bias correcting percentile - based bootstrap as discussed in @xcite : @xmath63 , \\infty\\bigr ) , \\\\",
    "c^l_{1-\\alpha}\\bigl(\\theta(t)\\bigr ) & = & \\bigl(-\\infty , 2\\hat { \\theta}(t ) - q_{1-\\alpha}\\bigl[\\hat{\\theta}^{*}(t)\\bigr]\\bigr).\\end{aligned}\\ ] ] at any particular poin @xmath24 , @xmath64 and @xmath65 can be interpreted as the set of all @xmath66 such that we fail to reject the null that @xmath67 and @xmath68 , respectively . as such , if our lower equivalence band at time @xmath24 , @xmath69 , is outside of @xmath64 , then we can reject the null that @xmath70 at the point @xmath24 . likewise",
    ", if @xmath71 is outside of @xmath65 , then we can reject the null that @xmath72 at the point @xmath24 .",
    "our upper and lower pointwise confidence interval for @xmath73 take on a different form .",
    "this is because dispersion measures are not typically variance stabilized . in such cases ,",
    "conventional bootstrap intervals fail to attain their advertised coverage probabilities in small samples .",
    "we imagine that most test statistics for testing equivalence of dispersion will be based on the sample variance . for many distributions ( including the normal ) , transforming by the logarithm results in an estimator whose variance is stabilized .",
    "hence , we instead construct upper and lower one - sided confidence intervals for the variance stabilized quantity @xmath74 , and then utilize the monotonicity of the log transform to result in confidence intervals for  @xmath73 , @xmath75 , \\infty\\bigr ) , \\\\",
    "c^l_{1-\\alpha}\\bigl(\\lambda(t)\\bigr ) & = & \\bigl(0 , \\bigl(\\hat { \\lambda}(t)\\bigr)^2\\times q_{\\alpha}\\bigl[1/\\hat { \\lambda}^{*}(t)\\bigr]\\bigr].\\end{aligned}\\ ] ] these intervals can be used to test whether @xmath73 is below the upper equivalence band and above the lower equivalence band at any point @xmath24 .",
    "if one is concerned about the log transform providing variance stabilization , another approach to constructing these confidence intervals would be to estimate a variance stabilizing transformation within the bootstrap framework [ see @xcite ] .",
    "we now have tests for whether or not we have equivalence of location and spread at any point @xmath24 . to test for overall equivalence , we conduct tests at each domain point based on the @xmath58 pointwise interval at all points @xmath76 and reject the null of nonequivalence only if all of the individual tests result in a rejection .",
    "to see why there is no need to correct for simultaneous comparisons , let @xmath77 be a partition of the domain where @xmath78 contains the points for which the null hypothesis is true and @xmath79 contains the points for which the alternative is true for any true metric of equivalence in the set of nonequivalence .",
    "then , the probability of a false rejection is bounded as follows : @xmath80 hence , pointwise @xmath8 tests of hypothesis guarantee size of at most @xmath8 .",
    "in fact , if one had further information regarding the correlation between test statistics , these tests could be done at a size larger than @xmath8 , since our decision to reject nonequivalence is an intersection of tests . as an example , if our function were defined on a grid of size @xmath81 , our test statistics were independent , and we wanted an overall size of @xmath82 , we could then run our tests using @xmath83 . in the absence of such knowledge , conducting the pointwise tests at size @xmath8 is actually a tight upper bound . to see this , consider an equivalence metric that is in the equivalence region at all points along the domain except for @xmath84 , at which its value equals that of the equivalence band . if the probabilities of correct rejection at all points @xmath85 are sufficiently close to one , then essentially the type one error rate is the size of the test at @xmath84 , which is @xmath8 . in section  [ secsize",
    "] , we give an example where the overall size approaches the upper bound @xmath8 .      for paired functions ( commonly arising in comparison studies where simultaneous measurements using two devices are possible ) ,",
    "slight alterations are required in the bootstrapping procedure .",
    "we again use the difference in mean functions , @xmath86 and the ratio of variance functions @xmath87 , as metrics for equivalence .",
    "let @xmath88 be the paired curves , and let @xmath89 denote the total number of pairs .",
    "the bootstrap procedure is as follows :    sample @xmath89 pairs of curves _ with replacement _ from the original sample .",
    "compute the pointwise mean curve from these samples and the pointwise variance curves for each population .",
    "denote these as @xmath90 $ ] and @xmath91 $ ] .",
    "compute @xmath56 and @xmath57 .",
    "record this value .",
    "now that our bootstrap samples have been acquired , the rest of the procedure is identical to that explained in section  [ secindependent ] .",
    "we now describe a nonparametric bootstrap procedure for paired random effects and paired responses .",
    "the procedure for nonmatched data would replace sampling pairs with sampling individually from two populations and , hence , we omit its discussion herein .",
    "see @xcite for an overview of random effect bootstrapping procedures .",
    "suppose our data consist of @xmath92 individuals with pairs of random effects @xmath93 \\stackrel { \\mathrm{i.i.d.}}{\\sim}f$ ] with mean @xmath94 $ ] and variance @xmath95 $ ] .",
    "for each individual @xmath96 $ ] , we observe @xmath97 pairs of curves with @xmath98 \\stackrel{\\mathrm{i.i.d.}}{\\sim } g_i$ ] with mean @xmath99 $ ] and variance @xmath100 $ ] .",
    "let @xmath101 denote the total number of curves .",
    "our test for equivalence will , as before , focus on the location metric @xmath102 and metric of equivalence of error variabilities , @xmath103 . as described in section  [ secbands ] ,",
    "we also include a third metric , the ratio of random effect variances of the two populations : @xmath104 .",
    "let @xmath105 be the overall mean curve for coordinate @xmath106 and let @xmath107 be the mean curve for coordinate @xmath106 of individual @xmath25 .",
    "now , define @xmath108 , and let @xmath109 .",
    "our estimators for these metrics of equivalence will be based on their univariate random effect counterparts derived via anova .",
    "see @xcite for a description of methods for univariate random effect analysis .",
    "begin by defining our estimate of the random effect variance curve by @xmath110 , @xmath111 .",
    "then , we define our test statistics as @xmath112 and @xmath113 .",
    "our estimators for the random effects will be @xmath114 . based on these",
    ", we estimate our location metric , @xmath12 , by @xmath115 .",
    "denote @xmath116 .",
    "we then consider these @xmath117 pairs as a reservoir from which to draw error functions in the bootstrap simulation , rather than maintaining a correspondence between random effects and residuals from that random effect s group .",
    "this ignores the sample covariance between residuals from the same group and slight heteroscedasticity if the design is unbalanced .",
    "we doubt that this would have a substantial impact on the inference being performed ( which the simulation studies of section  [ seccompare ] seem to suggest ) , but leave a proper investigation for future work .    before beginning the bootstrap ,",
    "we adjust our estimates of the random effects such that the ratio of the variances of the pool of random effects used in the bootstrap matches up with our estimate of the random effect variance .",
    "we define the following adjusted random effects : @xmath118 here , @xmath119 is the standard deviation of our estimated group means evaluated pointwise .",
    "this transformation guarantees that the variances of the random effects used in the bootstrap are the same as our estimate of that variance .",
    "as noted in @xcite and @xcite , this step is required to assure that the confidence intervals produced by the bootstrap procedure are consistent .",
    "we then proceed as follows :    sample @xmath92 pairs of random effects from @xmath120\\}$ ] with replacement .",
    "call them @xmath121^{*}\\}$ ] . the first pair drawn",
    "gets assigned @xmath49 as the number of pairs of curves to be drawn within that group , the second gets assigned @xmath50 , etc .    for each @xmath25 ,",
    "draw @xmath97 pairs of residuals with replacement from @xmath122\\}$ ] . call these @xmath123^*\\}$ ] .",
    "define @xmath124^ * = [ \\hat{a}_{i,1}(\\cdot ) , \\hat{a}_{i,2}(\\cdot)]^{*}+ [ r_{i,1,k}(\\cdot ) , r_{i,2,k}(\\cdot)]^*$ ] .",
    "estimate @xmath125 based on the bootstrap sample @xmath126^*\\}$ ] .",
    "estimate @xmath127 based on these quantities .",
    "we can create pointwise @xmath58 confidence intervals for @xmath128 and @xmath16 just as we did in section  [ secindependent ] . for @xmath129 , we define our confidence intervals in the same manner as we did with @xmath16 , @xmath130 , \\infty\\bigl ) , \\\\",
    "c^l_{1-\\alpha}\\bigl(\\psi(t)\\bigr ) & = &   \\bigl(0 , \\bigl(\\hat{\\psi}(t ) \\bigr)^2\\times q_{\\alpha}\\bigl[1/\\hat{\\psi}^{*}(t ) \\bigr]\\bigr].\\end{aligned}\\ ] ] as before , these confidence intervals can be used to test whether @xmath131 is below the upper equivalence band and above the lower equivalence band at any point @xmath24 .",
    "we now conduct our equivalence test using the methods described in section  [ secfrequentist ] for paired random effects .",
    "we drew 10,000 bootstrap samples and used @xmath82 to carry out these tests .",
    "we find that figure  [ figbootlocation ] is a powerful visual display of the results of this tost procedure . in each plot",
    ", we display the upper and lower equivalence bands .",
    "we also display the upper band of the region @xmath132 and the lower band of the region @xmath133 .",
    "recall that we can reject the null if the upper equivalence band lies entirely outside the region @xmath132 and if the lower equivalence band lies entirely outside the region @xmath133 .",
    "hence , it is sufficient to check whether or not either the upper or lower equivalence band at any point intersect the region defined by the overlap of the two one - sided confidence regions , which is shaded in the plots .",
    "intersection implies failure to reject , and lack thereof implies rejection of nonequivalence in favor of equivalence .",
    "based on figure  [ figbootlocation ] , we conclude that we can suggest equivalence for our locational metric , but fail to reject the null of nonequivalence for variability of both errors and random effects .",
    "we believe it will always be the case that a two - sided test for the variability of random effects is appropriate , as deviations in either direction indicate substantial differences in the distribution of the individual level mean curves ; however , for certain applications ( ours included ) , lower error variance will be strictly preferred . if we thus restrict ourselves to only having the ratio of error variances below the upper equivalence threshold , then we would also reject the null of noninferiority of error variability .",
    "note that there does appear to be an inflation of error variance by a factor of 1.5 at the beginning of each breath for slp relative to spirometry . though the ratio between the two variances is high at this point",
    ", the actual magnitude of the variances at the beginning of these curves is extremely small for both devices , which results in the high value for the ratio of variances .",
    "as in the frequentist case , we suggest using functional measures of location and spread to assess practical equivalence , however , carrying out a tost hypothesis test is not required within the bayesian paradigm . rather than conducting a stochastic proof by contradiction , the bayesian paradigm allows us to directly compute posterior probabilities of our functional metrics of equivalence falling entirely within specified equivalence ranges .",
    "that is , the bayesian paradigm allows for direct computation of @xmath134 for each of the equivalence hypotheses . in light of this , we propose that the researcher conduct the following three steps when using the bayesian framework for equivalence testing :    define an equivalence region through expert consultation .",
    "define a probability value , call it @xmath135 , such that if @xmath136 , equivalence may be suggested . using the suggestions of @xcite and @xcite",
    ", a value of @xmath137 or @xmath138 may be appropriate .",
    "specify prior distributions for the metrics of equivalence that are commensurate with the researcher s prior belief of the alternative being true relative to the null .",
    "the specifics of this implementation depend on the types of prior distributions used to model the parameters and data . in section  [ secbayesmethod ]",
    "we discuss the use of gaussian processes in modeling both our data and parameters and describe a model that allows for specification of priors and posterior inference for our metrics of equivalence .",
    "though gaussian processes are a rich and flexible class of distributions for functional data , a valuable extension of our work would be conducting bayesian equivalence testing for functional data using nonparametric models .",
    "kaufman and sain ( @xcite ) discuss using functional anova modeling within the bayesian paradigm .",
    "they begin by assuming that the functional data are realizations of an underlying gaussian process with a mean function depending on the factor levels and a covariance function that describes the dependence between points along the function s domain .",
    "they further assume that the covariance between errors can be aptly specified as a member of the class of matrn covariance functions [ @xcite ] .",
    "the specification of a correlation function works to impose smoothness between estimated function values and to allow for interpolation at unobserved domain values .",
    "gaussian process priors with matrn covariance functions are used for the mean functions themselves , which allows for the incorporation of a priori beliefs about both smoothness and location .",
    "the assumption of homoscedastic variances along the function s domain is problematic for us , as allowing the error and random effect variances to change with time is vital to our investigation of equivalence .",
    "we consider a more flexible class of covariance and cross - covariance functions : @xmath139 . here , @xmath140 is the error standard deviation function for device @xmath106 evaluated at time @xmath24 , and @xmath141 is either the correlation function for device @xmath106 for observations at times @xmath24 and @xmath34 if @xmath142 or the cross - correlation function between the error at time @xmath24 for device @xmath25 and the error at time @xmath34 for device @xmath106 if @xmath143 .    to simplify notation , let @xmath144 denote the set containing all of our parameters .",
    "then , we can write our multivariate gaussian process model for our responses : @xmath145 \\right| \\xi \\stackrel{\\mathrm{indep}}{\\sim } \\operatorname{mvgp } \\left ( \\left [ \\begin{array}{c } \\alpha_{i,1 } ( \\cdot ) \\\\ \\alpha_{i,2 } ( \\cdot ) \\end{array } \\right ] , \\left [ \\begin{array}{c@{\\quad}c } \\mathbf{v}_{\\epsilon,1 , 1}(\\cdot,\\cdot ) & \\mathbf{v}_{\\epsilon,1,2}(\\cdot,\\cdot ) \\\\ \\mathbf{v}_{\\epsilon , 1 , 2}(\\cdot,\\cdot ) & \\mathbf{v}_{\\epsilon,2 , 2}(\\cdot,\\cdot ) \\end{array } \\right ] \\right).\\ ] ] note that , in practice , our response functions are measured only at a predetermined set of grid points , @xmath146 . to distinguish this , let the notation @xmath147 $ ] represent the vector whose coordinates are the response as measured at each of the @xmath148 grid points , and let the analogous notation hold for the functional parameters of our models",
    ". hence , @xmath149'$ ] represents a @xmath150 vector . using the decomposition proposed in @xcite ,",
    "our covariance functions evaluated at @xmath151 can be described in matrix notation as @xmath152 , where @xmath153 denotes a @xmath154 matrix whose diagonal elements are @xmath155 .",
    "our assumption of a multivariate gaussian process results in @xmath156 $ ] following a multivariate normal distribution when we consider observations at the set of gridpoints @xmath151 with the fixed grid analogues for the mean and covariance structure .",
    "our data set consists of a total of 453 breaths collected from 16 individuals , where each breath was measured at 25 equispaced time points using both slp and spirometry .",
    "our desire to model cross - covariances between devices results in our matrices of observations being 50 dimensional . for modeling the error correlation , this is not an issue , as we have 453 observations , however , as we only have 16 individuals , a simplifying assumption must be made to proceed . in many functional data settings ,",
    "the goal of the data analysis is mean function estimation and prediction at new locations ( kriging ) . to facilitate this",
    ", modelers typically restrict themselves to a particular class of correlation functions .",
    "unfortunately , the distribution of posterior variance functions is highly dependent on the correlation structure .",
    "hence , misspecification of the correlation model can result in estimates for variance parameters that are biased and wildly misleading . as we would like to conduct inference for the ratio of variance functions of both errors and random effects , we are left searching for an alternative .",
    "more advanced methods that make no assumptions on the correlation function class have been suggested in the geostatistics literature [ see @xcite ] and elsewhere [ see @xcite ] , but none of these works have directly focused on the accuracy of the resultant variance estimates .",
    "estimation of correlation functions for repeatedly observed functional data remains an active area of research , particularly in the regime where the number of functional observations is small relative to the grid size .",
    "our recommendation is that if the researcher has sufficient data to flexibly model the correlation structure of both the random effects and the errors , then this should be the course pursued .",
    "as we do not , we instead make a modeling decision that will facilitate valid inference for our variance functions .",
    "we assume the following structure for the correlation of our errors and random effects : @xmath157 we thus primarily focus on the marginal distributions for estimation of our mean functions and variances .",
    "this has the obvious drawback of not fully exploiting the functional nature of our data , but allows for estimation of marginal variances without the risk of biases due to misspecification of the correlation structure .",
    "this is an interesting instance where the simplifying assumptions made to facilitate inference would not necessarily align with ones made if the goal was estimation of mean functions or prediction of values at unmeasured locations . in the latter case",
    ", one would likely enforce a restriction to a specific class of correlation functions which would result in both smooth curve estimates and a principled manner by which interpolation and prediction could be performed ; however , this would result in misleading estimates for the variance components of the model , which is unacceptable for testing equivalence of variance functions . in section  [ seccompare ]",
    "we investigate the ramifications of this modeling decision on the resultant inference .",
    "specification of priors for @xmath158 and @xmath159 must be done carefully , as practical equivalence of error variability is tested using a function of these parameters .",
    "we model these functions as themselves being realizations of independent stochastic processes . specifically , we extend the work of @xcite to the functional regime by modeling the standard deviation curves as emanating from log - gaussian processes : @xmath160 where @xmath161 is a standard matrn correlation function [ @xcite ] with smoothness parameter @xmath162 .",
    "we use the ratio @xmath163 as our comparative measure for the error variability of the two devices .",
    "our prior on the standard deviations yields the following prior for this ratio : @xmath164 this is a @xmath165 mixture of two log - gaussian processes with medians at the upper and lower equivalence thresholds respectively .",
    "hence , we can place prior probabilities on falling within the equivalence region by careful choices of @xmath166 . borrowing from the frequentist paradigm",
    "in which it is incumbent upon the researcher to prove his or her hypothesis beyond a reasonable doubt , we set the values of these hyperparameters such that the a priori probability of equivalence is quite small .",
    "we set @xmath167 and @xmath168 , which results in a prior probability of falling entirely within the equivalence region of @xmath169 .    for the correlations resulting from the paired nature of our data , we set @xmath170 $ ] for all @xmath24 .    for our random effects , @xmath171\\}$ ]",
    ", we use a hierarchical gaussian process prior : @xmath172 \\stackrel{\\mathrm{i.i.d.}}{\\sim } \\operatorname{gp } \\left ( \\left [ \\begin{array}{c}\\mu_1(\\cdot ) \\\\",
    "\\mu_2(\\cdot ) \\end{array } \\right ] , \\left [ \\begin{array}{c@{\\quad}c } \\mathbf{v}_{\\alpha,1 , 1}(\\cdot,\\cdot ) & \\mathbf{v}_{\\alpha , 1,2}(\\cdot,\\cdot ) \\\\",
    "\\mathbf{v}_{\\alpha,1,2 } ( \\cdot,\\cdot ) & \\mathbf{v}_{\\alpha,2 , 2}(\\cdot,\\cdot ) \\end{array } \\right ] \\right).\\ ] ]    the priors on the variance functions of our random effects , @xmath173 $ ] , and the correlation structure are identical to the one used for the error variances .",
    "the posterior distribution for the difference between the device specific curves , @xmath174 , is of interest for assessing locational equivalence .",
    "thus , proper attention must be paid to the prior placed on @xmath175 such that the prior does not unduly force the posterior distribution toward the prespecified equivalence region .",
    "our priors for @xmath176 and @xmath177 are as follows : @xmath178 where @xmath179 is a matrn correlation function with smoothness parameter @xmath162 .",
    "this then implies that our difference of means has the following prior : @xmath180 in other words , our prior on the difference in device means is a @xmath165 mixture of two gaussian processes , with means at the upper and lower equivalence thresholds respectively .",
    "we choose a prior that places 1% likelihood in the equivalence region and the remaining 99% outside of it . to achieve this",
    ", we fixed a value of @xmath181 , and then used the ` uniroot ( ) ` and ` pmvnorm ( ) ` functions in ` r ` [ @xcite ] to solve for the value of @xmath182 such that @xmath183 .",
    "this value was found to be 0.1 .",
    "note that if one has a sense of an appropriate basis for the mean functions , one could place a prior @xmath184 instead of @xmath185 .",
    "this could allow for regularization of the functional fits based on this basis while not restricting them to entirely follow said basis , and would still facilitate our strategy of putting priors on equivalence commensurate with prior knowledge .      before conducting inference based on our model specification",
    ", we must devise a sampling schema for the posterior distribution of our parameters .",
    "we use a metropolis - within - gibbs sampling algorithm ; see the supplementary materials [ @xcite ] for details .",
    "( top left ) , @xmath186 ( top right ) , and @xmath187 ( bottom ) , along with upper and lower equivalence bands . ]",
    "to conduct our posterior analysis , we ran our gibbs sampler from three distinct starting values for 10,500 iterations per starting value ( for a total of 31,500 iterations ) .",
    "we discarded the first 500 iterations as burn - in for each chain and took every 10 samples thenceforth for a total of 1000 samples per starting value , which were then chained together , resulting in 3000 roughly independent samples .",
    "see the supplementary material [ @xcite ] for convergence diagnostics .",
    "figure  [ figdeltapost ] shows the posterior distribution for the three metrics of interest .",
    "we summarize the posterior distributions of our metrics of equivalence by the posterior mean curve and 95% simultaneous posterior bands .",
    "these bands are computed using the multiplier based method of @xcite .",
    "the posterior bands are unnecessary for inference , as the computation of @xmath188 depends solely on how many posterior curves fall within the equivalence region , but nonetheless provide a useful graphical aid . for our locational metric , @xmath13",
    ", we found that all 3000 of our samples from the posterior distribution fell within the prespecified equivalence range , suggesting overwhelming evidence in favor of the hypothesis that these two curves , in terms of location , can be considered practically equivalent .",
    "for the ratio of error variances , @xmath189 , we note that if it is the case that lower variability is strictly more desirable , then 2998 out of 3000 samples fall strictly below the upper equivalence band ; however , if one desires adherence to the lower equivalence band as well , then our posterior probability of equivalence is @xmath190 , since our posterior bands regularly violate the lower tolerance threshold toward the middle of the breaths ( around @xmath191 ) . for the ratio of random effect variances , @xmath192",
    ", we note that although the posterior median falls well within the equivalence range , only 18.2% of the posterior samples fell entirely within the equivalence region . hence , although we can suggest equivalence of both means and error variances , we lack sufficient power to suggest equivalence of random effect variances .",
    "we have presented methods for equivalence testing within the frequentist and bayesian paradigms . from a pragmatic perspective , the relative computational intensity of both methods is of interest to practitioners . in this respect ,",
    "our frequentist method is dominant , as within each bootstrap iteration , only simple vector operations are required .",
    "the bayesian approach requires sampling from multivariate distributions , matrix multiplication , matrix inversion , and determinant calculation within each step .",
    "furthermore , thinning of one out of every 10 iterations was required .",
    "hence , to get the same effective sample size , we needed to do 10 times as many iterations for the frequentist procedure as we did for the bayesian one . to attain 1000 independent samples via the bayesian methodology , we needed to run 10,500 iterations of our sampling algorithm , which took 22.6 minutes on a personal laptop with 4  gb ram and a 2.7 ghz processor .",
    "the bootstrap procedure took 16.1 seconds to run 1000 iterations on the same laptop .",
    "this discrepancy will only increase as the granularity of the grid the user implements increases , as both determinant and inverse calculation are @xmath193 in their simplest implementation .",
    "frequentist and bayesian inference are not coherent with one another , in that frequentist inference has a built in preference for the null hypothesis . for the frequentist ,",
    "the null is the status quo , and the goal of the inference is to refute it via a `` proof by contradiction . ''",
    "the bayesian framework , on the other hand , allows the user to put varying degrees of a priori preference on one hypothesis versus the other . in our bayesian analysis",
    "we have placed heavy preference on the null and thus require very strong evidence from the data to put the posterior probability in the proper region , but this may not always be appropriate .",
    "the bayesian paradigm allows for a principled manner for incorporating the results of past studies in the form of the priors placed on equivalence vs nonequivalence , a feature not offered by the frequentist framework .    with these caveats in mind ,",
    "we investigate the size and power of our methodologies , using the threshold of @xmath82 in the frequentist procedure . for our bayesian procedure ,",
    "we use @xmath194 as our threshold for the posterior probability of equivalence . in our investigation",
    ", we continue to place heavy a priori preference on nonequivalence for our bayesian methodology .",
    "we restrict our investigation to the type i error rates of our tests for location and error variances .",
    "we simulate 20 matched pair random effects , and then simulate 20 matched functional responses for each subpopulation .",
    "this results in 400 breaths total . to investigate the true size of our methods",
    ", we define a sequence of true values for our metrics of equivalence where equivalence is violated at one point along the domain , and the other points move farther and farther into the equivalence region . these sequences and numerical labels are shown in figure  [ figsizesequence ] .",
    "the remaining values of parameters needed for simulation are based on the posterior means from our data set . additionally , we used an estimate of the correlation structure of our error functions as the true correlation for simulating both error functions and random effect functions .",
    "this allows us to assess the robustness of our bayesian procedure to the assumption of section  [ seccorrelation ]    for each of the nine function values in the sequence , we simulated 500 data sets and ran both the frequentist and bayesian methdologies on them .",
    "figure  [ figsizesequence ] shows the result of this study .",
    "we see that for testing the equivalence of mean functions , the bayesian procedure is far more conservative than our frequentist procedure , which appears to be due to the assumption on the correlation structure made in our bayesian procedure .",
    "as expected , the frequentist procedure is initially conservative , but has size that approaches 0.05 as the test becomes increasingly reliant on our data s behavior at one domain point ( the one at which equivalence is violated ) .",
    "figure  [ figsizesequence ] also demonstrates that the test is roughly unbiased in terms of purported size .",
    "for testing the equivalence of variances , the bayesian and frequentist procedures initially exhibit similar type i error rates , and also both appear to be slighty anti - conservative ; however , the bayesian procedure is anti - conservative to a far more egregious degree by the end of the sequence of functions , having an estimated size of 0.072 for the 9th function in the sequence versus an estimated size of 0.056 for the frequentist procedure at this value for the true ratio of error variances .",
    "( top ) and @xmath189 ( bottom ) along with upper and lower equivalence bands used for type i error study . ]      to investigate the power of our methods , we define a sequence of true values for our metrics of equivalence that fall entirely between the upper and lower equivalence thresholds .",
    "these sequences and numerical labels are shown in figure  [ figpowersequence ] . the rest of our simulation procedure mirrors that of our simulation for testing the type i error rate .",
    "figure  [ figpowersequence ] shows the results of this study .",
    "we see that for testing equivalence of means , the frequentist procedure appears to be substantially more powerful than its bayesian counterpart . for testing equivalence of variances ,",
    "the frequentist and bayesian procedures behave quite similarly , with no clear indication that one procedure is any more powerful than the other .",
    "( top ) and @xmath189 ( bottom ) along with upper and lower equivalence bands used for the power study . ]",
    "we have presented a broad framework for equivalence testing when one s data are intrinsically functional .",
    "this framework begins with definitions of metrics of equivalence , and correspondingly with the establishment of upper and lower equivalence bands which are themselves functions of the continuum over which the functional data is defined .",
    "we have stressed the importance of using metrics that are able to discern similarity of location and of spread , as neither individually is sufficient for suggesting equivalence .",
    "we illustrated the proper use of these frameworks using data from a method comparison study assessing the performance of a new device for testing pulmonary function , slp , relative to the gold standard for pulmonary diagnoses , the spirometer .",
    "our model presently makes an assumption that all individuals are drawn from the same population . for our application",
    "this makes sense , as we are solely looking at healthy individuals . for other applications ,",
    "the individuals for which repeated measurements are attained may be draws from multiple populations . in our application",
    ", one could potentially have individuals of varying degrees of pulmonary health ( e.g. , healthy , asthmatic , smokers ) .",
    "our model can easily adapt to this , as this simply requires adding an additional level to the hierarchy .",
    "we could either say that health level specific means are drawn from a population with an overall mean , and then individual means are drawn from these health level specific populations , or we could model the health level means as fixed effects and result in a functional mixed effects model .",
    "using the difference between mean functions to test locational disparity is a natural choice , and the extent to which magnitude of differences are important can be controlled by tightening or loosening the equivalence bands . for testing the disparity between variances of both errors and random effects , we have followed the prevalent choice in the scalar equivalence testing literature [ see @xcite ] and have used the ratio between variances , @xmath195 . on the one hand , this unitless measure has appeal in that it has potential for standardization across applications . on the other hand , we lose a sense of the absolute difference between the quantities . for some applications ,",
    "the difference between a variance of 0.01 and 0.02 could be inconsequential , yet the difference between 0.04 and 0.08 could be enough to warrant using one device over another . if one were using ratios for assessing a discrepancy , however , these quantities would be identically different .",
    "we thus suggest that the difference between variances , @xmath196 , may be an additional metric for equivalence that could be used in tandem with the ratio of variances to test for equivalence of variability .",
    "note that there may be additional facets of the underlying distributions of functions to be addressed beyond location and variability , depending on the application .",
    "for example , one may be interested not only in the difference in the mean functions being within an equivalence region , but also in the derivative of the difference between mean functions being small in absolute value .",
    "we leave the development of proper methodology for these questions as a topic for future research , but the strategy of supplying upper and lower equivalence bands would certainly be appropriate .    we hope that this paper serves as a valuable contribution to the literature on equivalence testing and that its extension to the realm of functional data will be useful for a host of applied users , including but not limited to practitioners looking to compare devices whose measurements can not be summarized as scalar quantities .",
    "comparison studies are of the utmost importance , as oftentimes the emergence of newer and better devices can have salubrious outcomes for society in general .",
    "our goal is that this paper properly emphasizes the importance of equivalence testing in general , and provides traction for researchers who aim to suggest that two populations of functions are practically equivalent rather than to suggest that they are different .",
    "we would like to thank the anonymous associate editor and referees for their invaluable feedback during the review process , which led to marked improvements in the quality of the article .",
    "we would also like to thank dr .",
    "joan lasenby and stuart bennet of the signal processing and communications laboratory , department of engineering , university of cambridge for introducing us to the problem and providing us with the data set used herein ."
  ],
  "abstract_text": [
    "<S> equivalence testing for scalar data has been well addressed in the literature , however , the same can not be said for functional data . the resultant complexity from maintaining the functional structure of the data , rather than using a scalar transformation to reduce dimensionality , renders the existing literature on equivalence testing inadequate for the desired inference . </S>",
    "<S> we propose a  framework for equivalence testing for functional data within both the frequentist and bayesian paradigms . </S>",
    "<S> this framework combines extensions of scalar methodologies with new methodology for functional data . </S>",
    "<S> our frequentist hypothesis test extends the two one - sided testing ( tost ) procedure for equivalence testing to the functional regime . </S>",
    "<S> we conduct this tost procedure through the use of the nonparametric bootstrap . </S>",
    "<S> our bayesian methodology employs a functional analysis of variance model , and uses a flexible class of gaussian processes for both modeling our data and as prior distributions . through our analysis </S>",
    "<S> , we introduce a model for heteroscedastic variances within a gaussian process by modeling variance curves via log - gaussian process priors . </S>",
    "<S> we stress the importance of choosing prior distributions that are commensurate with the prior state of knowledge and evidence regarding practical equivalence . </S>",
    "<S> we illustrate these testing methods through data from an ongoing method comparison study between two devices for pulmonary function testing . in so doing , we provide not only concrete motivation for equivalence testing for functional data , but also a blueprint for researchers who hope to conduct similar inference . </S>"
  ]
}