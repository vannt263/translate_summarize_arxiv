{
  "article_text": [
    "promising fields of application for machine learning are the internet of things ( iot ) @xcite and industry 4.0 @xcite environments . in these fields ,",
    "machine learning models anticipate future device states by combining knowledge about device attributes with historic sensor time series .",
    "they permit the classification of devices ( e.g. hard drives ) into risk classes with respect to a specific defect @xcite .",
    "both fields are driven by the availability of cheap sensors and advancing connectivity between devices , which increases the need for machine learning on temporally annotated data .    in most cases",
    "the volume of the generated time series data forbids their transport to centralized databases @xcite .",
    "instead , algorithms for an efficient reduction of the data volume by means of feature extraction and feature selection are needed @xcite .",
    "furthermore , for online applications of machine learning it is important to continuously select relevant features in order to deal with concept drifts caused by qualitative changes of the underlying dynamics @xcite .",
    "therefore , for industrial and other applications , one needs to combine distributed feature extraction methods with a scalable feature selection , especially for problems where several time series and meta - information have to be considered per label / target @xcite . for time series classification , it proved to be efficient to apply comprehensive feature extraction algorithms and then filter the respective features @xcite .    motivated by industrial applications for machine learning models we are extending the approach of @xcite and propose ( ` fresh ` ) .",
    "the algorithm characterizes time series with comprehensive and well - established feature mappings and considers additional features describing meta - information . in a second step",
    ", each feature vector is individually and independently evaluated with respect to its significance for predicting the target under investigation .",
    "the result of these tests is a vector of p - values , quantifying the significance of each feature for predicting the label / target .",
    "this vector is evaluated on basis of the benjamini - yekutieli procedure @xcite in order to decide which features to keep .",
    "the proposed algorithm is evaluated on all binary classification problems of the ucr time series classification archive @xcite as well as time series data from a production line optimization project and simulated time series from a stochastic process with underlying qualitative change of dynamics @xcite .",
    "the results are benchmarked against well - established feature selection algorithms like linear discriminant analysis @xcite and the boruta algorithm @xcite , but also against dynamic time warping @xcite .",
    "the analysis shows that the proposed method outperforms boruta based feature selection approaches as well as dynamic time warping based approaches for problems with large feature sets and large time series samples .",
    "this contribution closes with a summary and an outlook on future work .",
    "temporally annotated data come in three different variants @xcite : temporally invariant information ( e.g. the manufacturer of a device ) , temporally variant information , which change irregularly ( e.g. process states ) , and temporally variant information with regularly updated values ( e.g. sensor measurements ) .",
    "the latter describe the continuously changing state @xmath0 of a system or device @xmath1 with respect to a specific measurement of sensor @xmath2 , which is repeated in intervals of length @xmath3 .",
    "this sampling captures the state of the system or device under investigation as a sequence @xmath4 with @xmath5 .",
    "such kind of sequences are called time series and are abbreviated by @xmath6 here , we are considering @xmath7 devices with @xmath8 different time series per device .",
    "therefore , we are dealing with @xmath9 values describing the ensemble under investigation . in order to characterize a time series with respect to its dynamics and",
    "reduce the data volume , a mapping @xmath10 is introduced , which captures a specific aspect @xmath11 of the time series .",
    "one example for such mapping might be the maximum operator @xmath12 which quantifies the maximal value ever recorded for time series @xmath13 .",
    "this kind of lower dimensional representation is called a _ feature _ , which is a measurable characteristics of the considered time series . other examples for feature mappings @xmath14 of time",
    "series might be their mean , the number of peaks with a certain steepness , their periodicity , a global trend , etc .",
    "comprehensive collections of time series feature mappings are discussed by @xcite and @xcite . @xcite",
    "propose to use more than 9000 features from 1000 different feature generating algorithms .",
    "now , consider @xmath15 different time series feature mappings , which are applied to all @xmath16 time series recorded from @xmath17 sensors of @xmath18 devices ( fig .",
    "[ fig : ffe_in_detail ] ) .",
    "the resulting feature matrix @xmath19 has @xmath18 rows ( one for each device ) and @xmath20 columns with @xmath21 denoting the number of features generated from device specific meta - information .",
    "each column of @xmath22 comprises a vector @xmath23 capturing a specific characteristic of all considered devices .",
    "fig : ffe_in_detail",
    "typically , time series are noisy and contain redundancies . therefore , one should keep the balance between extracting meaningful but probably fragile features and robust but probably non - significant features . some features such as the @xmath24",
    "will not be heavily influenced by outliers , others such as @xmath25 will be intrinsically fragile . the choice of the right time series feature mappings is crucial to capture the right characteristics for the task at hand .",
    "a meaningless feature describes a characteristic of the time series that is not useful for the classification or regression task at hand . @xcite",
    "considered a binary target @xmath26 , stating that the relevance of feature x is measured as the difference between the class conditional distributions @xmath27 and @xmath28 .",
    "we adopt this definition and consider a feature @xmath29 being relevant for the classification of the binary target @xmath26 if those distributions are not equal . in general , a feature @xmath29 is relevant for predicting target @xmath26 if and only if @xmath30 the condition from equation   is equivalent to @xmath31 we will use the statistical independence to derive a shorter definition of a relevant feature :    a feature @xmath32 is _ relevant _ or _ meaningful _ for the prediction of @xmath26 if and only if @xmath32 and @xmath26 are not statistically independent .",
    "[ section_tests ]    for every extracted feature @xmath33 we will deploy a singular statistical test checking the hypotheses @xmath34 the result of each hypothesis test @xmath35 is a so - called p - value @xmath36 , which quantifies the probability that feature @xmath32 is not relevant for predicting @xmath26 .",
    "small p - values indicate features , which are relevant for predicting the target .    based on the vector @xmath37 of all hypothesis tests ,",
    "a multiple testing approach will select the relevant features ( sec .",
    "[ subsec_multitest ] ) .",
    "we propose to treat every feature uniquely by a different statistical test , depending on wether the codomains of target and feature are binary or not . the usage of one general feature test for all constellations is not recommended .",
    "specialized hypothesis tests yield a higher statistical power due to more assumptions about the codomains that can be used during the construction of those tests .",
    "the proposed feature significance tests are based on nonparametric hypothesis tests , that do not make any assumptions about the distribution of the variables , which ensures robustness of the procedure .",
    "* exact fisher test of independence : * this feature significance test can be used if both the target and the inspected feature are binary .",
    "fisher s exact test @xcite is based on the contingency table formed by @xmath38 and @xmath26 .",
    "it inspects if both variables are statistically independent , which corresponds to the hypotheses from eq@xmath39  .",
    "fisher s test belongs to the class of exact tests . for such tests ,",
    "the significance of the deviation from a null hypothesis ( e.g. , the p - value ) can be calculated exactly , rather than relying on asymptotic results .",
    "* kolmogorov - smirnov test ( binary feature ) : * this feature significance test assumes the feature to be binary and the target to be continuous . in general , the kolmogorov - smirnov ( ks ) test is a non - parametric and stable goodness - of - fit test that checks if two random variables @xmath40 and @xmath41 follow the same distribution @xcite : @xmath42 by conditionally modeling the distribution function of target @xmath26 on the two possible values @xmath43 of the feature @xmath32 we can use the ks test to check if the distribution of @xmath26 differs given different values of @xmath32 . setting @xmath44 and @xmath45 results in @xmath46 the hypotheses from eq@xmath39   and are equivalent as demonstrated in the chain of equations in .",
    "hence , the ks test can address the feature relevance of @xmath32 .",
    "* kolmogorov - smirnov test ( binary target ) : * when the target is binary and the feature non - binary , we can deploy the kolmogorov - smirnov test again .",
    "we have to switch roles of target and feature variable , resulting in the testing of the following hypothesis : @xmath47 this time @xmath48 and @xmath49 are the two possible values of @xmath26 and @xmath50 is the conditional density function of @xmath32 given @xmath26 .",
    "this hypothesis is also equivalent to the one in eq@xmath39  .",
    "* kendal rank test : * this filter can be deployed if neither target nor feature are binary .",
    "kendall s rank test @xcite checks if two continuous variables may be regarded as statistically dependent , hence naturally fitting our hypotheses from eq@xmath39  .",
    "it is a non - parametric test based on kendall s rank statistic @xmath51 , measuring the strength of monotonic association between @xmath32 and @xmath26 .",
    "the calculation of the rank statistic is more complex when ties are involved @xcite , i.e. feature or target are categorical .",
    "when comparing multiple hypotheses simultaneously , errors in the inference tend to accumulate @xcite . in this context ,",
    "a wrongly added feature is a feature @xmath32 for which the null hypothesis @xmath52 has been rejected by the respective feature significance test , even though @xmath52 is true .",
    "if we want to control the percentage of irrelevant added features we have to control the percentage of wrongly rejected null hypothesis among all hypothesis . in multiple testing , this ratio of the expected proportion of erroneous rejections among all rejections is called false discovery rate ( fdr ) .",
    "the fdr as a measure of the accumulated statistical error was suggested by @xcite .",
    "later the non - parametric benjamini - yekutieli procedure was proposed .",
    "based on the p - values it tells which hypotheses to reject while still controlling the fdr under any dependency structure between those hypotheses @xcite .",
    "it will be the last component of our filtered feature extraction algorithm .",
    "the procedure searches for the first intersection between the ordered sequence of p - values @xmath53 ( dotted blue curves in fig .",
    "[ fig : by_fdr ] ) with a linear sequence ( green lines in fig .",
    "[ fig : by_fdr ] ) @xmath54 here , @xmath55 is the number of all null hypotheses and @xmath56 is the fdr level that the procedure controls .",
    "it will reject all hypotheses belonging to p - values that have a lower value than the p - value at the intersection , see the left side of fig .",
    "[ fig : by_fdr_focus ] .",
    "we propose * * f**eatu**r**e * * e**xtraction based on * * s**calable * * h**ypothesis tests ( ` fresh ` ) for parameter @xmath57 $ ] , given by the following three steps :    1 .",
    "perform a set of @xmath55 univariate feature mappings as introduced in sec .",
    "[ subsec_mapping ] on @xmath58 different time series to create the features @xmath32 , @xmath59 .",
    "2 .   for each generated feature @xmath60 perform exactly one hypothesis test for the hypothesis @xmath52 from equation . to do so ,",
    "take the corresponding feature significance test from sec .",
    "[ subsec_featfilt ] . calculate the p - values @xmath61 of the tests .",
    "3 .   perform the benjamini - yekutieli procedure under correction for dependent hypotheses @xcite for a fdr level of @xmath56 on the collected p - values @xmath61 to decide which null hypothesis @xmath35 to reject ( c.f .",
    "[ subsec_multitest ] ) . only return features @xmath32 for which the respective hypothesis @xmath35 was rejected by the procedure .",
    "a problem of many filter feature methods such as the one we utilize in steps @xmath62 and @xmath63 of ` fresh ` is the redundancy in the feature selection . as long as features are considered",
    "associated with the target , they will all be selected by the filter even though many of them are highly correlated to each other @xcite . for example @xmath24 and @xmath64 are highly correlated in the absence of outliers and therefore we expect ` fresh ` to either select or drop both @xmath24 and @xmath64 at the same time . to avoid generating a group of highly correlated features we propose to add another step to ` fresh ` :    * normalize the features and perform a principal performance analysis ( pca ) . keep the principal components with highest eigenvalue describing @xmath65 percent of the variance .",
    "this step will reduce the number of features and the obtained principal components are de - correlated , orthogonal variables @xcite .",
    "one could perform step @xmath66 between steps @xmath67 and @xmath62 of ` fresh ` to get rid of the correlations between the created variables early .",
    "then the feature significance tests in step @xmath62 of ` fresh ` will take principal components instead of the original features as input .",
    "we will denote this variant of ` fresh ` as @xmath68(efore ) .",
    "also , one could perform step @xmath66 after the ` fresh ` algorithm , directly after step @xmath63 .",
    "this means that the pca will only process those features , which are found relevant by the ` fresh ` algorithm instead of processing all features .",
    "this variant of ` fresh ` is called @xmath69(fter ) .",
    "in the following , the performance of ` fresh ` , its two variants from sec .  [ sub : variants_fresh ] and other time series feature extraction methods are compared . the evaluation is done with respect to both the meaningfulness of the extracted features as well as the time it takes to extract the features . while doing so , all feature extraction methods operate on the same feature mappings , the differences lay only in the used feature selection process .      `",
    "fresh ` is parameterized with @xmath70 , cf .",
    "its variants , which apply a pca , are using @xmath71 ( sec .  [ sub : variants_fresh ] ) . `",
    "full_x ` uses all the features , which are created during step @xmath67 of ` fresh ` ( sec .",
    "[ sec : fresh ] ) without any subsequent filtering .",
    "features contained in ` full_x ` will be filtered by applying the ` boruta ` feature selection algorithm @xcite , or by a forward selection with a linear discriminant analysis classifier denoted ` lda ` .",
    "further , the direct classifier ` dtw_nn ` , a nearest neighbor search under the dynamic time warping distance , is considered @xcite . those six different extraction methods and ` dtw_nn",
    "` were each picked for a reason . `",
    "dtw_nn ` is reported to reach the highest accuracy rates among other time series classifiers , ` lda ` was the first proposed algorithm to automatically extract features from time series @xcite and ` boruta ` is a promising feature selection algorithm that incorporates interactions between features .",
    "to guarantee reproducibility we use all 31 time series data sets from the ucr time series archive @xcite containing a binary classification problem . to compare the runtime of the different methods , time series of flexible length and sample number belonging to two classes",
    "are generated by simulating the stochastic dynamics of a dissipative soliton @xcite .",
    "the last data source originates from the production of , extracted during the german research project .",
    "the project demonstrates a typical application of industrial time series analysis , aiming to predict the passing or failing of product specification testings based on timely annotated data .",
    "it contains 26 univariate meta - variables forming the baseline feature set extended by 20 different sensor time series having up to 44 data points for each sample .",
    "the data set contained 1554 samples of two classes _ `` broken '' _ and _ `` not broken '' _ each , in total 3108 samples .",
    "due to limitations regarding the size of this paper , we do not address the regression performance of ` fresh ` and variants yet . in a future work ,",
    "we catch up on this .      for the data sets from the ucr time series repository as well as the data , the underlying structure and therefore the relevant features are unknown .",
    "we can not compare the different methods on their ability to extract meaningful features because we do not know which features are meaningful and which are not .",
    "also , we can not compare the extracted features to direct classifiers such as ` dtw_nn ` .",
    "therefore , we evaluate the performance of the feature extraction algorithms by comparing the performance of a classification algorithm on the extracted features .",
    "hereby , we assume that more meaningful features will result in a better classification result . to investigate the feature extraction methods under different conditions and to make sure that the feature filtering is not tuned to a specific classifier",
    ", the classification task is solved by a collection of a one layer neural network / perceptron ( ` nn ` ) , a logistic regression model ( ` lr ` ) , a support vector machine ( ` svm ` ) , a random forest classifier ( ` rfc ` ) and an adaboost classifier ( ` abc ` ) .",
    "the hyperparameters for those methods are not optimized to get an unbiased view on the meaningfulness of the extracted features , instead the default values from the python package scikitlearn version @xmath72 were used @xcite .",
    "for every data set , all available samples will be used to perform the feature extraction itself .",
    "then , one third of the samples are randomly picked for a test set , the remaining two thirds are used to train the classifiers .",
    "we define the index set for the classification algorithms as @xmath73 and for the inspected feature extraction methods we define @xmath74 .",
    "then , the accuracy of a classifier @xmath75 on the test part of the data set @xmath76 for the features generated by method @xmath77 is denoted as @xmath78 .",
    "further , we calculate the mean of the accuracy of the five classification algorithms , which is denoted by @xmath79 . `",
    "dtw_nn ` itself does not perform any feature extraction and its accuracy is directly calculated by predicting on the test set .",
    "it is denoted by @xmath80 .",
    "now , we are able to denote the average accuracy over all 31 ucr data sets with binary classification tasks @xcite for each method @xmath81 by @xmath82 , cf . columns 3 and 7 of tab@xmath39  [ tab : evalution_mean ] .    from the @xmath83 column of tab@xmath39  [ tab : evalution_mean ]",
    "we can observe that ` fresh_pcaa ` dominated the feature based approaches on the ucr data sets but it was not able to beat ` dtw_nn ` . on the data , ` dtw_nn ` could only operate on one type of time series without the univariate features .",
    "this seems to be the reasons why ` boruta ` and ` fresh_pcaa ` beat the accuracy of ` dtw_nn ` as shown in the @xmath84 column of tab .",
    "[ tab : evalution_mean ] . here",
    "` fresh_pcaa ` again achieved the highest accuracy among all feature based approaches .",
    "further , we count how often a feature extraction method @xmath85 reaches the highest accuracy for the 155 classifier/ data set combinations on the 31 ucr data sets among all six feature extraction methods in @xmath86 while a draw counts for both methods : @xmath87 the evaluation metric @xmath88 is reported in the @xmath89 column of tab@xmath39  [ tab : evalution_mean ] . again ` fresh_pcaa ` achieved the best result , for over half of the 155 inspected combinations it had the highest reported accuracy and again ` boruta ` came in second with @xmath90 combinations on average . regarding both accuracy metrics , ` fresh_pcaa ` seems to be favorable over the other considered feature based approaches , with ` boruta ` coming close .    [ cols= \" < , > , > , > , > , > , > , > \" , ]      the second and sixth column of tab@xmath39  [ tab : evalution_mean ] contains the average _ pipeline runtime _ , which is the combined runtime of feature extraction , training of a classifier and predicting .",
    "for ` dtw_nn ` the pipeline runtime captures just the fitting and predicting steps as no features are extracted .",
    "there are tradeoffs between the number of extracted features and the time spent on feature extraction which force us to consider the pipeline runtime instead of the extraction runtime .",
    "e.g. , time spent in the extraction process can be compensated by a lower number of extracted features which reduces the time spent for training the final classifier .",
    "analog to the accuracy evaluation metrics , @xmath91 denotes the pipeline runtime in seconds of classifier @xmath75 and feature extraction method @xmath92 on data set @xmath76 . in the same way",
    ", @xmath93 denotes the average pipeline runtime in seconds over all 5 classifiers and @xmath94 the runtime of fitting and predicting by ` dtw_nn ` .",
    "the average pipeline runtimes over all ucr data sets for each method @xmath81 are denoted by @xmath95 .",
    "all calculations are executed on a single computational core in order to increase comparability .",
    "the full feature matrix ` full_x ` is the fastest extraction algorithm in our comparisons on both the ucr and the data , as seen in the @xmath96 and @xmath97 column of tab@xmath39  [ tab : evalution_mean ] .",
    "saved time for the fitting of the feature selection algorithm compensated for the five classifiers having to be trained on more features .",
    "accordingly , the pca step of ` fresh_pcaa ` saves so much time for the fitting of the classifiers that this step is basically `` free '' , while ` fresh_pcaa ` has a mean pipeline runtime of @xmath98 seconds . on average , ` fresh ` takes @xmath99 seconds .",
    "the same can be observed on the data where ` full_x ` , ` fresh ` and ` fresh_pcaa ` all had similar pipeline runtimes even though the number of extracted features varied greatly .",
    "the low average pipeline runtime of just @xmath100 seconds for ` dtw_nn ` in the @xmath97 column of tab .",
    "[ tab : evalution_mean ] is due to the classification algorithm only considering one type of time series while the extraction methods operate on 20 different time series .",
    "apart from the observed runtimes , we are interested in the feature extraction method s ability to scale with an increasing number of feature mappings , time series length and device numbers . as expected , fig .",
    "[ fig : runtime_length_ts_n_samples ] shows that all considered feature extraction methods  in contrast to ` dtw_nn `  scale linearly with an increasing length of the time series or increasing number of samples .",
    "this is due to the considered feature mapping having a linear runtime with respect to the length of the time series .",
    "however , fig .",
    "[ fig : runtime_fsa_nfeatures ] shows that , among the feature based approaches , only ` fresh ` and ` fresh_pcaa ` scale linear with an increasing number of features ( e.g. due to more devices , feature mappings or types of time series ) .      the number of features extracted by algorithm @xmath92 on the data set @xmath76 are denoted by @xmath101 . again",
    "this can be averaged over the ucr data sets resulting in @xmath102 , cf .",
    "rightmost column of tab@xmath39  [ tab : evalution_mean ] .    during our simulations ,",
    "` fresh_pcaa ` was able to reduce the number of features drastically .",
    "for the ucr data , it reduces 161 considered feature mappings to an average number of @xmath103 features while on the data it reduces the @xmath104 calculated features to @xmath105 . `",
    "fresh_pcab ` only selected four respective two features on average , which may explain its low accuracies .",
    "if one wishes to extract a minimal set of relevant features , we recommend to deploy ` fresh_pcaa ` , as it extracted the second lowest number of features but achieved the highest and second highest accuracies .",
    "we proposed ` fresh ` as a highly scalable feature extraction algorithm .",
    "our simulations showed that , in contrast to other considered methods , ` fresh ` is able to scale with the number of feature mappings and samples as well as with the amount of different types and length of the time series .",
    "while doing so , it is extracting meaningful features as demonstrated by competitive accuracies .    the relative bad performance of @xmath68 seems to originate in the pca step selecting features only based on their ability to explain the variance in the input variables and not in their significace to predict the target variable . by this",
    ", relevant information for the classification or regression task can get lost . on the other hand , the combination of ` fresh ` with a subsequent pca filtering to reduce the number of redundant and highly correlated features , denoted as ` fresh_pcaa ` was overall the most competitive feature based method in our evaluation . on the ucr data , it achieved the second highest and on the data it reached the highest accuracy .",
    "further , it had the second lowest number of extracted features .",
    "it is common knowledge that the quality of feature engineering is a crucial success factor for supervised machine learning in general @xcite and for time series analysis in particular @xcite .",
    "but comprehensive domain knowledge is needed in order to perform high quality feature engineering .",
    "contrarily , it is quite common for machine learning projects that data scientists start with limited domain knowledge and improve their process understanding while continuously discussing their models with domain experts .",
    "this is basically the reason , why dedicated time series models are very hard to build from scratch .    our experience with data science projects in the context of iot and industry 4.0 applications showed that it is very important to identify relevant time series features in an early stage of the project in order to engineer more specialized features in discussions with domain experts .",
    "the ` fresh ` algorithm supports this approach by applying a huge variety of established time series feature mappings to different types of time series and meta - information simultaneously and identifies relevant features in a robust manner .",
    "we observe that features extracted by ` fresh ` contribute to a deeper understanding of the investigated problem , because each feature is intrinsically related to a distinct property of the investigated system and its dynamics .",
    "this fosters the interpretation of the extracted features by domain experts and allows for the engineering of more complex , domain specific features including dedicated time series models , such that their predictions in return might become a future feature mapping for ` fresh ` .",
    "we have already mentioned that ` fresh ` has been developed in the course of iot and industry 4.0 projects . especially for predictive maintenance applications with limited numbers of samples and high level of noise in e.g. sensor readings",
    ", it has been proven as crucial to filter irrelevant features in order to prevent overfitting . to ensure a robust and scalable filtering , we consider each feature importance individually .",
    "this causes several implications :    * ` fresh ` is robust in the sense of classical statistics , because the hypothesis tests and the benjamini - yekutieli procedure do not make any assumptions about the probability distribution or dependence structure between the features . here",
    ", robustness refers to the insensitivity of the estimator to outliers or violations in underlying assumptions @xcite . * ` fresh ` is not considering the meaningfulness of interactions between features by design .",
    "hence , in its discussed form it will not find meaningful feature combinations such as chessboard variables ( * ? ? ?",
    "however , in our evaluation process the feature selection algorithm boruta , which considers feature interactions , was not able to beat the performance of ` fresh ` .",
    "further , it is possible for ` fresh ` to incorporate combinations of features and pre - defined interactions as new features themselves . * ` fresh ` is scalable due to the parallelity of the feature calculation and hypothesis tests ( see the two topmost tiers in fig .  [",
    "fig : ffe_in_detail ] ) and can be trivially parallelized and even distributed over several computational units . in addition , the feature filter process has computational costs compared to feature calculation and significance testing .",
    "therefore , ` fresh ` scales linearly with the number of extracted features , length of the time series , and number of considered time series . * a side effect of ensuring robustness and testing features individually is that ` fresh ` tends to extract highly correlated features , which could result in poor classification performance .",
    "we propose to combine ` fresh ` with a subsequent pca , which has been discussed as ` fresh_pcaa ` in sec .",
    "[ sub : variants_fresh ] and indeed improved the performance significantly .",
    "@xcite proposed to divide feature selection into two flavors , the _ minimal optimal problem _ is finding a set consisting of all strongly relevant attributes and a subset of weakly relevant attributes such that all remaining weakly relevant attributes contain only redundant information .",
    "the _ all - relevant problem _ is finding all strongly and weakly relevant attributes .",
    "the first problem is way harder than the second , even asymptotically intractable for strictly positive distributions @xcite . accordingly ,",
    "` fresh ` solves the second , easier problem as we extract every relevant feature , even though it might be a duplicate or highly correlated to another relevant feature ( cf .",
    "@xcite separated feature selection algorithms into two categories , the _ wrapper model _ and the _ filter model_. while the selection of wrapper models is based on the performance of a learning algorithm on the selected set of features , filter models use general characteristics to derive a decision about which features to keep .",
    "filter models are further divided into feature weighting algorithms and subset search algorithms , which evaluate the goodness of features individually or through subsets . according to this definition , the feature selection part of ` fresh `",
    "is a filter model , more precisely , a feature weighting algorithm .    `",
    "fresh ` contains a feature selection part on basis of hypothesis tests and the benjamini - yekutieli procedure , which of course can be used as a feature selection algorithm itself .",
    "but , due to its systematic incorporation of scalable time series feature mappings and the proposed decomposition in computing tiers ( fig .  [ fig : ffe_in_detail ] )",
    "it is especially applicable to the needs of mass time series feature extraction and is considered as a feature extraction algorithm . by applying a multiple testing algorithm , `",
    "fresh ` avoids the _",
    "`` look - elsewhere effect '' _ @xcite which is a statistically significant observation arising by chance due to the high number of tested hypotheses .",
    "this effect triggered a recent discussions about the use of p - values in scientific publications @xcite .",
    "there are both structural and statistical approaches to extract patterns from time series .",
    "many statistical approaches rely on structures that allow the usage of genetic algorithms .",
    "they express the feature pattern for example as a tree @xcite .",
    "while doing so , they aim for the best pattern and the most explaining features by alternating and optimizing the used feature mappings .",
    "in contrast , ` fresh ` extracts the best fitting of a fixed set of patterns .    as an example for",
    "a structured pattern extraction , in @xcite the authors search for six morphology types : constant , straight , exponential , sinusoidal , triangular , and rectangular phases .",
    "those phases are detected by structure detectors which then output a new time series whose values stand for the identified structure .",
    "based on this structure a domain - independent structural pattern recognition system is utilized to substitute the original time series signal by a known pattern . due to its fixed patterns , ` fresh ` can be considered to be a structured pattern extractor .",
    "of course , there are other promising approaches like the combination of nearest neighbor search with dynamic time warping @xcite , which is specialized on considering an ensemble of exactly one dedicated time series type and can not take meta - information into account . for binary classifications it scales with @xmath106 @xcite with @xmath107 and @xmath108 being the number of devices in the train and test set , respectively .",
    "this approach also has the disadvantage that all data have to be transmitted to a central computing instance .",
    "the extraction algorithm most similar to ours is presented by @xcite .",
    "it applies a linear estimator with greedy search and a constant initial model to identify the most important features , which has been considered in this paper as lda .",
    "the evaluation has shown , that ` fresh ` outperforms the approach of @xcite . also , `",
    "fresh ` provides a more general approach to time series feature extraction , because it is able to extract features for regression tasks and not only for classification .    despite the applications for time series classification and regression , the feature selection approach of ` fresh `",
    "could be included into kernel methods @xcite and therefore should find broad applicability in the machine learning community .",
    "in this work , ( ` fresh ` ) for time series classification and regression was introduced .",
    "it combines well established feature extraction methods with a scalable feature selection based on non - parametric hypothesis tests and the benjamini - yekutieli procedure . `",
    "fresh ` is highly parallel and suitable for distributed iot and industry 4.0 applications like predictive maintenance or process line optimization , because it allows to consider several different time series types per label and additionally takes meta - information into account .",
    "the latter has been demonstrated on basis of a process line optimization of project .",
    "our evaluation for ucr time series classification tasks has shown that ` fresh ` in combination with a subsequent pca outperforms all other feature extraction algorithms with respect to scalability and achieved accuracy .",
    "on the data set , it was even able to achieve a higher accuracy than a nearest neighbor search under dynamic time warping .",
    "the parallel nature of ` fresh ` with respect to both feature extraction and filtering makes it highly applicable in situations where data is fragmented over a widespread infrastructure and computations can not be performed on centralized infrastructure . due to its robustness and applicability to machine learning problems in the context of iot and industry 4.0",
    ", we are expecting that ` fresh ` will find widespread application .",
    "this research was funded in part by the german federal ministry of education and research under grant number ( project ) .",
    "yoav benjamini and yosef hochberg . controlling the false discovery rate : a practical and powerful approach to multiple testing .",
    "_ journal of the royal statistical society .",
    "series b ( methodological ) _ , pages 289300 , 1995 .",
    "vernica boln - canedo , noelia snchez - maroo , and amparo alonso - betanzos .",
    "_ feature selection for high - dimensional data_. artificial intelligence : foundations , theory , and algorithms .",
    "springer international publishing , 2015 .",
    "yanping chen , eamonn keogh , bing hu , nurjahan begum , anthony bagnall , abdullah mueen , and gustavo batista .",
    "the ucr time series classification archive .",
    "http://www.cs.ucr.edu/~eamonn/time_series_data/ , 07 2015 .",
    "maximilian christ , frank kienle , and andreas  w. kempa - liehr .",
    "time series analysis in industrial applications . in _ workshop on extreme value and time series analysis _ , kit karlsruhe , 03 2016 .",
    "doi : 10.13140/rg.2.1.3130.7922 .",
    "damian  r. eads , daniel hill , sean davis , simon  j. perkins , junshui ma , reid  b. porter , and james  p. theiler .",
    "genetic algorithms and support vector machines for time series classification . in bosacchi",
    "et  al . , editor , _ proc .",
    "spie 4787 , applications and science of neural networks , fuzzy systems , and evolutionary computation v _ , pages 7485 , 12 2002 .",
    "ramez elmasri and jae  young lee .",
    "implementation options for time - series data . in opher",
    "etzion , sushil jajodia , and suryanarayana sripada , editors , _ temporal databases : research and practice _ ,",
    "pages 115128 .",
    "springer berlin heidelberg , 1998 .",
    "url http://dx.doi.org/10.1007/bfb0053700 .",
    "jayavardhana gubbi , rajkumar buyya , slaven marusic , and marimuthu palaniswami .",
    "internet of things ( iot ) : a vision , architectural elements , and future directions . _ future generation computer systems _ , 290 ( 7):0 1645 , 09 2013 .",
    "jestinah m  mahachie john , francois van  lishout , elena  s gusareva , and kristel van  steen . a robustness study of parametric and non - parametric tests in model - based multifactor dimensionality reduction for epistasis detection .",
    "_ biodata mining _ , 60 ( 1):0 1 , 2013 .",
    "kenji kira and larry  a. rendell . a practical approach to feature selection . in _ proceedings of the ninth international workshop on machine learning _ ,",
    "ml92 , pages 249256 .",
    "morgan kaufmann publishers inc . , 1992 .",
    "roland nilsson , jos  m. pea , johan bjrkegren , and jesper tegnr .",
    "consistent feature selection for pattern recognition in polynomial time . _ the journal of machine learning research _ , 8:0 589612 , 2007 .",
    "fabian pedregosa , gal varoquaux , alexandre gramfort , vincent michel , bertrand thirion , olivier grisel , mathieu blondel , peter prettenhofer , ron weiss , vincent dubourg , and others .",
    "scikit - learn : machine learning in python .",
    "_ the journal of machine learning research _ , 12:0 28252830 , 2011 .",
    "xiaoyue wang , abdullah mueen , hui ding , goce trajcevski , peter scheuermann , and eamonn keogh .",
    "experimental comparison of representation methods and distance measures for time series data .",
    "_ data mining and knowledge discovery _ , 260 ( 2):0 275309 , 2013 ."
  ],
  "abstract_text": [
    "<S> the all - relevant problem of feature selection is the identification of all strongly and weakly relevant attributes . </S>",
    "<S> this problem is especially hard to solve for time series classification and regression in industrial applications such as predictive maintenance or production line optimization , for which each label or regression target is associated with several time series and meta - information simultaneously . here </S>",
    "<S> , we are proposing an efficient , scalable feature extraction algorithm , which filters the available features in an early stage of the machine learning pipeline with respect to their significance for the classification or regression task , while controlling the expected percentage of selected but irrelevant features .    </S>",
    "<S> the proposed algorithm combines established feature extraction methods with a feature importance filter . </S>",
    "<S> it has a low computational complexity , allows to start on a problem with only limited domain knowledge available , can be trivially parallelized , is highly scalable and based on well studied non - parametric hypothesis tests . </S>",
    "<S> we benchmark our proposed algorithm on all binary classification problems of the ucr time series classification archive as well as time series from a production line optimization project and simulated stochastic processes with underlying qualitative change of dynamics . </S>"
  ]
}