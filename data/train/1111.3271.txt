{
  "article_text": [
    "the most celebrated result in markov decision process ( mdp ) theory is _",
    "bellman s optimality principle _ , which can be stated as follows .",
    "( we assume that the reader is already generally familiar with mdps . )",
    "let @xmath0 be the state at ( discrete ) time @xmath1 and @xmath2 the reward received if action @xmath3 is taken at state @xmath0 ( the stagewise reward ) .",
    "let @xmath4 be optimal cumulative reward starting at state @xmath5 .",
    "then , bellman s principle states that for each time @xmath1 , @xmath6\\}\\ ] ] where @xmath7 is the random next state with distribution depending on @xmath0 and @xmath3 .",
    "moreover , replacing max by argmax on the right - hand side gives the optimal action at @xmath0 ( i.e. , it characterizes the optimal policy ) .",
    "but bellman s principle is more than just an equation  it embodies an idea that has become almost fundamentally axiomatic in markov decision theory .",
    "this idea is that the optimal policy solves the optimization problem not just at the initial state @xmath8 but also at all states reachable from it .    in this paper , we consider mdps with explicit constraints . such constrained mdps have been studied for at least a couple of decades and continues to draw interest ( see , e.g. , @xcite@xcite ) .",
    "we are interested here in a particular paper by haviv @xcite , who raises an issue that has not been addressed in the literature .",
    "basically , haviv constructs an example of a constrained mdp in which the optimal policy starting at the initial state @xmath5 is no longer optimal at states other than @xmath5 , not even at a state @xmath9 that is reachable from @xmath5 .",
    "he laments that this means that bellman s principle is violated .",
    "we will explore haviv s issue thoroughly .",
    "in particular , we will show that there is some preservation of bellman s principle , provided we account for the fact that some of the `` slackness '' in the constraint is spent in going from @xmath5 to a reachable @xmath9 .",
    "so , if we consider the optimal policy @xmath10 starting at state @xmath5 , the optimality of @xmath10 at state @xmath9 is with respect to a _ different _",
    "problem , one where the constraint is modified with the `` residual slackness . '' in analyzing haviv s problem , we will present some known results , some new results , and some related examples along the way to help us understand and resolve the problem .",
    "our analysis highlights the important maxim that when imposing constraints on a decision problem , the constraints should apply only to those things over which we have control .",
    "in @xcite , haviv gives an example ( reproduced in fig .",
    "[ fig : havivf1 ] ) in which he shows that , given an optimal policy for an optimization problem starting at some state @xmath5 , the policy is not optimal with respect to the same problem starting at a reachable state @xmath9 .",
    "the structure of the problem is a _ multichain _",
    "mdp with initial state @xmath5 , which is transient .",
    "there are three recurrent subchains that could be reached from @xmath5 .",
    "there is no reward for being in chain  1 , while the stagewise reward is @xmath11 at every state in chain  2 and @xmath12 at every state in chain  3 .",
    "the constraint is that the expected frequency of visits to states in @xmath13 must not exceed @xmath14 ( think of states in @xmath15 as the `` bad '' states ) . while in chain  @xmath16 ( @xmath17 ) , the frequency of visits to @xmath18 is as shown in fig .  [",
    "fig : havivf1 ] ( e.g. , @xmath19 for @xmath20 ) .",
    "there is only one state in which an action decision must be made : in state @xmath9 , we can choose either action @xmath3 or @xmath21 .",
    "a quick examination of haviv s problem shows that there is only one feasible policy : at state @xmath9 , select action @xmath3 . selecting action @xmath21 at state",
    "@xmath9 would violate the constraint , because the resulting markov chain would visit states in @xmath15 with frequency @xmath22 .",
    "however , if the starting state were @xmath9 , we would want to pick action @xmath21 , because this leads to chain  3 where the stagewise reward exceeds that of chain  2 , and the frequency of visits to @xmath15 in chain  3 ( @xmath23 ) is @xmath24 , which does not exceed the constraint of @xmath14 . as noted before , this leads to haviv s lament ",
    "bellman s principle is violated , because the optimal policy starting at state @xmath5 is no longer optimal starting at state @xmath9 , even though @xmath9 is reachable from @xmath5 . as haviv points out in @xcite and",
    "we will emphasize again later , the issue is related to the multichain nature of the example : that there are transient states and recurrent subchains that are not reachable from each other .",
    "more specifically , haviv s problem illustrates that as far as optimality of a policy is concerned , arriving at state @xmath9 from @xmath5 is different from starting at @xmath9 . from this point of view",
    ", the issue raised by haviv appears to be related to that of _ time consistency _ in risk averse multistage stochastic programming , identified in a recent paper by shapiro @xcite .",
    "the same issue is also discussed in the economics literature on multistage decision problems arising in dynamic portfolios ; see , e.g. , @xcite , @xcite .",
    "this issue has been recognized for some time in the context of time - varying preferences @xcite , @xcite and game - theoretic formalisms of such changing tastes @xcite , @xcite .    in lamenting the violation of bellman s principle ,",
    "haviv quotes denardo @xcite on the principle of optimality : `` an optimal policy has the property that whatever the initial node ( state ) and initial arc ( decision ) are , the remaining arcs ( decisions ) must constitute an optimal policy with regard to the node ( state ) resulting from the first transition . ''",
    "but , we ask , must the policy be optimal with respect to the _ same _ problem ?",
    "indeed , denardo concedes that : `` the term _ principle _ of optimality is , however , somewhat misleading ; it suggests that this is a fundamental truth , not a consequence of more primitive things . ''",
    "we will show that for a constrained mdp , the optimal policy starting at one state is optimal with respect to a problem with a _ modified _ constraint at each reachable state .",
    "basically , in going from state @xmath5 to @xmath9 , we have `` spent '' some of the constraint , so the `` residual '' constraint is reduced .",
    "we submit that this is not an unreasonable predicament , and still satisfies denardo s version of bellman s principle .",
    "moreover , the articulation of bellman s principle we derive here is a consequence of basic optimality conditions ( see theorems  [ thm : opt_av ] and [ thm : optreach ] ) , which we argue are instances of `` more primitive things '' referred to by denardo .",
    "we first provide a framework for analyzing mdps with inequality constraints , of the kind that is considered by haviv @xcite .",
    "we have to set this up more rigorously than the statement of bellman s equation in the last section , because : ( 1 ) we wish to incorporate explicit inequality constraints ; ( 2 ) we consider the case of expected long - term average reward ( where bellman s equation looks slightly different ) ; and ( 3 ) we need sufficient generality for multichain problems . for this reason , we need some formal notation :    * state space : @xmath25 , assumed countable .",
    "* state sequence : @xmath26 . * stagewise reward : @xmath27 * stagewise constraint : @xmath28 * if @xmath5 is a state and @xmath3 an action , we write @xmath29 for the conditional expectation given @xmath30 . for example , if @xmath31 is a given function , then @xmath32 $ ] means that @xmath33 is distributed according to the transition probability distribution given @xmath34 , and @xmath32 $ ] is the conditional expectation of @xmath35 with respect to this distribution given @xmath34 .",
    "if a policy @xmath36 is given , then instead of writing @xmath37 $ ] , we simply write @xmath38 $ ] . similarly , given a policy @xmath36 and an initial state @xmath39 , the distribution of the markov process @xmath40 is well defined , and we write @xmath41 for the conditional expectation with respect to this distribution . * similarly , for conditional probability given an initial state @xmath39 and policy @xmath36 , we use the notation @xmath42 and @xmath43 .",
    "we use `` @xmath43-a.s . '' to mean almost surely ( with probability one ) with respect to the probability measure @xmath43 . * for a vector @xmath44 , we write @xmath45 to mean nonnegativity of each component .      fix a state @xmath46 and set the initial state @xmath8 . let @xmath47 and @xmath48 the objective function is given by @xmath49,\\ ] ] and the constraint function by @xmath50 .",
    "\\label{eqn : w}\\ ] ] with this notation , the optimization problem given @xmath8 is as follows : @xmath51 note that this form of the problem is sufficiently general to cover other inequality constraints : @xmath52 , @xmath53 , etc .",
    "first , we give sufficient conditions under which a policy is optimal with respect to ( [ eqn : opt ] ) .",
    "though stated formally , we provide this result not to claim any novelty in it , but merely so that we can use it as a rigorous platform on which to frame our analysis . indeed , similar results can be found in the book by altman @xcite , though not exactly in this form ( which is constructed explicitly for the convenience of our current purposes ) .",
    "we also provide a proof , using only elementary and familiar arguments , similar to those in the book by ross @xcite .",
    "[ thm : opt_av ] fix @xmath46 and set the initial state @xmath8 .",
    "suppose there exist a policy @xmath10 , a vector @xmath54 , a constant @xmath55 , and a bounded function @xmath31 such that the following hold :    ( a1 ) : :    @xmath56 ( a2 ) : :    @xmath57 ( a3 ) : :    @xmath58 ( a4 ) : :    @xmath59\\}$ ] for @xmath60 ,    @xmath61-a.s .",
    "( a5 ) : :    @xmath62\\}$ ] for @xmath60 ,    @xmath61-a.s .",
    "then @xmath10 is optimal with respect to ( [ eqn : opt ] ) and @xmath63 .",
    "let @xmath36 be a feasible policy .",
    "( note that @xmath10 is feasible by assumption ( a1 ) . )",
    "then by assumption ( a4 ) , @xmath61-a.s .  for @xmath64 , @xmath65\\ } \\\\",
    "& \\geq r(x_t,\\pi(x_t ) ) + \\mu^{\\top}c(x_t,\\pi(x_t ) ) + \\ex_{x_t}^\\pi[l^*(x_{t+1 } ) ] \\end{aligned}\\ ] ] with equality if @xmath66 ( by assumption ( a5 ) ) .",
    "now multiply throughout by @xmath67 and sum from @xmath68 to @xmath69 to obtain @xmath70,\\end{aligned}\\ ] ] which can be written as @xmath71-l^*(x_t)\\right)\\\\ & \\mbox{\\qquad } + \\frac{1}{t}\\ex_{x_{t-1}}^\\pi[l^*(x_t)].\\end{aligned}\\ ] ] next , take limits as @xmath72 , take expectation @xmath41 , use the boundedness assumption on @xmath73 , and use the fact that @xmath74=\\ex_{x_0}^\\pi[l^*(x_t)]$ ] ( for @xmath75 ) to obtain @xmath76 with equality if @xmath66 . because @xmath36 is feasible , @xmath77 . hence , because @xmath57 by assumption ( a2 ) , @xmath78 now , for @xmath66 , we use assumption ( a3 ) to obtain @xmath79 and in particular @xmath80 .",
    "this completes the proof . @xmath81",
    "the equation @xmath82\\}\\end{aligned}\\ ] ] has the resemblance of bellman s equation for the unconstrained case .",
    "indeed , we can think of this optimality condition for the constrained case as bellman s equation associated with an unconstrained mdp with stagewise reward @xmath83 ( called the _",
    "lagrangian reward _ ) .",
    "however , the optimality conditions ( a1a5 ) include not only bellman s equation , but also conditions ( equation and inequalities ) akin to karush - kuhn - tucker ( kkt ) conditions .",
    "this form of the optimality condition benefits from the usual interpretation of the _ multiplier _ vector @xmath84 as a `` price '' vector , and suggests the possibility of approaching the problem using duality principles ( though we do not pursue this line of approach any further here ) .",
    "we say that a state @xmath9 is _ reachable _ from @xmath5 at time @xmath85 under policy @xmath36 if , given @xmath8 , we have @xmath86 .",
    "we say that @xmath9 is _ reachable _ from @xmath5 under @xmath36 if there exists @xmath85 such that it is reachable at @xmath1 .",
    "next , we show that the sufficient conditions in theorem  [ thm : opt_av ] are enough for the _ same _ @xmath73 to satisfy the bellman s equation at every reachable state .",
    "fix @xmath46 .",
    "suppose there exist a policy @xmath10 , a vector @xmath54 , a constant @xmath55 , and a bounded function @xmath31 such that assumptions ( a1a5 ) hold .",
    "then , for each state @xmath87 reachable from @xmath5 under @xmath10 , @xmath88\\ } \\\\",
    "\\pi^*(y ) & \\in { \\mathop{\\mathrm{argmax}}}_a \\{r(y , a ) + \\mu^{\\top}c(y , a ) + \\ex_{y , a}[l^*(x')]\\},\\end{aligned}\\ ] ] where @xmath89 is distributed according to the transition distribution given @xmath90 .",
    "suppose there is some state @xmath87 reachable from @xmath39 under @xmath10 such that @xmath91\\}$ ] .",
    "since @xmath9 is reachable , there is some @xmath92 such that @xmath93 .",
    "let @xmath94 be the event that @xmath95\\}$ ] .",
    "then , by assumption , @xmath96 . however , we can also write @xmath97 which is a contradiction .    a similar argument yields @xmath98\\}$ ] .",
    "this completes the proof .",
    "@xmath81    the theorem above immediately implies that conditions ( a4 ) and ( a5 ) ( along sample paths ) hold for any state @xmath9 reachable from @xmath5 .",
    "[ cor : bellman ] fix @xmath46 .",
    "suppose there exist a policy @xmath10 , a vector @xmath54 , a constant @xmath55 , and a bounded function @xmath31 such that assumptions ( a1a5 ) hold .",
    "let @xmath87 be reachable from @xmath5 under @xmath10 , and suppose we set the initial state to be @xmath99 . then , @xmath61-a.s .  for @xmath100 , @xmath101\\ } \\\\ & \\pi^*(x_t)\\\\   & \\in { \\mathop{\\mathrm{argmax}}}_a \\{r(x_t , a ) + \\mu^{\\top}c(x_t , a ) + \\ex_{x_t , a}[l^*(x_{t+1})]\\}.\\end{aligned}\\ ] ]    the result above shows that bellman s _ equation _ holds at all reachable states .",
    "specifically , ( a4 ) and ( a5 ) hold for any state reachable from @xmath5 ( with the objective function value @xmath4 and multiplier vector @xmath84 ) . but",
    "this is not enough to show that @xmath10 is _ optimal _ at any state reachable from @xmath5 .",
    "the key hurdle is feasibility ( i.e. , ( a1 ) ) . to be specific ,",
    "suppose that state @xmath9 is reachable from @xmath5 under @xmath10 .",
    "in general , it is not true that @xmath10 is optimal with respect to the problem @xmath102 indeed , it is easy to construct examples for which @xmath10 is not feasible for the above problem ( e.g. , haviv s example @xcite ) .",
    "however , a modification to the constraint ( which depends on @xmath5 ) gives us an optimization problem starting at @xmath9 for which @xmath10 is indeed optimal , as stated below .",
    "first , we need some additional notation .",
    "given @xmath8 , let @xmath9 be reachable from @xmath5 at time @xmath1 under @xmath10 . define @xmath103 \\frac { \\pr_{x_0}^{\\pi^*}\\{x_t \\neq y\\ } } { \\pr_{x_0}^{\\pi^*}\\{x_t = y\\}}.\\ ] ] note that if @xmath104\\geq 0 $ ] , then @xmath105 .",
    "moreover , the smaller the value of @xmath106 , the larger the value of @xmath107 .",
    "[ thm : optreach ] fix @xmath46 .",
    "suppose there exist a policy @xmath10 , a vector @xmath54 , a constant @xmath55 , and a bounded function @xmath31 such that assumptions ( a1a5 ) hold with @xmath8 .",
    "let @xmath9 be reachable from @xmath5 at time @xmath1 under @xmath10 .",
    "then @xmath10 is optimal with respect to the problem @xmath108 and @xmath109 .",
    "we have , given @xmath8 , @xmath110 \\\\ & = \\ex_{x_0}^{\\pi^*}\\left[\\lim_{t\\to\\infty } \\frac{1}{t } \\sum_{k=0}^{t } c(x_k,\\pi^*(x_k ) ) \\right.\\\\ & \\left.\\mbox{\\qquad } + \\frac{t - t}{t } \\left ( \\frac{1}{t - t } \\sum_{k = t}^{t-1 } c(x_k,\\pi^*(x_k))\\right ) \\right ] \\\\ & = \\ex_{x_0}^{\\pi^*}\\left [ \\ex_{x_{t}}^{\\pi^*}\\left [   \\lim_{t\\to\\infty } \\frac{1}{t - t } \\sum_{k = t}^{t-1 } c(x_k,\\pi^*(x_k ) ) \\right ] \\right ] \\\\ & = \\ex_{x_0}^{\\pi^*}\\left[w^{\\pi^*}(x_{t})\\right ] \\\\ & = w^{\\pi^*}(y)\\pr_{x_0}^{\\pi^*}\\{x_t = y\\ } + \\ex_{x_0}^{\\pi^*}\\left[w^{\\pi^*}(x_t)|x_t\\neq y\\right ] \\pr_{x_0}^{\\pi^*}\\{x_t\\neq y\\ } \\\\ &",
    "= \\left(w^{\\pi^*}(y ) -c_y(x)\\right)\\pr_{x_0}^{\\pi^*}\\{x_t = y\\}.\\end{aligned}\\ ] ] by assumption , @xmath111 , which implies that @xmath112 .",
    "moreover , because @xmath113 , we have @xmath114 .    now , set the initial condition @xmath99 .",
    "define a new stagewise constraint function @xmath115 ( subtracting the same constant for each @xmath3 ) and let @xmath116 , which is the expected average constraint function defined accordingly using @xmath117 , analogous to ( [ eqn : w ] ) . from the above",
    ", we have @xmath118 and @xmath119 .",
    "by corollary  [ cor : bellman ] , @xmath61-a.s .  for @xmath100 , @xmath120\\}.\\end{aligned}\\ ] ]",
    "subtract @xmath121 from both sides to obtain @xmath122\\}.\\end{aligned}\\ ] ] finally , again by corollary  [ cor : bellman ] , @xmath61-a.s .  for @xmath100 , @xmath123\\}.\\ ] ] note that if we substitute @xmath117 for @xmath124 , the above still holds",
    ". we can now apply theorem  [ thm : opt_av ] to obtain the desired result .",
    "@xmath81    the theorem above has the interpretation of bellman s principle for constrained problems . recall that in the unconstrained case , this principle states that if @xmath10 is an optimal policy for a problem starting at some state @xmath5 , then it is also optimal for a problem starting at any state @xmath9 reachable from @xmath5 .",
    "the main wrinkle in the constrained case is that the constraint for the problem starting at @xmath9 is different from starting at @xmath5 , because we have to take into account how the constraint function @xmath125 depends on other states that can be reached .",
    "basically , @xmath126 plays the role of a `` residual slackness '' of the constraint at the reachable state @xmath9 , after the sojourn from @xmath5 to @xmath9 .",
    "note that if @xmath127 , then constraint is more stringent at @xmath9 . in this case",
    ", we can interpret @xmath126 as the constraint that is `` spent '' in going from @xmath5 to @xmath9 . on the other hand ,",
    "if @xmath128 , then we _ gain _ some slackness in going from @xmath5 to @xmath9 ( i.e. , constraint is _ less _ stringent ) .",
    "we can use theorem  [ thm : optreach ] to construct the optimization problem starting at @xmath9 for which the given policy is indeed optimal .",
    "we use the notation @xmath129 for the indicator function of @xmath15 , so that @xmath130 if @xmath131 , and @xmath132 otherwise .",
    "we have :    * @xmath133 * @xmath134 * @xmath135",
    "so , instead of needing the expected frequency of visits to @xmath15 not to exceed @xmath14 , at state @xmath9 the constraint becomes @xmath136 ( more stringent ) .",
    "in other words , we `` spent '' @xmath137 of the constraint in going from @xmath5 to @xmath9 , and the `` residual '' constraint starting at state @xmath9 is that the frequency of visits to @xmath15 should not exceed @xmath136 . in this case , clearly only action @xmath3 is feasible at @xmath9 .",
    "would our modified form of bellman s principle satisfy haviv ?",
    "we suspect not .",
    "haviv s point is that intuition dictates that the optimal policy should pick action @xmath21 at state @xmath9 , though he acknowledges that such a policy would not be feasible with respect to the problem ( [ eqn : opt ] ) .",
    "he therefore goes on to argue that the _ form _ of the constraint in ( [ eqn : opt ] ) is problematic .",
    "the version of bellman s principle in theorem  [ thm : optreach ] is not entirely satisfactory because , one could argue , the constraint should not change depending on what happened in the past .    this is related to the issue of _ time consistency _ in @xcite .",
    "shapiro @xcite defines time consistency as `` the requirement that at every state of the system our ` optimal ' decisions should not depend on scenarios which we already know can not happen in the future . '' in haviv s example , once we are in state @xmath9 , we know that we will not enter chain  1 .",
    "yet , it is the frequency of visits to states in @xmath20 within chain  1 that causes action @xmath21 to be infeasible at state @xmath9 .",
    "this seems to be a legitimate concern .",
    "we further illustrate this concern below by applying our result to a different example .",
    "this example is in contrast to haviv s , because it turns out that at reachable states that are unlikely to be visited , the constraint might be unreasonably relaxed .",
    "consider the problem in fig .",
    "[ fig : squander ] .",
    "starting at state @xmath5 , the process will go to either state @xmath9 or @xmath138 depending on whether or not we win the lottery , respectively .",
    "the probability of winning the lottery is ( realistically ) a small number @xmath139 , as shown in the figure .",
    "if we do not win the lottery , we have the choice of whether or not to buy a yacht . depending on our choice",
    ", we will end up in one of two possible subchains . in the unlikely event",
    "that we _ do _ win the lottery , we have the choice of whether or not to squander all our money . again , depending on this choice , we end up in one of two possible subchains . within each subchain ,",
    "the stagewise reward at all states is fixed at the value shown in the figure ( e.g. , @xmath140 in chain  1 ) .",
    "these reward values are meant to signify the level enjoyment of life within these subchains .        in this example",
    ", the constraint is that the expected frequency of visits to states in @xmath141 should not exceed @xmath142 .",
    "this constraint reflects the desire that we limit the probability that we will go broke ( have no money ) before retiring .",
    "the states in the problem that represent being broke are those in @xmath141 .",
    "the frequency of visits to the `` bad '' states in each of the subchains is shown as @xmath143 , @xmath144 , in fig .",
    "[ fig : squander ] .",
    "it is clear that because @xmath139 is taken to be very small , it is overwhelmingly likely that we will enter state @xmath138 , in which case we can not afford to buy a yacht  doing so would send us into chain  3 , where the frequency of visiting `` bad '' states is @xmath145 , exceeding @xmath142 .",
    "but what about in state @xmath9 , which corresponds to winning the lottery ?    in this problem , it turns out that @xmath146 .",
    "so , depending on how small @xmath139 is , @xmath126 can be made arbitrarily negative .",
    "specifically , for @xmath147 , the optimal action at state @xmath9 is to squander . to be sure",
    ", it is not that we can spend more if we win the lottery , but that because it is so unlikely that we win , once we win we can do whatever we like without violating the constraint .",
    "this clearly illustrates that the form of the constraint is problematic , as haviv points out .",
    "how then can we resolve haviv",
    "s problem ?",
    "haviv advocates the use of _ sample - path _ constraints , where we remove the expectation in the constraint function in ( [ eqn : w ] ) and require instead that the inequality be satisfied with probability one . in our notation , this would correspond to , given @xmath8 , @xmath148 it is clear that with such a constraint , a policy @xmath36 is feasible at @xmath5 if and only if feasible at each state reachable from @xmath5 .",
    "note that this modification to the constraint immediately alleviates the problem illustrated in fig .",
    "[ fig : squander ] .",
    "in contrast to the previous form of the constraint , it would no longer be feasible to squander our money even if we win the lottery .      to illustrate this point",
    "further , consider the problem in fig .",
    "[ fig : lottery ] , which is very similar to fig .",
    "[ fig : squander ] but simpler . in the current problem",
    ", again we have the ( unlikely ) event of winning the lottery .",
    "however , regardless of winning , we have the decision of whether or not to buy a yacht . depending on whether or not we win the lottery and",
    "what decision we make about the yacht , we will enter one of four subchains , wherein there is some probability of going broke before retiring ( as before , these are shown as @xmath143 , @xmath144 , in fig .",
    "[ fig : lottery ] ) .",
    "the stagewise reward values shown in the figure are again meant to signify our enjoyment of life within these subchains .        as in the problem of fig .",
    "[ fig : squander ] , if we impose sample - path constraints , we will quickly arrive at the conclusion that in state @xmath138 , we can not decide to buy a yacht because doing so would violate the constraint in chain  3 .",
    "however , in state @xmath9 , where we have won the lottery , we can in fact buy a yacht ; doing so would not violate the constraint .",
    "the optimal choice at state @xmath9 is indeed to buy a yacht , leading to maximal enjoyment of life ( within this example ) .",
    "suppose we make the modification from expected constraint to sample - path constraint in haviv s problem .",
    "specifically , we now require that , with probability one , the frequency of visits to @xmath15 not exceed @xmath14 . then , _ no policy is feasible _ , because there is a @xmath149 probability that the process will enter chain  1 , where the frequency of visits to @xmath15 is @xmath19 .",
    "haviv @xcite does point this out , but does not provide a resolution to it .    in other words , using a sample - path constraint does not resolve haviv s problem , because no policy would be feasible .",
    "moreover , for some class of constrained mdps ( including haviv s ) , sample - path constraints can be converted to _ equivalent _ expected constraints .",
    "these are what we might call _ trans - policy decomposable _ mdps ( see @xcite ) . basically , to convert sample - path constraints into expected constraints in such mdps",
    ", we impose an expected constraint at each subchain . for example",
    ", for subchain @xmath150 , use the constraint function @xmath151 in the expected form of the constraint .    in the problems of fig .",
    "[ fig : squander ] and fig .",
    "[ fig : lottery ] , for example , we do not need to impose sample - path constraints ; instead , we can impose the usual ( expected ) form of the constraint in each of the four subchains .",
    "if we do so , these problems would no longer suffer from haviv s problem , and the optimal policy would be equally optimal at all reachable states without having to change the constraint .",
    "the conversion of sample - path constraints into equivalent expected constraints highlights an issue that is , at heart , what gives rise to haviv s problem : at state @xmath9 in fig .",
    "[ fig : havivf1 ] , we have no control over whether the process will enter chain  1 .",
    "indeed , this tells us that when imposing expected constraints in the subchains , we should not impose them at _ all _ subchains .",
    "in particular , we should not impose any constraint at chain  1 , because we have no control ( at @xmath9 ) over whether or not we enter it .",
    "the constraints should be imposed only at chains 2 and 3 , which depend on a decision over which we have control . if we do this , then even haviv s original problem in fig .",
    "[ fig : havivf1 ] would be resolved : the optimal policy with respect to initial state @xmath5 is equally optimal ( and feasible ) at state @xmath9 , and would select action @xmath21 as desired .",
    "another way to express this observation is that constraints should be imposed only on the _ consequence _ of decisions , expressing conditions on the desired ( or undesired ) impact of decisions once they are made . in haviv s",
    "example , expressing the constraint at state @xmath5 does not properly reflect the impact of actions at @xmath9 , which do not control whether or not the system enters chain  1 .",
    "the same would be true even if we modify the example to include action choices at state @xmath5 ( e.g. , we can control the probability of entering chain  1 ) .",
    "the constraint at state @xmath5 would still not properly reflect the impact of actions at @xmath9 , which do not control entry into chain  1 , giving rise to haviv s lament .",
    "r. kihlstrom , `` risk aversion and the elasticity of substitution in general dynamic portfolio theory : consistent planning by forward looking , expected utility maximizing investors , '' _ j. math .",
    "_ , vol .  45 , no .  9 - 10 , pp .  634663 , sep .  2009"
  ],
  "abstract_text": [
    "<S> we consider an example by haviv ( 1996 ) of a constrained markov decision process that , in some sense , violates bellman s principle . </S>",
    "<S> we resolve this issue by showing how to preserve a form of bellman s principle that accounts for a change of constraint at states that are reachable from the initial state .    </S>",
    "<S> _ keywords : _ markov decision processes ; constrained optimization ; bellman ; time consistency . </S>"
  ]
}