{
  "article_text": [
    "multivariate regression model is a key statistical tool for analyzing dataset with multiple responses .",
    "a standard approach is to decompose the multivariate regression model and fit each response via a marginal univariate regression model .",
    "however , this approach is suboptimal in general as it completely ignores the dependency structure among the responses .",
    "for example , the gene expressions of many genes are strongly correlated due to the shared genetic variants or other unmeasured common regulators ( kendziorski et al . , 2006 ) .",
    "with the dependency structure appropriately incorporated , one would naturally expect a more efficient multivariate regression model in terms of both estimation and prediction .",
    "furthermore , the dependency structure among the responses can be nicely interpreted in a graphical model under the multivariate gaussian assumption ( edwards , 2000 ) , where two gaussian responses are connected in the graph if the corresponding entry in the precision matrix ( inverse covariance matrix ) is nonzero .    in literature , to model the multivariate regression problem , breiman and friedman ( 1997 ) proposed the curd and whey method to improve the prediction performance by utilizing the dependency among responses .",
    "the curd part fits a univariate regression model for each response against the covariates , and the whey part refits each response against the fitted values from the curd part .",
    "however , the method is developed in the low dimensional setup , and does not address the challenges when the data dimension is diverging .",
    "yuan et al .",
    "( 2007 ) and chen and huang ( 2012 ) proposed the high dimensional reduced - rank regression model , which assumes that all marginal regression functions reside in a common low dimensional space .",
    "this approach focuses on dimension reduction and largely replies on the reduced - rank assumption .",
    "turlach et al .",
    "( 2005 ) imposed the sparsity in the regression model through a @xmath2-norm penalty of the coefficient matrix .",
    "this method is able to identify sparsity , but may produce bias for model estimation due to the @xmath2-norm penalty",
    ". the recent work by rothman et al .",
    "( 2010 ) , yin and li ( 2011 ) and lee and liu ( 2012 ) formulated the multivariate regression problem in a penalized log - likelihood framework , so that it allows joint estimation of the multivariate regression model and the conditional gaussian graphical model .",
    "this formulation requires an alternating optimization scheme , which is computationally expensive and can not guarantee global optimum .    in this paper",
    ", we propose a multivariate conditional regression model to tackle the multivariate regression problem with diverging dimension .",
    "the key idea is to formulate the protblem as the conditional log - likelihood function of each response conditioned on the covariates and other responses .",
    "the conditional log - likelihood function is then equipped with the adaptive lasso penalty ( zou , 2006 ) to facilitate joint estimation of the sparse multivariate regression coefficient matrix and the sparse precision matrix .",
    "the proposed model leads to a series of augmented adaptive lasso regression models , which can be efficiently solved by any existing optimization package .",
    "more importantly , its asymptotic properties are established in terms of the estimation consistency and selection consistency with diverging dimension . in specific ,",
    "the dimension of covariates and the number of responses are allowed to diverge in an exponential order of the sample size .",
    "numerical experiments with both simulated and real examples also support the effectiveness of the proposed method .",
    "the rest of the paper is organized as follows .",
    "section 2 provides a brief introduction to the multivariate regression model , with an emphasis on the penalized log - likelihood method .",
    "section 3 describes the proposed penalized conditional log - likelihood method in details , with theoretical justification in section 4 and numerical experiments in section 5 .",
    "section 6 contains a discussion , and the appendix is devoted to the technical proofs .",
    "in a multivariate regression setting , supposed that the training dataset consists of @xmath3 , where @xmath4 and @xmath5 .",
    "let @xmath6 and @xmath7 be the @xmath8 design matrix and @xmath9 response matrix , and let @xmath10 and @xmath11 be the @xmath12-th covariate and the @xmath13-th response . for simplicity , the covariates and responses are centered , so that @xmath14 a standard multivariate regression model is then formulated as @xmath15 where @xmath16 with @xmath17 being the regression coefficient for the @xmath13-th response , and @xmath18 with @xmath19 being the @xmath20-th error vector .",
    "the random vector @xmath21 s are assumed to be independent and identically sampled from a @xmath22-dimensional gaussian distribution @xmath23 with positive definite @xmath24 .    the maximum likelihood formulation of ( [ eqn : multivar ] ) , after dropping constant terms , yields that @xmath25 where @xmath26 is also positive definite and known as the precision matrix .",
    "the precision matrix is closely connected with the gaussian graphical models ( edward , 2000 ) since the conditional dependency structure among the responses can be fully determined by @xmath27 .",
    "specifically , @xmath28 implies that the @xmath29-th and @xmath30-th responses are conditionally independent given the covariates and other response variables .",
    "when the dimension of covariates is large , it is generally believed that the responses only rely on a small proportion of them , while other covariates are noise and provide no information about the responses at all .",
    "in addition , when the number of responses is large , the dependency structure among responses becomes sparse as some responses may have little relationship with each other .",
    "therefore , penalized log - likelihood approach has been widely employed to analyze the multivariate regression model in literature , including rothman et al .",
    "( 2010 ) , yin and li ( 2011 ) and lee and liu ( 2012 ) .",
    "the penalized likelihood approach can be formulated as @xmath31 where @xmath32 and @xmath33 are sparsity - encouraging penalties , such as the adaptive lasso penalties @xmath34 and @xmath35 with weights @xmath36 and @xmath37 , and @xmath38 and @xmath39 are two tuning parameters . to optimize ( [ eqn : multi_pls ] ) , alternative updating scheme is used .",
    "it updates @xmath40 and @xmath27 separately pretending the other party is fixed . in specific , when @xmath40 is fixed , ( [ eqn : multi_pls ] ) can be solved via the graphical lasso algorithm ( friedman , 2008 ) , and when @xmath27 is fixed , ( [ eqn : multi_pls ] ) can be solved via the coordinate descent algorithm ( lee and liu , 2012 ) . however , as pointed out in yin and li ( 2011 ) and lee and liu ( 2012 ) , the alternative updating scheme can not guarantee the global optimum , and is often computationally expensive and thus not practically scalable .",
    "in this section , a new estimation method based on penalized conditional log - likelihood is developed for jointly estimating the sparse multivariate regression coefficient matrix and the sparse precision matrix .",
    "the key idea is motivated from the simple fact that given the model @xmath41 in ( [ eqn : multivar ] ) , @xmath42 for any @xmath43 , where @xmath44 denotes the response matrix without @xmath45 , @xmath46 denotes the coefficient matrix without @xmath47 , @xmath48 , @xmath47 stays the same as in ( [ eqn : multivar ] ) , and @xmath49 since @xmath50 is always positive , it follows from ( [ eqn : spar_omega ] ) that @xmath51 , where @xmath52 with @xmath53 for convenience .",
    "consequently , the sparsity in @xmath27 can be determined by whether @xmath54 or not , and the sparsity in @xmath40 can be determined by whether @xmath55 or not .    to allow joint estimation of the sparse multivariate regression coefficient matrix and the sparse precision matrix",
    ", we then formulate the model in ( [ eqn : cond_dist ] ) as a series of penalized conditional regressions of each response against the covariates and other responses . in specific , for the @xmath13-th response , @xmath56 where @xmath57 is the usual euclidean norm , @xmath58 and @xmath59 are the adaptive lasso penalties . when @xmath60 in ( [ eqn : cond_pls ] ) is replaced by an initial consistent estimate @xmath61 , the final formulation for the proposed multivariate conditional regression model is @xmath62 the following computing algorithm can be employed to solve ( [ eqn : cond_pls0 ] ) .    ' '' ''    : + _ step 1 .",
    "_ initialize @xmath63 , @xmath36 and @xmath37 .",
    "+   _ step 2 .",
    "_ for @xmath43 , solve ( [ eqn : cond_pls ] ) for @xmath64 and @xmath65 .    ' '' ''    as computational remarks , @xmath63 can be initialized by the separate lasso regression ignoring the dependency structure .",
    "the weights @xmath36 and @xmath66 are set as @xmath67 and @xmath68 as in zou ( 2006 ) , where @xmath69 and @xmath70 are any consistent estimates of @xmath71 and @xmath72 , respectively . since",
    "( [ eqn : cond_pls ] ) is a convex optimization problem , its global minimum can be obtained by any available adaptive lasso regression procedure .",
    "furthermore , the coordinate descent algorithm ( friedman et al . , 2007 ) can be employed to further improve the computational efficiency of solving ( 6 ) .",
    "more importantly , _",
    "step 2 _ fits the adaptive lasso regression model ( 6 ) for each @xmath13 , and thus can be easily parallelized and distributed to multiple computing nodes .",
    "therefore , _",
    "algorithm 1 _ is scalable and can efficiently handle dataset with big size .    when identifying the sparsity in the conditional graphical model defined by @xmath27 , the symmetry of @xmath27 implies that @xmath73 , and thus @xmath74 .",
    "consequently , additional refinement is necessary to correct the possible inconsistency in @xmath75 .",
    "similar as in meinshausen and b@xmath76hlmann ( 2006 ) , one natural way is to set @xmath77 or a less conservative way is to set @xmath78 in the numerical experiments , the less conservative way is used and the resultant selection performance in @xmath79 appears to be satisfactory .",
    "this section establishes the asymptotic properties of the proposed multivariate conditional regression model in terms of the selection and estimation accuracy .",
    "let @xmath80 be the true regression coefficient matrix , @xmath81 be the inverse of the true covariance matrix @xmath82 , and @xmath83 be defined as in ( [ eqn : spar_omega ] ) with @xmath84 .",
    "the selection accuracy is measured by the sign agreement between @xmath85 and @xmath86 , and the estimation accuracy is quantified by the asymptotic normality of @xmath87 .    without loss of generality , we assume that @xmath88 for all @xmath29 s , and denote @xmath89 let @xmath90 , @xmath91 , @xmath92 , @xmath93 , @xmath94 , and @xmath95 .",
    "let @xmath96 , @xmath97 , @xmath98 , and @xmath99 .",
    "denote @xmath100 and @xmath101 as the minimum and maximum eigenvalues of a matrix @xmath102 .",
    "denote @xmath103 and @xmath104 as the tuning parameters used in the initial lasso regression and ( [ eqn : cond_pls ] ) , respectively .",
    "the following technical conditions are assumed .    1 .",
    "there exists a positive constant @xmath105 such that @xmath106 and @xmath107 .",
    "in addition , @xmath108 .",
    "2 .   for some integers",
    "@xmath109 , @xmath110 , @xmath111 and a positive constant @xmath112 , @xmath113 3 .",
    "let @xmath114 and @xmath115 be defined as below , @xmath116    assumption ( a1 ) implies that @xmath117 , @xmath118 , and @xmath119 .",
    "assumption ( a2 ) is similar as the restricted eigenvalue assumption in bickel et al .",
    "( 2008 ) and zhou et al .",
    "it implies that for any subset @xmath120 with @xmath121 , we have @xmath122 , where @xmath123 assumption ( a3 ) is similar as the condition in zhao and yu ( 2006 ) and meinshausen ( 2007 ) , and implies that the nonzero @xmath124 and @xmath125 will not decay too fast to be dominated by the noise terms .    [ thm : sel_cons ] ( selection consistency )",
    "supposed that conditions ( a1)-(a3 ) are satisfied with @xmath126 and @xmath127 , the initial @xmath69 and @xmath70 are set as the solution of the separate lasso regression , and @xmath104 . then as @xmath128 , @xmath129 when @xmath130 , @xmath131 , and @xmath132 .",
    "[ thm : est_cons ] ( asymptotic normality ) supposed that the conditions in theorem [ thm : sel_cons ] are satisfied .",
    "let @xmath133 , where @xmath134 is any @xmath135 vector with unit length , and @xmath136 is the principle submatrix of @xmath137 defined by @xmath138 .",
    "then @xmath139 when @xmath140 , @xmath141 and @xmath142 .",
    "theorems 1 and 2 show that with consistent initial estimates of @xmath40 and @xmath27 , the proposed multivariate conditional regression model is able to achieve both selection consistency and the asymptotic normality .",
    "this section examines the effectiveness of the proposed multivariate conditional regression model on a variety of simulated examples and a real application to the glioblastoma cancer dataset ( tcga , 2008 ) .",
    "the proposed multivariate conditional regression model with the adaptive lasso penalty , denoted as amcr , is compared against the alternative updating algorithm in ( [ eqn : multi_pls ] ) ( alt ; yin and li , 2011 ; lee and liu , 2012 ) , and the separate lasso regression ( sep ; estimating each @xmath47 and @xmath143 separately ) .",
    "the comparison is conducted with respect to the estimation and selection accuracy of @xmath144 and @xmath145 . in specific ,",
    "the estimation accuracy of @xmath144 is measured by the frobenius norm @xmath146 , the matrix 1-norm @xmath147 , and the matrix @xmath148-norm @xmath149 , where @xmath150 .",
    "the estimating accuracy of @xmath145 is not reported as the primary interest is the sparsity inferred by @xmath145 , and the proposed method does not produce @xmath145 directly .",
    "the selection accuracy of @xmath144 and @xmath145 is measured by the symmetric difference @xmath151 where @xmath152 and @xmath153 are the active sets defined by @xmath144 and @xmath145 , and @xmath154 denotes the set cardinality .",
    "we also report the specificity ( spe ) , sensitivity ( sen ) and matthews correlation coefficient ( mcc ) scores , defined as @xmath155 where tp , tn , fp and fn are the numbers of true positives , true negatives , false positives and false negatives in identifying the nonzero elements in @xmath144 or @xmath145 , and  positive \" refers to the nonzero entries .",
    "furthermore , tuning parameters are used in most penalized log - likelihood formulations to balance the model estimation and model complexity .",
    "for example , the tuning parameters @xmath38 and @xmath39 in ( [ eqn : multi_pls ] ) and ( [ eqn : cond_pls ] ) control the tradeoff between the sparsity and the estimation accuracy of the multivariate regression models . in the numerical experiments",
    ", we employed bayesian information criterion ( bic ; schwarz , 1978 ) to select the tuning parameters , which is shown to perform well in tuning penalized likelihood method ( want et al . , 2007 ) .",
    "the bic criterion is minimized through a grid search on a two - dimensional equally - spaced grid @xmath156 ; @xmath157 .",
    "other data adaptive model selection criteria , such as cross validation , can be employed as well ( lee and liu , 2012 ) .",
    "the simulated examples follow the same setup as in li and gui ( 2006 ) , fan et al .",
    "( 2009 ) , peng et al .",
    "( 2009 ) and yin and li ( 2011 ) .",
    "first , each entry of the precision matrix @xmath27 is generated from the product of a bernoulli random variable with success rate proportional to @xmath158 and a uniform random variable on @xmath159\\cup[0.5 , 1]$ ] . for each row ,",
    "all off - diagonal entries are divided by the sum of the absolute value of the off - diagonal entries multiplied by 3/2 .",
    "the final precision matrix @xmath27 is obtained by symmetrizing the generated matrix and setting the diagonal entries as 1 .",
    "next , each entry of the coefficient matrix @xmath40 is generated from the product of a bernoulli random variable with success rate proportional to @xmath160 and a uniform random variable on @xmath161\\cup[v_m , 1]$ ] , where @xmath162 is the minimum absolute value of the nonzero entries in @xmath27 .",
    "finally , with the generated @xmath27 and @xmath40 , each entry of the covariate matrix @xmath163 is generated independently from @xmath164 , and the response vector is generated from @xmath165 ."
  ],
  "abstract_text": [
    "<S> multivariate regression model is a natural generalization of the classical univariate regression model for fitting multiple responses . in this paper </S>",
    "<S> , we propose a high - dimensional multivariate conditional regression model for constructing sparse estimates of the multivariate regression coefficient matrix that accounts for the dependency structure among the multiple responses . </S>",
    "<S> the proposed method decomposes the multivariate regression problem into a series of penalized conditional log - likelihood of each response conditioned on the covariates and other responses . </S>",
    "<S> it allows simultaneous estimation of the sparse regression coefficient matrix and the sparse inverse covariance matrix . </S>",
    "<S> the asymptotic selection consistency and normality are established for the diverging dimension of the covariates and number of responses . </S>",
    "<S> the effectiveness of the proposed method is also demonstrated in a variety of simulated examples as well as an application to the glioblastoma multiforme cancer data .    </S>",
    "<S> .1 in key words : covariance selection , gaussian graphical model , large @xmath0 small @xmath1 , multivariate regression , regularization </S>"
  ]
}