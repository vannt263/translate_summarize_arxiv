{
  "article_text": [
    "support vector machine ( svm ) is a highly developed classification method that is widely used in real - world data analysis  @xcite .",
    "the most popular implementation is called @xmath0-svm , which uses the maximum margin criterion with a penalty for misclassification . the positive parameter @xmath0 tunes the balance between the maximum margin and penalty . as a result",
    ", the classification problem can be formulated as a convex quadratic problem based on training data . a separating hyper - plane for classification",
    "is obtained from the optimal solution of the problem .",
    "furthermore , complex non - linear classifiers are obtained by using the reproducing kernel hilbert space ( rkhs ) as a statistical model of the classifiers  @xcite .",
    "there are many variants of svm for solving binary classification problems , such as @xmath1-svm , e@xmath1-svm , least square svm  @xcite .",
    "moreover , the generalization ability of svm has been analyzed in many studies  @xcite .    in practical situations , however , svm has drawbacks .",
    "the remarkable feature of the svm is that the separating hyperplane is determined mainly from misclassified samples .",
    "thus , the most misclassified samples significantly affect the classifier , meaning that the standard svm is extremely fragile to the presence of outliers . in @xmath0-svm ,",
    "the penalties of sample points are measured in terms of the hinge loss , which is a convex surrogate of the 0 - 1 loss for misclassification .",
    "the convexity of the hinge loss causes svm to be unstable in the presence of outliers , since the convex function is unbounded and puts an extremely large penalty on outliers .",
    "one way to remedy the instability is to replace the convex loss with a non - convex bounded loss to suppress outliers .",
    "loss clipping is a simple method to obtain a bounded loss from a convex loss  @xcite .",
    "for example , clipping the hinge loss leads to the ramp loss  @xcite .    in mathematical statistics ,",
    "robust statistical inference has been studied for a long time .",
    "a number of robust estimators have been proposed for many kinds of statistical problems  @xcite . in mathematical analysis , one needs to quantify the influence of samples on estimators . here",
    ", the influence function , change of variance , and breakdown point are often used as measures of robustness . in",
    "machine learning literature , these measures are used to analyze the theoretical properties of svm and its robust variants . in @xcite , the robustness of a learning algorithm using a convex loss function was investigated on the basis of an influence function defined over an rkhs . when the influence function is uniformly bounded on the rkhs ,",
    "the learning algorithm is regarded to be robust against outliers .",
    "it was proved that the quadratic loss function provides a robust learning algorithm for classification problems in this sense  @xcite . from the standpoint of the breakdown point ,",
    "however , convex loss functions do not provide robust estimators , as shown in  ( * ? ? ?",
    "* chap .  5.16 ) . in @xcite , yu et al . showed a convex loss clipping that yields a non - convex loss function and proposed a convex relaxation of the resulting non - convex optimization problem to obtain a computationally efficient learning algorithm .",
    "they also studied the robustness of the learning algorithm using the clipped loss .    in this paper , we provide a detailed analysis on the robustness of svms .",
    "in particular , we deal with a robust variant of kernel - based @xmath1-svm . the standard @xmath1-svm",
    "@xcite has a regularization parameter @xmath1 , and it is equivalent with @xmath0-svm ; i.e. , both methods provide the same classifier for the same training data , if the regularization parameters , @xmath1 and @xmath0 , are properly tuned .",
    "we also introduce a new robust variant called robust @xmath2-svm that has another learning parameter @xmath3 .",
    "the parameter @xmath4 denotes the ratio of samples to be removed from the training dataset as outliers .",
    "when the ratio of outliers in the training dataset is bounded above by @xmath4 , robust @xmath2-svm is expected to provide a robust classifier .",
    "robust @xmath2-svm is closely related to the robust outlier detection ( rod ) algorithm  @xcite .",
    "indeed , rod is to robust @xmath2-svm what @xmath0-svm is to @xmath1-svm  @xcite .",
    "our main contribution is to derive the _",
    "exact _ finite - sample breakdown point of robust @xmath2-svm .",
    "the finite - sample breakdown point indicates the largest amount of contamination such that the estimator still gives information about the non - contaminated data  ( * ? ? ?",
    "* chap.3.2 ) .",
    "we show that the finite - sample breakdown point of robust @xmath2-svm is equal to @xmath4 , if @xmath1 and @xmath4 satisfy simple inequalities .",
    "conversely , we prove that the finite - sample breakdown point is strictly less than @xmath4 , if these key inequalities are violated .",
    "the theoretical analysis partly depends on the boundedness of the kernel function used in the statistical model . as a result",
    ", one can specify the region of the learning parameters @xmath2 such that robust @xmath2-svm has the desired robustness property .",
    "this property will be of great help to reduce the number of candidate learning parameters @xmath2 , when the grid search of learning parameters is conducted with cross validation .",
    "some of previous studies are related to ours .",
    "in particular , the breakdown point was used to assess the robustness of kernel - based estimators in @xcite . in that paper , the influence of a single outlier is considered for a general class of robust estimators . in contrast , we focus on a variant of svm and provide a detailed analysis of the robustness property based on the breakdown point . in our analysis ,",
    "an arbitrary number of outliers is taken into account .",
    "the paper is organized as follows . in section  [ sec : brief_introduction ] , we introduce the problem setup and briefly review the topic of learning algorithms using the standard @xmath1-svm . section  [ sec : robust_nu_svm ] is devoted to the robust variant of @xmath1-svm .",
    "we show that the dual representation of robust @xmath2-svm has an intuitive interpretation , that is of great help to compute the breakdown point .",
    "an optimization algorithm is also presented . in section  [ sec : breakdown_point ] , we introduce a finite - sample breakdown point as a measure of robustness .",
    "then , we evaluate the breakdown point of robust @xmath2-svm . in section  [ sec : asymptotic_properties ] , we investigate the statistical asymptotic properties of the proposed method on the basis of order statistics .",
    "section  [ sec : numerical_experiments ] examines the generalization performance of robust @xmath2-svm via numerical experiments .",
    "the conclusion is in section  [ sec : concluding_remarks ] .",
    "detailed proofs of the theoretical results are presented in the appendix .",
    "let us summarize the notations used throughout this paper .",
    "let @xmath5 be the set of natural numbers , and let @xmath6 $ ] for @xmath7 denote a finite set of @xmath5 defined as @xmath8 .",
    "the set of all real numbers is denoted as @xmath9 .",
    "the function @xmath10_+$ ] is defined as @xmath11 for @xmath12 . for a finite set @xmath13 ,",
    "the size of @xmath13 is expressed as @xmath14 . for a reproducing kernel hilbert space ( rkhs )",
    "@xmath15 , the norm on @xmath15 is denoted as @xmath16 .",
    "see @xcite for a description of rkhs .",
    "let @xmath17 ( resp .",
    "@xmath18 ) be an @xmath19-dimensional vector of all ones ( resp .",
    "all zeros ) .",
    "let us introduce the classification problem with an input space @xmath20 and binary output labels @xmath21 . given i.i.d .",
    "training samples @xmath22\\}\\subset\\mathcal{x}\\times\\{+1,-1\\}$ ] drawn from a probability distribution over @xmath23 , a learning algorithm produces a decision function @xmath24 such that its sign provides a prediction of output labels for input points over test samples .",
    "the decision function @xmath25 predicts the correct label on the sample @xmath26 if and only if the inequality @xmath27 holds .",
    "the product @xmath28 is called the margin of the sample @xmath26 for the decision function  @xmath29  @xcite . to make an accurate decision function",
    ", the margins on the training dataset should take large positive values .    in kernel - based @xmath1-svm  @xcite , an rkhs @xmath15 endowed with a kernel function @xmath30",
    "is used to estimate the decision function @xmath31 , where @xmath32 and @xmath33 .",
    "the misclassification penalty is measured by the hinge loss .",
    "more precisely , @xmath1-svm produces a decision function @xmath31 as the optimal solution of the convex problem , @xmath34_+\\\\    \\displaystyle      \\st\\ f\\in\\mathcal{h},\\ b,\\rho\\in\\rbb ,    \\end{array}\\end{aligned}\\ ] ] where @xmath35_+$ ] is the hinge loss of the margin with the threshold @xmath36 .",
    "the second term @xmath37 is the penalty for the threshold parameter  @xmath36 .",
    "the parameter @xmath1 in the interval @xmath38 is the regularization parameter .",
    "usually , the range of @xmath1 that yields a meaningful classifier is narrower than the interval @xmath38 , as shown in @xcite .",
    "the first term in is a regularization term to avoid overfitting to the training data .",
    "a large positive margin is preferable for each training data .",
    "the representer theorem  @xcite indicates that the optimal decision function of is of the form , @xmath39 for @xmath40 .",
    "the input point @xmath41 with a non - zero coefficient @xmath42 is called a support vector .",
    "the regularization parameter @xmath1 provides a lower bound on the fraction of support vectors .",
    "thanks to the representer theorem , even when @xmath15 is an infinite dimensional space , the above optimization problem can be reduced to a finite dimensional quadratic convex problem .",
    "this is the great advantage of using rkhs for non - parametric statistical inference  @xcite .",
    "as pointed out in @xcite , @xmath1-svm is closely related to a financial risk measure called conditional value at risk ( cvar )  @xcite .",
    "roughly speaking , the cvar of samples @xmath43 at level @xmath44 such that @xmath45 is defined as the average of its @xmath1-tail , i.e. , @xmath46 , where @xmath47 is a permutation on @xmath6 $ ] such that @xmath48 holds . in the literature , @xmath49 is defined as the negative margin @xmath50 . for a regularization parameter",
    "@xmath1 satisfying @xmath45 and a fixed decision function @xmath31 , the objective function in is expressed as @xmath51_+ \\\\ &   =   \\frac{1}{2}\\|f\\|_{\\mathcal{h}}^2+\\nu\\cdot\\frac{1}{\\nu{m}}\\sum_{i=1}^{\\nu{m}}r_{\\sigma(i)}. \\end{aligned}\\ ] ] details are presented in theorem  10 of  @xcite . hence , @xmath1-svm yields a decision function that minimizes the sum of the regularization term and the cvar of the negative margins at level @xmath1 .    in @xmath0-svm , the decision function",
    "is obtained by solving @xmath52_+\\\\    \\displaystyle \\st\\ f\\in\\mathcal{h},\\ b\\in\\rbb .",
    "\\end{array}\\end{aligned}\\ ] ] note that the threshold in the hinge loss is fixed to one in @xmath0-svm , whereas @xmath1-svm determines the threshold with the optimal solution @xmath36 .",
    "a positive regularization parameter @xmath53 is used instead of @xmath1 . for each",
    "training data , @xmath1-svm and @xmath0-svm can be made to provide the same decision function by appropriately tuning @xmath1 and @xmath0 . in this paper , we focus on @xmath1-svm and its robust variants rather than @xmath0-svm .",
    "the parameter @xmath1 has the explicit meaning shown above , and this interpretation will be significant when we derive the robustness property of our method .    in the robust @xmath0-svm proposed in  @xcite , the hinge loss @xmath54_{+}$ ] in is replaced with the so - called ramp loss @xmath55_{+}\\}$ ] . by truncating the hinge loss , the influence of outliers is suppressed , and the estimated classifier is expected to be robust against outliers included in the training data .",
    "here , we propose a robust @xmath2-svm that is a robust variant of @xmath1-svm . to remove the influence of outliers , we introduce the outlier indicator , @xmath56 $ ] , for each training sample , where @xmath57 is intended to indicate that the sample @xmath58 is an outlier . the same idea is used in  @xcite .",
    "assume that the ratio of outliers is less than or equal to @xmath4 , and define the finite set @xmath59 as @xmath60 for @xmath1 and @xmath4 such that @xmath61 , robust @xmath2-svm is formalized using rkhs @xmath15 as @xmath62_+ ,   \\\\",
    "\\displaystyle\\ \\st\\ \\      f\\in\\mathcal{h},\\ \\      { \\eta}=(\\eta_1,\\ldots,\\eta_m)^t\\in{}e_\\mu,\\ \\ b,\\rho\\in\\rbb .",
    "\\end{array}\\end{aligned}\\ ] ] the optimal solution , @xmath32 and @xmath33 , provides the decision function @xmath31 for classification . influence from samples with large negative margins",
    "is removed by setting @xmath63 to zero . throughout the paper",
    ", we will assume that @xmath64 and @xmath65 are natural numbers to avoid technical difficulties .",
    "robust @xmath2-svm is closely related to the robust outlier detection ( rod ) algorithm  @xcite . about modified algorithms of rod and robust @xmath2-svm ,",
    "the equivalence is shown in  @xcite . in rod ,",
    "the classifier is given by the optimal solution of @xmath66_{+ } , \\\\",
    "\\displaystyle \\st\\   f\\in\\mathcal{h},\\ \\",
    "b\\in\\rbb,\\\\    \\phantom{\\st}\\ { \\eta}=(\\eta_1,\\ldots,\\eta_m)^t\\in{}[0,1]^m,\\,\\     \\sum_{i=1}^{m}\\eta_i\\geq{}m(1-\\mu ) ,    \\end{array}\\end{aligned}\\ ] ] where @xmath67 is a regularization parameter . in the original rod ,",
    "the linear kernel is used . to obtain the classifier",
    ", the rod algorithm solves a semidefinite relaxation of the above problem .",
    "furthermore , robust @xmath2-svm is related to cvar at levels @xmath1 and @xmath4 .",
    "indeed , for the parameters , @xmath1 and @xmath4 , and a fixed decision function @xmath31 , the objective function in is represented as @xmath68_+ \\nonumber\\\\ & =   \\min_{\\rho\\in\\rbb}\\    \\frac{1}{2}\\|f\\|_{\\mathcal{h}}^2 -\\nu\\rho   + \\frac{1}{m}\\sum_{i=1}^{m}\\big[\\rho+r_i\\big]_+    -\\max_{{\\eta}\\in{e_\\mu}}\\frac{1}{m}\\sum_{i=1}^m(1-\\eta_i)r_i   \\label{eqn : dc - representation } \\\\ & =   \\frac{1}{2}\\|f\\|_{\\mathcal{h}}^2    + ( \\nu-\\mu)\\cdot\\frac{1}{(\\nu-\\mu)m}\\sum_{i=\\mu{m}+1}^{\\nu{m}}\\!\\!r_{\\sigma(i ) } ,   \\label{eqn : diff_cvar}\\end{aligned}\\ ] ] where @xmath69 is the negative margin and @xmath70 is its sort in the descending order defined in section  [ sec : brief_introduction ] .",
    "the second term in is the average of the negative margins included in the middle interval presented in figure  [ fig : margin_hist ] , and it is expressed by the difference of cvars at levels @xmath1 and @xmath4 .",
    "the learning algorithm based on this interpretation is proposed in  @xcite under the name cvar-@xmath71-svm .",
    "the two methods can be shown to be equivalent by setting @xmath72 and @xmath73 . in this paper ,",
    "the learning algorithm based on is referred to as robust @xmath2-svm to emphasize that it is a robust variant of @xmath1-svm .",
    "$ ] for a fixed decision function @xmath31 . ]",
    "the representer theorem ensures that the optimal decision function of is represented by @xmath74 when the kernel function of the rkhs @xmath15 is given by @xmath75 . as in the case of the standard @xmath1-svm , the number of support vectors ,",
    "i.e. , the input points @xmath76 such that @xmath77 , is bounded below by @xmath78 . in addition , the kkt condition of leads to the fact that any support vector @xmath76 satisfies @xmath79 .",
    "it is hard to obtain a global optimal solution of , since the objective function is non - convex . as shown in   @xcite",
    ", the objective function in is expressed as a difference of convex functions  ( dc ) by using a cvar representation .",
    "hence , the dc algorithm  @xcite and convex - concave programming ( cccp )  @xcite are available to efficiently obtain a stationary point of .",
    "the same approach is taken by robust @xmath0-svm using the ramp loss  @xcite .    in algorithm",
    "[ alg : dc_alg_robust_numusvm ] , the dc algorithm for robust @xmath2-svm based on the expression is presented .",
    "the derivation of the dc algorithm is presented in appendix  [ appendix : dca ] .",
    "algorithm  [ alg : dc_alg_robust_numusvm ] is guaranteed to converge in a finite number of iterations . in the dc algorithm , a monotone decrease of the objective value",
    "is generally guaranteed , and in algorithm  [ alg : dc_alg_robust_numusvm ] , the objective value in each iteration is determined by @xmath80 , which can take only a finite number of distinct values .",
    "a stationary point is obtained when the objective value is unchanged .",
    "the above argument is based on a convergence analysis of robust @xmath0-svm using the ramp loss  @xcite .",
    "in addition , an argument based on polyhedral dc programming shows that the algorithm converges after a finite number of iterations  @xcite .",
    "one can use another stopping rule such that the algorithm terminates when the same @xmath80 is obtained in two consecutive iterations . if the cyclic phenomenon of @xmath81 is prohibited in some way , convergence in a finite number of iterations is guaranteed .",
    "gram matrix @xmath82 defined as @xmath83 $ ] , and training labels @xmath84 .",
    "the matrix @xmath85 is defined as @xmath86 .",
    "let @xmath31 be an initial decision function .",
    "compute the sort @xmath87 of the negative margin @xmath50 , and set @xmath88 for @xmath89 $ ] .",
    "let @xmath81 be @xmath90 .",
    "set @xmath91 and @xmath92 .",
    "compute the optimal solution @xmath93 of the problem @xmath94 set @xmath95 , where @xmath96 denotes component - wise multiplication of two vectors .",
    "compute @xmath36 and @xmath97 using @xmath98 , where @xmath99 . the decision function @xmath100 .      the partial dual problem of with a fixed outlier indicator @xmath101 has an intuitive geometric picture .",
    "some variants of @xmath1-svm can be geometrically interpreted on the basis of the dual form  @xcite . substituting into the objective function in",
    ", we obtain the lagrangian of problem   with a fixed @xmath102 as @xmath103 where non - negative slack variables @xmath104 $ ] are introduced to represent the hinge loss . here , the parameters @xmath105 and @xmath106 for @xmath89 $ ] are non - negative lagrange multipliers . for a fixed @xmath80 ,",
    "the lagrangian is convex in the parameters @xmath107 , and @xmath108 and concave in @xmath109 and @xmath110 .",
    "hence , the min - max theorem  ( * ? ? ?",
    "* proposition  6.4.3 ) yields @xmath111    let us give a geometric interpretation of the above expression .",
    "for the training data @xmath22\\}$ ] , the convex sets , @xmath112 $ ] and @xmath113 $ ] , are defined as the reduced convex hulls of data points for each label , i.e. , @xmath114\\\\   & =   \\bigg\\ {    \\sum_{i : y_i=\\pm1}\\gamma_i'k(\\cdot , x_i)\\in\\mathcal{h}:\\!\\!\\ !   \\sum_{i : y_i=\\pm1}\\gamma_i'=1,\\   0\\leq \\gamma_i'\\leq \\frac{2\\eta_i}{(\\nu-\\mu)m } \\",
    "\\text{for $ i$ such that $ y_i=\\pm1 $ }    \\bigg\\}. \\end{aligned}\\ ] ] the coefficients @xmath115 $ ] in @xmath116 $ ] are bounded above by a non - negative real number that is usually less than one .",
    "hence , the reduced convex hull is a subset of the convex hull of the data points in the rkhs @xmath15 .",
    "each reduced convex hull is regarded as the domain of the input samples of each label . accordingly , let @xmath117 $ ] be the minkowski difference of two subsets , @xmath118= \\mathcal{u}_{\\eta}^{+}[\\nu,\\mu;d ] \\ominus \\mathcal{u}_{\\eta}^{-}[\\nu,\\mu;d ] , \\end{aligned}\\ ] ] where @xmath119 of subsets @xmath13 and @xmath120 denotes @xmath121 .",
    "eventually , for each @xmath80 , the optimal value in the above is represented by @xmath122\\right\\}. \\end{aligned}\\ ] ] hence , the optimal value of is @xmath123 , where @xmath124}\\|f\\|_{\\mathcal{h}}^2 . \\end{aligned}\\ ] ]    therefore , the dual form of robust @xmath2-svm is expressed as the maximization of the minimum distance between two reduced convex hulls , @xmath112 $ ] and @xmath113 $ ] .",
    "the estimated decision function in robust @xmath2-svm is provided by the optimal solution of up to a scaling factor depending on @xmath125 . moreover ,",
    "the optimal value is proportional to the squared rkhs norm of the function @xmath126 in the decision function @xmath31 .",
    "let us describe how to evaluate the robustness of learning algorithms .",
    "there are a number of robustness measures for evaluating the stability of estimators . for example",
    ", the influence function evaluates the infinitesimal bias of the estimator caused by a few outliers included in the training samples .",
    "the gross error sensitivity is the worst - case infinitesimal bias defined with the influence function  @xcite . in this paper , we use the _ finite - sample breakdown point _ , and it will be referred to as the breakdown point for short . the breakdown point quantifies the degree of impact that the outliers have on the estimators when the contamination ratio is not necessarily infinitesimal  @xcite . in this section , we present an exact evaluation of the breakdown point of robust @xmath2-svm .    the breakdown point indicates the largest amount of contamination such that the estimator still gives information about the non - contaminated data  ( * ? ? ?",
    "* chap.3.2 ) .",
    "more precisely , for an estimator @xmath127 based on a dataset @xmath128 of size @xmath19 that takes a value in a normed space , the finite - sample breakdown point is defined as @xmath129 where @xmath130 is the family of datasets of size @xmath19 including at least @xmath131 elements in common with the non - contaminated dataset @xmath128 , i.e. , @xmath132\\}\\subset\\mathcal{x}\\times\\{+1,-1\\}\\,:\\,|d'\\cap{d}|\\geq{m-\\kappa}\\,\\big\\}. \\end{aligned}\\ ] ] for simplicity , the dependency of @xmath133 on the data set @xmath128 is dropped .",
    "the condition of the breakdown point @xmath134 can be rephrased as @xmath135 where @xmath136 is the norm on the normed space . in most cases of interest , @xmath134 does not depend on the dataset @xmath128 .",
    "for example , the breakdown point of the one - dimensional median estimator is @xmath137 .    to start with ,",
    "let us derive a lower bound of the breakdown point for the optimal value of problem   that is expressed as @xmath138 up to a constant factor . as shown in section  [ subsec : dual_problem ] , the boundedness of @xmath138 is equivalent to the boundedness of the rkhs norm of @xmath32 in the estimated decision function @xmath31 . given a labeled dataset @xmath22\\}$ ] , let us define the label ratio @xmath139 as @xmath140    [ theorem : breakdown - point - optvalue ] let @xmath128 be a labeled dataset of size @xmath19 with a positive label ratio @xmath139 .",
    "for the parameters @xmath141 such that @xmath142 and @xmath143 , we assume @xmath144 . then , the following two conditions are equivalent .",
    "( i ) : :    the inequality @xmath145 holds .",
    "( ii ) : :    uniform boundedness , @xmath146 holds , where    @xmath147 is the family of contaminated    datasets defined from  @xmath128 .    the proof is given in appendix  [ appendix : proof_breakdown_obj ] .",
    "the inequality @xmath144 is a requisite condition .",
    "if this inequality is violated , the majority of , say , positive labeled samples in the non - contaminated training dataset can be replaced with outliers .",
    "in such a situation , the statistical features in the original dataset will not be retained .",
    "indeed , if @xmath148 holds , @xmath149 is unbounded over @xmath150 regardless of @xmath1 .",
    "since it is proved by a rigorous description of the above intuitive interpretation , the proof is omitted .",
    "theorem  [ theorem : breakdown - point - optvalue ] indicates that the breakdown point of the rkhs element in the estimated decision function is greater than or equal to @xmath4 , if @xmath4 and @xmath1 satisfy inequality  .",
    "conversely , if the inequality @xmath151 is violated , the breakdown point of robust @xmath2-svm does not reach @xmath4 , even though @xmath65 samples are removed from the training data .",
    "in addition , the inequality indicates the trade - off between the ratio of outliers @xmath4 and the ratio of support vectors @xmath125 .",
    "this result is reasonable .",
    "the number of support vectors corresponds to the dimension of the statistical model .",
    "when the ratio of outliers is large , a simple statistical model should be used to obtain robust estimators .",
    "if there is no outlier in training data , i.e. , @xmath152 , inequality reduces to @xmath153 . for the standard @xmath1-svm ,",
    "this is a necessary and sufficient condition for the optimization problem to be bounded  @xcite .    when the contamination ratio in a training dataset is greater than @xmath4 , the estimated decision function is not necessarily bounded .",
    "[ theorem : breakdown - point_upper_bound ] suppose that @xmath1 and @xmath4 are rational numbers such that @xmath154 and @xmath155 .",
    "then , there exists a dataset @xmath128 of size @xmath19 with the label ratio @xmath139 such that @xmath144 and @xmath156 hold , where @xmath157 is defined from @xmath128 .    the proof is given in appendix  [ appendix : upperbound_breakdown_opt ] .",
    "theorems  [ theorem : breakdown - point - optvalue ] and [ theorem : breakdown - point_upper_bound ] lead to the fact that the breakdown point of the function part @xmath32 in the estimated decision function @xmath158 is exactly equal to @xmath159 , when the learning parameters of the robust @xmath2-svm satisfy @xmath144 and @xmath151 .",
    "otherwise , the breakdown point of @xmath160 is strictly less than @xmath4 .",
    "we show the robustness of the bias term @xmath97 .",
    "let @xmath161 be the estimated bias parameter obtained by robust @xmath2-svm from the training dataset  @xmath128 .",
    "we will derive a lower bound of the breakdown point of the bias term .",
    "then , we will show that the breakdown point of robust @xmath2-svm with a bounded kernel is given by a simple formula .",
    "[ theorem : bounded_bias ] let @xmath128 be an arbitrary dataset of size @xmath19 with a positive label ratio @xmath139 .",
    "suppose that @xmath1 and @xmath4 satisfy @xmath61 , @xmath143 , and @xmath144 .",
    "for a non - negative integer @xmath162 , we assume @xmath163 then , uniform boundedness @xmath164 holds , where @xmath165 is defined from @xmath128 .    the proof is given in appendix  [ appendix : proof_breakdown_bias ] .",
    "note that the inequality   is a sufficient condition of inequality  .",
    "theorem  [ theorem : bounded_bias ] guarantees that the breakdown point of the estimated decision function @xmath166 is not less than @xmath167 when holds .",
    "when the kernel function is bounded , the boundedness of the function part @xmath32 in the decision function @xmath166 almost guarantees the boundedness of the bias term @xmath97 .",
    "[ theorem : bounded_kernel_bounded_bias ] let @xmath128 be an arbitrary dataset of size @xmath19 with a positive label ratio @xmath139 . for the parameters @xmath141 such that @xmath61 and @xmath143 , suppose that @xmath144 and @xmath168 hold .",
    "in addition , assume that the kernel function @xmath75 of the rkhs @xmath15 is bounded , i.e. , @xmath169",
    ". then , uniform boundedness , @xmath170 holds , where @xmath147 is defined from @xmath128 .",
    "the proof is given in appendix  [ appendix : proof_bounded_kernel_breakdown_bias ] . compared with theorem  [ theorem : bounded_bias ] in which arbitrary kernel functions are treated , theorem  [ theorem : bounded_kernel_bounded_bias ] ensures that a tighter lower bound of the breakdown point is obtained for bounded kernels .",
    "the above result agrees with those of other studies .",
    "the authors of @xcite proved that bounded kernels produce robust estimators for regression problems in the sense of bounded response , i.e. , robustness against a single outlier .",
    "[ cols=\"^,^ \" , ]     figure  [ fig : estimator_and_testerror ] shows the results of the numerical experiments .",
    "the maximum norm of the estimated decision function is plotted for the parameter @xmath171 on the same axis as fig .",
    "[ fig : breakdown - ponit ] . the top ( bottom ) panels show the results for a gaussian ( linear ) kernel .",
    "the left and middle columns show the maximum norm of @xmath160 and @xmath97 , respectively .",
    "the maximum test errors are presented in the right column . in all panels ,",
    "the red points denote the top 50 percent of values , and the asterisk ( @xmath172 ) is the point that violates the inequality @xmath151 . in this example",
    ", the numerical results agree with the theoretical analysis in section  [ sec : breakdown_point ] ; i.e. , the norm becomes large when the inequality @xmath173 is violated .",
    "accordingly , the test error gets close to @xmath174no information for classification .",
    "even when the unbounded linear kernel is used , robustness is confirmed for the parameters in the left lower region in the right panel of fig .",
    "[ fig : breakdown - ponit ] .    in the bottom right panel ,",
    "the test error gets large when the inequality @xmath151 holds .",
    "this result comes from the problem setup .",
    "even with non - contaminated data , the test error of the standard @xmath1-svm is approximately @xmath174 , because the linear kernel works poorly for spiral data .",
    "thus , the worst - case test error can go beyond @xmath174 .",
    "for the parameter at which is violated , the test error is always close to @xmath174 .",
    "thus , a learning method with such parameters does not provide any useful information for classification .",
    "ccc + ) is the point that violates the inequality @xmath173 .",
    ", title=\"fig : \" ] & ) is the point that violates the inequality @xmath173 .",
    ", title=\"fig : \" ] & ) is the point that violates the inequality @xmath173 .",
    ", title=\"fig : \" ] + plot of @xmath175 & plot of @xmath176 & plot of test error +   +   + ) is the point that violates the inequality @xmath173 .",
    ", title=\"fig : \" ] & ) is the point that violates the inequality @xmath173 .",
    ", title=\"fig : \" ] & ) is the point that violates the inequality @xmath173 .",
    ", title=\"fig : \" ] + plot of @xmath175 & plot of @xmath176 & plot of test error +      we compared the generalization ability of the robust @xmath2-svm with existing classifiers such as standard @xmath1-svm and robust @xmath0-svm using the ramp loss .",
    "the datasets are presented in table  [ table : sim_result ] .",
    "all the datasets are provided in the mlbench and kernlab libraries of the r language  @xcite .",
    "in all the datasets , the number of positive samples is less than or equal to that of negative samples . before running the learning algorithms , we standardized each input variable with mean zero and standard deviation one .",
    "we randomly split the dataset into training and test sets .",
    "to evaluate the robustness , the training data was contaminated by outliers .",
    "more precisely , we randomly chose positive labeled samples in the training data and changed their labels to negative ; i.e. , we added outliers by flipping the labels .",
    "after that , robust @xmath2-svm , robust @xmath0-svm using the ramp loss , and the standard @xmath1-svm were used to obtain classifiers from the contaminated training dataset .",
    "the prediction accuracy of each classifier was then evaluated over test data that had no outliers .",
    "linear and gaussian kernels were employed for each learning algorithm .",
    "the learning parameters , such as @xmath177 , and @xmath0 , were determined by conducting a grid search based on five - fold cross validation over the training data . for robust @xmath2-svm",
    ", the parameter @xmath178 was selected from the region @xmath179 in . for standard @xmath1-svm , the candidate of the regularization parameter @xmath1",
    "was selected from the interval @xmath180 , where @xmath181 is the label ratio of the contaminated training data . for robust @xmath0-svm",
    ", the regularization parameter @xmath0 was selected from the interval @xmath182 $ ] . in the grid search of the parameters , 24 or 25",
    "candidates were examined for each learning method .",
    "thus , we needed to solve convex or non - convex optimization problems more than @xmath183 times in order to obtain a classifier .",
    "the above process was repeated 30 times , and the average test error was calculated .",
    "the results are presented in table  [ table : sim_result ] . for",
    "non - contaminated training data , robust @xmath2-svm and robust @xmath0-svm were comparable to the standard @xmath1-svm .",
    "when the outlier ratio is high , we can conclude that robust @xmath2-svm and robust @xmath0-svm tend to work better than the standard @xmath1-svm . in this experiment",
    ", the kernel function does not affect the relative prediction performance of these learning methods .",
    "in large datasets such as spam and satellite , robust @xmath2-svm tends to outperform robust @xmath0-svm .",
    "when learning parameters , such as @xmath141 , and @xmath0 , are appropriately chosen by using a large dataset , learning algorithms with plural learning parameters clearly work better than those with a single learning parameter .",
    "in addition , in robust @xmath0-svm , there is a difficulty in choosing the regularization parameter .",
    "indeed , the parameter @xmath0 does not have a clear meaning , and thus , it is not straightforward to determine the candidates of @xmath0 in the grid search optimization .",
    "in contrast , the parameter @xmath1 in @xmath1-svm and its robust variant has a clear meaning , i.e. , a lower bound of the ratio of support vectors and an upper bound of the margin error on the training data  @xcite . such clear meaning is of great help to choose candidate points of regularization parameters .",
    "we conducted another experiment in which the learning parameters @xmath141 and @xmath0 were determined using only one validation set , i.e. , non - cross validation ( the details are not presented here ) .",
    "the dataset was split into training , validation and test sets .",
    "the learning parameters , @xmath184 , and @xmath0 , that minimized the prediction error on the validation set were selected .",
    "this method greatly reduced the computational cost of the cross validation .",
    "however , robust @xmath2-svm did not necessarily produce a better classifier compared with the other methods . since robust @xmath2-svm has two learning parameters , we need to carefully select them using cross validations rather than simple validations in order to achieve high prediction accuracy .",
    "rccccccc + & & & + outlier & & & @xmath1-svm & & & & @xmath1-svm + 0%&.258(.032)&.270(.038 ) & * .256(.051 ) & & * .179(.038 ) & .188(.043 ) & .181(.039 ) + 5% & * .256(.039)&.273(.047)&.258(.046)&&.225(.042 ) & .229(.051 ) & * .224(.061 ) + 10% & * .297(.060)&.306(.067)&.314(.060)&&.249(.059 ) & * * .230(.046 ) & .259(.062 ) + 15% & * .329(.061)&.339(.064)&.345(.062)&&.280(.053 ) & * .280(.050 ) & .294(.064 ) +   +   + & & & + outlier & & & @xmath1-svm & & & & @xmath1-svm + 0%&.033(.010 ) & .035(.008 ) & * .033(.006)&&*.032(.008)&.035(.012 ) & .033(.010 ) + 5%&.034(.009 ) & * .034(.010 ) & .043(.015)&&*.032(.005)&.033(.007 ) & .033(.006 ) + 10%&.055(.015 ) & * .051(.026 ) & .076(.036 ) & & * * .035(.008)&.043(.025 ) & .038(.008 ) + 15%&.136(.058 ) & * .120(.050 ) & .148(.058)&&.160(.083 ) & * .145(.070 ) & .150(.110 ) +   +   + & & & + outlier & & & @xmath1-svm & & & & @xmath1-svm + 0%&.237(.018 ) & * .232(.014 ) & .246(.018)&&*.238(.021 ) & .240(.019 ) & .243(.022 ) + 5%&.239(.019 ) & * .237(.016 ) & .269(.036)&&*.264(.025 ) & .267(.024 ) & .273(.024 ) + 10%&**.280(.046 ) & .299(.042 ) & .330(.030)&&.302(.039 ) & * .293(.036 ) & .315(.038 ) + 15%&**.338(.042 ) & .349(.030 ) & .351(.026)&&*.344(.028 ) & .344(.031 ) & .353(.016 ) +   +   + & & & + outlier & & & @xmath1-svm & & & & @xmath1-svm + 0%&.083(.005 ) & .088(.006 ) & * .083(.005 ) & & .081(.005 ) & .086(.006 ) & * .081(.006 ) + 5% & * * .094(.008 ) & .104(.013 ) & .109(.010 ) & & .095(.008 ) & .097(.009 ) & * .095(.008 ) + 10% & * * .129(.022 ) & .152(.020 ) & .166(.067 ) & & * .129(.015 ) & .133(.017 ) & .141(.030 ) + 15% & * * .201(.029 ) & .240(.030 ) & .256(.091 ) & & * * .206(.018 ) & .223(.030 ) & .240(.055 ) +   +   + & & & + outlier & & & @xmath1-svm & & & & @xmath1-svm + 0%&.097(.004 ) & .096(.003 ) & * * .094(.003 ) & & .069(.031 ) & .067(.004 ) & * * .063(.004 ) + 5%&.101(.003 ) & * .100(.005 ) & .100(.004 ) & & * .072(.015 ) & .078(.007 ) & .078(.043 ) + 10% & * * .148(.020 ) & .161(.026 ) & .161(.019 ) & & * .117(.034 ) & .126(.040 ) & .137(.027 ) +",
    "we presented robust @xmath2-svm and studied its statistical properties .",
    "the robustness property was analyzed by computing the exact breakdown point . as a result",
    ", we obtained inequalities for the learning parameters @xmath1 and @xmath4 that guarantee the robustness of the learning algorithm .",
    "the statistical theory of the l - estimator was then used to investigate the asymptotic behavior of the classifier .",
    "numerical experiments showed that the inequalities are critical to obtaining a robust classifier .",
    "the prediction accuracy of the proposed method was numerically compared with those of other methods , and it was found that the proposed method with carefully chosen learning parameters delivers more robust classifiers than those of other methods such as standard @xmath1-svm and robust @xmath0-svm using the ramp loss . in the future , we will explore the robustness properties of more general learning methods .",
    "another important issue is to develop efficient optimization algorithms .",
    "although the dc algorithm  @xcite and convex relaxation  @xcite are promising methods , more scalable algorithms will be required to deal with massive datasets that are often contaminated by outliers .",
    "according to , the objective function of the robust @xmath2-svm is expressed as @xmath185 using the convex functions @xmath186 and @xmath187 defined as @xmath188,\\\\   \\psi_1({\\alpha},b)&=\\max_{{\\eta}\\in{e_\\mu}}\\frac{1}{m}\\sum_{i=1}^{m}(1-\\eta_i)r_i , \\end{aligned}\\ ] ] where @xmath49 is the negative margin @xmath189 and @xmath82 is the gram matrix defined by @xmath190 $ ] .",
    "let @xmath191 be the solution obtained after @xmath192 iterations of the dc algorithm .",
    "then , the solution is updated to the optimal solution of @xmath193 where @xmath194 with @xmath195 is an element of the subgradient of @xmath187 at @xmath196 .",
    "the subgradient of @xmath187 is given as @xmath197 where @xmath198 denotes the convex hull of the set @xmath199 .",
    "as shown in algorithm  [ alg : dc_alg_robust_numusvm ] , a parameter @xmath81 that meets the condition in the above subgradient is obtained from the sort of the negative margin of the decision function defined from @xmath196 .",
    "the dual problem of is presented in , up to a constant term that is independent of the optimization parameter .",
    "the proof is decomposed into two lemmas .",
    "lemma  [ lemma : breakdown - point - er - svm ] shows that condition ( i ) is sufficient for condition ( ii ) , and lemma  [ lemma : unbounded - breakdown - point - er - svm-1 ] shows that condition ( ii ) does not hold if inequality is violated . for the dataset @xmath22\\}$ ] , let @xmath200 and @xmath201 be the index sets defined as @xmath202 .",
    "when the parameter @xmath4 is equal to zero , the theorem holds according to the argument on the standard @xmath1-svm  @xcite .",
    "below , we assume @xmath203 .      [",
    "proof of lemma [ lemma : breakdown - point - er - svm ] ] we show that @xmath204 $ ] is not empty for any @xmath150 . for a parameter @xmath4",
    "such that @xmath144 , let @xmath205 be a positive constant satisfying @xmath206 .",
    "then , is expressed as @xmath207 . for a contaminated dataset @xmath208\\}\\in\\mathcal{d}_{\\mu{m}}$ ] , let us define @xmath209 as an index set such that @xmath210 for @xmath211 is replaced with @xmath212 as an outlier . in the same way , @xmath213 is defined for negative samples in @xmath128 .",
    "therefore , for any index @xmath214 in @xmath215 or @xmath216 , we have @xmath217",
    ". the assumptions of the theorem ensure @xmath218 . from @xmath219 , we obtain @xmath220 given @xmath221 , the sets @xmath222 $ ] and @xmath223 $ ] are defined by @xmath224\\\\    & =    \\bigg\\ {   \\sum_{i : y_i'=\\pm1}\\gamma_i'k(\\cdot , x_i')\\in\\mathcal{h}\\,:\\,\\sum_{i : y_i'=\\pm1}\\!\\gamma_i'=1,\\     0\\leq \\gamma_i'\\leq \\frac{2\\eta_i}{(\\nu-\\mu)m } , \\ \\",
    "\\text{$\\gamma_i'=0 $ for $ i\\not\\in{}i_{\\pm}\\setminus\\widetilde{i}_{\\pm}$ }    \\bigg\\}.    \\end{aligned}\\ ] ] note that @xmath225\\subset\\mathcal{u}_{\\eta}^{\\pm}[\\nu,\\mu;d']$ ] holds because of the additional constraint , @xmath226 for @xmath227 .",
    "in addition , we have @xmath225 \\subset \\mathrm{conv}\\{k(\\cdot , x_i)\\,:\\,i\\in{}i_{\\pm}\\}$ ] , since only the element @xmath228 with @xmath229 , i.e. @xmath230 , can have a non - zero coefficient @xmath231 in @xmath225 $ ] .",
    "we prove that @xmath222 $ ] and @xmath223 $ ] are not empty .",
    "the size of the index sets @xmath232 is bounded below by @xmath233 the inequality @xmath151 is equivalent to @xmath234 .",
    "hence , we have @xmath235 implying that the sets @xmath225 $ ] are not empty .",
    "indeed , the coefficients defined by @xmath236 for @xmath237 and otherwise @xmath238 admit all the constraints in @xmath225 $ ] .",
    "since @xmath239\\subset\\mathcal{u}_{\\eta}^{\\pm}[\\nu,\\mu;d']$ ] holds , we have @xmath240\\subset\\mathcal{v}_{\\eta}[\\nu,\\mu;d']$ ] , where @xmath241   =   \\widetilde{\\mathcal{u}}_{\\eta}^{+}[\\nu,\\mu;d']\\ominus\\widetilde{\\mathcal{u}}_{\\eta}^{-}[\\nu,\\mu;d']$ ] .",
    "now , let us prove the inequality @xmath242 }    \\|f\\|_\\mathcal{h}^2 < \\infty .",
    "\\end{aligned}\\ ] ] the above argument leads to @xmath243 }",
    "\\|f\\|_{\\mathcal{h}}^2    \\leq     \\min_{f\\in\\widetilde{\\mathcal{v}}_{\\eta}[\\nu,\\mu;d']}\\|f\\|_{\\mathcal{h}}^2<\\infty   \\end{aligned}\\ ] ] for any @xmath244 .",
    "let us define @xmath245=\\mathrm{conv}\\{k(\\cdot , x_i)\\,:\\,i\\in{i}_{+}\\}\\ominus\\mathrm{conv}\\{k(\\cdot , x_i)\\,:\\,i\\in{i}_{-}\\}$ ] for the original dataset  @xmath128 .",
    "then , the inclusion relation @xmath225\\subset\\mathrm{conv}\\{k(\\cdot , x_i)\\,:\\,i\\in{}i_{\\pm}\\}$ ] leads to @xmath241\\subset\\mathcal{c}[d]$ ] .",
    "hence , we obtain @xmath246 } \\|f\\|_{\\mathcal{h}}^2 \\\\    & \\leq    \\max_{\\eta\\in{e}_\\mu}\\min_{f\\in\\widetilde{\\mathcal{v}}_{\\eta}[\\nu,\\mu;d ' ] }   \\|f\\|_{\\mathcal{h}}^2 \\\\    & \\leq\\     \\max_{\\eta\\in{e}_\\mu}\\max_{f\\in\\widetilde{\\mathcal{v}}_{\\eta}[\\nu,\\mu;d ' ] } \\|f\\|_{\\mathcal{h}}^2 \\\\    & \\leq\\     \\max_{f\\in\\mathcal{c}[d]}\\|f\\|_{\\mathcal{h}}^2 \\\\    & < \\infty .    \\end{aligned}\\ ] ] the boundedness of @xmath247}\\|f\\|_{\\mathcal{h}}^2 $ ] comes from the compactness of @xmath245 $ ] and the continuity of the norm .",
    "more precisely , it is bounded above by twice the maximum eigenvalue of the gram matrix defined from the non - contaminated data @xmath128 .",
    "the upper bound does not depend on the contaminated dataset @xmath150 .",
    "thus , the first inequality of holds .      [ proof of lemma  [ lemma : unbounded - breakdown - point - er - svm-1 ] ] we use the same notation as in the proof of lemma  [ lemma : breakdown - point - er - svm ] . without loss of generality , we assume @xmath250 . the parameter @xmath4 is expressed as @xmath206 for @xmath205 .",
    "we prove that there exists a feasible parameter @xmath244 and a contaminated training set @xmath251\\}\\in\\mathcal{d}_{\\mu{m}}$ ] such that @xmath252=\\emptyset$ ] .",
    "the construction of the dataset @xmath253 is illustrated in figure  [ fig : plot - lemma_opt_infinity ] .",
    "suppose that @xmath254 and @xmath255 and that @xmath256 holds for all @xmath257 , meaning that all outliers in @xmath253 are made by flipping the labels of the negative samples in @xmath128 .",
    "this is possible , because @xmath258 holds .",
    "the inequality @xmath248 leads to @xmath259 the outlier indicator @xmath260 is defined by @xmath261 for @xmath65 samples in @xmath216 , and @xmath262 otherwise .",
    "this assignment is possible because @xmath263 then , we have @xmath264 from and , we have @xmath265 in addition , @xmath266 holds only when @xmath267 .",
    "therefore , we have @xmath268=\\emptyset$ ] .",
    "the infeasibility of the dual problem means the unboundedness of the primal problem .",
    "hence , there exists a contaminated dataset @xmath150 and an outlier indicator @xmath269 such that @xmath270}\\|f\\|_{\\mathcal{h}}^2=\\infty   \\end{aligned}\\ ] ] holds .        for a rational number @xmath272",
    ", there exists an @xmath7 such that @xmath273 and @xmath274 hold . for such @xmath19 ,",
    "let @xmath22\\}$ ] be a training data such that @xmath275 and @xmath276 , where the index sets @xmath277 are defined in the proof of appendix  [ appendix : proof_breakdown_obj ] . since the label ratio of @xmath128 is @xmath278 , and we have @xmath144 . for @xmath157 defined from @xmath128 , let @xmath251\\}\\in\\mathcal{d}_{\\mu{m}+1}$ ] be a contaminated dataset of @xmath128 such that @xmath279 outliers are made by flipping the labels of the negative samples in @xmath128 . thus , there are @xmath65 negative samples in @xmath253 .",
    "let us define the outlier indicator @xmath260 such that @xmath261 for @xmath65 negative samples in @xmath253 .",
    "then , any sample in @xmath253 with @xmath262 should be a positive one .",
    "hence , we have @xmath268=\\emptyset$ ] .",
    "the infeasibility of the dual problem means that the primal problem is unbounded .",
    "thus , we obtain @xmath280 .        the non - contaminated dataset is denoted as @xmath22\\}$ ] . for the dataset @xmath128 , let @xmath283 and @xmath201 be the index sets defined by @xmath202 . under the conditions of theorem  [",
    "theorem : bounded_bias ] , inequality holds . given a contaminated dataset @xmath208\\}\\in\\mathcal{d}_{\\mu{m}-\\ell}$ ] , let @xmath284 be the negative margin of @xmath285 , i.e. , @xmath286 for @xmath212 . for @xmath33 , the function @xmath287 is defined as @xmath288 where the index set @xmath289 is given by @xmath290\\,:\\,\\mu{m}+1\\leq{j}\\leq\\nu{m}\\ }   \\end{aligned}\\ ] ] for the sorted negative margins , @xmath291 . for simplicity ,",
    "we drop the dependency of the permutation @xmath47 on @xmath97 .",
    "the estimated bias term @xmath292 is the optimal solution of @xmath287 because of .",
    "the function @xmath287 is continuous .",
    "in addition , @xmath287 is linear on the interval such that @xmath289 is unchanged .",
    "hence , @xmath287 is a continuous piecewise linear function .",
    "below , we prove that the minimum solution of @xmath287 is bounded regardless of the contaminated dataset @xmath293 .",
    "for the non - contaminated data @xmath128 , let @xmath294 be a positive real number such that @xmath295 the existence of @xmath294 is guaranteed .",
    "indeed , one can choose @xmath296 because the rkhs norm of @xmath297 is uniformly bounded above for @xmath298 and @xmath128 is a finite set .",
    "for the contaminated dataset @xmath299\\}\\in\\mathcal{d}_{\\mu{m}-\\ell}$ ] , let us define the index sets @xmath300 and @xmath301 for each label by @xmath302\\,:\\,y_i'=\\pm1\\},\\\\   i_{\\mathrm{in},\\pm}'&=\\{i\\in{}i_{\\pm } ' \\,:\\ , |f_{d'}(x_i')|\\leq{}r\\ } , \\\\",
    "i_{\\mathrm{out},\\pm}'&=\\{i\\in{}i_{\\pm}'\\,:\\ , |f_{d'}(x_i')|>r\\}. \\end{aligned}\\ ] ] for any non - contaminated sample @xmath210 , we have @xmath303 .",
    "hence , @xmath212 for @xmath304 should be an outlier that is not included in @xmath128 .",
    "this fact leads to @xmath305          let us prove the first statement .",
    "if @xmath306 holds , we have @xmath310 from the definition of the index set @xmath311 .",
    "let us consider two cases : ( i ) for all @xmath312 , @xmath313 holds , and ( ii ) there exists an index @xmath314 such that @xmath315 .    for a fixed @xmath97 such that @xmath306 , let us assume ( i ) above .",
    "then , for any index @xmath214 in @xmath316 , we have @xmath317 , meaning that @xmath318 .",
    "hence , the size of the set @xmath316 is less than or equal to @xmath319 .",
    "therefore , the size of the set @xmath320 is greater than or equal to @xmath321 .",
    "the first inequality of leads to @xmath322 .",
    "therefore , in the set @xmath289 , the number of negative samples is more than the number of positive samples .    for a fixed @xmath97 such that @xmath306 , let us assume ( ii ) above . due to the inequality  , for any index @xmath323",
    ", the negative margin @xmath284 is at the top @xmath64 of those ranked in the descending order .",
    "hence , the size of the set @xmath320 is greater than or equal to @xmath324 . therefore , the size of the set @xmath316 is less than or equal to @xmath325 .",
    "the second inequality of leads to @xmath326 .",
    "also in the case of ( ii ) , the negative label dominates the positive label in the set @xmath289 .    for negative ( resp .",
    "positive ) samples , the negative margin is expressed as @xmath327 ( resp .",
    "@xmath328 ) with a constant @xmath329 .",
    "thus , the continuous piecewise linear function @xmath287 is expressed as @xmath330 where @xmath331 are constants as long as @xmath289 is unchanged . as proved above",
    ", @xmath332 is a positive integer , since negative samples are more than positive samples in @xmath289 , when @xmath306 . as a result",
    ", the optimal solution of the bias term should satisfy @xmath333 in the same manner , one can prove the second statement by using the fact that @xmath307 is a sufficient condition of @xmath334 then , we have @xmath335 in summary , we obtain @xmath336      we use the same notation as in the proof of theorem  [ theorem : bounded_bias ] in appendix  [ appendix : proof_breakdown_bias ] .",
    "note that inequality holds under the assumption of theorem  [ theorem : bounded_kernel_bounded_bias ] .",
    "the reproducing property of the rkhs inner product yields @xmath337 for any @xmath299\\}\\in\\mathcal{d}_{\\mu{m}}$ ] due to the boundedness of the kernel function and inequality  .",
    "hence , for a sufficiently large @xmath338 , the sets @xmath339 and @xmath340 become empty for any @xmath150 .    under inequality , suppose that @xmath313 holds for all @xmath312 . then , for @xmath341 , we have @xmath317 .",
    "thus , @xmath318 holds .",
    "since @xmath339 is the empty set , @xmath316 is also the empty set .",
    "therefore , @xmath342 has only negative samples .",
    "let us consider the other case ; i.e. , there exists an index @xmath314 such that @xmath315 .",
    "assuming that @xmath168 , one can prove that the negative labels dominate the positive labels in @xmath289 in the same manner as the proof of theorem  [ theorem : bounded_bias ] .",
    "eventually , for any @xmath150 , the function @xmath287 is strictly increasing for @xmath306 . in the same way",
    ", one can prove that @xmath287 is strictly decreasing for @xmath307 .",
    "moreover , for any @xmath150 and for @xmath343 , one can prove that the absolute value of the slope of @xmath287 is bounded below by @xmath308 according to the argument in the proof of theorem  [ theorem : bounded_bias ] . as a result",
    ", we obtain @xmath344 .                  d.  j. crisp and c.  j.  c. burges . a geometric interpretation of @xmath1-svm classifiers . in s.",
    "a. solla , t.  k. leen , and k .-",
    "mller , editors , _ advances in neural information processing systems 12 _ , pages 244250 .",
    "mit press , 2000 .              f.  perez - cruz , j.  weston , d.  j.  l. hermann , and b.  schlkopf .",
    "extension of the @xmath1-svm range for classification . in",
    "_ advances in learning theory : methods , models and applications 190 _ , pages 179196 , amsterdam , 2003 .",
    "ios press .",
    "a.  takeda , h.  mitsugi , and t.  kanamori . a unified robust classification model . in john langford and joelle pineau , editors , _ proceedings of the 29th international conference on machine learning ( icml-12 ) _ ,",
    "icml 12 , pages 129136 , new york , ny , usa , july 2012 . omnipress .",
    "a.  takeda and m.  sugiyama .",
    "-support vector machine as conditional value - at - risk minimization . in william",
    "w. cohen , andrew mccallum , and sam  t. roweis , editors , _ icml _ , volume 307 of _ acm international conference proceeding series _ , pages 10561063 .",
    "acm , 2008 .",
    "p.  tsyurmasto , s.  uryasev , and j.  gotoh .",
    "support vector classification with positive homogeneous risk functionals .",
    "technical report , research report 2013 - 4 , department of industrial and systems engineering , university of florida.(downloadable from www .",
    "edu / uryasev / publications/ ) , 2013 .            y.  yu , m.  yang , l.  xu , m.  white , and d.  schuurmans .",
    "relaxed clipping : a global training method for robust regression and classification . in _ neural information processing systems _",
    ", pages 25322540 . mit press , 2010 ."
  ],
  "abstract_text": [
    "<S> the support vector machine ( svm ) is one of the most successful learning methods for solving classification problems . despite its popularity </S>",
    "<S> , svm has a serious drawback , that is sensitivity to outliers in training samples . </S>",
    "<S> the penalty on misclassification is defined by a convex loss called the hinge loss , and the unboundedness of the convex loss causes the sensitivity to outliers . to deal with outliers , </S>",
    "<S> robust variants of svm have been proposed , such as the robust outlier detection algorithm and an svm with a bounded loss called the ramp loss . in this paper </S>",
    "<S> , we propose a robust variant of svm and investigate its robustness in terms of the breakdown point . </S>",
    "<S> the breakdown point is a robustness measure that is the largest amount of contamination such that the estimated classifier still gives information about the non - contaminated data . </S>",
    "<S> the main contribution of this paper is to show an exact evaluation of the breakdown point for the robust svm . for learning parameters such as the regularization parameter in our algorithm </S>",
    "<S> , we derive a simple formula that guarantees the robustness of the classifier . </S>",
    "<S> when the learning parameters are determined with a grid search using cross validation , our formula works to reduce the number of candidate search points . </S>",
    "<S> the robustness of the proposed method is confirmed in numerical experiments . </S>",
    "<S> we show that the statistical properties of the robust svm are well explained by a theoretical analysis of the breakdown point . </S>"
  ]
}