{
  "article_text": [
    "convolutional codes were first introduced by elias more than 50 years ago [ 1 ] . they have been widely used in various modern communications systems , such as space and satellite communications , cellular mobile , digital video broadcasting etc .",
    "its popularity stems from its simple encoder structure , which can be implemented by using shift registers .",
    "the main complexity associated with systems using convolutional coding is situated in the decoder .",
    "decoding essentially consists of finding an optimal path in a trellis based graph .",
    "various decoding algorithms have been developed to achieve the optimal decoding performance in the most efficient manner .",
    "the viterbi algorithm ( va ) has been known as a maximum - likehood ( ml ) decoding method , which minimizes the sequence error rate [ 2 - 4 ] .",
    "it exhaustively searches all states of the trellis over a fixed length window and finds a most likely information sequence . in the standard va ,",
    "the decoder produces hard - decision outputs , which are the estimates of transmitted binary information symbols . in [ 5 , 8 ] ,",
    "the va is modified to deliver not only the most - likely binary signal sequence , but also the soft output containing the a posteriori probabilities ( apps ) of the transmitted binary symbols .",
    "the soft - output va ( sova ) is especially useful when decoding concatenated codes , such as turbo codes , as it provides soft input for the next decoding stage and thus improved performance",
    ".    there exists another class of non - linear decoding algorithms , called maximum a posteriori probability ( map ) decoding .",
    "it was first proposed by bahl , cocke , jelinek and raviv ( bcjr ) in 1974 [ 6 ] .",
    "it performs symbol by symbol decoding and uses the symbol error rate as the optimization criterion .",
    "both the input and output of the decoder are soft information signals . compared to the va , the soft - input - soft - output ( siso ) map can provide the optimal symbol - by - symbol app , and thus can fully exploit the full benefits of soft - decision decoding in iterative decoding process of concatenated codes .",
    "the bcjr map decoding is a bi - directional decoding process , consisting of a forward and a backward recursion process , which dominates the main complexity of a decoder . in each direction",
    ", the decoder infers the probabilities of current states and information symbols based on the probabilities of the previous states in the forward and backward trellis , the received signal , the channel state and the a priori probabilities of the transmitted signals .",
    "the complexity of forward and backward recursion exponentially increases with the constraint length of convolutional codes .    in this paper , we revisit the forward , backward and bidirectional bcjr siso map decoding of rate-1 convolutional codes .",
    "we observe some duality properties between a siso forward / backward bcjr map decoder of a convolutional code and its encoder .",
    "the forward and backward decoder of a rate-1 convolutional code can actually be represented by its corresponding dual encoder using shift registers in the complex number field .",
    "this significantly reduces the the original exponential computational complexity of map forward and backward recursion to the linear complexity .",
    "similarly the bidirectional bcjr map decoding can be implemented by linearly combining the outputs of the dual siso encoders of the respective forward and backward decoders .",
    "with logarithm of the soft coded symbol estimate , directly obtained from the received signals , as the input to the dual encoder , the dual encoder output produces the logarithm of the soft symbol estimates of the binary information symbols .",
    "we found that the dual encoder structure of a code depends on whether the code is recursive or not .",
    "we divide the rate-1 convolutional codes into three classes , feedback only convolutional ( fbc ) code , feed - forward only convolutional ( ffc ) code and general convolutional ( gc ) code .",
    "the dual encoder structure is derived for each class of codes .",
    "the remainder of the paper is organized as follows . in section",
    "ii , we first briefly review the bcjr forward decoding algorithm and derive the dual encoder structures of bcjr forward decoders for three classes of rate-1 convolutional codes .",
    "the duality for backward decoding is presented in section iii .",
    "the representation of bidirectional map decoding by using the derived dual encoder structures of forward and backward decoding is described in section iv .",
    "simulation results are shown in section v. conclusions are drawn in section vi .",
    "in this section , we first revisit the forward bcjr map decoding algorithm .",
    "we will focus on the decoding of a single constituent convolutional code of rate-1 .",
    "let @xmath0 be a binary information symbol sequence to be transmitted , where @xmath1 is the frame length .",
    "let @xmath2 be the binary codeword of @xmath3 , generated by the binary code generator polynomial @xmath4 , and @xmath5 be the modulated symbol sequence of @xmath6 . for simplicity , we consider a bpsk modulation .",
    "let @xmath7 denote the received signal sequence at the output of channel .",
    "based on the encoder structure , we define three different classes of convolutional codes .",
    "let @xmath8 and @xmath9 , where @xmath10 is called the degree of polynomials @xmath11 and @xmath12 .",
    "we define a convolutional code , generated by @xmath13 , as a feedback - only convolutional ( fbc ) code , a code generated by @xmath14 as a feed - forward only convolutional ( ffc ) code , and a code generated by @xmath15 , as a general convolutional ( gc ) code .",
    "we will investigate the forward decoding process of these three classes of convolutional codes .      in this subsection , we first investigate the forward decoding of an fbc code . to gain better insight into the decoding process ,",
    "let us first look at the following example .",
    "* example 1 : * we consider a fbc code with the generator polynomial of @xmath16 , for which the encoder and trellis diagram are shown in fig .",
    "[ fig0 ] . in the trellis diagram ,",
    "the state is labeled as @xmath17 , where @xmath18 , @xmath19 is the value of the @xmath20-th encoder shift register content .",
    "each branch in the trellis is labeled as @xmath21 where @xmath22 and @xmath23 denote the encoder input and output , respectively .",
    ", scaledwidth=100.0% ]    let @xmath24 , @xmath25 , denote the a posteriori probabilities ( app ) of the encoded symbol @xmath26 , given the received signal @xmath27 , where @xmath28 is the transmitted binary coded symbol at time @xmath29 .",
    "let us further denote @xmath30 , @xmath31 , @xmath32 .",
    "now let us follow the bcjr map forward decoding algorithm to use @xmath33 to calculate the apps of binary information symbols @xmath34 .",
    "let @xmath35 represent the probability of information symbol @xmath36 , @xmath37=0 , 1 , given the received signals @xmath38 .",
    "it can be calculated in the following recursive way @xmath39 @xmath40 where @xmath41 is the set of trellis branches from the state @xmath42 at time @xmath29 - 1 to the state @xmath43 at time @xmath29 , that are caused by the input binary symbol @xmath44 , and @xmath45 represents the encoder output of the corresponding trellis branch .",
    "let @xmath46 represent the states of @xmath47 at time @xmath29 , respectively .",
    "let @xmath48 and @xmath49 denote the soft symbol estimate sequence of codeword @xmath6 and information sequence @xmath3 , respectively .",
    "we assume that 0 and 1 are modulated into the symbol 1 and -1 . then the soft symbol estimates @xmath50 and @xmath51 , which represent the probabilistic average of estimates of symbols @xmath52 and @xmath53 given @xmath54 , can be calculated as @xmath55    then by using eqs .",
    "( [ eq1 ] ) and ( [ eq2 ] ) alternatively in example 1 , we get    \\(1 ) at time @xmath56 ,    @xmath57 ; @xmath58 ; @xmath59 ; @xmath60 ;    @xmath61 ;  @xmath62 ;    \\(2 ) at time @xmath63 , the received signal is @xmath64 , and the input to the decoder is the apps of @xmath65 , given by @xmath66 and @xmath67 , respectively .",
    "then we have    @xmath68 ; @xmath69 ; @xmath70 ; @xmath71 ;    @xmath72 ; @xmath73 ;    and    @xmath74    \\(3 ) at time @xmath75 , the input to the decoder is the apps of @xmath76 , @xmath77 and @xmath78 .",
    "we have    @xmath79 ; @xmath80 ; @xmath81 ; @xmath82 ;    @xmath83 ; @xmath84 ;    and    @xmath85    \\(4 ) similarly , at time 3 , we have    @xmath86 ; @xmath87 ; @xmath88 ; @xmath89 ;    @xmath90 ;    @xmath91 ;    and    @xmath92    at any time instant @xmath93 , we can generalize that    @xmath94 ; @xmath95 ; @xmath96 ; @xmath97 ;    @xmath98 ;    @xmath99 ;    and @xmath100 where    @xmath101    therefore , the decoder input and its output soft symbol estimates , @xmath50 and @xmath51 , for the code , generated by @xmath16 , have the following relationship @xmath102    by taking the natural logarithm of both sides of the above equation , we get @xmath103    we define the decoder with the input and output being the logarithm of the soft symbol estimates ( sse ) of the coded symbols and sses of the information symbols , as the log - domain soft - input - soft - output ( siso ) decoder . as shown in fig .",
    "[ fig1 ] , the siso decoder can be implemented by adding a logarithm module and an exponential module at the front and rear end of the log - domain siso decoder , respectively .            based on eq .",
    "[ eq4 ] , log - domain siso forward decoding of the code @xmath16 can be implemented by using the convolutional encoder , generated by the generator polynomial @xmath104 , as shown in fig .",
    "[ fig2 ] . here",
    "the addition operation in the encoder is not carried out in the binary domain as in conventional convolutional encoders , but in the complex number domain .",
    "[ eq4 ] and fig .",
    "[ fig2 ] reveal a duality relationship of the binary encoder and siso forward decoder of a rate-1 feedback only convolutional code .",
    "this can be generalized to any fbc codes as summarized in the following theorem .",
    "* theorem 1 - forward decoding duality of a feedback only convolutional ( fbc ) code : * for a fbc code , generated by a generator polynomial @xmath13 , we define its dual encoder as the encoder with the inverse generator polynomial of @xmath105 , given by @xmath106 .",
    "then the log - domain siso forward decoding of the fbc code can be simply implemented by its dual encoder in a complex field .",
    "this duality property is shown in fig .",
    "[ fig3 ] .",
    "proof : see appendix a. +          in this sub - section , we investigate the forward decoding of a ffc code . as will be shown in the following example",
    ", the duality property shown in theorem 1 does not apply to such codes .",
    "* example 2 * : we consider a ffc code with the generator polynomial of @xmath107 for which the trellis diagram and encoder are shown in fig .",
    "[ fig3_0 ] .",
    ", scaledwidth=90.0% ]    let @xmath108 represent the output of the log - domain dual encoder , generated based on theorem 1 , with the generator polynomial of @xmath109 .",
    "the table i compares @xmath108 with the actual forward map decoding soft output @xmath110 .",
    "their differences are highlighted in the dashed - line boxes .",
    ".comparison of the dual encoder output calculated based on theorem 1 @xmath108 with the actual forward map decoding soft output @xmath110 [ cols=\"^,^,^,^,^ \" , ]     [ fonr_table ]    we can prove that for any ffc codes , we can always find a modified dual decoder to implement a map forward decoder without changing its actual generator polynomial .",
    "this is summarized in theorem 2 .",
    "before we present the new theorem , we first define a _ minimum complementary polynomial_. for a given polynomial @xmath111 , we define the _ minimum complementary polynomial _ as the polynomial of the smallest degree , @xmath112 such that @xmath113    since @xmath111 always divides @xmath114 , the minimum complementary polynomial of @xmath11 always exists .",
    "* theorem 2 - forward decoding duality of a feed - forward only convolutional ( ffc ) code : * for a ffc code , generated by a generator polynomial @xmath14 , let @xmath115 represent its _ minimum complementary polynomial _ of degree @xmath116 .",
    "the log - domain siso forward decoding of the ffc code can be implemented by its dual encoder with the generator polynomial of @xmath117    proof : see appendix b. + theorem 2 can be easily extended to a general convolutional ( gc ) code as shown in the following corollary .",
    "* corollary 1 - forward decoding duality of a general convolutional ( gc ) code * : for a gc code , generated by a generator polynomial @xmath118 , let @xmath115 be the degree-@xmath116 minimum complementary polynomial of @xmath11 .",
    "the log - domain siso forward decoding of the gc code can be simply implemented by its dual encoder with the generator polynomial of @xmath119 where @xmath120    this relationship of a binary encoder and its dual encoder is shown in fig .",
    "corollary 1 can be directly derived from theorem 2 , so we skip its proof here .",
    "in this section , we investigate the map backward decoding of rate-1 convolutional codes and derive its dual encoder structure . before discussing the backward decoding , we first define a reverse memory - labeling of a general convolutional ( gc ) code . given the encoder of a gc code with rational generator polynomial @xmath121 , if we change the labeling of the @xmath29-th shift register in the encoder from @xmath122 to @xmath123 , and change their respective feed - forward coefficient from @xmath124 to @xmath125 , @xmath29=@xmath126 , and feedback coefficients from @xmath34 to @xmath127 , @xmath29=@xmath126 , we will derive an encoder with a new trellis .",
    "the resulting encoder is referred to as the _ reverse memory - labeling encoder _ of @xmath12",
    "[ fig6_1 ] and [ fig6_2 ] show the encoder and the reverse memory - labeling encoder of @xmath12 .    in a bcjr map backward decoding",
    ", the received signals are decoded backward in a time - reverse order .",
    "that is , given the received signal sequence @xmath7 , the order of signals to be decoded is from @xmath128 , @xmath129 , till @xmath130 . in order to decode the received signals backward , the decoder has to follow the trellis in a reverse direction",
    "[ fig7_1 ] and [ fig7_2 ] show the encoder and trellis of the code with the generator polynomial @xmath131 .",
    "[ fig7_3 ] shows the backward trellis . for the decoder with the backward trellis in fig .",
    "[ fig7_3 ] , the input to the decoder is at the right hand side of the decoder and its output is at the left hand side , which operates in a reverse direction of the conventional decoder . fig .",
    "[ fig7_4 ] shows the corresponding forward representation of the backward trellis , where the decoder input and output are changed to the conventional order .",
    "the forward representation of the backward trellis can be implemented by an encoder shown in fig .",
    "[ fig7_5 ] .",
    "when we compare figs .",
    "[ fig7_1 ] and [ fig7_5 ] , it can be easily seen that the encoder in fig .",
    "[ fig7_1 ] is the encoder of code @xmath131 and that in fig .",
    "[ fig7_5 ] is its encoder with the _ reverse memory - labeling_.    this relationship of the encoders for the forward and backward trellises can be extended to general rate-1 convolutional codes , as shown in the following theorem .",
    "* theorem 3 * : given an encoder with a generator polynomial @xmath121 , the forward representation of its backward trellis can be implemented by its _ reverse memory - labeling encoder _ of the same generator polynomial @xmath12 .",
    "proof : see appendix c. + from theorem 2 , we know that the log - domain siso forward decoding of a given general convolutional ( gc ) encoder with a generator polynomial @xmath132 can be implemented by its dual encoder with the generator polynomial @xmath133 , where @xmath115 is the degree-@xmath116 minimum complementary polynomial of @xmath11 . then according to theorem 3 , the log - domain siso backward decoding of the gc code can be implemented by the _",
    "reverse memory - labeling encoder _ of @xmath134 . by combining theorems 2 and 3",
    ", we can obtain the backward decoding duality , which is summarized in the following theorem .",
    "* theorem 4 - backward decoding duality of a general convolutional ( gc ) code : * we consider a general convolutional encoder with a generator polynomial of @xmath121 .",
    "let @xmath115 be the degree-@xmath116 minimum complementary polynomial of @xmath11 .",
    "its log - domain siso backward decoding can be implemented by its dual encoder with _ reverse memory - labeling _ and the generator polynomial of @xmath135 this duality is shown in fig .",
    "[ fig8 ] .        from theorem 4 , we can easily derive the backward decoding duality of a feed - forward only convolutional ( ffc ) code , summarized in the following corollary",
    ".    * corollary 2 - backward decoding duality of a feed - forward only convolutional ( ffc ) code : * for a ffc code , generated by a generator polynomial @xmath136 , let @xmath115 be the degree-@xmath116 minimum complementary polynomial of @xmath11 .",
    "its log - domain siso backward decoding can be implemented by its dual encoder with _ reverse memory - labeling _ and the generator polynomial @xmath137    corollary 2 can be proved in the same way as theorem 4 , so we skip the proof here .    for a feedback only convolutional ( fbc ) code , we can prove that backward decoding does not contribute to the map calculation .",
    "the bcjr map decoding is exactly the same as the forward decoding .",
    "this is summarized in the following theorem .",
    "* theorem 5 - decoding duality of a feedback only convolutional ( fbc ) code : * for a fbc code , generated by a generator polynomial @xmath13 , the map forward decoding is in fact equivalent to the bcjr map decoding .",
    "its log - domain siso decoder can be simply implemented by the dual encoder for the map forward decoding with the inverse generator polynomial of @xmath105 , given by @xmath138 .",
    "proof : see appendix d. + from theorem 5 , we can see that the map decoder of a fbc code can be implemented by its dual encoder using shift registers .",
    "this significantly reduces the decoding complexity .",
    "in the previous two sections , we have introduced the duality of channel encoding and siso map forward / backward decoding . based on the derived encoding - decoding duality properties , in this section , we represent the bidirectional bcjr map decoder by linearly combining outputs of the dual encoders for the forward and backward decoders . by comparing the bidirectional bcjr map decoding outputs with the forward and backward dual encoder outputs , combining coefficients",
    "are identified through computer search such that the resulting combined forward and backward dual encoder outputs are exactly the same as the bidirectional map decoding outputs . in this paper",
    ", we found the expressions of these combining coefficients for some commonly used 4-state and 8-state ffc and gc codes .",
    "the expressions for higher - states codes can be obtained in the same way .",
    "let us first call the dual encoder of the forward and backward decoding as the _ forward dual encoder _ and _ backward dual encoder _",
    ", respectively .",
    "let @xmath139 and @xmath140 represent the soft outputs of the forward and backward dual encoders .",
    "they can be calculated based on theorems 1 - 4 in sections ii and iii , respectively .",
    "then the bcjr siso map decoder output , @xmath141 , can be represented as the following linear combination of forward and backward dual encoder outputs , @xmath142 where @xmath143 and @xmath144 are the combining coefficients in real domain applied to the forward and backward dual encoder outputs , respectively .",
    "the combining coefficients for some 4-state and 8-state codes are shown below .",
    "the combining coefficients for the 4 states gc code [ 5/7]@xmath145 can be calculated as     = = ( 1+o/^2_c_k)/4,&for @xmath147 + ( 1+e/^2_c_k)/4,&for @xmath148 .    where @xmath50 is the soft symbol estimate of the received encoded symbol @xmath28 , where @xmath149 , @xmath1 is the frame of codeword , and @xmath150 @xmath151 where @xmath152 is the ceiling operation , representing the smallest integer not less than @xmath22 .",
    "the combining coefficients for the 4-state code [ 5]@xmath145 can be calculated as ,     = = ( 1+e)/4,&for @xmath147 + ( 1+o)/4,&for @xmath148 .    where the variables @xmath154 and @xmath155 are defined in ( [ eq : odd ] ) and ( [ eq : even ] ) .",
    "let us first define , @xmath156 where @xmath157 ; @xmath158 , and @xmath159 represents the modulo operation of @xmath160 .",
    "we will assume @xmath161 , if the index of @xmath162 in eq .",
    "( [ eq:7_fonr_1 ] ) is less than 1 .",
    "we further define the intermediate variables , @xmath163 and @xmath164 as , @xmath165 + 1}^{\\lceil k/3\\rceil}i_{\\langle k-1\\rangle_3,i}}{\\prod_{j=0}^{[k/3]}i_{\\langle k-1\\rangle_3,j}}. \\label{eq:7_fonr_3}\\end{aligned}\\ ] ] where @xmath166 $ ] is the round - off operation of @xmath22 , representing the integer closest to @xmath22 .",
    "then the combining coefficients for the 4-state code [ 7]@xmath145 can be calculated as , @xmath167      let us define @xmath169 where @xmath170 and @xmath171 . we assume , @xmath172 , if the subscript of @xmath173 in the above equation is less than one .",
    "we also define the intermediate variables , @xmath174 , @xmath175 and @xmath176 as , @xmath177 + 1}^{\\lceil k/4\\rceil}m_{0,i}}{\\prod_{j=0}^{[k/4]+1}m_{0,j}},\\\\ & & z_k=\\frac{\\prod_{i=[k/4]+1}^{\\lceil k/4\\rceil}m_{2,i}}{\\prod_{j=0}^{[k/4]+1}m_{2,j}}.\\end{aligned}\\ ] ] then the forward and backward combining coefficients for the 8-state code [ 17]@xmath145 can be calculated as    = ( 1+r_kr_k_4 + 1+(r_k+r_k_4 + 1)y_k)/8&for @xmath178 + ( 1+r_kr_k_4 + 1+(r_k+r_k_4 + 1)z_k)/8&for @xmath179    = ( 1+r_kr_k_4 + 1+(r_k+r_k_4 + 1)/y_k)/8&for @xmath178 + ( 1+r_kr_k_4 + 1+(r_k+r_k_4 + 1)/z_k)/8&for @xmath180      the forward and backward combining coefficients for the 8-state code [ 15/13]@xmath145 can be calculated as @xmath182 @xmath183 where @xmath184 and @xmath185 where @xmath186 , if the subscript of @xmath162 is less than one and greater than @xmath1 .",
    "to understand how to calculate the combining coefficients described in the above equations , let us now look at the following example .",
    "* example 3 : calculation of combining coefficients for the [ 7]@xmath145 ffc code *    from ( [ eq:7_fonr_1 ] ) we have , @xmath187 from ( [ eq:7_fonr_2 ] ) , if we define @xmath188 then @xmath163 can be calculated as @xmath189    from ( [ eq:7_fonr_comp_1 ] ) , we can see that , @xmath163 actually repeats every three terms .    similarly , from ( [ eq:7_fonr_3 ] ) , we have @xmath190 thus , @xmath191 that is , @xmath164 also follows a pattern over time . to calculate the @xmath164 for @xmath192 , we just need to do small modifications of @xmath193 .",
    "in this section , we provide the simulation results .",
    "all simulations are performed for the bpsk modulation and a frame size of @xmath1=128 symbols over awgn channels .    figs .",
    "[ 57_gr_sim ] to [ 15_fonr_sim ] show the bit error rate ( ber ) performance of various 4-state and 8-state gc and ffc codes , where the curve forward dual encoder + backward dual encoder refers to the direct summation of forward and backward dual encoder outputs , i.e. , @xmath139+@xmath140 , and the curve linear combination of forward and backward dual encoder refers to the optimal combining described in section iv , i.e. , @xmath194+@xmath195 .    from figures",
    ", we can see that direct summation of forward and backward dual encoder outputs has about @xmath196 performance loss when compared to the bidirectional bcjr map decoding for the gc code [ 5/7]@xmath145 at the ber of @xmath197 .",
    "this performance loss is reduced to around @xmath198 , @xmath199 , and @xmath200 for @xmath201_8 $ ] ffc , @xmath202_8 $ ] ffc , and @xmath203_8 $ ] ffc codes , respectively and increased to about @xmath204 for the @xmath205_8 $ ] gc code . however , when we apply the linear combination detailed in section iv to the forward and backward dual encoder outputs , their performance is exactly the same as the bcjr map decoding .",
    "one particular point needs to be noted is that for the ffc code [ 5]@xmath145 the direct summation of forward and backward dual encoder outputs has the same performance as the map decoding , so no linear combination is actually required .",
    "@xmath145 gc code over awgn channels , scaledwidth=100.0% ]    @xmath145 ffc code over awgn channels , scaledwidth=100.0% ]    @xmath145 ffc code over awgn channels , scaledwidth=100.0% ]    @xmath145 gc code over awgn channels , scaledwidth=100.0% ]    @xmath145 ffc code over awgn channels , scaledwidth=100.0% ]    @xmath145 ffc code over awgn channels , scaledwidth=100.0% ]",
    "in this paper , we revisited the bcjr siso map forward and backward decoding process for the rate-1 convolutional codes .",
    "dual encoder structures of forward and backward decoding for three different classes of rate-1 convolutional codes are derived .",
    "the input to the dual encoder is the logarithm of soft symbol estimates of the coded symbols obtained from the received signals , and the dual encoder output produces the logarithm of the soft symbol estimates of the information symbols . for the general convolutional ( gc )",
    "codes , generated by a generator polynomial @xmath206 , the forward and backward decoding can be implemented by their corresponding dual encoders , which are generated by the polynomial , @xmath207 , where @xmath115 is the _ minimum complementary polynomial _ of @xmath11 .",
    "the feed - forward only convolutional ( ffc ) code is just a special case of gc code , so it has the same dual encoder structures as the gc code .",
    "the derived duality property significantly reduced the the computational complexity of map forward and backward recursion from exponential to linear .",
    "similarly , the bidirectional map decoder of gc and ffc codes can be implemented by linearly combining the outputs of dual encoders for the forward and backward decoding . for a feedback",
    "only convolutional ( fbc ) code @xmath208 , the bidirectional map siso decoder is equivalent to the dual encoder for the forward decoding , with the generator polynomial @xmath209 .    in this paper , we have only focused on a class of rate-1 convolutional codes .",
    "its significance is mainly as component codes in concatenated coding schemes , such as turbo coding .",
    "actually the encoding - decoding duality derived in this paper can also be applied to other codes and other applications .",
    "for example , the transmission of digital signals in the presence of inter - symbol interference ( isi ) can also be represented by a convolutional encoding process .",
    "the channel transfer function of a isi channel can be represented by a rate-1 convolutional encoder .",
    "thus the encoding - decoding duality properties can also be directly applied to facilitate the map channel detection in isi channels . with the logarithm of the soft symbol estimates of isi channel outputs as the input to the dual encoder",
    ", the output of dual encoder will produce the map detection output of isi channels .",
    "similarly , these duality properties should exist for other linear codes , which can be represented by a trellis diagram .",
    "let us consider a feedback only convolutional ( fbc ) code , generated by a generator polynomial @xmath210 its encoder is shown in fig .",
    "let @xmath211 represent the state of memory @xmath20 at time @xmath29 . then according to fig .",
    "[ fig3 ] we have @xmath212 @xmath213 where all summations are done in gf(2 ) .",
    "let @xmath219 denote the probability of memory @xmath220 and @xmath221 denote the probability of state @xmath43 at time @xmath29 .",
    "let @xmath222 be the @xmath10-dimensional binary representation of @xmath43 and @xmath223 be the binary representation of @xmath42 . at time @xmath29 , with input @xmath28 , the state transits from @xmath223 to @xmath224= @xmath225 .",
    "then we have @xmath226 where @xmath227 , @xmath228=@xmath229 and @xmath230=@xmath231 , for @xmath232=@xmath233 .",
    "then by using the following relationship between the llr and soft symbol estimate , @xmath241 @xmath242 ( [ eq20 ] ) can be further written as @xmath243 where @xmath244 denotes the soft symbol estimate of symbol @xmath245 .",
    "obviously @xmath246 when @xmath247 and @xmath248 when @xmath249 .",
    "thus @xmath250 .",
    "let us first examine the forward binary decoding . based on the code generator polynomials",
    ", we can easily derive the binary decoder of codes generated by @xmath11 and @xmath254=@xmath255 , as shown in fig .",
    "[ fig10_1 ] and [ fig10_2 ] , respectively . as can be seen from these figures ,",
    "the binary decoder of each of these two codes is equivalent to the encoder generated by its respective inverse polynomial .",
    "let @xmath222 and @xmath223 be the @xmath10-dimensional binary representation of @xmath43 and @xmath42 .",
    "let @xmath256 and @xmath257 be the ( @xmath258)-dimensional binary representation of @xmath259 and @xmath260 .",
    "assume that at time @xmath29 , with input @xmath28 , the state transits from @xmath261 to @xmath262 in the binary decoder of fig .",
    "[ fig10_1 ] and transits from @xmath263 to @xmath264 in [ fig10_2 ] . for a binary input sequence @xmath265",
    ", it is well known that the polynomials @xmath266 and @xmath267 generate the same codeword .",
    "we thus have @xmath268 @xmath269      when the terms in the summation of the right - hand side in ( [ eq29 ] ) and ( [ eq30 ] ) are statistically independent , we can use the l - sum theory to further expand these two equations . however , we can easily check that the terms @xmath272 , @xmath273 , in ( [ eq29 ] ) , are not independent .",
    "now let us prove that @xmath274 , @xmath275 are statistically independent random variables .          since @xmath274 ,",
    "@xmath277 are statistically independent random variables , we can use the l - sum theory [ 7 ] to expand the right - hand side of ( [ eq31 ] ) . by following a similar calculation as in appendix",
    "a , we can obtain the following equation @xmath283 and , @xmath284 where @xmath285 , @xmath286 , @xmath287 and @xmath288 denotes the soft symbol estimate of symbol @xmath289 , @xmath290 , @xmath291 , and @xmath28 , respectively .",
    "based on ( [ eq34 ] ) and ( [ eq35 ] ) , we can derive the siso decoder structure , shown in fig .",
    "[ fig11 ] , implemented with the encoder with the generator polynomial of @xmath292          assume that the encoder with the generator polynomial @xmath12 in fig .",
    "[ fig6_1 ] transits from the state @xmath293 at time @xmath29 - 1 to the state @xmath294 at time @xmath29 with input @xmath34 , then we have @xmath295 and the corresponding trellis output at time @xmath29 is given by @xmath296    to prove theorem 3 , we now only need to prove that with input @xmath34 its _ reverse memory - labeling _ encoder transits from the state @xmath297 at time @xmath29 - 1 to the state @xmath298 at time @xmath29 and generate the same encoder output .",
    "now let us consider the _",
    "reverse memory - labeling _ encoder with the generator polynomial @xmath12 in fig .",
    "[ fig6_2 ] . with the state @xmath297 at time @xmath29 - 1 and input @xmath34 , the state at time @xmath29 of the _",
    "reverse memory - labeling _",
    "encoder is given by @xmath299 @xmath300 where in the step @xmath301 of ( [ eq38 ] ) we have used eq .",
    "( [ eq36 ] ) .      from ( [ eq38]-[eq40 ] ) , we can see that with input @xmath34 the reverse memory - labeling encoder transits from the state @xmath303 at time @xmath304 to the state @xmath298 and generates the same encoder output as the encoder with the generator polynomial @xmath12 .        to prove theorem 5 ,",
    "let us first examine the backward decoding of a fbc code . at the encoder of a fbc code in fig .",
    "5 , with input @xmath34 , the state transits from @xmath305 at time @xmath304 to @xmath306 = @xmath307 at time @xmath29 , where @xmath28 is the encoder output .",
    "the state transition is shown in the fig .",
    "[ fig12 ] , where @xmath308 , @xmath309 , @xmath37=0 or 1 , @xmath310 , @xmath311 , and @xmath312 .                                from the above equation",
    ", we can see that @xmath334 is the same for all states when @xmath335 .",
    "therefore , the backward decoding does not have any contribution in the probability calculation of the bcjr decoding .",
    "this proves that the bcjr forward decoding is exactly the same as the bcjr map decoding for the fbc codes ."
  ],
  "abstract_text": [
    "<S> in this paper , we revisit the forward , backward and bidirectional bahl - cocke - jelinek - raviv ( bcjr ) soft - input soft - output ( siso ) maximum a posteriori probability ( map ) decoding process of rate-1 convolutional codes . from this </S>",
    "<S> we establish some interesting duality properties between encoding and decoding of rate-1 convolutional codes . </S>",
    "<S> we observe that the forward and backward bcjr siso map decoders can be simply represented by their dual siso channel encoders using shift registers in the complex number field . </S>",
    "<S> similarly , the bidirectional map decoding can be implemented by linearly combining the outputs of the dual siso encoders of the respective forward and backward decoders . </S>",
    "<S> the dual encoder structures for various recursive and non - recursive rate-1 convolutional codes are derived .    </S>",
    "<S> convolutional codes ,  map decoding ,  encoding and decoding duality , dual encoder , bidirectional decoding </S>"
  ]
}