{
  "article_text": [
    "deterministic computer models are used to simulate a wide variety of physical processes ( sacks et al . , 1989 ; santner et al . , 2003 ) .",
    "oftentimes , a single run of the code requires considerable computational effort , making it infeasible to continually exercise the simulator .",
    "instead , experimenters attempt to explore the computer model response ( and to some extent the physical process ) using a limited number of computer model runs .    in some applications ,",
    "several simulators of the physical process are available to describe the same system ( craig et al . , 1998 ;",
    "kennedy and ohagan , 2000 ; qian et al . , 2006 and 2008 ; reese et al . , 2004 ; cumming and goldstein , 2009 ) , each with different levels of fidelity .",
    "the varying levels of fidelity can occur , for example , because of the presence of reduced order physics in lower fidelity models , different levels of accuracy specified for numerical solvers or solutions obtained on finer grids . in these cases ,",
    "a higher fidelity model is thought to better represent the physical process than a lower fidelity model , but also takes more computer time to produce an output than a lower fidelity model .",
    "so , combining relatively cheap lower fidelity model runs with more costly high fidelity runs to emulate the high fidelity model has been an significant problem of interest ( kennedy and ohagan , 2000 ; qian et al . ,",
    "2006 and 2008 ) .",
    "another important application of computer models is that of _ calibration _",
    "( e.g. , kennedy and ohagan , 2001 ; higdon et al . , 2004 ) where the aim is to combine simulator outputs with physical observations to build a predictive model and also estimate unknown parameters that govern the behaviour of the computer model .",
    "the latter endeavour amounts to solving a sort of inverse problem , while the former activity is a type of regression problem .",
    "motivated by applications at the center for radiative shock hydrodynamics ( crash ) at the university of michigan , the aim of this work is to develop new methodology to combine outputs from simulators with different levels of fidelity and field observations to make predictions of the physical system with associated measurements of uncertainty . in the spirit similar to kennedy and ohagan ( 2000 and 2001 ) and higdon et al .",
    "( 2004 ) , we propose a predictive model that incorporates computer model outputs and field data , while attempting to find optimal values for some input parameters ( i.e. _ calibration parameters _ ) .",
    "different models are specified for each source of data ( kennedy and ohagan , 2000 ; qian et al . , 2006 and 2008 ) .",
    "the approach calibrates each computer model to the next highest level of fidelity model , and the simulator of the highest fidelity is then calibrated to the field measurements .",
    "all the response surfaces are gaussian process ( gp ) models and the various sources of information that inform predictions of the physical system are combined with a bayesian hierarchical model .",
    "the paper is organized as follows : in section [ sec : method ] , we will introduce the proposed methodology and the gp models involved , along with the relevant priors . the framework for prediction",
    "will be discussed at the end of the section .",
    "a simple example from the literature and an application from crash are used to demonstrate the proposed approach in section [ sec : eg ] .",
    "further discussion follows in section [ sec : discussion ] with some concluding remarks in section [ sec : conclusion ] .",
    "in this section , a bayesian hierarchical model that calibrates multi - fidelity computer simulators is proposed .",
    "the higher fidelity code is assumed to better represent the real world process but is assumed to require more computing resources to simulate the system . for ease of presentation and notation , we present the case where there are only two computer simulators  a high fidelity and a low fidelity model .",
    "it is easy to extend the proposed methodology to cases with more than two simulators , and this setting is discussed in section [ sec : discussion ] .      throughout this work ,",
    "the computer models are assumed to be deterministic mathematical functions that map inputs to outputs .",
    "the computer codes have two types of inputs : ( i ) _ design variables _ , @xmath0 , that are adjustable or measurable in the field experiments ; and ( ii ) _ calibration parameters _ , @xmath1 , whose values are thought to impact the physical system , but are unknown a priori .",
    "the latter inputs can only be adjusted within the simulator , but are not measurable in the field . in model calibration problems ,",
    "the issue is to build a predictive model for the field process and also to estimate the unknown calibration parameters .",
    "a unique feature of the application that motivated the current work is that the calibration parameters for the computer models are not all the same .",
    "some of the calibration parameters , @xmath2 , are shared among the simulators , whereas others are required inputs only to individual simulators .",
    "the vectors of calibration inputs exclusive to the high and low fidelity models are denoted as @xmath3 and @xmath4 , respectively .",
    "first consider the low fidelity computer model with inputs @xmath5",
    "( i.e. , the design variables and calibration parameters that are shared and unshared with the high fidelity simulator ) , where @xmath6 , @xmath7 and @xmath8 .",
    "outputs @xmath9 from the low fidelity simulator , @xmath10 , are written as : @xmath11 similarly , the high fidelity simulator , @xmath12 , has inputs @xmath13 , where @xmath14 , and output @xmath15 : @xmath16    both simulators are used to describe the same physical process , but will not always give the same response .",
    "there are a few obvious reasons why this is the case .",
    "the lower fidelity model is inferior to the high fidelity simulator since it may , for example , fail to capture some processes that the high fidelity code can more accurately model . furthermore , the two codes do not share all of the same inputs .",
    "the inputs @xmath17 only appear in the high fidelity model and thus , any impact that these variables have on the output can not be captured by the low fidelity model .",
    "similarly , the inputs @xmath18 appear only in the low fidelity model . to address these issues , we take the approach of writing the high fidelity simulator as a discrepancy adjusted version of the low fidelity model ( e.g. kennedy and ohagan , 2000 ; qian et al . , 2006 ) : @xmath19    specifying the first term in ( [ eqn : sim2withsim1 ] ) as @xmath20 amounts to partially calibrating ( partially in the sense that the other calibration parameters must still be estimated ) the first computer model to the second computer model . furthermore , the discrepancy , @xmath21 , represents the systematic differences between the partially calibrated low fidelity model and the high fidelity code .",
    "lastly , notice that @xmath21 is a function of not only the design variables - as in kennedy and ohagan ( 2001 ) - but also @xmath22 .",
    "the calibration parameters are included in this discrepancy term because they can be modified in the high fidelity code .",
    "therefore , this discrepancy term captures the systematic differences in the outputs from the two models over values of the design variables and the changes in the calibration inputs @xmath23 and @xmath24 .",
    "in addition to the simulations , there are also field observations that are used to inform predictions .",
    "since the higher fidelity simulator is assumed to better represent the physical process than the low fidelity simulator , it is natural to model the field observations with the simulator of higher fidelity .",
    "similar to kennedy and ohagan ( 2001 ) , a discrepancy function , @xmath25 , is used to capture the systematic inadequacy of the high fidelity simulator .",
    "the field observations are noisy versions of the mean process , and independent and identically distributed ( iid ) observational errors are included in our specification . for input setting , @xmath0 , the field process is written as : @xmath26 where @xmath27 . substituting ( [ eqn : sim2withsim1 ] ) into ( [ eqn : fieldsim2 ] ) allows the field observations to be written:@xmath28    so , the response surface for the field data is written as the sum of the calibrated low fidelity simulator , the calibrated discrepancy between the two different simulators , the discrepancy between the high fidelity model and the data , and observational error . from here on out , we describe the response surfaces for the low and high fidelity simulators and the field data using the framework described in equations ( [ eqn : lfsim ] ) , ( [ eqn : sim2withsim1 ] ) and ( [ eqn : fieldfinal ] ) , respectively .",
    "it is possible at this point to envision applications with more than two simulators , each ranked from low to high by levels of fidelity .",
    "the above framework can be extended to these cases .",
    "for example , in the case where there are three simulators of different fidelity , @xmath29 , @xmath30 and @xmath31 , where @xmath29 is of the lowest fidelity and @xmath31 is best at describing the physical process .",
    "outputs from @xmath29 and @xmath30 are modelled as above .",
    "similarly , @xmath31 will be modelled using @xmath29 and some discrepancy functions , and the field observations will be modelled with the highest fidelity simulator @xmath31 .",
    "more on this in section [ sec : discussion ] .      to make predictions of the physical system ,",
    "response surfaces for the computer model and discrepancies must be estimated .",
    "we follow the common practice of emulating simulator responses using a gp ( e.g. , see sacks et al . ,",
    "the reason for this , in general , boils down to the success of the gp as a non - parametric regression estimator and also the ability of the gp model to provide a basis for statistical inference for the outputs of deterministic computer codes . from a bayesian viewpoint in this context",
    ", one can think of the gp as a prior distribution over the class of functions produced by the low fidelity simulator and the discrepancies , respectively .",
    "we begin by first considering the specification for the low fidelity simulator .",
    "the outputs are treated as a realization of a random function of the form : @xmath32 where @xmath33 are regression functions , @xmath34 is the vector of unknown regression coefficients , and @xmath35 is a mean zero gp .",
    "we follow the convention of most computer model applications by specifying the mean function as a constant , @xmath36 , and model the response surface through the covariance structure .",
    "the covariance between observations at inputs @xmath37 and @xmath38 is specified as @xmath39 & = & \\frac{1}{\\lambda_{\\eta_l } } { \\displaystyle\\prod}_{s=1}^{p}\\rho_{\\eta_l , s}^{4\\left(x_{i , s}-x_{j , s}\\right)^2 } { \\displaystyle\\prod}_{s=1}^{m_f } \\rho_{\\eta_l , p+s}^{4\\left(t_{f , i , s}-t_{f , j , s}\\right)^2}{\\displaystyle\\prod}_{s=1}^{m_l } \\rho_{\\eta_l , p+m_f+s}^{4\\left(t_{l , i , s}-t_{l , j , s}\\right)^2}\\nonumber\\\\ & = & \\frac{1}{\\lambda_{\\eta_l } } r\\left(\\left({\\mathbf{x}}_i , { \\mathbf{t}}_{f , i},{\\mathbf{t}}_{l , i}\\right ) , \\left({\\mathbf{x}}_j , { \\mathbf{t}}_{f , j},{\\mathbf{t}}_{l , j}\\right),{\\boldsymbol}\\rho_{\\eta_l}\\right ) , \\label{eqn : eta1cov}\\end{aligned}\\ ] ] where @xmath40 is the marginal precision of the simulator @xmath41 .",
    "the @xmath42-vector @xmath43 @xmath44 is the vector of correlation parameters that govern the dependence in each of the component directions of @xmath0 , @xmath2 and @xmath4 ( e.g. , sacks et al . , 1989 ; higdon et al . , 2004 and 2008 ; linkletter et al . , 2006 ) .",
    "the discrepancy , @xmath21 , captures the systematic differences between the high and low fidelity simulators as a function of the inputs , ( @xmath45 ) , that are adjustable in the high fidelity model .",
    "continuing as above , @xmath21 is modelled as mean zero gp with covariance :    @xmath46 & = & \\frac{1}{\\lambda_2 }   { \\displaystyle\\prod}_{s=1 } ^p \\rho_{2,s}^{4\\left(x_{i , s}-x_{j , s}\\right)^2}{\\displaystyle\\prod}_{s=1}^{m_f } \\rho_{2,p+s}^{4\\left(t_{f , i , s}-t_{f , j , s}\\right)^2 } { \\displaystyle\\prod}_{s=1 } ^{m_h } \\rho_{2,p+m_f+s}^{4\\left(t_{h , i , s}-t_{h , j , s}\\right)^2}\\nonumber\\\\ & = & \\frac{1}{\\lambda_{2 } } r\\left(\\left({\\mathbf{x}}_i , { \\mathbf{t}}_{f , i},{\\mathbf{t}}_{h , i}\\right ) , \\left({\\mathbf{x}}_j , { \\mathbf{t}}_{f , j},{\\mathbf{t}}_{h , j}\\right),{\\boldsymbol}\\rho_{2}\\right ) , \\label{eqn : deltaccov}\\end{aligned}\\ ] ]    where the marginal precision of the discrepancy function is @xmath47 , and the vector of correlation parameters for the inputs is @xmath48 .",
    "the function @xmath25 is the discrepancy between the response from high fidelity simulator and the physical process .",
    "again , a zero mean gp is chosen .",
    "let @xmath49 denote the marginal precision of the discrepancy function , @xmath25 , and the vector @xmath50 be the correlation parameters for the @xmath51 design variables .",
    "the covariance function is written as    @xmath52 & = & \\frac{1}{\\lambda_f } { \\displaystyle\\prod}_{s=1}^p \\rho_{f , s}^{4\\left(x_{i , s}-x_{j , s}\\right)^2}\\nonumber\\\\ & = & \\frac{1}{\\lambda_f } r\\left(\\left({\\mathbf{x}}_i , { \\mathbf{x}}_j\\right),{\\boldsymbol}\\rho_f \\right ) .",
    "\\label{eqn : deltafcov}\\end{aligned}\\ ] ]    we define the vector of all observations and simulation outputs as @xmath53 , where @xmath54 is the vector of @xmath55 field measurements , and the vectors @xmath56 and @xmath57 simulated runs from the high and low fidelity simulators are @xmath58 and @xmath59 , respectively . to simplify notation , denote @xmath60 , @xmath61 and @xmath62 .",
    "the likelihood for @xmath63 is    @xmath64    where @xmath65 is the constant mean vector and    @xmath66    the covariance matrix @xmath67 is the covariance between all the outputs and is obtained by applying equation ( [ eqn : eta1cov ] ) to each pair of the @xmath68 input settings of the observed and simulated data .",
    "similarly , the covariance matrix @xmath69 describes the relationship of the systematic difference between the two simulators and hence , is obtained by applying equation ( [ eqn : deltaccov ] ) to each pair of the @xmath55 input settings of the observed data @xmath70 and @xmath56 input settings of the simulated data @xmath71 .",
    "it is a square matrix of order @xmath72 .",
    "equation ( [ eqn : deltafcov ] ) is only applied to each pair of the @xmath55 input settings for the covariance matrix @xmath73 .",
    "the covariance matrix for the measurement error ( @xmath74 ) is given by the @xmath75 diagonal matrix @xmath76 and @xmath77 is the precision parameter of the observational error .",
    "the posterior distribution of calibration and statistical model parameters , @xmath78 , takes the form    @xmath79    where we abuse notation for prior distributions and denote the prior for @xmath80 , @xmath81 and @xmath82 as @xmath83 , @xmath84 and @xmath85 , respectively . + in practice , we have to estimate @xmath80 , @xmath81 and @xmath82 .",
    "we use markov chain monte carlo ( mcmc ) to sample from the posterior distribution of the parameters , given observations and simulations . in order to simplify the prior specification ,",
    "the responses are standardized to have mean zero and variance 1 ( e.g. , linkletter et al . , 2006 ) .",
    "hence , the prior for the precision of the marginal variance , @xmath40 , is chosen to encourage its values to be close to @xmath86 - the idea being that the low fidelity model should capture much of the signal in the observations .",
    "we use a gamma distribution for the prior for @xmath40 :    @xmath87    when expert knowledge is unavailable , we have found that @xmath88 ( higdon et al . , 2004 )",
    "works reasonably well as the choice centers the prior distribution at 1 with a reasonably large variance , thereby allowing for a fairly broad exploration of the posterior .",
    "similarly , the priors chosen for the remaining precision parameters are also gamma distributions .",
    "we also use the default values proposed by higdon et al .",
    "( 2004 ) for the hyperparameters of priors for the remaining precision parameters .",
    "the default choice of shape and scale parameters are @xmath89 and @xmath90 .",
    "this specification implies a relatively uninformative prior for these precision parameters , which encourages the data to choose a suitable value by itself .",
    "the components in @xmath82 are bounded within the unit interval .",
    "hence , a natural choice of prior for @xmath91 is the beta distribution of the following form :    @xmath92 conventionally , the beta priors are flat and centered at 1 with small variance ( williams et al . , 2006 ) .",
    "this is based on the prior belief that all the inputs are equally uncorrelated to the simulator and allow the data to decide the dependence of the simulator on the different inputs by moving the @xmath93 s away from 1 in the posterior . in our experience , the default choices for these parameters ,",
    "@xmath94 and @xmath90 , suggested by higdon et al .",
    "( 2004 ) and williams et al .",
    "( 2006 ) encourage strong enough dependence in each of the parameters and work well in general .    the posterior distribution for each parameter",
    "is explored using mcmc .",
    "specifically , single - site metropolis updates ( metropolis et al . , 1953 )",
    "are used for the components of @xmath82 and @xmath80 .",
    "proposals are made for each of these parameters from a uniform distribution centered at the parameter s current value .",
    "the widths of the uniform distributions ( one for each component parameter ) are pre - computed by running short mcmc runs and choosing a width that gives an acceptance rate of about 0.44 ( gelman et al . , 2004 ) .",
    "although this adjustment does not guarantee an acceptance probability of 0.44 , we have found this procedure is helpful at choosing widths resulting in acceptance ratios between 0.25 and 0.75 and , more importantly , encourages the mcmc to converge .",
    "good default choices for the widths for the updates can also be found using the method proposed by graves ( 2005 ) . for each of the precision parameters , we used hastings updates ( hastings , 1970 ) , where the proposed value is drawn from a uniform distribution centered at the current parameter value , with a width that is proportional to the parameter s current value .",
    "we have found that a width that is @xmath95 times the current parameter value ( proposed by higdon et al . ,",
    "2008 ) works fairly well in general .      the main goal of this endeavour is prediction .",
    "given the posterior realizations from ( [ eqn : zpostdensity ] ) , predictions of the field measurement , @xmath96 , can be made at a new input setting @xmath97 .",
    "the joint distribution between @xmath63 and @xmath96 , conditional on the parameters @xmath98 , @xmath81 and @xmath82 , is : @xmath99 where the covariance matrix , @xmath100 , is analogous to the covariance in ( [ eqn : zcovmat ] ) - there is an extra row and column in @xmath100 as a result of appending @xmath96 to @xmath63 .    through the usual properties of the multivariate normal distribution ,",
    "the predictive distribution of @xmath96 , conditional on the parameters , is : @xmath101 where @xmath102 and @xmath103 .",
    "the matrices @xmath104 are sub - matrices of @xmath100 where @xmath105the sub - matrix @xmath106 is an @xmath107 matrix , while @xmath108 and @xmath109 are of dimension @xmath110 and @xmath111 , respectively .",
    "the remaining sub - matrix , @xmath112 , is a scalar .    to make predictions",
    ", we first sample a vector of parameters from ( [ eqn : zpostdensity ] ) .",
    "next , conditional on the sampled parameters , a prediction is sampled from ( [ eqn : ypred ] ) .",
    "the sampling of parameters and predictions are repeated many times to provide estimated posterior quantities ( e.g. , posterior mean , variance or prediction intervals ) .",
    "in this section , two examples are presented .",
    "the first example is a simple computer model that is used to demonstrate the proposed approach . after illustrating the implementation of our approach and some diagnostics to assess the adequacy of the model fit",
    ", a small simulation study is carried out to investigate the predictive performance of the proposed methodology .",
    "the second example is the application that motivated this work , and involves a radiative shock experiment conducted at crash .",
    "the main goal is to predict the observed field measurements given the outputs from two computer models and some field trials .",
    "we begin with the  toy \" example in bastos and ohagan ( 2009 ) , with some slight alterations .",
    "that is , the setting has been modified to accommodate two computer models and field experiments .",
    "in addition , we refashion the computer model to include two design variables , a common calibration parameter and calibration parameters that exist in each computer model , respectively . for simplicity ,",
    "all the input settings and calibration parameters are chosen from the unit interval .",
    "we specify the low fidelity model as : @xmath113 the high fidelity model is defined as the low fidelity response model plus a discrepancy term : @xmath114    to illustrate the proposed approach , we simulate outputs from the respective models .",
    "following loeppky et al .",
    "( 2009 ) , we used a 40 run random latin hypercube design ( mackay et al . , 1979 ) for the low fidelity simulator .",
    "since , in practice , the high fidelity model is likely to be more computationally expensive than the low fidelity model , only 10 runs are generated  also chosen using a random latin hypercube design .    in most computer model applications",
    ", there are relatively few field observations .",
    "consequently to mimic this setting , only 3 field observations were simulated from the mathematical model : @xmath115 where @xmath116 .    in this",
    "set - up , the true value of the common calibration parameter is chosen to be @xmath117 , while the calibration parameter appearing only in the high and low fidelity models are chosen to be @xmath118 and @xmath119 , respectively .",
    "figure [ fig : surfaces ] displays the response surfaces for the two computer models and also the mean response surface for the field process .",
    "a quick glance at the figure reveals that the high fidelity model appears closer to the mean process than the low fidelity model .",
    "this represents the framework we are working within insofar as the high fidelity model is expected to be more like the true system than the low fidelity model .    )",
    "-([eqn : toydeltaf ] ) . ]    the posterior distribution of the model parameters was sampled using mcmc as outlined in section [ sec : hyperprior ] .",
    "the mcmc chain is initialized with @xmath120 ( i.e. , the centre of the input space ) , @xmath121 , @xmath122 and all the correlation parameters , @xmath82 are chosen to be @xmath123 as we assume that the simulator and discrepancy terms are dependent on all the inputs . through visual inspection of the traceplots ( not shown ) , we found that , for the data encountered in this example , convergence is achieved in the first 1,000 steps are so .",
    "the mcmc was run for 10,000 steps , where the first 2,000 steps are treated as burn - in and discarded in further analysis .",
    "in addition to the data simulated from ( [ eqn : toyeta ] )  ( [ eqn : toydeltaf ] ) used to fit the proposed model ( i.e. , the training set ) , a validation dataset was generated from ( [ eqn : toydeltaf ] ) , so that the predictive performance can be evaluated .",
    "the validation set consisted of 25 field observations with input settings , @xmath0 , chosen using random latin hypercube sampling .",
    "we use the posterior mean prediction at @xmath0 to estimate @xmath124 .",
    "figure [ fig : samplepred ] shows the predicted versus actual values for each of the validation points .",
    "the figure shows that the predictive model performs reasonably well since the points center around the @xmath125 line .",
    "line ) ]    figure [ fig : predvsstuff ] displays the deviations of the predictions from the true values plotted against the predictions and also the input settings in each dimension . in each case , no obvious pattern is found in the plots , suggesting the outputs have similar degree of smoothness across the input space and that no obvious systematic behaviour was unaccounted for .    ;",
    "( c ) prediction error against @xmath126 . ]",
    "while not the specific goal of the proposed methodology , we now consider the estimation of the calibration parameters .",
    "figure [ fig : toytheta ] shows the estimated one - dimensional and two - dimensional marginal posterior distributions of the calibration parameters .",
    "solid vertical lines are plotted at the true values of the calibration parameters . in general , these posterior distributions can be interpreted as representing the uncertainty in the calibration parameters given the very limited number of observations and small numbers of simulations from imperfect computer models .",
    "a quick glance at the plots reveals that , except for @xmath127 , the calibration parameters are not being constrained by the data .",
    "it is not too surprising that we can constrain @xmath127 , but not the calibration other parameters , since there are more outputs ( comparisons between the low and high fidelity models ) to inform this parameter .",
    "the inability to constrain the other calibration parameters is due to the presence of the discrepancy terms @xmath25 , and the dearth of data .",
    "figure [ fig : toy40theta ] shows the estimated posterior distributions of the calibration parameters for different sample sizes .",
    "panels ( a ) , ( b ) and ( c ) are the results of the simulations with ( a ) @xmath128 , @xmath129 , ( b ) @xmath130 and ( c ) @xmath131 .",
    "the first case was chosen as a more simulation rich version of the above example .",
    "comparing panel ( a ) of figure [ fig : toy40theta ] with the results in figure [ fig : toytheta ] , we see that the mode of the posterior distribution of @xmath127 is closer to the true value ( solid line ) and there is less variability in the posterior distribution when there are more simulations .",
    "however , very little is learned about the calibration parameters @xmath132 and @xmath133 .",
    "to gain more information on these parameters , there needs to be more field observations .",
    "panels ( b ) and ( c ) consider cases where the number of simulations and field trials is larger than before . as the number of observations and simulations increases ,",
    "the model is able to better estimate the calibration parameters .",
    "an interesting observation is that the shared calibration parameter @xmath133 is better constrained in panel ( b ) than @xmath132 .",
    "the reason for this , we surmise , is that given the same number of field trials both the low and high fidelity models help inform @xmath133 , but only the high fidelity model directly informs @xmath132 . when there are relatively many simulations and observations , all of the calibration parameters tend to be well constrained ( panel ( c ) ) .",
    "a subsequent simulation study is performed to compare predictions of the new model with approaches that only use some of the simulations .",
    "models d1 and d2 are implementations of the kennedy and ohagan ( 2001 ) approach using only the low fidelity and only the high fidelity outputs , respectively .",
    "predictions from these models are compared with those from model d3 , the proposed methodology . in other words",
    ", we are investigating whether the proposed approach of combining all simulations and observations is better in some sense than the kennedy and ohagan ( 2001 ) method using one of either the low fidelity model or high fidelity model outputs alone .",
    "the simulation study is carried out as follows .",
    "using random latin hypercube sampling , 100 sets of training and validation data are first generated independently .",
    "each training set contains the same number of outputs as the above : 40 simulated values from the low fidelity simulator , 10 computer runs from the high fidelity simulator and 3 field observations . for each simulated training set , models d1 , d2 and d3 are estimated , and predictions of the validation set are obtained from each model .",
    "the predictions are evaluated by computing the root mean squared prediction errors ( rmspe ) for the validation data .",
    "this is done for each of the 100 simulated training and validation datasets .",
    "the simulation study results are summarized in figure [ fig : toyboxplot ] .",
    "figure [ fig : toyboxplot ] reveals that the rmspe from the proposed model is consistently smaller than rmspe of the other two models .",
    "interestingly , in panel ( a ) , we notice that the rmspe is larger for the high fidelity model than the low fidelity model .",
    "this is the result of having relatively few runs of the high fidelity code .",
    "looking at figure [ fig : toy20boxplot ] , when @xmath128 and @xmath129 , prediction using the higher fidelity outputs does better than prediction using only the low fidelity outputs . in either case , the proposed approach that uses all sources of data tends to do better in terms of rmspe .    in general",
    ", we found that the proposed model that makes use of all the simulations works well in making predictions for the physical system .",
    "the simulation demonstrates that more efficient estimation is gained through this approach .",
    "although calibration is not the priority , we come across a similar issue encountered by kennedy and ohagan ( 2001 )  calibration is difficult with limited amounts of data .",
    "however , as the number of outputs increases , more information is available to calibrate the parameters of interest . in the case of calibration ,",
    "it is important to note what is being achieved .",
    "that is , the posterior distributions reflect the uncertainty in the calibration parameters given the observations and the imperfect computer models .      the application that motivated the proposed methodology arises from radiative shock experiments at crash .",
    "figure [ fig : tube ] gives a diagram of the system that we want to predict . in the physical experiments , a high energy laser pulse irradiates a thin disk of beryllium at the front end of a xenon filled tube",
    "the energy deposited in the surface causes the beryllium to ablate . a shock wave is then driven by the ablation pressure through the beryllium disk . after the shock wave breaks out of the beryllium disk , the disk acts as a piston , propagating the shock at a high speed into the xenon .",
    "when the xenon is shocked , it is heated to temperatures well over 100,000 @xmath134k and emits thermal x - ray radiation .",
    "these shocks are considered radiative when the radiation energy flux from the shock is high enough to impact the structure of the shock wave .",
    "details regarding the radiative shock physics can be found in drake et al .",
    "the radiating shock experiments that we are concerned with can be viewed as small - scale experiments for understanding astrophysical shock waves and other high temperature phenomena ( mcclarren et al . , 2011 ; drake et al . , 2011 ) .",
    "several measurements of interest are taken from each shock experiment and also simulations .",
    "we focus here on the time taken for the shock wave to exit the beryllium disk ( breakout time ) .        using two different radiation - hydrodynamics codes ( 1d - crash and 2d - crash )",
    ", we aim to predict the shock breakout time .",
    "the 2d - crash code includes two - dimensional processes and interactions that the one - dimensional code , or 1d - crash , does not . as a result",
    ", the 2d - crash model is assumed to be able to model the experiments better than the 1d - crash code , but it is also more computationally expensive .",
    "the design variables for this experiment are the thickness of the beryllium disk ( @xmath135 ) and laser energy ( @xmath126 ) .",
    "the electron flux limiter is calibration input to both computer models and is denoted as @xmath136 .",
    "the laser energy scale factor is an additional calibration parameter , @xmath137 , required by the 1d - crash code but not the 2d - crash simulator .",
    "the high fidelity computer code has two calibration inputs ",
    "beryllium gamma ( @xmath138 ) and wall opacity scale factor ( @xmath139 ) .",
    "all the inputs are scaled to the unit interval before fitting the data to the proposed model .",
    "we have 365 simulations from 1d - crash and 104 2d - crash runs available .",
    "the designs for each computer experiment were latin hypercube designs , optimized using a space - filling criterion ( johnson et al . , 1990 ) .",
    "there are also 8 experiments that were conducted at the omega laser facility at the university of rochester where the breakout time was recorded ( boehly et al . , 1997 ) .",
    "the mcmc was set up as in the previous examples , with one exception . from previous usage of the laser",
    ", it was known that the variance of the observation error was about @xmath140 seconds - or approximately @xmath86 after standardizing .",
    "a gamma distribution with shape and scale parameter @xmath141 was chosen for the prior of @xmath77 .",
    "this is an informative prior that tightly centers the gamma distribution at 1 .",
    "the widths for the metropolis updates are chosen as outlined in section [ sec : hyperprior ] .",
    "we found that convergence was achieved shortly after 1,000 mcmc steps .",
    "so , the mcmc was run for 10,000 steps and the first 2,000 were discarded as burn - in .",
    "similar to the previous example , the deviations of the predictions from the observed breakout times are plotted against the predictions and the two input settings ( diagnostic plots not shown ) .",
    "no obvious pattern is found in any of the diagnostic plots , thereby suggesting that the model fit is adequate .",
    "a leave - one - out study is conducted to evaluate the predictive ability of the new approach .",
    "that is , we delete an observation , fit the proposed model and predict the deleted observation .",
    "this is done for each of the 8 observations .",
    "figure [ fig : predloo ] is a plot of the resulting predictions against the observed breakout time .",
    "the 95% posterior prediction interval for each point is shown in the figure .",
    "the predictions are fairly close to the observed values and , thus most points are near to the @xmath125 line",
    ". however , the second observation from the left gives a prediction interval that fails to capture the observation .",
    "@xmath142    plots of the marginal posterior distributions of the calibration parameters are shown in figure [ fig : thetahist ] .",
    "the posterior distributions for all the calibration parameters , except the energy scale factor , are not constrained in this application",
    ". however , the posterior distributions have clear modes to suggest plausible values for the calibration parameters .",
    "so far , we have focused on the setting where there are only two computer models .",
    "the new methodology , however , can easily be extended to model applications that involve more than two simulators . here , such extensions , as well as limitations of this model , are mentioned .",
    "suppose that there are @xmath143 simulators denoted as @xmath144 for @xmath145 , where @xmath146 is the next highest level of fidelity model from @xmath147 .",
    "the simulators share the same design variables , @xmath0 , and some common calibration parameters , @xmath2 .",
    "the additional calibration parameters required by each of the respective computer model are denoted as @xmath148 , for @xmath145 .",
    "the outputs of the lowest fidelity computer model are denoted as : @xmath149    the outputs from the higher fidelity simulators can then be written as a combination of the lowest fidelity simulator and discrepancy functions that capture the systematic differences between each pair of simulators . for @xmath150 ,",
    "the simulated outputs are written : @xmath151    field measurements are also available to build the predictive model .",
    "the experimental observations are represented with the highest fidelity simulator and are written as the sum of the low fidelity simulator and discrepancy functions : @xmath152 where @xmath153 measures the discrepancy between the highest fidelity computer model and physical process .",
    "the response surfaces of the different sources of data are modelled with gps with mean and covariance functions discussed in section [ sec : gpprior ] .",
    "some care should be taken in the prior specification for the precision parameters for the gps .",
    "we have found that the default choices of prior distributions outlined in section [ sec : gpprior ] work fine in most cases ( e.g. , the simulations in section [ sec : toyeg ] ) .",
    "however , for some datasets , extremely large values of @xmath77 are observed .",
    "this amounts to essentially a model with no measurement error and discrepancies that are interpolating the noise .",
    "we noticed the phenomenon when the default priors are used for the crash example .",
    "this can also happen with the model proposed by kennedy and ohagan ( 2001 ) . in our case",
    ", we avoided this problem because we had a more informative prior distribution for @xmath77 .",
    "alternatively , one can address this issue by rejecting small values of a precision parameter in the mcmc ( this was done in higdon et al .",
    "( 2004 ) ) , or at the design stage by taking replicate field observations .    a further note of caution with respect to the experimental design . the design regions for the computer experiments should coincide to avoid uncertainty due to extrapolation in the discrepancies between models .",
    "suppose for example , the design for @xmath2 in the low fidelity simulator explores a much larger region than the design for the high fidelity model .",
    "when predictions are made , the proposed approach averages over the posterior distribution of the calibration parameters . for values of @xmath154 from the posterior that are outside of the range explored by the design of the high fidelity model , the proposed approach extrapolates @xmath21 .",
    "this results in larger prediction intervals .",
    "lastly , we do not address problems with design variables that appear in only some , but not all , simulators .",
    "this is a topic for future work .",
    "a new methodology , which combines outputs from multi - fidelity computer models and field observations , is proposed .",
    "the approach successfully uses a bayesian hierarchical model to make predictions of the physical system with associated measurements of uncertainty ( e.g. , posterior variance or prediction intervals ) .",
    "different gps are used to model the various response surfaces .",
    "the real example that motivated this work used two simulators of the process , but methodology can be easily extended to cases with more than two simulators .",
    "boehly , t. r. , brown , d. l. , craxton , r. s. , keck , r. l. , knauer , j. p. , kelly , j. h. , kessler , t. j. , kumpan , s. a. , loucks , s. j. , letzring , s. a. , marshall , f. j. , mccrory , r. l. , morse , s. f. b. , seka , w. , soures , j. m. and verdon , c. p. ( 1997 ) . `` initial performance results of the omega laser system '' _ optics communications _ , * 133 * , 495 - 506 .",
    "craig , p. s. , goldstein , m. , seheult , a. h. and smith , j. a. ( 2001 ) .",
    "`` constructing partial priors specifications for models of complex physics systems '' .",
    "_ journal of the american statistical association _ , * 96 * , 717 - 729 .",
    "drake , r. p. , doss , f. w. , mcclarren , r. g. , adams , m. l. , amato , n. , bingham , d. , chou , c. c. , distefano , c. , fidkowski , k. , fryxell , b. , gombosi , t. i. , grosskopf , m. j. , holloway , j. p. , van der holst , b. , huntington , c. m. , karni , s. , krauland , c. m. , kuranz , c. c. , larsen , e.,van leer , b. , mallick , b. , marion , d. , martin , w. , morel , j. e. , myra , e. s. , nair , v. , powell , k. g. , rauchwerger , l. , roe , p. , rutter , e. , sokolov , i. v. , stout , q. , torralva , b. r. , toth , g. , thornton , k. and visco , a. j. ( 2011 ) .",
    "`` radiative effects in radiative shocks in shock tubes '' . _ high energy density physics _ , * 7 * , 130 - 140 .",
    "higdon , d. , kennedy , m. , cavendish , j. , cafeo , j. and ryne , r. d. ( 2004 ) .",
    " combining field data and computer simulations for calibration and prediction \" . _ siam journal of scientific computing _ , * 26 * , 448 - 466 .",
    "higdon , d. , gattiker , j. , williams , b. and rightley , m. ( 2008 ) .",
    " combining field data and computer simulations for calibration and prediction \"",
    ". _ journal of the american statistical association _ , * 103 * , 570 - 583 .",
    "mcclarren , r.g . ,",
    "ryub , d. , drake , p. , grosskopf , m. , bingham , d. , chou , c - c .",
    ", fryxell , b. , van der holst , b. , holloway , j.p .",
    ", kuranz , c.c . ,",
    "mallick , b. , rutter , e. and torralva , b. ( 2011 ) .",
    " a physics informed emulator for laser - driven radiating shock simulations. _ reliability engineering and system safety _ , * 96 * , 1194 - 1207 .",
    "qian , z. g. , seepersad , c. c. , joseph , v.r . ,",
    "allen , j.k . and wu , j. c. f. ( 2006 ) .",
    "`` building surrogate models based on detailed and approximate simulations '' . _ asme journal of mechanical design _ , * 128 * , 668 - 677 .",
    "rougier , j. , sexton , d. m. h. , murphy , j. m. and stainforth , d. ( 2009 ) .",
    "`` analyzing the climate sensitivity of the hadsm3 climate model using ensembles from different but related experiments '' .",
    "_ journal of climate _ ,",
    "* 22 * , 3540 - 3557 .",
    "williams , b. , higdon , d. , gattiker , j. , moore , l. , mckay , m. and keller - mcnulty , s. ( 2006 ) .",
    " combining experimental data and computer simulations , with an application to flyer plate experiments \" .",
    "_ bayesian analysis _ ,",
    "* 1 * , 765 - 792 ."
  ],
  "abstract_text": [
    "<S> computer codes are widely used to describe physical processes in lieu of physical observations . in some cases , more than one computer simulator , each with different degrees of fidelity , </S>",
    "<S> can be used to explore the physical system . in this work , </S>",
    "<S> we combine field observations and model runs from deterministic multi - fidelity computer simulators to build a predictive model for the real process . </S>",
    "<S> the resulting model can be used to perform sensitivity analysis for the system , solve inverse problems and make predictions . </S>",
    "<S> our approach is bayesian and will be illustrated through a simple example , as well as a real application in predictive science at the center for radiative shock hydrodynamics at the university of michigan . + key words : computer experiment ; gaussian process ; markov chain monte carlo . </S>"
  ]
}