{
  "article_text": [
    "we consider the problem of binary classification , where we are given a sample @xmath1 of @xmath2 i.i.d points , @xmath3 , @xmath4 , and we are required to learn a classifier @xmath5 using @xmath1 .",
    "binary classification is a well studied problem in machine learning  @xcite .",
    "one of the simplest classification algorithm is the k - nearest neighbour ( knn ) algorithm .",
    "the knn algorithm takes a majority vote over the @xmath6 nearest neighbours of a test point @xmath0 in order to determine the label of @xmath0 .",
    "the knn algorithm , among others , belongs to the class of local learning algorithms that take into account only the local information around the test point @xmath0 in deciding the label of @xmath0 .",
    "cortes and vapnik  @xcite introduced the celebrated support vector machine ( svm ) , which learns a decision boundary by maximizing the margin .",
    "zhang et al .",
    "@xcite , and blanzieri  et al .",
    "@xcite independently proposed a classification algorithm called _ local support vector machines _",
    "( lsvms ) and applied it to remote sensing and visual recognition tasks respectively .",
    "lsvms exploit locality , like knn , along with the idea of large margin classification , to learn a global non - linear classifier , by learning an svm locally at each test point . by using a large margin approach lsvms",
    "inherit large margin classifier s robustness to data perturbation . by utilizing only local information ,",
    "lsvms avoid relying on the global geometry of the distribution .",
    "this is a good strategy when our data lies on a manifold , where the geodesic distance between close points is approximately euclidean , but the same is not true for points far away .",
    "a prime example is the task of image recognition .",
    "segata et al .",
    "@xcite compared their implementation of approximate lsvm with a standard rbf svm in libsvm for the 2-spirals dataset .",
    "the 2-spirals dataset is a 2-dimensional dataset where the data lives on a manifold .",
    "the accuracy of lsvm was 88.47% whereas that of svm was only 85.29% , and that of knn was 88.43% .",
    "another scenario where local learning is more beneficial than global learning is when data is multimodal and/or heterogeneous .",
    "for example , suppose given census data we are required to classify people as belonging to the high income group ( hig ) or the middle income group ( mig ) .",
    "a global hig vs mig classifier might be hard to build since the notion of hig / mig changes with states / counties .",
    "however , it is better to utilize local information , such as information from a particular county , to build multiple local classifiers .",
    "our global classifier is then a collection of many such local classifiers.the problem of webspam detection is another example where data is heterogeneous .",
    "a page might be webspam for one category but may not be for another . in such cases in order to categorize a webpage as spam or not , it is easier to build multiple local classifiers rather than a single global classifier .",
    "a numerical illustration was provided by cheng et al .",
    "@xcite on a one - vs - all classification problem on the covtype dataset , their implementation of lsvm registered an accuracy of about 90% , whereas the accuracy of rbf svm was only 86.21% and that of knn was 67.40% . since a one - vs - all classification problem makes the dataset multimodal and heterogeneous ( due to grouping of multiple classes as one single class ) , the superior performance of lsvms over svms illustrates the power of local learning in such settings .",
    "a practical advantage of lsvms is that they can exploit fast algorithms for range search  @xcite , and various other approximation techniques  @xcite , along with parallel computing architectures in order to learn on large datasets .",
    "svms are well understood both theoretically and practically  @xcite .",
    "however , no theoretical understanding yet exists for lsvms .",
    "the empirical success of lsvms begs a theoretical understanding of such techniques .",
    "our work is the first attempt to provide a theoretical understanding of lsvms .",
    "our contributions are as follows :    we provide a formulation of lsvms , which generalizes previous formulations due to blanzieri et al .",
    "@xcite , and zhang et al .",
    "@xcite , and provide the first statistical analysis of locally linear support vector machines ( llsvms ) , the simplest kind of lsvms .",
    "our formulation ( section  ) is similar to cheng et al .",
    "@xcite but is more directly motivated from local polynomial regression , and hence the role of a smoothing kernel in our formulation , is much more cleaner than their use of an unspecified weight function .",
    "our formulation makes explicit the direct connections to local polynomial fitting via the use of polynomial mercer kernels .",
    "this allows us to view lsvms as approximating the decision boundary locally using smooth functions , a novel interpretation .",
    "in theorem   ( section  ) we provide sufficient conditions , which guarantee that , for any given point @xmath0 , the prediction of llsvms at @xmath0 matches that of the bayes classifier , establishing pointwise consistency .",
    "these are conditions on the distribution and the model parameters @xmath7 .",
    "the llsvm problem at any point @xmath0 minimizes the sample version of the stochastic objective : @xmath8 . in theorem   ( section  )",
    "we provide a high probability bound of @xmath9 for the difference between the smallest value of this stochastic objective and , its value at the solution obtained by solving the llsvm optimization problem .",
    "this result tells us how quickly the stochastic objective , at the solution of the llsvm problem , converges to its true minimum value .",
    "define the l risk of any function @xmath10 as @xmath11 .",
    "in theorem   ( section  ) we establish an upper bound on the gap between the @xmath12 risk of the global function learnt by llsvms , with bandwidth @xmath13 , and regularization @xmath14 , and the empirical @xmath12 risk of llsvms , via uniform stability bounds .",
    "this gap decays as @xmath15 .",
    "notice that while theorems  , are pointwise results , theorem   involves the global classifier learnt by solving the llsvm problem at each training point @xmath16 with parameters @xmath7 .",
    "hence this is a `` global '' result .",
    "theorems  ,   suggest that llsvms should work well in low dimensions , or if the data lies in a low - dimensional manifold .",
    "this justifies the empirical findings of segata et al . , and",
    "cheng et al .",
    "our formulation for llsvms is directly motivated from a certain technique in nonparametric regression called local linear regression ( llr )  @xcite . in llr one",
    "fits a non - linear regression function by fitting a linear function locally at each point .",
    "the idea of local linear fit is inspired by the fact that any differentiable function can be well approximated locally via linear functions .",
    "hence llr locally approximates the underlying regression function with a linear function .",
    "llsvms adopt a similar approach by making local linear fits to the underlying non - linear decision boundary . in order to classify an unseen point @xmath0 , llsvms",
    "solve the problem @xmath17 where @xmath18 is a smoothing kernel with bandwidth @xmath13 and @xmath19 is a lipschitz , convex upper bound to the 0 - 1 loss . in this paper we will be concerned with the hinge loss @xmath20 , which is used in svms .",
    "some popular examples of smoothing kernels are the epanechnikov kernel , the rectangular kernel  @xcite . replacing the term @xmath21 in equation   with the term @xmath22 , where @xmath23 is a kernel map induced by a mercer kernel",
    ", we get lsvms .",
    "@xmath24 our formulation of lsvms as shown in equation   strictly generalizes the formulation of both blanzieri  et al . and",
    "zhang et al .. strictly speaking , their algorithm used a rectangular smoothing kernel with the bandwidth equal to the distance of the @xmath25 nearest neighbour of the test point @xmath0 in the training set . in comparison",
    "our formulation uses a smoothing kernel that allows the formulation to down - weight points in a smooth fashion . the vector @xmath26 that is learnt by solving the optimization problem",
    "is used for classification at @xmath0 only .",
    "hence , unlike linear svms , llsvms are still non - linear as the linear fits are only local , and the smoothing kernel precisely determines the locality at each @xmath0 . to see a simple example of the influence of a smoothing kernel , consider llsvms with the hinge loss .",
    "standard primal - dual calculations yield @xmath27 where @xmath28 are the dual variables .",
    "if one uses a finite tailed smoothing kernel such as an epanechnikov kernel or a rectangular kernel , then the points @xmath16 which are outside the bandwidth of the kernel , i.e. @xmath18=0 , have no effect on @xmath26 .",
    "hence , the resulting llsvm does not care about these points and tries to maximize the margin in the input space using points that are close to @xmath0 .",
    "mercer kernels , that arise out of the kernel map @xmath29 , on the other hand have nothing to do with locality .",
    "instead they allow us to fit non - linear functions .",
    "if one uses a polynomial mercer kernel of degree @xmath30 , in conjunction with a smoothing kernel , then it is equivalent to making local degree @xmath30 approximations to the boundary function .",
    "while such approximations are potentially more powerful than local linear approximations , one would require stronger conditions such as existence of higher order derivatives of the decision boundary , to justify local polynomial approximations . to avoid making such strong assumptions we shall focus on llsvms in this paper .",
    "the formulation of cheng et al .",
    "@xcite is similar to the optimization problem  , but uses an unspecified weight function , @xmath31 , in the place of @xmath18 . while the importance of the smoothing kernel , and its interaction with mercer kernels has been distilled in our formulation , the importance and impact of the weight function in the formulation of cheng et al .",
    "was not done clearly .    * related work . * kernel based rules ( kbrs ) have been proposed as a nonparametric classification method ( see chapter 10 in  @xcite ) and are essentially a simplified version of llsvms .",
    "kbrs predict the label of a point @xmath0 as @xmath32 .",
    "this can be seen as using equation   but with all @xmath28 s set to a constant .",
    "however , for llsvms these @xmath33 values themselves depend on the training data , and hence results from the kbrs literature do not transfer to our case .",
    "learning multiple local classifiers has also been done by first clustering the data and then learning a classifier in each of these clusters  @xcite , or by using a baseline classifier  @xcite to find regions where the classifier commits errors and then learning a dedicated classifiers for each of these erroneous regions .",
    "all these algorithms are different from llsvms as they learn a finite mixture of local classifiers from the training data and classify the test point as per the appropriate mixture component .",
    "in contrast llsvms learn a local classifier on demand for each test point .",
    "ensemble methods also learn multiple classifiers and combine them to learn a global model .",
    "however , the classification model is fixed and does not change from one test point to another .    * notation . * @xmath34{\\mbox{$\\;\\stackrel{\\mbox{\\tiny\\rm def}}{=}\\;$}}\\{1,\\ldots , n\\}$ ] .",
    "let @xmath35 denote a @xmath30 dimensional ball of radius @xmath13 centered around @xmath36 .",
    "also , let @xmath37 .",
    "throughout the paper we shall use @xmath38 to denote a vector learned by using the training data set with a 1 appended to each training point in the data set as the @xmath39 dimension . if @xmath40 then @xmath41 where @xmath42 . denote by @xmath43 an arbitary measurable function .",
    "finally since most of our results are `` pointwise results '' , we shall use @xmath0 to represent an arbitrary point , and all `` local quantities '' will be defined w.r.t .",
    "we now state the assumptions and our first main result .",
    "a0 : the domain @xmath44 is compact , @xmath45 for all @xmath36 in @xmath46 , and the marginal distribution on @xmath46 is absolutely continuous w.r.t .",
    "the lebesgue measure .",
    "a1 : let @xmath47 denote the class of functions that are at least once differentiable on @xmath46 .",
    "we assume that @xmath48\\in c^1 $ ] , and as a result @xmath49 . such smoothness assumptions ( and stronger ones )",
    "have been used to study minimax rates for classification in  @xcite .",
    "the impact of a1 is two fold .",
    "firstly the minimizer of @xmath12 risk is a function of @xmath50 . for hinge loss",
    "this function is @xmath51 .",
    "the same holds true even for `` local '' versions of @xmath12 risk and the @xmath52 risk . since @xmath53",
    ", one can invoke continuity arguments , to guarantee a small enough radius @xmath13 , where the minimizer of the @xmath12 risk is a smooth function .",
    "hence , one can restrict the search for an optimal function to @xmath47 .",
    "the definition of such local quantities is done in section  .",
    "a2 : @xmath54 is a finite tailed smoothing kernel function that satisfies @xmath55 ( positive kernel ) , @xmath56 for all @xmath57 , vanishes for all @xmath58 , and @xmath59 for all @xmath60 .",
    "assumption a2 are standard assumptions from the nonparametric estimation literature  @xcite .",
    "the finite tail assumption of the smoothing kernel simplifies proofs and should be easy to relax .",
    "a3 : for all @xmath0 in @xmath46 , @xmath61=0 $ ] .",
    "a3 allows us , in the limit , to approximate the minimum local @xmath12 risk using only linear functions , and therefore allows us to model non - linear decision boundaries via locally linear fits .",
    "a4 : let @xmath62 be the region of intersection of a halfspace and @xmath63 , such that @xmath64 . then a.s .",
    "@xmath65 , @xmath66 .",
    "a4 requires that the mass in @xmath63 for small @xmath13 is spread out and is not all located in a small region in @xmath63 .    as a simple example , consider the setup where the marginal distribution has uniform density on @xmath67 $ ] , and the kernel function is the epanechnikov kernel . under this setting , for any @xmath68",
    ", we get @xmath61=\\lim_{\\sigma\\rightarrow 0}\\frac{3}{8\\sigma}\\int_{x_0-\\sigma}^{x_0+\\sigma}|x - x_0|(1-(\\frac{x - x_0}{\\sigma})^2)~\\mathrm{d}x\\leq \\lim_{\\sigma\\rightarrow 0}\\frac{3\\sigma}{16}=0 $ ] .",
    "the same result applies even for @xmath69 .",
    "hence assumption a3 is satisfied . to verify the validity of a4 ,",
    "it is enough to see that @xmath70 } \\frac{3}{4\\sigma}\\int_{x_0-\\sigma}^{x_0+\\theta}(1-(\\frac{x - x_0}{\\sigma})^2)~\\mathrm{d}x=1/4 $ ] .",
    "hence @xmath71 .",
    "finally , we shall work with only the hinge loss .",
    "hence , whenever we refer to @xmath12 risk we basically mean the risk due to the hinge loss .",
    "we are now in a position to state our first result regarding pointwise consistency of llsvms .",
    "[ thm : main ] given an @xmath72 , if assumptions a1-a4 hold , then there exists an @xmath73 such that an llsvm that solves the problem   at @xmath0 , agrees with the bayes classifier at @xmath0 , for all @xmath74 , and appropriate @xmath75 that satisfy @xmath76 , such that @xmath77 for some @xmath78 .",
    "theorem   provides us with conditions on @xmath79 to guarantee that the learnt llsvm makes a bayes consistent decision at an arbitrary test point @xmath0 . like kbr , svms , and lpr , our results require @xmath2 to grow and @xmath7 to decay at certain rates that are precisely captured by theorem  . in classification ,",
    "global consistency results  @xcite are proved which demonstrate that that the 0 - 1 risk of the classifier converges to that of a bayes classifier asymptotically .",
    "such global consistency results are asymptotic in nature . in comparison",
    "we prove that , at an arbitrary @xmath0 , we can choose sufficiently large amount of data , and appropriate parameter settings @xmath7 ( depending on @xmath2 ) such that the llsvms decision matches that of the bayes classifier at @xmath0 . for these reasons",
    "it seems inappropriate to compare consistency results of svms with those for pointwise consistency of llsvms .",
    "proving a global consistency result for llsvms remains an open problem , that we intend to tackle in the future . in the case of lpr , however , pointwise properties has been investigated  @xcite , such as how quickly the squared loss of an lpr estimator at point @xmath0 converges to the squared loss of the true function . here",
    "we are guaranteed that as @xmath80 , and with appropriate @xmath13 , and with any degree of the polynomial , the excess error at @xmath0 converges to 0 .",
    "however , as stated above , we can prove that we can predict the label of @xmath0 correctly with a finite amount of data .",
    "it is inappropriate to compare the results of lpr and llsvms , since lpr requires prediction of a real valued quantity , whereas llsvms are concerned with prediction of a binary label . as we mentioned in the related work section , the proof strategy that was used to prove",
    "the consistency of kernel based rules does not work for our case .",
    "techniques from the literature for lpr  @xcite can not be used for proving the pointwise consistency result for llsvms .",
    "this is because , in lpr we are interested in the squared loss of the estimator .",
    "squared loss allows a bias - variance decomposition , and the analysis requires the analysis of this decomposition .",
    "however , in classification we are concerned with 0 - 1 loss , which does not allow such a decomposition .      as the proof of theorem",
    "is quite involved we shall first present an overview of our proof . since the statement of theorem   is for each point @xmath0 , we will define certain local quantities , and use them throught our proof .",
    "our proof has three main steps .",
    "we first establish the approximation properties of our function class .",
    "we then make a connection between 0 - 1 risk and the @xmath12 risk , since the llsvm problem works with the @xmath12 risk . finally , we need a bound on the estimation error of llsvm , which roughly says , how good is the llsvm objective as a proxy to the expected local @xmath12 risk .",
    "we shall explain these three main steps in greater detail now .",
    "we borrow some of the ideas from the proof of consistency of svms by steinwart  @xcite , and shall make appropriate comparisons whenever required .    the first step ( lemma  ) is to establish the local approximation properties of linear functions . in order to do",
    "so we define the _ regularized local _ @xmath12 _ risk _ , @xmath81 $ ] , and its corresponding unregurlarized version , called _ local _",
    "@xmath12 _ risk _ , @xmath82 $ ] .",
    "the minimizer of _ regularized local _",
    "@xmath12 _ risk _ among linear functions is denoted as @xmath83 . in lemma",
    "we prove that for small enough @xmath7 , the minimum of local @xmath12 risk among @xmath47 functions , i.e. @xmath84 , can be well approximated by @xmath85 .",
    "a similar type of result , although with global quantities , was proved by steinwart for svms . however , there are two main differences .",
    "firstly steinwart s proof exploited the universal properties of rkhs spaces . since we work with linear kernels which are not universal , steinwart s arguments do not apply here .",
    "we instead use local approximation of @xmath47 functions by linear functions , which is made possible by a simple use of taylor s expansion .",
    "secondly while we work with @xmath47 functions , steinwart s proof works with the space of all measurable functions .",
    "this is because their proof does not make any assumptions on the smoothness of @xmath86 .",
    "however , our assumption a1 guarantees that it is enough to work with just @xmath47 functions .    the second step ( lemma  ) connects @xmath12 risk with 0 - 1 risk . in order to do so we define the local risk of a function @xmath10 , as @xmath87k(x , x_0,\\sigma)]$ ] . the excess local risk of @xmath10 is simply @xmath88 , which we prove in lemma   to be equal to @xmath89 . in lemma",
    "we prove that , for small enough @xmath13 , the difference between the local @xmath12 risk of a function , @xmath10 , and a function , in @xmath47 , with the smallest local risk , is an upper bound on the excess local 0 - 1 risk of @xmath10 .",
    "this result is nothing but a local version of the result that was first stated in  @xcite .    in the third step , via lemmas - we bound the deviation of the empirical local risk ,  @xmath90 , from the local risk , @xmath91 , for the solution of problem  .",
    "this is done via uniform stability arguments  @xcite .",
    "a similar result was also used by steinwart , albeit , for global quantities .",
    "the fourth and final step puts together all these results to establish conditions for a.s .",
    "convergence of the sequence @xmath92 $ ] to @xmath84 .",
    "we then use this stochastic convergence along with assumption a4 to establish theorem  .",
    "the proof of this final step exploits the fact that @xmath50 is a continuous function .",
    "[ lem : bayes ] let @xmath93 .",
    "then , @xmath94 .",
    "hence @xmath95 .",
    "we have @xmath96 hence , @xmath97 now by definition the above term is non - negative for all measurable functions @xmath10 .",
    "hence in @xmath63 the behavior of @xmath98 is exactly the same as that of bayes classifier .",
    "the above lemma tells us that even though the local risk uses a kernel function to weight the loss function , the minimizer of the local 0 - 1 risk , in a @xmath13 neighborhood of @xmath0 , behaves like the bayes optimal classifier .",
    "this simple yet crucial result , would not be valid if one used a kernel that could take negative values ( negative kernels ) .",
    "i.e. with a negative kernel it is not possible to guarantee that @xmath99\\geq 1/2 $ ] .    under assumptions a1-a3 , at any point @xmath100",
    ", @xmath101 satisfies the property @xmath102=0.\\ ] ] [ lem : continuity ]    step 1 .",
    "we shall begin by proving the following statement .",
    "@xmath103 fix a @xmath104 , and let @xmath105 be given . since @xmath106 is a continuous convex function , hence it is possible to find atleast one @xmath107 with @xmath108 , such that @xmath109    since @xmath110 is continuous in @xmath14 , there exists a @xmath111 such that for all @xmath112 .",
    "now for any @xmath113 , we get @xmath114 since @xmath115 was arbitrary equation   follows.step 2 . in the second step",
    "we prove that @xmath116=0.$ ] suppose the real valued function @xmath117 is the minimizer of @xmath118 for @xmath119 . by taylor expansion we have @xmath120 .",
    "hence , @xmath121\\\\    \\leq \\bbe~[o(||x - x_0|| ) k(x , x_0,\\sigma)]\\rightarrow 0,\\nonumber\\end{gathered}\\ ] ] where the last step is due to a2 .",
    "this completes the proof of our second part .",
    "[ lem : l_is_good ] * * s**uppose @xmath122 .",
    "then for a sufficiently small @xmath13 , such that @xmath123 for any @xmath124 , we get @xmath125 .    define @xmath126 .",
    "@xmath127\\nonumber\\\\        & { \\mbox{$\\;\\stackrel{\\mbox{\\rm ( a)}}{\\leq}\\;$}}\\bbe[(1- \\eta(x)l(f^{*}_l)-(1-\\eta(x))l(f^{*}_l))k(x , x_0,\\sigma)\\bbm1_{\\delta}]\\nonumber\\\\        & { \\mbox{$\\;\\stackrel{\\mbox{\\rm ( b)}}{\\leq}\\;$}}\\rx0(f)-\\rx0(f^*_l))\\nonumber    \\end{aligned}\\ ] ] in step ( a ) we used the fact that for the hinge loss @xmath128 , and in step ( b ) we used the fact that on the event @xmath129 , it is better to predict using the 0 function rather than predicting with @xmath10 .",
    "we now need the notion of uniform stability to establish the concentration result , which were outlined in the proof overview . roughly uniform stability  @xcite bounds the difference in loss of a learning algorithm , at any arbitrary point , due to removal of any one point from the training dataset .",
    "[ lem : concentration ] llsvms obtained by solving the optimization problem   at any point @xmath0 has uniform stability of @xmath130 w.r.t .",
    "the loss function @xmath131 .",
    "let @xmath26 , @xmath132 be the llsvms learned at @xmath0 using data sets @xmath133 respectively . for any @xmath134 ,",
    "we have @xmath135 hence it is enough to bound @xmath136 . by definition",
    "both @xmath137 are solutions of their respective convex optimization problem .",
    "let @xmath138 where @xmath139 is an element of the subgradient of @xmath12 at the appropriate arguement .",
    "we have @xmath140 .",
    "hence @xmath26 is an optimal solution of the minimization problem : @xmath141 , and we have @xmath142 .",
    "we get @xmath143 @xmath144 where the inequality in step ( a ) uses properties of convex functions . using equations  ,",
    "we get @xmath145 .",
    "@xcite [ lem : bousquet_bound ] let @xmath146 be the hypothesis learnt by an algorithm @xmath147 on dataset @xmath1 , such that @xmath148 .",
    "suppose @xmath147 has uniform stability @xmath149 w.r.t @xmath19 .",
    "then , @xmath150 , we have @xmath151\\leq \\exp(-2n\\epsilon^2/(4n\\beta+m_1)^2).\\ ] ]    [ lem : concentration2 ] for any point @xmath152 we have @xmath153 \\leq \\exp\\biggl(\\frac{-2n\\lambda^2\\sigma^{4d}\\epsilon^2}{(8m^2+\\lambda\\sigma^d+m\\sqrt{\\lambda\\sigma^d})^2}\\biggr).\\ ] ]    the desired result follows from lemmas - and by substituting @xmath154 for @xmath155 and @xmath156 for @xmath157 in lemma  , and by susbtituting @xmath158 , which was obtained by using the fact that hinge loss is 1-lipschitz .",
    "* proof of theorem  . *",
    "the proof is in two parts . in the first part we shall prove that under the conditions stated in the premise of the theorem @xmath159 a.s .",
    "the second part then uses this almost sure convergene of local risk to guarantee that @xmath26 and @xmath160 agree on the label of @xmath0 . fix any @xmath161",
    "let @xmath162 .",
    "define @xmath163 . for appropriately chosen values of @xmath164",
    "we have with probability atleast @xmath165 @xmath166 in the above equations step ( a ) follows from lemma  , and hence there is a failure probability of at most @xmath167 . step ( b ) follows from the fact that @xmath26 is the minimizer of @xmath168 , and step ( c ) uses the hoeffding inequality , and incurs a failure probability of @xmath169 .",
    "choosing small enough @xmath170 inequality ( d ) follows from lemma  . applying lemma   we get with probability atleast @xmath165 , @xmath171 step ( a ) follows from equation  , and the fact that the marginal distribution on @xmath46 is absolutely continuous .",
    "the absolute continuity gurantees that @xmath172 . if @xmath173 we conclude that @xmath174 in probability . since for data - dependent choices of @xmath7 that satisfy @xmath175 , we get @xmath176 , hence by borel - cantelli lemma the convegence @xmath177 also happens almost surely .",
    "l0.30 [ fig : myfig ]     we shall now prove the second part . if @xmath178 , then the prediction of llsvms at point @xmath0 is irrelevant . hence let @xmath179 .",
    "the proof is the same if @xmath180 .",
    "choose @xmath181 such that @xmath182 . notice that because of continuity @xmath183 has the same sign everywhere in @xmath184 ( see figure ( 1 ) ) .",
    "from a5 we are guaranteed that there exists @xmath185 such that for all @xmath186 , we have @xmath187 .",
    "let @xmath188 .",
    "now from the first part of the proof we know that @xmath159 almost surely .",
    "this guarantees that there exists a sufficiently large @xmath189 such that for appropriate @xmath190 , and an appropriate choice of @xmath14 , we get @xmath191=1.\\ ] ] now for the above choice of @xmath192 , represent by @xmath129 the region of disagreement between @xmath26 and @xmath160 .",
    "assume that @xmath193 .",
    "since @xmath183 has the same sign everywhere in @xmath63 , we get @xmath194 , and hence the volume of @xmath129 is at least half of @xmath63 . hence @xmath195 which is a contradiction to equation  .",
    "hence @xmath196 and @xmath26 agree on the label of @xmath0 .",
    "llsvms solves a local optimization problem that can be seen as minimizing an empirical version of the stochastic objective @xmath197 .",
    "it is then natural to ask as to how quickly does the value of the stochastic objective for @xmath198 converge to the minima of the stochastic objective ? in theorem   we demonstrate , via stability arguments , that for an arbitrary test point @xmath0 , this convergence happens at the rate of @xmath199 . in theorem",
    "we establish generalization bounds for a global classifier learnt by solving llsvm s at any randomly chosen point @xmath36 , in terms of the empirical error of llsvms .",
    "due to lack of space the proofs are postponed to the supplement .",
    "[ thm : stochastic ] with probability at least @xmath200 over the random input training set we have @xmath201    * discussion of theorem  * . in theorem",
    ", it might be possible to improve the dependence on @xmath2 from @xmath202 to @xmath203 via the peeling idea  @xcite . based on  @xcite",
    ", we conjecture that the dependence on @xmath14 is optimal , while the dependence on @xmath13 may be improved from @xmath204 to @xmath205 .",
    "[ thm : risk_bound_for_llsvm ] let @xmath206 be the vector obtained by solving the llsvm problem , with parameters @xmath7 , at a randomly drawn point @xmath36 . with probability at least @xmath200 over the random sample , we have @xmath207    * discussion of theorem  * . without any further noise assumptions ,",
    "the dependence on @xmath79 is optimal . with the tsybakov s",
    "@xcite noise assumption , it is possible to improve the dependence on @xmath2 .",
    "the exponential dependence on @xmath30 is expected , and is typical of nonparametric methods .",
    "for convenience we shall begin with a risk bound from  @xcite .",
    "this risk bound relies on the notion of uniform stability . for any learning algorithm a that learns a function @xmath208 after having trained on the dataset @xmath1 the uniform stability quantifies the absolute maginitude of the change in loss suffered by the algorithm at any arbitrary point in the space if an arbitrary @xmath16 is removed from the training dataset .",
    "the precise definition is as follows    [ def : defn_uniform_stability ]  @xcite an algorithm a has uniform stability @xmath149 w.r.t the loss function @xmath12 if : @xmath209    [ lem : bousquet_bound ] let @xmath147 be an algorithm with uniform stability @xmath149 w.r.t a loss function @xmath210 , for all @xmath211 and all set s. then for any @xmath212 , and any @xmath213 , the following bound holds true with probability atleast @xmath200 over the random draw of the sample s. @xmath214    with probability @xmath200 over the random input training set we have @xmath215 where @xmath216\\\\ m_1&\\leq\\frac{l(0)}{n}\\sum_{j=1}^n k(x_j , x_0,\\sigma)+l(0)k_m+k_mm\\sqrt{\\frac{2l(0)}{n\\lambda}\\sum_{j=1}^n k(x_j , x_0,\\sigma)}\\end{aligned}\\ ] ]    the proof is via stability arguments .",
    "let @xmath211 .",
    "consider the loss function @xmath217.\\end{gathered}\\ ] ] it is enough to bound the stability of llsvm s w.r.t the above loss function and also upper bound the above loss . in order to upper bound the stability of llsvm",
    "s it is enough to upper bound for all @xmath218 the quantity @xmath219 , where @xmath220 is the dataset obtained from @xmath1 by deleting the point @xmath221 , and @xmath222 is the llsvm learnt at @xmath0 with @xmath220 . from equation  ( [ eqn : defn_q ] )",
    "it is clear that @xmath223 is @xmath14 strongly convex in @xmath224 in @xmath225 norm .",
    "hence by strong convexity @xmath226 similarily we have @xmath227 from equations  ( [ eqn : strong_convexity1],[eqn : strong_convexity2 ] ) we get @xmath228 we shall now upper and lower bound the rightmost and the leftmost terms respectively . doing this",
    "will enable us to bound the stability .",
    "differentiating equation  ( [ eqn : defn_q ] ) w.r.t @xmath224 we get @xmath229 now in order to bound the rightmost term of equation  ( [ eqn : lower_bound_upper_bound ] ) we use equation  ( [ eqn : derivative_q ] ) to get @xmath230\\leq \\\\",
    "\\qquad\\qquad\\qquad||\\hatwunderstarminusireg-\\hatwunderstarreg||~ ||\\lambda\\hatwunderstarminusireg+\\partial l(y\\langle \\hatwunderstarminusireg\\rangle)yk(x , x_0,\\sigma)x||\\end{gathered}\\ ] ] where the last inequality follows from cauchy - schwartz inequality .",
    "we shall begin by bounding @xmath231 .",
    "now by the definition of @xmath232 we get @xmath233 now consider the following convex optimization problem @xmath234    it is trivial to verify using equations  ( [ eqn : derivative_equations1],[eqn : derivative_equations2 ] ) that @xmath235 , and hence from convex analysis we know that @xmath26 is the optimal solution of the convex optimization problem @xmath236 .",
    "also @xmath237 .",
    "hence we get @xmath238 where the second inequality is due to the fact that @xmath19 is a convex loss function , and hence @xmath239 , and the last inequality due to cauchy - schwartz and the fact that @xmath19 is @xmath240 lipschitz .",
    "hence we get @xmath241 finally we have by the optimality of @xmath232 @xmath242 using equations  ( [ eqn : lower_bound_upper_bound],[eqn : derivative_q],[eqn : chain_of_inequalities],[eqn : diff_norm_bound],[eqn : norm_bound ] ) we get @xmath243\\end{gathered}\\ ] ] one can use similar techniques to lower bound the leftmost term in equation  ( [ eqn : lower_bound_upper_bound ] ) to get @xmath244      \\end{gathered}\\ ] ] using the fact that @xmath245 and equations  ( [ eqn : lower_bound_q],[eqn : upper_bound_q ] ) we get @xmath246\\ ] ] in order to apply theorem  ( [ lem : bousquet_bound ] ) it is enough to upper bound @xmath247 .",
    "we have @xmath248 now applying theorem  ( [ lem : bousquet_bound ] ) to llsvm s with the loss function @xmath249 and since @xmath250 we get the desired result .",
    "[ thm : risk_bound_for_llsvm ] let @xmath206 be the solution obtained by solving the llsvm problem at @xmath36 . with probability at least @xmath200 over the random sample for an llsvm , we have @xmath251    by lemma   we are done if we can upper bound the loss suffered by llsvms at any point , and the stability of llsvms w.r.t the loss @xmath252 .",
    "we have @xmath253 , where we used the upper bound on @xmath254 presented in equation 9 of lemma 5 in the main paper .",
    "finally @xmath255 .",
    "apply lemma   with @xmath256 and @xmath257 to finish the proof .",
    "our results guarantee that the decision of an llsvm learnt at @xmath0 matches that of the bayes classifier after having seen enough data . an important open problem is to establish global bayes consistency of llsvms .",
    "it is not clear to us if the pointwise consistency result can be used to do so .",
    "theorem   currently does not exploit our large margin formulation .",
    "a natural extension of this theorem would be to establish a result that depends on some kind of a local notion of margin .",
    "our current results depend on the dimensionality of the ambient space .",
    "it should be possible , under appropriate manifold assumptions  @xcite ,  @xcite to improve this dependency to use the intrinsic dimension ."
  ],
  "abstract_text": [
    "<S> we provide a formulation for local support vector machines ( lsvms ) that generalizes previous formulations , and brings out the explicit connections to local polynomial learning used in nonparametric estimation literature . </S>",
    "<S> we investigate the simplest type of lsvms called local linear support vector machines ( llsvms ) . </S>",
    "<S> for the first time we establish conditions under which llsvms make bayes consistent predictions at each test point @xmath0 . </S>",
    "<S> we also establish rates at which the local risk of llsvms converges to the minimum value of expected local risk at each point @xmath0 . </S>",
    "<S> using stability arguments we establish generalization error bounds for llsvms . </S>"
  ]
}