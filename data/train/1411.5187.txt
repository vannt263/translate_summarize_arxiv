{
  "article_text": [
    "over the years , there has been growing interest in the recovery of high dimensional signals from a small number of measurements .",
    "this new paradigm , so called compressed sensing ( cs ) , relies on the fact that many naturally acquired high dimensional signals inherently have low dimensional structure .",
    "in fact , since many real world signals can be well approximated as sparse signals ( i.e. , only a few entries of signal vector are nonzero ) , cs techniques have been applied to a variety of applications including data compression , source localization , wireless sensor network , medical imaging , data mining , to name just a few .    over the years , various signal recovery algorithms for cs have been proposed . roughly speaking ,",
    "these approaches are categorized into two classes .",
    "the first approach is based on a deterministic signal model , where an underlying signal is seen as a deterministic vector and the sparsity promoting cost function ( e.g. , @xmath0-norm ) is employed to solve the problem .",
    "these approaches include the basis pursuit ( bp ) @xcite , orthogonal matching pursuit ( omp ) @xcite , cosamp @xcite , and subspace pursuit @xcite .",
    "the second approach is based on the probabilistic signal model , where the signal sparsity is described by the _ a priori _ distribution of the signal and bayesian framework is employed in finding the sparse solution @xcite .",
    "when the multiple measurement vectors ( mmv ) from different source signals with common support are available , accuracy of the sparse signal recovery can be improved dramatically by performing joint processing of these vectors @xcite .",
    "since the algorithms based on mmv usually performs better than those relying on single measurement vector , many efforts have been made in recent years to develop an efficient sparse recovery algorithm .",
    "the mmv - based recovery algorithms targeted for the deterministic signal recovery include the mixed - norm solution @xcite and convex relaxation @xcite while the probabilistic approaches include the mmv sparse bayesian learning ( sbl ) method @xcite , block - sbl @xcite , auto - regressive sbl @xcite , and kalman filtering - based sbl ( ksbl ) @xcite .    in this work ,",
    "we are primarily concerned with the mmv - based signal recovery problem when the observation vectors are sequentially acquired . to be specific ,",
    "we express the @xmath1 observation vector @xmath2 acquired at time index @xmath3 as @xmath4 where @xmath5 is the @xmath6 system matrix , @xmath7 is the @xmath8 source signal vector , and @xmath9 are the @xmath1 noise vector .",
    "we assume that @xmath9 is modeled as a zero mean complex gaussian random vector , i.e. , @xmath10 .",
    "our goal in this setup is to estimate the source signal @xmath7 using the sequence of the observations @xmath11 when 1 ) the source signal @xmath7 is sparse ( i.e. , the number nonzero elements in @xmath7 is small ) and 2 ) the dimension of the observation vector @xmath2 is smaller than that of the source vector @xmath12 . in particular , we focus on the scenario where the nonzero elements of @xmath7 change over time with certain temporal correlations . in this scenario",
    ", we assume that correlated time - varying signals are well modeled by gauss - markov process .",
    "note that this model is useful in capturing local dynamics of signals in linear estimation theory @xcite .",
    "the main purpose of this paper is to propose a new statistical sparse signal estimation algorithm for the sequential observation model we just described .",
    "the underlying assumption used in our model is that the nonzero amplitude of the sparse signals is changing in time , leading to different signal realizations for each measurement vector , yet the support of the signal amplitude is slowly varying so that the support remains unchanged over certain consecutive measurement vectors .",
    "we henceforth refer to this model as _ simultaneously sparse signal with locally common support _ since the support of the sparse signal remains constant over the fixed interval under this assumption .",
    "many of signal processing and wireless communication systems are characterized by this model .",
    "for example , this model matches well with the characteristics of multi - path fading channels for wireless communications where the channel impulse response @xmath7 should be estimated from the received signal @xmath2 .",
    "[ fig : cir ] shows a record of the channel impulse responses ( cir ) of underwater acoustic channels ( represented over the propagation delay and time domain ) measured from the experiments conducted in atlantic ocean in usa @xcite .",
    "we observe that when compared to the amplitude of the channel taps , the sparsity structure of the cir is varying slowly .",
    "thus , we can readily characterize this time - varying sparse signal using the correlated random process along with a deterministic binary parameter representing the existence of the signal .",
    "in recovering the original signal vector @xmath7 from the measurement vectors , we use the modified expectation - maximization ( em ) algorithm @xcite .",
    "the proposed scheme , dubbed as sparse - kalman - tree - search ( skts ) , consists of two main operations : 1 ) kalman smoothing to gather the _ a posteriori _ statistics of the source signals from individual measurement vector within the block of interest and 2 ) identification of the support of the sparse signal vector using a greedy tree search algorithm . treating the problem to identify the sparsity structure of the source signal as a combinatorial search , we propose a simple yet effective greedy tree search algorithm that examines the small number of promising candidates among all sparsity parameter vectors in the tree .    there",
    "exist several approaches to estimate the time - varying sparse signals under mmv model . in @xcite , reweighted @xmath0 optimization has been modified for the sequential dynamic filtering . in @xcite , modified sbl algorithm has been suggested to adopt autoregressive modeling . in @xcite , em - based adaptive filtering scheme has been proposed in the context of sparse channel estimation .",
    "other than these , notable approaches include turbo approximate message passing ( amp ) @xcite , lasso - kalman @xcite , and kalman filtered cs @xcite .",
    "we note that our work is distinct from these approaches in the following two aspects .",
    "first , in contrast to the previous efforts using continuous ( real - valued ) parameters to describe signal sparsity in @xcite , the proposed method employs the deterministic discrete ( binary ) parameter vector that captures the on - off structure of signal sparsity . due to the use of deterministic parameter vector ,",
    "an effort to deal with the probabilistic model on signal sparsity is unnecessary .",
    "also , since the search space is discretized , identification of parameter vector is done by the efficient search algorithm .",
    "second , while the recent work in @xcite estimates signal amplitude using kalman smoother and then identifies the support of sparse signal by thresholding of the innovation error norm , our work pursues direct estimation of the binary parameter vector using the modified em algorithm .",
    "we note that a part of this paper was presented in @xcite .",
    "the distinctive contribution of the present work is that the algorithm is developed in a more generic system model and practical issues ( e.g. , parameter estimation and iteration control ) and real - time implementation issues are elaborated .",
    "further , extensive simulations for the practical applications are conducted to demonstrate the superiority of the proposed method .",
    "[ t ]    the rest of this paper is organized as follows . in section [ sec : skts_proposed ] , we briefly explain the sparse signal model and then present the proposed method . in section [ sec : chan ] , we discuss the application of the proposed algorithm in the wireless channel estimation . in section [ sec : simul ] , the simulation results are provided , and section [ sec : conclusion ] concludes the paper .",
    "notation : uppercase and lowercase letters written in boldface denote matrices and vectors , respectively .",
    "superscripts @xmath13 and @xmath14 denote transpose and conjugate transpose ( hermitian operator ) , respectively .",
    "@xmath15 denotes the conjugation of the complex number @xmath16 .",
    "@xmath17 indicates an @xmath18-norm of a vector . for the @xmath19-norm , we abbreviate a subscript @xmath20 for simplicity .",
    "@xmath21 is a diagonal matrix having elements only on the main diagonal . @xmath22 and @xmath23 denote the real and imaginary parts of @xmath16 , respectively .",
    "@xmath24 $ ] denotes the expectation of a random variable @xmath25 and @xmath26 $ ] denotes the conditional expectation of @xmath25 given @xmath27 .",
    "@xmath28 $ ] means the expectation of @xmath25 given the deterministic parameter @xmath29 .",
    "the notations for covariance matrices are given by @xmath30 - e[\\mathbf{x}]e[\\mathbf{y}]^{h}$ ] and @xmath31 .",
    "@xmath32 means the probability of the event @xmath33 .",
    "@xmath34 denotes a trace operation of the matrix @xmath33 .",
    "@xmath35 is the element - by - element product ( hadamard product ) of the matrices @xmath33 and @xmath36 .",
    "@xmath37 denotes the @xmath38th coordinate vector .",
    "in this section , we consider the statistical estimation of the time - varying sparse signals from the sequentially collected observation vectors . as mentioned , our approach is based on the assumption that the support of the sparse signal varies slowly in time so that the multiple measurement vectors sharing common support can be used to improve the estimation quality of the sparse signals . in this section ,",
    "we first describe the simultaneously sparse signal model and then present the proposed sparse signal estimation scheme .",
    "we express a time - varying sparse signal @xmath7 as a product of a vector of random processes @xmath39 describing the amplitudes of nonzero entries in @xmath7 and the vector @xmath40^{t}$ ] indicating the existence of signal .",
    "that is , @xmath41 where @xmath38 is the block index , the entry of @xmath42 is either 0 or 1 depending on the existence of the signal @xmath43 and the time - varying amplitude @xmath44 is modeled as gauss - markov random process @xmath45 where @xmath46 is the process noise vector ( @xmath47 ) and @xmath48 is the state update matrix .",
    "note that the block index @xmath38 is associated with the interval of the length @xmath49 , @xmath50 $ ] .",
    "as mentioned , we assume that the support of the underlying sparse signals is locally time - invariant so that @xmath42 is constant in a block of consecutive measurement vectors . using this together with the observation model in ( [ eq : smodel ] ) , we obtain the simultaneously sparse signal model @xmath51 since @xmath7 follows gaussian distribution for the given @xmath42 , the _ a priori _ distribution of the source signal @xmath7 can be described by @xmath52)^{h}{\\rm cov}\\left(\\mathbf{h}_n \\right)^{-1 } ( \\mathbf{h}_{n}-e\\left[\\mathbf{h}_n ; \\mathbf{c}_{i } \\right ] ) \\right),\\end{aligned}\\ ] ] where @xmath53 & = { \\rm diag}(\\mathbf{c}_{i } ) e[\\mathbf{s}_{n } ] \\nonumber \\\\ { \\rm cov}\\left(\\mathbf{h}_n \\right ) & = { \\rm diag}(\\mathbf{c}_{i } ) { \\rm cov } ( \\mathbf{s}_{n}){\\rm diag}(\\mathbf{c}_{i } ) .",
    "\\label{eq : sttt}\\end{aligned}\\ ] ]      when the multiple measurement vectors @xmath54 in the @xmath38th block are available , the maximum likelihood ( ml ) estimate of @xmath42 is expressed as @xmath55 where @xmath56^{t}$ ] and @xmath57 is the sparsity order ( the number of nonzero entries ) of @xmath7 .",
    "note that the subscript @xmath58 denotes the set of time indices for the @xmath38th block .",
    "note also that the ml estimate @xmath59 is chosen among all candidates satisfying the sparsity constraint @xmath60 .",
    "once @xmath59 is obtained , we can estimate the amplitude vectors @xmath39 assuming that the signal support specified by @xmath59 is true .",
    "well known linear minimum mean square error ( lmmse ) estimator ( e.g. kalman smoother ) can be used to estimate @xmath39 and then @xmath59 and the estimate of @xmath39 are combined to produce a final estimate of @xmath7 . note",
    "that if the estimation of @xmath42 is correct , we can obtain the best achievable estimate of @xmath7 , which is equivalent to the solution attainable by so called  oracle estimator \" .",
    "since the ml problem in ( [ eq : mlsol ] ) involves the marginalization over all possible combinations of the latent variables @xmath61 , finding out the solution using the direct approach would be computationally unmanageable . perhaps , a better way to deal with the problem at hand is to use the em algorithm .",
    "recall that the em algorithm is an efficient means to find out the ml estimate or maximum a posteriori ( map ) estimate of statistical signal model in the presence of unobserved latent variables .",
    "the em algorithm generates a sequence of estimates @xmath62 , @xmath63 by alternating two major steps ( e - step and m - step ) , which are given , respectively    * * expectation step ( e - step ) * @xmath64,\\end{aligned}\\ ] ] * * maximization step ( m - step ) * @xmath65    where @xmath62 is the estimate of @xmath42 at the @xmath66-th iteration . although one can not guarantee finding out the global optimal solution of ( [ eq : mlsol ] ) using the em algorithm",
    ", we will empirically show that @xmath42 can be estimated accurately with a proper initialization of @xmath67 ( see section [ sec : simul ] ) .",
    "the goal of the e - step is to obtain a simple expression of the cost metric @xmath68 using the simultaneously sparse signal model .",
    "first , @xmath69 is expressed as @xmath70 noting that @xmath71 and @xmath72 , we have @xmath73 where @xmath74 and @xmath75 are the terms independent of @xmath42 .",
    "from ( [ eq : estep ] ) and ( [ eq : vv2 ] ) , we further have ( see appendix [ appen : q ] ) @xmath76 \\right )   \\nonumber \\\\ &",
    "-{\\rm tr}\\left [   \\mathbf{b}_{n } { \\rm diag}(\\mathbf{c}_{i } )   e\\left [ \\mathbf{s}_{n}\\mathbf{s}_{n}^{h}\\bigg|   \\mathbf{y}_{1:t } ; \\hat{\\mathbf{c}}_i^{(l ) } \\right ] { \\rm diag}(\\mathbf{c}_{i } )   \\mathbf{b}_{n}^{h }   \\right ]    \\bigg\\}. \\label{eq : qqq}\\end{aligned}\\ ] ] let @xmath77 and @xmath78 be the conditional mean and covariance of @xmath39 when @xmath79 and @xmath62 are given , i.e. , @xmath80 \\nonumber \\\\    \\sigma_{n|1:t } & = { \\rm cov}\\left [ \\mathbf{s}_{n}\\bigg|   \\mathbf{y}_{1:t } ; \\hat{\\mathbf{c}}_i^{(l ) } \\right ] . \\nonumber    \\end{aligned}\\ ] ] now we turn to the estimation of the _ a posteriori _ statistics @xmath77 and @xmath81 . in our work , we estimate @xmath77 and @xmath81 using kalman smoothing @xcite . when @xmath62 is given , from ( [ eq : blocks ] )",
    ", the system equations for kalman smoothing becomes @xmath82 we employ the fixed - interval kalman smoothing algorithm performing sequential estimation of @xmath77 and @xmath78 via forward and backward recursions in a block of observations @xmath83 .",
    "let @xmath84 and @xmath85 be the conditional mean and covariance given the first @xmath86 observation vectors , i.e. , @xmath87 $ ] and @xmath88 $ ] , then the fixed - interval kalman smoothing algorithm is summarized as    * * forward recursion rule : * @xmath89 * * backward recursion rule : * @xmath90    using @xmath77 and @xmath78 , @xmath91 can be rewritten as @xmath92   \\bigg\\}. \\label{eq : qb}\\end{aligned}\\ ] ] note that the second term in the right - hand side of ( [ eq : qb ] ) is expressed as ( see appendix [ appen : prf ] ) @xmath93 \\nonumber \\\\ & \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\ ; = \\mathbf{c}_{i}^{t } \\left({\\rm conj}\\left(\\mathbf{b}_{n}^{h } \\mathbf{b}_{n}\\right )   \\odot      \\left(\\sigma_{n|1:t } + \\widehat{\\mathbf{s}}_{n|1:t}\\widehat{\\mathbf{s}}_{n|1:t}^{h } \\right ) \\right ) \\mathbf{c}_{i } \\label{eq : sterm}.\\end{aligned}\\ ] ] from ( [ eq : qb ] ) and ( [ eq : sterm ] ) , we have @xmath94 further , by denoting @xmath95 we have @xmath96 in summary , the e - step performs the kalman smoothing operation in ( [ eq : f1])-([eq : be ] ) to estimate @xmath77 and @xmath78 and also operations in ( [ eq : cal1 ] ) and ( [ eq : cal2 ] ) to compute @xmath97 and @xmath98 used in the computation of @xmath91 .",
    "[ t ]    p15 cm input : @xmath99 and survival list @xmath98 + initialization : start with @xmath100^{t}\\}$ ] .",
    "+ for @xmath101 + @xmath102 + @xmath103 + let @xmath104 be the @xmath86th element of @xmath105 .",
    "+ if the @xmath106-th entry is already one , skip the loop . otherwise , set the @xmath106-th entry of @xmath104 to one .",
    "+ for @xmath107 , evaluate @xmath108 for all @xmath109 .",
    "+ if @xmath110 for any @xmath109 , then the candidate @xmath104 is duplicate node and hence we remove it .",
    "+ if @xmath111 , add @xmath104 into @xmath105 .",
    "+   +   + end + output : @xmath112 .",
    "+    [ tb : tree ]    in the m - step , we find @xmath42 maximizing @xmath91 in ( [ eq : finalexp ] ) as @xmath113 where @xmath114 . in finding @xmath115 , we need to check all possible combinations satisfying the sparsity constraint @xmath116 . since this brute force search is prohibitive for practical values of @xmath117",
    ", we consider a computationally efficient search algorithm returning a sub - optimal solution to the problem in ( [ eq : max ] ) .",
    "the proposed approach , which in essence builds on the greedy tree search algorithm , examines candidate vectors to find out the most promising candidate of @xmath42 in a cost effective manner .",
    "the tree structure used for the proposed greedy search algorithm is illustrated in fig .",
    "[ fig : tree ] . starting from a root node of the tree ( associated with @xmath118^{t}$ ] ) , we construct the layer of the tree one at each iteration . in the first layer of the tree , only one entry of @xmath42",
    "is set to one .",
    "for example , the nodes in the first layer of the tree are expressed as @xmath119^{t},\\cdots , \\mathbf{c}_{i}^{m } = [ 0 , \\cdots , 0 , 1]^{t}$ ] .",
    "as the layer increases , one additional entry is set to one and thus @xmath57 entries of @xmath42 are set to one in the @xmath57-th layer ( @xmath120 ) ( see fig . [",
    "fig : tree ] ) . at each layer of the tree",
    ", we evaluate the cost function @xmath121 for each node and then choose the @xmath122 best nodes whose cost function is maximal .",
    "the rest of nodes are discarded from the tree .",
    "the candidates of @xmath42 associated with the @xmath122 best nodes at each layer are called  survival list \" . for each node in the survival list ,",
    "we construct the @xmath123 child nodes in the second layer by setting one additional entry of @xmath42 to one^{t}$ ] is in the survival list , then the child nodes of @xmath42 becomes @xmath124^{t},\\cdots,\\mathbf{c}_{i } = [ 1 , 0 , \\cdots , 1]^{t}$ ] . ] .",
    "note that since we do not distinguish the order of the bit assertion in @xmath42 , two or more nodes might represent the same realization of @xmath42 during this process ( see fig . [",
    "fig : tree ] ) .",
    "when duplicate nodes are identified , we keep only one and discard the rest from the tree . after removing all duplicate nodes , we choose the @xmath122 best nodes and then move on to the next layer .",
    "this process is repeated until the tree reaches the bottom layer of the tree .",
    "we note that since the tree search complexity is proportional to the depth of the tree ( @xmath57 ) , the dimension of source vector ( @xmath117 ) , and the number of nodes being selected ( @xmath122 ) , one can easily show that the complexity of the proposed tree search is @xmath125 .",
    "hence , with small values of @xmath122 and @xmath57 , the computational complexity is reasonably small and proportional to the dimension @xmath117 of the source signal vector .",
    "the proposed tree search algorithm is summarized in table [ tb : tree ] .",
    "it is worth mentioning that one important issue to be considered is how to estimate the sparsity order @xmath57 .",
    "one simple way is to use the simple correlation method , where the observation vectors are correlated with the column vectors of @xmath5 and @xmath57 is chosen as the number of the column vectors whose absolute correlation exceeds the predefined threshold .",
    "while this approach is simple to implement , the performance might be affected by the estimation quality of @xmath57 .",
    "one can alternatively consider a simple heuristic that terminates the tree search when a big drop in the cost metric @xmath126 is observed .",
    "after all iterations are finished ( i.e. @xmath127 ) and @xmath128 is obtained , we use the kalman smoother once again to compute @xmath77 using the newly updated @xmath128 .",
    "the final estimate of @xmath7 is expressed as @xmath129    [ cols=\"<\",options=\"header \" , ]     [ tb : lte ]    note that the channel taps in the standard lte channel model are only approximately sparse . in order to determine the parameters of the gauss - markov process @xmath130 and @xmath131 for a given @xmath132 , we minimize the approximation error between the gauss - markov process and the jake s model as suggested in @xcite . using the cir estimates obtained by the sparse signal recovery algorithms ,",
    "the transmitted symbols are detected by the mmse equalizer in frequency domain .",
    "then , the channel decoder is followed to detect the information bits . to evaluate the performance of the recovery algorithms , we measure bit error rate ( ber ) at the output of the channel decoder .",
    "we test the performance of the channel estimators when the exact @xmath57-sparse channels are used .",
    "the sparsity order @xmath57 for these channels is set to @xmath133 and the dimension @xmath134 of the measurement vector is set to @xmath135 .",
    "note that when @xmath136 , the pilot resources occupy 3.12% of the overall ofdm resources .",
    "we assume that the sparsity structure remains unchanged over the block of @xmath137 pilot containing ofdm symbols .",
    "we set the doppler rate @xmath138 to @xmath139 . in fig .",
    "[ fig : perf ] ( a ) and ( b ) , we plot the mse and ber performance of the recovery algorithms as a function of snr . from the figure",
    ", we clearly observe that the skts algorithm performs best among all algorithms under test and also performs close to that of the oracle - based kalman smoother .",
    "we next investigate the performance of the proposed skts algorithm when the practical lte channel models are used . in this test",
    ", we observe the behavior of the algorithms for four distinctive scenarios : a ) eva channel with @xmath140 and @xmath141 , b ) eva channel with @xmath142 and @xmath141 , c ) epa channel with @xmath143 and @xmath144 , and d ) epa channel with @xmath145 , @xmath144 .",
    "we set @xmath146 and @xmath147 for eva and epa channel models since the eva channel exhibits longer delay spread . in fig .",
    "[ fig:3gpp ] , we observe that the skts algorithm maintains the performance gain over the competing algorithms for wide range of doppler rates . note that when compared to the results of the exact @xmath57-sparse channel model , we see that the performance gap between the skts and ksbl is a bit reduced .",
    "next , we compare the performance of the rt - skts described in section [ sec : skts_online ] with the original skts algorithm .",
    "in this simulations , we set @xmath136 and @xmath137 . for the rt - skts algorithm , we set @xmath148 . in order to test the performance in a harsh condition",
    ", we arbitrarily change the delay structure of the cir for every 30 observation vectors .",
    "to ensure the convergence of the online update strategy in ( [ eq : rup1 ] ) and ( [ eq : rup2 ] ) , we use the first 10 observation vectors for warming up purpose and then use the rest for measuring the mse performance . note",
    "that in practice , such warming up period would not be necessary since the support of channel vector would not be changed abruptly in many real applications . in fig .",
    "[ fig : on ] , we see that the rt - skts algorithm performs close to the original skts algorithm in low and mid range snr regime . in the high snr regime , however , the rt - skts algorithm suffers slight performance loss due to the approximation step of @xmath149 and @xmath150 .",
    "nevertheless , as shown in fig .",
    "[ fig : on ] and fig .",
    "[ fig : perf ] ( a ) , the rt - skts algorithm maintains the performance gain over the conventional channel estimators .      in this subsection , we investigate the performance of the skts algorithms in the reconstruction of the dynamic mri images . in our test , we use a sequence of @xmath151 dimensional cardiac images shown in fig .",
    "[ fig : cardiac ] images @xcite .",
    "the raw image data is available online @xcite . ] .",
    "we generate the measurements by performing two dimensional discrete wavelet transform ( dwt ) with a 2-level daubechies-4 wavelet , applying two dimensional dft matrix and taking the @xmath134 randomly chosen frequency - domain image samples . after adding the gaussian noise to the image",
    ", we recover the original image using the recovery algorithms .",
    "we set @xmath152 , which corresponds to about 35% of the image size ( i.e. , @xmath153 ) .",
    "we could empirically observe that the location of nonzero coefficients in wavelet image is slowly changing ( i.e. , support change occurs for only a few places ) , which matches well with our simultaneous sparse signal model . in order to capture the most of signal energy , we set @xmath154 for all images to the the number of coefficient containing @xmath155% of the signal energy . ] . in fig",
    "[ fig : cardiac ] , we plot the mse of the several image recovery schemes obtained for each image .",
    "the skts algorithm outperforms the basis pursuit denoising ( bpdn ) @xcite and rw1l - df @xcite and also performs close to the oracle - based kalman smoother .",
    "note that we could not include modified cs scheme in @xcite in our numerical experiments since large number of measurement samples is required for the first image .",
    "in this paper , we studied the problem to estimate the time - varying sparse signals when the sequence of the correlated observation vectors are available . in many signal processing and wireless communication applications , the support of sparse signals changes slowly in time and thus can be well modeled as simultaneously sparse signal , we proposed a new sparse signal recovery algorithm , referred to as sparse kalman tree search ( skts ) , that identifies the support of the sparse signal using multiple measurement vectors .",
    "the proposed skts scheme performs the kalman smoothing to extract the _ a posteriori _ statistics of the source signals and the greedy tree search to identify the support of the signal . from the case study of sparse channel estimation problem in orthogonal frequency division multiplexing ( ofdm ) and image reconstruction in dynamic mri",
    ", we demonstrated that the proposed skts algorithm is effective in recovering the dynamic sparse signal vectors .",
    "from ( [ eq : estep ] ) and ( [ eq : vv2 ] ) , we get @xmath156   \\\\ = & c '' + \\frac{1}{\\sigma_{w}^{2 } } \\sum_{n = ti+1}^{t(i+1)}\\bigg\\ { e\\left[{\\rm tr } \\left[2 { \\rm re}\\left (   \\mathbf{b}_{n } { \\rm diag}(\\mathbf{c}_{k } ) \\mathbf{s}_{n } \\mathbf{y}_{n}^{h } \\right)\\right ] \\bigg| \\mathbf{y}_{1:t } ; \\hat{\\mathbf{c}}_i^{(l ) } \\right ]     \\nonumber \\\\ & - e\\left[{\\rm tr}\\left [ \\mathbf{b}_{n } { \\rm diag}(\\mathbf{c}_{i } ) \\mathbf{s}_{n}\\mathbf{s}_{n}^{h } { \\rm diag}(\\mathbf{c}_{i } )   \\mathbf{b}_{n}^{h }   \\right]\\bigg|   \\mathbf{y}_{1:t } ; \\hat{\\mathbf{c}}_i^{(l ) } \\right ] \\bigg\\ }     \\\\ = & c '' + \\frac{1}{\\sigma_{w}^{2 } } \\sum_{n = ti+1}^{t(i+1)}\\bigg\\ { { \\rm tr } \\left[2 { \\rm re}\\left (   \\mathbf{b}_{n } { \\rm diag}(\\mathbf{c}_{i } ) e\\left[\\mathbf{s}_{n}\\bigg| \\mathbf{y}_{1:t } ; \\hat{\\mathbf{c}}_i^{(l ) } \\right ] \\mathbf{y}_{n}^{h } \\right)\\right ]     \\nonumber \\\\ & - { \\rm tr}\\left [ \\mathbf{b}_{n } { \\rm diag}(\\mathbf{c}_{i } )   e\\left [ \\mathbf{s}_{n}\\mathbf{s}_{n}^{h}\\bigg|   \\mathbf{y}_{1:t } ; \\hat{\\mathbf{c}}_i^{(l ) } \\right ] { \\rm diag}(\\mathbf{c}_{i } )   \\mathbf{b}_{n}^{h }   \\right ] \\bigg\\ } ,    \\\\\\end{aligned}\\ ] ] where @xmath75 and @xmath157 are the terms independent of @xmath42 . using the property of the trace ,",
    "i.e , @xmath158 , we have @xmath159 \\right ) \\nonumber \\\\ & - { \\rm tr}\\left [   \\mathbf{b}_{n } { \\rm diag}(\\mathbf{c}_{i } )   e\\left [ \\mathbf{s}_{n}\\mathbf{s}_{n}^{h}\\bigg|   \\mathbf{y}_{1:t } ; \\hat{\\mathbf{c}}_i^{(l ) } \\right ] { \\rm diag}(\\mathbf{c}_{i } )   \\mathbf{b}_{n}^{h }   \\right ]   \\bigg\\}\\end{aligned}\\ ] ]",
    "denoting @xmath160 as the transpose of the @xmath38th row vector of @xmath5 , we can express the lefthand term of ( [ eq : sterm ] ) as @xmath161 since @xmath162 and @xmath163 , we further have @xmath164\\mathbf{c}_{i},\\end{aligned}\\ ] ] and hence we finally have @xmath165                      e. j. candes , j. romberg , and t. tao ,  robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information , \" _ ieee trans . information theory _ , vol .",
    "489 - 509 , feb . 2006 .",
    "z. zhang and b. d. rao ,  sparse signal recovery with temporally correlated source vectors using sparse bayesian learning , \" _ ieee journal of selected topics in signal processing _ , vol . 5 , pp . 912 - 926 , sept .",
    "2011 .",
    "r. prasad , c. r. murphy and b. d. rao , ",
    "joint approximately sparse channel estimation and data detection in ofdm systems using sparse bayesian learning , \" _ ieee trans . signal process .",
    "62 , no . 14 , pp . 3591 - 3603 , july 2014 .",
    "j. w. choi , t. j. riedl , k. kim , a. c. singer , and j. c. preisig ,  adaptive linear turbo equalization over doubly selective channels , \" _ ieee journal of oceanic engineering _ , vol .",
    "473 - 489 , oct . 2011 .",
    "j. w. choi , k. kim , t. j. riedl , and a. c. singer ,  iterative estimation of sparse and doubly - selective multi - input multi - output ( mimo ) channel , \" _ proc .",
    "signals , systems and computers asilomar conference _ , nov .",
    "2009 , pp .",
    "620 - 624 .",
    "w. u. bajwa , j. haupt , a. m. sayeed and r. nowak , ",
    "compressed channel sensing : a new approach to estimating sparse multipath channels , \" _ proceedings of the ieee _",
    "98 , pp . 1058 - 1076 , june 2010 . c. r. berger , s. zhou , j. c. preisig and p. willett ,  sparse channel estimation for multicarrier underwater acoustic communication : from subspace methods to compressed sensing , \" _ ieee trans",
    ". signal process .",
    "1708 - 1721 , march 2010 ."
  ],
  "abstract_text": [
    "<S> in this paper , we propose a new sparse signal recovery algorithm , referred to as sparse kalman tree search ( skts ) , that provides a robust reconstruction of the sparse vector when the sequence of correlated observation vectors are available . </S>",
    "<S> the proposed skts algorithm builds on expectation - maximization ( em ) algorithm and consists of two main operations : 1 ) kalman smoothing to obtain the _ a posteriori _ statistics of the source signal vectors and 2 ) greedy tree search to estimate the support of the signal vectors . through numerical experiments , we demonstrate that the proposed skts algorithm is effective in recovering the sparse signals and performs close to the oracle ( genie - based ) kalman estimator .    </S>",
    "<S> compressed sensing , simultaneously sparse signal , multiple measurement vector , expectation - maximization ( em ) algorithm , maximum likelihood estimation </S>"
  ]
}