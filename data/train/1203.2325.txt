{
  "article_text": [
    "preconditioned krylov subspace methods @xcite are among the most popular iterative solvers for large sparse linear system of equations @xmath1 where @xmath2 is a nonsingular and nonsymmetric ( non - hermitian ) @xmath3 matrix and @xmath4 is an @xmath5-dimensional vector .",
    "sparse approximate inverse ( sai ) preconditioning aims to construct sparse approximations of @xmath6 directly and is nowadays one class of important general - purpose preconditioning for krylov solvers .",
    "there are two typical kinds of sai preconditioning approaches .",
    "one constructs a factorized sparse approximate inverse ( fsai ) .",
    "an effective algorithm of this kind is the approximate inverse ( ainv ) algorithm , which is derived from the incomplete ( bi)conjugation procedure @xcite .",
    "the other is based on f - norm minimization and is inherently parallelizable .",
    "it aims to construct @xmath7 by minimizing @xmath8 for a specified pattern of @xmath0 that is either prescribed in advance or determined adaptively , where @xmath9 denotes the f - norm of a matrix . a hybrid version , i.e.",
    ", the factorized approximate inverse ( fsai ) preconditioning based on f - norm minimization , has been introduced by kolotilina and yeremin@xcite .",
    "fsai is generalized to block form , called bfsai in @xcite . an adaptive algorithm in @xcite",
    "is presented that generates automatically the nonzero pattern of the bfsai preconditioner .",
    "in addition , the idea of f - norm minimization is generalized in @xcite by introducing a sparse readily inverted _ target _ matrix @xmath10 .",
    "@xmath0 is then computed by minimizing @xmath11 over a space of matrices with a prescribed sparsity pattern , where @xmath12 is the generalized f - norm defined by @xmath13 with @xmath14 being some symmetric ( hermitian ) positive definite matrix , the superscript @xmath10 denotes the transpose of a matrix or vector , and is replaced by the conjugate transpose for a complex matrix @xmath15 .",
    "a good comparison of factorized sai and f - norm minimization based sai preconditioning approaches can be found in @xcite .",
    "sais have been shown to provide effective smoothers for multigrid ; see , e.g. , @xcite . for a comprehensive survey on preconditioning techniques",
    ", we refer the reader to @xcite .    in this paper , we focus on f - norm minimization based sai preconditioning , where a central issue is to determine the sparsity pattern of @xmath0 effectively .",
    "there has been much work on a - priori pattern prescriptions , see , e.g. , @xcite .",
    "once the pattern of @xmath0 or its envelop is given , the computation of @xmath0 is straightforward by solving @xmath5 independent least squares ( ls ) problems and @xmath0 is then further sparsified generally .",
    "this is called a static sai preconditioning procedure .",
    "huckle @xcite has compared different a - priori sparsity patterns and established effective upper bounds for the sparsity pattern of @xmath0 obtained by the famous adaptive spai algorithm @xcite .",
    "he shows that the patterns of @xmath16 , @xmath17 and @xmath18 for small @xmath19 can be good envelop patterns of a good @xmath0 .",
    "these patterns are very useful for reducing communication times when distributing and then computing @xmath0 in a distributed and parallel computing environment .    for a general sparse matrix @xmath2 , however , determining an effective sparsity pattern of @xmath6",
    "is nontrivial .",
    "a - priori sparse patterns may not capture positions of large entries in @xmath6 effectively , or , they may capture the positions only when the patterns are unacceptably dense . then the storage becomes a bottleneck and the time for the construction of the matrix is impractical . to cope with this difficulty",
    ", a number of researchers have proposed adaptive strategies that start with a simple initial pattern and successively augment or adaptively adjust this pattern until @xmath0 is satisfied with certain accuracy , i.e. , @xmath20 for some norm , where @xmath21 is fairly small , or a maximum number of nonzero entries in @xmath0 is reached .",
    "this idea was first proposed by cosgrove _",
    "@xcite , and developed by grote and huckle @xcite , gould and scott @xcite and chow and saad @xcite . from @xcite it appears that the spai preconditioning proposed by grote and huckle @xcite is more robust than the one proposed by chow and saad @xcite .",
    "one of the key differences between these procedures is that they use different adaptive ways to generate sparsity patterns of @xmath0 by dropping entries of small magnitude so as to sparsify @xmath0 .",
    "recently , jia and zhu @xcite have proposed a power sparse approximate inverse ( psai ) procedure that determines the sparsity pattern of @xmath0 in a new adaptive way .",
    "furthermore , they have developed a practical psai algorithm with dropping , called psai(@xmath22 ) , that dynamically drops the entries in @xmath0 whose magnitudes are smaller than a prescribed tolerance @xmath22 during the process .",
    "extensive numerical experiments in @xcite demonstrate that the psai(@xmath22 ) is at least comparable to spai in @xcite .",
    "as is well - known there are three goals for using dropping strategies in the sai preconditioning procedure : ( i ) @xmath0 should be an effective preconditioner ( ii ) @xmath0 should be as sparse as possible so that it is cheap to set up and then to use in a krylov solver , when its pattern is determined adaptively , and ( iii ) @xmath0 should be as sparse as possible so as to be cheap to use in a krylov solver , when its sparsity pattern is prescribed . apparently , dropping is a key step and plays a central role in designing a robust sai preconditioning procedure .",
    "chow @xcite suggests a prefiltration strategy and drops the entries of @xmath2 itself that are below some tolerance before determining the pattern of @xmath0 .",
    "this prefiltration idea is also adopted in , e.g. , @xcite . instead of prefiltration",
    ", it may be more effective to apply the sparsification to @xmath0 after it has been computed , which is called postfiltration ; see , e.g. , @xcite .",
    "wang and zhang @xcite have proposed a multistep static sai preconditioning procedure that uses both preliftration and postfiltration .",
    "obviously , for a static sai procedure , postfiltration can not reduce the construction cost of @xmath0 ; rather , it only reduces the application cost of @xmath0 at each iteration of a krylov solver . for an adaptive sai procedure",
    ", a more effective approach is to dynamically drop entries of small magnitude as they are generated during the construction process .",
    "the approach is more appealing as it makes @xmath0 sparse throughout the whole setup process .",
    "as is clear , dropping is more important for an adaptive sai procedure than for a static one since it reduces the setup time of @xmath0 for the former but not for the latter . for sparsification applied to fsai , we refer the reader to @xcite .    in this paper , we are concerned with dropping tolerance strategies applied to the adaptive psai procedure .",
    "we have noticed that the dropping tolerances used in the literature are heuristic and empirical .",
    "one commonly takes some small quantities , say @xmath23 , as dropping tolerances .",
    "nevertheless , the mechanism for dropping tolerances is by no means so simple .",
    "empirically chosen tolerances are not necessarily robust , may not be effective , and might even lead to failure in preconditioning . obviously , improperly chosen large tolerances may lead to a sparser but ineffective @xmath0 , while tolerances that are too small may lead to a far denser but more effective preconditioner @xmath0 which is much more time consuming to apply .",
    "our experiments confirm these statements , and illustrate that simply taking seemingly small tolerances , as suggested in the literature , may produce a numerical singular @xmath0 , which can cause a krylov solver to fail completely .",
    "therefore , dropping tolerance selection criteria deserve attention and it is desirable to establish a mathematical theory that can reveal intrinsic relationships between the dropping tolerances and the quality of @xmath0 .",
    "such selection criteria enable the design of robust and effective sai preconditioning procedures .",
    "we point out that dropping has been extensively used in other important preconditioning techniques such as ilu factorizations @xcite .",
    "some effective selection criteria have been proposed for dropping tolerances in , e.g. , @xcite .",
    "it is distinctive that the setup time of good sparse approximate inverses overwhelms the cost of krylov solver iterations while this is not necessarily the case for ilu preconditioners .",
    "this is true in a parallel computing environment , though spai and psai(@xmath22 ) are inherently parallelizable .",
    "therefore , sai type preconditioners are particularly attractive for solving a sequence of linear systems with the same coefficient matrix , as has been addressed in the literature , e.g. , @xcite , where bicgstab preconditioned with the adaptive spai algorithm @xcite and the factorized ainv algorithm @xcite are experimentally shown to be faster than bicgstab preconditioned with ilu(0 ) , even in the sequential computing environment when more than one linear systems is solved .",
    "the goal of this paper is to analyze and establish a rigorous theory for the dropping tolerance selection criteria used in psai .",
    "the quality and non - singularity of @xmath0 obtained by psai depends on , and can be very sensitive to , the dropping tolerances . based on our theory , we propose an _ adaptive _ dropping criterion that is used to drop entries of small magnitude dynamically during the setup process of @xmath0 by psai .",
    "the criterion aims to make @xmath0 as sparse as possible , while possessing comparable quality to a possibly much denser @xmath0 obtained by psai without dropping . as a byproduct",
    ", the theory applies to static f - norm minimization based sai preconditioning procedures , and a similar dropping criterion is derived that runs postfiltration robustly after @xmath0 is computed by a static sai procedure , making @xmath0 and its sparsification of comparable preconditioning quality . as has been noted already , however , as compared to adaptive sai procedures , dropping in static sai procedures",
    "does not reduce the setup time of the preconditioner , rather it reduces the cost of applying the sparser @xmath0 in the krylov iteration .",
    "our numerical experiments illustrate that the dropping tolerance criteria work well in general , and that the quality and effectiveness of @xmath0 depends critically on , and is sensitive to , these criteria .",
    "in particular , the reported numerical results demonstrate that ( i ) smaller tolerances are not necessary since they may make @xmath0 denser and more time consuming to construct , while not offering essential improvements in the quality of @xmath0 , and ( ii ) larger tolerances may lead to a numerically singular @xmath0 so that preconditioning fails completely .",
    "the paper is organized as follows . in section 2 , we review the basic psai ( bpsai ) procedure without dropping and the psai(@xmath22 ) procedure with dropping @xcite . in section 3 , we present results and establish robust dropping tolerance selection criteria .",
    "in section 4 , we test psai(@xmath22 ) on a number of real world problems , justifying our theory and illustrating the robustness and effectiveness of our selection criterion for dropping tolerances .",
    "we also test the three static f - norm minimization based sai procedures with the patterns of @xmath16 , @xmath24 and @xmath25 and illustrate the effectiveness of our selection criterion for dropping tolerances . finally concluding remarks are presented in section 5 .",
    "the bpsai procedure is based on f - norm minimization and determines the sparsity pattern of @xmath0 adaptively during the process . according to the cayley  hamilton theorem , @xmath6 can be expressed as a matrix polynomial of @xmath2 of degree @xmath26 with @xmath27 : @xmath28 with @xmath29 , the identity matrix , and @xmath30 , being certain constants .    following @xcite , for @xmath31 , we shall denote by @xmath32 the entry of @xmath33 in position @xmath34 , @xmath35 , and set @xmath36 . for @xmath37 , define @xmath38 .",
    "let @xmath39 $ ] be an approximate inverse of @xmath2 .",
    "bpsai computes each @xmath40 , @xmath41 , by solving the ls problem @xmath42 where @xmath43 is the vector 2-norm and the matrix spectral norm and @xmath44 is the @xmath19th column of the @xmath3 identity matrix @xmath45 .",
    "we exit and output @xmath40 when the minimum in ( [ lsprob ] ) is less than a prescribed tolerance @xmath21 or @xmath46 exceeds @xmath47 .",
    "we comment that @xmath48 can be updated from the available @xmath49 very efficiently ; see @xcite for details .",
    "the bpsai procedure is summarized as algorithm [ alg1 ] , in which @xmath50 denotes the @xmath19th column of @xmath51 and @xmath52 .",
    "it is easily justified that if @xmath47 steps are performed then the sparsity pattern of @xmath0 is contained in that of @xmath53 .    for @xmath54 ,",
    "compute @xmath40 :    set @xmath55 , @xmath52 and take @xmath56 as the initial sparsity pattern of @xmath40 .",
    "choose an accuracy requirement @xmath21 and the maximum @xmath47 of outer loops .",
    "solve ( [ lsprob ] ) for @xmath40 and let @xmath57 .",
    "@xmath58 , and augment the set @xmath59 by bringing in the indices of the nonzero entries in @xmath60 .",
    "@xmath61 . set @xmath62 , and go to 3 ; set @xmath62 solve ( [ lsprob ] ) for updating @xmath40 and @xmath57 . if @xmath63 , then break .",
    "it is shown in ( * ? ? ?",
    "* theorem 1 ) that if @xmath2 is sparse _ irregularly _ , that is , there is at least one column of @xmath2 whose number of nonzero entries is considerably more than the average number of nonzero entries per column , then @xmath0 may become dense very quickly as @xmath46 increases .",
    "however , when most entries of @xmath6 are small , the corresponding entries of a good approximate inverse @xmath0 for @xmath6 are small too , and thus contribute very little to @xmath6 .",
    "therefore , in order to control the sparsity of @xmath0 and construct an effective preconditioner , we should apply dropping strategies to bpsai .",
    "psai(@xmath22 ) just serves this purpose .",
    "it aims to effectively determine an approximate sparsity pattern of @xmath6 and capture its large entries . at each while - loop in psai(@xmath22 ) , for the new available @xmath40 , entries of small magnitude below a prescribed tolerance @xmath22 are dropped and only large ones are retained .",
    "we describe the psai(@xmath22 ) algorithm as algorithm [ alg2 ] , in which the sparsity pattern of @xmath40 is denoted by @xmath64 , @xmath37 , which are updated according to steps 911 of algorithm 2 .",
    "hence , for every @xmath19 , we solve the ls problem @xmath65 similar to bpsai , @xmath66 can be updated from the available @xmath67 very efficiently .    for @xmath54 ,",
    "compute @xmath40 :    set @xmath55 , @xmath52 and @xmath68 as the initial sparsity pattern of @xmath40 .",
    "choose an accuracy requirement @xmath21 , dropping tolerance @xmath22 and the maximum @xmath47 of outer loops .",
    "solve ( [ lsprob2 ] ) for @xmath40 and let @xmath57 .",
    "@xmath58 , and augment the set @xmath59 by bringing in the indices of the nonzero entries in @xmath60 .",
    "@xmath61 . set @xmath62 , and go to 3 ; @xmath69 solve ( [ lsprob2 ] ) for @xmath40 and compute @xmath57 . if @xmath63 , perform 11 and break . drop the entries of small magnitude in @xmath40 whose sizes are below @xmath22 and delete the corresponding indices from @xmath70 .",
    "set @xmath62    from now on we denote by @xmath0 the preconditioners generated by either bpsai or psai(@xmath22 ) .",
    "we will distinguish them by @xmath0 and @xmath71 , respectively when necessary .",
    "the non - singularity and quality of @xmath0 by bpsai clearly depends on @xmath21 , while the situation becomes much more complicated for @xmath71 .",
    "we will consider these theoretical issues in the next section . at present",
    "it should be clear that for bpsai the non - singularity and quality of @xmath0 is determined by @xmath21 and @xmath47 , two parameters that control while - loop termination in algorithm [ alg1 ] .",
    "on the one hand , a smaller @xmath21 will generally give rise to higher quality but possibly denser preconditioner @xmath0 .",
    "as a result , more while - loops @xmath47 are used , so that the setup cost of @xmath0 is higher .",
    "we reiterate that it is also more expensive to apply a denser @xmath0 at each iteration of a krylov solver . on the other hand",
    ", a bigger @xmath21 may generate a sparser but less effective @xmath0 , so that the krylov solvers use more iterations to achieve convergence .",
    "unfortunately the selection of @xmath21 can only be empirical . as is standard in the literature , in numerical experiments we simply take @xmath21 to be a fairly small quantity , say @xmath72 .",
    "first of all , we should keep in mind that all sai preconditioning procedures are based on the basic hypothesis that the majority of entries of @xmath6 are small , that is , there exist sparse approximate inverses of @xmath2 .",
    "mathematically , this amounts to supposing that there exists ( at least ) a sparse @xmath0 such that the residual @xmath20 for some fairly small @xmath21 and some matrix norm @xmath73 .",
    "the size of @xmath21 is a reasonable measure for the quality of @xmath0 as an approximation to @xmath6 . generally speaking , the smaller @xmath21 , the more accurate @xmath0 as an approximation of @xmath6 .    in the following discussion",
    ", we will assume that bpsai produces a nonsingular @xmath0 satisfying @xmath20 for some norm , given fairly small @xmath21 .",
    "we comment that this is definitely achieved for a suitable @xmath47 . under this assumption , keep in mind that @xmath0 may be relatively dense but have many entries of small magnitude .",
    "psai(@xmath22 ) aims at dynamically dropping those entries of small magnitude below some absolute dropping tolerance @xmath22 during the setup of @xmath0 and computing a new sparser @xmath0 , so as to reduce storage memory and computational cost of constructing and applying @xmath0 as a preconditioner .",
    "we are concerned with two problems .",
    "the first problem is how to select @xmath22 to make @xmath0 nonsingular .",
    "as will be seen , since @xmath22 varies dynamically for each @xmath19 , @xmath41 , as @xmath46 increases from @xmath74 to @xmath47 in algorithm [ alg2 ] , we will instead denote it by @xmath75 when computing the @xmath19th column @xmath40 .",
    "the second is how to select the @xmath75 which are required to meet two requirements : ( i ) @xmath0 is as sparse as possible ; ( ii ) its approximation quality is comparable to that obtained by bpsai in the sense that the residuals of two @xmath0 have very comparable sizes . with such sparser @xmath0 , it is expected that krylov solvers preconditioned by bpasi and psai(@xmath22 ) , respectively , will use a comparable number of iterations to achieve convergence . if so , psai(@xmath22 ) will be considerably more effective than bpsai provided that @xmath0 obtained by psai(@xmath22 ) is considerably sparser than that provided by bpasi .",
    "as far as we are aware , these important problems have not been studied rigorously and systematically in the context of sai preconditioning .",
    "the establishment of robust selection criteria , @xmath75 , @xmath54 , for dropping tolerances that meet the two requirements is significant but nontrivial .    over the years",
    "the dropping reported in the literature has been empirical .",
    "one commonly applies a tolerance as follows : set @xmath76 to zero if @xmath77 , for some empirical value for @xmath22 , such as @xmath23 , see , e.g. @xcite . due to the absence of mathematical theory ,",
    "doing so is problematic , and one may either miss significant entries if @xmath22 is too large or retain too many superfluous entries if @xmath22 is too small . as a consequence ,",
    "@xmath0 may be of poor quality , or while a good approximate inverse it may be unduly denser than desirable , leading to considerably higher setup and application costs .    for general purposes , we should take the size of @xmath40 into account when dropping a small entry @xmath76 in @xmath40 .",
    "define @xmath78 to be the @xmath5-dimensional vector whose nonzero entries @xmath79 are those to be dropped in @xmath40 .",
    "precisely drop @xmath76 in @xmath40 when @xmath80 for some suitable norm @xmath73 , where @xmath81 is a relative dropping tolerance that is small and should be chosen carefully based on some mathematical theory . for suitably chosen @xmath81 ,",
    "our ultimate goal is to derive corresponding dropping tolerance selection criteria @xmath75 that are used to adaptively detect and drop small entries @xmath76 below the tolerance .    in what follows",
    "we establish a number of results that play a vital role in selecting the @xmath81 and @xmath75 effectively .",
    "the matrix norm @xmath73 denotes a general induced matrix norm , which includes the 1-norm and the 2-norm .",
    "[ thm1 ] assume that @xmath82 .",
    "then @xmath0 is nonsingular .",
    "define @xmath83 .",
    "if @xmath84 satisfies @xmath85 then @xmath71 is nonsingular .",
    "suppose that @xmath0 is singular and let @xmath86 with @xmath87 be an eigenvector associated with its zero eigenvalue(s ) , i.e. , @xmath88 .",
    "then for any induced matrix norm we have @xmath89 a contradiction to the assumption that @xmath90 .",
    "so @xmath0 is nonsingular .",
    "since @xmath91 from ( [ ineq ] ) we have @xmath92 on the other hand , since @xmath93 we get @xmath94 which means @xmath95 substituting ( [ ineq1 ] ) into ( [ ineq2 ] ) , we have @xmath96 from which it follows that @xmath97 in ( [ eq1 ] ) is nonsingular and so is @xmath71 .",
    "denote by @xmath71 the sparse approximate inverse of @xmath2 obtained by psai(@xmath22 ) . then @xmath71 aims to retain the entries @xmath76 of large magnitude and drop those of small magnitude in @xmath0 .",
    "the entries of small magnitude to be dropped are those nonzero ones in the matrix @xmath84 .",
    "so , @xmath71 is generally sparser than @xmath0 , and the number of its nonzero entries is equal to that of @xmath0 minus that of @xmath84 .    in order to get an @xmath71 comparable to @xmath0 as an approximation to @xmath6",
    ", we need to impose further restrictions on @xmath84 and @xmath21 , as indicated below .",
    "[ thm2 ] assume that @xmath82 .",
    "then @xmath0 is nonsingular .",
    "let @xmath83 .",
    "if @xmath98 then @xmath71 is nonsingular and @xmath99 specifically , if @xmath100 , then @xmath101 and @xmath102    the non - singularity of @xmath0 is already proved in theorem  [ thm1 ] . since @xmath84 satisfying ( [ tol_min ] ) must meet ( [ ineq ] ) , the non - singularity of @xmath71 follows from theorem  [ thm1 ] directly . from @xmath20 and ( [ tol_min ] )",
    ", we obtain @xmath103 ( [ tol2 ] ) and ( [ smallrd ] ) are direct from ( [ tol_min ] ) and ( [ rd ] ) , respectively .    in what follows",
    "we always assume that @xmath104 , so that ( [ tol2 ] ) is satisfied and the residual @xmath105 .",
    "this assumption is purely technical for the brevity and beauty of presentation .",
    "the case that @xmath106 can be treated accordingly .",
    "the later theorems can be adapted for this case , but are not considered here .",
    "it is known that @xmath0 is a good approximation to @xmath6 for a small @xmath21 .",
    "this theorem tells us that if dropping tolerances @xmath75 make @xmath84 satisfy ( [ tol2 ] ) then the @xmath71 and @xmath0 have comparable residuals and are approximate inverses of @xmath2 with comparable accuracy , provided that @xmath21 is fairly small . in this case , we claim that they possess a similar preconditioning quality for a krylov solver , and it is expected that the krylov solver preconditioned by @xmath71 and @xmath0 , respectively , use a comparable number of iterations to achieve convergence .    in the above results , the assumptions and bounds are determined by matrix norms , which are thus not directly applicable to meet our goals . to be more practical",
    ", we now present a theorem under the assumption that @xmath107 for @xmath54 , which is just the stopping criterion in algorithms  [ alg1][alg2 ] and the spai algorithm @xcite , etc .",
    ", where the norm is the 2-norm .",
    "[ thm3 ] for a given vector norm @xmath73 , let @xmath39 $ ] satisfy @xmath108 for @xmath54 and let @xmath109 $ ] with @xmath110 $ ] .",
    "if @xmath111 then @xmath112    let @xmath57 .",
    "then from @xmath113 we get @xmath114 from which , with the assumption of the theorem , ( [ 2eps ] ) holds .",
    "still , this theorem does not fit nicely for our use .",
    "for the later theoretical and practical background , we present a mixed norm result , which is a variant of theorem  [ thm3 ] .",
    "[ thm5 ] let @xmath39 $ ] satisfy @xmath115 for @xmath54 and let @xmath109 $ ] with @xmath110 $ ] .",
    "if @xmath116 then @xmath117    let @xmath57 .",
    "then from @xmath113 we get @xmath118 from which , with the assumption ( [ fksmallnorm1 ] ) , ( [ 2epsnorm2 ] ) holds .",
    "theorem  [ thm5 ] can not guarantee that @xmath0 and @xmath71 are nonsingular . in @xcite , grote and huckle",
    "have presented some theoretical properties of a sparse approximate inverse .",
    "particularly , for the matrix 1-norm , theorem 3.1 and corollary 3.1 of @xcite read as follows when applied to @xmath0 and @xmath71 defined by theorem  [ thm5 ] .",
    "[ grotehuckle ] let @xmath57 , @xmath119 and @xmath120 , @xmath121",
    ". then if @xmath122 and @xmath123 , we have @xmath124 furthermore , if @xmath125 and @xmath126 , respectively , @xmath0 and @xmath71 are nonsingular and @xmath127    theorem  [ thm5 ] indicates that given @xmath104 and @xmath47 , if the while - loop in bpsai terminates due to @xmath63 for all @xmath19 and dropping tolerance @xmath75 is selected such that ( [ fksmallnorm1 ] ) holds , then the corresponding columns of @xmath71 and @xmath0 are of similar quality provided that @xmath21 is fairly small .",
    "it is noted in @xcite for the spai that @xmath128 is usually much smaller than @xmath5 .",
    "this is also the case for bpsai and psai(@xmath22 ) .",
    "however , we should realize that such a sufficient condition is very conservative , as pointed out in @xcite .",
    "in practice , for a rather mildly small @xmath21 , say @xmath129 , @xmath0 is rarely singular .",
    "theorem  [ grotehuckle ] shows that @xmath71 and @xmath0 are approximate inverses of @xmath2 with similar accuracy and are expected to have a similar preconditioning quality .",
    "besides , since @xmath40 is generally denser than @xmath130 , @xmath131 is heuristically denser than @xmath132 , i.e. , @xmath133 is more than likely to be smaller than @xmath128 .",
    "consequently , @xmath134 is comparable to @xmath135 .",
    "this means that the bounds for @xmath71 are close to and furthermore may not be bigger than the corresponding ones for @xmath0 in theorem  [ grotehuckle ] , so @xmath71 and @xmath0 are approximations to @xmath6 with very similar accuracy or quality .",
    "theorems  [ thm2][grotehuckle ] are fundamental and relate the quality of @xmath71 to that of @xmath0 in terms of @xmath21 quantitatively and explicitly .",
    "they provide necessary ingredients for reasonably selecting relative dropping tolerance @xmath81 in ( [ ineq5 ] ) to get a possibly much sparser preconditioner @xmath71 that has a similar preconditioning quality to @xmath0 . in what follows",
    "we present a detailed analysis and propose robust selection criteria for dropping tolerance @xmath75 .",
    "for given @xmath47 , suppose that @xmath0 obtained by bpsai is nonsingular and satisfies @xmath108 for @xmath54 . to achieve our goal , the crucial point is how to combine ( [ ineq5 ] ) with condition ( [ fksmallnorm1 ] ) in theorem  [ thm5 ] in an organic and reasonable way .",
    "for the 1-norm in ( [ ineq5 ] ) , a unification of ( [ ineq5 ] ) and ( [ fksmallnorm1 ] ) means that @xmath136 at every while - loop in psai(@xmath22 ) , where the bound in the first relation is to be determined and the bound in the second relation is given explicitly .",
    "this is the starting point for the analysis determining how to drop the small entries @xmath76 in @xmath40 .    before proceeding , supposing that @xmath81 is given in a disguise , we investigate how to choose @xmath78 to make ( [ ineq6 ] ) hold",
    "obviously , it suffices to drop nonzero @xmath76 , @xmath137 in @xmath40 as long as its size is no more than the bounds in ( [ ineq6 ] ) divided by @xmath138 .",
    "since @xmath138 is not known a - priori , in practice we replace it by the currently available @xmath139 before dropping , which is an _ upper bound _ for @xmath138 .",
    "therefore , we should drop an @xmath76 when it satisfies @xmath140 given ( [ ineq6 ] ) , we comment that each of the above bounds may be correspondingly conservative as @xmath141 .",
    "but it seems hard , if not impossible , to replace the unknown @xmath138 by any other better computable estimate than @xmath139 .",
    "next we go to our central concern and discuss how to relate @xmath81 to @xmath21 so as to establish a robust selection criterion @xmath75 for dropping tolerances .",
    "precisely , as ( [ tolk ] ) has indicated , we aim at selecting suitable relative tolerance @xmath81 and then drop entries of small magnitude in @xmath40 below @xmath142 . by theorems  [ thm5][grotehuckle ] , the second bound in ( [ ineq6 ] ) and its induced bound in ( [ tolk ] ) serve to guarantee that @xmath71 has comparable preconditioning quality to @xmath0 .",
    "therefore , if an @xmath76 satisfies the second relation in ( [ tolk ] ) , it should be dropped .",
    "otherwise , if @xmath81 satisfies @xmath143 and we use the dropping criterion @xmath144for some @xmath19 , we would possibly drop an excessive number of nonzero entries and @xmath71 would be too sparse .",
    "the resulting @xmath71 may mean that ( [ fksmallnorm1 ] ) is not satisfied and that @xmath71 is a poor quality preconditioner , possibly also numerically singular , which could lead to a complete failure of the krylov solver .",
    "thus larger @xmath75 should not be selected .    on the other hand ,",
    "if we chose @xmath81 such that @xmath145 and took the dropping criterion @xmath146theorem  [ thm5 ] would hold and the preconditioning quality of @xmath71 would be guaranteed and comparable to that of @xmath0 .",
    "however , theorems  [ thm5][grotehuckle ] show that the accuracy of such @xmath71 can not be improved as approximate inverses of @xmath2 as @xmath81 and @xmath75 become smaller .",
    "computationally , it is crucial to realize that the smaller @xmath75 , generally the denser @xmath71 , leading to an increased setup cost for @xmath71 and more expensive application of @xmath71 in a krylov iteration . as a consequence ,",
    "such smaller @xmath75 are not desirable and may lower the efficiency of constructing @xmath71 .",
    "consequently such smaller values for @xmath75 should be abandoned .    in view of the above analysis",
    ", it is imperative that we find an optimal balance point .",
    "our above arguments have suggested an optimal and most effective choice for @xmath81 : we should select @xmath81 to make two bounds in ( [ ineq6 ] ) equal : @xmath147 from ( [ tolk ] ) , this selection leads to our ultimate dropping criterion @xmath148    we point out that since ( [ fksmallnorm1 ] ) is a sufficient but not necessary condition for ( [ 2epsnorm2 ] ) , @xmath75 defined above is also sufficient but not necessary for ( [ 2epsnorm2 ] ) .",
    "also , it may be conservative since we replace the smaller true value @xmath138 by its upper bound @xmath139 in the denominator . as a result",
    ", @xmath75 may be considerably smaller than it should be in an ideal case .",
    "we should note that @xmath81 and @xmath75 are varying parameters during the while - loop in algorithm 2 as @xmath139 changes when the while - loop @xmath46 increases from @xmath74 to @xmath47 .",
    "in the literature one commonly uses _ fixed _ dropping tolerance @xmath22 when constructing a sai preconditioner @xmath0 , which is , empirically and heuristically , taken as some seemingly small quantity , say @xmath149 , @xmath23 or @xmath150 , without taking @xmath21 into consideration ; see , e.g. , @xcite .",
    "our theory has indicated that the non - singularity and preconditioning quality of @xmath71 is critically dependent and possibly sensitive to the choice of the dropping tolerance . for fixed tolerances that are larger than that defined by ( [ muepsilon2 ] ) for some @xmath19 during the construction of @xmath71",
    ", we report numerical experiments that indicate that the resulting @xmath71 obtained by psai(@xmath22 ) can be _ exactly singular in finite precision arithmetic_. we also report experiments that show decreasing such large tolerances by one order of magnitude , can provide high quality and nonsingular @xmath71 .",
    "thus , the robustness and effectiveness of @xmath71 depends directly on the tolerance .",
    "we stress that theorems  [ thm1][grotehuckle ] hold for a generally given approximate inverse @xmath0 of @xmath2 and do not depend on a specific f - norm minimization based sai preconditioning procedure .",
    "note that , for all the static f - norm minimization based sai preconditioning procedures , the high quality @xmath0 constructed from @xmath2 itself are often quite dense and their applications in krylov solvers can be time consuming . to improve the overall performance of solving @xmath151 , one often sparsifies @xmath0 after its computation , by using postfiltration on @xmath0 to obtain a new sparser approximate inverse @xmath71 @xcite .",
    "however , as already stated in the introduction , postfiltration itself can not reduce the cost of constructing @xmath0 but can reduce the cost of applying @xmath0 in krylov iterations .    as a byproduct",
    ", our theory can be very easily adapted to a static f - norm minimization based sai preconditioning procedure .",
    "the difference and simplification is that , for a static sai procedure , @xmath81 in ( [ muepsilon2 ] ) and @xmath75 are fixed for each @xmath19 as @xmath40 and @xmath139 are already determined a - priori before dropping is performed on @xmath0 .",
    "practically , after computing @xmath0 by a static sai procedure , we record @xmath152 and compute the _ constants _",
    "@xmath139 for @xmath54 .",
    "assume that @xmath153 . then by ( [ tolk ] ) and ( [ muepsilon2 ] ) we drop @xmath76 whenever @xmath154 in such a way , based on theorem  [ thm5 ] we get a new sparser approximate inverse @xmath71 whose @xmath19th column @xmath130 satisfies @xmath155 .",
    "define @xmath156 .",
    "then theorem  [ grotehuckle ] holds .",
    "so @xmath71 has a similar preconditioning quality to the generally denser @xmath0 obtained by the static sai procedure without dropping .",
    "we reiterate , however , that in contrast to adaptive psai(@xmath22 ) where small entries below a tolerance are dropped immediately when they are generated during the while loop of algorithm 2 , the static sai procedure does not reduce the setup cost of @xmath71 since it performs sparsification only after computation of @xmath0 .",
    "there is relatively greater benefit in dropping in adaptive sai preconditioning .",
    "in this section we test a number of real world problems coming from scientific and engineering applications , which are described in table [ table1 ] .",
    "we shall demonstrate the robustness and effectiveness of our selection criteria for dropping tolerances applied to psai(@xmath22 ) and , as a byproduct , three f - norm minimization based static sai preconditioning procedures .",
    "the numerical experiments are performed on an intel(r ) core ( tm)2duo quad cpu e8400 @ @xmath157ghz processor with main memory 2 gb using matlab 7.8.0 with the machine precision @xmath158 under the linux operating system .",
    "preconditioning is from the right except pores_2 , for which we found that left preconditioning outperforms right preconditioning very considerably .",
    "it appears that the rows of pores_2s inverse can be approximated more effectively than its columns by psai(@xmath22 ) .",
    "krylov solvers employed are bicgstab and the restarted gmres(50 ) algorithms @xcite , and we use the codes from matlab 7.8.0 . we comment that if the output of iterations for the code bicgstab.m is @xmath19 , the dimension of the krylov subspace is @xmath159 and bicgstab performs @xmath159 matrix - vector products .",
    "the initial guess is always @xmath160 , and the right - hand side @xmath4 is formed by choosing the solution @xmath161^t$ ] .",
    "the stopping criterion is @xmath162 where @xmath163 is the approximate solution obtained by bicgstab or gmres(50 ) applied to the preconditioned linear system @xmath164 .",
    "we run all the algorithms in a sequential environment .",
    "we will observe that the setup cost for @xmath0 dominates the entire cost of solving @xmath151 .",
    "as stressed in the introduction , this is a distinctive feature of sai preconditioning procedures even in a distributed parallel environment .    .    in the experiments , we take different @xmath21 and suitably small integer @xmath47",
    "so as to control the quality of @xmath0 in algorithms  [ alg1][alg2 ] , i.e. , the bpsai and psai(@xmath22 ) algorithms , in which the while - loop terminates when @xmath165 or @xmath166 . in all the tables",
    ", we use the following notations :    * @xmath21 : the accuracy requirements in algorithms  [ alg1][alg2 ] ; * @xmath47 : the maximum while - loops that algorithms  [ alg1][alg2 ] allow ; * @xmath167 and @xmath168 : the iteration numbers of bicgstab and gmres(50 ) , respectively ; * @xmath169 : the sparsity of @xmath0 relative to @xmath2 ; * @xmath170 and @xmath171 : the minimum and maximum of @xmath75 defined by ( [ muepsilon2 ] ) for @xmath54 and @xmath37 ; * @xmath172 : the setup time ( in second ) of @xmath0 ; * @xmath173 ; * @xmath174 : the number of columns of @xmath0 that fail to meet the accuracy requirement @xmath21 ; * @xmath175 : flags convergence not attained within 1000 iterations .",
    "we report the results in tables  [ table2][static3 ] .",
    "our aims are four fold : ( i ) our selection criterion ( [ muepsilon2 ] ) for @xmath75 works very robustly and effectively since krylov solvers preconditioned by psai(@xmath22 ) and bpsai use almost the same iterations , the @xmath75 smaller than those defined by ( [ muepsilon2 ] ) are not necessary , rather they increase the total cost of solving linear systems since they do not improve the preconditioning quality of @xmath71 , increase the setup time of @xmath71 and make @xmath71 become denser .",
    "( ii ) the quality of @xmath71 depends on the choice of @xmath75 critically and an empirically chosen fixed small @xmath75 may produce a numerically singular @xmath71 .",
    "( iii ) @xmath75 of one order smaller than those in case ( ii ) may dramatically improve the preconditioning effectiveness of @xmath71 .",
    "this means that an empirically chosen @xmath22 may fail to produce a good preconditioner .",
    "( iv ) as a byproduct , we show that the selection criterion ( [ staticcriterion ] ) for @xmath75 works well for static f - norm minimization sai preconditioning procedures with three common prescribed patterns .",
    "we present the results on ( i)(iii ) in subsection 4.1 and the results on ( iv ) in subsection 4.2 , respectively .",
    "we shall illustrate that our dropping criterion ( [ muepsilon2 ] ) for @xmath75 is robust for various parameters @xmath21 and @xmath47 .",
    "we will show that for a smaller @xmath21 we need more while loops , and resulting @xmath71 are denser and cost more to construct , but are more effective for accelerating bicgstab and gmres(50 ) , that is , the krylov solvers use fewer iterations to achieve convergence .",
    "we also show that for fairly small @xmath176 , algorithms  [ alg1][alg2 ] can compute a good sparse approximation @xmath0 of @xmath6 with accuracy @xmath21 for small integer @xmath47 , and the maximum @xmath177 is needed for @xmath178 .",
    "we summarize the results obtained by the two krylov solvers with and without psai(@xmath22 ) preconditioning in table  [ table2 ] .",
    "we see that the two krylov solvers without preconditioning failed to solve most test problems within 1000 iterations while two krylov solvers are accelerated by psai(@xmath22 ) preconditioning substantially and they solved all the problems quite successfully except for @xmath179 and @xmath180 , where gmres(50 ) did not converge for fidap024 , fidap036 and sherman3 .",
    "particularly , the krylov solvers preconditioned by psai(@xmath22 ) solved sherman2 very quickly and converged within 10 iterations for three given @xmath181 , but they failed to solve the problem when no preconditioning is used .",
    "lcc@  c@  r@ ,  l@  c@  ccc@  c@  r@ ,  l@  c@  c@ & & & & & + matrix & @xmath182&@xmath168 & & @xmath183&@xmath172 & @xmath167&@xmath168&@xmath184&@xmath174 & & @xmath183&@xmath172 & @xmath167&@xmath168&@xmath184&@xmath174 + epb1 & 433&@xmath175 & & 3.20 & 112.36 & 120&272 & 0.20 & 0&&3.20 & 111.36 & 120&272 & 0.20 & 0 + fidap024 & @xmath175&@xmath175 & & 8.80 & 121.52 & 27&40 & 0.20 & 0&&8.80 & 121.52 & 27&40 & 0.20 & 0 + fidap028 & @xmath175&@xmath175 & & 9.97 & 423.75 & 31&42 & 0.26 & 29&&10.11 & 437.36 & 31&41 & 0.20 & 0 + fidap031 & @xmath175&@xmath175 & & 6.40 & 267.74 & 58&103 & 0.35 & 1&&6.41 & 269.70 & 58&102 & 0.20 & 0 + fidap036 & @xmath175&@xmath175 & & 5.78 & 63.88 & 34&48 & 0.20 & 0&&5.78 & 63.88 & 34&48 & 0.20 & 0 + nos3 & 213 & @xmath175 & & 3.89 & 3.72 & 49&98 & 0.20&0&&3.89 & 3.72 & 49&98 & 0.20&0 + nos6 & @xmath175&@xmath175 & & 2.73 & 0.48 & 19&24 & 0.20&0&&2.73 & 0.48 & 19&24 & 0.20&0 + orsirr_1 & @xmath175&@xmath175 & & 10.15 & 7.41 & 15&26 & 0.20 & 0&&10.15 & 7.41 & 15&26 & 0.20 & 0 + orsirr_2 & @xmath175&@xmath175 & & 10.71 & 6.53 & 16&25 & 0.20 & 0&&10.71 & 6.53 & 16&25 & 0.20 & 0 + orsreg_1 & 687&346 & & 9.16 & 19.49 & 18&29 & 0.20 & 0&&9.16 & 19.49 & 18&29 & 0.20 & 0 + pores_2 & @xmath175&@xmath175 & & 17.41&26.38&19&26&0.27&15&&17.66&27.53&19&27&0.20&0 + sherman1 & 356&@xmath175&&6.54 & 1.37 & 18&28 & 0.27 & 2&&6.58 & 1.38 & 18&28 & 0.20 & 0 + sherman2 & @xmath175&@xmath175&&3.40 & 6.58 & 4&6 & 0.20 & 0&&3.40 & 6.58 & 4&6 & 0.20 & 0 + sherman3 & @xmath175&@xmath175&&4.86 & 10.52 & 81&229 & 0.32 & 32&&4.90 & 10.71 & 81&228 & 0.20 & 0 + sherman4 & 101&377 & & 3.36 & 0.76 & 24&34 & 0.20 & 0&&3.36 & 0.76 & 24&34 & 0.20 & 0 + sherman5 & @xmath175&@xmath175 & & 3.34 & 4.89 & 21&30 & 0.20 & 0&&3.34 & 4.89 & 21&30 & 0.20 & 0 +    .3 cm    @cr@ ,  lcc@  c@  r@ ,  l@  c@  ccc@  c@  r@ ,  l@  c@  c@ & & & & & + matrix & @xmath182&@xmath168 & & @xmath183&@xmath172 & @xmath167&@xmath168&@xmath184&@xmath174 & & @xmath183&@xmath172 & @xmath167&@xmath168&@xmath184&@xmath174 + epb1 & 433&@xmath175 & & 1.17 & 36.71 & 170&408 & 0.30 & 0&&1.17 & 36.71 & 170&408 & 0.30 & 0 + fidap024 & @xmath175&@xmath175 & & 5.22 & 34.52 & 46&98 & 0.38 & 12&&5.27 & 34.91 & 46&97 & 0.30 & 0 + fidap028 & @xmath175&@xmath175 & & 5.48 & 113.09 & 64&168 & 0.33 & 10&&5.50 & 117.28 & 64&159 & 0.30 & 0 + fidap031 & @xmath175&@xmath175 & & 3.08 & 58.39 & 104&387 & 0.56 & 2&&3.09 & 59.18 & 104&444 & 0.30 & 0 + fidap036 & @xmath175&@xmath175 & & 2.51 & 12.73 & 69&119 & 0.30 & 0&&2.51 & 12.73 & 69&119 & 0.30 & 0 + nos3 & 213 & @xmath175 & & 1.65 & 1.29 & 69&144 & 0.30 & 0&&1.65 & 1.29 & 69&144 & 0.30 & 0 + nos6 & @xmath175&@xmath175 & & 0.94 & 0.20 & 35&37 & 0.30 & 0&&0.94 & 0.20 & 35&37 & 0.30 & 0 + orsirr_1 & @xmath175&@xmath175 & & 5.36 & 3.46 & 25&37 & 0.30 & 0&&5.36 & 3.46 & 25&37 & 0.30 & 0 + orsirr_2 & @xmath175&@xmath175 & & 5.66 & 3.05 & 23&36 & 0.30 & 0&&5.66 & 3.05 & 23&36 & 0.30 & 0 + orsreg_1 & 687&346 & & 4.02 & 7.31 & 27&47 & 0.30 & 0&&4.02 & 7.31 & 27&47 & 0.30 & 0 + pores_2 & @xmath175&@xmath175 & & 8.67 & 6.84 & 37 & 51&0.51&12&&8.78&7.31&37&50&0.30&0 + sherman1 & 356&@xmath175&&2.86 & 0.70 & 27&40 & 0.38 & 2&&2.89 & 0.74 & 27&40 & 0.30 & 0 + sherman2 & @xmath175&@xmath175&&2.74 & 4.54 & 4&7 & 0.30 & 0&&2.74 & 4.54 & 4&7 & 0.30 & 0 + sherman3 & @xmath175&@xmath175&&1.93 & 4.89 & 145&627 & 0.35 & 34&&1.96 & 5.07 & 143&900 & 0.30 & 0 + sherman4 & 101&377 & & 1.25 & 0.35 & 34&49 & 0.30 & 0&&1.25 & 0.35 & 34&49 & 0.30 & 0 + sherman5 & @xmath175&@xmath175 & & 1.57 & 2.05 & 29&43 & 0.30 & 0&&1.57 & 2.05 & 29&43 & 0.30 & 0 +    .3 cm    @cr@ ,  lcc@  c@  r@ ,  l@  c@  ccc@  c@  r@ ,  l@  c@  c@ & & & & & + matrix & @xmath182&@xmath168 & & @xmath183&@xmath172 & @xmath167&@xmath168&@xmath184&@xmath174 & & @xmath183&@xmath172 & @xmath167&@xmath168&@xmath184&@xmath174 + epb1 & 433&@xmath175 & & 0.60 & 22.23 & 237&474 & 0.40 & 0&&0.60 & 22.23 & 237&474 & 0.40 & 0 + fidap024 & @xmath175&@xmath175 & & 3.26 & 12.77 & 95&@xmath175 & 0.42 & 6&&3.28 & 11.54 & 91 & @xmath175 & 0.40 & 0 + fidap028 & @xmath175&@xmath175 & & 3.33 & 37.70 & 99&299 & 0.40 & 0&&3.33 & 37.70 & 99&299 & 0.40 & 0 + fidap031 & @xmath175&@xmath175 & & 1.66 & 18.70 & 137&@xmath175 & 0.65 & 2&&1.68 & 20.71 & 141&801 & 0.40 & 0 + fidap036 & @xmath175&@xmath175 & & 1.76 & 5.99 & 85&250 & 0.40 & 0&&1.76 & 5.99 & 85&250 & 0.40 & 0 + nos3 & 213 & @xmath175 & & 0.50 & 0.40 & 106&536 & 0.38 & 0&&0.50 & 0.40 & 106&536 & 0.38 & 0 + nos6 & @xmath175&@xmath175 & & 0.56 & 0.14 & 38&44 & 0.40 & 0&&0.56 & 0.14 & 38&44 & 0.40 & 0 + orsirr_1 & @xmath175&@xmath175 & & 3.19 & 1.79 & 37&59 & 0.39 & 0&&3.19 & 1.79 & 37&59 & 0.39 & 0 + orsirr_2 & @xmath175&@xmath175 & & 3.26 & 1.49 & 38&60 & 0.39 & 0&&3.26 & 1.49 & 38&60 & 0.39 & 0 + orsreg_1 & 687&346 & & 2.13 & 4.02 & 40&67 & 0.38 & 0&&2.13 & 4.02 & 40&67 & 0.38 & 0 + pores_2 & @xmath175&@xmath175&&3.53 & 1.97&53&146&0.68&3&&3.58&2.04&59&147&0.40&0 + sherman1 & 356&@xmath175&&1.62 & 0.49 & 37&60 & 0.43 & 1&&1.63 & 0.49 & 36&60 & 0.40 & 0 + sherman2 & @xmath175&@xmath175&&2.42 & 3.59 & 5&8 & 0.40 & 0&&2.42 & 3.59 & 5&8 & 0.40 & 0 + sherman3 & @xmath175&@xmath175&&1.15 & 3.33 & 201&@xmath175 & 0.40 & 0&&1.15 & 3.33 & 201 & @xmath175 & 0.40 & 0 + sherman4 & 101&377 & & 0.88 & 0.25 & 41&59 & 0.40 & 0&&0.88 & 0.25 & 41&59 & 0.40 & 0 + sherman5 & @xmath175&@xmath175 & & 1.18 & 1.64 & 35&53 & 0.40 & 0&&1.18 & 1.64 & 35&53 & 0.40 & 0 +    now we take a closer look at psai(@xmath22 ) .",
    "the table shows that for @xmath181 , algorithm  [ alg2 ] used @xmath185 to attain the accuracy requirements , respectively .",
    "if we reduced @xmath47 to @xmath186 , there are only very few columns of @xmath0 for only a few matrices which do not satisfy the accuracy requirements , but the corresponding @xmath184 are still reasonably small and exceed @xmath21 no more than twice .",
    "this indicates that the corresponding @xmath0 are still effective preconditioners , as confirmed by the iterations used , but they are generally less effective than the corresponding ones obtained by the bigger @xmath47 which guarantee that @xmath0 computed by psai(@xmath22 ) succeeds for very small @xmath47 .",
    "table  [ table2 ] clearly tells us that for a smaller @xmath21 , psai(@xmath22 ) needs larger @xmath47 for the while loop .",
    "but a remarkable finding is that psai(@xmath22 ) succeeds for very small @xmath47 . given a rather mildly small @xmath21 like 0.3 and the generality of test problems , these experiments suggests that we may well set @xmath187 as a default value in algorithm  [ alg2 ] .",
    "we observe from table  [ table2 ] that for each problem the smaller @xmath21 the fewer iterations the two krylov solvers use .",
    "however , in the experiments , we notice that , for all problems except fidap024 , fidap31 and sherman3 , for which gmres(50 ) failed when @xmath179 , and given @xmath21 and @xmath47 , setup time of ptime of m and krylov iterations only occupy a very small percent . as we have addressed in the introduction , this is a typical feature of an effective sai preconditioning procedure and has been recognized widely in the literature , e.g. , @xcite .",
    "this is true even in a parallel computing environment .",
    "moreover , our matlab codes have not been optimized , and thus may give rise to lower performance .",
    "thus , we do not list the time for the krylov iterations in table  [ table2 ] . with this in mind , we find from table  [ table2 ] that for the first five matrices , orsreg_1 and pores_2 , the sparsity and construction cost of @xmath0 increases considerably as @xmath21 decreases .",
    "overall , to tradeoff effectiveness and general application , @xmath188 is a good choice for accuracy and the maximum number of while loops in psai(@xmath22 ) should be @xmath189 .",
    "regarding table  [ table2 ] , we finally point out a very important fact : for each of the test problems and given three choices for @xmath21 , bpsai and psai(@xmath22 ) with our dropping criterion use exactly the same value for @xmath47 to yield preconditioners attaining accuracy @xmath21 .",
    "this fact is important because it illustrates that the latter behaves like the former with the same choice for @xmath47 , while obtaining an equally effective preconditioner at less computational cost for setup .",
    "the next results illustrate three considerations .",
    "first , choosing a smaller @xmath75 is not required because the resulting @xmath71 is more dense and costs more to set up but is not necessarily a better preconditioner .",
    "second , for an improperly chosen fixed small @xmath22 , that is , @xmath190 at some while - loops of algorithm  [ alg2 ] , psai(@xmath22 ) may produce a numerically singular @xmath71 which will cause the complete failure of the preconditioning .",
    "third , for a @xmath22 that produces a singular @xmath71 , reducing @xmath22 by one order of magnitude , will yield an @xmath71 which is a good preconditioner but is less effective than the @xmath71 obtained with @xmath75 defined by ( [ muepsilon2 ] ) .",
    "this illustrates that choosing a fixed @xmath22 empirically is at risk for generating an ineffective @xmath71 .    to illustrate the first consideration ,",
    "we use the three matrices orsirr_1 , orsirr_2 and orsreg_1 and use psai(@xmath22 ) with @xmath178 , @xmath191 and with @xmath75 ranging from a little smaller to considerably smaller than that indicated by ( [ muepsilon2 ] ) .",
    "specifically , denote the right hand side in ( [ muepsilon2 ] ) by rhs , then we use @xmath192 , @xmath193 , @xmath194 and @xmath195 , and investigate the impact of the choice for the tolerance on the quality , sparsity and computational cost of setup of @xmath71 .",
    "we report the results in table  [ smallmu ] , where the tolerance @xmath196 corresponds to the bpsai procedure .",
    "for the three matrices , as @xmath174 in table  [ table2 ] and @xmath184 in table  [ smallmu ] indicate , the approximate inverses @xmath0 obtained by psai(@xmath22 ) with these different tolerances @xmath75 and bpsai have attained the accuracy @xmath21 . for each of these three problems , we can easily observe that @xmath0 becomes increasingly denser as @xmath75 decreases and @xmath0 is the densest for @xmath196 .",
    "however , the preconditioning quality of denser @xmath0 is not improved , since the corresponding numbers of krylov iterations are almost the same , as shown by @xmath167 and @xmath168 .",
    "moreover , we can see that the setup time @xmath172 of @xmath0 increases as @xmath75 decreases .",
    "for all the other test problems in table  [ table1 ] , we have also made numerical experiments in the above way .",
    "we find that the sparsity and preconditioning quality of @xmath0 obtained by psai(@xmath22 ) with the five @xmath75 changes very little .",
    "this means that our dropping criterion ( [ muepsilon2 ] ) enables us to drop entries of small magnitude in @xmath0 and smaller @xmath75 does not help any .",
    "together with table  [ smallmu ] , we conclude that our dropping criterion is effective and robust and it is not necessary to take smaller @xmath75 in psai(@xmath22 ) .",
    "ccccccc & & @xmath197&@xmath198&@xmath199 & @xmath200&@xmath196 + * orsirr_1 & @xmath183 & 10.15 & 10.81 & 12.02 & 13.13 & 16.77 + & @xmath172 & 7.41 & 7.60 & 8.34 & 8.56 & 9.05 + & @xmath201 & 15 ,  26 & 15 ,  26 & 15 ,  26 & 15 ,  26 & 15 ,  26 + & @xmath184 & 0.199974 & 0.199971 & 0.199970 & 0.199970&0.199970 + * orsirr_2 & @xmath183 & 10.71 & 11.29 & 12.42 & 13.52 & 16.70 + & @xmath172 & 6.53 & 6.58 & 6.68 & 7.45 & 8.11 + & @xmath201 & 16 ,  25 & 16 ,  25 & 14 ,  25 & 14 ,  25&14 ,  25 + & @xmath184 & 0.199974 & 0.199970 & 0.199970 & 0.199970&0.199970 + * orsreg_1 & @xmath183 & 9.16 & 9.63 & 11.27 & 12.97 & 16.82 + & @xmath172 & 19.49 & 20.42 & 23.35 & 25.84&35.47 + & @xmath201 & 18 ,  29 & 18 ,  29 & 18 ,  29 & 18 ,  29 & 18 ,  29 + & @xmath184 & 0.199853 & 0.199841 & 0.199840 & 0.199839&0.199839 +    0.6 cm    ccccccc & nos3&nos6&orsirr_1&orsirr_2&orsreg_1&sherman5 + @xmath22&@xmath149&@xmath202&@xmath23&@xmath23&@xmath23&@xmath149 + @xmath184&3.00&1.93&285.17&71.34&23.12&24.71 + @xmath170&@xmath203&@xmath204&@xmath205 & @xmath205&@xmath206&@xmath207 + @xmath171&@xmath208&@xmath209&@xmath210 & @xmath211&@xmath212 & @xmath213 +    0.2 cm    ( a):bad @xmath22 resulting in numerically singular @xmath0 for @xmath214    0.4 cm    ccccccc matrix&@xmath22&@xmath172&@xmath183&@xmath167&@xmath168&@xmath184 + nos3 & @xmath23 & 3.53 & 0.67 & 162 & @xmath175&0.68 + nos6 & @xmath215 & 0.47 & 1.78 & 79&72&0.73 + orsirr_1 & @xmath150&7.09 & 2.31 & 21 & 34&14.11 + orsirr_2 & @xmath150&6.32 & 2.62 & 41 & 50&14.11 + orsreg_1 & @xmath150&17.86 & 3.05 & 25 & 39&2.33 + sherman5 & @xmath23&4.64 & 1.72 & 22 & 32&4.14 +    0.2 cm    ( b):good @xmath22 leading to effective @xmath0 for @xmath214 .    [ fig1 ]     for orsirr_2 obtained by psai(@xmath22 ) with bad and good fixed @xmath22 and adaptive @xmath75 defined by ( [ muepsilon2]),width=529 ]    to illustrate the second and third consideration , we investigate the behavior of @xmath0 obtained by psai(@xmath22 ) for improperly chosen dropping tolerance @xmath22 that seems small intuitively .",
    "we attempt to show that a choice of fixed @xmath22 that is apparently small , but bigger than that defined by ( [ muepsilon2 ] ) for some @xmath19 may produce a numerically singular @xmath0 . specifically , we take @xmath216 in the while - loop of algorithm  [ alg2 ] , where the right - hand side is just our dropping tolerance ( [ muepsilon2 ] ) .",
    "we drop the entries whose sizes are below such improper @xmath22 .",
    "table  [ tolsensitive](a ) lists the matrices , each with the dropping tolerance @xmath22 that leads to a numerically singular @xmath0 for @xmath217 .",
    "the @xmath170 and @xmath171 in table  [ tolsensitive](a ) denote the minimum and maximum of @xmath75 defined by ( [ muepsilon2 ] ) .",
    "however , if we decrease the tolerance @xmath22 by one order of magnitude , we will obtain good preconditioners ; see table  [ tolsensitive](b ) for details .",
    "we emphasize that for the given @xmath21 and @xmath47 and all the matrices in table  [ tolsensitive](b ) , psai(@xmath22 ) with dropping criterion ( [ muepsilon2 ] ) has computed the sparse approximations @xmath0 of @xmath6 with the desired accuracy @xmath21 , as shown in table  [ table2 ] .",
    "we see from table  [ tolsensitive ] ( a ) that the maximum residual @xmath184 for each problem is not small at all for the chosen bad fixed dropping tolerance @xmath22 .",
    "on the other hand , table  [ tolsensitive ] ( b ) indicates that the one order reduction of @xmath22 results in essential improvements on the effectiveness of preconditioners , not only delivering nonsingular @xmath0 but also accelerating the convergence considerably .",
    "these tests indicate that the non - singularity and quality of @xmath71 obtained by psai(@xmath22 ) can be very sensitive to the choice of dropping tolerance @xmath22 .",
    "however , compared with the corresponding results for @xmath218 on the same test problems in table  [ tolsensitive ] ( b ) and table [ table2 ] , we find that the preconditioner obtained by psai(@xmath22 ) with the good fixed tolerance @xmath22 is not so effective as that with @xmath75 defined by ( [ muepsilon2 ] ) , as shown by values of @xmath182 and @xmath219 . indeed , the preconditioners obtained by fixed tolerance @xmath22 do not satisfy the accuracy @xmath21 , as @xmath184 indicate .    to be more illustrative , for orsirr_2",
    "we depict the residual norms @xmath220 of three such @xmath0 obtained by psai(@xmath22 ) with the adaptive @xmath75 defined by ( [ muepsilon2 ] ) and bad to good fixed @xmath221 ; see figure  [ fig1 ] , where the solid line @xmath222 parallel to the @xmath223-axis denotes our accuracy requirement , the circle ` @xmath224 ' , the plus ` @xmath225 ' and the triangle ` @xmath226 ' are @xmath220 of each @xmath0 .",
    "we find from the figure that all the circles ",
    "@xmath224 fall below the solid line , meaning that psai(@xmath22 ) with @xmath75 defined by ( [ muepsilon2 ] ) computes all the columns of @xmath0 with desired accuracy ; many ` @xmath225 ' reside above the solid line and some of them are far away from @xmath178 and can be up to @xmath227 , indicating that @xmath0 obtained by psai(@xmath22 ) is very bad and of poor quality for preconditioning ; most of the triangles ` @xmath226 ' are below @xmath178 , and a small part of them is above it , revealing that @xmath0 is improved very substantially but is not so good like @xmath0 computed by psai(@xmath22 ) with @xmath75 defined by ( [ muepsilon2 ] )",
    ".    table  [ tolsensitive ] and figure  [ fig1 ] tell us that empirically chosen tolerances are problematic and susceptible to failure .",
    "in contrast , tables  [ table2][tolsensitive ] demonstrate that our selection criterion ( [ muepsilon2 ] ) is very robust for psai(@xmath22 ) .      as an application of our theory , in this subsection , we test the static f - norm minimization based sai preconditioning procedures with the three popular patterns of @xmath228 , @xmath229 and @xmath230 , respectively ; see @xcite for the effectiveness of these patterns .",
    "we attempt to show the effectiveness of dropping criterion ( [ staticcriterion ] ) and exhibit the sensitiveness of the preconditioning quality of @xmath0 to dropping tolerances @xmath75 .",
    "we first compute @xmath0 by predetermining its pattern and solving @xmath5 independent ls problems , and then get a sparser @xmath71 by dropping the entries of small magnitude in @xmath0 below the tolerance defined by ( [ staticcriterion ] ) or some empirically chosen ones .",
    "0.6 cm    cccccc & orsirr_1&orsirr_2&orsreg_1&pores_2&sherman5 + @xmath22&@xmath231&@xmath231&@xmath23&@xmath202&@xmath149 + @xmath184&1.32&1.00&15.2&18.0&24.7 + @xmath170&@xmath232&@xmath232&@xmath233 & @xmath234&@xmath235 + @xmath171&@xmath236&@xmath237&@xmath238 & @xmath239&@xmath240 +    0.2 cm    ( a):bad @xmath22 resulting in numerically singular @xmath71    0.4 cm    ccccccc matrix&@xmath22&@xmath172&@xmath183&@xmath167&@xmath168&@xmath184 + orsirr_1 & @xmath202&1.63 & 1.82 & 33 & 50&0.42 + orsirr_2 & @xmath202&1.31 & 2.69 & 32 & 48&0.42 + orsreg_1 & @xmath150&4.09 & 0.91 & 45 & 74&1.32 + pores_2 & @xmath215 & 3.10 & 2.47&124&158&1.74 + sherman5 & @xmath23&11.79 & 1.55 & 24 & 34&3.76 +    0.2 cm    ( b):good @xmath22 leading to effective @xmath71    we summarize the results in tables  [ saisensitive][static3 ] , where @xmath172 includes the time for predetermination of the pattern of @xmath0 , the computation of @xmath0 and the sparsification of @xmath0 , and @xmath241 and @xmath242 denote the cpu time in second of bicgstab and gmres(50 ) applied to solve the preconditioned linear systems .",
    "we observed that there are some columns whose residual norms @xmath243 are very small ( some are at the level of @xmath244 ) . therefore , to drop entries of small magnitude as many as possible , we replace those @xmath245 below @xmath246 by @xmath246 in ( [ staticcriterion ] ) .",
    "we test the static sai procedure with the pattern of @xmath228 .",
    "table  [ saisensitive](a ) lists the matrices , each with the fixed tolerance @xmath22 leading to a numerically singular @xmath0 and table [ saisensitive](b ) exhibits the good performance of @xmath71 generated from the static sai by decreasing the corresponding @xmath22 in table  [ saisensitive ] ( a ) by one order of magnitude .",
    "tables  [ static1][static3 ] show the results obtained by the three static sai procedures with dropping criterion ( [ staticcriterion ] ) .",
    "ccccccccc & @xmath172&@xmath183&@xmath167&@xmath168&@xmath247&@xmath248&@xmath184 + * orsirr_1 & @xmath0 & 1.65 & 8.36 & 29 & 45 & 0.03 & 0.08&0.42 + & @xmath71 & 1.78 & 4.54 & 29 & 45 & 0.01 & 0.06&0.42 + * orsirr_2 & @xmath0 & 1.48 & 8.62 & 30 & 44 & 0.02 & 0.04&0.42 + & @xmath71 & 1.56 & 5.24 & 30 & 44 & 0.02 & 0.03&0.42 + * orsreg_1 & @xmath0 & 4.39 & 7.53 & 28 & 51 & 0.04 & 0.09&0.42 + & @xmath71 & 4.69 & 2.95 & 33 & 51 & 0.01 & 0.08&0.42 + * pores_2 & @xmath0 & 2.74 & 9.25 & 52 & 118 & 0.09 & 0.14&0.94 + & @xmath71 & 3.00 & 4.98 & 52 & 118&0.06 & 0.13&0.94 + * sherman5 & @xmath0 & 13.26 & 8.39 & 22 & 31 & 0.04 & 0.05&0.32 + & @xmath71 & 14.07 & 3.54 & 22 & 31 & 0.02 & 0.04&0.32 +    [ fig2 ]     for orsirr_2 obtained by the static sai with bad and good fixed @xmath22 and @xmath75 defined by ( [ staticcriterion]),width=529 ]    ccccccccc & @xmath172&@xmath183&@xmath167&@xmath168&@xmath247&@xmath248&@xmath184 + * orsirr_1 & @xmath0 & 5.45 & 16.41 & 18 & 28 & 0.03 & 0.04&0.32 + & @xmath71 & 6.00 & 10.06 & 18 & 28 & 0.01 & 0.03&0.32 + * orsirr_2 & @xmath0 & 4.64 & 17.03 & 18 & 28 & 0.02 & 0.05&0.32 + & @xmath71 & 5.19 & 11.23 & 16 & 28 & 0.01 & 0.02&0.32 + * orsreg_1 & @xmath0 & 14.86 & 14.03 & 19 & 34 & 0.05 & 0.06&0.34 + & @xmath71 & 16.82 & 6.83 & 19 & 34 & 0.02 & 0.05&0.34 + * pores_2 & @xmath0 & 11.02 & 18.42 & 26 & 38 & 0.05 & 0.07&0.86 + & @xmath71 & 13.25 & 11.91 & 26 & 38&0.05 & 0.06&0.86 + * sherman5 & @xmath0 & 52.36 & 14.94 & 16 & 23 & 0.06 & 0.06&0.25 + & @xmath71 & 56.50 & 6.41 & 16 & 23 & 0.02 & 0.04&0.25 +    ccccccccc & @xmath172&@xmath183&@xmath167&@xmath168&@xmath247&@xmath248&@xmath184 + * orsirr_1 & @xmath0 & 15.37 & 27.79 & 13 & 20 & 0.02 & 0.02&0.24 + & @xmath71 & 18.26 & 18.39 & 13 & 20 & 0.01 & 0.02&0.24 + * orsirr_2 & @xmath0 & 12.11 & 28.84 & 14 & 19 & 0.02 & 0.03&0.24 + & @xmath71 & 14.20 & 20.26 & 14 & 19 & 0.02 & 0.02&0.24 + * orsreg_1 & @xmath0 & 49.42 & 22.77 & 14 & 24 & 0.05 & 0.04&0.31 + & @xmath71 & 57.22 & 12.75 & 14 & 24 & 0.02 & 0.04&0.31 + * pores_2 & @xmath0 & 41.83 & 30.84 & 16 & 26 & 0.06 & 0.05&0.68 + & @xmath71 & 49.71 & 19.46 & 16 & 26&0.05 & 0.04&0.68 + * sherman5 & @xmath0 & 129.25 & 22.53 & 14 & 19 & 0.06 & 0.06&0.20 + & @xmath71 & 138.41 & 9.09 & 14 & 19 & 0.02 & 0.03&0.20 +    singular @xmath71 as in table  [ saisensitive ] ( a ) lead to complete failure of preconditioning",
    ". we also see from the table that all the maximum residuals @xmath184 of @xmath71 for the five matrices are not small , meaning that the @xmath71 are definitely ineffective for preconditioning . but",
    "table  [ saisensitive ] ( b ) shows that the situation is improved drastically when the corresponding tolerances @xmath22 are decreased only by one order of magnitude .",
    "similar to psai(@xmath22 ) , the quality of static sai preconditioners depends on , and can be very sensitive to , the dropping tolerances .",
    "figure  [ fig2 ] depicts the residual norms of three @xmath71 obtained by the static sai procedure with the pattern of @xmath228 using the bad @xmath249 , the good @xmath250 and our criterion ( [ staticcriterion ] ) , which are denoted by the plus ` @xmath225 ' , the triangle  @xmath226 and circle ` @xmath224 ' , respectively , and the solid line @xmath251 parallel to the @xmath223-axis is the maximum column residual norm of @xmath71 obtained with ( [ staticcriterion ] ) .",
    "we see from the figure that @xmath71 constructed with ( [ staticcriterion ] ) and the good fixed tolerance @xmath250 are fairly good but the former one is more effective than the latter one , since the triangles ` @xmath226 ' are either indistinguishable with or a little bit higher than the corresponding circles ` @xmath224 ' . such effectiveness",
    "is also reflected in the values of @xmath182 and @xmath219 in table  [ saisensitive ] and table  [ static1 ] .",
    "in contrast , @xmath71 obtained with the tolerance @xmath249 has many columns , which are poorer than those of @xmath71 obtained with ( [ staticcriterion ] ) , since the ` @xmath225 ' are above the corresponding ` @xmath224 ' , and it has some columns whose residual norms reside above the solid line @xmath251 .    for tables",
    "[ static1][static3 ] , we see that each @xmath71 is sparser than the corresponding @xmath0 and it is cheaper to apply @xmath71 than @xmath0 in krylov solvers , as @xmath241 and @xmath242 indicate .",
    "furthermore , for each matrix , since we use ( [ staticcriterion ] ) to only drop the entries of small magnitude , two @xmath184 corresponding to each pair @xmath0 and @xmath71 are approximately the same and they are fairly small .",
    "so it is expected that each @xmath71 and the corresponding @xmath0 have very similar accelerating quality .",
    "this is indeed the case , because for all the problems but orsreg_2 , each krylov solver preconditioned by @xmath71 and the corresponding @xmath0 uses exactly the the same number of iterations to achieve convergence .",
    "for orsreg_2 in table  [ static1 ] , bicgstab preconditioned by @xmath71 uses only three more iterations than it preconditioned by @xmath0 .",
    "these results demonstrate that our selection criterion ( [ staticcriterion ] ) is effective and robust .",
    "compared with table  [ saisensitive ] , we see from table  [ static1 ] that the sai preconditioning with our criterion ( [ staticcriterion ] ) is more effective than that with the good fixed tolerance @xmath250 , since the maximum residual norms @xmath184 for the former are always not bigger than those for the latter and the krylov solvers preconditioned by the former used fewer iterations to achieve convergence . in addition , we notice from table  [ static3 ] that the pattern of @xmath252 leads to considerably denser @xmath0 and @xmath71 that are good approximate inverses but are much more expensive to compute , compared with the other two patterns",
    ". therefore , as far as the overall performance is concerned , this static sai procedure is less effective than the other two .",
    "selection criteria for dropping tolerances are vital to sai preconditioning .",
    "however , this important problem has received little attention and never been studied rigorously and systematically . for f - norm minimization",
    "based sai preconditioning , such criteria affect the non - singularity , the quality and effectiveness of a preconditioner @xmath0 . an improper choice of dropping tolerance may produce a numerically singular @xmath0 , causing the complete failure in preconditioning , or may produce a good but denser @xmath0 possibly at more cost for setup and application . to develop a robust psai(@xmath22 )",
    "preconditioning procedure , we have analyzed the effects of dropping tolerances on the non - singularity , quality and effectiveness of preconditioners .",
    "we have established some important and intimate relationships between them . based on them ,",
    "we have proposed adaptive robust selection criteria for dropping tolerances that can make @xmath0 as sparse as possible and of comparable quality to those obtained by bpsai , so that it is possible to lower the cost of setup and application .",
    "the theory on selection criteria has been adapted to static f - norm minimization based sai preconditioning procedures .",
    "numerical experiments have shown that our criteria work very well . however , we point out that it is more important and beneficial to perform dropping in the adaptive psai preconditioning procedure than a static sai one",
    ".    for general purposes and effectiveness , robust selection criteria for dropping tolerances also play a key role in other _ adaptive _ f - norm minimization based sai preconditioning procedures whenever dropping is used . just like for psai(@xmath22 ) , dropping criteria serve two purposes , one of which is to make an approximate inverse @xmath0 as sparse as possible and the other is to guarantee its comparable preconditioning quality to that obtained from sai procedure without dropping . for adaptive factorized sparse approximate inverse preconditioning , such as ainv type algorithms @xcite , dropping is equally important . different from f - norm minimization",
    "based sai preconditioning , the non - singularity of the factorized @xmath0 is guaranteed naturally .",
    "nonetheless , how to drop entries of small magnitude is nontrivial and has not yet been well studied .",
    "all of these are significant and are topics for further consideration .",
    "barrett , r. , berry , m. , chan , t. , demmel , j. , donato , j. , dongarra , j. , eijkhout , v. , romine , r. , van  der vorst , h. : templates for the solution of linear systems : building blocks for iterative methods .",
    "siam , philadelphia ( 1994 )                bergamaschi , l. , gambolati , g. , pini , g. : a numerical experimental study of inverse preconditioning for the parallel iterative solution to 3d finite element flow equations .",
    "journal of computational and applied mathematics * 210 * , 6470 ( 2007 )    bergamaschi , l. , martnez ,  . ,",
    "pini , g. : parallel preconditioned conjugate gradient optimization of the rayleigh quotient for the solution of sparse eigenproblems . applied mathematics and computation * 175 * , 16941715 ( 2006 )          carpentieri , b. , duff , i. , giraud , l. : some sparse pattern selection strategies for robust frobenius norm minimization preconditioners in electromagnetism .",
    "numerical linear algebra with applications * 7 * , 667685 ( 2000 )                                    kolotilina , l. , nikishin , a. , yeremin , a. : factorized sparse approximate inverse preconditionings .",
    "iv : simple approaches to rising efficiency . numerical linear algebra with applications * 6 * , 515531 ( 1999 )"
  ],
  "abstract_text": [
    "<S> dropping tolerance criteria play a central role in sparse approximate inverse preconditioning . </S>",
    "<S> such criteria have received , however , little attention and have been treated heuristically in the following manner : if the size of an entry is below some empirically small positive quantity , then it is set to zero . </S>",
    "<S> the meaning of `` small '' is vague and has not been considered rigorously . </S>",
    "<S> it has not been clear how dropping tolerances affect the quality and effectiveness of a preconditioner @xmath0 . in this paper , we focus on the adaptive power sparse approximate inverse algorithm and establish a mathematical theory on robust selection criteria for dropping tolerances . using the theory , we derive an adaptive dropping criterion that is used to drop entries of small magnitude dynamically during the setup process of @xmath0 . </S>",
    "<S> the proposed criterion enables us to make @xmath0 both as sparse as possible as well as to be of comparable quality to the potentially denser matrix which is obtained without dropping . as a byproduct , the theory applies to static f - norm minimization based preconditioning procedures , and a similar dropping criterion is given that can be used to sparsify a matrix after it has been computed by a static sparse approximate inverse procedure . </S>",
    "<S> in contrast to the adaptive procedure , dropping in the static procedure does not reduce the setup time of the matrix but makes the application of the sparser @xmath0 for krylov iterations cheaper . </S>",
    "<S> numerical experiments reported confirm the theory and illustrate the robustness and effectiveness of the dropping criteria . </S>"
  ]
}