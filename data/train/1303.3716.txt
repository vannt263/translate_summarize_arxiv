{
  "article_text": [
    "suppose we are given a set @xmath0 of @xmath1 data points in @xmath2 , and assume that @xmath3 , where the points in @xmath4 lie in a ( low - dimensional ) linear subspace @xmath5 of @xmath2 , and @xmath6 denotes a set of outliers .",
    "the association of the data points with the sets @xmath4 and @xmath6 , the number of subspaces @xmath7 , their dimensions @xmath8 , and their orientations are all unknown .",
    "we consider the problem of clustering the data points , i.e. , of finding the assignments of the points in @xmath0 to the sets @xmath4 and @xmath6 .",
    "note that once these associations have been identified , it is straightforward to extract the subspaces @xmath5 through principal component analysis ( pca ) .",
    "the problem we consider is known as subspace clustering and has applications in , e.g. , unsupervised learning , image processing , disease detection , and , in particular , computer vision , see , e.g. , @xcite and references therein .",
    "numerous approaches to subspace clustering are known .",
    "we refer to @xcite for an excellent overview .",
    "spectral clustering ( sc ) methods ( see @xcite for an introduction ) have found particularly widespread use . at the heart of sc",
    "lies the construction of an adjacency matrix @xmath9 , with the @xmath10th entry of @xmath11 measuring the similarity between the data points @xmath12 .",
    "a typical similarity measure is , e.g. , @xmath13 , where @xmath14 is some distance measure @xcite .",
    "taking @xmath15 to be the graph with adjacency matrix @xmath11 , the association of the points in @xmath0 to the sets @xmath4 ( outliers are typically removed in a preprocessing step ) is obtained by finding the connected components in @xmath15 , accomplished via singular value decomposition of the laplacian of @xmath15 followed by k - means clustering @xcite . whether a sc algorithm , or for that matter , any clustering algorithm , succeeds depends on the number of subspaces @xmath7 , their dimensions and relative orientations , and the number of points in each subspace .",
    "analytic results on the performance of sc methods are scarce .",
    "a notable exception is the _ sparse subspace clustering _ ( ssc ) algorithm , recently introduced by elhamifar and vidal @xcite . at the heart of this algorithm",
    "lies a clever construction of @xmath11 that uses ideas from sparse signal recovery .",
    "soltanolkotabi and cands @xcite presented an elegant ( geometric function ) analysis of ssc and proved that ssc succeeds under very general conditions .",
    "most importantly , it is shown in @xcite , using a probabilistic analysis , that ssc succeeds even when the subspaces @xmath5 intersect , which means the @xmath5 do not need to be independent or disjoint are called disjoint if @xmath16 for all @xmath17 , and independent if @xmath18 , where @xmath19 stands for direct sum .",
    "an independent set of subspaces is disjoint , but the converse is not necessarily true .",
    "two subspaces are said to intersect if @xmath20 . ] .",
    "moreover , soltanolkotabi and cands @xcite provide a clever extension of ssc that provably detects outliers . to construct the adjacency matrix @xmath11 ssc requires the solution of @xmath1 @xmath21-minimization problems , each in @xmath1 unknowns",
    "; this can pose significant computational challenges for large data sets .",
    "* contributions :",
    "* we introduce an algorithm , termed thresholding based subspace clustering ( tsc ) , which applies spectral clustering to an adjacency matrix @xmath11 obtained by thresholding correlations between the data points in @xmath0 .",
    "tsc is shown to succeed even when the subspaces intersect , and when their dimensions scale ( up to a log - factor ) linearly in the ambient dimension .",
    "while ssc shares these desirable properties , tsc is computationally much less demanding , as the construction of the adjacency matrix @xmath11 in the tsc algorithm requires the computation of @xmath22 inner products followed by thresholding only .",
    "moreover , the performance analysis of tsc , thanks to the algorithm s simplicity , does not need sophisticated mathematical tools ; it is based on fairly standard concentration results for order statistics only .    in practical applications",
    "the data points to be clustered are often subject to erasures , caused by , e.g. , scratches on images .",
    "the literature is essentially void of corresponding analytic performance results .",
    "we prove that tsc succeeds even when the data points in @xmath0 are subject to massive erasures .",
    "specifically , the number of erasures is allowed to scale ( up to a log - factor ) linearly in the ambient dimension .",
    "we finally propose a simple scheme that provably detects outliers , and we corroborate our findings by numerical results . proofs of the theorems in this paper , results on clustering noisy data points , and numerical results for real data sets are provided in @xcite .",
    "we finally note that lauer and schnorr @xcite also apply sc to an adjacency matrix constructed from correlations between data points , albeit , without thresholding . moreover , no analytic performance results are available for the algorithm in @xcite .",
    "* notation : * we use lowercase boldface letters to denote ( column ) vectors , e.g. , @xmath23 , and uppercase boldface letters to designate matrices , e.g. , @xmath11 . for the vector @xmath23 , @xmath24_q$ ] and @xmath25 denote the @xmath26th entry and for the matrix @xmath11 , @xmath27 stands for the entry in the @xmath28th row and @xmath29th column . the spectral norm of @xmath11 is @xmath30 @xmath31 , its frobenius norm is @xmath32 , and @xmath33 denotes the identity matrix .",
    "the superscript @xmath34 stands for transposition and @xmath35 for the natural logarithm .",
    "the cardinality of the set @xmath36 is @xmath37 .",
    "we write @xmath38 for a gaussian random vector with mean @xmath39 and covariance matrix @xmath40 .",
    "the unit sphere in @xmath2 is @xmath41 .",
    "the formulation introduced below assumes that outliers have already been removed from @xmath0 , e.g. , through the outlier detection scheme in sec .  [ sec : detoutl ] . given a set of data points @xmath0 and the parameter @xmath26 ( the choice of @xmath26",
    "is discussed below ) , the tsc algorithm consists of the following steps :    * step 1 : * for every @xmath42 , identify the set @xmath43 of cardinality @xmath26 such that @xmath44 and let @xmath45 be the vector with @xmath28th entry @xmath46 if @xmath47 , and @xmath48 if @xmath49 . construct the adjacency matrix according to @xmath50_i | + | [ { { \\mathbf z}}_i]_j |$ ] .",
    "* step 2 : * estimate the number of subspaces using the eigengap heuristic @xcite according to @xmath51 where @xmath52 are the eigenvalues of the normalized laplacian of the graph with adjacency matrix @xmath11 .",
    "* step 3 : * apply normalized sc @xcite to @xmath53 .",
    "tsc is said to succeed if the tsc subspace detection property according to the following definition holds .",
    "the tsc subspace detection property holds for @xmath54 and adjacency matrix @xmath11 if    @xmath55 only if @xmath56 and @xmath57 belong to the same set @xmath4    and if    for all @xmath58 , @xmath59 for at least @xmath60 pairs @xmath56 and @xmath57 that belong to the same set @xmath4 .",
    "[ def : lsdp ]    the idea behind def .",
    "[ def : lsdp ] , inspired by the @xmath21 subspace detection property introduced in @xcite , is the following .",
    "if the tsc subspace detection property holds , then each node in the graph @xmath15 with adjacency matrix @xmath11 is connected to at least @xmath60 other nodes , all of which correspond to points in the same subspace . in the sc step , the assignments of the points to clusters are then determined through identification of the connected components of @xmath15 .",
    "we will see in the numerical results section , that even if the tsc subspace detection property does not hold strictly , but the @xmath27 for pairs @xmath61 belonging to different subspaces are sufficiently small , sc can still yield the correct result .    * assumptions for performance analysis : * for expositional convenience we take all subspaces to have equal dimension @xmath62 , and let the number of points in each of the subspaces be @xmath63 , ( i.e. , @xmath64 ) .",
    "* choice of @xmath26 : * choosing @xmath26 too small / large will lead to over / under - estimation of the number of subspaces @xmath7 .",
    "a sensible choice is to take @xmath26 to be a fraction of @xmath63 .",
    "this motivates setting @xmath65 , where @xmath66 .",
    "the results we obtain will ensure that , under certain conditions , the tsc subspace detection property holds , provided that @xmath67 is not too small , while the specific choice of @xmath67 will not matter .",
    "moreover , numerical results in @xcite indicate that tsc is not sensitive to the specific choice of @xmath60 .",
    "in order to understand the impact of the relative orientations of the subspaces on the performance of tsc , we take the subspaces to be deterministic and the points in the subspaces to be random . w.l.o.g .",
    "we represent the points in @xmath5 as @xmath68 , @xmath69 , where @xmath70 and @xmath71 is a basis for the @xmath62-dimensional subspace @xmath5 .",
    "we present two results that depend on different notions of affinity between subspaces , namely    @xmath72 and ( * ? ? ?",
    "2.6 )    @xmath73 both of which can be interpreted as measures of the relative orientations of the subspaces . throughout this section , we assume that the @xmath71 are orthonormal bases , and hence @xmath74 . the relation between the two affinity notions",
    "is brought out by noting that @xmath75 while @xmath76 , where @xmath77 are the principal angles between @xmath78 and @xmath5 .",
    "suppose @xmath79 data points are chosen in each of the @xmath7 subspaces at random according to @xmath80 , where the @xmath81 are i.i.d .",
    "@xmath82 and @xmath83 . if @xmath84 then the tsc subspace detection property holds with probability at least @xmath85 [ thm : tscprob ] where @xmath86 and @xmath87 are absolute constants satisfying @xmath88 .",
    "[ thm : tscprob ] states that tsc succeeds with high probability if @xmath89 is sufficiently small .",
    "intuitively , we expect that clustering becomes easier when the number of data points in each subspace increases .",
    "[ thm : tscprob ] confirms this intuition as , for fixed @xmath62 , @xmath60 , and @xmath7 , the right hand side ( rhs ) of increases in @xmath67 ; moreover , the probability of success in thm .",
    "[ thm : tscprob ] increases in @xmath63 .",
    "if the number of subspaces , @xmath7 , increases , for fixed @xmath90 and @xmath63 , clustering intuitively becomes harder and , indeed , the rhs of is seen to decrease in @xmath7 .",
    "note that thm .",
    "[ thm : tscprob ] does not apply to subspaces that intersect as @xmath91 if @xmath78 and @xmath5 intersect and",
    "the rhs of is strictly smaller than @xmath92 .",
    "we next present a result analogous to thm .",
    "[ thm : tscprob ] that applies to intersecting subspaces .",
    "suppose @xmath93 data points are chosen in each of the @xmath7 subspaces at random according to @xmath80 , where the @xmath81 are i.i.d .",
    "uniform on @xmath94 and @xmath95 . if @xmath96 then the tsc subspace detection property holds with probability at least @xmath97 , [ thm : aff2 ] where @xmath98 is an absolute constant .",
    "the interpretation of thm .",
    "[ thm : aff2 ] is analogous to that of thm .",
    "[ thm : tscprob ] with the important difference that the rhs of , as opposed to the rhs of , decreases , albeit slowly , in @xmath63 ( recall that @xmath99 ) . for ssc a result in the flavor of thm .",
    "[ thm : aff2 ] was reported in ( * ? ? ?",
    "in practical applications the data points to be clustered are often corrupted by erasures , e.g. , images that need to be clustered could exhibit scratches . understanding the impact of erasures on clustering performance is obviously of significant importance .",
    "the literature seems , however , essentially void of corresponding analytic results . in the deterministic subspace setting such results will necessarily depend on the specific orientations of the subspaces . in the following ,",
    "we therefore take both the orientations of the subspaces and the points in each subspace to be random . specifically , we take the entries of the @xmath100 to be i.i.d .",
    "@xmath101 , which ensures that each of the @xmath71 is approximately orthonormal with high probability .",
    "suppose @xmath79 data points are chosen in each of the @xmath7 subspaces at random according to @xmath80 , where the @xmath81 are i.i.d .",
    "@xmath102 and @xmath83 .",
    "assume that in each @xmath57 up to @xmath103 entries ( possibly different for each @xmath57 ) are erased , i.e. , set to @xmath48 .",
    "let the entries of each matrix @xmath100 be i.i.d .  @xmath101 . if @xmath104 then the tsc subspace detection property holds with probability at least @xmath105 where @xmath106 are absolute constants .",
    "[ thm : fullyrandomnew ]    strikingly , thm .",
    "[ thm : fullyrandomnew ] shows that the number of erasures is allowed to scale ( up to a log - factor ) linearly in the ambient dimension . for the fully random data model used in thm .",
    "[ thm : fullyrandomnew ] we can furthermore conclude that tsc succeeds with high probability even when the dimensions of the subspaces scale ( up to a log - factor ) linearly in the ambient dimension . drawing such a conclusion from thm .",
    "[ thm : tscprob ] or thm .",
    "[ thm : aff2 ] seems difficult as the relation between @xmath107 , and @xmath7 is implicit in the affinity measures .",
    "these findings should , however , be taken with a grain of salt as the fully random subspace model ensures that the subspaces are approximately disjoint with high probability . in the erasure - free case ,",
    "i.e. , for @xmath108 , a result for ssc , analogous to thm .",
    "[ thm : fullyrandomnew ] , was reported in ( * ? ? ?",
    "[ sec : detoutl ] outliers are data points that do not lie in one of the low - dimensional subspaces @xmath5 and have no low - dimensional linear structure . here ,",
    "this is modeled by assuming random outliers distributed uniformly on the unit sphere of @xmath2 .",
    "the outlier detection criterion we employ does not need knowledge of the number of outliers @xmath109 and is based on the following observation .",
    "the maximum inner product between an outlier and any other point in @xmath0 is , with high probability , smaller than @xmath110 we therefore classify @xmath57 as an outlier if @xmath111 the maximum inner product between any point @xmath112 and the points in @xmath113 is unlikely to be smaller than @xmath114 .",
    "hence an inlier is unlikely to be misclassified as an outlier if @xmath115 is sufficiently small .",
    "suppose @xmath79 data points are chosen in each of the @xmath7 subspaces at random according to @xmath80 , where the @xmath81 are i.i.d .",
    "uniform on @xmath94 and each @xmath71 is orthonormal .",
    "let the @xmath109 outliers be i.i.d .",
    "uniform on @xmath116 .",
    "declare @xmath42 to be an outlier if @xmath117 .",
    "then , with @xmath118 , provided that @xmath119    with probability at least @xmath120 every outlier is detected and no point in a subspace is misclassified as an outlier .",
    "[ thm : outldete ]    since can be rewritten as @xmath121 we can conclude that outlier detection succeeds , even if the number of outliers scales exponentially in @xmath122 , i.e. , if @xmath62 is kept constant , exponentially in the ambient dimension !",
    "note that this result does not make any assumptions on the orientations of the subspaces @xmath5 .",
    "the outlier detection scheme proposed in @xcite allows to identify outliers under a very similar condition .",
    "however , it requires the solution of @xmath1 @xmath21-minimization problems , each in @xmath1 unknowns , while the algorithm proposed here needs to compute @xmath22 inner products followed by thresholding only .",
    "we use the performance measures employed in @xcite .",
    "the _ clustering error _ ( ce ) is defined as the ratio between the number of misclassified data points and the total number of points in @xmath0 . the error in estimating the number of subspaces @xmath7",
    "is denoted as el and takes on the value @xmath48 if the estimate is correct , else it is equal to @xmath92 . the _ feature detection error _ ( fde )",
    "is defined as @xmath123 where @xmath124 is the @xmath28th column of the adjacency matrix @xmath11 and @xmath125 is the vector containing the entries of @xmath126 corresponding to the subspace @xmath56 lives in . the fde measures to which extent points from different subspaces are connected in @xmath15 and is equal to zero if the tsc subspace detection property holds . *",
    "influence of @xmath62 , @xmath67 , and erasures : * we generate @xmath127 subspaces in @xmath128 at random , by choosing the corresponding @xmath71 uniformly at random from the set of orthonormal matrices in @xmath129 , and vary the number of points @xmath130 in each subspace .",
    "the points in the subspaces are chosen at random according to the probabilistic model in thm .",
    "[ eq : condtscaff2 ] .",
    "the results depicted in fig .  [ fig : varyd ] show , as indicated in sec .",
    "[ sec : intro ] , that tsc can , indeed , succeed even when the tsc subspace detection property does not hold . finally , we perform the same experiment , but erase the entries of @xmath56 with indices in @xmath131 , where @xmath131 is chosen independently for each @xmath56 and uniformly from @xmath132 . the results summarized in fig .  [",
    "fig : varydsc ] show that tsc succeeds , even when a large fraction of the entries is erased .",
    "* detection of outliers : * in order to allow for a comparison with the outlier detection scheme proposed in @xcite , we perform our experiment with the same parameters as used in ( * ? ? ?",
    "specifically , we set @xmath133 , vary @xmath134 , and generate @xmath135 subspaces and @xmath136 points in each subspace at random as in the previous paragraph . each of the @xmath137 outliers is chosen i.i.d .  uniformly on @xmath116 .",
    "note that we have as many outliers as inliers .",
    "we find a misclassification error probability of @xmath138 for @xmath134 , respectively .",
    "similar performance was reported for the scheme proposed in @xcite .",
    "e.  elhamifar and r.  vidal , `` clustering disjoint subspaces via sparse representation , '' in _ proc .",
    "of ieee international conference on acoustics , speech , and signal processing _",
    "2010 , pp . 19261929 ."
  ],
  "abstract_text": [
    "<S> we consider the problem of clustering a set of high - dimensional data points into sets of low - dimensional linear subspaces . </S>",
    "<S> the number of subspaces , their dimensions , and their orientations are unknown . </S>",
    "<S> we propose a simple and low - complexity clustering algorithm based on thresholding the correlations between the data points followed by spectral clustering . </S>",
    "<S> a probabilistic performance analysis shows that this algorithm succeeds even when the subspaces intersect , and when the dimensions of the subspaces scale ( up to a log - factor ) linearly in the ambient dimension . moreover , we prove that the algorithm also succeeds for data points that are subject to erasures with the number of erasures scaling ( up to a log - factor ) linearly in the ambient dimension . </S>",
    "<S> finally , we propose a simple scheme that provably detects outliers . </S>"
  ]
}