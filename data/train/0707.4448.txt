{
  "article_text": [
    "on large matrices are a cornerstone of computational linear algebra . with a few exceptions such as the approximate lanczos and power methods , most algorithms used by practitioners aimed to optimize speed of computation under the constraint of obtaining an exact result .",
    "recently , spurred by the seminal paper of frieze et al .",
    "@xcite , there has been a greater interest in finding algorithms which sacrifice the precision of the result for a gain in the speed of execution .",
    "consider the low - rank approximation problem ; i.e. , finding a matrix @xmath0 of rank at most @xmath1 which approximates a given matrix @xmath2 .",
    "the best matrix @xmath0 , best in the sense that it minimizes @xmath3 for any unitarily invariant norm ( e.g. , spectral or frobenius norms ) , can be obtained by computing the singular value decomposition ( svd ) of @xmath2 .",
    "( throughout this paper , we adopt the frobenius norm ; we use the notation @xmath0 to denote the best rank-@xmath1 approximation to @xmath2 and @xmath4 to denote an approximation to it  it will be easy to avoid confusion with @xmath5 , which is used to denote the @xmath6 column of @xmath2 ) but in some instances , evaluating the svd , which scales as @xmath7 where @xmath8 is the largest dimension of @xmath2 , may be too costly .",
    "frieze et al .  in  @xcite showed that @xmath0 can be reasonably well approximated by computing the svd of a subset of the columns of @xmath2 only , where the columns are sampled according to their relative powers ",
    "i.e . , @xmath9 , with the expected error coming from using the approximation @xmath4 instead of @xmath0 being of the form @xmath10",
    "in subsequent papers it has been argued that the additive error term in @xmath11 may be large , and thus other sampling techniques have been introduced to obtain relative approximation error bounds ( see , e.g. ,  @xcite ) .    in this paper",
    ", we address the sparse representation of linear operators for the approximation of matrix products .",
    "an important object that will appear in our study is the so - called nystrm method ( see section  [ sec : background ] ) to find a low - rank approximation to a positive kernel .",
    "this method , familiar to numerical analysts , has nowadays found applications beyond its original field , most notably in machine learning . in previous work",
    "@xcite , we proposed an approach for low - rank approximation in such applications ; here we show that the task of finding optimal sparse representation of linear operators , in order to evaluate their product , is related to the nystrm extension of a certain positive definite kernel .",
    "we will the use this connection to derive and bound the error of two new algorithms , which our simulations indicate perform well in practice .",
    "related to our work is that of  @xcite for matrix products . in that paper , drineas et al",
    ".  showed that a randomized algorithm sampling columns @xmath5 and rows @xmath12 of @xmath2 and @xmath13 in proportion to their relative powers @xmath14 and @xmath15 yields an expected error of @xmath16 .",
    "notice that this bound does not involve a low - rank approximation of @xmath2 or @xmath13 .",
    "in contrast , we obtain a randomized algorithm bound in which the approximating rank @xmath1 of a kernel related to @xmath2 and @xmath13 appears explicitly .",
    "the methods mentioned above are all _ adaptive _ , in the sense that they require some knowledge about @xmath2 and @xmath13 .",
    "a very simple non - adaptive method is given by an application of the johnson - lindenstrauss lemma : it is easy to show  @xcite that if @xmath17 is a @xmath18 matrix with independent unit normal entries and @xmath19 , then for @xmath20 we have that @xmath21 letting @xmath22 denote the @xmath6 row of @xmath2 and @xmath23 the @xmath24 column of @xmath13 , we observe the following element - wise relation : @xmath25 and thus we see that approximating @xmath26 by @xmath27 yields a good result with high probability .",
    "later we compare this method to our algorithms described below .",
    "the remainder of this paper is organized as follows . in section  [ sec : background ] , we briefly review the nystrm method used to approximate positive definite matrices  @xcite . in section  [ sec : mat_approx ] , we introduce the problem of approximating a matrix product and highlight two key aspects : the issue of best subset selection and the issue of optimal rescaling .",
    "we then solve the optimal rescaling problem and analyze a randomized and a deterministic algorithm for subset selection ; we conclude with simulations and a brief discussion of algorithmic complexity .",
    "to provide context for our results , we first introduce the so - called nystrm method to approximate the eigenvectors of a symmetric positive semi - definite ( spsd ) matrix .",
    "the nystrm method , familiar in the context of finite element methods , has found many applications in machine learning and computer vision in recent years ( see , e.g. ,  @xcite and references therein ) .",
    "we give here a brief overview : let @xmath28 \\times [ 0,1 ] \\rightarrow { \\mathbb{r}}$ ] be a positive semi - definite kernel and @xmath29 , @xmath30 , denote pairs of eigenvalues and eigenvectors such that @xmath31}k(x , y)f_i(y)dy=\\lambda_if_i(x).\\ ] ] the nystrm extension is a method to approximate the eigenvectors of @xmath32 based on a discretization of the interval @xmath33 $ ] . define the @xmath34 points @xmath35 by @xmath36 with @xmath37 , so that the @xmath35 s are evenly spaced along the interval @xmath33 $ ] .",
    "then form the _ gram matrix _ @xmath38 , which in turn is used to approximate   by a finite - dimensional spectral problem @xmath39 the nystrm extension then uses these @xmath40 to give an estimate @xmath41 of the @xmath6 eigenfunction as follows : @xmath42    this method can also be applied in the context of matrices",
    ". let @xmath43 be an @xmath44 spsd matrix , partitioned as @xmath45,\\ ] ] where @xmath46 and @xmath1 is typically much smaller than @xmath8 .",
    "it is then possible to approximate @xmath1 eigenvectors and eigenvalues of @xmath43 by using the eigendecomposition of @xmath47 as follows .",
    "define @xmath48 and @xmath49 with @xmath50 orthogonal and @xmath51 diagonal .",
    "the nystrm extension then tells us that an approximation for @xmath1 eigenvectors in @xmath52 is given by @xmath53.\\ ] ]    these approximations @xmath54 and @xmath55 in turn yield an approximation @xmath56 to @xmath43 as follows : @xmath57.\\ ] ] the quality of this approximation can then be measured as the ( e.g. , frobenius ) norm of the schur complement of @xmath47 in @xmath43 : @xmath58",
    "let @xmath59 and @xmath60 .",
    "we use the notation @xmath5 to denote the columns of @xmath2 and @xmath61 the rows of @xmath13 .",
    "we can write the product @xmath26 as the sum of rank - one matrices as follows : @xmath62    our approach to estimate the product @xmath26 , akin to model selection in statistics , will consist of keeping only a few terms in the sum of  ; this entails choosing a subset of columns of @xmath2 and of rows of @xmath13 , and rescaling their outer products as appropriate . to gain some basic insight into this problem",
    ", we may consider the following two extreme cases with @xmath63 and @xmath64 .",
    "first , suppose that the vectors @xmath65 and @xmath66 are collinear .",
    "then @xmath67 , hence we can recover the product without error by only keeping one term of the sum of   and rescaling it appropriately _ provided _ that we know the correlation between @xmath65 and @xmath66 . at the other extreme , if @xmath65 and @xmath66 are orthogonal , rescaling will not decrease the error no matter which term in   is kept .",
    "hence we see that there are two key aspects to the problem of sparse matrix product approximation as formulated above :    optimal model selection : :     +    which rows / columns should be retained ?",
    "optimal reweighting : :     +    how should these rows / columns be rescaled ?    as we show below , the latter of these problems can be solved exactly for a relatively low complexity . for the former , which is combinatorial in nature and seemingly much harder to solve , we give an efficient approximation procedure .",
    "we first consider the problem of optimal reweighting , conditioned upon a choice of subset .",
    "in particular , suppose that an oracle gives us the best subset @xmath68 of cardinality @xmath1 to estimate the product @xmath26 .",
    "without loss of generality , we assume that @xmath69 .",
    "we then have the following result characterizing how well one can estimate the product @xmath26 :    [ th : maintheo]let the @xmath44 spsd matrix @xmath43 be defined as @xmath70 ( i.e. , @xmath71 , where @xmath72 is the hadamard or entrywise product of matrices ) and have the partition @xmath73,\\ ] ] where @xmath74 without loss of generality , and @xmath47 is the corresponding principal submatrix .",
    "then the best approximation to the product @xmath26 using the terms @xmath75 is given by @xmath76 where @xmath77 and @xmath78 moreover , if @xmath79 is the @xmath80 matrix with all entries equal to one , then the squared approximation error in frobenius norm is given by @xmath81 with @xmath82 the schur complement of @xmath47 in @xmath43 .",
    "this result tells us how well we can approximate the product @xmath26 _ granted _ that we know only a few rows / columns of @xmath2 and @xmath13 , and their correlations with the remaining rows and columns .",
    "it also allows us to characterize the best subset @xmath83 of size @xmath1 ; it is the subset that minimizes @xmath84 .    given the subset @xmath83 of @xmath85 , we seek the best scaling factors @xmath86 to minimize the squared approximation error @xmath87 .",
    "we can write the squared error as @xmath88 by distributing the product and using the linearity of the trace , we get @xmath89 where we made use of the following equality : @xmath90 we now work towards rewriting   in a more manageable form . first , using the fact that @xmath91 , we see that @xmath92    by combining   and  , we have @xmath93    similarly , using  , we get after an easy computation that @xmath94    we now rewrite   in a more manageable form : @xmath95    the optimal weight vector is now obtained by setting the gradient of   to zero .",
    "hence we obtain @xmath96 which proves the first part of the statement .",
    "for the second part , first notice that if @xmath97 $ ] is the vector whose entries are all one , we have the following expression for @xmath98 : @xmath99[\\mathbf{1 } ] .\\ ] ]    hence , at the optimum , the error is @xmath100^t      [ q_j \\ y]^t { q_j}^{-1}[q_j \\ y][\\mathbf{1}]\\\\      & = { \\| { ab } \\| } ^2- [ \\mathbf{1}]^t \\widetilde q [ \\mathbf{1 } ] ,    \\end{aligned}\\ ] ] where we see that @xmath56 is the _",
    "nystrm approximation of _ @xmath43 as described in section  [ sec : background ] . using lemma  [ lem : normhad ] below",
    ", we have @xmath101^t(q-\\widetilde    q)[\\mathbf{1}],\\ ] ] which finishes the proof of the theorem .",
    "the proof of the second part of theorem [ th : maintheo ] is based on the identity proven below :    [ lem : normhad ] let @xmath2 and @xmath13 be real matrices of dimensions @xmath102 and @xmath103 , respectively , and let @xmath79 be the @xmath104 matrix with all entries equal to one .",
    "the following identity holds : @xmath105    recall that we can write the product @xmath26 as a sum of @xmath8 rank - one terms as follows : @xmath106 we thus have , by definition of the frobenius norm , that @xmath107    using the invariance of the trace with respect to cyclic permutations , the last equation yields @xmath108 and the relation   is proved .",
    "having shown a solution to the optimal reweighting problem according to theorem  [ th : maintheo ] , we now turn our attention to the companion problem of optimal subset selection . in order to minimize the approximation error",
    ", we have to find the subset @xmath83 whose associated schur complement @xmath109 has the lowest possible power along the one - dimensional subspace of @xmath110 spanned by the vector @xmath97 $ ] . determining the eigenvectors and eigenvalues of this schur complement , and relating them to @xmath2 and @xmath13 , is not an easy task . here",
    "we present two approximations : one based on a random choice of subsets , and an alternative `` greedy '' approach which yields a worst - case error bound .",
    "we first discuss a random oracle which outputs a subset @xmath83 with probability @xmath111 defined below .",
    "recall our earlier definition of the matrix @xmath112 according to theorem  [ th : maintheo ] ; this approach is motivated by the expression of the resultant squared error , conditioned upon having chosen a subset @xmath113 , as @xmath114 . since @xmath109 is positive definite , we have that @xmath115 is larger than the largest eigenvalue of @xmath109 , and we can bound this error as follows : @xmath116 note that equality is obtained when @xmath117 , and hence this bound is tight .",
    "we have investigated in  @xcite an algorithm to minimize @xmath118 , which has been shown to be effective in the context of low - rank covariance matrix approximation .",
    "returning to our random oracle , note that both @xmath119 and @xmath120 are positive definite , and thus by the schur theorem  @xcite , @xmath43 is also positive definite . from this , we conclude :    1 .   there exists a matrix @xmath121 such that @xmath122 ; 2 .",
    "all the principal minors @xmath123 of @xmath43 are positive .",
    "consequently , we assume here that an oracle returns a subset @xmath124 with probability @xmath125 where @xmath126 is a normalizing constant , and the second fact above ensures that this probability distribution is well defined .",
    "we may then adapt the following result from the proof of theorem  1 in  @xcite :    [ th : randomorac ] let @xmath127 be a positive quadratic form with eigenvalues @xmath128 . if @xmath129 is chosen with probability @xmath130 , then @xmath131    combining   with   leads , via jensen s inequality , to an upper bound on the average error of this approach to random subset selection : @xmath132 where @xmath133 is defined via the relation @xmath134 , and @xmath135 denotes the optimal rank-@xmath1 approximation to @xmath133 obtained by truncating its singular value decomposition .",
    "despite the appearance of the term @xmath136 in this bound , it serves to relate the resultant approximation quality to the ranks of @xmath2 and @xmath13 , a feature reinforcing the well - foundedness of the accompanying algorithm we present below .",
    "in particular , if @xmath137 , then the approximation error is zero as expected . for practical reasons",
    ", we may also wish to relate this error to the eigenvalues of @xmath2 and @xmath13 . to this end , let @xmath138 and @xmath139 be two @xmath44 matrices , @xmath140 , and let @xmath141 ( resp .",
    "@xmath142 , @xmath143 ) be the singular values of @xmath138 ( resp .",
    "@xmath139 , @xmath144 ) sorted in non - increasing order .",
    "we then have the following majorization relation  @xcite : @xmath145    in particular , if @xmath146 , @xmath147 , and @xmath148 , then the singular values of @xmath43 , @xmath138 , and @xmath139 are the squares of the singular values of @xmath149 and @xmath13 respectively : @xmath150 we may then conclude from   that @xmath151    although the approach presented above relies on an oracle to sample in proportion to @xmath123 , we will subsequently outline a realizable algorithm based on these results .",
    "recall that theorem  [ th : maintheo ] indicates we should ensure that the diagonal terms of @xmath152 are kept as small as possible .",
    "hence , as a _ deterministic _",
    "approximation to the optimal subset selection procedure , we may take @xmath83 such that it contains the indices of the @xmath1 largest terms @xmath153 . while yielding only a worst - case error bound",
    ", this approach has the advantage of being easily implementable ( as it does not require sampling according to @xmath123 ) ; it also appears to perform well in practice  @xcite .",
    "this greedy algorithm proceeds as follows :    given matrices @xmath154 and @xmath155 and a positive integer @xmath156 :    1 .",
    "set @xmath157 , @xmath158 , and take @xmath159 to be the indices of the @xmath1 largest elements of @xmath160 .",
    "set @xmath161 as @xmath162 , for @xmath163 .",
    "3 .   set @xmath164 as @xmath165 , for @xmath166 .",
    "4 .   set @xmath167 and @xmath168 for @xmath166 .",
    "return @xmath169 as an approximation to @xmath26 .",
    "since the error term is the sum of all the terms in the schur complement , we can look to bound its largest element . to this end",
    ", we have the following result :    [ prop : greedy ] the largest entry in @xmath109 is smaller than the largest diagonal element of @xmath152 in  .",
    "this lemma confirms that a good error - minimization strategy is to make sure that the diagonal terms of @xmath152 are as small as possible , or equivalently to take @xmath83 such that it contains the indices of the @xmath1 largest @xmath153 as per algorithm  [ alg : pseudocodemult ] .",
    "matrix product approximation error using the power rescaling of   applied to each of the four subset selection algorithms described in sec .",
    "[ sec : results ] ]    the proof of lemma  [ prop : greedy ] is based on the following set of simple results :    [ lem : lardiag ] if @xmath43 is a positive definite matrix , then @xmath170 is positive and on the diagonal of @xmath43 .",
    "since @xmath43 is positive definite , we know there exists a matrix @xmath171 such that @xmath172 by the cauchy - schwartz inequality , we have @xmath173 from which we deduce that one of the following inequalities has to be satisfied : @xmath174    now if we suppose that @xmath170 is not a diagonal element , the relations of   yield a contradiction  and hence the largest entry of @xmath43 is on its main diagonal .",
    "the entries of @xmath109 , the schur complement of @xmath47 in @xmath43 , can be characterized explicitly according to the following formula :    [ lem : crabtree ] let @xmath175 be a nonsingular leading principal submatrix of @xmath43 obtained by keeping the rows and columns with indices @xmath176",
    ". then @xmath109 , the schur complement of @xmath47 in @xmath43 , is given element - wise by @xmath177    furthermore , it is possible to bound the diagonal entries of @xmath109 as follows :    [ lem : fisch ] if @xmath47 is a positive definite matrix , then @xmath178    we are now ready to give the proof of lemma  [ prop : greedy ] :    the preceding two lemmas tell us that the diagonal entries of @xmath109 are bounded by @xmath179 ( i.e. , the largest diagonal element of @xmath152 , according to the partition of  ) . and using lemma  [ lem : lardiag ] , we know that every entry of @xmath109 is bounded by these diagonal entries .    lemma  [ prop : greedy ] can be further refined to give a worst - case error bound for deterministic matrix product approximation , conditioned on a choice of subset @xmath83 and the corresponding optimal reweighting procedure . appealing to the inequality of arithmetic and geometric means to further bound the elements of @xmath109 , the results of theorem  [ th : maintheo ] and lemmas  [ lem : lardiag][lem : fisch ]",
    "yield : @xmath180     matrix product approximation error using the optimal rescaling of theorem  [ th : maintheo ] applied to the subset selection algorithms described in sec .  [",
    "sec : results ] ]",
    "we now present preliminary experimental results and discuss the computational complexity of the algorithms under consideration .",
    "three sets of experiments were performed , in which we compared the performance of four subset selection methods : a baseline uniform sampling on @xmath1-subsets ; sampling according the row / column powers  @xcite ; sampling in proportion to the @xmath1-principal minors of @xmath43 according to  ; and selecting greedily according to step  1 of algorithm  [ alg : pseudocodemult ] .",
    "we also compared the choice of reweighting following subset selection , in one case applying the optimal reweighting of theorem  [ th : maintheo ] and in the other simply reweighting according to the row / column powers ( see  @xcite ) : @xmath181    to test these various experimental conditions , we drew 200 random matrices @xmath182 and @xmath183 in total , each having independent unit normal entries .",
    "we then averaged the error of the randomized algorithm over 20 trials per matrix product , and report relative error in  db as @xmath184 for each test condition .    in the first set of experiments , shown in figure  [ fig : fig1 ] , we compare the four different algorithms for subset selection described above , applied in conjunction with a reweighting according to row / column powers .",
    "the highest - error method in this case corresponds to choosing the subset @xmath83 uniformly at random , and thus should be understood as a baseline measure of performance as a function of approximant rank @xmath1 .",
    "it can also be seen that sampling @xmath83 according to the relative powers of the row / columns of @xmath2 and @xmath13 , and sampling via a metropolis - hastings algorithm ( with independent proposal distributions taken in proportion to row / column powers ) , yield similar results , with both improving upon the baseline performance .",
    "the best results in this case are obtained by the greedy subset selection method indicated by step 1 of algorithm  [ alg : pseudocodemult ] .    in a second set of experiments",
    ", we followed the same procedure as above to compare subset selection procedures , but applied the optimal reweighting of theorem  [ th : maintheo ] rather than a rescaling according to row / column powers .",
    "performance in this case is ( as expected ) seen to be better overall , but with the ordering of the methods unchanged .",
    "as our final experiment , we compare the method of algorithm  [ alg : pseudocodemult ] ( greedy subset selection followed by optimal rescaling ) to two non - adaptive methods : choosing row / columns of @xmath2 and @xmath13 uniformly at random and rescaling according to @xmath185 , and the simple johnson - lindenstrauss random projection approach outlined in section  [ sec : intro ] .",
    "these non - adaptive methods can be seen to yield significantly worse performance than algorithm  [ alg : pseudocodemult ] , suggesting its potential as a practical method of selecting sparse representations of linear operators that yield low approximation errors for the resultant matrix products .",
    "we conclude with a brief discussion of the algorithmic complexity of algorithm  [ alg : pseudocodemult ] .",
    "first , assume without loss of generality that @xmath186 , and recall that straightforward matrix multiplication requires @xmath187 operations , though the best algorithm known so far ( the coppersmith - winograd algorithm  @xcite ) can perform this operation in @xmath188 . evaluating @xmath160 in algorithm  [ alg : pseudocodemult ] requires the computation of @xmath189 inner products of @xmath190-or @xmath191-dimensional vectors , and hence requires @xmath192 operations .",
    "extracting the @xmath1 largest elements of a set of size @xmath190 , as is necessary to construct @xmath83 , can be done efficiently using a variation on the quicksort algorithm ( see  @xcite ) in @xmath193 .",
    "the matrix @xmath43 is symmetric and its diagonal is a restriction of @xmath160 .",
    "hence it requires the computation of an additional @xmath194 inner products , and thus @xmath195 operations .",
    "evaluating @xmath98 requires @xmath196 operations , taking into account the fact that @xmath1 terms of the sum also appear in @xmath43 . finally , evaluating @xmath197 can be done using gaussian elimination in @xmath198 operations .",
    "hence the overall complexity is given by @xmath199 .",
    "matrix product approximation error using non - adaptive random projections ( johnson - lindenstrauss ) , non - adaptive subset selection ( uniform ) , and adaptive subset selection ( algorithm  [ alg : pseudocodemult ] ) ]                  c.  k.  i. williams and m.  seeger , `` using the nystrm method to speed up kernel machines , '' in _ neural information processing systems _ ,",
    "t.  g. dietterich , s.  becker , and z.  ghahramani , eds . , pp .",
    "mit press , 2001 ."
  ],
  "abstract_text": [
    "<S> thus far , sparse representations have been exploited largely in the context of robustly estimating functions in a noisy environment from a few measurements . in this context , the existence of a basis in which the signal class under consideration is sparse is used to decrease the number of necessary measurements while controlling the approximation error . in this paper , we instead focus on applications in numerical analysis , by way of sparse representations of linear operators with the objective of minimizing the number of operations needed to perform basic operations ( here , multiplication ) on these operators . </S>",
    "<S> we represent a linear operator by a sum of rank - one operators , and show how a sparse representation that guarantees a low approximation error for the product can be obtained from analyzing an induced quadratic form . </S>",
    "<S> this construction in turn yields new algorithms for computing approximate matrix products .    </S>",
    "<S> = 1 </S>"
  ]
}