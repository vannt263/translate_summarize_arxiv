{
  "article_text": [
    "various systems in nature display patterns , forms , attractors and recurrent behavior , which are not caused by a law per se ; the ubiquity of such systems and similar statistical properties of their exhibit order has lead to the term `` universality '' , since such phenomena show up in cosmology , the fur of animals @xcite , chemical and physical systems @xcite , landscapes , biological prey - predator systems and endless many others @xcite .",
    "furthermore , because of universality , it turns out that the most simplistic mathematical models exhibit the same statistical properties when their parameters are tuned correctly . as",
    "such it suffices to study n - particle systems with simple , `` atomistic '' components and interactions since they already exhibit many non - trivial emergent properties in the large n limit .",
    "certain `` order '' parameters change behavior in a non - classical fashion , for specific noise levels . using the rich and deep knowledge gained in statistical physics about those systems , we map the mathematical properties and learn about novel behaviors in deep learning set ups .",
    "specifically we look at a collection of n units on a lattice with various pair interactions ; when the units are binary spins with values ( @xmath0 ) , the model is known as a curie - weiss model . from a physical point of view , this is one of the basic , analytically solvable models , which still possesses the rich emergent properties of critical phenomena .",
    "however , given its general mathematical structure , the model has already been used to explain population dynamics in biology @xcite , opinion formation in society @xcite , machine learning @xcite and many others @xcite .",
    "all those systems , with a rich and diverse origination , posses almost identical behavior at criticality . in the latter case of machine learning ,",
    "the curie - weiss model encodes information about fully connected and feed - forward architectures to first order .",
    "similar work was done in @xcite , where insights from ising models and fully connected layers are drawn and applied to net architectures ; in @xcite a natural link between the energy function and an autoencoder is established .",
    "we will address the generalisation of fully connected system and understand its properties , before moving to the deep learning network and applying there the same techniques and intuition .",
    "the article is organised as follows : section [ sect : crit_phenomen ] gives a short introduction of critical systems and appropriate examples from physics ; in section [ sect : crit_dl_nets ] we map a concrete , non - linear , feed forward net to its physical counterpart and discuss other architectures as well ; then we turn to investigating the practical question whether we can spot traces of criticality in current deep learning nets in [ sect : exp_results ] .",
    "finally we summarise our findings in [ sect : outlook ] and hint at future directions for the rich map between statistical systems and deep learning .",
    "critical phenomena were first thoroughly explained and analysed in the field of statistical mechanics , although they were observed in various other systems , but lacking a theoretical understanding .",
    "the study of criticality belongs to statistical physics and is an incredibly rich and wide field , hence we can only briefly summarise some few results of interest for the present article ; definitely a much more comprehensive coverage can be found , see e.g.@xcite . in a nutshell ,",
    "the subject is concerned with the behavior of systems in the neighbourhood of their critical points , @xcite .",
    "one thus looks at systems composed of ( families of ) many , identical particles , trying to derive properties for macroscopic parameters , such as density or polarisation from the microscopic properties and interactions of the particles ; statistical mechanics can hence be understood as a bridge between macroscopic phenomenology ( e.g. thermodynamics ) and microscopic dynamics ( e.g. molecular or quantum - mechanical interacting collections of particles ) . in a nutshell ,",
    "criticality is achieved when macroscopic parameters show anomalous , divergent behavior at a phase transition .",
    "depending on the system at hand , the parameters might be magnetisation , polarisation , correlation , density , etc .",
    "specifically it is the correlation function of the `` components '' which then displays divergent behavior , and signals strong coordinated group behavior over a wide range of magnitudes .",
    "usually it is the noise ( temperature ) which at certain values will induce the phase transition accompanied by the critical anomalous behavior . given its relevance in physics and also its mathematical analogy to our deep learning networks , we will briefly review here the curie - weiss model with non - constant coupling and examine its behavior at criticality .",
    "a simplistic , fully solvable model for a magnet is the curie - weiss model ( cw ) , @xcite .",
    "it possesses many interesting features , exhibits critical behavior and correctly predicts some of the experimental findings . as its mathematics is later on used in our deep learning setup , we will briefly present main properties and solutions for the sake of self - consistency .",
    "the hamiltonian of the cw model is given by    @xmath1    here the @xmath2 are a collection of interacting `` particles '' , in our physical case , spins , that interact with each other via the coupling @xmath3 ; they take values @xmath4 and interact pairwise with each other , at long distances ; the inclusion of a factor of @xmath5 multiplying the quadratic spin term makes this long - range interaction tractable in the large @xmath6 limit .",
    "furthermore , there is a directed external magnetic field which couples to every spin via @xmath7 .",
    "since the coupling between spins is a constant and since every spin interacts with every other spin ( except self - interactions , which is accounted by a factor of @xmath8 ) the hamiltonian can be rewritten to    @xmath9    with @xmath10 being the inverse temperature the partition function can be formulated    @xmath11\\end{aligned}\\ ] ]    which can be fully solved , @xcite , summing over each of the @xmath12 states ; given an explicit partition @xmath13 , the free energy can be computed via    @xmath14    once we have @xmath15 various macroscopic values of interest can be inferred such as the magnetisation of the system , aka first derivative of @xmath15 wrt .",
    "this is a so called `` order parameter '' , which carries various other denominations , such as polarisation , density , opinion imbalance , etc . depending on the system at hand .",
    "it basically measures how arranged or homogeneous the system is under the influence of the outside field which couples to the spins via @xmath7 .",
    "a full treatment and derivation of the model including all its critical behavior can be found in @xcite , from where we get the equation of state for the magnetisation    @xmath16    with @xmath17 .",
    "the analysis of this equation for various temperatures @xmath18 and couplings @xmath3 , @xmath7 reveals a phase transition at the critical temperature @xmath19 . introducing the dimensionless parameter @xmath20 and expanding ( [ magnetisation ] ) in small couplings",
    "the famous power law dependence on temperature for the magnetisation emerges :    @xmath21    here we recognise one of the very typical power laws which are ubiquitous to critical systems .",
    "the quantity we are most interested in though is the second derivative of the free energy @xmath15 wrt .",
    "@xmath7 , which is basically the 2-point correlation function of the spins @xmath2 .",
    "again , expanding the second derivative of the free energy in small couplings and looking in the neighbourhood of the critical temperature @xmath22 yields    @xmath23    again displaying power law behavior with a power coefficient @xmath24 .",
    "the innocent looking equation [ crit_corr ] has actually tremendous consequences , as it implies that correlation does not simply restrict to nearest neighbours but goes over very long distances only slowly decaying ; further , because of the power law behavior , there will be self - similar , fractal patterns in the system : islands of equal magnetisation will form within other islands and so on , all the way through all scales .",
    "also , the correlation diverges at the criticality point @xmath22 .",
    "we will carry out the explicit calculations for our case of interest - non - constant matrix couplings - later one , in section [ sect : real_nets ] .",
    "two of the main motivations why we look for criticality and exploit on it in artificial networks , are the universal arising of this phenomenon as well as various hints of its occurrence in biological @xcite and neural systems @xcite ; once systems get `` sizable '' enough , gaining complexity , critical behavior emerges , which also applies to man - made nets @xcite .",
    "various measures can be formulated to detect criticality , and they all show power law distribution behavior . in the world wide web ,",
    "e.g. the number of links to a source , and the number of links away from a source , both exhibit power law distribution    @xmath25    for some power coefficient @xmath26 .",
    "similar behavior can be uncovered in various other networks , if sizable enough , such as citation behavior of scientific articles , social networks , etc .",
    "a simple , generic metric to detect criticality in networks is the degree distribution , defined as the number of ( weighted ) links connecting to one node .",
    "further , also the correlation between nodes is non - trivial , such that nodes with similar degree have higher probability of being connected than nodes of different degree @xcite , chapter vii .",
    "we will follow a similar path as proposed above and grow an experimental network with new nodes having the simplest preferential and directed attachment towards existing nodes , as a function of their degree :    @xmath27    here , @xmath28 denotes the probability that some node will grow a link to another node of degree @xmath29 .",
    "hence , every new node , will _ prefer _ nodes with higher degrees , leading to the overall power distribution observed in the real world systems .",
    "additional metrics we look at are single neuron activity as well as layer activity and pattern behavior ; more details on that in section [ sect : exp_results ] .",
    "we will focus now on a feed - forward network , with two layers , @xmath30 and @xmath31 connected via a weight matrix @xmath32 ; in order to probe our system for criticality , we write down its hamiltonian    @xmath33    which has been first formulated in the seminal paper @xcite . here , the values of the @xmath34 and @xmath7 are @xmath35 .",
    "further , by absorbing the biases @xmath36 in the usual way we can assume our weight matrix has the form :    @xmath37    while the @xmath38 read @xmath39 .    this hamiltonian describes a two layer net containing rectified linear units ( relu ) in the @xmath7-layer with a common bias term @xmath40 .",
    "the weight matrix @xmath32 sums the binary inputs coming from the @xmath30 and those are fed into @xmath36 ; depending whether the relu threshold has been reached , @xmath30 is activated , hence the binary values allowed for both , inputs and @xmath7-layer .",
    "further , we show in appendix [ app1 ] , that the partition function is up to a constant the same for the units taking values in @xmath41 or @xmath35 . by redefining @xmath42 we",
    "can then formulate the partition function as    @xmath43    where @xmath44 is the inverse temperature @xmath45 .",
    "this is the partition function of a bipartite graph with non - constant connection matrix @xmath46 .",
    "however , it turns out , that the partition function of the fully connected layer is the highest contribution ( 1st order ) of our feed forward network ( see appendix [ app2 ] ) , hence further simplifying the expression to    @xmath47    we will now proceed and compute the free energy @xmath15 , defined as @xmath48 , using the procedure presented in @xcite . from the free energy we then find all quantities of interest , especially the 2-point correlation function of the neurons .      in order to solve the cw model",
    "analytically , one has to perform the sum over spins , which is hindered by the quadratic term @xmath49 .",
    "the standard way to overcome this problem is the gaussian linearisation trick which replaces quadratic term by its square root - linear in @xmath2 and one additional continuous variable - the `` mean '' field , which is being integrated over entire @xmath50 :    @xmath51    which in physics , is known as the hubbard  stratonovich transform .",
    "unfortunately our coupling is not scalar , and hence we will linearise the sum term by term to keep track of all the weight matrix entries . first we will insert @xmath6 identities via the dirac delta function into our hamiltonian as used in ( [ part_fc ] ) :    @xmath52    with the definition of the delta function @xmath53 the partition function ( [ part_fc ] ) reads now    @xmath54    as already stated , we could perform the sum over the binary units @xmath2 , since they show up linearly in the exponential after the change of variables via delta identity ; we effectively converted the sum over binary values @xmath41 into integrals over @xmath50 , leading to    @xmath55    with a generalised hamiltonian    @xmath56\\nonumber\\\\     = & -\\frac{\\beta}{2n}\\textstyle\\sum_{ij } w_{ij}v_iv_j - \\beta h\\sum_iv_i\\nonumber\\\\   & + \\sum_i\\big [ u_iv_i -\\ln{(\\cosh{u_i } ) } \\big]\\end{aligned}\\ ] ]    ultimately we are interested in the free energy per unit , which contains the partition function , via    @xmath57    from @xmath15 we can now obtain all quantities of interest via derivatives , in our case with respect to @xmath40 .",
    "the partition function @xmath13 still contains a product of double integrals , which can be solved via the saddle point approximation ; we recall here the one - dimensional case    @xmath58    where @xmath59 is the stationary value of @xmath60 and @xmath61 is in our case the hessian evaluated at the stationary point :    @xmath62    while @xmath63 is given in ( [ gen_ham ] ) .",
    "the expression [ z_mft ] can now be computed by applying simultaneously the saddle point conditions for both integrals .",
    "the stationarity conditions to contain @xmath40 as well , hence the explicit equations are @xmath40 dependent ] for @xmath38 and @xmath64 give    @xmath65    which combined deliver the self consistency mean field equation of the fully connected layer ( [ sum_mf ] ) .",
    "further , denoting @xmath66 the the hamiltonian satisfying the stationarity conditions , it reads    @xmath67    equation ( [ saddle_ham ] ) already displays manifestly the consistency equation for the mean field , as taking the first derivative wrt .",
    "@xmath38 leaves exactly the consistency equation over per its construction ;    now we can rewrite the free energy ( [ helmholtz ] ) as    @xmath68\\sim",
    "\\lim_{n\\to\\infty}\\\\     & \\big[\\frac{1}{2n^2}\\sum_{ij } w_{ij}v_iv_j + \\frac{1}{n^2}\\sum_{ij}\\ln [ w_{ij}(1-v^2_i)-1 ] \\nonumber\\\\ & -   \\frac{t}{n}\\sum_i\\ln\\cosh\\beta(\\textstyle\\sum_{j}w_{ij}v_j / n + h)\\big ] \\nonumber\\end{aligned}\\ ] ]    we need to address now the large @xmath6 limit ; obviously the second term coming from the determinant clearly vanishes in the large-@xmath6 limit , as the logarithm is slowly increasing , while we divide through @xmath69 ; the first term - a double sum over @xmath38 is of order @xmath69 and hence a well defined average in the limit ; the last term - @xmath70 , when expanded , is again linear in the sum is an average , hence well defined in the limit ; after expansion , we re left with the outer sum ( over @xmath71 ) , which is again a well defined average when divided by @xmath6 ] , and hence a well defined average after dividing through @xmath6 , hence we re left with the free energy    @xmath73\\nonumber\\end{aligned}\\ ] ]    we re at the point now , where all quantities of interest can be derived from the free energy @xmath15 ; the order parameter ( aka magnetisation when dealing with spins ) per unit is defined as    @xmath74    the second term on the right vanishes identically , as we recognize it being evaluated at the stationarity condition @xmath75 for the hamiltonian .",
    "the contribution of the first term is :    @xmath76    which is ( the weighted sum version of ) the iconic self - consistency mean field equation of the cw magnet ( [ magnetisation ] ) .",
    "the critical point , @xmath77 is located where the correlation function diverges for @xmath78 ; the 2-point correlation function ( aka susceptibility when dealing with spins ) is the second derivative of f , i.e. the derivative of ( [ sum_mf ] ) wrt .",
    "@xmath40 :    @xmath79    where we used the original equation ( [ sum_mf ] ) for taking the derivatives .",
    "it is worth contemplating first equations ( [ sum_mf ] ) and ( [ c_p ] ) .",
    "they both capture the essence of the criticality of our system , including it s power law behavior .",
    "when the weight matrix reduces to a scalar coupling , both equations reduce to the classical cw system and display the behavior shown in ( [ mag_cw ] ) and ( [ crit_corr ] ) . furthermore , eq . ( [ c_p ] )",
    "encodes all the information needed for finding the critical point of matrix system at hand ; we recall that all @xmath80s ( and their derivatives ) are already implicitly `` solved '' in terms of @xmath40 and @xmath32 via the stationarity equation ( [ sum_mf ] ) and hence the @xmath38 are just place holders for functions of @xmath46 and @xmath40 ; we re thus left with a non - linear system of first order differential equations in @xmath6 variables , which will produce poles for specific values of the couplings and temperature at criticality .",
    "after investigating criticality through the partition function in our theoretical setup , now we turn to a practical question : do current deep learning networks exhibit critical behaviour , or put it differently , can we spot traces of critical phenomena in them ? instead of directly attacking the partition function of real world deep neural nets , we start with the practical observation , that systems at around criticality show off power law distributions in certain internal attributes .",
    "concretely for networks @xcite we look for traces of power laws in weight distributions , layer activation pattern frequencies , single node activation frequencies and average layer activations . in the following",
    "we will present experimental results for multilayer feed - forward networks , convolutional neural nets and autoencoders .    for all networks we ran experiments on the cifar-10 dataset , training each models for 200 epochs using relu activations and adam optimizer without gradient clipping and run inferences for 100 epochs .",
    "the feed forward network had 3 layers with 500 , 400 and 200 nodes , the cnn had 3 convolutional layers followed by 3 fully connected layers and the autoencoder had one layer with 500 nodes .    for weight distributions we looked at sums of absolute values of the outgoing weights at each node , as a weighted order of the node . in fig .",
    "[ fig : ff_weight_w_3_500_400_300_200_200_weight_plot ] we have a log - log plot of counts versus the node order as defined above , and detect no linear behavior .            for layer activation patterns we counted the frequency of each layer activations through the inference epochs .",
    "figures [ fig : ff_act_500_400_300_200_200_act_plot_layer_2 ] and [ fig : ae_act_500_act_plot_layer_1 ] are log - log plots of layer activation frequencies versus their respective counts for the feed - forward layer the autoencoder .",
    "as we see , the hidden layer activation pattern frequencies of the autoencoder resembles a truncated straight line , indicating that learning hidden features in unsupervised manner can give rise for scale free , power law phenomena in accordance with the findings of @xcite , but no other architectures show traces of any power law .    for single node activation frequencies we counted the frequency of each node activations through the inference epochs .",
    "figures [ fig : ff_node_act_500_400_300_200_200_node_act_plot_layer_2 ] and [ fig : cnn_node_act_conv3_local3_node_act_plot_layer_2 ] depict the behavior of feed - forward and cn network .",
    "the flat , nearly horizontal line in the latter architecture is again a sign of missing exponent whatsoever .    as a last measure we employed the sum of activations defined as the average activations on each layer throughout the inference epochs .",
    "spontaneous and detectable criticality did not arise in classical architectures so the next step will be to create and experiment with systems that have induced criticality and learning rules that take into account criticality . our first approach was to grow a fully connected net using the preferential attachment algorithm to induce at least some power law in node weights , and use the fully connected net as a hidden to hidden module .",
    "we further experimented with different solutions , regarding input and read out of activations from this hidden to hidden module , without changing the power law distribution .",
    "( this would simulate a system located at a critical state , with power law weight distribution ) .",
    "our findings so far show that learning in these systems is very unstable without any advancement in learning and inference .",
    "the fundamental missing part is how to naturally induce a critical state in a network , which is equipped with learning rules that inherently take into account the critical state . for",
    "that we need new architectures and new learning rules , derived from the critical point equations ( [ c_p ] ) .",
    "summary : in this article we make our first steps in investigating the relationship between criticality and deep learning networks . after a short introduction of criticality in statistical physics and real world networks we started with the theoretical setup of a fully connected layer .",
    "we used continuous mean field approximation techniques to tackle the partition function of the system ending up with a system of differential equations that determine the critical behaviour of the system .",
    "these equations can be the starting point for a possible network architecture with induced criticality and learning rules exploiting criticality .",
    "after that we presented results of experiments aiming to find traces of power law distributions in current deep learning networks such as multilayer feed - forward nets , convolutional networks and autoencoders .",
    "the results - except for the autoencoder - were affirmative in the negative sense , setting up as next the necessity to create networks with induced criticality and learning rules that exploit the critical state .",
    "outlook : obviously the fully connected layer , which can be solved analytically on the theoretical side is of limited importance , as it translates into a rather simplistic architecture ; more realistic , widely used set - ups , e.g. convolutional or recurrent nets , do very well contain the feed - forward mechanism , but are strongly deviating and hence only partially mapped to our theoretical treatment ; it would definitely be essential to address theoretically the convolution mechanism of deep nets and establish a link between the theoretical and experimental side ; also inducing criticality into the net via eq .",
    "( [ c_p ] ) could prove beneficial and might very well affect learning behavior and flow on the surface on the loss function .",
    "we here show that the partition function with hamiltonian @xmath81 who s units are taking values in @xmath35 has the same qualities as encoded in the partition function with hamiltonian @xmath82 , who s units take values in @xmath41 .",
    "we rewrite the hamiltonian in ( [ h01 ] ) with units taking values in @xmath41 ( using einstein s summation convention over double indices ) :    @xmath83    where the @xmath84 and @xmath85 take values in @xmath41 .",
    "carrying now the multiplications in ( [ h01p ] ) yields    @xmath86    with @xmath87 .",
    "hence when computing the partition z with ( [ h01p ] ) we obtain    @xmath88    where the right hand side is the original hamiltonian with a shifted coupling @xmath89 .",
    "the additional constant @xmath90 factors out completely and hence when taking the logarithm and the second derivative it wo nt change the outcome .",
    "also we note that the second derivative wrt .",
    "@xmath89 is @xmath91 .",
    "we consider here the hamiltonian of the bi - partite graph connected via weight matrix @xmath46 ( with einstein summation convention ) :              the sum over the @xmath85 is understood as a collection of @xmath12 terms , each corresponding to a unique combination of 0 s and 1 s in the vector of length @xmath6 representing that specific state of the spins ; however , the sum can be conveniently written as a product of @xmath6 binary summands , where each contains exactly the two possible states of the @xmath71th spin - this is where the product over @xmath96 comes from in upper formula .",
    "expanding now to lowest order in @xmath46 we obtain          a few notes are in place regarding eq .",
    "( [ hopf_bolz ] ) : the matrix @xmath100 is now symmetric by construction and hence mediates between equally sized ( actually identical ) layers ; further , all higher terms of the @xmath101 function are even , hence all contributions are higher order , symmetric interactions of the layer @xmath84 with itself ."
  ],
  "abstract_text": [
    "<S> motivated by the idea that criticality and universality of phase transitions might play a crucial role in achieving and sustaining learning and intelligent behaviour in biological and artificial networks , we analyse a theoretical and a pragmatic experimental set up for critical phenomena in deep learning . on the theoretical side , we use results from statistical physics to carry out critical point calculations in feed - forward / fully connected networks , while on the experimental side we set out to find traces of criticality in deep neural networks . </S>",
    "<S> this is our first step in a series of upcoming investigations to map out the relationship between criticality and learning in deep networks . </S>"
  ]
}