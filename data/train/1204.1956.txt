{
  "article_text": [
    "developing tools for automatic comprehension and classification of data web pages , newspaper articles , images , genetic sequences , user ratings  is a holy grail of machine learning .",
    "_ topic modeling _ is an approach that has proved successful in all of the aforementioned settings , though for concreteness here we will focus on uncovering thematic structure of a corpus of documents ( see e.g. @xcite , @xcite ) .    in order to learn structure",
    "one has to posit the _ existence _ of structure , and in topic models one assumes a _ generative model _ for a collection of documents .",
    "specifically , each document is represented as a vector of word - frequencies ( the _ bag of words _ representation ) .",
    "seminal papers in theoretical cs ( papadimitriou et al .",
    "@xcite ) and machine learning ( hofmann s _ probabilistic latent semantic analysis _",
    "@xcite ) suggested that documents arise as a convex combination of ( i.e. distribution on ) a small number of _ topic _ vectors , where each topic vector is a distribution on words ( i.e. a vector of word - frequencies ) .",
    "each convex combination of topics thus is itself a distribution on words , and the document is assumed to be generated by drawing @xmath0 independent samples from it .",
    "subsequent work makes specific choices for the distribution used to generate topic combinations the well - known _ latent dirichlet allocation _ ( lda ) model of blei et al  @xcite hypothesizes a _ dirichlet _ distribution ( see section  [ sec : dirichlet ] ) .    in machine learning , the prevailing approach is to use local search ( e.g. @xcite ) or other heuristics @xcite in an attempt to find a _ maximum likelihood _ fit to the above model .",
    "for example , fitting to a corpus of newspaper articles may reveal @xmath1 topic vectors corresponding to , say , politics , sports , weather , entertainment etc .",
    ", and a particular article could be explained as a @xmath2-combination of the topics politics , sports , and entertainment . unfortunately ( and not surprisingly ) , the maximum likelihood estimation is @xmath3-hard ( see section  [ sec : mle ] ) and consequently when using this paradigm , it seems necessary to rely on unproven heuristics even though these have well - known limitations ( e.g. getting stuck in a local minima @xcite ) .",
    "the work of papadimitriou et al  @xcite ( which also formalized the topic modeling problem ) and a long line of subsequent work have attempted to give _",
    "provable _ guarantees for the problem of learning the model parameters _ assuming the data is actually generated from it .",
    "_ this is in contrast to a maximum likelihood approach , which asks to find the closest - fit model for arbitrary data .",
    "the principal algorithmic problem is the following ( see section  [ sec : our ] for more details ) :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ * meta problem in topic modeling : * _ there is an unknown topic matrix @xmath4 with nonnegative entries that is dimension @xmath5 , and a stochastically generated unknown matrix @xmath6 that is dimension @xmath7 .",
    "each column of @xmath8 is viewed as a probability distribution on rows , and for each column we are given @xmath9 i.i.d .",
    "samples from the associated distribution .",
    "_    * goal : * _ reconstruct @xmath4 and parameters of the generating distribution for @xmath6 .",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    the challenging aspect of this problem is that we wish to recover _ nonnegative _ matrices @xmath10 with small inner - dimension @xmath11 . the general problem of finding nonnegative factors @xmath10 of specified dimensions when given the matrix @xmath8 ( or a close approximation )",
    "is called the _ nonnegative matrix factorization _ ( nmf ) problem  ( see  @xcite , and @xcite for a longer history ) and it is np - hard  @xcite . lacking a tool to solve such problems , theoretical work",
    "has generally relied on the _ singular value decomposition _ ( svd ) which given the matrix @xmath8 will instead find factors @xmath12 with both positive and negative entries .",
    "svd can be used as a tool for clustering  in which case one needs to assume that each document has only _ one _ topic . in papadimitriou et al @xcite this is called the _ pure documents _ case and is solved under strong assumptions about the topic matrix @xmath4 ( see also  @xcite and @xcite which uses the method of moments instead ) . alternatively",
    ", other papers use svd to recover the _",
    "span _ of the columns of @xmath4 ( i.e. the topic vectors )  @xcite , @xcite , @xcite , which suffices for some applications such as computing the inner product of two document vectors ( in the space spanned by the topics ) as a measure of their _",
    "similarity_.    these limitations of existing approaches either restricting to one topic per document , or else learning only the span of the topics instead of the topics themselves  are quite serious . in practice documents are much more faithfully described as a distribution on topics and indeed for a wide range of applications one needs the actual topics and not just their span  such as when browsing a collection of documents without a particular query phrase in mind , or tracking how topics evolve over time ( see @xcite for a survey of various applications ) . here",
    "we consider what we believe to be a much weaker assumption  _ separability_. indeed , this property has already been identified as a natural one in the machine learning community @xcite and has been empirically observed to hold in topic matrices fitted to various types of data  @xcite .",
    "separability requires that each topic has some near - perfect indicator word",
    " a word that we call the _ anchor word _ for this topic that appears with reasonable probability in that topic but with negligible probability in all other topics ( e.g. , `` soccer '' could be an anchor word for the topic `` sports '' ) .",
    "we give a formal definition in section  [ sec : our ] .",
    "this property is particularly natural in the context of topic modeling , where the number of distinct words ( dictionary size ) is very large compared to the number of topics . in a typical application , it is common to have a dictionary size in the thousands or tens of thousands , but the number of topics is usually somewhere in the range from @xmath1 to @xmath13 . note that separability does _ not _ mean that the anchor word always occurs ( in fact , a typical document may be very likely to contain _ no _ anchor words ) .",
    "instead , it dictates that when an anchor word does occur , it is a strong indicator that the corresponding topic is in the mixture used to generate the document .",
    "recently , we gave a polynomial time algorithm to solve nmf under the condition that the topic matrix @xmath4 is separable @xcite .",
    "the intuition that underlies this algorithm is that the set of anchor words can be thought of as extreme points ( in a geometric sense ) of the dictionary .",
    "this condition can be used to identify all of the anchor words and then also the nonnegative factors .",
    "ideas from this algorithm are a key ingredient in our present paper , but our focus is on the question :    what if we are not given the true matrix @xmath8 , but are instead given a few samples ( say , @xmath13 samples ) from the distribution represented by each column ?",
    "the main technical challenge in adapting our earlier nmf algorithm is that each document vector is a _ very poor _ approximation to the corresponding column of @xmath8 it is _ too noisy _ in any reasonable measure of noise .",
    "nevertheless , the core insights of our nmf algorithm still apply .",
    "note that it is impossible to learn the matrix @xmath6 to within arbitrary accuracy .",
    "( indeed , this is information theoretically impossible even if we knew the topic matrix @xmath4 and the distribution from which the columns of @xmath6 are generated . ) so we _ can not _ in general give an estimator that converges to the true matrix @xmath6 , and yet we _ can _ give an estimator that converges to the true topic matrix @xmath4 ! ( for an overview of our algorithm , see the first paragraph of section  [ sec : mainalg ] . ) we hope that this application of our nmf algorithm is just a starting point and other theoretical results will start using nmf as a replacement for svd  just as nmf has come to replace svd in several applied settings .      here",
    ", we give a more precise description of the generative model that we will be interested in .",
    "the model specifies a @xmath14 dimension matrix @xmath4 in which each column has unit @xmath15-norm .",
    "hence we can think of each column in @xmath4 as a distribution on words , and we will call @xmath4 the _ topic matrix_. to generate a document , we first need to determine the composition of the document in terms of these @xmath11 topics .",
    "this composition is chosen according to a distribution @xmath16 that is itself a distribution on distributions .",
    "we can now generate a document as follows :    * sample a distribution from @xmath16  the result is a vector @xmath17 in @xmath18 which has unit @xmath15-norm which describes document @xmath19 as a mixture of topics .",
    "* repeat @xmath0 times : choose a word according to the distribution @xmath20 .",
    "this process generates a ( multiset ) of @xmath0 words that we think of as a `` bag - of - words '' representation document @xmath19 .",
    "our goal is to learn the parameters of this generative process ",
    "namely , the topic matrix @xmath4 and the distribution @xmath16 .",
    "it seems challenging to learn the parameters without _ any _ assumption on the topic matrix @xmath4 or on the distribution @xmath16 . in the theory community ,",
    "problems of this nature have been considered in the context of text mining and information retrieval @xcite , @xcite , @xcite , collaborative filtering @xcite , @xcite and community discovery in social networks @xcite , but under much stronger assumptions .",
    "the assumptions made in these previous works fall into one of two categories :    in one line of research , one typically assumes that each document is about only _",
    "one _ topic .",
    "so the distribution @xmath16 is a distribution on elements of @xmath21 $ ] ( instead of a distribution on distributions on @xmath21 $ ] ) .",
    "the seminal work of papadimitriou et al @xcite considered the above generative model , under the restriction that each document is about only one topic and additionally that the distributions associated with distinct topics are nearly _ disjoint_. in this case , papadimitriou et al @xcite demonstrated that svd recovers the topic matrix @xmath4 and thus were the first to offer a theoretical explanation for the success of latent semantic indexing ( lsi ) @xcite in practice .",
    "recently , anandkumar et al @xcite gave an algorithm that provably recovers the topic matrix @xmath4 given just the assumption that each document is about only one topic and their approach is based on the method of moments .",
    "another related work is that of mcsherry @xcite who considered the problem of clustering in a certain generative model for random graphs .",
    "mcsherry @xcite assumed that each node is characterized by one of @xmath22 types , and that the probability that a link is present between two nodes is only a function of the pair of types .",
    "if a certain technical condition is met , mcsherry proved that one could recover a ( mostly ) accurate clustering of the nodes into types .",
    "however , it is widely recognized in the machine learning community @xcite , @xcite , @xcite that documents are much more faithfully described as a distribution on topics in the sense that generative models that incorporate this recover more meaningful topics . in particular , consider a generative model in which each document is about only one topic .",
    "this model would predict that if we find two large documents whose one hidden topic is the same , the empirical distribution of words for each of these two documents should be virtually identical . and just as documents are better represented as a distribution on topics , so too are members in a social network better represented as a distribution on types ( to model notions of overlapping communities ) .    in another line of research",
    ", the theoretical goal is to recover the span of the topics instead of the actual distributions associated with each topic .",
    "collaborative filtering uses an identical generative model , with `` documents '' replaced by users and `` words '' replaced by items .",
    "the goal here is to use information about which user has purchased which items to recommend items to each user that he is the most likely to purchase in the future .",
    "kumar et al @xcite gave a recommendation system under the assumption that each column in @xmath4 has disjoint support , and kleinberg and sandler @xcite , @xcite gave a recommendation system if the `` @xmath15-condition number '' of @xmath4 is lower bounded .",
    "however , collaborative filtering is quite different from our applications in that one only needs to know the column span of @xmath4 to give good recommendations .",
    "relatedly , in information retrieval one often wants to compute the inner - product ( in the space of topics ) of two documents as a measure of their similarity . if the topics are orthogonal ( either disjoint support or we allow _ negative _ contributions to words ) , the inner - product is only a function of the column span of @xmath4 .",
    "and indeed azar et al @xcite gave an explanation for the success of lsi by proving that the svd recovers the matrix @xmath4 up to rotations .",
    "of course , actually knowing the topics ( and not just their span ) is of crucial importance in many applications . in practice ,",
    "a major application of topic modeling is to use the derived set of topics to organize and annotate a large collection of documents .",
    "for example , imagine browsing this collection without a particular query phrase in mind , or we could also use knowledge of these topics to highlight the parts of a document that pertain to different subjects .",
    "yet another application is to use this type of information to track how the topics of interest ( in , say , a collection of academic documents ) changes over time .",
    "all of these types of applications are out of reach given only knowledge of the span of the topics .",
    "now we precisely define the topic modeling ( learning ) problem which was informally introduced above .",
    "there is an unknown _ topic matrix _",
    "@xmath4 which is dimension @xmath14 ( i.e. @xmath23 is the dictionary size ) and each column of @xmath4 is a distribution on @xmath24 $ ] . there is an unknown @xmath25 matrix @xmath6 whose each column is itself a distribution ( aka convex combination ) on @xmath26 $ ] .",
    "the columns of @xmath6 are i.i.d .",
    "samples from a distribution @xmath27 which belongs to a known family , e.g. , dirichlet distributions , but whose parameters are unknown .",
    "thus each column of @xmath8 ( being a convex combination of distributions ) is itself a distribution on @xmath24 $ ] , and the algorithm s input consists of @xmath0 i.i.d .",
    "samples for each column of @xmath8 . here",
    "@xmath0 is the document size and is assumed to be a constant for simplicity .",
    "our algorithm can be easily adapted to work when the documents have different sizes .",
    "the algorithm s running time will necessarily depend upon various model parameters , since distinguishing a very small parameter from @xmath28 imposes a lower bound on the number of samples needed .",
    "the first such parameter is a quantitative version of _ separability _ , which was presented above as a natural assumption in context of topic modeling .",
    "[ @xmath29-separable topic matrix ] [ def : psep ] an @xmath5 matrix @xmath4 is _",
    "@xmath29-separable _ if for each @xmath30 there is some row @xmath31 of @xmath4 that has a single nonzero entry which is in the @xmath32 column and it is at least @xmath29 .",
    "the next parameter measures the lowest probability with which a topic occurs in the distribution that generates columns of @xmath6 .",
    "the _ topic imbalance _ of the model is the ratio between the largest and smallest expected entries in a column of @xmath6 , in other words , @xmath33 } \\frac{{\\mathop{\\bf e\\/}}[x_i]}{{\\mathop{\\bf e\\/}}[x_j]}$ ] where @xmath34 is a random weighting of topics chosen from the distribution .    finally , we require that topics stay identifiable despite sampling - induced noise . to formalize this , we define a matrix that will be important throughout this paper :    [ def : varcovar ] if @xmath27 is the distribution that generates the columns of @xmath6 , then @xmath35 is defined as an @xmath36 matrix whose @xmath37th entry is @xmath38 $ ] where @xmath39 is a vector chosen from @xmath16 .",
    "let @xmath40 be a lower bound on the @xmath15-condition number of the matrix @xmath35 .",
    "this is defined in section  [ sec : cond ] , but for a @xmath36 matrix it is within a factor of @xmath41 of the smallest singular value . our algorithm will work for any @xmath42 , but the number of documents we require will depend ( polynomially ) on @xmath43 :    [ thm : main ] there is a polynomial time algorithm that learns the parameters of a topic model if the number of documents is at least @xmath44 where the three numbers @xmath45 are as defined above .",
    "the algorithm learns the topic - term matrix @xmath4 up to additive error @xmath46 .",
    "moreover , when the number of documents is also larger than @xmath47 the algorithm can learn the topic - topic covariance matrix @xmath35 up to additive error @xmath46 .",
    "there is an algorithm that learns the topic matrix @xmath4 and with high probability up to an additive error of @xmath46 from at most @xmath48 documents sampled from any topic model which has the three numbers @xmath45 as defined above .",
    "the algorithm runs in time polynomial in the number of documents .",
    "furthermore , we also recover the topic - topic covariance matrix @xmath35 to within an additive @xmath46 .    as noted earlier , we are able to recover the topic matrix even though we do not always recover the parameters of the column distribution @xmath27 . in some special cases",
    "we can also recover the parameters of @xmath27 , e.g. when this distribution is dirichlet , as happens in the popular _ latent dirichlet allocation _",
    "( lda ) model @xcite . in section  [ sec : conddir ] we compute a lower bound on the @xmath42 parameter for the dirichlet distribution , which allows us to apply our main learning algorithm , and also the parameters of @xmath16 can be recovered from the co - variance matrix @xmath49 ( see section  [ sec : recovdir ] ) .",
    "recently the basic lda model has been refined to allow correlation among different topics , which is more realistic .",
    "see for example the _ correlated topic model _ ( ctm ) @xcite and the _ pachinko allocation model _",
    "( pam ) @xcite .",
    "a compelling aspect of our algorithm is that it extends to these models as well : we can learn the topic matrix , even though we can not always identify @xmath27 .",
    "( indeed , the distribution @xmath16 in the pachinko is not even identifiable : two different sets of parameters can generate exactly the same distribution )    [ [ comparison - with - existing - approaches . ] ] comparison with existing approaches .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    \\(i ) we rely crucially on separability .",
    "but note that this assumption is weaker in some sense than the assumptions in _ all _ prior works that provably learn the topic matrix .",
    "they assume a single topic per document , which can be seen as a strong separability assumption about @xmath6 instead of @xmath4 in _ every _ column of @xmath6 only one entry is nonzero .",
    "by contrast , separability only assumes a similar condition for a negligible fraction namely , @xmath11 out of @xmath23 of rows of @xmath4 . besides",
    ", it is found to actually hold in topic matrices found using current heuristics .",
    "( ii ) needless to say , existing theoretical approaches for recovering topic matrix @xmath4 could nt handle topic correlations at all since they only allow one topic per document .",
    "( iii ) we remark that prior approaches that learn the span of @xmath4 instead of @xmath4 needed strong concentration bounds on eigenvalues of random matrices , and thus require substantial document sizes ( on the order of the number of words in the dictionary ! ) .",
    "_ by contrast we can work with documents of @xmath50 size .",
    "_    in fact , we obtain a number of interesting corollaries directly from our main result . our original motivation was to give an algorithm to learn the parameters of a _ latent dirichlet allocation _ ( lda ) model @xcite .",
    "this is a special case of the general topic model in which the columns of @xmath6 are generated by a dirichlet distribution .",
    "this model is immensely popular in the machine learning community @xcite and in section  [ sec : conddir ] we show a condition number lower bound for the matrix @xmath35 and hence we can apply our main learning algorithm . moreover , in section  [",
    "sec : recovdir ] we give an algorithm that actually learns the parameters of the dirichlet distribution ( as opposed to just learning the matrix @xmath35 ) .",
    "central to our arguments will be various notions of matrices being `` far '' from being low - rank .",
    "the most interesting one for our purposes was introduced by kleinberg and sandler @xcite in the context of collaborative filtering ; and can be thought of as an @xmath15-analogue to the smallest singular value of a matrix .",
    "if matrix @xmath51 has nonnegative entries and all rows sum to @xmath52 then its @xmath15 condition number @xmath53 is defined as :    @xmath54    if @xmath51 does not have row sums of one then @xmath53 is equal to @xmath55 where @xmath56 is the diagonal matrix such that @xmath57 has row sums of one .",
    "for example , if the rows of @xmath51 have disjoint support then @xmath58 and in general the quantity @xmath53 can be thought of a measure of how close two distributions on _ disjoint _ sets of rows can be .",
    "note that , if @xmath59 is an @xmath23-dimensional real vector , @xmath60 and hence ( if @xmath61 is the smallest singular value of @xmath51 ) , we have : @xmath62    the above notions of condition number will be most relevant in the context of the topic - topic covariance matrix @xmath35 .",
    "we shall always use @xmath42 to denote the @xmath15 condition number of @xmath35 .",
    "the definition of condition number will be preserved even when we estimate the topic - topic covariance matrix using random samples .",
    "[ lem : converger ] when @xmath63 , with high probability the matrix @xmath64 is entry - wise close to @xmath35 with error @xmath65 .",
    "further , when @xmath66 where @xmath67 is topic imbalance , the matrix @xmath68 has @xmath15 condition number at least @xmath69 .",
    "since @xmath70 = r(\\mathcal{t})$ ] , the first part is just by chernoff bound and union bound .",
    "the further part follows because @xmath35 is @xmath42 robustly simplicial , and the error can change the @xmath15 norm of @xmath71 for any unit @xmath72 by at most @xmath73 .",
    "the extra factor @xmath74 comes from the normalization to make rows of @xmath68 sum up to 1 .    in our previous work on nonnegative matrix factorization",
    "@xcite we defined a different measure of `` distance '' from singular which is essential to the polynomial time algorithm for nmf :    [ def : robustsimp ] if each column of a matrix @xmath4 has unit @xmath15 norm , then we say it is _",
    "@xmath75-robustly simplicial _ if no column in @xmath4 has @xmath15 distance smaller than @xmath75 to the convex hull of the remaining columns in @xmath4 .",
    "the following claim clarifies the interrelationships of these latter condition numbers .",
    "[ claim : conditionrelation ] ( i ) if @xmath4 is @xmath29-separable then @xmath76 has @xmath15 condition number at least @xmath29 .",
    "( ii ) if @xmath76 has all row sums equal to @xmath52 then @xmath4 is @xmath75-robustly simplicial for @xmath77 .",
    "we shall see that the @xmath15 condition number for product of matrices is at least the product of @xmath15 condition number .",
    "the main application of this composition is to show that the matrix @xmath78 ( or the empirical version @xmath79 ) is at least @xmath80-robustly simplicial .",
    "the following lemma will play a crucial role in analyzing our main algorithm :    [ lem : compose ] if @xmath51 and @xmath81 are matrices with @xmath15 condition number @xmath82 and @xmath83 , then @xmath84 is at least @xmath85 .",
    "specificially , when @xmath4 is @xmath29-separable the matrix @xmath86 is at least @xmath87-robustly simplicial .",
    "the proof is straight forward because for any vector @xmath59 , we know @xmath88 . for the matrix @xmath78 , by claim  [ claim : conditionrelation ]",
    "we know the matrix @xmath76 has @xmath15 condition number at least @xmath29 .",
    "hence @xmath89 is at least @xmath90 and again by claim  [ claim : conditionrelation ] the matrix is @xmath91-robustly simplicial .",
    "a key ingredient is an approximate nmf algorithm from @xcite , which can recover an approximate nonnegative matrix factorization @xmath92 when the @xmath15 distance between each row of @xmath93 and the corresponding row in @xmath8 is small .",
    "we emphasize that this is not enough for our purposes , since the term - by - document matrix @xmath93 will have a substantial amount of noise ( when compared to its expectation ) precisely because the number of words in a document @xmath0 is much smaller than the dictionary size @xmath23 .",
    "rather , we will apply the following algorithm ( and an improvement that we give in section  [ sec : betternmf ] ) to the gram matrix @xmath94 .",
    "[ thm : separablenoise ]    suppose @xmath95 where @xmath6 and @xmath96 are normalized to have rows sum up to 1 ,",
    "@xmath4 is separable and @xmath6 is @xmath42-robustly simplicial .",
    "let @xmath97 .",
    "there is a polynomial time algorithm that given @xmath93 such that for all rows @xmath98 , finds a @xmath99 such that @xmath100 .",
    "further every row @xmath101 in @xmath99 is a row in @xmath93 .",
    "the corresponding row in @xmath96 can be represented as @xmath102 . here",
    "@xmath103 is a vector in the convex hull of other rows in @xmath6 with unit length in @xmath15 norm .    in this paper",
    "we need a slightly different goal here than in @xcite .",
    "our goal is not to recover estimates to the anchor words that are close in @xmath15-norm but rather to recover almost anchor words ( word whose row in @xmath4 has almost all its weight on a single coordinate ) .",
    "hence , we will be able to achieve better bounds by treating this problem directly , and we give a substitute for the above theorem .",
    "we defer the proof to section  [ sec : betternmf ] .",
    "[ thm : betterseparablenoise ] suppose @xmath95 where @xmath6 and @xmath96 are normalized to have rows sum up to 1 , @xmath4 is separable and @xmath6 is @xmath42-robustly simplicial .",
    "when @xmath104 there is a polynomial time algorithm that given @xmath93 such that for all rows @xmath105 , finds @xmath11 row ( almost anchor words ) in @xmath93 .",
    "the @xmath30-th almost anchor word corresponds to a row in @xmath96 that can be represented as @xmath106 . here",
    "@xmath103 is a vector in the convex hull of other rows in @xmath6 with unit length in @xmath15 norm .",
    "first it is important to understand why separability helps in nonnegative matrix factorization , and specifically , the exact role played by the anchor words . suppose the nmf algorithm is given a matrix @xmath107 .",
    "if @xmath4 is @xmath29-separable then this means that @xmath4 contains a diagonal matrix ( up to row permutations ) .",
    "thus a scaled copy of each row of @xmath51 is present as a row in @xmath107 .",
    "in fact , if we knew the anchor words of @xmath4 , then by looking at the corresponding rows of @xmath107 we could``read off '' the corresponding row of @xmath51 ( up to scaling ) , and use these in turn to recover all of @xmath4 .",
    "thus the anchor words constitute the `` key '' that `` unlocks '' the factorization , and indeed the main step of our earlier nmf algorithm was a geometric procedure to identify the anchor words .",
    "when one is given a noisy version of @xmath107 , the analogous notion is `` almost anchor '' words , which correspond to rows of @xmath107 that are `` very close '' to rows of @xmath51 ; see theorem  [ thm : betterseparablenoise ] .",
    "now we sketch how to apply these insights to learning topic models .",
    "let @xmath96 denote the provided term - by - document matrix , whose each column describes the empirical word frequencies in the documents .",
    "it is obtained from sampling @xmath8 and thus is an extremely noisy approximation to @xmath8 .",
    "our algorithm starts by forming the gram matrix @xmath108 , which can be thought of as an empirical word - word covariance matrix .",
    "in fact as the number of documents increases @xmath109 tends to a limit @xmath110,$ ] implying @xmath111 .",
    "( see lemma  [ lem : convergeq ] . )",
    "imagine that we are given the exact matrix @xmath112 instead of a noisy approximation .",
    "notice that @xmath112 is a product of _ three _ nonnegative matrices , the first of which is @xmath29-separable and the last is the transpose of the first .",
    "nmf at first sight seems too weak to help find such factorizations .",
    "however , if we think of @xmath112 as a product of _ two _ nonnegative matrices , @xmath4 and @xmath86 , then our nmf algorithm @xcite can at least identify the anchor words of @xmath4 . as noted above , these suffice to recover @xmath86 , and then ( using the anchor words of @xmath4 again ) all of @xmath4 as well . see section  [ subsec : idealanchor ] for details .",
    "of course , we are not given @xmath112 but merely a good approximation to it .",
    "now our nmf algorithm allows us to recover `` almost anchor '' words of @xmath4 , and the crux of the proof is section  [ subsec : almostanchor ] showing that these suffice to recover provably good estimates to @xmath4 and @xmath113 .",
    "this uses ( mostly ) bounds from matrix perturbation theory , and interrelationships of condition numbers mentioned in section  [ sec : cond ] .    for simplicity",
    "we assume the following condition on the topic model , which we will see in section  [ subsec : freqwlog ] can be assumed without loss of generality :    ( * ) _ the number of words , @xmath23 , is at most @xmath114 .",
    "_    please see algorithm  [ alg : main ] : main algorithm for description of the algorithm .",
    "note that @xmath68 is our shorthand for @xmath115 , which as noted converges to @xmath35 as the number of documents increases .    1",
    ".   query the oracle for @xmath116 documents , where @xmath117 2 .",
    "split the words of each document into two halves , and let @xmath93 , @xmath118 be the term - by - document matrix with first and second half of words respectively .",
    "3 .   compute word - by - word matrix @xmath119 4 .",
    "apply the  robust nmf `` algorithm of theorem  [ thm : betterseparablenoise ] to @xmath112 which returns @xmath11 words that are ' ' almost \" the anchor words of @xmath4 .",
    "5 .   use these @xmath11 words as input to recover with almost anchor words to compute @xmath64 and @xmath4    here it is polynomial in @xmath120 instead of @xmath121 .",
    "the problem is if we apply the algorithm naively then we only get @xmath15 error guarantee on the whole vector , but when we try to estimate @xmath122 $ ] there are only @xmath11 entries ( out of @xmath23 ) so we really need a very small error .",
    "the error we need is still inverse polynomial in @xmath29 , but ideally i would hope the whole algorithm to be inverse polynomial in @xmath123 .",
    "this may still be doable .",
    "i also tried to consider other norms instead of @xmath15 .",
    "in fact the analysis we had should all work out for any norm if we lose some polynomial in @xmath11 .",
    "but after some thought i think we should stick with @xmath15 because robustly simplicial in @xmath15 norm seems to be the weakest ( and therefore works in more situations ) .",
    "end discussion      we first describe how the recovery procedure works in an `` idealized '' setting ( algorthm  [ alg : recoveranchor],recover with true anchor words ) , when we are given the exact value of @xmath124 and a set of anchor words ",
    "one for each topic .",
    "we can permute the rows of @xmath4 so that the anchor words are exactly the first @xmath11 words .",
    "therefore @xmath125 where @xmath56 is a diagonal matrix .",
    "note that @xmath56 is not necessarily the identity matrix ( nor even a scaled copy of the identity matrix ) , but we do know that the diagonal entries are at least @xmath29 .",
    "we apply the same permutation to the rows and columns of @xmath112 . as shown in figure  [ fig : recover ] , if we look at the submatrix formed by the first @xmath11 rows and @xmath11 columns , it is exactly @xmath126 . similarly , the submatrix consisting of the first @xmath11 rows is exactly @xmath127 .",
    "we can use these two matrices to compute @xmath68 and @xmath4 , in this idealized setting ( and we will use the same basic strategy in the general case , but need only be more careful about how we analyze how errors compound in our algorithm ) .    1",
    ".   permute the rows and columns of @xmath112 so that the anchor words appear in the first @xmath11 rows and columns 2 .",
    "compute @xmath128 ( which is equal to @xmath129 ) 3 .",
    "solve for @xmath130 : @xmath131 .",
    "4 .   output @xmath132 .",
    "5 .   output @xmath133 .",
    "our algorithm has exact knowledge of the matrices @xmath126 and @xmath127 , and so the main task is to recover the diagonal matrix @xmath56 . given @xmath56",
    ", we can then compute @xmath4 and @xmath68 ( for the dirichlet allocation we can also compute its parameters - i.e. the @xmath134 so that @xmath135 ) .",
    "the key idea to this algorithm is that the row sums of @xmath136 and @xmath127 are the same , and we can use the row sums of @xmath136 to set up a system of linear constraints on the diagonal entries of @xmath137 .    [",
    "lem : recoverrealanchor ] when the matrix @xmath112 is exactly equal to @xmath124 and we know the set of anchor words , recover with true anchor words outputs @xmath4 and @xmath68 correctly .",
    "the lemma is straight forward from figure  [ fig : recover ] and the procedure . by figure",
    "[ fig : recover ] we can find the exact value of @xmath127 and @xmath126 in the matrix @xmath112 .",
    "step 2 of recover computes @xmath138 by computing @xmath139 .",
    "the two vectors are equal because @xmath4 is the topic - term matrix and its columns sum up to 1 , in particular @xmath140 .    in step 3 ,",
    "since @xmath68 is invertible by lemma  [ lem : converger ] , @xmath56 is a diagonal matrix with entries at least @xmath29 , the matrix @xmath126 is also invertible .",
    "therefore there is a unique solution @xmath141 . also @xmath142 and hence @xmath143 . finally , using the fact that @xmath144 , the output in step 4 is just @xmath145 , and the output in step 5 is equal to @xmath68 .     +      what if we are not given the exact anchor words , but are given words that are `` close '' to anchor words ? as we noted , in general we can not hope to recover the true anchor words , but even a good approximation will be enough to recover @xmath68 and @xmath4 .",
    "when we restrict @xmath4 to the rows corresponding to `` almost '' anchor words , the submatrix will not be diagonal .",
    "however , it will be close to a diagonal in the sense that the submatrix will be a diagonal matrix @xmath56 multiplied by @xmath146 , and @xmath146 is close to the identity matrix ( and the diagonal entries of @xmath56 are at least @xmath147 ) .",
    "here we analyze the same procedure as above and show that it still recovers @xmath4 and @xmath68 ( approximately ) even when given `` almost '' anchor words instead of true anchor words . for clarity we state the procedure again in algorithm  [ alg : recoveralmostanchor ] :",
    "recover with almost anchor words .",
    "the guarantees at each step are different than before , but the implementation of the procedure is the same . notice that here we permute the rows of @xmath4 ( and hence the rows and columns of @xmath112 ) so that the `` almost '' anchor words returned by theorem  [ thm : separablenoise ] appear first and the submatrix @xmath4 on these rows is equal to @xmath148 .    here",
    ", we still assume that the matrix @xmath112 is exactly equal to @xmath124 and hence the first @xmath11 rows of @xmath112 form the submatrix @xmath149 and the first @xmath11 rows and columns are @xmath150 .",
    "the complication here is that @xmath151 is not necessarily equal to @xmath137 , since the matrix @xmath146 is not necessarily the identity .",
    "however , we can show that @xmath151 is `` close '' to @xmath137 if @xmath146 is suitably close to the identity matrix ",
    "i.e. given good enough proxies for the anchor words , we can bound the error of the above recovery procedure .",
    "we write @xmath152 .",
    "intuitively when @xmath153 has only small entries @xmath146 should behave like the identity matrix . in particular",
    ", @xmath154 should have only small off - diagonal entries .",
    "we make this precise through the following lemmas :    [ lem : inversee1 ] let @xmath152 and @xmath155 , then @xmath156 is a vector with entries in the range @xmath157 $ ] .",
    "@xmath146 is clearly invertible because the spectral norm of @xmath153 is at most @xmath158 .",
    "let @xmath159 . since @xmath152 we multiply @xmath146 on both sides to get @xmath160 .",
    "let @xmath161 be the largest absolute value of any entry of @xmath162 ( @xmath163 ) .",
    "consider the entry @xmath30 where @xmath161 is achieved , we know @xmath164 thus @xmath165 . now",
    "all the entries in @xmath166 are within @xmath167 in absolute value , and we know that @xmath168 .",
    "hence all the entries of @xmath162 are in the range @xmath157 $ ] , as desired .",
    "[ lem : inversee2 ] let @xmath152 and @xmath155 , then the columns of @xmath169 have @xmath15 norm at most @xmath167 .",
    "without loss of generality , we can consider just the first column of @xmath170 , which is equal to @xmath171 , where @xmath172 is the indicator vector that is one on the first coordinate and zero elsewhere .",
    "the approach is similar to that in lemma  [ lem : inversee1 ] .",
    "let @xmath173 .",
    "left multiply by @xmath174 and we obtain @xmath175 . hence @xmath176 .",
    "let @xmath161 be the largest absolute value of entries of @xmath177 ( @xmath163 ) .",
    "let @xmath30 be the entry in which @xmath161 is achieved .",
    "then @xmath178 therefore @xmath179 .",
    "further , the @xmath180 .",
    "now we are ready to show that the procedure recover with almost anchor words succeeds when given `` almost '' anchor words :    1 .",
    "permute the rows and columns of @xmath112 so that the `` almost '' anchor words appear in the first @xmath11 rows and columns .",
    "2 .   compute @xmath181 ( which is equal to @xmath182 ) 3 .",
    "solve for @xmath130 : @xmath183 .",
    "4 .   output @xmath184 .",
    "output @xmath185 .",
    "[ lem : recoveralmostanchor ] when the matrix @xmath112 is exactly equal to @xmath124 , the matrix @xmath4 restricted to almost anchor words is @xmath148 where @xmath186 has @xmath15 norm @xmath187 when viewed as a vector , procedure recover with almost anchor words outputs @xmath4 such that each column of @xmath4 has @xmath15 error at most @xmath188 .",
    "the matrix @xmath68 has additive error @xmath189 whose @xmath15 norm when viewed as a vector is at most @xmath190 .",
    "since @xmath112 is exactly @xmath124 , our algorithm is given @xmath149 and @xmath150 with no error . in step 3 , since @xmath56 , @xmath146 and @xmath68 are all invertible , we have @xmath191 ideally we would want @xmath192 , and indeed @xmath193 . from lemma  [ lem : inversee1 ]",
    ", the vector @xmath194 has entries in the range @xmath157 $ ] , thus each entry of @xmath151 is within a @xmath195 multiplicative factor from the corresponding entry in @xmath137 .    consider the output in step 4 .",
    "since @xmath56 , @xmath146 , @xmath68 are invertible , the first output is @xmath196 our goal is to bound the @xmath15 error of the columns of the output compared to the corresponding columns of @xmath4 . notice that it is sufficient to show that the @xmath197 row of @xmath198 is close ( in @xmath15 distance ) to the indicator vector @xmath199 .    for each @xmath19 ,",
    "@xmath200    again , without loss of generality we can consider just the first row . from lemma  [ lem : inversee2 ] @xmath201 has @xmath15 distance at most @xmath167 to @xmath202 .",
    "@xmath203 has entries in the range @xmath204 $ ] . and",
    "so @xmath205 the last term can be bounded by @xmath206 .",
    "consider the first term on the right hand side : the vector @xmath207 has one non - zero entry ( the first one ) whose absolute value is at most @xmath208 .",
    "hence , from lemma  [ lem : inversee2 ] the first term can be bounded by @xmath209 , and this implies the claim .",
    "the first row of @xmath210 is @xmath211 where @xmath212 is a vector with @xmath15 norm at most @xmath213 .",
    "so every column of @xmath4 is recovered with @xmath15 error at most @xmath188 .",
    "consider the second output of the algorithm .",
    "the output is @xmath214 and we can write @xmath215 and @xmath216 . the leading error are @xmath217 and hence the @xmath15 norm of the leading error term ( when treated as a vector ) is at most @xmath188 and other terms are of order @xmath218 and can safely be bounded by @xmath167 for suitably small @xmath46 ) .",
    "finally we consider the general case ( in which there is additive noise in step 1 ) : we are not given @xmath124 exactly .",
    "we are given @xmath112 which is close to @xmath124 ( by lemma  [ lem : convergeq ] ) .",
    "we will bound the accumulation of this last type of error .",
    "suppose in step @xmath52 of recover we obtain @xmath219 and @xmath220 and furthermore the entries of @xmath221 and @xmath222 have absolute value at most @xmath223 and the matrix @xmath224 has @xmath15 norm @xmath225 when viewed as a vector .",
    "[ lem : recoverwitherror ] if @xmath226 are sufficiently small , recover outputs @xmath4 such that each entry of @xmath4 has additive error at most @xmath227 .",
    "also the matrix @xmath68 has additive error @xmath189 whose @xmath15 norm when viewed as a vector is at most @xmath228 .",
    "the main idea of the proof is to write @xmath220 as @xmath229 . in this way",
    "the error @xmath224 can be translated to an error @xmath230 on @xmath146 and lemma  [ lem : recoveralmostanchor ] can be applied .",
    "the error @xmath221 can be handled similarly .",
    "we shall follow the proof of lemma  [ lem : recoveralmostanchor ] .",
    "first can express the error term @xmath224 instead as @xmath231 .",
    "this is always possible because all of @xmath56 , @xmath146 , @xmath68 are invertible .",
    "moreover , the @xmath15 norm of @xmath230 when viewed as a vector is at most @xmath232 , because this norm will grow by a factor of at most @xmath120 when multiplied by @xmath137 , a factor of at most 2 when multiplied by @xmath154 and at most @xmath233 when multiplied by @xmath234 .",
    "the bound of @xmath235 comes from lemma  [ lem : converger ] , we lose an extra @xmath236 because @xmath68 may not have rows sum up to 1 .",
    "hence @xmath237 and the additive error for @xmath150 can be transformed into error in @xmath146 , and we will be able to apply the analysis in lemma  [ lem : recoveralmostanchor ] .    similarly , we can express the error term @xmath221 as @xmath238 .",
    "entries of @xmath239 have absolute value at most @xmath240 .",
    "the right hand side of the equation in step 3 is equal to @xmath241 so the error is at most @xmath223 per entry . following the proof of lemma  [ lem : recoveralmostanchor ] , we know @xmath242 has diagonal entries within @xmath243 .",
    "now we consider the output .",
    "the output for @xmath76 is equal to @xmath244 here we know @xmath245 has @xmath15 norm at most @xmath246 per row , @xmath247 is a diagonal matrix with entries in @xmath248 , entries of @xmath239 has absolute value @xmath249 . following the proof of lemma  [ lem : recoveralmostanchor ] the final entry - wise error of @xmath4 is roughly the sum of these three errors , and is bounded by @xmath250 ( notice that lemma  [ lem : recoveralmostanchor ] gives bound for @xmath15 norm of rows , which is stronger .",
    "here we switched to entry - wise error because the entries of @xmath221 are bounded while the @xmath15 norm of @xmath221 might be large ) .",
    "similarly , the output of @xmath68 is equal to @xmath251 .",
    "again we write @xmath215 and @xmath216 . the extra term @xmath252 is small because the entries of @xmath212 are at most to @xmath253 ( otherwise @xmath242 wo nt be close to identity ) .",
    "the error can be bounded by @xmath250 .    now in order to prove our main theorem we just need to show that when number of documents is large enough , the matrix @xmath112 is close to the @xmath124 , and plug the error bounds into lemma  [ lem : recoverwitherror ] .      here",
    "we show that the matrix @xmath112 indeed converges to @xmath254 when @xmath116 is large enough .",
    "[ lem : convergeq ] when @xmath255 , with high probability all entries of @xmath256 have absolute value at most @xmath257 .",
    "further , the @xmath15 norm of rows of @xmath112 are also @xmath257 close to the @xmath15 norm of the corresponding row in @xmath258 .",
    "we shall first show that the expectation of @xmath112 is equal to @xmath124 where @xmath68 is @xmath115 .",
    "then by concentration bounds we show that entries of @xmath112 are close to their expectations .",
    "notice that we can also hope to show that @xmath112 converges to @xmath259 .",
    "however in that case we will not be able to get the inverse polynomial relationship with @xmath0 ( indeed , even if @xmath0 goes to infinity it is impossible to learn @xmath35 with only one document ) .",
    "replacing @xmath35 with the empirical @xmath68 allows our algorithm to perform better when the number of words per document is larger .    since the documents are independent , to analyze @xmath112 it is enough to analyze one document .",
    "let @xmath59 be the @xmath30-th column of @xmath6 ( @xmath260 ) .",
    "the vector @xmath261 is a vector with @xmath15 norm 1 because @xmath262 , every column of @xmath4 has @xmath15 norm 1 and all entries are nonnegative",
    ". therefore we view @xmath261 as a probability distribution over words .",
    "as the algorithm specifies , the @xmath30-th column of @xmath93 comes from @xmath263 samples from the probability distribution @xmath261 ; the @xmath30-th column of @xmath118 comes from @xmath263 independent samples from @xmath261 .",
    "hence we get @xmath264 = { \\mathop{\\bf e\\/}}[\\tilde{m}'_i ] = \\frac{n}{2 } ax$ ] .",
    "condition on @xmath59 the two columns are independent , therefore @xmath265",
    "= \\frac{n^2}{4 } ax x^t a^t$ ] .    to show",
    "the expectation is correct we observe that conditioned on @xmath6 , the entries of two matrices @xmath93 and @xmath118 are independent .",
    "their expectations are both @xmath266 .",
    "therefore ,    @xmath267 = \\frac{4}{mn^2}{\\mathop{\\bf e\\/}}[\\tilde{m } \\tilde{m}'^t ] = \\frac{1}{m } \\left(\\frac{2}{n } { \\mathop{\\bf e\\/}}[\\tilde{m}]\\right ) \\left(\\frac{2}{n } { \\mathop{\\bf e\\/}}[\\tilde{m}'^t]\\right)= \\frac{1}{m }   a w w^t a^t = a ra^t.\\ ] ]    we still need to show that @xmath112 is close to this expectation .",
    "this is not surprising because @xmath112 is the average of @xmath116 independent samples ( of @xmath268 ) .",
    "further , the variance of each entry in @xmath269 can be bounded because @xmath93 and @xmath118 also come from independent samples .",
    "for any @xmath30 , @xmath270 , @xmath271 , let @xmath272 be the probability distribution that @xmath273 and @xmath274 are sampled from , then @xmath275 is distributed as @xmath276 and @xmath277 is distributed as @xmath278 .",
    "the variance of these two variables are less than @xmath279 no matter what @xmath72 is by the properties of binomial distribution .",
    "conditioned on the vector @xmath72 these two variables are independent , thus the variance of their product is at most @xmath280 .",
    "the variance of any entry in @xmath269 is at most @xmath281 .",
    "higher moments can be bounded similarly and they satisfy the assumptions of bernstein inequalities . thus by bernstein inequalities the probability that any entry is more than @xmath257 away from its true value is much smaller than @xmath282 .",
    "the further part follows from the observation that the @xmath15 norm of a row in @xmath112 is proportional to the number of appearances of the word .",
    "as long as the number of appearances concentrates the error in @xmath15 norm must be small .",
    "the words are all independent ( conditioned on @xmath6 ) so this is just a direct application of chernoff bounds .",
    "we are now ready to prove theorem  [ thm : main ] :    by lemma  [ lem : convergeq ] we know when we have at least @xmath283 documents , @xmath112 is entry wise close to @xmath124 . in this case error per row for theorem  [ thm : separablenoise ]",
    "is at most @xmath284 because in this step we can assume that there are at most @xmath285 words ( see section  [ subsec : freqwlog ] ) and to normalize the row we need a multiplicative factor of at most @xmath286 ( we shall only consider rows with @xmath15 norm at least @xmath287 , with high probability all the anchor words are in these rows ) .",
    "the @xmath42 parameter for theorem  [ thm : betterseparablenoise ] is @xmath288 by lemma  [ lem : converger ] .",
    "thus the almost anchor words found by the algorithm has weight at least @xmath289 on diagonals .",
    "the error for @xmath150 is at most @xmath290 , the error for any entry of @xmath149 and @xmath291 is at most @xmath292 .",
    "therefore by lemma  [ lem : recoverwitherror ] the entry - wise error for @xmath4 is at most @xmath293 .",
    "when @xmath294 the error is bounded by @xmath46 . in this case",
    "we need    @xmath295    the latter constraint comes from lemma  [ lem : converger ] .    to get within @xmath46 additive error for the parameter @xmath296",
    ", we further need @xmath68 to be close enough to the variance - covariance matrix of the document - topic distribution , which means @xmath116 is at least    @xmath297      [ lem : separablerow ] suppose each entry of @xmath112 is within @xmath223 of the corresponding entry of @xmath298 .",
    "let @xmath68 and @xmath51 be as described in the first step of recover , then @xmath299 is entry - wise close to @xmath300 with additive error @xmath301 .",
    "since we assumed each word has probability at least @xmath302 , and the norm of a row of @xmath112 is proportional to the number of appearances of the corresponding word , we know the @xmath15 norm of any row in @xmath112 must be at least @xmath303 .",
    "thus after normalization ( each row has @xmath15 norm 1 ) every entry in @xmath112 will have error @xmath304 .",
    "there are at most @xmath114 words , thus each row has @xmath15 error at most @xmath305 .",
    "suppose this is small enough , we treat @xmath300 as the @xmath6 in the theorem . by corollary  [ cor : robustsimplicial ]",
    ", we know @xmath300 is @xmath90 robustly simplicial .",
    "now we apply theorem  [ thm : separablenoise ] to find @xmath68 that is close to @xmath300 in @xmath15 norm with error @xmath306 .",
    "note : here we actually get @xmath15 norm instead of entry - wise error .",
    "we might be able to do something better if we use the `` further '' part in the theorem ( which was not in the original theorem and i added intentionally here )    if @xmath112 is entry - wise close to @xmath298 with @xmath223 additive error @xmath223 , let @xmath307 bethe matrix described in step 2 of recover , then @xmath307 is entry - wise close to @xmath51 with additive error @xmath308 .",
    "( sketch ) as in lemma  [ lem : separablerow ] , the matrix @xmath68 is close to the normalized version of @xmath309 .",
    "let @xmath81 be the set of separable rows , let @xmath56 be the set of rows returned by the algorithm .",
    "we use @xmath310 to denote the matrix @xmath76 restricted to the set of columns @xmath81 ; @xmath311 to denote the matrix @xmath4 restricted to the set of rows @xmath81",
    ". then we have @xmath76 restricted to the columns in @xmath81 ( @xmath312 ) is a diagonal matrix .",
    "thus if we look at the matrix @xmath300 restricted to these columns in @xmath81 , it will be equal to a scaled version of @xmath51 .",
    "let @xmath313 where @xmath314 is a diagonal matrix , then @xmath68 restricted to columns in @xmath81 is close to @xmath315 where @xmath316 is some other diagonal matrix .",
    "in the original @xmath112 we know the corresponding submatrix ( @xmath317 ) is close to @xmath318 and @xmath319 is known ( remember @xmath68 is just the normalization of some rows in @xmath112 ) , the error is at most @xmath225 .",
    "we also know that @xmath112 is almost symmetric ( if there is no error it is symmetric ) .",
    "thus the submatrices at the transposed position should be very close .",
    "that is , @xmath320 is close to @xmath321 , with error @xmath322 .",
    "thus we know @xmath320 is close to @xmath323 , with error @xmath324 . finally , since @xmath325 is also close to @xmath309 , the rows @xmath325 and @xmath326 are close up to scaling .",
    "thus @xmath327 is close to @xmath320 with error @xmath225 .",
    "we know @xmath327 is entry - wise close to the matrix @xmath328 .",
    "the matrix @xmath319 is also known ( and has entries at least @xmath302 ) so we know @xmath329 , we just need to scale the rows of this matrix so it has column sum @xmath52 ( at this point we know we have @xmath330 ) and then take the transpose to recover @xmath51 .",
    "the error can be bounded by @xmath331 .",
    "note : again i do nt think this is the best way to do it .",
    "however i still can not figure out how to do it with much better parameters .",
    "alternate proof for lemma 2.6    suppose @xmath332 , where @xmath333 is the diagonal matrix that ensures the rows of @xmath51 has @xmath15 norm 1 .",
    "the @xmath11 rows in @xmath4 that corresponds to almost anchor words will be @xmath148 , where @xmath56 is a diagonal and @xmath146 is a matrix that is close to identity by theorem  [ thm : separablenoise ] as we explained before .",
    "consequently the matrix @xmath68 ( recall @xmath68 contains @xmath11 rows in @xmath112 corresponding to the almost anchor words ) will be equal to @xmath334 ( plus @xmath223 entry - wise error , however this error is so small that it will be hidden in the @xmath335 notations ) .",
    "since the rows of second term @xmath336 all have @xmath15 norm 1 , the @xmath30-th entry of @xmath314 ( recall that it should be the @xmath15 norm of the @xmath30-th row of @xmath68 ) will be the @xmath15 norm of the @xmath30-th row of @xmath337 .",
    "since @xmath146 is a matrix that is really close to identity , we shall show @xmath337 is close to a diagonal and @xmath338 is almost identity .",
    "this explains why in step 2 of recover we need to multiply @xmath339 by @xmath340 .    by theorem  [ thm :",
    "separablenoise ] , the matrix @xmath146 is a matrix whose rows have at most @xmath341 fraction of weight in off - diagonal entries . also , the entries in @xmath333 are just the expectations of @xmath342 s when @xmath343 are chosen from @xmath344 . as assumed the diagonal entries in @xmath345",
    "are all within ratio @xmath67 .",
    "therefore the matrix @xmath346 is a matrix with at most @xmath347 fraction of weight on off diagonal entries in each row .",
    "when multiplied by @xmath340 it is just a matrix with rows sum to 1 and more than @xmath348 weight on diagonals .",
    "now consider @xmath349 , we know @xmath350 should be close to @xmath351 . in particular",
    ", the matrix @xmath352 is close to @xmath51 with error at most @xmath347 per row .",
    "again we hope since @xmath146 is close to identity if there is a matrix @xmath316 such that @xmath353 has row sum 1 , the matrix @xmath353 should be close to @xmath51 . to prove this rigorously we use the @xmath15 condition number of the matrix @xmath51 .    we shall first write @xmath354 , then we have @xmath353 is equal to @xmath355 where @xmath356 .",
    "now notice that the matrix @xmath357 is close to @xmath51 .",
    "therefore @xmath357 already has row sums close to 1 .",
    "thus the matrix @xmath358 must have small entries because otherwise @xmath359 can not have all rows sum up to 1 .",
    "we know @xmath357 is close to @xmath51 because @xmath360 , the row sums can be differ from 1 by at most @xmath361 ( first term comes from the approximation of @xmath51 , second term comes from @xmath362 ) .",
    "the final matrix we get will be equal to @xmath363 where @xmath364 is set to make all rows sum up to @xmath52 .",
    "since @xmath357 already has row sums close to @xmath52 we know @xmath358 must have row sums smaller than @xmath365 in absolute value .",
    "that is , if we view @xmath364 as the vector @xmath366 ( this is exactly the vector generated by the diagonal entries of @xmath364 ) , it is equal to @xmath367 where @xmath72 is a vector with entries at most @xmath368 in absolute value . by definition of @xmath15 condition number",
    "the @xmath15 norm of @xmath366 is at most @xmath369 if @xmath370 .",
    "however we know @xmath371 is very close to @xmath372 ( indeed , @xmath357 is the product of 3 matrices , @xmath373 , @xmath374 and @xmath375 because they are almost diagonal and it is clear from the definition that @xmath376 ) .",
    "thus everything in @xmath358 can be bounded by @xmath377 .",
    "the result of step 2 is just @xmath355 and it is close to @xmath51 because @xmath357 is almost @xmath51 and all entries in @xmath358 are small .",
    "suppose @xmath68 is entry - wise close to @xmath300 with additive error @xmath225 and @xmath307 is entry - wise close to @xmath51 with additive error @xmath378 .",
    "then set @xmath379 , and @xmath380 is entry - wise close to @xmath4 with additive error @xmath381 .",
    "we will make use of the following matrix formula : @xmath382 setting @xmath383 and @xmath384 .",
    "first , we need to bound the operator norm of @xmath385 and we can accomplish this by noting that @xmath386 and this implies @xmath387 since @xmath51 is an @xmath36 matrix .",
    "let @xmath388 be an arbitrary column of @xmath4 .",
    "we can now bound @xmath389 also consider the quantity @xmath390 we will bound the largest singular value of this matrix ( as the product of the largest singular values of the constituent matrices ) , and this will complete the proof of the lemma .",
    "the smallest singular value of @xmath391 is at least @xmath392 .",
    "consequently the largest singular value of @xmath393 is at most @xmath394 hence the matrix @xmath380 is entry - wise close to @xmath4 with additive error @xmath395    if @xmath307 is entry - wise close to @xmath51 with error @xmath378 , @xmath115 is entry - wise close to @xmath344 with error @xmath65 , step 4 of recover get @xmath115 ( and @xmath344 ) with entry - wise error @xmath396 .",
    "all entries in @xmath344 ( and thus with high probability all entries in @xmath115 ) are larger than @xmath397 .",
    "after normalization every entry in @xmath51 should be at least @xmath398 .",
    "thus @xmath378 entry - wise additive error is at least as strong as @xmath399 entry - wise multiplicative error .",
    "similarly , @xmath115 is close to @xmath344 with @xmath400 multiplicative error .",
    "since @xmath344 is symmetric , the correct scaling @xmath319 should make the resulting matrix @xmath401 close to symmetric .",
    "although we just use the first row and first column these entries have only @xmath402 multiplicative error .",
    "thus the final matrix is close to @xmath344 with multiplicative error @xmath403 .",
    "once we know @xmath344 it is easy to get the @xmath296 vector for dirichlet if the distribution is really a dirichlet .      above we assumed that the number of distinct words is small . here , we give a simple gadget that shows in the general case we can assume that this is the case at the loss of an additional additive @xmath46 in our accuracy :    the general case can be reduced to an instance in which there are at most @xmath114 words all of which ( with at most one exception ) occur with probability at least @xmath302 .",
    "in fact , we can collect all words that occur infrequently and `` merge '' all of these words into a aggregate word that we will call the _",
    "runoff word_. to this end , we call a word large if it appears more than @xmath404 times in @xmath405 documents , and otherwise we call it small . indeed , with high probability all large words are words that occur with probability at least @xmath302 in our model . also , all words that has a entry larger than @xmath46 in the corresponding row of @xmath4 will appear with at least @xmath406 probability , and is thus a large word with high probability .",
    "we can merge all small words ( i.e. rename all of these words to a single , new word ) .",
    "hence we can apply the above algorithm ( which assumed that there are not too many distinct words ) . after we get a result with the modified documents we can ignore the _ runoff words _ and assign @xmath28 weight for all the small words .",
    "the result will still be correct up to @xmath46 additive error .",
    "here we demonstrate that the parameters of a dirichlet distribution can be ( robustly ) recovered from just the covariance matrix @xmath49 .",
    "hence an immediate corollary is that our main learning algorithm can recover both the topic matrix @xmath4 and the distribution that generates columns of @xmath6 in a _ latent dirichlet allocation _ ( lda )",
    "model @xcite , provided that @xmath4 is separable .",
    "we believe that this algorithm may be of practical use , and provides the first alternative to local search and ( unproven ) approximation procedures for this inference problem @xcite , @xcite , @xcite .",
    "the dirichlet distribution is parametrized by a vector @xmath296 of positive reals is a natural family of continuous multivariate probability distributions .",
    "the support of the dirichlet distribution is the unit simplex whose dimension is the same as the dimension of @xmath296 .",
    "let @xmath296 be a @xmath11 dimensional vector .",
    "then for a vector @xmath407 in the @xmath11 dimensional simplex , its probability density is given by    @xmath408 = \\frac{\\gamma(\\sum_{i=1}^r \\alpha_i)}{\\prod_{i=1}^r\\gamma(\\alpha_i ) } \\prod_{i=1}^r \\theta_i^{\\alpha_i-1},\\ ] ]    where @xmath409 is the gamma function .",
    "in particular , when all the @xmath410 s are equal to one , the dirichlet distribution is just the uniform random distribution over the probability simplex .",
    "the expectation and variance of @xmath411 s are easy to compute given the parameters @xmath296 .",
    "we denote @xmath412 , then the ratio @xmath413 should be interpreted as the `` size '' of the @xmath30-th variable @xmath411 , and @xmath414 shows whether the distributions is concentrated in the interior ( when @xmath414 is large ) or near the boundary ( when @xmath414 is small ) .",
    "the first two moments of dirichlet distribution is listed as below :    @xmath415 = \\frac{\\alpha_i}{\\alpha_0}.\\ ] ] @xmath416 = \\left\\ { \\begin{array}{cl } \\frac{\\alpha_i\\alpha_j}{\\alpha_0(\\alpha_0 + 1 ) } & \\mbox{when } i\\neq j \\\\",
    "\\frac{\\alpha_i(\\alpha_i+1)}{\\alpha_0(\\alpha_0 + 1 ) } & \\mbox{when } i = j \\end{array}\\right . .\\ ] ]    suppose the dirichlet distribution has @xmath417 and the sum of parameters is @xmath414 ; we give an algorithm that computes close estimates to the vector of parameters @xmath296 given a sufficiently close estimate to the co - variance matrix @xmath49 ( theorem  [ thm : dirrecov ] ) . combining this with theorem  [ thm : main ]",
    ", we obtain the following corollary :    [ thm : dirichlet ] there is an algorithm that learns the topic matrix @xmath4 with high probability up to an additive error of @xmath46 from at most @xmath418 documents sampled from the lda model and runs in time polynomial in @xmath23 , @xmath116 .",
    "furthermore , we also recover the parameters of the dirichlet distribution to within an additive @xmath46 .",
    "our main goal in this section is to bound the @xmath15-condition number of the dirichlet distribution ( section  [ subsec : condfordirichlet ] ) , and using this we show how to recover the parameters of the distribution from its covariance matrix ( section  [ subsec : recoverdirichlet ] ) .      [ subsec : condfordirichlet ]    there is a well - known meta - principle that if a matrix @xmath6 is chosen by picking its columns independently from a fairly diffuse distribution , then it will be far from low rank .",
    "however , our analysis will require us to prove an explicit lower bound on @xmath419 .",
    "we now prove such a bound when the columns of @xmath6 are chosen from a dirichlet distribution with parameter vector @xmath296 .",
    "we note that it is easy to establish such bounds for other types of distributions as well .",
    "recall that we defined @xmath49 in section  [ sec : back ] , and here we will abuse notation and throughout this section we will denote by @xmath344 the matrix @xmath49 where @xmath16 is a dirichlet distribution with parameter @xmath296 .",
    "let @xmath420 .",
    "the mean , variance and co - variance for a dirichlet distribution are well - known , from which we observe that @xmath421 is equal to @xmath422 when @xmath423 and is equal to @xmath424 when @xmath425 .",
    "[ lem : dirichcondn ] the @xmath15 condition number of @xmath344 is at least @xmath426 .    as the entries @xmath421 is @xmath422 when @xmath423 and @xmath424 when @xmath425 , after normalization @xmath344 is just the matrix @xmath427 where @xmath428 is outer product and @xmath429 is the identity matrix .",
    "let @xmath59 be a vector such that @xmath262 and @xmath430 achieves the minimum in @xmath431 and let @xmath432 and let @xmath433 be the complement .",
    "we can assume without loss of generality that @xmath434 ( otherwise just take @xmath435 instead ) .",
    "the product @xmath436 is @xmath437 .",
    "the first term is a nonnegative vector and hence for each @xmath438 , @xmath439 .",
    "this implies that @xmath440    the following is now an easy corollary from the composition lemma ( lemma  [ lem : compose ] ) and lemma  [ lem : dirichcondn ] .",
    "[ cor : robustsimplicial ] if @xmath4 is @xmath29-separable then @xmath441 is @xmath442 robustly - simplicial .",
    "if the number of documents @xmath116 is large enough then in a topic model the matrix @xmath115 converges to @xmath35 . indeed ,",
    "when @xmath116 is large enough @xmath113 also has all the properties we need with high probability :      [ subsec : recoverdirichlet ] when the variance covariance matrix @xmath344 is recovered with error @xmath443 in @xmath15 norm when viewed as a vector , we can use algorithm  [ alg : dirichlet ] : dirichlet to compute the parameters for the dirichlet .    1 .",
    "set @xmath444 .",
    "2 .   let @xmath30 be the row with smallest @xmath15 norm , let @xmath445 and @xmath446 .",
    "3 .   set @xmath447 .",
    "output @xmath448 .",
    "[ thm : dirrecov ] when the variance covariance matrix @xmath344 is recovered with error @xmath443 in @xmath15 norm when viewed as a vector , the procedure dirichlet(@xmath68 ) learns the parameter of the dirichlet distribution with error at most @xmath449 .",
    "the @xmath413 s all have error at most @xmath443 .",
    "the value @xmath450 is @xmath451 and the value @xmath72 is @xmath452 . since @xmath453 we know",
    "the error for @xmath454 is at most @xmath455 .",
    "finally we need to bound the denominator @xmath456 ( since @xmath457 ) .",
    "thus the final error is at most @xmath458 .",
    "suppose the dirichlet distribution has @xmath417 and the sum of parameters is @xmath414 ; we give an algorithm that computes close estimates to the vector of parameters @xmath296 given a sufficiently close estimate to the co - variance matrix @xmath49 ( theorem  [ thm : dirrecov ] ) . combining this with theorem  [ thm : main ] , we obtain the following corollary :    [ thm : dirichlet ] there is an algorithm that learns the topic matrix @xmath4 with high probability up to an additive error of @xmath46 from at most @xmath418 documents sampled from the lda model and runs in time polynomial in @xmath23 , @xmath116 .",
    "furthermore , we also recover the parameters of the dirichlet distribution to within an additive @xmath46 .",
    "the proof of this theorem can be found in appendix  [ sec : dirichletappendix ] .",
    "in this section , we prove theorem  [ thm : betterseparablenoise ] , which we restate here :    suppose @xmath95 where @xmath6 and @xmath96 are normalized to have rows sum up to 1 , @xmath4 is separable and @xmath6 is @xmath42-robustly simplicial .",
    "when @xmath104 there is a polynomial time algorithm that given @xmath93 such that for all rows @xmath105 , finds @xmath11 row ( almost anchor words ) in @xmath93 .",
    "the @xmath30-th almost anchor word corresponds to a row in @xmath96 that can be represented as @xmath106 . here",
    "@xmath103 is a vector in the convex hull of other rows in @xmath6 with unit length in @xmath15 norm .",
    "the major weakness of the algorithm in @xcite is that it only considers the @xmath15 norm .",
    "however in an @xmath11 dimensional simplex there is another natural measure of distance more suited to our purposes : since each point is a unique convex combination of the vertices , we can view this convex combination as a probability distribution and use statistical distance ( on this representation ) as a norm for points inside the simplex .",
    "we will in fact need a slight modification to this norm , since we would like it to extend to points outside the simplex too :    [ @xmath459-close ] a point @xmath460 is @xmath459-close to @xmath461 if and only if @xmath462    intuitively think of point @xmath460 is @xmath459-close to @xmath461 if @xmath461 is @xmath46 close in @xmath15 distance to some point @xmath112 , where @xmath112 is a convex combination of the rows of @xmath463 that places at least @xmath464 weight on @xmath460 . notice that this definition is not a distance since it is not symmetric , but we will abuse notation and nevertheless call it a distance function .",
    "we remark that this distance is easy to compute : to check whether @xmath460 is @xmath459-close to @xmath461 we just need to solve a linear program that minimizes the @xmath15 distance when the @xmath465 vector is a probability distribution with at least @xmath464 weight on @xmath19 ( the constraints on @xmath465 are clearly all linear ) .    the following lp    @xmath466 \\quad   c_k & \\ge & 0 \\\\ c_j & \\ge & 1-\\delta.\\end{aligned}\\ ] ]    it is easy to see that if the lp is feasible then @xmath460 is @xmath459-close to @xmath467 .    using this lp",
    "we can compute the `` distance '' in this new definition .",
    "in particular we can compute the neighborhood of a row @xmath460 :    we also consider all points that a row @xmath460 is close to , this is called the neighborhood of @xmath460 .",
    "the @xmath468-neighborhood of @xmath460 are the rows @xmath461 such that @xmath460 is @xmath468-close to @xmath461 .",
    "for each point @xmath460 , we know its original ( unperturbed ) point @xmath469 is in a convex combination of @xmath470 s : @xmath471 .",
    "separability implies that for any column index @xmath30 there is a row @xmath472 in @xmath4 whose only nonzero entry is in the @xmath32 column .",
    "then @xmath473 and consequently @xmath474 . let us call these rows @xmath475 for all @xmath30 the _",
    "canonical rows_. from the above description the following claim is clear .",
    "[ claim : repsentationwitherror ] every row @xmath460 has @xmath15-distance at most @xmath167 to the convex hull of canonical rows .",
    "we have : @xmath476 and we can bound the right hand side by @xmath206",
    ".    the algorithm will distinguish rows that are close to vertices and rows that are far by testing whether each row is close ( in @xmath15 norm ) to the convex hull of rows outside its neighborhood .",
    "in particular , we define a robust loner as :    we call a row @xmath460 a robust - loner if it has @xmath15 distance at most @xmath167 to the convex hull of rows that are outside its @xmath477 neighborhood .    our goal is to show that a row is a robust loner if and only if it is close to some row in @xmath6 .",
    "the following lemma establishes one direction :    [ lem : farfromcanonical ] if @xmath478 is smaller than @xmath479 , the point @xmath460 can not be @xmath477-close to the canonical row that corresponds to @xmath480 .",
    "assume towards contradiction that @xmath460 is @xmath477-close to the canonical row @xmath461 which is a perturbation of @xmath480 . by definition",
    "there must be probability distribution @xmath481 over the rows such that @xmath482 , and @xmath483 .",
    "now we instead consider the unperturbed matrix @xmath96 , since every row of @xmath463 is @xmath46 close ( in @xmath15 norm ) to @xmath96 we know @xmath484 .",
    "now we represent @xmath485 and @xmath486 as convex combinations of rows of @xmath6 and consider the coefficient on @xmath480 .",
    "clearly @xmath487 so the coefficient is 1 .",
    "but for @xmath486 , since @xmath488 and the coefficient @xmath489 , we know the coefficient of @xmath480 in the sum must be strictly smaller than @xmath490 . by the robustly simplicial assumption @xmath485 and @xmath486",
    "must be more than @xmath491 apart in @xmath15 norm , which contradicts our assumption .    as a corollary",
    ":    if @xmath478 is smaller than @xmath492 for all @xmath493 , the row @xmath460 can not be a robust loner .    by the above lemma",
    ", we know the canonical rows are not in the @xmath494 neighborhood of @xmath460 .",
    "thus by claim  [ claim : repsentationwitherror ] the row is close to the convex hull of canonical rows and can not be a robust loner .",
    "next we prove the other direction : a canonical row is necessarily a robust loner :    all canonical rows are robust loners .",
    "suppose @xmath461 is a canonical row that corresponds to @xmath480 .",
    "we first observe that all the rows that are outside the @xmath477 neighborhood of @xmath460 must have @xmath495 .",
    "this is because when @xmath496 we have @xmath497 . if we replace @xmath498 by @xmath460 and @xmath480 by the corresponding canonical row , the distance is still at most @xmath167 and the coefficient on @xmath461 is at least @xmath499 . by definition",
    "the corresponding row @xmath460 must be in the neighborhood of @xmath461 .",
    "now we try to represent @xmath461 with convex combination of rows that has @xmath495 .",
    "however this is impossible because every point in the convex combination will also have weight smaller than @xmath499 on @xmath480 , while @xmath485 has weight 1 on @xmath480 .",
    "the @xmath15 distance between @xmath500 and the convex combination of the @xmath469 s where @xmath495 is a least @xmath188 by robust simplicial property .",
    "even when the points are perturbed by @xmath46 ( in @xmath15 ) the distance can change by at most @xmath167 and is still more than @xmath167 .",
    "therefore @xmath461 is a robust loner .",
    "now we can prove the main theorem of this section :    suppose we know @xmath42 and @xmath501.when @xmath42 is so small we have the following claim :    if @xmath478 and @xmath502 is at least @xmath492 , and @xmath503 , then @xmath460 can not be @xmath504-close to @xmath461 and vice versa .",
    "the proof is almost identical to lemma  [ lem : farfromcanonical ] .",
    "also , the canonical row that corresponds to @xmath480 is @xmath504 close to all rows with @xmath505 .",
    "thus if we connect two robust loners when one is @xmath504 close to the other , the connected component of the graph will exactly be a partition according to the row in @xmath6 that the robust loner is close to .",
    "we pick one robust loner in each connected component to get the almost anchor words .",
    "now suppose we do nt know @xmath42 . in this case",
    "the problem is we do nt know what is the right size of neighborhood to look at .",
    "however , since we know @xmath506 , we shall first run the algorithm with @xmath507 to get @xmath11 rows @xmath99 that are very close to the true rows in @xmath6 .",
    "it is not hard to show that these rows are at least @xmath69 robustly simplicial and at most @xmath508 robustly simplicial .",
    "therefore we can compute the @xmath509 parameter for this set of rows and use @xmath510 as the @xmath42 parameter .",
    "here we prove that computing the maximum likelihood estimate ( mle ) of the parameters of a topic model is @xmath3-hard .",
    "we call this problem the topic model maximum likelihoood estimation ( tm - mle ) problem :    given @xmath116 documents and a target of @xmath11 topics , the tm - mle problem asks to compute the topic matrix @xmath4 that has the largest probability of generating the observed documents ( when the columns of @xmath6 are generated by a uniform dirichlet distribution ) .",
    "surprisingly , this appears to be the first proof that computing the mle estimate in a topic model is indeed computationally hard , although its hardness is certainly to be expected . on a related note , sontag and roy",
    "@xcite recently proved that _ given the topic matrix _ and a document , computing the maximum a posteriori ( map ) estimate for the distribution on topics that generated this document is @xmath3-hard . here",
    "we will establish that tm - mle is @xmath3-hard via a reduction from the min - bisection problem : in min - bisection the input is a graph with @xmath23 vertices ( @xmath23 is an even integer ) , and the goal is to partition the vertices into two equal sized sets of @xmath511 vertices each so as to minimize the number of edges crossing the cut .",
    "there is a polynomial time reduction from min - bisection to tm - mle ( @xmath512 ) .",
    "suppose we are given an instance @xmath357 of the min - bisection problem with @xmath23 vertices and @xmath116 edges .",
    "we will now define an instance of the tm - mle problem .",
    "first , we set the number of words to be @xmath23 . for each word @xmath30 , we construct @xmath513 documents each of which contain the word @xmath30 twice and no other words .",
    "for each edge in the graph @xmath357 , we construct a document whose two words correspond to the endpoints of the edge .",
    "suppose that @xmath514 is generated by the dirichlet distribution @xmath515 .",
    "consequently the probability that words @xmath30 and @xmath19 appear in a document with only two words is exactly @xmath516 .",
    "we can take the expectation of this term over the dirichlet distribution @xmath515 and hence the probability that a document ( with exactly two words ) contains the words @xmath30 and @xmath19 is @xmath517 = \\frac{1}{3 } ( a^i \\cdot a^j ) + \\frac{1}{6 } ( a^i_1a^j_2 + a^j_1 a^i_2)\\ ] ] in the tm - mle problem , our goal is to maximize the following objective function ( which is the @xmath518 of the probability of generating the collection of documents ) :    @xmath519.\\ ] ]    for any bisection , we define a canonical solution : the first topic is uniform on all words on one side of the bisection and the second topic is uniform on all words on the other side of the bisection.to prove the correctness of our reduction , a key step is to show that any candidate solution to the mle problem must be close to a canonical solution .",
    "in particular , we show the following :    1 .",
    "the rows @xmath520 have almost the same @xmath15 norm .",
    "2 .   in each row @xmath520 ,",
    "almost all of the weight will be in one of the two topics .    indeed",
    ", canonical solutions have large objective value .",
    "any canonical solution has objective value at least @xmath521 ( this is because documents with same words contribute @xmath522 and documents with different words contribute at least @xmath523 ) .    recall , in our reduction @xmath0 is large .",
    "roughly , if one of the rows has @xmath15 norm that is bounded away from @xmath524 by at least @xmath525 , the contribution ( to the objective function ) of documents with a repeated word will decrease significantly and the solution can not be optimal . to prove this we use the fact that the function @xmath526 is concave .",
    "therefore when one of the rows has @xmath15 norm more than @xmath527 , the optimal value for documents with a repeated word will be attained when all other rows have the same @xmath15 norm @xmath528 .",
    "using a taylor expansion , we conclude that the sum of terms for documents with a repeated word will decrease by at least @xmath529 which is much larger than any effect the remaining @xmath116 documents can recoup . in fact",
    ", an identical argument establishes that in each row @xmath520 , the topic with smaller weight will always have weight smaller than @xmath525 .",
    "now we claim that among canonical solutions , the one with largest objective value corresponds to a minimum bisection .",
    "the proof follows from the observation that the value of the objective function is @xmath530 for canonical solutions , where @xmath22 is the number of edges cut by the bisection .",
    "in particular , the objective function of the minimum bisection will be at least an additive @xmath531 larger than the objective function of a non - minimum bisection .",
    "however , even if the canonical solution is perturbed by @xmath525 , the objective function will only change by at most @xmath532 , which is much smaller than @xmath533 . and",
    "this completes our reduction .",
    "we remark that the canonical solutions in our reduction are all _ separable _ , and hence this reduction applies even when the topic matrix @xmath4 is known ( and required ) to be separable .",
    "so , even in the case of a separable topic matrix , it is @xmath3-hard to compute the mle .",
    "we expect that versions of our algorithm may indeed be practical , and are investigating this possibility .",
    "our machine learning colleagues suggest that real - life topic matrices satisfy even stronger _ separability _",
    "assumptions , e.g. , the presence of _ many _ anchor words per topic instead of a single one .",
    "this is a promising suggestion , but leveraging it in our algorithm is an open problem .",
    "is separability necessary for allowing polynomial - time algorithms for the learning problems considered here ?",
    "in other words , is the problem difficult if the topic matrix @xmath4 is not _",
    "separable _ ?",
    "average - case intractability seems more plausible here than np - completeness .",
    "we thank dave blei , ravi kannan , david minmo , sham kakade , david sontag for many helpful discussions throughout various stages of this work .    99    a. anandkumar , d. hsu and s. kakade .",
    "a method of moments for mixture models and hidden markov models .",
    "arxiv , 2012 .",
    "s. arora , r. ge , r. kannan and a. moitra . computing a nonnegative matrix factorization  provably .",
    "2012 , to appear .",
    "y. azar , a. fiat , a. karlin , f. mcsherry and j. saia .",
    "spectral analysis of data . , pp . 619626 , 2001 .",
    "introduction to probabilistic topic models . , pp . 7784 , 2012 .",
    "personal communication .",
    "d. blei , a. ng and m. jordan .",
    "latent dirichlet allocation . , pp . 9931022 , 2003 .",
    "preliminary version in _ nips _ 2001 .",
    "d. blei and j. lafferty . a correlated topic model of science .",
    ", pp . 1735 , 2007 .",
    "d. blei and j. lafferty .",
    "dynamic topic models . , pp . 113120 , 2006 .",
    "j. cohen and u. rothblum .",
    "nonnegative ranks , decompositions and factorizations of nonnegative matices . , pp . 149168 , 1993 .",
    "s. deerwester , s. dumais , t. landauer , g. furnas and r. harshman .",
    "indexing by latent semantic analysis . , pp . 391407 , 1990 .",
    "a.  p. dempster , n.  m. laird , and d.  b. rubin .",
    "maximum likelihood from incomplete data via the em algorithm . , pp . 138 , 1977 .",
    "d. donoho and v. stodden .",
    "when does non - negative matrix factorization give the correct decomposition into parts ?",
    ", 2003 .",
    "g. golub and c. van loan .",
    "the johns hopkins university press , 1996 .",
    "n. gravin , j. lasserre , d. pasechnik and s. robins .",
    "the inverse moment problem for convex polytopes .",
    ", 2012 , to appear .",
    "t. hofmann .",
    "probabilistic latent semantic analysis . , pp .",
    "289296 , 1999 .",
    "non - negative matrix factorization with sparseness constraints . , pp . 14571469 , 2004 .",
    "a. hyvrinen , j. karhunen and e. oja . .",
    "wiley interscience , 2001 .",
    "m. jordan , z. ghahramani , t. jaakola and l. saul .",
    "introduction to variational methods for graphical models . , pp . 183233 , 1999 .",
    "j. kleinberg and m. sandler .",
    "using mixture models for collaborative filtering . , pp . 4969 , 2008 .",
    "preliminary version in _ stoc _ 2004 .    j. kleinberg and m. sandler .",
    "convergent algorithms for collaborative filtering . ,",
    "pp . 110 , 2003 .",
    "r. kumar , p. raghavan , s. rajagopalan and a. tomkins .",
    "recommendation systems : a probabilistic analysis . , pp . 4261 , 2001 .",
    "preliminary version in _ focs _ 1998 .",
    "d. lee and h. seung .",
    "learning the parts of objects by non - negative matrix factorization .",
    "788 - 791 , 1999 .",
    "d. lee and h. seung .",
    "algorithms for non - negative matrix factorization . , pp .",
    "556562 , 2000 .",
    "w. li and a. mccallum .",
    "pachinko allocation : dag - structured mixture models of topic correlations .",
    "633 - 640 , 2007 .",
    "j. matousek . .",
    "springer , 2002 .",
    "f. mcsherry .",
    "spectral partitioning of random graphs . , pp . 529537 , 2001 .",
    "c. papadimitriou , p. raghavan , h. tamaki and s. vempala .",
    "latent semantic indexing : a probabilistic analysis . , pp .",
    "217235 , 2000 .",
    "preliminary version in _ pods _ 1998 .",
    "r.  a. redner and h.  f. walker .",
    "mixture densities , maximum likelihood and the em algorithm .",
    "195 - 239 , 1984 .",
    "d. sontag and d. roy .",
    "complexity of inference in latent dirichlet allocation . , pp . 10081016 , 2011 .",
    "w. xu and x. liu and y. gong .",
    "document clustering based on non - negative matrix factorization . , pp . 267273 , 2003 .",
    "s. vavasis . on the complexity of nonnegative matrix factorization . ,",
    "1364 - 1377 , 2009 .    m. wainwright and m. jordan .",
    "graphical models , exponential families , and variational inference . , pp . 1305 , 2008 .",
    "if @xmath226 are sufficiently small , recover outputs @xmath4 such that each entry of @xmath4 has additive error at most @xmath227 .",
    "also the matrix @xmath68 has additive error @xmath189 whose @xmath15 norm when viewed as a vector is at most @xmath228 .",
    "there is a polynomial time reduction from min - bisection to tm - mle ( @xmath512 ) ."
  ],
  "abstract_text": [
    "<S> _ topic modeling _ is an approach used for automatic comprehension and classification of data in a variety of settings , and perhaps the canonical application is in uncovering thematic structure in a corpus of documents . </S>",
    "<S> a number of foundational works both in machine learning @xcite and in theory @xcite have suggested a probabilistic model for documents , whereby documents arise as a convex combination of ( i.e. distribution on ) a small number of _ topic _ vectors , each topic vector being a distribution on words ( i.e. a vector of word - frequencies ) . </S>",
    "<S> similar models have since been used in a variety of application areas ; the _ latent dirichlet allocation _ or lda model of blei et al .  </S>",
    "<S> is especially popular .    </S>",
    "<S> theoretical studies of topic modeling focus on learning the model s parameters _ assuming the data is actually generated from it . </S>",
    "<S> _ existing approaches for the most part rely on _ singular value decomposition _ ( svd ) , and consequently have one of two limitations : these works need to either assume that each document contains only one topic , or else can only recover the _ span _ of the topic vectors instead of the topic vectors themselves .    </S>",
    "<S> this paper formally justifies _ nonnegative matrix factorization _ ( nmf ) as a main tool in this context , which is an analog of svd where all vectors are nonnegative . using this tool we give the first polynomial - time algorithm for learning topic models without the above two limitations . </S>",
    "<S> the algorithm uses a fairly mild assumption about the underlying topic matrix called _ separability _ , which is usually found to hold in real - life data . </S>",
    "<S> a compelling feature of our algorithm is that it generalizes to models that incorporate topic - topic correlations , such as the _ correlated topic model _ ( ctm ) and the _ pachinko allocation model _ </S>",
    "<S> ( pam ) .    </S>",
    "<S> we hope that this paper will motivate further theoretical results that use nmf as a replacement for svd  just as nmf has come to replace svd in many applications . </S>"
  ]
}