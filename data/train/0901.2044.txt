{
  "article_text": [
    "let @xmath1 be independent random variables with common unknown density @xmath2 in @xmath3 .",
    "let @xmath4 be a finite set of functions with @xmath5 , called a dictionary .",
    "we consider estimators of @xmath2 that belong to the linear span of @xmath4",
    ". we will be particularly interested in the case where @xmath6 .",
    "denote by @xmath7 the linear combinations @xmath8 let us mention some examples where such estimates are of importance :    * _ estimation in sparse mixture models_. assume that the density @xmath2 can be represented as a finite mixture @xmath9 where @xmath10 are known probability densities and @xmath11 is a vector of mixture probabilities .",
    "the number @xmath12 can be very large , much larger than the sample size @xmath13 , but we believe that the representation is sparse , that is , that very few coordinates of @xmath11 are nonzero , with indices corresponding to a set @xmath14 . our goal is to estimate the weight vector @xmath11 by a vector @xmath15 that adapts to this unknown sparsity and to identify @xmath16 , with high probability . * _ adaptive nonparametric density estimation .",
    "_ assume that the density @xmath2 is a smooth function , and @xmath4 are the first @xmath12 functions from a basis in @xmath17 .",
    "if the basis is orthonormal , a natural idea is to estimate @xmath2 by an orthogonal series estimator which has the form @xmath18 with @xmath19 having the coordinates @xmath20 .",
    "however , it is well known that such estimators are very sensitive to the choice of @xmath12 , and a data - driven selection of @xmath12 or thresholding is needed to achieve adaptivity ( cf . ,",
    "e.g. , @xcite ) ; moreover , these methods have been applied with @xmath21 .",
    "we would like to cover more general problems where the system @xmath22 is not necessarily orthonormal , even not necessarily a basis , @xmath12 is not necessarily smaller than @xmath13 , but an estimate of the form @xmath23 still achieves , adaptively , the optimal rates of convergence . * _ aggregation of density estimators . _",
    "assume now that @xmath24 are some preliminary estimators of @xmath2 constructed from a training sample independent of @xmath25 , and we would like to aggregate @xmath24 .",
    "this means that we would like to construct a new estimator , the aggregate , which is approximately as good as the best among @xmath24 or approximately as good as the best linear or convex combination of @xmath24 .",
    "general notions of aggregation and optimal rates are introduced in @xcite .",
    "aggregation of density estimators is discussed in @xcite and more recently in @xcite where one can find further references .",
    "the aggregates that we have in mind here are of the form @xmath23 with suitably chosen weights @xmath26 .",
    "in this paper we suggest a data - driven choice of @xmath15 that can be used in all the examples mentioned above and also more generally .",
    "we define @xmath15 as a minimizer of an @xmath0-penalized criterion , that we call spades ( sparse density estimation ) .",
    "this method was introduced in @xcite .",
    "the idea of @xmath0-penalized estimation is widely used in the statistical literature , mainly in linear regression where it is usually referred to as the lasso criterion @xcite . for gaussian sequence models or for regression with an orthogonal design matrix the lasso is equivalent to soft thresholding @xcite .",
    "model selection consistency of the lasso type linear regression estimators is treated in many papers including @xcite .",
    "recently , @xmath0-penalized methods have been extended to nonparametric regression with general fixed or random design @xcite , as well as to some classification and other more general prediction type models @xcite .    in this paper",
    "we show that @xmath0-penalized techniques can also be successfully used in density estimation . in section [ sec2 ]",
    "we give the construction of the spades estimates and we show that they satisfy general oracle inequalities in section [ sec3 ] . in the remainder of the paper",
    "we discuss the implications of these results for two particular problems , identification of mixture components and adaptive nonparametric density estimation . for the application of spades in aggregation problems",
    "we refer to @xcite .",
    "section [ sec4 ] is devoted to mixture models .",
    "a vast amount of literature exists on estimation in mixture models , especially when the number of components is known ; see , for example , @xcite for examples involving the em algorithm .",
    "the literature on determining the number of mixture components is still developing , and we will focus on this aspect here .",
    "recent works on the selection of the number of components ( mixture complexity ) are @xcite . a consistent selection procedure specialized to gaussian mixtures is suggested in @xcite .",
    "the method of @xcite relies on comparing a nonparametric kernel density estimator with the best parametric fit of various given mixture complexities .",
    "nonparametric estimators based on the combinatorial density method ( see @xcite ) are studied in @xcite .",
    "these can be applied to estimating consistently the number of mixture components , when the components have known functional form .",
    "both @xcite can become computationally infeasible when @xmath12 , the number of candidate components , is large .",
    "the method proposed here bridges this gap and guarantees correct identification of the mixture components with probability close to 1 .    in section [ sec4 ]",
    "we begin by giving conditions under which the mixture weights can be estimated accurately , with probability close to 1 .",
    "this is an intermediate result that allows us to obtain the main result of section [ sec4 ] , correct identification of the mixture components .",
    "we show that in identifiable mixture models , if the mixture weights are above the noise level , then the components of the mixture can be recovered with probability larger than @xmath27 , for any given small @xmath28 .",
    "our results are nonasymptotic , they hold for any @xmath12 and @xmath13 . since the emphasis here is on correct component selection , rather than optimal density estimation , the tuning sequence that accompanies the @xmath0 penalty needs to be slightly larger than the one used for good prediction .",
    "the same phenomenon has been noted for @xmath0-penalized estimation in linear and generalized regression models ; see , for example , @xcite .",
    "section [ sec5 ] uses the oracle inequalities of section [ sec3 ] to show that spades estimates adaptively achieve optimal rates of convergence ( up to a logarithmic factor ) simultaneously on a large scale of functional classes , such as hlder , sobolev or besov classes , as well as on the classes of sparse densities , that is , densities having only a finite , but unknown , number of nonzero wavelet coefficients .",
    "section [ sec61 ] offers an algorithm for computing the spades .",
    "our procedure is based on coordinate descent optimization , recently suggested by @xcite . in section",
    "[ sec : gbm ] we use this algorithm together with a tuning parameter chosen in a data adaptive manner .",
    "this choice employs the generalized bisection method first introduced in @xcite , a computationally efficient method for constructing candidate tuning parameters without performing a grid search .",
    "the final tuning parameter is chosen from the list of computed candidates by using a 10-fold cross - validated dimension - regularized criterion .",
    "the combined procedure works very well in practice , and we present a simulation study in section [ sec63 ] .",
    "consider the @xmath17 norm @xmath29 associated with the inner product @xmath30 for @xmath31 .",
    "note that if the density @xmath2 belongs to @xmath17 and @xmath32 has the same distribution as @xmath33 , we have , for any @xmath34 , @xmath35 where the expectation is taken under @xmath2 .",
    "moreover , @xmath36 in view of identity ( [ id1 ] ) , minimizing @xmath37 in @xmath38 is the same as minimizing @xmath39 the function @xmath40 depends on @xmath2 but can be approximated by its empirical counterpart @xmath41 this motivates the use of @xmath42 as the empirical criterion ; see , for instance , @xcite .",
    "we define the penalty @xmath43 with weights @xmath44 to be specified later , and we propose the following data - driven choice of @xmath38 : @xmath45\\\\[-8pt ] & = & \\mathop{\\arg\\min}_{\\lambda\\in\\mathbb r^m } \\biggl\\ { - \\frac{2}{n}\\sum _ { i=1}^n \\mathsf{f}_\\lambda(x_i ) + \\| \\mathsf{f}_\\lambda\\|^2 + 2 \\sum _ { j=1}^m \\omega_j    @xmath2 that we will further call the _ spades estimator _ is defined by @xmath46 it is easy to see that , for an orthonormal system @xmath22 , the spades estimator coincides with the soft thresholding estimator whose components are of the form @xmath47 where @xmath48 and @xmath49 .",
    "we see that in this case @xmath44 is the threshold for the @xmath50th component of a preliminary estimator @xmath51 .",
    "the spades estimate can be easily computed by convex programming even if @xmath6 .",
    "we present an algorithm in section [ sec6 ] below .",
    "spades retains the desirable theoretical properties of other density estimators , the computation of which may become problematic for @xmath6 .",
    "we refer to @xcite for a thorough overview on combinatorial methods in density estimation , to @xcite for density estimation using support vector machines and to @xcite for density estimates using penalties proportional to the dimension .",
    "for any @xmath52 , let @xmath53 be the set of indices corresponding to nonzero components of @xmath38 and @xmath54 its cardinality . here",
    "@xmath55 denotes the indicator function .",
    "furthermore , set @xmath56 for @xmath57 , where @xmath58 denotes the variance of random variable @xmath59 and @xmath60 is the @xmath61 norm .",
    "we will prove sparsity oracle inequalities for the estimator @xmath62 , provided the weights @xmath44 are chosen large enough .",
    "we first consider a simple choice : @xmath63 where @xmath64 is a user - specified parameter and @xmath65 the oracle inequalities that we prove below hold with a probability of at least @xmath66 and are nonasymptotic : they are valid for all integers @xmath12 and @xmath13 .",
    "the first of these inequalities is established under a coherence condition on the `` correlations '' @xmath67 for @xmath52 , we define a local coherence number ( called _ maximal local coherence _ ) by @xmath68 and @xmath69      [ th : mutcoh ] assume that @xmath70 for @xmath57 .",
    "then with probability at least @xmath66 for all @xmath52 that satisfy @xmath71 and all @xmath72 , we have the following oracle inequality : @xmath73    note that only a condition on the local coherence ( [ mutcoh ] ) is required to obtain the result of theorem [ th : mutcoh ] .",
    "however , even this condition can be too strong , because the bound on `` correlations '' should be _ uniform _ over @xmath74 ; cf . the definition of @xmath75 .",
    "for example , this excludes the cases where the `` correlations '' can be relatively large for a small number of pairs @xmath76 and almost zero for otherwise . to account for this situation",
    ", we suggest below another version of theorem [ th : mutcoh ] . instead of maximal local coherence",
    ", we introduce _ cumulative local coherence _ defined by @xmath77",
    "assume that @xmath70 for @xmath57 .",
    "then with probability at least @xmath66 for all @xmath52 that satisfy @xmath78 and all @xmath72 , we have the following oracle inequality : @xmath79    theorem [ th : cumcoh ] is useful when we deal with sparse gram matrices @xmath80 that have only a small number @xmath81 of nonzero off - diagonal entries .",
    "this number will be called a _ sparsity index _ of matrix @xmath82 , and is defined as @xmath83 where @xmath84 is the @xmath76th entry of @xmath82 and @xmath85 denotes the cardinality of a set @xmath86 .",
    "clearly , @xmath87 .",
    "we therefore obtain the following immediate corollary of theorem [ th : cumcoh ] .",
    "[ cor:1 ] let @xmath82 be a gram matrix with sparsity index @xmath81 .",
    "then the assertion of theorem [ th : cumcoh ] holds if we replace there ( [ cumcoh ] ) by the condition @xmath88    we finally give an oracle inequality , which is valid under the assumption that the gram matrix @xmath82 is positive definite .",
    "it is simpler to use than the above results when the dictionary is orthonormal or forms a frame .",
    "note that the coherence assumptions considered above do not necessarily imply the positive definiteness of  @xmath82 .",
    "vice versa , the positive definiteness of @xmath82 does not imply these assumptions .",
    "[ thm:1a ] assume that @xmath70 for @xmath57 and that the gram matrix @xmath82 is positive definite with minimal eigenvalue larger than or equal to @xmath89 .",
    "then , with probability at least @xmath66 , for all @xmath72 and all @xmath52 , we have @xmath90 where @xmath91    we can consider some other choices for @xmath44 without affecting the previous results .",
    "for instance , @xmath92 or @xmath93 with @xmath94 yield the same conclusions .",
    "these modifications of ( [ a ] ) prove useful , for example , for situations where @xmath10 are wavelet basis functions ; cf .",
    "section [ sec5 ] .",
    "the choice ( [ c ] ) of @xmath44 has an advantage of being completely data - driven .",
    "[ thm:3 ] theorems [ th : mutcoh][thm:1a ] and corollary [ cor:1 ] hold with the choices ( [ bb ] ) or ( [ c ] ) for the weights @xmath44 without changing the assertions .",
    "they also remain valid if we replace these @xmath44 by any @xmath95 such that @xmath96 .",
    "if @xmath44 is chosen as in ( [ c ] ) , our bounds on the risk of spades estimator involve the random variables @xmath97 .",
    "these can be replaced in the bounds by deterministic values using the following lemma .",
    "[ oneside ] assume that @xmath98 for @xmath99 .",
    "then @xmath100    from theorem [ thm:3 ] and lemma [ oneside ] we find that , for the choice of @xmath44 as in ( [ c ] ) , the oracle inequalities of theorems [ th : mutcoh][thm:1a ] and corollary [ cor:1 ] remain valid with probability at least @xmath101 if we replace the @xmath44 in these inequalities by the expressions @xmath102 where @xmath103 .",
    "we first prove the following preliminary lemma .",
    "define the random variables @xmath104 and the event @xmath105    [ prelim ] assume that @xmath98 for @xmath99 .",
    "then for all @xmath52 we have that , on the event @xmath86 , @xmath106    by the definition of @xmath107 , @xmath108 for all @xmath52 .",
    "we rewrite this inequality as @xmath109 then , on the event @xmath86 , @xmath110 add @xmath111 to both sides of the inequality to obtain @xmath112 where we used that @xmath113 for @xmath114 and the triangle inequality .    for the choice ( [ a ] ) for @xmath44",
    ", we find by hoeffding s inequality for sums of independent random variables @xmath115 with @xmath116 that @xmath117    proof of theorem [ th : mutcoh ] in view of lemma [ prelim ] , we need to bound @xmath118 .",
    "set @xmath119 then , by the definition of @xmath120 , @xmath121 since @xmath122 we obtain @xmath123 the left - hand side can be bounded by @xmath124 using the cauchy ",
    "schwarz inequality , and we obtain that @xmath125 which immediately implies @xmath126 hence , by lemma [ prelim ] , we have , with probability at least @xmath66 , @xmath127 for all @xmath52 that satisfy relation ( [ mutcoh ] ) , we find that , with probability exceeding @xmath66 , @xmath128 after applying the inequality @xmath129 ( @xmath130 ) for each of the last two summands , we easily find the claim .",
    "proof of theorem [ th : cumcoh ] the proof is similar to that of theorem [ th : mutcoh ] . with @xmath131",
    "we obtain now the following analogue of ( [ aa ] ) : @xmath132 hence , as in the proof of theorem [ th : mutcoh ] , we have @xmath133 and using the inequality @xmath134 , we find @xmath135 note that ( [ b1 ] ) differs from ( [ b ] ) only in the fact that the factor @xmath136 on the right - hand side is now replaced by @xmath137 . up to this modification , the rest of the proof is identical to that of theorem [ th : mutcoh ] .",
    "proof of theorem [ thm:1a ] by the assumption on @xmath82 , we have @xmath138 by the cauchy ",
    "schwarz inequality , we find @xmath139 combination with lemma [ prelim ] yields that , with probability at least @xmath66 , @xmath140 where @xmath141 . applying the inequality @xmath129 ( @xmath130 ) for each of the last two summands in ( [ simple ] ) , we get the result .",
    "proof of theorem [ thm:3 ] write @xmath142 for the choice of @xmath44 in ( [ bb ] ) . using bernstein s exponential inequality for sums of independent random variables",
    "@xmath143 with @xmath116 , we obtain that @xmath144\\\\[-8pt ] & \\le & \\sum_{j=1}^m \\exp\\biggl ( -\\frac{n \\bar\\omega_j^2/4}{2\\operatorname{var}(f_j(x_1 ) ) + 2 l_j \\bar\\omega_j /3 } \\biggr)\\nonumber\\\\ & \\le & m\\exp\\bigl ( -nr^2(\\delta/2)\\bigr ) = \\delta/2.\\nonumber\\end{aligned}\\ ] ] let now @xmath44 be defined by ( [ c ] ) .",
    "then , using ( [ bern ] ) , we can write @xmath145 define @xmath146 and note that @xmath147 then @xmath148 which is less than @xmath149 . plugging this in ( [ bern1 ] )",
    "concludes the proof .",
    "proof of lemma [ oneside ] using bernstein s exponential inequality for sums of independent random variables @xmath150 and the fact that @xmath151 , we find @xmath152 which implies the lemma .",
    "in this section we assume that the true density @xmath2 can be represented as a finite mixture @xmath153 where @xmath14 is unknown , @xmath154 are known probability densities and @xmath155 for all @xmath156 .",
    "we focus in this section on model selection , that is , on the correct identification of the set @xmath16",
    ". it will be convenient for us to normalize the densities @xmath154 by their @xmath157 norms and to write the model in the form @xmath158 where @xmath14 is unknown , @xmath159 are known functions and @xmath160 for all @xmath156 .",
    "we set @xmath161 , where @xmath162 .    for clarity of exposition , we consider a simplified version of the general setup introduced above .",
    "we compute the estimates of @xmath11 via ( [ original ] ) , with weights defined by [ cf .",
    "( [ a ] ) ] : @xmath163 where @xmath164 is a constant that we specify below , and for clarity of exposition we replaced all @xmath165 by an upper bound @xmath166 on @xmath167 . recall that , by construction , @xmath168 for all @xmath50 . under these assumptions condition ( [ mutcoh ] )",
    "takes the form @xmath169 we state ( [ sim ] ) for the true vector @xmath11 in the following form :    [ conda ] @xmath170 where @xmath171 and @xmath172 .",
    "similar conditions are quite standard in the literature on sparse regression estimation and compressed sensing ; cf . , for example ,",
    "the difference is that those papers use the empirical version of the correlation @xmath173 and the numerical constant in the inequality is , in general , different from @xmath174 .",
    "note that condition [ conda ] is quite intuitive .",
    "indeed , the sparsity index @xmath175 can be viewed as the effective dimension of the problem .",
    "when @xmath175 increases the problem becomes harder , so that we need stronger conditions ( smaller correlations @xmath173 ) in order to obtain our results .",
    "the interesting case that we have in mind is when the effective dimension @xmath175 is small , that is , the model is sparse",
    ".    the results of section [ sec3 ] are valid for any @xmath176 larger or equal to @xmath177 .",
    "they give bounds on the predictive performance ofspades .",
    "as noted in , for example , @xcite , for @xmath0-penalized model selection in regression , the tuning sequence @xmath44 required for correct selection is typically larger than the one that yields good prediction .",
    "we show below that the same is true for selecting the components of a mixture of densities .",
    "specifically , in this section we will take the value @xmath178 we will use the following corollary of theorem [ th : mutcoh ] , obtained for @xmath179 .",
    "[ cormixt ] assume that condition [ conda ] holds .",
    "then with probability at least @xmath180 , we have @xmath181    inequality ( [ buna ] ) guarantees that the estimate @xmath107 is close to the true @xmath11 in @xmath0 norm , if the number of mixture components @xmath175 is substantially smaller than @xmath182 .",
    "we regard this as an intermediate step for the next result that deals with the identification of @xmath16 .",
    "we now show that @xmath16 can be identified with probability close to 1 by our procedure .",
    "let @xmath183 be the set of indices of the nonzero components of @xmath107 given by ( [ original ] ) . in what follows we investigate when @xmath184 for a given @xmath185 .",
    "our results are nonasymptotic , they hold for any fixed @xmath12 and @xmath13 .",
    "we need two conditions to ensure that correct recovery of @xmath16 is possible .",
    "the first one is the identifiability of the model , as quantified by condition [ conda ] above .",
    "the second condition requires that the weights of the mixture are above the noise level , quantified by @xmath176 .",
    "we state it as follows :    [ condb ] @xmath186 where @xmath187 and @xmath176 is given in ( [ newr ] ) .",
    "[ whole ] let @xmath188 be a given number .",
    "assume that conditions [ conda ] and [ condb ] hold",
    ". then @xmath189 .",
    "since all @xmath190 are nonnegative , it seems reasonable to restrict the minimization in ( [ original ] ) to @xmath38 with nonnegative components .",
    "inspection of the proofs shows that all the results of this section remain valid for such a modified estimator .",
    "however , in practice , the nonnegativity issue is not so important .",
    "indeed , the estimators of the weights are quite close to the true values and turn out to be positive for positive @xmath190 . for example , this was the case in our simulations discussed in section [ sec6 ] below .",
    "on the other hand , adding the nonnegativity constraint in ( [ original ] ) introduces some extra burden on the numerical algorithm . more generally , it is trivial to note that the results of this and previous sections extend verbatim to the setting where @xmath191 with @xmath192 being any subset of @xmath193 .",
    "then the minimization in ( [ original ] ) should be performed on @xmath192 , in the theorems of section [ sec3 ] we should replace @xmath52 by @xmath194 and in this section @xmath11 should be supposed to belong to @xmath192 .",
    "proof of theorem [ whole ] we begin by noticing that @xmath195 and we control each of the probabilities on the right - hand side separately .    _ control of @xmath196_. by the definitions of the sets @xmath197 and @xmath16 , we have @xmath198 we control the last probability by using the characterization ( [ cond ] ) of @xmath107 given in lemma [ sol ] of the .",
    "we also recall that @xmath199 , since we assumed that the density of @xmath200 is the mixture @xmath201 .",
    "we therefore obtain , for @xmath202 , @xmath203 to bound ( [ zeroo ] ) , we use hoeffding s inequality , as in the course of lemma [ prelim ] .",
    "we first recall that @xmath204 for all @xmath205 and that , by condition [ condb ] , @xmath206 , with @xmath207 .",
    "therefore , @xmath208\\\\[-8pt ] & & \\qquad \\leq\\mathbb{p } \\biggl ( \\biggl| \\frac{1}{n}\\sum_{i=1}^{n}f_k(x_i ) - \\mathbb{e}f_k(x_1 ) \\biggr| \\geq",
    "2\\sqrt{2}rl \\biggr ) \\leq\\frac{\\delta}{m^2}. \\nonumber\\end{aligned}\\ ] ] to bound ( [ zero ] ) , notice that , by conditions [ conda ] and [ condb ] , @xmath209 where the penultimate inequality holds since , by definition , @xmath210 and the last inequality holds by corollary [ cormixt ] .    combining the above results , we obtain @xmath211    _ control of @xmath212 . _",
    "let @xmath213 let @xmath214 consider the random event @xmath215 let @xmath216 be the vector that has the components of @xmath217 given by ( [ ( 2.9 ) ] ) in positions corresponding to the index set @xmath16 and zero components elsewhere .",
    "by the first part of lemma [ sol ] in the , we have that @xmath218 is a solution of ( [ original ] ) on the event @xmath219 .",
    "recall that @xmath107 is also a solution of ( [ original ] ) . by the definition of the set @xmath197",
    ", we have that @xmath220 for @xmath221 . by construction , @xmath222 for some subset @xmath223 .",
    "by the second part of lemma [ sol ] in the , any two solutions have nonzero elements in the same positions . therefore , @xmath224 on @xmath219 .",
    "thus , @xmath225\\\\[-8pt ] & \\leq&\\sum_{k \\notin i^*}\\mathbb{p } \\biggl ( \\biggl|\\frac{1}{n}\\sum _ { i=1}^{n}f_{k}(x_i ) - ef_k(x_1 ) \\biggr|",
    "\\geq2\\sqrt{2}rl \\biggr ) \\nonumber\\\\ & & { } + \\sum_{k \\notin i^ * } \\mathbb{p } \\biggl(\\sum_{j \\in i^*}|\\tilde{\\mu}_j - \\lambda^*_j| |\\langle f_j , f_k \\rangle    reasoning as in ( [ unu ] ) above , we find @xmath226    to bound the last sum in ( [ eq : main ] ) , we first notice that theorem [ th : mutcoh ] [ if we replace there @xmath227 by the larger value @xmath228 ; cf .",
    "theorem [ thm:3 ] ] applies to @xmath217 given by  ( [ ( 2.9 ) ] ) . in particular , @xmath229 therefore , by condition [ conda ] , we have @xmath230 which holds since @xmath231 . collecting all the bounds above , we obtain @xmath232 which concludes the proof .",
    "consider an ensemble of @xmath12 gaussian densities @xmath154 s in @xmath3 with means @xmath233 and covariance matrices @xmath234 , where @xmath235 is the unit @xmath236 matrix . in",
    "what follows we show that condition [ conda ] holds if the means of the gaussian densities are well separated and we make this precise below .",
    "therefore , in this case , theorem [ whole ] guarantees that if the weights of the mixture are above the threshold given in condition [ condb ] , we can recover the true mixture components with high probability via our procedure .",
    "the densities are @xmath237 where denotes the euclidean norm .",
    "consequently , @xmath238 with @xmath239 .",
    "recall that condition [ conda ] requires @xmath240 let @xmath241 and @xmath242 . via simple algebra , we obtain @xmath243 therefore , condition [ conda ] holds if @xmath244 using this and theorem [ whole ] , we see that spades identifies the true components in a mixture of gaussian densities if the square euclidean distance between any two means is large enough as compared to the largest variance of the components in the mixture .",
    "note that condition [ condb ] on the size of the mixture weights involves the constant  @xmath166 , which in this example can be taken as @xmath245 where @xmath246 .",
    "often both the location and scale parameters are unknown .",
    "in this situation , as suggested by the associate editor , the spades procedure can be applied to a family of densities with both scale and location parameters chosen from an appropriate grid . by theorem [ th :",
    "mutcoh ] , the resulting estimate will be a good approximation of the unknown target density . an immediate modification of theorem [ whole ] , as in @xcite , further guarantees that spades identifies correctly the important components of this approximation .",
    "we assume in this section that the density @xmath2 is defined on a bounded interval of @xmath247 that we take without loss of generality to be the interval @xmath248 $ ] .",
    "consider a countable system of functions @xmath249 in @xmath157 , where the set of indices @xmath250 satisfies @xmath251 , @xmath252 , for some constant @xmath253 , and where the functions @xmath254 satisfy @xmath255 for all @xmath256 and for some @xmath257 .",
    "examples of such systems @xmath258 are given , for instance , by compactly supported wavelet bases ; see , for example , @xcite . in this case @xmath259 for some compactly supported function @xmath260 .",
    "we assume that @xmath258 is a frame , that is , there exist positive constants @xmath261 and @xmath262 depending only on @xmath258 such that , for any two sequences of coefficients @xmath263 , @xmath264 , @xmath265\\\\[-8pt ] & \\le & c_2 \\sum_{l=-1}^\\infty\\sum_{k\\in v(l)}(\\beta_{lk}-\\beta_{lk}')^2.\\nonumber\\end{aligned}\\ ] ] if @xmath258 is an orthonormal wavelet basis , this condition is satisfied with .    now , choose @xmath266 , where @xmath267 is such that @xmath268",
    ". then also @xmath269 .",
    "the coefficients @xmath270 are now indexed by @xmath271 , and we set by definition @xmath272 for @xmath273 .",
    "assume that there exist coefficients @xmath274 such that @xmath275 where the series converges in @xmath157 .",
    "then theorem [ thm:1a ] easily implies the following result .",
    "[ th : adnonp ] let @xmath24 be as defined above with @xmath269 , and let @xmath44 be given by ( [ c ] ) for @xmath276 .",
    "then for all @xmath277 , @xmath52 we have , with probability at least @xmath278 , @xmath279\\\\[-8pt ] & & \\hspace*{14.2pt}{}+\\sum_{(l , k)\\in j(\\lambda ) } \\biggl[\\frac1{n}\\sum_{i=1}^n \\psi_{lk}^2(x_i)\\frac{\\log n}{n } + 2^l \\biggl(\\frac{\\log n}{n } \\biggr)^2 \\biggr ] \\biggr),\\nonumber\\end{aligned}\\ ] ] where @xmath280 is a constant independent of @xmath2 .",
    "this is a general oracle inequality that allows one to show that the estimator @xmath281 attains minimax rates of convergence , up to a logarithmic factor simultaneously on various functional classes .",
    "we will explain this in detail for the case where @xmath2 belongs to a class of functions @xmath282 satisfying the following assumption for some @xmath283 :    [ condc ] for any @xmath284 and any @xmath285 there exists a sequence of coefficients @xmath286 such that @xmath287 for a constant @xmath288 independent of @xmath2 .",
    "it is well known that condition [ condc ] holds for various functional classes @xmath282 , such as hlder , sobolev , besov classes , if @xmath258 is an appropriately chosen wavelet basis ; see , for example , @xcite and the references cited therein . in this case",
    "@xmath289 is the smoothness parameter of the class .",
    "moreover , the basis @xmath258 can be chosen so that condition [ condc ] is satisfied with @xmath288 independent of @xmath289 for all @xmath290 , where @xmath291 is a given positive number .",
    "this allows for adaptation in @xmath289 .    under condition [ condc ]",
    ", we obtain from ( [ th : adnonp1 ] ) that , with probability at least @xmath278 , @xmath292\\\\[-8pt ] & & \\hspace*{157.9pt } { } + 2^l \\biggl(\\frac{\\log n}{n } \\biggr)^2 \\biggr ] \\biggr).\\nonumber\\end{aligned}\\ ] ] from ( [ th : adnonp2 ] ) and the last inequality in ( [ snorm ] ) we find for some constant @xmath293 , with probability at least @xmath278 , @xmath294\\\\[-8pt ] & = & o \\biggl ( \\biggl(\\frac{\\log n}{n } \\biggr)^{-2s/(2s+1 ) } \\biggr),\\nonumber\\end{aligned}\\ ] ] where the last expression is obtained by choosing @xmath295 such that @xmath296 .",
    "it follows from ( [ th : adnonp3 ] ) that @xmath281 converges with the optimal rate ( up to a logarithmic factor ) simultaneously on all the functional classes satisfying condition [ condc ] .",
    "note that the definition of the functional class is not used in the construction of the estimator @xmath281 , so this estimator is optimal adaptive in the rate of convergence ( up to a logarithmic factor ) on this scale of functional classes for @xmath290 .",
    "results of such type , and even more pointed ( without extra logarithmic factors in the rate and sometimes with exact asymptotic minimax constants ) , are known for various other adaptive density estimators ; see , for instance , @xcite and the references cited therein .",
    "these papers consider classes of densities that are uniformly bounded by a fixed constant ; see the recent discussion in  @xcite .",
    "this prohibits , for example , free scale transformations of densities within a class .",
    "inequality ( [ th : adnonp3 ] ) does not have this drawback .",
    "it allows to get the rates of convergence for classes of unbounded densities @xmath2 as well .",
    "another example is given by the classes of sparse densities defined as follows : @xmath297\\to\\mathbb r\\dvtx\\mbox{$f$ is a probability density and } |\\{j\\dvtx\\langle f , f_j\\rangle   \\ne0\\ } |\\le m \\bigr\\},\\ ] ] where @xmath298 is an unknown integer . if @xmath24 is a wavelet system as defined above and @xmath299 , then under the conditions of theorem [ th : adnonp ] for any @xmath300 we have , with probability at least @xmath278 , @xmath301 \\biggr ) .\\ ] ] from ( [ th : adnonp5 ] ) , using lemma [ oneside ] and the first two inequalities in ( [ snorm ] ) , we obtain the following result .    [ cor : twee ] let the assumptions of theorem [ th : adnonp ] hold .",
    "then , for every @xmath302 and @xmath277 , @xmath303\\\\[-8pt ] \\eqntext{\\forall m\\le m,}\\end{aligned}\\ ] ] where @xmath304 is a constant depending only on @xmath166",
    ".    corollary [ cor : twee ] can be viewed as an analogue for density estimation of the adaptive minimax results for @xmath305 classes obtained in the gaussian sequence model @xcite and in the random design regression model @xcite .",
    "in this section we describe the algorithm used for the minimization problem ( [ original ] ) and we assess the performance of our procedure via a simulation study .      since the criterion given in ( [ original ] )",
    "is convex , but not differentiable , we adopt an optimization by coordinate descent instead of a gradient - based approach ( gradient descent , conjugate gradient , etc . ) in the spirit of @xcite .",
    "coordinate descent is an iterative greedy optimization technique that starts at an initial location @xmath52 and at each step chooses one coordinate @xmath306 of @xmath38 at random or in order and finds the optimum in that direction , keeping the other variables @xmath307 fixed at their current values . for convex functions , it usually converges to the global optimum ; see @xcite .",
    "the method is based on the obvious observation that for functions of the type @xmath308 where @xmath309 is a generic convex and differentiable function , @xmath310 is a given parameter , and @xmath311 denotes the @xmath0 norm , the optimum in a direction @xmath306 is to the left , right or at @xmath113 , depending on the signs of the left and right partial derivatives of @xmath312 at zero .",
    "specifically , let @xmath313 denote the partial derivative of @xmath309 with respect to @xmath270 , and denote by @xmath314 the vector @xmath38 with the @xmath50th coordinate set to 0 .",
    "then , the minimum in direction @xmath50 of @xmath315 is at @xmath314 if and only if @xmath316 .",
    "this observation makes the coordinate descent become the iterative thresholding algorithm described below .",
    "given @xmath317 , initialize all @xmath270 , @xmath318 , for example , with @xmath319 .    1 .",
    "choose a direction @xmath320 and set @xmath321 .",
    "2 .   if @xmath322 , then set @xmath323 , otherwise obtain @xmath38 by line minimization in direction @xmath50 .",
    "3 .   if @xmath324 , go to 1 , where @xmath325 is a given precision level .    for line minimization",
    ", we used the procedure ` linmin ` from numerical recipes @xcite , page 508 .",
    "we apply the coordinate descent algorithm described above to optimize the function @xmath315 given by ( [ original ] ) , where the tuning parameters @xmath44 are all set to be equal to the same quantity @xmath317 .",
    "the theoretical choice of this quantity described in detail in the previous sections may be too conservative in practice . in this section",
    "we propose a data driven method for choosing the tuning parameter @xmath317 , following the procedure first introduced in @xcite , which we briefly describe here for completeness .",
    "the procedure chooses adaptively the tuning parameter from a list of candidate values , and it has two distinctive features : the list of candidates is not given by a fine grid of values and the adaptive choice is not given by cross - validation , but by a dimension stabilized cross - validated criterion .",
    "we begin by describing the principle underlying our construction of the set of candidate values which , by avoiding a grid search , provides significant computational savings .",
    "we use a generalization of the bisection method to find , for each @xmath326 , a preliminary tuning parameter @xmath327 that gives a solution @xmath328 with exactly @xmath205 nonzero elements .",
    "formally , denote by @xmath329 the number of nonzero elements in the @xmath38 obtained by minimizing ( [ original ] ) with @xmath330 for a given value of the tuning parameter @xmath317 .",
    "the generalized bisection method will find a sequence of values of the tuning parameter , @xmath331 , such that @xmath332 , for each @xmath333 .",
    "it proceeds as follows , using a queue consisting of pairs @xmath334 such that @xmath335 .",
    "initialize all @xmath336 with @xmath337",
    ".   choose @xmath338 very large , such that @xmath339 .",
    "choose @xmath340 , hence , @xmath341 .",
    "2 .   initialize a queue @xmath342 with the pair @xmath343 .",
    "3 .   pop the first pair @xmath344 from the queue .",
    "4 .   take @xmath345 .",
    "compute @xmath346 .",
    "if @xmath347 , make @xmath348 . 6 .   if @xmath349 and @xmath350 , add @xmath351 to the back of the queue . 7 .",
    "if @xmath352 and @xmath353 , add @xmath354 to the back of the queue .",
    "if the queue is not empty , go to 3 .",
    "this algorithm generalizes the basic bisection method ( bbm ) , which is a well - established computationally efficient method for finding a root @xmath355 of a function @xmath356 ; see , for example , @xcite .",
    "we experimentally observed ( see also @xcite for a detailed discussion ) that using the gbm is about 50 times faster than a grid search with the same accuracy .",
    "our procedure finds the final tuning parameter @xmath317 by combining the gbm with the dimension stabilized @xmath357-fold cross - validation procedure summarized below .",
    "let @xmath358 denote the whole data set , and let @xmath359 be a partition of @xmath358 in @xmath357 disjoint subsets .",
    "let @xmath360 .",
    "we will denote by @xmath361 a candidate tuning parameter determined using the gbm on @xmath362 .",
    "we denote by @xmath363 the set of indices corresponding to the nonzero coefficients of the estimator of @xmath38 given by ( [ original ] ) , for tuning parameter @xmath361 on @xmath362 .",
    "we denote by @xmath364 the minimizers on @xmath362 of the unpenalized criterion @xmath365 , with respect only to those @xmath366 with @xmath367 .",
    "let @xmath368 , computed on @xmath369 . with this notation",
    ", the procedure becomes the following :      given : a data set @xmath358 partitioned into @xmath357 disjoint subsets , @xmath370 .",
    "let @xmath360 for all @xmath50 .    1 .",
    "for each @xmath371 and each fold @xmath50 of the partition , @xmath372 : + use the gbm to find @xmath361 and @xmath363 such that @xmath373 on @xmath362 .",
    "+ compute @xmath374 , as defined above , on @xmath369 .",
    "2 .   for each @xmath375 : + compute @xmath376 .",
    "3 .   obtain @xmath377 4 .",
    "with @xmath378 from step 3 , use the bbm on the whole data set @xmath358 to find the tuning sequence @xmath379 and then compute the final estimators using the coordinate descent algorithm and tuning paramemter @xmath380 .",
    "in all the the numerical experiments described below we took the number of splits @xmath381 .",
    "we recall that the theoretical results of section [ sec41 ] show that for correct identification of the mixture components one needs to work with a value of the tuning sequence that is slightly larger than the one needed for good approximations with mixtures of a given density . a good practical approximation of the latter tuning value is routinely obtained by cross - validation ; this approximation is , however , not appropriate if the goal is correct selection , when the theoretical results indicate that a different value is needed .",
    "our modification of the cross - validated loss function via a bic - type penalty is motivated by the known properties of the bic - type criteria to yield consistent model selection in a large array of models ; see , for example , @xcite for results on regression models .",
    "the numerical experiments presented below show that this is also the case for our criterion in the context of selecting mixture components .",
    "the theoretical investigation of this method is beyond the scope of this paper and will be undertaken in future research .      in this subsection",
    "we illustrate the performance of our procedure via a simulation study .",
    "we begin by investigating the ability of spades , with its tuning parameter chosen as above , to ( i ) approximate well , with respect to the @xmath157 norm , a true mixture ; ( ii ) to identify the true mixture components .",
    "we conducted a simulation study where the true density is a mixture of gaussian densities with @xmath382 and , respectively , @xmath383 true mixture components .",
    "the mixture components are chosen at random from a larger pool of @xmath12 gaussians @xmath384 , @xmath385 , where for @xmath382 we take @xmath386 , and for @xmath387 we take @xmath388 .",
    "these choices for @xmath389 ensure that the identifiability condition ( [ condmu ] ) is satisfied .",
    "the true components correspond to the first @xmath175 gaussian densities from our list , and their weights in the true mixture are all equal to @xmath390 .",
    "the maximum size @xmath12 of the candidate list we considered is @xmath391 , for @xmath392 and @xmath393 , for @xmath383 .",
    "all the results obtained below are relative to @xmath394 simulations . each time",
    ", a sample of size @xmath13 is obtained from the true mixture and is the input of the procedure described in section [ sec : gbm ] .",
    "error @xmath395 for @xmath396 , respectively , @xmath397 .",
    "the error bars are the 25 and 75 percentiles . ]",
    "we begin by evaluating the accuracy with respect to the @xmath157 norm of the estimates of @xmath398 .",
    "we investigate the sensitivity of our estimates relative to an increase in the dictionary size and @xmath175 . in figure",
    "[ fig : err_m ] , we plot the median over 100 simulations of @xmath399 versus the size @xmath12 of the dictionary , when the true mixture cardinality is @xmath392 ( left panel ) and @xmath383 ( right panel ) .",
    "for @xmath392 we considered three instances of sample sizes @xmath400 and we varied @xmath12 up to 200 .",
    "for @xmath383 we considered three larger instances of sample sizes @xmath401 and we varied @xmath12 up to 600 .",
    "these experiments provide strong support for our theoretical results : the increase in @xmath12 does not significantly affect the quality of estimation , and an increase in @xmath175 does . for larger values of @xmath175",
    "we need larger sample sizes to obtain good estimation accuracy .",
    "obtained from 100 runs , for @xmath396 , respectively , @xmath397 . ]",
    "we next investigated the ability of the spades to find the exact mixture components .",
    "figure [ fig : eye_m ] shows a plot of the percentage of times the exact mixture components were found versus @xmath12 .",
    "we considered the same combinations @xmath402 as in figure [ fig : err_m ] .",
    "again , observe that the performance does not seriously degrade with the dictionary size @xmath12 , and is almost unaffected by its increase once a threshold sample size is being used .",
    "however , notice that on the difference from the results presented in figure [ fig : err_m ] , correct identification is poor below the threshold sample size , which is larger for larger @xmath175 .",
    "this is in accordance with our theoretical results : recall condition [ condb ] of section [ sec41 ] on the minimum size of the mixture weights .",
    "indeed , we designed our simulations so that the weights are relatively small for @xmath383 , they are all equal to @xmath403 , and a larger sample size is needed for their correct identification .     of the @xmath157 error @xmath404 and the percentage of times @xmath405 . in this example , @xmath406 . ]    finally , we evaluated in figure [ fig : err_d ] the dependence of the error and hit rate ( i.e. , the percentage of times @xmath405 ) on the smallest distance @xmath407 and [ fig : eye_m ] above were obtained for the value @xmath408 , which satisfies the theoretical requirement for correct mixture identification . on the other hand , @xmath409  can be smaller for good @xmath157 mixture approximation .",
    "it is interesting to see what happens when @xmath409 decreases , so that the mixture elements become very close to one another . in figure",
    "[ fig : err_d ] we present the simulations for @xmath392 , @xmath410 and @xmath411 , which is sufficient to illustrate this point .",
    "we see that , although the @xmath157 error increases slightly when @xmath412 decreases , the deterioration is not crucial .",
    "however , as our theoretical results suggest , the percentage of times we can correctly identify the mixture decreases to zero when the dictionary functions are very close to each other .      in a second set of experiments our aim was to approximate a two - dimensional probability density on a thick circle ( cf .",
    "the left panel of figure [ fig : plot_rec ] ) with a mixture of isotropic gaussians .",
    "a sample of size 2000 from the circle density is shown in the middle panel of figure [ fig : plot_rec ] .",
    "we use a set of isotropic gaussian candidates with covariance @xmath413 centered at some of the 2000 locations , such that the euclidean distance between the means of any two such gaussians is at least 1 .",
    "we select from these candidate mixture densities in a greedy iterative manner , each time choosing one of the 2000 locations that is at distance at least 1 from each of those already chosen . as a result",
    ", we obtain a dictionary of @xmath414 candidate densities .",
    "the circle density can not be exactly represented as a finite mixture of gaussian components .",
    "this is a standard instance of many practical applications in computer vision , as the statistics of natural images are highly kurtotic and can not be exactly approximated by isotropic gaussians .",
    "however , in many practical applications a good approximation of an object that reflects its general shape is sufficient and constitutes a first crucial step in any analysis .",
    "we show below that spades offers such an approximation .",
    "depending on the application , different trade - offs between the number of mixture components ( which relates to the computational demand of the mixture model ) and accuracy might be appropriate .",
    "for example , in real - time applications a small number of mixture elements would be required to fit into the computational constraints of the system , as long as there is no significant loss in accuracy .",
    "for the example presented below we used the gbm to determine the mixture weights @xmath415 , for mixtures with @xmath416 components .",
    "let @xmath417 , where we recall that the loss function @xmath418 is given by ( [ eq : loss ] ) above .",
    "we used the quantity @xmath419 to measure the accuracy of the mixture approximation . in figure",
    "[ fig : loss ]     as a function of the mixture components @xmath205 . ]",
    "we plotted @xmath420 as a function of @xmath205 and used this plot to determine the desired trade - off between accuracy and mixture complexity .",
    "based on this plot , we selected the number of mixture components to be 80 ; indeed , including more components does not yield any significant improvement .",
    "the obtained mixture is displayed in the right panel of figure [ fig : plot_rec ] .",
    "we see that it successfully approximates the circle density with a relatively small number of components .",
    "[ sol ] let @xmath217 be given by ( [ ( 2.9 ) ] ) .",
    "then @xmath421 is a minimizer in @xmath52 of @xmath422 on the random event @xmath219 defined in ( [ b ] ) .",
    "since @xmath309 is convex , by standard results in convex analysis , @xmath424 is a minimizer of @xmath309 if and only if @xmath425 where @xmath426 is the subdifferential of @xmath423 : @xmath427 where @xmath428 , & \\quad if $ \\lambda_k = 0$.}\\ ] ] therefore , @xmath429 minimizes @xmath430 if and only if , for all @xmath371 , @xmath431 we now show that @xmath432 with @xmath217 given in ( [ ( 2.9 ) ] ) satisfies ( [ cond ] ) and ( [ condd ] ) on the event @xmath433 and therefore is a minimizer of @xmath423 on this event . indeed , since @xmath217 is a minimizer of the convex function @xmath434 given in ( [ reduce ] ) , the same convex analysis argument as above implies that @xmath435 note that on the event @xmath433 we also have @xmath436 here @xmath437 denotes the @xmath205th coordinate of @xmath438 . the above three displays and the fact that @xmath439 , show that @xmath438 satisfies conditions ( [ cond ] ) and ( [ condd ] ) and is therefore a minimizer of @xmath423 on the event @xmath219 .",
    "we now prove the second assertion of the lemma . in view of ( [ cond ] ) ,",
    "the index set @xmath440 of the nonzero components of any minimizer @xmath429 of @xmath423 satisfies @xmath441 therefore , if for any two minimizers @xmath442 and @xmath443 of @xmath423 we have @xmath444 then @xmath440 is the same for all minimizers of @xmath423 .",
    "thus , it remains to show ( [ same1 ] ) .",
    "we use simple properties of convex functions .",
    "first , we recall that the set of minima of a convex function is convex .",
    "then , if @xmath442 and @xmath443 are two distinct points of minima , so is @xmath445 , for any @xmath446 .",
    "rewrite this convex combination as @xmath447 , where @xmath448 .",
    "recall that the minimum value of any convex function is unique .",
    "therefore , for any @xmath449 , the value of @xmath423 at @xmath450 is equal to some constant @xmath253 : @xmath451 by taking the derivative with respect to @xmath452 of @xmath453 , we obtain that , for all @xmath449 , @xmath454 by continuity of @xmath455 , there exists an open interval in @xmath456 on which @xmath457 is constant for all @xmath50 .",
    "therefore , on that interval , @xmath458 where @xmath459 does not depend on @xmath452 .",
    "this is compatible with @xmath460 , only if @xmath461 and , therefore , @xmath462 which is the desired result .",
    "this completes the proof of the lemma .",
    "birg , l. and massart , p. ( 1997 ) . from model selection to adaptive estimation . in _",
    "festschrift for lucien lecam : research papers in probability and statistics _ ( d. pollard , e. torgersen and g. yang , eds . ) 5587 .",
    "springer , new york .",
    "bunea , f. ( 2008 ) .",
    "consistent selection via the lasso for high dimensional approximating regression models . in",
    "_ pushing the limits of contemporary statistics : contributions in honor of jayanta k. ghosh _ ( b. clarke and s. ghosal , eds . ) * 3 * 122137 .",
    "ims , beachwood , oh .        bunea , f. , tsybakov , a. b. and wegkamp , m. h. ( 2006 ) .",
    "aggregation and sparsity via @xmath0-penalized least squares . in _ proceedings of 19th annual conference on learning theory , colt 2006_.",
    "_ lecture notes in artificial intelligence _",
    "* 4005 * 379391 .",
    "springer , heidelberg .",
    "friedman , j. , hastie , t. and tibshirani , r. ( 2010 ) .",
    "regularization paths for generalized linear models via coordinate descent .",
    "_ journal of statistical software _ * 33 * 1 .",
    "golubev , g. k. ( 1992 ) .",
    "nonparametric estimation of smooth probability densties in @xmath157",
    ". _ probl .",
    "_ * 28 * 4454 .",
    "samarov , a. and tsybakov , a. ( 2007 ) .",
    "aggregation of density estimators and dimension reduction . in _ advances in statistical modeling and inference .",
    "essays in honor of kjell a. doksum _ ( v. nair , ed . ) 233251 .",
    "world scientific , singapore .",
    "tsybakov , a. b. ( 2003 ) . optimal rates of aggregation . in _ proceedings of 16th annual conference on learning theory ( colt ) and 7th annual workshop on kernel machines_. _ lecture notes in artificial intelligence _ * 2777*. springer , heidelberg ."
  ],
  "abstract_text": [
    "<S> this paper studies sparse density estimation via @xmath0 penalization ( spades ) . </S>",
    "<S> we focus on estimation in high - dimensional mixture models and nonparametric adaptive density estimation . </S>",
    "<S> we show , respectively , that spades can recover , with high probability , the unknown components of a mixture of probability densities and that it yields minimax adaptive density estimates . </S>",
    "<S> these results are based on a general sparsity oracle inequality that the spades estimates satisfy . </S>",
    "<S> we offer a data driven method for the choice of the tuning parameter used in the construction of spades . </S>",
    "<S> the method uses the generalized bisection method first introduced in @xcite . </S>",
    "<S> the suggested procedure bypasses the need for a grid search and offers substantial computational savings . </S>",
    "<S> we complement our theoretical results with a simulation study that employs this method for approximations of one and two - dimensional densities with mixtures . </S>",
    "<S> the numerical results strongly support our theoretical findings .    ,    , +    and    .    . </S>"
  ]
}