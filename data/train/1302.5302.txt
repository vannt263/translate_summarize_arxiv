{
  "article_text": [
    "the rise of social media and other forms of user - generated content challenges the traditional notion of search as operating on either static documents collections or document collections that evolve slowly enough where periodically running a batch indexer ( e.g. , every hour ) suffices .",
    "we focus on real - time search in the context of twitter :  users demand to know what s happening _ right now _ , especially in response to breaking news stories and other shared events such as hurricanes in the northeastern united states , the death of prominent figures , or televised political debates . for this",
    ", they often turn to real - time search .",
    "the context of this study is twitter s earlybird retrieval engine  @xcite , which serves over two billion queries a day with an average query latency of 50 ms .",
    "usually , tweets are searchable within 10 seconds after creation ( most of the latency is from the processing pipeline  tweet indexing itself takes less than a millisecond ) .",
    "the service as a whole is of course a complex , distributed system with many components . in this paper , we focus on one aspect  dynamic memory allocation policies for postings allocation .",
    "a key feature of earlybird is that it incrementally indexes tweets as they are posted and makes them immediately searchable .",
    "the indexing process naturally requires allocating space for postings in a dynamic manner  we adopt a zero - copy approach that yields non - contiguous postings lists .",
    "the fundamental challenge boils down to a `` goldilocks problem '' , since memory today remains a scarce resource .",
    "a policy that is too aggressive in allocating memory for postings leads to inefficient utilization , because much of the allocated space will be empty . on the other hand ,",
    "a policy that is too conservative slows the system , since memory allocation is a relatively costly operation and postings lists will become fragmented .",
    "ideally , we d like to strike a balance between the two extreme and find a `` sweet spot '' that balances speed with utilization .",
    "we present a dynamic postings allocation policy that allocates increasingly - larger `` slices '' from a small number of memory pools . the production system , which we previously described in busch et al .",
    "@xcite , deploys a particular instantiation of a general framework , which we articulate for the first time here . until now",
    ", we have not thoroughly explored alternative parameter settings in a rigorous and controlled manner .",
    "thus , the contribution of this paper is a detailed study of the design space for dynamic postings allocation in the context of our basic framework :  we present both an analytical model for estimating time and space costs , which is subsequently validated by experiments on real data .",
    "to set the stage , we begin by discussing differences and similarities between real - time search and `` traditional '' ( e.g. , web ) search . first , two similarities :    = 1em    _ low - latency , high - throughput query evaluation .",
    "_ users are impatient and demand results quickly .    _ in - memory indexes .",
    "_ the only practical way to achieve necessary performance requirements is to maintain all index structures in memory .",
    "there are important differences as well :    = 1em    _ immediate data availability . _ in real - time search , documents arrive rapidly , and users expect content to be searchable within seconds .",
    "this means that the indexer must achieve both low latency and high throughput .",
    "this requirement departs from common assumptions that indexing can be considered a batch operation .",
    "although web crawlers achieve high throughput , it is generally not expected that crawled content be indexed immediately  an indexing delay of minutes to hours may be acceptable .",
    "this allows efficient indexing with batch processing frameworks such as mapreduce  @xcite .",
    "in contrast , real - time search demands that documents be searchable in _",
    "seconds_.    _ shared mutable state .",
    "_ a real - time search engine must handle shared mutable state in a multi - threaded execution environment with concurrent indexing and retrieval operations . in contrast , concurrency - related challenges are simpler to handle in web search :  for example , it is possible to atomically `` swap out '' an old index with an updated new index without service disruption . such a design would be impractical in real - time search .",
    "_ dominance of the temporal signal . _",
    "the nature of real - time search means that temporal signals are important for relevance ranking .",
    "this contrasts with web search , where document timestamps have a relatively minor role in determining relevance ( news search being the obvious exception ) .",
    "this holds implications for how postings should be organized in index structures .",
    "twitter s production real - time search service is a complex distributed system spanning many machines , the details of which are beyond the scope of this paper . in this study , we specifically focus on earlybird , which is the core retrieval engine . for the purposes of this paper , earlybird receives boolean queries and returns tweets that satisfy the query , sorted in",
    "reverse chronological order .",
    "no relevance scoring is performed , which is , functionally speaking , handled by another component .",
    "incoming tweets are hash partitioned across a number of replicated earlybird instances , so that each individual instance serves a fraction of all tweets .",
    "to understand our contributions , it is necessary to first provide some technical background .",
    "here , we summarize material presented in a previous paper  @xcite , but refer the reader to the original source for details .      earlybird is built on top of the open - source lucene search engine and adapted to meet the demands of real - time search discussed in section  [ section : bg ] .",
    "the system is written completely in java , primarily for three reasons :  to take advantage of the existing lucene java codebase , to fit into twitter s jvm - centric development environment , and to take advantage of the easy - to - understand memory model for concurrency offered by java and the jvm .",
    "although this decision poses inherent challenges in terms of performance , with careful engineering and memory management we believe it is possible to build systems that are comparable in performance to those written in c / c++ .    as with nearly all modern retrieval engines , earlybird maintains an inverted index :  postings are maintained in forward chronological order ( most recent last ) but are traversed _ backwards _",
    "( most recent first ) ; this is accomplished by maintaining a pointer to the current end of each postings list ( more details in the next section ) .",
    "earlybird supports a full boolean query language consisting of conjunctions ( ands ) , disjunctions ( ors ) , negations ( nots ) , and phrase queries .",
    "results are returned in reverse chronological order , i.e. , most recent first .",
    "boolean query evaluation is relatively straightforward , and in fact we use lucene query operators `` out of the box '' , e.g. , conjunctive queries correspond to postings intersections , disjunctive queries correspond to unions , and phrase queries correspond to intersections with positional constraints .",
    "lucene provides an abstraction for postings lists and traversing postings  we provide an implementation for our custom indexes , and are able to reuse existing lucene query evaluation code .",
    "a particularly noteworthy aspect of earlybird is the manner in which it handles shared mutable state ( concurrent index reads and writes ) using lightweight memory barriers .",
    "as this is not germane to the subject of this paper , we refer the reader elsewhere  @xcite for details . however , it is worth mentioning that the general strategy for handling concurrency is to limit the scope of data structures that hold shared mutable state .",
    "this is accomplished as follows :  each instance of earlybird manages multiple index segments ( currently 12 ) , and each segment holds a relatively small number of tweets ( currently , @xmath0 million tweets ) . ingested tweets first fill up a segment before proceeding to the next one .",
    "therefore , at any given time , there is at most one index segment actively being modified , whereas the remaining segments are read - only . once an index segment ceases to accept new tweets",
    ", we can convert it from a write - friendly structure into an optimized and compressed read - only structure .    due to this design ,",
    "our paper is only concerned with the active index segment within an earlybird instance :  only for that index do we need to allocate memory for postings dynamically .",
    "this is described in more detail next .      as we argued in section  [ section : bg ] , the dominance of the temporal signal is a major distinguishing characteristic of real - time search , compared to traditional ( web ) search .",
    "the implication of this is that it would be desirable to traverse postings in reverse temporal order for query evaluation .",
    "although this is not an absolute requirement , such a traversal order is the most convenient .    following this reasoning further",
    ", it appears that existing approaches to index structure organization are not appropriate .",
    "the information retrieval literature discusses two types of indexes :  document sorted and frequency / impact sorted .",
    "the latter seems unsuited for real - time search .",
    "what about document - sorted indexes ? if we assign document ids to new tweets in ascending order , there are two obvious possibilities when indexing new documents :    first , we could append new postings to the ends of postings lists .",
    "however , this would require us to read postings _ backwards _ to achieve a reverse chronological traversal order .",
    "unfortunately , this is not directly compatible with modern index compression techniques .",
    "typically , document ids are converted into document gaps , or differences between consecutive document ids .",
    "these gaps are then compressed with integer coding techniques such as @xmath1 codes , rice codes , or pfordelta  @xcite .",
    "it would be tricky for gap - based compression to support backwards traversal .",
    "prefix - free codes ( @xmath1 and rice codes ) are meant to be decoded only in the forward direction .",
    "more recent techniques such as pfordelta are block - based , in that they code relatively large blocks of integers ( e.g. , 128 document ids ) at a time . reconciling this with the desire to have low - latency indexing would require additional complexity , although none of these issues are technically insurmountable .    alternatively , we could prepend new postings to the beginnings of postings lists .",
    "this would allow us to read postings in the forward direction and preserve a reverse chronological traversal order .",
    "however , this presents memory management challenges , i.e. , how would space for new postings be allocated ?",
    "we are unaware of any work that has explored this strategy .",
    "note that the nave implementation using linked lists would be hopelessly inefficient :  linked list traversal is slow due to the lack of reference locality and predictable memory access patterns .",
    "furthermore , linked lists have rather large memory footprints due to object overhead and the need to store `` next '' pointers .",
    "based on the above analysis , it does not appear that real - time search capabilities can be efficiently realized with obvious extensions or adaptations of existing techniques .",
    "earlybird implements the following solution :  each posting is simply a 32-bit integer24 bits are devoted to storing the document i d and 8 bits for the term position . since tweets are limited to 140 characters , 8 bits are sufficient to hold term positions .",
    "therefore , a list of postings is simply an integer array , and indexing new documents involves inserting elements into a pre - allocated array .",
    "postings traversal in reverse chronological order corresponds to iterating through the array backwards .",
    "this organization also allows every array position to be a possible entry point for postings traversal to evaluate queries . in addition",
    ", it allows for binary search ( to find a particular document i d ) , and does nt require any additional skip - pointers  @xcite to enable faster traversal through the postings lists .",
    "finally , this organization is cache friendly , since array traversal involves linear memory scans and this predictable access pattern provides prefetch cues to the hardware .",
    "in essence , the design punts on the problem of postings compression  but we feel that this is a reasonable design choice given its simplicity and the above advantages . furthermore , since the active index segment holds relatively few tweets , a particular segment does nt spend much time in the uncompressed state . once an index segment stops accepting new tweets , it is converted into an optimized read - only structure :  we apply a variant of pfordelta after reversing the order of the postings .",
    "having provided adequate background , we finally arrive at the heart of this paper :  the allocation of space for postings lists .",
    "obviously , this process needs to be dynamic , since postings list growth is only bounded by the size of the collection itself .",
    "there are a few challenges to overcome :  postings lists vary significantly in size , since term and document frequencies are zipfian ( roughly ) . as a result",
    ", it is tricky to choose the correct amount of memory to allocate for each term s postings ( i.e. , size of the integer array ) . selecting a value that is too large leads to inefficient memory utilization , because most of the allocated space for storing postings will be empty . on the other hand , selecting a value that is too small leads to waste :  time , obviously , for memory allocation ( which is a relatively costly operation ) , but also space because non - contiguous postings require pointers to chain together ( in the limit , allocating one posting at a time is akin to a linked list ) .",
    "furthermore , during postings traversal , blocks that are too small result in suboptimal memory access patterns ( e.g. , due to cache misses , lack of memory prefetching , etc . ) .",
    "this is exactly the `` goldilocks problem ''",
    "we described in the introduction .",
    "[ figure : postings - slices ]    our approach to address these issues is to create four separate `` pools '' for holding postings .",
    "conceptually , each pool can be treated as an unbounded integer array . in practice ,",
    "pools are large integer arrays allocated in @xmath2 element blocks ; that is , if a pool fills up , another block is allocated , growing the pool . in each pool , we allocate `` slices '' , which hold individual postings belonging to a term . in each pool ,",
    "the slice sizes are fixed :  they are @xmath3 , @xmath4 , @xmath5 , and @xmath6 , respectively ( see figure  [ figure : postings - slices ] ) . for convenience , we will refer to these as pools 1 through 4 , respectively .",
    "when a term is first encountered , a @xmath3 integer slice is allocated in the first pool , which is sufficient to hold postings for the first two term occurrences .",
    "when the first slice runs out of space , another slice of @xmath4 integers is allocated in pool 2 to hold the next @xmath7 term occurrences ( 32 bits are used to serve as the `` previous '' pointer , discussed below ) . after running out of space",
    ", slices are allocated in pool 3 to store the next @xmath8 term occurrences and finally @xmath9 term occurrences in pool 4 .",
    "additional space is allocated in pool 4 in @xmath6 integer blocks as needed .",
    "one advantage of this strategy is that no array copies are required as postings lists grow in length  which means that there is no garbage to collect .",
    "however , the tradeoff is that postings are non - contiguous and we need a mechanism to link the slices together . addressing slice positions is accomplished using 32-bit pointers :  2 bits are used to address the pool , 1929 bits are used to address the slice index , and 111 bits are used to address the offset within the slice .",
    "this creates a symmetry in that postings and addressing pointers both fit in a standard 32-bit integer .",
    "the dictionary maintains pointers to the current `` tail '' of the postings list using this addressing scheme ( thereby marking where the next posting should be inserted and where query evaluation should begin ) .",
    "pointers in the same format are used to `` link '' the slices in different pools together and , possibly , multiple slices in pool 4 . in all but the first pool ,",
    "the first 32 bits of each slice are used to store this `` previous '' pointer .    to conclude this section , we provide some performance figures , summarized from  @xcite .",
    "the basic configuration of an earlybird server is a commodity machine with two quad - core processors and 72 gb memory .",
    "a fully - loaded active index segment with 16 million documents occupies about 6.7 gb memory .",
    "on such a segment , we achieve 17000 queries per second with a 95th percentile latency of @xmath10100 ms and 99th percentile latency of @xmath10200 ms using 8 searcher threads . in a stress test , we evaluated earlybird indexing performance under near 100% cpu utilization .",
    "we achieve 7000 tweets per second ( tps ) indexing rate at 95th percentile latency of 150 ms and 99th percentile latency of 180 ms .",
    "indexing latency is relatively insensitive to tweet arrival rate ; at 1000 tps we observe roughly the same latencies as at 7000 tps .",
    "it is evident that earlybird represents a specific instantiation of a general solution to the problem of dynamically allocating postings for real - time search :  from a small number of large memory pools , we allocate increasingly larger slices for postings as more term occurrences are encountered . within this general framework",
    ", a particular instantiation can be described by @xmath11 , the slice size settings ( as powers of two ) , where @xmath12 is the number of pools .",
    "for example , in the production deployment , @xmath13 . for best utilization of bits in addressing pointers , it is helpful to restrict @xmath14 to a power of two also .",
    "note that this framework provides a general solution to real - time indexing ( not only tweets ) :  we simply assume that slices hold spaces for postings and pointers to previous slices . in the case of tweets , both postings and pointers are 32-bit integers , but nothing in our model precludes other encodings .",
    "thus , for the remainder of this paper , we measure postings in terms of `` memory slots '' . for simplicity , we assume that pointers also fit in a memory slot , but if this is nt the case , a small constant factor adjustment will suffice .    how `` optimal '' is the current production deployment , compared to alternative configurations ? prior to this study , we have not attempted to answer this question in a rigorous , controlled fashion . in this paper , we tackle this question as follows :  first , we define a cost model in terms of speed and memory usage , the two characteristics we seek to balance .",
    "second , we develop an analytical model that allows us to assess the time and space costs of a particular configuration . finally , for promising configurations identified by our analytical model , we follow up with experiments .",
    "since our analytical model makes use of real data to estimate parameters , we begin by describing our datasets . for tweets , we used the tweets2011 corpus created for the trec 2011 microblog track .",
    "the corpus is comprised of approximately 16 million tweets over a period of two weeks ( 24th january 2011 until 8th february , inclusive ) which covers both the time period of the egyptian revolution and the us superbowl .",
    "different types of tweets are present , including replies and retweets .",
    "the corpus represents a sample of the entire tweet stream , but since tweets are hash partitioned across multiple earlybird instances in production , experiments on these tweets is a reasonably accurate facsimile of studying an individual earlybird instance .",
    "even though we have access to all tweets , we purposely conducted experiments on this publicly available collection so that others will be able to replicate our results .",
    "three different sets of queries were used in our evaluation .",
    "first , we took the trec 2005 terabyte track `` efficiency '' queries ( 50,000 queries total ) .",
    "second , we sampled 100,000 queries randomly from the aol query log  @xcite , which contains around 10 million queries in total .",
    "our sample preserves the original query length distribution . finally , we used queries from the trec 2011 microblog track .",
    "however , since there were only 50 queries ( which is insufficient for efficiency experiments ) , we augmented the queries by first generating the power set of all query terms and then used the `` related queries '' api of a commercial search engine to harvest query variants . in this way",
    ", we were able to construct a set of approximately 3100 queries .",
    "our choice of these three datasets represented an attempt to balance several factors .",
    "although we have access to actual twitter query logs , experiments on them would have several drawbacks :  first , due to their proprietary nature , our results would not be replicable .",
    "second , the majority of twitter queries are trending hashtags ( or queries containing trending hashtags ) , which are not particularly interesting from an efficiency point of view ( similar to head navigational queries in web search ) . furthermore",
    ", we d like to study the types of information needs that real - time search _ could _ solve , not exactly what the service is doing right now .",
    "thus , triangulating based on three query sets paints a more complete picture :  the aol queries represent general web queries ; the trec efficiency queries are representative of _ ad hoc _ queries , closer to the `` torso '' of the query distribution ( mostly informational , as opposed to navigational ) ; finally , the trec microblog queries represent a forward - looking conception of what real - time search might evolve into ( at least according to retired intelligence analysts at nist ) .",
    "finally , all three of our datasets are available to researchers ( we intend to release our expanded microblog queries ) .",
    "given a collection of documents @xmath15 and a set of queries @xmath16 , we define a cost function for memory usage . the total memory `` wasted '' is equal to the memory allocated for postings minus the size of the postings list ( i.e. , number of postings ) , summed across all terms @xmath17 in the collection : @xmath18}\\ ] ] since the size of postings is constant for a given collection",
    ", we can simply define the memory cost as follows ( which we d like to minimize ) : @xmath19    similarly , let us define the time cost as the time it would take to read all postings ( end to end ) for all query terms in each query of @xmath16 .",
    "@xmath20 note that this cost function does not actually take into account time spent in query evaluation ( e.g. , intersection of postings lists for conjunctive query processing ) .",
    "we decided to factor out those costs for two reasons :  first , to support a simpler model ( since a large number of postings traversal techniques are available , each with different optimizations and tradeoffs ) .",
    "second , even if we wished to , it is unclear how we could analytically model postings intersection time , which is a function of term occurrences in real - world data .",
    "the advantage of our model is that instantiating it with parameters is fairly easy .",
    "if we assume that term frequencies in a collection follow a zipfian distribution ( a standard assumption in information retrieval ) , we can analytically estimate the memory cost for various @xmath21 configurations .",
    "similarly , if the postings length distribution of query terms is known , we can analytically model the time cost as well . with models of the two costs",
    ", we can find configurations that strike a desired memory / speed balance .",
    "the remainder of this section explains how we accomplish this .",
    "given that the frequency of a term @xmath17 in a collection is @xmath22 , and the pool settings is @xmath11 , we can calculate the exact number of memory slots required to hold the postings list of term @xmath17 .",
    "let us define a step function @xmath23 that maps a frequency to the number of memory slots required by configuration @xmath21 .",
    "first , we recursively define a set of thresholds @xmath24 s on the frequencies as follows : s is a finite set . ]",
    "@xmath25 for each term frequency interval @xmath26 the value of the step function @xmath23 can be computed as follows : @xmath27 this function computes the amount of memory ( i.e. , number of slots ) that needs to be allocated to store pointers along with the actual postings .",
    "given function @xmath23 , we can rewrite equation  ( [ equation : memorycost ] ) as : @xmath28 where @xmath29 is the frequency of term @xmath17 , and @xmath30 is the size of the vocabulary . making a standard simplifying assumption , if we rank the terms in the collection with respect to their frequencies , the resulting pairs of @xmath31 ( where @xmath32 is normalized ) form a zipfian distribution , with the following probability mass function ( pmf ) : @xmath33 where @xmath34 is the @xmath35 generalized harmonic number , and @xmath36 is a parameter . from equation  ( [ equation : zipfian : pmf ] ) , one can estimate a term frequency given the rank of term @xmath37 and the total number of terms in the collection @xmath38 as : @xmath39 thus , we can rewrite equation  ( [ equation : memorycost : direct ] ) as follows : @xmath40 where @xmath41 is the rank ( with respect to frequency ) of a term in the collection .",
    "equation  ( [ equation : memorycost : zipfian ] ) gives an analytical model for estimating the memory cost of indexing a particular collection , given @xmath38 ( total number of terms ) and the characteristic zipfian parameter @xmath36 .",
    "furthermore , we can speed up the computation of equation  ( [ equation : memorycost : zipfian ] ) by exploiting the fact that the pmf of a zipfian distribution is a one - to - one function . in this way ,",
    "based on the definition of the step function @xmath23 , we have : @xmath42 therefore , we can rewrite equation  ( [ equation : memorycost : zipfian ] ) as follows by substituting the above in the definition of @xmath23 : @xmath43    to summarize , given a characteristic zipfian parameter @xmath36 , the total number of terms @xmath38 , and a configuration @xmath21 , we can compute the memory cost of indexing a particular collection in closed form .",
    "we now turn to our analytical model of time cost , that of the sum of reading postings lists corresponding to all query terms .",
    "let us assume that the cost of reading postings for a configuration @xmath21 is equal to the sum of two components :  ( 1 ) the cost of a sequential scan of equivalent postings lists stored as contiguous arrays and ( 2 ) the cost of following all pointers that link together non - contiguous slices between different pools .",
    "the first component is the same for all configurations ( give a collection ) so we can ignore as a constant .",
    "the number of pointers for a term @xmath17 with frequency @xmath22 can be computed easily given a particular configuration @xmath21 , so we can redefine our cost function as follows : @xmath44 where @xmath45 is the cost of following a pointer and @xmath46 is the number of pointers needed in a particular postings list given a configuration @xmath21 .",
    "the number of pointers can be easily estimated given the step function @xmath23 defined in section  [ section : memorycost ] .",
    "thus , assuming we have an estimate of the distribution of @xmath46 ( from a query log ) , we are able to analytically compute a time cost .    what about @xmath45 , the cost of following a particular pointer ? where exactly does this cost come from ?",
    "although all our index structures are held in main memory , latencies can still vary by orders of magnitude due to the design of cache hierarchies in modern processor architectures .",
    "reading contiguous blocks of postings ( in a slice ) is a very fast operation since ( 1 ) neighboring postings are likely to be on the same cache line , and ( 2 ) predictable memory access when striding postings means that pre - fetching is likely to occur . on the other hand , when posting traversal reaches the end of a slice , the algorithm needs to follow the pointer to the next slice and begin reading there  most of the time , this will result in a cache miss , which will trigger a reference to main memory , which is significantly slower .",
    "therefore , the cost @xmath45 is dominated by the cost of a cache miss . however , since we model @xmath45 as a constant , it is not necessary to estimate its actual value  therefore , our analytical time costs are modeled in abstract units of @xmath45 .    to summarize",
    ", we can analytically estimate the time cost if we are given a hypothetical postings length distribution of query terms and the cost of a cache miss using equation  ( [ equation : cachemisscost ] ) .",
    "we stress that this model is overly simplistic and does not account for time spent intersecting postings . nevertheless , this simplification is acceptable since we use the analytical model only to guide our experiments on real data , and in our empirical results we do measure end - to - end query latency .",
    "[ figure : spacetime ]",
    "given a set of configurations @xmath47 , we can estimate the memory cost @xmath48 as well as the simplified time cost @xmath49 for any configuration @xmath50 .",
    "however , to complete our model we need to know the total number of terms @xmath38 , size of the vocabulary @xmath30 , and parameter @xmath36 . to determine these values ,",
    "we divided the tweets2011 collection into two equally - sized partitions and used the first half for parameter estimation ; the second half is used in our actual experiments ( described later ) .",
    "we determined @xmath36 to be @xmath51 , and @xmath30 and @xmath38 to @xmath52 and @xmath53 respectively .    as explained in section  [ section : timecost ] , in order to estimate the time cost we need the distribution of length of postings for a set of query terms :  this is shown for all three query sets in figure  [ figure : querycfdist ] .",
    "this figure shows that the overall distribution is similar among all query sets .",
    "in particular , the distribution from the aol and terabyte queries are nearly identical . data from the microblog queries give rise to a similarly shaped distribution , although with less emphasis at the extremes ( both very common and very rare terms ) .",
    "given all these parameters , as well as the set of configurations @xmath54 , we estimated the time cost and the memory cost for each configuration . on a scatter plot of the time versus memory cost , each configuration @xmath50 would represent a point : points closer to the origin would be considered `` better '' configurations ( faster , less memory ) .",
    "our strategy for exploring the configuration space was to first use our analytical model to quickly determine the tradeoffs associated with a large set of configurations , and then from those select a subset on which to run actual experiments .",
    "we considered slice sizes between 0 and 12 ( inclusive ) and pool sizes between 4 and 8 ( inclusive ) another experiment specifically focused on four - pool configurations ( as in the production system ) .",
    "within these ranges , we computed the memory and time cost for all possible configurations . since a scatter plot of all configurations would not be readable , we grouped the configurations into equally - sized buckets in terms of memory cost , and from each bucket , we picked the configuration that has the smallest time cost .",
    "figure  [ figure : spacetime ] shows the scatter plot constructed in this manner , using the aol queries for the time cost estimates ( results using other queries look nearly identical , and are not shown for space considerations ) .",
    "the right plot shows only four - pool configurations ; the left plot shows all pool sizes between 4 and 8 ( inclusive ) .",
    "based on these figures , we selected a set of candidate configurations that appear to present good time / cost tradeoffs .",
    "as our analytical models demonstrate , after a certain point the memory costs increase while the time costs level off , thereby making most of the configurations uninteresting .",
    "the more preferable configurations are those that appear near the origin in plots in figure  [ figure : spacetime ] . the configurations selected for experimental analysis",
    "are noted .",
    "there is one additional issue we consider .",
    "given that earlybird maintains several index segments in memory ( one `` active '' , the rest read - only ) , it has easy access to historical term statistics from preceding index segments .",
    "it stands to reason that we can take advantage of this information .",
    "although it seems obvious that such statistics would help , there are countervailing considerations as well .",
    "we have found that there is a great deal of `` churn '' in tweet content  @xcite ; for example , approximately 7% of the top 10,000 terms ( ordered by frequency ) from one day are no longer in the top 10,000 on the next day .",
    "this makes sense since discussions on twitter evolve quickly in response to breaking news events and idiosyncratic internet memes .",
    "therefore , using term statistics may not actually help :  a term that appeared frequently in the previous index segment may be related to a news story that is no longer `` hot '' , and as a result we might over - allocate memory and waste space .    to empirically determine how these factors play out on real data , we experimented with different policies for allocating the first slice ( i.e. , instead of always starting from the first pool , choose a pool with a larger slice size ) .",
    "we refer to this as the starting pool ( sp ) policy :    = 1em    * sp(@xmath55 ) * : this is the default policy that does not take advantage of any term frequency history .",
    "every allocation starts from the first memory pool ( i.e. , @xmath55 ) .",
    "* sp(@xmath56 ) * : this policy starts indexing a term @xmath17 from the memory pool with the smallest slice size that is larger than the given historical frequency @xmath57 , i.e. , from the previous index segment .",
    "that is , start from pool @xmath58 if @xmath59 or pool @xmath12 if @xmath60 .",
    "* sp(@xmath61 ) * : according to this policy , indexing starts from the memory pool with the largest slice size that is smaller than the given historical frequency of a term @xmath57 .",
    "that is , start from pool @xmath58 if @xmath62 or pool @xmath12 if @xmath60 .",
    "* sp(@xmath63 ) * : based on this policy , if the frequency of a term @xmath57 is greater than or equal to the slice size of the last pool ( i.e. , @xmath64 ) , then indexing for that term starts from the last pool .",
    "otherwise , indexing starts from the default pool , @xmath55 .",
    "function @xmath63 is @xmath65 if @xmath66 and @xmath55 otherwise .",
    "this basically divides postings into `` long '' and `` short '' , with the last slice size as the break point .    in all of the above policies ,",
    "when we encounter an out - of - vocabulary term while indexing , we default to starting from the first memory pool ( i.e. , @xmath55 ) .    using the above schemes , we integrate history into our allocation policies .",
    "therefore , our experiments explore not only the impact of different pool configurations , but also the role of history in improving cost .",
    "to isolate only the effects that we are after , our experiments were not conducted on the codebase of the live production system , but rather a separate implementation , which was also implemented in java .",
    "this allowed us to separate unrelated issues , such as management of multiple segments , query brokering , and synchronization of data structures from the core problem of memory allocation .",
    "experiments were performed on a server running red hat linux , with dual intel xeon `` westmere '' quad - core processors ( e5620 2.4ghz ) and 128 gb ram .",
    "this particular architecture has a 64 kb l1 cache per core , split between data and instructions ; a 256 kb l2 cache per core ; and a 12 mb l3 cache shared by all cores of a single processor .",
    "however , all experiments were run on a single thread .",
    "our metrics were as follows :  evaluation of memory usage is quantified in terms of memory slots allocated once all tweets have been indexed ( denoted @xmath67 ) .",
    "similarly , time costs were measured with different queries after all the tweets have been indexed .",
    "this is a simplification , since in the production system query evaluation is interleaved with indexing .",
    "however , in production , concurrency is managed by an elaborate set of memory barriers , which is not germane to the current study . for our first time metric",
    ", we computed the per query average time to read postings for all query terms in their entirety , i.e. ,    @xmath68    unlike estimates from our analytical model @xmath49 , experimental costs are measured in milliseconds .",
    "in addition , we measured the per query average time to retrieve @xmath69 results in conjunctive query processing mode , i.e. , the most recent 100 hits that contain all query terms ( we denote this @xmath70 ) .",
    "we used a simple linear merge algorithm to perform postings intersection . note that although more effective algorithms are available ( e.g. , svs  @xcite )",
    ", it remains an open question whether they are suitable for our type of index .",
    "those techniques implicitly assume contiguous postings lists , since they use variants of binary search to seek through postings .",
    "we felt that to isolate the effects of different query evaluation algorithms , this was a reasonable choice .    so that we can evaluate the impact of different policies for taking advantage of term history , we divided the tweets2011 corpus roughly in half ( chronologically ) .",
    "all experiments were run on the second half , using statistic from the first half ( where appropriate ) .",
    "note that , somewhat coincidentally , half of the tweets2011 corpus corresponds roughly to the size of the index segments deployed in production , adding realism to our results .",
    "[ cols= \" < , > , > , > , > , > , > , > \" , ]      in our second set of experiments , we investigated the impact of starting pool policies .",
    "as previously described , we divided the tweets2011 corpus in half , gathered term statistics from the first half , and performed experiments on the second half .",
    "experiments focused on particularly interesting pool configurations from the previous results :  @xmath71 , @xmath72 , and the default production configuration , @xmath73 .",
    "when taking advantage of historical term statistics , there are many issues at play .",
    "first , we would expect faster query evaluation since the postings lists are more likely to be contiguous .",
    "this suggests less time overall when traversing all postings ( @xmath67 ) , although the impact on @xmath70 is unknown since top 100 retrieval is unlikely to require traversal of all postings . in terms of space ,",
    "there are two considerations :  starting at larger slices might save memory due to fewer pointers ; on the other hand , if past statistics are not entirely predictive , memory will be wasted .",
    "how these factors balance out is an empirical question .",
    "table  [ table : results : historybased ] shows results for various settings on our three sets of queries .",
    "time is measured across 3 trials with 95% confidence intervals and the table is organized in a similar manner as table  [ table : results ] .",
    "note that sp(@xmath55 ) is equivalent to using no term statistics , and is exactly the same as in table  [ table : results ] ( row duplicated here for convenience ) .",
    "results show that in all cases different sp policies waste space ( i.e , result in a larger memory footprint ) , without a clear convincing gain in speed .",
    "for example , the most aggressive policy sp(@xmath74 ) is the most wasteful ( 816% more memory ) . despite the intuitive appeal of using historical term statistics ,",
    "there does not seem to be a benefit , at least for the policies we studied .",
    "the problem of incremental indexing , of course , is not new .",
    "however , the literature generally explores different points in the design space .",
    "previous work typically makes the assumption that the inverted lists ( i.e. , postings ) are too large to fit in memory and therefore the index must reside on disk .",
    "most algorithm operate by buffering documents and performing in - memory inversion  ( e.g. ,  @xcite ) , up to the capacity of a memory buffer .",
    "after the buffer is exhausted , inverted lists are flushed to disk ; after repeated cycles of this process , we now face the challenge of how to integrate the in - memory portion of the index with one or more index segments that have been written to disk .",
    "there are three general strategies .",
    "the simplest is to rebuild the on - disk index in its entirely whenever the in - memory buffer is exhausted .",
    "this strategy is useful as a baseline , but highly inefficient in practice .",
    "the second option is to modify postings in - place on disk whenever possible  @xcite , for example , by `` eagerly '' allocating empty space at the end of existing inverted lists for additional postings .",
    "however , no `` pre - allocation '' heuristic can perfectly predict postings that have yet to be encountered , so inevitably there is either not enough space or space is wasted . for the in - place strategy ,",
    "if insufficient free space is available , to keep the postings contiguous , the indexer must relocate the entire inverted list elsewhere , requiring expensive disk seeks for copying the data .",
    "the third strategy avoids expensive random accesses by merging in - memory and portions of on - disk inverted lists whenever the memory buffer fills up  @xcite :  index merging takes advantage of the good bandwidth of disk reads and writes .",
    "in particular , lester et al .",
    "@xcite advocate a geometric partitioning and hierarchical merging strategy that limits the number of outstanding partitions , similar to  @xcite .",
    "one challenge of all three strategies described above is the handling of concurrent queries while in - memory and on - disk indexes are being processed .",
    "no matter what strategy , the operations will take a non - trivial amount of time , during which an operational system must continue serving queries efficiently .",
    "many of the papers cited above do not discuss concurrent query evaluation .",
    "in contrast , this is an important aspect of our work in building a production system ( although not specifically the focus of this paper ) .    in the buffer - and - flush approach , margaritis and anastasiadis",
    "@xcite present an interesting alternative beyond the three strategies discussed above .",
    "they make a slightly different design choice :  when the in - memory buffer reaches capacity , instead of flushing the _ entire _ in - memory index , they choose to flush only a portion of the term space ( a contiguous range of terms based on lexicographic sort order ) , performing a merge with the corresponding on - disk portions of the inverted lists .",
    "the advantage of this is that it does not lead to a proliferation of index segments , compared to the work of lester et al .",
    "@xcite .",
    "other than the obvious difference of in - memory vs.  on - disk storage of the index , there is another more subtle point that distinguishes previous work from the earlybird design .",
    "the approaches above generally try to keep postings lists contiguous  and for good reason , since disk seeks are expensive .",
    "there is , however , substantial cost in maintaining contiguity in terms of disk operations that are needed at index time .",
    "in contrast , since earlybird index structures are in main memory , we found it acceptable for postings to be discontiguous . while it is true that traversing non - contiguous postings in memory results in cache misses , the cost of a cache miss is less in relative terms than a disk seek .",
    "discontiguous inverted lists allow us to implement a zero - copy approach to indexing  once postings are written , we never need to copy them . in a managed memory environment such as the jvm",
    ", this leads to far less pressure on the garbage collector , since buffer copying yields garbage objects .    in another work , lempel et al .",
    "@xcite eschew inverted indexes completely and incrementally build document - centered representations , from which postings list are dynamically constructed and cached only in response to queries .",
    "the assumption is that more `` heavyweight '' index processes will run periodically ( e.g. , every 30 minutes ) , so that all other data structures can be considered transient .",
    "although this design appears to be justified for the particular search environment explored ( corporate intranet ) , these assumptions do not appear to be workable for our setting .",
    "another interesting point in the design space is represented by google s percolator architecture  @xcite , which is built on top of bigtable  @xcite  a distributed , multi - dimensional sparse sorted map based log - structured merge trees .",
    "percolator supports incremental data processing through _ observers _ , similar to database triggers , which provide cross - row transactions , whereas bigtable only supports single - row transactions .",
    "this architecture represents a very different design from our system , which makes a fair comparison difficult .",
    "the performance figures reported by the authors suggest that earlybird is much faster in indexing , but in fairness , this is an apples - to - oranges comparison .",
    "percolator was designed to encompass the entire webpage ingestion pipeline , handling not only indexing but other aspects of document processing as well  whereas earlybird is highly specialized for building in - memory inverted indexes .    finally , a few notes about our strategy for allocating postings slices from fixed - size pools :  there are some similarities we can point to in previous work , but some important differences as well . with the in - place update strategy where extra space for postings is pre - allocated , it is not much of a stretch to implement fixed block sizes that are powers of two .",
    "brown et al .",
    "@xcite allocate space for on - disk postings in sizes of 16 , 32 , 64 , 128 , @xmath75 8192 .",
    "however , a few important differences ( beyond in - memory vs. on - disk ) :  brown et al .  copy postings",
    "each time a new block is allocated to preserve contiguity , whereas we do nt .",
    "in addition , the paper leaves open the method by which those blocks are allocated  whereas we describe a specific implementation based on fixed slice sizes in large pools ( supporting efficient memory allocation , compact pointer addressing , etc . ) .    tracing the lineage of various storage allocation mechanisms further back in time",
    ", we would arrive at a rich literature on general - purposes memory allocation for heap - based languages ( e.g. , malloc in c ) . according to the taxonomy of wilson et al .",
    "@xcite , earlybird s allocation strategy would be an example of segregated free lists , an approach that dates back to the 1960s .",
    "of course , since we re allocating memory for the very specific purpose of storing postings , we can accomplish the task much more efficiently since there are much tighter constraints , e.g. , no memory fragmentation , fixed sizes , etc .",
    "nevertheless , it would be fair to think of our work as a highly - specialized variant of general purpose memory allocators for heap - based languages .",
    "although the problem of online indexing is not new , we explore a part of the design space that makes fundamentally different assumptions compared to previous work :  we consider index structures that are completely in memory and applications that have much tighter index latency requirements .",
    "there are many challenges for such applications , and we examined in depth one particular issue  dynamic postings allocation  within a general framework for incremental indexing .",
    "our results are interesting in and of themselves , but we hope to achieve the broader goal of bringing real - time search problems to the attention of the research community . hopefully , this will spur more work in this area .",
    "this work has been supported in part by nsf under awards iis-0916043 , iis-1144034 , and iis-1218043 .",
    "any opinions , findings , or conclusions are the authors and do not necessarily reflect those of the sponsor . the first author s deepest gratitude goes to katherine , for her invaluable encouragement and wholehearted support .",
    "the second author is grateful to esther and kiri for their loving support and dedicates this work to joshua and jacob ."
  ],
  "abstract_text": [
    "<S> we explore a real - time twitter search application where tweets are arriving at a rate of several thousands per second . </S>",
    "<S> real - time search demands that they be indexed and searchable immediately , which leads to a number of implementation challenges . in this paper </S>",
    "<S> , we focus on one aspect : dynamic postings allocation policies for index structures that are completely held in main memory . the core issue can be characterized as a `` goldilocks problem '' . because memory remains today a scare resource , an allocation policy that is too aggressive leads to inefficient utilization , while a policy that is too conservative is slow and leads to fragmented postings lists . </S>",
    "<S> we present a dynamic postings allocation policy that allocates memory in increasingly - larger `` slices '' from a small number of large , fixed pools of memory . through analytical models and experiments , </S>",
    "<S> we explore different settings that balance time ( query evaluation speed ) and space ( memory utilization ) . </S>"
  ]
}