{
  "article_text": [
    "nonlinear regression effects may be modeled via additive regression models of the form @xmath0 where the functions @xmath1 have unspecified functional form .",
    "an approach which allows flexible representation of the functions @xmath1 is the expansion in basis functions @xcite . to prevent overfitting ,",
    "there are two general approaches . in the first approach",
    ", each function @xmath2 is the sum of only a small set of basis functions , @xmath3 the basis functions @xmath4 are chosen adaptively by a selection procedure .",
    "the second approach ( that is outlined in section [ sec : splines ] ) circumvents the problem of basis function selection .",
    "instead , we allow a generous amount @xmath5 of basis functions in the expansion ( [ eq : expansion ] ) .",
    "as this usually leads to high - dimensional and highly correlated data , we penalize the coefficients @xmath6 in the estimation process @xcite .",
    "quite generally , a different approach to deal with high dimensionality is to use dimension reduction techniques such as partial least squares ( pls ) @xcite .",
    "the main idea is to build a few components from the predictor variables and to regress @xmath7 onto these components .",
    "a short overview on pls can be found in section [ sec : pls ] .    as a linear approach",
    ", pls probably fails to yield high prediction accuracy in the case of nonlinear relationships between predictors and responses as in ( [ eq : model ] ) . in order to incorporate nonlinear structures",
    ", it might be advisable to transform the original predictors preliminarily to a pls regression .",
    "this approach has been proposed by and in different variants .",
    "the method proposed by is based on a variant of pls that may be computed via an iterative algorithm .",
    "they suggest an approach that incorporates splines transformations of the predictors within each iteration of the iterative algorithm .",
    "in contrast , the method proposed by is global .",
    "the predictors are first transformed using splines basis functions as a preliminary step , then pls regression is performed on the transformed data matrix .",
    "the choice of the degree @xmath8 of the polynomial pieces and of the number of knots is performed by an either ascending or descending search procedure that is not automatic .    for large numbers of variables , this search procedure is computationally intensive and might overfit the training data . in the present article",
    ", we suggest an alternative approach based on the penalty strategy of . as described in section [ sec : splines ] , we transform the initial data matrix nonlinearly using b - splines basis functions . our new method , which we call penalized pls , is based on the following principle .",
    "the equivalent of penalizing the ( higher order ) differences of adjacent b - splines coefficients is , in the framework of dimension reduction , the penalization of ( higher order ) differences of adjacent weights .    in section",
    "[ sec : penpls ] , we introduce an adaptation of the principle of penalization to pls .",
    "more precisely , we present a penalized version of the optimization problem attached to pls .",
    "although the motivation stems from its use for b - splines transformed data , the proposed approach is very general and can be adapted to other penalty terms or to other dimension reduction techniques such as principal components analysis .",
    "it turns out that the new method shares a lot of properties of pls and that its computation requires virtually no extra costs .",
    "we highlighten the close connection between penalized pls and preconditioned linear systems .",
    "it is already known that pls is equivalent to the conjugate gradient method @xcite applied to the set of normal equations associated to a linear regression problem .",
    "we prove that penalized pls corresponds to a conjugate gradient method for a preconditioned set of normal equations , where the preconditioner depends on the penalty term .",
    "furthermore , we show that this new technique is closely related to the so - called kernel trick .",
    "more precisely , we prove that penalized pls is equivalent to ordinary pls using a generalized inner product that is defined by the penalty term . in sections",
    "[ sec : birth ] and [ sec : polymer ] , we illustrate our method on different data sets .",
    "in the rest of the paper , we restrict ourselves to a univariate response . in section [ sec : conclusion ] , we stress that the extension of our method to a multivariate response is straightforward .",
    "let us consider the general linear regression problem .",
    "we want to predict a univariate response variable @xmath9 using @xmath10 predictor variables @xmath11 based on a finite set @xmath12 of observations .",
    "we set @xmath13 and require for simplicity of notation that both @xmath14 and @xmath7 are centered .",
    "if we assume that the relationship between predictors and response is linear , this relationship can be represented in compact form by @xmath15 here , @xmath16 is the @xmath10-dimensional vector of regression coefficients and @xmath17 is the vector of residuals .",
    "+ when @xmath18 , the usual regression tools such as ordinary least squares ( ols ) regression can not be applied to estimate @xmath16 since the @xmath19 covariance matrix @xmath20 ( which has rank at most @xmath21 ) is singular . from a technical point of view , this may be solved by replacing the inverse of the covariance matrix by a generalized inverse . however , for @xmath18 , ols usually fits the training data perfectly and one can not expect the method to perform well on a new data set .",
    "partial least squares ( pls ) @xcite is an alternative regression tool which is more appropriate in the case of highly correlated predictors and high - dimensional data .",
    "pls is a standard tool for analyzing chemical data @xcite , and in recent years , the success of pls has lead to applications in other scientific fields such as physiology @xcite or bioinformatics @xcite .",
    "the main idea of pls is to build orthogonal components @xmath22 from the original predictors @xmath14 and to use them as predictors in a least squares regression .",
    "there are different pls techniques to extract these components , and each of them gives rise to a different variant of pls .",
    "it is not our aim to explain all variants and we focus on two of them .",
    "an overview on different forms of pls can be found in .",
    "a component is a linear combination of the original predictors that hopefully reflects the relevant structure of the data .",
    "pls is similar to principal components regression ( pcr ) .",
    "the difference is that pcr extracts components that explain the variance in the predictor variables whereas pls extracts components that have a large covariance with @xmath7 .",
    "we now formalize this concept .",
    "a latent component @xmath23 is a linear combination @xmath24 of the predictor variables .",
    "the vector @xmath25 is usually called the weight vector .",
    "we want to find a component with maximal covariance to @xmath7 , that is we want to maximize the empirical squared covariance @xmath26 we have to constrain @xmath25 in order to obtain identifiability , choosing @xmath27 using lagrangian multipliers , we conclude that the solution @xmath28 is  up to a scaling factor ",
    "equal to @xmath29 .",
    "let us remark that ( [ eq : crit1 ] ) and ( [ eq : constr1 ] ) are equivalent to @xmath30 the solution of ( [ eq : critneu ] ) is only unique up to a scalar .",
    "the normalization of the weight vectors @xmath25 to length @xmath31 is not essential for the pls algorithm and pls algorithms differ in the way they scale the weight vectors and components . in this paper",
    ", we present all algorithms without the scaling of the vectors , in order to keep the notation as simple as possible .",
    "subsequent components @xmath32 are chosen such that they maximize ( [ eq : crit1 ] ) and that all components @xmath33 are mutually orthogonal . in pls",
    ", there are different techniques to extract subsequent components , and each technique gives rise to a variant of pls .",
    "we briefly introduce two of them . in the method called simpls @xcite ,",
    "one computes for the @xmath34th component , @xmath35 alternatively , one can deflate the original predictor variables @xmath14 . that is",
    ", we only consider the part of @xmath14 that is orthogonal onto all components @xmath36 . for any matrix @xmath37 ,",
    "let us denote by @xmath38 the orthogonal projection onto the space that is spanned by the columns of @xmath37 . in matrix notation",
    ", we have @xmath39 the deflation of @xmath14 with respect to the components @xmath40 is defined as @xmath41 for the computation of the @xmath34th component , @xmath14 is replaced by @xmath42 in ( [ eq : crit1 ] ) .",
    "this method is called the nipals algorithm @xcite .",
    "the two methods are equivalent if @xmath7 is univariate in the sense that we end up with the same components @xmath43 @xcite . in this paper",
    ", we use the nipals algorithm . in summary ,",
    "the pls algorithm is described in algorithm [ algo : pls1 ] .",
    "[ algo : pls1 ]    pls used to be overlooked by statisticians and was considered an algorithm rather than a sound statistical model .",
    "this attitude is in parts understandable , as in the early literature on the subject , pls was explained solely in terms of formulas as in algorithm [ algo : pls1 ] . due to its success in applications ,",
    "the interest in the statistical properties of pls has risen .",
    "it can be related to other dimension reduction techniques such as principal components regression and ridge regression and these methods can be cast under a unifying framework @xcite .",
    "the shrinkage properties of pls have been studied extensively @xcite .",
    "furthermore , it can be shown that pls is closely connected to krylov subspaces and the conjugate gradient method @xcite .",
    "we discuss this method in more detail in section [ sec : penpls ] .",
    "let us return to the pls algorithm . with @xmath44 denoting the collection of components , the fitted response is given by @xmath45 in order to obtain the response for new observations , we have to determine the vector of regression coefficients @xmath46 .",
    "therefore , a representation of the components @xmath47 as a linear combination of the original predictors @xmath14 is needed . in other words , we have to derive weight vectors @xmath48 with @xmath49 they are in general different from the `` pseudo '' weight vectors @xmath50 that are computed by the nipals algorithm . in order to avoid redundancy ,",
    "the derivation of these weight vectors is deferred until section [ sec : penpls ] .",
    "it should be noted that the number @xmath51 of pls components is an additional model parameter that has to be estimated .",
    "one way of determining @xmath51 is by cross - validation .",
    "the fitting of generalized additive models by use of penalized regression splines has become a widely used tool in statistics .",
    "starting with the seminal paper by , the approach has been extended and applied in various publications @xcite .",
    "the basic concept is to expand the additive component of each variable @xmath52 in basis functions as in ( [ eq : expansion ] ) and to estimate the coefficients by penalization techniques .",
    "as suggested in , b - splines are used as basis functions yielding so - called p - splines ( for penalized b - splines ) .",
    "splines are one - dimensional piecewise polynomial functions",
    ". the points at which the pieces are connected are called knots or breakpoints .",
    "we say that a spline is of order @xmath8 if all polynomials are of degree @xmath53 and if the spline is @xmath54 times continuously differentiable at the breakpoints .",
    "a particular efficient set of basis functions are b - splines @xcite .",
    "the number of basis functions depends on the order of the splines and the number of breakpoints . for a given variable @xmath52",
    ", we consider a set of corresponding b - splines basis functions @xmath55 .",
    "these basis functions define a nonlinear map @xmath56 by performing such a transformation on each of the variables @xmath57 , the observation vector @xmath58 turns into a vector @xmath59 of length @xmath60 . here",
    "@xmath61 is the function defined by the b - splines .",
    "the resulting data matrix obtained by the transformation of @xmath14 has dimensions @xmath62 and will be denoted by @xmath63 in the rest of the paper . in the examples in sections",
    "[ sec : birth ] and [ sec : polymer ] , we consider the most widely used cubic b - splines , i.e. we choose @xmath64 .",
    "the estimation of ( [ eq : model ] ) is transformed into the estimation of the @xmath60-dimensional vector that consists of the coefficients @xmath6 : @xmath65 as explained above , the vector @xmath16 determines a nonlinear , additive function @xmath66 as @xmath63 is usually high - dimensional , the estimation of @xmath16 by minimizing the squared error @xmath67 usually leads to overfitting . following , we use for each variable many basis functions ,",
    "say @xmath68 , and estimate by penalization .",
    "the idea is to penalize the second derivative of the function @xmath69 .",
    "show that the following difference penalty term is a good approximation of the penalty on the second derivative of @xmath69 , @xmath70 these are also called the second - order differences of adjacent parameters .",
    "the difference operator @xmath71 has the form @xmath72 the coefficients @xmath73 control the amount of penalization . this penalty term can be expressed in terms of a penalty matrix @xmath74 .",
    "we denote by @xmath75 the @xmath76 matrix @xmath77 that defines the first order difference operator . setting @xmath78",
    "we conclude that the penalty term equals @xmath79 here @xmath80 is the @xmath19 diagonal matrix containing @xmath81 on its diagonal and @xmath82 is the kronecker product .",
    "the generalization of this method to higher - order differences of the coefficients of adjacent b - splines is straightforward .",
    "we simply replace @xmath83 by @xmath84 to summarize , the penalized least squares criterion has the form @xmath85 with the penalty matrix @xmath74 defined as @xmath86 this is a symmetric matrix that is positive semidefinite .",
    "we now introduce a general framework to combine pls with penalization terms .",
    "we remark that this is not limited to spline transformed variables or to the special shape of the penalty matrix @xmath74 that is defined in ( [ eq : p ] ) . for this reason , we present the new method in terms of the original data matrix @xmath14 and only demand that @xmath74 is a symmetric matrix such that @xmath87 is positive definite .    again , we restrict ourselves to univariate responses @xmath7 . penalized pls for multivariate responses",
    "is briefly discussed in section [ sec : conclusion ] .",
    "we modify the optimization criterion ( [ eq : critneu ] ) of pls in the following way .",
    "the first component @xmath88 is defined by the solution of the problem @xmath89 using lagrangian multipliers , we obtain the solution @xmath90 with @xmath91 .",
    "subsequent weight vectors and components are computed by deflating @xmath14 as described in ( [ eq : deflation ] ) and then maximizing ( [ eq : critpen ] ) with @xmath14 replaced by @xmath42 . in particular , we can compute the weight vectors and components of penalized pls by simply replacing @xmath92 by ( [ eq : penweight ] ) in algorithm [ algo : pls1 ] .",
    "we now present results on penalized pls that allow us to compute its regression vectors efficiently .",
    "note that all results on penalized pls also hold for ordinary pls if we choose @xmath93 .",
    "let @xmath94 denote the matrices of components and weight vectors respectively .",
    "[ lemma : r ] the matrix @xmath95 is upper bidiagonal , that is @xmath96 if @xmath97 or @xmath98 .",
    "the matrix @xmath99 is invertible .",
    "furthermore , the columns of @xmath100 and the columns of @xmath101 span the same space .",
    "this is an extension of a result for ordinary pls that can be found e.g. in .",
    "the proof can be found in the appendix .",
    "we can now determine the regression coefficients for penalized pls .",
    "[ regvector ] the penalized pls regression vector obtained after @xmath51 steps is @xmath102 in particular , the penalized pls estimator is the solution of the constrained minimization problem @xmath103    we deduce from lemma [ lemma : r ] that the columns of @xmath104 span the same space as the columns of @xmath100 . as pls is ordinary least squares regression with predictors @xmath105 , we have @xmath106 the second statement can be proven by noting that the ols minimization problem with constraints ( [ eq : restr.ols ] ) is equivalent to an unconstrained minimization problem for @xmath107 with @xmath108 .",
    "if we plug this into the formula for the ols estimator , we obtain ( [ eq : beta ] ) .",
    "formula ( [ eq : beta ] ) is beneficial for theoretical purposes but it is computationally inefficient .",
    "we now show how the calculation can be done in a recursive and faster way .",
    "the key point is to find `` effective '' weight vectors @xmath48 such that for every @xmath34 @xmath109 this can be done by exploiting the fact that @xmath99 is bidiagonal .    [ pro : tildew ] the effective weight vectors @xmath48 defined in ( [ eq : tildew ] ) and the regression vectors of penalized pls are determined by setting @xmath110 and @xmath111 and computing iteratively @xmath112    the proof can be found in the appendix . combining this result with the pls algorithm [ algo : pls1 ] ,",
    "we obtain the penalized pls algorithm [ algo : penpls ] .",
    "[ algo : penpls ]      it is well - known that pls is closely connected to krylov subspaces and conjugate gradient methods . quite generally , linear regression problems can be transformed into algebraic problems in the following way .",
    "the ols estimator is the solution of the minimization problem @xmath113 this is equivalent to finding the solution of the associated normal equation @xmath114 with @xmath115 and @xmath116 .",
    "if the matrix @xmath117 is invertible , the solution of the normal equations is the ols estimator @xmath118 .",
    "if @xmath117 is singular , the solution of ( [ eq : normal ] ) with minimal euclidean norm is @xmath119 .",
    "we already mentioned in section [ sec : pls ] that in the case of high dimensional data , the matrix @xmath117 is often ( almost ) singular and that the ols estimator performs poorly on new data sets .",
    "a popular strategy is to regularize the least squares criterion ( [ eq : ls ] ) in the hope of improving the performance of the estimator .",
    "this corresponds to finding approximate solutions of ( [ eq : normal ] ) .",
    "for example , ridge regression corresponds to the solution of the modified normal equations @xmath120 here @xmath121 is the ridge parameter .",
    "principal components regression uses the eigen decomposition of @xmath117 @xmath122 and approximates @xmath117 and @xmath123 via the first @xmath51 eigenvectors @xmath124 it can be shown that the pls estimators are equal to the approximate solutions of the conjugate gradient method @xcite .",
    "this is a procedure that iteratively computes approximate solutions of ( [ eq : normal ] ) by minimizing the quadratic function @xmath125 along directions that are @xmath126-orthogonal .",
    "the approximate solution obtained after @xmath51 steps is equal to the pls estimator obtained after @xmath51 iterations .    the conjugate gradient algorithm is in turn closely related to krylov subspaces and the lanczos algorithm @xcite .",
    "the latter is a method for approximating eigenvalues .",
    "the connection between pls and these methods is well - elaborated in .",
    "we now establish a similar connection between penalized pls and the above mentioned methods . set @xmath127 recall that @xmath128 is a symmetric and positive definite matrix that is determined by the penalty term @xmath74 .",
    "we now illustrate that penalized pls finds approximate solutions of the preconditioned normal equation @xmath129    [ lemmakrylov ] the space spanned by the weight vectors @xmath130 of penalized pls is the same as the space spanned by the krylov sequence @xmath131    this is the generalization of a result for ordinary pls and can be proven via induction .",
    "details are given in the appendix .",
    "we denote by @xmath132 the space that is spanned by the krylov sequence ( [ eq : krylov ] ) .",
    "this space is called a krylov space .",
    "[ corkrylov ] the penalized pls estimator is the solution of the optimization problem @xmath133    this follows immediately from proposition [ regvector ] and the fact that the weight vectors span the krylov space @xmath134 .",
    "we now present the conjugate gradient method for the equation @xmath135 the conjugate gradient method is normally applied if the involved matrix is symmetric .",
    "note that in general , the matrix @xmath136 is not symmetric with respect to the canonical inner product , but with respect to the inner product @xmath137 defined by @xmath138 .",
    "we can rewrite the quadratic function @xmath139 defined in ( [ eq : cg ] ) as @xmath140 we replace the canonical inner product by the inner product defined by @xmath138 and minimize this function iteratively along directions that are @xmath136-orthogonal .",
    "we start with an initial guess @xmath141 and define @xmath142 .",
    "the quantity @xmath143 is the search direction and @xmath144 is the residual . for a given direction @xmath143",
    ", we have to determine the optimal step size , that is we have to find @xmath145 it is straightforward to check that @xmath146 the new approximate solution is then @xmath147 after updating the residuals via @xmath148 we define a new search direction @xmath149 that is @xmath150-orthogonal to the previous search directions .",
    "this is ensured by projecting the residual @xmath144 onto the space that is @xmath136-orthogonal to @xmath151 .",
    "we obtain @xmath152    [ thm : cg ] the penalized pls algorithm is equal to the conjugate gradient algorithm for the preconditioned system ( [ eq : precg ] ) .",
    "the presentation of the conjugate gradient method above and the proof of its equivalence to penalized pls are an extension of the corresponding results for pls that is given in .",
    "the proof can be found in the appendix .",
    "note that there is a different notion of conjugate gradients for preconditioned systems @xcite .",
    "we transform the preconditioned equation ( [ eq : cg ] ) by postmultiplying with @xmath128 : @xmath153 as the matrix @xmath154 is symmetric , we can apply the ordinary conjugate gradient algorithm to this equation .",
    "this approach differs from the one described above .",
    "suppose that @xmath155 is regular .",
    "after at most @xmath10 iterations , the penalized pls estimator equals the ols estimator .    using ( [ eq : restr.ols ] )",
    ", the above statement is equivalent to showing that @xmath156 hence , we have to show that there is a polynomial @xmath157 of degree @xmath158 such that @xmath159 .",
    "as @xmath128 is invertible , the ols estimator is @xmath160 as @xmath161 is the product of two symmetric matrices and @xmath128 is positive definite , @xmath161 has a real eigendecomposition , @xmath162 we define the polynomial @xmath157 via the at most @xmath10 equations @xmath163 it follows immediately that @xmath164 .",
    "this concludes the proof .",
    "the computation of the penalized pls estimator as presented in algorithm [ algo : penpls ] involves matrices and vectors of dimension @xmath165 and @xmath10 respectively .",
    "if the number of predictors @xmath10 is very large , this leads to high computational costs . in this subsection",
    ", we show that we can represent this algorithm in terms of matrices and vectors of dimension @xmath166 and @xmath167 respectively .",
    "let us define the @xmath168 matrix @xmath169 via @xmath170 this matrix is called the gram matrix or the kernel matrix of @xmath14 .",
    "we conclude from corollary [ corkrylov ] that the penalized pls estimator obtained after @xmath51 steps is an element of the krylov space @xmath171 .",
    "it follows that we can represent the penalized pls estimator as @xmath172 here , the krylov space @xmath173 is the space spanned by the vectors @xmath174 analogously , we can represent the effective weight vectors by @xmath175 it follows from the definition of the deflation step that @xmath176 we conclude that the weight vector @xmath50 is simply @xmath177 if we plug in these representations into the penalized pls algorithm [ algo : penpls ] , we obtain algorithm [ algo : kernelpls ] that depends only on @xmath169 and @xmath7 .",
    "[ algo : kernelpls ]    a kernel version of pls has already been defined in in order to speed up the computation of pls .",
    "we repeat that the speed of the kernel version of penalized pls does not depend on the number of predictor variables at all but on the number of observations .",
    "this implies that  from an algorithmic point of view  there are no restrictions in terms of the number of predictor variables .",
    "the importance of this so - called `` dual '' representation also becomes apparent if we want to extend pls to nonlinear problems by using the kernel trick . in this paper",
    ", the kernel trick appears in two different versions .",
    "let us only consider the case of ordinary pls on b - splines transformed variables . recall that in ( [ eq : z ] )",
    ", we transform the original data @xmath14 using a nonlinear function @xmath61 defined by the b - splines .",
    "as algorithm [ algo : kernelpls ] only relies on inner products between observations , the nonlinear transformation does not increase the computational costs .",
    "we only have to compute the kernel matrix of inner products @xmath178 this implies that we do not have to map the data points explicitly using a function @xmath61 .",
    "it suffices to compute the function @xmath179 the function @xmath180 is called a kernel .",
    "the replacement of the usual inner product by kernel is known as the kernel trick and has turned up to be very popular in the machine learning community . instead of defining a nonlinear map @xmath61",
    ", we define a `` valid '' kernel function @xmath181 .",
    "e.g. , polynomial relationships can be modeled via kernels of the form @xmath182 furthermore , it is possible to define kernels for complex data structures as graphs or text .",
    "literature on the kernel trick and its applications is abundant .",
    "a detailed treatise of the subject can be found in .",
    "a nonlinear version of pls using the kernel trick is presented in .",
    "if we represent penalized pls in terms of the kernel matrix @xmath169 , we realize that penalized pls is closely connected to the kernel trick in other respects . using algorithm [ algo : kernelpls ] or the definition of the kernel matrix @xmath169",
    ", we realize that penalized pls equals ordinary pls with the canonical inner product replaced by the inner product @xmath183 this function is called a linear kernel .",
    "why is this a sensible inner product ?",
    "let us consider the eigendecomposition of the penalty matrix , @xmath184 .",
    "we prefer direction @xmath185 such that @xmath186 is small , that is we prefer directions that are defined by eigenvectors @xmath187 of @xmath74 with a small corresponding eigenvalue @xmath188 .",
    "if we represent the vectors @xmath189 and @xmath190 in terms of the eigenvectors of @xmath74 , @xmath191 we conclude that @xmath192 this implies that directions @xmath187 with a small eigenvalue @xmath188 receive a higher weighting than directions with a large eigenvalue .",
    "[ sec : example ] in this section , we analyze a real data set describing pregnancy and delivery for @xmath193 infants who are sent to a neonatal intensive care unit after birth .",
    "the data are taken from the r @xcite software package exactmaxsel and are introduced in .",
    "our goal is to predict the number of days spent in the neonatal intensive care unit ( y ) based on the following predictors : birth weight ( in g ) , birth height ( in cm ) , head circumference ( in cm ) , term ( in week ) , age of the mother ( in year ) , weight of the mother before pregnancy ( in kg ) , weight of the mother before delivery ( in kg ) , height of the mother ( in cm ) , time ( in month ) .",
    "some of the predictors are expected to be strongly associated with the response ( e.g. , birth weight , term ) , in contrast to poor predictors like time or height of the mother .",
    "the parameter settings are as follows .",
    "we make the simplifying assumption that @xmath194 , which reduces the problem of selecting the optimal smoothing parameter to a one - dimensional problem .",
    "as already mentioned above , we use cubic splines .",
    "furthermore , the order of difference of adjacent weights is set to @xmath195 .",
    "the shape of the fitted functions @xmath2 depends on the two model parameters @xmath196 and @xmath51 .",
    "we first illustrate that the number @xmath51 of penalized pls components controls the smoothness of the estimated functions . for this purpose",
    ", we only consider the predictor variable `` weight '' .",
    "figure [ fig : weight ] displays the fitted functions obtained by penalized pls for @xmath197 and 4 different numbers of components @xmath198 . for small values of @xmath51 ,",
    "the obtained functions are smooth . for higher values of @xmath51 ,",
    "the functions adapt themselves more and more to the data which leads to overfitting for high values of @xmath51 .",
    "we compare our novel method to pls without penalization as described in @xcite and the ` gam ( ) ` package in ` r ` .",
    "this is the implementation of an adaptive selection procedure for the basis functions in ( [ eq : expansion ] ) .",
    "more details can be found in and .",
    "this is the standard tool for estimating generalized additive models .",
    "the optimal parameter values of ( penalized ) pls are determined by computing the leave - one - out squared error .",
    "we remark that the split into training and test set is done before transforming the original predictors using b - splines . in order to have comparable results ,",
    "we normalize the response such that @xmath199 .",
    "the results are summarized in table [ tab : results2 ] .",
    "penalized pls is the best out of the three method . in particular",
    ", it receives a considerably lower error than pls without penalization .",
    "this data set consists of @xmath200 predictor variables and four response variables .",
    "the number of observations is @xmath201 .",
    "the data are taken from a polymer test plant .",
    "it can be downloaded from ftp://ftp.cis.upenn.edu / pub / ungar / chemdata/. the predictor variables are measurements of controlled variables in a polymer processing plant ( e.g. temperatures , feed rates ... ) .",
    "no more details on the variables are given due to confidentiality reasons .",
    "as in the last section , we first scale each response variable to have a variance equal to @xmath31 .",
    "again , we compare penalized pls to pls and ` gam ( ) ` .",
    "the results are summarized in table [ tab : results3 ] .",
    "for all four response variables , penalized pls is better than pls without penalization .",
    "penalized pls is also better that gam for three out of the four response variables , although the difference is considerably smaller .",
    "in this work , we proposed an extension of partial least squares regression using penalization techniques . apart from its computational efficiency ( it is virtually as fast as pls ) , it also shares a lot of mathematical properties of pls .",
    "our novel method obtains good results in applications . in the two examples that are discussed , penalized",
    "pls clearly outperforms pls without penalization .",
    "furthermore , the results indicate that it is a competitor of ` gam ( ) ` in the case of very high - dimensional data .",
    "we might think of other penalty terms . consider a preconditioned version of pls by giving weights to the predictor variables .",
    "higher weights are given to those predictor variables that are highly correlated to the response .",
    "these weights can be expressed in terms of a penalty term .",
    "combine pls with an additive penalty term to data derived from near infra red spectroscopy .",
    "the penalty term controls the smoothness of the regression vector .",
    "the introduction of a penalty term can easily be adapted to other dimension reduction techniques .",
    "for example for principal components analysis , the penalized optimization criterion is @xmath202 pls can handle multivariate responses @xmath203 .",
    "the natural extension of criterion ( [ eq : crit1 ] ) is the following .",
    "@xmath204 using lagrangian multipliers , we deduce that the solution is the eigenvector of the matrix @xmath205 that corresponds to the largest eigenvalue of @xmath206 .",
    "this eigenvector is usually computed in an iterative fashion .",
    "if we want to apply penalized pls for multivariate responses , we compute @xmath207 the solution fulfills @xmath208 this is called a generalized eigenvalue problem or a matrix pencil .",
    "note that for multivariate @xmath203 , the equivalence of simpls and nipals does not hold , so we expect the penalized versions of these methods to be different as well",
    ". there are kernel versions for pls with multivariate @xmath203 @xcite , hence we can also represent multivariate penalized pls in terms of kernel matrices .",
    "this research was supported by the deutsche forschungsgemeinschaft ( sfb 386 , `` statistical analysis of discrete structures '' ) .",
    "xx    boulesteix , a .-",
    "2006 , ` maximally selected chi - square statistics for ordinal variables ' , _ biometrical journal _ * 48 * ,  451462 .",
    "boulesteix , a .-",
    "strimmer , k. 2006 , ` partial least squares : a versatile tool for the analysis of high - dimensional genomic data ' , _ briefings in bioinformatics _ . to appear .",
    "butler , n.  denham , m. 2000 , ` the peculiar shrinkage properties of partial least squares regression ' , _ journal of the royal statistical society b _ * 62 * ,  585593 .    de  boor , c. 1978 , _ a practical guide to splines _ , springer .",
    "de  jong , s. 1993 , ` simpls : an alternative approach to partial least squares regression ' , _ chemometrics and intelligent laboratory systems _ * 18 * ,  251  263 .",
    "de  jong , s. 1995 , ` pls shrinks ' , _ journal of chemometrics _ * 9 * ,  323326 .",
    "durand , j.  f. 2001 , ` local polynomial additive regression through pls and splines : plss ' , _ chemometrics and intelligent laboratory systems _ * 58 * ,  235246 .",
    "durand , j.  f.  sabatier , r. 1997 , ` additive splines for partial least squares regression ' , _ journal of the american statistical association _ * 92 * ,  15461554 .",
    "eilers , p.  marx , b. 1996 , ` flexible smoothing with b - splines and penalties ' , _ statistical science _ * 11 * ,  89121 .",
    "frank , i.  friedman , j. 1993 , ` a statistical view of some chemometrics regression tools ' , _ technometrics _ * 35 * ,  109135 .",
    "golub , g.  van loan , c. 1983 , _ matrix computation _ , john hopkins university press , baltimore .",
    "goutis , c. 1996 , ` partial least squares yields shrinkage estimators ' , _ the annals of statistics _ * 24 * ,  816824 .",
    "goutis , c.  fearn , t. 1996 , ` partial least squares regression on smooth factors ' , _ journal of the american statistical association _ * 91 * ,  627632 .",
    "hastie , t.  tibshirani , r. 1990 , _ generalized additive models _ , chapman and hall .",
    "helland , i. 1988 , ` on the structure of partial least squares regression ' , _ communications in statistics , simulation and computation _ * 17*(2 ) ,  581607 .",
    "hestenes , m.  stiefel , e. 1952 , ` methods for conjugate gradients for solving linear systems ' , _ journal of research of the national bureau of standards _ * 49 * ,  409436 .",
    "kondylis , a.  whittaker , j. 2006 , ` preconditioning krylov spaces and variable selection in plsr ' , _ preprint _ .",
    "lanczos , c. 1950 , ` an iteration method for the solution of the eigenvalue problem of linear differential and integral operators ' , _ journal of research of the national bureau of standards _ * 45 * ,  225280 .",
    "manne , r. 1987 , ` analysis of two partial - least - squares algorithms for multivariate calibration ' , _ chemometrics and intelligent laboratory systems _ * 2 * ,  187197 .",
    "martens , h.  naes , t. 1989 , _ multivariate calibration _ , wiley , new york .",
    "phatak , a.  de  hoog , f. 2003 , ` exploiting the connection between pls , lanczos , and conjugate gradients : alternative proofs of some properties of pls ' , _ journal of chemometrics _ * 16 * ,  361367 .    2005 , _ r : a language and environment for statistical computing _ ,",
    "r foundation for statistical computing , vienna , austria .",
    "3 - 900051 - 07 - 0 .",
    "rnnar , s. , lindgren , f. , geladi , p.  wold , s. 1994 , ` a pls kernel algorithm for data sets with many variables and fewer objects , part i : theory and applications ' , _ journal of chemometrics _ * 8 * ,  111125 .",
    "rosipal , r.  krmer , n. 2006 , overview and recent advances in partial least squares , _ in _ ` subspace , latent structure and feature selection techniques ' , lecture notes in computer science , springer , pp .",
    "rosipal , r.  trejo , l. 2001 , ` kernel partial least squares regression in reproducing kernel hilbert spaces ' , _ journal of machine learning research _ * 2 * ,  97123 .",
    "rosipal , r. , trejo , l.  matthews , b. 2003 , kernel pls - svc for linear and nonlinear classification , _ in _ ` proceedings of the twentieth international conference on machine learning ' , washington , dc , pp .",
    "640647 .",
    "ruppert , d. 2002 , ` selecting the number of knots for penalized splines ' , _ journal of computational and graphical statistics _ * 11 * ,  735757 .",
    "schlkopf , b.  smola , a. 2002 , _ learning with kernels .",
    "support vector machines , regularization , optimization , and beyond .",
    "_ , the mit press .    stone , m.  brooks , r. 1990 , ` continuum regression : cross - validated sequentially constructed prediction embracing ordinary least squares , partial least squares and principal components regression ( with discussion ) ' , _ journal of the royal statistical society b _ * 52 * ,  237269 .",
    "wold , h. 1975 , path models with latent variables : the nipals approach , _ in _ h.  b. et  al . , ed .",
    ", ` quantitative sociology : international perspectives on mathematical and statistical model building ' , academic press , pp .",
    "307357 .",
    "wold , s. , ruhe , h. , wold , h.  iii , w.  d. 1984 , ` the collinearity problem in linear regression . the partial least squares ( pls ) approach to generalized inverses ' , _ siam journal of scientific and statistical computations _ * 5 * ,  735743 .",
    "wood , s. 2006 , _ generalized additive models : an introduction with r _ , chapman and hall .",
    "wood , s.  n. 2000 , ` modelling and smoothing parameter estimation with multiple quadratic penalties ' , _ journal of the royal statistical society b _ * 62*(2 ) ,  413428 .",
    "we recall that for @xmath209 @xmath210 the last equality follows from the fact that the components @xmath33 are mutually orthogonal .",
    "in particular , we obtain @xmath211    first note that ( [ defl2 ] ) is equivalent to @xmath212 .",
    "it follows that @xmath213 as all components @xmath33 are mutually orthogonal , @xmath214 we conclude that @xmath99 is an upper triangular matrix with all diagonal elements @xmath215 .",
    "furthermore , it follows from ( [ rij ] ) that all vectors @xmath216 are linear combinations of the components @xmath217 .",
    "this implies that the columns of @xmath101 and the columns of @xmath100 span the same space .",
    "finally , we have to show that @xmath99 is bidiagonal . to prove this , we show that @xmath218 for @xmath219 .",
    "the condition @xmath220 implies ( recall ( [ defl1 ] ) ) that @xmath221 and consequently @xmath222 this implies that for @xmath223 @xmath224    for @xmath225 , we have @xmath226 as @xmath227 . for a general @xmath34",
    ", we have @xmath228 the last equality holds as @xmath229 is bidiagonal .",
    "using formula ( [ eq : proj ] ) for the projection operator , it follows that @xmath230 we conclude that @xmath231 the regression estimate after @xmath232 steps is @xmath233 this concludes the proof .",
    "we use induction .",
    "for @xmath234 we know that @xmath235 . for a fixed @xmath236",
    ", we conclude from the induction hypothesis and lemma [ lemma : r ] that every vector @xmath185 that lies in the span of @xmath105 is of the form @xmath237 we conclude that @xmath238 and that @xmath239            this corresponds to the iterative definition of @xmath242 .",
    "we only have to show that @xmath243 note that @xmath244 as @xmath245 is @xmath136-orthogonal onto all directions @xmath246 , the proof is complete .      as the search directions @xmath245 span the krylov space @xmath134",
    ", we can replace the matrix @xmath247 in ( [ eq : beta ] ) by the matrix @xmath248 . as",
    "the search directions are @xmath136-orthogonal , we have @xmath249 and this equals the formula in lemma [ lem : xm ] ."
  ],
  "abstract_text": [
    "<S> we propose a novel method to model nonlinear regression problems by adapting the principle of penalization to partial least squares ( pls ) . starting with a generalized additive model , </S>",
    "<S> we expand the additive component of each variable in terms of a generous amount of b - splines basis functions . in order to prevent overfitting and to obtain smooth functions </S>",
    "<S> , we estimate the regression model by applying a penalized version of pls . </S>",
    "<S> although our motivation for penalized pls stems from its use for b - splines transformed data , the proposed approach is very general and can be applied to other penalty terms or to other dimension reduction techniques . </S>",
    "<S> it turns out that penalized pls can be computed virtually as fast as pls . </S>",
    "<S> we prove a close connection of penalized pls to the solutions of preconditioned linear systems . in the case of high - dimensional data , </S>",
    "<S> the new method is shown to be an attractive competitor to other techniques for estimating generalized additive models . if the number of predictor variables is high compared to the number of examples , traditional techniques often suffer from overfitting . </S>",
    "<S> we illustrate that penalized pls performs well in these situations .    </S>",
    "<S> * keywords * : generalized additive model , dimension reduction , nonlinear regression , conjugate gradient </S>"
  ]
}