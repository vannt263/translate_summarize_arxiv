{
  "article_text": [
    "the objective of this paper is consistent estimation and selection of regression coefficients in models with a large number of endogenous regressors .",
    "consider the linear model @xmath5 where @xmath6 is a zero - mean random error possibly correlated with @xmath7 and @xmath8 is an unknown vector of parameters of our main interests .",
    "the @xmath9 component of @xmath8 is denoted by @xmath10 .",
    "a component in the @xmath11dimensional vector @xmath7 is said to be _ endogenous _ if it is correlated with @xmath6 ( i.e. , @xmath12 ) and _ exogenous _ otherwise ( i.e. , @xmath13 ) . without loss of generality , i will assume all regressors are endogenous throughout the rest of this paper for notational convenience ( a modification to allow mix of endogenous and exogenous regressors is trivial . ) .",
    "when endogenous regressors are present , the classical least squares estimator will be inconsistent for @xmath8 ( i.e. , @xmath14 ) even when the dimension @xmath15 of @xmath8 is small relative to the sample size @xmath0 .",
    "the classical solution to this problem of endogenous regressors supposes that there is some @xmath16-dimensional vector of instrumental variables , denoted by @xmath17 , which is observable and satisfies @xmath18 for all @xmath19 . in particular , the two - step estimation procedures including the two - stage least square ( 2sls ) estimation and the control function approach play an important role in accounting for endogeneity that comes from individual choice or market equilibrium ( e.g. , wooldrige , 2002 ) . consider the following `` first stage '' equations for the components of @xmath7 @xmath20 for each @xmath21 ,",
    "@xmath22 is a @xmath23 vector of instrumental variables , and @xmath24 a zero - mean random error which is uncorrelated with @xmath22 , and @xmath25 is an unknown vector of nuisance parameters .",
    "i will refer to the equation in ( 1 ) as the main equation ( or second - stage equation ) and the equations in ( 2 ) as the first - stage equations . throughout the rest of this paper",
    ", i will impose the following assumption . without loss of generality",
    ", this assumption implies a triangular simultaneous equations model structure . +   + * assumption 1.1 * : the data * @xmath26 * are _ i.i.d . _ with finite second moments ; @xmath27 for all @xmath21 and @xmath28 for all @xmath29 .",
    "+   + statistical estimation and variable selection in the high - dimensional setting is concerned with models in which the dimension of the parameters of interests is larger than the sample size . in the past decade , a tremendous increase of research activities in this field has been facilitated by the advances in data collection technology . in the literature on high - dimensional sparse linear regression models ,",
    "a great deal of attention has been given to the @xmath30penalized least squares .",
    "in particular , the lasso and the dantzig selector are the most studied techniques ( see , e.g. , tibshirani , 1996 ; cands and tao , 2007 ; bickel , ritov , and tsybakov , 2009 ; belloni , chernozhukov , and wang , 2011 ; belloni and chernozhukov , 2011b ; loh and wainwright , 2012 ; negahban , ravikumar , wainwright , and yu , 2012 ) .",
    "variable selection when the dimension of the problem is larger than the sample size has also been studied in the likelihood method setting with penalty functions other than the @xmath30norm ( see , e.g. , fan and li , 2001 ; fan and lv , 2011 ) .",
    "lecture notes by koltchinskii ( 2011 ) , as well as recent books by bhlmann and van de geer ( 2011 ) and wainwright ( 2014 ) have given a more comprehensive introduction to high - dimensional statistics .",
    "the lasso procedure is a combination of the residual sum of squares and a @xmath30regularization defined by the following program @xmath31 denote the minimizer to the above program by @xmath32",
    ". a necessary and sufficient condition of @xmath32 is that @xmath33 belongs to the subdifferential of the convex function @xmath34 .",
    "this implies that the lasso solution @xmath32 satisfies the constraint @xmath35 the dantzig selector of the linear regression function is defined as a vector having the smallest @xmath30norm among all @xmath36 satisfying the above constraint , i.e. , @xmath37 recently , these @xmath30penalized techniques have been applied in a number of economics studies .",
    "caner ( 2009 ) studies a lasso - type gmm estimator . rosenbaum and tsybakov ( 2010 )",
    "study the high - dimensional errors - in - variables problem where the non - random regressors are observed with additive error and they present an application to hedge fund portfolio replication .",
    "lecture notes by belloni and chernozhukov ( 2011b ) discuss the @xmath30based penalization methods with various econometric problems including earning regressions and instrumental selection in angrist and krueger data ( 1991 ) .",
    "belloni and chernozhukov ( 2011a ) study the @xmath3-penalized quantile regression and illustrate its use on an international economic growth application .",
    "belloni , chen , chernozhukov , and hansen ( 2012 ) estimate the optimal instruments using the lasso and in an empirical example dealing with the effect of judicial eminent domain decisions on economic outcomes , they find the lasso - based instrumental variable estimator outperforms an intuitive benchmark .",
    "belloni , chernozhukov , and hansen ( 2012 ) propose robust methods for inference on the effect of a treatment variable on a scalar outcome in the presence of very many controls with an application to abortion and crime .",
    "fan , lv , and li ( 2011 ) review the literature on sparse high - dimensional econometric models including the vector autoregressive model for measuring the effects of monetary policy , panel data model for forecasting home price , and volatility matrix estimation in finance .",
    "their discussion is not restricted to @xmath30based regularization methods .",
    "high dimensionality arises in the triangular simultaneous equations structure ( 1 ) and ( 2 ) when the dimension @xmath15 of @xmath8 is large relative to the sample size @xmath0 ( namely , @xmath38 ) or when the dimension @xmath39 of @xmath25 is large relative to the sample size @xmath0 ( namely , @xmath40 ) for at least one @xmath41 . in this paper ,",
    "i consider the scenario where the number of non - zero coefficients in @xmath8 and @xmath25 is small relative to @xmath0 ( i.e. , @xmath8 and @xmath25for @xmath21 are _ exactly _ _ sparse _ ) .",
    "the case where @xmath40 for at least one @xmath41 but @xmath42 has been considered by belloni and chernozhukov ( 2011b ) , where they showed the instruments selected by the lasso technique in the first - stage regression can produce an efficient estimator with a small bias at the same time . to the best of my knowledge , the case where @xmath38 and @xmath43 for all @xmath41 , or the case where @xmath38 and @xmath40 for at least one @xmath41 in the context of triangular simultaneous equations with two - stage estimation has not been studied in the literature . in both cases ,",
    "one can still use the ideas of the 2sls estimation together with the lasso technique .",
    "for instance , in the case where @xmath38 and @xmath43 for all @xmath41 , one can obtain the fitted regressors by a standard least square estimation on each of the first - stage equations separately as usual and then apply a lasso - type technique with these fitted regressors in the second - stage regression .",
    "similarly , in the case where @xmath38 and @xmath40 for all @xmath41 , one can obtain the fitted regressors by performing a regression with a lasso - type estimator on each of the first - stage equations separately and then apply another lasso - type estimator with these fitted regressors in the second - stage regression .",
    "compared to existing two - stage techniques which limit the number of regressors entering the first - stage equations or the second - stage equation or both , the two - stage estimation procedures with @xmath30regularization in both stages are more flexible and particularly powerful for applications in which the vector of parameters of interests is sparse and there is lack of information about the relevant explanatory variables and instruments . in terms of practical implementations ,",
    "these above - mentioned high - dimensional two - stage estimation procedures are intuitive and can be easily implemented using existing software packages for the standard lasso - type technique for linear models without endogeneity . in analyzing the statistical properties of these estimators , the extension from models with a few endogenous regressors to models with many endogenous regressors ( @xmath38 ) in the context of triangular simultaneous equations with two - stage estimation",
    "is not obvious .",
    "this paper aims to explore the validity of these two - step estimation procedures for the triangular simultaneous linear equation models in the high - dimensional setting under the sparsity scenario .    in the presence of endogenous regressors",
    ", the direct implementation of the lasso or dantzig selector fails as sparsity of coefficients in equation ( 1 ) does not correspond to sparsity of linear projection coefficients . the linear instrumental variable model with a single or a few endogenous regressors and many instruments has been studied in the econometrics literature on high dimensional models . for example ,",
    "belloni and chernozhukov ( 2011b ) consider the following triangular simultaneous equation model : @xmath44 with @xmath45 here @xmath46 , @xmath47 , and @xmath48 denote wage , education ( the endogenous regressor ) , and a vector of other explanatory variables ( the exogenous regressors ) respectively , and @xmath17 denotes a vector of instrumental variables that have direct effect on education but are uncorrelated with the unobservables ( i.e. , @xmath6 ) such as innate abilities in the wage equation .    in many applications ,",
    "the number of endogenous regressors is also large relative to the sample size .",
    "one example concerns the nonparametric regression model with endogenous explanatory variables .",
    "consider the model @xmath49 where @xmath50 and @xmath51 is an unknown function of interest .",
    "assume @xmath52 for all @xmath19 .",
    "suppose we want to approximate @xmath53 by linear combinations of some set of basis functions , i.e. , @xmath54 , where @xmath55 are some known functions .",
    "then , we end up with a linear regression model with many endogenous regressors .",
    "empirical examples of many endogenous regressors can be found in hedonic price regressions of consumer products ( e.g. , personal computers , automobiles , pharmaceutical drugs , residential housing , etc . )",
    "sold within a market ( say , market _ i _ ) or by a firm ( say , firm _ i _ ) .",
    "there are two major issues with using firm",
    "_ i _ s ( or , market _",
    "s ) product characteristics as the explanatory variables .",
    "first , the number of explanatory variables formed by the characteristics ( and the transformations of these characteristics ) of products such as personal computers , automobiles , and residential houses can be very large .",
    "for example , in the study of hedonic price index analysis in personal computers , the data considered by benkard and bajari involved 65 product characteristics ( benkard and bajari , 2005 ) .",
    "together with the various transformations of these characteristics , the number of the potential regressors can be very large .",
    "on the other hand , it is plausible that only a few of these variables matter to the underlying prices but which variables constitute the relevant regressors are unknown to the researchers .",
    "housing data also tends to exhibit a similar high - dimensional but sparse pattern in terms of the underlying explanatory variables ( e.g. , lin and zhang , 2006 ; ravikumar , et .",
    "al , 2009 ) .",
    "second , firm _ i _",
    "s product characteristics are likely to be endogenous because just like price , product characteristics are typically choice variables of firms , and it is possible that they are correlated with unobserved components of price ( ackerberg and crawford , 2009 ) . an alternative is to use other firms ( other markets ) product characteristics as the instruments for firm _ i _ s ( market _ i _",
    "s ) product characteristics . in demand estimation literature , this type of instruments are sometimes referred to as blp instruments , e.g. , berry , et .",
    ", 1995 ( respectively , hausman instruments , e.g. , nevo , 2001 ) .",
    "another empirical example of many endogenous regressors concerns the study of network or community influence .",
    "for example , manresa ( 2013 ) looks at how a firm s production output is influenced by the investment of other firms . as a future extension",
    ", she suggests an alternative model that looks at the network influence in terms of the output of the other firms rather than their investment : @xmath56 @xmath57 denotes a vector of exogenous regressors specific to firm _",
    "i _ ( e.g. , investment ) at period _ t_. @xmath58 and @xmath59 are the fixed effects of firm _ i _ and period _ t , _ respectively .",
    "notice that @xmath60 , the output of other firms enters the right - hand - side of the equations above as additional regressors and @xmath61 , @xmath62 , and @xmath63 are interpreted as the network influence arising from other firms output on firm _",
    "i _ s output .",
    "furthermore , the influence _ _ on firm _",
    "i _ from firm _",
    "j _ is allowed to differ from the influence on firm _ j _ from firm _",
    "i_. endogeneity arises from the simultaneity of the output variables when @xmath64 ( e.g. , presence of unobserved network characteristics that are common to all firms ) . as a result ,",
    "the number of endogenous regressors in the model above is of the order @xmath65 , which exceeds the number of periods _",
    "t _ in the application considered by manresa ( 2013 ) .",
    "the case of many endogenous regressors and many instrumental variables has been studied in the context of generalized method of moments by fan and liao ( 2011 ) , and gautier and tsybakov ( 2011 ) .",
    "fan and liao show that the penalized gmm and penalized empirical likelihood are consistent in both estimation and selection .",
    "gautier and tsybakov propose a new estimation procedure called the self tuning instrumental variables ( stiv ) estimator based on the moment conditions @xmath18 .",
    "they discuss the stiv procedure with estimated linear projection type instruments , akin to the 2sls procedure , and find it works successfully in simulation .",
    "gautier and tsybakov also speculate on the rate of convergence for this type of two - stage estimation procedures when both stage equations are in the high - dimensional settings . as will be shown in the subsequent section",
    ", the results in this paper partially confirm their conjecture .    in the low - dimensional setting ,",
    "the properties of the 2sls and gmm estimators are well - understood .",
    "however , it is unclear how the regularized 2sls procedures compare to the regularized gmm procedures in the high - dimensional and sparse setting , so it is important to study these regularized two - stage high - dimensional estimation procedures in depth .",
    "another important contribution of this paper is to introduce a set of assumptions that are suitable for showing estimation consistency and selection consistency of the two - step type of high - dimensional estimators .",
    "when endogeneity is absent from model ( 1 ) , there is a well - developed theory on what conditions on the design matrix @xmath66 are sufficient ( sufficient and necessary ) for an @xmath30based regularized estimator to consistently estimate ( select ) @xmath8 . in some situations",
    "one can impose these conditions directly as an assumption on the underlying design matrix .",
    "however , when employing a regularized 2sls estimator in the context of triangular simultaneous linear equation models in the high - dimensional setting , namely , ( 1 ) and ( 2 ) , there is no guarantee that the random matrix @xmath67 ( with @xmath68 obtained from regressing @xmath69 on the instrumental variables ) would automatically satisfy these previously established conditions for estimation or selection consistency .",
    "this paper explicitly proves that these conditions for estimation consistency indeed hold for @xmath67 with high probability under a broad class of sub - gaussian design matrices formed by the instrumental variables allowing for arbitrary correlations among the covariates .",
    "it also establishes the sample size required for @xmath67 to satisfy these conditions .",
    "furthermore , with an additional stronger assumption on the structure of the design matrices formed by the instrumental variables , this paper shows @xmath67 also satisfies the conditions for selection consistency under a stronger sample size requirement . in summary , the aims of this paper , as mentioned earlier , are to provide a theoretical justification that has not been given in literature for these regularized 2sls procedures in the high - dimensional setting .",
    "i begin in section 2 with background on the standard lasso theory of high - dimensional estimation techniques as well as basic definitions and notation used in this paper .",
    "results regarding the estimation consistency and selection consistency of the high - dimensional 2sls procedure under the sparsity scenario are established in section 3 .",
    "section 4 presents simulation results .",
    "section 5 concludes this paper and discusses future extensions .",
    "all the proofs are collected in the appendix ( section 6 ) .",
    "* notation*. for the convenience of the reader , i summarize here notations to be used throughout this paper .",
    "the @xmath70 norm of a vector @xmath71 is denoted by @xmath72 , @xmath73 where @xmath74 when @xmath75 and @xmath76 when @xmath77 . for a matrix @xmath78 , write @xmath79 to be the elementwise @xmath80- norm of @xmath81 .",
    "the @xmath82-operator norm , or spectral norm of the matrix @xmath81 corresponds to its maximum singular value ; i.e. , it is defined as @xmath83 , where @xmath84 . the @xmath80 matrix norm ( maximum absolute row sum ) of @xmath81",
    "is denoted by @xmath85 ( note the difference between @xmath86 and @xmath87 ) .",
    "i make use of the bound @xmath88 for any symmetric matrix @xmath78 . for a matrix @xmath89 , denote its minimum eigenvalue and maximum eigenvalue by @xmath90 and @xmath91 , respectively . for functions",
    "@xmath92 and @xmath93 , write @xmath94 to mean that @xmath95 for a universal constant @xmath96 and similarly , @xmath97 to mean that @xmath98 for a universal constant @xmath99 .",
    "@xmath100 when @xmath94 and @xmath97 hold simultaneously .",
    "for some integer @xmath101 , the @xmath102-ball of radius @xmath103 is given by @xmath104 where @xmath105 .",
    "similarly , the @xmath82-ball of radius @xmath106 is given by @xmath107 . also , write @xmath108 and @xmath109 . for a vector @xmath110 ,",
    "let @xmath111 be its support , i.e. , the set of indices corresponding to its non - zero components @xmath112 .",
    "the cardinality of a set @xmath113 is denoted by @xmath114 .",
    "+   + i will begin with a brief review of the case where all components in @xmath69 in ( 1 ) are _ exogenous_. assume the number of regressors @xmath15 in equation ( 1 ) grows with and exceeds the sample size @xmath0 .",
    "let us focus on the class of models where @xmath8 has at most @xmath115 non - zero parameters , where @xmath115 is also allowed to increase to infinity with @xmath0 but slowly compared to @xmath0 .",
    "consider the following lasso program : @xmath116 where @xmath117 is some tuning parameter .",
    "alternatively , we can consider a constrained version of the lasso @xmath118 by lagrangian duality theory , the above two programs are equivalent . for example , for any choice of radius @xmath119 in the constrained variant of the lasso , there is a tuning parameter @xmath120 such that solving the lagrangian form of the lasso is equivalent to solving the constrained version .",
    "consider the constrained lasso program above with radius @xmath121 . with this setting ,",
    "the true parameter vector @xmath8 is feasible for the problem . by definition ,",
    "the estimate @xmath32 minimizes the quadratic loss function @xmath122 over the @xmath30ball of radius @xmath123 .",
    "as @xmath0 increases , we expect that @xmath8 should become a near - minimizer of the same loss , so that @xmath124 .",
    "but when does closeness in the loss imply that the error vector @xmath125 is also small ?",
    "the link between the excess loss @xmath126 and the size of the error @xmath127 is the hessian of the loss function , @xmath128 , which captures the curvature of the loss function . in the low - dimensional setting where @xmath129 , as long as @xmath130 , we are guaranteed that the hessian matrix , @xmath131 , of the loss function is positive definite , i.e. , @xmath132 for @xmath133 . in the high - dimensional setting with @xmath134 , the hessian is a @xmath135 matrix with rank at most @xmath0 , so that it is impossible to guarantee that it has a positive curvature in all directions .",
    "the restricted eigenvalue ( re ) condition is one of the plausible ways to relax the stringency of the uniform curvature condition .",
    "the re condition assumes that the hessian matrix , @xmath131 , of the loss function is positive definite on a restricted set ( the choice of this set is associated with the @xmath30penalty and to be explained shortly ) . in the high - dimensional setting , it is well - known that the re condition is a sufficient condition for @xmath70- consistency of the lasso estimate @xmath32 ( see , e.g. , bickel , et .",
    "al . , 2009 ; meinshausen and yu , 2009 ; raskutti et al . , 2010 ; bhlmann and van de geer , 2011 ; loh and wainwright , 2012 ; negahban , et .",
    "al . , 2012 ) . in this paper",
    ", i will use the following definition ( see , negahban , et .",
    "al . , 2012 ; wainwright , 2014 ) . *",
    "* + * * + * definition 1 * ( re ) : the matrix @xmath66 satisfies the re condition over a subset @xmath136 with parameter @xmath137 if @xmath138 where @xmath139 with @xmath140 denoting the vector in @xmath141 that has the same coordinates as @xmath142 on @xmath143 and zero coordinates on the complement @xmath144 of @xmath143 .",
    "+   + when the unknown vector @xmath145 is exactly sparse , a natural choice of @xmath143 is the support set of @xmath8 , i.e. , @xmath146 .",
    "re is a weaker condition than other restrictions in the literature including the pairwise incoherence condition ( donoho , 2006 ; gautier and tsybakov , 2011 , proposition 4.2 ) and the restricted isometry property ( cands and tao , 2007 ) . as shown by bickel et al . , 2009",
    ", the restricted isometry property implies the re condition but not vice versa .",
    "additionally , raskutti et al . ,",
    "2010 give examples of matrix families for which the re condition holds , but the restricted isometry constants tend to infinity as @xmath147 grow .",
    "furthermore , they show that even when a matrix exhibits a high amount of dependency among the covariates , it might still satisfy re . to be more precise ,",
    "they show that , if @xmath66 is formed by independently sampling each row @xmath148 , then there are strictly positive constants @xmath149 , depending only on the positive definite matrix @xmath89 , such that @xmath150 with probability at least @xmath151 for some universal constants @xmath152 and @xmath153 .",
    "the bound above ensures the re condition holds with @xmath154 and @xmath155 as long as @xmath156 . to see this",
    ", note that for any @xmath157 , we have @xmath158 . given the lower bound above , for any @xmath159 , we have the lower bound @xmath160 where the final inequality follows as long as @xmath161 .",
    "an appropriate choice of the tuning parameter @xmath162 in the lasso program ensures @xmath163 .",
    "this fact can be formalized in the following proposition . +   + * proposition 2.1*. for the linear model @xmath164 where @xmath13 , if we solve the lasso program with parameter @xmath165 , then the error @xmath166 .",
    "+   + * proof*. define the lagrangian @xmath167 .",
    "since @xmath32 is optimal , we have @xmath168 some algebraic manipulation of the _ basic inequality _ above yields @xmath169 where the last line applies the assumption on @xmath162 .",
    "@xmath170    rudelson and zhou ( 2011 ) as well as loh and wainwright ( 2012 ) extend this type of re analysis from the case of gaussian designs to the case of sub - gaussian designs .",
    "the sub - gaussian assumption says that the explanatory variables need to be drawn from distributions with well - behaved tails like gaussian .",
    "in contrast to the gaussian assumption , sub - gaussian variables constitute a more general family of distributions . in this paper ,",
    "i make use of the following definition for a sub - gaussian matrix . + * * + * definition 2 * : a random variable @xmath69 with mean @xmath171 $ ] is sub - gaussian if there is a positive number @xmath172 such that @xmath173\\leq\\exp(\\sigma^{2}t^{2}/2)\\qquad\\textrm{for all}\\ , t\\in\\mathbb{r},\\ ] ] and a random matrix @xmath174 is sub - gaussian with parameters @xmath175 if ( a ) each row @xmath176 is sampled independently from a zero - mean distribution with covariance @xmath177 , ( b ) for any unit vector @xmath178 , the random variable @xmath179 is sub - gaussian with parameter at most @xmath180 . +",
    "* * + for example , if @xmath174 is formed by independently sampling each row @xmath181 , then the resulting matrix * @xmath174 * is a sub - gaussian matrix with parameters * @xmath182 * , recalling * @xmath183 * denotes the spectral norm of @xmath177 .",
    "suppose from performing a first - stage regression on each of the equations in ( 2 ) separately , we obtain estimates @xmath184 and let @xmath185 for @xmath21 . denote the fitted regressors from the first - stage estimation by @xmath68 , where @xmath186 .",
    "for the second - stage regression , consider the following lasso program : @xmath187 the following is a standard assumption in the literature on sparsity for high - dimensional linear models .",
    "+   + * assumption 3.1 * : the numbers of regressors @xmath188 and @xmath189 for every @xmath21 in ( 1 ) and ( 2 ) can grow with and exceed the sample size @xmath0 . * * the number of non - zero components in @xmath25 is at most @xmath190 for all @xmath21 , and the number of non - zero components in @xmath8 is at most @xmath191 .",
    "both @xmath1 and @xmath2 can increase to infinity with @xmath0 but slowly compared to @xmath0 .",
    "+   + i first present a general bound on the statistical error measured by the quantity @xmath192 . +   + * lemma 3.1 * ( general upper bound on the @xmath4error ) .",
    "let @xmath193 and @xmath194 .",
    "suppose the random matrix @xmath195 satisfies the re condition ( 3 ) with @xmath155 and the vector @xmath8 is supported on a subset @xmath196 with its cardinality @xmath197 .",
    "if a solution @xmath198 , defined in ( 4 ) has @xmath162 satisfying @xmath199 for any given @xmath0 , then there is a constant @xmath200 such that @xmath201    the proof for lemma 3.1 is provided in section 6.1 .",
    "notice that the choice of @xmath162 in lemma 3.1 depends on unknown quantities and therefore lemma 3.1 does not provide guidance to practical implementation .",
    "rather , it should be viewed as an intermediate lemma for proving consistency of the two - stage estimator later on . in the appendix ( section 6 )",
    "we show that the term @xmath202 can be bounded from above and the order of the resulting upper bound can be used to set the tuning parameter @xmath162 . in order to apply lemma 3.1 to prove consistency ,",
    "we need to show ( i ) @xmath193 satisfies the re condition ( 3 ) with @xmath155 and ( ii ) the term @xmath203 with high probability , and then we can show @xmath204 by choosing @xmath205 .",
    "the assumption @xmath206 will therefore imply the @xmath82-consistency of @xmath198 .",
    "applying lemma 3.1 to the triangular simultaneous equations model ( 1 ) and ( 2 ) requires additional work to establish conditions ( i ) and ( ii ) discussed above , which depends on the specific first - stage estimator for @xmath68 .",
    "it is worth mentioning that , while in many situations one can impose the re condition as an assumption on the design matrix ( e.g. , belloni and chernozhukov , 2011b ; belloni , chen , chernozhukov , and hansen , 2012 ) in analyzing the consistency property of the lasso , appropriate analysis is needed in this paper to verify that @xmath207 satisfies the re condition because @xmath68 is obtained from a first - stage estimation and there is no guarantee that the random matrix @xmath207 would automatically satisfy the re condition .",
    "to the best of my knowledge , previous literature has not dealt with this issue directly .",
    "consequently , the re analysis introduced in this paper is particularly useful for analyzing the statistical properties of the two - step type of high - dimensional estimators in the simultaneous equations model context .",
    "as discussed previously , this paper focuses on the case where @xmath38 and @xmath43 for all @xmath41 and the case where @xmath38 and @xmath40 for at least one @xmath41 .",
    "the following two subsections present results concerning estimation consistency and variable - selection consistency for the exact sparsity case .      to derive the non - asymptotic bounds and asymptotic properties ( i.e. , estimation consistency and selection consistency ) for @xmath198 , i impose the following regularity conditions . +",
    "* * + * assumption 3.2 * : the error terms @xmath208 and @xmath209 for @xmath21 are _ i.i.d .",
    "_ zero - mean sub - gaussian vectors with parameters @xmath210 and @xmath211 , respectively .",
    "* * the random matrix * @xmath212 * is * * sub - gaussian with parameters @xmath213 for @xmath21 . *",
    "* + * * + * assumption 3.3 * : for every @xmath21 , @xmath214 .",
    "the matrix @xmath215 is sub - gaussian with parameters @xmath216 where the * * @xmath41**th column of @xmath217 is @xmath218 . * * + * * + * assumption 3.4 * : for every @xmath21 , @xmath219 where @xmath220 . the matrix @xmath221 is sub - gaussian with parameters @xmath222 where the * * @xmath41**th column of @xmath223 is @xmath224 .",
    "* * + * * + * assumption 3.5a * : the first - stage estimator @xmath225 satisfies the bound @xmath226 with probability at least @xmath227 for some universal constants @xmath152 and @xmath153 , where * * @xmath228 and @xmath229 . * * + * * + * assumption 3.5b * : the first - stage estimator @xmath225 satisfies the bound * @xmath230 * with probability at least @xmath227 for some universal constants @xmath152 and @xmath153 , where * * @xmath228 and @xmath229 . * * + * * + * assumption 3.6 * : for every @xmath21 , the first - stage estimator @xmath184 achieves the selection consistency ( i.e. , it recovers the true support @xmath231 ) or has at most @xmath232 components that are different from the components in @xmath231 where @xmath233 , with probability at least @xmath227 for some universal constants @xmath152 and @xmath153 , where * * @xmath228 . for simplicity",
    ", we consider the case where the first - stage estimator recovers the true support @xmath231 for every @xmath21 . * * + * * + * remarks * + assumption 3.2 is common in the literature ( see , loh and wainwright , 2012 ; negahban , et . al 2012 ; rosenbaum and tsybakov , 2013 ) .",
    "the assumption that * @xmath212 * is * * sub - gaussian with parameters @xmath213 for all @xmath41 provides a primitive condition which guarantees that the random matrix formed by the instrumental variables satisfies the re condition with high probability .",
    "based on the second part of assumption 3.2 that * @xmath212 * is * * sub - gaussian with parameters @xmath213 for all @xmath41 , we have that @xmath234 and @xmath235 are sub - gaussian vectors where @xmath220 .",
    "therefore , the conditions that @xmath215 is a sub - gaussian matrix with parameters @xmath216 where the * * @xmath41**th column of @xmath217 is @xmath218(assumption 3.3 ) and @xmath221 is a sub - gaussian matrix with parameters @xmath222 where the * * @xmath41**th column of @xmath223 is @xmath224 ( assumption 3.4 ) are mild extensions . in terms of the instrumental variables and their linear combinations , assumptions 3.2 - 3.4 together with assumption 3.5a ( or 3.5b ) on the first - stage estimation error provide primitive conditions which guarantee that the random matrix @xmath207 formed by the fitted regressors @xmath236 for @xmath21 satisfies the re condition with high probability .    for assumptions 3.5a(b ) , many existing high - dimensional estimation procedures such as the lasso or dantzig selector ( see , e.g. , cands and tao , 2007 ; bickel , et .",
    "al , 2009 ; negahban , et .",
    "al . 2012 ) simultaneously satisfy the error bounds @xmath226 ( assumption 3.5a ) and @xmath230 ( assumption 3.5b ) with high probability .",
    "the reason i introduce assumptions 3.5a and 3.5b separately will be explained shortly .",
    "it is worth noting that while the @xmath30error ( @xmath4error ) from applying the lasso on a single first - stage equation should be of the order @xmath237 ( respectively , @xmath238 ) with probability at least @xmath239 , the extra term @xmath240 in the errors @xmath241 and @xmath242 and the probability guarantee @xmath227 with which these errors hold comes from the application of a union bound which takes into account the fact that there are @xmath15 endogenous regressors in the main equation and hence , @xmath15 equations to estimate in the first - stage . as a result , it is not hard to see that the sample size required for consistently estimating @xmath15 equations simultaneously when a lasso - type procedure is applied on each of the first - stage equations separately should satisfy @xmath243 as opposed to the condition @xmath244 for the case where a single equation is estimated with a lasso - type procedure .",
    "assumption 3.6 says that the first - stage estimators correctly select the non - zero coefficients with probability close to 1 .",
    "in analogy to the various sparsity assumptions on the true parameters in the high - dimensional statistics literature ( including the case of _ exact sparsity _ assumption meaning that the true parameter vector has only a few non - zero components , or _",
    "approximate sparsity _ assumption based on imposing a certain decay rate on the ordered entries of the true parameter vector ) , assumption 3.6 can be interpreted as an _ exact sparsity _",
    "constraint on the first - stage estimate * @xmath184 * for * @xmath21 * , in terms of the @xmath102- ball , given by @xmath245 it is known that under some stringent conditions such as the `` irrepresentable condition '' ( zhao and yu , 2006 ; bhlmann and van de geer , 2011 ) or the `` mutual incoherence condition '' ( wainwright , 2009 ) together with the `` beta - min condition '' ( bhlmann and van de geer , 2011 ) , lasso and dantzig types of selectors can recover the support of the true parameter vector with high probability .",
    "the `` irrepresentable condition '' , as discussed in bhlmann and van de geer , 2011 , is in fact a sufficient and necessary condition to achieve variable - selection consistency with the lasso .",
    "furthermore , they show that the `` irrepresentable condition '' implies the re condition .",
    "assumption 3.6 is the key condition that differentiates the upper bounds in the two theorems to be presented immediately .",
    "similar to the problem of estimating @xmath15 equations as in the discussion of assumptions 3.5a(b ) , the sample size required for consistently selecting the coefficients in each of the @xmath15 equations simultaneously when a lasso - type selector is applied on each of the first - stage equations separately should satisfy @xmath246 as opposed to the condition @xmath247 for the case where a single equation is estimated with a lasso - type selector .",
    "in addition , the `` beta - min '' condition for consistent selection in the @xmath15-equation problem needs to satisfy @xmath248 as opposed to @xmath249 for the consistent selection in a single equation problem .",
    "first , i present two results for the case where @xmath38 and @xmath40 for at least one @xmath41 . as discussed earlier ,",
    "the key difference between the two theorems is that the bound in the second theorem hinges on the additional assumption that the first - stage estimators correctly select the non - zero coefficients with probability close to 1 , i.e. , assumption 3.6 . with this assumption ,",
    "when the first - stage estimation error dominates the second - stage error , the statistical error of the parameters of interests in the main equation can be bounded by the first - stage estimation error in @xmath4norm . however , without assumption 3.6 , the statistical error of the parameters in the main equation needs to be bounded by the first - stage estimation error in @xmath30norm . * * +   + * theorem 3.2 * ( upper bound on the @xmath4error and estimation consistency ) : suppose assumptions 1.1 , 3.1 - 3.3 , and 3.5a hold",
    ". then , if in `` @xmath250 '' ( similarly , @xmath251 in `` @xmath252 '' in theorem 3.3 , @xmath251 in `` @xmath253 '' in corollary 3.4 , @xmath251 in `` @xmath254 '' in theorem 3.5 , and @xmath251 in `` @xmath255 '' in theorem 3.6 ) is replaced by @xmath256 , the statistical error of the parameters in the main equation will have the same scaling in terms of @xmath257 , @xmath15 , @xmath1 , @xmath2 , and @xmath0 as before with the only changes to the constants in @xmath258 and @xmath259 . ] @xmath260 and the tuning parameter @xmath162 satisfies @xmath261 we have @xmath262 where @xmath263 with probability at least @xmath264 for some universal positive constants @xmath152 and @xmath153 .",
    "if we also have @xmath265 , then in front of these scaling conditions for consistency in theorem 3.2 ( as well as in the subsequent theorems 3.3 , 3.5 , 3.6 , and corollary 3.4 ) comes from the simple inequality @xmath266 . ]",
    "the two - stage estimator @xmath198 is @xmath4consistent for * @xmath8*. +   + * theorem 3.3 * ( an improved upper bound on the @xmath4error and estimation consistency ) : suppose assumptions 1.1 , 3.1 - 3.4 , 3.5b , and 3.6 hold . then , if @xmath267}\\max\\left\\ { k_{1}^{3 - 2r}\\log d,\\ , k_{1}^{3 - 2r}\\log p,\\ , k_{1}^{r}k_{2}\\log d,\\ , k_{1}^{r}k_{2}\\log p\\right\\ } \\right\\ }   & = & o(1)\\\\ \\frac{k_{1}\\log\\max(d,\\ , p)}{n } & = & o(1),\\end{aligned}\\ ] ] and the tuning parameter @xmath162 satisfies @xmath268 we have , @xmath269 with probability at least @xmath264 for some universal positive constants @xmath152 and @xmath153 , where @xmath258 and @xmath259 are defined in theorem 3.2 .",
    "if we also have @xmath270 , then the two - stage estimator @xmath198 is @xmath4consistent for * @xmath8*. + the proofs for theorems 3.2 and 3.3 are provided in sections 6.2 and 6.3 , respectively .    the proofs for theorems 3.2 and 3.3 each consist of two parts .",
    "the first part is to show @xmath207 satisfies the re condition ( 3 ) and the second part is to bound the term @xmath202 from above . based on lemma 3.1 , the upper bound on @xmath202 pins down the scaling requirement of @xmath162 , as mentioned previously .",
    "the scaling conditions of @xmath0 and @xmath162 depend on the sparsity parameters @xmath1 and @xmath2 , which are typically unknown .",
    "nevertheless , i will assume that upper bounds on @xmath1 and @xmath2 are available , i.e. , we know that @xmath271 and @xmath272 for some integers @xmath273 and @xmath274 that grow with @xmath0 just like @xmath1 and @xmath2 .",
    "meaningful values of @xmath273 and @xmath274 are small relative to @xmath0 presuming that only a few regressors are relevant .",
    "this type of upper bound assumption on the sparsity is called _ sparsity certificate _ in the literature ( see , e.g. , gautier and tsybakov , 2011 ) .    in theorems 3.2 and 3.3",
    ", we see that the statistical errors of the parameters of interests in the main equation depend on @xmath275 , @xmath276 , @xmath277 , @xmath278 , @xmath279 , and @xmath280 . in the simple case of @xmath281 ( for example , @xmath282 with probability 1 as in a high - dimensional linear regression model without endogeneity ) ,",
    "the @xmath4errors in theorems 3.2 and 3.3 reduce to @xmath283 , where the factor @xmath284 has a natural interpretation of an inverse signal - to - noise ratio .",
    "for instance , when @xmath217 is a zero - mean gaussian matrix with covariance @xmath285 , one has @xmath286 , so @xmath287 which measures the inverse signal - to - noise ratio of the regressors in a high - dimensional linear regression model without endogeneity . hence ,",
    "the statistical error of the parameters of interests in the main equation matches the scaling of the upper bound for the lasso in the context of the high - dimensional linear regression model without endogeneity , i.e. , @xmath288 .",
    "the terms @xmath280 in theorems 3.2 and 3.3 are related to the degree of dependency between the columns of the design matrices formed by the instrumental variables and their linear combinations .",
    "for instance , for any @xmath289 and @xmath21 , notice that @xmath290 the higher dependency between the columns of the design matrix @xmath291 we have , the greater @xmath292 is , and the harder the estimation problem becomes . in the special case of @xmath293 , @xmath294 , and @xmath286 , @xmath295 , where the multiplier @xmath296 in @xmath258 is the inverse signal - to - noise ratio of @xmath217 scaled by @xmath297 .    under the assumption that the first - stage estimators correctly select the non - zero coefficients with high probability ( assumption 3.6 ) ,",
    "the scaling of the sample size required * * in theorem 3.3 is guaranteed to be no greater ( and in some cases strictly smaller ) than that in theorem 3.2 .",
    "for instance , if @xmath298 , then letting @xmath299 yields",
    "@xmath300 in this example , theorem 3.2 suggests that the choice of sample size needs to satisfy @xmath301 and @xmath302 while theorem 3.3 suggests that the choice of sample size only needs to satisfy @xmath303 and @xmath304 .    from theorem 3.2 ( respectively , theorem 3.3 )",
    ", we see that the estimation error of the parameters of interests in the main equation is of the order of the maximum of the first - stage estimation error in @xmath4norm multiplied by a factor of @xmath305 ( respectively , @xmath306 ) and the second - stage estimation error . upon the additional condition that the first - stage estimators correctly select the non - zero coefficients with probability close to 1 , note",
    "that the bound on the @xmath4error of @xmath198 in theorem 3.3 is improved upon that in theorem 3.2 by a factor of @xmath307 * * if the first term in the braces dominates the second one .",
    "it is possible that the error bound and scaling of the sample size required in theorem 3.2 is suboptimal .",
    "section 5 provides a heuristic argument that may potentially improve the bound on the @xmath4error of @xmath198 in theorem 3.2 when the first - stage estimates fail to satisfy the exact sparsity constraint specified by the @xmath102- ball discussed earlier .",
    "intuitively , the most direct effect on the @xmath4error of the second - stage estimate @xmath198 should be attributed to the @xmath4errors ( rather than the selection performance per se ) of the first - stage estimates . imposing the exact sparsity constraint ,",
    "namely , selection consistency on the first - stage estimates such as assumption 3.6 is an example of showing how special structures that impose a certain decay rate on the ordered entries of the first - stage estimates from the @xmath30regularized procedure can be utilized to tighten the @xmath4error bound .",
    "the estimation error of the parameters of interests in the main equation can be bounded by the maximum of a term involving the first - stage estimation error and a term involving the second - stage estimation error , which partially confirms is achievable for the triangular simultaneous linear equations models , a minimax lower bound result needs to be established in future work . ]",
    "the speculation in gautier and tsybakov ( 2011 ) ( section 7.2 ) that the two - stage estimation procedure can achieve the estimation error of an order @xmath308 .",
    "my results show that @xmath308 * * is achieved either when the second - stage estimation error dominates the first - stage estimation error , or when @xmath15 is large relative to @xmath257 . * * in the case where the second - stage estimation error dominates the first - stage estimation error , the statistical error of the parameters of interests in the main equation matches ( up to a factor of @xmath309 ) the order of the upper bound for the lasso estimate in the context of the high - dimensional linear regression model without endogeneity , i.e. , @xmath288 .",
    "an example of the second case where @xmath15 is large relative to @xmath257 is when the first - stage estimation concerns regressions in low - dimensional settings and the result for this specific example is formally stated in corollary 3.4 below . + * * + * corollary 3.4 * ( first - stage estimation in low - dimensional settings ) : suppose assumptions 1.1 , 3.2 , and 3.3 hold .",
    "assume the number of regressors @xmath188 in ( 1 ) can grow with and exceed the sample size @xmath0 ; the number of non - zero components in @xmath8 is at most @xmath2 , which is allowed to increase to infinity with @xmath0 but slowly compared to @xmath0 ; and @xmath310 and does _ not _ grow with @xmath0 .",
    "suppose that the first - stage estimator @xmath311 satisfies the bound * @xmath312 * with probability at least @xmath313 .",
    "then , if @xmath314 and the tuning parameter @xmath162 satisfies @xmath315 we have @xmath316 with probability at least @xmath313 , where @xmath258 and @xmath259 are defined in theorem 3.2 .",
    "if we also have @xmath317 , then the two - stage estimator @xmath198 is @xmath4consistent for * @xmath8*. + note that corollary 3.4 is a special case of theorem 3.3 and hence the result is obvious from theorem 3.3 .    under the condition that the first - stage estimators correctly select the non - zero coefficients with probability close to 1",
    ", we can also compare the high - dimensional two - stage estimator @xmath198 * * with another type of multi - stage procedure .",
    "these multi - stage procedures include three steps . in the first step , one carries out the same first - stage estimation as before such as applying the lasso or dantzig selector . under some stringent conditions that guarantee the selection - consistency of these first - stage estimators ( such as the `` irrepresentable condition '' or the `` mutual incoherence condition '' described earlier )",
    ", we can recover the supports of the true parameter vectors with high probability .",
    "in the second step , we apply ols with the regressors in the estimated support set to obtain @xmath318 for @xmath21 . in the third step ,",
    "we apply a lasso technique to the main equation with these fitted regressors based on the second - stage ols estimates .",
    "this type of procedure is in the similar spirit as the literature on sparsity in high - dimensional linear models without endogeneity ( see , e.g. , cands and tao , 2007 ; belloni and chernozhukov , 2013 )",
    ".    under this three - stage procedure , corollary 3.4 above tells us that the statistical error of the parameters of interests in the main equation * * is of the order * @xmath319 * , which is at least as good as @xmath198 . nevertheless , this improved statistical error is at the expense of imposing stringent conditions that ensure the first - stage estimators to achieve selection consistency .",
    "these assumptions only hold in a rather narrow range of problems , excluding many cases where the design matrices exhibit strong ( empirical ) correlations",
    ". if these stringent conditions in fact do not hold , then the three - stage procedure may not work . on the other hand , even in the absence of the selection - consistency in the first - stage estimation ,",
    "@xmath198 is still a valid procedure and the bound as well as the consistency result in theorem 3.2 still hold .",
    "therefore , * @xmath198 * may be more appealing in the sense that it works for a broader range of problems in which the first - stage design matrices ( formed by the instruments ) @xmath212 for @xmath21 exhibit a high amount of dependency among the covariates .    for theorems 3.2 and 3.3 ,",
    "the results are derived for the case where each of the first - stage equations is estimated separately with a lasso - type procedure .",
    "depending on the specific structures of the first - stage equations , other methods that take into account the interrelationships between these equations might yield a smaller first - stage estimation error and consequently a potential improvement on the @xmath4error of @xmath198 .",
    "this paper does not pursue these more efficient first - stage estimators but rather considers the extensions of theorems 3.2 and 3.3 in the following manner .",
    "notice that for theorem 3.2 ( or theorem 3.3 ) , we give an explicit form of the first - stage estimation error in assumptions 3.5a ( respectively , 3.5b ) and as discussed earlier , lasso type of techniques yield these estimation errors .",
    "however , the estimation error of the parameters of interests in the main equation can be bounded by the maximum of a term involving the first - stage estimation error in @xmath4norm multiplied by a factor of @xmath305 ( or @xmath306 if the first - stage estimators correctly select the non - zero coefficients with probability close to 1 ) and a term involving the second - stage estimation error , which holds for general first - stage estimation errors as long as @xmath320 forerror and the @xmath4error . ]",
    "this claim is formally stated in theorems 3.5 and 3.6 below .",
    "+   + * theorem 3.5 * : suppose assumptions 1.1 and 3.1 - 3.3 hold . also , assume the first - stage estimator @xmath311 satisfies the bound @xmath321 with probability @xmath322 .",
    "then , if @xmath323 and the tuning parameter @xmath162 satisfies @xmath324 we have @xmath325 where @xmath326 with probability at least @xmath327 for some universal positive constants @xmath152 and @xmath153 .",
    "if we also have @xmath328 , then the two - stage estimator @xmath198 is @xmath4consistent for * @xmath8*. * * + * * + * theorem 3.6 * : suppose assumptions 1.1 , 3.1 - 3.4 , and 3.6 hold . also , assume the first stage estimator @xmath311 satisfies the bound * @xmath329 * with probability @xmath322 .",
    "then , if +   + @xmath330}\\max\\left\\ { k_{1}^{2 - 2r}m^{2}(d,\\ , p,\\ , k_{1},\\ , n),\\,\\frac{k_{1}^{r}k_{2}\\log d}{n},\\,\\frac{k_{1}^{r}k_{2}\\log p}{n}\\right\\ } \\right\\ } $ ] @xmath331 @xmath332 and the tuning parameter @xmath162 satisfies    @xmath333 we have @xmath334 with probability at least @xmath327 for some universal positive constants @xmath152 and @xmath153 , where @xmath258 and @xmath259 are defined in theorem 3.5 . * * if we also have @xmath335 , then the two - stage estimator @xmath198 is @xmath4consistent for * @xmath8*. + the proofs for theorems 3.5 and 3.6 are provided in section 6.4 .    upon an additional condition that the first - stage estimators correctly select the non - zero coefficients with probability close to 1 , note",
    "that the bound on the @xmath4error of @xmath198 in theorem 3.6 is improved upon that in theorem 3.5 by a factor of @xmath307 * * if the first term in the braces dominates the second one .",
    "the scaling of the sample size required in theorem 3.6 is also improved upon that in theorem 3.5 .      in this subsection ,",
    "i address the following question : * * given an optimal two - stage lasso solution @xmath198 , when do we have @xmath336\\rightarrow1 $ ] ? that is , when can we conclude @xmath198 correctly selects the non - zero coefficients in the main equation with high probability ?",
    "this property is referred to as _ variable - selection consistency_. * * for consistent variable selection with the standard lasso in the context of linear models without endogeneity , it is known that the so - called `` neighborhood stability condition '' ( meinshausen and bhlmann , 2006 ) for the design matrix , re - formulated in a nicer form as the `` irrepresentable condition '' by zhao and yu , 2006 , is sufficient and necessary . a further refined analysis is given in wainwright ( 2009 ) , which presents under a certain `` incoherence condition '' the smallest sample size needed to recover a sparse signal . in this paper ,",
    "i adopt the analysis by wainwright ( 2009 ) , ravikumar , wainwright , and lafferty ( 2010 ) , and wainwright ( 2014 ) * * to analyze the selection consistency of @xmath198 .",
    "in particular , i need the following assumptions",
    ". +   + * assumption 3.7 * : * @xmath337\\left[\\mathbb{e}(x_{1,j(\\beta^{*})}^{*t}x_{1,j(\\beta^{*})}^{*})\\right]^{-1}\\right\\vert _ { \\infty}\\leq1-\\phi$ ] * for some @xmath338 $ ] . * * + * * + * assumption 3.8 * : the smallest eigenvalue of the submatrix @xmath339 $ ] satisfies the bound @xmath340\\right)\\geq c_{\\min}>0.\\ ] ] * * + * remarks * + assumption 3.7 , the so - called `` mutual incoherence condition '' originally formalized by wainwright ( 2009 ) , captures the intuition that the large number of irrelevant covariates can not exert an overly strong effect on the subset of relevant covariates . in the most desirable case , the columns indexed by @xmath341 would all be orthogonal to the columns indexed by @xmath342 * * and then we would have @xmath343 . * * in the high - dimensional setting , this perfect orthogonality is not possible , but one can still hope for a type of `` near orthogonality '' to hold .",
    "notice that in order for the left - hand - side of the inequality in assumption 3.7 to always fall in @xmath344 , one needs some type of normalization on the matrix @xmath345 for all @xmath21 .",
    "one possibility is to impose a column normalization as follows : @xmath346 under assumptions 1.1 and 3.3 , we know that each column @xmath347 , @xmath21 is consisted of _ i.i.d .",
    "_ sub - gaussian variables .",
    "without loss of generality , we can assume @xmath348 for all @xmath21 .",
    "consequently , the normalization above follows from a standard bound for the norms of zero - mean sub - gaussian vectors and a union bound @xmath349\\geq1 - 2\\exp(-cn+\\log p)\\geq1 - 2\\exp(-c^{'}n),\\ ] ] where the last inequality follows from @xmath350 .",
    "for example , if @xmath217 has a gaussian design , then we have @xmath351 where @xmath352 corresponds to the maximal variance of any element of @xmath217(see raskutti , et .",
    "al , 2011 ) .",
    "assumption 3.8 is required to ensure that the model is identifiable even if the support set @xmath146 were known _ a _ _",
    "priori_. assumption 3.8 is relatively mild compared to assumption 3.7 . + * * + *",
    "theorem 3.7 * ( selection consistency ) : * * suppose assumptions 1.1 , 3.1 - 3.3 , 3.5a , 3.7 , and 3.8 hold .",
    "if @xmath353 and the tuning parameter @xmath162 satisfies @xmath261 then , we have : ( a ) the lasso has a unique optimal solution @xmath198 , ( b ) the support @xmath354 , @xmath355 where @xmath356 with probability at least @xmath264 , ( d ) if @xmath357 , then * @xmath358 * and hence @xmath198 is variable - selection consistent , i.e. , * @xmath359*. * * + * * + * theorem 3.8 * ( selection consistency ) : suppose assumptions 1.1 , 3.1 - 3.4 , 3.5b , 3.6 - 3.8 hold .",
    "if @xmath360}\\max\\left\\ { k_{1}^{3 - 2r}\\log d,\\ , k_{1}^{3 - 2r}\\log p,\\ , k_{1}^{r}k_{2}\\log d,\\ , k_{1}^{r}k_{2}\\log p\\right\\ } \\right\\ }   & = & o(1),\\\\ \\frac{1}{n}k_{1}k_{2}^{2}\\log\\max(d,\\ , p ) & = & o(1),\\end{aligned}\\ ] ] and the tuning parameter @xmath162 satisfies @xmath268 then , we have : ( a ) the lasso has a unique optimal solution @xmath198 , ( b ) the support @xmath354 , and @xmath361 with probability at least @xmath264 , where @xmath258 and @xmath259 are defined in theorem 3.7 , ( d ) if @xmath362 , then * @xmath358 * and hence @xmath198 is variable - selection consistent , i.e. , * @xmath359*. + the proofs for theorems 3.7 and 3.8 are provided in section 6.6 .",
    "the proof for theorems 3.7 and 3.8 hinges on an intermediate result that shows the `` mutual incoherence '' assumption on @xmath363 $ ] ( the population version of @xmath364 ) guarantees that , with high probability , analogous conditions hold for the estimated quantity @xmath207 , formed by the fitted regressors from the first - stage regression .",
    "this result is established in lemma 6.5 in section 6.5 .",
    "the proofs for theorems 3.7 and 3.8 are based on a construction called primal - dual witness ( pdw ) method developed by wainwright ( 2009 ) ( also see wainwright , 2014 ) .",
    "this method constructs a pair @xmath365 .",
    "when this procedure succeeds , the constructed pair is primal - dual optimal , and acts as a witness for the fact that the lasso has a unique optimal solution with the correct signed support .",
    "the procedure is described in the following .    1 .",
    "set @xmath366 .",
    "2 .   obtain @xmath367 by solving the oracle subproblem @xmath368 and choose @xmath369 , where @xmath370 denotes the set of subgradients at @xmath371 for the function @xmath372 .",
    "3 .   solve for @xmath373 via the zero - subgradient equation @xmath374 and check whether or not the _ strict dual feasibility _",
    "condition @xmath375 holds .",
    "theorems 3.7 and 3.8 include four parts .",
    "part ( a ) guarantees the uniqueness of the optimal solution of the two - stage lasso procedure , @xmath198 ( from the proofs for theorems 3.7 and 3.8 , we have that @xmath376 where @xmath371 is the solution obtained in step 2 of the pdw construction above ) . based on this uniqueness claim , one can then talk unambiguously about the support of the two - stage lasso estimate .",
    "part ( b ) guarantees that the lasso does not falsely include elements that are not in the support of @xmath8 .",
    "part ( c ) ensures that @xmath377 is uniformly close to @xmath378 in the @xmath379norm in the @xmath379bound in theorems 3.7 and 3.8 seems to be extra and removing it may require a more involved analysis in future work . ] .",
    "notice that the @xmath379bound in part ( c ) of theorem 3.8 is improved by a factor of @xmath307 upon that in part ( c ) of theorem 3.7 if the first term in the braces dominates the second one .",
    "also , the scaling of the sample size required in theorem 3.8 is improved upon that in theorem 3.7 .",
    "similar observations were made earlier when we compared the bound in theorem 3.2 with the bound in theorem 3.3 ( or , the bound in theorem 3.5 with the bound in theorem 3.6 ) .",
    "again , these observations are attributed to that the additional assumption of the first - stage estimators correctly selecting the non - zero coefficients ( assumption 3.6 ) is imposed in theorem 3.8 but not in theorem 3.7 .",
    "recall earlier comparison between theorem 3.2 and theorem 3.3 in the scaling of the required sample size .",
    "a similar comparison can be made between theorem 3.7 and theorem 3.8 . under the assumption that the first - stage estimators correctly select the non - zero coefficients with high probability ( assumption 3.6 ) ,",
    "the scaling of the sample size required * * in theorem 3.8 is guaranteed to be no greater ( and in some cases strictly smaller ) than that in theorem 3.7 .",
    "for instance , if @xmath298 , then by letting @xmath299 , @xmath380 in this example , theorem 3.7 suggests that the choice of sample size needs to satisfy @xmath381 and @xmath382 while theorem 3.8 suggests that the choice of sample size only needs to satisfy @xmath383 and @xmath384 .",
    "however , as discussed previously , it is possible that the error bound and scaling of the sample size required in theorem 3.7 is suboptimal .",
    "section 5 provides a heuristic argument that may potentially improve the bound in theorem 3.7 when the first - stage estimates fail to satisfy the exact sparsity constraint specified by the @xmath102- ball .",
    "the last claim is a consequence of this uniform norm bound : as long as the minimum value of @xmath385 over @xmath342 is not too small , then the two - stage lasso does not falsely exclude elements that are in the support of @xmath8 with high probability .",
    "the minimum value requirement of @xmath385 over @xmath342 is comparable to the so - called `` beta - min '' condition in bhlmann and van de geer ( 2011 ) . combining the claims from ( b ) and ( d )",
    ", the two - stage lasso is variable - selection consistent with high probability .",
    "in this section , simulations are conducted to gain insight on the finite sample performance of the regularized two - stage estimators .",
    "i consider the triangular simultaneous equations model ( 1 ) and ( 2 ) from section 1 where @xmath386 for all @xmath21 , @xmath387 are @xmath388 , and @xmath389 have the following joint normal distribution @xmath390 the matrix @xmath391 is a @xmath392 matrix of normal random variables with identical variances @xmath393 , and @xmath394 is independent of @xmath395 for all @xmath21 . with this setup ,",
    "i simulate 1000 sets of @xmath396 where @xmath0 is the sample size ( i.e. , the number of data points ) in each set , and perform 14 monte carlo simulation experiments constructed from various combinations of model parameters ( @xmath257 , @xmath1 , @xmath15 , @xmath2 , @xmath8 , @xmath276 , and @xmath275 ) , the design of @xmath17 , the random matrix formed by the instrumental variables , as well as the types of first - stage and second - stage estimators employed ( lasso vs. ols ) . for each replication",
    "@xmath397 , i compute the estimates @xmath398 of the main - equation parameters @xmath8 , @xmath4errors of these estimates , @xmath399 , and selection percentages of @xmath398 ( computed by the number of the elements in @xmath398 sharing the same sign as their corresponding elements in @xmath8 , divided by the total number of elements in @xmath8 ) .",
    "table 4.1 displays the designs of the 14 experiments . for experiment 1 and experiments 3 - 14",
    ", i set the number of parameters in each first - stage equation @xmath400 , the number of parameters in the main equation @xmath401 , the number of non - zero parameters in each first - stage equation @xmath402 , the number of non - zero parameters in the main equation @xmath403 . also , choose @xmath404 , @xmath405 for all @xmath406 ; and @xmath407 , @xmath408 . for convenience , in the following discussion",
    ", i will refer to those non - zero parameters as `` relevant '' parameters and those zero parameters as `` irrelevant '' parameters .",
    "experiment 2 sets @xmath409 , @xmath410 , @xmath404 , and @xmath407 .",
    "the motivations of these experiments are explained in the following discussion . +   +   +       & & & & & & & & & & & & & &   +   & & & & & & & & & & & & & &   +   & & & & & & & & & & & & & &   +   +   & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 +   & 100 & 4 & na & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 +   & 4 & 4 & na & 4 & 4 & 4 & 4 & 4 & 4 & 4 & 4 & 4 & 4 & 4 +   & 50 & 5 & 50 & 50 & 50 & 50 & 50 & 50 & 50 & 50 & 50 & 50 & 50 & 50 +   & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 & 5 +   & * 1 * & * 1 * & * 1 * & * 1 * & * 1 * & * 1 * & * 1 * & * 1 * & * 1 * & * 1 * & * 1 * & * 1 * & * 1 * & * * 0.01 * * +   & * 0 * & na & * 0 * & * 0 * & * 0 * & * 0 * & * 0 * & * 0 * & * 0 * & * 0 * & * 0 * & * 0 * & * 0 * & * * 0 * * +   & * 1 * & * 1 * & na & * 1 * & * 1 * & * 1 * & * 1 * & * 1 * & * 1 * & * 1 * & * 1 * & * 1 * & * 1 * & * * 1 * * +   & * 0 * & na & na & * 0 * & * 0 * & * 0 * & * 0 * & * 0 * & * 0 * & * 0 * & * 0 * & * 0 * & * 0 * & * * 0 * * +   & 0.4 & 0.4 & 0.4 & 0.4 & 0.4 & 0.4 & 1 & 0.4 & 0.4 & 0.4 & 1 & 0.4 & 0.4 & 0.4 +   & 0.4 & 0.4 & na & 0.4 & 0.4 & 0.4 & 0.4 & 1 & 0.4 & 0.4 & 0.4 & 1 & 0.4 & 0.4 +   & 1 & 1 & na & 1 & 1 & 1 & 1 & 1 & 0.4 & 1 & 1 & 1 & 0.4 & 1 +   & no & no & na & no & no & no & no & no & no & yes & yes & yes & yes &",
    "no +   & lasso & ols & na & ols & lasso & ols & lasso & lasso & lasso & lasso & lasso & lasso & lasso & lasso +   & lasso & ols & lasso & lasso & ols & ols & lasso & lasso & lasso & lasso & lasso & lasso & lasso & lasso +     +    the baseline experiment ( experiment 1 ) applies the two - stage lasso procedure to the endogenous sparse linear model with a triangular simultaneous equations structure ( 1 ) and ( 2 ) . for each data point",
    "@xmath411 , the instruments @xmath391 is a @xmath392 matrix of independent standard normal random variables . as",
    "a benchmark for experiment 1 , experiment 2 concerns the classical 2sls procedure when both stage equations are in the low - dimensional setting and the supports of the true parameters in both stages are known _ a priori_. as another benchmark for experiment 1 , experiment 3 applies a one - step lasso procedure ( without instrumenting the endogenous regressors ) to the same main equation model ( 1 ) as in experiment 1 .",
    "experiments 4 - 6 concern , in a relatively large sample size setting with sparsity , the performance of alternative `` partially '' regularized or non - regularized estimators : first - stage - ols - second - stage - lasso ( experiment 4 ) , first - stage - lasso - second - stage - ols ( experiment 5 ) , and first - stage - ols - second - stage - ols ( experiment 6 ) .",
    "experiments 7 - 14 return to the two - stage lasso procedure with changes applied to the model parameters that generate the data .",
    "experiment 7 ( experiment 8) increases the standard deviation of the `` noise '' in the main equation , @xmath276 ( respectively , the standard deviation of the `` noise '' in the first - stage equations , @xmath275 ) ; experiment 9 reduces @xmath393 , the standard deviation of the `` signal '' , i.e. , the instrumental variables ; experiment 10 introduces correlations between the rows of the design matrix @xmath391 .",
    "notice that each row of @xmath412 is associated with each of the endogenous regressors and the row - wise correlation in @xmath391 hence introduces correlations between the `` purged '' regressors @xmath347 and @xmath413 for all @xmath29 .",
    "the level of the correlation is set to @xmath414 , i.e. , @xmath415 for @xmath29 and @xmath416 ( notice that we still have @xmath417 for @xmath418 and @xmath21 ; i.e. , there is no column - wise correlation in @xmath391 ) .",
    "experiment 11 ( experiment 12 ) increases the `` noise '' level in the main equation ( respectively , the `` noise '' level in the first - stage equations ) and introduces the correlations between the `` purged '' regressors @xmath347 and @xmath413 for all @xmath29 simultaneously .",
    "experiment 13 reduces the `` signal '' level of the instrumental variables and introduces the correlations between the `` purged '' regressors @xmath347 and @xmath413 for all @xmath29 simultaneously .",
    "experiment 14 reduces the magnitude of @xmath419 from @xmath420 to @xmath421 .",
    "the tuning parameters @xmath422 in the first - stage lasso estimation ( in experiments 1 , 5 , 7 - 14 ) are chosen according to the standard lasso theory of high - dimensional estimation techniques ( e.g. , bickel , 2009 ) ; in particular , @xmath423 . the tuning parameters",
    "@xmath424 in the second - stage lasso estimation ( in experiments 1 , 3 , 4 , 7 - 14 ) are chosen according to the scaling condition in theorem 3.3 ; in particular , @xmath425 in experiments 1 , 3 , 4 , 7 - 13 and @xmath426 in experiment 14 .",
    "the value of @xmath424 in experiments 1 , 3 , 4 , 7 - 13 exceeds the value of @xmath424 in experiments 14 by a factor of @xmath427 .",
    "this adjustment reflects the fact that the non - zero parameters @xmath428 in experiments 1 , 3 , 4 , 7 - 13 exceed the non - zero parameters @xmath429 in experiment 14 by a factor of @xmath427 .",
    "figure 4.1a plots ( in ascending values ) the 1000 estimates of @xmath430 when the sample size @xmath431 .",
    "the estimates of other `` relevant '' main - equation parameters behave similarly as the estimates of @xmath430 .",
    "figure 4.1b plots ( in ascending values ) the 1000 estimates of @xmath432 when the sample size @xmath431 .",
    "the estimates of other `` irrelevant '' main - equation parameters behave similarly as the estimates of @xmath432 .",
    "the sample size 47 satisfies the scaling condition in theorem 3.3 . with the choice of @xmath400 , @xmath402 , @xmath401 , @xmath403 in experiments 1 and 3 , the sample size @xmath431 represents a high - dimensional setting with sparsity .",
    "figure 4.1c ( figure 4.1d ) is similar to figure 4.1a ( figure 4.1b ) except that the sample size @xmath433 .",
    "with the 1000 estimates of the main - equation parameters from experiments 1 - 3 , table 4.2 shows the mean of the @xmath4errors of these estimates ( computed as @xmath434 ) , the mean of the selection percentages ( computed in a similar fashion as the mean of the @xmath4errors of the estimates of @xmath8 ) , the mean of the squared @xmath4errors ( i.e. , the _ sample _ _ mean squared error _ , smse , computed as @xmath435 ) , and the sample squared bias @xmath436 ( where @xmath437 for @xmath406 ) . to provide a sense of",
    "how well the first - stage estimates behave , table 4.2 also displays the `` averaged '' mean of the @xmath4errors of the first - stage estimates ( computed as @xmath438 ) , the `` averaged '' mean of the selection percentages of the first - stage estimates ( computed in a similar fashion as the `` averaged '' mean of the @xmath4errors of the first - stage estimates ) , the `` averaged '' mean of the squared @xmath4errors ( i.e. , the `` averaged '' smse , computed as @xmath439 ) , and the `` averaged '' sample squared bias @xmath440 ( where @xmath441 for @xmath406 and @xmath442 ) .    compared to the two - stage lasso procedure , in estimating the `` relevant '' main - equation parameters with both sample sizes @xmath431 and @xmath433 , figures 4.1a and 4.1c show that the classical 2sls procedure where the supports of the true parameters in both stages are known _ a priori _ produces larger estimates while the one - step lasso procedure ( without instrumenting the endogenous regressors ) produces smaller estimates .",
    "the two - stage lasso outperforms the classical 2sls above the @xmath443 percentile of the estimates while underestimates the `` relevant '' main - equation parameters below the @xmath443 percentile relative to the classical 2sls procedure . the one - step lasso procedure ( without instrumenting the endogenous regressors )",
    "produces the poorest estimates of the `` relevant '' main - equation parameters .",
    "the mean @xmath444 of the 1000 estimates @xmath445 from the two - stage lasso is @xmath446 ( respectively , @xmath447 from the classical 2sls and @xmath448 from the one - step lasso ) when @xmath431 and @xmath449 ( respectively , @xmath447 from the classical 2sls and @xmath450 from the one - step lasso ) when @xmath433 .",
    "the fact that the two - stage lasso yields smaller estimates of the `` relevant '' main - equation parameters relative to the classical 2sls for both sample sizes is most likely due to the shrinkage effect from the @xmath30penalization in the second - stage estimation of the two - stage lasso procedure .    in estimating the `` irrelevant '' main - equation parameters , the estimates of @xmath451 from both the two - stage lasso and the one - step lasso are exactly @xmath33 at the @xmath452 percentile , the median , and the @xmath453percentile when @xmath431 and @xmath433 .",
    "the mean statistics of the estimates of @xmath451 range from @xmath454 ( @xmath455 ) to @xmath456 ( @xmath457 ) when @xmath431 , and @xmath458 ( @xmath33 ) to @xmath459 ( @xmath460 ) when @xmath433 for the two - stage lasso ( respectively , the one - step lasso ) .",
    "table 4.2 shows that the selection percentages of the main - equation estimates from the two - stage lasso and the one - step lasso are high for the designs considered .",
    "figures 4.1b and 4.1d show that , in estimating the `` irrelevant '' main - equation parameters , the one - step lasso performs slightly better relative to the two - stage lasso procedure below the @xmath461 percentile and above the @xmath462 percentile .",
    "in terms of estimation errors and sample bias , from table 4.2 we see that the mean of the @xmath4errors of the estimates @xmath198 of @xmath8 ( or the `` averaged '' mean of the @xmath4errors of the first - stage estimates ) from the two - stage lasso are greater than those of @xmath463 ( respectively , of the first - stage estimates ) from the classical 2sls procedure for both @xmath431 and @xmath433 . as @xmath0 increases ,",
    "the mean of the @xmath4errors of @xmath198 and the mean of the @xmath4errors of @xmath463 become very close to each other as in the case when @xmath433 . also , the sample bias of @xmath198 ( or , the `` averaged '' sample bias of the first - stage estimates ) from the two - stage lasso are greater by a magnitude of @xmath464 ( respectively , @xmath465 ) than those of @xmath463 ( respectively , of the first - stage estimates ) from the classical 2sls procedure for both sample sizes .    for more investigation on how the @xmath4error and sample bias of @xmath198 compare to those of @xmath463 ,",
    "i have also considered designs where @xmath276 and/or @xmath275 are increased or decreased while everything else in experiments 1 and 2 remains the same . in these modified designs except for those with very large values of @xmath276 under @xmath431 , the mean of the @xmath4errors of @xmath198 are generally greater than those of @xmath463 .",
    "the sample bias of @xmath198 are consistently greater by a magnitude of @xmath466 than those of @xmath463 for both sample sizes .",
    "this suggests that the shrinkage effect from the @xmath30penalization in both the first and second stage estimations of the two - stage lasso procedure might have made its bias term converge to zero at a slower rate relative to the classical 2sls for the designs considered here . whether this conjecture holds true for general designs is an interesting question for further research .",
    "compared to the two - stage lasso and the classical 2sls , the one - step lasso procedure without instrumenting the endogenous regressors yields the largest @xmath4errors as well as sample bias of the main - equation estimates for both @xmath431 and @xmath433 , which is expected .",
    "finally notice that the @xmath4errors ( and the sample bias ) shrink as the sample size increases .",
    "@xmath467 being proportional to @xmath468 is a known fact in low - dimensional settings . from section 3.1",
    ", we also have that the upper bounds for @xmath192 are proportional to @xmath468 up to factors involving @xmath469 , @xmath240 , @xmath1 , and @xmath2 . +   +   +    [ cols=\"^ \" , ]      +   +    lllll|c|c|c|c|c|c|c|c|c| & & & & &   +   & & & & & mean &   +   & & & & & exp .",
    "# & 1 & 7 & 8 & 9 & 10 & 11 & 12 & 13 +   & & & & & @xmath470stage select % & 97.3 & 94.3 & 93.8 & 97.3 & 87.0 & 85.4 & 87.8 & 87.1 +   & & & & & @xmath461-stage @xmath4err & 0.288 & 0.422 & 0.376 & 0.497 & 0.365 & 0.557 & 0.471 & 0.626 +   & & & & & @xmath471stage select % & 97.7 & 97.7 & 90.8 & 97.7 & 97.7 & 97.7 & 90.8 & 97.7 +   & & & & & @xmath471stage @xmath4err & 0.349 & 0.349 & 0.789 & 0.552 & 0.352 & 0.352 & 0.793 & 0.557 +     +   +   + in the final experiment 14 where @xmath472 ( as opposed to @xmath473 in the previous experiments ) , based on the estimates obtained from the two - stage lasso procedure , i count the number of occurrences that each estimate @xmath474 equals exactly @xmath33 , respectively , over the 1000 replications ( table 4.5 ) . because the `` relevant '' main - equation parameters are reduced by a factor of 100 , it is clearly more difficult for the two - stage lasso procedure to distinguish the `` relevant '' coefficients from the `` irrelevant '' coefficients and table 4.5 verifies this .",
    "recall in experiments 10 - 13 , by introducing correlations between the `` purged '' regressors @xmath347 and @xmath413 for all @xmath29 , the estimates of the `` irrelevant '' main - equation parameters become worse . on the other hand , making the `` relevant '' main - equation parameters sufficiently smaller results in worse estimates of the `` relevant '' main - equation parameters .",
    "this observation confirms part ( d ) of theorems 3.7 and 3.8 ; i.e. , the violation of the `` beta - min '' condition can lead the lasso to mistake the `` relevant '' coefficients for the `` irrelevant '' coefficients . in terms of the @xmath4errors and overall selection percentages , from table 4.5",
    "we see that poorer estimation of the `` relevant '' parameters also results in larger @xmath4errors in experiment 1 exceeds the parameters @xmath429 in experiments 14 by a factor of @xmath427 , the @xmath4error in experiment 14 is adjusted as @xmath475^{1/2}$ ] .",
    "the unadjusted @xmath4error in experiment 14 is 0.388 . ] and worse selection percentages , as expected .",
    "the significant drop in the overall selection percentages suggests that not only the estimation of the `` relevant '' coefficients becomes less accurate in experiment 14 but also the estimation of the `` irrelevant '' coefficients .",
    "+   +    ccccccc|c||c|c|c|c|c||c|c| & & & & & & &   +   & & & & & & & & & &   +   & & & & & & & & @xmath476 & @xmath477 & @xmath478 & @xmath479 & @xmath480 & &   +   & & & & & & & exp . 1 & 0 & 0 & 0 & 0 & 0 & 97.3 & 0.288 +   & & & & & & & exp . 14 & 187 & 187 & 218 & 194 & 193 & 57.7 & 11.5 +     +",
    "this paper has explored the validity of the two - stage estimation procedure for sparse linear models in high - dimensional settings with possibly many endogenous regressors . in particular , the number of endogenous regressors in the main equation and the number of instruments in the first - stage equations are permitted to grow with and exceed @xmath0 .",
    "sufficient scaling conditions on the sample size for estimation consistency in @xmath4 norm and variable - selection consistency of the high - dimensional two - stage estimators have been established .",
    "depending on the underlying assumptions that are imposed , the upper bounds on the @xmath4error and the sample size required to obtain these consistency results differ by factors involving the sparsity parameters @xmath1 and/or @xmath2 .",
    "simulations are conducted to gain insight on the finite sample performance of the high - dimensional two - stage estimator . * *    the approach and results of this paper suggest a number of possible extensions including the ones listed in the following , which are left to future research . + * * + * _ revisiting the bound in theorem 3.2_*. as discussed earlier , assumption 3.6 can be interpreted as a sparsity constraint on the first - stage estimate * @xmath184 * for * @xmath21 * , in terms of the @xmath102- ball , given by @xmath245 the sparsity constraint ( namely , the selection consistency ) regarding these first - stage estimates is guaranteed under some conditions that may be violated in many problems .",
    "it seems possible to extend assumption 3.6 to the following _ approximate sparsity _",
    "constraint on the first - stage estimates * * in terms of @xmath3- balls , given by @xmath481 if the first - stage estimation employs the lasso or dantzig selector or some other procedures with the @xmath30type of regularization , then we are guaranteed to have @xmath482 for every @xmath21 .",
    "depending on the type of sparsity assumptions imposed on the first - stage estimates , the statistical error of the high - dimensional two - stage estimator @xmath198 in * * @xmath4**norm * * and the required sample size differ .",
    "an inspection of the proof for theorem 3.2 suggests that the error bound and requirement of the sample size in theorem 3.2 will hold regardless of the sparsity assumption on the first - stage estimates .",
    "* * however , under these special structures that impose a certain decay rate on the ordered entries of the first - stage estimates , the bound and scaling of the required sample size in theorem 3.2 is likely to be suboptimal . * * to obtain sharper results , the proof technique adopted for showing theorem 3.3 seems more appropriate .",
    "* * i give a heuristic truncation argument to illustrate how the proof for theorem 3.3 might be extended to allow the weaker sparsity constraint ( in terms of @xmath30balls ) on the first - stage lasso estimates .",
    "suppose for every @xmath21 , we choose the top @xmath483 coefficients of @xmath184 in absolute value , then the fast decay imposed by the @xmath3- ball condition on @xmath184 arising from the lasso procedure would mean that the remaining @xmath484 coefficients would have relatively little impact . with this intuition , the proof follows as if assumption 3.6 were imposed with the only exception that we also need to take into account the approximation error arising from the the remaining @xmath484 coefficients of @xmath184 .",
    "+ * * + * the * * _ approximate sparsity _ * * case*. it is useful to extend the analysis for the high - dimensional 2sls estimator to the _ approximate sparsity _ case , i.e. , most of the coefficients in the main equation and/or the first - stage equations are too small to matter .",
    "one can have the approximate sparsity assumption in the first - stage equations only ( and assume the main equation parameters are sparse ) , the main equation only ( and assume the first - stage equations parameters are sparse ) or both - stage equations .",
    "when the first - stage equations parameters are approximately sparse , the argument in the proof for theorem 3.2 can still be carried through while the proof for theorem 3.3 is no longer meaningful . + * * + * _ control function approach in high - dimensional settings_*. as an alternative to the `` two - stage '' estimation proposed here , it would be interesting to explore the validity of the high - dimensional two - stage estimators based on the `` control function '' approach in the high - dimensional setting .",
    "when both the first and second - stage equations are in low - dimensional settings ( i.e. , @xmath42 and @xmath43 for all @xmath21 ) and the supports of the true parameters in both stages are known _ a priori _ , the 2sls procedure is algebraically equivalent to a `` control function '' estimator of @xmath8 that includes first - stage residuals @xmath485 as `` control variables '' in the regression of @xmath46 on @xmath7 ( e.g. , garen , 1984 ) .",
    "such algebraic equivalence no longer holds for regularized estimators because the regularization employed destroys the projection algebra .",
    "the extension for the 2sls estimator from low - dimensional settings to high - dimensional settings is somewhat more natural than the extension for the two - stage estimator based on the control function approach .",
    "one question to ask is : under what conditions can we translate the sparsity or approximate sparsity assumption on the coefficients @xmath8 in the triangular simultaneous equations model ( 1 ) and ( 2 ) to the sparsity or approximate sparsity assumption on the coefficients @xmath8 and @xmath486 in the model @xmath487 where @xmath488 ?",
    "a simple sufficient condition for such a translation is to impose the joint normality assumption of the error terms @xmath6 and @xmath489 .",
    "then , by the property of multivariate normal distributions , we have @xmath490 if we further assume only a few of the correlation coefficients @xmath491 ( associated with the covariance matrix @xmath492 ) are non - zero or most of these correlation coefficients are too small to matter , the sparsity or approximate sparsity can be carried to the model @xmath487 .",
    "then , we can obtain consistent estimates of @xmath493 , @xmath494 , from the first - stage regression by either a standard least square estimator when the first - stage regression concerns a small number of regressors relative to @xmath0 , or a least square estimator with @xmath3- regularization ( the lasso or dantzig selector ) when the first - stage regression concerns a large number of regressors relative to @xmath0 , and then apply a lasso technique in the second stage as follows @xmath495 the statistical properties of @xmath496 can be analyzed in the same way as those of @xmath198 .",
    "how this argument can be extended to non - gaussian error settings is an interesting question for future research .",
    "+   + * _ minimax lower bounds for the high - dimensional linear models with endogeneity_*. it would be worthwhile to establish the minimax lower bounds on the parameters in the main equation for the linear models in high - dimensional settings with endogeneity . in particular , the goal is to derive lower bounds on the estimation error achievable by any estimator , regardless of its computational complexity .",
    "obtaining lower bounds of this type is useful because on one hand , if the lower bound matches the upper bound up to some constant factors , then there is no need to search for estimators with a lower statistical error ( although it might still be useful to study estimators with lower computational costs ) . on the other hand ,",
    "if the lower bound does not match the best known upper bounds , then it is worthwhile to search for new estimators that potentially achieve the lower bound .",
    "to the best of my knowledge , in econometric literature , there has been only limited attention given to the minimax rates of linear models with endogeneity in high - dimensional settings .",
    "for technical simplifications , in the following proofs , i assume without loss of generality that the first moment of @xmath497 is zero for all @xmath19 ( if it is not the case , we can simply subtract their population mean ) .",
    "also , for notational simplicity , assume @xmath386 for all @xmath21 ; additionally , as in most high - dimensional statistics literature , i assume the regime of interest is @xmath498 and @xmath499 ( except for corollary 3.4 where @xmath500 is assumed ) . the modification to allow @xmath129 or @xmath501 or @xmath502 for some @xmath41 and @xmath503 is trivial . also , as a general rule for the proofs , @xmath504 constants denote positive constants that do not involve @xmath0 , @xmath15 , @xmath257 , @xmath1 and @xmath2 but possibly the sub - gaussian parameters defined in assumptions 3.2 - 3.4 ; @xmath505 constants denote universal positive constants that are independent of both @xmath0 , @xmath15 , @xmath257 , @xmath1 and @xmath2 as well as the sub - gaussian parameters .",
    "the specific values of these constants may change from place to place .",
    "* proof*. first , write @xmath506 where @xmath507 . define @xmath508 and the lagrangian @xmath509 .",
    "since @xmath198 is optimal , we have @xmath510 some algebraic manipulation of the _ basic inequality _ above yields @xmath511 where the last inequality holds as long as @xmath512 .",
    "consequently , @xmath513 .",
    "note that we also have @xmath514 since we assume in lemma 3.1 that the random matrix @xmath515 satisfies the re condition ( 3 ) with @xmath155 , we have @xmath516      as discussed in section 3 , the @xmath82-consistency of @xmath198 requires verifications of two conditions : ( i ) @xmath515 satisfies the re condition ( 3 ) with @xmath155 , and ( ii ) the term @xmath517 with high probability .",
    "this is done via lemmas 6.1 and 6.2 . + * * + * lemma 6.1 * ( re condition ) : under assumptions 1.1 , 3.1 , 3.3 , 3.5a and the condition @xmath518 we have , for some universal constants @xmath505 , @xmath152 , and @xmath153 , @xmath519 with probability at least @xmath520 , where @xmath521 * * + * proof .",
    "* we have @xmath522 which implies * @xmath523 * to bound the term @xmath524 , let us first fix @xmath525 * * and bound the @xmath525 element of the matrix * * @xmath526 . notice that @xmath527 under assumptions 3.2 and 3.3 , we have that the random matrix * @xmath212 * is * * a sub - gaussian with parameters at most @xmath213 for all @xmath21 , and @xmath528 is a sub - gaussian vector with a parameter at most @xmath277 for every @xmath529 .",
    "therefore , by lemma 6.8 and an application of union bound , * * we have @xmath530\\leq6p^{2}d\\exp(-cn\\min\\{\\frac{t^{2}}{\\sigma_{x^{*}}^{2}\\sigma_{z}^{2}},\\,\\frac{t}{\\sigma_{x^{*}}\\sigma_{z}}\\}),\\ ] ] so as long as @xmath531 ,    @xmath532\\leq c_{1}\\exp(-c_{2}\\log\\max(p,\\ , d)),\\ ] ] where * * @xmath533 , @xmath152 , and @xmath153 are some universal constants . under assumption 3.5a , if @xmath531 , then , * @xmath534 * with probability at least @xmath520 .    to bound the term @xmath535 , again let us first fix @xmath525 * * and bound the @xmath525 element of the matrix * * @xmath536 . using the similar argument as above ,",
    "if @xmath531 , we have , @xmath537 with probability at least @xmath520 .",
    "putting everything together , under the condition @xmath538 and applying lemma 6.10 with @xmath539 , we have @xmath540 with probability at least @xmath541 ( given @xmath542 and @xmath134 is the regime of our interests ) , where @xmath543 , @xmath544 , and @xmath545 . * * notice the last inequality can be written in the form @xmath546 where @xmath547 , and the second inequality follows since @xmath531 .",
    "@xmath170 +   + in proving lemma 3.1 , upon our choice of @xmath162 , we have shown * @xmath548 * which implies @xmath549 . therefore , if we have the scaling @xmath550 so that @xmath551 then , * @xmath552 * provided @xmath275 , @xmath553 , @xmath277 , @xmath554 , and @xmath555 are bounded from above while @xmath278 and @xmath279 are bounded away from @xmath33 .",
    "the above inequality implies re ( 3 ) . * * + * * + * lemma 6.2 * ( upper bound on @xmath202 ) : under assumptions 1.1 , 3.1 - 3.3 , 3.5a , and the condition @xmath556 , we have @xmath557 where @xmath558 with probability at least @xmath559 for some universal constants @xmath152 and @xmath153 .",
    "+   + * proof*. we have @xmath560\\\\   & = & \\frac{1}{n}x^{*t}\\left[(x^{*}-\\hat{x})\\beta^{*}+\\boldsymbol{\\eta}\\beta^{*}+\\epsilon\\right]+\\frac{1}{n}(\\hat{x}-x^{*})^{t}\\left[(x^{*}-\\hat{x})\\beta^{*}+\\boldsymbol{\\eta}\\beta^{*}+\\epsilon\\right].\\end{aligned}\\ ] ] hence , @xmath561 we need to bound each of the terms on the right - hand - side of the above inequality .",
    "let us first bound @xmath562 .",
    "we have @xmath563.\\ ] ] for any @xmath529 , we have @xmath564 in proving lemma 6.1 , under the condition @xmath565 , we have , @xmath566 with probability at least @xmath520 .",
    "therefore , @xmath567 the term @xmath568 can be bounded using a similar argument and we have , @xmath569 with probability at least @xmath520 . for the term @xmath570 , we have @xmath571 with probability at least @xmath572 .",
    "the last inequality follows from lemma 6.8 and assumption 1.1 that @xmath573 for all @xmath574 as well as assumption 3.2 that @xmath209 is an _",
    "_ zero - mean sub - gaussian vector with parameter @xmath211 for @xmath21 , and the random matrix * @xmath212 * is * * sub - gaussian with parameters @xmath213 for @xmath21 . for the term @xmath575",
    ", we have , @xmath576 with probability at least @xmath520 . again",
    ", the last inequality follows from lemma 6.8 and assumption 1.1 that @xmath573 for all @xmath574 as well as assumption 3.2 .          under the conditions @xmath582 and @xmath583 ( the @xmath2 factor in the choice of @xmath162 comes from the simple inequality @xmath584 by exploring the sparsity of @xmath8 ) , combining lemmas 3.1 , 6.1 , and 6.2",
    ", we have @xmath262 where @xmath263 with probability at least @xmath559 for some universal positive constants @xmath152 and @xmath153 , which proves theorem 3.2 .",
    "@xmath170      again , we verify the conditions : i ) @xmath515 satisfies the re condition ( 3 ) with @xmath155 , and ( ii ) the term @xmath585 with high probability .",
    "this is done via lemmas 6.3 and 6.4 . + * * + * lemma 6.3 * ( re condition ) : let @xmath586 $ ] . under assumptions 1.1 , 3.1 , 3.3 , 3.4 , 3.5b , 3.6 , and the condition @xmath587 , we have , for some universal constants @xmath505 , @xmath588 , @xmath152 , and @xmath153 , @xmath589 with probability at least @xmath520 , where @xmath590v^{0}\\right|\\right\\ } , \\\\",
    "b_{3 } & = & \\max\\left\\ { \\sigma_{w}^{2},\\;\\sup_{v\\in\\mathbb{k}(2s,\\ , p)\\times\\mathbb{k}^{2}(k_{1},\\ , d_{1})\\times ... \\times\\mathbb{k}^{2}(k_{1},\\ , d_{p})}\\left|v^{0t}\\left[\\mathbb{e}(v^{j^{'}}\\mathbf{z}_{1j^{'}}^{t}\\mathbf{z}_{1j}v^{j})\\right]v^{0}\\right|\\right\\ } .\\end{aligned}\\ ] ] * * + * proof*. again , @xmath591 to bound the above terms , i apply a discretization argument motivated by the idea in loh and wainwright ( 2012 ) .",
    "this type of argument is often used in statistical problems requiring manipulating and controlling collections of random variables indexed by sets with an infinite number of elements . for the particular problem in this paper ,",
    "i work with the product space @xmath592 and @xmath593 . for @xmath594 and @xmath595 ,",
    "recall the notation @xmath596 . given @xmath597 and @xmath598 ,",
    "define @xmath599 and @xmath600 .",
    "note that @xmath601 and @xmath602 with @xmath603 $ ] .",
    "the choice of @xmath103 is explained in the proof for lemma 6.10 . if @xmath604 is a @xmath605-cover of @xmath606",
    "( @xmath607 is a @xmath605-cover of @xmath608 ) , for every @xmath609 ( @xmath610 ) , we can find some @xmath611 ( @xmath612 ) such that @xmath613 ( @xmath614 ) , where @xmath615 ( respectively , @xmath616 ) . by ledoux and talagrand ( 1991 )",
    ", we can construct @xmath617 with @xmath618 and @xmath619 .",
    "therefore , for @xmath620 , there is some @xmath608 and @xmath612 such that @xmath621 with @xmath614 .",
    "recall for the @xmath525 element of the matrix * * @xmath526 , we have @xmath622 let @xmath623 .",
    "notice that , under assumptions 3.5b and 3.6 , @xmath624 and @xmath625 for every @xmath21 .",
    "define @xmath626 and hence , @xmath627 .",
    "therefore , there is some @xmath606 with @xmath628 and @xmath611 ( where @xmath604 is a @xmath605-cover of @xmath606 ) such that @xmath629 with @xmath613 .",
    "denote a matrix @xmath81 by @xmath630 $ ] , where the @xmath525 element of @xmath81 is @xmath631 .",
    "define @xmath632 . hence , @xmath633 @xmath634v^{0}\\right|\\\\   & \\leq & b_{1}^{-1}\\sqrt{\\frac{k_{1}\\log\\max(p,\\ , d)}{n}}\\{\\max_{i^{'},\\ , i}\\left|t_{i^{'}}^{0t}\\left[\\frac{1}{n}x_{j^{'}}^{*t}\\mathbf{z}_{j}t_{i}^{j}-\\mathbb{e}(x_{1j^{'}}^{*}\\mathbf{z}_{1j}t_{i}^{j})\\right]t_{i^{'}}^{0}\\right|\\\\   &   & + \\sup_{v\\in s_{v}}\\left|t_{i^{'}}^{0t}\\left[\\frac{1}{n}x_{j^{'}}^{*t}\\mathbf{z}_{j}\\triangle v^{j}-\\mathbb{e}(x_{1j^{'}}^{*}\\mathbf{z}_{1j}\\triangle v^{j})\\right]t_{i^{'}}^{0}\\right|+\\sup_{v\\in s_{v}}2\\left|\\triangle v^{0t}\\left[\\frac{1}{n}x_{j^{'}}^{*t}\\mathbf{z}_{j}t_{i}^{j}-\\mathbb{e}(x_{1j^{'}}^{*}\\mathbf{z}_{1j}t_{i}^{j})\\right]t_{i^{'}}^{0}\\right|\\\\   &   & + \\sup_{v\\in s_{v}}2\\left|\\triangle v^{0t}\\left[\\frac{1}{n}x_{j^{'}}^{*t}\\mathbf{z}_{j}\\triangle v^{j}-\\mathbb{e}(x_{1j^{'}}^{*}\\mathbf{z}_{1j}\\triangle v^{j})\\right]t_{i^{'}}^{0}\\right|+\\sup_{v\\in s_{v}}\\left|\\triangle v^{0t}\\left[\\frac{1}{n}x_{j^{'}}^{*t}\\mathbf{z}_{j}t_{i}^{j}-\\mathbb{e}(x_{1j^{'}}^{*}\\mathbf{z}_{1j}t_{i}^{j})\\right]\\triangle v^{0}\\right|\\\\   &   & + \\sup_{v\\in s_{v}}\\left|\\triangle v^{0t}\\left[\\frac{1}{n}x_{j^{'}}^{*t}\\mathbf{z}_{j}\\triangle v^{j}-\\mathbb{e}(x_{1j^{'}}^{*}\\mathbf{z}_{1j}\\triangle",
    "v^{j})\\right]\\triangle v^{0}\\right|\\}\\\\   & \\leq & b_{1}^{-1}\\sqrt{\\frac{k_{1}\\log\\max(p,\\ , d)}{n}}\\{\\max_{i^{'},\\ , i}\\left|t_{i^{'}}^{0t}\\left[\\frac{1}{n}x_{j^{'}}^{*t}\\mathbf{z}_{j}t_{i}^{j}-\\mathbb{e}(x_{1j^{'}}^{*}\\mathbf{z}_{1j}t_{i}^{j})\\right]t_{i^{'}}^{0}\\right|\\\\   &   & + \\sup_{v\\in s_{v}}\\frac{1}{9}\\left|v^{0t}\\left[\\frac{1}{n}x_{j^{'}}^{*t}\\mathbf{z}_{j}v^{j}-\\mathbb{e}(x_{1j^{'}}^{*}\\mathbf{z}_{1j}v^{j})\\right]v^{0}\\right|+\\sup_{v\\in s_{v}}\\frac{2}{9}\\left|v^{0t}\\left[\\frac{1}{n}x_{j^{'}}^{*t}\\mathbf{z}_{j}v^{j}-\\mathbb{e}(x_{1j^{'}}^{*}\\mathbf{z}_{1j}v^{j})\\right]v^{0}\\right|\\\\   &   & + \\sup_{v\\in s_{v}}\\frac{2}{81}\\left|v^{0t}\\left[\\frac{1}{n}x_{j^{'}}^{*t}\\mathbf{z}_{j}v^{j}-\\mathbb{e}(x_{1j^{'}}^{*}\\mathbf{z}_{1j}v^{j})\\right]v^{0}\\right|+\\sup_{v\\in s_{v}}\\frac{1}{81}\\left|v^{0t}\\left[\\frac{1}{n}x_{j^{'}}^{*t}\\mathbf{z}_{j}v^{j}-\\mathbb{e}(x_{1j^{'}}^{*}\\mathbf{z}_{1j}v^{j})\\right]v^{0}\\right|\\\\   &   & + \\sup_{v\\in s_{v}}\\frac{1}{729}\\left|v^{0t}\\left[\\frac{1}{n}x_{j^{'}}^{*t}\\mathbf{z}_{j}v^{j}-\\mathbb{e}(x_{1j^{'}}^{*}\\mathbf{z}_{1j}v^{j})\\right]v^{0}\\right|\\},\\end{aligned}\\ ] ] where the last inequality uses the fact that @xmath635 and @xmath636 .",
    "therefore , @xmath637v^{0}\\right|\\ ] ] @xmath638t_{i^{'}}^{0}\\\\   & \\leq & 2b_{1}^{-1}\\sqrt{\\frac{k_{1}\\log\\max(p,\\ , d)}{n}}\\max_{i^{'},\\ , i}t_{i^{'}}^{0t}\\left[\\frac{1}{n}x_{j^{'}}^{*t}\\mathbf{z}_{j}t_{i}^{j}-\\mathbb{e}(x_{1j^{'}}^{*}\\mathbf{z}_{1j}t_{i}^{j})\\right]t_{i^{'}}^{0}.\\end{aligned}\\ ] ] under assumptions 3.3 and 3.4 , @xmath528 is a sub - gaussian vector with parameter at most @xmath277 for every @xmath529 , and @xmath639 is a sub - gaussian vector with parameter at most @xmath640 .",
    "an application of lemma 6.8 and a union bound yields @xmath641v^{0}-v^{0t}\\left[\\mathbb{e}(x_{1j^{'}}^{*}\\mathbf{z}_{1j}v^{j})\\right]v^{0}\\right|\\geq t\\right)\\leq81^{2sk_{1}}81^{2s}2\\exp(-cn\\min(\\frac{t^{2}}{\\sigma_{x^{*}}^{2}\\sigma_{w}^{2}},\\,\\frac{t}{\\sigma_{x^{*}}\\sigma_{w}})),\\ ] ] where the exponent @xmath642 in @xmath643 uses the fact that there are at most @xmath644 non - zero components in @xmath610 and hence only @xmath644 out of @xmath15 entries of @xmath645 will be multiplied by a non - zero scalar , which leads to a reduction of dimensions .",
    "a second application of a union bound over the @xmath646 choices of @xmath647 and respectively , the @xmath648 choices of @xmath649 yields @xmath650v^{0}-v^{0t}\\left[\\mathbb{e}(x_{1j^{'}}^{*}\\mathbf{z}_{1j}v^{j})\\right]v^{0}\\right|\\geq t\\right)\\ ] ] @xmath651 with the choice of @xmath603 $ ] from the proof for lemma 6.10 and @xmath652 for some universal constant @xmath653 , we have @xmath654\\right|\\ ] ]      with probability at least @xmath656 ( given @xmath542 and @xmath134 is the regime of our interests ) .",
    "therefore , we have @xmath657v^{0}\\right|\\right)b_{1}^{-1}\\sqrt{\\frac{k_{1}\\log\\max(p,\\ , d)}{n}}\\\\   & + & c^{'}b_{1}^{-1}k_{1}^{3/2-r}\\sqrt{\\frac{\\log\\max(p,\\ , d)}{n}}\\sigma_{x^{*}}\\sigma_{w}\\\\   & \\leq & cb_{2}b_{1}^{-1}k_{1}^{3/2-r}\\sqrt{\\frac{\\log\\max(p,\\ , d)}{n}},\\end{aligned}\\ ] ] where @xmath658v^{0}\\right|\\right\\ } $ ] .",
    "notice that the term @xmath659v^{0}\\right|\\ ] ] is bounded above by the spectral norm of the matrix @xmath660 $ ] for some @xmath661 .",
    "the term @xmath662 can be bounded using a similar argument .",
    "in particular , for the @xmath525 element of the matrix * * @xmath536 , we have @xmath663 combining with * @xmath664 * define @xmath665 .",
    "after some tedious algebra , we obtain @xmath666 @xmath667v^{0}\\right|\\\\   & \\leq & b_{1}^{-2}\\frac{k_{1}\\log\\max(p,\\ , d)}{n}\\{\\max_{i^{''},\\ , i^{'},\\ , i}\\left|t_{i^{''}}^{0t}\\left[\\frac{1}{n}t_{i^{'}}^{j^{'}t}\\mathbf{z}_{j^{'}}^{t}\\mathbf{z}_{j}t_{i}^{j}-\\mathbb{e}(t_{i^{'}}^{j^{'}}\\mathbf{z}_{1j^{'}}^{t}\\mathbf{z}_{1j}t_{i}^{j})\\right]t_{i^{''}}^{0}\\right|\\\\   &   & + \\frac{3439}{6561}\\sup_{v\\in s_{v}}\\left|v^{0t}\\left[\\frac{1}{n}v^{j^{'}t}\\mathbf{z}_{j^{'}}^{t}\\mathbf{z}_{j}v^{j}-\\mathbb{e}(v^{j^{'}}\\mathbf{z}_{1j^{'}}^{t}\\mathbf{z}_{1j}v^{j})\\right]v^{0}\\right|\\}.\\end{aligned}\\ ] ] hence , * @xmath668v^{0}\\right|\\ ] ] @xmath669t_{i^{''}}^{0}\\right|\\\\   & \\leq & 3b_{1}^{-2}\\frac{k_{1}\\log\\max(p,\\ , d)}{n}\\max_{i^{''},\\ , i^{'},\\ , i}\\left|t_{i^{''}}^{0t}\\left[\\frac{1}{n}t_{i^{'}}^{j^{'}t}\\mathbf{z}_{j^{'}}^{t}\\mathbf{z}_{j}t_{i}^{j}-\\mathbb{e}(t_{i^{'}}^{j^{'}}\\mathbf{z}_{1j^{'}}^{t}\\mathbf{z}_{1j}t_{i}^{j})\\right]t_{i^{''}}^{0}\\right|.\\end{aligned}\\ ] ] * an application of lemma 6.8 and a sequence of union bounds yields * @xmath670v^{0}-v^{0t}\\left[\\mathbb{e}(v^{j^{'}}\\mathbf{z}_{1j^{'}}^{t}\\mathbf{z}_{1j}v^{j})\\right]v^{0}\\right|\\geq t\\right)\\ ] ] @xmath671 * under the choice of @xmath603 $ ] from the proof for lemma 6.10 and @xmath672 for some universal constant @xmath673 , we have , @xmath674\\right|\\ ] ] @xmath675v^{0}-v^{0t}\\left[\\mathbb{e}(v^{j^{'}}\\mathbf{z}_{1j^{'}}^{t}\\mathbf{z}_{1j}v^{j})\\right]v^{0}\\right|\\right)b_{1}^{-2}\\frac{k_{1}\\log\\max(p,\\ , d)}{n}\\\\   & \\leq & c^{''}b_{1}^{-2}\\frac{k_{1}^{2-r}\\log\\max(p,\\ , d)}{n}\\sigma_{w}^{2}\\end{aligned}\\ ] ] with probability at least @xmath656 ( given @xmath542 and @xmath134 is the regime of our interests ) .",
    "therefore , we have @xmath676 @xmath677v^{0}\\right|\\right)b_{1}^{-2}\\frac{k_{1}\\log\\max(p,\\ , d)}{n}+c^{''}b_{1}^{-2}\\frac{k_{1}^{2-r}\\log\\max(p,\\ , d)}{n}\\sigma_{w}^{2}\\ ] ] @xmath678 where @xmath679v^{0}\\right|\\right\\ } $ ] .",
    "notice that the term @xmath680v^{0}\\right|\\ ] ] is bounded above by the spectral norm of the matrix @xmath681 $ ] for some @xmath682 .",
    "* @xmath685 * implies @xmath686 therefore , applying lemma 6.10 by choosing @xmath603 $ ] , under the condition @xmath687 we have @xmath688 which can be written in the form @xmath589 with probability at least @xmath520 , where @xmath689 , @xmath690 , and @xmath690 are defined in the statement of lemma 6.3.@xmath170    again , recalling in proving lemma 3.1 , upon our choice @xmath162 , we have shown * @xmath548 * and @xmath549 . therefore , if we have the scaling @xmath691}\\max\\left\\ { k_{1}^{3 - 2r}\\log d,\\ , k_{1}^{3 - 2r}\\log p,\\ , k_{1}^{r}k_{2}\\log d,\\ , k_{1}^{r}k_{2}\\log p\\right\\ } } { n}=o(1),\\ ] ] so that @xmath692 then , * @xmath552 * provided @xmath275 , @xmath693 , @xmath277 , @xmath554 , and @xmath555 are bounded from above while @xmath278 and @xmath279 are bounded away from @xmath33 .",
    "the above inequality implies re ( 3 ) .",
    "because the argument for showing lemma 6.1 and that it implies re ( 3 ) also works under the assumptions of lemma 6.3 , we can combine the scaling @xmath694 from the proof for lemma 6.1 with the scaling @xmath695}\\max\\left\\ { k_{1}^{3 - 2r}\\log d,\\ , k_{1}^{3 - 2r}\\log p,\\ , k_{1}^{r}k_{2}\\log d,\\ , k_{1}^{r}k_{2}\\log p\\right\\ } } { n}=o(1)$ ] from above to obtain a more optimal scaling of the required sample size @xmath696}\\max\\left\\ { k_{1}^{3 - 2r}\\log d,\\ , k_{1}^{3 - 2r}\\log p,\\ , k_{1}^{r}k_{2}\\log d,\\ , k_{1}^{r}k_{2}\\log p\\right\\ } \\right\\ } = o(1).\\ ] ] * * + * lemma 6.4 * ( upper bound on @xmath202 ) : under assumptions 1.1 , 3.1 - 3.4 , 3.5b , 3.6 , and the condition @xmath697 , we have @xmath698 with probability at least @xmath559 for some universal constants @xmath152 and @xmath153 , where @xmath699 and @xmath700 are defined in lemma 6.2 . +   + * proof*. recall ( 5 ) from the proof for lemma 6.2 .",
    "let us first bound @xmath562 .",
    "for any @xmath529 , we have @xmath701 in proving lemma 6.3 , we have shown that with a covering subset argument , the @xmath525 element of the matrix * * @xmath526 can be rewritten as follows .",
    "@xmath702 hence , @xmath703 @xmath704 with a similar argument as in the proof for lemma 6.3 , we obtain @xmath705 @xmath706 consequently , under the condition @xmath707 , we have @xmath708\\right|_{\\infty}\\ ] ] @xmath709 with probability at least @xmath520 .",
    "this implies , @xmath710 with probability at least @xmath520 .",
    "notice that by the definition of @xmath711 , @xmath712 to bound the term @xmath535 , recalling from the proof for lemma 6.3 , again with a covering subset argument , the @xmath525 element of the matrix * * @xmath536 can be rewritten as follows +   + @xmath713 +   + with a similar argument as in the proof for lemma 6.3 , we obtain @xmath714 @xmath715 consequently , under the condition @xmath707 , @xmath716\\right|_{\\infty}\\ ] ] @xmath717 with probability at least @xmath520 .",
    "this implies , @xmath718 with probability at least @xmath520 .",
    "notice that by the definition of @xmath711 , @xmath719 with exactly the same discretization argument as above , we can show that , with probability at least @xmath520 , latexmath:[\\ ] ] hence , @xmath951 from the proof for lemma 6.2 , we have latexmath:[\\[\\begin{aligned }                            the proof for part ( ii ) of lemma 6.13 follows from the similar argument for proving part ( i ) except that we bound the terms @xmath956 , @xmath957 , @xmath575 , and @xmath580 using the discretization argument as in the proof for lemma 6.4 .",
    "@xmath170            benkard , c. l. , and p. bajari ( 2005 ) .",
    "`` hedonic price indexes with unobserved product characteristics , and application to personal computers '' .",
    "_ journal of business and economic statistics _",
    ", 23 , 61 - 75 .",
    "belloni , a. , and v. chernozhukov ( 2011b ) .",
    "high dimensional sparse econometric models : an introduction , in : inverse problems and high dimensional estimation , stats in the chteau 2009 , alquier , p. , e. gautier , and g. stoltz , eds . , _ lecture notes in statistics _ , 203 , 127 - 162 , springer , berlin .",
    "negahban , s. , p. ravikumar , m. j. wainwright , and b. yu ( 2012 ) .",
    "`` a unified framework for high - dimensional analysis of m - estimators with decomposable regularizers '' . _ statistical science _ , _ _ 27 , 538 - 557",
    ".                  rosenbaum , m. , and a. b. tsybakov ( 2013 ) .",
    "`` improved matrix uncertainty selector '' , in : from probability to statistics and back : high - dimensional models and processes - a festschrift in honor of jon a. wellner , banerjee , m. et al .",
    "eds , _ ims collections _ , 9 , 276 - 290 , institute of mathematical statistics .",
    "wainwright , j. m. ( 2009 ) .",
    "`` sharp thresholds for high - dimensional and noisy sparsity recovery using @xmath3- constrained quadratic programming ( lasso ) '' .",
    "_ ieee trans .",
    "information theory _",
    ", 55 : 2183 - 2202 ."
  ],
  "abstract_text": [
    "<S> this paper explores the validity of the two - stage estimation procedure for sparse linear models in high - dimensional settings with possibly many endogenous regressors . in particular , the number of endogenous regressors in the main equation and the instruments in the first - stage equations can grow with and exceed the sample size @xmath0 . </S>",
    "<S> the analysis concerns the _ exact sparsity _ case , i.e. , the maximum number of components in the vectors of parameters in the first - stage equations , @xmath1 , and the number of non - zero components in the vector of parameters in the second - stage equation , @xmath2 , are allowed to grow with @xmath0 but slowly compared to @xmath0 . </S>",
    "<S> i consider the high - dimensional version of the two - stage least square estimator where one obtains the fitted regressors from the first - stage regression by a least square estimator with @xmath3- regularization ( the lasso or dantzig selector ) when the first - stage regression concerns a large number of instruments relative to @xmath0 , and then construct a similar estimator using these fitted regressors in the second - stage regression.the main theoretical results of this paper are non - asymptotic bounds from which i establish sufficient scaling conditions on the sample size for estimation consistency in @xmath4norm and variable - selection consistency ( i.e. , the two - stage high - dimensional estimators correctly select the non - zero coefficients in the main equation with high probability ) . </S>",
    "<S> depending on the underlying assumptions that are imposed , the upper bounds on the @xmath4error and the sample size required to obtain these consistency results differ by factors involving @xmath1 and/or @xmath2 . </S>",
    "<S> simulations are conducted to gain insight on the finite sample performance of the high - dimensional two - stage estimator . </S>",
    "<S> +   +   + jel classification : c13 , c31 , c36 + keywords : high - dimensional statistics ; lasso ; sparse linear models ; endogeneity ; two - stage estimation </S>"
  ]
}