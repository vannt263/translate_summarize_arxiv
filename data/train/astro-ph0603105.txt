{
  "article_text": [
    "nearly all signals from astrophysical sources can be represented as electric fields comprised of gaussian noise .",
    "these noiselike signals have zero mean .",
    "all information about the source is contained in the variance of the electric field , and in covariances between different polarizations , positions , times , or frequency ranges .",
    "the intensity , for example , is simply the sum of the variances in 2 basis polarizations .",
    "more generally , all the stokes parameters can be expressed in terms of the variances and covariances of of these 2 polarizations .",
    "similarly , in interferometry , the covariance of electric fields at different positions is the visibility , the fourier transform of source structure . in correlation",
    "spectroscopy , the covariances of electric field at different time separations , expressed as the autocorrelation function , are the fourier transform of the spectrum . because the signals are drawn from gaussian distributions , their variances and covariances completely characterize them .",
    "the single known exception to this rule of gaussian statistics is radiation from pulsars , under certain observing conditions @xcite .    particularly at wavelengths of a millimeter or more ,",
    "covariances are usually estimated by correlation .",
    "( actually correlation is used at all wavelengths , but at wavelengths shortward of a millimeter quantum - mechanical processes come into the picture , complicating it ) .",
    "correlation involves forming products of samples of the two signals .",
    "the average of many such products approximates the covariance . in mathematical terms , for two signals @xmath2 and @xmath3 , the covariance is @xmath4 , where the angular brackets @xmath5 represent a statistical average over an ensemble of all statistically - identical signals .",
    "correlation approximates this enormously infinite average with a finite average over @xmath6 samples of @xmath2 and @xmath3 : @xmath7 . here , the subscript `` @xmath8 '' reflects the fact that @xmath2 and @xmath3 are unquantized ; their accuracy is not limited to a finite number of quantized levels",
    ". the subscript `` @xmath9 '' indicates sampling at the nyquist rate , as i assume ( see @xcite ) .    because the number of samples in most measurements of correlation is large",
    ", the results of a finite correlation follow a gaussian distribution .",
    "this is a consequence of the central limit theorem .",
    "thus , one expects a set of identical measurements of @xmath10 to be fully characterized by their mean , @xmath11 , and their standard deviation , @xmath12 .",
    "the mean is the deterministic part of the measurement ; it provides an estimate of @xmath1 .",
    "the standard deviation characterizes the random part of the measurement , and is often called `` noise '' ( but is to be distinguished from the noiselike signals @xmath2 and @xmath3 that are being correlated ) . in principle , the best measurement minimizes the random part , while preserving the relation between the deterministic part and @xmath1 .",
    "the signal - to - nose ratio of the correlation , @xmath13 , provides a figure of merit that quantifies the relative sizes of deterministic and random parts ; see , for example , @xcite .",
    "the electric field is commonly digitized before correlation .",
    "digitization includes sampling and quantization .",
    "sampling involves averaging the signal over short time windows ; it thus restricts the range of frequencies that can be uniquely represented . for simplicity , in this paper i will restrict discussion to `` video '' or `` baseband '' signals , for which a frequency range of 0 up to some maximum frequency is present ; and that they are sampled at the nyquist rate , or at half the shortest period represented .",
    "i also assume that the signals are `` white , '' in the sense that samples @xmath14 and @xmath15 are correlated only if @xmath16 ; and that the signals are stationary , so that the correlation of @xmath14 and @xmath15 is independent of @xmath17 .",
    "these assumptions limit the influence of sampling .",
    "i will discuss spectrally - varying signals elsewhere @xcite .",
    "quantization limits the values that can be represented , so that the digitized signal imperfectly represents the actual signal .",
    "quantization thus introduces changes in both the mean correlation @xmath18 and its standard deviation @xmath19 . here",
    "the subscript `` @xmath20 '' represents the fact that the quantized signal can take on @xmath20 discrete values .",
    "the mean and standard deviation of @xmath21 can be calculated from the statistics of the quantized signals @xmath22 and @xmath23 and the details of the quantization scheme .",
    "a number of previous authors have addressed the effects of quantization on correlation of noiselike signals ( see , for example , @xcite , chapter 8 , and references therein ) . notably , @xcite found the average correlation and the standard deviation for two - level quantization : the case where quantization reduces the signals to only their signs .",
    "@xcite found the average correlation and its standard deviation , for small normalized covariance @xmath24 , for four - level correlation : quantization reduces the signals to signs and whether they lie above or below some threshold @xmath25 .",
    "he found the optimal values of @xmath25 , and the relative weighting of points above and below the threshold @xmath26 , as quantified by the signal - to - noise ratio @xmath27 .",
    "@xcite generalized this to a broader range of quantization schemes , and studied effects of oversampling .",
    "@xcite examined gaussian noise and a signal of general form in the small - signal limit .",
    "they devise a criterion for the accuracy with which a quantization scheme represents a signal , and show that this yields the highest signal - to - noise ratio for @xmath24 .",
    "most recently , @xcite examined the case of many - level correlators .",
    "they use a criterion similar to that of @xcite to calculate the optimal level locations for various numbers of levels .",
    "@xcite also find the mean spectrum for a spectrally - varying source , as measured by an autocorrelation spectrometer ; they find that quantization introduces a uniform offset to to the spectrum , and scales the spectrum by a factor , and calculate the offset and factor .",
    "@xcite provide an extensive analysis of errors in a 3-level correlator .    in this paper ,",
    "i calculate the average correlation and its standard deviation for nonvanishing covariance @xmath1 .",
    "i provide exact expressions for these quantities , and approximations valid through fourth order in @xmath1 .",
    "interestingly , noise actually declines for large @xmath1 , for correlation of quantized signals .",
    "indeed , the signal - to - noise ratio for larger @xmath1 can actually exceed that for correlation of an unquantized signal . in other words ,",
    "correlation of a quantized signal can provide a more accurate measure of @xmath1 than would correlation of the signal before quantization .",
    "this fact is perhaps surprising ; it reflects the fact that correlation is not always the most accurate way to determine the covariance of two signals .",
    "the organization of this paper is as follows : in section 2 , i review the statistics of the correlation of unquantized , or continuously variable , complex signals . in section 3 ,",
    "i present expressions that give the average correlation and the standard deviation , in terms of integrals involving the characteristic curve .",
    "i include statistics of real and imaginary parts , which are different .",
    "i present expansions of these integrals as a power series in @xmath1 . in section 4",
    "i discuss computer simulations of correlation to illustrate these mathematical results .",
    "i summarize the results in the final section .",
    "consider two random , complex signals @xmath2 and @xmath3 .",
    "suppose that each of these signals is a random variable drawn from a gaussian distribution .",
    "suppose that the signals @xmath2 and @xmath3 are correlated , so that they are , in fact , drawn from a joint gaussian joint probability density function @xmath28 @xcite . without loss of generality , i assume that each signal has variance of 2 : @xmath29 in this expression , the angular brackets @xmath30 denote an statistical average : in other words , an average over all systems with the specified statistics .",
    "this choice for a variance of 2 for @xmath2 and @xmath3 is consistent with the literature on this subject , much of which treats real signals ( rather than complex ones ) , drawn from gaussian distributions with unit variance @xcite .",
    "of course , the results presented here are easily scaled to other variances for the input signal .",
    "i demand that the signals themselves have no intrinsic phase : in other words , the statistics remain invariant under the transformation @xmath31 , and @xmath32 , where @xmath33 is an arbitrary overall phase",
    ". it then follows that @xmath34 from these facts , one finds : @xmath35 \\re [ x ] \\rangle & = & \\langle \\im [ x ] \\im [ x ] \\rangle = 1 \\label{re_im_sigmas}\\\\ \\langle \\re [ x ] \\im [ x ] \\rangle & = & 0 , \\nonumber\\end{aligned}\\ ] ] and similarly for @xmath3 .",
    "thus , real and imaginary parts are drawn from gaussian distributions with unit variance .",
    "the distributions are circular in the complex plane for both @xmath2 and @xmath3 .    without loss of generality ,",
    "i assume that the normalized covariance of the signals @xmath1 is purely real : @xmath36 one can always make @xmath1 purely real by rotating @xmath2 ( or @xmath3 ) in the complex plane : @xmath37 , and @xmath38 .",
    "note that , because of the absence of any intrinsic phase , @xmath35 \\re [ y ] \\rangle & = & \\langle \\im [ x ] \\im [ y ] \\rangle = \\rho \\label{re_im_rho}\\\\ \\langle \\re [ x ] \\im [ y ] \\rangle & = & \\langle \\im [ x ] \\re [ y ] \\rangle = 0 , \\nonumber\\end{aligned}\\ ] ] so that @xmath39 in other words , the real parts of @xmath2 and @xmath3 are correlated ; and the imaginary parts of @xmath2 and @xmath3 are correlated ; but real and imaginary parts are uncorrelated . in mathematical terms , real parts ( or imaginary parts ) are drawn from the bivariate gaussian distribution : @xmath40 where @xmath41 stands for either @xmath42 , \\re[y])$ ] or @xmath43,\\im[y])$ ] .",
    "the distributions for real and imaginary parts are identical , but real and imaginary parts are uncorrelated",
    ". therefore , @xmath44 , \\re [ y]\\right)\\times p_2\\left(\\im [ x ] , \\im [ y]\\right ) .",
    "\\label{product_2_bivariates}\\end{aligned}\\ ] ]        consider the product of two random complex signals , drawn from gaussian distributions as in the previous section , sampled in time .",
    "suppose that the signals are not quantized : they can take on any complex value .",
    "the product of a pair of samples @xmath45 does not follow a gaussian distribution . rather , the distribution of @xmath46 is the product of an exponential of the real part , multiplied by the modified bessel function of the second kind of order zero of the magnitude of @xmath46 @xcite .",
    "however , the average of many such products , averaged over a large number of pairs of samples , approaches a gaussian distribution , as the central limit theorem implies .",
    "in such a large but finite sum , @xmath47 provides an estimate of the covariance @xmath1 . here",
    "the index @xmath17 runs over the samples , commonly samples taken at different times .",
    "the total number of samples correlated is @xmath6 .",
    "henceforth i assume that in all summations , indices run from @xmath48 to @xmath6 .",
    "the subscript @xmath8 on the correlation @xmath10 again indicates that the correlation has been formed for variables @xmath14 , @xmath49 that can take on any of an infinite number of values ; in other words , it indicates that @xmath14 and @xmath49 have not been quantized .",
    "the mean correlation is equal to the covariance , in a statistical average : @xmath50\\rangle   = { { 1}\\over{2}}\\left(\\langle\\re[x_i]\\re[y_i]\\rangle + \\langle\\im[x_i]\\im[y_i]\\rangle\\right ) = \\rho , \\label{r_bar}\\ ] ] where i used the assumption that the phase of @xmath1 is zero , eq .",
    "[ re_im_rho ] .",
    "this can also be seen from the mean of the distribution of the products @xmath45 @xcite , or simply by integrating over the joint distribution of @xmath14 and @xmath49 , eqs .",
    "[ bivariate_gaussian_dist ] and  [ product_2_bivariates ] .      because the distribution of @xmath10 is gaussian , the distribution of @xmath10 is completely characterized by its mean , eq .  [ r_bar ] , and by the variances @xmath51 and",
    "suppose that the samples @xmath14 and @xmath49 are independent ; in mathematical terms , suppose that @xmath53 if they are not independent the results will depend on the correlations among samples ; this case is important if , for example , the signal has significant spectral structure .",
    "@xcite find the average spectrum in this case ; i will discuss the noise in future work . here",
    "i consider only independent samples . in that case , @xmath54 where i have separated the terms with @xmath16 from those with @xmath55 , and appealed to the facts that @xmath14 and @xmath15 are covariant only if @xmath16 ( `` white '' signals ) , and that for @xmath16 the statistics are stationary in @xmath17 . for gaussian variables with",
    "zero mean @xmath56 , all moments are related to the second moments : @xmath57 so that : @xmath58 therefore , @xmath59 an analogous calculation yields @xmath60 and so @xmath61 i combine these facts to find the means and standard deviations of the real and imaginary parts of the measured correlation , @xmath10 : @xmath62\\rangle&=&\\langle r_{\\infty}\\rangle \\phantom{\\haf\\left\\{\\langle r_{\\infty}r_{\\infty}^*\\rangle+\\langle r_{\\infty}r_{\\infty}\\rangle\\right\\}-^2}= \\rho   \\label{r_infty_avgsd } \\\\ \\langle\\im [ r_{\\infty}]\\rangle & &   \\phantom{\\haf\\left\\{\\langle r_{\\infty}r_{\\infty}^*\\rangle+\\langle r_{\\infty}r_{\\infty}\\rangle\\right\\}-\\langle r_{\\infty}\\rangle^2}= 0   \\nonumber \\\\ \\langle\\re[r_{\\infty}]^2\\rangle - \\langle \\re [ r_{\\infty}]\\rangle^2 & = &   \\haf\\left\\{\\langle r_{\\infty}r_{\\infty}^*\\rangle+\\langle r_{\\infty}r_{\\infty}\\rangle\\right\\}-\\langle r_{\\infty}\\rangle^2= { { 1}\\over{2 n_q}}(1+\\rho^2 ) \\nonumber \\\\ \\langle\\im [ r_{\\infty}]^2\\rangle & = & \\haf\\left\\{\\langle r_{\\infty}r_{\\infty}^*\\rangle-\\langle r_{\\infty}r_{\\infty}\\rangle\\right\\ } \\phantom { - \\langle r_{\\infty}\\rangle^2 } = { { 1}\\over{2 n_q}}(1-\\rho^2)\\nonumber .\\end{aligned}\\ ] ]    if the number of independent samples @xmath6 is large , the central limit theorem implies that @xmath63 $ ] and @xmath64 $ ] are drawn from gaussian distributions .",
    "the means and variances of these distributions , as given by eq .",
    "[ r_infty_avgsd ] , completely characterize @xmath10 . the fact that the real part of @xmath10 has greater standard deviation than the imaginary part reflects the presence of self - noise or source - noise .",
    "sometimes this is described as the contribution of the noiselike signal to the noise in the result .",
    "commonly , and often realistically , astrophysicists suppose that @xmath1 measures the intensity of one signal that has been superposed with two uncorrelated noise signals to produce @xmath2 and @xmath3 ( see @xcite ) .",
    "a change in @xmath1 then corresponds to a change in the intensities @xmath65 and @xmath66 as well .",
    "here , i suppose that @xmath67 , while @xmath1 varies . the results presented here can be scaled to those for the alternative interpretation .",
    "the signal - to - noise ratio ( snr ) for @xmath63 $ ] is : @xmath68 ) = { { \\langle \\re [ r_{\\infty}]\\rangle}\\over { \\sqrt{\\langle \\re [ r_{\\infty}]^2\\rangle-\\langle \\re [ r_{\\infty}]\\rangle^2 } } } = { \\sqrt{2 n_q}}{{\\rho}\\over{\\sqrt{1 + \\rho^2}}}. \\label{snrinfty}\\ ] ] note that for a given number of observations @xmath6 , snr increases with @xmath1 ; the increase is proportional for @xmath24",
    ".    a related quantity to snr is the rms phase , statistically averaged over many measurements .",
    "the phase is @xmath69=\\tan^{-1}(\\im [ r_{\\infty}]/\\re [ r_{\\infty}])$ ] . when the number of observations is sufficiently large , and the true value of the phase is @xmath70 , as assumed here , the standard deviation of the phase is @xmath71 ^ 2\\rangle   = \\sqrt { \\langle \\im [ r_{\\infty}]^2 \\rangle } /\\langle \\re [ r_{\\infty}]\\rangle $ ] . the inverse of the standard deviation of the phase ( in radians ) is analogous to the snr for the real part , eq .  [ snrinfty ]",
    "this snr for phase is : @xmath72 ) =   { { \\langle \\re [ r_{\\infty}]\\rangle } \\over{\\sqrt{\\langle \\im [ r_{\\infty}]^2\\rangle } } } = { \\sqrt{2 n_q } } { { \\rho}\\over{\\sqrt{1-\\rho^2 } } } .",
    "\\label{snrphase}\\ ] ] for constant @xmath6 , the snr of the phase increases with @xmath1 , and increases much faster than proportionately for @xmath73 .",
    "quantization converts the variables @xmath2 , @xmath3 to discrete variables @xmath74 , @xmath75 . these discrete variables depend on @xmath2 and @xmath3 through a multiple step function , known as a characteristic curve .",
    "each step extends over some range @xmath76 $ ] of the unquantized signal , and is given some weight @xmath77 in correlation .",
    "the function @xmath78 denotes the characteristic curve , where again @xmath79 stands for either @xmath80 $ ] or @xmath81 $ ] .",
    "the complex quantized variable @xmath74 is thus given by @xmath82 ) + i \\hat x(\\im[x])$ ] . the same characteristic curve is applied to real and imaginary parts .",
    "i hold open the possibility that the characteristic curves @xmath78 and @xmath83 are different .",
    "i assume in this paper that the characteristic curves are antisymmetric : @xmath84 , and similarly for @xmath85 .",
    "@xcite describe effects of departures from antisymmetry , and how antisymmetry can be enforced .",
    "figure [ 4_level ] shows a typical characteristic curve for 4-level ( or 2-bit ) sampling .",
    "systems with @xmath20 levels of quantization can be described by analogous , more complicated characteristic curves , and corresponding sets of weights @xmath86 and levels @xmath87 and @xmath88 @xcite . for @xmath20-level sampling ,",
    "the correlation of @xmath6 quantized samples is @xmath89    in practical correlators , deviations from theoretical performance can often be expressed as deviations of the characteristic curve from its desired form . in principle , these can be measured by counting the numbers of samples in the various quantization ranges , and using the assumed gaussian distributions of the input signals to determine the actual levels @xmath90 and @xmath91 . @xcite",
    "present an extensive discussion of such errors , and techniques to control and correct them .",
    "although the results presented below are applicable to more complicated systems , the 4-level correlator will be used as a specific example in this paper , with correlation @xmath92 . for 4-level sampling ,",
    "commonly a sign bit gives the sign of @xmath79 , and an amplitude bit assigns weight @xmath48 if @xmath93 is less than some threshold @xmath94 , and weight @xmath26 if @xmath93 is greater than @xmath94 .",
    "together , sign and amplitude bits describe the 4 values possible for @xmath78 .",
    "other types of correlators , including 2-level , 3-level , or `` reduced '' 4-level ( in which case the smallest product , for @xmath95 , @xmath96 , is ignored ) , can be formed as special cases or sums of four - level correlators @xcite .",
    "ideally , from measurement of of the quantized correlation @xmath97 one can estimate the true covariance @xmath1 .",
    "the statistical mean of @xmath97 is : @xmath98 \\re [ \\hat y ] + \\im [ \\hat x]\\im [ \\hat y ] ) + i ( -\\re [ \\hat x ] \\im [ \\hat y ] + \\im [ \\hat x]\\re [ \\hat y ] )   \\rangle .",
    "\\nonumber\\end{aligned}\\ ] ] because @xmath99 $ ] depends only on @xmath100 $ ] and @xmath101 $ ] depends only on @xmath102 $ ] , and @xmath100 $ ] and @xmath102 $ ] are completely independent ( and similarly for @xmath103 $ ] and @xmath104 $ ] ) , @xmath105\\im [ \\hat y]\\rangle=\\langle \\im[\\hat x]\\re [ \\hat y]\\rangle=0.\\ ] ] thus , the imaginary part of @xmath106 , which involves products of these statistically independent terms , has average zero ( eq .",
    "[ re_im_sigmas ] ) . for the real part , @xmath107 \\re [ \\hat y]\\rangle = \\langle \\im [ \\hat x]\\im [ \\hat y]\\rangle , \\ ] ] where i use the assumption that the characteristic curves are identical for real and imaginary parts , and that real and imaginary parts of @xmath2 and @xmath3 have identical statistics ( eqs",
    ".  [ re_im_sigmas ] , [ re_im_rho ] ) .",
    "i use the bivariate gaussian distribution for real and imaginary parts to find a formal expression for the statistical average of the correlation : @xmath108 \\re [ \\hat y]\\rangle = \\upsilon_{xy}\\equiv \\int dx dy p_2(x , y ) \\hat x(x)\\hat y ( y ) .",
    "\\label{r_m_exact}\\ ] ] this integral defines @xmath109 . for the assumed antisymmetric characteristic curves @xmath78 and @xmath83",
    ", one can easily show that @xmath110 .",
    "in other words , the ensemble - averaged quantized correlation is an increasing function of the covariance @xmath1 , for completely arbitrary quantizer settings ( so long as the characteristic curves are antisymmetric ) .",
    "the discussion of this section reduces the calculation of the average quantized correlation to that of integrating @xmath111 over each rectangle in a grid , with the edges of the rectangles given by the thresholds in the characteristic curves @xcite .",
    "the function @xmath109 depends on @xmath1 through @xmath111 .",
    "this function is usually expanded through first order in @xmath1 , because @xmath1 is small in most astrophysical observations .",
    "the integral @xmath109 and similar integrals can be converted into one - dimensional integrals for easier analysis .",
    "if one defines @xmath112 then the fourier transform of @xmath113 is equal to that of @xmath114 , as one finds from integration by parts .",
    "thus , @xmath115 the integral @xmath109 is the sum of one such integral and one such constant for each step in the characteristic curve .",
    "this one - dimensional form is useful for numerical evaluation and expansions . @xcite",
    "also present an interesting expansion of @xmath109 in hermite polynomials , in @xmath116 .",
    "this section presents an exact expression for the variance of the correlation of a quantized signal , when averaged over the ensemble of all statistically identical measurements .",
    "real and imaginary parts of @xmath97 have different variances .",
    "this requires calculation of both @xmath117 and @xmath118 .",
    "note that : @xmath119 where the sum over @xmath55 is simplified by the fact that samples at different times are uncorrelated , and by eq .",
    "[ r_m_avg ] .",
    "i expand the first average in the last line : @xmath120 ^ 2 \\re [ \\hat y_i]^2 + \\im [ \\hat x_i]^2 \\im [ \\hat y_i]^2 + \\re [ \\hat x_i]^2 \\im [ \\hat y_i]^2 + \\im [ \\hat x_i]^2 \\re [ \\hat y_i]^2 \\rangle \\label{xi_xis_yi_yis } \\\\ & = & \\langle \\re [ \\hat x_i]^2 \\re [ \\hat y_i]^2\\rangle + \\langle\\im [ \\hat x_i]^2 \\im [ \\hat y_i]^2 \\rangle + \\langle \\re [ \\hat x_i]^2\\rangle\\langle \\im [ \\hat y_i]^2\\rangle + \\langle\\im [ \\hat x_i]^2 \\rangle\\langle\\re [ \\hat y_i]^2\\rangle , \\nonumber\\end{aligned}\\ ] ] where i have used the fact that the real part of @xmath74 has zero covariance with the imaginary part of @xmath75 , and vice versa .",
    "because the real and imaginary parts are identical , this sum can be expressed formally in terms of the integrals : @xmath121 ^ 2 \\re [ \\hat y_i]^2\\rangle =   \\langle\\im [ \\hat x_i]^2 \\im [ \\hat y_i]^2 \\rangle & = &   \\upsilon_{x2y2}\\equiv   \\int dx\\ , dy \\ ; p(x , y ) \\ ; \\hat x(x)^2 \\ ; \\hat y(y)^2 , \\label{upsx2y2_ax2_ay2 } \\\\ \\langle\\re[\\hat x_i]^2\\rangle=\\langle\\im[\\hat x_i]^2\\rangle & = & a_{x2 } \\equiv   \\int dx { { 1}\\over{\\sqrt{2\\pi } } } e^{-\\haf x^2 } \\hat x(x)^2    \\nonumber \\\\",
    "\\re [ \\hat y_i]^2\\rangle = \\langle\\im [ \\hat y_i]^2\\rangle & = & a_{y2 } \\equiv   \\int dy { { 1}\\over{\\sqrt{2\\pi}}}e^{-\\haf y^2 } \\hat y(y)^2 . \\nonumber\\end{aligned}\\ ] ] these expressions defines @xmath122 , @xmath123 , and @xmath124 .",
    "thus , @xmath125 note that in these expressions @xmath123 and @xmath124 are constants that depend on the characteristic curve , but not on @xmath1 ; whereas @xmath109 and @xmath122 depend on @xmath1 in complicated ways , as well as on the characteristic curve .",
    "similarly , @xmath126 i again expand the first sum in the last line : @xmath127 ^ 2\\re [ \\hat y_i]^2+\\im [ \\hat x_i]^2\\im [ \\hat y_i]^2 % -\\re [ \\hat x_i]^2\\im [ \\hat y_i]^2-\\im [ \\hat x_i]^2\\re [ \\hat y_i]^2 \\label{expand_4part_sum } \\\\ % & & \\quad + 4\\re [ \\hat x_i]\\im [ \\hat x_i]\\re [ \\hat y_i]\\im [ \\hat y_i ] % \\rangle \\nonumber \\\\ % & = &   % \\langle \\re [ \\hat x_i]^2 \\re [ \\hat y_i]^2\\rangle +   % \\langle \\im [ \\hat x_i]^2 \\im [ \\hat y_i]^2\\rangle -   % \\langle \\re [ \\hat x_i]^2 \\im [ \\hat y_i]^2\\rangle -   % \\langle \\im [ \\hat x_i]^2 \\re [ \\hat y_i]^2\\rangle +   % 4 \\langle \\re [ \\hat x_i]\\re [ \\hat y_i]\\rangle\\langle\\im [ \\hat x_i]\\im [ \\hat y_i]\\rangle \\\\ & = &   2 \\upsilon_{x2y2 } - 2 a_{x2}a_{y2 } + 4 \\upsilon_{xy}^2 .",
    "\\label{expand_4part_sum } % \\nonumber\\end{aligned}\\ ] ] where i omit the imaginary terms , all of which average to zero .",
    "therefore : @xmath128    using the same logic as in the derivation of eq .",
    "[ r_infty_avgsd ] , eqs .",
    "[ r_m_r_m*_exact ] and  [ r_m_r_m_exact ] can be used to find the means and standard deviations of the real and imaginary parts of @xmath97 : @xmath129\\rangle & = & \\upsilon_{xy } \\label{sds_re_im_r_m } \\\\ \\langle \\im [ \\hat r_m]\\rangle & = & 0 \\nonumber \\\\",
    "\\re [ \\hat r_m]^2\\rangle - \\langle \\re [ \\hat r_m]\\rangle^2    & = & { { 1}\\over{2 n_q}}\\left(\\upsilon_{x2y2}-(\\upsilon_{xy})^2\\right)\\nonumber \\\\",
    "\\im [ \\hat r_m]^2\\rangle & = & { { 1}\\over{2 n_q}}\\left(a_{x2}a_{y2 } -(\\upsilon_{xy})^2\\right ) .",
    "\\nonumber\\end{aligned}\\ ] ] again , note that @xmath123 and @xmath124 are constants that depend on the characteristic curve , but not on the covariance @xmath1 , whereas @xmath109 and @xmath122 depend on the actual value of @xmath1 as well as the characteristic curve .",
    "for particular characteristic curves , and particular values of @xmath1 , these expressions nevertheless yield the mean correlation , and the standard deviations of real and imaginary parts about the mean",
    ". figures  [ re_plot ] through  [ ratio_plot ] show examples , and compare them with the approximate results from the following section .",
    "note that , because @xmath109 is an increasing function of @xmath1 , the standard deviation of the imaginary part decreases with increasing covariance @xmath1 .",
    "this holds for arbitrary quantizer parameters , so long as the characteristic curves are antisymmetric .",
    "in other words , the noise in the imaginary part always decreases when the correlation increases .    for uncorrelated signals , @xmath130 .",
    "one finds then that @xmath131 and @xmath132 . in this case",
    "both real and imaginary parts have identical variances , as they must : @xmath133 ^ 2\\rangle - \\langle \\re [ \\hat r_m]\\rangle^2    =   \\langle \\im [ \\hat r_m]^2\\rangle = { { 1}\\over{2 n_q}}a_{x2}a_{y2 } , \\quad { \\rm for\\ } \\rho=0.\\ ] ] this recovers the result of @xcite and others for the noise .    if the characteristic curves are identical , so that : @xmath134 , then if the signals are identical : @xmath135 , one finds that @xmath136 .",
    "under these assumptions then @xmath137 ^ 2\\rangle = 0 $ ] .",
    "unfortunately the expressions for the mean correlation and the noise , for quantized signals , both depend in a complicated way on the covariance @xmath1 , the quantity one seeks to measure .",
    "often the covariance @xmath1 is small .",
    "various authors discuss the correlation @xmath97 of quantized signals @xmath74 and @xmath75 to first order in @xmath1 , as is appropriate in the limit @xmath138 .",
    "@xcite also calculate @xmath139 for @xmath140 ; as they point out , this case is important for autocorrelation spectroscopy . @xcite",
    "present an expression for @xmath141 for a 3-level correlator , and present several useful approximate expressions for the inverse relationship @xmath142 . here",
    "i find the mean correlation @xmath143 through fourth order in @xmath1 .    for small covariance @xmath1",
    ", one can expand @xmath144 in eq .",
    "[ bivariate_gaussian_dist ] as a power series in @xmath1 : @xmath145 .",
    "\\nonumber   \\label{bivariate_gaussian_dist_expansion}\\end{aligned}\\ ] ] note that the coefficient of each term in this expansion over @xmath1 can be separated into two factors that depend on either @xmath79 alone or @xmath85 alone .",
    "the extension to higher powers of @xmath1 is straightforward , and the higher - order coefficients have this property as well .",
    "as noted above , i assume that the characteristic curve is antisymmetric : @xmath84 .",
    "the integral @xmath109 ( eq .  [ r_m_exact ] )",
    "involves first powers of the functions @xmath78 and @xmath83 . in this integral",
    ", only terms odd in both @xmath79 and @xmath85 match the antisymmetry of the characteristic curve , and yield a nonzero result .",
    "such terms are also odd in @xmath1 , as is seen from inspection of eq .",
    "[ pdistexpansion ] .",
    "the first - order terms thus involve the integrals : @xmath146 where again @xmath79 and @xmath85 can stand for either real or imaginary parts of @xmath2 and @xmath3 . here , i consider terms up to order 3 in @xmath1 .",
    "one thus encounters the further integrals : @xmath147 and the analogous expression for @xmath148 .",
    "therefore , through fourth order in @xmath1 : @xmath149 \\nonumber \\\\ & \\approx & b_x b_y \\,\\rho + { { 1}\\over{6 } } ( 3 b_x - d_x)(3 b_y - d_y ) \\,\\rho^3 + ... .",
    "\\nonumber \\end{aligned}\\ ] ] for thresholds @xmath150 the linear approximation is quite accurate @xcite ; however , for other values of @xmath151 the higher - order terms can become important .",
    "our notation differs from that of previous authors ; our @xmath152 is equal to @xmath153 $ ] of @xcite and @xcite .",
    "it is equal to the @xmath154 of @xcite .",
    "figure  [ re_plot ] shows typical results of the expansion of eq .  [ hat_rm_bar_approx ] , for a 4-level correlator , and compares this estimate for @xmath155 with the results from direct integration of eq .",
    "[ r_m_exact ] over rectangles in the @xmath156 plane . in this example , @xmath157 . for both @xmath158 and @xmath159",
    ", @xmath143 is relatively flat , with a sharp upturn very close to @xmath140 .",
    "however , in both cases , but especially for @xmath159 , the curve of @xmath92 bends upward well before @xmath140 , so that the linear approximation is good only for relatively small @xmath1 .",
    "expressions for the noise in the integral involve the integral @xmath122 ( eq .  [ sds_re_im_r_m ] ) .",
    "this integral involves only the squares of the characteristic curves @xmath160 and @xmath161 . because the characteristic curves are antisymmetric about 0 , their squares are symmetric : @xmath162 and @xmath163 .",
    "therefore , the only contributions come from terms in the expansion of @xmath28 ( eq .  [ pdistexpansion ] ) that are even in @xmath79 and @xmath85 .",
    "one thus encounters the integrals : @xmath164 and analogously for @xmath124 , @xmath165 , and @xmath166 .",
    "then , through fourth order in @xmath1 , @xmath167 \\nonumber \\\\ & \\approx & a_{x2}a_{y2}+\\haf(a_{x2}-c_{x2})(a_{y2}-c_{y2})\\rho^2+\\otf(3 a_{x2}-6 c_{x2}+e_{x2})(3 a_{y2}-6 c_{y2}+e_{y2})\\rho^4 ... . \\nonumber\\end{aligned}\\ ] ] i find the standard deviations of real and imaginary parts from eqs .",
    "[ sds_re_im_r_m ] ,  [ hat_rm_bar_approx ] , and  [ upsilon_x2y2_approx ] : @xmath168 ^ 2\\rangle-\\langle\\re[\\hat r_m]\\rangle^2 & \\approx & { { 1}\\over{2 n_q } } \\bigl(\\bigl\\ { a_{x2}a_{y2 } \\bigr\\ } \\label{hat_rm_sd_approx}\\\\ & & \\phantom{{{1}\\over{2 n_q } } \\bigl ( } + \\bigl\\{\\haf(a_{x2}-c_{x2})(a_{y2}-c_{y2})-b_{x}^2b_{y}^2\\bigr\\ } \\rho^2 \\nonumber \\\\ & & \\phantom{{{1}\\over{2 n_q } } \\bigl ( } + \\bigl\\{\\otf(3 a_{x2}-6 c_{x2}+e_{x2})(3 a_{y2}-6 c_{y2}+e_{y2 } ) \\nonumber \\\\ & & \\phantom{{{1}\\over{2 n_q } } \\bigl(+\\bigl\\ { } \\quad -\\thd b_x(3 b_x - d_x ) b_y(3 b_y - d_y)\\bigr\\}\\rho^4 ... \\bigr ) \\nonumber",
    "\\\\ \\langle\\im[\\hat r_m]^2\\rangle & \\approx & { { 1}\\over{2 n_q}}\\bigl ( \\bigl\\ { a_{x2}a_{y2 } \\bigr\\ } \\nonumber \\\\ & & \\phantom { { { 1}\\over{2 n_q}}\\bigl ( } -\\bigl\\ { b_x^2b_y^2 \\bigr\\ } \\rho^2 \\nonumber \\\\ & & \\phantom { { { 1}\\over{2 n_q}}\\bigl ( } -\\bigl\\{\\thd b_x(3 b_x - d_x ) b_y(3 b_y - d_y)\\bigr\\}\\rho^4 ... \\bigr ) .",
    "\\nonumber\\end{aligned}\\ ] ] i have used the fact that @xmath143 is purely real ; this is a consequence of the assumption that @xmath1 is purely real .",
    "figure  [ sd_plot ] shows examples of the standard deviations of @xmath169 $ ] and @xmath170 $ ] for 2 choices of @xmath25 .",
    "these are the noise in estimates of the correlation .",
    "note that the noise varies with @xmath1 .",
    "the quadratic variation of these quantities with @xmath1 is readily apparent .",
    "the higher - order variation is more subtle , although it does lead to an upturn of the standard deviation of @xmath169 $ ] near @xmath171 , for @xmath158 .",
    "the series expansions become inaccurate near @xmath140 , as expected .",
    "the standard deviation of @xmath169 $ ] can also increase , instead of decrease , for large @xmath1 .",
    "such an increase is more common for parameter choices with @xmath172 .",
    "again , note that the standard deviation of the imaginary part always decreases with increasing @xmath1 .",
    "the signal - to - noise ratio ( snr ) for a quantizing correlator is the quotient of the mean and variance of @xmath97 : the results of   [ mean_approx_sec ] and  [ noise_approx_sec ] .",
    "i recover the results of @xcite for the snr for a quantizing correlator , by using our approximate expressions through first order in @xmath1 ( see also @xcite , @xcite , @xcite ) : @xmath173 ) \\approx { { \\langle \\re [ \\hat r_m ] \\rangle}\\over{\\sqrt{\\langle \\re [ \\hat r_m]^2\\rangle- \\langle \\re [ \\hat r_m]\\rangle^2 } } }   \\approx \\sqrt{2 n_q}{{b_x b_y}\\over{\\sqrt{a_{x2}a_{y2 } } } } \\rho.\\ ] ] for a 4-level correlator , a snr of @xmath174 is attained for @xmath175 , @xmath25=1 , in the limit @xmath138 .",
    "many 4-level correlators use these values . the maximum value for @xmath176 is actually obtained for @xmath177 , @xmath178 , for which @xmath179 .",
    "this adjustment of quantization constants provides a very minor improvement in snr .",
    "for nonvanishing @xmath1 , the optimum level settings depend upon the covariance @xmath1 .",
    "the signal - to - noise ratio is @xmath180)&= & \\sqrt{2 n_q } { { \\upsilon_{xy}}\\over{\\sqrt{\\upsilon_{x2y2}-(\\upsilon_{xy})^2 } } } .",
    "\\label{snr_rm}\\end{aligned}\\ ] ] this can be approximated using the expansions for @xmath109 and @xmath122 ; figure  [ snr_plot ] shows the results .",
    "note that in the examples in the figure , the snr for the quantized correlations actually curve above that for the unquantized correlation beyond @xmath181 ; this indicates that correlation of quantized signals can actually yield higher signal - to - noise ratio than would be obtained from correlating the same signals before quantization .",
    "this results from the decline in noise with increasing @xmath1 visible in fig .",
    "[ sd_plot ] .    for a proper comparison of snrs",
    ", one must compare with the snr obtained for non - quantized correlation , @xmath182)$ ] , eq .",
    "[ snrinfty ] .",
    "one finds : @xmath183)\\over { { { \\cal r}_{\\infty}(\\re [ \\hat r_m ] ) } } } = { { \\upsilon_{xy}}\\over{\\sqrt{\\upsilon_{x2y2}-(\\upsilon_{xy})^2 } } } { { \\sqrt{1-\\rho^2}}\\over{\\rho } } .\\end{aligned}\\ ] ] figure  [ ratio_plot ] shows this ratio for 2 choices of @xmath25 .",
    "the ratio can exceed 1 , again indicating that quantized correlation provides a more accurate result than would correlation of an unquantized signal .",
    "the snr for a measurement of phase for quantized correlation is the inverse of the standard deviation of the phase , as discussed in   [ snrinfty ] .",
    "for quantized correlation , this is @xmath184 ) =   { { \\re [ r_{m}]}\\over{\\sqrt{\\langle \\im [ r_{m}]^2\\rangle } } } = { \\sqrt{2 n_q } } { { \\upsilon_{xy}}\\over{\\sqrt{a_{x}a_{y}-(\\upsilon_{xy})^2 } } } .\\ ] ] again , because @xmath109 always increases with @xmath1 , the snr of the phase always increases with increasing covariance .    the ratio of the snr for phase to that for correlation of an unquantized signal , @xmath185)/ { { { \\cal r}_{\\infty}(\\phi [ r_{\\infty}])}}$ ] ( see eq .",
    "[ snrphase ] ) , provides an interesting comparison . for @xmath138 ,",
    "the statistics for the imaginary part of the correlation are identical to those for the real part ( as they must be ) , and the highest snr for the phase is given by the quantizer parameters that are optimal for the real part , traditionally @xmath158 , @xmath175 .",
    "this ratio is approximately constant with @xmath1 up to @xmath186 , and then decreases rather rapidly .",
    "simulations suggest that quantized correlation is less efficient than unquantized for measuring phase ; however i have not proved this in general .",
    "simulation of a 4-level correlator provides a useful perspective .",
    "i simulated such a correlator by generating two sequences of random , complex numbers , @xmath14 and @xmath49 .",
    "the real parts of @xmath14 and @xmath49 are drawn from one bivariate gaussian distribution , and their imaginary parts from another independent one ( see eq .",
    "[ product_2_bivariates ] ) .",
    "these bivariate gaussian distributions can be described equivalently as elliptical gaussian distributions , with major and minor axes inclined to the coordinate axes @xmath187 $ ] and @xmath188 $ ] and the corresponding axes for the imaginary parts .",
    "@xcite gives expressions that relate the semimajor and semiminor axes and angle of inclination of an elliptical gaussian distribution to the normalized covariance @xmath1 and variances @xmath189 and @xmath190 .",
    "for the special case of @xmath191 used in this work , the major axis always lies at angle @xmath192 to both coordinate axes , along the line @xmath193 .",
    "the semimajor axis @xmath194 and semiminor axis @xmath195 are then given by @xmath196    to form the required elliptical distributions , i drew pairs of elements from a circular gaussian distribution , using the box - muller method ( see @xcite ) .",
    "i scaled these random elements so that their standard deviations were @xmath194 and @xmath195 .",
    "i then rotated the resulting 2-element vector by @xmath192 to express the results in terms of @xmath79 and @xmath85 .",
    "i repeated the procedure for the imaginary part .",
    "i quantized the sequences @xmath14 and @xmath49 according to the 4-level characteristic curve shown in figure  [ 4_level ] , to yield @xmath197 and @xmath198 .",
    "both the unquantized and the quantized sequences were correlated by forming the products @xmath199 and @xmath200 , respectively , and results were averaged over @xmath201 instances of the index @xmath17 .",
    "this procedure yields one realization each of @xmath0 and @xmath92 .",
    "i found that values for @xmath6 smaller than about @xmath202 could produce significant departures from gaussian statistics for @xmath92 , particularly for larger values of @xmath1 .",
    "i repeated the process to obtain 4096 different realizations of @xmath0 and @xmath92 .",
    "i found the averages and standard deviations for the real and imaginary parts for this set of realizations .",
    "figures  [ re_plot ] through  [ ratio_plot ] show these statistical results of the simulations , and compare them with the mathematical results of the preceding sections .",
    "clearly , the agreement is good .    in graphical form , samples of the correlation form an elliptical gaussian distribution in the complex plane , centered at the mean value of correlation @xmath11 or @xmath203 , as the case may be .",
    "the principal axes of the distribution lie along the real and imaginary directions ( or , more generally , the directions in phase with @xmath1 and out of phase with @xmath1 ) .",
    "the lengths of these principal axes are the variances of real and imaginary parts .",
    "the fundamental result of this paper is that a change in covariance @xmath1 affects quantized correlation @xmath97 differently from unquantized correlation @xmath10 . for unquantized correlation ,",
    "an increase in covariance @xmath1 increases noise for estimation of signal amplitude . for quantized correlation ,",
    "an increase in @xmath1 can increase or decrease amplitude noise .",
    "for both quantized and unquantized correlation , an increase in @xmath1 leads to a decrease in phase noise . in this work",
    ", i arbitrarily set the phase of @xmath1 to 0 , so that amplitude corresponds to the real part , and phase to the imaginary part of the estimated correlation .",
    "the net noise ( summed , squared standard deviations of real and imaginary parts ) can decrease ( or increase ) with increasing @xmath1 : noise is not conserved .",
    "i present expressions for the noise as a function of quantization parameters , both as exact expressions that depend on @xmath1 and on power - series expansions in @xmath1 .",
    "these expressions , and a power - series expansion for the mean correlation , are given through fourth order in @xmath1 .",
    "the increase in noise with covariance @xmath1 for analog correlation , sometimes called source noise or self - noise , is sometimes ascribed to the contribution of the original , noiselike signal to the noise of correlation .",
    "this idea is difficult to generalize to comparisons among quantized signals , because such comparisons require additional assumptions about changes in quantizer levels and the magnitude of the quantized signal when the covariance changes .",
    "these comparisons are simpler for multiple correlations derived from a single signal ( as , for example , for the correlation function of a spectrally - varying signal ) , and i will discuss them in that context elsewhere @xcite .",
    "the discussion in this paper is limited to `` white '' signals , without spectral variation and with only a single independent covariance .      one interesting consequence of these results is that signal - to - noise ratio of correlation can actually be greater for quantized signals , than it would be for correlation of the same signals before quantization . at small covariance @xmath1 ,",
    "snr is always lower for quantized signals , but this need not be the case for covariance @xmath204 .",
    "this appears to present a paradox , because the process of quantization intrinsically destroys information : the quantized signals @xmath197 , @xmath198 contain less information than did the original signals @xmath14 , @xmath49 .",
    "however , correlation of unquantized signals also destroys information : it converts @xmath14 and @xmath49 to the single quantity @xmath205 .",
    "different information is destroyed in the two cases .",
    "moreover , correlation does not always yield the most accurate estimate of the covariance @xmath1 . as a simple example , consider the series @xmath206 and @xmath207 . here ,",
    "one easily sees that @xmath79 and @xmath85 are highly correlated . if @xmath79 and @xmath85 are known to be drawn from gaussian distributions with unit standard deviation , eq .",
    "[ rotated_ellipse ] suggests that @xmath209 .",
    "however , the correlation is @xmath210 .",
    "clearly @xmath0 is not an optimal measurement of @xmath1 .",
    "i will discuss strategies for optimal estimates of covariance elsewhere @xcite .",
    "sometimes effects of quantization are described as `` quantization noise '' : an additional source of noise that ( like `` sky noise '' or `` receiver noise '' ) reduces the correlation of the desired signal .",
    "however , unlike other sources of noise , quantization destroys information in the signals , rather than adding unwanted information .",
    "the discussion of the preceding section suggests that the amount of information that quantization destroys ( or , more loosely , the noise that it adds ) depends on what information is desired ; and that correlation removes information as well . unless the covariance @xmath1 is small , effects of quantization can not be represented as a one additional , independent source of noise , in general .",
    "the primary result of this paper is that for quantized correlation , noise can increase or decrease when covariance increases ; whereas for continuous signals it increases .",
    "this fact is important for applications requiring accurate knowledge of the noise level ; as for example in studies of rapidly - varying strong sources such as pulsars , where one wishes to know whether a change in correlation represents a significant change in the pulsar s emission ; or for single - dish or interferometric observations of intra - day variable sources , where one wishes to know whether features that may appear and disappear are statistically significant .",
    "a second result of this paper is that the signal - to - noise ratio for quantized correlation can be quite different from that expected for a continuum source , or for a continuum source with added noise .",
    "this effect is most important for large correlation , @xmath211 .",
    "correlation this large is often observed for strong sources , such as the strongest pulsars , or maser lines ; and for the strongest continuum sources observed with sensitive antennas .",
    "for example , at arecibo observatory a strong continuum source easily dominates the system temperature , at many observing frequencies .",
    "the effect will be even more common for some proposed designs of the square kilometer array .",
    "many sources show high correlation that varies dramatically with frequency ; such sources include scintillating pulsars , and maser lines .",
    "typically observations of these source involve determination of the full correlation function , and a fourier transform to obtain an estimated cross - power or autocorrelation spectrum .",
    "i discuss the properties of noise for this analysis elsewhere .",
    "an interesting question is whether one can take advantage of the higher snr afforded by quantization at high @xmath1 even for weakly - correlated signals , perhaps by adding an identical signal to each of 2 weakly covariant signals and so increasing their covariance @xmath1 , before quantizing them .",
    "the answer appears to be `` no '' . as a simple example , consider a pair of signals with covariance of @xmath212 .",
    "after correlation of @xmath213 instances of the signal , using a 4-level correlator with @xmath158 and @xmath175 , the snr is 20 , and one can determine at a level of 2 standard deviations whether @xmath214 or @xmath215 .",
    "if a single signal , with 4.6 times greater amplitude , is added to both of the original signals , then these 2 cases correspond to @xmath216 or @xmath217 . to distinguish them at 2 standard deviations",
    "requires a snr of 1400 , requiring @xmath218 samples of the quantized correlation .",
    "thus , the increase in snr is more than outweighed by the reduction in the influence on the observable .      in this paper ,",
    "i consider the result of quantizing and correlating two complex noiselike signals , @xmath2 and @xmath3 with normalized covariance @xmath1 .",
    "the signals are assumed to be statistically stationary , `` white , '' and sampled at the nyquist rate .",
    "the correlation @xmath0 provides a measurement of @xmath1 .",
    "the variation of @xmath0 about that mean , characterized by its standard deviation , provides a measure of the random part of the measurement , or noise .",
    "i suppose that the signals @xmath2 and @xmath3 are quantized to form @xmath74 and @xmath75 .",
    "i suppose that the characteristic curves that govern quantization are antisymmetric , with real and imaginary parts subject to the same characteristic curve .",
    "i recover the classic results for the noise for @xmath219 , and for the mean correlation , to first order in @xmath1 , in the limit @xmath138 @xcite .",
    "i find exact expressions for the mean correlation and the noise , and approximations valid through fourth order in @xmath1 .",
    "i compare results with simulations .",
    "agreement is excellent for the exact forms , and good for @xmath1 not too close to 1 , for the approximate expressions .",
    "i find that for nonzero values of @xmath1 , the noise varies , initially quadratically , with @xmath1 .",
    "i find that the noise in an estimate of the amplitude of @xmath1 can decrease with increasing @xmath1 ; this is opposite the behavior of noise for correlation of unquantized signals , for which noise always increases with @xmath1 .",
    "the mean correlation can increase more rapidly than linearly with @xmath1 .",
    "the signal - to - noise ratio ( snr ) for correlation of quantized signals can be greater than that for correlation of unquantized signals , for @xmath211 .",
    "in other words , correlation of quantized signals can be more efficient than correlation of the same signals before quantization , as a way of determining the covariance @xmath1 .",
    "i am grateful to the drao for supporting this work with extensive correlator time .",
    "i also gratefully acknowledge the vsop project , which is led by the japanese institute of space and astronautical science in cooperation with many organizations and radio telescopes around the world .",
    "i thank an anonymous referee for useful comments .",
    "the u.s . national science foundation provided financial support .",
    "99 anantharamaiah , k. r. , deshpande , a. a. , radhakrishnan , v. , ekers , r. d. , cornwell , t. j. , goss , w. m. 1991 , asp conf .",
    "ser . 19 : iau colloq . 131 : radio interferometry .",
    "theory , techniques , and applications , san francisco : astronomical society of the pacific , p. 6",
    "bowers , f.k . , & klingler , r.j .",
    "1974 , astron .",
    "& astrophys .",
    "ser . , 15 , 373 cooper , b.f.c .",
    "1970 , austjphys , 23 , 521 daddario , l.r . ,",
    "thompson , a.r . ,",
    "schwab , f.r .",
    ", & granlund , j. 1984 , radio sci . , 19 , 931 gwinn , c.r .",
    "2001 , apj , 554 , 1197 gwinn , c.r .",
    "2003 , in preparation .",
    "hagen , j.b . , & farley , d.t .",
    "1973 , radio sci , 8 , 775 jenet , f.a . , &",
    "anderson , s.b .",
    "1998 , pasp , 110 , 1467 jenet , f.a . ,",
    "anderson , s.b . , &",
    "prince t.a .",
    "2001 , apj , 558 , 302 kashlinsky , a. , hernndez - monteagudo , c. , & atrio - barandela , f. 2001 , apj , 557 , l1 kokkeler , a. b. j. , fridman , p. , & van ardenne , a. 2001 , experimental astronomy , 11 , 33 kulkarni , s.r .",
    "1989 , aj , 98 , 1112 meyer , s.l . 1975 , data analysis for scientists and engineers ( wiley : new york ) press , w.h . , flannery , b.p . ,",
    "teukolsky , s.a . , &",
    "vetterling , w.t .",
    "1989 , numerical recipes , cambridge uk : cambridge univ .",
    "press thompson , a.r . ,",
    "moran , j.m .",
    ", & swenson , g.w .",
    "1986 , interferometry and synthesis in radio astronomy , ( new york : wiley ) van vleck , j.h . , &",
    "middleton , d. 1966 , proc .",
    "ieee , 54 , 2"
  ],
  "abstract_text": [
    "<S> i calculate the statistics of correlation of two digitized noiselike signals : drawn from complex gaussian distributions , sampled , quantized , correlated , and averaged . averaged over many such samples , the correlation @xmath0 approaches a gaussian distribution . the mean and variance of @xmath0 </S>",
    "<S> fully characterize the distribution of @xmath0 . </S>",
    "<S> the mean corresponds to the reproducible part of the measurement , and the variance corresponds to the random part , or noise . </S>",
    "<S> i investigate the case of non - negligible covariance @xmath1 between the signals . </S>",
    "<S> noise in the correlation can increase or decrease , depending on quantizer parameters , when @xmath1 increases . </S>",
    "<S> this contrasts with correlation of continuously - valued or unquantized signals , for which the noise in phase with @xmath1 increases with increasing @xmath1 , and noise out of phase decreases . </S>",
    "<S> indeed , for some quantizer parameters , i find that correlation of quantized signals provides a more accurate estimate of @xmath1 than would correlation without quantization . </S>",
    "<S> i present analytic results in exact form and as polynomial expansions , and compare these mathematical results with results of computer simulations .    </S>",
    "<S> epsf    1 truein </S>"
  ]
}