{
  "article_text": [
    "the proliferation of online services and the thriving electronic commerce overwhelms us with alternatives in our daily lives . to handle this information overload and to help users in efficient decision making , recommender systems ( rs ) have been designed .",
    "the goal of rss is to recommend personalized items for online users when they need to choose among several items .",
    "typical problems include recommendations for which movie to watch , which jokes / books / news to read , which hotel to stay at , or which songs to listen to .",
    "one of the most popular approaches in the field of recommender systems is _ collaborative filtering _",
    "the underlying idea of cf is very simple : users generally express their tastes in an explicit way by rating the items .",
    "cf tries to estimate the users preferences based on the ratings they have already made on items and based on the ratings of other , similar users .",
    "for a recent review on recommender systems and collaborative filtering , see e.g. , @xcite .",
    "novel advances on cf show that _ dictionary learning _",
    "based approaches can be efficient for making predictions about users preferences @xcite .",
    "the dictionary learning based approach assumes that ( i ) there is a latent , unstructured feature space ( hidden representation ) behind the users ratings , and ( ii ) a rating of an item is equal to the product of the item and the user s feature . to increase the generalization capability , usually @xmath0 regularization is introduced both for the dictionary and for the users representation .",
    "there are several problems that belong to the task of dictionary learning @xcite , a.k.a .",
    "matrix factorization @xcite .",
    "this set of problems includes , for example , ( sparse ) principal component analysis @xcite , independent component analysis @xcite , independent subspace analysis @xcite , non - negative matrix factorization @xcite , and _ structured dictionary _ learning , which will be the target of our paper .",
    "one predecessor of the structured dictionary learning problem is the _ sparse coding _",
    "task @xcite , which is a considerably simpler problem . here",
    "the dictionary is already given , and we assume that the observations can be approximated well enough using only a few dictionary elements . although finding the solution that uses the minimal number of dictionary elements is np hard in general @xcite , there exist efficient approximations .",
    "one prominent example is the lasso approach @xcite , which applies convex @xmath1 relaxation to the code words .",
    "lasso does not enforce any _ group _ structure on the components of the representation ( covariates ) .    however , using _ structured sparsity _",
    ", that is , forcing different kind of structures ( e.g. , disjunct groups , trees ) on the sparse codes can lead to increased performances in several applications . indeed , as it has been theoretically proved recently structured sparsity can ease feature selection @xcite , and makes possible robust compressed sensing with substantially decreased observation number @xcite .",
    "many other real life applications also confirm the benefits of structured sparsity , for example ( i ) automatic image annotation @xcite , ( ii ) group - structured feature selection for micro array data processing @xcite , ( iii ) multi - task learning problems ( a.k.a .",
    "transfer learning ) @xcite , ( iv ) multiple kernel learning @xcite , ( v ) face recognition @xcite , and ( vi ) structure learning in graphical models @xcite . for an excellent review on structured sparsity , see @xcite .",
    "all the above mentioned examples only consider the structured sparse coding problem , where we assume that the dictionary is already given and available to us .",
    "a more interesting ( and challenging ) problem is the combination of these two tasks , i.e. , learning the best structured dictionary and structured representation .",
    "this is the _ structured dictionary learning _",
    "( sdl ) problem .",
    "sdl is more difficult ; one can find only few solutions in the literature @xcite . this novel field is appealing for ( i ) transformation invariant feature extraction @xcite , ( ii ) image denoising / inpainting @xcite , ( iii ) background subtraction @xcite , ( iv ) analysis of text corpora @xcite , and ( v ) face recognition @xcite .",
    "* our goal * is to extend the application domain of sdl in the direction of collaborative filtering .",
    "with respect to cf , further constraints appear for sdl since ( i ) online learning is desired and ( ii ) missing information is typical .",
    "there are good reasons for them : novel items / users may appear and user preferences may change over time . adaptation to users also motivate online methods .",
    "online methods have the additional advantage with respect to offline ones that they can process more instances in the same amount of time , and in many cases this can lead to increased performance . for a theoretical proof of this claim , see @xcite .",
    "furthermore , users can evaluate only a small portion of the available items , which leads to incomplete observations , missing rating values . in order to cope with these constraints of the collaborative filtering problem",
    ", we will use a novel extension of the structured dictionary learning problem , the so - called online group - structured dictionary learning ( osdl ) @xcite .",
    "osdl allows ( i ) overlapping group structures with ( ii ) non - convex sparsity inducing regularization , ( iii ) partial observation ( iv ) in an online framework .",
    "our paper is structured as follows : we briefly review the osdl problem , its cost function , and optimization method in section  [ sec : osdl problem ] .",
    "we cast the cf problem as an osdl task in section  [ sec : osdl via cf ] .",
    "numerical results are presented in section  [ sec : numerical results ] .",
    "conclusions are drawn in section  [ sec : conclusions ]",
    ".    * notations .",
    "* vectors ( @xmath2 ) and matrices ( @xmath3 ) are denoted by bold letters .",
    "@xmath4 represents the diagonal matrix with coordinates of vector @xmath2 in its diagonal .",
    "the @xmath5 coordinate of vector @xmath2 is @xmath6 .",
    "notation @xmath7 means the number of elements of a set and the absolute value for a real number . for set @xmath8",
    ", @xmath9 denotes the coordinates of vector @xmath10 in @xmath11 . for matrix @xmath12 , @xmath13 stands for the restriction of matrix @xmath3 to the rows @xmath11 .",
    "@xmath14 and @xmath15 denote the identity and the null matrices , respectively .",
    "@xmath16 is the transposed form of @xmath3 . for a vector ,",
    "the @xmath17 operator acts coordinate - wise .",
    "the @xmath18 ( quasi-)norm of vector @xmath19 is @xmath20 ( @xmath21 ) .",
    "@xmath22 denotes the @xmath23 unit sphere in @xmath24 .",
    "the point - wise and scalar products of @xmath25 are denoted by @xmath26 $ ] and by @xmath27 , respectively . for a set system @xmath28 ,",
    "the coordinates of vector @xmath29 are denoted by @xmath30 ( @xmath31 ) , that is , @xmath32 .",
    "@xmath33 is the projection of point @xmath34 to the convex closed set @xmath35 .",
    "partial derivative of function @xmath36 w.r.t .",
    "variable @xmath37 in @xmath38 is @xmath39 .",
    "the non - negative ortant of @xmath24 is @xmath40 . for sets ,",
    "@xmath41 and @xmath42 denote direct product and difference , respectively .",
    "in this section we briefly review the osdl approach , which will be our major tool to solve the cf problem .",
    "the osdl cost function is treated in section  [ sec : osdl : cost ] , its optimization idea is detailed in section  [ sec : osdl : optimization ] .",
    "the online group - structured dictionary learning ( osdl ) task is defined with the following quantities .",
    "let the dimension of the observations be denoted by @xmath43 .",
    "assume that in each time instant ( @xmath44 ) a set @xmath45 is given , that is , we know which coordinates are observable at time @xmath46 , and the observation is @xmath47 .",
    "our goal is to find a dictionary @xmath48 that can approximate the observations @xmath47 well from the linear combination of its columns .",
    "the columns of @xmath49 are assumed to belong to a closed , convex , and bounded set @xmath50 .",
    "to formulate the cost of dictionary @xmath49 , first a _",
    "fixed _ time instant @xmath46 , observation @xmath47 , dictionary @xmath49 is considered , and the hidden representation @xmath51 associated to this @xmath52 triple is defined .",
    "representation @xmath51 is allowed to belong to a closed , convex set @xmath53 ( @xmath54 ) with certain structural constraints .",
    "the structural constraint on @xmath51 are expressed by making use of a given @xmath28 group structure , which is a set system ( also called hypergraph ) on @xmath55 .",
    "it is also assumed that weight vectors @xmath56 ( @xmath31 ) are available for us and that they are positive on @xmath57 and @xmath58 otherwise .",
    "representation @xmath59 belonging to a triple @xmath60 is defined as the solution of the structured sparse coding task @xmath61,\\label{eq : l}\\end{aligned}\\ ] ] where @xmath62 denotes the loss , @xmath63 , and @xmath64 is the structured regularizer associated to @xmath28 and @xmath65 , @xmath66 . here",
    ", the first term of is responsible for the quality of approximation on the observed coordinates , whereas for @xmath67 the other term [ ] constrains the solution according to the group structure @xmath28 similarly to the sparsity inducing regularizer @xmath68 in @xcite : it eliminates the terms @xmath69 @xmath70 by means of @xmath71 .",
    "the osdl problem is defined as the minimization of the cost function : @xmath72 that is , the goal is to minimize the average loss belonging to the dictionary , where @xmath73 is a non - negative forgetting factor . if @xmath74 , the classical average @xmath75 is recovered .    as an example , let @xmath76 ( @xmath77 ) , @xmath78 . in this case , columns of @xmath49 are restricted to the euclidean unit sphere and we have no constraints for @xmath59 .",
    "now , let @xmath79 and @xmath80 , where @xmath81 represents the @xmath5 node and its children in a fixed tree .",
    "then the coordinates @xmath82 are searched in a hierarchical tree structure and the hierarchical dictionary @xmath49 is optimized accordingly .",
    "optimization of cost function is equivalent to the joint optimization of dictionary @xmath49 and representation @xmath83 : @xmath84 where @xmath85 .",
    "\\label{eq : f(d,{alfa})}\\ ] ] @xmath49 is optimized by using the sequential observations @xmath47 online in an alternating manner :    1 .",
    "the actual dictionary estimation @xmath86 and sample @xmath87 is used to optimize for representation @xmath88 .",
    "2 .   for the estimated representations @xmath83 , the dictionary estimation @xmath89",
    "is derived from the quadratic optimization problem @xmath90      note that is a non - convex optimization problem with respect to @xmath59 .",
    "the variational properties of norm @xmath91 can be used to overcome this problem .",
    "one can show , alike to @xcite , that by introducing an auxiliary variable @xmath92 , the solution @xmath59 of the optimization task is equal to the solution of : @xmath93 @xmath94 @xmath95 and @xmath96 .",
    "the optimization of can be carried out by iterative alternating steps .",
    "one can minimize the quadratic cost function on the convex set @xmath97 for a given @xmath98 with standard solvers @xcite .",
    "then , one can use the variation principle and find solution @xmath99 for a fixed @xmath59 by means of the explicit expression @xmath100 note that for numerical stability , smoothing @xmath101 ( @xmath102 ) is suggested in practice .",
    "the block - coordinate descent ( bcd ) method @xcite is used for the optimization of @xmath49 : columns @xmath103 in @xmath49 are optimized one - by - one by keeping the other columns ( @xmath104 ) fixed . for a given @xmath105 , @xmath106 is quadratic in @xmath103 .",
    "the minimum is found by solving @xmath107 , and then this solution is projected to the constraint set @xmath108 ( @xmath109 ) .",
    "one can show by executing the differentiation that @xmath110 satisfies the linear equation system @xmath111 where @xmath112,\\end{aligned}\\ ] ] matrices @xmath113 are diagonal , @xmath114 , and @xmath115 is the diagonal matrix representation of the @xmath116 set ( for @xmath117 the @xmath118 diagonal is 1 and is @xmath58 otherwise ) .",
    "it is sufficient to update statistics @xmath119 online for the optimization of @xmath106 , which can be done exactly for @xmath113 and @xmath120 : @xmath121 where @xmath122 and the recursions are initialized by ( i ) @xmath123 , @xmath124 for @xmath74 and ( ii ) in an arbitrary way for @xmath125 . according to numerical experiences",
    ", @xmath126 is a good approximation for @xmath127 with the actual estimation @xmath128 and with initialization @xmath129 .",
    "it may be worth noting that the convergence speed is often improved if statistics are updated in mini - batches @xmath130 .",
    "we formulate the cf task as an osdl optimization problem in section  [ sec : cf casted as osdl ] . according to the cf literature ,",
    "oftentimes neighbor - based corrections improve the precision of the estimation .",
    "we also use this technique ( section  [ sec : nn correction ] ) to improve the osdl estimations .",
    "below , we transform the cf task into an osdl problem .",
    "consider the @xmath131 user s known ratings as osdl observations @xmath87 .",
    "let the optimized group - structured dictionary on these observations be @xmath49 .",
    "now , assume that we have a test user and his / her ratings , i.e. , @xmath132 .",
    "the task is to estimate @xmath133 , that is , the missing coordinates of @xmath37 ( the missing ratings of the user ) that can be accomplished as follows :    1 .",
    "remove the rows of the non - observed @xmath134 coordinates from @xmath49 .",
    "the obtained @xmath135 sized matrix @xmath136 and @xmath137 can be used to estimate @xmath59 by solving the structured sparse coding problem .",
    "2 .   using the estimated representation @xmath59 , estimate @xmath37 as @xmath138      according to the cf literature , neighbor based correction schemes may further improve the precision of the estimations @xcite .",
    "this neighbor correction approach    * relies on the assumption that similar items ( e.g. , jokes / movies ) are rated similarly and * can be adapted to osdl - based cf estimation in a natural fashion .    here",
    ", we detail the idea .",
    "let us assume that the similarities @xmath139 ( @xmath140 ) between individual items are given .",
    "we shall provide similarity forms in section  [ sec : item similarities ] .",
    "let @xmath141 be the osdl estimation for the rating of the @xmath142 non - observed item of the @xmath131 user ( @xmath143 ) , where @xmath144 is the @xmath142 row of matrix @xmath145 , and @xmath146 is computed according to section  [ sec : cf casted as osdl ] .",
    "let the prediction error on the observable item neighbors ( @xmath105 ) of the @xmath142 item of the @xmath131 user ( @xmath147 ) be @xmath148 .",
    "these prediction errors can be used for the correction of the osdl estimation ( @xmath149 ) by taking into account the @xmath150 similarities : @xmath151,\\text { or}\\label{eq : xhat}\\\\      \\hat{x}_{kt } & = \\gamma_0({\\mathbf}{d}_k\\bm{\\alpha}_t ) + \\gamma_1 \\left[\\frac{\\sum_{j\\in o_t\\backslash \\{k\\}}s_{kj}({\\mathbf}{d}_j\\bm{\\alpha}_t - x_{jt})}{\\sum_{j\\in o_t\\backslash \\{k\\}}s_{kj}}\\right],\\label{eq : xhat - with-0}\\end{aligned}\\ ] ] where @xmath143 . here",
    ", is analogous to the form of @xcite , is a simple modification : it modulates the first term with a separate @xmath152 weight .",
    "we have chosen the jester dataset ( section  [ sec : jester ] ) for the illustration of the osdl based cf approach .",
    "it is a standard benchmark for cf .",
    "we detail our preferred item similarities in section  [ sec : item similarities ] . to evaluate the cf based estimation , we use the performance measures given in section  [ sec : performance measure ] .",
    "section  [ sec : evaluation ] is about our numerical experiences .",
    "the dataset @xcite contains @xmath153 ratings from @xmath154 users to @xmath155 jokes on a continuous @xmath156 $ ] range .",
    "the worst and best possible gradings are @xmath157 and @xmath158 , respectively .",
    "a fixed @xmath159 element subset of the jokes is called gauge set and it was evaluated by all users .",
    "two third of the users have rated at least @xmath160 jokes , and the remaining ones have rated between @xmath161 and @xmath162 jokes .",
    "the average number of user ratings per joke is @xmath163 .      in the neighbor correction step or",
    "we need the @xmath150 values representing the similarities of the @xmath5 and @xmath118 items .",
    "we define this value as the similarity of the @xmath5 and @xmath118 rows ( @xmath164 and @xmath103 ) of the optimized osdl dictionary @xmath49 @xcite : @xmath165 where @xmath166 is the parameter of the similarity measure .",
    "quantities @xmath150 are non - negative ; if the value of @xmath150 is close to zero ( large ) then the @xmath5 and @xmath118 items are very different ( very similar ) .      in our numerical experiments we used the rmse ( root mean square error ) and the mae ( mean absolute error ) measure for the evaluation of the quality of the estimation , since these are the most popular measures in the cf literature .",
    "the rmse and mae measure is the average squared / absolute difference of the true and the estimated rating values , respectively : @xmath167 where @xmath168 denotes either the validation or the test set .",
    "here we illustrate the efficiency of the osdl - based cf estimation on the jester dataset ( section  [ sec : jester ] ) using the rmse and mae performance measures ( section  [ sec : performance measure ] ) .",
    "we start our discussion with the rmse results .",
    "the mae performance measure led to similar results ; for the sake of completeness we report these results at the end of this section . to the best of our knowledge ,",
    "the top results on this database are rmse = @xmath169 @xcite and rmse = @xmath170 @xcite .",
    "both works are from the same authors .",
    "the method in the first paper is called item neighbor and it makes use of only neighbor information . in @xcite ,",
    "the authors used a bridge regression based unstructured dictionary learning model  with a neighbor correction scheme , they optimized the dictionary by gradient descent and set @xmath171 to 100 .",
    "these are our performance baselines .    to study the capability of the osdl approach in cf , we focused on the following issues :    * is structured dictionary @xmath49 beneficial for prediction purposes , and how does it compare to the dictionary of classical ( unstructured ) sparse dictionary ?",
    "* how does the osdl parameters and the similarity / neighbor correction applied affect the efficiency of the prediction ?",
    "* how do different group structures @xmath28 fit to the cf task ?    in our numerical studies we chose the euclidean unit sphere for @xmath172 ( @xmath77 ) , and @xmath78 , and no additional weighting was applied ( @xmath173 , @xmath174 , where @xmath175 is the indicator function ) .",
    "we set @xmath91 of the group - structured regularizer @xmath68 to @xmath176 .",
    "group structure @xmath28 of vector @xmath59 was realized on    * a @xmath177 toroid ( @xmath178 ) with @xmath79 applying @xmath179 neighbors to define @xmath28 . for @xmath180 ( @xmath181 ) the classical sparse representation based dictionary",
    "is recovered . * a hierarchy with a complete binary tree structure . in this case : * * @xmath79 , and group @xmath57 of @xmath82 contains the @xmath5 node and its descendants on the tree , and * * the size of the tree is determined by the number of levels @xmath182 .",
    "the dimension of the hidden representation is then @xmath183 .",
    "the size @xmath184 of mini - batches was set either to @xmath185 , or to @xmath186 and the forgetting factor @xmath73 was chosen from set @xmath187 .",
    "the @xmath188 weight of structure inducing regularizer @xmath68 was chosen from the set @xmath189 .",
    "we studied similarities @xmath190 , @xmath191 [ see - ] with both neighbor correction schemes [ - ] . in",
    "what follows , corrections based on and will be called @xmath190 , @xmath191 and @xmath192 , @xmath193 , respectively .",
    "similarity parameter @xmath194 was chosen from the set @xmath195 . in the bcd step of the optimization of @xmath49",
    ", @xmath196 iterations were applied . in the @xmath59 optimization step ,",
    "we used @xmath196 iterations , whereas smoothing parameter @xmath197 was @xmath198 .",
    "we used a @xmath199 random split for the observable ratings in our experiments , similarly to @xcite :    * training set ( @xmath200 ) was further divided into 2 parts : * * we chose the @xmath201 observation set @xmath202 randomly , and optimized @xmath49 according to the corresponding @xmath87 observations , * * we used the remaining @xmath203 for validation , that is for choosing the optimal osdl parameters ( @xmath204 or @xmath182 , @xmath188 , @xmath73 ) , bcd optimization parameter ( @xmath184 ) , neighbor correction ( @xmath190 , @xmath191 , @xmath192 , @xmath193 ) , similarity parameter ( @xmath194 ) , and correction weights ( @xmath205s in or ) . *",
    "we used the remaining @xmath203 of the data for testing .",
    "the optimal parameters were estimated on the validation set , and then used on the test set .",
    "the resulting rmse / mae score was the performance of the estimation .      in this section",
    "we provide results using toroid group structure .",
    "we set @xmath206 .",
    "the size of the toroid was @xmath207 , and thus the dimension of the representation was @xmath208 .    in the * first experiment * we study how the size of neighborhood ( @xmath204 ) affects the results .",
    "this parameter corresponds to the `` smoothness '' imposed on the group structure : when @xmath180 , then there is no relation between the @xmath209 columns in @xmath49 ( no structure ) .",
    "as we increase @xmath204 , the @xmath210 feature vectors will be more and more aligned in a smooth way . to this end , we set the neighborhood size to @xmath180 ( no structure ) , and then increased it to @xmath211 , @xmath212 , @xmath213 , @xmath214 , and @xmath196 .",
    "for each @xmath215 , we calculated the rmse of our estimation , and then for each fixed ( @xmath216 ) pair , we minimized these rmse values in @xmath194 .",
    "the resulting validation and test surfaces are shown in fig .",
    "[ fig : torus : validation surfaces ] . for the best ( @xmath216 ) pair",
    ", we also present the rmse values as a function of @xmath194 ( fig .",
    "[ fig : torus : validation vs test curve ] ) . in this illustration",
    "we used @xmath192 neighbor correction and @xmath217 mini - batch size .",
    "we note that we got similar results using @xmath218 too .",
    "our results can be summarized as follows .",
    "* for a fixed neighborhood parameter @xmath204 , we have that : * * the validation and test surfaces are very similar ( see fig .  [",
    "fig : torus : validation surfaces](e)-(f ) ) .",
    "it implies that the validation surfaces are good indicators for the test errors . for the best @xmath204 , @xmath188 and @xmath73 parameters",
    ", we can observe that the validation and test curves ( as functions of @xmath194 ) are very similar .",
    "this is demonstrated in fig .",
    "[ fig : torus : validation vs test curve ] , where we used @xmath219 neighborhood size and @xmath192 neighbor correction .",
    "we can also notice that ( i ) both curves have only one local minimum , and ( ii ) these minimum points are close to each other .",
    "* * the quality of the estimation depends mostly on the @xmath188 regularization parameter .",
    "as we increase @xmath204 , the best @xmath188 value is decreasing . * * the estimation is robust to the different choices of forgetting factors ( see fig .",
    "[ fig : torus : validation surfaces](a)-(e ) ) .",
    "in other words , this parameter @xmath73 can help in fine - tuning the results .",
    "* structured dictionaries ( @xmath220 ) are advantageous over those methods that do not impose structure on the dictionary elements ( @xmath180 ) . for @xmath192 and @xmath193 neighbor corrections ,",
    "we summarize the rmse results in table  [ tab : torus : perf : r ] .",
    "based on this table we can conclude that in the studied parameter domain * * the estimation is robust to the selection of the mini - batch size ( @xmath184 ) .",
    "we got the best results using @xmath217 . similarly to the role of parameter @xmath73 , adjusting @xmath184 can be used for fine - tuning .",
    "* * the @xmath192 neighbor correction lead to the smallest rmse value . * * when we increase @xmath204 up to @xmath219 , the results improve .",
    "however , for @xmath221 , the rmse values do not improve anymore ; they are about the same that we have using @xmath219 . * * the smallest rmse we could achieve was @xmath222 , and the best known result so far was rmse = @xmath169 @xcite .",
    "this proves the efficiency of our osdl based collaborative filtering algorithm . * * we note that our rmse result seems to be significantly better than the that of the competitors : we repeated this experiment @xmath196 more times with different randomly selected training , test , and validation sets , and our rmse results have never been worse than @xmath223 .     +    , regularization weight @xmath224 , forgetting factor @xmath225 , mini - batch size @xmath217 , and similarity parameter @xmath226 .",
    "the applied neighbor correction was @xmath192.,width=234 ]    in the * second experiment * we studied how the different neighbor corrections ( @xmath190 , @xmath191 , @xmath192 , @xmath193 ) affect the performance of the proposed algorithm .",
    "to this end , we set the neighborhood parameter to @xmath219 because it proved to be optimal in the previous experiment .",
    "our results are summarized in table  [ tab : torus : perf : s ] . from these results",
    "we can observe that    * our method is robust to the selection of correction methods .",
    "similarly to the @xmath73 and @xmath184 parameters , the neighbor correction scheme can help in fine - tuning the results . *",
    "the introduction of @xmath152 in with the application of @xmath192 and @xmath193 instead of @xmath190 and @xmath191 proved to be advantageous in the neighbor correction phase .",
    "* for the studied cf problem , the @xmath192 neighbor correction method ( with @xmath217 ) lead to the smallest rmse value , @xmath222 .",
    "* the @xmath227 setting yielded us similarly good results .",
    "even with @xmath218 , the rmse value was @xmath228 .",
    ".performance ( rmse ) of the osdl prediction using toroid group structure ( @xmath28 ) with different neighbor sizes @xmath204 ( @xmath180 : unstructured case ) .",
    "first - second row : mini - batch size @xmath217 , third - fourth row : @xmath218 .",
    "odd rows : @xmath192 , even rows : @xmath193 neighbor correction . for fixed @xmath184 ,",
    "the best performance is highlighted with boldface typesetting . [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]     [ tab : mae , torus : perf : s ]",
    "we have dealt with collaborative filtering ( cf ) based recommender systems and extended the application domain of structured dictionaries to cf .",
    "we used online group - structured dictionary learning ( osdl ) to solve the cf problem ; we casted the cf estimation task as an osdl problem .",
    "we demonstrated the applicability of our novel approach on joke recommendations .",
    "our extensive numerical experiments show that structured dictionaries have several advantages over the state - of - the - art cf methods : more precise estimation can be obtained , and smaller dimensional feature representation can be sufficient by applying group structured dictionaries .",
    "moreover , the estimation behaves robustly as a function of the osdl parameters and the applied group structure .",
    "the project is supported by the european union and co - financed by the european social fund ( grant agreements no .",
    "tmop 4.2.1/b-09/1/kmr-2010 - 0003 and kmop-1.1.2 - 08/1 - 2008 - 0002 ) .",
    "the research was partly supported by the department of energy ( grant number desc0002607 ) .",
    "d.  m. witten , r.  tibshirani , and t.  hastie , `` a penalized matrix decomposition , with applications to sparse principal components and canonical correlation analysis , '' _ biostatistics _ , vol .",
    "10 , no .  3 , pp .",
    "515534 , 2009 .",
    "j.  a. tropp and s.  j. wright , `` computational methods for sparse solution of linear inverse problems , '' _ proc . of the ieee special issue on applications of sparse representation and compressive",
    "sensing _ ,",
    "98 , no .  6 , pp . 948958 , 2010 ."
  ],
  "abstract_text": [
    "<S> structured sparse coding and the related structured dictionary learning problems are novel research areas in machine learning . in this paper </S>",
    "<S> we present a new application of structured dictionary learning for collaborative filtering based recommender systems . </S>",
    "<S> our extensive numerical experiments demonstrate that the presented technique outperforms its state - of - the - art competitors and has several advantages over approaches that do not put structured constraints on the dictionary elements .    </S>",
    "<S> collaborative filtering , structured dictionary learning </S>"
  ]
}