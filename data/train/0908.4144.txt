{
  "article_text": [
    "boosting algorithms @xcite have become very successful in machine learning .",
    "this study revisits _",
    "logitboost_@xcite under the framework of _ adaptive base class boost ( abc - boost ) _ in @xcite , for multi - class classification .",
    "we denote a training dataset by @xmath0 , where @xmath1 is the number of feature vectors ( samples ) , @xmath2 is the @xmath3th feature vector , and @xmath4 is the @xmath3th class label , where @xmath5 in multi - class classification .    both _",
    "logitboost_@xcite and _ mart _ ( multiple additive regression trees)@xcite algorithms can be viewed as generalizations to the logistic regression model , which assumes the class probabilities @xmath6 to be @xmath7 while traditional logistic regression assumes @xmath8 , _ logitboost _ and _ mart _ adopt the flexible `` additive model , '' which is a function of @xmath9 terms : @xmath10 where @xmath11 , the base learner , is typically a regression tree .",
    "the parameters , @xmath12 and @xmath13 , are learned from the data , by maximum likelihood , which is equivalent to minimizing the _ negative log - likelihood loss _",
    "@xmath14 where @xmath15 if @xmath16 and @xmath17 otherwise .    for identifiability , the `` sum - to - zero '' constraint , @xmath18 ,",
    "is usually adopted @xcite .      as described in alg .",
    "[ alg_logitboost ] , @xcite builds the additive model ( [ eqn_f_m ] ) by a greedy stage - wise procedure , using a second - order ( diagonal ) approximation , which requires knowing the first two derivatives of the loss function ( [ eqn_loss ] ) with respective to the function values @xmath19 .",
    "@xcite obtained : @xmath20 those derivatives can be derived by assuming no relations among @xmath19 , @xmath21 to @xmath22 .",
    "however , @xcite used the `` sum - to - zero '' constraint @xmath18 throughout the paper and they provided an alternative explanation .",
    "@xcite showed ( [ eqn_mart_d1d2 ] ) by conditioning on a `` base class '' and noticed the resultant derivatives are independent of the particular choice of the base class .",
    "0 : @xmath15 , if @xmath23 , @xmath24 otherwise .",
    "+ 1 : @xmath25 ,   @xmath26 ,  @xmath21 to @xmath22 ,  @xmath27 to @xmath1 + 2 : for @xmath28 to @xmath9 do + 3 : for @xmath29 to @xmath22 , do + 4 : compute @xmath30 . + 5 : compute @xmath31 .",
    "+ 6 : fit the function @xmath32 by a weighted least - square of @xmath33 to @xmath2 with weights @xmath34 .",
    "+ 7 : @xmath35 + 8 : end + 9 : @xmath36 ,  @xmath21 to @xmath22 ,  @xmath27 to @xmath1 + 10 : end    at each stage , _ logitboost _ fits an individual regression function separately for each class . this is analogous to the popular _ individualized regression _ approach in multinomial logistic regression , which is known @xcite to result in loss of statistical efficiency , compared to the full ( conditional ) maximum likelihood approach .    on the other hand , in order to use trees as base learner , the diagonal approximation appears to be a must , at least from the practical perspective .",
    "@xcite derived the derivatives of ( [ eqn_loss ] ) under the sum - to - zero constraint . without loss of generality",
    ", we can assume that class 0 is the base class .",
    "for any @xmath37 , @xmath38 the base class must be identified at each boosting iteration during training .",
    "@xcite suggested an exhaustive procedure to adaptively find the best base class to minimize the training loss ( [ eqn_loss ] ) at each iteration .",
    "@xcite combined the idea of _ abc - boost _ with _ mart_. the algorithm , _ abc - mart _ , achieved good performance in multi - class classification on the datasets used in @xcite .",
    "we propose _ abc - logitboost _ , by combining _",
    "abc - boost _ with _ robust logitboost_@xcite .",
    "our extensive experiments will demonstrate that _ abc - logitboost _ can considerably improve _ logitboost _ and _ abc - mart _ on a variety of datasets .",
    "our work is based on _ robust logitboost_@xcite , which differs from the original _ logitboost _ algorithm .",
    "thus , this section provides an introduction to _",
    "robust logitboost_.    @xcite commented that _ logitboost _ ( alg . [ alg_logitboost ] ) can be numerically unstable .",
    "the original paper@xcite suggested some `` crucial implementation protections '' on page 17 of @xcite :    * in line 5 of alg .",
    "[ alg_logitboost ] , compute the response @xmath33 by @xmath39 ( if @xmath40 ) or @xmath41 ( if @xmath42 ) .",
    "* bound the response @xmath43 by @xmath44 $ ] .",
    "note that the above operations are applied to each individual sample .",
    "the goal is to ensure that the response @xmath43 is not too large ( note that @xmath45 always ) . on the other hand",
    ", we should hope to use larger @xmath43 to better capture the data variation .",
    "therefore , the thresholding occurs very frequently and it is expected that some of the useful information is lost .",
    "@xcite demonstrated that , if implemented carefully , _",
    "logitboost _ is almost identical to _ mart_. the only difference is the tree - splitting criterion .",
    "consider @xmath1 weights @xmath46 , and @xmath1 response values @xmath47",
    ", @xmath48 to @xmath1 , which are assumed to be ordered according to the sorted order of the corresponding feature values .",
    "the tree - splitting procedure is to find the index @xmath49 , @xmath50 , such that the weighted mean square error ( mse ) is reduced the most if split at @xmath49 .",
    "that is , we seek @xmath49 to maximize @xmath51\\end{aligned}\\ ] ] where @xmath52 , @xmath53 , and @xmath54 .",
    "after simplification , we obtain @xmath55 ^ 2}{\\sum_{i=1}^s w_i}+\\frac{\\left[\\sum_{i = s+1}^n z_iw_i\\right]^2}{\\sum_{i = s+1}^{n } w_i } - \\frac{\\left[\\sum_{i=1}^n z_iw_i\\right]^2}{\\sum_{i=1}^n w_i}\\end{aligned}\\ ] ] plugging in @xmath56 , and @xmath57 as in alg .",
    "[ alg_logitboost ] , yields , @xmath58 ^ 2}{\\sum_{i=1}^s p_{i , k}(1-p_{i , k})}+\\frac{\\left[\\sum_{i = s+1}^n r_{i , k } - p_{i , k } \\right]^2}{\\sum_{i = s+1}^{n } p_{i , k}(1-p_{i , k } ) } - \\frac{\\left[\\sum_{i=1}^n r_{i , k } - p_{i , k } \\right]^2}{\\sum_{i=1}^n p_{i , k}(1-p_{i , k})}.\\end{aligned}\\ ] ] because the computations involve @xmath59 as a group , this procedure is actually numerically stable . + in comparison , _",
    "mart_@xcite only used the first order information to construct the trees , i.e. , @xmath60 ^ 2+\\left[\\sum_{i = s+1}^n r_{i , k } - p_{i , k } \\right]^2 - \\left[\\sum_{i=1}^n r_{i , k } - p_{i , k } \\right]^2.\\end{aligned}\\ ] ]      1 : @xmath25 , @xmath26 , @xmath21 to @xmath22 , @xmath27 to @xmath1 + 2 : for @xmath28 to @xmath9 do + 3 : for @xmath29 to @xmath22 do + 4 : @xmath61-terminal node regression tree from @xmath62 , + : with weights @xmath63 as in section 2.1 . + 5 : @xmath64 + 6 : @xmath65 + 7 : end + 8 : @xmath36 ,  @xmath21 to @xmath22 ,  @xmath27 to @xmath1 + 9 : end    alg .",
    "[ alg_robust_logitboost ] describes _ robust logitboost _ using the tree - splitting criterion developed in section [ sec_split ] .",
    "note that after trees are constructed , the values of the terminal nodes are computed by @xmath66 which explains line 5 of alg .",
    "[ alg_robust_logitboost ] .",
    "[ alg_robust_logitboost ] has three main parameters , to which the performance is not very sensitive , as long as they fall in some reasonable range .",
    "this is a very significant advantage in practice .",
    "the number of terminal nodes , @xmath67 , determines the capacity of the base learner .",
    "@xcite suggested @xmath69 .",
    "@xcite commented that @xmath70 is unlikely . in our experience , for large datasets ( or moderate datasets in high - dimensions ) , @xmath71 is often a reasonable choice ; also see @xcite .",
    "the shrinkage , @xmath68 , should be large enough to make sufficient progress at each step and small enough to avoid over - fitting .",
    "@xcite suggested @xmath72 .",
    "normally , @xmath73 is used .    the number of iterations , @xmath9 , is largely determined by the affordable computing time .",
    "a commonly - regarded merit of boosting is that over - fitting can be largely avoided for reasonable @xmath67 and @xmath68 .",
    "1 : @xmath25 ,   @xmath26 ,  @xmath21 to @xmath22 ,  @xmath27 to @xmath1 + 2 : for @xmath28 to @xmath9 do + 3 : for @xmath74 to @xmath22 , do + 4 : for @xmath29 to @xmath22 , @xmath75 , do + 5 : @xmath61-terminal node regression tree from @xmath76 + : with weights @xmath77 , as in section 2.1 .",
    "+ 6 : @xmath78 +   + 7 : @xmath79 + 8 : end + 9 : @xmath80 + 10 : @xmath81 + 11 : @xmath82 + 12 : end + 13 : @xmath83 + 14 : @xmath84 + 15 : @xmath36 + 16 : end    the recently proposed _",
    "abc - boost _ @xcite algorithm consists of two key components :    1 .   using the widely - used _ sum - to - zero _",
    "constraint@xcite on the loss function , one can formulate boosting algorithms only for @xmath22 classes , by using one class as the * base class*. 2 .   at each boosting iteration ,",
    "* adaptively * select the base class according to the training loss .",
    "@xcite suggested an exhaustive search strategy .",
    "@xcite combined _ abc - boost _ with _ mart _ to develop _ abc - mart_. @xcite demonstrated the good performance of _ abc - mart _ compared to _",
    "mart_. this study will illustrate that * _ abc - logitboost _ * , the combination of _ abc - boost _ with _ ( robust ) logitboost _ , will further reduce the test errors , at least on a variety of datasets .",
    "[ alg_abc - logitboost ] presents _ abc - logitboost _ , using the derivatives in ( [ eqn_abc_derivatives ] ) and the same exhaustive search strategy as in _ abc - mart_. again , _",
    "abc - logitboost _ differs from _ abc - mart _ only in the tree - splitting procedure ( line 5 in alg .",
    "[ alg_abc - logitboost ] ) .",
    "table [ tab_data ] lists the datasets in our experiments , which include all the datasets used in @xcite , plus _",
    "mnist10k_. ] .",
    ".for _ letter , pendigits , zipcode , optdigits _ and _ isolet _ , we used the standard ( default ) training and test sets . for _ covertype",
    "_ , we use the same split in @xcite . for _ mnist10k _ , we used the original 10000 test samples in the original _ mnist _ dataset for training , and the original 60000 training samples for testing .",
    "also , as explained in @xcite , _ letter2k _",
    "( _ letter4k _ ) used the last 2000 ( 4000 ) samples of _ letter _ for training and the remaining 18000 ( 16000 ) for testing , from the original _",
    "letter _ dataset . [ cols=\"<,>,>,>,>\",options=\"header \" , ]     [ tab_covertype ]    the results on _ covertype _ are reported differently from other datasets .",
    "_ covertype _ is fairly large . building a very large model ( e.g.",
    ", @xmath85 boosting steps ) would be expensive .",
    "testing a very large model at run - time can be costly or infeasible for certain applications ( e.g. , search engines ) .",
    "therefore , it is often important to examine the performance of the algorithm at much earlier boosting iterations .",
    "table [ tab_covertype ] shows that _ abc - logitboost _ may improve _ logitboost _ as much as @xmath86 , as opposed to the reported @xmath87 in table [ tab_summary ] .",
    "[ tab_letter2k ]      [ tab_letter4k ]      [ tab_letter ]      [ tab_pendigits ]      [ tab_zipcode ]      [ tab_optdigits ]      for this dataset , @xcite only experimented with @xmath88 for _ mart _ and _ abc - mart_. we add the experiment results for @xmath89 .",
    "[ tab_isolet ]    [ tab_isolet_mart ]",
    "multi - class classification is a fundamental task in machine learning .",
    "this paper presents the _ abc - logitboost _ algorithm and demonstrates its considerable improvements over _ logitboost _ and _ abc - mart _ on a variety of datasets .",
    "+ there is one interesting uci dataset named _ poker _ , with 25k training samples and 1 million testing samples .",
    "our experiments showed that _ abc - boost _ could achieve an accuracy @xmath90 ( i.e. , the error rate @xmath91 ) .",
    "interestingly , using libsvm , an accuracy of about @xmath92 was obtained .",
    "we will report the results in a separate paper ."
  ],
  "abstract_text": [
    "<S> we develop _ abc - logitboost _ , based on the prior work on _ abc - boost_@xcite and _ robust logitboost_@xcite . </S>",
    "<S> our extensive experiments on a variety of datasets demonstrate the considerable improvement of _ abc - logitboost _ over _ logitboost _ and _ abc - mart_. </S>"
  ]
}