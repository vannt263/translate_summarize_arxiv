{
  "article_text": [
    "in this paper we consider a _ state space model _ where a sequence @xmath1 is modeled as a noisy observation of a markov chain @xmath2 , called the _ state sequence _ , which is hidden .",
    "the observed values of @xmath3 are conditionally independent given the hidden states @xmath4 and the corresponding conditional distribution of @xmath5 depends on @xmath6 only . when operating on a model of this form the _ joint smoothing distribution _",
    ", that is , the joint distribution of @xmath7 given @xmath8 , and its marginals will be of interest . of particular interest",
    "is the _ filter distribution _ , defined as the marginal of this law with respect to  the component @xmath9",
    "is referred to .",
    "computing these posterior distributions will be the key issue when filtering the hidden states as well as performing inference on unknown model parameters .",
    "the posterior distribution can be recursively updated as new observations become available  making single - sweep processing of the data possible  by means of the so - called _ smoothing recursion_. however , in general this recursion can not be applied directly since it involves the evaluation of complicated high - dimensional integrals .",
    "in fact , closed form solutions are obtainable only for linear / gaussian models ( where the solutions are acquired using the _ disturbance smoother _ ) and models where the state space of the latent markov chain is finite .",
    "_ sequential monte carlo _ ( smc ) _ methods _ , often alternatively termed _ particle filters _ , provide a helpful tool for computing approximate solutions to the smoothing recursion for general state space models , and the field has seen a drastic increase in interest over recent years .",
    "these methods are based on the principle of , recursively in time , approximating the smoothing distribution with the empirical measure associated with a weighted sample of _ particles_. at present time there are various techniques for producing and updating such a particle sample ( see * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) . for a comprehensive treatment of the theoretical aspects of smc methods we refer to the work by @xcite .",
    "in this article we analyse the _ auxiliary particle filter _ ( apf ) proposed by @xcite , which has proved to be one of the most useful and widely adopted implementations of the smc methodology . unlike the traditional _ bootstrap particle filter _",
    "@xcite , the apf enables the user to affect the particle sample allocation by designing freely a set of _ first - stage importance weights _ involved in the selection procedure .",
    "prevalently , this has been used for assigning large weight to particles whose offsprings are likely to land up in zones of the state space having high posterior probability . despite its obvious appeal , it is however not clear how to optimally exploit this additional degree of freedom .    in order to better understand this issue , we present an asymptotical analysis ( being a continuation of @xcite and based on recent results by @xcite on weighted systems of particles ) of the algorithm . more specifically , we establish clts ( theorems  [ th : clt : tss ] and [ th : clt : ssapf ] ) , with explicit expressions of the asymptotic variances , for two different versions ( differentiated by the absence / presence of a concluding resampling pass at the end of each loop ) of the algorithm under general model specifications .",
    "the convergence bear upon an increasing number of particles , and a recent result in the same spirit has , independently of @xcite , been stated in the manuscript @xcite . using these results , we also  and this is the main contribution of the paper  identify first - stage importance weights which are asymptotically most efficient .",
    "this result provides important insights in optimal sample allocation for particle filters in general , and we also give an interpretation of the finding in terms of variance reduction for stratified sampling .",
    "in addition , we prove ( utilising a decomposition of the monte carlo error proposed by @xcite and refined by @xcite ) time uniform convergence in @xmath0 ( theorem  [ th : main : deviation : theorem ] ) under more stringent assumptions of ergodicity of the conditional hidden chain .",
    "with support of this stability result and the asymptotic analysis we conclude that inserting a final selection step at the end of each loop is  at least as long as the number of particles used in the two stages agree  superfluous , since such an operation exclusively increases the asymptotic variance .",
    "finally , in the implementation section ( section  [ section : implementations ] ) several heuristics , derived from the obtained results , for designing efficient first - stage weights are discussed , and the improvement implied by approximating the asymptotically optimal first - stage weights is demonstrated on several examples .",
    "we denote by @xmath10 , @xmath11 , and @xmath12 the state space , transition kernel , and initial distribution of @xmath4 , respectively , and assume that all random variables are defined on a common probability space @xmath13 .",
    "in addition we denote by @xmath14 the state space of @xmath3 and suppose that there exists a measure @xmath15 and , for all @xmath16 , a non - negative function @xmath17 such that , for @xmath18 , @xmath19 , @xmath20 .",
    "introduce , for @xmath21 , the vector notation @xmath22 ; similar notation will be used for other quantities .",
    "the joint smoothing distribution of denoted by @xmath23 and a straightforward application of bayes s formula shows that @xmath24 for sets @xmath25 .",
    "we will throughout this paper assume that we are given a sequence @xmath26 of _ fixed _ observations , and write , for @xmath16 , @xmath27 .",
    "moreover , from now on we let the dependence on these observations of all other quantities be implicit , and denote , since the coming analysis is made exclusively _ conditionally _ on the given observed record , by @xmath28 and @xmath29 the conditional probability measure and expectation with respect to  these observations .",
    "[ section : two - stage : sampling ] let us recall the apf  algorithm by @xcite .",
    "assume that we at time @xmath30 have a particle sample @xmath31 ( each random variable @xmath32 taking values in @xmath33 ) providing an approximation @xmath34 of the joint smoothing distribution @xmath35 , where @xmath36 and @xmath37 , @xmath38 .",
    "then , when the observation @xmath39 becomes available , an approximation of @xmath40 is obtained by plugging the empirical measure @xmath41 into the recursion , yielding , for @xmath42 , @xmath43 here we have introduced , for @xmath44 and @xmath45 , the unnormalised kernels @xmath46 and @xmath47 . simulating from @xmath48 consists in extending the trajectory @xmath49 with an additional component being distributed according to the _ optimal kernel _ , that is , the distribution of @xmath50 conditional on @xmath51 _ and _ the observation @xmath52 .",
    "now , since we want to form a new weighted sample approximating @xmath40 , we need to find a convenient mechanism for sampling from @xmath53 given @xmath54 . in most cases cases it is possible  but generally computationally expensive  to simulate from @xmath53 directly using _ auxiliary accept - reject sampling _ ( see * ? ? ?",
    "* ; * ? ? ?",
    "a computationally cheaper ( see * ? ? ?",
    "* , for a discussion of the acceptance probability associated with the auxiliary accept - reject sampling approach ) solution consists in producing a weighted sample approximating @xmath53 by sampling from the importance sampling distribution @xmath55 here @xmath56 , @xmath38 , are positive numbers referred to as _ first - stage weights _",
    "* use the term _ adjustment multiplier weights _ ) and in this article we consider first - stage weights of type @xmath57 for some function @xmath58 .",
    "moreover , the pathwise proposal kernel @xmath59 is , for @xmath49 and @xmath60 , of form @xmath61 with @xmath62 being such that @xmath63 for all @xmath16 .",
    "thus , a draw from @xmath64 is produced by extending the trajectory @xmath49 with an additional component obtained by simulating from @xmath65 .",
    "it is easily checked that for @xmath66 , @xmath67 an updated weighted particle sample @xmath68 targeting @xmath53 is hence generated by simulating @xmath69 particles @xmath70 , @xmath71 , from the proposal @xmath72 and associating with these particles the _ second - stage weights _ @xmath73 , @xmath71 . by the identity function in , only a single term of the sum will contribute to the second - stage weight of a particle .",
    "finally , in an _ optional _ second - stage resampling pass a uniformly weighted particle sample @xmath74 , still targeting @xmath53 , is obtained by resampling @xmath75 of the particles @xmath70 , @xmath76 , according to the normalised second - stage weights .",
    "note that the number of particles in the last two samples , @xmath69 and @xmath75 , may be different .",
    "the procedure is now repeated recursively ( with @xmath77 , @xmath38 ) and is initialised by drawing @xmath78 , @xmath38 , independently from @xmath79 , where @xmath80 , yielding @xmath81 with @xmath82 , @xmath83 . to summarise , we obtain , depending on whether second - stage resampling is performed or not , the procedures described in algorithms  1 and  2 .",
    "@xmath84 approximates @xmath35 .",
    "draw indices @xmath85 from the set @xmath86 multinomially with respect to the normalised weights @xmath87 , @xmath88 ; simulate @xmath89 $ ] , and set @xmath90 $ ] and @xmath73 . draw indices @xmath91 from the set @xmath92 multinomially with respect to the normalised weights @xmath93 , @xmath94 , and set @xmath95 .",
    "finally , reset the weights : @xmath96 .",
    "take @xmath97 as an approximation of @xmath40 .",
    "@xmath98 approximates @xmath35 .",
    "draw indices @xmath85 from the set @xmath86 multinomially with respect to the normalised weights @xmath99 , @xmath88 ; simulate @xmath100 $ ] , and set @xmath101 $ ] and @xmath73 .",
    "take @xmath102 as an approximation of @xmath40 .",
    "we will use the term apf  as a family name for both these algorithms and refer to them separately as _ two - stage sampling particle filter _ ( tsspf ) and _ single - stage auxiliary particle filter _ ( ssapf ) .",
    "note that we by letting @xmath103 , @xmath38 , in algorithm  [ alg : single : step : aux ] obtain the bootstrap particle filter suggested by @xcite .",
    "the resampling steps of the apf  can of course be implemented using techniques ( e.g. , _ residual _ or _",
    "systematic _ resampling ) different from multinomial resampling , leading to straightforward adaptions not discussed here .",
    "we believe however that the results of the coming analysis are generally applicable and extendable to a large class of selection schemes .",
    "the issue whether second - stage resampling should be performed or not has been treated by several authors , and the theoretical results on the particle approximation stability and asymptotic variance presented in the next section will indicate that the second - stage selection pass should , at least for the case @xmath104 , be canceled , since this exclusively increases the sampling variance .",
    "thus , the idea that the second - stage resampling pass is necessary for preventing the particle approximation from degenerating does not apparently hold . recently , a similar conclusion was reached in the manuscript @xcite .",
    "the advantages of the apf  not possessed by standard smc methods is the possibility of , firstly , choosing the first - stage weights @xmath56 arbitrarily and , secondly , letting @xmath75 and @xmath69 be different ( tsspf  only ) . appealing to common sense",
    ", smc methods work efficiently when the particle weights are well - balanced , and @xcite propose several strategies for achieving this by adapting the first - stage weights . in some cases it is possible to fully adapt the filter to the model ( see section  [ section : implementations ] ) , providing exactly equal importance weights ; otherwise , @xcite suggest , in the case @xmath105 and @xmath106 , the generic first - stage importance weight function @xmath107 $ ] , @xmath108 .",
    "the analysis that follows will however show that this way of adapting the first - stage weights is not necessarily good in terms of asymptotic ( as @xmath75 tends to infinity ) sample variance ; indeed , using first - stage weights given by @xmath109 can be even detrimental for some models .",
    "introduce , for any probability measure @xmath110 on some measurable space @xmath111 and @xmath110-measurable function @xmath112 satisfying @xmath113 , the notation @xmath114 .",
    "moreover , for any two transition kernels @xmath115 and @xmath116 from @xmath117 to @xmath118 and @xmath118 to @xmath119 , respectively , we define the product transition kernel @xmath120 , for @xmath121 and @xmath122 .",
    "a set @xmath123 of real - valued functions on @xmath124 is said to be _ proper _ if the following conditions hold : @xmath125 @xmath123 is a linear space ; @xmath126 if @xmath127 and @xmath112 is measurable with @xmath128 , then @xmath129 ; @xmath130 for all @xmath131 , the constant function @xmath132 belongs to @xmath123 .",
    "from @xcite we adapt the following definitions .",
    "a weighted sample @xmath133 on @xmath134 is said to be _ consistent _ for the probability measure @xmath110 and the ( proper ) set @xmath135 if , for any @xmath136 , as @xmath137 , @xmath138    a weighted sample @xmath133 on @xmath134 is called _ asymptotically normal _",
    "( abbreviated a.n . ) for @xmath139 if , as @xmath140 , @xmath141 \\stackrel{\\mathcal{d}}{\\longrightarrow } \\mathcal{n}[0 , \\sigma^2(f ) ] \\quad \\mathrm{for\\ any\\ } f \\in \\mathsf{a } { \\;},\\\\ & a_n^2 ( { \\omega_{m}^n})^{-1 } \\sum_{i=1}^{m_n } ( { \\omega_{m}^{n , i}})^2 f({\\ifthenelse{\\equal{}{}}{\\xi_{0:m}^{n , i}}{\\xi_{0:m}^{n , i } ( ) } } ) \\stackrel{{\\operatorname{\\mathbb{p}}}}{\\longrightarrow } \\gamma f \\quad \\mathrm{for\\    any\\ } f \\in \\mathsf{w } { \\;},\\\\ & a_n ( { \\omega_{m}^n})^{-1 } \\max_{1 \\leq i \\leq m_n } { \\omega_{m}^{n , i } } \\stackrel{{\\operatorname{\\mathbb{p}}}}{\\longrightarrow } 0 { \\;}. \\end{split}\\ ] ]    the main contribution of this section is the following results , which establish consistency and asymptotic normality of weighted samples produced by the tsspf  and ssapf  algorithms . for all @xmath18 ,",
    "we define a transformation @xmath142 on the set of @xmath35-integrable functions by @xmath143({x}_{0:k } ) { \\triangleq}f({x}_{0:k } ) - { \\phi_{k } } f { \\ ; } , \\quad { x}_{0:k } \\in { \\mathsf{x}}^{k+1 } { \\;}.\\ ] ] in addition , we impose the following assumptions .",
    "hyp : clt : assumption for all @xmath144 , @xmath145 and @xmath146 , where @xmath147 and @xmath148 are defined in and , respectively .",
    "clt : initial - sample    * @xmath149 is a proper set and @xmath150 is a function satisfying , for all @xmath151 and @xmath152 , @xmath153 . *",
    "the initial sample @xmath154 is consistent for @xmath155 $ ] and a.n . for @xmath156",
    "[ th : clt : tss ] assume * ( a[hyp : hyp : clt : assumption ] ) * and * ( a[hyp : clt : initial - sample ] ) * with @xmath157 $ ] . in the setting of algorithm  [ alg : tss ] ,",
    "suppose that the limit @xmath158 exists , where @xmath159 $ ] .",
    "define recursively the family @xmath160 by @xmath161 furthermore , define recursively the family @xmath162 of functionals @xmath163 by @xmath164 + \\frac{\\sigma_k^2 \\ {     { h^{\\mathrm{u}}_{k}}(\\cdot , { \\phi_{k+1}}[f ] ) \\ } + \\beta { \\phi_{k } } \\ {     { { t}_{k } } { r_{k}^{\\mathrm{p } } } ( \\cdot , { { w}_{k+1}}^2     { \\phi_{k+1}}^2[f ] ) \\ } \\ , { \\phi_{k } }      { { t}_{k}}}{[{\\phi_{k } } { h^{\\mathrm{u}}_{k}}({\\mathsf{x}}^{k+2})]^2 }    { \\;}.   \\end{gathered}\\ ] ] then each @xmath165 is a proper set for all @xmath144 .",
    "moreover , each sample @xmath166 produced by algorithm  [ alg : tss ] is consistent for @xmath167 $ ] and asymptotically normal for @xmath168 $ ] .    the proof is found in appendix  [ section : appendix : a ] , and as a by - product a similar result for the ssapf ( algorithm  [ alg : single : step : aux ] ) is obtained .",
    "[ th : clt : ssapf ] assume * ( a[hyp : hyp : clt : assumption ] ) * and * ( a[hyp : clt : initial - sample])*. define the families @xmath169 and @xmath170 by @xmath171 and , with @xmath172 , @xmath173 ^ 2 \\in \\tilde{\\mathsf{w}}_k , { { w}_{k+1 } } f^2 \\in",
    "{ \\mathsf{l}^{1}}({\\mathsf{x}}^{k+2},\\ { \\phi_{k+1 } } )   \\big \\ } { \\;}.\\end{gathered}\\ ] ] furthermore , define recursively the family @xmath174 of functionals @xmath175 by @xmath176 ) \\ } + { \\phi_{k } } \\ {     { { t}_{k } } { r_{k}^{\\mathrm{p } } } ( \\cdot , { { w}_{k+1}}^2     { \\phi_{k+1}}^2[f ] ) \\ } \\ , { \\phi_{k } }      { { t}_{k}}}{[{\\phi_{k } } { h^{\\mathrm{u}}_{k}}({\\mathsf{x}}^{k+2})]^2 }    { \\ ; } , \\quad \\tilde{\\sigma}_0 { \\triangleq}\\sigma_0 { \\;},\\ ] ] and the measures @xmath177 by @xmath178 then each @xmath179 is a proper set for all @xmath180 .",
    "moreover , each sample @xmath181 produced by algorithm  [ alg : single : step : aux ] is consistent for @xmath182 $ ] and asymptotically normal for @xmath183 $ ] .    under the assumption of bounded likelihood and second - stage importance weight functions @xmath184 and @xmath148 , one can show that the clts stated in theorems  [ th : clt : tss ] and [ th : clt : ssapf ] indeed include any functions having finite second moments with respect to  the joint smoothing distributions ; that is , under these assumptions the supplementary constraints on the sets and are automatically fulfilled .",
    "this is the contents of the statement below .",
    "assumption : bdd : likelihood for all @xmath18 , @xmath185 and @xmath186 .",
    "[ cor : a : is : l2 ] assume * ( a[hyp : assumption : bdd : likelihood ] ) * and let @xmath187 and @xmath170 be defined by and , respectively , with @xmath188 .",
    "then , for all @xmath189 , @xmath190 and @xmath191 .    for a proof , see section  [ section : proof : corollary : a : is : l2 ] .",
    "interestingly , the expressions of @xmath192 and @xmath193 differ , for @xmath194 , _ only on the additive term _ @xmath195 $ ] , that is , the variance of @xmath112 under @xmath40 .",
    "this quantity represents the cost of introducing the second - stage resampling pass , which was proposed as a mean for preventing the particle approximation from degenerating . in the coming section  [ section :",
    "bounds : on : lp : error : and : bias ] we will however show that the approximations produced by the ssapf  are already stable for a finite time horizon , and that additional resampling is superfluous . thus , there are indeed reasons for strongly questioning whether second - stage resampling should be performed at all , at least when the same number of particles are used in the two stages .      in this part",
    "we examine , under suitable regularity conditions and for a finite particle population , the errors of the approximations obtained by the apf  in terms @xmath0 bounds and bounds on the bias .",
    "we preface our main result with some definitions and assumptions .",
    "denote by @xmath196 space of bounded measurable functions on @xmath124 furnished with the supremum norm @xmath197 , the _ oscillation semi - norm _",
    "( alternatively termed the _ global modus of continuity _ ) be defined by @xmath198 .",
    "furthermore , the @xmath0 norm of a stochastic variable @xmath4 is denoted by @xmath199 $ ] .",
    "when considering sums , we will make use of the standard convention @xmath200 if @xmath201 .    in the following",
    "we will assume that all measures @xmath202 , @xmath16 , have densities @xmath203 with respect to  a common dominating measure @xmath204 on ( @xmath205 , @xmath206 ) .",
    "moreover , we suppose that the following holds .",
    "assumption : regularity : q    * @xmath207 , @xmath208 . * for all @xmath209 , @xmath210 .    under * ( a[hyp : assumption : regularity : q ] ) * we define @xmath211    assumption : finite : weights for all @xmath18 , @xmath212 .",
    "assumption  * ( a[hyp : assumption : regularity : q ] ) * is now standard and is often satisfied when the state space @xmath205 is compact and implies that the hidden chain , when evolving conditionally on the observations , is geometrical ergodic with a mixing rate given by @xmath213 . for comprehensive treatments of such stability properties within the framework of state space models we refer to @xcite .",
    "finally , let @xmath214 be the set of bounded measurable functions @xmath112 on @xmath215 of type @xmath216 for some function @xmath217 . in this",
    "setting we have the following result , which is proved in section  [ section : proof : main : deviation : theorem ] .",
    "[ th : main : deviation : theorem ] assume * ( a[hyp : assumption : bdd : likelihood ] ) * , * ( a[hyp : assumption : regularity : q ] ) * , * ( a[hyp : assumption : finite : weights ] ) * , and let @xmath218 for @xmath219 .",
    "let @xmath220 be a weighted particle sample produced by algorithm  @xmath221 , @xmath222 , with @xmath223 .",
    "then the following holds true for all @xmath224 and @xmath222 .",
    "* for all @xmath225 , @xmath226 { \\;},\\end{gathered}\\ ] ] * @xmath227 - { \\phi_{n}}f_i \\right| \\\\",
    "\\leq b \\frac{{\\operatorname{osc}}(f_i)}{(1 - \\rho)^2 } \\left [ \\frac{1}{r_n(r ) { \\epsilon_-}^2 }     \\sum_{k=1}^n \\frac{\\left \\| { { w}_{k } } \\right    \\|_{{\\mathsf{x}}^{k+1 } , \\infty}^2 \\left \\| { { t}_{k-1 } } \\right    \\|_{{\\mathsf{x}}^k , \\infty}^2}{({\\mu}{g_{k}})^2 } \\rho^{0 \\vee ( i - k ) } \\right.\\\\ + \\bigg . \\frac{{\\mathbbm{1}}\\ { r = 1 \\}}{n } \\left ( \\frac{\\rho}{1 - \\rho } +   n - i \\right ) + \\frac{\\left      \\| { { w}_{0 } } \\right    \\|_{{\\mathsf{x } } , \\infty}^2}{n ( \\nu { g_{0}})^2 } \\rho^i \\bigg ] { \\;}.\\end{gathered}\\ ] ]    here @xmath228 is defined in , and @xmath229 and @xmath230 are universal constants such that @xmath229 depends on @xmath231 only .",
    "especially , applying , under the assumption that all fractions @xmath232 are uniformly bounded in @xmath30 , theorem  [ th : main : deviation : theorem ] for @xmath233 , yields error bounds on the approximate filter distribution which are _ uniformly bounded _ in @xmath234 .",
    "from this it is obvious that the first - stage resampling pass is enough to preserve the sample stability . indeed , by avoiding second - stage selection according to algorithm  [ alg : single : step : aux ] we can , since the middle terms in the bounds above cancel in this case , obtain even _ tighter _ control of the @xmath0 error for a fixed number of particles .",
    "the formulas and for the asymptotic variances of the tsspf  and ssapf  may look complicated at a first sight , but by carefully examining the same we will obtain important knowledge of how to choose the first - stage importance weight functions @xmath147 in order to robustify the apf  .    assume that we have run the apf  up to time @xmath30 and are about to design suitable first - stage weights for the next iteration .",
    "in this setting , we call a first - stage weight function @xmath235 $ ] , possibly depending on the target function @xmath236 and satisfying * ( a[hyp : hyp : clt : assumption ] ) * , _ optimal _ ( at time @xmath30 ) if it provides a minimal increase of asymptotic variance at a single iteration of the apf  algorithm , that is , if @xmath237 \\}(f ) \\leq \\sigma^2_{k+1}\\ { t \\}(f)$ ] ( or @xmath238 \\}(f ) \\leq \\tilde{\\sigma}^2_{k+1}\\ { t \\}(f)$ ] ) for all other measurable and positive weight functions @xmath239 . here",
    "we let @xmath240 denote the asymptotic variance induced by @xmath239 .",
    "define , for @xmath49 , @xmath241({x}_{0:k } ) { \\triangleq}\\sqrt{\\int_{\\mathsf{x}}{g_{k+1}}^2(x_{k+1 } ) \\left [ \\frac{{\\mathrm{d}}{q _ { } } ( x_k , \\cdot)}{{\\mathrm{d}}{r_{k}}(x_k ,    \\cdot)}(x_{k+1 } ) \\right]^2   { \\phi_{k+1}}^2[f]({x}_{0:k+1 } ) \\ , { r_{k}}(x_k , { \\mathrm{d}}x_{k+1 } ) }    { \\;},\\ ] ] and let @xmath242 $ ] denote the second - stage importance weight function induced by @xmath243 $ ] according to .",
    "we are now ready to state the main result of this section . the proof is found in section  [ section : proof : optimal : weights ] .",
    "[ th : optimal : weights ] let @xmath18 and define @xmath244 by .",
    "then the following is valid .",
    "* let the assumptions of theorem  [ th : clt : tss ] hold and suppose that @xmath245 \\in { \\mathsf{l}^{2}}({\\mathsf{x}}^{k+1 } , { \\phi_{k } } ) , { { w}_{k+1}}^\\ast [ f ' ] \\in { \\mathsf{l}^{1}}({\\mathsf{x}}^{k+2 } , { \\phi_{k+1}})\\}$ ] . then @xmath244 is optimal for algorithm  [ alg : tss ] and the corresponding minimal variance is given by @xmath246 + \\frac{\\sigma_k^2 [    { h^{\\mathrm{u}}_{k}}(\\cdot , { \\phi_{k+1}}[f ] ) ] + \\beta ( { \\phi_{k } } { { t}_{k}}^\\ast[f])^2}{[{\\phi_{k } }    { h^{\\mathrm{u}}_{k}}({\\mathsf{x}}^{k+2})]^2 } { \\;}.\\ ] ] * let the assumptions of theorem  [ th : clt : ssapf ] hold and suppose that @xmath247 \\in { \\mathsf{l}^{2}}({\\mathsf{x}}^{k+1 } , { \\phi_{k } } ) , { { w}_{k+1}}^\\ast [ f ' ] \\in { \\mathsf{l}^{1}}({\\mathsf{x}}^{k+2 } , { \\phi_{k+1}})\\}$ ] .",
    "then @xmath244 is optimal for algorithm  [ alg : single : step : aux ] and the corresponding minimal variance is given by @xmath248 ) ] + ( { \\phi_{k } } { { t}_{k}}^\\ast[f])^2}{[{\\phi_{k } }    { h^{\\mathrm{u}}_{k}}({\\mathsf{x}}^{k+2})]^2 } { \\;}.\\ ] ]    the functions @xmath244 have a natural interpretation in terms of optimal sample allocation for _ stratified sampling_. consider the mixture @xmath249 , each @xmath250 being a measure on some measurable space @xmath111 and @xmath251 , and the problem of estimating , for some given @xmath252-integrable target function @xmath112 , the expectation @xmath253 . in order to relate this to the particle filtering paradigm",
    ", we will make use of algorithm  [ alg : biased : stratified : sampling ] .",
    "draw an index @xmath254 multinomially with respect to  @xmath255 , @xmath256 , @xmath257 ; simulate @xmath258 , and compute the weights @xmath259 take @xmath260 as an approximation of @xmath252 .",
    "in other words , we perform monte carlo estimation of @xmath253 by means of sampling from some proposal mixture @xmath261 and forming a self - normalised estimate  cf . the technique applied in section  [ section : two - stage : sampling ] for sampling from @xmath53 . in this",
    "setting , the following clt can be established under weak assumptions : @xmath262 \\stackrel{\\mathcal{d}}{\\longrightarrow } \\mathcal{n } \\left[0 , \\sum_{j=1}^d \\frac{w_j^2      \\alpha_j(f)}{\\tau_j } \\right ] { \\;},\\ ] ] with , for @xmath263 , @xmath264 ^ 2 \\pi^2[f](x ) \\ , \\nu_i({\\mathrm{d}}x ) \\quad \\textrm{and } \\quad \\pi[f](x ) { \\triangleq}f(x ) - \\pi f { \\;}.\\ ] ] minimising the asymptotic variance @xmath265 $ ] with respect to  @xmath266 , @xmath267 , e.g. , by means of the lagrange multiplicator method ( the details are simple ) , yields the optimal weights @xmath268 ^ 2 \\pi^2[f](x ) \\ , \\nu_i({\\mathrm{d}}x ) } { \\;},\\ ] ] and the similarity between this expression and that of the optimal first - stage importance weight functions @xmath244 is striking .",
    "this strongly supports the idea of interpreting optimal sample allocation for particle filters in terms of variance reduction for stratified sampling .",
    "as shown in the previous section , the utilisation of the optimal weights provides , for a given sequence @xmath269 of proposal kernels , the most efficient of all particle filters belonging to the large class covered by algorithm  [ alg : single : step : aux ] ( including the standard bootstrap filter and any fully adapted particle filter ) . however , exact computation of the optimal weights is in general infeasible by two reasons : firstly , they depend ( via @xmath270 $ ] ) on the expectation @xmath271 , that is , the quantity that we aim to estimate , and , secondly , they involve the evaluation of a complicated integral .",
    "a comprehensive treatment of the important issue of how to approximate the optimal weights is beyond the scope of this paper , but in the following three examples we discuss some possible heuristics for doing this .      in order to form an initial idea of the performance of the optimal ssapf in practice ,",
    "we apply the method to a first order ( possibly nonlinear ) autoregressive model observed in noise : @xmath272 with @xmath273 and @xmath274 being mutually independent sets of standard normal distributed variables such that @xmath275 is independent of @xmath276 , @xmath277 , and @xmath278 is independent of @xmath6 , @xmath276 , @xmath279 . here",
    "the functions @xmath280 and @xmath281 are measurable , and @xmath282 .",
    "as observed by @xcite , it is , for all models of form , possible to propose new particle using the optimal kernel directly , yielding @xmath283 and , for @xmath284 , @xmath285 ^ 2}{2 { \\tilde{\\sigma}_{k}}^2 ( x ) } \\right \\ } { \\;},\\ ] ] with @xmath286 denoting the density of @xmath62 with respect to  the lebesque measure , and @xmath287 { \\tilde{\\sigma}_{k}}^2(x ) { \\ ; } , \\quad { \\tilde{\\sigma}_{k}}^2(x ) { \\triangleq}\\frac{\\sigma_v^2 \\sigma_w^2(x)}{\\sigma_v^2 +    \\sigma_w^2(x ) } { \\;}.\\ ] ] for the proposal it is , for @xmath288 , valid that @xmath289 { \\;},\\ ] ] and since the right hand side does not depend on @xmath290 we can , by letting @xmath291 , @xmath292 , obtain second - stage weights being indeed unity ( providing a sample of genuinely @xmath53-distributed particles ) .",
    "when this is achieved , @xcite call the particle filter _ fully adapted_. there is however nothing in the previous theoretical analysis that supports the idea that aiming at evenly distributed second - stage weights is always convenient , and this will also be illustrated in the simulations below . on the other hand , it is possible to find _ cases _ when the fully adapted particle filter is very close to being optimal ; see again the following discussion .    in this part we will study the following two special cases of :    * @xmath293 and @xmath294 + for a linear / gaussian model of this kind",
    ", exact expressions of the optimal weights can be obtained using the kalman filter .",
    "we set @xmath295 and let the latent chain be put at stationarity from the beginning , that is , @xmath296 $ ] . in this setting , we simulated , for @xmath297 , a record @xmath298 of observations and estimated the filter posterior means ( corresponding to projection target functions @xmath299 , @xmath108 ) along this trajectory by applying ( 1 ) ssapf  based on true optimal weights , ( 2 ) ssapf  based on the generic weights @xmath109 of @xcite , and ( 3 ) the standard bootstrap particle filter ( that is , ssapf  with @xmath300 ) . in this first experiment , the prior kernel @xmath11 was taken as proposal in all cases , and since the optimal weights are derived using asymptotic arguments we used as many as @xmath301 particles for all algorithms .",
    "the result is displayed in figure  [ fig : lin : gauss : aux : vs : boot : a ] , and it is clear that operating with true optimal allocation weights improves  as expected  the mse performance in comparison with the other methods . + the main motivation of @xcite for introducing auxiliary particle filtering was to robustify the particle approximation to outliers .",
    "thus , we mimic ( * ? ? ?",
    "* example 7.2.3 ) and repeat the experiment above for the observation record @xmath302 , standard deviations @xmath303 , @xmath304 , and the smaller particle sample size @xmath305 . note the large discrepancy of the last observation @xmath306 , which in this case is located at a distance of 20 standard deviations from the mean of the stationary distribution .",
    "the outcome is plotted in figure  [ fig : lin : gauss : aux : vs : boot : b ] from which it is evident that the particle filter based on the optimal weights is the most efficient also in this case ; moreover , the performance of the standard auxiliary particle filter is improved in comparison with the bootstrap filter .",
    "figure  [ fig : weight : functions ] displays a plot of the weight functions @xmath307 and @xmath308 for the same observation record .",
    "it is clear that @xmath308 is not too far away from the optimal weight function ( which is close to symmetric in this extreme situation ) in this case , even if the distance between the functions as measured with the supremum norm is still significant .",
    "+   ( unbroken line ) and @xmath308 ( dashed line ) in the presence of an outlier . ] + finally , we implement the fully adapted filter ( with proposal kernels and first stage - weights given by and , respectively ) and compare this with the ssapf  based on the same proposal and optimal first - stage weights , the latter being given by , for @xmath108 and @xmath309 defined in , @xmath310({x}_{0:k } ) & \\propto h_k(x_k ) \\sqrt{\\int_{\\mathbb{r}}{\\phi_{k+1}}^2[{\\pi_{k+1}}]({x}_{k+1 } ) \\ , { r_{k}}(x_k , { \\mathrm{d}}x_{k+1 } ) } \\\\   & = h_k(x_k ) \\sqrt{{\\tilde{\\sigma}_{k}}^2(x_k ) + { \\tilde{m}_{k}}^2(x_k ) - 2    { \\tilde{m}_{k}}(x_k ) { \\phi_{k+1}}{\\pi_{k+1 } } +    { \\phi_{k+1}}^2 { \\pi_{k+1 } } }   \\end{split}\\ ] ] in this case .",
    "we note that @xmath309 , that is , the first - stage weight function for the fully adapted filter , enters as a factor in the optimal weight function .",
    "moreover , recall the definitions of @xmath311 and @xmath312 ; in the case of very informative observations , corresponding to @xmath313 , it holds that @xmath314 and @xmath315 with good precision for moderate values of @xmath316 ( that is , values not too far away from the mean of the stationary distribution of @xmath4 ) .",
    "thus , the factor beside @xmath309 in is more or less constant in this case , implying that the fully adapted and optimal first - stage weight filters are close to equivalent .",
    "this observation is perfectly confirmed in figure  [ fig : fa : vs : opt : filter : a ] which presents mse performances for @xmath317 , @xmath318 , and @xmath305 . in the same figure ,",
    "the bootstrap filter and the standard auxiliary filter based on generic weights are included for a comparison , and these ( particularly the latter ) are marred with significantly larger monte carlo errors .",
    "on the contrary , in the case of non - informative observations , that is , @xmath319 , we note that @xmath320 , @xmath321 and conclude that the optimal kernel is close the prior kernel @xmath11 .",
    "in addition , the exponent of @xmath309 vanishes , implying uniform first - stage weights for the fully adapted particle filter .",
    "thus , the fully adapted filter will be close to the bootstrap filter in this case , and figure  [ fig : fa : vs : opt : filter : b ] seems to confirm this remark .",
    "moreover , the optimal first - stage weight filter does clearly better than the others in terms of mse performance . *",
    "@xmath322 and @xmath323 + here we deal with the classical gaussian autoregressive conditional heteroscedasticity ( arch ) model ( see * ? ? ?",
    "* ) observed in noise .",
    "since the nonlinear state equation precludes exact computation of the filtered means , implementing the optimal first - stage weight ssapf  is considerably more challenging in this case .",
    "the problem can however be tackled by means of an introductory _ zero - stage _ simulation pass , based on @xmath324 particles , in which a crude estimate of @xmath325 is obtained . for instance , this can be achieved by applying the standard bootstrap filter with multinomial resampling . using this approach , we computed again mse values for the bootstrap filter , the standard ssapf  based on generic weights , the fully adapted ssapf , and the ( approximate ) optimal first - stage weight ssapf , the latter using the optimal proposal kernel .",
    "each algorithm used @xmath326 particles and the number of particles in the prefatory pass was set to @xmath327 , implying only a minor additional computational work .",
    "an imitation of the true filter means was obtained by running the bootstrap filter with as many as @xmath328 particles . in compliance with the foregoing , we considered the case of informative ( figure  [ fig : arch : fa : vs : boot : a ] ) as well as non - informative ( figure  [ fig : arch : fa : vs : boot : b ] ) observations , corresponding to @xmath329 and @xmath330 , respectively .",
    "since @xmath331 , @xmath315 in the latter case , we should , in accordance with the previous discussion , again expect the fully adapted filter to be close to that based on optimal first - stage weights .",
    "this is also confirmed in the plot .",
    "for the former parameter set , the fully adapted ssapf  exhibits a mse performance close to that of the bootstrap filter , while the optimal first - stage weight ssapf is clearly superior .      as a final example we consider the canonical discrete - time _",
    "stochastic volatility _",
    "( sv ) _ model _ @xcite given by @xmath332 where @xmath282 , and @xmath273 and @xmath333 are as in example  [ section : nonlinear : gaussian : model ] . here",
    "@xmath4 and @xmath3 are log - volatility and log - returns , respectively , where the former are assumed to be stationary .",
    "also this model was treated by @xcite , who discussed approximate full adaptation of the particle filter by means of a second order taylor approximation of the concave function @xmath334 . more specifically , by multiplying the approximate observation density obtained in this way with @xmath335 , @xmath336 , yielding a gaussian approximation of the optimal kernel density",
    ", nearly even second - stage weights can be obtained .",
    "we proceed in the same spirit , approximating however directly the ( log - concave ) function @xmath337 by means of a second order taylor expansion of @xmath338 $ ] around the mode @xmath339 ( obtained using newton iterations ) of the same : @xmath340 { q_{}}[x,{\\bar{m}_{k}}(x ) ] \\exp \\left\\ { - \\frac{1}{2 { \\bar{\\sigma}_{k}}^2(x)}[x ' - { \\bar{m}_{k}}(x)]^2 \\right\\ } { \\;},\\ ] ] with ( we refer to * ? ? ?",
    "* , for details ) @xmath341 being the inverted negative of the second order derivative , evaluated at @xmath339 , of @xmath342 $ ] .",
    "thus , by letting , for @xmath336 , @xmath343 , we obtain @xmath344 { q_{}}[x,{\\bar{m}_{k}}(x_k ) ] { \\;},\\ ] ] and letting , for @xmath108 , @xmath345 { q_{}}[x_k,{\\bar{m}_{k}}(x_k)]$ ] will imply a nearly fully adapted particle filter .",
    "moreover , by applying the approximate relation to the expression of the optimal weights , we get ( cf . ) @xmath346({x}_{0:k } ) \\approx \\int_{{\\mathbb{r } } } { r_{k}}^{\\mathrm{u}}(x_k , x ' ) \\ , { \\mathrm{d}}x ' \\sqrt { \\int_{\\mathbb{r}}{\\phi_{k+1}}^2[{\\pi_{k+1}}]({x } ) \\ , { r_{k}}(x_k , { \\mathrm{d}}x ) } \\propto \\\\ { \\bar{\\sigma}_{k}}(x_k ) { g_{k+1}}[{\\bar{m}_{k}}(x_k ) ] { q_{}}[x,{\\bar{m}_{k}}(x_k ) ] \\sqrt{{\\bar{\\sigma}_{k}}^2(x_k ) +    { \\bar{m}_{k}}^2(x_k ) - 2 { \\bar{m}_{k}}(x_k )    { \\phi_{k+1}}{\\pi_{k+1 } } + { \\phi_{k+1}}^2 { \\pi_{k+1 } } } { \\;}. \\end{gathered}\\ ] ] in this setting , we conducted a numerical experiment where the two filters above were , again together with the bootstrap filter and the auxiliary filter based on the generic weights @xmath109 , run for the parameters @xmath347 ( estimated by * ? ? ?",
    "* from daily returns on the u.  s. dollar against the u.  k. pound stearling from the first day of trading in 1997 and for the next 200 days ) . to make the filtering problem more challenging , we used a simulated record @xmath298 of observations arising from the initial state @xmath348 , being above the 2% quantile of the stationary distribution of @xmath4 , implying a sequence of relatively impetuously fluctuating log - returns .",
    "the number of particles was set to @xmath349 for all filters , and the number of particles used in the prefatory filtering pass ( in which a rough approximation of @xmath350 in was computed using the bootstrap filter ) of the ssapf  filter based on optimal first - stage weights was set to @xmath351 ; thus , running the optimal first - stage weight filter is only marginally more demanding than running the fully adapted filter .",
    "the outcome is displayed in figure  [ fig : sv : fa : vs : boot ] .",
    "it is once more obvious that introducing approximate optimal first - stage weights significantly improves the performance also for the the sv model , which is recognised as being specially demanding as regards state estimation .    ) , the ssapf  based on optimal weights ( @xmath352 ) , the ssapf  based on the generic weights @xmath109 ( @xmath353 ) , and the fully adapted ssapf  ( @xmath354 ) for the sv model in section  [ section : sv ] .",
    "the mse values are computed using 5,000 particles and 400 runs of each algorithm . ]",
    "let us recall the updating scheme described in algorithm  [ alg : tss ] and formulate it in the following four isolated steps : @xmath355 where we have set @xmath356 , @xmath71 .",
    "now , the asymptotic properties stated in theorem  [ th : clt : tss ] are established by a chain of applications of ( * ? ? ?",
    "* theorems  14 ) .",
    "we will proceed by induction : assume that the uniformly weighted particle sample @xmath166 is consistent for @xmath182 $ ] and asymptotically normal for @xmath357 $ ] , with @xmath165 being a proper set and @xmath358 such that @xmath359 , @xmath360 , @xmath361 .",
    "we prove , by analysing each of the steps * ( i  iv ) * , that this property is preserved through one iteration of the algorithm .    *",
    "( i)*. define the measure @xmath362 by applying ( * ? ? ?",
    "* theorem  1 ) for @xmath363 , @xmath364 , @xmath365 , and @xmath366 , we conclude that the weighted sample @xmath367 is consistent for @xmath368 = [ { \\mathsf{l}^{1}}({\\mathsf{x}}^{k+1},\\mu_k),\\mu_k]$ ] . here",
    "the equality is based on the fact that @xmath369 , where the second factor on the right hand side is bounded by assumption  * ( a[hyp : hyp : clt : assumption])*. in addition , by applying ( * ? ? ?",
    "* theorem  1 ) we conclude that @xmath370 is asymptotically normal for @xmath371 , where @xmath372 are proper sets , and @xmath373 =   \\frac{\\sigma_k^2 [ { { t}_{k}}(f - \\mu_k    f)]}{({\\phi_{k } } { { t}_{k}})^2 } { \\ ; } , \\quad f \\in \\mathsf{a}_{\\mathrm{\\mathbf{i}},k } { \\;},\\\\ \\gamma_{\\mathrm{\\mathbf{i}},k}f & { \\triangleq}\\frac{{\\phi_{k}}({{t}_{k}}^2    f)}{({\\phi_{k } } { { t}_{k}})^2 } { \\ ; } , \\quad f \\in \\mathsf{w}_{\\mathrm{\\mathbf{i}},k } { \\;}. \\end{split}\\ ] ]    * ( ii)*. by using ( * ? ? ? * theorems  3 and 4 ) we deduce that @xmath374 is consistent for @xmath375 $ ] and a.n . for @xmath376",
    "@xmath377 $ ] , where @xmath378 is a proper set , and @xmath379 + \\sigma^2_{\\mathrm{\\mathbf{i}},k}(f ) = \\beta \\mu_k[(f - \\mu_k f)^2 ] + \\frac{\\sigma_k^2 [ { { t}_{k}}(f    - \\mu_k f)]}{({\\phi_{k } } { { t}_{k}})^2 } { \\ ; } , \\quad f \\in \\mathsf{a}_{\\mathrm{\\mathbf{ii}},k } { \\;}.\\ ] ]    * ( iii)*. we argue as in step * ( i ) * , but this time for @xmath380 , @xmath381 , and @xmath382 , @xmath383 , providing the target distribution @xmath384 this yields , applying ( * ? ? ?",
    "* theorems  1 and 2 ) , that @xmath385 is consistent for @xmath386",
    "\\\\ = \\left [ { \\mathsf{l}^{1}}({\\mathsf{x}}^{k+2 } , { \\phi_{k+1}}),{\\phi_{k+1 } } \\right ] { \\;},\\end{gathered}\\ ] ] where follows , since @xmath387 , from * ( a[hyp : hyp : clt : assumption ] ) * , and a.n . for @xmath388 @xmath389 . here",
    "@xmath390 and @xmath391 are proper sets .",
    "in addition , from the identity we obtain that @xmath392 ) = 0 { \\;},\\ ] ] where @xmath393 is defined in , yielding @xmath394)}{\\mu_k { r_{k}^{\\mathrm{p } } }       { { w}_{k+1 } } } \\right\\ } + \\frac{\\beta \\mu_k { r_{k}^{\\mathrm{p } } } ( \\ {     { { w}_{k+1 } }    { \\phi_{k+1}}[f ] - { r_{k}^{\\mathrm{p}}}(\\cdot , { { w}_{k+1 } }    { \\phi_{k+1}}[f ] ) \\}^2 ) } { ( \\mu_k { r_{k}^{\\mathrm{p } } } { { w}_{k+1}})^2}\\\\ & = \\frac{\\beta \\mu_k ( \\ {    { r_{k}^{\\mathrm{p}}}({{w}_{k+1}}{\\phi_{k+1}}[f ] ) \\}^2 ) } { ( \\mu_k    { r_{k}^{\\mathrm{p } } } { { w}_{k+1}})^2 } + \\frac{\\sigma_k^2 \\ { { { t}_{k } }    { r_{k}^{\\mathrm{p}}}(\\cdot , { { w}_{k+1}}{\\phi_{k+1}}[f ] ) \\}}{({\\phi_{k } }     { { t}_{k}})^2(\\mu_k { r_{k}^{\\mathrm{p } } } { { w}_{k+1}})^2 } \\\\ & \\hspace{30 mm } + \\frac{\\beta \\mu_k { r_{k}^{\\mathrm{p } } } ( \\{{{w}_{k+1 } }    { \\phi_{k+1}}[f ] -   { r_{k}^{\\mathrm{p}}}(\\cdot , { { w}_{k+1 } }    { \\phi_{k+1}}[f ] ) \\}^2 ) } { ( \\mu_k { r_{k}^{\\mathrm{p } } }    { { w}_{k+1}})^2 } { \\ ; } , \\quad f \\in \\mathsf{a}_{\\mathrm{\\mathbf{iii}},k+1 } { \\;}. \\end{split}\\ ] ] now , applying the equality @xmath395 ) \\}^2 + { r_{k}^{\\mathrm{p}}}(\\cdot ,    \\{{{w}_{k+1}}{\\phi_{k+1}}[f ] -     { r_{k}^{\\mathrm{p}}}(\\cdot , { { w}_{k+1 } } { \\phi_{k+1}}[f ] ) \\}^2 ) \\\\",
    "= { r_{k}^{\\mathrm{p}}}(\\cdot , { { w}_{k+1}}^2   { \\phi_{k+1}}^2[f ] )   { \\;},\\end{gathered}\\ ] ] provides the variance @xmath396 ) \\ } \\ , { \\phi_{k } }     { { t}_{k } } + \\sigma_k^2 \\ { { h^{\\mathrm{u}}_{k}}(\\cdot , { \\phi_{k+1}}[f ] )     \\}}{[{\\phi_{k } } { h^{\\mathrm{u}}_{k}}({\\mathsf{x}}^{k+2})]^2 } { \\ ; } , \\quad f",
    "\\in   \\mathsf{a}_{\\mathrm{\\mathbf{iii}},k+1 } { \\;}.\\ ] ]      * ( iv)*. the consistency for @xmath399 $ ] of the uniformly weighted particle sample @xmath400 follows from ( * ? ? ?",
    "* theorem  3 ) .",
    "in addition , applying ( * ? ? ?",
    "* theorem  4 ) yields that the same sample is a.n . for @xmath401",
    "@xmath402 $ ] , with @xmath403 being proper set , and , for @xmath404 , @xmath405 + \\sigma^2_{\\mathrm{\\mathbf{iii}},k+1}(f ) { \\ ; } , \\\\ \\end{split}\\ ] ] with @xmath406 being defined by .",
    "this concludes the proof of the theorem .",
    "we pick @xmath407 and prove that the constraints of the set @xmath408 defined in are satisfied under assumption  * ( a[hyp : assumption : bdd : likelihood])*. firstly , by jensen s inequality , @xmath409 } \\hspace{20mm}\\\\ & = { \\phi_{k } } \\ { { { t}_{k } } [ { r_{k}^{\\mathrm{p}}}(\\cdot ,    { { w}_{k+1}}|f|)]^2 \\ } \\\\ & \\leq { \\phi_{k } } [ { { t}_{k } } { r_{k}^{\\mathrm{p}}}(\\cdot ,    { { w}_{k+1}}^2 f^2 ) ] \\\\ & =   { \\phi_{k } } { h^{\\mathrm{u}}_{k } } ( { { w}_{k+1 } } f^2 ) \\\\ & \\leq { \\left \\| { { w}_{k+1 } } \\right \\|_{{\\mathsf{x}}^{k+2 } , \\infty } } { \\phi_{k } } { h^{\\mathrm{u}}_{k } } ( { \\mathsf{x}}^{k+2 } ) \\ , { \\phi_{k+1 } } ( f^2 ) < \\infty { \\ ; } , \\end{split}\\ ] ] and similarly , @xmath410 ^ 2 \\ } \\leq { \\left \\| { g_{k+1 } } \\right \\|_{{\\mathsf{x } } , \\infty } } { \\phi_{k } } { h^{\\mathrm{u}}_{k } } ( { \\mathsf{x}}^{k+2 } ) \\ , { \\phi_{k+1 } } ( f^2 ) < \\infty { \\;}.\\ ] ] from this , together with the bound @xmath411 we conclude that @xmath412 .",
    "define , for @xmath415 and @xmath416 as defined in theorem  [ th : main : deviation : theorem ] , the particle measures @xmath417 playing the role of approximations of the smoothing distribution @xmath35 .",
    "let @xmath418 ; then the particle history up to the different steps of loop @xmath419 , @xmath420 , of algorithm  @xmath221 , @xmath421 , is modeled by the filtrations @xmath422 $ ] , @xmath423 $ ] , and @xmath424 respectively . in the coming proof",
    "we will describe one iteration of the apf  algorithm by the following two operations .",
    "where , for @xmath25 , @xmath426}{{\\phi^n_{k } }    { { t}_{k } } } { \\;},\\end{gathered}\\ ] ] for some index @xmath427 ( given @xmath428 , the particles @xmath70 , @xmath429 , are i.i.d . ) . here",
    "the initial weights @xmath430 are all equal to one for @xmath431 .",
    "the second operation is valid since , for any @xmath432 , @xmath433 the fact that the evolution of the particles can be described by two monte carlo operations involving conditionally i.i.d .",
    "variables makes it possible to analyse the error using the marcinkiewicz - zygmund inequality ( see * ? ? ?",
    "* ) .    using this , set , for @xmath434 , @xmath435 with , for @xmath49 , @xmath436 here we apply the standard convention @xmath437 if @xmath438 .",
    "for @xmath439 we define @xmath440 with , for @xmath441 , @xmath442 } { \\;}.\\ ] ]      the following powerful decomposition is an adaption of a similar one derived by ( * ? ? ?",
    "* lemma  7.2 ) ( the standard sisr case ) , being in turn a refinement of a decomposition originally presented by @xcite .",
    "[ lemma : key : decomposition ] let @xmath446 .",
    "then , for all @xmath447 , @xmath224 , and @xmath448 , @xmath449 where @xmath450({\\ifthenelse{\\equal{}{}}{\\tilde{\\xi}_{0:k}^{n , i}}{\\tilde{\\xi}_{0:k}^{n , i}()}})}{\\sum_{j=1}^{r_n(r ) } \\frac{{\\mathrm{d}}{\\alpha_{k}^n}}{{\\mathrm{d}}{\\varphi_{k}^n}}({\\ifthenelse{\\equal{}{}}{\\tilde{\\xi}_{0:k}^{n , j}}{\\tilde{\\xi}_{0:k}^{n , j } ( ) } } ) } - { \\alpha_{k}^n }   { \\psi_{k : n}}[f ]   { \\ ; } ,",
    "\\\\    b_k^n ( f ) & { \\triangleq}\\frac{\\sum_{i=1}^n \\frac{{\\mathrm{d}}{\\beta_{k}^n}}{{\\mathrm{d}}{\\tilde{\\phi}^n_{k}}}({\\ifthenelse{\\equal{}{}}{\\xi_{0:k}^{n , i}}{\\xi_{0:k}^{n , i } ( ) } } )      { \\psi_{k : n}}[f]({\\ifthenelse{\\equal{}{}}{\\xi_{0:k}^{n , i}}{\\xi_{0:k}^{n , i}()}})}{\\sum_{j=1}^n \\frac{{\\mathrm{d}}{\\beta_{k}^n}}{{\\mathrm{d}}{\\tilde{\\phi}^n_{k}}}({\\ifthenelse{\\equal{}{}}{\\xi_{0:k}^{n , j}}{\\xi_{0:k}^{n , j } ( ) } } ) } -    { \\beta_{k}^n } { \\psi_{k : n}}[f ] { \\ ; } , \\\\    c^n ( f ) & { \\triangleq}\\frac{\\sum_{i=1}^n \\frac{{\\mathrm{d}}\\beta_{0|n}}{{\\mathrm{d}}\\varsigma}({\\ifthenelse{\\equal{}{}}{\\xi_{0}^{n , i}}{\\xi_{0}^{n , i } ( ) } } )      { \\psi_{0:n}}[f]({\\ifthenelse{\\equal{}{}}{\\xi_{0}^{n , i}}{\\xi_{0}^{n , i}()}})}{\\sum_{j=1}^n \\frac{{\\mathrm{d}}\\beta_0}{{\\mathrm{d}}\\varsigma}({\\ifthenelse{\\equal{}{}}{\\xi_{0}^{n , i}}{\\xi_{0}^{n , i } ( ) } } ) } - { \\phi_{n } }    { \\psi_{0:n}}[f ] { \\ ; } , \\end{split}\\ ] ] and the operators @xmath451 , @xmath452 , are , for some fixed points @xmath453 , defined by @xmath454 : { x}_{0:k } \\mapsto \\frac{{h^{\\mathrm{u}}_{k } } \\cdots    { h^{\\mathrm{u}}_{n-1 } } f({x}_{0:k})}{{h^{\\mathrm{u}}_{k } } \\cdots { h^{\\mathrm{u}}_{n-1}}({x}_{0:k } ,    { \\mathsf{x}}^{n+1 } ) } - \\frac{{h^{\\mathrm{u}}_{k } } \\cdots { h^{\\mathrm{u}}_{n-1 } }    f(\\hat{{x}}_{0:k})}{{h^{\\mathrm{u}}_{k } } \\cdots    { h^{\\mathrm{u}}_{n-1}}(\\hat{{x}}_{0:k } , { \\mathsf{x}}^{n+1 } ) } { \\;}.\\ ] ]    consider the decomposition @xmath455 \\\\   + { \\mathbbm{1}}\\{r = 1\\ } \\sum_{k=0}^{n-1 } \\left [    \\frac{{\\phi^n_{k } } { h^{\\mathrm{u}}_{k } } \\cdots { h^{\\mathrm{u}}_{n-1 } }      f}{{\\phi^n_{k } } { h^{\\mathrm{u}}_{k } } \\cdots { h^{\\mathrm{u}}_{n-1 } } ( { \\mathsf{x}}^{n+1 } ) } - \\frac{{\\tilde{\\phi}^n_{k } } { h^{\\mathrm{u}}_{k } } \\cdots { h^{\\mathrm{u}}_{n-1 } }      f}{{\\tilde{\\phi}^n_{k } } { h^{\\mathrm{u}}_{k } } \\cdots { h^{\\mathrm{u}}_{n-1 } } ( { \\mathsf{x}}^{n+1 } ) } \\right ] \\\\ + \\frac{{\\tilde{\\phi}^n_{0 } } { h^{\\mathrm{u}}_{0 } } \\cdots { h^{\\mathrm{u}}_{n-1 } }      f}{{\\tilde{\\phi}^n_{0 } } { h^{\\mathrm{u}}_{0 } } \\cdots { h^{\\mathrm{u}}_{n-1 } } ( { \\mathsf{x}}^{n+1 } ) } -    { \\phi_{n}}f { \\;}.\\end{gathered}\\ ] ] we will show that the three parts of this decomposition are identical with the three parts of . for @xmath144",
    "it holds that , using the definitions and of @xmath456 and @xmath457 , respectively , and following the lines of ( * ? ? ?",
    "* lemma  7.2 ) , @xmath458 \\\\ & = { \\varphi_{k}^n } \\left [ \\frac {      { { w}_{k}}(\\cdot ) { h^{\\mathrm{u}}_{k } } \\cdots    { h^{\\mathrm{u}}_{n-1 } } ( \\cdot , { \\mathsf{x}}^{n+1 } ) ( { \\phi^n_{k-1 } } { { t}_{k-1}})}{{\\phi^n_{k-1 } } { h^{\\mathrm{u}}_{k-1 } } \\cdots    { h^{\\mathrm{u}}_{n-1}}({\\mathsf{x}}^{n+1 } ) } \\left\\ { { \\psi_{k : n}}[f](\\cdot ) + \\frac{{h^{\\mathrm{u}}_{k } } \\cdots { h^{\\mathrm{u}}_{n-1}}f ( \\hat{{x}}_{0:k})}{{h^{\\mathrm{u}}_{k } } \\cdots    { h^{\\mathrm{u}}_{n-1}}(\\hat{{x}}_{0:k } , { \\mathsf{x}}^{n+1 } ) } \\right\\ } \\right ] \\\\ & = { \\alpha_{k}^n } \\left [ { \\psi_{k : n}}[f](\\cdot ) + \\frac{{h^{\\mathrm{u}}_{k } } \\cdots { h^{\\mathrm{u}}_{n-1}}f ( \\hat{{x}}_{0:k})}{{h^{\\mathrm{u}}_{k } } \\cdots    { h^{\\mathrm{u}}_{n-1}}(\\hat{{x}}_{0:k } , { \\mathsf{x}}^{n+1 } ) } \\right ] \\\\ & = { \\alpha_{k}^n } { \\psi_{k : n}}[f ] + \\frac{{h^{\\mathrm{u}}_{k } } \\cdots { h^{\\mathrm{u}}_{n-1}}f ( \\hat{{x}}_{0:k})}{{h^{\\mathrm{u}}_{k } } \\cdots { h^{\\mathrm{u}}_{n-1}}(\\hat{{x}}_{0:k } , { \\mathsf{x}}^{n+1 } ) } { \\;}. \\end{split}\\ ] ] moreover , by definition , @xmath459({\\ifthenelse{\\equal{}{}}{\\tilde{\\xi}_{0:k}^{n , i}}{\\tilde{\\xi}_{0:k}^{n , i}()}})}{\\sum_{j=1}^{r_n(r ) } \\frac{{\\mathrm{d}}{\\alpha_{k}^n}}{{\\mathrm{d}}{\\varphi_{k}^n}}({\\ifthenelse{\\equal{}{}}{\\tilde{\\xi}_{0:k}^{n , j}}{\\tilde{\\xi}_{0:k}^{n , j } ( ) } } ) } + \\frac{{h^{\\mathrm{u}}_{k } } \\cdots { h^{\\mathrm{u}}_{n-1 } } f(\\hat{{x}}_{0:k})}{{h^{\\mathrm{u}}_{k } } \\cdots    { h^{\\mathrm{u}}_{n-1}}(\\hat{{x}}_{0:k } , { \\mathsf{x}}^{n+1 } ) } { \\;},\\ ] ] yielding @xmath460    similarly , for @xmath431 , using the definition of @xmath461 , @xmath462 \\\\ & = { \\beta_{k}^n } \\left [ { \\psi_{k : n}}[f](\\cdot ) + \\frac{{h^{\\mathrm{u}}_{k } } \\cdots { h^{\\mathrm{u}}_{n-1}}f ( \\hat{{x}}_{0:k})}{{h^{\\mathrm{u}}_{k } } \\cdots    { h^{\\mathrm{u}}_{n-1}}(\\hat{{x}}_{0:k } , { \\mathsf{x}}^{n+1 } ) } \\right ] \\\\ & = { \\beta_{k}^n } { \\psi_{k : n}}[f ] + \\frac{{h^{\\mathrm{u}}_{k } } \\cdots { h^{\\mathrm{u}}_{n-1}}f ( \\hat{{x}}_{0:k})}{{h^{\\mathrm{u}}_{k } } \\cdots { h^{\\mathrm{u}}_{n-1}}(\\hat{{x}}_{0:k } , { \\mathsf{x}}^{n+1 } ) } { \\ ; } , \\end{split}\\ ] ] and applying the obvious relation @xmath463({\\ifthenelse{\\equal{}{}}{\\xi_{0:k}^{n , i}}{\\xi_{0:k}^{n , i}()}})}{\\sum_{j=1}^n \\frac{{\\mathrm{d}}{\\beta_{k}^n}}{{\\mathrm{d}}{\\tilde{\\phi}^n_{k}}}({\\ifthenelse{\\equal{}{}}{\\xi_{0:k}^{n , j}}{\\xi_{0:k}^{n , j } ( ) } } ) } + \\frac{{h^{\\mathrm{u}}_{k } } \\cdots { h^{\\mathrm{u}}_{n-1 } } f(\\hat{{x}}_{0:k})}{{h^{\\mathrm{u}}_{k } } \\cdots    { h^{\\mathrm{u}}_{n-1}}(\\hat{{x}}_{0:k } , { \\mathsf{x}}^{n+1 } ) } { \\;},\\ ] ] we obtain the identity @xmath464 the equality @xmath465 follows analogously .",
    "this completes the proof of the lemma .",
    "* a trivial adaption of ( * ? ? ?",
    "* lemmas  7.3 and  7.4 ) gives that @xmath466",
    "\\right \\|_{{\\mathsf{x}}^{k+1 } , \\infty } } \\leq { \\operatorname{osc}}(f_i ) \\rho^{0",
    "\\vee    ( i - k ) } { \\ ; } , \\quad { \\left \\| \\frac{{\\mathrm{d}}{\\alpha_{k}^n}}{{\\mathrm{d}}{\\varphi_{k}^n } } \\right \\|_{{\\mathsf{x}}^{k+1 } , \\infty } } \\leq \\frac{\\left \\| { { w}_{k } } \\right    \\|_{{\\mathsf{x}}^{k+1 } , \\infty } \\left \\| { { t}_{k-1 } } \\right    \\|_{{\\mathsf{x}}^{k } , \\infty}}{{\\mu}{g_{k } } ( 1-\\rho ) { \\epsilon_- } } { \\;}.\\ ] ] * by mimicking the proof of ( * ? ? ?",
    "* proposition  7.1(i ) ) , that is , applying the identity @xmath467 to each @xmath468 and using twice the marcinkiewicz - zygmund inequality together with the bounds , we obtain the bound @xmath469 where @xmath229 is a constant depending on @xmath231 only .",
    "we refer to ( * ? ? ?",
    "* proposition  7.1 ) for details . * for @xmath431 , inspecting the proof of ( * ? ? ?",
    "* lemma  7.4 ) yields immediately @xmath470 and repeating the arguments of the previous item for @xmath471 yields @xmath472 * the arguments above apply directly to @xmath473 , providing + @xmath474      the proof of ( ii ) ( which mimics the proof of ( * ? ? ?",
    "* proposition  7.1(ii ) ) ) follows analogous lines ; indeed , repeating the arguments of ( i ) above for the decomposition @xmath475 gives us the bounds @xmath476 \\right| & \\leq b \\frac { { \\operatorname{osc}}(f_i ) \\left \\| { { w}_{k } } \\right    \\|_{{\\mathsf{x}}^{k+1 } , \\infty}^2 \\left \\| { { t}_{k-1 } } \\right    \\|_{{\\mathsf{x}}^{k } , \\infty}^2 } { ( { \\mu}{g_{k}})^2 ( 1-\\rho)^2 { \\epsilon_-}^2 } \\rho^{0 \\vee    ( i - k ) } { \\ ; } , \\\\",
    "n \\left| { \\mathbb{e}}\\left [ b_k^n(f_i ) \\right ] \\right| & \\leq b \\frac{{\\operatorname{osc}}(f_i)}{(1-\\rho)^2 } \\rho^{0 \\vee ( i - k ) } { \\ ; } , \\\\",
    "n \\left| { \\mathbb{e}}\\left [ c^n(f_i ) \\right ] \\right| & \\leq b \\frac{{\\operatorname{osc}}(f_i ) \\left      \\| { { w}_{0 } } \\right \\|_{{\\mathsf{x } } , \\infty}^2}{(\\nu { g_{0}})^2 ( 1-\\rho)^2 } \\rho^i { \\;}. \\end{split}\\ ] ] we again refer to (",
    "* proposition  7.1(ii ) ) for details , and summing up concludes the proof .      the statement is a direct implication of hlder s inequality .",
    "indeed , let @xmath147 be any first - stage importance weight function and write @xmath477)^2 & = \\ { { \\phi_{k } } ( { { t}_{k}}^{1/2 } { { t}_{k}}^{-1/2 } { { t}_{k}}^\\ast[f ] ) \\}^2 \\\\ & \\leq { \\phi_{k } } { { t}_{k } } \\ , { \\phi_{k } } \\ { { { t}_{k}}^{-1 } ( { { t}_{k}}^\\ast [ f])^2 \\ } { \\;}. \\end{split}\\ ] ] now the result follows by the formula , the identity @xmath478 ) ^2 \\ } = { \\phi_{k } } \\ {    { { t}_{k } } { r_{k}^{\\mathrm{p } } } ( \\cdot , { { w}_{k+1}}^2    { \\phi_{k+1}}^2[f ] ) \\ } { \\;},\\ ] ] and the fact that we have equality in for @xmath479 $ ]",
    ".                                olsson , j. , capp , o. , douc , r. , moulines ,  .",
    "( 2006 ) sequential monte carlo smoothing with application to parameter estimation in non - linear state space models",
    ". technical report , lund university . to appear in _",
    "bernoulli_.    olsson , j. , douc , r. , moulines ,  .",
    "( 2006 ) improving the two - stage sampling algorithm : a statistical perspective .",
    "in _ on bounds and asymptotics of sequential monte carlo methods for filtering , smoothing , and maximum likelihood estimation in state space models _ , pp .  143181 .",
    "doctorial thesis , lund university .",
    "pitt , m.  k. , and shephard , n. ( 1999b ) time varying covariances : a factor stochastic volatility approach ( with discussion ) . in _",
    "bayesian statistics _",
    "bernardo , j.  m. , berger , j.  o. , dawid , a.  p. , smith , a.  f.  m. ) , * 6*. oxford university press : oxford ."
  ],
  "abstract_text": [
    "<S> in this article we study asymptotic properties of weighted samples produced by the auxiliary particle filter ( apf ) proposed by @xcite . besides establishing a central limit theorem ( clt ) for smoothed particle estimates </S>",
    "<S> , we also derive bounds on the @xmath0 error and bias of the same for a finite particle sample size . by examining the recursive formula for the asymptotic variance of the clt </S>",
    "<S> we identify first - stage importance weights for which the increase of asymptotic variance at a single iteration of the algorithm is minimal . in the light of these findings , we discuss and demonstrate on several examples how the apf  algorithm can be improved . </S>"
  ]
}