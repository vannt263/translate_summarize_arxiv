{
  "article_text": [
    "`` cache - coherent nonuniform memory access '' ( ccnuma ) is the preferred system architecture for multisocket shared - memory servers today . in ccnuma , main memory",
    "is logically shared , meaning that all memory locations can be accessed by all sockets and cores in the system transparently . however , since main memory is physically distributed , i.e. , partitioned in so - called _ locality domains _ ( lds ) , access bandwidths and latencies may vary , depending on which core accesses a certain part of memory .",
    "access is fastest from the cores directly attached to a domain .",
    "nonlocal accesses are mediated by some inter - domain network , which is also capable of maintaining cache coherency throughout the system .",
    "the big advantage of ccnuma is that the available main memory bandwidth scales with the number of lds , and shared - memory nodes with hundreds of domains can be built .",
    "many applications in science and engineering rely on large memory bandwidth ; computational fluid dynamics ( cfd ) and sparse matrix eigenvalue solvers are typical examples .",
    "however , applications using shared - memory programming models like , e.g. , openmp  @xcite , tbb  @xcite , or posix threads , should make sure that locality of access is maintained . massive performance breakdowns may be observed when nonlocal ( inter - ld ) accesses or contention on an ld s memory bus become bottlenecks  @xcite .",
    "one should add that the current openmp standard , although it is the dominant threading model for scientific user codes , does not contain any features that would enable ccnuma access optimizations .",
    "most operating systems support a _ first touch _ ccnuma placement policy : after allocation ( using , e.g. , ` malloc ( ) ` ) , the mapping of logical to physical memory addresses is not established yet ; the first write access to an allocated memory page will map the page into the locality domain of the core that executed the write .",
    "this makes it straightforward to optimize parallel memory access in applications that have regular memory access patterns .",
    "if the loop(s ) that initialize array data are parallelized in exactly the same way and use the same access patterns as the loops that use the data later , nonlocal data transfer can be minimized . a prerequisite for first touch initialization to work reliably",
    "is that threads are not allowed to move freely through the shared - memory machine but maintain their affinity to the core they were initially bound to .",
    "some threading models discourage the use of strong thread - core affinity , but numerically intensive high - performance parallel applications usually benefit from it . operating systems often provide libraries and tools to enable a more fine - grained control over page placement . under linux ,",
    "the ` numactl ` command and the ` libnuma ` library are part of every standard distribution .",
    ">    ' '' ''    l|cccc    & & * istanbul & * nehalem ep & * nehalem ex + type & & opteron 8431 & xeon x5550 & xeon x7560 + frequency [ ghz ] & & 2.41 & 2.66 & 2.27 + cores per chip & & 6 & 4 & 8 + sockets per system & & 4 & 2 & 4 + l1 size [ kb ] & & 64 & 32 & 32 + l2 size [ kb ] & & 512 & 256 & 256 + l3 size [ mb ] & & 5 & 8 & 24 + l3 cache group [ cores ] & & 6 & 4 & 8 + sockets per system & & 4 & 2 & 4 + ccnuma interconnect & & hypertransport ( ht ) & quickpath ( qpi ) & quickpath ( qpi ) + stream copy bandwidth [ gbytes / sec ] & & & & +  full system & & 38.6 ( nt ) & 36.6 ( nt ) & 33.4 +  socket & & 9.9 ( nt ) & 18.9 ( nt ) & 8.15 + * * *    topology of the amd istanbul test system with four locality domains . the intel nehalem ex system is very similar but has eight instead of six cores per socket , and each socket has direct qpi connections to all other sockets . ]",
    "topology of the intel nehalem ep test system with two locality domains . ]",
    "unfortunately the `` first touch '' scheme does not work in all cases .",
    "sometimes memory access can not be organized in contiguous data streams or , even if that is possible , the problem itself may be irregular and show strong load imbalance if a simple static work distribution is chosen .",
    "_ dynamic scheduling _ is the general method for handling the latter case . the openmp standard  @xcite provides the ` dynamic ` and ` guided ` scheduling types for worksharing loops , and the ` task ` construct for task - based parallelism . in intel threading building blocks ( tbb ) @xcite , a _ task _ is the central scheduling entity as well , and distribution of tasks across threads is fully dynamic .",
    "if the additional overhead for dynamic scheduling is negligible for the application at hand , these approaches are ideal on uma ( uniform memory access ) systems like the now outdated single - core multi - socket smp nodes , or multi - core chips with `` isotropic '' caches , i.e. , where each cache level is either exclusive to one core or shared among all cores on a chip . on ccnuma systems , however , dynamic scheduling leads to nonlocal memory accesses and contention on the ld s memory buses .",
    "the simplest option to choose is then to distribute memory pages across locality domains in a cyclic fashion using , e.g. , the above - mentioned numa tools , which will lead to at least a certain degree of parallel memory access . under linux",
    ", one may write :    .... > env omp_num_threads=8 numactl -i 0 - 3 ./a.out ....",
    "this will start the ( openmp ) binary with eight threads and make sure that memory pages are mapped cyclically across four lds ( 03 ) .",
    "initialization inside the program is then insignificant for the placement unless special libraries are used , so it may be done sequentially just as well .",
    "the purpose of this work is to demonstrate that a simple user - level software layer can make close to optimal ccnuma page placement possible even with dynamic scheduling or tasking , by sorting tasks upon initialization into a number of _",
    "locality queues_. we will show that our scheme works for openmp tasking and parallel tbb constructs , and compare it to the `` affinity partitioner '' in tbb  @xcite , which has a similar purpose .",
    "contrary to the assumption that tasking causes `` random '' page access , the order in which tasks are submitted to the execution thread pool can have a noticeable impact on performance .      using",
    "the default first - touch policy with parallel initialization is a simple optimization technique for memory - bound shared - memory parallel code , but ccnuma awareness is unfortunately not yet well established among application programmers in science and engineering .",
    "moreover , although introducing multiple execution queues with a work - stealing scheme on top is not new , the possibilities for enhancing ccnuma access locality under dynamic task scheduling _ with user code only _ and within the capabilities of current compilers and os environments have not been explored in great detail .",
    "most work concentrates on low - level thread scheduling techniques for various threading models ( mostly openmp and cilk ) , either runtime - based @xcite , os - based @xcite , or even hardware - based  @xcite .",
    "automatic page migration @xcite can enhance locality significantly , but is again not generally applicable and must necessarily employ , to varying extent , heuristic methods to decide about page placement .    the method proposed here",
    "consists of a thin software layer that effectively modifies the task scheduling algorithm employed by the compiler and runtime system , based on locality information that can either be supplied by the user or obtained automatically , depending on the situation .",
    "we have chosen three ccnuma - type systems for performing benchmarks ( see table  [ tab : systems : overview ] ) .",
    "the six - core amd `` istanbul '' ( see fig .",
    "[ fig : i4 ] ) and quad - core intel `` nehalem ep '' processors ( see fig .",
    "[ fig : nep ] ) have been on the market for some time ; the eight - core `` nehalem ex '' , however , has been introduced only recently .",
    "our early - access nehalem ex benchmark system was equipped with only half the maximum number of memory boards per socket , which leads to a reduction of the effective main memory bandwidth by a factor of two .",
    "although of minor importance for the results presented here , this is of course not a desirable configuration for a production system .",
    "all systems ran current linux kernels .",
    "the intel c++ compiler in version 11.1.064 and tbb version 3.0 ( open source variant ) were used for the benchmarks .",
    "all three systems have a similar maximum bandwidth as measured by the stream copy benchmark  @xcite , which models closely the memory access behavior of the jacobi solver .",
    "nontemporal stores ( `` nt '' ) were used if appropriate ; nt stores bypass the cache hierarchy and can improve store bandwidth by avoiding the write - allocate cache line transfer on store misses .",
    "as a simple benchmark we choose a 3d six - point jacobi solver with constant coefficients as recently studied extensively by datta et al .",
    "the site update function , @xmath0   ~,\\end{aligned}\\ ] ] is evaluated for each lattice site in a 3d loop nest .",
    "each site update ( in the following called `` lup '' ) incurs six loads and one store , of which , at large problem sizes , one load and one store cause main memory traffic if suitable spatial blocking is applied .",
    "this leads to a code balance of 8/3 bytes per flop ( assuming that nontemporal stores are used so that a store miss does not cause a cache line write - allocate transfer ) , so the code is clearly memory - bound on all current cache - based architectures . in what follows we use a problem size of @xmath1@xmath2@xmath3 sites ( @xmath413 gb of memory for both grids and double precision variables ) and a blocksize of 600@xmath210@xmath2100 ( @xmath5@xmath2@xmath6@xmath2@xmath7 , with @xmath8 being the inner [ fast ] index ) sites , unless otherwise noted .",
    "this is close to the optimal block dimensions on all architectures considered here . in a standard openmp - parallel implementation",
    ", the update loop nest iterates over all blocks in turn , and standard worksharing parallelization is done over the three collapsed blocking loops ( first - touch initialization is performed via the identical scheme ) :    .... # pragma omp parallel for \\",
    "collapse(3 ) schedule(runtime )    for(int ib=0 ; ib < no_of_i_blocks ; + + ib ) {      for(int jb=0 ; jb < no_of_j_blocks ; + + jb ) {        for(int kb=0 ; kb < no_of_k_blocks ; + + kb ) {          jacobi_sweep_block(ib , jb , kb ) ;    } } } ....    note that with the standard ` k ` blocksize being equal to the extent of the lattice in that direction ( which is required to make best use of the hardware prefetching capabilities on the processors used ) , ` no_of_k_blocks ` is equal to one .",
    "the ` jacobi_sweep_block ( ) ` function performs one jacobi sweep , i.e. , one update per lattice site , over all sites in the block determined by its parameters . in case of dynamic loop scheduling there is a choice as to how parallel first - touch initialization should be done ; both ` static,1 ` ( round robin ) and plain ` static ` scheduling will be investigated .",
    "note that this simple benchmark is not a typical application scenario for tasking , since the load is evenly distributed and parallelization with standard openmp loop worksharing constructs is straightforward .",
    "however , it provides a well - controlled environment for showing the effects of dynamic scheduling and the limitations of runtime systems .",
    "moreover , even applications with very regular access patterns can benefit from task - based parallelism , because functional decomposition into `` communicating '' and `` computing '' tasks is greatly simplified .",
    "this has been demonstrated recently in the context of a 3d particle - in - cell code  @xcite . when using a threading model together with message passing ( mpi ) in hybrid shared / distributed - memory programming it is also vital to reduce per - node performance variations , since those will limit scalability of the whole application .",
    "we will briefly comment on this problem below .",
    "for openmp we enforced strict thread - core affinity in all benchmark runs by using the linux ` sched_setaffinity ( ) ` function . in production environments ,",
    "more user - friendly tools like hwloc  @xcite or likwid - pin  @xcite are certainly preferable . in tbb ,",
    "the concept of a `` thread '' or its affinity to a piece of hardware is not made explicit for the programmer ; a simple ` parallel_for ` loop with the number of iterations equal to the number of spawned threads is repeated until each thread was assigned a `` dummy '' task for the sole purpose of calling ` sched_setaffinity ( ) ` and establishing a fixed thread - core mapping .",
    "[ [ impact - of - suboptimal - page - placement ] ] impact of suboptimal page placement + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the horizontal lines in all panels of fig .",
    "[ fig : perf - all ] illustrate the impact of suboptimal page placement on the solver s performance .",
    "the lowest performance is consistently achieved with purely sequential initialization , i.e. , with a serial initialization loop , and static worksharing . in this limit",
    ", the memory interface of a single ld becomes a bottleneck and the cores in all but this single domain have to access their data via the ccnuma network . round - robin placement as established , e.g. , with the ` numactl ` tool , and boosts performance significantly by enabling at least some level of parallelism .",
    "optimal bandwidth utilization is of course reached with static , parallel first - touch placement , and comes close to the stream copy numbers in table  [ tab : systems : overview ] . on a uma system ( or within a single ccnuma domain ) , all three lines would match .",
    "the penalty for round - robin placement is especially large for the nehalem ep system , since it has the strongest `` numa effect '' ( bandwidth reduction for nonlocal access ) . on the other hand",
    ", the performance level for sequential placement is particularly low on nehalem ex , which can be attributed to the fact that our ea system is extremely bandwidth - starved due to the lack of half the memory boards per ld .    performance variability with openmp tasking ( left ) and the tbb ` parallel_for ` construct ( right ) .",
    "median , @xmath925% , and @xmath945% quantiles are indicated ( 100 samples each ) . ]",
    "note that the impact of scheduling overhead is not investigated here .",
    "if the amount of work per task is small , dynamic scheduling can potentially be hazardous for performance  @xcite .",
    "in contrast to standard worksharing loop parallelization , tasking in openmp requires to split the problem into a number of work `` packages '' , called _ tasks _ , each of which must be submitted to an internal pool via the ` omp task ` directive . for the jacobi solver we define one task to be a single block of the size specified above .",
    "this is in contrast to standard static loop worksharing , where one parallelized loop iteration consisted of several blocks with different coordinates .",
    "the tasks are produced ( `` submitted '' ) by a single thread and consumed by all threads and in a 3d loop nest :    .... # pragma omp parallel {    # pragma omp single    {      for(int ib=0 ; ib < no_of_i_blocks ; + + ib ) {        for(int jb=0 ; jb < no_of_j_blocks ; + + jb ) {           for(int kb=0 ; kb < no_of_k_blocks ; + + kb ) {            # pragma omp task              jacobi_sweep_block(ib , jb , kb ) ;      } } }    } } ....    submitting the tasks in parallel is possible but did not make any difference in the parameter ranges considered here .",
    "this parallel block is actually a `` worksharing '' construct , since all threads that are waiting in the implicit barrier at the end of the ` omp single ` construct execute tasks that have been submitted by the one thread that entered the ` single ` region . after finishing the submit loop nest",
    ", this thread will join the others .",
    "in contrast to the code above , which submits tasks in ` jb ` direction first ( `` ijk '' ; the single block in ` kb ` direction does not count ) , the loop nest order can be reversed ( `` kji '' ) , leading to a functionally equivalent code .",
    "there is also a choice as to how first - touch initialization should be performed , so we compare ` static ` and ` static,1 ` scheduling ( `` s '' vs. `` s-1 '' ) for loop initialization .",
    "the left column of panels in fig .",
    "[ fig : perf - all ] shows performance results on all platforms . the four combinations of ijk / kji submit order with static / static,1 initialization are indicated below the graph . in general , this code is never faster than standard static worksharing with round - robin placement .",
    "combining static initialization with ijk submit order seems to be especially unfortunate .",
    "the large impact of submit and initialization orders can be explained by assuming that there is only a limited number of `` queued '' , i.e. , unprocessed tasks allowed at any time . in the course of executing the submission loop , this limit is reached very quickly and the submitting thread is used for processing tasks for some time . from our measurements ,",
    "the limit is set to roughly 256 tasks with the compiler used ( current gnu compilers have the same limit ) .",
    "one ` ib`-`jb ` layer of the grid comprises 60 tasks ( with the chosen problem and block sizes ) , and 240 layers are available , which amounts to 14400 tasks in total . with static scheduling on initialization",
    ", one block of 256 consecutive tasks is usually associated with a single locality domain ( rarely two ) , hence the serialization of memory access .",
    "choosing ` static,1 ` scheduling for initialization , each row of @xmath10 consecutive blocks ( @xmath10 being the number of threads per socket ) is placed into a different locality domain , but 256 tasks comprise only slightly more than four layers . assuming that the order of execution for tasks resembles ` static,1 ` loop workshare scheduling because each thread is served a task in turn , the number of lds to be accessed in parallel is limited ( although it is hard to predict the actual level of parallelism , since it is also influenced by the number of threads per ld ) .",
    "finally , by choosing the kji submission loop order , consecutive tasks cycle through locality domains , and parallelism is as expected from dynamic loop scheduling . in all cases ,",
    "performance variability is surprisingly small ( see left panel in fig .",
    "[ fig : nv ] ) .    these observations document that it is nontrivial to employ tasking on ccnuma systems and reach at least the performance level of standard dynamic loop scheduling or round - robin page placement . in the next section",
    "we will demonstrate how task scheduling under locality constraints can be optimized by `` overriding '' part of the openmp task scheduling by user program logic .",
    "each task , which equals one lattice block ( or tile ) in our case , is associated with a c++ object ( of type ` block ` ) and equipped with an integer locality variable .",
    "this variable denotes the locality domain the block was placed in upon initialization .",
    "the submission loop now takes the following form :    .... # pragma omp parallel {    # pragma omp single    {      for(int ib=0 ; ib < no_of_i_blocks ; + + ib ) {        for(int jb=0 ; jb < no_of_j_blocks ; + + jb ) {           for(int kb=0 ; kb < no_of_k_blocks ; + + kb ) {            block * b = blocks[ib][jb][kb ] ;            queues[b->locality()].enqueue(b ) ;            # pragma omp task              process_block_from_queue(queues ) ;      } } }    } } ....    the ` queues ` object is a ` std::vector < > ` of ` std::queue < > ` objects , each associated with one locality domain , and each protected from concurrent access via an openmp lock . calling the ` enqueue ( ) ` method of a queue",
    "appends a block object to it . as shown above , blocks are sorted into those _ locality queues _ according to their respective locality variables .",
    "one openmp task , executed by the ` process_block_from_queue ( ) ` function , now consists of two parts :    1 .   figuring out which ld the executing thread belongs to 2 .",
    "dequeuing the oldest waiting block in the locality queue belonging to this domain and calling ` jacobi_sweep_block ( ) ` for it    if the local queue of a thread is empty , other queues are tried in a spin loop until a block is found ( `` work stealing '' ) :    .... void process_block_from_queue(locality_queues \\                  & queues ) {    // ...    bool found = false ;    block * b ;    int ld = ld_id[omp_get_thread_num ( ) ] ;    while ( ! found ) {      found = queues[ld].dequeue(p ) ;      if ( ! found ) {        ld = ( ld + 1 ) % queues.size ( ) ;      }    }    jacobi_sweep_block(b->ib , b->jb , b->kb ) ; } ....    the global ` ld_id ` vector must be preset with a correct mapping of thread numbers to locality domains .",
    "it is possible with the described scheme that some task executes a block just queued before the corresponding task is actually submitted .",
    "this is however not a problem because the number of submitted tasks is always equal to the number of queued blocks , and no task will ever be left waiting for new blocks forever .",
    "note that scanning other queues if a thread s local queue is empty gives load balancing priority over strict access locality , which may or may not be desirable depending on the application .",
    "the team of threads in one locality domain shares one queue , so scheduling is still purely dynamic inside an ld .    the second column of panels in fig .",
    "[ fig : perf - all ] shows performance results : for ` static ` initialization and the ijk submission order , the limited overall number of waiting tasks has the same consequences as with plain tasking ( see sect .  [",
    "sec : tasking ] ) . in this case",
    ", although the queuing mechanism is in effect , a single queue holds most of the tasks at any point in time .",
    "all threads are served from this queue and thus mostly execute in a single ld . however , using the alternate kji submission order or ` static,1 ` initialization , all queues are fed in parallel and threads can always be served tasks from their local queue .",
    "performance then comes close to static scheduling within a 10% margin .",
    "one should note that a similar effect could have been achieved with nested parallelism , using one thread per ld in the outer parallel region and several threads ( one per core ) in the nested region .",
    "however , we believe our approach to be more powerful and easier to apply if properly wrapped into c++ logic that takes care of affinity and work distribution . moreover , the thread pooling strategies employed by many current compilers inhibit sensible affinity mechanisms when using nested openmp constructs .",
    "the universal tbb construct for task - parallel execution is the ` parallel_for ` function . initializing all blocks by `` first touch '' and performing a domain sweep looks as follows :    .... tbb::parallel_for (    tbb::blocked_range_3d < int > (                 0 , no_of_i_blocks , 1 ,                  0 , no_of_j_blocks , 1 ,                  0 , no_of_k_blocks , 1 ) ,     touch_block(blocks ) ) ;    tbb::parallel_for (    tbb::blocked_range_3d < int > (                 0 , no_of_i_blocks , 1 ,                  0 , no_of_j_blocks , 1 ,                  0 , no_of_k_blocks , 1 ) ,    update_block(blocks ) ) ; ....    the ` tbb::blocked_range_3d < > ` object encodes the way the three - dimensional domain ( of blocks ) is cut into subdomains .",
    "here we have specified that the smallest unit in each coordinate direction is a single block . in tbb the user must provide a c++ class that implements ` operator ( ) ` ( i.e. , a _ functor _ ) , which takes a reference to the range object and performs the actual `` work '' :    .... class update_block {    blocks & m_blocks ;     public :    update_block(blocks & b )       : m_blocks(b ) { }      void operator()(tbb::blocked_range_3d < int >                                       & subrange ) {      // ... iteration loop nest       //      over subrange - > bi , bj , bk        jacobi_sweep_block(ib , jb , kb ) ;      // ... end iteration loop nest    } // ... } ; ....    the ` subrange ` parameter to the functor may encode a single block or a consecutive range of blocks along all coordinates ; this is a decision made at runtime by tbb .    the third column of panels in fig .",
    "[ fig : perf - all ] shows performance results for tbb with the scheme just described , comparing the situation with and without binding threads to cores ( `` p '' vs. `` n - p '' ) and without using the affinity partitioner ( `` n - a '' , see below ) . since first - touch placement is done via a ` parallel_for ` loop , page mapping is dynamic and performance is close to the round - robin placement case with standard openmp worksharing , as expected .",
    "the mediocre results on the istanbul system are surprising ; it is as yet unclear why tbb should perform worse than openmp with our locality optimizations employed .",
    "tbb provides a user - friendly way to specify that affinity information is important for performance .",
    "the ` tbb::parallel_for ` function takes an optional `` partitioner '' argument , which can be set to ` tbb::affinity_partitioner ` . in this case",
    "tbb stores information about thread - task affinity in an internal data structure on the first call to ` tbb::parallel_for ` . on subsequent parallel loops ,",
    "the scheduler tries to map tasks to the same threads as before , thereby establishing access locality automatically .",
    "the affinity partitioner must thus be specified on both the initialization and update loops .",
    "the third column of panels shows performance results with this optimization ( `` a '' ) , with and without binding threads to cores ( `` p '' vs. `` n - p '' ) . obviously the affinity partitioner can significantly improve locality of access and is able to match the performance of openmp tasking with locality queues",
    "it is possible to adapt the locality queue mechanism to tbb as well , by letting the ` update_block ( ) ` functor enqueue the blocks in the assigned subrange into the appropriate locality queues , and updating the same number of blocks ( preferably ) from the executing thread s local queue . instead of ` std : queue < > ` , the ` tbb::concurrent_queue < > ` container is used here since it provides automatic fine - grained locking .",
    "however , the performance benefit compared to the affinity partitioner is marginal ( see the fourth column of panels in fig .  [",
    "fig : perf - all ] ) . this can be attributed to the fact that submission order ( as defined in the openmp tasking versions ) can not be controlled in this setting . using a one - dimensional partitioner or a ` parallel_do ` construct could enable finer control over page placement , but the expected additional benefit is small .",
    "we have demonstrated how locality queues can be employed to optimize parallel memory access on ccnuma systems when openmp tasking or the tbb ` parallel_for ` construct is used .",
    "locality queues substitute the uncontrolled , dynamic task scheduling by a static and a dynamic part .",
    "the latter is mostly restricted to the cores in one numa domain , providing full dynamic load balancing on the locality domain ( ld ) level .",
    "scheduling between domains is static , but load balancing is given priority over strictly local access by a work stealing scheme .",
    "the larger the number of threads per ld , the more dynamic the task distribution , so our scheme will get more interesting in view of future many - core processors . using locality queues with tbb s ` parallel_for ` construct does not outperform the built - in affinity partitioner , but the impact on ` parallel_do ` can not be inferred from this result , and is yet to be investigated .",
    "note that the concept would in principle work also without thread - core affinity because the current locality domain i d of a thread could be determined at any time , and the static mapping of threads to lds would become obsolete .",
    "future work encompasses the application of the concept to real application codes , notably sparse matrix eigenvalue solvers , where load balancing and overlapping computation with communication may be achieved in a natural way by tasking .",
    "further potentials , not restricted to ccnuma architectures , may be found in the possibility to implement temporal blocking ( doing more than one time step on a block to reduce pressure on the memory subsystem @xcite ) by associating one locality queue to a number of cores that share a cache level . as an advantage over static temporal blocking , no frequent global barriers would be required .",
    "fruitful discussions with michael meier , gerhard wellein and thomas zeiser are gratefully acknowledged .",
    "we thank intel germany for providing early access hardware and technical support .",
    "this work was supported by bmbf via grant no .",
    "01ih08003a ( project skalb ) .",
    "xx the openmp api specification for parallel programming .",
    "j.  reinders : intel threading building blocks .",
    "oreilly , isbn  978 - 0596514808 ( 2007 ) . g.  hager , g.  wellein : introduction to high performance computing for scientists and engineers .",
    "crc press , isbn  978 - 1439811924 ( 2010 ) .",
    "k.  datta , m.  murphy , v.  volkov , s.  williams , j.  carter , l.  oliker , d.  patterson , j.  shalf , k.  yelick : stencil computation optimization and autotuning on state - of - the - art multicore architectures .",
    "proceedings of sc08 , austin , tx , nov .  1521 , 2008 . f.  broquedis , n.  furmento , b.  goglin , r.  namyst , p.  wacrenier : dynamic task and data placement over numa architectures : an openmp runtime perspective .",
    "iwomp 2009 , dresden , germany , june 0305 , 2009 .",
    "lecture notes in computer science , vol .",
    "springer - verlag , berlin , heidelberg , 79 - 92 .",
    "doi : 10.1007/978 - 3 - 642 - 02303 - 3_7 f.  broquedis , n.  furmento , b.  goglin , p.  wacrenier , r.  namyst : forestgomp : an efficient openmp environment for numa architectures .",
    "international journal of parallel programming , springer ( 2010 ) .",
    "doi : 10.1007/s10766 - 010 - 0136 - 3 e.  ben amos : cilk on cc - numa machines .",
    "master s thesis , tel aviv university ( 2006 ) .",
    "j.  meng , j.w .",
    "sheaffer , k.  skadron : exploiting inter - thread temporal locality for chip multithreading .",
    "ipdps 2010 , doi : 10.1109/ipdps.2010.5470465 f.  schmidt , c.  v.  praun : programming for cache locality on cmps with memory temperatures ( abstract ) , poster and work in progress session at eurosys , april 2009 .",
    "r.  yang , j.  antony , a.  rendell : effective use of dynamic page migration on numa platforms : the gaussian chemistry code on the sunfire x4600m2 system .",
    "ispan 2009 , 6368 .",
    "doi : 10.1109/i - span.2009.127 stream : sustainable memory bandwidth in high performance computers .",
    "a.  koniges , r.  preissl , j.  kim , d.  eder , a.  fisher , n.  masters , v.  mlaker , s.  ethier , w.  wang , m.  head - gordon : application acceleration on current and future cray platforms .",
    "cug 2010 , the cray user group meeting , edinburgh , scotland , may 2010 .",
    "http://www.nersc.gov/news/reports/technical/cug2010alice.pdf portable hardware locality ( hwloc ) .",
    "http://www.open-mpi.org/projects/hwloc/ likwid  a lightweight tool collection for multithreaded high performance programming .",
    "j. treibig , g. wellein , g. hager : efficient multicore - aware parallelization strategies for iterative stencil computations ."
  ],
  "abstract_text": [
    "<S> task parallelism as employed by the openmp task construct or some intel threading building blocks ( tbb ) components , although ideal for tackling irregular problems or typical producer / consumer schemes , bears some potential for performance bottlenecks if locality of data access is important , which is typically the case for memory - bound code on ccnuma systems . </S>",
    "<S> we present a thin software layer ameliorates adverse effects of dynamic task distribution by sorting tasks into locality queues , each of which is preferably processed by threads that belong to the same locality domain . </S>",
    "<S> dynamic scheduling is fully preserved inside each domain , and is preferred over possible load imbalance even if nonlocal access is required , making this strategy well - suited for typical multicore - mutisocket systems . </S>",
    "<S> the effectiveness of the approach is demonstrated by using a blocked six - point stencil solver as a toy model . </S>"
  ]
}