{
  "article_text": [
    "linear mixed models have been studied for a long time theoretically , and have many applications , for example , longitudinal data analysis in biostatistics , panel data analysis in econometrics , and small area estimation in official statistics .",
    "the problem of selecting explanatory variables in linear mixed models is important and has been investigated in the literature .",
    "@xcite is a good survey on model selection in linear mixed models .    when the purpose of the variable selection is to find a set of significant variables for a good prediction , akaike - type information criteria @xcite are well - known methods",
    "however , the akaike information criterion ( aic ) based on marginal likelihood , which integrates out likelihood with respect to random effects , is not appropriate when the prediction is focused on random effects .",
    "then , @xcite proposed considering akaike - type information based on the conditional density given the random effects and proposed the conditional aic ( caic ) . here",
    ", we provide a brief explanation of the caic concept .",
    "let @xmath0 be a conditional density function of @xmath1 given @xmath2 , where @xmath1 is an observable random vector of the response variables , @xmath3 is a vector of the unknown parameters , and @xmath2 is a random vector of the random effects .",
    "let @xmath4 be a density function of @xmath2 .",
    "then , @xcite proposed measuring the prediction risk of the plug - in predictive density @xmath5 relative to kullback ",
    "leibler divergence : @xmath6 f(\\y|\\b,\\bta ) \\pi(\\b|\\bta ) \\dd\\y\\dd\\b , \\label{eqn : ckl}\\end{aligned}\\ ] ] where @xmath7 is an independent replication of @xmath1 given @xmath2 , and @xmath8 and @xmath9 are some predictor or estimator of @xmath2 and @xmath3 , respectively .",
    "the caic is an ( asymptotically ) unbiased estimator of a part of the risk in ( [ eqn : ckl ] ) , which is called the conditional akaike information ( cai ) , given by @xmath10 the caic as a variable selection criterion in linear mixed models has been studied by @xcite , @xcite , @xcite , @xcite , @xcite , and others .",
    "furthermore , the caic has been constructed as a variable selection criterion in generalized linear mixed models by @xcite , @xcite , @xcite , @xcite , and others .",
    "considering the prediction problem , it is often the case that the values of covariates in the predictive model are different from those in the observed model , which we call covariate shift . here",
    ", we call the model in which @xmath1 is the vector of the response variables the `` observed model , '' and we call the model in which @xmath7 is the vector of the response variables the `` predictive model . ''",
    "it is noted that the term `` covariate shift '' was first used by @xcite , who defined it as the situation in which the distribution of the covariates in the predictive model differs from that in the observed model . in this study , although we treat the covariates as non - random , we use the same term `` covariate shift '' as @xcite .",
    "even when the information about the covariates in the predictive model can be used , most of the akaike - type information criteria do not use it .",
    "this is because it is assumed that the predictive model is the same as the observed model for deriving the criteria .",
    "as for the abovementioned caic , the conditional density of @xmath1 given @xmath2 and that of @xmath7 given @xmath2 are the same , both of which are denoted by @xmath11 . on the other hand , under the covariate shift , the conditional density of @xmath7 given @xmath2 is different from that of @xmath1 given @xmath2 and is denoted by @xmath12 .",
    "when the aim of the variable selection is to choose the best predictive model , it is not appropriate to use the covariates only in the observed model",
    ". then , we redefine the cai under covariate shift , as follows , @xmath13 and construct an information criterion as an asymptotically unbiased estimator of the cai . @xcite",
    "considered a similar problem in the multivariate linear regression model and proposed a variable selection criterion .",
    "it is important to note that we do not assume that the candidate model is overspecified , in other words , that the candidate model includes the true model .",
    "although most of the akaike - type information criteria make the overspecified assumption , this is not appropriate for estimating the cai under covariate shift .",
    "we discuss this point in section [ subsec : drawback ] .    as an important applicable example of covariate shift , we focus on small area prediction , which is based on a finite super - population model .",
    "we consider the situation in which we are interested in the finite subpopulation ( area ) mean of some characteristic and that some values in the subpopulation are observed through some sampling procedure .",
    "when the sample size in each area is small , the problem is called small area estimation . for details about small area estimation ,",
    "see @xcite , @xcite , @xcite , and others .",
    "the model - based approach in small area estimation often assumes that the finite population has a super - population with random effects and borrows strength from other areas to estimate ( predict ) the small area ( finite subpopulation ) mean .",
    "the well - known unit - level model is the nested error regression model ( nerm ) , which is a kind of linear mixed model , and was introduced by @xcite .",
    "the nerm can be used when the values of the auxiliary variables for the units with characteristics of interest ( response variables in the model ) are observed through survey sampling .",
    "this is the observed model in the framework of our variable selection procedure . on the other hand ,",
    "two types of predictive model can be considered .",
    "one is the unit - level model , which can be used in the situation in which the values of the auxiliary variables are known for all units .",
    "the other is the area - level model , which can be used in the situation in which each mean of the auxiliary variables is known for each small area .",
    "the latter is often the case in official statistics and the model introduced by @xcite is often used in this case .",
    "the rest of this paper is organized as follows . in section [ sec : setup ] , we explain the setup of variable selection problem . in section [ sec : criteria ] , we define the cai under covariate shift in linear mixed models and obtain an asymptotically unbiased estimator of the cai . in section [ sec : ex ] , we provide an example of covariate shift , which is focused on small area prediction . in section",
    "[ sec : simu ] , we investigate the numerical performance of the proposed criteria by simulations , one of which is design - based simulation based on a real dataset of land prices .",
    "all the proofs are given in the appendix .",
    "we focus on the variable selection of the fixed effects .",
    "first , we consider the collection of candidate models as follows .",
    "let @xmath14 matrix @xmath15 consist of all the explanatory variables and assume that @xmath16 .",
    "in order to define candidate models by the index set @xmath17 , suppose that @xmath17 denotes a subset of @xmath18 containing @xmath19 elements , _",
    "i.e. _ , @xmath20 , and @xmath21 consists of @xmath19 columns of @xmath15 indexed by the elements of @xmath17 .",
    "we define the class of the candidate models by @xmath22 , namely , the power set of @xmath23 , in which we call @xmath23 the full model .",
    "we assume that the true model exists in the class of the candidate models @xmath24 , which is denoted by @xmath25 .",
    "it is noteworthy that the dimension of the true model is @xmath26 , which is abbreviated to @xmath27 .",
    "we next introduce the terms `` overspecified '' and `` underspecified '' models .",
    "candidate model @xmath17 is overspecified if @xmath28 $ ] , which means that @xmath29 is in the column space of @xmath21 following @xcite or @xcite .",
    "the set of overspecified models are denoted by @xmath30 . on the other hand , candidate model @xmath17 is underspecified when @xmath31 $ ] .",
    "the set of underspecified models is denoted by @xmath32 .",
    "it is important to note that most of the akaike - type information criteria are derived under the assumption that the candidate model is overspecified .",
    "however , the assumption is not appropriate for considering the covariate shift , which is explained in section [ subsec : drawback ] .",
    "thus , we derive the criterion without the overspecified assumption .    in the following two subsections ,",
    "we clarify the observed model and predictive model for deriving the criteria .",
    "the candidate observed model @xmath17 is the linear mixed model @xmath33 where @xmath1 is an @xmath34 observation vector of response variables , @xmath21 and @xmath35 are @xmath36 and @xmath37 matrixes of covariates , respectively , @xmath38 is a @xmath39 vector of regression coefficients , @xmath40 is a @xmath41 vector of random effects , and @xmath42 is an @xmath34 vector of random errors .",
    "let @xmath40 and @xmath42 be mutually independent and @xmath43 , @xmath44 , where @xmath45 and @xmath46 are @xmath47 and @xmath48 positive definite matrixes and @xmath49 is a scalar .",
    "we assume that @xmath45 and @xmath46 are known and @xmath49 is unknown .",
    "the true observed model @xmath25 is @xmath50 where @xmath51 , @xmath52 and @xmath53 is @xmath54 vector of regression coefficients , whose @xmath55 components are exactly @xmath56 and the rest of the components are not @xmath56 .",
    "the fact that we can write the true model as the equation above implies that the true model exists in the class of candidate models @xmath24 .",
    "note that @xmath15 is @xmath14 matrix of covariates for the full model @xmath23 .",
    "then , the marginal distribution of @xmath1 is @xmath57 where @xmath58 . for the true model , the conditional density function of @xmath1 given @xmath59 and the density function of @xmath59",
    "are denoted by @xmath60 and @xmath61 , respectively .      the candidate predictive model @xmath17 is the linear mixed model , which has the same regression coefficients @xmath38 and random effects @xmath40 as the candidate observed model @xmath17 , but different covariates , namely , @xmath62 where @xmath7 is @xmath63 random vector of the target of prediction , @xmath64 and @xmath65 are @xmath66 and @xmath67 matrixes of covariates whose columns correspond to those of @xmath21 and @xmath35 , respectively , and @xmath68 is @xmath63 vector of random errors , which is independent of @xmath40 and @xmath42 and is distributed as @xmath69 , where @xmath70 is a known @xmath71 positive definite matrix .",
    "we assume that we know the values of @xmath64 and @xmath65 in the predictive model and that they are not necessarily the same as those of @xmath21 and @xmath35 in the observed model .",
    "we call this situation covariate shift",
    ". the conditional density function of @xmath7 given @xmath40 for the model @xmath17 is denoted by @xmath72 .",
    "the true predictive model @xmath25 is @xmath73 where @xmath74 is @xmath75 matrix of covariates and @xmath76 .",
    "then , the marginal distribution of @xmath7 is @xmath77 where @xmath78 . for the true model , the conditional density function of @xmath7 given @xmath59",
    "is denoted by @xmath79 .",
    "the cai introduced by @xcite measures the prediction risk of the plug - in predictive density @xmath80 , where @xmath81 and @xmath82 are maximum likelihood estimators of @xmath38 and @xmath49 , respectively , which are given as @xmath83 respectively , and @xmath84 is the empirical bayes estimator of @xmath40 for quadratic loss , which is given by @xmath85 then , the cai under covariate shift is @xmath86 \\\\ = & \\ e^{(\\y,\\b_*)}e^{\\ybt|\\b _ * } \\left [ m\\log(2\\pi\\sih_j^2 ) + \\log|\\rbt| + ( \\ybt - \\xbt(j)\\bbeh_j -\\zbt\\bbh_j)^\\tp \\rbt^{-1}(\\ybt - \\xbt(j))\\bbeh_j -\\zbt\\bbh_j ) / \\sih_j^2 \\right],\\end{aligned}\\ ] ] where @xmath87 and @xmath88 denote expectation with respect to the joint distribution of @xmath89 and the conditional distribution of @xmath7 given @xmath59 , namely @xmath90 , respectively . taking expectation with respect to @xmath90 and @xmath91 for @xmath92",
    ", we obtain @xmath93,\\ ] ] where @xmath94      most of the akaike - type information criteria are derived under the assumption that the candidate model includes the true model , namely , the overspecified assumption .",
    "although the assumption seems to be too strong , the influence is restrictive in practice .",
    "this is because the likelihood part of the criterion is a naive estimator of the risk function , namely , the cai in the context of the caic .    under the covariate shift situation ,",
    "however , we can not construct the likelihood part as a good estimator of the cai .",
    "that is , the drawback of overspecified assumption is more serious in the situation of covariate shift than the usual one . in section [ subsec : simu_bias ] , we show that an unbiased estimator of the cai under the overspecified assumption @xmath95 in ( [ eqn : cscaic ] ) has large bias for estimating the cai of the underspecified models .",
    "thus , we evaluate and estimate the cai directly both for the overspecified models and underspecified models in the following subsection , which is essential work in selecting variables in covariate shift .",
    "we evaluate the cai in ( [ eqn : cscai ] ) both for the overspecified model and for the underspecified model .",
    "we assume that the full model @xmath23 is overspecified , that is , the collection of the overspecified models @xmath96 is not an empty set . in addition , we assume that the size of the response variable in the predictive model @xmath97 is of order @xmath98 .",
    "when the candidate model @xmath17 is overspecified , @xmath99 follows the chi - squared distribution .",
    "then , we can evaluate the expectation in ( [ eqn : cscai ] ) exactly .",
    "however , for the underspecified model , this is not true . in this case , we asymptotically approximate the cai as the following theorem .",
    "[ thm : caim ] for the overspecified case , it follows that @xmath100 + \\log |\\rbt| + r^*$ ] , where @xmath101 for @xmath102 $ ] and @xmath103 . for the underspecified case ,",
    "@xmath104 is approximated as @xmath105 + \\log|\\rbt| + r^ * + r_1 + r_2 + r_3 + r_4 + o(n^{-1}),\\ ] ] where @xmath106 and @xmath107 for @xmath108 , @xmath109 and @xmath110 when the candidate model @xmath17 is overspecified , it follows that @xmath111 , @xmath112 , @xmath113 , and @xmath114 are exactly @xmath56 .",
    "because the approximation of cai in ( [ eqn : caiapp ] ) includes unknown parameters , we have to provide an estimator of cai for practical use .",
    "first , we obtain estimators of @xmath111 and @xmath112 , which are polynomials of @xmath115 .",
    "we define @xmath116 , @xmath117 , and @xmath118 as @xmath119 and @xmath120 when @xmath121 , it follows that @xmath122 where @xmath123 denotes the beta distribution .",
    "this implies that @xmath124 for the overspecified case . for the underspecified case ,",
    "on the other hand , it follows that @xmath125 = \\la^k + o(n^{-1})$ ] as @xmath126 for @xmath127 .",
    "then , we obtain an estimator of @xmath112 in the approximation of cai given by ( [ eqn : caiapp ] ) , which is given as follows : @xmath128    because @xmath111 is of order @xmath98 , we have to estimate @xmath115 with higher - order accuracy in order to obtain an estimator of @xmath111 whose bias is of order @xmath129 for the underspecified case . to this end",
    ", we expand @xmath130 up to @xmath129 as @xmath131 then , we obtain an estimator of @xmath111 given as @xmath132    we now have the following lemma , which can be proved using appendix c and appendix d of @xcite .    [ lem : rh1 ] when the candidate model @xmath17 is underspecified , @xmath133 in ( [ eqn : rh1 ] ) and @xmath134 in ( [ eqn : rh2 ] ) are asymptotically unbiased estimators of @xmath111 and @xmath112 , respectively , whose bias is of order @xmath129 , that is , @xmath135 when the candidate model @xmath17 is overspecified , it follows that @xmath136 .",
    "we next consider estimation procedures of @xmath113 and @xmath114 , which are complex functions of unknown parameters .",
    "we observe @xmath113 and @xmath114 as functions of @xmath137 , that is , @xmath138 , @xmath139 and substitute their unbiased estimators @xmath140 , which are given by @xmath141 then , the plug - in estimators of @xmath113 and @xmath114 are @xmath142 where @xmath143 for @xmath144 because @xmath113 is of order @xmath98 , the plug - in estimator @xmath145 has second - order bias .",
    "then , we correct the bias by analytical method based on taylor series expansions .",
    "we observe that expectation of the plug - in estimator @xmath146 is expanded as @xmath147 = r_3(\\bta _ * ) + b_1(\\bta _ * ) + b_2(\\bta _ * ) + o(n^{-2}),\\ ] ] where @xmath148 is second - order bias and @xmath149 is third - order bias of @xmath146 , that is , @xmath150 and @xmath151 , respectively .",
    "because @xmath152 and @xmath153 are independent , it follows that @xmath154 \\right ] + { 1 \\over 2 } { \\partial^2r_3(\\bta _ * ) \\over ( \\partial \\si_*^2)^2 } e[(\\sit^2 - \\si_*^2)^2],\\ ] ] where @xmath155 = \\si_*^2(\\x(\\om)^\\tp\\bsi^{-1}\\x(\\om))^{-1}$ ] and @xmath156 = 2(\\si_*^2)^2 / ( n - p_\\om)$ ] .",
    "second - order partial derivatives of @xmath113 are given by the following lemma .",
    "[ lem : der ] the second - order partial derivative of @xmath157 with respect to @xmath53 is @xmath158 where @xmath159 .",
    "the second - order partial derivative of @xmath157 with respect to @xmath160 is @xmath161    when the candidate model @xmath17 is overspecified , second - order bias @xmath148 can be simplified to @xmath162,\\ ] ] because @xmath163 and @xmath164 , which implies that @xmath165 and that @xmath166 .",
    "however , we can not know which candidate models are overspecified .",
    "then , we propose the following bias - corrected estimator of @xmath113 : @xmath167    [ lem : r34 t ] both for the cases in which the candidate model @xmath17 is overspecified and @xmath17 is underspecified , @xmath168 and @xmath169 in ( [ eqn : r3tt ] ) and ( [ eqn : r4 t ] ) are asymptotically unbiased estimators of @xmath113 and @xmath114 , whose bias is of order @xmath129 , that is , @xmath170    using @xmath133 , @xmath134 , @xmath168 , and @xmath169 given by ( [ eqn : rh1 ] ) , ( [ eqn : rh2 ] ) , ( [ eqn : r3tt ] ) , and ( [ eqn : r4 t ] ) , respectively , we construct an estimator of cai as follows : @xmath171 then , we obtain the following theorem , which is shown by theorem [ thm : caim ] and lemmas [ lem : rh1][lem : r34 t ] .",
    "[ thm : caih ] both for the cases in which the candidate model @xmath17 is overspecified and @xmath17 is underspecified , @xmath172 in ( [ eqn : caih ] ) is a second - order asymptotically unbiased estimator of @xmath104 , that is , @xmath173    when the sample size @xmath174 is small , second - order accuracy seems not to be sufficient for the overspecified model .",
    "actually , as the result in the simulation study shows , the estimate of the cai of the true model has relatively large bias , although the estimation of the true model is important .",
    "moreover , some of the other information criteria , which include the caic of @xcite , are exact unbiased estimators of the information of the overspecified candidate model .",
    "thus , we should improve the estimators of @xmath113 and @xmath114 to remove the bias that is of order @xmath129 . to this end",
    ", we adopt a parametric bootstrap method .",
    "bootstrap sample @xmath175 is generated by @xmath176 where @xmath177 and @xmath178 are generated by the following distribution : @xmath179 then , we use the following estimator of @xmath114 : @xmath180\\ ] ] where @xmath181 denotes expectation with respect to the bootstrap distribution and @xmath182 is @xmath183 for @xmath184 as for @xmath113 , it follows from ( [ eqn : r3bias ] ) that @xmath185 = r_3(\\btat ) + b_1(\\btat ) + b_2(\\btat ) + o_p(n^{-2}).\\ ] ] however , @xmath186 has a bias with order @xmath129 , that is ,",
    "@xmath187 = b_1(\\bta _ * ) + b_{11}(\\bta _ * ) + o(n^{-2}),\\ ] ] where @xmath188 .",
    "because this bias is not negligible when we want to estimate @xmath113 with third - order accuracy , we estimate the bias by bootstrap method as follows : @xmath189 - b_1(\\btat).\\ ] ] then , we obtain an estimator of @xmath113 , which is given as @xmath190 + \\widehat{b_{11 } } \\non\\\\ = & \\ 2r_3(\\btat ) - e_\\btat[r_3(\\btat^\\dag ) ] + e_\\btat[b_1(\\btat^\\dag ) ] - b_1(\\btat ) .",
    "\\label{eqn : r3h}\\end{aligned}\\ ] ]    [ lem : r34h ] both for the cases in which the candidate model @xmath17 is overspecified and @xmath17 is underspecified , @xmath191 and @xmath192 in ( [ eqn : r3h ] ) and ( [ eqn : r4h ] ) are asymptotically unbiased estimators of @xmath113 and @xmath114 , whose bias is of order @xmath193 , that is , @xmath194    using @xmath133 , @xmath134 , @xmath191 , and @xmath192 given by ( [ eqn : rh1 ] ) , ( [ eqn : rh2 ] ) , ( [ eqn : r3h ] ) , and ( [ eqn : r4h ] ) , we obtain an estimator of cai as follows : @xmath195 which improves @xmath172 in unbiasedness .",
    "then , we obtain the following theorem , which is proved by theorem [ thm : caih ] and lemma [ lem : r34h ] .",
    "[ thm : caihs ] when the candidate model @xmath17 is overspecified , @xmath196 in ( [ eqn : caihs ] ) is a third - order asymptotically unbiased estimator of @xmath104 , that is , @xmath197 when the candidate model @xmath17 is underspecified , @xmath196 is a second - order asymptotically unbiased estimator of @xmath104 , that is , @xmath198",
    "a typical example of the covariate shift situation appears in the small area prediction problem .",
    "the model for small area prediction supposes that the observed small area data have a finite population , which has the super - population model with random effects , one of which is the well - known nerm proposed by @xcite .",
    "let @xmath199 and @xmath200 denote the value of a characteristic of interest and its @xmath19-dimensional auxiliary variable for the @xmath201th unit of the @xmath202th area for @xmath203 and @xmath204 .",
    "note that @xmath200 is a subvector of @xmath205 , which is the vector of the explanatory variables in the full model @xmath23 , and we hereafter abbreviate the model index @xmath17 and write @xmath206 instead of @xmath200 , @xmath207 instead of @xmath19 , etc .",
    "then , the nerm is @xmath208 where @xmath209 is a @xmath210 vector of regression coefficients , @xmath211 is a random effect for the @xmath202th area , and @xmath211s and @xmath212s are mutually independently distributed as @xmath213 and @xmath214 , respectively .",
    "we consider the situation in which only @xmath215 values of the @xmath199s are observed through some sampling procedure .",
    "we define the number of unobserved variables in the @xmath202th area by @xmath216 and let @xmath217 .",
    "suppose , without loss of generality , the first @xmath215 elements of @xmath218 are observed , which are denoted by @xmath219 , and @xmath220 are unobserved .",
    "then , the observed model is defined as @xmath221 which corresponds to ( [ eqn : omodel ] ) with @xmath222 for @xmath223 , @xmath224 for @xmath225 , @xmath226 for @xmath227 , @xmath228 and @xmath229 , where @xmath230 denotes an @xmath231 vector of 1s and @xmath232 . in the derivation of our proposed criteria , we assume that the covariance matrix of @xmath2 is @xmath233 for a known matrix @xmath45",
    ". however , in the nerm , @xmath45 includes the parameter @xmath234 , which is usually unknown and has to be estimated . in this case",
    ", we propose that @xmath45 in the bias correction should be replaced with its plug - in estimator @xmath235 . the influence caused by the replacement",
    "may be limited because @xmath234 is the nuisance parameter when we are interested in selecting only explanatory variables .",
    "@xcite discussed the problem in their remark 3.1 .",
    "we consider two types of predictive models .",
    "the first can be used in the situation in which all @xmath206s are available .",
    "then , the predictive model , which we call the `` unit - level predictive model , '' is defined by @xmath236 which corresponds to ( [ eqn : pmodel ] ) with @xmath237 for @xmath238 , @xmath239 for @xmath240 , @xmath241 for @xmath242 , @xmath243 .",
    "note that @xmath244 .    in the problem of small area prediction",
    ", we often encounter the situation in which all @xmath206s are not observed but the area mean @xmath245 is known and we are interested in predicting @xmath246 , which is the mean of finite population @xmath218 , by using the value of @xmath247 .",
    "then , the second type of predictive model , which we call the `` area - level predictive model , '' is defined as @xmath248 where @xmath249 , the mean of unobserved variables , @xmath250 , calculated from @xmath251 and @xmath252 , and @xmath253 is distributed as @xmath254 .",
    "the model ( [ eqn : p2nerm ] ) corresponds to ( [ eqn : pmodel ] ) with @xmath255 , @xmath256 , @xmath257 and @xmath258 for @xmath259 .",
    "note that @xmath260 .",
    "after selecting explanatory variables with our proposed criteria , we predict @xmath261 by the empirical best linear unbiased predictor @xmath262 and obtain a predictor of the finite population mean @xmath246 , which is given as @xmath263 thus , covariate shift appears in standard models for small area prediction and the proposed criterion is important and useful in such a situation .",
    "in this subsection , we compare the performance of the criteria by measuring the bias of estimating the cai .",
    "we consider a class of the nested candidate models @xmath264 for @xmath265 where @xmath266 .",
    "the true observed model is the nerm in ( [ eqn : onerm ] ) with @xmath267 and @xmath268 for @xmath203 .",
    "we consider the unit - level predictive model ( [ eqn : p1nerm ] ) for the first experiment and the area - level predictive model ( [ eqn : p2nerm ] ) for the second experiment .",
    "the explanatory variables in the full model @xmath205 s ( @xmath269 ) are independently generated by @xmath270 , where @xmath271 for @xmath272 .",
    "the true coefficient vector @xmath53 is @xmath273 for @xmath274 and @xmath275s ( @xmath276 ) are generated by @xmath277 for a uniform random variable @xmath278 on the interval @xmath279 .",
    "the values of the explanatory variables @xmath206s and the vector of regression coefficients @xmath53 are fixed through simulations .    for comparison ,",
    "we consider the exact unbiased estimator , which is derived under the assumption that the candidate model is overspecified , given by @xmath280 where the bias correction term @xmath281 is @xmath282 + \\tr[\\rbt^{-1}\\a(\\x^\\tp\\bsi^{-1}\\x)^{-1}\\a^\\tp ] \\right\\ } \\\\ & + { n \\over n - p_j } \\left\\ { -\\tr[\\r\\bsi^{-1 } ] + \\tr[\\r\\p_j ] \\right\\}.\\end{aligned}\\ ] ]    rrrrrr & & & + & & & & & +   + @xmath283 & 206.38 & & -33.439 & -0.33371 & -0.080972 + @xmath284 & 152.81 & & -18.840 & -0.2414 & -0.16414 + @xmath285 & 140.79 & & -18.556 & -0.23026 & -0.46789 + @xmath286 & 132.61 & & -11.451 & 0.26445 & -0.19514 + @xmath287 & 116.51 & & -0.0019291 & 1.5979 & 0.49514 + @xmath288 & 122.46 & & -0.050686 & 0.77756 & 0.15429 + @xmath289 & 128.88 & & 0.0086256 & 0.09468 & 0.09468 +   + @xmath283 & 233.35 & & -0.22534 & 0.11255 & 0.1159 + @xmath284 & 189.54 & & 9.4310 & 0.24145 & 0.21667 + @xmath285 & 177.28 & & 14.197 & 0.42246 & 0.37347 + @xmath286 & 163.62 & & -0.76563 & 0.32597 & 0.02934 + @xmath287 & 152.94 & & 0.13627 & 0.8566 & 0.25115 + @xmath288 & 156.73 & & 0.068668 & 0.53817 & 0.15002 + @xmath289 & 161.65 & & 0.083897 & 0.015869 & 0.015869 +   + @xmath283 & 299.12 & & 4.3775 & 0.084838 & 0.084654 + @xmath284 & 252.60 & & 6.1677 & 0.24072 & 0.23581 + @xmath285 & 250.27 & & -5.1825 & -0.016634 & 0.010911 + @xmath286 & 208.25 & & 2.6115 & 0.36013 & 0.23929 + @xmath287 & 197.52 & & 0.25682 & 0.53321 & 0.26101 + @xmath288 & 200.38 & & 0.24977 & 0.40855 & 0.25647 + @xmath289 & 203.57 & & 0.22713 & 0.21272 & 0.21272 +    [ tab : biasu ]    rrrrrr & & & + & & & & & +   + @xmath283 & 61.095 & & -10.429 & -0.27157 & -0.21359 + @xmath284 & 46.635 & & 5.4222 & 0.40292 & -0.055653 + @xmath285 & 50.744 & & -10.285 & 0.36735 & -0.23857 + @xmath286 & 47.323 & & -0.77484 & 1.3412 & 0.36198 + @xmath287 & 45.735 & & -0.21108 & 2.2751 & 0.60073 + @xmath288 & 49.452 & & -0.32466 & 0.82334 & 0.017969 + @xmath289 & 52.805 & & -0.29278 & -0.082743 & -0.082743 +   + @xmath283 & 95.393 & & -5.4716 & 0.12331 & 0.13930 + @xmath284 & 70.056 & & 17.521 & 0.39905 & 0.36599 + @xmath285 & 66.412 & & 21.039 & 0.59611 & 0.53872 + @xmath286 & 61.310 & & 5.1740 & 0.58453 & 0.10853 + @xmath287 & 60.532 & & 0.23723 & 1.0853 & 0.25515 + @xmath288 & 63.109 & & 0.13276 & 0.50306 & 0.096935 + @xmath289 & 65.379 & & 0.16648 & -0.0017143 & -0.0017143 +   + @xmath283 & 98.841 & & 21.017 & 0.18161 & 0.17565 + @xmath284 & 94.83 & & 10.059 & 0.31607 & 0.30894 + @xmath285 & 88.346 & & 5.6464 & 0.13835 & 0.11302 + @xmath286 & 78.383 & & 7.8734 & 0.46942 & 0.27593 + @xmath287 & 77.794 & & 0.18930 & 0.54202 & 0.17009 + @xmath288 & 79.277 & & 0.18908 & 0.41365 & 0.18534 + @xmath289 & 81.216 & & 0.15395 & 0.11783 & 0.11783 +    [ tab : biasa ]    tables [ tab : biasu ] and [ tab : biasa ] report the true values of the cai and relative bias of estimating the cai by the criteria @xmath172 in ( [ eqn : caih ] ) , @xmath196 in ( [ eqn : caihs ] ) , and @xmath95 in ( [ eqn : cscaic ] ) , for the experiment using the unit - level predictive model and for that using the area - level predictive model , respectively .",
    "we handle the cases in which the number of the areas @xmath290 .",
    "the true values of cai in each candidate model are calculated based on ( [ eqn : cscai ] ) with 10,000 monte carlo iterations .",
    "the relative bias of estimating the cai by the criteria is defined as @xmath291 - \\cai \\over \\cai},\\ ] ] where @xmath292 , @xmath95 , and expectation is computed based on 1,000 replications .",
    "the bootstrap sample size is 1,000 for obtaining @xmath196 . from the tables ,",
    "we observe the following facts .",
    "first , @xmath95 has large bias for underspecified models , that is , @xmath293 , and @xmath286 , while the modified estimators of the cai , @xmath172 and @xmath196 , have smaller bias for both overspecified and underspecified models .",
    "second , @xmath196 can estimate the cai more unbiasedly than @xmath172 can for the case of small sample size because @xmath196 is a third - order asymptotically unbiased estimator of the cai . in particular ,",
    "the improvement is remarkable for the true model @xmath287 , which is important for variable selection .",
    "however , the relative bias of @xmath172 , which is the second - order asymptotically unbiased estimator of the cai , becomes smaller as the sample size becomes larger and the difference in performance between @xmath172 and @xmath196 is not very significant .      in this subsection , we investigate the numerical performance of the small area prediction problem explained in section [ sec : ex ] .",
    "we conduct design - based simulation based on a real dataset .",
    "we use the posted land price data along the keikyu train line , which connects the suburbs in kanagawa prefecture to the tokyo metropolitan area .",
    "this dataset was also used by @xcite , who studied modification of the caic .",
    "we analyze the land price data in 2001 with covariates for 47 stations that we consider as small areas , and let @xmath294 . in the original sample ,",
    "there are @xmath215 sampled land spots for the @xmath202 area , and the total sample size is @xmath295 . we generate a synthetic population of size @xmath296 by resampling with replacement from the original dataset using selection probabilities inversely proportional to sample weights .",
    "this method of making a synthetic population was also used by @xcite .",
    "then , we select 200 independent random samples , each of size @xmath297 , from the fixed synthetic population by sampling from each area based on simple random sampling without replacement and with sample size of each area equal to that of original dataset @xmath215 .",
    "the characteristic of interest is the land price ( yen in hundreds of thousands ) per @xmath298 of the @xmath201th spot in the @xmath202th area , denoted by @xmath299 , and the target is the mean of the land price in each area @xmath300 for @xmath301 , where @xmath302 is the size of the @xmath202th area ( subpopulation ) . as discussed in section",
    "[ sec : ex ] , we adopt model - based estimation of finite subpopulation mean @xmath303 by using nerm . for selecting the explanatory variables in nerm",
    ", we use our proposed criterion @xmath172 in ( [ eqn : caih ] ) for comparison with the conventional caic by @xcite .",
    "however , because the land price data are right - skewed , we undertake log - transformation , namely , @xmath304 , and fit @xmath199 with nerm in ( [ eqn : fnerm ] ) .",
    "the dataset includes the following auxiliary variables .",
    "@xmath305 denotes the floor - area ratio of spot @xmath201 in the @xmath202th area , @xmath306 is the time it takes by train from station @xmath202 to tokyo station around 9:00 am , @xmath307 is the geographical distance from spot @xmath201 to nearby station @xmath202 and @xmath308 denotes the time it takes on foot from spot @xmath201 to nearby station @xmath202 . as the candidate explanatory variables , we consider 7 variables @xmath305 , @xmath306 , @xmath309 , @xmath307 , @xmath310 , @xmath308 , and @xmath311 , which are denoted by @xmath312 , and @xmath313 denotes a constant term . using the criteria , we select the best combination of these variables .    based on the best model selected by the criteria ,",
    "we obtain a predictor @xmath314 of the finite subpopulation mean of the land price in the @xmath202th area .",
    "however , log - transformed variable @xmath199 is used in the nerm , and thus , we have to modify the predictor ( [ eqn : fp ] ) .",
    "the best predictor of out - of - sample @xmath315 is the conditional expectation given the data @xmath1 , namely , @xmath316 = e [ \\exp(y_{ik } ) \\mid \\y ] $ ] , where @xmath317 .",
    "because the conditional mean and variance of @xmath199 given @xmath1 , denoted by @xmath318 and @xmath319 , are @xmath320 it follows that @xmath321 substituting @xmath322 with @xmath323 , we obtain the empirical best predictor ( ebp ) : @xmath324 where @xmath322 is some estimator of @xmath323",
    ". then , we use the following predictor of @xmath303 : @xmath325 as @xmath322 , we use unbiased estimators of @xmath326 and @xmath327 proposed by @xcite and the gls estimator of @xmath209 .",
    "we measure the performance of this design - based simulation by mean squared error ( mse ) of the predictor , @xmath328 where @xmath329 and @xmath330 are @xmath202th finite subpopulation mean and its predictor of the @xmath331th sampled data .",
    "we construct @xmath172s using the unit - level predictive model ( [ eqn : p1nerm ] ) and the area - level predictive model ( [ eqn : p2nerm ] ) , and let @xmath332 and @xmath333 denote the corresponding mses . to compare the performance ,",
    "we compute the ratio of mses as follows : @xmath334 where @xmath335 is the mse of the predictor based on the caic of @xcite .",
    "figure [ fig : mse ] shows the results . although the performance of @xmath172 based on the unit - level predictive model is similar to the caic of @xcite , @xmath172 based on the area - level predictive model has much better performances in most areas .",
    "it is valuable to point out that the mse of the predictor of the finite subpopulation mean can be improved using our proposed criteria , which motivates us to use them for variable selection in the small area prediction problem .",
    "based on area - level predictive model ( solid line ) and to @xmath172 based on unit - level predictive model ( dashed line ) ]    * acknowledgments *    we are grateful to professor j.n.k .",
    "rao for his helpful comment and for holding a seminar at statistics canada during our stay in ottawa .",
    "this research was supported by grant - in - aid for scientific research from the japan society for the promotion of science , grant numbers 16k17101 , 16h07406 , 15h01943 and 26330036 .",
    "because @xmath81 and @xmath82 are mutually independent , the cai in ( [ eqn : cscai ] ) can be rewritten as @xmath336 + \\log|\\rbt| + n\\cdot e\\big [ ( n\\sih_j^2 / \\si_*^2)^{-1 } \\big ] \\big\\ { \\tr(\\rbt^{-1}\\bla ) + e[\\a^\\tp\\rbt^{-1}\\a / \\si_*^2 ] \\big\\},\\ ] ] for the overspecified case , we can easily evaluate the cai , noting that @xmath99 follows chi - squared distribution with @xmath337 degrees of freedom and is independent of @xmath81 .",
    "thus , it suffices to show that the cai is evaluated as ( [ eqn : caiapp ] ) for the underspecified case .    from ( b.4 ) in @xcite , we can evaluate @xmath338 $ ] as follows : @xmath339 = { \\la \\over n } \\left\\ { 1 + { -2\\la^2 + ( p_j+4)\\la \\over n } \\right\\ } + o(n^{-3}).\\ ] ] we next evaluate @xmath340 $ ] .",
    "let @xmath341 .",
    "then , we can rewrite @xmath342 in @xmath343 as @xmath344 next , we can rewrite @xmath345 in @xmath343 as @xmath346 then , we obtain @xmath347 moreover , it follows that @xmath348 thus , @xmath340 $ ] can be evaluated as @xmath349 = \\tr[\\rbt^{-1}\\a(\\x(j)^\\tp\\bsi^{-1}\\x(j))^{-1}\\a^\\tp ] + \\bbe_*^\\tp\\b^\\tp\\rbt^{-1}\\b\\bbe _ * / \\si_*^2.\\ ] ] it follows from ( [ eqn : se ] ) and ( [ eqn : ara ] ) that @xmath350\\ { \\tr(\\rbt^{-1}\\bla ) + e[\\a^\\tp\\rbt^{-1}\\a / \\si_*^2 ] \\ } \\\\ = & \\ ( \\ga + \\bbe_*^\\tp\\b^\\tp\\rbt^{-1}\\b\\bbe _ * / \\si_*^2 ) \\times \\left\\ { \\la + { -2\\la^3 + ( p_j+4)\\la^2 \\over n } \\right\\ } + o(n^{-1}),\\end{aligned}\\ ] ] which shows that the cai is approximated to ( [ eqn : caiapp ] ) .",
    "@xmath351          first , note that @xmath356 $ ] is expanded as @xmath357 = r_3(\\bta _ * ) + b_1(\\bta _ * ) + o(n^{-1}),\\ ] ] where @xmath148 is given as ( [ eqn : b1 ] ) . because @xmath150 , it follows that @xmath358 , which shows that @xmath359 = r_3 + o(n^{-1}).\\ ] ] in the same way , we obtain @xmath360 = e[r_4(\\btat ) ] = r_4(\\bta _ * ) + o(n^{-1})$ ] .",
    "@xmath351      it follows from ( [ eqn : r3h ] ) that @xmath361 = e \\left [ 2r_3(\\btat ) - e_\\btat[r_3(\\btat^\\dag ) ] + e_\\btat[b_1(\\btat^\\dag ) ] - b_1(\\btat ) \\right].\\ ] ] because @xmath356 $ ] is expanded as @xmath356 = r_3(\\bta _ * ) + b_1(\\bta _ * ) + b_2(\\bta _ * ) + o(n^{-2})$ ] , we observe that @xmath362 \\right ] = & \\ 2\\left\\ { r_3(\\bta _ * ) + b_1(\\bta _ * ) + b_2(\\bta _ * ) \\right\\ } - e\\left [ r_3(\\btat ) + b_1(\\btat ) + b_2(\\btat ) \\right ] + o(n^{-2 } ) \\\\ = & \\ r_3(\\bta _ * ) + b_1(\\bta _ * ) + b_2(\\bta _ * ) - e\\left [ b_1(\\btat ) + b_2(\\btat ) \\right ] + o(n^{-2}).\\end{aligned}\\ ] ] moreover , because @xmath363 = b_1(\\bta _ * ) + b_{11}(\\bta _ * ) + o(n^{-2})$ ] and @xmath364 = b_2(\\bta _ * ) + o(n^{-2})$ ] , the equation above can be rewritten as @xmath365 \\right ] = r_3(\\bta _ * ) - b_{11}(\\bta _ * ) + o(n^{-2}).\\ ] ] next , it is observed that @xmath366 - b_1(\\btat ) \\right ] = & \\ e [ b_1(\\btat ) + b_{11}(\\btat ) ] - \\ { b_1(\\bta _ * ) + b_{11}(\\bta _ * ) \\ } + o(n^{-2 } ) \\non\\\\ = & \\ b_{11}(\\bta _ * ) + o(n^{-2 } ) .",
    "\\label{eqn : bias2}\\end{aligned}\\ ] ] thus , it follows from ( [ eqn : bias1 ] ) and ( [ eqn : bias2 ] ) that @xmath361 = r_3(\\bta _ * ) + o(n^{-2}).\\ ] ] similarly , we show that @xmath367 = r_4(\\bta _ * ) + o(n^{-2})$ ] .",
    "@xmath351      akaike , h. ( 1973 ) .",
    "information theory and an extension of the maximum likelihood principle . in _",
    "2nd international symposium on information theory _ , ( petrov , b.n . and csaki . , f. , eds . ) , 267281 , akademia kiado , budapest .      battese , g.e . , harter , r.m . , and fuller , w.a .",
    "( 1988 ) . an error - components model for prediction of county crop areas using survey and satellite data . _ journal of the american statistical association _ , * 83 * , 2836 .                                saefken , b. , kneib , t. , van waveren , c.s . , and greven , s. ( 2014 ) . a unifying approach to the estimation of the conditional akaike information in generalized linear mixed models .",
    "_ electronic journal of statistics _ ,",
    "* 8 * , 201225 .",
    "yu , d. , zhang , x. , and yau , k.k.w .",
    "information based model selection criteria for generalized linear mixed models with unknown variance component parameters . _ journal of multivariate analysis _ , * 116 * , 245262 ."
  ],
  "abstract_text": [
    "<S> in this study , we consider the problem of selecting explanatory variables of fixed effects in linear mixed models under covariate shift , which is when the values of covariates in the predictive model differ from those in the observed model . </S>",
    "<S> we construct a variable selection criterion based on the conditional akaike information introduced by @xcite . </S>",
    "<S> we focus especially on covariate shift in small area prediction and demonstrate the usefulness of the proposed criterion . </S>",
    "<S> in addition , numerical performance is investigated through simulations , one of which is a design - based simulation using a real dataset of land prices .    </S>",
    "<S> : akaike information criterion ; conditional aic ; covariate shift ; linear mixed model ; small area estimation ; variable selection . </S>"
  ]
}