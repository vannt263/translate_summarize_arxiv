{
  "article_text": [
    "there has been considerable amount of work on learning when labeling is expensive , such as techniques on transductive inference and active learning . with the emergence of crowdsourcing services , like amazon mechanical turk , labeling costs in many applications",
    "have dropped dramatically .",
    "large amounts of labeled data can now be gathered at low price . due to a lack of domain expertise and misaligned incentives ,",
    "however , labels provided by crowdsourcing workers are often noisy . to overcome the quality issue ,",
    "each item is usually simultaneously labeled by several workers , and then we aggregate the multiple labels with some manner , for instance , majority voting .",
    "an advanced approach for label aggregation is suggested by dawid and skene[1 ] .",
    "they assume that each worker has a latent confusion matrix for labeling .",
    "the off - diagonal elements represent the probabilities that a worker mislabels an arbitrary item from one class to another while the diagonal elements correspond to her accuracy in each class .",
    "worker confusion matrices and true labels are jointly estimated by maximizing the likelihood of observed labels .",
    "one may further assume a prior distribution over worker confusion matrices and perform bayesian inference [ 2][3][4 ] .",
    "the method of dawid - skene ( 1979 ) implicitly assumes that a worker performs equally well across all items in a common class . in practice , however , it is often the case that one item is more difficult to label than another . to address this heterogeneous issue ,",
    "zhou et al.(2012)[5 ] propose a minimax entropy principle for crowdsourcing .",
    "it results in that each item is associated with a latent confusion vector besides a latent confusion matrix for each worker .",
    "observed labels are determined jointly by worker confusion matrices and item confusion vectors through an exponential family model .",
    "moreover , it turns out that the probabilistic labeling model can be equivalently derived from a natural assumption of objective measurements of worker ability and item difficulty",
    ". such kinds of objectivity arguments have been widely discussed in the literature of mental test theory [ 6][7 ] .",
    "all the above approaches are for aggregating multiclass labels and in many scenarios , the labels are ordinal .",
    "( 2014)[8 ] proposed a work to aggregate votes using minimax conditional entropy for ordinal labels .",
    "most of the methods use statistical methods to aggregate the observed labels by transforming them to some probability or entropy measures .",
    "but , there has been no work that operates directly on the observed labels .",
    "we present a method to learn the aggregates of the votes using clustering .",
    "we first show the formulation that allows us to use clustering as an approximation of the vote aggregation stratagem .",
    "we first draw a parallel between the restricted boltzmann machine ( rbm ) learning and the expectation maximization ( em ) algorithm of the david - skene algorithm and then show that gaussian - softmax rbms[9 ] can be approximated by a gaussian mixture model ( gmm ) , whose specific conditions lead to a direct mapping to the traditional k - means algorithm[10][11 ] .",
    "to then elucidate the clustering paradigm , we perform clustering using the @xmath0-rbm model as proposed in [ 14 ] .",
    "the restricted boltzmann machine is a bipartite , undirected graphical model with visible ( observed ) units and hidden ( latent ) units .",
    "the rbm can be understood as an mrf with latent factors that explains the input visible data using binary latent variables .",
    "the rbm consists of visible data @xmath1 of dimension @xmath2 that can take real values or binary values , and stochastic binary variables @xmath3 of dimension @xmath0 .",
    "the parameters of the model are the weight matrix @xmath4 that defines a potential between visible input variables and stochastic binary variables , the biases @xmath5 for visible units , and the biases @xmath6 for hidden units . when the visible units are real - valued , the model is called the gaussian rbm , and its joint probability distribution can be defined as follows :    @xmath7    @xmath8    where z is a normalization constant . the conditional distribution of this model can be written as :    @xmath9    @xmath10    where @xmath11 is the sigmoid function , and @xmath12 is a gaussian distribution . here , the variables in a layer ( given the other layers ) are conditionally independent , and thus we can perform block gibbs sampling in parallel .",
    "the rbm can be trained using sampling - based approximate maximum - likelihood , e.g. , contrastive divergence approximation [ 12 ] .",
    "after training the rbm , the posterior ( equation 2 ) of the hidden units ( given input data ) can be used as feature representations for classification tasks .",
    "we define the gaussian - softmax rbm as the gaussian rbm with a constraint that at most one hidden unit can be activated at a time given the input , i.e. , @xmath13 .",
    "the energy function of the gaussian - softmax rbm can be written in a vectorized form as follows :    @xmath8    subject to @xmath13 .",
    "for this model , the conditional probabilities of visible or hidden units given the other layer can be computed as :    @xmath14    @xmath15    where @xmath16 is the @xmath17-th column of the @xmath18 matrix , often denoted as a `` basis '' vector for the @xmath17-th hidden unit . in this model , there are @xmath19 possible configurations ( i.e. , all hidden units are 0 , or only one hidden unit @xmath20 is 1 for some @xmath17 ) .",
    "the k - means clustering is an unsupervised algorithm that assigns clusters to data points .",
    "the algorithm can be written as    * randomly choose k cluster centers , @xmath21 .",
    "* assign an incoming data point @xmath22 to the nearest cluster center @xmath23 * @xmath24 becomes centroid of the cluster .",
    "@xmath25    the procedure is repeated till convergence , that is all points have been assigned the best cluster centers and over many trials @xmath26 , we take the best possible solution as the cluster assignment .",
    "the gaussian mixture model is a directed graphical model where the likelihood of visible units is expressed as a convex combination of gaussians .",
    "the likelihood of a gmm with @xmath27 gaussians can be written as follows :    @xmath28    for the rest of the paper , we denote the gmm with shared spherical covariance as gmm(@xmath29 ) , when @xmath30 for all @xmath31 . for the gmm with arbitrary positive definite covariance matrices",
    ", we will use the shorthand notation gmm(@xmath32 ) .",
    "k - means can be understood as a special case of gmm with spherical covariance by letting @xmath33 [ 13 ] .",
    "compared to gmm , the training of k - means is highly efficient ; therefore it is plausible to train k - means to provide an initialization of a gmm . then the gmm is trained with em algorithm .",
    "the em algorithm learns the parameters to maximize the likelihood of the data as described by equation [ gmm ] .",
    "the following section outlines the proof for vote aggregation as a special case of clustering problems , when trying to model the problem using rbms .",
    "rbms are learned in a manner so as to minimize the negative log - likelihood of the data . in a vote aggregation setup ,",
    "the data is the observed labels .",
    "thus , we can see that learning from rbms are similar to aggregating votes from the dawid - skene algorithm which also minimizes the negative log - likelihood of the observed labels .",
    "but in the training of rbms , we often encounter the normalization constant @xmath34 , which is intractable and this makes it difficult to train an rbm , and we need to approximate @xmath34 to learn the ideal parameters for the same . hence , it becomes difficult to directly apply rbms to aggregate votes .      in this section , we show that a gaussian rbm with softmax hidden units can be converted into a gaussian mixture model , and vice versa .",
    "this connection between mixture models and rbms with a softmax constraint completes the chain of links between k - means , gmms and gaussian - softmax rbms and helps us to visualize vote aggregation as a clustering problem .",
    "as equation [ gsrbm ] shows , the conditional probability of visible units given the hidden unit activations for gaussian - softmax rbm follows a gaussian distribution . from this perspective",
    ", the gaussian - softmax rbm can be viewed as a mixture of gaussians whose mean components correspond to possible hidden unit configurations . in this section ,",
    "we show an explicit equivalence between these two models by formulating the conversion equations between gmm(@xmath35 ) with @xmath27 gaussian components and the gaussian - softmax rbm with @xmath0 hidden units .",
    "* proposition * the mixture of @xmath36 gaussians with shared spherical covariance of @xmath37 is equivalent to the gaussian - softmax rbm with k hidden units .",
    "we prove the following conversions by showing the following conversions .",
    "* 1 . from gaussian - softmax rbm to gmm(@xmath29 ) : *    we begin by decomposing the chain rule : @xmath38    where    @xmath39    since there are only a finite number of hidden unit configurations , we can explicitly enumerate the prior probabilities :    @xmath40    if we define @xmath41 , then we have @xmath42 .",
    "in fact , @xmath43 can be calculated analytically ,    @xmath44    using this definition , we can show the equivalence as , @xmath45    * 2 . from gmm(@xmath35 ) to gaussian - softmax rbm : *    we will also show this by construction .",
    "suppose we have the following gaussian mixture with @xmath27 components and the shared spherical covariance @xmath37 : @xmath46    this gmm can be converted to a gaussian - softmax rbm with the following transformations : @xmath47 @xmath48 @xmath49    it is easy to see that the conditional distribution @xmath50 can be formulated as a gaussian distribution with mean @xmath51 , which is identical to that of the gaussian - softmax rbm .",
    "further , we can recover the posterior probabilities of the hidden units given visible units as the follows :    @xmath52    therefore , a gaussian mixture can be converted to an equivalent gaussian rbm with a softmax constraint .",
    "gmms learn a density function over the data , while trying to maximize its likelihood . from maximum likelihood estimation , the equation",
    "a gmm tries to learn is @xmath53 .",
    "but since we do not know @xmath54 s , we maximize the marginal likelihood , which is given by @xmath55 , where @xmath56 is the number of clusters .    from the gaussian bayes classifier , @xmath57 , that is , @xmath58",
    "p(y = i)\\ ] ]    when @xmath59 is spherical , with same @xmath60 for all classes , @xmath61 $ ] .",
    "if each @xmath22 belongs to one class @xmath62 , marginal likelihood is given by :    @xmath63\\ ] ]    for estimating the parameters , we maximize the log - likelihood with respect to all clusters and this gives , @xmath64\\ ] ]    equivalently , minimizing the negative log - likelihood gives , @xmath65\\ ] ]    which is the same as the k - means function . we thus show that the vote aggregation methodolody when applied from an rbm model perspective can be viewed as a clustering problem , one of k - means specifically .",
    "thus , we can consider vote aggregation learned by maximizing the likelihood of observed labels to be a clustering problem .",
    "our framework uses @xmath0 component rbms .",
    "each component rbm learns one non - linear subspace .",
    "the visible units @xmath66 , @xmath67 correspond to an @xmath68 dimensional visible ( input ) space and the hidden units @xmath69 correspond to a learned non - linear @xmath70-dimensional subspace .",
    "the @xmath0-rbm model has @xmath0 component rbms .",
    "each of these maps a set of sample points @xmath71 to a projection in @xmath72 .",
    "each component rbm has a set of symmetric weights ( and asymmetric biases ) @xmath73 that learns a non - linear subspace .",
    "note that these weights include the forward and backward bias terms .",
    "the error of reconstruction for a sample @xmath74 given by the @xmath56th rbm is simply the squared euclidean distance between the data point @xmath74 and its reconstruction by the @xmath56th rbm , computed using .",
    "we denote this error by @xmath75 . the total reconstruction error @xmath76 in any iteration @xmath77 is given by @xmath78 .",
    "the @xmath0 rbms are trained simultaneously . during the rbm training ,",
    "we associate data points with rbms based on how well each component rbm is able to reconstruct the data points .",
    "a component rbm is trained only on the training data points associated with it .",
    "the component rbms are given random initial weights @xmath79 .",
    "as in traditional k - means clustering , the algorithm alternates between two steps : ( 1 ) computing association of a data point with a cluster and ( 2 ) updating the cluster parameters . in @xmath0-rbms , the @xmath80th data point",
    "is associated with the @xmath56th rbm ( cluster ) if its reconstruction error from that rbm is lowest compared to other rbms , i.e. if @xmath81 .",
    "once all the points are associated with one of the rbms the weights of the rbms are learnt in a batch update . in hard clustering",
    "the data points are partitioned into the clusters exhaustively ( i.e. each data point must be associated with some cluster ) and disjointly ( i.e. each data point is associated with only one cluster ) .",
    "in contrast with k - means where the update of the cluster center is a closed form solution given the data association with clusters , in @xmath0-rbms the weights are learned iteratively .",
    "in this section , we report empirical results of our method on real crowdsourced data .",
    "we consider the @xmath82 error metric .",
    "let us denote by @xmath83 the true rating and @xmath84 the estimate .",
    "the error metrics is defined as : ( 1 ) @xmath82 = @xmath85 .",
    "all the research ( code and datasets ) is reproducible and is available at : https://github.com/gupta-abhay/deep-voteaggregate .      *",
    "web search relevance rating * the web search relevance rating dataset contains 2665 query - url pairs and 177 workers[5 ] .",
    "give a query - url pair , a worker is required to provide a rating to measure how the url is relevant to the query .",
    "the rating scale is 5-level : perfect , excellent , good , fair , or bad . on average ,",
    "each pair was labeled by 6 different workers , and each worker labeled 90 pairs .",
    "more than 10 workers labeled only one pair .",
    "* dog image labeling * we chose the images of 4 breeds of dogs from the stanford dogs dataset [ 8 ] : norfolk terrier ( 172 ) , norwich terrier ( 185 ) , irish wolfhound ( 218 ) , and scottish deerhound ( 232 ) .",
    "the numbers of the images for each breed are in the parentheses .",
    "there are 807 images in total .",
    "a worker labeled an image at most once , and each image was labeled 10 times .",
    "there are four architectures considered for both the datasets .",
    "we consider two rbms , binary - binary rbms and gaussian - binary rbms .",
    "the architectures are the following :      1 .",
    "binary - binary rbm with 30 visible units and 5 hidden units . 2 .",
    "binary - binary rbm with 18 visible units and 3 hidden units . 3 .",
    "gaussian - binary rbm with 6 visible units and 5 hidden units .",
    "gaussian - binary rbm with 6 visible units and 3 hidden units .      1 .",
    "binary - binary rbm with 40 visible units and 4 hidden units . 2 .",
    "binary - binary rbm with 20 visible units and 2 hidden units . 3 .",
    "gaussian - binary rbm with 10 visible units and 4 hidden units .",
    "gaussian - binary rbm with 10 visible units and 2 hidden units .",
    "we report the results , both @xmath82 and @xmath86 errors of the architectures considered in tables [ web - label ] and [ dog - label ] .",
    "the @xmath82 error of the dawid - skene model on the web search data is 0.17 and the error on the dog data is 0.21 .",
    ".error metrics for web - search data [ cols=\"^,^,^,^,^\",options=\"header \" , ]      all the results are done over and average of 20 runs .",
    "we see from the results that the results of one - hot encodings are the best among all the proposed architectures , for both the web and dog datasets",
    ". this can be because rbms capture binary data and thus it is able to capture the one - hot encodings in a good manner .",
    "also , we see that in the web data , when we use gaussian binary rbms , we get 100% error .",
    "this may be because gaussian sampling of the data is not ideal for this dataset .",
    "on trying cd - k above @xmath87 , we get huge reconstruction errors for every data point . however , between cd-1 and cd-2 , cd-2 outperforms cd-1 .",
    "also , pcd gives huge reconstruction errors for the web dataset , but gave results comparable to cd-1 for the dog dataset .",
    "we give a plot for the average reconstruction error per sample as the rbm proceeds for the web dataset in figure [ web - sample ] .",
    "[ 1 ] dawid , a. p. and skene , a. m. maximum likeihood estimation of observer error - rates using the em algorithm .",
    "journal of the royal statistical society , 28(1):2028 , 1979 .",
    "[ 6 ] rasch , g. on general laws and the meaning of measurement in psychology . in proceedings of the 4th berkeley symposium on mathematical statistics and probability , volume 4 , pp .",
    "321 - 333 , berkeley , ca , 1961 .",
    "[ 8 ] zhou , d. , liu , q. , platt , j. and meek , c. aggregating ordinal labels from crowds by minimax conditional entropy . in proceedings of the 31st international conference on machine learning pp .",
    "262 - 270 .",
    "[ 10 ] sohn , k. , jung , d.y . ,",
    "lee , h. and hero iii , a.o . , 2011 ,",
    "efficient learning of sparse , distributed , convolutional feature representations for object recognition . in computer vision ( iccv ) , 2011 ieee international conference on ( pp .",
    "2643 - 2650 ) . ieee .",
    "[ 14 ] chandra , s. , kumar , s. and jawahar , c.v . , 2013 .",
    "learning multiple non - linear sub - spaces using k - rbms . in proceedings of the ieee conference on computer vision and pattern recognition ( pp . 2778 - 2785 ) ."
  ],
  "abstract_text": [
    "<S> an important way to make large training sets is to gather noisy labels from crowds of non experts . </S>",
    "<S> we propose a method to aggregate noisy labels collected from a crowd of workers or annotators . </S>",
    "<S> eliciting labels is important in tasks such as judging web search quality and rating products . </S>",
    "<S> our method assumes that labels are generated by a probability distribution over items and labels . </S>",
    "<S> we formulate the method by drawing parallels between gaussian mixture models ( gmms ) and restricted boltzmann machines ( rbms ) and show that the problem of vote aggregation can be viewed as one of clustering . </S>",
    "<S> we use @xmath0-rbms to perform clustering . </S>",
    "<S> we finally show some empirical evaluations over real datasets . </S>"
  ]
}