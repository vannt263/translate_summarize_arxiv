{
  "article_text": [
    "in this paper we consider parameter estimation in a distribution - free version of the standard ( gaussian ) factor analysis ( fa ) model , with special emphasis on the case of more variables than observations .",
    "the fa model means describing a sample @xmath4 of @xmath3-dimensional vectors as @xmath5 here @xmath6 is the mean value vector , @xmath7 is a @xmath8 coefficients ( loadings ) matrix , @xmath9 , and the @xmath10s are mutually independent latent @xmath11-vectors ( factor scores ) , standardized to zero mean and unit covariance matrix @xmath12 ( for identifiability ) .",
    "the @xmath13s are assumed mutually independent @xmath3-vectors with uncorrelated components and diagonal covariance matrix @xmath14 .",
    "also , @xmath10 and @xmath13 should be mutually independent . in matrix form",
    "we write as @xmath15 , with the vectors of as rows .",
    "usually , normality of @xmath16 and @xmath17 in is assumed , and more observations than variables , that is @xmath18",
    ". then gaussian maximum likelihood methods can be used , and are more or less standard . however , in recent years interest has increased both in more robust methods and in methods for the case of more variables than observations , @xmath0 . among papers having appeared after the comprehensive review by bartholomew & knott ( 1999 , ch .",
    "3 ) , we mention robertson & symons ( 2007 ) , who study extension of gaussian maximum likelihood to the case @xmath0 , and a number of papers by trendafilov and unkel , in particular trendafilov & unkel ( 2011 ) and unkel & trendafilov ( 2010a&b ) , also dealing with the case @xmath0 but proposing alternative models and estimation methods .",
    "trendafilov & unkel ( 2011 ) appear skeptical to the results of robertson & symons ( 2007 ) , and proclaim that when @xmath0 the model assumption of the latter , that @xmath14 is positive definite , is inconsistent with their own model for data .",
    "that is certainly right , and we argue below ( sec .",
    "6 ) that the model for data used by trendafilov & unkel is artificial and unrealistic .",
    "our main aim , however , is to show that the fitting of models of type in the case of large @xmath3 is not problematic , and that in any case there is no need to assume normality .",
    "we will first derive some basic distribution - free properties of model .",
    "these are expressed in a normalization of the @xmath19-components by @xmath20 , shown to be suitable for our purpose .",
    "it will turn out without difficulties that these properties lead to estimating equations that are the same as the well - known likelihood equations for @xmath18 , thus yielding distribution - free support to the normality - based mle .",
    "another well - known technique for dimension reduction is principal components analysis ( pca ) .",
    "pca aims at describing as much as possible of @xmath21 by a number of principal components ( pcs , linear forms in @xmath19 ) .",
    "there is no model behind pca , but sometimes the pcs are regarded as representing latent variables in a different , less well - defined way .",
    "pca techniques also have a role in factor analysis . due to its scale - dependence",
    ", the choice of scaling is important .    in the very special case",
    "when the error @xmath17 vanishes , i.e.  @xmath22 in , @xmath23 can be determined by a pca on @xmath21 , or estimated by a pca on the sample covariance matrix @xmath24 ( or an svd on the @xmath19-data matrix itself ) .",
    "similarly , if @xmath14 were not zero but regarded as known , we could subtract it from @xmath21 or @xmath24 and in this way open for use of pca .",
    "this was the basis for the early principal factor analysis method of fitting the fa model : use some initial @xmath14 to subtract from @xmath24 , find pcs yielding an estimate of @xmath25 , use this to calculate a new @xmath14 , etc .",
    "such methods were found inefficient and unstable , however . in particular",
    "they were not scale invariant , in contrast to gaussian ml ( see bartholomew & knott , 1999 , sec .",
    "3.17 ) . from the time when ml methods became computationally feasible and attractive ( jreskog , 1967 , lawley , 1967 ) , ml",
    "estimation has widely replaced the principal factor analysis method .    in the present paper a new distribution - free method for fa model fitting is proposed , that utilizes principal components of a naturally _ rescaled _ instead of _ reduced _ sample covariance matrix . to our surprise",
    "we have not seen this approach in the literature .",
    "the methodology has the following properties :    * it yields the same equations as gaussian ml ",
    "fa for @xmath26 , and therefore supports the use of these estimation equations even when the gaussian distribution is questionable ; * it is scale invariant in the sense mentioned above ; * without problems , it allows more variables than observations ( @xmath0 ) ; * it yields estimated or predicted factor scores of high precision when @xmath3 is large .",
    "the basic model properties to be derived in the next section will naturally lead to estimating equations for distribution - free parameter estimation .",
    "different iterative methods to solve these equations are discussed in section 3 .",
    "use of singular value decompositions ( svd ) will not only make the computations fast , but also yield some further insight ( sec .",
    "the svd tool is used in sec .  5 to yield expressions for factor scores and residuals .",
    "these are compared in sec .  6 with the model properties of trendafilov & unkel ( 2011 ) . finally , in sec .",
    "7 , the recommended iteration method is successfully tried on gene expression data with @xmath27 .",
    "as mentioned above , we assume we have a sample of multivariate @xmath19-data @xmath28 , @xmath29 , @xmath30 .",
    "we will later assume that the @xmath19-sample is mean - standardized , so we need only consider the sample covariance matrix @xmath31 and the corresponding population covariance matrix @xmath21 .",
    "in the next section , we concentrate on @xmath21 , so the sample size @xmath32 and its relation to the dimension @xmath3 will not yet be a question .",
    "for the fa model , the population covariance matrix @xmath21 ( @xmath33 ) is @xmath34 there is a rotational ambiguity in the loading parameters of this representation . for uniqueness we will use the same",
    "well - known and natural constraint as in the standard gaussian ml approach : @xmath35 this demand will be equivalent with an assumption that the @xmath8 matrix @xmath36 has orthogonal columns .",
    "our motivation to make this particular choice will be clear below .    as mentioned in sec .  1",
    ", classical principal factor analysis requires an initial or current estimate of @xmath14 to be subtracted from @xmath24 , so that ideally we would get @xmath23 .",
    "pca is now used on the resulting reduced covariance matrix @xmath37 . below we will instead use a rescaled covariance matrix , that will be demonstrated to have much better properties .",
    "consider rescaling the vector @xmath19 to @xmath38 , neglecting for a moment the fact that @xmath20 is unknown ( later we will update @xmath20 iteratively ) .",
    "this will make all observation components have the same error variance .",
    "the total covariance matrix @xmath39 for a @xmath40-vector is @xmath41 where @xmath42 denotes the @xmath33 identity matrix and @xmath43 is @xmath8 , cf .  .",
    "because of assumption we know that @xmath44 has orthogonal columns , and it follows that these columns are eigenvectors of the matrix @xmath39 . in a condensed representation we can write @xmath45 where @xmath46 is a diagonal @xmath47 matrix with the corresponding eigenvalues as diagonal elements , that is @xmath48 the sum of these @xmath11 eigenvalues is @xmath49 if @xmath11 latent factors are both necessary and sufficient for the model to hold , precisely these @xmath11 eigenvalues of @xmath39 will be @xmath50 . for a complete set of eigenvectors of @xmath39 , we need to supplement @xmath44 by @xmath51 vectors spanning the orthogonal complement of the space spanned by @xmath44 .",
    "they will all have the eigenvalue 1 .",
    "equation does not specify the length of the eigenvectors in @xmath44 .",
    "for that reason we also introduce the corresponding set of normalized eigenvectors @xmath52 , @xmath53 which is a @xmath8 matrix of @xmath54 orthonormal eigenvectors .",
    "thus , @xmath55 , the @xmath56 identity matrix .",
    "the matrix @xmath52 of course satisfies the same relation as @xmath44 : @xmath57    thus , if we knew @xmath20 and @xmath21 , we could form @xmath39 and calculate its first ( = largest ) @xmath11 eigenvectors @xmath52 , with their eigenvalues @xmath46 , and solve for the loadings matrix @xmath58 : @xmath59 and @xmath60 this tells how we can compute @xmath7 as a function of @xmath20 and @xmath21 .",
    "in addition , yields a trivially simple formula for the diagonal matrix @xmath14 as a function of @xmath7 , given @xmath21 : @xmath61 where diag stands for the diagonal part of the matrices , as a vector .",
    "an equivalent alternative is @xmath62 here the left hand side can be obtained by elementwise multiplication of the diagonals of @xmath63 and @xmath21 , or equivalently as @xmath64 .",
    "for parameter estimation based on data , the formulae above can be used with @xmath24 inserted for @xmath21 : this yields an estimating equation for @xmath7 as @xmath65 here it is indicated that @xmath66 from is a function of @xmath20 , and that @xmath52 and @xmath67 are obtained from @xmath68 and not from the theoretical @xmath39 .",
    "the other estimating equation is obtained from formula or with @xmath24 for @xmath21 : @xmath69 we thus want a solution of these two estimating equations relating @xmath20 and @xmath7 .",
    "when @xmath0 , these estimating equations turn out to be identically the same as the gaussian model likelihood equations .",
    "this can be taken either as a robustness argument for the gaussian ml estimates , or as well as a strong argument for the distribution - free method , at least for large @xmath32 .",
    "they are also generally quite intuitive .",
    "formula is an obvious demand , and formula or is a truncated pca on @xmath23 after a suitable , albeit parameter - dependent rescaling .",
    "there is no explicit solution to the set of equations for @xmath7 and @xmath14 .",
    "thus we have to use some iterative method , and a partial choice is obvious : select @xmath14 in some way and use this @xmath14 in a calculation of a corresponding @xmath7 , to be used to update @xmath14 , etc .",
    "the step yielding @xmath7 will be taken as given in most of the sequel .",
    "the question remains how to update @xmath14 .",
    "unless some care is used , such equations might yield impossible diagonal elements for @xmath14 .",
    "we return to this question in the next paragraph .",
    "there are alternative estimation methods to ml proposed in the fa literature . among unweighted and weighted ls metods ,",
    "the one denoted @xmath70 in bartholomew & knott ( 1999 ) appears to be of particular interest in the present context , since it weights data by @xmath71 , thus corresponding to our transformation of data . for given @xmath20 , the @xmath70 method yields",
    "identically the same estimating equation for @xmath7 as the ml method .",
    "to estimate @xmath14 by the @xmath70 method is ( quoting bartholomew & knott ) a good deal more complicated .",
    "the choice of @xmath20 should be such that the sum of squared differences from 1 of the @xmath51 smallest eigenvalues of @xmath72 is as small as possible , under the constraint that they are all @xmath73 .",
    "this constraint , however , excludes the case of a singular @xmath24 and in particular the case @xmath0 , and the method is therefore of little interest here .",
    "another type of estimation method are the estimation procedures in for example trendafilov & unkel ( 2011 ) , jointly estimating @xmath74 , @xmath7 and @xmath14 .",
    "they are based on a different model with additional constraints , which are not adequate in the present setting .",
    "they will be further commented in section  [ sec : u&t ] .",
    "the pair of estimating equations and leads naturally to an iterative procedure , where we start with a provisional @xmath20 , calculate @xmath7 by , calculate a new @xmath20 by , etc .",
    "such calculations are simplified by use of svd on the sample of @xmath40-vectors , see next section . however , some variants are possible when using the equations for @xmath20 .",
    "the simplest version is to use to express the new @xmath14 , in component form @xmath75 with the current @xmath7 , based on the previous @xmath20 , on the right hand side .",
    "this procedure has a long history , where it turned out to often converge slowly and sometimes to stop before true convergence was achieved .",
    "even worse , the iteration could sometimes yield one or more negative @xmath14 components , known as generalized heywood cases .",
    "this might be because the best values had not yet been found , but a contributing reason could be the wrong @xmath11 or an otherwise inadequate model . for these reasons",
    ", this iteration procedure for gaussian ml estimation was abandoned , and replaced by a step of direct likelihood maximization to yield @xmath20 for given @xmath7 ( jreskog , 1967 ; lawley , 1967 ) .",
    "another alternative is to use the em algorithm ( rubin & thayer , 1982 ) .",
    "the equivalent formula suggests a different iteration procedure than .",
    "calculate the new @xmath20 by , with the current @xmath44 on the right hand side .",
    "this yields the iteration step in component form given by @xmath76 one advantage of this is that it yields a positive @xmath14 whatever is the current @xmath44 . on the other hand ,",
    "our experiences indicate that it is a slower algorithm , and we do not recommend it .",
    "theoretical investigation of the rate of convergence of these methods is difficult , due to the updating of eigenvectors involved . on the other hand",
    ", we have used the updating formula on data with large @xmath3 ( @xmath27 ) without any problems , see further discussion in section  [ sec : svd ] and section  [ sec : genedata ] .",
    "let @xmath77 be the @xmath78 matrix of column mean - centered @xmath19-data , and correspondingly @xmath79 for a provisional @xmath20 .",
    "a convenient procedure for carrying out the computations above is to calculate and use the singular value decomposition ( svd ) of the matrix @xmath80 , given @xmath20 : @xmath81 where @xmath82 ( @xmath78 if @xmath26 ) and @xmath83 ( @xmath33 ) have orthonormal columns ( the left and right singular vectors ) , and @xmath84 is a diagonal @xmath33 matrix whose diagonal elements , the singular values , are , in decreasing order , the square roots of the eigenvalues of @xmath85 .",
    "when @xmath0 , less than @xmath32 singular values can be positive ( typically @xmath86 ) , and then we let @xmath82 and @xmath84 be @xmath87 , and @xmath83 be @xmath88 .",
    "the right singular vectors forming @xmath83 are the orthonormal eigenvectors of @xmath89 ( or of the covariance matrix @xmath90 ) .",
    "corresponding to the fa model , we truncate the svd by using only the first @xmath11 singular vectors , @xmath91 ( @xmath92 ) and @xmath93 ( @xmath8 ) , say , corresponding to @xmath52 .",
    "that is , we partition @xmath80 as @xmath94 where @xmath95 , etc .",
    "note that it does not affect @xmath96 whether @xmath26 or @xmath0 , but only the second term , where @xmath97 is either @xmath98 or @xmath99 , respectively .",
    "since @xmath93 is formed by the normalized eigenvectors of @xmath100 with the @xmath11 highest eigenvalues , and these are given by the diagonal @xmath101 , we can identify @xmath102 and @xmath103 from equation .",
    "thus the estimating equation for @xmath7 can be expressed in terms of @xmath93 and @xmath104 , and for the estimation of @xmath7 ( given @xmath20 ) we will need only @xmath96 .",
    "more precisely , @xmath105 in combination with @xmath106 iteration step for @xmath14 takes the following form in terms of @xmath93 and @xmath104 : @xmath107 the alternative iteration step takes the form @xmath108    the right hand side of may alternatively be expressed as @xmath109 which shows that it is obtained by replacing the first @xmath11 singular values or eigenvalues in @xmath110 by the value 1 .",
    "consequently , the iteration method can not possibly yield zero or negative values in @xmath14 in any iteration step ( presuming start values are positive ) .",
    "what might possibly go wrong , as indicated by , is that @xmath111 is not positive definite . in the case",
    "@xmath0 , however , we give below some more results about @xmath101 and @xmath112 , showing that we need not worry .",
    "note first that when @xmath20 and @xmath7 satisfy the estimating equations , all the @xmath3 diagonal elements of @xmath113 are 1 , so its eigenvalues sum to @xmath3 . at the same time , @xmath114 thus , under the same conditions , @xmath115    if @xmath11 is not higher than motivated by data , we expect the diagonal matrix @xmath116 in to have all its diagonal elements positive .",
    "when @xmath26 , this can fail , and the estimation process too .",
    "when @xmath117 , however , the diagonal elements are necessarily positive , at least in a vicinity of the estimation point . to see this ,",
    "note first that @xmath112 contains less than @xmath118 positive values , but has @xmath119 .",
    "thus , the average value is at least @xmath120 . since the @xmath11 diagonal values in @xmath121 are larger than this , by selection , the corresponding elements of @xmath116 are necessarily positive , which was to be shown .",
    "in passing , we supplement by an expression for the average of the @xmath11 first eigenvalues of @xmath110 , cf .  .",
    "this average can be written @xmath122 where @xmath123 is the inverse of the harmonic mean of the @xmath3 unique factor variance proportions @xmath124 , @xmath125 this is seen by subtracting @xmath51 from @xmath126 . note the proportionality to the dimension @xmath3 in the second term of @xmath127 , showing the benefit of large @xmath3 .",
    "note also that when @xmath11 is increased , @xmath128 will also increase .",
    "the svd approach can be used to obtain relatively directly the most common estimates or predictions of the scores @xmath10 , or the whole @xmath92 scores matrix @xmath74 with the @xmath16-vectors as rows . as usual in the context of scores estimation / prediction , we provisionally regard the parameters as known ( but they are of course estimated ) .",
    "the bartlett scores , or weighted least squares scores regressing @xmath77 on @xmath7 , are given by @xmath129 so first we can note that with @xmath80 as data , bartlett scores are standard ( i.e.  equal weights ) least squares scores . continuing from , @xmath130 using the fact that @xmath131 .",
    "this implies that the bartlett score components are proportional to the svd vectors @xmath91 .",
    "more precisely , since @xmath132 , we achieve the following estimation / prediction formula ( two equivalent versions related by ):",
    "@xmath133 to the right of @xmath134 is a diagonal matrix that scales the @xmath135th column of @xmath91 by the factor @xmath136 , @xmath137 .",
    "thus , this is bartlett s formula in a disguised but computationally convenient form . typically ,",
    "if @xmath3 is large and @xmath11 is not too large , all @xmath11 @xmath138-values will be large ( proportionally to @xmath3 , cf . ) , and then with good approximation @xmath139 .",
    "if we instead predict the scores @xmath74 by the linear regression of @xmath74 on the observed @xmath77-data ( or on @xmath80 ) , the best linear predictor @xmath140 is given by the so called regression or thomson scores @xmath141 the difference from is the diagonal matrix factor @xmath142 ( cf .",
    "bartholomew & knott , 1999 , sec .  3.24 , or krzanowski & marriott , 1995 , sec .  12.27 ) .",
    "again , if @xmath3 is large , but not @xmath11 , @xmath143 .    for high dimension @xmath3 but small or moderate sample size @xmath32 we can not expect high precision in the estimation of @xmath7 or @xmath20 .",
    "estimation / prediction of the scores @xmath10 , however , will be more precise with increasing @xmath3 . more precisely , it can be shown that under mild conditions the variance of the factor estimator / predictor @xmath144 or @xmath140 goes to zero as @xmath3 increases but @xmath11 and @xmath32 are kept constant . to be specific ,",
    "consider the bartlett score vector @xmath145 for an arbitrary observation @xmath146 , @xmath147 .",
    "first , if the difference between @xmath148 and @xmath44 is still neglected , formula yields the well - known result @xmath149 due to , we may conclude that this diagonal matrix will have small elements when @xmath3 is large and @xmath11 is not too large .    the argument above is not justified when @xmath150 , however . in that case",
    ", let us still regard @xmath148 as given , but with @xmath151 differing from the right @xmath14 .",
    "formula should then replaced by @xmath152 this will differ from the corresponding element of by less than a factor @xmath153 we do not know the true @xmath154-values , but if there are no components with quite little estimated noise @xmath155 , and provided the elements of @xmath156 are quite small , we can feel sure the precision in @xmath144 is high .    when the scores matrix @xmath74 has been estimated / predicted , we can form the matrix of residuals , for example @xmath157 . in order to make them all comparable on the same scale , we must variance - standardize to @xmath158 .",
    "now note that @xmath159 so the standardized residuals matrix is @xmath160 thus , the sum over @xmath161 of the mean squared standardized residuals is @xmath162 .",
    "this may be compared with the result , which tells that the trace of @xmath163 is only @xmath51 , and not @xmath3 , so the mean squared standardized residuals are `` too small '' , and must be normalized by @xmath51 instead of @xmath3 to have the right average size over @xmath161 .",
    "this corresponds to the residual degrees of freedom for unbiased variance estimation in a linear model for @xmath80 , regarding @xmath7 as given and the @xmath164 free elements of @xmath74 as unknowns .",
    "in recent years , methods have been advocated for fitting fixed factor models to data , where also @xmath74 is regarded as a set of unknown parameters , see the review by unkel & trendafilov ( 2010b ) .",
    "several papers by those two authors treat the case @xmath0 .",
    "the methods of unkel & trendafilov ( 2010a ) and trendafilov & unkel ( 2011 ) proceed from a least squares method minimizing a loss function based on the frobenius norm of data matrices .",
    "quite generally , the fixed model requires more restrictions than the random model , for uniqueness , and when @xmath0 .",
    "the authors are led to impose special constraints .",
    "let us write @xmath165 , so we can let @xmath166 exist also when @xmath14 contains zero variances .",
    "the papers referred to above assume the model satisfies the constraints @xmath167 , @xmath168 , and ( unless @xmath0 ) @xmath169 .",
    "when @xmath0 they find that @xmath170 can not be fulfilled , because the rank of @xmath166 can be at most @xmath32 , and conclude that they need to allow at least @xmath171 unique factors to have zero variances , corresponding to a singular @xmath14 .",
    "in that situation they weaken the constraint @xmath170 to the eigenvector relation @xmath172 .    on the other hand , a result by robertson & symons ( 2007 ) states that the gaussian model likelihood typically ( depending on @xmath11 ) has a unique global maximum also when @xmath0 , and with a _",
    "trendafilov & unkel ( 2011 ) correctly remark that this result is not consistent with their own model . that the rank of @xmath166 can be at most @xmath32 ( or @xmath86 , considering that data are centered )",
    "is trivially true for the sample of data , but not for the underlying statistical models assumed by robertson & symons ( 2007 ) and by us in the present paper .",
    "our conclusion is that their constraints are artificial , and that their method only represents a constrained partitioning of data , and that it does not represent the fitting of a reasonable statistical model .",
    "we shed further light on this situation here by comparing with our distribution - free but ml - related approach as far as it leads to the eigenvector relation for @xmath44 and the bartlett scores for estimating the scores matrix @xmath74 , with any given @xmath20 : + the constraint @xmath167 is satisfied also for the fitted random model and its bartlett scores @xmath144 , according to section  [ sec : scores ] .",
    "+ the constraint @xmath168 is not exactly consistent with bartlett scores but with the large @xmath3 approximation @xmath139 .",
    "+ the constraints @xmath169 for @xmath18 and @xmath173 for @xmath0 are not consistent with our fitted model . and other features of our fitted model , in particular since it does not allow noise outside the diagonal of @xmath174 . + nor",
    "is the constraint consistent with bartlett scores and other features of our model .    as their first illustration , trendafilov & unkel ( 2011 ) use thurstone s 26-variable box data , consisting of a set of @xmath175 boxes and @xmath176 variables for each box , representing various aspects of size .",
    "when they fit a model with three factors ( @xmath177 ) , they get 13 or 14 zero - valued @xmath154-values ( depending on algorithm ) .",
    "when we fit our model we clearly get no more than 6 zeros , and they can be explained by the peculiarities of the data set .",
    "in fact , there are only three original variables in the data set : length , width and height .",
    "all other variables are constructed as functions of them . in a model with three latent factors ,",
    "the factors turn out to be precisely length , width and height , and that explains three zeros .",
    "three other variables are linear functions of length , width and height , and that explains the remaining three zeros .",
    "so for example adding a little computer - generated random measurement noise to the variables makes the zero variances disappear completely .",
    "thus , all their zero unique factor variances are not really due to @xmath150 , but to a combination of their assumed artificial data structure ( model ) and associated fitting method , and the peculiarities of the data set .",
    "an example of more applied relevance is studied in section  [ sec : genedata ] .",
    "we tried the model and the iteration methods on a microarray data set from alon et al ( 1999 ) , with 62 tissue samples ( a colon cancer sample from each of 40 individuals and non - cancer samples from 22 of these individuals ) , and @xmath1 genes ( selected by theses authors from a larger set of genes ) .",
    "the data are available on www.bioconductor.org , from where they were fetched .",
    "the data have earlier been used for illustrative purposes by mclachlan et al ( 2003 , 2004 ) .",
    "the response was taken to be the gene expression on log scale ( natural log ) .",
    "each gene was mean- and variance - standardized , but no other normalization of the data was made .",
    "none of the biological structure imposed by the experiment was used in the model , since our aim was not to draw biological conclusions but only to try our methods for model fitting .",
    "we tested the estimation method on the data of all tissue samples ( @xmath178 ) , but mostly on the data of only non - cancer tissue ( @xmath2 ) .",
    "the iteration method was found to be slower and generally inferior to the method .",
    "the experiences from running the iteration method were extremely satisfactory .",
    "the method converged in about 10 iterations for small @xmath11 and not more than 20 to 30 iterations for larger @xmath11 , somewhat also depending on the choice of starting values for @xmath14 .",
    "the time per iteration step seemed to be slowly increasing with @xmath11 , but even with an extremely large @xmath11 , @xmath179 say , iterations did not require more time than a second each , on an ordinary laptop .",
    "there was no problem of heywood type during the iterations .",
    "even if the minimum of the unique factor variances in @xmath14 naturally decreased with @xmath11 , it was in no case estimated to be zero ( we tried @xmath11-values up to 20 for @xmath178 , and @xmath180 for @xmath2 ) .",
    "after quite few iterations , @xmath181 was reasonably close to @xmath51 , cf . .",
    "the statements about @xmath181 and about the minimum of the unique factor variances are illustrated in figures 1 and 2 below , showing how these quantities rapidly converge as the iteration number increases .",
    "both for a small factor dimension ( @xmath182 ) and a moderate ( @xmath183 ) or large such dimension ( @xmath180 ) there are no problems at all , but @xmath184 is also included for the little bump it shows in figure 1 .",
    "starting values were @xmath185 for all @xmath135 .",
    "we have thus found substantial support for the conjecture , that the iteration method works so well not _ despite _ the large @xmath3-value , but _",
    "due to _ the large @xmath3 .",
    "summing up , we have come to the following conclusions from the investigations in this paper .",
    "distribution - free estimating equations for the parameters of the standard fa model ( with random factors ) , and , are easily derived in a set - up where variables are variance - normalized by their specific factor standard deviations ( @xmath20 ) .",
    "this theory extends the gaussian likelihood equations both to distribution - free settings and to the case @xmath0 .",
    "the estimating equations are conveniently expressed by use of a singular value decomposition ( svd ) under the same normalization .",
    "an iteration scheme that has been much used for mle computation when @xmath26 , but also criticized as unreliable in such cases , is shown to have much stronger properties when @xmath0 .",
    "the theoretical results are supported empirically in an illustration with @xmath27 , where the method was seen to converge quite rapidly .",
    "another result for situations of type @xmath27 is that even though the model parameters can not be precisely estimated when @xmath32 is small , the factor scores can be precisely estimated / predicted when @xmath3 is large .",
    "alon , u. et al . ( 1999 ) .",
    "broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissue probed by oligonucleotide arrays .",
    "usa _ * 96 * , 67456750 .",
    "+ bartholomew , d.j . &",
    "knott , m. ( 1999 ) .",
    "_ latent variable models and factor analysis _ , 2nd edn .",
    "arnold , london + jreskog , k.g .",
    "some contributions to maximum likelihood factor analysis .",
    "_ psychometrika _ * 32 * , 443482 .",
    "+ krzanowski , w.j .",
    "& marriott , f.h.c . ( 1995 ) .",
    "_ multivariate analysis , part 2_. arnold , london + lawley , d.n .",
    "some new results in maximum likelihood factor analysis .",
    "edinburgh a _ * 67 * , 256264 .",
    "+ mclachlan , g.j .",
    ", peel , d. & bean , r.w .",
    "modelling high - dimensional data by mixtures of factor analyzers . _",
    "stat . & data analysis _ * 41 * , 379388 .",
    "+ mclachlan , g.j . ,",
    "do , k .- a . &",
    "ambroise , c. ( 2004 ) .",
    "_ analyzing microarray gene expression data_. wiley , hoboken .",
    "+ rubin , d.b . & thayer , d.t .",
    "em algorithms for ml factor analysis .",
    "_ psychometrika _ * 32 * , 443482 .",
    "+ robertson , d. & symons , j. ( 2007 ) . maximum likelihood factor analysis with rank - deficient sample covariance matrices . _ journal of multivariate analysis _ * 98 * , 813828 .",
    "+ trendafilov , n.t . &",
    "unkel , s. ( 2011 ) . exploratory factor analysis of data matrices with more variables than observations .",
    "_ j. comp . graph .",
    "* 20 * , 874891 .",
    "+ unkel , s & trendafilov , n.t .",
    "( 2010a ) .",
    "a majorization algorithm for simultaneous parameter estimation in robust exploratory factor analysis . _ comp .",
    "stat . & data analysis _ * 54 * , 33483358 .",
    "+ unkel , s & trendafilov , n.t .",
    ". simultaneous parameter estimation in exploratory factor analysis : an expository review . _ int",
    "rev . _ * 78 * , 363382 .",
    "+ addresses : + rolf sundberg , mathem . statistics , stockholm university , sweden , rolfs@math.su.se ; + uwe feldmann , medical biometry , university of saarland , germany , uf@med-imbei.uni-saarland.de + corresponding author : rolf sundberg     of the sum of unique factor eigenvalues , @xmath186 , eq .",
    ".@xmath182 : ",
    "( black)@xmath183 : - - - - - - ( blue)@xmath184 : @xmath187 ( red)@xmath180 : - @xmath188 - @xmath188 - ( brown ) , width=302,height=302 ]"
  ],
  "abstract_text": [
    "<S> we here provide a distribution - free approach to the random factor analysis model . </S>",
    "<S> we show that it leads to the same estimating equations as for the classical ml estimates under normality , but more easily derived , and valid also in the case of more variables than observations ( @xmath0 ) . for this case </S>",
    "<S> we also advocate a simple iteration method . in an illustration with @xmath1 and @xmath2 </S>",
    "<S> it was seen to lead to convergence after just a few iterations . </S>",
    "<S> we show that there is no reason to expect heywood cases to appear , and that the factor scores will typically be precisely estimated / predicted as soon as @xmath3 is large . </S>",
    "<S> we state as a general conjecture that the nice behaviour is not despite @xmath0 , but because @xmath0 .    _ </S>",
    "<S> key words : _ efa ; fa ; fixed point iterations ; likelihood equations ; more variables than observations ; svd . </S>"
  ]
}