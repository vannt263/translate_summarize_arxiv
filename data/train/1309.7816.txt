{
  "article_text": [
    "jakob bernoulli s _ ars conjectandi _ established the field of probability theory , and founded a long and remarkable mathematical development of deducing patterns to be observed in sequences of random events .",
    "the theory of statistical inference works in the opposite direction , attempting to solve the inverse problem of deducing plausible models from a given set of observations .",
    "laplace pioneered the study of this inverse problem , and indeed he referred to his method as that of inverse probability .",
    "the likelihood function , introduced by @xcite , puts this inversion front and centre , by writing the probability model as a function of unknown parameters in the model .",
    "this simple , almost trivial , change in point of view has profoundly influenced the development of statistical theory and methods . in the early days ,",
    "computing data summaries based on the likelihood function could be computationally difficult , and various _ ad hoc _ simplifications were proposed and studied . by the late 1970s , however , the widespread availability of computing enabled a parallel development of widespread implementation of likelihood - based inference .",
    "the development of simulation and approximation methods that followed meant that both bayesian and non - bayesian inferences based on the likelihood function could be readily obtained .    as a result",
    ", construction of the likelihood function , and various summaries derived from it , is now a nearly ubiquitous starting point for a great many application areas .",
    "this has a unifying effect on the field of applied statistics , by providing a widely accepted standard as a starting point for inference . with the explosion of data collection in recent decades",
    ", realistic probability models have continued to grow in complexity , and the calculation of the likelihood function can again be computationally very difficult .",
    "several lines of research in active development concern methods to compute approximations to the likelihood function , or inference functions with some of the properties of likelihood functions , in these very complex settings .    in the following section",
    ", i will summarize the standard methods for inference based on the likelihood function , to establish notation , and then in section  [ sec3 ] describe some aspects of more accurate inference , also based on the likelihood function . in section  [ sec4",
    "] , i  describe some extensions of the likelihood function that have been proposed for models with complex dependence structure , with particular emphasis on composite likelihood .",
    "suppose we have a probability model for an observable random vector @xmath0 of the form @xmath1 , where @xmath2 is a vector of unknown parameters in the model , and @xmath3 is a density function with respect to a dominating measure , usually lebesgue measure or counting measure , depending on whether our observations are discrete or continuous .",
    "typical models used in applications assume that @xmath2 could potentially be any value in a set  @xmath4 ; sometimes @xmath4 is infinite - dimensional , but more usually @xmath5 .",
    "the inverse problem mentioned in section  [ sec1 ] is to construct inference about the value or values of @xmath6 that could plausibly have generated an observed value @xmath7 .",
    "this is a considerable abstraction from realistic applied settings ; in most scientific work such a problem will not be isolated from a series of investigations , but we can address at least some of the main issues in this setting .    the likelihood function is simply @xmath8 i.e. , there is an equivalence class of likelihood functions @xmath9 , and only relative ratios @xmath10 are uniquely determined . from a mathematical point of view , ( [ likelihood ] ) is a trivial re - expression of the model @xmath1 ; the re - ordering of the arguments is simply to emphasize in the notation that we are more interested in the @xmath11-section for fixed @xmath12 than in the @xmath12-section for fixed @xmath2 .",
    "used directly with a given observation @xmath13 , @xmath14 provides a ranking of relative plausibility of various values of @xmath2 , in light of the observed data",
    ".    a form of direct inference can be obtained by plotting the likelihood function , if the parameter space is one- or two - dimensional , and several writers , including fisher , have suggested declaring values of @xmath2 in ranges determined by likelihood ratios as plausible , or implausible ; for example , @xcite suggested that values of @xmath2 for which @xmath15 , be declared ` implausible ' , where @xmath16 is the maximum likelihood estimate of @xmath2 , i.e. , the value for which the likelihood function is maximized , over @xmath2 , for a given @xmath12 .",
    "in general study of statistical theory and methods we are usually interested in properties of our statistical methods , in repeated sampling from the model @xmath17 , where @xmath18 is the notional ` true ' value of @xmath2 that generated the data .",
    "this requires considering the distribution of @xmath19 , or relative ratios such as @xmath20 . to this end",
    ", some standard summary functions of @xmath19 are defined .",
    "writing @xmath21 we define the _ score function _",
    "@xmath22 , and the observed and expected fisher information functions : @xmath23    if the components of @xmath24 are independent , then @xmath25 is a sum of independent random variables , as is @xmath26 , and under some conditions on the model the central limit theorem for @xmath27 leads to the following asymptotic results , as @xmath28 : @xmath29 where we suppress the dependence of each derived quantity on @xmath24 ( and on @xmath30 ) for notational convenience .",
    "these results hold under the model @xmath1 ; a more precise statement would use the true value @xmath18 in @xmath31 , @xmath32 , and @xmath33 above , and the model @xmath17 .",
    "however , the quantities @xmath34 , @xmath35 and @xmath36 , considered as functions of both @xmath2 and @xmath24 , are approximate _ pivotal quantities _ , i.e. , they have a known distribution , at least approximately . for @xmath37 we could plot , for example , @xmath38 as a function of @xmath2 , where @xmath39 is the standard normal distribution function , and obtain approximate @xmath40-values for testing any value of @xmath37 for fixed @xmath12 .",
    "the approach to inference based on these pivotal quantities avoids the somewhat artificial distinction between point estimation and hypothesis testing .",
    "when @xmath37 , an approximately standard normal pivotal quantity can be obtained from ( [ lrt ] ) as @xmath41^{1/2 } { \\mathop{\\longrightarrow}^{\\mathcal{l}}}n(0 , 1).\\ ] ]    the likelihood function is also the starting point for bayesian inference ; if we model the unknown parameter as a random quantity with a postulated prior probability density function @xmath42 , then inference given an observed value @xmath43 is based on the posterior distribution , with density @xmath44 bayesian inference is conceptually straightforward , given a prior density , and computational methods for estimating the integral in the denominator of ( [ bayes ] ) , and associated integrals for marginal densities of components , or low - dimensional functions of @xmath2 , have enabled the application of bayesian inference in models of considerable complexity .",
    "two very useful methods include laplace approximation of the relevant integrals , and markov chain monte carlo simulation from the posterior .",
    "difficulties with bayesian inference include the specification of a prior density , and the meaning of probabilities for parameters of a mathematical model .",
    "one way to assess the influence of the prior is to evaluate the properties of the resulting inference under the sampling model , and under regularity conditions similar to those needed to obtain ( [ score ] ) , ( [ mle ] ) and ( [ lrt ] ) , a normal approximation to the posterior density can be derived : @xmath45 implying that inferences based on the posterior are asymptotically equivalent to those based on @xmath46 .",
    "this simple result underlines the fact that bayesian inference will in large samples give approximately correct inference under the model , and also that to distinguish between bayesian and non - bayesian approaches we need to consider the next order of approximation .",
    "if @xmath47 , then ( [ score])([lrt ] ) can be used to construct confidence regions , or to test simple hypotheses of the form @xmath48 , but in many settings @xmath2 can usefully be separated into a parameter of interest @xmath49 , and a nuisance parameter @xmath50 , and analogous versions of the above limiting results in this context are @xmath51 where @xmath52 is the profile log - likelihood function , @xmath53 is the constrained maximum likelihood estimate of the nuisance parameter @xmath50 when @xmath49 is fixed , @xmath54 is the dimension of @xmath49 , and @xmath55 is the fisher information function based on the profile log - likelihood function .",
    "the third result ( [ lrt2 ] ) can be used for model assessment among nested models ; for example , the exponential distribution is nested within both the gamma and weibull models , and a test based on @xmath56 of , say , a gamma model with unconstrained shape parameter , and one with the shape parameter set equal to 1 , is a test of fit of the exponential model to the data ; the rate parameter is the nuisance parameter @xmath50 .",
    "the use of the log - likelihood ratio to compare two non - nested models , for example a log - normal model to a gamma model , requires a different asymptotic theory ( @xcite , @xcite , ch .",
    "a related approach to model selection is based on the akaike information criterion , @xmath57 where @xmath58 is the dimension of @xmath2 .",
    "just as only differences in log - likelihoods are relevant , so are differences in @xmath59 : for a sequence of model fits the one with the smallest value of @xmath59 is preferred .",
    "the @xmath59 criterion was developed in the context of prediction in time series , but can be motivated as an estimate of the kullback - leibler divergence between a fitted model and a notional ` true ' model .",
    "the statistical properties of @xmath59 as a model selection criterion depend on the context ; for example for choosing among a sequence of regression models of the same form , model selection using @xmath59 is not consistent ( @xcite , @xcite , ch .",
    "several related versions of model selection criterion have been suggested , including modifications to @xmath59 , and a version motivated by bayesian arguments , @xmath60 where @xmath30 is the sample size for the model with @xmath58 parameters .",
    "the approximate inference suggested by the approximate pivotal quantities ( [ score2 ] ) , ( [ mle2 ] ) and ( [ lrt2 ] ) is obtained by treating the profile log - likelihood function as if it were a genuine log - likelihood function , i.e. as if the true value of @xmath50 were @xmath61 .",
    "this can be misleading , because it does not account for the fact that the nuisance parameter has been estimated .",
    "one familiar example is inference for the variance in a normal theory linear regression model ; the maximum likelihood estimate is @xmath62 which has expectation @xmath63 , where @xmath64 is the dimension of @xmath65 .",
    "although this estimator is consistent as @xmath66 with @xmath64 fixed , it can be a poor estimate for finite samples , especially if @xmath64 is large relative to @xmath30 , and the divisor @xmath67 is used in practice .",
    "one way to motivate this is to note that @xmath68 is unbiased for @xmath69 ; an argument that generalizes more readily is to note that the likelihood function @xmath70 can be expressed as @xmath71 where @xmath72 is proportional to the density of @xmath73 and @xmath74 is the marginal density of @xmath75 or equivalently @xmath76 .",
    "the unbiased estimate of @xmath69 maximizes the second component @xmath74 , which is known as the restricted likelihood , and estimators based on it often called `` reml '' estimators .",
    "higher order asymptotic theory for likelihood inference has proved to be very useful for generalizing these ideas , by refining the profile log - likelihood to take better account of the nuisance parameter , and has also provided more accurate distribution approximations to pivotal quantities .",
    "perhaps most importantly , for statistical theory , higher order asymptotic theory helps to clarify the role of the likelihood function and the prior in the calibration of bayesian inference .",
    "these three goals have turned out to be very intertwined . to illustrate some aspects of this , consider the marginal posterior density for @xmath49 , where @xmath77 : @xmath78 laplace approximation to the numerator and denominator integrals leads to @xmath79 where @xmath80 is the block of the observed fisher information function corresponding to the nuisance parameter @xmath50",
    ", @xmath81 has been computed using the partitioned form to give the second expression in ( [ laplace2 ] ) , and in the third expression @xmath82 when renormalized to integrate to one , this laplace approximation has relative error @xmath83 in independent sampling from a model that satisfies various regularity conditions similar to those needed to show the asymptotic normality of the posterior @xcite .",
    "these expressions show that an adjustment for estimation of the nuisance parameter is captured in @xmath84 , and this adjustment can be included in the profile log - likelihood function , as in the third expression in ( [ laplace2 ] ) , or tacked onto it , as in the second expression .",
    "the effect of the prior is isolated from this nuisance parameter adjustment effect , so , for example , if @xmath85 , and the priors for @xmath49 and @xmath50 are independent , then the form of the prior for @xmath50 given @xmath49 does not affect the approximation .",
    "the adjusted profile log - likelihood function @xmath86 is the simplest of a number of modified profile log - likelihood functions suggested in the literature for improved frequentist inference in the presence of nuisance parameters , and was suggested for general use in @xcite , after reparametrizing the model to make @xmath49 and @xmath50 orthogonal with respect to expected fisher information , i.e. , @xmath87 .",
    "this reparameterization makes it at least more plausible that @xmath49 and @xmath50 could be modelled as _ a priori _ independent , and also ensures that @xmath88 , rather than the usual @xmath89 .",
    "a number of related , but more precise , adjustments to the profile log - likelihood function have been developed from asymptotic expansions for frequentist inference , and take the form @xmath90 where @xmath91 ; see , for example , @xcite ( @xcite ) and @xcite .",
    "the change from @xmath92 to @xmath93 is related to the orthogonality conditions ; in ( [ mpl ] ) orthogonality of parameters is not needed , as the expression is parameterization invariant .",
    "inferential statements based on approximations from ( [ score2])([lrt2 ] ) , with @xmath86 or @xmath94 substituting for the profile log - likelihood function , are still valid and are more accurate in finite samples , as they adjust for errors due to estimation of @xmath50 .",
    "they are still first - order approximations , although often quite good ones .",
    "one motivation for these modified profile log - likelihood functions , and inference based on them , is that they approximate marginal or conditional likelihoods , when these exist .",
    "for example , if the model is such that @xmath95 then inference for @xmath49 can be based on the marginal likelihood for @xmath49 based on @xmath96 , and the theory outlined above applies directly .",
    "this factorization is fairly special ; more common is a factorization of the form @xmath97 : in that case to base our inference on the likelihood for @xmath49 from @xmath96 would require further checking that little information is lost in ignoring the second term",
    ". arguments like these , applied to special classes of model families , were used to derive the modified profile log - likelihood inference outlined above .",
    "a related development is the improvement of the distributional approximation to the approximate pivotal quantity ( [ root ] ) .",
    "the laplace approximation ( [ laplace2 ] ) can be used to obtain the bayesian pivotal , for scalar @xmath49 , @xmath98 where @xmath99^{1/2 } , \\\\",
    "\\label{qb}q_b(\\psi ) & = & -\\ell_{\\mathrm{p}}'(\\psi ) j_{\\mathrm{p}}^{-1/2}(\\hat\\psi ) \\biggl\\{\\frac{|j_{\\lambda\\lambda}(\\psi,\\hat\\lambda_\\psi ) |}{|j_{\\lambda\\lambda}(\\hat\\psi,\\hat\\lambda)| } \\biggr \\}^{1/2}\\frac{\\pi(\\hat\\psi,\\hat\\lambda)}{\\pi(\\psi,\\hat\\lambda_\\psi)}\\end{aligned}\\ ] ] and the approximation in ( [ rstarb ] ) is to the posterior distribution of @xmath100 , given @xmath12 , and is accurate to @xmath83 .",
    "there is a frequentist version of this pivotal that has the same form : @xmath101 where @xmath102 is given by ( [ root2 ] ) , but the expression for @xmath103 requires additional notation , and indeed an additional likelihood component .",
    "in the special case of no nuisance parameters @xmath104 in ( [ qf1 ] ) , we have assumed that there is a one - to - one transformation from @xmath12 to @xmath105 , and that we can write the log - likelihood function in terms of @xmath106 and then differentiate it with respect to @xmath107 , for fixed @xmath108 .",
    "expression ( [ qf2 ] ) is equivalent , but expresses this sample space differentiation through a data - dependent reparameterization @xmath109 , where the derivative with respect to @xmath110 is a directional derivative to be determined .",
    "the details are somewhat cumbersome , and even more so for the case of nuisance parameters , but the resulting @xmath111 approximate pivotal quantity is readily calculated in a wide range of models for independent observations @xmath112 .",
    "detailed accounts are given in @xcite , @xcite , @xcite , @xcite and @xcite ( @xcite , ch .",
    "8.6 ) ; the last emphasizes implementation in a number of practical settings , including generalized linear models , nonlinear regression with normal errors , linear regression with non - normal errors , and a number of more specialized models .    from a theoretical point of view ,",
    "an important distinction between @xmath113 and @xmath111 is that the latter requires differentiation of the log - likelihood function on the sample space , whereas the former depends only on the observed log - likelihood function , along with the prior .",
    "the similarity of the two expressions suggests that it might be possible to develop prior densities for which the posterior probability bounds are guaranteed to be valid under the model , at least to a higher order of approximation than implied by ( [ bayes2 ] ) , and there is a long line of research on the development of these so - called `` matching priors '' ; see , for example , @xcite .",
    "while the asymptotic results of the last section provide very accurate inferences , they are not as straightforward to apply as the first order results , especially in models with complex dependence . they do shed light on many aspects of theory , including the precise points of difference , asymptotically , between bayesian and nonbayesian inference . and",
    "the techniques used to derive them , saddlepoint and laplace approximations in the main , have found application in complex models in certain settings , such as the integrated nested laplace approximation of @xcite .    a glance at any number of papers motivated by specific applications , though , will confirm that likelihood summaries , and in particular computation of the maximum likelihood estimator , are often the inferential goal , even as the models become increasingly high - dimensional .",
    "this is perhaps a natural consequence of the emphasis on developing probability models that could plausibly generate , or at least describe , the observed responses , as the likelihood function is directly obtained from the probability model .",
    "but more than this , inference based on the likelihood function provides a standard set of tools , whose properties are generally well - known , and avoids the construction of _ ad hoc _ inferential techniques for each new application .",
    "for example , @xcite write `` the likelihood framework is an efficient way to extract information from a neural spike train  we believe that greater use of the likelihood based approaches and goodness - of - fit measures can help improve the quality of neuroscience data analysis '' .    a number of inference functions based on the likelihood function , or meant to have some of the key properties of the likelihood function ,",
    "have been developed in the context of particular applications or particular model families . in some cases the goal is to find ` reasonably reliable ' estimates of a parameter , along with an estimated standard error ; in other cases the goal is to use approximate pivotal quantities like those outlined in section  [ sec2 ] in settings where the likelihood is difficult to compute .",
    "the goal of obtaining reliable likelihood - based inference in the presence of nuisance parameters was addressed in section  [ sec3 ] . in some settings ,",
    "families of parametric models are too restrictive , and the aim is to obtain likelihood - type results for inference in semi - parametric and non - parametric settings .      in many applications with longitudinal , clustered , or spatial data , the starting point is a generalized linear model with a linear predictor of the form @xmath114 , where @xmath115 and @xmath116 are @xmath117 and @xmath118 , respectively , matrices of predictors , and @xmath119 is a @xmath46-vector of random effects .",
    "the marginal distribution of the responses requires integrating over the distribution of the random effects @xmath119 , and this is often computationally infeasible .",
    "many approximations have been suggested : one approach is to approximate the integral by laplace s method @xcite , leading to what is commonly called penalized quasi - likelihood , although this is different from the penalized versions of composite likelihood discussed below .",
    "the term quasi - likelihood in the context of generalized linear models refers to the specification of the model through the mean function and variance function only , without specifying a full joint density for the observations .",
    "this was first suggested by @xcite , and extended to longitudinal data in @xcite and later work , leading to the methodology of generalized estimating equations , or gee .",
    "@xcite compared penalized quasi - likelihood to pairwise likelihood , discussed in section  [ sec4.3 ] , in simulations of multivariate probit models for binary data with random effects . in general",
    "penalized quasi - likelihood led to estimates with larger bias and variance than pairwise likelihood .",
    "a different approach to generalized linear mixed models has been developed by lee and nelder ; see , for example , @xcite and @xcite , under the name of @xmath120-likelihood .",
    "this addresses some of the failings of the penalized quasi - likelihood method by modelling the mean parameters and dispersion parameters separately .",
    "the @xmath120-likelihood for the dispersion parameters is motivated by reml - type arguments not unrelated to the higher order asymptotic theory outlined in the previous section .",
    "there are also connections to work on prediction using likelihood methods @xcite .",
    "likelihood approaches to prediction have proved to be somewhat elusive , at least in part because the ` parameter ' to be predicted is a random variable , although bayesian approaches are straightforward as no distinction is made between parameters and random variables .",
    "composite likelihood is one approach to combining the advantages of likelihood with computational feasibility ; more precisely it is a collection of approaches .",
    "the general principle is to simplify complex dependence relationships by computing marginal or conditional distributions of some subsets of the responses , and multiplying these together to form an inference function .    as an _ ad hoc _ solution it has emerged in several versions and in several contexts in the statistical literature ;",
    "an important example is the pseudo - likelihood for spatial processes proposed in @xcite ( @xcite ) . in studies of large networks ,",
    "computational complexity can be reduced by ignoring links between distant nodes , effectively treating sub - networks as independent . in gaussian process models with high - dimensional covariance matrices , assuming",
    "sparsity in the covariance matrix is effectively assuming subsets of variables are independent .",
    "the term composite likelihood was proposed in @xcite , where the theoretical properties of composite likelihood estimation were studied in some generality .",
    "we suppose a vector response of length @xmath46 is modelled by @xmath121 . given a set of events @xmath122 , the composite likelihood function",
    "is defined as @xmath123 and the composite log - likelihood function is @xmath124 because each component in the sum is the log of a density function , the resulting score function @xmath125 has expected value @xmath126 , so has at least one of the properties of a genuine log - likelihood function .    relatively simple and widely used examples of composite likelihoods include independence composite likelihood , @xmath127 pairwise composite likelihood @xmath128 and pairwise conditional composite likelihood @xmath129 where @xmath130 and @xmath131 are the marginal densities for a single component and a pair of components of the vector observation , and the density in ( [ clcond ] ) is the conditional density of one component , given the remainder .",
    "many similar types of composite likelihood can be constructed , appropriate to time series , or spatial data , or repeated measures , and so on , and the definition is usually further extended by allowing each component event to have an associated weight @xmath132 . indeed one of the difficulties of studying the theory of composite likelihood is the generality of the definition .    inference based on composite likelihood is constructed from analogues to the asymptotic results for genuine likelihood functions .",
    "assuming we have a sample @xmath133 of independent observations of @xmath12 , the composite score function , @xmath134 is used as an estimating function to obtain the maximum composite likelihood estimator @xmath135 , and under regularity conditions on the full model , with @xmath136 and fixed @xmath137 , we have , for example , @xmath138 where @xmath139 is the @xmath140 godambe information matrix , and @xmath141 are the variability and sensitivity matrix associated with @xmath142 .",
    "the analogue of ( [ lrt ] ) is @xmath143 where @xmath144 are the eigenvalues of @xmath145 .",
    "neither of these results is quite as convenient as the full likelihood versions , and in particular contexts it may be difficult to estimate @xmath146 accurately , but there are a number of practical settings where these results are much more easily implemented than the full likelihood results , and the efficiency of the methods can be quite good .    a number of applied contexts are surveyed in @xcite . as just one example , developed subsequently , @xcite investigate pairwise composite likelihood for max - stable processes , developed to model extreme values recorded at a number @xmath147 of spatially correlated sites .",
    "although the form of the @xmath147-dimensional density is known , it is not computable for @xmath148 , although expressions are available for the joint density at each pair of sites .",
    "composite likelihood seems to be particularly important for various types of spatial models , and many variations of it have been suggested for these settings .    in some applications , particularly for time",
    "series , but also for space - time data , a  sample of independent observations is not available , and the relevant asymptotic theory is for @xmath149 , where @xmath46 is the dimension of the single response .",
    "the asymptotic results outlined above will require some conditions on the decay of the dependence among components as the ` distance ' between them increases .",
    "asymptotic theory for pairwise likelihood is investigated in @xcite for linear time series , and in @xcite for max - stable processes in space and time .",
    "composite likelihood can also be used for model selection , with an expression analogous to @xmath59 , and for bayesian inference , after adjustment to accommodate result ( [ clrt ] ) .",
    "_ statistica sinica _ * 21 * , # 1 is a special issue devoted to composite likelihood , and more recent research is summarized in the report on a workshop at the banff international research station @xcite .      in some applications",
    ", a flexible class of models can be constructed in which the nuisance ` parameter ' is an unknown function .",
    "the most widely - known example is the proportional hazards model of @xcite for censored survival data ; but semi - parametric regression models are also widely used , where the particular covariates of interest are modelled with a low - dimensional regression parameter , and other features expected to influence the response are modelled as ` smooth ' functions .",
    "@xcite developed inference based on a partial likelihood , which ignored the aspects of the likelihood bearing on the timing of failure events , and subsequent theory based on asymptotics for counting processes established the validity of this approach .",
    "in fact , @xcite s partial likelihood can be viewed as an example of composite likelihood as described above , although the theory for general semi - parametric models seems more natural .",
    "@xcite showed that partial likelihood can be viewed as a profile likelihood , maximized over the nuisance function , and discussed a class of semi - parametric models for which the profile likelihood continues to have the same asymptotic properties as the usual parametric profile likelihood ; the contributions to the discussion of their results provide further insight and references to the extensive literature on semi- and non - parametric likelihoods .",
    "there is , however , no guarantee that asymptotic theory will lead to accurate approximation for finite samples ; it would presumably have at least the same drawbacks as profile likelihood in the parametric setting .",
    "improvements via modifications to the profile likelihood , as described above in the parametric case , do not seem to be available in these more general settings .",
    "some semi - parametric models are in effect converted to high - dimensional parametric models through the use of linear combinations of basis functions ; thus the linear predictor associated with a component @xmath150 might be @xmath151 , or @xmath152 .",
    "the log - likelihood function for models such as these is often regularized , so that @xmath153 is replaced by @xmath154 , where @xmath155 is a penalty function such as @xmath156 or @xmath157 , and @xmath50 a tuning parameter .",
    "many of these extensions , and the asymptotic theory associated with them , are discussed in @xcite ( @xcite , ch .",
    "penalized likelihood using squared error is reviewed in @xcite ; the @xmath72 penalty has been suggested as a means of combining likelihood inference with variable selection ; see , for example , @xcite .    penalized composite likelihoods",
    "have been proposed for applications in spatial analysis ( @xcite , @xcite ; @xcite , @xcite ; @xcite , @xcite ) , gaussian graphical models @xcite , and clustered longitudinal data @xcite .",
    "the difference between semi - parametric likelihoods and nonparametric likelihoods is somewhat blurred ; both have an effectively infinite - dimensional parameter space , and as discussed in @xcite and the discussion , conditions on the model to ensure that likelihood - type asymptotics still hold can be quite technical .",
    "empirical likelihood is a rather different approach to non - parametric models first proposed by @xcite ; a recent discussion is @xcite .",
    "empirical likelihood assumes the existence of a finite - dimensional parameter of interest , defined as a functional of the distribution function for the data , and constructs a profile likelihood by maximizing the joint probability of the data , under the constraint that this parameter is fixed .",
    "this construction is particularly natural in survey sampling , where the parameter is often a property of the population ( @xcite , @xcite ; @xcite , @xcite ) .",
    "distribution theory for empirical likelihood more closely follows that for usual parametric likelihoods .",
    "simulation of the posterior density by markov chain monte carlo methods is widely used for bayesian inference , and there is an enormous literature on various methods and their properties . some of these methods can be adapted for use when the likelihood function itself can not be computed , but it is possible to simulate observations from the stochastic model ; many examples arise in statistical genetics .",
    "simulation methods for maximum likelihood estimation in genetics was proposed in @xcite ; more recently sequential monte carlo methods ( see , for example , @xcite ) and abc ( approximate bayesian computation ) methods ( @xcite , @xcite ; @xcite , @xcite ) are being investigated as computational tools .",
    "a reviewer of an earlier draft suggested that a great many applications , especially involving very large and/or complex datasets , take more algorithmic approaches , often using techniques designed to develop sparse solutions , such as wavelet or thresholding techniques , and that likelihood methods may not be relevant for these application areas .",
    "certainly a likelihood - based approach depends on a statistical model for the data , and for many applications under the general rubric of machine learning these may not be as important as developing fast and reliable approaches to prediction ; recommender systems are one such example .",
    "there are however many applications of ` big data ' methods where statistical models do provide some structure , and in these settings , as in the more classical application areas , likelihood methods provide a unifying basis for inference .",
    "this research was partially supported by the natural sciences and engineering research council .",
    "thanks are due to two reviewers for helpful comments on an earlier version ."
  ],
  "abstract_text": [
    "<S> i review the classical theory of likelihood based inference and consider how it is being extended and developed for use in complex models and sampling schemes . </S>"
  ]
}