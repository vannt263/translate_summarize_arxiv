{
  "article_text": [
    "in @xcite ( see appendix for an authors version of this article ) , we proposed a maximum likelihood approach for blindly separating a linear - quadratic mixture defined by ( eq .",
    "( 2 ) in @xcite ) : @xmath0 where @xmath1 and @xmath2 are two independent sources .",
    "the log - likelihood for @xmath3 samples of the mixed signals @xmath4 and @xmath5 reads ( eq . ( 12 ) in @xcite ) : @xmath6+e_t[\\log{f_{s_2}(s_2(t ) ) } ] -e_t[\\log{|j(s_1(t),s_2(t))| } ] \\label{finalcost_e.eq}\\ ] ] where @xmath7 $ ] represents the time average operator on the @xmath3 samples , @xmath8 and @xmath9 are the probability density functions ( pdf ) of the sources @xmath1 and @xmath2 and @xmath10 is the jacobian of the mixture which reads ( eq . ( 4 ) in @xcite ) @xmath11 maximizing the log - likelihood requires that its gradient with respect to the parameter vector @xmath12 $ ] , _ i.e. _ @xmath13 , vanishes . defining the score functions of the two sources as ( eq .",
    "( 13 ) in @xcite ) @xmath14 we can write ( eq . ( 14 ) in @xcite ) @xmath15-e_t[\\psi_2(s_2)\\frac{\\partial s_2}{\\partial { \\bf w}}]-e_t[\\frac{1}{j}\\frac{\\partial j}{\\partial { \\bf w } } ] \\label{gradient_e.eq}\\ ] ] rewriting ( [ mixture_model_e.eq ] ) in the vector form @xmath16 and considering @xmath17 as the independent variable and @xmath18 as the dependent variable , we can write , using implicit differentiation ( eq . (",
    "15 ) in @xcite ) @xmath19 which yields ( eq . ( 16 ) in @xcite ) @xmath20 note that @xmath21 is the jacobian matrix of the mixing model . considering ( [ mixture_model_e.eq ] ) , we can write ( appendix in @xcite ) + @xmath22 and @xmath23 , which implies , from ( [ dsdw_e.eq ] ) @xmath24 and yields ( eq . ( 19 ) in @xcite ) @xmath25 $ } \\nonumber \\\\ \\frac{\\partial s_2}{\\partial { \\bf w}}=\\frac{1}{j}\\mbox{\\huge$[$}(l_2+q_2s_2)s_2 \\;,\\ ; ( 1-q_1s_2)s_1 \\ ; , ( l_2+q_2s_2)s_1s_2 \\;,\\ ; ( 1-q_1s_2)s_1s_2 \\mbox{\\huge$]$ } \\label{ds1s2dw_e.eq}\\end{aligned}\\ ] ] using ( [ ds1s2dw_e.eq ] ) , we obtain the first two terms of the gradient ( [ gradient_e.eq ] ) . to obtain the third term , we need to compute @xmath26 .",
    "this partial derivative was computed inaccurately in @xcite so that equations ( 20 ) , and thus ( 17 ) , in @xcite are erroneous .",
    "in @xcite we did not consider the implicit dependence of @xmath1 and @xmath2 on @xmath27 and computed the derivative of @xmath10 with respect to @xmath17 ignoring this dependence . considering @xmath28 ,",
    "the correct equation for @xmath29 reads @xmath30 equation ( 20 ) in @xcite only corresponded to the first term on the right side of the above relation which reads , following ( [ jacobian_e.eq ] ) , as : @xmath31 $ } \\label{djdw_scte_e.eq}\\ ] ] we now compute the gradient ( [ djdw_e.eq ] ) entirely . considering ( [ jacobian_e.eq ] ) , we can write @xmath32 \\label{djdws_e.eq}\\ ] ] using ( [ djdw_e.eq ] ) , ( [ djdw_scte_e.eq ] ) , ( [ djdws_e.eq ] ) and ( [ dsdw_new_e.eq ] ) we finally obtain the following equation which must replace the equation ( 20 ) in @xcite @xmath33 \\nonumber \\\\",
    "\\label{djdw_new_e.eq}\\end{aligned}\\ ] ] inserting ( [ ds1s2dw_e.eq ] ) and ( [ djdw_new_e.eq ] ) in ( [ gradient_e.eq ] ) , we obtain the following expression for the gradient which must replace equation ( 17 ) in @xcite @xmath34 $ } \\nonumber \\\\ \\label{gradient_new_e.eq}\\end{aligned}\\ ] ]    9 s. hosseini , y. deville , blind maximum likelihood separation of a linear - quadratic mixture , proceedings of the fifth international conference on independent component analysis and blind signal separation ( ica 2004 ) , pp .",
    "694 - 701 , issn 0302 - 9743 , isbn 3 - 540 - 23056 - 4 , springer - verlag , vol .",
    "lncs 3195 , granada , spain , sept .",
    "22 - 24 , 2004 .",
    "it is well known that the independence hypothesis is not sufficient for separating general nonlinear mixtures because of the very large indeterminacies which make the nonlinear bss problem ill - posed .",
    "a natural idea for reducing the indeterminacies is to constrain the structure of mixing and separating models to belong to a certain set of transformations .",
    "this supplementary constraint can be viewed as a regularization of the initially ill - posed problem .    in this paper",
    ", we study a linear - quadratic mixture model which may be considered as the simplest ( nonlinear ) version of a general polynomial model . our main aim is to develop an approach which can be easily extended to higher - order polynomial models .",
    "hence , we propose a recurrent separating structure whose realization does not require the knowledge of the explicit form of the inverse of the mixing model .",
    "we develop a rigorous method to identify the parameters of the separating structure in a maximum likelihood framework .",
    "the algorithm is developed so that the inverse of the mixing structure is not required to be known .",
    "thus , it can be extended to more general polynomial mixtures .",
    "suppose @xmath35 and @xmath36 are two independent random signals . given the following nonlinear instantaneous mixture model @xmath37 we would like to estimate @xmath35 and @xmath36 up to a permutation and a scaling factor ( and possibly an additive constant )",
    ". for simplicity , let s denote @xmath38 and @xmath39 .",
    "@xmath1 and @xmath2 will be referred to as the _ sources _ in the following .",
    "( [ firstmodel.eq ] ) can be rewritten as @xmath40 in which @xmath41 and @xmath42 represent the linear contributions of the sources in the mixture , and @xmath43 and @xmath44 represent the quadratic contributions .",
    "the negative signs are chosen for simplifying the notations of the separating structure .",
    "solving the model ( [ mixture_model.eq ] ) for @xmath1 and @xmath2 leads to the following two pairs of solutions , which may be considered as two direct separating structures : @xmath45 where @xmath46 , @xmath47 , @xmath48 , @xmath49 , @xmath50 , @xmath51 and @xmath52 .",
    "it can be easily verified that @xmath53 , where @xmath10 is the jacobian of the mixing model ( [ mixture_model.eq ] ) and reads @xmath54 according to the variation domain of the two sources , three different cases may be considered :    \\1 ) @xmath55 for all the values of @xmath1 and @xmath2 . in this case",
    "( [ inverse.eq ] ) becomes : @xmath56 @xmath57 thus , the first direct separating structure in ( [ inverse.eq ] ) leads to the actual sources and the second direct separating structure leads to another solution , equivalent to the first one up to a permutation , a scaling factor , and an additive constant .",
    "\\2 ) @xmath58 for all the values of @xmath1 and @xmath2 . in this case",
    ", the first structure leads to the permuting solution , defined by ( [ pair2.eq ] ) , and the second structure to the actual sources @xmath59 .",
    "an example is shown in fig .",
    "[ case2.fig ] for the numerical values @xmath60 , @xmath61 , @xmath62 , @xmath63 and @xmath64 $ ] .",
    "\\3 ) @xmath58 for some values of the sources and @xmath55 for the other values . in this case",
    ", each structure leads to the non - permuted sources ( [ pair1.eq ] ) for some values of the observations and to the permuted sources ( [ pair2.eq ] ) for the other values .",
    "an example is shown in fig .",
    "[ case3.fig ] ( with the same coefficients as in the second case , but for ) .",
    "the permutation effect is clearly visible in the figure .",
    "one may also remark that the straight line @xmath65 in the source plane is mapped to a conic section in the observation plane ( shown by asterisks ) .",
    "thus , it is clear that the direct structures may be used for separating the sources if the jacobian of the mixing model is always negative or always positive , _",
    "i.e. _ for all the source values .",
    "otherwise , although the sources are separated _ sample by sample _",
    ", each retrieved signal contains samples of the two sources .",
    "this problem arises because the mixing model ( [ mixture_model.eq ] ) is not bijective .",
    "this theoretically insoluble problem should not discourage us .",
    "in fact , our final objective is to extend the idea developed in the current study to more general polynomial models which will be used to approximate the nonlinear mixtures encountered in the real world .",
    "if these real - world nonlinear models are bijective , we can logically suppose that the coefficients of their polynomial approximations take values which make them bijective on the variation domains of the sources .",
    "thus , in the following , we suppose that the sources and the mixture coefficients have numerical values ensuring that the jacobian @xmath10 of the mixing model has a constant sign .",
    "the natural idea to separate the sources is to form a direct separating structure using any of the equations in ( [ inverse.eq ] ) , and to identify the parameters @xmath66 , @xmath67 , @xmath68 and @xmath69 by optimizing an independence measuring criterion .",
    "although this approach may be used for our special mixing model ( [ mixture_model.eq ] ) , as soon as a more complicated polynomial model is considered , the solutions @xmath70 can no longer be determined so that the generalization of the method to arbitrary polynomial models seems impossible . to avoid this limitation , we propose a recurrent structure shown in fig .",
    "[ model.fig ] .",
    "note that , for @xmath71 , this structure is reduced to the basic hrault - jutten network .",
    "it may be checked easily that , for fixed observations defined by ( [ mixture_model.eq ] ) , @xmath72 and @xmath73 corresponds to a steady state for the structure in figure [ model.fig ] .",
    "the use of this recurrent structure is more promising because it can be easily generalized to arbitrary polynomial models .",
    "however , the main problem with this structure is its stability .",
    "in fact , even if the mixing model coefficients are exactly known , the computation of the structure outputs requires the realization of the following recurrent iterative model @xmath74 where a loop on @xmath75 is performed for each couple of observations @xmath76 until convergence is achieved .",
    "it can be shown that this model is locally stable at the separating point @xmath77 , if and only if the absolute values of the two eigenvalues of the jacobian matrix of ( [ recurrent.eq ] ) are smaller than one . in the following ,",
    "we suppose that this condition is satisfied .",
    "let @xmath78 be the joint pdf of the sources , and assume that the mixing model is bijective so that the jacobian of the mixing model has a constant sign on the variation domain of the sources .",
    "the joint pdf of the observations can be written as @xmath79 taking the logarithm of ( [ pdf1.eq ] ) , and considering the independence of the sources , we can write : @xmath80 given n samples of the mixtures @xmath81 and @xmath82 , we want to find the maximum likelihood estimator for the mixture parameters @xmath83 $ ] .",
    "this estimator is obtained by maximizing the joint pdf of all the observations ( supposing that the parameters in @xmath17 are constant ) , which is equal to @xmath84 if @xmath85 and @xmath86 are two i.i.d .",
    "sequences , @xmath87 and @xmath88 are also i.i.d . so that @xmath89 and @xmath90 .",
    "the cost function to be maximized can be defined as @xmath91 , which will be denoted using the temporal averaging operator @xmath7 $ ] as @xmath92\\ ] ] using ( [ logf1.eq ] ) : @xmath6+e_t[\\log{f_{s_2}(s_2(t ) ) } ] -e_t[\\log{|j(s_1(t),s_2(t))| } ] \\label{finalcost.eq}\\ ] ] maximizing this cost function requires that its gradient with respect to the parameter vector @xmath17 , _ i.e. _ @xmath13 , vanishes . defining the score functions of the two sources as @xmath14 and considering that @xmath93 , we can write @xmath15-e_t[\\psi_2(s_2)\\frac{\\partial s_2}{\\partial { \\bf w}}]-e_t[\\frac{1}{j}\\frac{\\partial j}{\\partial { \\bf w } } ] \\label{gradient.eq}\\ ] ] rewriting ( [ mixture_model.eq ] ) in the vector form @xmath16 and considering @xmath17 as the independent variable and @xmath18 as the dependent variable ,",
    "we can write , using implicit differentiation @xmath19 which yields @xmath94 note that @xmath21 is the jacobian matrix of the mixing model . using ( [ gradient.eq ] ) and ( [ dsdw.eq ] ) , the gradient of the cost function @xmath95 with respect to the parameter vector @xmath17 is equal to ( see the appendix for the computation details ) @xmath96 $ } \\label{dldw.eq}\\end{aligned}\\ ] ] in practice , the actual sources and their density functions are unknown and will be replaced by the reconstructed sources , _",
    "i.e. _ by the outputs of the separating structure of fig [ model.fig ] , @xmath97 , in an iterative algorithm",
    ". the score functions of the reconstructed sources can be estimated by any of the existing parametric or non - parametric methods . in our work",
    ", we used a kernel estimator based on third - order cardinal splines . using ( [ dldw.eq ] ) ,",
    "the cost function ( [ finalcost.eq ] ) can be maximized by a gradient ascent algorithm which updates the parameters by the rule @xmath98 .",
    "the learning rate parameter @xmath99 must be chosen carefully to avoid the divergence of the algorithm .",
    "note that the algorithm does not require the knowledge of the explicit inverse of the mixing model ( direct separating structures ( [ inverse.eq ] ) ) .",
    "hence , it can be easily extended to more general polynomial mixing models .      considering ( [ mixture_model.eq ] ) , we can write + @xmath22 and @xmath23 , which implies , from ( [ dsdw.eq ] ) @xmath100 which yields @xmath25 $ } \\nonumber \\\\ \\frac{\\partial s_2}{\\partial { \\bf w}}=\\frac{1}{j}\\mbox{\\huge$[$}(l_2+q_2s_2)s_2 \\;,\\ ; ( 1-q_1s_2)s_1 \\ ; , ( l_2+q_2s_2)s_1s_2 \\;,\\ ; ( 1-q_1s_2)s_1s_2 \\mbox{\\huge$]$ } \\label{ds1s2dw.eq}\\end{aligned}\\ ] ] considering ( [ jacobian.eq ] ) @xmath101 $ } \\label{djdw.eq}\\end{aligned}\\ ] ] ( [ dldw.eq ] ) follows directly from ( [ gradient.eq ] ) , ( [ ds1s2dw.eq ] ) and ( [ djdw.eq ] ) ."
  ],
  "abstract_text": [
    "<S> an error occurred in the computation of a gradient in @xcite . the equations ( 20 ) in appendix and ( 17 ) in the text were not correct . </S>",
    "<S> the current paper presents the correct version of these equations . </S>"
  ]
}