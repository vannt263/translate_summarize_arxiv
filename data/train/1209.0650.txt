{
  "article_text": [
    "we are interested in evaluating the integral of a function @xmath3 over a compact domain .",
    "it is a simple matter to map any compact domain into the unit hypercube , so we need to evaluate @xmath4^d } dx\\,f(x)=\\int_0 ^ 1 dx_1\\cdots\\int_0 ^ 1 dx_d\\,f(x_1,\\ldots , x_d).\\ ] ] @xmath5 is just the average value of @xmath6 over a uniform probability distribution which vanishes outside the unit hypercube , we denote this average value by @xmath7 .    defining the sample average @xmath8 over a set of @xmath9 uniformly distributed random points @xmath10^d , i=1,\\ldots , d\\}$ ] to be @xmath11 the weak law of large numbers  @xcite allows the identification @xmath12 assuming only that the integral exists . strictly speaking",
    "the weak law of large numbers states that , with probability arbitrarily close to one , @xmath8 will become arbitrarily close to @xmath7 for sufficiently large  @xmath9 .",
    "the central limit theorem makes the stronger statement that the probability distribution of @xmath8 tends to a gaussian with mean @xmath7 and variance @xmath13 , @xmath14 where the variance of the distribution of @xmath6 values is @xmath15^d } dx\\,\\bigl(f(x ) -",
    "\\langle f\\rangle\\bigr)^2      = \\langle f^2\\rangle - \\langle f\\rangle^2,\\ ] ] but it requires stronger assumptions that we shall discuss shortly .",
    "an unbiased estimate of the variance is given by @xmath16 where of course @xmath17 the estimate of the integral @xmath8 is within one standard deviation @xmath18 of the true value @xmath19 about 68% of the time .",
    "note that the error is proportional to @xmath20 independent of the dimension @xmath0 of the integral . from this fact stems the great utility of monte carlo for integration in many dimensions compared to numerical quadrature .",
    "generally for numerical quadrature ( trapezoid rule , simpson s rule etc . )",
    "the error is @xmath21 where @xmath22 is the grid spacing and @xmath23 is a small number . with a fixed budget of function evaluations , @xmath9 , on a regular grid each axis",
    "must be divided into @xmath24 segments .",
    "so @xmath25 and thus the error is @xmath26 ; therefore in dimension @xmath27 the monte carlo error is smaller .",
    "an intuitive explanation for this is that a random sample is more homogeneous than a regular grid  @xcite .      if the integrand has a singularity within or on the boundary of the integration region extra care is required .",
    "let us consider the proof of the central limit theorem .",
    "the probability distribution @xmath28 for the values @xmath29 when @xmath30 is chosen from the distribution @xmath31  is @xmath32 for which @xmath33 we define the generating function for connected moments as the logarithm of the fourier transform of  @xmath28 @xmath34 assuming that can be expanded in an asymptotic series @xmath35 where coefficients @xmath36 ( cumulants ) @xmath37 are all finite .",
    "we consider the distribution function @xmath38 for the sample average @xmath8 defined in equation   @xmath39 and the corresponding generating function @xmath40 @xmath41 since the points @xmath42 were chosen independently factorises to give @xmath43^n      = nw_f\\left(\\frac kn\\right),\\ ] ] expanding which gives an ( asymptotic ) expansion in powers of @xmath44 @xmath45 ignoring terms of @xmath46 and taking the inverse fourier transform gives @xmath47 namely a gaussian with mean @xmath7 and variance  @xmath13 .",
    "however , if any of the moments @xmath48 are not finite then the expansion is not justified .",
    "a simple class of integrals with divergent higher moments is @xmath49 with @xmath50 .",
    "if @xmath51 this integral is well - defined but has an infinite number of divergent moments . following equation   @xmath52 for @xmath53 $ ] , and zero elsewhere .",
    "the cumulants are combinations of the moments @xmath54 which diverge as @xmath55 if @xmath56 , or equivalently every cumulant @xmath48 with @xmath57 diverges .",
    "what this means in practice is that although estimating such integrals by monte carlo is allowed  the weak law of large numbers assures us that as long as @xmath9 is large enough the estimate will converge but it gives no indication of how large @xmath9 should be  it is misleading to estimate the error from the variance alone .",
    "this is because the distribution of the estimates of the integral is not gaussian even if the variance exists . in practice one",
    "obtains non - gaussian distributions with `` fat tails '' , for some examples see figure [ fig : x0.5 ] .",
    "we may also observe that a singularity in the integrand does not necessarily lead to infinite cumulants : for the function @xmath58 a similar analysis to that given above shows that @xmath59 for @xmath60 and hence @xmath61 so all its cumulants are finite , and therefore the monte carlo estimates of the integral do have a gaussian distribution as the number of samples @xmath62 .",
    "for all convergent integrals monte carlo provides an estimate of the mean .",
    "however , if any moment diverges the distribution of the means is not gaussian , and in general even if the variance exists it gives an underestimate of the `` width '' of the probability distribution .",
    "this is often the case in practice , for example when evaluating feynman parameter integrals which are integrable but not square integrable .",
    "if the monte carlo integrator is treated as a black box the quoted error from the standard deviation will often be an underestimate of the true error . with these provisos on the applicability of monte carlo integration we now turn to our main topic , a new method of adaptive monte carlo integration .",
    "the monte carlo scheme of section 1 ( nave monte carlo ) can be improved by variance reduction schemes  @xcite : the basic idea is to use some information about the integral in order to reduce the variance sample average .",
    "we describe two methods : importance sampling and subtraction .",
    "let @xmath63 be a probability distribution , normalised to one , that closely approximates our function over the interval .",
    "if we generate random points @xmath30 chosen from the distribution @xmath64 and construct the average @xmath65 over these points then we have an estimate of the integral @xmath66 and the variance @xmath67 the value @xmath5 of does not depend on @xmath64 , but the corresponding variance of does , so we may minimise the variance by varying the function @xmath64 subject to the constraint that it is correctly normalised @xmath68 . performing the required functional differentiation with the lagrange multiplier  @xmath69 @xmath70}{\\delta\\rho(x)}=0\\ ] ] we obtain the result @xmath71 the corresponding value for the minimal variance is @xmath72 which vanishes if @xmath6 has the same sign everywhere in the domain of integration .      in this case",
    "we have @xmath73 + \\int dx\\,g(x ) ,    \\label{condef}\\ ] ] where the integral of @xmath74 is known exactly : the corresponding variance is @xmath75 ^ 2 .",
    "\\label{convdef}\\ ] ] as before the integral @xmath5 of is independent of @xmath74 whereas the variance is not , so we may minimise the latter by varying  @xmath74 .",
    "this time there is no constraint to be imposed will be a piecewise constant approximation to  @xmath6 , but we do not impose this as a constraint . ] and we find @xmath76 and the optimal variance @xmath77 . we therefore choose @xmath78 such that @xmath78 is a good approximation to  @xmath79 .    both of these methods assume knowledge of the function that one may not have for complicated multidimensional integrals that arise in practice , thus it is necessary to find a way to construct @xmath64 or @xmath74 from a function treated as a black box .",
    "the usual way to construct @xmath63 is the method of @xcite , vegas .",
    "the key feature of this algorithm is that it is adaptive , that is , it automatically constructs a probability density that approximates @xmath80 .",
    "our analysis of the variance reduction in  [ sec : is ] assumed that the function is representable as a product of functions in each variable , although the correctness of the global monte carlo estimate of the integral does not depend upon this .",
    "such a factorisation depends on the co - ordinate system , see @xcite : in the following we will always work in cartesian co - ordinates and assume that the function approximately factorises .",
    "each axis is divided into a number of bins and we select a point with each coordinate lying equiprobably in any bin along the corresponding axis ; the points are thus chosen to lie in equiprobably in any of the boxes that are defined by the intersection of the bins .",
    "the probability distribution @xmath64 in is @xmath81 where @xmath82 is the volume of the box @xmath83 containing  @xmath30 . to converge to the optimal grid",
    "the bins are resized so that each bin makes an equal contribution to the integral of @xmath80 .",
    "the analysis in @xcite assumes that the probability distribution @xmath84 factorises but does not explicitly assume that the integrand @xmath6 does . in this case the variance is @xmath85 and minimisation with respect to any particular @xmath86 subject to the normalisation constraint @xmath87 gives @xmath88 this gives the optimal solution for @xmath86 when all the other @xmath89 are fixed , but does not immediately give the optimal solution for all the factors of @xmath64 simultaneously .",
    "if we make the further assumption that @xmath90 then the solution reduces to that which we found in  [ sec : is ] , namely @xmath91 and hence @xmath92 .",
    "we may identify some issues with this approach .",
    "consider the optimal distribution @xmath93 that vegas is trying to find .",
    "@xmath94 of equation is not necessarily zero unless @xmath79 has the same sign everywhere .",
    "a simple example where @xmath95 is in one dimension with @xmath96 , which gives @xmath97 . in simple cases it is possible to divide up the integration region into parts",
    "in which the sign of the integrand does not change , however this assumes detailed knowledge of the function and may not be feasible in a high number of dimensions .",
    "compare this to the subtraction method , for which @xmath98 in this case .",
    "secondly , there seems to be no automatic method for deciding when vegas has converged to a ( near ) optimal grid .",
    "usually it is advised to use as few function evaluations as possible until the grid approximately converges and then sample on the optimal grid . to test this one",
    "computes the @xmath2 statistic .",
    "if on each iteration of vegas an independent estimate of the integral @xmath99 is produced using @xmath100 function evaluations ; after @xmath101 iterations the variables @xmath102 will have a normal distribution with mean zero and variance @xmath103 , assuming that @xmath100 is sufficiently large for central limit theorem to apply .",
    "therefore the sum of their squares @xmath104 has a @xmath2 distribution with @xmath101 degrees of freedom @xmath105 for @xmath106 .",
    "sadly , we can not compute this @xmath2 since it requires the _ exact _ values of @xmath5 and @xmath107 .",
    "if we use the estimate @xmath108 then it is easy to show that the resulting quantity @xmath109 still has a @xmath2 distribution but now with @xmath110 degrees of freedom .",
    "however , since @xmath103 occurs in the denominator of @xmath2 attempts to replace it with some stochastic estimate will tend to give anomalous large values for @xmath2 or , to make a more precise statement , the distribution will not be a @xmath2 distribution but something with fatter tails .    finally , we observe that the vegas rebinning algorithm will tend to produce small bins where the function value is large . if the integrand has a relatively flat but high plateau with steep edges this approach will tend to calculate the integral well in the flat region , where it is easy , and miss the regions of large variance which require more function evaluations to estimate accurately .",
    "we will see an explicit example in section  [ sec : numex ] .",
    "our algorithm uses two adaptations , subtraction and rebinning , and uses student s @xmath1-test as a robust trigger to decide when to apply them .    as in all such adaptive algorithms ,",
    "we trigger an adaptation whenever there is statistically significant evidence that the current histogram parameters are not optimal , and the nave goal is that the histogram parameters will converge to their optimal values thereby minimising the variance of our monte carlo estimate of the integral .",
    "we say `` nave '' because this clearly can not happen : we are constructing an ergodic markov process and such a process must converge to a fixed point distribution of histogram parameters which is non - zero everywhere  it can not converge to a single point in the parameter space . of course",
    ", we may hope that the markov process will converge to a distribution of histogram parameters strongly peaked about the optimal ones , but how well this may be achieved depends in a complicated way upon lots of the algorithmic details and approximations : for example , how many samples we have to take in each bin before using the central limit theorem to assert that their mean is normally - distributed , or how much we choose to damp the rebinning adaptations to try to avoid instabilities .",
    "we should put such worries in context , it is easy to see that for _ any _ adaptive monte carlo integration scheme there are functions for which they will give an answer with an unreliable estimate of its error .",
    "a simple example is an approximation to a @xmath111 function within a region where the function is zero : not only will monte carlo algorithms not `` see '' the @xmath111 function , but adaptive importance sampling will make it less likely for them to do so .",
    "we construct an approximation function @xmath112 adaptively from the monte carlo process .",
    "like @xcite we assume that the integrand may be reasonably approximated by a product of one - dimensional histograms with @xmath101 bins along each axis .",
    "this means we only have to accumulate @xmath113 rather than @xmath114 values , and more significantly we can afford to evaluate a reasonably large number of samples per bin whereas it would be impossible to evalute even one sample per box in practice for @xmath115 in @xmath116 dimensions for example .",
    "however , we stress that the this is not a requirement of our method : the idea of using adaptive subtractions as well as adaptive importance sampling is independent of the choice of histogram representation .",
    "subtraction could be used together with recursive stratified sampling @xcite for example , and the function approximation stored not as a product of histograms as we will describe below but as independent values in each box as required .",
    "we should also stress that the monte carlo algorithm will give the correct value for the integral even if the integrand is not well approximated by a product of histograms , or even as a product at all .",
    "all that happens in such cases is that we have no good reasons to expect our adaptations to significantly reduce the statistical error in the result .",
    "we describe a method closely following vegas , assuming that our function is representable as a product of one - dimensional functions .",
    "each axis is divided into a number of bins , with a bin along an axis defined to be the cartesian product of an interval along that axis and the unit interval along every other axis .",
    "the intersection of @xmath0 bins , one along each axis , will be called a box .",
    "we generate quasi - random points for the monte carlo sampling by choosing a random permutation of the bins and a random point in each bin .",
    "this ensures that each bin has an equal number of samples while still sampling the entire space homogeneously .",
    "this implements the importance sampling distribution @xmath64 , as the probability of selecting a sample point @xmath30 within any box @xmath117 is @xmath118 , and therefore the probability density @xmath119 where @xmath120 is the volume of the box  @xmath117 .",
    "initially we choose all the histogram bins to have equal width , and therefore all the boxes to have the same volume and therefore @xmath63 equal everywhere , for want of any better information .",
    "likewise , we initially set the subtraction function @xmath121 everywhere .",
    "we generate a set @xmath122 of quasi - random points chosen from the distribution @xmath64 , and for each bin @xmath123 along each axis @xmath124 we accumulate the quantities @xmath125 for @xmath126 .",
    "@xmath127 is just the number of samples in the bin @xmath128 , so with our quasi - random number strategy for generating sample points it is just equal to the integer @xmath129 for all bins , so we do not really need to accumulate it , although we might if we used a different random point generator .",
    "we define @xmath130 where @xmath131 means that @xmath42 falls within @xmath128 , that is the @xmath132 coordinate @xmath133 of the point @xmath42 lies in the interval @xmath128 along the @xmath132 axis .",
    "if @xmath6 is a product of one - dimensional functions , @xmath134 then @xmath135 moreover , if the average value of @xmath136 in the bin @xmath128 is @xmath137 with @xmath138 being the bin width , then @xmath139 where @xmath140 is the integral we wish to evaluate .",
    "we thus may construct an estimate of the value of the function @xmath79 as @xmath141 where @xmath142 for @xmath143 , latexmath:[$v_b = |\\beta_1|\\cdots    containing  @xmath30 , and @xmath145 is the current `` global '' estimator for the integral .",
    "we use this function for our subtraction adaptation , that is we take @xmath146 in  , and evaluate @xmath147      = \\int \\rho(x)dx\\,\\frac{f(x)-\\hat f(x)}{\\rho(x ) }    \\label{eqn : iprime}\\ ] ] by monte carlo .",
    "the integral of @xmath112 is known exactly @xmath148 whence we can use and set @xmath149 .      after making a subtraction as described in section  [ sec : sub ]",
    "the integrand is everywhere approximately zero to the best of our current knowledge , so the usual importance sampling analysis of section  [ sec : is ] does not give us any useful way of choosing the function  @xmath64 .",
    "we may therefore choose @xmath64 so as to make the variance constant over all boxes , which in our case corresponds to making it constant for all bins along each axis .",
    "we introduce an estimator for the variance with the bin @xmath128 @xmath150 and our rebinning step adjusts the bin parameters to make the integral of this histogram constant within each new bin .",
    "an interpolation is performed along each axis to get the value of the approximation function in the new bins .",
    "the interpolation uses cubic splines to fit the current function approximation in the current set of bins : cubic splines perform better for some of our test integrals than lower - order interpolation , although there is no intrinsic reason to prefer them . when the new set of bins have been found the value of the interpolation in the centre of that bin",
    "is used for @xmath151 . as with importance sampling",
    "it is possible that the algorithm will rebin too much and narrow the bins very strongly about regions of high variance , thus we damp the rebinning algorithm by adding a small constant variance to each bin so that no bin can have width zero .",
    "the second part of   tells us how to make a subtraction even after changing the importance sampling @xmath64 : we just keep the old bins to evaluate @xmath112 ( perhaps packaging them in a closure ) while generating points according to the new bins specified by @xmath64 .",
    "the value of @xmath112 for the next iteration is accumulated in the new bins .",
    "interpolation was easier to use in our current implementation and allows us to combine estimates from different iterations and avoid starting again after each rebinning .",
    "if our two adaptations have achieved their goal then the estimates @xmath152 of @xmath153/\\rho(x)$ ] within bin @xmath128 should have mean zero and equal variance , and if we average enough samples within each bin to be able to apply the central limit theorem then they should follow a gaussian distribution . even though we do not know the exact value of the variance , we can test this hypothesis without any further approximations by computing the quantity @xmath154 where @xmath155 is the average of @xmath156 over all bins and @xmath157 an unbiased estimate of the variance of @xmath158 .",
    "@xmath1  has a student @xmath1-distribution with @xmath159 degrees of freedom , where student s distribution with @xmath160 degrees of freedom is @xmath161    let @xmath162 where @xmath163 is the inverse cumulative student distribution , so we expect that @xmath164 with probability @xmath165",
    ". therefore , if @xmath166 we may exclude the hypothesis that we have gaussian distributed bin values with mean zero and the same variance with probability @xmath165 and carry out a new adaptation .",
    "this should give a more stable condition for convergence towards the optimal approximation function .",
    "each axis could have a separate trigger , but there does not seem to be any obvious advantage , just as there seems to be no reason to have a different number of bins @xmath101 along each axis .    the rebinning algorithm is a markov process that is hopefully converging to the `` optimal '' bin distribution .",
    "however , as we stated before , an ergodic markov process _ can not _ converge to a delta function , so our automated rebinning procedure can not find the optimal integration parameters ( the bin widths and subtraction values ) . in practice",
    "this manifests itself as an instability in the rebinning algorithm .",
    "this is not a problem specific to our method but is shared by all automatic rebinning schemes ; we hope and expect that our use of student s test as a trigger will be more stable that those that use a @xmath2 trigger , but to make this into a more quantitative statement would require at least a significant restriction on the class of integrands under consideration .",
    "to demonstrate the procedure of adaptive subtraction and how it compares to adaptive importance sampling , we use the c language implementation of vegas importance sampling in the gnu scientific library ( gsl ) for comparison  @xcite .",
    "we first choose an example that illustrates the strengths of our method , @xmath167 with @xmath9 chosen to make the integral @xmath168 .",
    "the integrand is approximately constant throughout most of the integration region except at the very edges where it rises rapidly .",
    "figure [ fig : edge](a ) shows the integrand after subtraction with the new bins , chosen such that the variance in each is equal .",
    "the integration grid is finest in regions where the function is changing rapidly , this is as it should be , integrating regions where the integrand is approximately constant requires far fewer function evaluations .",
    "compare to part  ( b ) of the figure which shows the bin distribution produced by importance sampling .",
    "the bin distribution has moved to where the function itself is largest .",
    "we integrate , @xmath169^d } dx_1\\cdots dx_d\\,t(x_1) ... t(x_d )    \\label{tst1}\\ ] ] for various values of @xmath0 and plot the relative error in figure  [ fig : tanh_dim ] .",
    "this figure shows the log of the standard error over the integral approximation using importance sampling and our subtraction method .",
    "we use @xmath170 evaluations , with the number of bins chosen in both cases so that there are @xmath171 or @xmath172 calls per bin .",
    "this gives four opportunities to recalculate the bin distribution ( rebin ) where we apply student s test to decide . in almost every case , with @xmath173 probability",
    "only one rebinning is necessary .",
    "we also check the @xmath2 for the importance sampling case the same number of times and rebin if the @xmath2 per degree of freedom of the integral estimates differs from one by more than  @xmath174 .",
    "this tends to rebin on every step .",
    "we could combine all the estimates , weighted by their standard deviation however this sometimes underestimates the error . instead the number we quote comes from @xmath175 calls on the bin distribution calculated by the process above .",
    "figure  [ fig : tanh_dim ] shows for this example our algorithm gives more accurate results .",
    "for the integral in @xmath176 dimensions we plot the value of the @xmath1 statistic in figure [ fig : chistu](a ) , compared with @xmath177 for @xmath178 and @xmath179 , ( @xmath180 calls per iteration , @xmath172 calls per bin ) for a number of iterations .",
    "we see that student s test triggers only on the first iteration and then not after , the approximation function is doing as good a job as can be expected .",
    "the closest equivalent for importance sampling is the @xmath2 per degree of freedom which we show in figure [ fig : chistu](b ) .",
    "this is far from @xmath181 at every iteration and the rebinning algorithm triggers at every step .",
    "we could relax the condition on the @xmath2 , e.g. , @xmath182 to rebin less often but this example is indicative of both methods , student s test triggers relatively infrequently and the @xmath2 test triggers more frequently , meaning we are less certain when we have found the optimal bin distribution .",
    "another test integrand is , @xmath183 in one way this is an excellent test case since the integrand oscillates and so implies that even with optimal binning importance sampling can not reduce the variance to zero .",
    "however this example shows a distressing feature of our algorithm .",
    "as equation shows when calculating an estimate for the function in each bin we have to divide by the total integral , if this is zero there will be problems in @xmath184 . in practice",
    "it is unlikely that we have to integrate a function whose integral is exactly zero .",
    "the root of this problem is that our knowledge of the integrand is represented as a product of histograms , so if the integral of the histogram along any axis vanishes we have no information about how the integrand behaves along the other axes .    if the value of the subtraction function in every box were stored this would not be an issue .",
    "we have tried this approach , which is feasible for small dimensions and small number of bins , on this integrand . in @xmath185 dimensions with @xmath186 function evaluations to accumulate the function approximation , an additional @xmath186 to evaluate the integral and @xmath187 bins per axis we accumulated the approximation in each of the @xmath188 boxes to obtain the estimates @xmath189 so we do indeed find that subtraction reduces the variance significantly .",
    "the integral @xmath169^d } dx_1\\cdots dx_d \\prod_i^n \\sqrt{\\frac m\\pi }      \\exp\\left(-m(x_i-\\half ) ^2\\right )       = \\left[\\erf\\left(\\frac{\\sqrt m}2\\right)\\right]^d    \\label{tst3}\\ ] ] is another interesting case .",
    "when the parameter @xmath190 is small , the gaussian is broad and our integration scheme approximates the true value well .",
    "however as the dimension increases our method fails to find the peak of the gaussian as well as vegas and the integration starts to break down .",
    "the subtraction method can be improved by increasing the number of points per bin , however it still does slightly worse than vegas .",
    "this is possibly due to the interpolation performed during the rebinning step failing to reconstruct the approximation function .",
    "there is no reason for subtraction to be better in this case and we find , for a fixed number of calls as the dimension increases and therefore , the number of bins per axis reduces for this function subtraction gets worse more quickly than importance sampling .",
    "importance sampling will do better than subtraction in some cases and vice versa therefore combining the two approaches may be beneficial .",
    "we can do subtraction for some percentage of our total number of function evaluations , generating an approximation function and then do importance sampling on @xmath191 for the remainder . or we could do an initial run of importance sampling , then construct the approximation function on the bins with width given by  @xmath64 .",
    "figure [ fig : exp2 ] shows the integral in various dimensions using both varieties of mixed sampling .",
    "the mixed sampling is done by using @xmath192 of the function evaluations on one kind of sampling and then using the grid and approximation function found this way for the rest of the calls .",
    "the mixed strategies seem to give good results reproducing the superiority of subtraction in low dimensions and the stability of importance sampling in higher .    as an interesting practical example we perform the integral derived from the feynman parameterization of the scalar feynman diagram shown in figure  [ fig : phi ] in @xmath176-dimensional euclidean space , and given explicitly in a file attached to this paper",
    ".     graph .",
    "the distribution is non - gaussian in both cases with values differing significantly from the mean occuring far more often than if it were gaussian  the `` fat tail '' of the distribution.,scaledwidth=75.0% ]    this integral is known to converge ( its exact value is @xmath193 ) but it is not necessarily square integrable and its higher moments may not exist .",
    "we plot the distribution obtained from evaluating the integral @xmath194 times by subtraction and importance sampling .",
    "the distribution looks distinctly non - gaussian .",
    "it is peaked around the exact value but were the standard deviation used to estimate the error it would be incorrect .",
    "this kind of problem with parameter integrals arises often in practice and we advise caution in interpreting the output of any monte carlo calculation .",
    "we have reviewed monte carlo integration , pointing out some often overlooked issues when the integral exists but the variance or some higher moments do not .",
    "this leads to non - gaussian distributions and inaccurate estimates of the error .",
    "we have proposed a new algorithm for numerical monte carlo integration based on adaptive subtraction .",
    "we expect this method to have an advantage over importance sampling monte carlo schemes for some integrands .",
    "we have implemented our proposed adaptive subtraction method and found it to be better than importance sampling in some cases .",
    "a combination of the two methods can be better than either separately .",
    "we have given an explicit program , panic which is competitive with vegas .",
    "panic also shares the assumption of factorisability into a product of functions with vegas , thus improvements to vegas that work around this assumption , e.g. ,  @xcite , will also work with subtraction and are an interesting future direction .",
    "the subtraction idea can work with any monte carlo scheme and the subtraction function can even be constructed independently , in this paper we have seen good results when combined with importance sampling .",
    "our code , panic , is included .",
    "adk would like to thank benny lautrup , who wrote one of the seminal papers in the field @xcite , for useful discussions and for the acronym  panic ."
  ],
  "abstract_text": [
    "<S> this paper investigates a class of algorithms for numerical integration of a function in @xmath0 dimensions over a compact domain by monte carlo methods . </S>",
    "<S> we construct a histogram approximation to the function using a partition of the integration domain into a set of bins specified by some parameters . </S>",
    "<S> we then consider two adaptations ; the first is to subtract the histogram approximation , whose integral we may easily evaluate explicitly , from the function and integrate the difference using monte carlo ; the second is to modify the bin parameters in order to make the variance of the monte carlo estimate of the integral the same for all bins . </S>",
    "<S> this allows us to use student s @xmath1-test as a trigger for rebinning , which we claim is more stable than the @xmath2 test that is commonly used for this purpose . </S>",
    "<S> we provide a program that we have used to study the algorithm for the case where the histogram is represented as a product of one - dimensional histograms . </S>",
    "<S> we discuss the assumptions and approximations made , as well as giving a pedagogical discussion of the myriad ways in which the results of any such monte carlo integration program can be misleading .    </S>",
    "<S> # 1([#1 ] ) # 1 # 2    # 1#2 # 1/#2 o </S>"
  ]
}