{
  "article_text": [
    "the belle experiment@xcite is the kek @xmath3-factory project with an asymmetric e@xmath4e@xmath5 collision to explore cp violation in @xmath3 meson system .",
    "the data - taking stared from june 1999 , and the accelerator ( kekb ) @xcite gradually improved its performance .",
    "the integrated luminosity logged by the belle detector has reached 120 fb@xmath0 in march 2003 .",
    "this corresponds to the fact that more than 120 m @xmath1 pairs have been recorded in our tape storage .",
    "this is obviously the largest @xmath1 data sample at the @xmath2 energy region in the world .",
    "the kekb accelerator is still updating their luminosity records thanks to excellent operations .",
    "the integrated luminosity per day has been approaching 500 pb@xmath0 .    from the computing model",
    "s point of view , a large data sample is a challenging issue for cpu power as well as for storage .    in daily event processing , we have to process beam data without any delay for online data acquisition .",
    "to do so , we need enough cpu power and a stable dst production .",
    "furthermore , we have to reprocess the entire data sample whenever we have a major update of reconstruction codes as well as calibration constants . here",
    ", the whole event data have to be reconstructed from rawdata using the same version of the codes and constants to controll the systematic errors in user analyses .    considering the large amount of accumulated beam data , it is unrealistic for users to make their analyses in an efficient way using the whole data sets .",
    "thus , the belle data processing also incorporates the data skimming , leading to reduced data files based on a first event selection and called physics skims .    for the monte carlo ( mc ) data ,",
    "we require large amount of statistics , at least 3 times larger than beam data to evaluate systematic effect related to the detector acceptance , event reconstruction and so on .    in this paper",
    ", we first introduce the belle computing software(section [ sect : software_tools ] ) and system(section [ sect : belle_computing_system ] ) including pc farms and its upgrade(section [ sect : pc_farms ] ) . then , in section [ sect : dst_production ] , we will explain the scheme of the dst production / reprocessing and mention how mc events have been produced(section [ sect : mc_production ] ) . finally summary and future plan will be given .",
    "we describe here the core software of event processing which is based on `` home - made '' tools .",
    "( belle analysis framework ) is a unique framework for us from daq to final user analyses .",
    "it is composed by a set of modules written in c++ , each one having at least the following structure : a beginning of run , an event processing and an end of run function .",
    "histograming is also included in the structure of modules .",
    "these modules are compiled as shared objects and are dynamically plugged in when b.a.s.f .",
    "runs as shown in figure  [ fig : event_flow ] .",
    "this framework utilizes event - by - event parallel processing on smp using a fork function .",
    "the data transfer between modules and i / o is managed by panther .",
    "this is based upon a bank system composed of tables .",
    "the contents of tables are defined in ascii format before loading modules and user has to include this file in the code as header files . in each tables",
    ", one can pick up any value corresponding to each attribute and pointer which relates one table to the other .",
    "this contains compression capability using zlib utility .",
    "panther is only a data management system in the belle experiment . from rawdata to user analyses , any stage of events can be consistently handled only with panther .",
    "the typical event size is 35 kb for rawdata and for 60 kb for reconstructed dst data .",
    "the dst data contains a lot of information of results from each detector analyses .",
    "the quantity of information stored in these dst files is usually too large for a standard physics analysis . to reduce the data size",
    ", we produce compact format of the dst data(``mini - dst '' ) , where the size is 12 kb per hadronic event .",
    "the detector calibration constants , used to process data , are stored in a postgresql@xcite database . at kek ,",
    "we have two database servers and one is mirrored to the other . in each institution",
    ", there is a database server of which the substance is periodically imported from the kek main server .    the event flow of dst production is schematically shown in figure  [ fig : event_flow ] .",
    "each step like calibration and unpacking is performed by a set of modules which are loaded on the b.a.s.f . platform .",
    "the reconstructed data are transfered in the panther format between different modules . at the end , the data are compressed .    the update of the library is usually done a couple of times in a year . in the last year",
    ", we had a major update in april , where the tracking software for low momentum region was improved .",
    "once a new library is released , all events taken so far are again processed ( reprocessing ) from rawdata in order to produce new dst and mini - dst file .",
    "the simulation is performed with geant3 packages@xcite by adding an interface to b.a.s.f .",
    "then , mc data follow the same treatment as the real data , including the detector calibration constants database . for every significant library changes , as in spring 2002 , the mc data samples are generated using the same version of basf library than real data .",
    "figure  [ fig : belle_computing ] is an overview of our computing system which consists of the three principal networks . the first one is connected between computing servers for batch jobs and the pc farms ( described in the next section ) for dst / mc productions .",
    "this system is composed of 38 sun hosts as computing machines linked to pc farms via a gigabit ethernet switch ( `` computing network '' ) .",
    "these sun hosts are operated under the lsf batch qeueing utility .",
    "20 sun hosts out of 38 are equipped with 2 dtf2 tape drives each and are connected to the sony tape robotic library of 500 tb capacity .",
    "the pc farms are used for the dst / mc productions .",
    "rawdata from the belle detector are sent to an online tape server linked to the tape drives , by which rawdata are written onto a dtf2 tape .",
    "the second network is used for user analyses and data storage system .",
    "the 9 work group servers are connected to a swiching device of a gigabit ehternet called `` internal network '' .",
    "the hierarchy mass storage ( 120 tb capacity ) with the 4 tb staging disk in addition to the 8 tb file servers are also connected .",
    "the other network switch ( `` user network '' ) links between the work group servers and 100 user pc s .",
    "everybody can use each pc for her / his analysis , or login to one of the work group servers and analyze data interactively . for the batch system",
    ", user can submit batch jobs to the computing servers from the work group servers .",
    "the cpu s are summarised in table  [ tb : belle - cpu ] .",
    ".summary of cpu s for the belle computing system . [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     the failure rate of the reprocessing is tabulated in table  [ tb : fail_rate ] .",
    "the main trouble comes from the comminication error among pc hosts , where possible improvement of the signal handling of each pc could be expected to reduce this error .",
    "however , our total error rate is still small enough and the whole comuputing system has been efficiently working .",
    "the mc data consists of 3 type of physics events of @xmath6 , @xmath7 and continuum .",
    "we produce these mc data using the real beam conditions , like beam pipe background and interaction point(ip ) position .",
    "these conditions depending on the run number , we called this procedure a `` run - by - run '' production of mc data . moreover , as it was already mentionned , we generate mc data with 3 times larger statistics than real data .",
    "figure  [ fig : mc_status ] represents how we have produced mc events in 2002 .",
    "major update of the simulation and reconstruction software was done in april , and since then we have started the mc production . in october , a minor change of the simulation code in order to match to the modification of the beam triggering scheme was made .",
    "the pc farms were shared between the dst and the mc production and the allocation of the cpu power can be easily modified according to the situation .        for the mc production ,",
    "the computing resources at remote sites have been actively used .",
    "the upper figure  [ fig : mc_remote ] indicates that total cpu power at remote sites is about 300 ghz .",
    "this is comparable to that of kek availble for the mc production .",
    "the mc events produced outside kek amounts to 44 % of all of events produced(shown in figure  [ fig : mc_remote ] ) .",
    "these mc data are basically sent to kek via network and are saved in the disk . in case that the disk does not have enough space , mc events were copied onto dtf2 tape instead .",
    "the mc data tapes are released in the tape library and then user can access these data in batch jobs .",
    "the typical size corresponds to around 6 tb in 6-months mc production .",
    "more remote institutions are expected to join in producing the mc events this year .",
    "the belle computing system has been operated in an efficient way so that we have successfully reprocess more than 250 fb@xmath0 of real beam data so far . for the mc data , event samples 3 times larger than beam data",
    "have been produced at kek and at remote sites .",
    "a higher luminosity @xmath3-factory machine at kek ( superkekb ) is being proposed as an upgrade plan of the present experiment@xcite .",
    "this plan aims to achieving @xmath8 @xmath9sec@xmath0 or more luminosity , corresponding to a data size of 1 pb for 1-year operation . to handle this amount of data",
    ", we may have to introduce grid technology , for instance , for efficient usage of resources at remote sites .",
    "the new computing model for the superkekb experiment will be presented in letter of intent submitted in the end of 2003 .      9 belle collaboration , a.  abashian _ et al_. , nucl .",
    "instr . and meth .",
    "a*479 * , 117(2002 ) , http://belle.kek.jp/. e.  kikutani(ed . ) , kek preprint 2001 - 157(2001 ) to appear in nucl .",
    "instr . and meth . a , http://www - kekb.kek.jp/. r.  brun _ et al_. , geant3.21 cern report no.dd/ee/84-1(1987 ) . .",
    "the science information network dedicated to academic usages in japan . for detail , see http://www.sinet.ad.jp/. i.  abe _ et al_. , expression of interest in a high luminosity upgrade of the kekb collider and the belle detector , 2002 january ."
  ],
  "abstract_text": [
    "<S> we describe the offline computing system of the belle experiment , consisting of a computing farm with one thousand ia-32 cpus . up to now </S>",
    "<S> , the belle experiment has accumulated more than 120 fb@xmath0 of data , which is the world largest @xmath1 sample at the @xmath2 energy . </S>",
    "<S> the data have to be processed with a single version of reconstruction software and calibration constants to perform precise measurements of @xmath3 meson decays . </S>",
    "<S> in addition , monte carlo samples three times larger than the real beam data are generated . to fullfill our computing needs , </S>",
    "<S> we have constructed the computing system with 90(300 ) quad(dual ) cpu pc servers from multiple vendors as a central processing system . </S>",
    "<S> the details of this computing system and performance of data processing with the current model are presented . </S>"
  ]
}