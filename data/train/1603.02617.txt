{
  "article_text": [
    "object registration is an important task in computer vision that determines the translation and the rotation of an object with respect to a reference coordinate frame . by utilizing such a task",
    ", one can propose promising solutions for various problems related to scene understanding , augmented reality , control and navigation of robotics , _",
    "etc_. recent developments on visual depth sensors and their increasing ubiquity have allowed researchers to make use of the information acquired from these devices to facilitate the registration .",
    "+ when the target point cloud is cleanly segmented , iterative closest point ( icp ) algorithm @xcite , point - to - model based methods @xcite and point - to - point techniques @xcite demonstrate good results . however , the performances of these approaches are severely degraded by the challenges such as heavy occlusion and clutter , and similar looking distractors . in order to address these challenges ,",
    "several learning based methods formulate occlusion aware features @xcite , derive patch - based ( local ) descriptors @xcite or encode the contextual information of the objects with simple depth pixels @xcite and integrate into random forests .",
    "particularly , iterative random forest algorithms such as latent - class hough forest ( lchf ) @xcite and iterative multi - output random forest ( imorf ) @xcite obtain the state - of - the - art accuracy on pose estimation .",
    "on the other hand , these methods rely on scale - invariant features and the exploitation of the rich discriminative information that is inherently embedded into the scale - variability is one important point overlooked .",
    "+        [ fig1 ]    unlike the aforementioned learning - based methods , novatnack _ et al . _",
    "@xcite utilize the detailed information of the scale variation in order to register the range images in a coarse - to - fine fashion .",
    "they extract and match conventional salient 3d key points .",
    "however , real depth sensors have several imperfections such as missing depth values , noisy measurements , foreground fattening , _",
    "etc_. salient feature points tend to be located on these deficient parts of the depth images , and hence , they are rather unstable @xcite . in such a scenario ,",
    "3d reconstruction methods that provide more reliable shape information can be utilized @xcite .",
    "implicit b - splines ( ibs ) @xcite are yet other techniques that can provide shape descriptors through their zero - sets and reconstruct surfaces .",
    "these techniques are based on the locally controlled functions that are combined via their control points and this local control allows patch - based object representation .",
    "+ our architecture is originated from these observations .",
    "we integrate the coarse - to - fine registration approach presented in @xcite into the random forests @xcite using the histogram of the control points ( hocp ) that we adapt from recently introduced ibss @xcite .",
    "we train our forest only from positive samples and learn the detailed information of the scale - variability during training .",
    "we normalize every training point cloud into a unit cube and then generate a set of scale - space images , each of which is separated by a constant factor .",
    "the patches extracted from the images in this set are represented with the scale - variant hocp features . during inference , the patches centred on the pixels that belong to the background and foreground clutters are removed iteratively using the most confident hypotheses and the test image is updated .",
    "since this removal process decreases the standard deviation of the test point cloud , subsequent normalization applied to the updated test image increases the relative scale of the object ( foreground pixels ) in the unit cube .",
    "more discriminative descriptors ( control points ) are computed at higher scales and this ensures further refinement of the object pose .",
    "note that we employ a compositional approach , that is , we concurrently detect the object in the target region and estimate its pose by aligning the patches in order to increase robustness across clutter .",
    "figure [ fig1 ] depicts a sample result of our architecture . to summarize ,",
    "our main contributions are as follows :    * to the best of our knowledge this is the first time we adapt an implicit object representation , implicit b - spline , into a `` scale - variant '' patch descriptor and associate with the random forests .",
    "* we introduce a novel iterative algorithm for the hough forests : it finds out an initial hypothesis and improves its confidence iteratively by extracting more discriminative `` scale - variant '' descriptors due to the elimination of the background / foreground clutter .",
    "a large number of studies have been proposed for the object registration , ranging from the point - wise correspondence based methods to the learning based approaches .",
    "iterative closest point ( icp ) algorithm , originally presented in @xcite , requires a good initialization in order not to be trapped in a local minimum during fine tuning .",
    "this requirement is reduced in @xcite providing globally optimal registration by the integration of a global branch - and - bound ( bnb ) optimization into the local icp .",
    "the point - wise correspondence problem is converted into a point - to - model registration in @xcite .",
    "the object model is represented with implicit polynomials ( ip ) and the distance between the test point set and this model is minimized via the levenberg - marquardt algorithm ( lma ) .",
    "the study @xcite that utilizes 3d ips for 6 dof pose estimation on ultrasound images is further extended in @xcite by a coarse - to - fine ip - driven registration strategy .",
    "the point - to - point techniques build point - pair features for sparse representations of the test and the model point sets @xcite .",
    "_ align two noisy point clouds of real scenes by finding correct point - to - point correspondences between the point feature histograms ( pfh ) and feed this alignment to an icp algorithm for fine tuning @xcite .",
    "the votes of the matching features are accumulated in @xcite to hypothesise the poses of the cluttered and partially occluded objects .",
    "_ @xcite propose point - pair features for both rgb and depth channels and they are conducted in a voting scheme to hypothesise the rotation and translation parameters of the objects in the cluttered scenes . despite achieving good registration results , these techniques underperform when the scenes are under heavy occlusion and clutter , and the target objects geometry are indistinguishable from background clutter .",
    "+ learning - based methods have good generalization across severe occlusion and clutter @xcite .",
    "the state - of - the - art accuracy on registration is acquired by the iterative random forest algorithms , particularly @xcite and @xcite , which form a basis for our _ iterative hough forest _ architecture .",
    "tejani s patch - based strategy @xcite refines the initially hypothesised object pose by iteratively updating the object class distributions in the leaf nodes during testing .",
    "iterative multi output random forest ( imorf ) @xcite jointly predicts the head pose , the facial expression and the landmark positions .",
    "the relations between these tasks are modelled so that their performances are iteratively improved with the extraction of more informative features . whilst these approaches rely on the scale - invariant features to improve the confidence of a pose hypothesis , inspiring by @xcite , we design scale - variant features getting more discriminative with the increase in the scale .",
    "novatnack et al .",
    "@xcite introduce a framework that registers the range images in a coarse - to - fine fashion by utilizing the detailed information provided by the scale variation .",
    "the shape descriptors with the coarsest scale are matched initially and a rough alignment is achieved since fewer features are extracted in coarser scales .",
    "the descriptor matching at higher scales results improved predictions of the pose .",
    "in this section we detail our registration approach by firstly describing the computation procedure of the hocp features .",
    "we then present how to encode the discriminative information of these scale - variant features into the forest .",
    "finally , we demonstrate how to exploit the learnt shape information in a coarse - to - fine fashion to refine the pose hypotheses .",
    "we demonstrate the computation procedure of the hocp features over a positive depth image selected from the training dataset .",
    "it is initially normalized into a unit cube and then new point clouds at different scales are sampled as follows : @xmath0 with @xmath1 where @xmath2 $ ] is the world coordinate vector of the original foreground point cloud , @xmath3 is the mean of @xmath4 , @xmath5 $ ] is the normalized foreground pixels , @xmath6 is the number of the scales , @xmath7 is the scale factor and @xmath8 is the scale . the constant @xmath9 takes real numbers to generate the point clouds at different scales , starting from @xmath10 that corresponds to the initial normalization . a training image and its samples at different scales",
    "are shown in fig .",
    "[ fig2 ] ( a ) . +        once we generate a set of scale - space images ( fig .",
    "[ fig2 ] ( a ) ) , we represent these point clouds with the control point descriptors first globally .",
    "the descriptor computation procedure is the same as presented in @xcite .",
    "the unit cube is split into an @xmath11 voxel grid where @xmath12 is the ibs resolution .",
    "each descriptor @xmath13 is defined with an index - weight pair : the index number indicates the vertex of this grid at which the related control point is located .",
    "the weight informs the descriptor significance about the control of the geometry to be represented .",
    "the scale - space images in fig .",
    "[ fig2 ] ( a ) are globally represented in fig .",
    "[ fig2 ] ( b ) .",
    "we next partition the global representation at each scale into patches .",
    "we express the patch size @xmath14 in image pixels and it is a constant that depicts the ratio between the sizes of the extracted patch and the bounding box of the global point cloud . a window with the specified patch size is traversed in the unit cube of each scale - space image and the patches are extracted around non - zero pixels .",
    "each patch has its own implicit volumetric representation , formed by the closest control points to the patch center , the ones lying inside the window along depth direction .",
    "the patches sampled at different scales in fig .",
    "[ fig2 ] ( c ) represent the same shape .",
    "however , their volumetric descriptions ( green ) are getting more discriminative as the scale increases , since the greater number of descriptors are computed at higher scales .",
    "we encode this discriminative information into histograms in spherical coordinates .",
    "each of the patch centres is coincided with the center of a sphere .",
    "the control points of the patch are described by the log of the radius @xmath15 , the cosine of the inclination @xmath16 and the azimuth @xmath17 .",
    "then , the sphere is divided into the bins and the relation between the bin numbers @xmath18 and the histogram coordinates @xmath19 is given as follows @xcite : @xmath20 where @xmath21 and @xmath22 are the radii of the nested spheres with the minimum and the maximum volumes , @xmath23 are the cartesian coordinates of each descriptor with radius @xmath24 .",
    "@xmath22 equals to the distance between the patch center and the farthest descriptor of the related patch .",
    "the numbers of the control points in each bin are counted and stored in a @xmath25 dimensional feature vector @xmath26 .",
    "the volumetric descriptions in fig .",
    "[ fig2 ] ( c ) are shown with their related histograms .",
    "thus , the sample shape ( patch ) is represented with the scale - variant hocp features .",
    "the proposed iterative hough forest is the combination of randomized binary decision trees .",
    "it is trained only on foreground synthetically rendered depth images of the object of interest .",
    "we generate a set of scale - space images from each training point cloud and sample a set of annotated patches @xmath27 as follows @xcite : @xmath28 where @xmath29 is the patch centre in pixels , @xmath30 is the 3d offset between the centres of the patch and the object , @xmath31 is the rotation parameters of the point cloud from which the patch @xmath32 is extracted and @xmath33 is the depth map of the patch .",
    "+ each tree is constructed by using a subset @xmath34 of the annotated training patches @xmath35 .",
    "we randomly select a template patch @xmath36 from @xmath34 and assign it to the root node .",
    "we measure the similarity between @xmath36 and each patch @xmath37 in @xmath34 as follows :    * * depth check : * the depth values of the descriptors @xmath38 and @xmath39 that represent the patches @xmath37 and @xmath36 are checked , and the spatially inconsistent ones in @xmath38 are removed as in @xcite , generating @xmath40 that includes the spatially consistent descriptors of the patch @xmath37 . * * similarity measure : * using @xmath40 , the feature vector @xmath41 is generated and the @xmath42 norm between this vector and @xmath43 is measured : @xmath44 * * similarity score comparison : * each patch is passed either to the left or the right child nodes according to the split function that compares the score of the similarity measure @xmath45 and a randomly chosen threshold @xmath46 .",
    "the main reason why we apply a depth check to the patches is to remove the structural perturbations , due to occlusion , clutter @xcite .",
    "these perturbations most likely occur on the patches extracted along depth discontinuities such as the contours of the objects of interest .",
    "they cause to diverge a test patch ( occluded / cluttered ) from its positive correspondence by changing its representation , @xmath22 of the sphere , and the histogram coordinates consequently .",
    "+ a group of candidate split functions are produced at each node by using a set of randomly assigned patches @xmath47 and thresholds @xmath48 .",
    "the one that best optimize the offset and pose regression entropy @xcite is selected as the split function .",
    "each tree is grown by repeating this process recursively until the forest termination criteria are satisfied .",
    "when the termination conditions are met , the leaf nodes are formed and they store votes for both the object center @xmath49 and the object rotation @xmath50 .",
    "the proposed architecture registers the object of interest in two steps : the _ initial registration _ and the _ iterative pose refinement_. the _ initial registration _ roughly aligns the test object and this alignment is further improved by the _ iterative pose refinement_. + consider an object that was detected by a coarse bounding box , @xmath51 , as shown in the leftmost image of fig .",
    "[ fig3 ] ( a ) . at an iteration instant @xmath52 ,",
    "the following quantities are defined :    * @xmath53 : the history of the object position .",
    "* @xmath54 : the history of the object rotation .",
    "* @xmath55 : the history of the inputs ( noise removals ) applied to the test image .",
    "* @xmath56 : the history of the set of the feature vectors where @xmath57 . * @xmath58 : the object scale ( the scale of the foreground pixels ) in the unit cube at iteration @xmath52 ( see eq . [ eq2 ] ) .",
    "we formulate the _ initial registration _ as follows : @xmath59        we find the best parameters that maximize the joint posterior density of the initial object position @xmath60 and the initial object rotation @xmath61 .",
    "this initial registration process is illustrated in fig .",
    "[ fig3 ] ( a ) .",
    "the test image is firstly normalized into a unit cube . unlike training , this is a `` single '' scale normalization that corresponds to @xmath10 ( see eq .",
    "[ eq1 ] ) .",
    "the patches extracted from the globally represented point cloud are described with the hocp features and passed down all the trees .",
    "we determine the effect that all patches have on the object pose by accumulating the votes stored in the leaf nodes as in @xcite and approximate the initial registration given in eq .",
    "[ eq8 ] . once the initial hypothesis @xmath62",
    "is obtained , the pixels that belong to the background / foreground clutter @xmath63 are removed from @xmath51 according to the following criterion : @xmath64 with @xmath65 where @xmath66 and @xmath67 are the depth maps of the hypothesis @xmath68 at iteration @xmath52 , and of the @xmath51 , @xmath69 and @xmath70 are the scaling coefficients .",
    "the efficacy of @xmath71 is illustrated in fig .",
    "[ fig3 ] . in the rightmost image of fig .",
    "[ fig3 ] ( a ) , the test image and the initial hypothesis are overlaid .",
    "this hypothesis is exploited and the test image is updated by @xmath72 as in eq .",
    "this updated image is shown in fig .",
    "[ fig3 ] ( b ) and assigned as input for the @xmath73 iteration .",
    "it is normalized and represented globally .",
    "note how the object `` scale '' ( @xmath74 ) in the unit cube is relatively increased and more discriminative descriptors @xmath75 are extracted ( compare with the initial registration ) .",
    "this is mainly because of that the standard deviation of the input image is decreased since we removed foreground / background clutter .",
    "the resultant hypothesis of the @xmath73 iteration is shown on the right .",
    "the extraction of more discriminative descriptors and the noise removal process result more accurate and confident hypothesis .",
    "this pose refinement process is iteratively performed until the maximum iteration is reached ( see fig .",
    "[ fig3 ] ( c ) ) : @xmath76 we approximate the registration hypothesis at each iteration by using the stored information in the leaf nodes as we do in the initial registration .",
    "we have analysed the icvl dataset @xcite and have found that the `` coffee cup '' and the `` camera '' are some of the best demonstrable objects to test and compare our registration architecture with the state - of - the - art methods since they are located in highly occluded and cluttered scenes .",
    "we further process the test images of these objects to generate a new test dataset according to the following criteria :    * since the hocp features are scale - variant , the depth values of the training and the test images should be close to each other up to a certain degree . in this study , we train the forests at a single depth value , @xmath77 mm , and test with the images at the range of @xmath78 $ ] mm . *",
    "the test object instances located at the range of @xmath78 $ ] mm are assumed as detected by coarse bounding boxes ( see fig .",
    "[ fig1 ] ) .",
    "the image regions included in these bounding boxes are cropped and the new test dataset is generated ( @xmath79 `` coffee cup '' and @xmath80 `` camera '' rgbd test images ) .",
    "the maximum depth is @xmath81 and the number of the maximum samples at each leaf node is @xmath82 for each tree .",
    "every forest is the ensemble of @xmath83 trees with these termination criteria .",
    "our experiments are two folds : _ intraclass _ and _ interclass_. both experiments use the metric proposed in @xcite to determine whether a registration hypothesis is correct .",
    "this metric outputs a score @xmath84 that calculates the distance between the ground truth and estimated poses of the test object .",
    "the registration hypothesis that ensures the following inequality is considered as correct : @xmath85 where @xmath86 is the diameter of the 3d model of the test object and @xmath87 is a constant that determines the coarseness of an hypothesis that is assigned as correct .",
    "we set @xmath87 to @xmath88 in the intraclass and interclass experiments .",
    "these experiments are performed on the `` coffee cup '' dataset to determine the optimal parameters of the proposed approach .",
    "the effect of the patch size @xmath14 is firstly examined by setting the ibs resolution @xmath12 to @xmath89 , the hocp feature dimension @xmath90 to @xmath91 in addition to the previously defined forest parameters .",
    "we test the patch sizes @xmath92 .",
    "the resultant precision - recall ( pr ) curves are shown in fig . [ fig4 ] ( a ) .",
    "when we increase the patch size until it is @xmath93 times of the bounding box , the registration performance is improved since the greater patches can encode more discriminative shapes .",
    "we continue to extend the patch size till it is @xmath94 times of the bounding box and observe that the performance is degraded since these patches tend to contain the noisy parts of the scene . according to this figure and their corresponding f1 scores ( see table [ table_params ] ) ,",
    "we choose @xmath95 as the optimal patch size .",
    "+ by using the selected patch size , we next tune the ibs resolution @xmath12 and the hocp feature dimension @xmath90 .",
    "we test the combinations of @xmath96 and @xmath97 , the ones that are the most applicable @xmath98 pairs to represent @xmath95 patch size .",
    "the pr curves of these combinations are depicted in fig .",
    "[ fig4 ] ( b ) and the corresponding f1 scores are illustrated in table [ table_params ] .",
    "we take into account both the memory consumption and the accuracy , and agree on the values of @xmath99 & @xmath100 .",
    "patch size and set @xmath99 , @xmath100 .",
    "for the corresponding f1 scores , see table [ table_params].,height=172 ]    .,height=96 ]    the last parameter we test in the intraclass experiments is the iteration number .",
    "we test several _ iterative hough forests with histogram of control points _ each of which has @xmath101 and @xmath102 iterations , respectively .",
    "their pr curves are shown in fig .",
    "[ fig4 ] ( c ) .",
    "as expected , the forests that use greater number of iterations show better performances ( see table [ table_params ] ) since more discriminative features are extracted thanks to the noise removal process .",
    "these experiments are conducted on the `` coffee cup '' and the `` camera '' datasets to compare our approach with the state - of - the - art methods including the latent - class hough forests ( lchf ) @xcite trained separately on the color gradient ( lchf - rgb ) , the surface normal ( lchf - depth ) and the color gradient + the surface normal ( lchf - rgbd ) features . in order to make a fair comparison between methods , we train and test these versions of the lchf by using the authors software .",
    "the forest parameters are the same as our own approach .",
    "+ according to the f1 scores in table [ table_comparison ] , we observe that the lchf trained on the color gradient features underperforms other methods .",
    "the main reason of this underperformance is the distortion along the object borders arising from the occlusion and the clutter , that is , the distortion of the color gradient information in the test process .",
    "when we train the lchf by only using the depth information , we infer that the surface normals outperform the color gradients .",
    "the combined utilization of the color gradients and the surface normals in the lchf produces approximately the same results as the lchf - depth .",
    "our approach with the iterative pose refinement outperform other methods .",
    "regarding the camera object , we observe that the registration performances of all methods are relatively decreased .",
    "this is mainly because of that this object has large amount of missing depth pixels in addition to severe occlusion and clutter .",
    "figure [ fig6 ] illustrates several qualitative results of our approach on the camera and the coffee cup objects .",
    ".f1 scores determined for different patch sizes , ibs resolution ( n ) & feature dimension ( d ) and number of iteration [ cols=\"^,^,^,^ \" , ]     [ table_comparison ]",
    "in this study , we have proposed a novel architecture , _ iterative hough forest with histogram of control points _ , for 6 dof object registration from depth images .",
    "we have introduced the histogram of the control points , a scale - variant patch representation , and have encoded their rich discriminative information into the random forests .",
    "we train our forest using only the positive samples . during testing",
    ", we first roughly align the object and then iteratively refine this alignment .",
    "the experimental results report that our approach show better registration performance than the state - of - the - art methods . in the future",
    ", we plan to engineer a variable patch size approach and integrate it into the proposed iterative hough forest architecture for further exploitation of the rich discriminative information provided by the hocp features .",
    "b.  zheng , r.  ishikawa , j.  takamatsu , t.  oishi , and k.  ikeuchi , `` a coarse - to - fine ip - driven registration for pose estimation from single ultrasound image '' , _ computer vision and image understanding _ , 2013 ,",
    "vol.117 , no.12 , ( pp .",
    "1647 - 1658 ) .",
    "s.  hinterstoisser , v.  lepetit , s.  ilic , s.  holzer , g.  bradski , k.  konolige , and n.  navab , `` model based training , detection and pose estimation of texture - less 3d objects in heavily cluttered scenes '' , _ accv _ , 2012 ."
  ],
  "abstract_text": [
    "<S> state - of - the - art techniques proposed for 6d object pose recovery depend on occlusion - free point clouds to accurately register objects in 3d space . to reduce this dependency , </S>",
    "<S> we introduce a novel architecture called _ iterative hough forest with histogram of control points _ that is capable of estimating occluded and cluttered objects 6d pose given a candidate 2d bounding box . </S>",
    "<S> our _ iterative hough forest _ is learnt using patches extracted only from the positive samples . </S>",
    "<S> these patches are represented with _ </S>",
    "<S> histogram of control points ( hocp ) _ , a `` scale - variant '' implicit volumetric description , which we derive from recently introduced implicit b - splines ( ibs ) . the rich discriminative information provided by this scale - variance is leveraged during inference , where the initial pose estimation of the object is iteratively refined based on more discriminative control points by using our _ iterative hough forest_. we conduct experiments on several test objects of a publicly available dataset to test our architecture and to compare with the state - of - the - art . </S>"
  ]
}