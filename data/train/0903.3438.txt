{
  "article_text": [
    "mixed level orthogonal arrays ( oas , for short ) are fundamental to experimental design . each row of an array",
    "is thought of as a run of an experiment ; each entry of the row is the value of a parameter of the system being tested .",
    "the goal of the experiment is to test as wide a range of parameter values of the system as possible . the number of parameters and which values these parameters can take ( i.e. , the row length and the alphabets where the row entries take their values ) are determined by the characteristics of the system being tested .",
    "the remaining parameters of an oa are its number of rows @xmath0 and its strength @xmath1 .",
    "the strength of an oa is @xmath1 when the oa is capable of exploring all possible interactions of up to @xmath1 number of parameters of the system ( see definition [ d : oa ] ) .",
    "@xmath0 is the number of experiments that the oa describes . a high @xmath1 and a low @xmath0 is desirable .",
    "the rao bound ( see below ) , first proved for fixed level orthogonal arrays by rao @xcite , gives a lower bound on @xmath0 in terms of @xmath1 , the row length and the system parameters ( i.e. , the row length and alphabet sizes ) .",
    "our first object of study is this bound and the goal is to develop algorithms that compute exactly and approximately the right side of this bound .",
    "the rao bound is a necessary bound , all oas satisfy it .",
    "there are also sufficient bounds that arise from constructions .",
    "one well known construction method for ordinary oas is by taking the dual of error correcting codes @xcite .",
    "@xcite generalizes this idea by defining error - block codes , which are error - correcting codes in which one can specify what alphabet to be used for each entry of the code word .",
    "furthermore , @xcite notes that the duals of error block codes are mixed level orthogonal arrays .",
    "we use this idea and construction of error block codes in @xcite to obtain construction of orthogonal arrays whose parameters satisfy a gilbert - varshamov ( gv ) type bound ( see below ) . our second object of study is this bound .    in subsection [ ss : complexity ] we calculate the computational complexity of directly computing the rao bound and the gv bound . we see that this complexity is polynomial in the strength parameter , and the degree of the polynomial is one more the number of different type of alphabets used in the oa . if many different types of alphabets are used in an oa , which is typical in real life experimental designs , the rao and the gv bounds become inefficient to compute directly from their original representations and .",
    "this potentially high complexity of the direct computation of these bounds justifies the construction of new algorithms to compute them . in the present paper",
    "we develop three algorithms for this purpose . the simple result that underlies these is an expectation representation of the rao and the gv bounds that we derive in sections [ s : exp ] and [ s : gv ] .",
    "the expectation is that of a function of a random walk whose increments are either @xmath2 or @xmath3 with equal probability .",
    "the walk takes @xmath4 steps , the row length of the oa , and accumulates a cost throughout its excursion as follows : if the walk goes up at the @xmath5 step , the accumulated cost increases by a factor one less the alphabet size of the @xmath5 factor of the oa .",
    "the aforementioned representation is the expectation of this accumulated cost over sample paths which are less than @xmath6 at the last step of the random walk for the rao bound and less than @xmath7 for the gv bound .",
    "once these expectation representations are available , it is straightforward to use them in several ways to obtain algorithms to compute the bounds .",
    "the markov property of the underlying walk gives the recursive formula .",
    "the complexity of this formula is a second order polynomial in the strength parameter and is far less than the original formulas when the number of alphabets is large .",
    "the asymptotic behavior of bounds such as the rao and the gv bounds is a basic question to ask .",
    "@xcite carries out an asymptotic analysis of the gv bound for orthogonal arrays with two alphabets . to our knowledge , no results concerning the asympotic behavior of either the gv or the rao bound for general mixed level orthogonal arrays is available in the current literature . with our expectation representation",
    "an asymptotical analysis of these bounds becomes what is called a large deviations analysis ( ld ) in probability theory and we use the methods of the ld theory to carry it out . in section [ s : ld ] we use the stochastic optimal control approach to ld @xcite to show that the right side of the rao bound grows exponentially in the row length @xmath4 and identify the growth rate .",
    "following @xcite , we use a relative entropy representation of our expectation of interest to write it as a discrete time stochastic optimal control problem . under proper",
    "scaling , this control problem converges to a limit deterministic calculus of variations problem .",
    "similar to @xcite , the connection between the prelimit and the limit problems is established using the hamilton jacobi bellman ( hjb ) equation associated with the limit problem ( see section [ s : ld ] for the rao bound and in section [ s : gv ] for the gv bound ) .",
    "this analysis provides our second approximation algorithm . to the authors s knowledge the idea of using the limit hjb equation to compute large deviation limits first appeared in @xcite in the context of analysis of queuing systems .",
    "the asymptotic analysis gives good approximations in an exponential scale .",
    "more accurate approximations can be obtained using simulation , which is possible because we have the expectation representations and . however , these are expectations over sets with small probability ( i.e. , rare ) for reasonable values of the strength parameter @xmath1 . for such expectations ,",
    "ordinary simulation would require a great number of samples for reliable estimates .",
    "a remedy to this is importance sampling , which means to change the sampling distribution to a distribution under which the set over which expectation is taken is not rare anymore .",
    "one modifies the estimator accordingly by multiplying it with a likelihood ratio to account for the change of the sampling distribution .",
    "is is a well known idea , it goes back at least to 1949 , see , for example , @xcite , and the references therein .    for our problem",
    ", an importance sampling distribution will be one under which with high probability our random walk remains below @xmath7 or @xmath6 at its final step .",
    "there are many such distributions . among these",
    ", one would like to choose a distribution that minimizes the variance of the is estimator .",
    "it is well known that an exact solution of this optimization problem is as difficult as directly computing the expectation @xcite . in situations such as the one covered in this article where the object of study is a sequence of expectations decaying or growing exponentially in a parameter , a compromise is to choose a sequence of estimators whose variance decay or grow exponentially at a rate twice the asymptotic decay or growth rate of the expectation itself . such a sequence",
    "is called asymptotically optimal , see @xcite and @xcite and the references therein . to obtain such a sequence",
    "we will follow @xcite and represent the variance minimization problems in is once again as a sequence of stochastic optimal control problems . under proper scaling , these also converge to the same limit control problem as the one that emerges in the large deviations analysis .",
    "theorem [ t : optimal ] asserts that a simple change of measure based on a piecewise linear subsolution of the hjb equation of the limit control problem is asymptotically optimal .",
    "this idea of using subsolutions to construct is algorithms is from @xcite and is called the subsolution approach to is .",
    "the use of randomized algorithms for counting is one of the central ideas in statistics .",
    "the use of importance sampling for this purpose seems to be relatively new .",
    "@xcite is the first article that we are aware of that uses importance sampling for purposes of counting .",
    "more recent articles since this work include @xcite .",
    "the current work seems to be the first to use the subsolution method to construct asymptotically optimal is algorithms for counting .",
    "the plan of the paper is as follows .",
    "the next section gives the definition of an orthogonal array and states the rao and the gv bounds .",
    "it computes the computational complexity of the original combinatorial representation of these bounds .",
    "section [ s : exp ] derives the expectation representation of the rao bound and states the exact recursive algorithm to compute it ( equation ) .",
    "section [ s : ld ] carries out the large deviations analysis of the expectation representation of the rao bound .",
    "the final result here is theorem [ t : convergence ] with characterizes the growth rate of the bound as a finite dimensional concave maximization problem .",
    "the dimension of the problem is the number of alphabets used in the oa .",
    "section [ s : is ] uses the ideas in the above paragraphs to construct an asymptotically optimal is algorithm to estimate the rao bound , the final result is theorem [ t : optimal ] .",
    "section [ s : gv ] does for the gv bound what was done for the rao bound in sections [ s : ld ] and [ s : is ] .",
    "this generalization requires only minor modifications .",
    "section [ s : numerical ] provides numerical results that gives evidence that the constructed algorithms are effective in practice as well .",
    "we begin with the following definition from @xcite .    [",
    "d : oa ] a matrix @xmath8 is said to be an @xmath9 if it has the following structure :    1 .",
    "@xmath8 has @xmath0 rows , 2",
    ".   row length of @xmath8 is @xmath10 ; the first @xmath11 components of each row are from the alphabet @xmath12 the second @xmath13 components from @xmath14 , ... , the last @xmath15 components from @xmath16 3 .",
    "take any @xmath1 columns @xmath17 of @xmath8 and call the matrix formed by these columns @xmath18 .",
    "take any string @xmath19 of length @xmath1 such that @xmath20 letter of @xmath19 comes from the alphabet corresponding to column @xmath21 . count the number times @xmath19 occurs as a row of @xmath18 .",
    "this count is the same for all @xmath19 .",
    "the last item is the orthogonality property and @xmath1 is the _ strength _ of the orthogonal array .",
    "this type of arrays are called mixed level because the columns are allowed to be from different alphabets ( second property above ) .",
    "the parameters of any mixed level orthogonal array has to satisfy the rao bound : @xmath22 this bound corresponds to the sphere packing bound for error block codes . for @xmath23",
    "was proved in @xcite , for the proof of the general case see @xcite .",
    "the duality idea mentioned in the introduction and block error code constructions implied by theorem 3.1 in @xcite give mixed level orthogonal arrays whose parameters satisfy the following conditions : @xmath24 where @xmath25 is a prime power , @xmath26 this is a sufficient bound ; that is , it is known that oa s with these parameters do exist .",
    "bounds like are called gilbert - varshamov type bounds in coding theory .",
    "the right side of has essentially the same structure as that of .",
    "the key difference between these bounds is the upper limit of the outer sum : goes up to @xmath6 whereas goes up to @xmath7 .    in the next subsection we will study the computational complexity of directly evaluating or .",
    "it follows from their definitions that the evaluations of and have the same computational complexity .",
    "therefore , it is enough to consider one of them .",
    "the right side of involves a partitioning of each @xmath27 less than @xmath6 into a sum of @xmath28 integers .",
    "the number of such partitions is @xmath29 then the number of operations needed to compute the right side of is bounded below by @xmath30 where @xmath31 is a constant that depends only on @xmath28 .",
    "if the strength parameter @xmath1 grows linearly in @xmath4 , i.e. , if @xmath32 , where @xmath33 , a direct computation of requires @xmath34 operations .",
    "the present paper is aimed at finding methods to compute and more efficiently .",
    "the next section presents a simple probabilistic representation of , which forms the basis for all of the results and algorithms presented in this paper .",
    "let @xmath35 be independent and identically distributed ( iid ) bernoulli random variables with @xmath36 let @xmath37 define the following `` running cost : '' @xmath38 can be written in the form @xmath39 =   { \\mathbb e}\\left [ 1_{\\{s_n \\le t/2\\ } } \\prod_{j=1}^{n } 2r(x_j , j ) \\right].\\ ] ] this is an expectation over the trajectories of @xmath40 that stay below the level @xmath6 at step @xmath4 .",
    "at each step the random walk accumulates a running cost @xmath41 ; the cost depends on the step number and the current step .",
    "the random walk can be thought of as a scan of the letters of a row of the array . at each step",
    "we flip a coin to decide whether the current letter will be included in the computation .",
    "if the decision is yes , i.e. , if @xmath42 and the random walk goes up , then the current bound is multiplied with @xmath43 where @xmath44 is the alphabet size of the letter we are going over ( this is the @xmath45 term in ) . the first sum in group trajectories according to their positions at step @xmath4 . for a position @xmath46 , the second sum in partition these @xmath27 up - steps into different cost regions and the binomial coefficients count the number of possible ways @xmath47 up - steps",
    "can be taken in @xmath48 steps .",
    "[ [ a - simple - recursive - algorithm - to - compute - the - rao - bound ] ] a simple recursive algorithm to compute the rao bound + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    our first method to compute is a recursive algorithm that computes the bound exactly . for integers @xmath49 and @xmath50 define @xmath51.\\ ] ]",
    "the rao bound in terms of @xmath52 is @xmath53 because @xmath35 are iid and @xmath54 are their sum , @xmath52 satisfies @xmath55 for @xmath56 and @xmath57 .",
    "in addition , we have the boundary conditions @xmath58 for @xmath59 and @xmath60 for @xmath61 .",
    "these give an algorithm that takes only @xmath62 steps to compute the rao bound .",
    "if we write the strength parameter @xmath1 as a fraction @xmath63 of @xmath4 as @xmath32 then the complexity analysis in the previous chapter implies that the direct evaluation of will take at least @xmath34 operations . whereas the computation of the same bound using will only take @xmath64 operations",
    "the goal of this section is an asymptotic analysis of the right side of as @xmath65 . in order for this analysis to be meaningful",
    "@xmath1 and @xmath66 need to grow with @xmath4 as well .",
    "therefore we assume that @xmath67 the asymptotic analysis of now consists of evaluating @xmath68.\\ ] ]    for the evaluation of , we will follow @xcite and begin by representing the @xmath69 $ ] term in it as a discrete time stochastic optimal control problem as follows .    [",
    "p : relentrep ] the following identity holds : @xmath70 \\\\ & ~~= \\sup _ { \\mbox { \\begin{minipage}{2 cm }          \\begin{center}\\tiny   $ \\bar{p}(\\cdot|\\cdot,\\cdot)$\\\\ ~\\\\ $ \\bar{p}(s_n\\le\\frac { \\mu n}{2 } ) = 1$\\notag          \\end{center } \\end{minipage } } }   \\hspace{-0.5 cm } \\bar{\\mathbb e}\\left [ \\sum_{j=1}^n \\log r(x_j , j ) - \\log \\bar{p}(x_{j}|j , s_{j-1})\\right],\\end{aligned}\\ ] ] where the @xmath71 is over all transition probabilities @xmath72",
    "@xmath73 @xmath74 $ ] that give the probability of the steps @xmath2 and @xmath3 given the current position of and the current step number of the random walk @xmath75 and @xmath76 is the probability distribution defined by these measures on the path space of the random walk .",
    "the proof of this result is similar to that of proposition 1.4.2 @xcite and is omitted .",
    "the sup on the right side of over all markov chains on the sample paths of @xmath40 such that the @xmath77 step is less than @xmath6 with probability @xmath3 .",
    "the @xmath78 term inside the sup corresponds to the entropy of @xmath79 .",
    "define @xmath80 and @xmath81 and let @xmath82 be the entropy function . as we observed earlier",
    ", the right side of is a stochastic optimal control problem . upon dividing it by @xmath4 and scaling the time and space parameters with @xmath83 , and sending @xmath4 to @xmath84",
    "one obtains the following limit deterministic optimal control problem : @xmath85dt,\\ ] ] where the @xmath71 is over all measurable functions on @xmath74 $ ] with values in @xmath74 $ ] such that @xmath86 .",
    "the rigorous connection between this optimal control problem and can be established in several ways .",
    "for example , one can use the weak convergence approach of @xcite .",
    "another approach is via the hjb equation associated with the limit control problem and a verification argument , which is followed in @xcite . in this paper",
    "we will take this second path because the same method will also allow us to prove the asymptotic optimality of an is estimator based on a subsolution of the limit hjb equation .      for @xmath87",
    ", @xmath88 is a strictly concave function with no @xmath1 dependence",
    ". then jensen s inequality implies that the optimal trajectory needs to be a straight line between times @xmath89 and @xmath90 .",
    "therefore , it is enough to consider the optimization problem over piecewise linear continuous paths and the @xmath71 in equals @xmath91 where the @xmath71 is subject to @xmath92 the objective function of this finite dimensional constrained optimization problem is strictly concave and its constraints linear .",
    "therefore , a straightforward use of a lagrange multiplier converts the problem to a one of root finding of a one dimensional monotone function .    in the next subsection",
    "we will prove that a function defined based on satisfies an hjb equation .",
    "we will use this fact to prove the convergence of to .",
    "one obtains the following proposition by ordinary calculus and implicit differentiation .",
    "[ p : regularity ] @xmath102 is smooth except for @xmath122 where it has directional derivative @xmath123 which is defined as @xmath124 .",
    "higher order @xmath1 partial derivatives similarly exists .",
    "in particular for any @xmath1 and @xmath115 we have : @xmath125 where @xmath126 .",
    "now we state the hjb equation satisfied by @xmath102 .",
    "[ t : hjb ] the following dynamic programming equation holds : @xmath127 } \\",
    "{ \\tilde{r}(t ) \\theta + h(\\theta ) + v_x(x , t ) \\theta + v_t(x , t ) \\}\\ ] ] for @xmath128 .",
    "take @xmath128 , a small @xmath129 and @xmath130.$ ] implies @xmath131 \\delta .",
    "\\end{aligned}\\ ] ] because @xmath132 and @xmath133 exist , dividing both sides of the last display by @xmath134 and letting @xmath135 gives : @xmath136 because this is true for all @xmath130 $ ] we have : @xmath137 } \\",
    "{ \\tilde{r}(t ) \\theta + h(\\theta ) +   v_x(x , t ) \\theta + v_t(x , t ) \\}\\ ] ] one replaces @xmath138 with @xmath139 by taking @xmath140 to be the optimal control @xmath141      in this subsection we formally connect the sequence of stochastic optimal control problems in to the limit control problem and its solution developed in the previous subsection .",
    "figure [ f : limit ] gives the level curves of @xmath120 and @xmath142 where @xmath143\\ ] ] for @xmath144 , @xmath145 , @xmath146 , @xmath147 and @xmath148 this figure suggests that @xmath149 for all values of @xmath150 .",
    "our main convergence theorem , which we state and prove next , concerns the special case when @xmath151 .",
    "[ t : convergence ] the large deviations limit in equals @xmath98 , i.e. , @xmath152 =   \\sup   \\left\\ { \\sum_{i=1}^\\sigma   a_i\\left ( \\theta_i \\log ( s_i -1 )   + h(\\theta_i ) \\right)\\right\\},\\ ] ] where the @xmath71 is over @xmath153    the proof will be a verification argument using @xmath102 and the hjb equation . by proposition [ p : relentrep ]",
    "there exists @xmath154 such that @xmath155      =      \\bar{\\mathbb e}\\left [ \\sum_{j=1}^n r(x_j , j ) -       \\log \\bar{p}^n(x_j|j , s_{j-1 } ) \\right ] + \\epsilon(n)\\ ] ] where @xmath156 and @xmath157 is expectation with respect to @xmath158 .      letting @xmath4 go to infinity yields @xmath160",
    "\\\\ & \\ge \\limsup \\frac{1}{n}\\log   { \\mathbb e}\\left [ 1_{\\{s_n \\le t_n/2 \\ } } \\prod_{j=1}^n 2r(x_j , j)\\right ] -\\epsilon.\\end{aligned}\\ ] ]    for the reverse inequality we first note that the result of the optimization in is continuous in the strength parameter @xmath63 which appears in the constraint .",
    "let @xmath161 be the optimizers of when the @xmath63 in is replaced with @xmath162 where @xmath163 is a small constant .",
    "let @xmath164 and @xmath165 be the measure on the path space of @xmath166 corresponding to @xmath167 .",
    "we would like to use @xmath165 on the right side of to get a lower bound on its left side .",
    "once this is done the law of large numbers would give us the bound we desire .",
    "the only problem is @xmath168 so @xmath165 is not included in the set of measures over which the right side of is optimized .",
    "this is a minor technical problem and can be handled as follows . by definition @xmath169",
    "is iid for @xmath170 .",
    "therefore the ordinary law of large numbers is applicable and gives : @xmath171 then , the fact that @xmath172 is not a major problem and can be dealt with by simply conditioning it on @xmath173 .",
    "the expectation representation of the rao bound brings to mind the possibility of estimating it using simulation . because the strength parameter @xmath1 is usually a fraction of @xmath4 and because the aforementioned expectation is over the set @xmath183 , for large values of @xmath4 a direct simulation would require too many samples of @xmath184 to converge .",
    "one can instead use _ importance sampling _ , which means to sample from a new simulation measure under which @xmath183 is not rare .",
    "the samples are scaled by the radon nikodym derivative of the original measure with respect to the new sampling measure so that the simulation algorithm still estimates the probability under the original measure .",
    "the main problem in is is the choice of the new sampling distribution .",
    "one tries to choose it so that it is practical to sample from it and that it nearly minimizes estimator variance . in the next subsection",
    "we briefly introduce the main ideas of is in a general setting before we focus on its use in our current setup .    is is a well known method for estimating small probabilities , a very partial list of articles and books on the subject are @xcite .",
    "these works contain many more references to important works on the subject .",
    "take a probability space @xmath185 and a measurable integrable function @xmath186 suppose @xmath187 is a probability measure on @xmath188 with respect to which @xmath189 is absolutely continuous .",
    "we have the following basic identity : @xmath190 = \\int_{\\omega } f(\\omega)dp(\\omega ) =   \\int_{\\omega } f(\\omega ) \\frac{dp}{d\\hat{p}}(\\omega ) d\\hat{p}(\\omega ) = \\hat{\\mathbb e}\\left [ f \\frac{dp}{d\\hat{p } } \\right],\\ ] ] where @xmath191 is the radon nikodym derivative of @xmath189 with respect to @xmath187 and @xmath192 [ @xmath193 is expectation with respect to @xmath189 [ @xmath187 ] .",
    "the identity suggests the following simulation algorithm to compute @xmath195 $ ] .",
    "simulate iid copies @xmath196 , @xmath197 ,  , @xmath198 of @xmath199 from @xmath187 and use the following to estimate @xmath195 $ ] : @xmath200 by the law of large numbers @xmath201 $ ] which by equals @xmath195 $ ] .",
    "furthermore by the linearity of the expectation and one also has @xmath202 = \\hat{\\mathbb e}[\\hat{f}(1)]= { \\mathbb e}[f].$ ] therefore @xmath203 is an unbiased estimator of @xmath195 $ ] that converges to this values as @xmath204 .",
    "this method of estimating @xmath195 $ ] is called importance sampling ( is ) .",
    "is is especially useful when @xmath205 is small .",
    "in such a case , ordinary monte carlo will require a large number of samples @xmath206 for a reliable estimate of @xmath195.$ ] one could choose @xmath187 so that @xmath207 is no longer small and hope to reduce the number of samples required for a good estimate . a simple way to choose @xmath187 so that this happens is to minimize the variance of the is estimator @xmath203 .",
    "because @xmath203 is unbiased its variance depends on @xmath187 only through its second moment which equals @xmath208/n$ ] . here",
    "@xmath0 is the number of samples used in the estimation and is taken to be a constant .",
    "therefore , for a good is estimator one tries to solve the following optimization problem : @xmath209 =   \\inf_{\\hat{p } } \\hat{\\mathbb e } \\left [ \\left(f \\frac{dp}{d\\hat{p } } \\right)^2 \\right ] & =   \\inf_{\\hat{p } } { \\mathbb e } \\left [ f^2 \\frac{dp}{d\\hat{p } }   \\right ] , \\intertext{where the $ \\inf$ is over all $ \\hat{p}$ with respect to which $ 1_{\\{f \\neq 0\\ } } dp$ is absolutely continuous . the exact solution to",
    "this problem turns out to have an easy description .",
    "it is simple to prove that $ d\\hat{p}^ * = \\frac{f } { \\mathbb{e}[f ] } dp$ is actually the minimizer of \\eqref{e : generalisinf } and hence we have that } & = { \\mathbb e } \\left [ f^2 \\frac{dp}{d\\hat{p}^ * }   \\right]= ( { \\mathbb e}[f])^2 . \\notag\\end{aligned}\\ ] ] then @xmath210)^2/n$ ] is the smallest possible second moment for an is estimator which uses @xmath0 samples and the estimator defined by @xmath211 has zero variance .      as is well known in the is literature , @xmath211 is not a practical simulation measure because knowing it requires knowing @xmath195 $ ] which is the very quantity that is not known and whose estimation is sought .",
    "therefore , one usually seeks an almost minimizer of to conduct a good is simulation .",
    "if there is a sequence @xmath212 whose expectation is sought , one way to find almost minimizers to is to conduct an asymptotic analysis of the sequence of optimization problems given by and the sequence @xmath212 .",
    "if these converge in some sense to a relatively simple limit problem then the optimizers of this limit problem can inform the construction of almost minimizers to for the estimation of @xmath213 $ ] .",
    "one setup where such an asymptotic analysis is possible is when the underlying measure @xmath189 is that of a markov process and @xmath214 \\doteq \\gamma\\ ] ] exists and is nonzero .",
    "as the reader have already seen in the previous section , the problem in this article falls into this category .",
    "when the limit exists , one can define an asymptotic optimality condition for a sequence of is changes of measure as follows .",
    "jensen s inequality and the unbiasedness of @xmath215 implies @xmath216 \\ge \\liminf_n \\frac{2}{n }   \\log { \\mathbb e}[\\hat{f}(1)]\\equiv 2\\gamma.\\ ] ] in other words , the exponential growth rate of the second moment of any sequence of is samples is at least twice that of @xmath213.$ ] a sequence of is estimators is said to be _ asymptotically optimal _ if the lower bound is achieved , i.e. , if @xmath217 =      \\limsup_n \\frac{1}{n}\\log{\\mathbb e}\\left[f(1)^2\\frac{dp}{d\\hat{p}^*_n } \\right ] \\le 2\\gamma.\\ ] ]      in the context of estimating the expectation representation of the rao bound using is , @xmath212 in is @xmath218 where @xmath219 is the symmetric random walk with increments @xmath169 defined earlier .",
    "the reason is is necessary is because of the @xmath220 term . if we take @xmath221 with @xmath222 , as @xmath4 goes to @xmath84 the probability of @xmath184 being less than @xmath6 goes to @xmath2 exponentially . in order to simulate @xmath223 and @xmath75 using importance sampling",
    "one specifies a sampling distribution @xmath224 , @xmath225 and @xmath226 and simulates @xmath223 from this distribution as follows .",
    "one sets @xmath227 . at step @xmath27 of the simulation a random increment @xmath35",
    "is sampled from the distribution @xmath228 and sets @xmath229 .",
    "note that the distribution of the increment @xmath35 is allowed to depend on the current position of the random walk @xmath75 .",
    "let @xmath76 denote the probability measure on the sample paths of @xmath184 defined by the transition probability @xmath230 then the radon nikodym derivative @xmath231 equals @xmath232 .",
    "then , the is estimator of @xmath213 $ ] using @xmath233 sample paths is @xmath234 where @xmath235 denotes the @xmath236 independent sample path used in the simulation .",
    "the increments @xmath237 are iid copies of the increment process @xmath223 sampled from @xmath238 .",
    "then , by theorem [ t : convergence ] the optimality condition for the is estimator is @xmath239 \\le 2 v(0,0).\\ ] ]      in the next subsection we will show that a sampling distribution @xmath240 based on the large deviations analysis of the previous section satisfies , i.e. , is asymptotically optimal .",
    "it turns out that for the proof we wo nt need a complete asymptotic analysis of the is optimization problem .",
    "however , we include the following formal derivation of the limit optimization problem because it elucidates the direct connection between is and large deviations analysis .",
    "as the reader will see , this connection is very general and not limited to the current problem and has been known at least heuristically for a long time , see for example @xcite in the context of queuing networks .",
    "a more rigorous and clear connection has been established recently in @xcite .",
    "now we proceed with our formal derivation . for the present case ,",
    "the is optimization problem becomes @xmath241 .",
    "\\intertext{this equals } & \\inf_{\\hat{p } }   \\sup_{\\bar{p } : \\bar{p}(s_n \\le t_n / 2 ) }   \\bar{\\mathbb e}\\left [ \\sum_{j=1}^{n } 2\\log r(x_j , j ) - \\log \\hat{p}(x_j|j , s_j ) - \\log\\bar{p}(x_j| j , s_j ) \\right ] \\notag \\intertext{by a direct generalization of proposition \\ref{p : relentrep } to the present case .",
    "it can be shown that this expression is convex in $ \\hat{p}$ and concave in $ \\bar{p}$ and therefore the order of the $ \\inf$ and $ \\sup$ can be switched without effecting the result . once this is done the optimization in $ \\hat{p}$",
    "gives the optimizer $ \\hat{p}^ * = \\bar{p}$ and the problem reduces to } & \\sup_{\\bar{p } : \\bar{p}(s_n \\le t_n / 2 ) }   \\bar{\\mathbb e}\\left [ \\sum_{j=1}^{n } 2\\log r(x_j , j ) -   2\\log\\bar{p}(x_j| j , s_j ) \\right ] \\notag\\end{aligned}\\ ] ] and this is the same problem as in the representation except for a factor of @xmath242 .",
    "we know from the analysis of the previous section that when scaled by @xmath4 this problem converges to @xmath243 ds,\\ ] ] where the @xmath71 is over measurable @xmath244 such that @xmath245 this is the same as , again except for a factor of @xmath242 . finally , and as before , because @xmath246 is concave and independent of @xmath1 for @xmath247 the last problem reduces to @xmath248 where the @xmath71 subject to @xmath249 therefore the limit optimization problems for the large deviations analysis and importance sampling are the same modulo a factor of @xmath242 .",
    "in particular , the minimizers @xmath250 of are also the minimizers of .",
    "there are many asymptotically optimal is sampling measures to estimate .",
    "for example , one is @xmath167 of .",
    "the problem with this change of measure is that it requires the solution of at every step of the random walk @xmath184 . for large @xmath4",
    "this is inefficient .",
    "a much preferable situation is a fixed change of measure , i.e. , a change of measure @xmath238 that does nt depend on @xmath1 and @xmath115 . in the estimation of the rao bound",
    ", we expect such a change of measure to exist for two reasons 1 ) the underlying process is iid and one dimensional 2 ) the probability of interest concerns exit from a region with only one boundary point . for more on these points",
    "we refer the reader to @xcite and @xcite .",
    "let us now construct a fixed change of measure for our problem .",
    "let @xmath251 to be the unique minimizers of and define @xmath252 @xmath167 is almost fixed in the sense that it only depends on the step number @xmath253 and not on the position @xmath115 of the random walk @xmath184 .",
    "the dependence on @xmath253 is very intuitive and simple : each block of the orthogonal array has its own fixed jump probability @xmath254 , for the steps corresponding to the @xmath5 block one uses this fixed probability to sample the increments of @xmath184 .    before we state and",
    "prove our theorem which asserts that an is estimation based on is asymptotically optimal , we would like to make some comments and setup several things that we will need in the proof .",
    "let us begin with the computation of .",
    "one simply uses and with @xmath255 and @xmath256 .",
    "then @xmath257 where @xmath118 is the unique solution of @xmath258 therefore , one can compute the is change of measure @xmath167 of by simply solving the simple one dimensional problem to identify @xmath118 before the simulation begins . throughout the simulation",
    "no further computation will be necessary to calculate @xmath167 .",
    "this is a great advantage over an is simulation based on which would require the solution of at every step of the simulated random walk @xmath219 .",
    "[ [ subsolutions ] ] subsolutions + + + + + + + + + + + +    a function @xmath102 that satisfies @xmath259 } \\ { \\tilde{r}(t ) \\theta + h(\\theta ) + v_x(x , t ) \\theta + v_t(x , t ) \\ } \\ge 0\\ ] ] is called a _",
    "subsolution _ to the pde . in the next paragraph",
    "we will construct a subsolution to and the proof of asymptotic optimality will be a control theoretic verification argument based on this subsolution .",
    "this technique is from the `` subsolution approach '' to is which was first developed in the context of queuing networks in @xcite . for a more general development see @xcite .",
    "the paper that precedes these articles and which introduced many of the ideas that underlie the subsolution approach is @xcite .",
    "other articles using the approach include @xcite .",
    "usually , the subsolution approach is very useful for constructing good is algorithms .",
    "this is the case in most of the aforementioned references . in the present case",
    ", we already have a simple algorithm and we will use the approach to prove that our algorithm is optimal . for the subsolution , let us call it @xmath260 , we set @xmath261 for all @xmath150 and choose @xmath262 so that @xmath260 solves : @xmath263 these define @xmath260 up to an additive constant .",
    "this is sufficient for our needs since only the increments and partial derivatives of @xmath260 appear in a verification argument . by its construction @xmath260",
    "is piecewise affine , continuous and in fact a solution ( and hence a subsolution ) to .    _",
    "@xmath260 is a solution to and , as we have already noted in theorem [ t : hjb ] , so is @xmath102 defined in . evidently @xmath264 .",
    "this is a common situation in optimal control , that is , an hjb equation may have many solutions .",
    "what makes @xmath102 unique is that it is the maximal solution to . for more on these issues and",
    "a great deal of more information on stochastic optimal control we refer the reader to @xcite . _    besides being a solution to here are two properties of @xmath260 that play a key role in the optimality proof .",
    "[ l : wxlt0 ] @xmath265 and @xmath266 .",
    "let @xmath267 denote the left side of .",
    "@xmath268 is a decreasing function of @xmath107 , with @xmath269 and @xmath270 .",
    "@xmath271 , because each @xmath44 is an integer greater than @xmath3 .",
    "then @xmath272 it follows that @xmath273 . by definition @xmath274 and",
    "this is the first part of this lemma .    by their definition",
    "@xmath254 satisfy @xmath275 , see and .",
    "let @xmath276 .",
    "one can write @xmath277 as the following telescoping sum : @xmath278 where the @xmath71 is subject to .",
    "this last quantity by definition equals @xmath279 .",
    "this concludes the proof of the second part of this lemma .",
    "it follows directly from the definitions of @xmath262 and @xmath254 that @xmath280 let @xmath35 be a bernoulli random variable with @xmath281 . for integers @xmath115 and @xmath282 , one can represent the previous display probabilistically as @xmath283 = 1\\ ] ]    _ the way it is presented above , seems unmotivated .",
    "one should think of it as a multiplicative representation of .",
    "one can derive directly from first representing the optimization problem in that display as a trivial game and then using a representation result similar to .",
    "for a similar argument , see ( * ? ? ?",
    "* lemma 2.5.2 ) . _    [ t : optimal ] the is estimator based on @xmath167 of is asymptotically optimal .",
    "the following proof follows the same steps as the optimality proof given in @xcite . it is simpler because there is a fixed time horizon @xmath4 so no truncation of time is needed .    to ease notation let @xmath284 denote @xmath285 .",
    "define @xmath286 it follows from that @xmath287 is a martingale and that @xmath288 = 1.\\ ] ] we saw in lemma [ l : wxlt0 ] that @xmath265 , therefore @xmath289 on @xmath290 .",
    "the last two displays imply @xmath291.\\ ] ] taking the @xmath78 of both sides , dividing by @xmath4 and letting @xmath4 go to @xmath84 proves that holds for the change of measure @xmath292 .",
    "i.e. , the is change algorithm defined by this change of measure is asymptotically optimal , which is what we wanted to prove .",
    "the results derived for the rao bound in sections [ s : exp ] , [ s : ld ] and [ s : is ] can be derived for the gilbert - varshamov bound .",
    "the analysis and the results are essentially the same , the main difference is that @xmath63 replaces @xmath293 in and other similar places .",
    "the key quantity in is @xmath294    let @xmath184 , @xmath35 and @xmath41 be defined as in section [ s : exp ] .",
    "the expectation representation of is @xmath295.\\ ] ] this is exactly the same as , except for the following differences .    1 .",
    "the expectation is over a random walk that takes @xmath296 steps , rather than @xmath4 , 2 .",
    "there is a @xmath297 factor in front , 3 .",
    "the expectation is over those trajectories such that @xmath298 rather than @xmath299 .",
    "as was the case in section [ s : ld ] the asymptotic analysis of will involve a @xmath300 scaling . under this scaling the asymptotics of",
    "is the same as that of @xmath301.\\ ] ] let @xmath66 , @xmath302 , @xmath303 , @xmath63 be as in .",
    "theorem [ t : convergence ] implies @xmath304 =   \\sup   \\left\\ { \\sum_{i=1}^\\sigma   a_i\\left ( \\theta_i \\log ( s_i -1 )   + h(\\theta_i ) \\right)\\right\\},\\ ] ] where the @xmath71 is over @xmath305 if @xmath306 then @xmath307 is not a rare event and there is no need for is to simulate effectively , one can use straight forward monte carlo for this purpose . otherwise , theorem [ t : optimal ]",
    "implies that the minimizers of define an asymptotically optimal is change of measure to estimate .",
    "we used the octave numerical computation environment @xcite for the numerical computations in this section .        consider the following parameter values for an orthogonal array : @xmath308 , alphabet sizes @xmath19 = [ 13 10 7 5 ] , the block lengths @xmath309 = [ 20 20 20 20 ] and @xmath310",
    ". then @xmath311 the scaled strength parameter @xmath312 , and length parameters @xmath313 $ ] .",
    "we solve with the above parameter values to get the large deviation decay rate @xmath315 .",
    "then the large deviation estimate of the rao bound is @xmath316 which is about three times larger than the actual bound found above .",
    "this type of inaccuracy is expected since an ld analysis only identifies the exponential growth rate .",
    "we know from section [ s : is ] that if the optimizers of are used as an is change of measure in the resulting is algorithm is asymptotically optimal .",
    "the optimizers of for the above value of parameter values is @xmath317 below are five estimation results using this algorithm with @xmath318 sample paths .",
    "the standard error column presents the estimated standard deviation @xmath319 .",
    "the informal @xmath320 confidence intervals are @xmath321 $ ] .",
    "a comparison of the asymptotic versions of the rao and the gv bounds is given figure [ f : comparison ] .",
    "take @xmath322 , @xmath145 , @xmath323 , @xmath324 and @xmath325 , @xmath326 .",
    "the following graph depicts the rao and the gv asymptotic bounds for @xmath327.$ ]    the large gap between them is due to the difference of a factor of @xmath242 between the constraints of the rao and the gv bounds .",
    "the gv bound is flat for larger values of @xmath63 .",
    "this is because for these values of @xmath63 the unique global maximizer of satisfies the constraint ."
  ],
  "abstract_text": [
    "<S> mixed level orthogonal arrays are basic structures in experimental design . </S>",
    "<S> we develop three algorithms that compute rao and gilbert - varshamov type bounds for mixed level orthogonal arrays . </S>",
    "<S> the computational complexity of the terms involved in these bounds can grow fast as the parameters of the arrays increase and this justifies the construction of these algorithms . </S>",
    "<S> the first is a recursive algorithm that computes the bounds exactly , the second is based on an asymptotic analysis and the third is a simulation algorithm . </S>",
    "<S> they are all based on the representation of the combinatorial expressions that appear in the bounds as expectations involving a symmetric random walk . </S>",
    "<S> the markov property of the underlying random walk gives the recursive formula to compute the expectations . a large deviation ( ld ) analysis of the expectations </S>",
    "<S> provide the asymptotic algorithm . </S>",
    "<S> the asymptotically optimal importance sampling ( is ) of the same expectation provides the simulation algorithm . </S>",
    "<S> both the ld analysis and the construction of the is algorithm uses a representation of these problems as a sequence of stochastic optimal control problems converging to a limit calculus of variations problem . </S>",
    "<S> the construction of the is algorithm uses a recently discovered method of using subsolutions to the hamilton jacobi bellman equations associated with the limit problem . </S>"
  ]
}