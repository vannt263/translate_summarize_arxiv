{
  "article_text": [
    "motivated by an array of applications , matrix completion has attracted significant recent attention in different fields including statistics , applied mathematics and electrical engineering .",
    "the central goal of matrix completion is to recover a high - dimensional low - rank matrix based on a subset of its entries .",
    "applications include recommender systems @xcite , genomics @xcite , multi - task learning @xcite , sensor localization @xcite , and computer vision @xcite , among many others .",
    "matrix completion has been well studied under the uniform sampling model , where observed entries are assumed to be sampled uniformly at random .",
    "the best known approach is perhaps the constrained nuclear norm minimization ( nnm ) , which has been shown to yield near - optimal results when the sampling distribution of the observed entries is uniform @xcite . for estimating approximately low - rank matrices from uniformly sampled noisy observations , several penalized or constrained nnm estimators , which are based on the same principle as the well - known lasso and dantzig selector for sparse signal recovery , were proposed and analyzed @xcite .",
    "in many applications , the entries are sampled independently but not uniformly .",
    "in such a setting , @xcite showed that the standard nnm methods do not perform well , and proposed a weighted nnm method , which depends on the true sampling distribution . in the case of unknown sampling distribution",
    ", @xcite introduced an empirically - weighted nnm method .",
    "@xcite studied a max - norm constrained minimization method for the recovery of a low - rank matrix based on the noisy observations under the non - uniform sampling model .",
    "it was shown that the max - norm constrained least squares estimator is rate - optimal under the frobenius norm loss and yields a more stable approximate recovery guarantee with respect to the sampling distributions .",
    "the focus of matrix completion has so far been on the recovery of a low - rank matrix based on independently sampled entries .",
    "motivated by applications in genomic data integration , we introduce in this paper a new framework of matrix completion called _ structured matrix completion _",
    "( smc ) , where a subset of the rows and a subset of the columns of an approximately low - rank matrix are observed and the goal is to reconstruct the whole matrix based on the observed rows and columns .",
    "we first discuss the genomic data integration problem before introducing the smc model .",
    "when analyzing genome - wide studies ( gws ) of association , expression profiling or methylation , ensuring adequate power of the analysis is one of the most crucial goals due to the high dimensionality of the genomic markers under consideration .",
    "because of cost constraints , gws typically have small to moderate sample sizes and hence limited power .",
    "one approach to increase the power is to integrate information from multiple gws of the same phenotype .",
    "however , some practical complications may hamper the feasibility of such integrative analysis .",
    "different gws often involve different platforms with distinct genomic coverage .",
    "for example , whole genome next generation sequencing ( ngs ) studies would provide mutation information on all loci while older technologies for genome - wide association studies ( gwas ) would only provide information on a small subset of loci . in some settings",
    ", certain studies may provide a wider range of genomic data than others .",
    "for example , one study may provide extensive genomic measurements including gene expression , mirna and dna methylation while other studies may only measure gene expression .    to perform integrative analysis of studies with different extent of genomic measurements , the naive complete observation",
    "only approach may suffer from low power .",
    "for the gwas setting with a small fraction of loci missing , many imputation methods have been proposed in recent years to improve the power of the studies .",
    "examples of useful methods include haplotype reconstruction , @xmath0-nearest neighbor , regression and singular value decomposition methods @xcite .",
    "many of the haplotype phasing methods are considered to be highly effective in recovering missing genotype information @xcite .",
    "these methods , while useful , are often computationally intensive . in addition , when one study has a much denser coverage than the other , the fraction of missingness could be high and an exceedingly large number of observation would need to be imputed .",
    "it is unclear whether it is statistically or computationally feasible to extend these methods to such settings .",
    "moreover , haplotype based methods can not be extended to incorporate other types of genomic data such as gene expression and mirna data .",
    "when integrating multiple studies with different extent of genomic measurements , the observed data can be viewed as complete rows and columns of a large matrix @xmath1 and the missing components can be arranged as a submatrix of @xmath1 . as such , the missingness in @xmath1 is structured by design . in this paper",
    ", we propose a novel smc method for imputing the missing submatrix of @xmath1 . as shown in section [ application.sec ] , by imputing the missing mirna measurements and constructing prediction rules based on the imputed data , it is possible to significantly improve the prediction performance .",
    "motivated by the applications mentioned above , this paper considers smc where a subset of rows and columns are observed .",
    "specifically , we observe @xmath2 rows and @xmath3 columns of a matrix @xmath4 and the goal is to recover the whole matrix .",
    "since the singular values are invariant under row / column permutations , it can be assumed without loss of generality that we observe the first @xmath5 rows and @xmath6 columns of @xmath1 which can be written in a block form : @xmath7c } a_{11 } & a_{12 } & m_1 \\\\ a_{21 } & { \\color{gray } a_{22 } } & p_1-m_1\\\\ \\end{block } \\end{blockarray}\\ ] ] where @xmath8 , @xmath9 , and @xmath10 are observed and the goal is to recover the missing block @xmath11 .",
    "see figure [ fig : a_z_illustration](a ) in section [ sec.procedure ] for a graphical display of the data . clearly there is no way to recover @xmath11 if @xmath1 is an arbitrary matrix .",
    "however , in many applications such as genomic data integration discussed earlier , @xmath1 is approximately low - rank , which makes it possible to recover @xmath11 with accuracy . in this paper , we introduce a method based on the singular value decomposition ( svd ) for the recovery of @xmath11 when @xmath1 is approximately low - rank .",
    "it is important to note that the observations here are much more  structured \" comparing to the previous settings of matrix completion . as the observed",
    "entries are in full rows or full columns , the existing methods based on nnm are not suitable . as mentioned earlier , constrained nnm methods",
    "have been widely used in matrix completion problems based on independently observed entries . however , for the problem considered in the present paper , these methods do not utilize the structure of the observations and do not guarantee precise recovery even for exactly low - rank matrix @xmath1 ( see remark [ rm : nuclear_norm_fail ] in section [ sec.procedure ] ) .",
    "numerical results in section [ simulation.sec ] show that nnm methods do not perform well in smc . in this paper",
    "we propose a new smc method that can be easily implemented by a fast algorithm which only involves basic matrix operations and the svd .",
    "the main idea of our recovery procedure is based on the schur complement . in the ideal case when @xmath1 is exactly low rank , the schur complement of the missing block , @xmath12 , is zero and thus @xmath13 can be used to recover @xmath11 exactly .",
    "when @xmath1 is approximately low rank , @xmath13 can not be used directly to estimate @xmath11 . for this case , we transform the observed blocks using svd ; remove some unimportant rows and columns based on thresholding rules ; and subsequently apply a similar procedure to recover @xmath11 . both its theoretical and numerical properties are studied .",
    "it is shown that the estimator recovers low - rank matrices accurately and is robust against small perturbations .",
    "a lower bound result shows that the estimator is rate optimal for a class of approximately low - rank matrices .",
    "although it is required for the theoretical analysis that there is a significant gap between the singular values of the true low - rank matrix and those of the perturbation , simulation results indicate that this gap is not really necessary in practice and the estimator recovers @xmath1 accurately whenever the singular values of @xmath1 decay sufficiently fast .",
    "the rest of the paper is organized as follows . in section [ sec.procedure ] , we introduce in detail the proposed smc methods when @xmath1 is exactly or approximately low - rank .",
    "the theoretical properties of the estimators are analyzed in section [ analysis.sec ] .",
    "both upper and lower bounds for the recovery accuracy under the schatten-@xmath14 norm loss are established .",
    "simulation results are shown in section [ simulation.sec ] to investigate the numerical performance of the proposed methods .",
    "a real data application to genomic data integration is given in section [ application.sec ] .",
    "section [ discussion.sec ] discusses a few practical issues related to real data applications . for reasons of space",
    ", the proofs of the main results and additional simulation results are given in the supplement @xcite .",
    "some key technical tools used in the proofs of the main theorems are also developed and proved in the supplement .",
    "in this section , we propose procedures to recover the submatrix @xmath11 based on the observed blocks @xmath8 , @xmath9 , and @xmath10 .",
    "we begin with basic notation and definitions that will be used in the rest of the paper .    for a matrix @xmath15 , we use @xmath16}$ ] to represent its sub - matrix with row indices @xmath17 and column indices @xmath18 .",
    "we also use the matlab syntax to represent index sets . specifically for integers",
    "@xmath19 ,  @xmath20 \" represents @xmath21 ; and  : \" alone represents the entire index set .",
    "therefore , @xmath22}$ ] stands for the first @xmath23 columns of @xmath15 while @xmath24}$ ] stands for the @xmath25 rows of @xmath15 . for the matrix @xmath1 given in , we use the notation @xmath26 and @xmath27 to denote @xmath28^{\\intercal}$ ] and @xmath29 $ ] , respectively . for a matrix @xmath30 ,",
    "let @xmath31 be the svd , where @xmath32 with @xmath33 being the singular values of @xmath34 in decreasing order .",
    "the smallest singular value @xmath35 , which will be denoted by @xmath36 , plays an important role in our analysis .",
    "we also define @xmath37 and @xmath38 . for @xmath39 , the schatten-@xmath14 norm @xmath40 is defined to be the vector @xmath14-norm of the singular values of @xmath34 , i.e. @xmath41 .",
    "three special cases are of particular interest : when @xmath42 , @xmath43 is the nuclear ( or trace ) norm of @xmath34 and will be denoted as @xmath44 ; when @xmath45 , @xmath46 is the frobenius norm of @xmath34 and will be denoted as @xmath47 ; when @xmath48 , @xmath49 is the spectral norm of @xmath34 that we simply denote as @xmath50 . for any matrix @xmath51",
    ", we use @xmath52 to denote the projection operator onto the column space of @xmath15 . throughout",
    ", we assume that @xmath1 is _ approximately rank @xmath23 _ in that for some integer @xmath53 , there is a significant gap between @xmath54 and @xmath55 and the tail @xmath56 is small .",
    "the gap assumption enables us to provide a theoretical upper bound on the accuracy of the estimator , while it is not necessary in practice ( see section [ simulation.sec ] for more details ) .",
    "we begin with the relatively easy case where @xmath1 is exactly of rank @xmath23 . in this case",
    ", a simple analysis indicates that @xmath1 can be perfectly recovered as shown in the following proposition .",
    "[ th : noiseless ] suppose @xmath1 is of rank @xmath23 , the svd of @xmath8 is @xmath57 , where @xmath58 and @xmath59 . if @xmath60 ) = { { \\rm rank}}\\left(\\begin{bmatrix } a_{11}\\\\ a_{21 } \\end{bmatrix } \\right ) = { { \\rm rank}}(a ) = r,\\ ] ] then @xmath61 and @xmath11 is exactly given by @xmath62    [ rm : nuclear_norm_fail ] under the same conditions as proposition [ th : noiseless ] , the nnm @xmath63 fails to guarantee the exact recovery of @xmath11 .",
    "consider the case where @xmath1 is a @xmath64 matrix with all entries being 1 .",
    "suppose we observe arbitrary @xmath5 rows and @xmath6 columns , the nnm would yield @xmath65 with all entries being @xmath66 ( see lemma [ lm : nuclear_norm_1 ] in the supplement ) .",
    "hence when @xmath67 , i.e. , when the size of the observed blocks are much smaller than that of @xmath1 , the nnm fails to recover exactly the missing block @xmath11 .",
    "see also the numerical comparison in section [ simulation.sec ] .",
    "the nnm also fails to recover @xmath11 with high probability in a random matrix setting where @xmath68 with @xmath69 and @xmath70 being i.i.d .",
    "standard gaussian matrices .",
    "see lemma [ lm : nuclear_norm_fail ] in the supplement for further details .",
    "in addition to , other variations of nnm have been proposed in the literature , including _ penalized _",
    "nnm @xcite , @xmath71 and _ constrained _ nnm with relaxation @xcite , @xmath72 where @xmath73 and @xmath74 is the tunning parameter",
    ". however , these nnm methods may not be suitable for smc especially when only a small number of rows and columns are observed . in particular , when @xmath75 , @xmath1 is well spread in each block @xmath76 , we have @xmath77\\|_\\ast \\ll\\|a\\|_\\ast$ ] , @xmath78_\\ast \\ll \\|a\\|_\\ast$ ] .",
    "thus , @xmath79 in the other words , imputing @xmath11 with all zero yields a much smaller nuclear norm than imputing with the true @xmath11 and hence nnm methods would generally fail to recover @xmath11 under such settings .",
    "proposition [ th : noiseless ] shows that , when @xmath1 is exactly low - rank , @xmath11 can be recovered precisely by @xmath80 .",
    "unfortunately , this result heavily relies on the exactly low - rank assumption that can not be directly used for approximately low - rank matrices .",
    "in fact , even with a small perturbation to @xmath1 , the inverse of @xmath8 makes the formula @xmath80 unstable , which may lead to the failure of recovery . in practice , @xmath1 is often not exactly low rank but approximately low rank .",
    "thus for the rest of the paper , we focus on the latter setting .",
    "let @xmath81 be the svd of an approximately low rank matrix @xmath1 and partition @xmath82 and @xmath83 into blocks as @xmath84c }    u_{11 } & u_{12 } & m_1 \\\\",
    "u_{21 } & u_{22 } & p_1 - m_1\\\\ \\end{block } \\end{blockarray},\\ v = \\begin{blockarray}{ccc }   r & p_2-r & \\\\ \\begin{block}{[cc]c }    v_{11 } & v_{12 } & m_2 \\\\",
    "v_{21 } & v_{22 } & p_2 - m_2\\\\ \\end{block } \\end{blockarray } , \\",
    "\\sigma =   \\begin{blockarray}{ccc }   r & p_2-r & \\\\ \\begin{block}{[cc]c }    \\sigma_{1 } & 0 & r",
    "\\\\    0 & \\sigma_{2 } & p_1-r\\\\ \\end{block } \\end{blockarray}\\ ] ] then @xmath1 can be decomposed as @xmath85 where @xmath86 is of rank @xmath23 with the largest @xmath23 singular values of @xmath1 and @xmath87 is general but with small singular values .",
    "then @xmath88c }    u_{11}\\sigma_{1}v_{11}^{\\intercal } &    u_{11}\\sigma_{1}v_{21}^{\\intercal } & m_1 \\\\",
    "u_{21}\\sigma_{1}v_{11}^{\\intercal } &    u_{21}\\sigma_{1}v_{21}^{\\intercal } & p_1-m_1\\\\ \\end{block } \\end{blockarray } , \\quad \\mbox{and } \\quad a_{-\\max(r ) } = u_{\\bullet 2}\\sigma_2 v_{\\bullet 2}^{\\intercal } .\\ ] ] here and in the sequel , we use the notation @xmath89 and @xmath90 to denote @xmath91^{\\intercal}$ ] and @xmath92 $ ] , respectively .",
    "thus , @xmath86 can be viewed as a rank-@xmath23 approximation to @xmath1 and obviously @xmath93 we will use the observed @xmath8 , @xmath9 and @xmath10 to obtain estimates of @xmath94 , @xmath95 and @xmath96 and subsequently recover @xmath11 using an estimated @xmath97 .    when @xmath23 is known , i.e. , we know where the gap is located in the singular values of @xmath1 , a simple procedure can be implemented to estimate @xmath11 as described in algorithm 1 below by estimating @xmath94 and @xmath95 using the principal components of @xmath26 and @xmath98 .",
    "input : @xmath99 .",
    "calculate the svd of @xmath26 and @xmath98 to obtain @xmath100 .",
    "suppose @xmath101 are orthonormal basis of @xmath102 .",
    "we estimate the column space of @xmath103 and @xmath104 by @xmath105 } , \\hat",
    "n = v^{(1 ) } _ { [ : , 1:r]}. $ ] finally we estimate @xmath11 as @xmath106    [ al : with_r ]    however , algorithm 1 has several major limitations .",
    "first , it relies on a given @xmath23 which is typically unknown in practice .",
    "second , the algorithm need to calculate the matrix divisions , which may cause serious precision issues when the matrix is near - singular or the rank @xmath23 is mis - specified . to overcome these difficulties",
    ", we propose another algorithm which essentially first estimates @xmath23 with @xmath107 and then apply algorithm 1 to recover @xmath11 . before introducing the algorithm of recovery without knowing @xmath23 , it is helpful to illustrate the idea with heat maps in figures [ fig : a_z_illustration ] and [ fig : thresholding ] .",
    "[ fig : a_z_illustration ]    [ fig : thresholding ]    our procedure has three steps .    1 .   first , we move the significant factors of @xmath108 and @xmath98 to the front by rotating the columns of @xmath26 and the rows of @xmath27 based on the svd , @xmath109 after the transformation , we have @xmath110 , @xmath111 clearly @xmath1 and @xmath112 have the same singular values since the transformation is orthogonal .",
    "as shown in figure [ fig : a_z_illustration](b ) , the amplitudes of the columns of @xmath113^{\\intercal}$ ] and the rows of @xmath114 $ ] are decaying .",
    "2 .   when @xmath1 is exactly of rank @xmath23 , the @xmath115 rows and @xmath116 columns of @xmath112 are zero . due to the small perturbation term @xmath87 , the back columns of @xmath117 and rows of @xmath118 are small but non - zero .",
    "in order to recover @xmath86 , the best rank @xmath23 approximation to @xmath1 , a natural idea is to first delete these back rows of @xmath118 and columns of @xmath119 , i.e. the @xmath115 rows and @xmath116 columns of @xmath112 .",
    "+ however , since @xmath23 is unknown , it is unclear how many back rows and columns should be removed",
    ". it will be helpful to have an estimate for @xmath23 , @xmath107 , and then use @xmath120}$ ] , @xmath121}$ ] and @xmath122}$ ] to recover @xmath11 .",
    "it will be shown that a good choice of @xmath123 would satisfy that @xmath124}$ ] is non - singular and @xmath125}z_{11,[1:\\hat r , 1:\\hat r]}^{-1}\\| \\leq t_r$ ] , where @xmath126 is some constant to be specified later .",
    "our final estimator for @xmath23 would be the largest @xmath107 that satisfies this condition , which can be identified recursively from @xmath127 to 1 ( see figure [ fig : thresholding ] ) .",
    "3 .   finally ,",
    "similar to , @xmath11 can be estimated by @xmath128}z_{11 , [ 1:\\hat r , 1:\\hat r]}^{-1}z_{12 , [ 1:\\hat r , : ] } , \\ ] ]    the method we propose can be summarized as the following algorithm .",
    "input : @xmath129 .",
    "thresholding level : @xmath130 , ( or @xmath131 ) .",
    "calculate the svd @xmath132 , @xmath133 .",
    "calculate @xmath134 @xmath135 ( use iteration to find @xmath123 ) calculate @xmath136 ( or @xmath137 ) by solving linear equation system , @xmath138 } z_{11 , [ 1:s , 1:s]}^{-1 } \\quad ( \\text{or } \\quad d_{c , s } = z_{11 , [ 1:s , 1:s]}^{-1}z_{12 , [ 1:s , : ] } ) \\ ] ] @xmath139 ; * break * from the loop ; @xmath140 . finally we calculate the estimate as @xmath141}z_{11 , [ 1:\\hat r , 1:\\hat r]}^{-1}z_{12 , [ 1:\\hat r , : ] } \\ ] ]    [ al : without_r ]    it can also be seen from algorithm [ al : without_r ] that the estimator @xmath123 is constructed based on either the row thresholding rule @xmath142 or the column thresholding rule @xmath143 .",
    "discussions on the choice between @xmath144 and @xmath145 are given in the next section .",
    "let us focus for now on the row thresholding based on @xmath146}z_{11 , [ 1:s , 1:s]}^{-1}$ ] .",
    "it is important to note that @xmath147}$ ] and @xmath148}$ ] approximate @xmath149 and @xmath150 , respectively .",
    "the idea behind the proposed @xmath123 is that when @xmath151 , @xmath152}$ ] and @xmath153}$ ] are nearly singular and hence @xmath144 may either be deemed singular or with unbounded norm . when @xmath154 , @xmath155}$ ] is non - singular with @xmath156 bounded by some constant , as we show in theorem [ th : main_without_r ] .",
    "thus , we estimate @xmath107 as the largest @xmath23 such that @xmath155}$ ] is non - singular with @xmath157 .",
    "in this section , we investigate the theoretical properties of the algorithms introduced in section [ sec.procedure ] .",
    "upper bounds for the estimation errors of algorithms 1 and 2 are presented in theorems [ th : hat a ] and [ th : main_without_r ] , respectively , and the lower - bound results are given in theorem [ th : lower_bound ] .",
    "these bounds together establish the optimal rate of recovery over certain classes of approximately low - rank matrices .",
    "the choices of tuning parameters @xmath126 and @xmath131 are discussed in corollaries [ cr : random_column_row ] and [ cr : randomuv ] .",
    "[ th : hat a ] suppose @xmath158 is given by the procedure of algorithm 1 .",
    "assume @xmath159 then for any @xmath39 , @xmath160    it is helpful to explain intuitively why condition is needed . when @xmath1 is approximately low - rank , the dominant low - rank component of @xmath1 , @xmath86 , serves as a good approximation to @xmath1 , while the residual @xmath87 is  small \" .",
    "the goal is to recover @xmath86 well . among the three observed blocks ,",
    "@xmath8 is the most important and it is necessary to have @xmath86 dominating @xmath87 in @xmath8 . note that @xmath161 } + a_{-\\max(r ) , [ 1:m_1 , 1:m_2]}$ ] , @xmath162 } ) = \\sigma_r(u_{11}\\sigma_1v_{11}^{\\intercal } ) \\geq \\sigma_{\\min}(u_{11})\\sigma_r(a)\\sigma_{\\min}(v_{11}),\\ ] ] @xmath163}\\| = \\|u_{12}\\sigma_2v_{12}^{\\intercal}\\| \\leq \\sigma_{r+1}(a).\\ ] ] we thus require condition in theorem [ th : hat a ] for the theoretical analysis .",
    "theorem [ th : hat a ] gives an upper bound for the estimation accuracy of algorithm 1 under the assumption that there is a significant gap between @xmath54 and @xmath55 for some known @xmath23 .",
    "it is noteworthy that there are possibly multiple values of @xmath23 that satisfy condition .",
    "in such a case , the bound applies to all such @xmath23 and the largest @xmath23 yields the strongest result .",
    "we now turn to algorithm",
    "[ al : without_r ] , where the knowledge of @xmath23 is not assumed .",
    "theorem [ th : main_without_r ] below shows that for properly chosen @xmath126 or @xmath131 , algorithm 2 can lead to accurate recovery of @xmath11 .",
    "[ th : main_without_r ] assume that there exists @xmath164 $ ] such that @xmath165 let @xmath126 and @xmath131 be two constants satisfying @xmath166 then for @xmath39 , @xmath167 given by algorithm 2 satisfies @xmath168 when @xmath107 is estimated based on the thresholding rule @xmath169 or @xmath170 , respectively .",
    "besides @xmath54 and @xmath55 , theorems [ th : hat a ] and [ th : main_without_r ] involve @xmath171 and @xmath172 , two important quantities that reflect how much the low - rank matrix @xmath173 is concentrated on the first @xmath5 rows and @xmath6 columns .",
    "we should note that @xmath171 and @xmath172 depend on the singular vectors of @xmath1 and @xmath54 and @xmath55 are the singular values of @xmath1 .",
    "the lower bound in theorem [ th : lower_bound ] below indicates that @xmath171 , @xmath172 , and the singular values of @xmath1 together quantify the difficulty of the problem : recovery of @xmath11 gets harder as @xmath171 and @xmath172 become smaller or the @xmath174 singular values become larger .",
    "define the class of approximately rank-@xmath23 matrices @xmath175 by @xmath176    [ th : lower_bound ] suppose @xmath177 and @xmath178 , then for all @xmath39 , @xmath179    theorems [ th : hat a ] , [ th : main_without_r ] and [ th : lower_bound ] together immediately yield the optimal rate of recovery over the class @xmath180 , @xmath181    since @xmath103 and @xmath104 are determined by the svd of @xmath1 and @xmath182 and @xmath172 are unknown based only on @xmath183 and @xmath10 , it is thus not straightforward to choose the tuning parameters @xmath126 and @xmath131 in a principled way .",
    "theorem [ th : main_without_r ] also does not provide information on the choice between row and column thresholding .",
    "such a choice generally depends on the problem setting .",
    "we consider below two settings where either the row / columns of @xmath1 are randomly sampled or @xmath1 is itself a random low - rank matrix . in such settings , when @xmath1 is approximately rank @xmath23 and at least @xmath184 number of rows and columns are observed , algorithm",
    "[ al : without_r ] gives accurate recovery of @xmath1 with fully specified tuning parameter .",
    "we first consider in corollary [ cr : random_column_row ] a fixed matrix @xmath1 with the observed @xmath5 rows and @xmath6 columns selected uniformly randomly .",
    "[ cr : random_column_row ] let @xmath81 be the svd of @xmath185 . set @xmath186 let @xmath187 and @xmath188 be respectively the index set of the observed @xmath5 rows and @xmath6 columns",
    ". then @xmath1 can be decomposed as @xmath189 } , \\ ; a_{21 } = a_{[\\omega_1^c , \\omega_2 ] } , \\ ; a_{12 } = a_{[\\omega_1 , \\omega_2^c ] } , \\ ; a_{22 } = a_{[\\omega_1^c , \\omega_2^c]}.\\ ] ]    1 .",
    "let @xmath17 and @xmath18 be independently and uniformly selected from @xmath190 and @xmath191 with or without replacement , respectively .",
    "suppose there exists @xmath192 such that @xmath193 and the number of rows and number of columns we observed satisfy @xmath194 algorithm 2 with either column thresholding with the break condition @xmath195 where @xmath196 or row thresholding with the break condition @xmath197 where @xmath198 satisfies , for all @xmath39 , @xmath199 2 .",
    "if @xmath17 is uniformly randomly selected from @xmath200 with or without replacement ( @xmath18 is not necessarily random ) , and there exists @xmath201 such that @xmath202 and the number of observed rows satisfies @xmath203 then algorithm 2 with the break condition @xmath142 where @xmath204 satisfies , for all @xmath39 , @xmath205 3 .   similarly , if @xmath18 is uniformly randomly selected from @xmath206 with or without replacement ( @xmath17 is not necessarily random ) and there exists @xmath201 such that @xmath207 and the number of observed columns satisfies @xmath208 then algorithm 2 with the break condition @xmath170 where @xmath209 satisfies , for all @xmath39 , @xmath210    the quantities @xmath211 and @xmath212 in corollary [ cr : random_column_row ] measure the variation of amplitude of each row or each column of @xmath86 . when @xmath211 and @xmath212 become larger , a small number of rows and columns in @xmath86 would have larger amplitude than others , while these rows and columns would be missed with large probability in the sampling of @xmath213 , which means the problem would become harder .",
    "hence , more observations for the matrix with larger @xmath211 and @xmath212 are needed as shown in .",
    "we now consider the case where @xmath1 is a random matrix .",
    "[ cr : randomuv ] suppose @xmath185 is a random matrix generated by @xmath214 , where the singular values @xmath215 and singular space @xmath216 are fixed , and @xmath15 has orthonormal columns that are randomly sampled based on the haar measure .",
    "suppose we observe the first @xmath5 rows and first @xmath6 columns of @xmath1 .",
    "assume there exists @xmath217 such that @xmath218 then there exist uniform constants @xmath219 such that if @xmath220 , @xmath167 is given by algorithm [ al : without_r ] with the break condition @xmath169 , where @xmath204 , we have for all @xmath39 , @xmath221 parallel results hold for the case when @xmath15 is fixed and @xmath216 has orthonormal columns that are randomly sampled based on the haar measure , and we observe the first @xmath5 rows and first @xmath6 columns of @xmath1 .",
    "assume there exists @xmath222 such that @xmath223 then there exist unifrom constants @xmath224 such that if @xmath225 , @xmath167 is given by algorithm 2 with column thresholding with the break condition @xmath197 , where @xmath226 , we have for all @xmath39 , @xmath227",
    "in this section , we show results from extensive simulation studies that examine the numerical performance of algorithm [ al : without_r ] on randomly generated matrices for various values of @xmath228 , @xmath229 , @xmath5 and @xmath6 .",
    "we first consider settings where a gap between some adjacent singular values exists , as required by our theoretical analysis .",
    "then we investigate settings where the singular values decay smoothly with no significant gap between adjacent singular values .",
    "the results show that the proposed procedure performs well even when there is no significant gap , as long as the singular values decay at a reasonable rate .",
    "we also examine how sensitive the proposed estimators are to the choice of the threshold and the choice between row and column thresholding .",
    "in addition , we compare the performance of the smc method with that of the nnm method . finally , we consider a setting similar to the real data application discussed in the next section .",
    "results shown below are based on 200 - 500 replications for each configuration .",
    "additional simulation results on the effect of @xmath5 , @xmath6 and ratio @xmath230 are provided in the supplement . throughout , we generate the random matrix a from @xmath231 , where the singular values of the diagonal matrix @xmath215 are chosen accordingly for different settings . the singular spaces @xmath15 and @xmath216 are drawn randomly from the haar measure . specifically , we generate i.i.d .",
    "standard gaussian matrix @xmath232 and @xmath233 , then apply the qr decomposition to @xmath234 and @xmath235 and assign @xmath15 and @xmath216 with the @xmath236 part of the result .",
    "we first consider the performance of algorithm [ al : without_r ] when a significant gap between the @xmath237 and @xmath238 singular values of @xmath1 .",
    "we fixed @xmath239 and choose the singular values as @xmath240 here @xmath23 is the rank of the major low - rank part @xmath86 , @xmath241 is the gap ratio between the @xmath237 and @xmath238 singular values of @xmath1 .",
    "the average loss of @xmath167 from algorithm 2 with the row thresholding and @xmath242 under both the spectral norm and frobenius norm losses are given in figure [ fig : gap ] .",
    "the results suggest that our algorithm performs better when @xmath23 gets smaller and gap ratio @xmath243 gets larger .",
    "moreover , even when @xmath244 , namely there is no significant gap between any adjacent singular values , our algorithm still works well for small @xmath23 . as will be seen in the following simulation studies , this is generally the case as long as the singular values of @xmath1 decay sufficiently fast .     and @xmath55 .",
    "the singular value values of @xmath1 are given by , @xmath245 , and @xmath246.,title=\"fig : \" ]   and @xmath55 .",
    "the singular value values of @xmath1 are given by , @xmath245 , and @xmath246.,title=\"fig : \" ]    [ fig : gap ]    we now turn to the settings with the singular values being @xmath247 and various choices of @xmath248 , @xmath228 and @xmath229 .",
    "hence , no significant gap between adjacent singular values exists under these settings and we aim to demonstrate that our method continues to work well .",
    "we first consider @xmath249 , @xmath250 and let @xmath248 range from 0.3 to 2 . under this setting",
    ", we also study how the choice of thresholds affect the performance of our algorithm . for simplicity ,",
    "we report results only for row thresholding as results for column thresholding are similar .",
    "the average loss of @xmath167 from algorithm 2 with @xmath251\\}$ ] under both the spectral norm and frobenius norm are given in figure [ fig : c_test ] .",
    "in general , the algorithm performs well provided that @xmath248 is not too small and as expected , the average loss decreases with a higher decay rate in the singular values .",
    "this indicates that the existence of a significant gap between adjacent singular values is not necessary in practice , provided that the singular values decay sufficiently fast .",
    "when comparing the results across different choices of the threshold , @xmath252 as suggested in our theoretical analysis is indeed the optimal choice .",
    "thus , in all subsequent numerical analysis , we fix @xmath252 .",
    "[ cols=\"^,^ \" , ]     in summary , the results shown above suggest that our smc procedure accurately recovers the leading pcs of the mirna variables .",
    "in addition , adding @xmath253 obtained from imputation using the proposed smc method could significantly improve the prediction performance , which confirms the value of our method for integrative genomic analysis .",
    "when comparing to the nnm method , the proposed smc method produces summaries of mirna that is more correlated with the truth and yields leading pcs that are more predictive of oc survival .",
    "the present paper introduced a new framework of smc where a subset of the rows and columns of an approximately low - rank matrix are observed .",
    "we proposed an smc method for the recovery of the whole matrix with theoretical guarantees .",
    "the proposed procedure significantly outperforms the conventional nnm method for matrix completion , which does not take into account the special structure of the observations . as shown by our theoretical and numerical analyses ,",
    "the widely adopted nnm methods for matrix completion are not suitable for the smc setting .",
    "these nnm methods perform particularly poorly when a small number of rows and columns are observed .",
    "the key assumption in matrix completion is the matrix being approximately low rank .",
    "this is reasonable in the ovarian cancer application since as indicated in the results from the tcga study ( cancer genome atlas research network , 2011 ) , the patterns observed in the mirna signature are highly correlated with the patterns observed in the gene expression signature .",
    "this suggests the high correlation among the selected gene expression and mirna variables .",
    "results from the imputation based on the approximate low rank assumption given in section [ application.sec ] are also encouraging with promising correlations with true signals and good prediction performance from the imputed mirna signatures .",
    "we expect that this imputation method will also work well in genotyping and sequencing applications , particularly for regions with reasonably high linkage disequilibrium .",
    "another main assumption that is needed in the theoretical analysis is that there is a significant gap between the @xmath237 and @xmath238 singular values of @xmath1 .",
    "this assumption may not be valid in real practice .",
    "in particular , the singular values of the ovarian dataset analyzed in section [ application.sec ] is decreasing smoothly without a significant gap .",
    "however , it has been shown in the simulation studies presented in section [ simulation.sec ] that , although there is no significant gap between any adjacent singular values of the matrix to be recovered , the proposed smc method works well as long as the singular values decay sufficiently fast .",
    "theoretical analysis for the proposed smc method under more general patterns of singular value decay warrants future research .    to implement the proposed algorithm 2 ,",
    "major decisions include the choice of threshold values and choosing between column thresholding and row thresholding . based on both theoretical and numerical studies ,",
    "optimal threshold values can be set as @xmath254 for column thresholding and @xmath255 for row thresholding .",
    "simulation results in section [ simulation.sec ] show that when both rows and columns are randomly chosen , the results are very similar . in the real data applications , the choice between row thresholding and column thresholding depends on whether the rows or columns are more  homogeneous \" , or closer to being randomly sampled .",
    "for example , in the ovarian cancer dataset analyzed in section [ application.sec ] , the rows correspond to the patients and the columns correspond to the gene expression levels and mirna levels .",
    "thus the rows are closer to random sample than the columns , consequently it is more natural to use the row thresholding in this case .",
    "we have shown both theoretically and numerically in sections [ analysis.sec ] and [ simulation.sec ] that algorithm",
    "[ al : without_r ] provides a good recovery of @xmath11 .",
    "however , the naive implementation of this algorithm requires @xmath256 matrix inversions and multiplication operations in the for loop that calculates @xmath156 ( or @xmath257 ) , @xmath258 . taking into account the relationship among @xmath259 ( or @xmath260 ) for different @xmath261 s",
    ", it is possible to simultaneously calculate all @xmath156 ( or @xmath257 ) and accelerate the computations . for reasons of space",
    ", we leave optimal implementation of algorithm 2 as future work .",
    "we thank the editor , associate editor and referee for their detailed and constructive comments which have helped to improve the presentation of the paper .",
    "supplement to  structured matrix completion with",
    "we consider the effect of the number of the observed rows and columns on the estimation accuracy .",
    "we let @xmath245 , let the singular values of @xmath1 be @xmath262 and let @xmath5 and @xmath6 vary from @xmath263 to @xmath264 .",
    "the singular spaces @xmath15 and @xmath216 are again generated randomly from the haar measure .",
    "the estimation errors of @xmath265 from algorithm 2 with row thresholding and @xmath242 over different choices of @xmath5 and @xmath6 are shown in figure [ fig : m_varies ] .",
    "[ fig : m_varies ]    as expected , the average loss decreases as @xmath5 or @xmath6 grows . another",
    "interesting fact is that the average loss is approximately symmetric with respect to @xmath5 and @xmath6 .",
    "this implies that even with different numbers of observed rows and columns , algorithm 2 has similar performance with row thresholding or column thresholding .",
    "we are also interested in the performance of algorithm 2 as @xmath228 and the ratio @xmath266 vary . to this end",
    ", we consider the setting where @xmath267 , @xmath268 , and the singular values of @xmath1 are chosen as @xmath262 .",
    "the results are shown in figure [ fig : m_ratio ] .",
    "it can be seen that when @xmath266 increases , the recovery is generally more accurate ; when @xmath266 is kept as a constant , the average loss does decrease but not converge to zero as @xmath228 increases .",
    "[ fig : m_ratio ]",
    "we collect important technical tools in this section .",
    "the first lemma is about the inequalities of singular values in the perturbed matrix .",
    "[ lm : x_y_sv ] suppose @xmath269 , @xmath270 , @xmath271 , @xmath272 ,    1 .",
    "@xmath273 for @xmath274 ; 2 .   if we further have @xmath275 , we must have @xmath276 , @xmath277 for @xmath274 .",
    "[ lm : schatten_q ] suppose @xmath278 are two arbitrary matrices , denote @xmath279 , @xmath280 as the schatten-@xmath14 norm and spectral norm respectively , then we have @xmath281    the following two lemmas provide examples that illustrate nnm fails to recover @xmath167 .",
    "[ lm : nuclear_norm_fail ] assume @xmath282 , where @xmath69 and @xmath70 are two i.i.d .",
    "standard gaussian matrices .",
    "let @xmath1 is divided into blocks as .",
    "suppose @xmath283 then the nnm fails to recover @xmath11 with probability at least @xmath284 .",
    "[ lm : nuclear_norm_1 ] denote @xmath285 as the @xmath286-dimensional vector with all entries 1 .",
    "suppose @xmath287 , and @xmath1 is divided into blocks as",
    ". then the nnm yields @xmath288    the following result is on the norm of a random submatrix of a given orthonormal matrix .",
    "[ lm : u_omega ] suppose @xmath289 is a fixed matrix with orthonormal columns ( hence @xmath290 ) .",
    "denote @xmath291 .",
    "suppose we uniform randomly draw @xmath292 rows ( with or without replacement ) from @xmath15 and note the index as @xmath213 and denote @xmath293 when @xmath294 for some @xmath295 and @xmath296 , we have @xmath297 with probability @xmath298 .",
    "the following results is about the spectral norm of the submatrix of a random orthonormal matrix .",
    "[ lm : u_haar ] suppose @xmath289 ( @xmath290 ) is with random orthonormal columns with haar measure . for all @xmath299 ,",
    "there exists constant @xmath300 depending only on @xmath301 such that when @xmath302 , we have @xmath303 } ) \\leq \\|u_{[1:n , : ] } \\| \\leq \\sqrt{\\frac{\\alpha_2 n}{p}}\\ ] ] with probability at least @xmath304 .",
    "* proof of lemma [ lm : x_y_sv ] . *    1 .   first , by a well - known",
    "fact about best low - rank approximation , @xmath305 hence , @xmath306 similarly @xmath307 .",
    "when we further have @xmath308 , we know the column space of @xmath309 and @xmath310 are orthogonal , then we have @xmath311 , which means @xmath312 .",
    "next , note that @xmath313 if we note @xmath314 as the @xmath23-th largest eigenvalue of the matrix , then we have @xmath315    @xmath316    * proof of lemma [ lm : schatten_q ] . * since @xmath317{\\sum_{i}\\sigma^q_i(xy ) } , \\quad \\|x\\|_q = \\sqrt[q]{\\sum_i\\sigma_i^q(x)},\\ ] ] it suffices to show @xmath318 . to this end",
    ", we have @xmath319 which finishes the proof of this lemma.@xmath316    since @xmath320 and @xmath321 and their submatrices are all i.i.d .",
    "standard matrices , by the random matrix theory ( corollary 5.35 in @xcite ) , for @xmath322 , we have with probability at least @xmath323 , the following inequalities hold , @xmath324 @xmath325 } b_2^t\\| \\leq ( \\sqrt{m_1 } + \\sqrt{r } + t)(\\sqrt{p_2 } + \\sqrt{r } + t ) \\overset{\\eqref{ineq : condition_nuclear}}{\\leq } \\left(\\frac{1}{4}\\sqrt{p_1 } + t\\right)\\left(\\frac{21}{20}\\sqrt{p_2 } + t\\right)\\ ] ] and @xmath326 } b_{2 , [ 1:m_2 , : ] } ^t\\| \\leq ( \\sqrt{p_1 } + \\sqrt{r } + t)(\\sqrt{m_2 } + \\sqrt{r } + t ) \\\\ \\overset{\\eqref{ineq : condition_nuclear}}{\\leq } & \\left(\\frac{21}{20}\\sqrt{p_1 } + t\\right)\\left(\\frac{1}{4}\\sqrt{p_2 } + t\\right ) . \\end{split}\\ ] ] denote @xmath327 and set @xmath328 . since @xmath329 , , we have @xmath330 and @xmath331 hence , with probability at least @xmath284 , @xmath332 , which implies that the nnm fails to recover @xmath11 . @xmath316    for convenience , we denote @xmath333 for any two real numbers @xmath334 .",
    "first , we can extend the unit vectors @xmath335 , @xmath336 and @xmath337 into orthogonal matrices , which we denote as @xmath338 , @xmath339 , @xmath340 , @xmath341 . next , for all @xmath342 , we must have @xmath343 where @xmath344 are with the first entry @xmath345 , @xmath346 and @xmath347 respectively and other entries 0 .",
    "therefore , we can see @xmath348_{[1,1 ] } \\end{bmatrix}\\right\\|_\\ast\\ ] ] and the equality holds if and only if @xmath349 is zero except the first entry .    by some calculation , we can see the nuclear norm of 2-by-2 matrix @xmath350 achieves its minimum if and only if @xmath351 hence , @xmath352 achieves the minimum of @xmath353 if and only if @xmath354 which means the minimizer @xmath355 .",
    "@xmath316    the proof of this lemma relies on operator - bernstein s inequality for sampling ( theorem 1 in @xcite ) . for two symmetric matrices @xmath1 , @xmath34",
    ", we say @xmath356 if @xmath357 is positive definite .",
    "by assumption , \\{@xmath358 } are uniformly random samples ( with or without replacement ) from @xmath359 .",
    "suppose @xmath360 then @xmath361 are symmetric matrices , @xmath362 are uniformly random samples ( with or without replacement ) from @xmath363 .",
    "in addition , we have @xmath364 @xmath365 @xmath366 for all @xmath295 , by theorem 1 in @xcite , @xmath367 the last inequality is due to the assumption that @xmath368 @xmath316    by the assumption on @xmath292 , we have @xmath369 or @xmath370 . when @xmath371 , we know @xmath372 and @xmath373 } = u$ ] is an orthogonal matrix , which means is clearly true .",
    "hence , we only need to prove the theorem under the assumption that @xmath374 is true . in this case",
    ", we must have @xmath370 .",
    "since @xmath15 has random orthonormal columns with haar measure , for any fixed vector @xmath375 , @xmath376 is identitical distributed as @xmath377 hence , @xmath378}v$ ] is identical distributed with @xmath379 and @xmath380}v\\|_2 \\text { is identical distributed as } \\sqrt{(\\sum_{i=1}^nx_i^2)(\\sum_{i=1}^px_i^2)^{-1}},\\ ] ] which is the also the square root of beta distribution .",
    "denote @xmath381 by lemma 1 in @xcite , when @xmath382 are i.i.d .",
    "standard normal , we have @xmath383 @xmath384 both hold with probability at least @xmath385 . here",
    "we let @xmath386 be small enough and only depending on @xmath301 such that @xmath387 combining the previous inequalities and , we have for any fixed unit vector @xmath388 , @xmath389}v\\|_2 ^ 2 \\leq \\frac{\\alpha_2 ' n}{p}\\ ] ] with probability at least @xmath385 , where @xmath390 only depends on @xmath391 .",
    "next , based on lemma 2.5 in @xcite , we can construct an @xmath392-net on the unit sphere of @xmath393 as @xmath34 , such that @xmath394 , where @xmath395 is to be determined later . under the event that",
    "@xmath396 , we suppose @xmath397}v\\|_2 ^ 2,\\quad \\kappa_2 = \\max_{\\|v\\|_2 = 1 } \\|u_{[1:n , : ] } v\\|_2 ^ 2.\\ ] ] for any @xmath398 in the unit sphere of @xmath393 , there must exists @xmath399 such that @xmath400 , which yields , @xmath401}v\\|_2 \\leq \\|u_{[1:n , : ] } v'\\|_2 + \\|u_{[1:n , : ] } ( v - v')\\|_2\\leq \\sqrt{\\alpha_2'n / p } + \\kappa_2\\varepsilon\\ ] ] @xmath401}v\\|_2 \\geq \\|u_{[1:n , : ] } v'\\|_2 - \\|u_{[1:n , : ] } ( v - v')\\|_2\\geq \\sqrt{\\alpha_1'n / p } - \\varepsilon \\kappa_2\\ ] ] these implies that @xmath402 , @xmath403 .",
    "hence , we can take @xmath392 depending on @xmath301 such that @xmath404 , @xmath405 , which implies .",
    "finally we estimate the probability that the event @xmath406 happens .",
    "we choose @xmath407 that only depends on @xmath408 and @xmath409 . if @xmath370 , @xmath410 so @xmath411 finally , we finish the proof of the lemma by setting @xmath412.@xmath316",
    "we prove proposition [ th : noiseless ] , theorems [ th : hat a ] and [ th : main_without_r ] , lemma [ lm : hat_r geq r ] , lemma [ lm : inequalities_theorem_2 ] , theorem [ th : lower_bound ] , corollary [ cr : random_column_row ] and corollary [ cr : randomuv ] in this section .",
    "since @xmath98 is of rank @xmath23 , which is the same as @xmath1 , all rows of @xmath1 must be linear combinations of the rows of @xmath98 .",
    "this implies all rows of @xmath108 is a linear combination of @xmath8 . since rank(@xmath108)@xmath413",
    ", we must have @xmath414 .",
    "besides , @xmath415 since @xmath8 is a submatrix of @xmath1 .",
    "so @xmath416 .",
    "simiarly , rows of @xmath26 is the linear combination of @xmath8 , so we have @xmath417 namely rows of @xmath10 is a linear combination of @xmath8 . by the argument before , we know @xmath11 can be represented as the same linear combination of @xmath9 as @xmath10 by @xmath8 , so we have @xmath418 which concludes the proof . @xmath316",
    "suppose @xmath419 are column orthonormalized matrices of @xmath103 and @xmath104 .",
    "@xmath420 and @xmath421 are the first @xmath23 left singular vectors of @xmath98 and @xmath108 , respectively . also , recall that we use @xmath422 to represent the projection onto the column space of @xmath15 .    1 .",
    "we first give the lower bound for @xmath423 , @xmath424 by the unilateral perturbation bound result in @xcite .",
    "since , @xmath425v^{\\intercal } , \\quad      p_{u_{11}^\\bot}a_{1\\bullet } = p_{u_{11}^\\bot}u_{1\\bullet}\\sigma v^{\\intercal } = [ 0 , p_{u_{11}^\\bot}u_{12}\\sigma_2]v^{\\intercal},\\ ] ] by @xmath216 is an orthogonal matrix , we can see @xmath426 ) \\geq \\sigma_{r}(u_{11}\\sigma_1 ) \\geq \\sigma_r(a)\\sigma_{\\min}(u_{11}),\\ ] ] @xmath427 so @xmath428 .",
    "besides , @xmath429 .",
    "apply the unilateral perturbation bound result in @xcite by setting @xmath430 , @xmath431 , we have @xmath432 moreover , @xmath433   \\mbox{diag}(\\sigma_1,\\sigma_2 )          v^{\\intercal } = [ u_{11}\\sigma_1~u_{12}\\sigma_2]v^{\\intercal } , $ ] and hence , @xmath434v^{\\intercal } \\cdot p_{v\\cdot[u_{11}\\sigma_1\\quad p_{u_{11 } } u_{12}\\sigma_2]^{\\intercal}}\\right\\|\\\\      = &   \\left\\|[0 \\quad p_{u_{11}^\\bot}u_{12}\\sigma_2 ] \\cdot p_{[u_{11}\\sigma_1\\quad p_{u_{11 } } u_{12}\\sigma_2]^{\\intercal}}\\right\\|      =   \\sup_{x\\in \\mathbb{r}^{p_2 } , \\|x\\|_2 = 1 } [ 0 \\quad p_{u_{11}^\\bot}u_{12}\\sigma_2 ] \\cdot p_{[u_{11}\\sigma_1\\quad p_{u_{11 } } u_{12}\\sigma_2]^{\\intercal } } x.      \\end{split}\\ ] ] when @xmath435 , let @xmath436 denote the projection of @xmath437 onto the column space of @xmath438^{\\intercal}$ ] .",
    "then @xmath439 and @xmath436 is in the column space of @xmath438^{\\intercal}$ ] .",
    "hence , @xmath440}\\|_2 } { \\|y_{[(m_1 + 1 ) : p_1]}\\|_2 } \\geq \\frac{\\sigma_{\\min}(u_{11}\\sigma_1 ) } { \\|p_{u_{11}}u_{12}\\sigma_2\\| } \\geq \\frac{\\sigma_{\\min}(u_{11})\\sigma_r(a ) } { \\sigma_{r+1}(a ) } \\ \\mbox{and } \\      \\|y_{[(m_1 + 1):p_1 ] } \\|_2 ^ 2 + \\|y_{[1:m_1]}\\|_2 ^ 2 \\leq 1,\\ ] ] which implies @xmath441}\\|_2 ^ 2 \\leq { \\sigma_{r+1}^2(a)}/{\\sigma^2_{\\min}(u_{11})\\sigma^2_r(a)+\\sigma^2_{r+1}(a)}$ ] .",
    "hence for all @xmath442 such that @xmath443 , @xmath444 \\cdot p_{[u_{11}\\sigma_1\\quad p_{u_{11 } } u_{12}\\sigma_2]^{\\intercal } } x\\right\\| \\leq & \\|p_{u_{11}^\\bot } u_{12}\\sigma_2\\|\\cdot \\|y_{[m_1 + 1 : p_1]}\\|_2\\\\      \\leq & \\sigma_{r+1}(a)\\frac{\\sigma_{r+1}(a)}{\\sqrt{\\sigma_{r+1}^2(a ) + \\sigma_{\\min}^2(u_{11})\\sigma_r^2(a)}}.      \\end{split}\\ ] ] this yields @xmath445 combining , we have @xmath446 since @xmath447 , we have @xmath448 similarly , we also have @xmath449 .",
    "2 .   following by , @xmath450 let  l \" ,  m \" ,  r \" stand for  left \" ,  middle \" and  right \" , @xmath451 @xmath452 @xmath453 by lemma [ lm : schatten_q ] in the supplement , we can see the following properties of these matrices , @xmath454 @xmath455 @xmath456 @xmath457 @xmath458 @xmath459 @xmath460 by , and the assumption , we can see @xmath461 , so @xmath462 @xmath463 finally , since @xmath464 , we have @xmath465      we only present proof for row thresholding as the column thresholding is essentially the same by working with @xmath466 .",
    "suppose @xmath101 are orthonormal basis of column vectors of @xmath102 .",
    "we denote @xmath467 } = \\hat m$ ] , @xmath468 } = \\hat n$ ] , which are exactly the same as the @xmath469 and @xmath470 in algorithm 1 .",
    "similarly to the proof of theorem [ th : hat a ] , we have . due to the assumption that @xmath471 , yields @xmath472 as shown in the supplementary material , we have        1 .   note that @xmath474 , we consider the decompositions of @xmath112 and let @xmath475 @xmath476 } = u _ { [ : , 1:\\hat r]}^{(2)\\intercal}u_{11}\\sigma_1v_{11}^{\\intercal}v^{(1 ) } _ { [ : , 1:\\hat r ] } + u^{(2)\\intercal } _ { [ : , 1:\\hat r]}u_{12}\\sigma_2v_{12}^{\\intercal}v^{(1 ) } _ { [ : , 1:\\hat r ] } \\triangleq b_{m,\\hat r } + e_{m , \\hat r},\\ ] ] @xmath477 } = u_{21}\\sigma_1v_{11}^{\\intercal}v^{(1 ) } _ { [ : , 1:\\hat r ] } + u_{22}\\sigma_2v_{12}^{\\intercal}v^{(1 ) } _ { [ : , 1:\\hat r ] } \\triangleq b_{l,\\hat r } + e_{l , \\hat r},\\ ] ] @xmath478 } = u _ { [ : , 1:\\hat r]}^{(2)\\intercal}u_{11}\\sigma_1v_{21}^{\\intercal } + u^{(2)\\intercal } _ { [ : , 1:\\hat r]}u_{12}\\sigma_2v_{22}^{\\intercal } \\triangleq b_{r,\\hat r } + e_{r , \\hat r}.\\ ] ] note that the square matrix @xmath479}m \\in \\mathbb{r}^{r \\times r}$ ] is a submatrix of @xmath480}m \\in \\mathbb{r}^{\\hat r \\times r}$ ] , we know @xmath481}^{(2)\\intercal}m)\\geq \\sigma_{\\min}(u _ { [ : , 1:r]}^{(2)\\intercal}m ) = \\sigma_{\\min}(\\hat",
    "m m ) \\overset{\\eqref{eq : hatmm_hatnn}}{\\geq } \\sqrt{\\frac{3824}{3825}}.\\ ] ] similarly , @xmath482}^{(1)\\intercal}n ) \\geq \\sqrt{\\frac{3824}{3825}}$ ]",
    ". by @xmath101 are the orthonormal basis of column vectors of @xmath102 , we have @xmath483 , @xmath484 , and @xmath485}^{(2)\\intercal } u_{11 } )      \\geq & \\sigma_{\\min}(u _ { [ : , 1:\\hat r]}^{(2)\\intercal}m)\\sigma_{\\min}(m^{\\intercal } u_{11 } ) \\geq \\sqrt{\\frac{3824}{3825}}\\sigma_{\\min}(u_{11 } ) ;      \\end{split}\\ ] ] similarly , we also have @xmath486}^{(1)\\intercal } v_{11})\\geq \\sqrt{\\frac{3824}{3825}}\\sigma_{\\min}(v_{11}).\\ ] ] and immediately yield @xmath487 besides , we also have @xmath488 2 .",
    "next , we consider the svd of @xmath124}$ ] @xmath489 } = j\\lambda k^{\\intercal},\\quad j,\\lambda , k\\in\\mathbb{r}^{\\hat r\\times \\hat r}.\\ ] ] for convenience , we denote @xmath490 } ,   \\lambda_2 = \\lambda_{[(r+1):\\hat r , ( r+1):\\hat r]},$ ] @xmath491},\\quad j_2 = j _ { [ : , ( r+1 ) : \\hat r]},\\quad k_1 = k _ { [ : , 1:r]},\\quad k_2 = k _ { [ : , ( r+1 ) : \\hat r]},\\ ] ] suppose @xmath492 is an orthonormal basis of the column space of @xmath493 ; @xmath494 is an orthonormal basis of the column space of @xmath495 .",
    "denote @xmath496 as the linear span of the column space of the matrix .",
    "we want to show @xmath497 is close to @xmath498 ; while @xmath499 is close to @xmath500 .",
    "so in the rest of this step , we try to establish bounds for @xmath501 and @xmath502 .",
    "actually , @xmath503 } = b_{m , \\hat r } + e_{m , \\hat r } = \\left ( b_{m , \\hat r } + p_{m_z}e_{m , \\hat r}\\right )   + p_{m_z^\\perp}e_{m , \\hat r}.\\ ] ] now we set @xmath504 , @xmath505 , then we have @xmath506 besides , by the definition of @xmath493 and @xmath507 we know @xmath508 . also based on the definition of @xmath310 , we know @xmath509 .",
    "now the unilateral perturbation bound in @xcite yields @xmath510 the right hand side of the inequality above is an increasing function of @xmath511 . since @xmath512 , @xmath513",
    "similarly , we also have @xmath514 3 .",
    "we next derive useful expressions of @xmath11 and @xmath167 .",
    "first we introduce the following quantities , @xmath515}k_1 \\overset{\\eqref{eq : z_11_decompose}}{= } j_1^{\\intercal}b_{m,\\hat r}k_1 + j_1^{\\intercal}e_{m , \\hat r } k_1 \\triangleq b_{m1 } + e_{m1},\\ ] ] @xmath516}k_2 \\overset{\\eqref{eq : z_11_decompose}}{= } j_2^{\\intercal}b_{m,\\hat r}k_2 + j_2^{\\intercal}e_{m ,",
    "\\hat r } k_2 \\triangleq b_{m2 } + e_{m2},\\ ] ] @xmath517}k_1 \\overset{\\eqref{eq : z_21_decompose}}{= } b_{l,\\hat r}k_1 + e_{l , \\hat r } k_1 \\triangleq b_{l1 } + e_{l1},\\ ] ] @xmath517}k_2 \\overset{\\eqref{eq : z_21_decompose}}{= } b_{l,\\hat r}k_2 + e_{l , \\hat r } k_2 \\triangleq b_{l2 } + e_{l2},\\ ] ] @xmath518 } \\overset{\\eqref{eq : z_12_decompose}}{= } j_1^{\\intercal}b_{r,\\hat r } + j_1^{\\intercal}e_{r , \\hat r } \\triangleq b_{r1 } + e_{r1},\\ ] ] @xmath519 } \\overset{\\eqref{eq : z_12_decompose}}{= } j_2^{\\intercal}b_{r,\\hat r } + j_2^{\\intercal}e_{r ,",
    "\\hat r } \\triangleq b_{r2 } + e_{r2}.\\ ] ] + since @xmath520}^{(1 ) } k_1 \\left(j_1^{\\intercal } u^{(2)\\intercal } _ { [ : , 1:\\hat r ] } u_{11 } \\sigma_1v_{11}^{\\intercal}v_{[:,1:\\hat r]}^{(1 ) } k_1\\right)^{-1 } j_1^{\\intercal } u _ { [ : , 1:\\hat r]}^{(2)\\intercal}u_{11}\\sigma_1v_{21}^{\\intercal } =   u_{21}\\sigma_1v_{21}^{\\intercal } ,      \\end{split}\\ ] ] we can characterize @xmath521 by these new notations as @xmath522 @xmath523}z_{11 , [ 1:\\hat r , 1:\\hat r]}^{-1}z_{12,[1:\\hat r , :] }      \\overset{\\eqref{eq : z_11_svd}}{= } z_{21 , [: , 1:\\hat r]}k \\left(j^{\\intercal}z_{11 , [ 1:\\hat r , 1:\\hat r]}k\\right)^{-1}j^{\\intercal}z_{12,[1:\\hat r , :] } \\nonumber \\\\      = & \\left(z_{21,[1:\\hat r ] } k_1 + z_{21,[1:\\hat r ] } k_2\\right ) \\left(j_1^{\\intercal}z_{11,[1:\\hat r , 1:\\hat r]}k_1 + j_2^{\\intercal}z_{11,[1:\\hat r , 1:\\hat r]}k_2\\right)^{-1}\\left(j_1^{\\intercal}z_{12 , [ 1:\\hat r ] } + j_2^{\\intercal } z_{12,[1:\\hat r]}\\right ) \\nonumber \\\\      \\overset{\\eqref{eq : jk_begin}-\\eqref{eq : jk_last}}{= } & \\sum_{k=1}^2 ( b_{lk } + e_{lk})(b_{mk } + e_{mk})^{-1}(b_{rk } + e_{rk } ) \\label{eq : hata_22_proof }              \\end{aligned}\\ ] ] 4 .",
    "we now establish a number of bounds for the terms on the right hand side of - .",
    "+ [ lm : inequalities_theorem_2 ] based on the assumptions above , we have @xmath524 @xmath525 @xmath526 @xmath527 @xmath528 + the proof of lemma [ lm : inequalities_theorem_2 ] is given in the supplement . 5 .",
    "we finally give the upper bound of @xmath529 . by and",
    ", we can split the loss as , @xmath530 we will analyze them separately .",
    "first , @xmath531 ; second , @xmath532 the analysis of @xmath533 is similar to the proof of theorem [ th : hat a ] .",
    "we have @xmath534 from , , , and the fact that @xmath535 and @xmath536 , @xmath537 this concludes the proof.@xmath316      in order to prove this lemma , we just need to prove that the for - loop in algorithm 2 will break for some @xmath538 .",
    "this can be shown by proving the break condition @xmath539 } z_{11 , [ 1:s , 1:s]}^{-1}\\| \\leq t_r,\\ ] ] hold for @xmath154 .",
    "we adopt the definitions in , , , then we have @xmath540 } & = u^{(2)\\intercal } _ { [ : , 1:r]}a_{11}v^{(1 ) } _ { [ : , 1:r ] } = \\hat m^{\\intercal } a_{11 } \\hat n\\\\ & = \\hat m^{\\intercal}u_{11}\\sigma_1 v_{11}^{\\intercal } \\hat",
    "n + \\hat m^{\\intercal } u_{12}\\sigma_2 v_{12}^{\\intercal } \\hat n \\\\ & = b_m + e_m , \\end{split}\\ ] ] @xmath541 } = a_{21}v^{(1 ) } _ { [ : , 1:r ] } = \\left(u_{21}\\sigma_1v_{11}^{\\intercal } + u_{22}\\sigma_2 v_{12}^{\\intercal}\\right)\\hat n = b_l + e_l.\\ ] ] hence , @xmath542}z_{11 , [ 1:r , 1:r]}^{-1}\\right\\|   = & \\|(b_l+e_l)(b_m+e_m)^{-1}\\|\\\\ \\leq & \\left\\|b_lb_m^{-1}\\sum_{i=0}^{\\infty}(-e_m b_m^{-1})^i \\right\\| + \\left\\|e_l b_m^{-1}\\sum_{i=0}^\\infty(-e_m b_m^{-1})^i\\right\\|\\\\ \\leq & \\left(\\|b_lb_m^{-1}\\| + \\|e_l\\|\\|b_m^{-1}\\|\\right)\\frac{1}{1-\\|e_m b_m^{-1}\\|}\\\\ \\overset{\\eqref{ineq : sigma_min b_m},\\eqref{ineq : b_l1b_m1 ^ -1}}{\\leq } & \\left(\\frac{\\sqrt{45/44}}{\\sigma_{\\min}(u_{11 } ) } + \\frac{45\\sigma_{r+1}(a)}{44\\sigma_r(a)\\sigma_{\\min}(u_{11})\\sigma_{\\min}(v_{11})}\\right ) \\frac{1}{1 -\\frac{45\\sigma_{r+1}(a)}{44\\sigma_r(a)\\sigma_{\\min}(u_{11})\\sigma_{\\min}(v_{11 } ) } } \\\\",
    "\\leq & \\frac{1.36}{\\sigma_{\\min}(u_{11 } ) } + 0.35 \\leq t_r , \\end{split}\\ ] ] which finished the proof of the lemma .",
    "@xmath316      first , since @xmath543 and @xmath544 are an orthonormal basis of @xmath493 and @xmath545 , we have @xmath546 and @xmath547 and @xmath548 which gives .",
    "\\left(j_1^{\\intercal}u _ { [ : , 1:\\hat r]}^{(2)\\intercal}u_{11}\\sigma_1 v_{11}^{\\intercal}v _ { [ : , 1:\\hat r]}^{(1)}k_1\\right)^{-1 } \\right\\| =   \\left\\|u_{21}\\left(j_1^{\\intercal}u _ { [ : , 1:\\hat r]}^{(2)\\intercal}u_{11}\\right)^{-1}\\right\\|\\\\ \\leq & \\frac{1}{\\sigma_{\\min}(j_1^{\\intercal}u _ { [ : , 1:\\hat r]}^{(2)\\intercal}u_{11 } ) } =   \\frac{1}{\\sigma_{\\min}(j_1^{\\intercal } p_{m_z } ( u^{(2)\\intercal } _ { [ : , 1:\\hat r]}u_{11 } ) ) } = \\frac{1}{\\sigma_{\\min}((j_1^{\\intercal } m_z ) ( m_z^{\\intercal } u^{(2)\\intercal } _ { [ : , 1:\\hat r]}u_{11}))}\\\\ \\leq & \\frac{1}{\\sigma_{\\min}(j_1^{\\intercal}m_z ) } \\cdot \\frac{1}{\\sigma_{\\min}(u_{[:,1:\\hat r]}^{(2)\\intercal } u_{11 } ) } \\overset{\\eqref{ineq : uu_11}\\eqref{ineq : j_1^tm_z}}{\\leq } \\frac{\\sqrt{3825/3824}}{\\sqrt{0.859}\\sigma_{\\min}(u_{11 } ) } , \\end{split}\\ ] ] which gives the first part of .",
    "here we used the fact that @xmath550}^{(1)}k_1 $ ] is a square matrix ; @xmath507 is the orthonormal basis of the column space of @xmath121 } = u _ { [ : , 1:\\hat r]}^{(2)\\intercal}u_{11}\\sigma_1 v_{11}^{\\intercal}v _ { [ : , 1:\\hat r]}^{(1)}$ ] .",
    "similarly we have the later part of , @xmath551 based on the definitions , we have the bound for all  @xmath552 \" terms in - , i.e. .",
    "now we move on to . by the svd of @xmath124}$ ] and the partition , we know @xmath553^{\\intercal}z_{11 , [ 1:\\hat r , 1:\\hat r ] } [ k_1 ~ k_2]\\right)^{-1 } = \\begin{bmatrix } \\lambda_1 & 0 \\\\ 0 & \\lambda_2 \\end{bmatrix}^{-1 } = \\begin{bmatrix } \\left(j_1^{\\intercal}z_{11 , [ 1:\\hat r , 1:\\hat r ] } k_1\\right)^{-1 } & 0 \\\\ 0 & \\left(j_2^{\\intercal}z_{11 , [ 1:\\hat r , 1:\\hat r ] } k_2\\right)^{-1 } \\end{bmatrix}.\\ ] ] hence",
    ", we have @xmath554 } k_2 \\left(j_2^{\\intercal}z_{11 , [ 1:\\hat r , 1:\\hat r ] } k_2\\right)^{-1}\\right\\|\\\\ = & \\left\\|z_{21 , [ : , 1:\\hat r ] } [ k_1 ~ k_2 ] \\left([j_1 ~ j_2]^{\\intercal}z_{11 , [ 1:\\hat r , 1:\\hat r ] } [ k_1 ~ k_2]\\right)^{-1 } - z_{21,[1:\\hat r ] } k_1 \\left(j_1^{\\intercal}z_{11 , [ 1:\\hat r , 1:\\hat r ] } k_1\\right)^{-1}\\right\\|\\\\ \\leq & \\left\\| z_{21 , [ : , 1:\\hat r ] } \\left(z_{11 , [ 1:\\hat r , 1:\\hat r]}\\right)^{-1 } \\right\\| + \\left\\|(b_{l1 } + e_{l1})(b_{m1 } + e_{m1})^{-1}\\right\\|\\\\ \\leq & t_r + \\left\\|b_{l1}\\cdot b_{m1}^{-1}\\sum_{i=0}^\\infty ( -e_{m1}b_{m1}^{-1})^i\\right\\| + \\left\\|e_{l1}\\cdot b_{m1}^{-1}\\sum_{i=0}^\\infty ( -e_{m1}b_{m1}^{-1})^i\\right\\| \\\\ \\leq & t_r + \\left(\\|b_{l1}b_{m1}^{-1}\\| + \\|e_{l1}\\|\\|b_{m1}^{-1}\\|\\right ) \\frac{1}{1 - \\|e_{m1}\\|\\|b_{m1}^{-1}\\|}\\\\ \\overset{\\eqref{ineq : sigma_min_b_m1}\\eqref{ineq : b_l1b_m1 ^ -1}\\eqref{ineq : e_mt}}{\\leq } & t_r + \\left(\\frac{\\sqrt{3825/3824}}{\\sqrt{0.859}\\sigma_{\\min}(u_{11 } ) } + \\frac{1}{3.43}\\right)\\cdot \\frac{1}{1 - 1/3.43 } , \\end{split}\\ ] ] which proves",
    ". since @xmath124 } = b_{m,\\hat r } + e_{m , \\hat r}$ ] and by definition , @xmath555 , by lemma [ lm : x_y_sv ] , we know @xmath556 } ) \\leq \\sigma_i(e_{m , \\hat r } ) , \\quad \\forall i\\geq 1.\\ ] ] then @xmath557 } k_2\\|_q + \\|e_{m2}\\|_q\\\\ = & \\sqrt[q]{\\sum_{i = r+1}^{\\hat r } \\sigma_i^q(z_{11 , [ 1:\\hat r , 1:\\hat r ] } ) } + \\|e_{m2}\\|_q \\leq \\sqrt[q]{\\sum_{i=1}^{\\hat r - r } \\sigma_i^q(e_{m , \\hat r } ) } + \\|e_{m2}\\|_q\\\\ \\leq & \\|e_{m , \\hat r}\\|_q + \\|e_{m2}\\|_q \\overset{\\eqref{ineq : e_mt}}{\\leq } 2\\|a_{-\\max(r)}\\|_q .",
    "\\end{split}\\ ] ] same to the process of , we know @xmath558k_1 } \\frac{1}{\\sigma_{\\min}(v_{11}^{\\intercal}v _ { [ : , 1:\\hat r]}^{(1)}k_1 ) } \\leq \\frac{\\sqrt{3825/3824}}{\\sqrt{0.859}\\sigma_{\\min}(v_{11})}.\\ ] ] also , @xmath559 .",
    "hence , @xmath560}^{(2)\\intercal}u_{11}\\sigma_1v_{21}^{\\intercal}\\|_q\\\\ = & \\|j_2^{\\intercal } u _ { [ : , 1:\\hat r]}^{(2)\\intercal}u_{11}\\sigma_1(v_{11}^{\\intercal}v _ { [ : , 1:\\hat r]}^{(1 ) } k_1 ) ( v_{11}^{\\intercal}v _ { [ : , 1:\\hat r]}^{(1 ) } k_1)^{-1}v_{21}^{\\intercal}\\|_q\\\\ \\leq & \\|b_{m2}\\|_q \\cdot \\|(v_{11}^{\\intercal}v _ { [ : , 1:\\hat r]}^{(1 ) } k_1)^{-1}\\| \\cdot \\|v_{21}^{\\intercal}\\|\\\\ \\overset{\\eqref{ineq : b_m2}\\eqref{ineq : v_11v_[]k_1}}{\\leq } & \\frac{2\\sqrt{3825/3824}}{\\sqrt{0.859}\\sigma_{\\min}(v_{11})}\\|a_{-\\max(r)}\\|_q . \\end{split}\\ ] ] which proves .",
    "@xmath316      the idea of proof is to construct two matrices @xmath561 both in @xmath562 such that they have the identical first @xmath5 rows and @xmath6 columns , but differ much in the remaining block .",
    "suppose @xmath563 are fixed numbers , @xmath392 is a small real number .",
    "we first consider the following 2-by-2 matrix @xmath564 suppose the larger and smaller singular value of @xmath565 are @xmath566 and @xmath567 , then we have @xmath568 as @xmath569 ; since @xmath570 , we also have @xmath571 as @xmath569 .",
    "if @xmath565 defined in has svd @xmath572 then we also have @xmath573 as @xmath569 .",
    "now we set @xmath574 , @xmath575 , @xmath576 , @xmath577 , where @xmath578 is some small positive number to be specify later .",
    "we construct @xmath579 and @xmath580 such that , @xmath581 @xmath582 here we use @xmath583 to note the identity matrix of dimension @xmath23 .",
    "then we construct @xmath584 and @xmath585 as @xmath586 where @xmath584 and @xmath585 are with identical first @xmath5 rows and @xmath6 columns .",
    "since the svd of @xmath565 is given as , the svd of @xmath584 can be written as @xmath587 where @xmath588 @xmath589 @xmath590 hence , @xmath591 @xmath592 also , @xmath593 as @xmath569 .",
    "so we have @xmath594 when @xmath392 is small enough .",
    "similarly @xmath595 when @xmath392 is small enough .",
    "now we also have @xmath596 , @xmath597 .",
    "@xmath598 .    finally for any estimate @xmath167",
    ", we must have @xmath599 as @xmath569 .",
    "since @xmath600 and are with identical first @xmath5 rows and @xmath6 columns , we must have @xmath601 let @xmath602 , since @xmath603 , we have @xmath604 which finished the proof of theorem . @xmath316      we first prove the second part of the corollary .",
    "we set @xmath605 .",
    "since @xmath22}\\in\\mathbb{r}^{p_1\\times r}$ ] is with orthonormal columns , by lemma [ lm : u_omega ] and @xmath606 we have @xmath607 } ) \\geq \\sqrt{\\frac{\\alpha m_1}{p_1}}\\ ] ] with probability at least @xmath608 .",
    "when holds , by the condition , we know @xmath609 when @xmath610 , we have @xmath611 hence we can apply theorem [ th : main_without_r ] , for @xmath39 we must have @xmath612 which finishes the proof of the second part of corollary [ cr : random_column_row ] .",
    "besides , the proof for the third part is the same as the second part after we take the transpose of the matrix .",
    "for the first part , the proof is also similar .",
    "again we set @xmath605 .",
    "then we have @xmath613 so @xmath614 } ) \\geq \\sqrt{\\frac{\\alpha m_1}{p_1 } } , \\quad \\sigma_{\\min}(v_{11 } ) = \\sigma_{\\min } ( v_{[\\omega_2 , 1:r ] } ) \\geq \\sqrt{\\frac{\\alpha m_2}{p_2}}\\ ] ] with probability at least @xmath615 .",
    "when holds , we have @xmath616 when @xmath242 or @xmath617 , similarly to the first part we have @xmath618 hence we can apply theorem [ th : main_without_r ] and get @xmath619 @xmath316      suppose @xmath620 , since @xmath22}\\in\\mathbb{r}$ ] is with random orthonormal columns of haar measure , we can apply lemma [ lm : u_haar ] and find some @xmath621 and @xmath622 such that when @xmath623 , @xmath624 } ) \\geq \\frac{136}{165}\\sqrt{\\frac{m_1}{p_1}}\\ ] ] with probability at least @xmath625 . when happen",
    ", we have @xmath626 @xmath627 hence we can apply theorem [ th : main_without_r ] , for @xmath39 , we have @xmath612 which finishes the proof of the corollary .",
    "@xmath316        first , we construct a grid @xmath628 of non - negative numbers based on a pre - selected positive integer @xmath629 .",
    "denote @xmath630 i.e. the largest singular value of the observed blocks .",
    "for penalized nuclear norm minimization , we let @xmath631 .",
    "next , for a given positive integer @xmath632 , we randomly divide the integer set @xmath633 into two groups of size @xmath634 , @xmath635 for @xmath636 times . for @xmath637 , we denote by @xmath638 and @xmath639 the index sets of the two groups for the @xmath640-th split . then the penalized nuclear norm minimization estimator is applied to the first group of data : @xmath641}$ ] , i.e. the data of the observation set @xmath642 , with each value of the tuning parameter @xmath643 and denote the result by @xmath644 .",
    "note that we did not use the observed block @xmath645}$ ] in calculating @xmath644 .",
    "instead , @xmath645}$ ] is used to evaluate the performance of the tunning parameter @xmath646 . set @xmath647_{[j_2^h , ( m_2 + 1):p_2 ] } - a_{[j_2^h , ( m_2 + 1):p_2 ] } \\right\\|_f^2.\\ ] ]",
    "finally , the tuning parameter is chosen as @xmath648 and the final estimator @xmath649 is calculated using this choice of the tuning parameter @xmath650 .",
    "in all the numerical studies with penalized nuclear norm minimization in sections [ simulation.sec ] and [ application.sec ] , we use 5-cross - validation ( i.e. , @xmath651 ) , @xmath652 to select the tuning parameter ."
  ],
  "abstract_text": [
    "<S> matrix completion has attracted significant recent attention in many fields including statistics , applied mathematics and electrical engineering . current literature on matrix completion </S>",
    "<S> focuses primarily on independent sampling models under which the individual observed entries are sampled independently . </S>",
    "<S> motivated by applications in genomic data integration , we propose a new framework of structured matrix completion ( smc ) to treat structured missingness by design . </S>",
    "<S> specifically , our proposed method aims at efficient matrix recovery when a subset of the rows and columns of an approximately low - rank matrix are observed . </S>",
    "<S> we provide theoretical justification for the proposed smc method and derive lower bound for the estimation errors , which together establish the optimal rate of recovery over certain classes of approximately low - rank matrices . </S>",
    "<S> simulation studies show that the method performs well in finite sample under a variety of configurations . </S>",
    "<S> the method is applied to integrate several ovarian cancer genomic studies with different extent of genomic measurements , which enables us to construct more accurate prediction rules for ovarian cancer survival .    </S>",
    "<S> * keywords : * constrained minimization , genomic data integration , low - rank matrix , matrix completion , singular value decomposition , structured matrix completion . </S>"
  ]
}