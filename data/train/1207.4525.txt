{
  "article_text": [
    "in the last decade , a growing number of large - scale knowledge bases have been created online .",
    "examples of domains include music , movies , publications and biological data , http://www.imdb.com/[imdb ] , http://www.informatik.uni-trier.de/~ley/db[dblp ] and http://www.uniprot.org/[unitprot ] . ] .",
    "as these knowledge bases sometimes contain both overlapping and complementary information , there has been growing interest in attempting to merge them by _ aligning _ their common elements .",
    "this alignment could have important uses for information retrieval and question answering .",
    "for example , one could be interested in finding a scientist with expertise on certain related protein functions ",
    "information which could be obtained by aligning a biological database with a publication one .",
    "unfortunately , this task is challenging to automate as different knowledge bases generally use different terms to represent their entities , and the space of possible matchings grows exponentially with the number of entities .",
    "a significant amount of research has been done in this area  particularly under the umbrella term of _ ontology matching _ @xcite .",
    "an ontology is a formal collection of world knowledge and can take different structured representations . in this paper",
    ", we will use the term _ knowledge base _ to emphasize that we assume very little structure about the ontology ( to be specified in section  [ sec : problem ] ) . despite the large body of literature in this area ,",
    "most of the work on ontology matching has been demonstrated only on fairly small datasets of the order of a few hundred entities .",
    "in particular , shvaiko and euzenat  @xcite identified _ large - scale evaluation _ as one of the ten challenges for the field of ontology matching .    in this paper",
    ", we consider the problem of aligning the _ instances _ in _ large _ knowledge bases , of the order of millions of entities and facts , where _ aligning _ means automatically identifying corresponding entities and interlinking them . our starting point was the challenging task of aligning the movie database to the wikipedia - based  @xcite , as another step towards the semantic web vision of interlinking different sources of knowledge which is exemplified by the linking open data initiative  @xcite .",
    "initial attempts to match entities to entities by naively exploiting string and neighborhood information failed , and so we designed ( ) , a scalable greedy iterative algorithm which is able to exploit previous matching decisions as well as the relationship graph information between entities .    the design decisions behind were both to be able to take advantage of the combinatorial structure of the matching problem ( by contrast with database record linkage approaches which make more independent decisions ) as well as to focus on a simple approach which could be scalable .",
    "works in two stages : it first starts with a small seed matching assumed to be of good quality .",
    "then the algorithm incrementally augments the matching by using _ both _ structural information and properties of entities such as their string representation to define a modular score function .",
    "some key aspects of the algorithm are that ( 1 ) it uses the current matching to obtain structural information , thereby harnessing information from previous decisions ; ( 2 ) it proposes candidate matches in a _",
    "local _ manner , from the structural information ; and ( 3 ) it makes greedy decisions , enabling a scalable implementation .",
    "a surprising result is that we obtained accurate large - scale matchings in our experiments despite the greediness of the algorithm .",
    "[ [ contributions ] ] contributions + + + + + + + + + + + + +    the contributions of the present work are the following :    1 .",
    "we present , a knowledge base alignment algorithm which can handle millions of entities .",
    "the algorithm is easily extensible with tailored scoring functions to incorporate domain knowledge .",
    "it also provides a natural tradeoff between precision and recall , as well as between computation and recall .",
    "2 .   in the context of testing the algorithm , we constructed two large - scale partially labeled knowledge base alignment datasets with hundreds of thousands of ground truth mappings .",
    "we expect these to be a useful resource for the research community to develop and evaluate new knowledge base alignment algorithms .",
    "we provide a detailed experimental comparison illustrating how improves over the state - of - the - art .",
    "is able to align knowledge bases with millions of entities with over 95% precision in less than two hours ( a 50x speed - up over  @xcite ) . on standard benchmark datasets ,",
    "obtains solutions with higher f - measure than the best previously published results .",
    "the remainder of the paper is organized as follows .",
    "section  [ sec : problem ] presents the knowledge base alignment problem with a real - world example as motivation for our assumptions .",
    "we describe the algorithm in section  [ sec : algorithm ] .",
    "we evaluate it on benchmark and on real - world datasets in section  [ sec : experiments ] , and situate it in the context of related work in section  [ sec : related ] .",
    "consider merging the information in the following two knowledge bases :    1 .   , a large semantic knowledge base derived from english wikipedia  @xcite , wordnet  @xcite and geonames .",
    "2 .   , a large popular online database that stores information about movies .    the information in is available as a long list of triples ( called _ facts _ ) that we formalize as : @xmath0 which means that the directed relationship @xmath1 holds from entity @xmath2 to entity @xmath3 , such as @xmath4 .",
    "the information from was originally available as several files which we merged into a similar list of triples .",
    "we call these two databases _ knowledge bases _ to emphasize that we are not assuming a richer representation , such as rdfs @xcite , which would distinguish between classes and instances for example . in the language of ontology matching ,",
    "our setup is the less studied _ instance matching _ problem , as pointed out by castano et al .",
    "@xcite , for which the goal is to match concrete instantiations of concepts such as specific actors and specific movies rather than the general actor or movie class .",
    "comes with an rdfs representation , but not ; therefore we will focus on methods that do not assume or require a class structure or rich hierarchy in order to find a one - to - one matching of instances between and .",
    "we note that in the full generality of the ontology matching problem , both the schema and the instances of one ontology are to be related with the ones of the other ontology .",
    "moreover , in addition to the ` issameas ` ( or `` @xmath5 '' ) relationship that we consider , these matching relationships could be ` ismoregeneralthan ` ( `` @xmath6 '' ) , ` islessgeneralthan ` ( `` @xmath7 '' ) or even ` haspartialoverlap ` . in our example , because the number of relations in the knowledge bases is relatively small ( 108 in and 10 in ) , we could align the relations manually , discovering six equivalent ones as listed in table  [ tab : relations ] . as we will see in our experiments ,",
    "focussing uniquely on the ` issameas ` type of relationship between instances of the two knowledge bases is sufficient in the - setup to cover most cases .",
    "the exceptions are rare enough for to obtain useful results while making the simplifying assumption that the alignment between the instances is _ injective _ ( 1 - 1 ) .",
    ".manually matched relations between and .",
    "the starred pairs are actually pairs of _ properties _ , as defined in the text.[tab : relations ] [ cols=\"^,^ \" , ]     [ [ relationships - vs .- properties ] ] relationships vs. properties + + + + + + + + + + + + + + + + + + + + + + + + + + + +    given our assumption that the alignment is 1 - 1 , it is important to distinguish between two types of objects which could be present in the list of triples : _ entities _ vs. _ literals_. by our definition , the _ entities _ will be the only objects that we will try to align  they will be objects like specific actors or specific movies which have a clear identity",
    ". the _ literals _ , on the other hand , will correspond to a value related to an entity through a special kind of relationship that we will call _ property_.",
    "the defining characteristic of literals is that it would not make sense to try to align them between the two knowledge bases in a 1 - 1 fashion .",
    "for example , in the triple @xmath8 , @xmath9 , @xmath10 , the object ` 1999 - 12 - 11 ` could be interpreted as a literal representing the value for the property ` wascreatedondate ` for the entity ` m1 ` .",
    "the corresponding property in our version of is ` hasproductionyear ` which has values only at the year granularity ( ` 1999 ` ) . the 1 - 1 restriction would prevent us to align both ` 1999 - 12 - 11 ` and ` 1999 - 12 - 10 ` to ` 1999 ` . on the other hand",
    ", we can use these literals to define a similarity score between entities from the two knowledge bases ( for example in this case , whether the year matches , or how close the dates are to each other ) .",
    "we will thus have two types of triples : entity - relationship - entity and entity - property - literal .",
    "we assume that the distinction between relationships and properties ( which depends on the domain and the user s goals ) is easy to make ; for example , in the dataset that we also used in our experiments , the entities would have unique identifiers but not the literals .",
    "figure  [ fig : example ] provides a concrete example of information presents in the two knowledge bases that we will keep re - using in this paper .",
    "we are now in a position to state more precisely the problem that we address .    *",
    "definition : * a _ knowledge base _ @xmath11 is a tuple + @xmath12 where @xmath13 , @xmath14 , @xmath15 and @xmath16 are sets of entities , literals , relationships and properties respectively ; @xmath17 is a set of relationship - facts whereas @xmath18 is a set of property - facts ( both can be represented as a simple list of triples ) . to simplify the notation , we assume that all inverse relations are also present in @xmath19  that is , if @xmath20 is in @xmath19 , we also have @xmath21 in @xmath19 , effectively doubling the number of possible relations in the @xmath11 .",
    "* problem : one - to - one alignment of instances between two knowledge bases . * given two knowledge bases @xmath22 and @xmath23 as well as a partial mapping between their corresponding relationships and properties , we want to output a 1 - 1 partial mapping @xmath24 from @xmath25 to @xmath26 which represents the semantically equivalent entities in the two knowledge bases ( by partial mapping , we mean that the domain of @xmath24 does not have to be the whole of @xmath25 ) .",
    "standard approaches for the ontology matching problem , such as rimom  @xcite , could be used to align small knowledge bases .",
    "however , they do not scale to millions of entities as needed for our task given that they usually consider all pairs of entities , suffering from a quadratic scaling cost .",
    "on the other hand , the related problem of identifying duplicate entities known as _ record linkage _ or _ duplicate detection _ in the database field , and _ co - reference resolution _ in the natural langue processing field , do have scalable solutions  @xcite , though these do not exploit the 1 - 1 matching combinatorial structure present in our task , which reduces their accuracy .",
    "more specifically , they usually make independent decisions for different entities using some kind of similarity function , rather than exploiting the competition between different assignments for entities .",
    "a notable exception is the work on _ collective _ entity resolution by bhattacharya and getoor  @xcite , solved using a greedy agglomerative clustering algorithm .",
    "the algorithm that we present in section  [ sec : algorithm ] can actually be seen as an efficient specialization of their work to the task of knowledge base alignment .",
    "another approach to alignment arises from the word alignment problem in natural language processing  @xcite , which has been formulated as a maximum weighted bipartite matching problem  @xcite ( thus exploiting the 1 - 1 matching structure ) .",
    "it also has been formulated as a quadratic assignment problem in  @xcite , which encourages neighbor entities in one graph to align to neighbor entities in the other graph , thus enabling alignment decisions to depend on each other  see the caption of figure  [ fig : example ] for an example of this in our setup .",
    "the quadratic assignment formulation  @xcite , which can be solved as an integer linear program , is np - hard in general though , and these approaches were only used to align at most one hundred entities . in the algorithm that we propose , we are interested in exploiting both the 1 - 1 matching constraint , as well as building on previous decisions , like these word alignment approaches , but in a scalable manner which would handle millions of entities . does this by greedily optimizing the quadratic assignment objective , as we will describe in section  [ ssec : greedy ] .",
    "finally , suchanek et al .",
    "@xcite recently proposed an ontology matching approach called that they have succeeded to apply on the alignment of to as well , though the scalability of their approach is not as clear , as we will explain in section  [ sec : related ] .",
    "we will provide a detailed comparison with in the experiments section .",
    "our main design choices result from our need for a fast algorithm for knowledge base alignment which scales to millions of entities . to this end",
    "we made the following assumptions :    * 1 - 1 matching and uniqueness .",
    "* we assume that the true alignment between the two @xmath11s is a partial function which is mainly 1 - 1",
    ". if there are duplicate entities inside a @xmath11 , will only align one of the duplicates to the corresponding entity in the other @xmath11 .",
    "* aligned relationships .",
    "* we assume that we are given a partial alignment between relationships and between properties of the @xmath11s .     and",
    "@xmath27 have no words in common , the fact that several of their respective neighbors are matched together is a strong signal that @xmath28 and @xmath27 should be matched together .",
    "this is a real example from the dataset used in the experiments and was able to correctly match all these pairs ( @xmath28 and @xmath27 are actually the same movie despite their different stored titles in each @xmath11 ) . ]",
    "the algorithm can be seen as the greedy optimization of an objective function which globally scores the suitability of a particular matching @xmath24 for a pair of given @xmath11s .",
    "this objective function will use two sources of information useful to choose matches : a similarity function between pairs of entities defined from their properties ; and a graph neighborhood contribution making use of neighbor pairs being matched ( see figure  [ fig : example ] for a motivation ) .",
    "let us encode the matching @xmath29 by a matrix @xmath30 with entries indexed by the entities in each @xmath11 , with @xmath31 if @xmath32 , meaning that @xmath33 is matched to @xmath34 , and @xmath35 otherwise .",
    "the space of possible 1 - 1 partial mappings is thus represented by the set of binary matrices : @xmath36 and @xmath37 .",
    "we define the following quadratic objective function which globally scores the suitability of a matching @xmath30 : @xmath38 , \\\\      & \\textrm{where } \\qquad g_{ij}(y ) \\doteq \\sum_{(k , l ) \\in \\mathcal{n}_{ij } } y_{kl } \\ , w_{ij , kl}.      \\end{split}\\ ] ] the objective contains linear coefficients @xmath39 which encode a similarity between entity @xmath28 and @xmath27 , as well as quadratic coefficients @xmath40 which control the algorithm s tendency to match @xmath28 with @xmath27 given that @xmath41 was matched to @xmath42 and @xmath41 are always entities in @xmath22 ; whereas @xmath27 and @xmath42 are in @xmath23 .",
    "@xmath2 could be in either @xmath11 . ] .",
    "@xmath43 is a local neighborhood around @xmath44 that we define later and which will depend on the graph information from the @xmath11s ",
    "@xmath45 is basically counting ( in a weighted fashion ) the number of matched pairs @xmath46 which are in the neighborhood of @xmath28 and @xmath27 .",
    "@xmath47 $ ] is a tradeoff parameter between the linear and quadratic contributions .",
    "our approach is motivated by the maximization problem : @xmath48 where the norm @xmath49 represents the number of elements matched and @xmath50 is an unknown upper - bound which represents the size of the best partial mapping which can be made from @xmath22 to @xmath23 .",
    "we note that if the coefficients are all positive ( as will be the case in our formulation  we are only encoding similarities and not repulsions between entities ) , then the maximizer @xmath51 will have @xmath52 .",
    "problem   is thus related to one of the variations of the quadratic assignment problems , a well - known np - complete problem in operational research  @xcite for the traditional description of the quadratic assignment problem and its relationship to our problem . ] . even though one could approximate the solution to the combinatorial optimization   using a linear program relaxation ( see lacoste - julien et al .",
    "@xcite ) , the number of variables is quadratic in the number of entities , and so is obviously not scalable .",
    "our approach is instead to _ greedily optimize _   by adding the match element @xmath53 at each iteration which increases the objective the most and selected amongst a small set of possibilities . in other words , the high - level operational definition of the algorithm is as follows :    1 .",
    "start with an initial good quality partial match @xmath54 .",
    "2 .   at each iteration @xmath55 , augment the previous matching with a new matched pair by setting @xmath53 for the @xmath44 which maximally increases @xmath56 , chosen amongst a small set @xmath57 of reasonable candidates which preserve the feasibility of the new matching .",
    "3 .   stop when the bound @xmath58 is reached ( and never undo previous decisions ) .",
    "having outlined the general framework , in the remainder of this section we will describe methods for choosing the similarity coefficients @xmath39 and @xmath40 so that they guide the algorithm towards good matchings ( section  [ sec : score ] ) , the choice of neighbors , @xmath43 , the choice of a candidate set @xmath57 , and the stopping criterion , @xmath50 .",
    "these choices influence both the speed and accuracy of the algorithm",
    ".    * compatible - neighbors .",
    "* @xmath43 should be chosen so as to respect the graph structure defined by the @xmath11 facts .",
    "its contribution in the objective crucially encodes the fact that a neighbor @xmath41 of @xmath28 being matched to a ` compatible ' neighbor @xmath42 of @xmath27 should encourage @xmath28 to be matched to @xmath27  see the caption of figure  [ fig : example ] for an example . here",
    ", compatibility means that they are related by the same relationship ( they have the same color in figure  [ fig : example ] ) .",
    "formally , we define : @xmath59 note that a property of this neighborhood is that @xmath60 iff @xmath61 , as we have that the relationship @xmath1 is matched to @xmath62 iff @xmath63 is matched to @xmath64 as well .",
    "this means that the increase in the objective obtained by adding @xmath44 to the current matching @xmath30 defines the following _ context dependent similarity score function _ which is used to pick the next matched pair in the step 2 of the algorithm : @xmath65    * information propagation on the graph . *",
    "the ` compatible - neighbors ` concept that we just defined is one of the most crucial characteristics of .",
    "it allows the information of a new matched pair to propagate amongst its neighbors .",
    "it also defines a powerful heuristic to suggest new candidate pairs to include in a small set @xmath57 of matches to choose from : after matching @xmath28 to @xmath27 , adds all the pairs @xmath46 from ` compatible - neighbors`@xmath44 as new candidates .",
    "this yields a fire propagation analogy for the algorithm : starting from an initial matching ( fire )  it starts to match their neighbors , letting the fire propagate through the graph .",
    "if the graph in each @xmath11 is well - connected in a similar fashion , it can visit most nodes this way .",
    "this heuristic enables to avoid the potential quadratic number of pairs to consider by only focussing its attention on the neighborhoods of current matches .    * stopping criterion .",
    "* terminates when the variation in the objective value , ` score`@xmath66 , of the latest added match @xmath44 falls below a threshold ( or the queue becomes empty ) .",
    "the threshold in effect controls the precision / recall tradeoff of the algorithm . by ensuring that the @xmath39 and @xmath45 terms are normalized between 0 and 1",
    ", we can standardize the scale of the threshold for different score functions . in our experiments , a threshold of 0.25 is observed to correlate well with a point at which the f - measure stops increasing and the precision is significantly decreasing .",
    "we present the pseudo - code for in table  [ tab : alg ] .",
    "we now elaborate on the algorithm design as well as its implementation aspects .",
    "we note that the ` score ` defined in   to greedily select the next matched pair is composed of a static term @xmath39 , which does not depend on the evolving matching @xmath30 , and a dynamic term @xmath67 , which depends on @xmath30 , though only through the local neighborhood @xmath43 .",
    "we call the @xmath68 component of the score function the graph contribution  its local dependence means that it can be updated efficiently after a new match has been added .",
    "we explain in more details the choice of similarity measures for these components in section  [ sec : score ] .    [",
    "[ initial - match - structure - m_0 ] ] initial match structure @xmath69 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the algorithm can take any initial matching seed assumed of good quality . in our current implementation , this is done by looking for entities with the same string representation ( with minimal standardization such as removing capitalization and punctuation ) with an _ unambiguous 1 - 1 match _  that is , we do not include an exact matched pair when more than two entities have this same string representation , thereby increasing precision .",
    "[ [ increasing - score - function - with - local - dependence ] ] increasing score function with local dependence + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the score function has a component @xmath39 which is static ( fixed at the beginning of the algorithm ) from the properties of entities such as their string representation , and a component @xmath70 which is dynamic , looking at how many neighbors are correctly matched .",
    "the dynamic part can actually only increase when new neighbors are matched , and only the scores of neighbors can change when a new pair is matched .",
    "[ [ optional - static - list - of - candidates - mathcals_0 ] ] optional static list of candidates @xmath71 + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    optionally , we can initialize @xmath72 with a static list @xmath71 which only needs to be scored once as any score update will come from neighbors already covered by step 11 of the algorithm .",
    "@xmath71 has the purpose to increase the possible exploration of the graph when another strong source of information ( which is not from the graph ) can be used . in our implementation , we use an inverted index built on words to efficiently suggest entities which have at least two words in common in their string representation as potential candidates .. ]    [ [ data - structures ] ] data - structures + + + + + + + + + + + + + + +    we use a binary heap for the priority queue implementation ",
    "insertions will thus be @xmath73 where @xmath74 is the size of the queue . because the score function can only increase as we add new matches",
    ", we do not need to keep track of stale nodes in the priority queue in order to update their scores , yielding a significant speed - up .",
    "an important factor for any matching algorithm is the similarity function between pairs of elements to match .",
    "designing good similarity functions has been the focus of much of the literature on record linkage , entity resolution , etc . , and because uses the score function in a modular fashion , is free to use most of them for the term @xmath39 as long as they can be computed efficiently .",
    "we provide in this section our implementation choices ( which were motivated by simplicity ) , but we note that the algorithm can easily handle more powerful similarity measures . the generic score function used by was given in  .",
    "in the current implementation , the static part @xmath39 is defined through the _ properties _ of entities only .",
    "the graph part @xmath70 depends on the _ relationships _ between entities ( as this is what determines the graph ) , as well as the previous matching @xmath30 .",
    "we also make sure that @xmath39 and @xmath75 stay normalized so that the score of different pairs are on the same scale .      the static property similarity measure is further decomposed in two parts : we single out a contribution coming from the string representation property of entities ( as it is such a strong signal for our datasets ) , and we consider the other properties together in a second term : @xmath76 where @xmath77 $ ] is a tradeoff coefficient between the two contributions set to 0.25 during the experiments .    [ [ string - similarity - measure ] ] string similarity measure + + + + + + + + + + + + + + + + + + + + + + + + +    for the string similarity measure",
    ", we primarily consider the number of words which two strings have in common , albeit weighted by their information content . in order to handle the varying lengths of strings",
    ", we use the jaccard similarity coefficient between the sets of words , a metric often used in information retrieval and other data mining fields  @xcite . the jaccard similarity between set @xmath78 and @xmath79",
    "is defined as @xmath80 , which is a number between 0 and 1 and so is normalized as required .",
    "we also add a smoothing term in the denominator in order to favor longer strings with many words in common over very short strings .",
    "finally , we use a _ weighted _ jaccard measure in order to capture the information that some words are more informative than others . in analogy to a commonly used feature in information retrieval",
    ", we use the idf ( inverse - document - frequency ) weight for each word . the weight for word @xmath81 in @xmath82",
    "is @xmath83 , where @xmath84 @xmath2 has word @xmath81 in its string representation@xmath85 . combining these elements",
    ", we get the following string similarity measure : @xmath86 where @xmath87 is the set of words in the string representation of entity @xmath2 and ` smoothing ` is the scalar smoothing constant ( we try different values in the experiments ) .",
    "using unit weights and removing the smoothing term would recover the standard jaccard coefficient between the two sets . as it operates on set of words , this measure is robust to word re - ordering , a frequently observed variation between strings representing the same entity in different knowledge bases .",
    "on the other hand , this measure is not robust to small typos or small changes of spelling of words .",
    "this problem could be addressed by using more involved string similarity measures such as _ approximate string matching _",
    "@xcite , which handles both word corruption as well as word reordering , though our current implementation only uses   for simplicity",
    ". we will explore the effect of different scoring functions in our experiments in section  [ sec : parameters ] .",
    "[ [ property - similarity - measure ] ] property similarity measure + + + + + + + + + + + + + + + + + + + + + + + + + + +    we recall that we assume that the user provided a partial matching between properties of both databases .",
    "this enables us to use them in a property similarity measure . in order to elegantly handle missing values of properties , varying number of property values present , etc .",
    ", we also use a smoothed weighted jaccard similarity measure between the sets of properties .",
    "the detailed formulation is given in appendix  [ ap : property ] for completeness , but we note that it can make use of a similarity measure between literals such a normalized distance on numbers ( for dates , years etc . ) or a string - edit distance on strings .",
    "we now introduce the part of the score function which enables to build on previous decisions and exploit the relationship graph information .",
    "we need to determine @xmath40 , the weight of the contribution of a neighboring matched pair @xmath46 for the score of the candidate pair @xmath44 .",
    "the general idea of the graph score function is to count the number of compatible neighbors which are currently matched together for a pair of candidates ( this is the @xmath45 contribution in  ) .",
    "going back at the example in figure  [ fig : example ] , there were three compatible matched pairs shown in the neighborhood of @xmath28 and @xmath27 .",
    "we would like to normalize this count by dividing by the number of possible neighbors , and we would possibly want to weight each neighbor differently .",
    "we again use a smoothed weighted jaccard measure to summarize this information , averaging the contribution from each @xmath11 .",
    "this can be obtained by defining @xmath40 = @xmath88 , where @xmath89 and @xmath90 are normalization factors specific to @xmath28 and @xmath27 in each database and @xmath91 is the weight of the contribution of @xmath41 to @xmath28 in @xmath22 ( and similarly for @xmath92 in @xmath23 ) .",
    "the graph contribution thus becomes : @xmath93 so let @xmath94 be the set of neighbors of entity @xmath28 in @xmath22 , i.e. @xmath95 ( and similarly for @xmath96 ) .",
    "then , remembering that @xmath97 for a valid partial matching @xmath98 , the following normalizations @xmath89 and @xmath90 will yield the average of two smoothed weighted jaccard measures for @xmath45 : @xmath99 we thus have @xmath100 for @xmath98 , keeping the contribution of each possible matched pair @xmath44 on the same scale in ` obj ` in  .",
    "the graph part of the score in   then takes the form : @xmath101 the summation over the first two terms yields @xmath45 and so is bounded by @xmath102 , but the summation over the last two terms could be greater than 1 in the case that @xmath44 is filling a ` hole ' in the graph ( thus increasing the contribution of many neighbors @xmath46 in ` obj ` in  ) .",
    "for example , suppose that @xmath28 has @xmath74 neighbors with degree 1 ( i.e. they only have @xmath28 as neighbor ) ; and the same thing for @xmath27 , and that they are all matched pairwise  figure  [ fig : example ] is an example of this with @xmath103 if we suppose that no other neighbors are present in the @xmath11 .",
    "suppose moreover that we use unit weights for @xmath91 and @xmath104 .",
    "then the normalization is @xmath105 for each @xmath106 ( as they have degree 1 ) ; and similarly for @xmath107 .",
    "the contribution of the sum over the last two terms in   is thus @xmath108 ( whereas in this case @xmath109 ) .",
    "* neighbor weight @xmath91 .",
    "* we finally need to specify the weight @xmath91 , which determines the strength of the contribution of the neighbor @xmath41 being correctly matched to the score of a suggested pair containing @xmath28 . in our experiments ,",
    "we consider both the constant weight @xmath110 and a weight @xmath91 that varies inversely with the number of neighbors entity @xmath41 has where the relationship is of the same type as the one with entity @xmath28 .",
    "the motivation for the latter is explained in appendix  [ ap : weights ] .",
    "we made a prototype implementation of in python and compared its performance on benchmark datasets as well as on large - scale knowledge bases .",
    "all experiments were run on a cluster node hexacore intel xeon e5650 2.66ghz with 46 gb of ram running linux .",
    "each knowledge base is represented as two text files containing a list of triples of relationships - facts and property - facts .",
    "the input to is a pair of such @xmath11s as well as a partial mapping between the relationships and properties of each @xmath11 which is used in the computation of the score in  , and the definition of ` compatible - neighbors `  .",
    "the output of is a list of matched pairs @xmath111 with their score information and the iteration number at which they were added to the solution .",
    "we evaluate the final alignment ( after reaching the stopping threshold ) by comparing it to ground truth using the standard metrics of precision , recall and f - measure on the number of _ entities _ correctly matched .",
    "divided by the number of entities with ground truth information in @xmath22 .",
    "we note that recall is upper bounded by precision because our alignment is a 1 - 1 function . ]",
    "the benchmark datasets are available together with corresponding ground truth data ; for the large - scale knowledge bases , we built their ground truth using web url information as described in section  [ sec : datasets ] .",
    "we found reasonable values for the parameters of by exploring its performance on the to pair ( the methodology is described in section  [ sec : parameters ] ) , and then kept them fixed for all the other experimental comparisons ( section  [ sec : exp1 ] and  [ sec : exp2 ] ) .",
    "this reflects the situation where one would like to apply to a new dataset without ground truth or to minimize parameter adaptation .",
    "the standard parameters that we used in these experiments are given in appendix  [ ap : params ] .",
    "our experiments were done both on several large - scale datasets and on some standard benchmark datasets from the ontology alignment evaluation initiative ( oaei ) ( table  [ tab : all_stats ] ) .",
    "we describe these datasets below .",
    "[ [ large - scale - datasets ] ] large - scale datasets + + + + + + + + + + + + + + + + + + + +    as mentioned throughout this paper so far , we used the dataset pair - as the main motivating example for developing and testing .",
    "we also test on the pair - , for which we could obtain a sizable ground truth .",
    "we describe here their construction . both and are available as lists of triples from their respective websites . and from : http://wiki.freebase.com/wiki/data_dumps [ ] .",
    "] , on the other hand , is given as a list of text files .",
    "there are different files for different categories , e.g. : actors , producers , etc .",
    "we use these categories to construct a list of triples containing facts about movies and people .",
    "because ignores relationships and properties that are not matched between the @xmath11s , we could reduce the size of and by keeping only those facts which had a 1 - 1 mapping with as presented in table  [ tab : largekb ] , and the entities appearing in these facts . to facilitate the comparison of with",
    ", the authors of   kindly provided us their own version of that we will refer from now on as  this version has actually a richer structure in terms of properties .",
    "we also kept in the relationships and properties which were aligned with those of ( table  [ tab : largekb ] ) .",
    "table  [ tab : all_stats ] presents the number of unique entities and relationship - facts included in the relevant reduced datasets .",
    "we constructed the ground truth for - by scraping the relevant wikipedia pages of entities to extract their link to the corresponding page , which often appears in the ` external links ' section .",
    "we then obtained the entity name by scraping the corresponding page and matched it to our constructed database by using string matching ( and some manual cleaning ) .",
    "we obtained 54k ground truth pairs this way .",
    "we used a similar process for - by accessing the urls which were actually stored in the database .",
    "this yielded 293k pairs , probably one of the largest knowledge bases alignment ground truth sets to date .",
    "[ [ benchmark - datasets ] ] benchmark datasets + + + + + + + + + + + + + + + + + +    we also tested on three benchmark dataset pairs provided by the ontology alignment evaluation initiative ( oaei ) , which allowed us to compare the performance of to some previously published methods  @xcite . from the oaei 2009 edition , we use the - instance matching benchmark from the domain of scientific publications .",
    "contains publications and authors as entities extracted from the search results of the search server .",
    "is a version of the dblp dataset listing publications from the computer science domain .",
    "the pair has one matched relationship , ` author ` , as well several matched properties such as ` year ` , ` volume ` , ` journal name ` , ` pages ` , etc .",
    "our goal was to align publications and authors .",
    "the other two datasets come from the person - restaurants ( pr ) task from the oaei 2010 edition , containing data about people and restaurants .",
    "in particular , there are - pairs where the second entity is a copy of the first with one property field corrupted , and - pairs coming from two different online databases that were manually aligned .",
    "all datasets were downloaded from the corresponding oaei webpages , with dataset sizes given in table  [ tab : all_stats ] .      in this experiment",
    ", we test the performance of on the three pairs of large - scale @xmath11s and compare it with  @xcite , which is described in more details in the related work section  [ sec : related ] .",
    "we also compare and with the simple baseline of doing the unambiguous exact string matching step described in section  [ ssec : implementation ] which is used to obtain an initial match @xmath69 ( called ) .",
    "table  [ tab : res_large ] presents the results .    despite its simple greedy nature which never goes back to correct a mistake ,",
    "obtains an impressive f - measure above 90% for all datasets , significantly improving over the baseline .",
    "we tried running  @xcite on a smaller subset of - , using the code available from its author s website .",
    "it did not complete its first iteration after a week of computation and so we halted it ( we did not have the ssd drive which seems crucial to reasonable running times ) .",
    "the results for in table  [ tab : res_large ] are thus computed using the prediction files provided to us by its authors on the - dataset . in order to better relate the - results with the - ones",
    ", we also constructed a larger ground truth reference on - by using the same process as described in section  [ sec : datasets ] . on both ground truth evaluations ,",
    "obtains a similar f - measure as , but in 50x less time . on the other hand",
    ", we note that is solving the more general problem of instances and schema alignment , and was not provided any manual alignment between relationships .",
    "the large difference of recall between and on the ground truth from  @xcite can be explained by the fact that more than a third of its entities had no neighbor ; whereas the process used to construct the new larger ground truth included only entities participating in movie facts and thus having at least one neighbor .",
    "the recall of actually increases for entities with increasing number of neighbors ( going from 68% for entities in the ground truth from  @xcite with 0 neighbor to 97% for entities with 5 + neighbors ) .",
    "about 2% of the predicted matched pairs from on - have no word in common and thus zero string similarity  difficult pairs to match without any graph information .",
    "examples of these pairs came from spelling variations of names , movie titles in different languages , foreign characters in names which are not handled uniformly or multiple titles for movies ( such as the ` blood in , blood out ' example of figure  [ fig : example ] ) .",
    "* error analysis .",
    "* examining the few errors made by , we observed the following types of matching errors : 1 ) errors in the ground truth ( either coming from the scraping scheme used ; or from wikipedia ( ) which had incorrect information ) ; 2 ) having multiple very similar entities ( e.g. mistaking the ` making of ' of the movie vs. the movie itself ) ; 3 ) pair of entities which shared exactly the same neighbors ( e.g. two different movies with exactly the same actors ) but without other discriminating information .",
    "finally , we note that going through the predictions of that had a low property score revealed a significant number of errors in the databases ( e.g. wildly inconsistent birth dates for people ) , indicating that could be used to highlight data inconsistencies between databases .      in this experiment",
    ", we test the performance of on the three benchmark datasets and compare them with the best published results so far that we are aware of :  @xcite for the person - restaurants datasets ( which compared favorably over objectcoref  @xcite ) ; and  @xcite for .",
    "table  [ tab : res_bench ] presents the results .",
    "we also include the results for as a simple baseline as well as , which is the algorithm without using the graph information at all is set to 0 ) and is only using the inverted index @xmath71 to suggest candidates  not the neighbors in @xmath43 . ] , to give an idea of how important the graph information is in these cases .",
    "interestingly , significantly improved the previous results without needing any parameter tweaking .",
    "the person - restaurants datasets did not have a rich relationship structure to exploit : each entity ( a person or a restaurant ) was linked to exactly one another in a 1 - 1 bipartite fashion ( their address ) .",
    "this is perhaps why is surprisingly able to _ perfectly _ match both the person and restaurants datasets . analyzing the errors made by",
    ", we noticed that they were due to a violation of the assumption that each entity is unique in each @xmath11 : the same address is represented as different entities in , and greedily matched the one which was not linked to another restaurant in , thus reducing the graph score for the correct match . could nt suffer from this problem , and thus obtained a perfect matching .",
    "the dataset has a more interesting relationship structure which is not just 1 - 1 : papers have multiple authors and authors have written multiple papers , enabling the fire propagation algorithm to explore more possibilities .",
    "however , it appears that a purely string based algorithm can already do quite well on this dataset  obtains a 89% f - measure , already significantly improving the previously best published results ( at 76% f - measure ) .",
    "improves this to 91% , and finally using the graph structure helps to improve this to 94% .",
    "this benchmark which has a medium size also highlights the nice scalability of : despite using the interpreted language python , our implementation runs in less than 10 minutes on this dataset , which can be compared to taking 36 hours on a 8-core server in 2009 .      in this section , we explore the role of different configurations for on the - pair , as well as determine which parameters to use for the other experiments .",
    "we recall that with the final parameters ( described in appendix  [ ap : params ] ) yields a 95% f - measure on this dataset ( second section of table  [ tab : res_large ] ) .",
    "experiments 5 and 6 which explore the optimal weighting schemes as well as the correct stopping threshold are described for completeness in appendix  [ ap : param_exp ] .      in this experiment",
    ", we explore the importance of each part of the score function by running with some parts turned off ( which can be done by setting the @xmath112 and @xmath113 tradeoffs to 0 or 1 ) .",
    "the resulting precision / recall curves are plotted in figure  [ fig : test_scores]a .",
    "we can observe that turning off the static part of the score ( string and property ) has the biggest effect , decreasing the maximum f - measure from 95% to about 80% ( to be contrasted with the 72% f - measure for as shown in table  [ tab : res_large ] ) . by comparing with",
    ", we see that including the graph information moves the f - measure from a bit below 85% to over 95% , a significant gain , indicating that the graph structure is more important on this dataset than the oaei benchmark datasets .     ]      in this experiment , we tested how important the size of the matching seed @xmath69 is for the performance of .",
    "we report the following notable results .",
    "we ran with no exact seed matching at all : we initialized it with a random exact match pair and let it explore the graph greedily ( with the inverted index still making suggestions ) .",
    "this obtained an even better score than the standard setup : 99% of precision , 94% recall and 96% f - measure , demonstrating that _ a good initial seed is actually not needed for this setup_. if we do not use the inverted index but initialize with the top 5% of the exact match sorted by their score in the context of the whole exact match , the performance drops a little , but is still able to explore a large part of the graph : it obtains 99% / 87% / 92% of precision / recall / f - measure , illustrating the power of the graph information for this dataset .",
    "we contrast here with the work already mentioned in section  [ ssec : approaches ] and provide further links . in the ontology matching literature , the only approach which was applied to datasets of the size that we considered in this paper is the recently proposed  @xcite , which solves the more general problem of matching instances , relationships and classes .",
    "the framework defines a normalized score between pairs of instances to match representing how likely they should be matched , and which depends on the matching scores of their compatible neighbors .",
    "the final scores are obtained by first initializing ( and fixing ) the scores on pairs of literals , and then propagating the updates through the relationship graph using a fixed point iteration , yielding an analogous fire propagation of information as , though it works with soft [ 0 - 1]-valued assignment whereas works with hard \\{0,1}-valued ones .",
    "the authors handle the scalability issue of maintaining scores for all pairs by using a sparse representation with various pruning heuristics ( in particular , keeping only the maximal assignment for each entity at each step , thus making the same 1 - 1 assumption that we did ) .",
    "an advantage of over is that it is able to include property values in its neighborhood graph ( it uses soft - assignments between them ) whereas only uses relationships given that a 1 - 1 matching of property values is not appropriate .",
    "we conjecture that this could explain the higher recall that obtained on entities which had no relationship neighbors on the - dataset . on the other hand , was limited to use a 0 - 1 similarity measure between property values for the large - scale experiments in  @xcite , as it is unclear how one could apply the same sparsity optimization in a scalable fashion with more involved similarity measures ( such as the idf one that is using ) .",
    "the use of a 0 - 1 similarity measure on strings could explain the lower performance of on the restaurants dataset in comparison to .",
    "we stress that is able in contrast to use sophisticated similarity measures in a scalable fashion , and had a 50x speed improvement over on the large - scale datasets .",
    "the algorithm is related to the collective entity resolution approach of bhattacharya and getoor  @xcite , which proposed a greedy agglomerative clustering algorithm to cluster entities based on previous decisions .",
    "their approach could handle constraints on the clustering , including a @xmath114 matching constraint in theory , though it was not implemented . a scalable solution for collective entity resolution",
    "was proposed recently in  @xcite , by treating the sophisticated machine learning approaches to entity resolution as black boxes ( see references therein ) , but running them on small neighborhoods and combining their output using a message - passing scheme .",
    "they do not consider exploiting a @xmath114 matching constraint though , as most entity resolution or record linkage work .",
    "the idea to propagate information on a relationship graph has been used in several other approaches for ontology matching  @xcite , though none were scalable for the size of knowledge bases that we considered .",
    "an analogous ` fire propagation ' algorithm has been used to align social network graphs in  @xcite , though with a very different objective function ( they define weights in each graphs and want to align edges which has similar weights ) .",
    "the heuristic of propagating information on a relationship graph is related to a well - known heuristic for solving constraint satisfactions problems known as constraint propagation  @xcite .",
    "ehrig and staab  @xcite mentioned several heuristics to reduce the number of candidates to consider in ontology alignment , including a similar one to ` compatible - neighbors ` , though they tested their approach only on a few hundred instances .",
    "finally , we mention that peralta  @xcite aligned the movie database movielens to imdb through a combination of steps of manual cleaning with some automation . could be considered as an alternative which does not require manual intervention apart specifying the score function to use .",
    "we have presented , a simple and scalable algorithm for the alignment of large - scale knowledge bases . despite making greedy decisions and never backtracking to correct decisions , obtained a higher f - measure than the previously best published results on the oaei benchmark datasets , and matched the performance of the more involved algorithm   while being 50x faster on large - scale knowledge bases of millions of entities .",
    "our experiments indicate that can obtain good performance over a range of datasets with the same parameter setting . on the other hand , is easily extensible to more powerful scoring functions between entities , as long as they can be efficiently computed .    some apparent limitations of are a ) that it can not correct previous mistakes and b ) can not handle alignments other than 1 - 1 . addressing these in a scalable fashion which preserves high accuracy are open questions for future work .",
    "we note though that the non - corrective nature of the algorithm did nt seem to be an issue in our experiments . moreover",
    ", pre - processing each knowledge base with a de - duplication method can help make the 1 - 1 assumption more reasonable , which is a powerful feature to exploit in an alignment algorithm .",
    "another interesting direction for future work would be to use machine learning methods to learn the parameters of more powerful scoring function . in particular , the ` learning to rank ' model seems suitable to learn a score function which would rank the correctly labeled matched pairs above the other ones .",
    "the current level of performance of already makes it suitable though as a powerful generic alignment tool for knowledge bases and hence takes us closer to the vision of linked open data and the semantic web .    *",
    "acknowledgments : * we thank fabian suchanek and pierre senellart for sharing their code and answering our questions about . we thank guillaume obozinski for helpful discussions .",
    "this research was supported by a grant from microsoft research ltd . and a research in paris fellowship .",
    "we describe here the property similarity measure used in our implementation .",
    "we use a smoothed weighted jaccard similarity measure between the sets of properties defined as follows .",
    "suppose that @xmath115 has properties @xmath116 with respective literal values @xmath117 , and that @xmath118 has properties @xmath119 with respective literal values @xmath120 . in analogy to the string similarity measure , we will also associate idf weights to the possible property values @xmath121 where @xmath122 @xmath2 has literal @xmath81 for property @xmath123 and @xmath124 is the total number of entities in knowledge base @xmath125 which have a value for property @xmath126 .",
    "we then define the following property similarity measure : @xmath127 where @xmath128 represents the property alignment : @xmath129 .",
    "@xmath130 is a @xmath131$]-valued similarity measure between literals ; it could be a normalized distance on numbers ( for dates , years , etc . ) , a string - edit distance on strings , etc .",
    "we recall that the the graph weight @xmath91 determines the strength of the contribution of the neighbor @xmath41 being correctly matched to the score of a suggested pair containing @xmath28 . in our experiments ,",
    "we consider both the constant weight @xmath110 and a weight @xmath91 that varies inversely with the number of neighbors entity @xmath41 has where the relationship is of the same type as the one with entity @xmath28 . to motivate the latter , we go back again to our running example of figure  [ fig : example ] , but switching the role of @xmath28 and @xmath41 as we need to look at the neighbors of @xmath41  this is illustrated in figure  [ fig : graph_weight ] and explained in its caption . in case",
    "there are multiple different relationships linking the same pair @xmath28 to @xmath41 , we take the maximum of the weights over these ( i.e. we pick the most informative information to weight it ) . so formally ,",
    "we have : @xmath132    we also point out that the normalization of @xmath45 in   is made over each @xmath11 independently , in contrast with the ` string ` and ` prop ` similarity measures   and   which are normalized in both @xmath11 jointly .",
    "the motivation for this is that the neighborhood size in and are overly asymmetric ( there is much more information about each movie in ) .",
    "the separate normalization means that as long as most of a neighborhood in _",
    "one _ @xmath11 is correctly aligned , the graph score will be high .",
    "the information about strings and properties is more symmetric in the @xmath11 pairs that we consider , so a joint normalization seems reasonable in this case .",
    "the quadratic assignment problem is traditionally defined as finding a bijection between @xmath50 facilities and @xmath50 locations which _ minimizes _ the expected cost of transport between the facilities .",
    "given that facilities @xmath28 and @xmath41 are assigned to locations @xmath27 and @xmath42 respectively , the cost of transport between facility @xmath28 and @xmath41 is @xmath133 , where @xmath134 is the expected number of units to ship between facilities @xmath28 and @xmath41 , and @xmath135 is the expected cost of shipment between locations @xmath27 and @xmath42 ( depending on their distance ) . in its more general form",
    "@xcite , the coefficients can be negative , and so there is no major difference between minimizing and maximizing , and we see that our optimization problem   is a special case of this .",
    "we use @xmath136 as the graph score tradeoff has the nice theoretical justification that it gives twice much more weight to the linear term than the quadratic term , a standard weighting scheme given that the derivative of the quadratic yields the extra factor of two to compensate . ] in   and @xmath137 as the property score tradeoff in  .",
    "we set the string score ` smoothing ` term in   as the sum of the maximum possible word weights in each @xmath11 ( @xmath138 ) .",
    "we use 0.25 as the score threshold for the stopping criterion ( step 6 in the algorithm ) , and stop considering suggestions from the inverted index on strings when their score is below 0.75 .",
    "we use as initial matching the unambiguous exact string comparison test as described in section  [ sec : algorithm ] .",
    "we use uniform weights @xmath110 for the matched neighbors contribution in the graph score  .",
    "we use a ` sim ` measure on property values as used in   which depends on the type of property literals : for dates and numbers , we simply use @xmath139-@xmath102 similarity ( 1 when they are equal ) with some processing  e.g. for dates , we only consider the year ; for secondary strings ( i.e. strings for other properties than the main string representation of an entity ) , we use a weighted jaccard measure on words as defined in   but with the idf weights derived from the strings appearing in this property only .",
    "we provide here the additional parameter experiments which were skipped from the main text for brevity .      in this experiment",
    ", we explored the effect of the weighting scheme for the three different score components ( string , property and graph ) by trying two options per component , with precision / recall curves given in figure  [ fig : test_weights ] . for string and property components , we compared uniform weights vs. idf weights . for the graph component , we compare uniform weights ( which surprisingly got the best result ) with the inverse number of neighbors weight proposed in  .",
    "overall , the effect for these variations was much smaller than the one for the score component experiment , with the biggest decrease of less than 1% f - measure obtained by using uniform string weights instead of the idf - scores .",
    "we also varied the 3 smoothing parameters ( one for each score component ) as well as the 2 tradeoff parameters linearly around their chosen values : the performance does not change much for changes of the order of 0.1 - 0.2 for the tradeoff , and 1.5 for the smoothing parameters ( stay with 1% range of f - measure ) .      in this experiment",
    ", we studied whether the score information correlated with changes in the precision / recall information , in order to determine a possible stopping threshold .",
    "we overlay in figure  [ fig : detailed_run ] the precision / recall at each iteration of the algorithm ( blue / red ) with the score ( in green ) of the matched pair chosen at this iteration ( as given by  ) .",
    "the vertical black dashed lines correspond to the iteration at which the score threshold of 0.35 and 0.25 are reached , respectively , which correlated with a drop of precision for the current predictions ( black line with diamonds ) and a leveling of the f - measure ( curved dashed black line ) , respectively .",
    "we note that this correlation was also observed on all the other datasets , indicating that this threshold is robust to dataset variations ."
  ],
  "abstract_text": [
    "<S> the internet has enabled the creation of a growing number of large - scale knowledge bases in a variety of domains containing complementary information . </S>",
    "<S> tools for automatically aligning these knowledge bases would make it possible to unify many sources of structured knowledge and answer complex queries . </S>",
    "<S> however , the efficient alignment of _ large - scale _ knowledge bases still poses a considerable challenge . here , we present ( ) , a simple algorithm for aligning knowledge bases with millions of entities and facts . </S>",
    "<S> is an iterative propagation algorithm which leverages both the structural information from the relationship graph as well as flexible similarity measures between entity properties in a greedy local search , thus making it scalable . despite its greedy nature , our experiments indicate that can efficiently match some of the world s largest knowledge bases with high precision . </S>",
    "<S> we provide additional experiments on benchmark datasets which demonstrate that can outperform state - of - the - art approaches both in accuracy and efficiency . </S>"
  ]
}