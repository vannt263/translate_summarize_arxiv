{
  "article_text": [
    "the statistical analysis of functional data , commonly referred to as functional data analysis ( fda ) , is an established area of statistics with a great number of practical applications ; see the books  @xcite and references therein for various examples . when the data is available as finely sampled curves , say in time , it is common to treat it as a collection of continuous - time curves or functions , each being observed in totality .",
    "these datasets are then termed `` functional , '' and various statistical procedures applicable in finite dimensions can be extended to this functional setting . among such procedures is principal component analysis ( pca ) , which is the focus of present work .",
    "if one thinks of continuity as a mathematical abstraction of reality , then treating functional data as continuous curves is arguably a valid modeling device .",
    "however , in practice , one is faced with finite computational resources and is forced to implement a ( finite - dimensional ) approximation of true functional procedures by some sort of truncation procedure , for instance , in the frequency domain .",
    "it is then important to understand the effects of this truncation on the statistical performance of the procedure . in other situations , such as in longitudinal data analysis  @xcite ,",
    "a  continuous curve model is justified as a hidden underlying generating process to which one has access only through sparsely sampled measurements in time , possibly corrupted by noise .",
    "studying how the time - sampling affects the estimation of the underlying functions in the presence of noise shares various common elements with the frequency - domain problem described above .",
    "the aim of this paper is to study effects of `` sampling''in a fairly general sense  on functional principal component analysis in smooth function spaces . in order to do so",
    ", we adopt a functional - theoretic approach by treating the sampling procedure as a ( continuous ) linear operator .",
    "this set - up provides us with a notion of sampling general enough to treat both the frequency - truncation and time - sampling within a unified framework .",
    "we take as our smooth function space a hilbert subspace @xmath0 of @xmath6 $ ] and denote the sampling operator by @xmath7 .",
    "we assume that there are functions @xmath8 , @xmath9 $ ] , in @xmath0 for @xmath10 , generated i.i.d . from a probabilistic model ( to be discussed ) .",
    "we then observe the collection @xmath11 in noise .",
    "we refer to the index @xmath5 as the number of _ statistical samples _ , and to the index @xmath3 as the number of _ functional samples_.    we analyze a natural @xmath4-estimator which takes the form of a regularized pca in  @xmath1 , and provide nonasymptotic bounds on the estimation error in terms of @xmath5 and  @xmath3 .",
    "the eigen - decay of two operators govern the rates , the product of the sampling operator @xmath12 and its adjoint , and the product of the map embedding @xmath0 in @xmath2 and its adjoint",
    ". our focus will be on the setting where @xmath0 is a reproducing kernel hilbert space ( rkhs ) , in which case the two eigen - decays are intimately related through the kernel function @xmath13 . in such cases ,",
    "the two components of the rate interact and give rise to optimal values for the number of functional samples ( @xmath3 ) in terms of the number of statistical samples ( @xmath5 ) or vice versa .",
    "this has practical appeal in cases where obtaining either type of samples is costly .    our model for the functions",
    "@xmath14 is an extension to function spaces of the _ spiked covariance model _ introduced by johnstone and his collaborators  @xcite , and studied by various authors ( e.g. ,  @xcite ) .",
    "we consider such models with @xmath15 components , each lying within the hilbert ball @xmath16 of radius @xmath17 , with the goal of recovering the @xmath15-dimensional subspace spanned by the spiked components in this functional model .",
    "we analyze our @xmath4-estimators within a high - dimensional framework that allows both the number of statistical samples @xmath5 and the number of functional samples @xmath3 to diverge together .",
    "our main theoretical contributions are to derive nonasymptotic bounds on the estimation error as a function of the pair @xmath18 , which are shown to be sharp ( minimax - optimal ) .",
    "although our rates also explicitly track the number of components @xmath15 and the smoothness parameter @xmath17 , we do not make any effort to obtain optimal dependence on these parameters .",
    "the general asymptotic properties of pca in function spaces have been investigated by various authors ( e.g. ,  @xcite ) .",
    "accounting for smoothness of functions by introducing various roughness / smoothness penalties is a standard approach , used in the papers  @xcite , among others .",
    "the problem of principal component analysis for sampled functions , with a similar functional - theoretic perspective , is discussed by besse and ramsey  @xcite for the noiseless case . a more recent line of work",
    "is devoted to the case of functional pca with noisy sampled functions  @xcite .",
    "cardot  @xcite considers estimation via spline - based approximation , and derives mise rates in terms of various parameters of the model .",
    "hall et al .",
    "@xcite study estimation via local linear smoothing , and establish minimax - optimality in certain settings that involve a fixed number of functional samples .",
    "both papers  @xcite demonstrate trade - offs between the numbers of statistical and functional samples ; we refer the reader to hall et al .",
    "@xcite for an illuminating discussion of connections between fda and lda approaches ( i.e. , having full versus sampled functions ) , which inspired much of the present work .",
    "we note that the regularization present in our @xmath4-estimator is closely related to classical roughness penalties  @xcite in the special case of spline kernels , although the discussion there applies to fully - observed functions , as opposed to the sampled models considered here .",
    "after initial posting of this work , we became aware of more recent work on sampled functional pca .",
    "working within the framework of hall et al .",
    "@xcite , the analysis of li and hsing  @xcite allows for more flexible sample sizes per curve ; they derive optimal uniform ( i.e. , @xmath19 ) rates of convergence for local linear smoothing estimators of covariance function and the resulting eigenfunctions .",
    "another line of work  @xcite has analyzed sampled forms of silverman s criterion  @xcite , with some variations .",
    "huang et al .",
    "@xcite derive a criterion based on rank - one approximation coupled with scale invariance considerations , combined with an extra weighting of the covariance matrix .",
    "xi and zhao  @xcite also show the consistency of their estimator for both regular and irregular sampling .",
    "the regular ( time ) sampling setup in both papers have an overlap with our work ; the eigenfunctions are assumed to lie in a second order sobolev space , corresponding to a special case of a rkhs .",
    "however , even in this particular case , our estimator is different , and it is an interesting question whether a version of the results presented here can be used to show the minimax optimality of these silverman - type criteria . there has also been recent work with emphasis on sampled functional covariance estimation , including the work of cai and yuan  @xcite , who analyze an estimator which can be described as regularized least - squares with penalty being the norm of tensor product of rkhs with itself .",
    "they provide rates of convergence for the covariance function , from which certain rates ( argued to be optimal within logarithmic factors ) for eigenfunctions follow .",
    "as mentioned above , our sampled model resembles very much that of spiked covariance model for high - dimensional principal component analysis .",
    "a line of work on this model has treated various types of sparsity conditions on the eigenfunctions  @xcite ; in contrast , here the smoothness condition on functional components translates into an ellipsoid condition on the vector principal components .",
    "perhaps an even more significant difference is that in this paper , the effective scaling of noise in @xmath1 is substantially smaller in some cases ( e.g. , the case of time sampling ) .",
    "this difference could explain why the difficulty of `` high - dimensional '' setting is not observed in such cases as one lets @xmath20 . on the other hand , a difficulty particular to our sampled model is the lack of orthonormality between components after sampling .",
    "it not only leads to identifiability issues , but also makes recovering individual components difficult .    in order to derive nonasymptotic bounds on our @xmath4-estimator , we exploit various techniques from empirical process theory ( e.g. ,  @xcite ) , as well as the concentration of measure ( e.g. ,  @xcite ) .",
    "we also exploit recent work  @xcite on the localized rademacher complexities of unit balls in a reproducing kernel hilbert space , as well as techniques from nonasymptotic random matrix theory , as discussed in davidson and szarek  @xcite , in order to control various norms of random matrices .",
    "these techniques allow us to obtain finite - sample bounds that hold with high probability , and are specified explicitly in terms of the pair @xmath18 , and the underlying smoothness of the hilbert space .",
    "the remainder of this paper is organized as follows .",
    "section  [ secbackground ] is devoted to background material on reproducing kernel hilbert spaces , adjoints of operators , as well as the class of sampled functional models that we study in this paper .",
    "in section  [ secmest ] , we describe @xmath4-estimators for sampled functional pca , and discuss various implementation details .",
    "section  [ secmain ] is devoted to the statements of our main results , and discussion of their consequences for particular sampling models . in subsequent sections , we provide the proofs of our results , with some more technical aspects deferred to the supplementary material  @xcite .",
    "section  [ secproofsubspace ] is devoted to bounds on the subspace - based error .",
    "we conclude with a discussion in section  [ secdiscuss ] . in the supplementary material",
    "@xcite , section 7 is devoted to proofs of bounds on error in the function space , whereas section 8 provides proofs of matching lower bounds on the minimax error , showing that our analysis is sharp .    _",
    "notation_. we will use to denote the hilbert ",
    "schmidt norm of an operator or a matrix .",
    "the corresponding inner product is denoted as @xmath21 . if @xmath22 is an operator on a hilbert space @xmath0 with an orthonormal basis @xmath23 , then @xmath24 . for a matrix @xmath25 , we have @xmath26 , the range as @xmath27 and the kernel as @xmath28 .",
    "in this section , we begin by introducing background on reproducing kernel hilbert spaces , as well as linear operators and their adjoints .",
    "we then introduce the functional and observation model that we study in this paper , and conclude with discussion of some approximation - theoretic issues that play an important role in parts of our analysis .",
    "we begin with a quick overview of some standard properties of reproducing kernel hilbert spaces ; we refer the reader to the books  @xcite and references therein for more details .",
    "a  reproducing kernel hilbert space ( or rkhs for short ) is a hilbert space @xmath0 of functions @xmath29 that is equipped with a symmetric positive semidefinite function @xmath30 , known as the kernel function .",
    "we assume the kernel to be continuous , and the set @xmath31 to be compact . for concreteness , we think of @xmath32 $ ] throughout this paper , but any compact set of @xmath33 suffices . for each @xmath34 ,",
    "the function @xmath35 belongs to the hilbert space @xmath0 and it acts as the _ representer of evaluation _ , meaning that @xmath36 for all @xmath37 .",
    "the kernel @xmath38 defines an integral operator @xmath39 on @xmath40 , mapping the function @xmath41 to the function @xmath42 . by the spectral theorem in hilbert spaces ,",
    "this operator can be associated with a sequence of eigenfunctions @xmath43 , in @xmath0 , orthogonal in @xmath0 and orthonormal in @xmath40 , and a sequence of nonnegative eigenvalues @xmath44 .",
    "most useful for this paper is the fact that any function @xmath37 has an expansion in terms of these eigenfunctions and eigenvalues , namely @xmath45 for some @xmath46 . in terms of this expansion",
    ", we have the representations @xmath47 and @xmath48 .",
    "many of our results involve the decay rate of these eigenvalues : in particular , for some parameter @xmath49 , we say that the kernel operator has eigenvalues with _",
    "polynomial-@xmath50 decay _ if there is a constant @xmath51 such that @xmath52 let us consider an example to illustrate .    in the case",
    "@xmath32 $ ] and @xmath53 , we can consider the kernel function @xmath54 .",
    "as discussed in appendix a of the supplementary material  @xcite , this kernel generates the class of functions @xmath55\\bigr ) \\mid f(0 ) = 0 , f \\mbox { absolutely continuous and } f ' \\in l^2\\bigl([0,1]\\bigr ) \\bigr\\}.\\ ] ] the class @xmath0 is an rkhs with inner product @xmath56 , and the ball @xmath16 corresponds to a sobolev space with smoothness @xmath53 . the eigen - decomposition of the kernel integral operator is @xmath57^{-2},\\qquad \\psi_k(t ) = \\sqrt{2 } \\sin\\bigl ( \\mu_k^{-1/2 } t \\bigr),\\qquad k=1,2,\\ldots.\\ ] ] consequently , this class has polynomial decay with parameter @xmath53 .",
    "we note that there are natural generalizations of this example to @xmath58 , corresponding to the sobolev classes of @xmath50-times differentiable functions ; for example , see the books  @xcite .    in this paper , the operation of generalized sampling is defined in terms of a bounded linear operator @xmath59 on the hilbert space .",
    "its adjoint is a mapping @xmath60 , defined by the relation @xmath61 for all @xmath37 and @xmath62 . in order to compute a representation of the adjoint",
    ", we note that by the riesz representation theorem , the @xmath63th coordinate of this mapping ",
    "namely , @xmath64_j$]can be represented as an inner product @xmath65 , for some element , and we can write @xmath66^t.\\ ] ] consequently , we have @xmath67 , so that for any @xmath62 , @xmath68 this adjoint operator plays an important role in our analysis .",
    "let @xmath69 be a fixed sequence of positive numbers , and let @xmath70 be a fixed sequence of functions orthonormal in @xmath6 $ ] . consider a collection of @xmath5 i.i.d .",
    "random functions @xmath71 , generated according to the model @xmath72 where @xmath73 are i.i.d .",
    "@xmath74 across all pairs @xmath75 .",
    "this model corresponds to a finite - rank instantiation of functional pca , in which the goal is to estimate the span of the unknown eigenfunctions @xmath76 .",
    "typically , these eigenfunctions are assumed to satisfy certain smoothness conditions ; in this paper , we model such conditions by assuming that the eigenfunctions belong to a reproducing kernel hilbert space @xmath0 embedded within @xmath6 $ ] ; more specifically , they lie in some ball in @xmath0 , @xmath77    for statistical problems involving estimation of functions , the random functions might only be observed at certain times @xmath78 , such as in longitudinal data analysis , or we might collect only projections of each @xmath79 in certain directions , such as in tomographic reconstruction . more concretely , in a _ time - sampling model _ , we observe @xmath3-dimensional vectors of the form @xmath80^t + \\sigma_0 w_i\\qquad\\mbox{for $ i = 1 , 2,\\ldots , n$},\\ ] ] where @xmath81 is a fixed collection of design points , and @xmath82 is a noise vector .",
    "another observation model is the _ basis truncation model _ in which we observe the projections of @xmath41 onto the first @xmath3 basis functions @xmath83 of the kernel operator ",
    "namely , @xmath84^t",
    "+ \\sigma_0 w_i\\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\eqntext{\\mbox{for $ i = 1 , 2,\\ldots , n$},}\\end{aligned}\\ ] ] where @xmath85 represents the inner product in @xmath6 $ ] .    in order to model these and other scenarios in a unified manner",
    ", we introduce a linear operator @xmath86 that maps any function @xmath87 in the hilbert space to a vector @xmath88 of @xmath3 samples , and then consider the linear observation model @xmath89 this model ( [ eqnlinobs ] ) can be viewed as a functional analog of the spiked covariance models introduced by johnstone  @xcite as an analytically - convenient model for studying high - dimensional effects in classical pca .",
    "both the time - sampling ( [ eqntimesamp ] ) and frequency truncation ( [ eqnbasistrun ] ) models can be represented in this way , for appropriate choices of the operator @xmath86 .",
    "recall representation ( [ eqnlinoprep ] ) of @xmath86 in terms of the functions @xmath90 .",
    "* for the time sampling model ( [ eqntimesamp ] ) , we set @xmath91 , so that by the reproducing property of the kernel , we have @xmath92 for all @xmath37 , and @xmath93 . with these choices , the operator @xmath86 maps",
    "each @xmath37 to the @xmath3-vector of rescaled samples @xmath94^t.\\ ] ] defining the rescaled noise @xmath95 yields an instantiation of model ( [ eqnlinobs ] ) which is equivalent to time - sampling ( [ eqntimesamp ] ) . * for the basis truncation model ( [ eqnbasistrun ] )",
    ", we set @xmath96 so that the operator @xmath12 maps each function @xmath37 to the vector of basis coefficients @xmath97^t$ ] .",
    "setting @xmath98 then yields another instantiation of model ( [ eqnlinobs ] ) , this one equivalent to basis truncation ( [ eqnbasistrun ] ) .    a remark on notation before proceeding : in the remainder of the paper , we use @xmath99 as shorthand notation for @xmath100 , since the index @xmath3 should be implicitly understood throughout our analysis .    in this paper",
    ", we provide and analyze estimators for the @xmath15-dimensional eigen - subspace spanned by @xmath101 , in both the sampled domain @xmath1 and in the functional domain . to be more specific , for @xmath102 ,",
    "define the vectors @xmath103 , and the subspaces @xmath104 and let @xmath105 and @xmath106 denote the corresponding estimators . in order to measure the performance of the estimators , we will use projection - based distances between subspaces .",
    "in particular , let @xmath107 and @xmath108 be orthogonal projection operators into @xmath109 and  @xmath105 , respectively , considered as subspaces of @xmath110 .",
    "similarly , let @xmath111 and @xmath112 be orthogonal projection operators into @xmath113 and @xmath106 , respectively , considered as subspaces of @xmath114 .",
    "we are interested in bounding the deviations @xmath115 where @xmath116 is the hilbert ",
    "schmidt norm of an operator ( or matrix ) .",
    "one object that plays an important role in our analysis is the matrix @xmath117 . from the form of the adjoint",
    ", it can be seen that @xmath118_{ij } = \\langle\\phi_i , \\phi_j \\rangle_{\\mathcal{h}}$ ] . for future reference ,",
    "let us compute this matrix for the two special cases of linear operators considered thus far :    * for the time sampling model ( [ eqntimesamp ] ) , we have @xmath91 for all @xmath119 , and hence @xmath118_{ij } = \\frac{1}{m}\\langle{\\mathbb{k}}(\\cdot , t_i),{\\mathbb{k}}(\\cdot , t_j)\\rangle_{{\\mathcal{h } } } = \\frac{1}{m } { \\mathbb{k}}(t_i , t_j)$ ] , using the reproducing property of the kernel .",
    "* for the basis truncation model ( [ eqnbasistrun ] ) , we have @xmath120 , and hence @xmath118_{ij } = \\langle\\mu_i \\psi_i,\\mu_j\\psi_j\\rangle_{{\\mathcal{h } } } = \\mu_i\\delta_{ij}$ ] .",
    "thus , in this special case , we have @xmath121 .    in general ,",
    "the matrix @xmath122 is a type of gram matrix , and so is symmetric and positive semidefinite .",
    "we assume throughout this paper that the functions @xmath90 are linearly independent in @xmath0 , which implies that @xmath122 is strictly positive definite .",
    "consequently , it has a set of eigenvalues which can be ordered as @xmath123 under this condition , we may use @xmath122 to define a norm on @xmath1 via @xmath124 .",
    "moreover , we have the following interpolation lemma , which is proved in appendix  b.1 of the supplementary material  @xcite :    [ leminterpolate ] for any @xmath37 , we have @xmath125 , with equality if and only if @xmath126 .",
    "moreover , for any @xmath127 , the function @xmath128 has smallest hilbert norm of all functions satisfying @xmath129 , and is the unique function with this property .",
    "this lemma is useful in constructing a function - based estimator , as will be clarified in section  [ secmest ] .    in our analysis of the functional error @xmath130",
    ", a number of approximation - theoretic quantities play an important role .",
    "as a mapping from an infinite - dimensional space @xmath0 to @xmath1 , the operator @xmath12 has a nontrivial nullspace . given the observation model ( [ eqnlinobs ] ) , we receive no information about any component of a function @xmath131 that lies within this nullspace . for this reason",
    ", we define the width of the nullspace in the @xmath2-norm , namely the quantity @xmath132 in addition , the observation operator @xmath12 induces a semi - norm on the space @xmath0 , defined by @xmath133_j^2.\\ ] ] it is of interest to assess how well this semi - norm approximates the @xmath2-norm .",
    "accordingly , we define the quantity @xmath134 which measures the worst - case gap between these two ( semi)-norms , uniformly over the hilbert ball of radius one , restricted to the subspace of interest @xmath135 . given knowledge of the linear operator @xmath12 , the quantity @xmath136 can be computed in a relatively straightforward manner . in particular , recall the definition of the matrix  @xmath122 , and let us define a second matrix @xmath137 with entries @xmath138 .",
    "[ lemdefect ] we have the equivalence @xmath139 where denotes the @xmath140-operator norm .",
    "see appendix b.2 of the supplementary material  @xcite for the proof of this claim .",
    "with this background in place , we now turn to the description of our @xmath4-estimator , as well as practical details associated with its implementation .      we begin with some preliminaries on notation , and our representation of subspaces .",
    "recall definition ( [ eqdefzfsffs ] ) of @xmath109 as the @xmath15-dimensional subspace of @xmath1 spanned by @xmath141 , where @xmath142 . our initial goal is to construct an estimate @xmath105 , itself an @xmath15-dimensional subspace , of the unknown subspace  @xmath109 .",
    "we represent subspaces by elements of the stiefel manifold @xmath143 , which consists of @xmath144 matrices @xmath145 with orthonormal columns @xmath146 a given matrix @xmath145 acts as a representative of the subspace spanned by its columns , denoted by @xmath147 . for any @xmath148",
    ", the matrix @xmath149 also belongs to the stiefel manifold , and since @xmath150 , we may call @xmath149 a version of @xmath145 .",
    "we let @xmath151 be the orthogonal projection onto @xmath147 . for two matrices @xmath152",
    ", we measure the distance between the associated subspaces via @xmath153 , where @xmath116 is the hilbert  schmidt ( or frobenius ) matrix norm .      with this notation",
    ", we now specify an @xmath4-estimator for the subspace @xmath154 .",
    "let us begin with some intuition .",
    "given the @xmath5 samples @xmath155 , let us define the @xmath156 sample covariance matrix @xmath157 .",
    "given the observation model ( [ eqnlinobs ] ) , a  straightforward computation shows that @xmath158 = \\sum_{j=1}^rs^2_j z^*_j \\bigl(z^*_j\\bigr)^t + \\sigma_m^2 i_{m}.\\ ] ] thus , as @xmath5 becomes large , we expect that the top @xmath15 eigenvectors of @xmath159 might give a good approximation to @xmath160 . by the courant  fischer variational representation , these @xmath15 eigenvectors can be obtained by maximizing the objective function @xmath161 over all matrices @xmath162",
    ".    however , this approach fails to take into account the smoothness constraints that the vectors @xmath142 inherit from the smoothness of the eigenfunctions @xmath163 . since @xmath164 by assumption",
    ", lemma  [ leminterpolate ] implies that @xmath165 consequently , if we define the matrix @xmath166 \\in{\\mathbb{r}}^{m\\times r}$ ] , then it must satisfy the _ trace smoothness condition _ @xmath167 this calculation motivates the constraint @xmath168 in our estimation procedure .",
    "based on the preceding intuition , we are led to consider the optimization problem @xmath169 where we recall that @xmath170 . given any optimal solution @xmath171 , we return the subspace @xmath172 as our estimate of @xmath109 .",
    "as discussed at more length in section  [ secimplementation ] , it is straightforward to compute @xmath171 in polynomial time .",
    "the reader might wonder why we have included an additional factor of two in this trace smoothness condition .",
    "this slack is actually needed due to the potential infeasibility of the matrix @xmath173 for to problem ( [ eqndiscretemest ] ) , which arises since the columns of @xmath173 are not guaranteed to be orthonormal . as shown by our analysis ,",
    "the additional slack allows us to find a matrix @xmath174 that spans the same subspace as @xmath173 , and is also feasible for to problem ( [ eqndiscretemest ] ) .",
    "more formally , we have :    [ lemfeasible ] under condition ( [ eqassumpa2 ] ) , there exists a matrix @xmath174 such that @xmath175    see appendix b.3 of the supplementary material  @xcite for the proof of this claim .",
    "having thus obtained an estimate is any collection of vectors that span @xmath105 .",
    "as we are ultimately only interested in the resulting functional `` subspace , '' it does not matter which particular collection we choose . ]",
    "@xmath176 of @xmath177 , we now need to construct a @xmath15-dimensional subspace @xmath106 of the hilbert space to be used as an estimate of @xmath178 .",
    "we do so using the interpolation suggested by lemma  [ leminterpolate ] . for each @xmath102 , let us define the function @xmath179 since @xmath180 by definition , this construction ensures that @xmath181 .",
    "moreover , lemma [ leminterpolate ] guarantees that @xmath182 has the minimal hilbert norm ( and hence is smoothest in a certain sense ) over all functions that have this property",
    ". finally , since @xmath12 is assumed to be surjective ( equivalently , @xmath122 assumed invertible ) , @xmath183 maps linearly independent vectors to linearly independent functions , and hence preserves dimension .",
    "consequently , the space @xmath184 is an @xmath15-dimensional subspace of @xmath0 that we take as our estimate of @xmath113 .      in this section ,",
    "we consider some practical aspects of implementing the @xmath4-estimator , and present some simulations to illustrate its qualitative properties .",
    "we begin by observing that once the subspace vectors @xmath185 have been computed , then it is straightforward to compute the function estimates @xmath186 , as weighted combinations of the functions @xmath90 .",
    "accordingly , we focus our attention on solving problem  ( [ eqndiscretemest ] ) .    on the surface ,",
    "problem ( [ eqndiscretemest ] ) might appear nonconvex , due to the stiefel manifold constraint .",
    "however , it can be reformulated as a semidefinite program ( sdp ) , a well - known class of convex programs , as clarified in the following :    [ lemimplement ] problem ( [ eqndiscretemest ] ) is equivalent to solving the sdp @xmath187\\\\[-8pt ] & & \\eqntext{\\mbox{such that $ { |\\!|\\!|}x { |\\!|\\!|}_{{2 } } \\leq1 $ , $ { \\operatorname{tr}}(x ) = r$ , and $ { \\bigl\\langle\\bigl\\langle}{k^{-1}},{x } { \\bigr\\rangle\\bigr\\rangle}\\leq2 r\\rho^2$}}\\end{aligned}\\ ] ] for which there always exists an optimal rank @xmath15 solution . moreover , by lagrangian duality , for some @xmath188 , the problem is equivalent to @xmath189 which can be solved by an eigen decomposition of @xmath190 .    as a consequence , for a given lagrange multiplier @xmath191 ,",
    "the regularized form of the estimator can be solved with the cost of solving an eigenvalue problem . for a given constraint @xmath192",
    ", the appropriate value of @xmath191 can be found by a path - tracing algorithm , or a simple dyadic splitting approach .    in practice where the radius @xmath17 is not known",
    ", one could use cross - validation to set a proper value for the lagrange multiplier @xmath191 .",
    "a possibly simpler approach is to evaluate @xmath193 for the optimal @xmath194 on a grid of @xmath191 and choose a value around which @xmath193 is least variable . as for the choice of the number of components  @xmath15 , a standard approach for choosing it would be to compute the estimator for different choices , and plot the residual sum of eigenvalues of the sample covariance matrix . as in ordinary pca ,",
    "an elbow in such a plot indicates a proper trade - off between the number of components to keep and the amount of variation explained .",
    "`` true '' principal components @xmath195 with signal - to - noise ratios @xmath196 and @xmath197 , respectively .",
    "the number of statistical and functional samples are @xmath198 and @xmath199 .",
    "subsequent rows show the corresponding estimators @xmath200 obtained by applying the regularized form ( [ eqsdpregver ] ) . ]    in order to illustrate the estimator , we consider the time sampling model ( [ eqntimesamp ] ) , with uniformly spaced samples , in the context of a first - order sobolev rkhs [ with kernel function @xmath201 .",
    "the parameters of the model are taken to be @xmath202 , @xmath203 , @xmath204 , @xmath199 and @xmath198 . the regularized form ( [ eqsdpregver ] ) of the estimator is applied , and the results are shown in figure  [ fig1 ] .",
    "the top row corresponds to the four `` true '' signals @xmath101 , the leftmost being @xmath205 ( i.e. , having the highest signal - to - noise ratio ) and the rightmost @xmath206 .",
    "the subsequent rows show the corresponding estimates @xmath207 , obtained using different values of @xmath191 .",
    "the second , third and fourth rows correspond to @xmath208 , @xmath209 and @xmath210 .",
    "one observes that without regularization ( @xmath211 ) , the estimates for the two weakest signals ( @xmath212 and @xmath206 ) are poor .",
    "the case @xmath213 is roughly the one which achieves the minimum for the dual problem .",
    "one observes that the quality of the estimates of the signals , and in particular the weakest ones , are considerably improved .",
    "the optimal ( oracle ) value of @xmath191 , that is , the one which achieves the minimum error between @xmath101 and @xmath214 , is @xmath215 in this problem .",
    "the corresponding estimates are qualitatively similar to those of @xmath213 and are not shown .",
    "the case @xmath210 shows the effect of over - regularization .",
    "it produces very smooth signals , and although it fails to reveal @xmath205 and @xmath216 , it reveals highly accurate versions of @xmath212 and @xmath206 .",
    "it is also interesting to note that the smoothest signal , @xmath206 , now occupies the position of the second ( estimated ) principal component .",
    "that is , the regularized pca sees an effective signal - to - noise ratio which is influenced by smoothness .",
    "this suggests a rather practical appeal of the method in revealing smooth signals embedded in noise .",
    "one can vary @xmath191 from zero upward , and if some patterns seem to be present for a wide range of @xmath191 ( and getting smoother as @xmath191 is increased ) , one might suspect that they are indeed present in data but masked by noise .",
    "we now turn to the statistical analysis of our estimators , in particular deriving high - probability upper bounds on the error of the subspace - based estimate @xmath105 , and the functional estimate @xmath106 . in both cases , we begin by stating general theorems that apply to arbitrary linear operators @xmath12theorems  [ thmsubspaceerr ] and  [ thmfuncerr ] , respectively  and then derive a number of corollaries for particular instantiations of the observation operator .",
    "we begin by stating high - probability upper bounds on the error @xmath217 of the subspace - based estimates .",
    "our rates are stated in terms of a function that involves the eigenvalues of the matrix @xmath218 , ordered as @xmath219 .",
    "consider the function @xmath220 given by @xmath221^{1/2}.\\ ] ] as will be clarified in our proofs , this function provides a measure of the statistical complexity of the function class @xmath222    we require a few regularity assumptions .",
    "define the quantity @xmath223 which measures the departure from orthonormality of the vectors @xmath224 in  @xmath1 .",
    "a straightforward argument using a polarization identity shows that @xmath225 is upper bounded ( up to a constant factor ) by the uniform quantity @xmath136 , as defined in equation ( [ eqndefnphidefect ] ) .",
    "recall that the random functions are generated according to the model @xmath226 , where the signal strengths are ordered as @xmath227 , and that @xmath228 denotes the noise standard deviation in the observation model ( [ eqnlinobs ] ) .    in terms of these quantities",
    ", we require the following assumptions :    @xmath229    the first part of condition ( a1 ) is to prevent the ratio @xmath230 from going to zero as the pair @xmath18 increases , where the constant @xmath231 is chosen for convenience .",
    "such a lower bound is necessary for consistent estimation of the eigen - subspace corresponding to @xmath232 .",
    "the second part of condition ( a1 ) , involving the constant @xmath233 , provides a lower bound on the signal - to - noise ratio @xmath234 .",
    "condition ( a2 ) is required to prevent degeneracy among the vectors @xmath142 obtained by mapping the unknown eigenfunctions to the observation space @xmath1 .",
    "[ in the ideal setting , we would have @xmath235 , but our analysis shows that the upper bound in ( a2 ) is sufficient . ] condition ( a3 ) is required so that the critical tolerance @xmath236 specified below is well - defined ; as will be clarified , it is always satisfied for the time - sampling model , and holds for the basis truncation model whenever @xmath237 .",
    "condition ( a4 ) is easily satisfied , since the right - hand side of ( [ eqassumpa4 ] ) goes to infinity while we usually take @xmath15 to be fixed .",
    "our results , however , are still valid if @xmath15 grows slowly with @xmath3 and @xmath5 subject to ( [ eqassumpa4 ] ) .",
    "[ thmsubspaceerr ] under conditions for a sufficiently small constant @xmath233 , let @xmath236 be the smallest positive number satisfying the inequality @xmath238 then there are universal positive constants @xmath239 such that @xmath240 \\geq1 - \\varphi(n , { \\epsilon_{m , n}}),\\ ] ] where @xmath241 .",
    "we note that theorem  [ thmsubspaceerr ] is a general result , applying to an arbitrary bounded linear operator @xmath12 .",
    "however , we can obtain a number of concrete results by making specific choices of this sampling operator , as we explore in the following sections .",
    "let us begin with the time - sampling model ( [ eqntimesamp ] ) , in which we observe the sampled functions @xmath242^t + \\sigma_0",
    "w_i\\qquad \\mbox{for $ i = 1 , 2,\\ldots , m$.}\\ ] ] as noted earlier , this set - up can be modeled in our general setting ( [ eqnlinobs ] ) with @xmath243 and @xmath244 .    in this case , by the reproducing property of the rkhs , the matrix @xmath245 has entries of the form @xmath246 . letting @xmath247 denote its ordered eigenvalues",
    ", we say that the kernel matrix @xmath122 has polynomial - decay with parameter @xmath49 if there is a constant @xmath248 such that @xmath249 for all @xmath250 .",
    "since the kernel matrix @xmath122 represents a discretized approximation of the kernel integral operator defined by @xmath38 , this type of polynomial decay is to be expected whenever the kernel operator has polynomial-@xmath50 decaying eigenvalues .",
    "for example , the usual spline kernels that define sobolev spaces have this type of polynomial decay  @xcite . in appendix",
    "a of the supplementary material  @xcite , we verify this property explicitly for the kernel @xmath251 that defines the sobolev class with smoothness @xmath53 .    for any such kernel , we have the following consequence of theorem  [ thmsubspaceerr ] :    [ cortimeemp ] consider the case of a time - sampling operator @xmath12 .",
    "in addition to conditions and , suppose that the kernel matrix @xmath122 has polynomial - decay with parameter @xmath49 .",
    "then we have @xmath252 \\geq1 - \\varphi(n , m),\\ ] ] where @xmath253 , and @xmath254 .",
    "\\(a ) disregarding constant pre - factors not depending on the pair @xmath18 , corollary  [ cortimeemp ] guarantees that solving problem ( [ eqndiscretemest ] ) returns a subspace estimate @xmath255 such that @xmath256 with high probability as @xmath257 increase .",
    "depending on the scaling of the number of time samples @xmath3 relative to the number of functional samples @xmath5 , either term in this upper bound can be the smallest ( and hence active ) one .",
    "for instance , it can be verified that whenever @xmath258 , then the first term is smallest , so that we achieve the rate @xmath259 .",
    "the appearance of the term @xmath260 is quite natural , as it corresponds to the minimax rate of a nonparametric regression problem with smoothness @xmath50 , based on @xmath3 samples each of variance @xmath261 .",
    "later , in section  [ seclower ] , we provide results guaranteeing that this scaling is minimax optimal under reasonable conditions on the choice of sample points ; in particular , see theorem  [ thmminimaxemp](a ) .",
    "\\(b ) to be clear , although bound ( [ eqntimeemp ] ) allows for the possibility that the error is of order _ lower than @xmath261 _ , we note that the probability with which the guarantee holds includes a term of the order @xmath262 .",
    "consequently , in terms of expected error , we can not guarantee a rate faster than @xmath261 .",
    "proof of corollary  [ cortimeemp ] we need to bound the critical value @xmath236 defined in the theorem statement ( [ eqndefnepscrit ] ) .",
    "define the function @xmath263 , and note that @xmath264 by construction . under the assumption of polynomial-@xmath50 eigendecay",
    ", we have @xmath265 and some algebra then shows that @xmath266 . disregarding constant factors , an upper bound on the critical @xmath236",
    "can be obtained by solving the equation @xmath267 doing so yields the upper bound @xmath268^{{{2",
    "\\alpha}/({2 \\alpha + 1})}}$ ] .",
    "otherwise , we also have the trivial upper bound @xmath269 , which yields the alternative upper bound @xmath270 . recalling that @xmath244 and combining the pieces yields the claim .",
    "notice that this last ( trivial ) bound on @xmath271 implies that condition ( a3 ) is always satisfied for the time - sampling model .",
    "we now turn to some consequences for the basis truncation model ( [ eqnbasistrun ] ) .",
    "[ cortrunemp ] consider a basis truncation operator @xmath12 in a hilbert space with polynomial-@xmath50 decay . under conditions and @xmath272 , we have @xmath273 \\geq1 - \\varphi(n , m),\\ ] ] where @xmath253 , and @xmath274 .",
    "we note that as long as @xmath275 , condition ( a3 ) is satisfied , since @xmath276 .",
    "the rest of the proof follows that of corollary  [ cortimeemp ] , noting that in the last step we have @xmath98 for the basis truncation model .      as mentioned earlier ,",
    "given the consistency of @xmath277 , the consistency of @xmath106 is closely related to approximation properties of the semi - norm @xmath278 induced by @xmath12 , and in particular how closely it approximates the @xmath2-norm .",
    "these approximation - theoretic properties are captured in part by the nullspace width @xmath279 and defect @xmath136 defined earlier in equations ( [ eqndefnphinull ] ) and ( [ eqndefnphidefect ] ) , respectively .",
    "in addition to these previously defined quantities , we require bounds on the following global quantity : @xmath280 a general upper bound on this quantity is of the form @xmath281 in fact , it is not hard to show that such a bound exists with @xmath282 and @xmath283 using the decomposition @xmath284 .",
    "however , this bound is not sharp .",
    "instead , one can show that in most cases of interest , the term @xmath285 is of the order of @xmath279 .",
    "there are a variety of conditions that ensure that @xmath285 has this scaling ; we refer the reader to the paper  @xcite for a general approach . here",
    "we provide a simple sufficient condition , namely , @xmath286 for a positive constant @xmath287 .",
    "[ lemremain ] under , bound ( [ equpboundrpremain ] ) holds with @xmath288 and @xmath289 .",
    "see appendix b.4 of the supplementary material  @xcite for the proof of this claim . in the sequel",
    ", we show that the first - order sobolev rkhs satisfies condition ( b1 ) .",
    "[ thmfuncerr ] suppose that condition holds , and the approximation - theoretic quantities satisfy the bounds @xmath290 and @xmath291 .",
    "then there is a constant @xmath292 such that @xmath293 ^ 2 \\bigr\\}\\ ] ] with the same probability as in theorem  [ thmsubspaceerr ] .    as with theorem  [ thmsubspaceerr ] , this is a generally applicable result , stated in abstract form . by specializing it to different sampling models , we can obtain concrete rates , as illustrated in the following sections .",
    "we begin by returning to the case of the time sampling model ( [ eqntimesamp ] ) , where @xmath243 . in this case ,",
    "condition ( b1 ) needs to be verified by some calculations .",
    "for instance , as shown in appendix a of the supplementary material  @xcite , in the case of the sobolev kernel with smoothness @xmath53 [ namely , @xmath251 ] , we are guaranteed that ( b1 ) holds with @xmath295 , whenever the samples @xmath296 are chosen uniformly over @xmath297 $ ] ; hence , by lemma  [ lemremain ] , @xmath289 . moreover , in the case of uniform sampling , we expect that the nullspace width @xmath279 is upper bounded by @xmath298 , and so will be proportional to @xmath299 in the case of a kernel operator with polynomial-@xmath50 decay .",
    "this is verified in  @xcite ( up to a logarithmic factor ) for the case of the first - order sobolev kernel . in appendix",
    "a of the supplementary material  @xcite , we also show that , for this kernel , @xmath300 ^ 2 $ ] is of the order @xmath299 , that is , of the same order as @xmath279 .",
    "[ cortimefunc ] consider the basis truncation model ( [ eqnbasistrun ] ) with uniformly spaced samples , and assume condition holds and that @xmath301 ^ 2 \\precsim m^{-2\\alpha}$ ] .",
    "then the @xmath4-estimator returns a subspace estimate @xmath106 such that @xmath302 with the same probability as in corollary  [ cortimeemp ] .    in this case",
    ", there is an interesting trade - off between the bias or approximation error which is of order @xmath303 and the estimation error .",
    "an interesting transition occurs at the point when @xmath304 , at which :    * the bias term @xmath303 becomes of the order @xmath261 , so that it is no longer dominant , and * for the two terms in the estimation error , we have the ordering @xmath305    consequently , we conclude that the scaling @xmath306 is the minimal number of samples such that we achieve an overall bound of the order @xmath261 in the time - sampling model . in section  [ seclower ] , we will see that these rates are minimax - optimal .      for the basis truncation operator  @xmath12 , we have @xmath307 so that condition ( b1 ) is satisfied trivially with @xmath295 . moreover , lemma  [ lemdefect ] implies @xmath308 .",
    "in addition , a function @xmath309 satisfies @xmath310 if and only if @xmath311 , so that @xmath312 consequently , we obtain the following corollary of theorem  [ thmfuncerr ] :    [ corbasistrunfunc ] consider the basis truncation model ( [ eqnbasistrun ] ) with a kernel operator that has polynomial-@xmath50 decaying eigenvalues",
    ". then the @xmath4-estimator returns a function subspace estimate @xmath313 such that @xmath314 with the same probability as in corollary  [ cortrunemp ] .    by comparison to corollary  [ cortimefunc ] , we see that the trade - offs between @xmath18 are very different for basis truncation .",
    "in particular , there is _ no interaction _ between the number of functional samples @xmath3 and the number of statistical samples @xmath5 .",
    "increasing @xmath3 only reduces the approximation error , whereas increasing @xmath5 only reduces the estimation error .",
    "moreover , in contrast to the time sampling model of corollary  [ cortimefunc ] , it is impossible to achieve the fast rate @xmath261 , regardless of how we choose the pair @xmath257 . in section  [ seclower ] , we will also see that the rates given in corollary  [ corbasistrunfunc ] are minimax optimal .",
    "we now turn to lower bounds on the minimax risk , demonstrating the sharpness of our achievable results in terms of their scaling with @xmath18 . in order to do so",
    ", it suffices to consider the simple model with a single functional component @xmath315 , so that we observe @xmath316 for @xmath317 , where @xmath318 are i.i.d .",
    "standard normal variates .",
    "the minimax risk over the unit ball of the function space @xmath0 in the @xmath12-norm is given by @xmath319 where the function @xmath131 ranges over the unit ball @xmath320 of some hilbert space , and @xmath321 ranges over measurable functions of the data matrix @xmath322 .",
    "[ thmminimaxemp ] suppose that the kernel matrix @xmath122 has eigenvalues with polynomial-@xmath50 decay and holds .",
    "for the time - sampling model , we have @xmath323    for the frequency - truncation model , with @xmath324 , we have @xmath325    note that part ( a ) of theorem  [ thmminimaxemp ] shows that the rates obtained in corollary  [ cortimefunc ] for the case of time - sampling are minimax optimal .",
    "similarly , comparing part ( b ) of the theorem to corollary  [ corbasistrunfunc ] , we conclude that the rates obtained for frequency truncation model are minimax optimal for @xmath326 $ ] . as will become clear momentarily ( as a consequence of our next theorem ) , the case @xmath327 is not of practical interest .",
    "we now turn to lower bounds on the minimax risk in the @xmath328 norm ",
    "namely @xmath329 obtaining lower bounds on this minimax risk requires another approximation property of the norm relative to .",
    "consider matrix @xmath330 with entries @xmath331 .",
    "since the eigenfunctions are orthogonal in @xmath2 , the deviation of @xmath332 from the identity measures how well the inner product defined by @xmath12 approximates the @xmath2-inner product over the first @xmath3 eigenfunctions of the kernel operator . for proving lower bounds , we require an upper bound of the form @xmath333 for some universal constant @xmath334 .",
    "as the proof will clarify , this upper bound is necessary in order that the kullback ",
    "leibler divergence  which controls the relative discriminability between different models  can be upper bounded in terms of the @xmath2-norm .    [ thmminimaxltwo ] suppose that condition holds , and the operator associated with kernel function @xmath38 of the reproducing kernel hilbert space @xmath0 has eigenvalues with polynomial-@xmath50 decay .",
    "for the time - sampling model , the minimax risk is lower bounded as @xmath335    for the frequency - truncation model , the minimax error is lower bounded as @xmath336    verifying condition ( b2 ) requires , in general , some calculations in the case of the time - sampling model .",
    "it is verified for uniform time - sampling for the first - order sobolev rkhs in appendix a of the supplementary material  @xcite .",
    "for the frequency - truncation model , condition ( b2 ) always holds trivially since @xmath337 . by this theorem",
    ", the @xmath338 convergence rates of corollaries  [ cortimefunc ] and  [ corbasistrunfunc ] are minimax optimal . also note that due to the presence of the approximation term @xmath299 in ( [ eqminimaxl2freq ] ) , the @xmath12-norm term @xmath339 is only dominant when @xmath340 implying that this is the interesting regime for theorem  [ thmminimaxemp](b ) .",
    "we now turn to the proofs of the results involving the error @xmath341 between the estimated @xmath105 and true subspace @xmath109 .",
    "we begin by proving theorem  [ thmsubspaceerr ] , and then turn to its corollaries .",
    "we begin with some preliminaries before proceeding to the heart of the proof .",
    "let us first introduce some convenient notation .",
    "consider the @xmath342 matrices @xmath343^t \\quad\\mbox{and}\\quad w:={\\left}[\\matrix { { w_{1 } } & { w_{2 } } & \\cdots & { w_{n } } } { \\right}]^t,\\ ] ] corresponding to the observation matrix @xmath344 and noise matrix @xmath345 , respectively .",
    "in addition , we define the matrix @xmath346 , and the diagonal matrix @xmath347 . recalling that @xmath348 , the observation model ( [ eqnlinobs ] ) can be written in the matrix form @xmath349 .",
    "moreover , let us define the matrices @xmath350 and @xmath351 . using this notation ,",
    "some algebra shows that the associated sample covariance @xmath352 can be written in the form @xmath353 where @xmath354 $ ] and @xmath355 .",
    "lemma  [ lemfeasible ] , proved in appendix b.3 of the supplementary material  @xcite , establishes the existence of a matrix @xmath356 such that @xmath357 . as discussed earlier , due to the nature of the steifel manifold , there are many versions of this matrix @xmath358 , and also of any optimal solution matrix @xmath171 , obtained via right multiplication with an orthogonal matrix .",
    "for the subsequent arguments , we need to work with a particular version of @xmath358 ( and @xmath171 ) that we describe here .",
    "let us fix some convenient versions of @xmath359 and @xmath171 . as a consequence of cs decomposition , as long as @xmath360 , there exist orthogonal matrices @xmath361 and an orthogonal matrix @xmath362 such that @xmath363 where @xmath364 and @xmath365 such that @xmath366 and @xmath367 .",
    "see bhatia  @xcite , theorem vii.1.8 , for details on this decomposition . in the analysis to follow , we work with @xmath368 and @xmath369 instead of @xmath359 and  @xmath171 . to avoid extra notation , from now on , we will use @xmath359 and @xmath171 for these new versions , which we refer to as _ properly aligned_. with this choice , we may assume @xmath370 in the cs decomposition ( [ eqcsdecompconseq ] ) .",
    "the following lemma isolates some useful properties of properly aligned subspaces :    [ lemmanageable ] let @xmath359 and @xmath171 be properly aligned , and define the matrices @xmath371 in terms of the cs decomposition ( [ eqcsdecompconseq ] ) , we have :    @xmath372    from the cs decomposition ( [ eqcsdecompconseq ] ) , we have @xmath373 from which relations ( [ eqpertmidproof2 ] ) and ( [ eqpertmidproof3 ] )",
    "follow . from decomposition ( [ eqcsdecompconseq ] ) and the proper alignment condition @xmath370",
    ", we have @xmath374\\\\[-8pt ] & = & 2 \\sum_{i=1}^r(1 - \\widehat{c}_i ) \\le2 \\sum_{i=1}^r \\bigl(1 - \\widehat{c}_i^2\\bigr ) = 2 \\sum _",
    "{ i=1}^r\\widehat{s}_i^2 = { |\\!|\\!|}\\widehat{p}{|\\!|\\!|}_{\\mathrm{hs}}^2,\\nonumber\\end{aligned}\\ ] ] where we have used the relations @xmath375 , @xmath376 $ ] and @xmath377 .      using the notation introduced in lemma  [ lemmanageable ] , our goal is to bound the hilbert ",
    "schmidt norm @xmath378 .",
    "without loss of generality we will assume @xmath379 throughout .",
    "recalling definition ( [ eqnsamsplit ] ) of the random matrix @xmath380 , the following inequality plays a central role in the proof :    [ lemperturbation ] under condition and @xmath381 , we have @xmath382 with probability at least @xmath383 .",
    "we use the shorthand notation @xmath384 for the proof .",
    "since @xmath359 is feasible and @xmath171 is optimal for problem ( [ eqndiscretemest ] ) , we have the basic inequality @xmath385 . using the decomposition @xmath386 and rearranging yields the inequality @xmath387 from definition ( [ eqnsamsplit ] ) of @xmath388 and @xmath389 , the left - hand side of the inequality ( [ eqpertmidproofinequ ] )",
    "can be lower bounded as @xmath390 where we have used ( 90 ) and ( 91 ) of appendix i several times ( cf . the supplementary material  @xcite ) .",
    "we note that @xmath391 and @xmath392 provided @xmath393 ; see equation ( 70 ) . to bound the minimum eigenvalue of @xmath394 , let @xmath395 denote the minimum singular value of the @xmath396 gaussian matrix @xmath397 .",
    "the following concentration inequality is well known ( cf .",
    "@xcite ) : @xmath398 \\le \\exp\\bigl(-t^2/2\\bigr)\\qquad\\mbox{for all $ t > 0$.}\\ ] ] since @xmath399 , we have that @xmath400 with probability at least @xmath401 .",
    "assuming @xmath402 and setting @xmath403 , we get @xmath404 with probability at least @xmath405 . putting the pieces together",
    "yields the claim .",
    "inequality ( [ eqnbasic ] ) reduces the problem of bounding @xmath406 to the sub - problem of studying the random variable @xmath407 .",
    "based on lemma  [ lemperturbation ] , our next step is to establish an inequality ( holding with high probability ) of the form @xmath408 where @xmath409 is some universal constant , @xmath233 is the constant in condition ( a1 ) and @xmath236 is the critical radius from theorem  [ thmsubspaceerr ] .",
    "doing so is a nontrivial task : both matrices @xmath410 and @xmath380 are random and depend on one another , since the subspace @xmath171 was obtained by optimizing a random function depending on @xmath380 .",
    "consequently , our proof of bound ( [ eqninitialbound ] ) involves deriving a uniform law of large numbers for a certain matrix class .",
    "suppose that bound ( [ eqninitialbound ] ) holds , and that the subspaces @xmath359 and @xmath171 are properly aligned .",
    "lemma  [ lemmanageable ] implies that @xmath411 , and since @xmath412 is a nondecreasing function , inequality ( [ eqninitialbound ] ) combined with lemma  [ lemperturbation ] implies that @xmath413 from which the claim follows as long as @xmath233 is suitably small ( e.g. , @xmath414 suffices ) .",
    "accordingly , in order to complete the proof of theorem  [ thmsubspaceerr ] , it remains to prove bound ( [ eqninitialbound ] ) , and the remainder of our work is devoted to this goal .",
    "given the linearity of trace , we can bound the terms @xmath415 and @xmath416 separately .",
    "let @xmath417 , @xmath418 and @xmath419 and @xmath420 denote the columns of @xmath171 , @xmath359 , @xmath421 and @xmath422 , respectively , where we recall the definitions of these quantities from equation ( [ eqnsamsplit ] ) and lemma  [ lemmanageable ] .",
    "note that @xmath423 . in appendix c.1 of the supplementary material  @xcite",
    ", we show that @xmath424 consequently , we need to obtain bounds on quantities of the form @xmath425 , where the vector @xmath426 is either fixed ( e.g. , @xmath427 ) or random ( e.g. , @xmath428 ) .",
    "the following lemmas provide us with the requisite bounds :    [ lemdeltaoneellipse ] we have @xmath429 with probability at least @xmath430 .",
    "[ lemdeltaonefixed ] we have @xmath431 \\geq1 - r^2 \\exp\\bigl(- \\kappa^2 r^{-2 } { n}/2 \\sigma^2\\bigr).\\ ] ]    see appendices c.2 and c.3 in the supplementary material  @xcite for the proofs of these claims .",
    "recalling definition ( [ eqnsamsplit ] ) of @xmath432 and using linearity of the trace , we obtain @xmath433 since @xmath434 , we have @xmath435\\\\[-8pt ] & \\leq & \\sigma^2 \\sum_{j=1}^r \\biggl\\ { 2 \\underbrace{\\bigl(\\widetilde{z}^*_j\\bigr)^t \\biggl ( \\frac{1}{{n}}w^tw- i_r \\biggr ) \\widehat{e}_j}_{t_1(\\widehat{e}_j ;",
    "\\widetilde{z}^*_j ) } + \\underbrace{\\frac{1}{{n } } \\| w\\widehat{e}_j \\|_2 ^ 2}_{t_2(\\widehat{e}_j ) } \\biggr\\},\\nonumber\\end{aligned}\\ ] ] where we have used the fact that @xmath436 = 2\\sum_j ( \\widehat{c}_j - 1 ) = -{|\\!|\\!|}\\widehat{e}{|\\!|\\!|}_{\\mathrm{hs}}^2 \\le 0 $ ] .",
    "the following lemmas provide high probability bounds on the terms @xmath437 and @xmath438 .",
    "[ lemtermone ] we have the upper bound @xmath439 with probability @xmath440 .",
    "[ lemtermtwo ] we have the upper bound @xmath441 with probability at least @xmath442 .",
    "see appendices c.4 and c.5 in the supplementary material  @xcite for the proofs of these claims .",
    "we studied the problem of sampling for functional pca from a functional - theoretic viewpoint .",
    "the principal components were assumed to lie in some hilbert subspace @xmath0 of @xmath338 , usually a rkhs , and the sampling operator , a bounded linear map @xmath443 .",
    "the observation model was taken to be the output of @xmath12 plus some gaussian noise .",
    "the two main examples of @xmath12 considered were time sampling , @xmath444_{j } = f(t_{j})$ ] and ( generalized ) frequency truncation @xmath444_{j } = \\langle\\psi_{j } , f \\rangle_{l^{2}}$ ] .",
    "we showed that it is possible to recover the subspace spanned by the original components , by applying a regularized version of pca in @xmath445 followed by simple linear mapping back to function space .",
    "the regularization involved the `` trace - smoothness condition '' ( [ eqntracesmooth ] ) based on the matrix @xmath446 whose eigendecay influenced the rate of convergence in @xmath445 .",
    "we obtained the rates of convergence for the subspace estimators both in the discrete domain , @xmath445 , and the function domain , @xmath338 .",
    "as examples , for the case of a rkhs @xmath0 for which both the kernel integral operator and the kernel matrix @xmath122 have polynomial-@xmath50 eigendecay ( i.e. , @xmath447 ) , the following rates in @xmath448-projection distance for subspaces in the function domain were worked out in detail :       the two terms in each rate can be associated , respectively , with the estimation error ( due to noise ) and approximation error ( due to having finite samples of an infinite - dimensional object ) .",
    "both rates exhibit a trade - off between the number of statistical samples ( @xmath5 ) and that of functional samples ( @xmath3 ) .",
    "the two rates are qualitatively different : the two terms in the time sampling case interact to give an overall fast rate of @xmath261 for the optimal trade - off @xmath449 , while there is no interaction between the two terms in the frequency truncation ; the optimal trade - off gives an overall rate of @xmath450 , a characteristics of nonparametric problems .",
    "finally , these rates were shown to be minimax optimal ."
  ],
  "abstract_text": [
    "<S> we consider the sampling problem for functional pca ( fpca ) , where the simplest example is the case of taking time samples of the underlying functional components . </S>",
    "<S> more generally , we model the sampling operation as a continuous linear map from @xmath0 to @xmath1 , where the functional components to lie in some hilbert subspace @xmath0 of @xmath2 , such as a reproducing kernel hilbert space of smooth functions . </S>",
    "<S> this model includes time and frequency sampling as special cases . </S>",
    "<S> in contrast to classical approach in fpca in which access to entire functions is assumed , having a limited number @xmath3 of functional samples places limitations on the performance of statistical procedures . </S>",
    "<S> we study these effects by analyzing the rate of convergence of an @xmath4-estimator for the subspace spanned by the leading components in a multi - spiked covariance model . </S>",
    "<S> the estimator takes the form of regularized pca , and hence is computationally attractive . </S>",
    "<S> we analyze the behavior of this estimator within a nonasymptotic framework , and provide bounds that hold with high probability as a function of the number of statistical samples @xmath5 and the number of functional samples @xmath3 . </S>",
    "<S> we also derive lower bounds showing that the rates obtained are minimax optimal . </S>"
  ]
}