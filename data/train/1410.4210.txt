{
  "article_text": [
    "sparse - group lasso ( sgl ) @xcite is a powerful regression technique in identifying and features simultaneously . to yield sparsity at both group and individual feature",
    ", sgl combines the lasso @xcite and group lasso @xcite penalties . in recent years",
    ", sgl has found great success in a wide range of applications , including but not limited to machine learning @xcite , signal processing @xcite , bioinformatics @xcite etc .",
    "many research efforts have been devoted to developing efficient solvers for sgl @xcite .",
    "however , when the feature dimension is extremely high , the complexity of the sgl regularizers imposes great computational challenges .",
    "therefore , there is an increasingly urgent need for nontraditional techniques to address the challenges posed by the massive volume of the data sources .",
    "recently , el ghaoui et al .",
    "@xcite proposed a promising feature reduction method , called _ safe screening _ , to screen out the so - called _ inactive _ features , which have zero coefficients in the solution , from the optimization .",
    "thus , the size of the data matrix needed for the training phase can be reduced , which may lead to substantial improvement in the efficiency of solving sparse models . by safe ,",
    "various exact and heuristic feature screening methods have been proposed for many sparse models such as lasso @xcite , group lasso @xcite , etc .",
    "it is worthwhile to mention that the discarded features by exact feature screening methods such as safe @xcite , dome @xcite and edpp @xcite are guaranteed to have zero coefficients in the solution .",
    "however , heuristic feature screening methods like strong rule @xcite may mistakenly discard features which have coefficients in the solution .",
    "more recently , the idea of exact feature screening has been extended to exact sample screening , which screens out the nonsupport vectors in svm @xcite and lad @xcite . as a promising data reduction tool",
    ", exact feature / sample screening would be of great practical importance because they can effectively reduce the data size without sacrificing the optimality @xcite .",
    "however , all of the existing feature / sample screening methods are only applicable for the sparse models with one sparsity - inducing regularizer . in this paper , we propose an exact two - layer feature screening method , called tlfre , for the sgl problem .",
    "the two - layer reduction is able to quickly identify the inactive groups and the inactive features , respectively , which are guaranteed to have zero coefficients in the solution . to the best of our knowledge",
    ", tlfre is the first screening method which is capable of dealing with multiple sparsity - inducing regularizers .",
    "we note that most of the existing exact feature screening methods involve an estimation of the dual optimal solution .",
    "the difficulty in developing screening methods for sparse models with multiple sparsity - inducing regularizers like sgl is that the dual feasible set is the sum of simple convex sets .",
    "thus , to determine the feasibility of a given point , we need to know if it is decomposable with respect to the summands , which is itself a nontrivial problem ( see section [ section : basics ] ) .",
    "one of our major contributions is that we derive an elegant decomposition method of any dual feasible solutions of sgl via the framework of fenchel s duality ( see section [ section : fenchel_dual_sgl ] ) .",
    "based on the fenchel s dual problem of sgl , we motivate tlfre by an in - depth exploration of its geometric properties and the optimality conditions in section [ section : screening_rules_sgl ] .",
    "we derive the set of the regularization parameter values corresponding to zero solutions . to develop tlfre",
    ", we need to estimate the upper bounds involving the dual optimal solution . to this end",
    ", we first give an accurate estimation of the dual optimal solution via the normal cones .",
    "then , we formulate the estimation of the upper bounds via nonconvex optimization problems .",
    "we show that these nonconvex problems admit closed form solutions .",
    "the rest of this paper is organized as follows . in section [ section : basics ] , we briefly review some basics of the sgl problem .",
    "we then derive the fenchel s dual of sgl with nice geometric properties under the elegant framework of fenchel s duality in section [ section : fenchel_dual_sgl ] . in section [ section :",
    "screening_rules_sgl ] , we develop the tlfre screening rule for sgl . to demonstrate the flexibility of the proposed framework , we extend tlfre to the nonnegative lasso problem in section [ section : nnlasso ] .",
    "experiments in section [ section : experiments ] on both synthetic and real data sets demonstrate that the speedup gained by the proposed screening rules in solving sgl and nonnegative lasso can be orders of magnitude .",
    "* notation * : let @xmath2 , @xmath3 and @xmath4 be the @xmath0 , @xmath1 and @xmath5 norms , respectively",
    ". denote by @xmath6 , @xmath7 , and @xmath8 the unit @xmath0 , @xmath1 , and @xmath5 norm balls in @xmath9 ( we omit the superscript if it is clear from the context ) .",
    "for a set @xmath10 , let @xmath11 be its interior .",
    "if @xmath10 is closed and convex , we define the projection operator as @xmath12 .",
    "we denote by @xmath13 the indicator function of @xmath10 , which is @xmath14 on @xmath10 and @xmath15 elsewhere .",
    "let @xmath16 be the class of proper closed convex functions on @xmath9 .",
    "for @xmath17 , let @xmath18 be its subdifferential .",
    "the domain of @xmath19 is the set @xmath20 . for @xmath21 ,",
    "let @xmath22_i$ ] be its @xmath23 component . for @xmath24",
    ", let @xmath25 if @xmath26 , and @xmath27 .",
    "we define @xmath28_i\\in      \\begin{cases }          \\textup{sign}([\\mathbf{w}]_i),\\hspace{2mm}\\textup{if}\\hspace{1mm}[\\mathbf{w}]_i\\neq0;\\\\          [ -1,1],\\hspace{7mm}\\textup{if}\\hspace{1mm}[\\mathbf{w}]_i=0 .",
    "\\end{cases }      \\right\\}\\end{aligned}\\ ] ] we denote by @xmath29",
    ". then , the shrinkage operator @xmath30 with @xmath31 is @xmath32_i=(|[\\mathbf{w}]_i|-\\gamma)_+\\textup{sgn}([\\mathbf{w}]_i),\\,i=1,\\ldots , n.\\end{aligned}\\ ] ]",
    "in this section , we briefly review some basics of sgl . let @xmath33 be the response vector and @xmath34 be the matrix of features . with the group information available ,",
    "the sgl problem @xcite is @xmath35 where @xmath36 is the number of features in the @xmath37 group , @xmath38 denotes the predictors in that group with the corresponding coefficient vector @xmath39 , and @xmath40 are positive regularization parameters . without loss of generality ,",
    "let @xmath41 and @xmath42 with @xmath43 .",
    "then , problem ( [ prob : sgl0 ] ) becomes : @xmath44 by the lagrangian multipliers method @xcite ( see the supplement ) , the dual problem of sgl is @xmath45 it is well - known that the dual feasible set of lasso is the intersection of closed half spaces ( thus a polytope ) ; for group lasso , the dual feasible set is the intersection of ellipsoids .",
    "surprisingly , the geometric properties of these dual feasible sets play fundamentally important roles in most of the existing screening methods for sparse models with one sparsity - inducing regularizer @xcite .    when we incorporate multiple sparse - inducing regularizers to the sparse models , problem ( [ prob : dual_sgl_lagrangian ] )",
    "indicates that the dual feasible set can be much more complicated .",
    "although ( [ prob : dual_sgl_lagrangian ] ) provides a geometric description of the dual feasible set of sgl , it is not suitable for further analysis .",
    "notice that , _ even the feasibility of a given point @xmath46 is not easy to determine _ , since it is nontrivial to tell if @xmath47 can be decomposed into @xmath48 with @xmath49 and @xmath50 . therefore , to develop screening methods for sgl , it is desirable to gain deeper understanding of the sum of simple convex sets .    in the next section ,",
    "we analyze the dual feasible set of sgl in depth via the fenchel s duality theorem .",
    "we show that for each @xmath51 , fenchel s duality naturally leads to an explicit decomposition @xmath52 , with one belonging to @xmath53 and the other one belonging to @xmath54 .",
    "this lays the foundation of the proposed screening method for sgl .",
    "in section [ subsection : fenchel_dual_sgl ] , we derive the fenchel s dual of sgl via fenchel s duality theorem .",
    "we then motivate tlfre and sketch our approach in section [ subsection : general_rule_sgl ] . in section [ subsection : lambda12_0_solution ] , we discuss the properties of the fenchel s dual of sgl and derive the set of @xmath55 leading to zero solutions .",
    "to derive the fenchel s dual problem of sgl , we need the fenchel s duality theorem as stated in theorem [ thm : fenchel_duality ] .",
    "the conjugate of @xmath17 is the function @xmath56 defined by @xmath57    [ thm : fenchel_duality ] let @xmath58 , @xmath59 , and @xmath60 be an affine mapping from @xmath61 to @xmath62 .",
    "let @xmath63 $ ] be primal and dual values defined , respectively , by the fenchel problems : @xmath64 one has @xmath65 . if , furthermore , @xmath19 and @xmath66 satisfy the condition @xmath67 , then the equality holds , i.e. , @xmath68 , and the supreme is attained in the dual problem if finite .",
    "we omit the proof of theorem [ thm : fenchel_duality ] since it is a slight modification of theorem 3.3.5 in @xcite .",
    "let @xmath69 , and @xmath70 be the second term in ( [ prob : sgl ] ) .",
    "then , sgl can be written as @xmath71 to derive the fenchel s dual problem of sgl , theorem [ thm : fenchel_duality ] implies that we need to find @xmath72 and @xmath73 .",
    "it is well - known that @xmath74 . therefore , we only need to find @xmath73 , where the concept _ infimal convolution _ is needed :    [ def : inf_conv ] let @xmath75 .",
    "the infimal convolution of @xmath76 and @xmath77 is defined by @xmath78 and it is exact at a point @xmath79 if there exists a @xmath80 such that @xmath81 @xmath82 is exact if it is exact at every point of its domain , in which case it is denoted by @xmath83 .    with the infimal convolution , we derive the conjugate function of @xmath66 in lemma [ lemma : conjugate_omega_sgl ] .",
    "[ lemma : conjugate_omega_sgl ] let @xmath84 , @xmath85 and @xmath86 .",
    "moreover , let @xmath87 , @xmath88 .",
    "then , the following hold :    1 .",
    "@xmath89 , 2 .   @xmath90    where @xmath91 is the sub - vector of @xmath79 corresponding to the @xmath37 group .",
    "to prove lemma [ lemma : conjugate_omega_sgl ] , we first cite the following technical result .",
    "[ thm : infconv_sum ] let @xmath92 .",
    "suppose there is a point in @xmath93 at which @xmath94 is continuous .",
    "then , for all @xmath95 : @xmath96",
    ".      \\end{aligned}\\ ] ]    we now give the proof of lemma [ lemma : conjugate_omega_sgl ] .    the first part can be derived directly by the definition as follows : @xmath97 @xmath98    to show the second part , theorem [ thm : infconv_sum ] indicates that we only need to show @xmath99 is exact ( note that @xmath100 and @xmath101 are continuous everywhere ) .",
    "let us now compute @xmath102 .",
    "@xmath103 to solve the optimization problem in ( [ eqn : infconv_sgl ] ) , i.e. , @xmath104 we can consider the following problem @xmath105 we can see that the optimal solution of problem ( [ prob : projection_sgl ] ) must also be an optimal solution of problem ( [ prob : infconv_sgl ] ) .",
    "let @xmath106 be the optimal solution of ( [ prob : projection_sgl ] ) .",
    "we can see that @xmath106 is indeed the projection of @xmath107 on @xmath54 , which admits a closed form solution : @xmath108_i=[\\mathbf{p}_{\\mathcal{b}_{\\infty}}(\\xi_g)]_i=          \\begin{cases }              1,\\hspace{7.5mm}{\\rm if}\\hspace{1mm}[\\xi_g]_i>1,\\\\              [ \\xi_g]_i,\\hspace{3mm}{\\rm if}\\hspace{1mm}|[\\xi_g]_i|\\leq1,\\\\              -1,\\hspace{5mm}{\\rm if}\\hspace{1mm}[\\xi_g]_i<-1 .",
    "\\end{cases }      \\end{aligned}\\ ] ] thus , problem ( [ prob : infconv_sgl ] ) can be solved as @xmath109 hence , the infimal convolution in eq .",
    "( [ eqn : infconv_sgl ] ) is exact and theorem [ thm : infconv_sum ] leads to @xmath110 which completes the proof .",
    "note that @xmath111 admits a closed form solution , i.e. , @xmath112_i=\\textup{sgn}\\left([\\xi_g]_i\\right)\\min\\left(\\left|[\\xi_g]_i\\right|,1\\right)$ ] .",
    "combining theorem [ thm : fenchel_duality ] and lemma [ lemma : conjugate_omega_sgl ] , the fenchel s dual of sgl can be derived as follows .",
    "[ thm : dual_sgl ] for the sgl problem in ( [ prob : sgl ] ) , the following hold :    1 .",
    "the fenchel s dual of sgl is given by : @xmath113 2 .",
    "let @xmath114 and @xmath115 be the optimal solutions of problems ( [ prob : sgl ] ) and ( [ prob : dual_sgl_fenchel ] ) , respectively .",
    "@xmath116    to show theorem [ thm : dual_sgl ] , we need the fenchel - young inequality as follows :    [ lemma : fy_inequality ] any point @xmath117 and @xmath118 in the domain of a function @xmath119 $ ] satisfy the inequality @xmath120 equality holds if and only if @xmath121 .",
    "we now give the proof of theorem [ thm : dual_sgl ] .",
    "we first show the first part . combining theorem [ thm : fenchel_duality ] and lemma [ lemma : conjugate_omega_sgl ]",
    ", the fenchel s dual of sgl can be written as : @xmath122 which is equivalent to problem ( [ prob : dual_sgl_fenchel ] ) .    to show the second half , we have the following inequalities by fenchel - young inequality : @xmath123 we sum the inequalities in ( [ ineqn : fy_f_sgl ] ) and ( [ ineqn : fy_omega_sgl ] ) together and get @xmath124 clearly , the left and right hand sides of inequality ( [ ineqn : weak_duality_sgl ] ) are the objective functions of the pair of fenchel s problems . because @xmath125 and @xmath126 , we have @xmath127 thus ,",
    "the equality in ( [ ineqn : weak_duality_sgl ] ) holds at @xmath114 and @xmath115 , i.e. , @xmath128 therefore , the equality holds in both ( [ ineqn : fy_f_sgl ] ) and ( [ ineqn : fy_omega_sgl ] ) at @xmath114 and @xmath115 . by applying lemma [ lemma : fy_inequality ] again , we have @xmath129 which completes the proof .",
    "( [ eqn : kkt1_sgl ] ) and eq .",
    "( [ eqn : kkt2_sgl ] ) are the so - called kkt conditions and can also be obtained by the lagrangian multiplier method see [ subsection : lagrangian_supplement ] in the supplement .",
    "[ remark : shrinkage_feasible_sgl ] we note that the shrinkage operator can also be expressed by @xmath130 therefore , problem ( [ prob : dual_sgl_fenchel ] ) can be written more compactly as @xmath131    * the equivalence between the dual formulations * for the sgl problem , its lagrangian dual in ( [ prob : dual_sgl_lagrangian ] ) and fenchel s dual in ( [ prob : dual_sgl_fenchel ] ) are indeed equivalent to each other .",
    "we bridge them together by the following lemma .",
    "[ lemma : infconv_sets ] let @xmath132 and @xmath133 be nonempty subsets of @xmath9 .",
    "then @xmath134 .    in view of lemmas [ lemma : conjugate_omega_sgl ] and",
    "[ lemma : infconv_sets ] , and recall that @xmath135 , we have @xmath136 combining eq .",
    "( [ eqn : conjugate_omega2_sgl ] ) and theorem [ thm : fenchel_duality ] , we obtain the dual formulation of sgl in ( [ prob : dual_sgl_lagrangian ] ) .",
    "therefore , the dual formulations of sgl in ( [ prob : dual_sgl_lagrangian ] ) and ( [ prob : dual_sgl_fenchel ] ) are the same .",
    "an appealing advantage of the fenchel s dual in ( [ prob : dual_sgl_fenchel ] ) is that we have a natural decomposition of all points @xmath137 : @xmath138 with @xmath139 and @xmath140 . as a result",
    ", this leads to a convenient way to determine the feasibility of any dual variable @xmath46 by checking if @xmath141 , @xmath88 .",
    "we motive the two - layer screening rules via the kkt condition in eq .",
    "( [ eqn : kkt2_sgl ] ) .",
    "as implied by the name , there are two layers in our method .",
    "the first layer aims to identify the inactive groups , and the second layer is designed to detect the inactive features for the remaining groups .    by eq .",
    "( [ eqn : kkt2_sgl ] ) , we have the following cases by noting @xmath142 and @xmath143 * case 1 . *",
    "if @xmath144 , we have @xmath145_i\\in      \\begin{cases }          \\alpha\\sqrt{n_g}\\frac{[\\beta_g^*(\\lambda,\\alpha)]_i}{\\|\\beta^*_g(\\lambda,\\alpha)\\|}+\\textup{sign}([\\beta_g^*(\\lambda,\\alpha)]_i ) , \\hspace{2mm}\\textup{if}\\hspace{1mm}[\\beta_g^*(\\lambda,\\alpha)]_i\\neq0,\\\\          [ -1,1],\\hspace{46.5mm}\\textup{if}\\hspace{1mm}[\\beta_g^*(\\lambda,\\alpha)]_i=0 .      \\end{cases}\\end{aligned}\\ ] ] in view of eq .",
    "( [ eqn : kkt1_nonzero_sgl ] ) , we can see that @xmath146_i\\right|\\leq 1\\hspace{1mm}\\textup{then}\\hspace{1 mm } [ \\beta^*_g(\\lambda,\\alpha)]_i=0.\\end{aligned}\\ ] ] * case 2 . *",
    "if @xmath147 , we have @xmath148_i\\in\\alpha\\sqrt{n_g}[\\mathbf{u}_g]_i+[-1,1],\\,\\|\\mathbf{u}_g\\|\\leq1.\\end{aligned}\\ ] ]    * the first layer ( group - level ) of tlfre * from ( [ eqn : c1_a_sgl ] ) in * case 1 * , we have @xmath149 clearly , ( [ rule1_sgl ] ) can be used to identify the inactive groups and thus a group - level screening rule .    * the second layer ( feature - level ) of tlfre *",
    "let @xmath150 be the @xmath23 column of @xmath151 .",
    "we have @xmath152_i=\\mathbf{x}_{g_i}^t\\theta^*(\\lambda,\\alpha)$ ] . in view of ( [ eqn : c1_b_sgl ] ) and ( [ eqn : c2_sgl ] )",
    ", we can see that @xmath153_i=0.\\end{aligned}\\ ] ] different from ( [ rule1_sgl ] ) , ( [ rule2_sgl ] ) detects the inactive features and thus it is a feature - level screening rule",
    ".    however , we can not directly apply ( [ rule1_sgl ] ) and ( [ rule2_sgl ] ) to identify the inactive groups / features because both need to know @xmath115 .",
    "inspired by the safe rules @xcite , we can first estimate a region @xmath154 containing @xmath115 .",
    "let @xmath155 .",
    "then , ( [ rule1_sgl ] ) and ( [ rule2_sgl ] ) can be relaxed as follows : @xmath156_i=0.\\end{aligned}\\ ] ]    inspired by ( [ rrule1_sgl ] ) and ( [ rrule2_sgl ] ) , we develop tlfre via the following three steps :    1 .",
    "given @xmath157 and @xmath158 , we estimate a region @xmath154 that contains @xmath115 .",
    "2 .   we solve for the supreme values in ( [ rrule1_sgl ] ) and ( [ rrule2_sgl ] ) .",
    "3 .   by plugging in the supreme values from * step 2 * , ( [ rrule1_sgl ] ) and ( [ rrule2_sgl ] ) result in the desired two - layer screening rules for sgl .      in this section ,",
    "we explore the geometric properties of the fenchel s dual of sgl in depth  based on which we can derive the set of parameter values such that the primal optimal solutions are 0 .",
    "we consider the sgl problem in ( [ prob : sgl ] ) and ( [ prob : sgl0 ] ) in section [ sssec : zero_sol_sgl ] and [ sssec : zero_sol_sgl0 ] , respectively .",
    "consider the sgl problem in ( [ prob : sgl ] ) .",
    "for notational convenience , let @xmath159 we denote the feasible set of the fenchel s dual of sgl by @xmath160 in view of problem ( [ prob : dual_sgl_fenchel ] ) [ or ( [ prob : dual_sgl_shrinkage_fenchel ] ) ] , we can see that @xmath115 is the projection of @xmath161 on @xmath162 , i.e. , @xmath163 thus , if @xmath164 , we have @xmath165 . moreover , by ( [ rule1_sgl ] ) , we can see that @xmath166 if @xmath161 is an _ interior _",
    "point of @xmath162 .",
    "indeed , we have the following stronger result .",
    "[ thm : lambda_alpha_sgl ] for the sgl problem , let @xmath167 . then , the following statements are equivalent :    @xmath168 ,    @xmath169 ,    @xmath166 ,    @xmath170 .",
    "the equivalence between ( i ) and ( ii ) can be see from the fact that @xmath171 .",
    "next , we show ( ii)@xmath172(iii ) .",
    "let us first show ( ii)@xmath173(iii ) .",
    "we assume that @xmath165 . by the kkt condition in ( [ eqn : kkt1_sgl ] )",
    ", we have @xmath174 .",
    "we claim that @xmath166 . to see this ,",
    "let @xmath175 with @xmath176 be another optimal solution of sgl .",
    "we denote by @xmath76 the objective function of sgl in ( [ prob : sgl ] ) .",
    "then , we have @xmath177 which contradicts with the assumption @xmath175 is also an optimal solution .",
    "this contradiction indicates that @xmath114 must be @xmath14 .",
    "the converse direction , i.e. , ( ii)@xmath178(iii ) , can be derived directly from the kkt condition in eq .",
    "( [ eqn : kkt1_sgl ] ) .",
    "finally , we show the equivalence ( i)@xmath172(iv ) .",
    "indeed , in view of the dual problem in ( [ prob : dual_sgl_shrinkage_fenchel ] ) , we can see that @xmath164 if and only if @xmath179 we note that @xmath180 is monotonically decreasing with respect to @xmath157 .",
    "thus , the inequality in ( [ ineqn : feasibility_y_sgl ] ) is equivalent to ( iv ) , which completes the proof .",
    "we note that @xmath181 in the definition of @xmath182 admits a closed form solution . for notational convenience ,",
    "let @xmath183 be the vector by taking absolute value of @xmath118 component - wisely and @xmath22^{(k)}$ ] be the vector consisting of the first @xmath184 components of @xmath118 .",
    "[ lemma : rho ] we sort @xmath185 in descending order and denote it by @xmath186 .    1 .",
    "if there exists @xmath187_k$ ] such that @xmath188_k)\\|=\\alpha\\sqrt{n_g}$ ] , then @xmath189_k$ ] .",
    "2 .   otherwise , let @xmath190_i)\\|$ ] , @xmath191 , and @xmath192 .",
    "there exists a @xmath184 such that @xmath193 , and @xmath194_{k+1},[\\mathbf{z}]_k)$ ] is the root of @xmath195^{(k)}\\|_1+\\|[\\mathbf{z}]^{(k)}\\|^2=0.\\ ] ]    we omit the proof of lemma [ lemma : rho ] because it is a direct consequence by noting that @xmath196 is piecewise quadratic .      theorem",
    "[ thm : lambda_alpha_sgl ] implies that the optimal solution @xmath114 is @xmath14 as long as @xmath164 .",
    "this geometric property also leads to an explicit characterization of the set of @xmath197 such that the corresponding solution of problem ( [ prob : sgl0 ] ) is @xmath14 .",
    "we denote by @xmath198 the optimal solution of problem ( [ prob : sgl0 ] ) .",
    "[ cor : lambdamx_sgl ] for the sgl problem in ( [ prob : sgl0 ] ) , let @xmath199 .",
    "then ,    1 .",
    "2 .   if @xmath201 or @xmath202 , then @xmath203 .",
    "before we prove corollary [ cor : lambdamx_sgl ] , we first derive the fenchel s dual of ( [ prob : sgl0 ] ) . by letting @xmath69 and @xmath204",
    ", the sgl problem in ( [ prob : sgl0 ] ) can be written as : @xmath205 then , by fenchel s duality theorem , the fenchel s dual problem of ( [ prob : sgl0 ] ) is @xmath206 let @xmath198 and @xmath207 be the optimal solutions of problem ( [ prob : sgl0 ] ) and ( [ prob : dual_sgl0_fenchel ] ) .",
    "the optimality conditions can be written as @xmath208 we denote by @xmath209 the feasible set of problem ( [ prob : dual_sgl0_fenchel ] ) .",
    "it is easy to see that @xmath210    we now present the proof of corollary [ cor : lambdamx_sgl ]",
    ".    for notational convenience , let    1 .",
    "@xmath211 , 2 .",
    "@xmath212 , 3 .",
    "@xmath203 , 4 .",
    "@xmath213 .",
    "the first half of the statement is ( iii)@xmath172(iv ) . indeed , by a similar argument as in the proof of theorem [ thm : lambda_alpha_sgl ]",
    ", we can see that the above statements are all equivalent to each other .",
    "we now show the second half .",
    "we first show that @xmath214 by the first half , we only need to show @xmath215 indeed , the definition of @xmath216 implies that @xmath217 we note that for any @xmath218 , we have @xmath219 therefore , we can see that @xmath220 the proof of ( [ statement : lambda1mx ] ) is complete .    similarly , to show that @xmath221 , we only need to show @xmath222 by the definition of @xmath223 , we can see that @xmath224 thus , we have @xmath211 , which completes the proof .",
    "we follow the three steps in section [ subsection : general_rule_sgl ] to develop tlfre . in section [",
    "subsection : estimation_dual_sgl ] , we give an accurate estimation of @xmath115 via normal cones @xcite .",
    "then , we compute the supreme values in ( [ rrule1_sgl ] ) and ( [ rrule2_sgl ] ) by solving nonconvex problems in section [ subsection : optimization_sgl ] .",
    "we present the tlfre rules in section [ subsection : screening_rules_sgl ] .      because of the geometric property of the dual problem in ( [ prob : dual_sgl_fenchel ] ) , i.e. , @xmath171",
    ", we have a very useful characterization of the dual optimal solution via the so - called normal cones @xcite .",
    "[ prop : normal_cone ] for a closed convex set @xmath225 and a point @xmath226 , the normal cone to @xmath10 at @xmath118 is defined by @xmath227 then , the following hold :    1 .",
    "2 .   @xmath229 .",
    "3 .   let @xmath230 .",
    "then , @xmath231 .",
    "4 .   let @xmath230 and @xmath232 .",
    "then , @xmath233 for all @xmath234 .    by theorem [ thm : lambda_alpha_sgl ] , @xmath235 is known if @xmath236 .",
    "thus , we can estimate @xmath115 in terms of @xmath235 . due to the same reason ,",
    "_ we only consider the cases with @xmath237 for @xmath115 _ to be estimated .    in many applications ,",
    "the parameter values that perform the best are usually unknown .",
    "to determine appropriate parameter values , commonly used approaches such as cross validation and stability selection involve solving sgl many times over a grip of parameter values .",
    "thus , given @xmath238 and @xmath239 , we can fix the value of @xmath158 each time and solve sgl by varying the value of @xmath157 .",
    "we repeat the process until we solve sgl for all of the parameter values .",
    "[ thm : estimation_sgl ] for the sgl problem in ( [ prob : sgl ] ) , suppose that @xmath235 is known with @xmath240 .",
    "let @xmath181 , @xmath88 , be defined by theorem [ thm : lambda_alpha_sgl ] .",
    "for any @xmath241 , we define @xmath242 then , the following hold :    1 .",
    "@xmath243 , 2 .",
    "@xmath244 .    1 .",
    "suppose that @xmath245 .",
    "theorem [ thm : lambda_alpha_sgl ] implies that @xmath246 and thus @xmath247 by the third part of proposition [ prop : normal_cone ] , we can see that @xmath248 thus , the statement holds for all @xmath245 .",
    "+ suppose that @xmath236 . by theorem [ thm : lambda_alpha_sgl ]",
    ", we have @xmath249 in view of the definition of @xmath250 , we have @xmath251 where @xmath252 is the number of feature contained in @xmath250 . moreover , it is easy to see that @xmath253 therefore ,",
    "to prove the statement , we need to show that @xmath254 recall remark [ remark : shrinkage_feasible_sgl ] , we have the following identity [ see eq .",
    "( [ eqn : shrinkage ] ) ] @xmath255 thus , we have @xmath256 consider the first term on the right hand side of eq .",
    "( [ eqn : normvec_bdy_sgl ] ) , we have @xmath257 let @xmath258_i>1\\}$ ] and @xmath259_i<-1\\}$ ] .",
    "we note that the second term on the right hand side of eq .",
    "( [ eqn : normvec_bdy_1_sgl ] ) can be written as @xmath260_i-1\\right)\\left([\\mathbf{p}_{\\mathcal{b}_{\\infty}}(\\mathbf{x}_*^t\\theta)]_i-1\\right)+\\sum_{j\\in\\mathcal{n}}\\left([\\mathbf{x}_*^t\\tfrac{\\mathbf{y}}{\\lambda_{\\rm max}^{\\alpha}}]_j+1\\right)\\left([\\mathbf{p}_{\\mathcal{b}_{\\infty}}(\\mathbf{x}_*^t\\theta)]_j+1\\right ) .",
    "\\end{aligned}\\ ] ] because @xmath261 , we can see that eq .",
    "( [ eqn : normvec_bdy_2_sgl ] ) is non - positive . therefore , by eq .",
    "( [ eqn : normvec_bdy_1_sgl ] ) , we have @xmath262 combining eq .",
    "( [ eqn : normvec_bdy_sgl ] ) and the inequality in ( [ eqn : normvec_bdy_3_sgl ] ) , we can see that the inequality in ( [ ineqn : normal_vector_boundary_sgl ] ) holds .",
    "thus , the statement holds for @xmath236 .",
    "this completes the proof .",
    "we now show the second half .",
    "it is easy to see that the statement is equivalent to @xmath263 thus , we will show that the inequality in ( [ ineqn : estimate1_sgl ] ) holds .",
    "+ because of the first half , we have @xmath264 by letting @xmath265 , the inequality in ( [ ineqn : n1_o_sgl ] ) leads to @xmath266 in view of the first half and by letting @xmath267 , the inequality in ( [ ineqn : n1_o_sgl ] ) leads to @xmath268 moreover , the first half also leads to @xmath269 .",
    "thus , we have @xmath270 by letting @xmath271 , the inequality in ( [ ineqn : n2_o_sgl ] ) results in @xmath272 we can see that the inequality in ( [ ineqn : n2_1_sgl ] ) is equivalent to @xmath273 + on the other hand , the right hand side of ( [ ineqn : estimate1_sgl ] ) can be rewritten as @xmath274 + in view of ( [ ineqn : n1_1_sgl ] ) , ( [ ineqn : n2_2_sgl ] ) and ( [ ineqn : n2_3_sgl ] ) , we can see that ( [ ineqn : estimate1_sgl ] ) holds if @xmath275 . indeed ,",
    "@xmath276 consider the first term on the right hand side of eq .",
    "( [ eqn : ivn_sgl ] ) . by the first half of ( [ ineqn : n1_r_sgl ] ) ,",
    "we have @xmath277 suppose that @xmath245 . by the second half of ( [ ineqn : n1_r_sgl ] ) , we can see that @xmath278 consider the second term on the right hand side of eq .",
    "( [ eqn : ivn_sgl ] ) .",
    "it is easy to see that @xmath279 combining ( [ ineqn : iyn_1_sgl ] ) , ( [ ineqn : iyn_2_sgl ] ) and eq .",
    "( [ eqn : ivn_2_sgl ] ) , we have @xmath275 , which completes the proof .    for notational convenience",
    ", we denote @xmath280 theorem [ thm : estimation_sgl ] shows that @xmath115 lies inside the ball of radius @xmath281 centered at @xmath282 .",
    "we solve the optimization problems in ( [ rrule1_sgl ] ) and ( [ rrule2_sgl ] ) . to simplify notations ,",
    "let @xmath283 theorem [ thm : estimation_sgl ] indicates that @xmath284 .",
    "moreover , we can see that @xmath285 , @xmath88 . to develop the tlfre rule by ( [ rrule1_sgl ] ) and ( [ rrule2_sgl ] )",
    ", we need to solve the following optimization problems : @xmath286      we consider the following equivalent problem of ( [ prob : supreme1_sgl ] ) : @xmath287 we can see that the objective function of problem ( [ prob : supreme11_sgl ] ) is _ continuously differentiable _ and the feasible set is a ball .",
    "thus , problem ( [ prob : supreme11_sgl ] ) is _ nonconvex _ because we need to _ maximize _ a convex function subject to a convex set .",
    "we first derive the necessary optimality conditions in lemma [ lemma : optimality_condition_supreme1_sgl ] and then deduce the closed form solutions of problems ( [ prob : supreme1_sgl ] ) and ( [ prob : supreme11_sgl ] ) in theorem [ thm : supreme1_sgl ] .    [",
    "lemma : optimality_condition_supreme1_sgl ] let @xmath288 be the set of optimal solutions of ( [ prob : supreme11_sgl ] ) and @xmath289 . then , the following hold : + suppose that @xmath290 is an interior point of @xmath291 .",
    "then , @xmath291 is a subset of @xmath54 .",
    "+ suppose that @xmath290 is a boundary point of @xmath291 .",
    "then , there exists @xmath292 such that @xmath293 suppose that there exists @xmath294 and @xmath295 .",
    "then , we have + @xmath296 and @xmath290 is a boundary point of @xmath291 , i.e. , @xmath297    the optimality condition in eq .",
    "( [ eqn : opt_condition_supreme1_sgl ] ) holds with @xmath298 .    to show lemma [ lemma : optimality_condition_supreme1_sgl ] , we need the following proposition .    [",
    "prop : optimal_condition_nonconvex ] suppose that @xmath299 and @xmath10 is a nonempty closed convex set . if @xmath300 is a local maximum of @xmath76 on @xmath10 , then @xmath301 .",
    "we now present the proof of lemma [ lemma : optimality_condition_supreme1_sgl ] .    to simplify notations ,",
    "let @xmath302 by eq .",
    "( [ def : shrinkage_sgl ] ) , we have @xmath303_i|-1)_+^2 .      \\end{aligned}\\ ] ] it is easy to see that @xmath304 is continuously differentiable .",
    "indeed , we have @xmath305 then , problem ( [ prob : supreme11_sgl ] ) can be written as @xmath306_i-1)_+^2:\\,\\xi_g\\in\\xi_g\\right\\ } ,      \\end{aligned}\\ ] ] where @xmath307 .",
    "then , proposition [ prop : optimal_condition_nonconvex ] results in @xmath308    1 .",
    "suppose that @xmath290 is an interior point of @xmath291 .",
    "then , we have @xmath309 . by eq .",
    "( [ eqn : general_opt_condition_supreme1_sgl ] ) , we can see that @xmath310 therefore , we have @xmath311 because @xmath312 ( see remark [ remark : shrinkage_feasible_sgl ] ) , eq .",
    "( [ eqn : xi_subset_c_supreme1_sgl ] ) implies that @xmath313 this completes the proof .",
    "2 .   suppose that @xmath290 is a boundary point of @xmath291 .",
    "we can see that @xmath314 then , eq .",
    "( [ eqn : opt_condition_supreme1_sgl ] ) follows by combining eq .",
    "( [ eqn : normal_cone_supreme1_sgl ] ) and the optimality condition in ( [ eqn : general_opt_condition_supreme1_sgl ] ) .",
    "3 .   suppose that there exists @xmath294 and @xmath295 .",
    "1 .   the definition of @xmath315 leads to @xmath316 moreover , we can see that @xmath290 is a boundary point of @xmath291 .",
    "because if @xmath290 is an interior point of @xmath291 , the first part implies that @xmath317 .",
    "this contradicts with the existence of @xmath315 .",
    "thus , @xmath290 must be a boundary point of @xmath291 , i.e. @xmath318 .",
    "because @xmath290 is a boundary point of @xmath291 , the second part implies that eq .",
    "( [ eqn : opt_condition_supreme1_sgl ] ) holds .",
    "moreover , from ( iiia ) , we know that @xmath296 . therefore , both sides of eq .",
    "( [ eqn : opt_condition_supreme1_sgl ] ) are nonzero and thus @xmath298 .",
    "this completes the proof .",
    "based on the necessary optimality conditions in lemma [ lemma : optimality_condition_supreme1_sgl ] , we derive the closed form solutions of ( [ prob : supreme1_sgl ] ) and ( [ prob : supreme11_sgl ] ) in the following theorem .",
    "the notations are the same as the ones in the proof of lemma [ lemma : optimality_condition_supreme1_sgl ] [ see eq .",
    "( [ eqn : c_r_supreme1_sgl ] ) and eq .",
    "( [ eqn : obj_fun_supreme1_sgl ] ) ] .",
    "[ thm : supreme1_sgl ] for problems ( [ prob : supreme1_sgl ] ) and ( [ prob : supreme11_sgl ] ) , let @xmath319 , @xmath320 and @xmath288 be the set of the optimal solutions .    1 .",
    "suppose that @xmath321 , i.e. , @xmath322 . let @xmath323 .",
    "then , @xmath324 2 .",
    "suppose that @xmath325 is a boundary point of @xmath54 , i.e. , @xmath326 .",
    "then , @xmath327 3 .",
    "suppose that @xmath328 , i.e. , @xmath329 . let @xmath330_i|=\\|\\mathbf{c}\\|_{\\infty}\\}$ ] .",
    "then , @xmath331_{i^*})\\mathbf{e}_{i^*}:\\,i^*\\in\\mathcal{i}^*\\right\\},\\hspace{1.5mm}\\textup{if}\\hspace{1mm}\\xi_g\\not\\subset\\mathcal{b}_{\\infty}\\,\\textup{and}\\,\\mathbf{c}\\neq0,\\\\          \\left\\{r\\cdot\\mathbf{e}_{i^ * } , -r\\cdot\\mathbf{e}_{i^*}:\\,i^*\\in\\mathcal{i}^*\\right\\},\\hspace{8.5mm}\\textup{if}\\hspace{1mm}\\xi_g\\not\\subset\\mathcal{b}_{\\infty}\\,\\textup{and}\\,\\mathbf{c}=0,\\\\          \\end{cases }          \\end{aligned}\\ ] ] where @xmath332 is the @xmath23 standard basis vector .    1 .",
    "suppose that @xmath321 . by the third part of lemma [ lemma : optimality_condition_supreme1_sgl ]",
    ", we have @xmath333 by eq .",
    "( [ eqn : opt_cond_case1_sgl ] ) , we can see that @xmath334 because otherwise we would have @xmath335 .",
    "moreover , we can only consider the cases with @xmath336 because @xmath337 and we aim to maximize @xmath338 .",
    "therefore , if we can find a solution with @xmath336 , there is no need to consider the cases with @xmath339 .",
    "+ suppose that @xmath336 . then , eq .  ( [ eqn : opt_cond_case1_sgl ] ) leads to @xmath340 in view of part ( iv ) of proposition [ prop : normal_cone ] and eq .",
    "( [ eqn : c_supreme1_sgl ] ) , we have @xmath341 therefore , eq .  ( [ eqn : xi_supreme1_sgl ] ) can be rewritten as @xmath342 combining eq .",
    "( [ eqn : opt_cond_case1_sgl ] ) and eq .",
    "( [ eqn : shrink_xi_c_supreme1_sgl ] ) , we have @xmath343 the statement holds by plugging eq .",
    "( [ eqn : mu_supreme1_sgl ] ) and eq .",
    "( [ eqn : projc_projxi_supreme1_sgl ] ) into eq .",
    "( [ eqn : xi_supreme1_sgl ] ) and eq .",
    "( [ eqn : shrink_xi_c_supreme1_sgl ] ) .",
    "moreover , the above discussion implies that @xmath288 only contains one element as shown in eq .",
    "( [ eqn : sol1_supreme1_sgl ] ) .",
    "2 .   suppose that @xmath325 is a boundary point of @xmath54 .",
    "then , we can find a point @xmath294 and @xmath295 . by the third part of lemma [ lemma : optimality_condition_supreme1_sgl ]",
    ", we also have eq .",
    "( [ eqn : opt_sol_bdy_sgl ] ) and eq .",
    "( [ eqn : opt_cond_case1_sgl ] ) hold .",
    "we claim that @xmath344 $ ] .",
    "the argument is as follows .",
    "+ suppose that @xmath336 . by the same argument as in the proof of the first part , we can see that eq .",
    "( [ eqn : shrink_xi_c_supreme1_sgl ] ) holds . because @xmath345 by eq .",
    "( [ eqn : opt_sol_bdy_sgl ] ) , we have @xmath346 .",
    "this implies that @xmath321 .",
    "thus , we have a contradiction , which implies that @xmath344 $ ] .",
    "+ let us consider the cases with @xmath347 . because @xmath337 [ see eq .",
    "( [ eqn : opt_cond_case1_sgl ] ) ] and we want to maximize @xmath338 , there is no need to consider the cases with @xmath339 if we can find solutions of problem ( [ prob : supreme1_sgl ] ) with @xmath347 .",
    "therefore , eq .  ( [ eqn : opt_cond_case1_sgl ] ) leads to @xmath348 by part ( iii ) of proposition [ prop : normal_cone ] , we can see that @xmath349 combining eq .",
    "( [ eqn : pxi_c_supreme1_sgl ] ) and eq .",
    "( [ eqn : opt_sol_bdy_sgl ] ) , the statement holds immediately , which confirms that @xmath347 .",
    "3 .   suppose that @xmath325 is an interior point of @xmath54",
    "we first consider the cases with @xmath317 .",
    "then , we can see that @xmath350 in other words , an arbitrary point of @xmath291 is an optimal solution of problem ( [ prob : supreme1_sgl ] ) .",
    "thus , we have @xmath351 on the other hand , we can see that @xmath352 therefore , we have @xmath353 and thus @xmath354 2 .",
    "suppose that @xmath355 , i.e. , there exists @xmath356 such that @xmath357 . by the third part of lemma [ lemma : optimality_condition_supreme1_sgl ] , we have eq .  ( [ eqn : opt_sol_bdy_sgl ] ) and eq .",
    "( [ eqn : opt_cond_case1_sgl ] ) hold .",
    "moreover , in view of the proof of the first and second part , we can see that @xmath339 .",
    "therefore , eq .  ( [ eqn : opt_cond_case1_sgl ] ) leads to @xmath358 by rearranging the terms of eq .",
    "( [ eqn : conv_comb_supreme1_sgl ] ) , we have @xmath359 because @xmath339 , eq .",
    "( [ eqn : conv_comb_supreme1_sgl ] ) implies that @xmath360 lies on the line segment connecting @xmath290 and @xmath325 .",
    "thus , we have @xmath361 therefore , to maximize @xmath362 , we need to minimize @xmath363 . because @xmath296 , we can see that @xmath360 is a boundary point of @xmath54 .",
    "therefore , we need to solve the following minimization problem : @xmath364 suppose that @xmath365 .",
    "we can see that the set of optimal solutions of problem ( [ prob : proj_faces_supreme1_sgl ] ) is @xmath366 for each @xmath367 , we set it as @xmath360 . in view of eq .",
    "( [ eqn : conv_comb2_supreme1_sgl ] ) and eq .",
    "( [ eqn : opt_sol_bdy_sgl ] ) , the statement follows immediately .",
    "+ suppose that @xmath368 .",
    "recall that @xmath369_{i^*}|=\\|\\mathbf{c}\\|_{\\infty}\\}$ ] .",
    "it is easy to see that @xmath370_k=              \\begin{cases }              \\textup{sgn}([\\mathbf{c}]_{i^*}),\\hspace{6mm}\\textup{if}\\hspace{1mm}k = i^*,\\\\              [ \\mathbf{c}]_{k},\\hspace{14mm}\\textup{otherwise } ,              \\end{cases }              i^*\\in\\mathcal{i}^ *              \\right\\}.              \\end{aligned}\\ ] ] we can see that @xmath371_{\\infty}|)\\textup{sgn}([\\mathbf{c}]_{i^*})\\mathbf{e}_{i^*},\\,i^*\\in\\mathcal{i}^*.              \\end{aligned}\\ ] ] for each @xmath372 , we set it to @xmath360 .",
    "then , we can see that the statement holds by eq .",
    "( [ eqn : conv_comb2_supreme1_sgl ] ) and eq .",
    "( [ eqn : opt_sol_bdy_sgl ] ) .",
    "this completes the proof .",
    "problem ( [ prob : supreme2_sgl ] ) can be solved directly via the cauchy - schwarz inequality .",
    "[ thm : supreme2_sgl ] for problem ( [ prob : supreme2_sgl ] ) , we have @xmath373 .    to simplify notations , let @xmath374 , @xmath375 and @xmath376 .",
    "therefore , the set @xmath154 in eq .",
    "( [ eqn : ball1_sgl ] ) can be written as @xmath377 then , problem ( [ prob : supreme2_sgl ] ) becomes @xmath378 we can see that @xmath379 thus , we have @xmath380 consider @xmath381 .",
    "it is easy to see that @xmath382 and @xmath383 therefore , we have @xmath384 which completes the proof .",
    "to develop the two - layer screening rules for sgl , we only need to plug the supreme values @xmath385 and @xmath386 in ( [ rrule1_sgl ] ) and ( [ rrule2_sgl ] ) .",
    "we present the tlfre rule as follows .",
    "[ thm : tlfre_sgl ] for the sgl problem in ( [ prob : sgl ] ) , suppose that we are given @xmath158 and a sequence of parameter values @xmath387 .",
    "moreover , assume that @xmath388 is known for an integer @xmath389 .",
    "let @xmath390 , @xmath391 and @xmath392 be given by eq .",
    "( [ eqn : kkt1_sgl ] ) , theorems [ thm : estimation_sgl ] and [ thm : supreme1_sgl ] , respectively . then",
    ", for @xmath88 , the following holds @xmath393 for the @xmath394 group that does not pass the rule in ( [ rule : l1 ] ) , we have @xmath395_i=0 $ ] if @xmath396    ( [ rule : l1 ] ) and ( [ rule : l2 ] ) are the first layer and second layer screening rules of tlfre , respectively .",
    "the framework of tlfre is applicable to a large class of sparse models with multiple regularizers . as an example",
    ", we extend tlfre to nonnegative lasso : @xmath397 where @xmath398 is the regularization parameter and @xmath399 is the nonnegative orthant of @xmath61 . in section [ ssec : fenchel_dual_nnlasso ] , we transform the constraint @xmath400 to a regularizer and derive the fenchel s dual of the nonnegative lasso problem .",
    "we then motivate the screening method  called dpc since the key step is to * * d**ecom**p**ose a * * c**onvex set via fenchel s duality theorem  via the kkt conditions in section [ ssec : general_rule_nnlasso ] . in section [ ssec : lambdamx_nnlasso ] , we analyze the geometric properties of the dual problem and derive the set of parameter values leading to zero solutions .",
    "we then develop the screening method for nonnegative lasso in section [ ssec : dpc_nnlasso ] .",
    "let @xmath401 be the indicator function of @xmath399 . by noting that @xmath402 for any @xmath398 , we can rewrite the nonnegative lasso problem in ( [ prob : nnlassoo ] ) as @xmath403 in other words , we incorporate the constraint @xmath400 to the objective function as an additional regularizer . as a result ,",
    "the nonnegative lasso problem in ( [ prob : nnlasso ] ) has two regularizers .",
    "thus , similar to sgl , we can derive the fenchel s dual of nonnegative lasso via theorem [ thm : fenchel_duality ] .",
    "we now proceed by following a similar procedure as the one in section [ subsection : fenchel_dual_sgl ] .",
    "we note that the nonnegative lasso problem in ( [ prob : nnlasso ] ) can also be formulated as the one in ( [ prob : general ] ) with @xmath404 and @xmath405 . to derive the fenchel s dual of nonnegative lasso",
    ", we need to find @xmath72 and @xmath73 by theorem [ thm : fenchel_duality ] .",
    "since we have already seen that @xmath406 in section [ subsection : fenchel_dual_sgl ] , we only need to find @xmath407 .",
    "the following result is indeed a counterpart of lemma [ lemma : conjugate_omega_sgl ] .",
    "[ lemma : conjugate_omega_nnlasso ] let @xmath85 , @xmath408 , and @xmath409 .",
    "then ,    1",
    ".   @xmath410 and @xmath411 , where @xmath412 is the nonpositive orthant of @xmath61 .",
    "@xmath413 , where @xmath414 .",
    "we omit the proof of lemma [ lemma : conjugate_omega_nnlasso ] since it is very similar to that of lemma [ lemma : conjugate_omega_sgl ] .",
    "consider the second part of lemma [ lemma : conjugate_omega_nnlasso ] .",
    "let @xmath415 , where  @xmath416 \" is defined component - wisely .",
    "we can see that @xmath417 on the other hand , lemma [ lemma : infconv_sets ] implies that @xmath418 thus , we have @xmath419 . the second part of lemma [ lemma : conjugate_omega_nnlasso ] decomposes each @xmath420 into two components : @xmath421 and @xmath422 that belong to @xmath54 and @xmath412 , respectively .    by theorem [ thm : fenchel_duality ] and lemma [ lemma : conjugate_omega_nnlasso ]",
    ", we can derive the fenchel s dual of nonnegative lasso in the following theorem ( which is indeed the counterpart of theorem [ thm : dual_sgl ] ) .",
    "[ thm : dual_nnlasso ] for the nonnegative lasso problem , the following hold :    1 .",
    "the fenchel s dual of nonnegative lasso is given by :",
    "@xmath423 2 .   let @xmath424 and @xmath425 be the optimal solutions of problems ( [ prob : nnlasso ] ) and ( [ prob : nnlasso_dual ] ) , respectively.then , @xmath426    we omit the proof of theorem [ thm : dual_nnlasso ] since it is very similar to that of theorem [ thm : dual_sgl ] .",
    "the key to develop the dpc rule for nonnegative lasso is the kkt condition in ( [ eqn : kkt2_nnlasso ] ) .",
    "we can see that @xmath427 and @xmath428_i= \\begin{cases } 0,\\hspace{13mm}\\mbox{if } [ \\mathbf{w}]_i>0,\\\\ \\rho,\\,\\rho\\leq0,\\hspace{2mm}\\mbox{if } [ \\mathbf{w}]_i=0,\\\\ \\end{cases } \\right\\}.\\end{aligned}\\ ] ] therefore , the kkt condition in ( [ eqn : kkt2_nnlasso ] ) implies that @xmath429_i>0,\\\\ \\varrho,\\,\\varrho\\leq1,\\hspace{2mm}\\mbox{if } [ \\beta^*(\\lambda)]_i=0 . \\end{cases}\\end{aligned}\\ ] ] by eq .",
    "( [ eqn : kkt3_nnlasso ] ) , we have the following rule : @xmath430_i=0.\\end{aligned}\\ ] ] because @xmath425 is unknown , we can apply ( [ r3 ] ) to identify the inactive features  which have @xmath14 coefficients in @xmath424 .",
    "similar to tlfre , we can first find a region @xmath154 that contains @xmath425 .",
    "then , we can relax ( [ r3 ] ) as follows : @xmath431_i=0.\\end{aligned}\\ ] ]    inspired by ( [ rrule3_nnlasso ] ) , we develop dpc via the following three steps :    1 .",
    "given @xmath157 , we estimate a region @xmath154 that contains @xmath425 .",
    "2 .   we solve the optimization problem @xmath432 .",
    "3 .   by plugging in @xmath433 computed from",
    "* step 2 * , ( [ rrule3_nnlasso ] ) leads to the desired screening method dpc for nonnegative lasso .      in view of the fenchel s dual of nonnegative lasso in ( [ prob : nnlasso_dual ] ) , we can see that the optimal solution is indeed the projection of @xmath434 onto the feasible set @xmath435 , i.e. , @xmath436 therefore , if @xmath437 , eq .",
    "( [ eqn : dual_proj_nnlasso ] ) implies that @xmath438 .",
    "if further @xmath161 is an interior point of @xmath439 , [ rrule3_nnlasso ] implies that @xmath440 .",
    "the next theorem gives the set of parameter values leading to @xmath14 solutions of nonnegative lasso .",
    "[ thm : lambdamx_nnlasso ] for the nonnegative lasso problem ( [ prob : nnlasso ] ) , let @xmath441 .",
    "then , the following statements are equivalent :    @xmath442 ,    @xmath443 ,    @xmath440 ,    @xmath444 .",
    "we omit the proof of theorem [ thm : lambdamx_nnlasso ] since it is very similar to that of theorem [ thm : lambda_alpha_sgl ] .",
    "we follow the three steps in section [ ssec : general_rule_nnlasso ] to develop the screening rule for nonnegative lasso .",
    "we first estimate a region that contains @xmath425 .",
    "because @xmath425 admits a closed form solution with @xmath444 by theorem [ thm : lambdamx_nnlasso ] , we focus on the cases with @xmath445 .",
    "[ thm : estimation_nnlasso ] for the nonnegative lasso problem , suppose that @xmath446 is known with @xmath447 .",
    "for any @xmath241 , we define @xmath448 then , the following hold :    1 .",
    "@xmath449 , 2 .",
    "@xmath450 .",
    "we only show that @xmath451 since the proof of the other statement is very similar to that of theorem [ thm : estimation_sgl ] .    by proposition",
    "[ prop : normal_cone ] and theorem [ thm : lambdamx_nnlasso ] , it suffices to show that @xmath452 because @xmath453 , we have @xmath454 .",
    "the definition of @xmath455 implies that @xmath456 .",
    "thus , the inequality in ( [ ineqn : xmx_nnlasso ] ) holds , which completes the proof .",
    "theorem [ thm : estimation_nnlasso ] implies that @xmath425 is in a ball  denoted by @xmath457of radius @xmath458 centered at @xmath459 .",
    "simple calculations lead to @xmath460 by plugging @xmath433 into ( [ rrule3_nnlasso ] ) , we have the dpc screening rule for nonnegative lasso as follows .",
    "for the nonnegative lasso problem , suppose that we are given a sequence of parameter values @xmath461 .",
    "then , @xmath462_i=0 $ ] if @xmath463 is known and the following holds : @xmath464",
    "we evaluate tlfre for sgl and dpc for nonnegative lasso in sections [ ssec : exp_sgl ] and [ ssec : exp_nnlasso ] , respectively , on both synthetic and real data sets . to the best of knowledge ,",
    "the tlfre and dpc are the first screening methods for sgl and nonnegative lasso , respectively .",
    "we perform experiments to evaluate tlfre on synthetic and real data sets in sections [ sssec : exp_syn_sgl ] and [ sssec : exp_adni_sgl ] , respectively . to measure the performance of tlfre ,",
    "we compute the _ rejection ratios _ of ( [ rule : l1 ] ) and ( [ rule : l2 ] ) , respectively .",
    "specifically , let @xmath465 be the number of features that have @xmath14 coefficients in the solution , @xmath466 be the index set of groups that are discarded by ( [ rule : l1 ] ) and @xmath467 be the number of inactive features that are detected by ( [ rule : l2 ] ) .",
    "the rejection ratios of ( [ rule : l1 ] ) and ( [ rule : l2 ] ) are defined by @xmath468 and @xmath469 , respectively .",
    "moreover , we report the _ speedup _ gained by tlfre , i.e. , the ratio of the running time of solver without screening to the running time of solver with tlfre .",
    "the solver used in this paper is from slep @xcite .    to determine appropriate values of @xmath158 and @xmath157 by cross validation or stability selection",
    ", we can run tlfre with as many parameter values as we need .",
    "given a data set , for illustrative purposes only , we select seven values of @xmath158 from @xmath470 .",
    "then , for each value of @xmath158 , we run tlfre along a sequence of @xmath471 values of @xmath157 equally spaced on the logarithmic scale of @xmath472 from @xmath473 to @xmath474 .",
    "thus , @xmath475 pairs of parameter values of @xmath55 are sampled in total .",
    "we perform experiments on two synthetic data sets that are commonly used in the literature @xcite .",
    "the true model is @xmath476 , @xmath477 .",
    "we generate two data sets with @xmath478 entries : synthetic 1 and synthetic 2 .",
    "we randomly break the @xmath479 features into @xmath480 groups . for synthetic 1 , the entries of the data matrix @xmath481 are i.i.d .",
    "standard gaussian with pairwise correlation zero , i.e. , @xmath482 . for synthetic 2 ,",
    "the entries of the data matrix @xmath481 are drawn from i.i.d .",
    "standard gaussian with pairwise correlation @xmath483 , i.e. , @xmath484 .",
    "to construct @xmath485 , we first randomly select @xmath486 percent of groups . then , for each selected group , we randomly select @xmath487 percent of features .",
    "the selected components of @xmath485 are populated from a standard gaussian and the remaining ones are set to @xmath14 .",
    "we set @xmath488 for synthetic 1 and @xmath489 for synthetic 2 .",
    "+    [ fig : synthetic1 ]    the figures in the upper left corner of fig .",
    "[ fig : synthetic1 ] and fig .",
    "[ fig : synthetic2 ] show the plots of @xmath490 ( see corollary [ cor : lambdamx_sgl ] ) and the sampled parameter values of @xmath157 and @xmath158 ( recall that @xmath41 and @xmath42 ) . for the other figures , the blue and red regions represent the rejection ratios of ( [ rule : l1 ] ) and ( [ rule : l2 ] ) , respectively .",
    "we can see that tlfre is very effective in discarding inactive groups / features ; that is , more than @xmath491 of inactive features can be detected . moreover",
    ", we can observe that the first layer screening ( [ rule : l1 ] ) becomes more effective with a larger @xmath158 . intuitively , this is because the group lasso penalty plays a more important role in enforcing the sparsity with a larger value of @xmath158 ( recall that @xmath41 ) .",
    "the top and middle parts of table [ table : tlfre_runtime_sync ] indicate that the speedup gained by tlfre is very significant ( up to @xmath492 times ) and tlfre is very efficient .",
    "compared to the running time of the solver without screening , the running time of tlfre is negligible .",
    "the running time of tlfre includes that of computing @xmath493 , @xmath88 , which can be efficiently computed by the power method @xcite . indeed , this can be shared for tlfre with different parameter values .     +    [ fig : synthetic2 ]     l c|c|c|c|c|c|c|c| & & @xmath494 & @xmath495 & @xmath496 & @xmath497 & @xmath498 & @xmath499 & @xmath500 +   + [ -2.5ex ] & & 298.36 & 301.74 & 308.69 & 307.71 & 311.33 & 307.53 & 291.24 + & & 0.77 & 0.78 & 0.79 & 0.79 & 0.81 & 0.79 & 0.77 + & & 10.26 & 12.47 & 15.73 & 17.69 & 19.71 & 21.95 & 22.53 + & & * 29.09 * & * 24.19 * & * 19.63 * & * 17.40 * & & * 14.01 * & * 12.93 * + & & 294.64 & 294.92 & 297.29 & 297.50 & 297.59 & 295.51 & 292.24 + & & 0.79 & 0.80 & 0.80 & 0.81 & 0.81 & 0.81 & 0.82 + & & 11.05 & 12.89 & 16.08 & 18.90 & 20.45 & 21.58 & 22.80 + & & * 26.66 * & * 22.88 * & * 18.49 * & * 15.74 * & * 14.55 * & * 13.69 * & * 12.82 * +       +    [ fig : adni_gmv ]     +     l c|c|c|c|c|c|c|c| & & @xmath494 & @xmath495 & @xmath496 & @xmath497 & @xmath498 & @xmath499 & @xmath500 +   + [ -2.5ex ] & & 30652.56 & 30755.63 & 30838.29 & 31096.10 & 30850.78 & 30728.27 & 30572.35 + & & 64.08 & 64.56 & 64.96 & 65.00 & 64.89 & 65.17 & 65.05 + & & 372.04 & 383.17 & 386.80 & 402.72 & 391.63 & 385.98 & 382.62 + & & * 82.39 * & * 80.27 * & * 79.73 * & * 77.22 * & * 78.78 * & * 79.61 * & * 79.90 * + & & 29751.27 & 29823.15 & 29927.52 & 30078.62 & 30115.89 & 29927.58 & 29896.77 + & & 62.91 & 63.33 & 63.39 & 63.99 & 64.13 & 64.31 & 64.36 + & & 363.43 & 364.78 & 386.15 & 393.03 & 395.87 & 400.11 & 399.48 + & & * 81.86 * & * 81.76 * & * 77.50 * & * 76.53 * & * 76.08 * & * 74.80 * & * 74.84",
    "* +    we perform experiments on the alzheimer s disease neuroimaging initiative ( adni ) data set ( http://adni.loni.usc.edu/ ) .",
    "the data matrix consists of @xmath501 samples with @xmath502 nucleotide polymorphisms ( snps ) , which are divided into @xmath503 groups .",
    "the response vectors are the grey matter volume ( gmv ) and white matter volume ( wmv ) , respectively .    the figures in the upper left corner of fig .",
    "[ fig : adni_gmv ] and fig .",
    "[ fig : adni_wmv ] show the plots of @xmath490 ( see corollary [ cor : lambdamx_sgl ] ) and the sampled parameter values of @xmath158 and @xmath157 .",
    "the other figures present the rejection ratios of ( [ rule : l1 ] ) and ( [ rule : l2 ] ) by blue and red regions , respectively .",
    "we can see that almost all of the inactive groups / features are discarded by tlfre .",
    "the rejection ratios of @xmath504 are very close to @xmath473 in all cases .",
    "table [ table : tlfre_runtime_real ] shows that tlfre leads to a very significant speedup ( about @xmath505 times ) . in other words , the solver without screening needs about eight and a half hours to solve the @xmath471 sgl problems for each value of @xmath158 .",
    "however , combined with tlfre , the solver needs only six to eight minutes .",
    "moreover , we can observe that the computational cost of tlfre is negligible compared to that of the solver without screening .",
    "this demonstrates the efficiency of tlfre .      in this experiment",
    ", we evaluate the performance of dpc on two synthetic data sets and six real data sets .",
    "we integrate dpc with the solver @xcite to solve the nonnegative lasso problem along a sequence of @xmath471 parameter values of @xmath157 equally spaced on the logarithmic scale of @xmath506 from @xmath507 to @xmath474 .",
    "the two synthetic data sets are the same as the ones we used in section [ sssec : exp_syn_sgl ] .",
    "to construct @xmath485 , we first randomly select @xmath508 percent of features .",
    "the corresponding components of @xmath485 are populated from a standard gaussian and the remaining ones are set to 0 .",
    "we list the six real data sets and the corresponding experimental settings as follows .    1 .",
    "* breast cancer data set * @xcite : this data set contains @xmath509 gene expression values of @xmath510 tumor samples ( thus the data matrix @xmath511 is of @xmath512 ) .",
    "the response vector @xmath513 contains the binary label of each sample .",
    "2 .   * leukemia data set * @xcite : this data set contains @xmath514 gene expression values of @xmath515 samples ( @xmath516 ) .",
    "the response vector @xmath517 contains the binary label of each sample .",
    "3 .   * prostate cancer data set * @xcite : this data set contains @xmath518 measurements of 132 patients ( @xmath519 ) . by protein",
    "mass spectrometry , the features are indexed by time - of - flight values , which are related to the mass over charge ratios of the constituent proteins in the blood .",
    "the response vector @xmath517 contains the binary label of each sample .",
    "4 .   * pie face image data set",
    "* @xcite : this data set contains @xmath520 gray face images ( each has @xmath521 pixels ) of @xmath522 people , taken under different poses , illumination conditions and expressions . in each trial",
    ", we first randomly pick an image as the response @xmath523 , and then use the remaining images to form the data matrix @xmath524 .",
    "we run @xmath471 trials and report the average performance of dpc .",
    "* mnist handwritten digit data set * @xcite : this data set contains grey images of scanned handwritten digits ( each has @xmath525 pixels ) .",
    "the training and test sets contain @xmath526 and @xmath527 images , respectively .",
    "we first randomly select @xmath528 images for each digit from the training set and get a data matrix @xmath529 .",
    "then , in each trial , we randomly select an image from the testing set as the response @xmath530 .",
    "we run @xmath471 trials and report the average performance of the screening rules .",
    "* street view house number ( svhn ) data set * @xcite : this data set contains color images of street view house numbers ( each has @xmath521 pixels ) , including @xmath531 images for training and @xmath532 for testing . in each trial , we first randomly select an image as the response @xmath533 , and then use the remaining ones to form the data matrix @xmath534 .",
    "we run @xmath535 trials and report the average performance .",
    "we present the _",
    "rejection ratios_the ratio of the number of inactive features identified by dpc to the actual number of inactive features  in fig .",
    "[ fig : dpc_rej_ratio ] .",
    "we also report the running time of the solver with and without dpc , the time for running dpc , and the corresponding _ speedup _ in table [ table : dpc_runtime ] .",
    "[ fig : dpc_rej_ratio ] shows that dpc is very effective in identifying the inactive features even for small parameter values : the rejection ratios are very close to @xmath536 for the entire sequence of parameter values on the eight data sets .",
    "table [ table : dpc_runtime ] shows that dpc leads to a very significant speedup on all the data sets .",
    "take mnist as an example .",
    "the solver without dpc takes @xmath537 minutes to solve the @xmath471 nonnegative lasso problems .",
    "however , combined with dpc , the solver only needs @xmath508 seconds .",
    "the speedup gained by dpc on the mnist data set is thus more than @xmath538 times .",
    "similarly , on the svhn data set , the running time for solving the @xmath471 nonnegative lasso problems by the solver without dpc is close to seven hours .",
    "however , combined with dpc , the solver takes less than two minutes to solve all the @xmath471 nonnegative lasso problems , leading to a speedup about @xmath539 times .",
    "moreover , we can also observe that the computational cost of dpc is very low  which is negligible compared to that of the solver without dpc .",
    "+    [ fig : dpc_rej_ratio ]     l c|c|c|c|c|c|c|c| & & synthetic 2 & breast cancer & leukemia & prostate cancer & pie & mnist & svhn +   + [ -2.5ex ] & 218.37 & 204.06 & 23.40 & 34.04 & 187.82 & 674.04 & 3000.69 & 24761.07 + & 0.31 & 0.29 & 0.03 & 0.06 & 0.23 & 1.16 & 3.53 & 30.59 + & 5.52 & 6.10 & 2.18 & 3.37 & 6.37 & 5.01 & 9.31 & 104.93 + & * 39.56 * & * 33.45 * & * 10.73 * & * 10.10 * & * 29.49 * & * 134.54 * & * 322.31 * & * 235.98 * +",
    "in this paper , we propose a novel feature reduction method for sgl via decomposition of convex sets .",
    "we also derive the set of parameter values that lead to zero solutions of sgl . to the best of our knowledge , tlfre is the first method which is applicable to sparse models with multiple sparsity - inducing regularizers .",
    "more importantly , the proposed approach provides novel framework for developing screening methods for complex sparse models with multiple sparsity - inducing regularizers , e.g. , @xmath0 svm that performs both sample and feature selection , fused lasso and tree lasso with more than two regularizers . to demonstrate the flexibility of the proposed framework",
    ", we develop the dpc screening rule for the nonnegative lasso problem .",
    "experiments on both synthetic and real data sets demonstrate the effectiveness and efficiency of tlfre and dpc .",
    "we plan to generalize the idea of tlfre to @xmath0 svm , fused lasso and tree lasso , which are expected to consist of multiple layers of screening .",
    "by introducing an auxiliary variable @xmath540 the sgl problem in ( [ prob : sgl ] ) becomes : @xmath541 let @xmath542 be the lagrangian multiplier , the lagrangian function is @xmath543 let @xmath544 to derive the dual problem , we need to minimize the lagrangian function with respect to @xmath545 and @xmath186 . in other words , we need to minimize @xmath546 and @xmath547 , respectively .",
    "we first consider @xmath548 by the fermat s rule , we have @xmath549 which leads to @xmath550 by noting that @xmath551 we have @xmath552 thus , we can see that @xmath553 moreover , because @xmath554 , eq .",
    "( [ eqn : opt_cond_minf1_sgl ] ) implies that @xmath555      in view of eq .",
    "( [ eqn : lagrangian_sgl ] ) , eq .",
    "( [ eqn : minf1_sgl ] ) , eq .",
    "( [ eqn : minf2_sgl ] ) and eq .",
    "( [ eqn : dual_feasible_lagrangian_sgl ] ) , the dual problem of sgl can be written as @xmath558 which is equivalent to ( [ prob : dual_sgl_lagrangian ] ) .",
    "recall that @xmath114 and @xmath115 are the primal and dual optimal solutions of sgl , respectively . by eq .",
    "( [ eqn : auxiliary_sgl ] ) , eq .",
    "( [ eqn : opt_cond0_minf1_sgl ] ) and eq .",
    "( [ eqn : opt_cond_minf2_sgl ] ) , we can see that the kkt conditions are @xmath559"
  ],
  "abstract_text": [
    "<S> sparse - group lasso ( sgl ) has been shown to be a powerful regression technique for simultaneously discovering group and within - group sparse patterns by using a combination of the @xmath0 and @xmath1 norms . </S>",
    "<S> however , in large - scale applications , the complexity of the regularizers entails great computational challenges . in this </S>",
    "<S> , we propose a novel * * t**wo-**l**ayer * * f**eature * * re**duction method ( tlfre ) for sgl via a decomposition of its dual feasible set . </S>",
    "<S> the -layer reduction is able to identify the inactive groups and the inactive features , respectively , which are guaranteed to be absent from the sparse representation and can be removed from the optimization . </S>",
    "<S> existing feature reduction methods are only applicable for sparse models with one sparsity - inducing regularizer . to our best knowledge </S>",
    "<S> , tlfre is _ the first one _ that is capable of dealing with _ multiple _ sparsity - inducing . </S>",
    "<S> moreover , tlfre has a very low computational cost and can be integrated with any existing solvers . </S>",
    "<S> we also develop a screening method  called dpc ( * * d**ecom**p**osition of * * c**onvex set)for the nonnegative lasso problem . </S>",
    "<S> experiments on both synthetic and real data sets show that tlfre and dpc improve the efficiency of sgl and nonnegative lasso by several orders of magnitude . </S>"
  ]
}