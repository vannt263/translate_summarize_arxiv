{
  "article_text": [
    "all astronomers recognize that spectroscopy offers a wealth of information that can help characterize the properties of the observing target . in the context of stellar astrophysics",
    ", spectroscopy plays many fundamental roles .",
    "the relative strengths and widths of stellar absorption lines provide access to physical properties like effective temperature ( @xmath2 ) and surface gravity ( @xmath3 ) , enabling model comparisons in the hertzsprung - russell diagram to estimate the masses and ages so crucial to understanding stellar evolution , as well as individual elemental abundances or the collective  metallicity \" ( typically parameterized as @xmath4}$ ] ) , facilitating studies of the chemical hallmarks of different stellar populations . with sufficient resolution",
    ", a spectrum also conveys information about rotation ( @xmath5 ) and kinematics ( e.g. , association with a cluster or companion through the radial velocity , @xmath6 ) .",
    "while many fields benefit from such spectroscopic measurements , they are of acute interest to the exoplanet community .",
    "there , all estimates of the planet properties are made _ relative _ to the host properties ( e.g. , the mass function and planet - to - host radius _ ratio _ are constrained with the radial velocity or transit techniques , respectively ) . moreover ,",
    "essential clues to the planet formation process are encapsulated in the dependences of planet frequency on host mass ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) and metallicity ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) .    the robust and quantitative extraction of physical ( or empirical ) parameters from an observed spectrum can be an extraordinary challenge .",
    "stellar models serve as comparative benchmarks to associate observed spectral features with the parameters of interest . generating a synthetic model spectrum involves a complex numerical treatment of the stellar structure and radiative transfer through the atmosphere ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "detailed models calibrated to individual stars are important , but rare ( e.g. , the sun , vega ) ; therefore , these stellar models are relatively untested in large swaths of parameter - space . moreover , they necessarily include simplifications to treat complicated physical processes ( e.g. , convection ) or computational limitations ( e.g. , boundary conditions ) , and often must rely on incomplete or inaccurate atomic and molecular information ( e.g. , opacities ) . in principle , the models could be improved with appropriate reference to spectroscopic data . nevertheless , they are remarkably successful in reproducing many diagnostic spectral features .",
    "there are various well - tested approaches being used in stellar astrophysics to compare these models with observed spectra and thereby infer basic parameters .",
    "perhaps the most common is a straightforward empirical technique that relies on distilling an information - rich subset of the data , usually in the form of spectral line equivalent widths and/or local continuum shapes",
    ". a combined sequence of the ratios of these quantities can be especially sensitive to a given model parameter ( e.g. , moog ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "* ) . this ",
    "indexing \" approach has the advantage of being trivially fast .",
    "but , each condensed relationship is only informative over a limited swath of parameter - space , and it potentially masks degeneracies that are encoded in the spectral line shapes .",
    "another standard approach exploits the cross - correlation of an observed spectrum with a suite of model templates to optimize a set of parameters , usually with some weighting applied to specific spectral regions ( e.g. , spc ; * ? ? ?",
    "* ) . in this case",
    ", the speed advantage is maintained ( perhaps enhanced ) and more data content is used ( particularly in the spectral dimension ) , thereby achieving higher precision even for data with comparatively low signal - to - noise .",
    "the disadvantage is that the model quality and parameter inferences are assessed in a heuristic ( rather than probabilistic ) sense , making it difficult to quantify uncertainty in the stellar parameters .",
    "a more direct method employs a pixel - by - pixel comparison between model and data .",
    "this has the benefits of increased parametric flexibility ( e.g. , one can fit for arbitrary abundances or structures ) and a proper inference framework ( usually a least - squares approach , although increasingly in a bayesian format ; * ? ? ?",
    "* ; * ? ? ?",
    "ultimately , rather than pre - computing a library of sythetic spectra , one would like to incorporate the spectral synthesis back - end ( e.g. , sme ; @xcite ) directly into the likelihood function , bypassing any interpolation when assessing the fit of stellar parameters in - between grid points in the library .",
    "unfortunately , this is not yet computationally feasible beyond a limited wavelength range .    in this article",
    ", we construct a flexible forward - modeling method for the general spectroscopic inference problem in a bayesian framework , building on the best aspects of the latter two approaches highlighted above .",
    "the key developments in this design include a spectral emulator to address the difficult task of interpolation in coarsely sampled synthetic spectral libraries and a non - trivial covariance matrix parameterized by both global ( stationary ) and local ( non - stationary ) gaussian process kernels .",
    "when combined with an appropriately sophisticated set of quantitative metrics for the relevant physical parameters , this method will efficiently propagate systematic uncertainties into the parameter inferences .",
    "ultimately , this approach could be employed to leverage spectroscopic data as a reference for improving the models .",
    "a complete overview of the methodology behind this approach is provided in section [ sec : method ] . some tests and example applications ( for a high resolution optical spectrum of an f star , and a medium - resolution near - infrared spectrum of a mid - m star )",
    "are described in section [ sec : examples ] .",
    "finally , a discussion of its potential utility , especially the possibility of extending it to develop data - driven spectral models , is provided in section [ sec : discussion ] .",
    "this section describes a generative bayesian modeling framework that confronts some of the key technical obstacles in the spectroscopic inference problem . the goal is to conservatively extract the maximal amount of information about a prescribed ( and usually degenerate ) parameter set by forward - modeling an observed spectrum , while also recognizing and explicitly accounting for the covariances ( and potentially biases ) introduced by pathologically imperfect models .",
    "the method is modular , and therefore can easily incorporate additional physical or nuisance parameters as desired without sacrificing an accurate reflection of the limitations in the data .",
    "the specific applications discussed here are related to the spectra of individual stars , but the methodology is generic ( and could be used for the composite spectra of unresolved stellar clusters , galaxies , etc . ) .",
    "figure  [ fig : flowchart ] serves as a graphical guide to the mechanics of this modeling framework , and the remainder of this section .",
    "first , a model spectrum is generated for a given set of physical parameters ( section  [ subsec : synthetic ] ; appendix  [ sec : appendix ] ) , and then post - processed to mimic reality using a set of observational and practical nuisance parameters ( section  [ subsec : postprocess ] ) .",
    "next , a direct , pixel - by - pixel comparison between the data and model spectra is made with a prescribed likelihood function and a parametric treatment of the covariances between pixel residuals ( section  [ subsec : likelihood ] ) .",
    "that process is iterated using markov chain monte carlo ( mcmc ) simulations in a multi - stage gibbs sampler to numerically explore the posterior probability density of the model conditioned on the data , and thereby to determine constraints on the parameters of interest ( section  [ subsec : mcmc ] ) . along the way",
    ", these procedures are illustrated with observations of the high resolution optical spectrum from a nearby f star .",
    "that specific application , along with some alternative demonstrations of the method , are discussed in more detail in section  [ sec : examples ] .",
    "there are many approaches for generating a model spectrum , @xmath7 , for a specific set of parameters , @xmath8}\\}$ ] . in the most direct case of spectral synthesis ,",
    "a model atmosphere structure is assembled and simulations of energy transport through it are conducted with a radiative transfer code ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) . however , in general this approach is often computationally prohibitive for most iterative methods of probabilistic inference .",
    "one partial compromise is to interpolate over a library of atmosphere structures that were pre - computed for a discrete set of parameter values , @xmath9 , for some arbitrary @xmath10 .",
    "then , perform a radiative transfer calculation with that interpolated atmosphere to synthesize @xmath11 ( e.g. , sme ; * ? ? ?",
    "a more common variant is to interpolate over a pre - synthesized library of model spectra , @xmath12 ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "although the former approach is preferable , the computational cost of repeated spectral synthesis is enough to make a detailed exploration of parameter space less appealing ( although see section  [ sec : discussion ] ) .",
    "although the framework we are advocating is applicable for _ any _  back - end \" that generates a model spectrum , it is illustrated here using the latter approach with the @xcite phoenix library .    in practice , this reliance on spectral interpolation within a model library requires a sophisticated treatment of associated uncertainties .",
    "the key problems are that the spectra themselves do not vary in a straightforward way as a function of @xmath10 ( especially within spectral lines ) , and that the typical model library is only sparsely sampled in @xmath10 . because of these issues , standard interpolation methods necessarily result in some information loss .",
    "the practical consequence is that the inferred posteriors on the model parameters are often sharply peaked near a grid point in the library , @xmath9 , potentially biasing the results and artificially shrinking the inferred parameter uncertainties ( e.g. , @xcite ) . to mitigate these effects ,",
    "we develop a spectral  emulator \" that smoothly interpolates in a sparse model library and records a covariance term to be used in the likelihood calculation that accounts for the associated uncertainties .",
    "the emulator is described in detail in appendix  [ sec : appendix ] .",
    "we first decompose the model library into a representative set of eigenspectra using a principal component analysis . at each gridpoint in the library",
    ", the corresponding spectrum can be reconstructed with a linear combination of these eigenspectra .",
    "the weights associated with each eigenspectrum contribution vary smoothly as a function of the parameters , and so are used to train a gaussian process to interpolate the weights associated with any arbitrary @xmath10 . in this way , the emulator delivers a probability distribution that represents the range of possible interpolated spectra . by then marginalizing over this distribution",
    ", we can modify the likelihood function to propagate the associated interpolation uncertainty . in the remainder of this section , the details of generating the reconstructed ( interpolated ) spectrum are not especially relevant ( see appendix  [ sec : appendix ] ) .",
    "typically , the  raw \" interpolated model spectrum @xmath13 that was generated above is highly over - sampled , and does not account for several additional observational and instrumental effects that become important in comparisons with real data .",
    "therefore , a certain amount of post - processing is required before assessing the model quality .",
    "we treat that post - processing in two stages . the first stage deals with an additional set of  extrinsic \" parameters , @xmath14 , that incorporate some dynamical considerations as well as observational effects related to geometry and the relative location of the target .",
    "the second stage employs a suite of nuisance parameters , @xmath15 , designed to forward model some imperfections in the data calibration .",
    "we can further divide @xmath14 into those parameters that impact the model primarily in the spectral or flux dimensions . for the former ,",
    "we consider three kernels that contribute to the line - of - sight velocity distribution function .",
    "the first , @xmath16 , treats the instrumental spectral broadening . for illustrative purposes , we assume @xmath16 is a gaussian with a mean of zero and a constant width @xmath17 at all @xmath18 , although more sophisticated forms could be adopted .",
    "the second , @xmath19 , characterizes the broadening induced by stellar rotation , parameterized by @xmath20 as described by ( * ? ? ?",
    "* his eq .  18.14 ) , the rotation velocity at the stellar equator projected on the line of sight ( where @xmath21 is the inclination of the stellar rotation axis ) . and",
    "the third , @xmath22 , incorporates the radial velocity through a doppler shift .",
    "the model spectrum is modified by the parameters @xmath23 $ ] through these kernels , using a convolution in velocity - space , @xmath24 and then re - sampled onto the discrete wavelengths corresponding to each data pixel , @xmath25 where the @xmath26 symbol denotes a re - sampling operator that maps the model spectrum onto the @xmath27-element model vector @xmath28 ( @xmath27 is the number of pixels in the spectrum ) .",
    "figure [ fig : broadening ] shows a ( condensed ) graphical representation of these post - processing steps .        at this stage ,",
    "the model is further modified in the flux dimension .",
    "a typical synthetic spectrum is computed as the flux that would be measured _ at the stellar surface _ , and so needs to be diluted by the subtended solid angle , @xmath29 , where @xmath30 is the stellar radius and @xmath31 is the distance .",
    "an additional wavelength - dependent scaling factor is applied to account for interstellar extinction , assuming some previously - derived extinction law @xmath32 ( e.g. , * ? ? ?",
    "* ) that is parameterized by @xmath33 .",
    "the parameters @xmath34 $ ] are then applied as @xmath35 with simplified notation such that @xmath36 $ ] , where @xmath37 $ ] .",
    "some spectral libraries provide spectra as with peak fluxes normalized to a constant value , in that case , @xmath38 will simply serve as an arbitrary scaling parameter .",
    "the procedure so far is composed of straightforward operations demanded by practical astronomical and computing issues . if the data were _ perfectly _ calibrated , we could proceed to a likelihood calculation that makes a direct comparison with @xmath39 .",
    "however , the calibration of the continuum shape for data with reasonably large spectral range is often not good enough to do this .",
    "a common example of this imperfect calibration can be readily seen when comparing the overlaps between spectral orders from echelle observations .",
    "even if such imperfections ( e.g. , in the flat field or blaze corrections , or perhaps more likely in the flux calibration process ) induce only minor , low - level deviations in the continuum shape , they can add up to a significant contribution in the likelihood function and thereby potentially bias the results .    the traditional approach to dealing with this issue has been avoidance ; a low - order polynomial or spline function is matched ( separately ) to the model and the data and then divided off to normalize the spectra . while this is straightforward to do for earlier type stars , it only masks the problem .",
    "this normalization procedure disposes of _ useful _ physical information content available in the continuum shape , and can be considerably uncertain in cases where the spectral line density is high ( e.g. , for cooler stellar photospheres ) .",
    "moreover , it can not propagate the uncertainty inherent in deriving the normalization functions into a proper inference framework .        instead",
    ", we employ a more rigorous approach that forward - models the calibration imperfections with a set of nuisance parameters that modify the shape of the model spectrum . by later marginalizing over these nuisance parameters ,",
    "we properly account for any uncertainties that these kinds of calibration imperfections induce on the stellar parameters of interest while also maintaining the useful information in the continuum shape . in practice , this is achieved by distorting segments of the model with polynomials , @xmath40 ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "figure [ fig : chebyshev ] demonstrates how these nuisance parameters are applied to the model . for @xmath41 spectral orders ,",
    "each denoted with index @xmath42 , the model spectrum can be decomposed as @xmath43 where @xmath44 is an @xmath45 degree chebyshev function .",
    "the @xmath46 coefficients are considered a set of nuisance parameters , @xmath47 $ ] .",
    "judicious priors can ensure that the real spectral features ( e.g. , molecular bands ) are not treated as residual calibration artifacts . the lowest - degree ( scaling )",
    "coefficient , @xmath48 , is degenerate with the solid angle , @xmath38 .",
    "therefore , we enforce an additional constraint that the mean of the polynomial is unity . for data with a single spectral order ,",
    "this means simply setting @xmath49 . in the multiple order case ,",
    "we assign @xmath49 in an arbitrary order as an anchor , but permit the @xmath48 in other orders to be different .",
    "the fit of the model spectrum is assessed by comparing to the data with a pixel - by - pixel likelihood calculation .",
    "if we denote the data spectrum as @xmath50 , then a corresponding residual spectrum ( an @xmath27-element vector ) can be defined for any input parameter set , @xmath51 to quantify the probability of the data conditioned on the model , we adopt a standard multi - dimensional gaussian likelihood function @xmath52^{1/2 } } \\exp\\left ( -\\frac{1}{2 }     { \\mathsf{r}}^{\\mathsf{t}}{\\mathsf{c}}^{-1 } { \\mathsf{r}}\\right )     \\label{eqn : likelihood}\\ ] ] that penalizes models which yield larger residuals and explicitly allows for covariances in the residual spectrum through the @xmath53 matrix @xmath54 . for practical reasons ,",
    "the log - likelihood is used as the quality metric , where @xmath55    the covariance matrix @xmath54 characterizes both the measurement uncertainty ( @xmath56 ;  noise \" ) in each pixel and the covariance between pixels .",
    "when using a spectral emulator to interpolate model spectra , @xmath54 will be the sum of the covariance matrix described here and the emulator matrix derived in appendix  [ sec : appendix ] ( eq .",
    "[ eqn : modc ] ) . in the special case where each pixel is independent , the covariance matrix is diagonal , @xmath57 , where @xmath58 is the uncertainty in pixel @xmath21 and @xmath59 is the kronecker delta function , and eq .",
    "[ eqn : lnlikelihood ] reduces to the familiar @xmath60 the sum of the square of the residuals weighted by their inverse variances .",
    "however , that simplification rarely applies in practice .",
    "a more complex covariance matrix is required , so that additional off - diagonal terms can be used to explicitly characterize ( 1 ) pixel - to - pixel covariances imposed by the discrete over - sampling of the line - spread function , and ( 2 ) highly correlated residuals as manifestations of systematic imperfections in the model library .",
    "the following sections describe how these issues are addressed by constructing a more sophisticated @xmath54 .",
    "astronomical spectrographs are designed to have the detector over - sample the instrumental line - spread function with at least a few pixels .",
    "therefore , adjacent pixels never record independent samples of the true spectrum . in that case , a difference between an observed and modeled spectral feature creates a correlated residual that spans multiple pixels .",
    "this can be demonstrated clearly in the autocorrelation of @xmath61 : a slight model mismatch will produce correlated residuals over a characteristic scale similar to the instrumental or rotation broadening kernel width ( whichever is larger ) .",
    "figure [ fig : class0 ] shows an example of these correlated residuals in real data ; a significant autocorrelation signal is seen on an @xmath628 pixel scale , corresponding to the 6.8 km s@xmath63 fwhm of @xmath16 .",
    "it is important to distinguish here between  noise \" and the fit residuals .",
    "noise introduced to the spectrograph by astrophysical or instrumental effects is generally uncorrelated with wavelength .",
    "the arrival and propagation of each photon through the instrument and into the detector can be considered an independent event .",
    "in essence , the noise itself is not correlated , but the fit residuals likely are . however , from a mathematical perspective the correlated residuals can be treated in the same way as correlated noise , by constructing a non - trivial covariance matrix with off - diagonal terms . in practice , this is achieved by parameterizing @xmath54 with a kernel that describes the covariance between any pair of pixels , indexed @xmath64 , representing wavelengths @xmath65 and @xmath66 .    for a well - designed spectrograph and sufficiently accurate model , this _ global _",
    "( i.e. , present throughout the spectrum ) covariance should have a relatively low amplitude and small correlation length . to describe that structure , we use a stationary covariance kernel ( or radial basis function ) with an amplitude that depends only on the velocity separation between two pixels , @xmath67 where @xmath68 is the speed of light .",
    "this kernel is used to characterize the covariance between pixel residuals , @xmath69 a variety of useful kernels have been developed in the field of gaussian processes to parameterize such a covariant structure ( e.g. , * ? ? ?",
    "* ; * ? ? ?",
    "* ) , and are seeing increased use in many areas of astrophysics ( for some specific examples in stellar and planetary applications , see * ? ? ?",
    "* ; * ? ? ?",
    "* ; * ? ? ?",
    "after some experimentation , we adopted the matrn  kernel with @xmath70 because it performed well at reproducing the appearance of realistic residuals for this specific problem . in this case , @xmath71 with @xmath72 $ ] , an amplitude ( @xmath73 ) and a scale ( @xmath74 ) .",
    "the @xmath75 are termed _",
    "hyperparameters _ here ; because a gaussian process describes a population of functions generated by random draws from a probability distribution set by a mean vector and a covariance matrix , the kernel parameters are naturally part of a hierarchical model . in this specific case ,",
    "the functions described by these hyperparameters represent many realizations of covariant residuals from a spectral fit .",
    "figure  [ fig : matrix ] shows an example of the gaussian process kernel and the covariant residuals that can be generated from it . to ensure that @xmath54 remains a relatively sparse matrix ( for computational expediency ) , we employ a hann window function @xmath76 to taper the kernel .",
    "the truncation distance @xmath77 can be set to a multiple of the scale ( we set @xmath78 ) .",
    "in addition to the global covariance structure , there can be local regions of highly correlated residuals .",
    "these patches of large @xmath61 are usually produced by pathologically incorrect spectral features in the model , due to systematic imperfections like missing opacity sources or poorly constrained atomic / molecular data ( e.g. , oscillator strengths ) .",
    "some representative examples are shown in figure  [ fig : badlines ] .",
    "to parameterize such regions in @xmath54 , we introduce a sequence of non - stationary kernels that explicitly depend on the actual wavelength values of a pair of pixels ( @xmath65 and @xmath66 ) , and not simply their separation ( @xmath79 ) .    assuming that these local residual features are primarily due to discrepancies in the spectral line depth ( rather than the line shape or central wavelength ) , a simple gaussian is a reasonable residual model . in that case",
    ", the pixel residuals of the @xmath80-th such local feature could be described as @xmath81\\ ] ] with peak amplitude @xmath82 , central wavelength @xmath83 , and width @xmath84 .",
    "we assume that the amplitude of this gaussian feature is drawn from a normal distribution @xmath85 with mean 0 and variance @xmath86 .",
    "the pixels in this gaussian - shaped residual are correlated because each pixel shares a common random scale factor ( @xmath82 ) .",
    "then , the covariance of any two pixels in this region is given by eq .",
    "[ eqn : expectation ] , where the expectation value is taken with respect to the probability distribution in eq .",
    "[ eqn : amplitude ] @xmath87   a_k \\exp \\left [ - \\frac{r^2(\\lambda_j , \\mu_k)}{2 \\sigma_k^2 } \\right ] \\right \\rangle \\nonumber \\\\ & = & \\langle a_k^2 \\rangle \\exp \\left [ - \\frac{r^2(\\lambda_i , \\mu_k ) + r^2(\\lambda_j , \\mu_k)}{2 \\sigma_k^2 } \\right ] \\nonumber \\\\ & = & a_k^2 \\exp \\left [ - \\ , \\frac{r^2(\\lambda_i , \\mu_k ) + r^2(\\lambda_j , \\mu_k)}{2 \\sigma_k^2}\\right ] .",
    "\\label{eqn : kregion}\\end{aligned}\\ ] ] the full local covariance kernel covering all of the possible gaussian residuals is composed of a linear combination of kernels , @xmath88 with a corresponding set of hyperparameters @xmath89 $ ] .",
    "note that we again taper the kernels with hann windows ( eq .  [ eqn : hann ] ) to ensure a sparse covariance matrix ; in this case , the truncation distance @xmath77 can be set to some multiple of the width parameter ( e.g. , @xmath90 ) . in effect , these kernels systematically down - weight the influence of strong residuals in the likelihood calculation , mitigating any potential bias they might induce on inferences of the interesting parameters ( @xmath91 ) .",
    "similar in spirit to robust linear regression and  bad data \" mixture models @xcite , these kernels provide a means for ( correlated ) outlier rejection that preserves the integrity of the probabilistic framework ( as opposed to the common manual or threshold - based techniques of masking or clipping ) .        in principle , the concept of these local kernels can be extended to account for more complex residual structures .",
    "for example , late - type stars with imperfectly modeled molecular bandheads may produce a complicated pattern of positive and negative residuals or a pronounced mismatch over a relatively large spectral scale .",
    "this phenomenologically different local covariance behavior can still be treated in this framework if an appropriate kernel morphology is adopted .",
    "we can now compute the covariance matrix employed in the likelihood calculation ( eq .  [ eqn : lnlikelihood ] ) as the linear combination of the trivial pixel - by - pixel noise matrix and the global and local kernels discussed above , @xmath92 with hyperparameters @xmath93 $ ] .",
    "the factor @xmath94 is a parameter that scales up the poisson noise in each pixel by a constant factor to account for additional detector or data reduction uncertainties ( e.g. , read noise , uncertainties in the spectral extraction procedure , etc . ) ; typically @xmath95 for well - calibrated optical spectra .",
    "if there are @xmath96 local covariance patches ( see section [ subsec : mcmc ] on how this is determined ) , then there are @xmath97 elements in the set of covariance hyperparameters , @xmath98 .",
    "figure [ fig : matrix ] provides a graphical illustration of how the kernels that comprise the covariance matrix are able to reproduce the structure present in a typical residual spectrum .",
    "the bayesian framework of this inference approach permits us to specify prior knowledge about the model parameters , @xmath99 .",
    "as will be discussed further in sections  [ sec : examples ] and [ sec : discussion ] , in most cases it is necessary to utilize some independent information ( e.g. , from asteroseismology constraints or stellar evolution models ) as a prior on the surface gravity .",
    "but otherwise we generally recommend a conservative assignment of uniform priors , such that @xmath100 is flat over the spectral library grid ( and zero elsewhere ) and @xmath101 is flat for physically meaningful values ( e.g. @xmath102 , @xmath103 , and @xmath104 )",
    ".        for ( early type ) stars with a clear continuum , it makes sense to assume flat priors on the polynomial parameters @xmath105 .",
    "however , information about the calibration accuracy ( e.g. , from comparisons of multiple calibration sources in the same observation sequence ) can be encoded into a simple prior on the chebyshev coefficients ; for example , gaussian priors with widths that represent the fractional variance between different derived calibration functions would be reasonable . for ( late type )",
    "stars with a poorly defined continuum , some judicious tapering of the priors ( such that small coefficients at high @xmath106 are preferred ) may be required to ensure that broad spectral features are not absorbed into the polynomial ( see section  [ sec : examples ] ) .    in general , uniform ( non - negative ) priors",
    "are recommended for the global kernel hyperparameters . for the local kernels , we typically adopt uniform priors for the amplitudes and means \\{@xmath107 , @xmath83 } , but construct a logistic prior for the widths \\{@xmath84 } that is flat below the width of the line - of - sight velocity distribution function ( defined as the convolution of three broadening kernels in eq .",
    "[ eqn : broadening ] ) , @xmath108 , and smoothly tapers to zero at larger values : @xmath109 such a prior formulation prevents local kernels from diffusing to large @xmath84 and low @xmath107 , since that kind of behavior is better treated by the global kernel .",
    "when modeling real data , there is no _ a priori _ information about the locations @xmath110 of the local kernels ; they are instantiated as needed ( see section  [ subsec : mcmc ] ) .",
    "however , using the knowledge gained from previous inferences of similar targets , one could instead start by instantiating kernels at the outset with priors on @xmath110 where there are known to be systematic issues with the synthetic spectra .",
    "the inference framework developed here has a natural blocked structure between the collections of  interesting \" parameters , @xmath111 $ ] , the nuisance parameters @xmath105 , and the covariance hyperparameters @xmath98 .",
    "the conditional dependencies of these parameters are shown graphically in figure  [ fig : pgm ] as a directed acyclic graph @xcite . to explore the posterior distribution , @xmath112 for this type of structure",
    ", it is convenient to employ markov chain monte carlo ( mcmc ) simulations with a blocked gibbs sampler coupled to the metropolis - hastings algorithm .",
    "this procedure works by sampling in a subset of parameters ( with metropolis - hastings proposals ) conditioned on the current ( fixed ) values of the other parameters . after each iteration",
    ", the gibbs sampler updates the sampled parameters and then cycles through all the ( previously fixed ) different parameter subsets in the same way ( for a more mathematical description , see chapter 11 of @xcite ) . a step - by - step prescription follows , where the @xmath113 iteration of the gibbs sampler is indexed with a superscript : + ( 1 ) initialize the parameters .",
    "one might set @xmath114 based on estimates in the literature or scaling behaviors , and make simple assumptions about @xmath115 .",
    "here , we set the chebyshev coefficients ( @xmath115 ) so that the polynomials are constant ( @xmath116 and @xmath117 , @xmath118 @xmath42 ) and assume only the trivial noise spectrum ( and spectral emulator kernel ; see appendix  [ sec : appendix ] ) contributes to the @xmath54 ( i.e. , @xmath119 ) .",
    "+ ( 2a ) start the @xmath113 iteration of the gibbs sampler .",
    "for each iteration of the metropolis - hastings algorithm , sample in @xmath91 to evaluate the posterior ( eq .  [ eqn : post ] ) following the framework laid out in sections [ subsec : likelihood ] and [ subsec : priors ] .",
    "this represents a  slice \" through the posterior space conditioned on the other parameters being held fixed ( @xmath120 and @xmath121 ) .",
    "then update @xmath122 .",
    "+ ( 2b ) for each spectral order , sample in the polynomial parameters @xmath105 and covariance hyperparameters @xmath98 , conditioned on the other parameters being held fixed @xmath123 . then update @xmath124 and @xmath125 .",
    "+ ( 3 ) repeat step  ( 2 ) for 20,000 samples .",
    "+ ( 4 ) repeat the procedure in steps  ( 1)(3 ) with different initializations , storing the samples for each markov chain .",
    "after removing the burn - in samples for each chain , we compute the gelman - rubin convergence diagnostic , @xmath126 ( * ? ? ?",
    "* their eq .",
    "if @xmath127 , we can be reasonably sure that all of the chains have converged to the posterior distribution .",
    "+ in step  ( 2b ) , local covariance kernels are instantiated according to the following procedure .",
    "first , an  average \" residual spectrum is generated by combining @xmath62500 residual spectra that were stored during a burn - in period using only the global kernels ( prior to this storage , the markov chain is thinned to account for autocorrelation of the posterior samples ) .",
    "this average spectrum is then iteratively examined for deviations outside a critical threshold .",
    "when a large residual is identified , a local kernel is introduced with a mean ( @xmath83 ) at its location . after some experimentation with different threshold criteria , we chose to instantiate when the local residual is @xmath1284@xmath129 the standard deviation in the average residual spectrum .",
    "alternative schemes , such as re - evaluating the kernel locations with each iteration of the gibbs sampler , yield similar results ; however , the adopted approach consistently converges with minimal computational overhead .",
    "once all the local kernels are instantiated , the gibbs sampler is run for another period of burn - in . ; in effect , the model will act as if the kernel were deleted automatically . ]",
    "this entire procedure can be a significant computational challenge .",
    "a typical spectrum has @xmath130 , and therefore the many evaluations of the matrix product @xmath131 in the likelihood calculation can be numerically expensive .",
    "however , because @xmath54 is a symmetric , positive semi - definite matrix , we can employ cholesky factorization to optimize the evaluation of the matrix product and avoid the direct calculation of the matrix inversion ( @xmath132 ) .",
    "for multi - order echelle spectra or multiple spectra of the same target ( perhaps taken with different instruments ) , the nuisance parameters for each segment of the spectrum are independent .",
    "this means that the computationally intensive steps of generating a model spectrum for a specific wavelength range and evaluating the likelihood can be parallelized .",
    "the only segment of the code that needs to be synchronized is the mcmc proposal of stellar parameters , which are shared between all chunks of the spectrum .",
    "the massive parallelization of this algorithm on a computer cluster therefore enables the simultaneous inference of interesting parameters over wide spectral ranges at high resolution , or from multiple datasets . to sample the posteriors in this mode",
    ", we extend the metropolis - hastings sampler included in the emcee package @xcite to function within a parallelized blocked gibbs sampler .",
    "the time required to thoroughly explore the posterior depends on both the data volume and the desired precision on the inference of the covariance hyperparameters .",
    "if only the stellar parameters @xmath91 are of interest , one can first optimize the kernel parameters and then proceed with them fixed , since the stellar parameter posteriors are relatively insensitive to the precise value of the kernel parameters ( once near their optimal value ) .",
    "a fit of an @xmath133 spectrum with @xmath12830 echelle orders takes @xmath622 hours ( parallelized on a cluster ) .",
    "if the full posteriors for the nuisance parameters are desired , the computation might take an order of magnitude longer .",
    "in this section , we illustrate how the modeling framework operates for two real datasets .",
    "the first is an elaboration of the example shown throughout section [ sec : method ] , using a high resolution optical spectrum of the f5 star ( and transiting exoplanet host ) wasp-14 @xcite .",
    "the second uses a medium resolution near - infrared spectrum of the m5 dwarf gliese 51 ( hereafter gl 51 ) , observed as part of the nasa / irtf library of spectral standards @xcite . in both cases , we sequentially build up the complexity of the modeling framework to demonstrate how each of the components described in section  [ sec : method ] affects the posteriors on the parameters of interest ( @xmath10 ) .",
    "we adopt the recent incarnation of the phoenix library @xcite for the models , although comment on systematic differences between libraries in section  [ sec : systematics ] .",
    "a high resolution ( @xmath134 ) optical spectrum of wasp-14 was obtained on 2009 june 14 using the tillinghast reflector echelle spectrograph ( tres ; * ? ? ?",
    "* ) on the fred lawrence whipple observatory 1.5 m telescope .",
    "tres delivers an echelle spectrum with 51 orders that cover the full optical range , from 38609100 .",
    "the data were reduced and calibrated using standard techniques in the tres pipeline ( cf .",
    ",  @xcite ; see @xcite for more specific details ) . at 5100 , the s / n is @xmath62150 per resolution element .",
    "following @xcite , we focus here on the central three tres orders , covering @xmath625100 - 5400 .",
    "we start with a  standard \" inference , using the most commonly employed likelihood function ( i.e. , @xmath135 , with a trivial covariance matrix using only the poisson uncertainties ) .",
    "interpolation in the model library is performed with a basic tri - linear algorithm ( in this specific case , @xmath136 has only three dimensions ) . to avoid a prominent systematic ( see section  [ sec : systematics ] ) , we fix the surface gravity to @xmath137 ( with a @xmath138-function prior ) .",
    "this independent prior information comes from the combination of a constraint on the mean stellar density based on exoplanet transit depth measurements and a comparison of optical photometry with stellar models in the color - magnitude diagram @xcite .",
    "the resulting marginal posteriors on @xmath139 and [ fe / h ] , listed in table  [ table : tests ] and shown in figure  [ fig : phoenix_posterior ] , are remarkably narrow  unbelievably so , given how subtly the spectrum changes over such small parameter deviations .",
    "for the second test , we increase the complexity of the covariance matrix by introducing the global kernel treatment discussed in section  [ subsec : global_covariance ] .",
    "we find non - negligible amplitudes and correlation lengths for these kernels , as would be expected for a typical correlated residual spectrum .",
    "with respect to the standard inference , the uncertainty associated with @xmath2 has increased by roughly a factor of three , but the @xmath4}$ ] posterior is only marginally broadened ( by @xmath6250% ) . upon closer inspection of the latter ,",
    "it becomes clear that the posterior has an artificially sharp peak located at a grid point in the model library ( @xmath4}= -0.5 $ ] ) .",
    "this ` noding ' is an artifact of naive interpolation over a sparsely - sampled dimension in the library grid ; when the uncertainty in the interpolation itself constitutes a significant fraction of the total error budget , the fit will be driven toward grid points ( where the interpolation error is naturally minimized ; see also @xcite ) . to mitigate this behavior , we need to employ an interpolation scheme that properly incorporates this kind of uncertainty .",
    "clc|r@ @xmath140 lr@ @xmath140 l[!bh ] ( 1 ) & linear & trivial & 6280 & 5 & @xmath1410.471 & 0.004 + ( 2 ) & linear & + @xmath142 & 6297 & 16 & @xmath1410.500 & 0.006 + ( 3 ) & emulator & + @xmath142 & 6281 & 26 & @xmath1410.482 & 0.012 + ( 4 ) & emulator & + @xmath143 & 6301 & 29 & @xmath1410.431 & 0.012            therefore , in a third test we implement the bayesian emulator described in appendix  [ sec : appendix ] to propagate uncertainty in the interpolation .",
    "this procedure successfully avoids the ` noding ' behavior in @xmath4}$ ] , and inflates the associated uncertainty by a factor of 2.5 compared to the  standard \" inference approach .",
    "the uncertainty on @xmath2 is now 5@xmath129 larger than in the original test .    finally ,",
    "in a fourth test we fold in the methodology for the local covariance kernels described in section  [ subsec : local_covariance ] .",
    "this has little effect on the widths of the parameter posteriors ( @xmath14410%  increase ) , but does shift their peaks to slightly higher values in both @xmath2 and @xmath4}$ ] .",
    "we suspect this is likely driven by a bias in the inference of @xmath4}$ ] , produced because the phoenix  models tend to have more ` outlier ' spectral lines with _ over - predicted _ line depths . without the local covariance kernels to downweight these outliers , the models tend toward lower metallicity to account for them .",
    "but when the local kernels are included , this bias is reduced and a more appropriate higher @xmath4}$ ] value is inferred .",
    "figure  [ fig : phoenix_residuals ] demonstrates how well the modeling framework can match the character of the residual spectrum when employing the sophisticated covariance matrix ( test 4 ) advocated here .",
    "a moderate resolution ( @xmath145 ) near - infrared spectrum of gl  51 was obtained on 2000 nov 6 using the spex instrument @xcite on the 2.3 m nasa infrared telescope facility ( irtf ) .",
    "spex is a cross - dispersed echelle spectrograph that covers the red - optical to thermal - infrared spectrum ( 0.75.5@xmath146 m ) in two settings .",
    "these data were obtained as part of the irtf spectral standard library project @xcite , and were processed through the well - vetted spextool reduction pipeline @xcite to deliver a fully calibrated spectrum . at 2.1@xmath146 m , the s / n is @xmath62400 per resolution element .",
    "clc|r@ @xmath140 lr@ @xmath140 l[!bh ] ( 1 ) & linear & trivial & 3256 & 3 & 0.89 & 0.01 + ( 2 ) & linear & + @xmath142 & 3022 & 35 & 0.00 & 0.03 + ( 3 ) & emulator & + @xmath142 & 3230 & 30 & 0.27 & 0.03 + ( 4 ) & emulator & + @xmath143 & 3180 & 35 & 0.28 & 0.04            modeling late - type stellar atmosphere structures and their spectra is considerably more complex than for sun - like stars , due to lingering uncertainties in the atmosphere physics and molecular opacities . especially confounding is the presence of complex condensates ( clouds ) at the coolest temperatures @xcite , making it considerably more challenging to determine ( sub- ) stellar properties @xcite .",
    "various approaches have been taken to infer the key parameters in the face of these difficulties , including iteratively masking regions with poor spectral agreement ( e.g. , * ? ? ?",
    "astutely , @xcite  note that such a scheme may exclude regions of the spectrum that contain intrinsically useful information for discriminating between physical properties , and that a more sophisticated approach would weight each spectral region based on its consistency with the data .",
    "the modeling framework that we have constructed here does exactly that .    as another demonstration of this framework , we carried out the sequence of tests outlined in the previous section for the @xmath1-band portion of the spex spectrum of gl  51 . following the analysis of similar data for this star by @xcite",
    ", we fix the surface gravity to @xmath147 based on a comparison with standard stellar evolution models .",
    "the test results are listed in table  [ table : tests_gl51 ] ; the posteriors for @xmath2 and @xmath4}$ ] are shown together in figure  [ fig : gl51_posterior ] .",
    "like wasp-14 , we find that a more appropriate treatment of the covariance matrix results in a substantial broadening of the parameter posteriors ; the uncertainties on @xmath2 and @xmath4}$ ] are inflated by a factor of @xmath6210 and 4 , respectively .",
    "however , in this case the parameter values ( posterior peaks ) also exhibit substantial movement along the sequence of tests .",
    "the underlying cause of this behavior lies with the and resonance line depths , which are systematically under - predicted in the phoenix library ( even for high metallicities ; see also @xcite ) .",
    "@xcite suggest that these discrepancies may be the consequence of inaccurate atomic data ( oscillator strengths and/or opacities ) .",
    "in the first test with a trivial covariance matrix , these ` outlier ' lines drive the model to favor a very high @xmath4}$ ] .",
    "but when we consider the more sophisticated versions of @xmath54 that employ gaussian processes to treat correlated residuals , the contribution of these features to the likelihood calculation is reduced , and therefore @xmath4}$ ] returns to a more appropriate range . because this portion of the spectrum has only these two outlier features , their influence can be mitigated either with a larger global covariance kernel amplitude ( @xmath73 ) , or with a smaller @xmath73 but significant contributions from local covariance kernels ( which explains why there is little difference between the posteriors in the third and fourth tests in the sequence ) . for reference ,",
    "figure  [ fig : gl51_residuals ] compares the observations with draws from the posterior distribution for the advocated modeling approach ( corresponding to the fourth test ) .",
    "the methodology behind the likelihood calculations we have developed could prove especially useful for spectroscopic inferences of the parameters of cool stars like gl  51 , where substantial uncertainties in their more complex atmospheres will naturally produce systematic deviations between models and data .",
    "however , many of those discrepancies will be manifested in molecular features , which likely result in considerably more complex residual structures than noted here ( e.g. , the tio bands in the red - optical ; see * ? ? ?",
    "* their fig .  9 ) .",
    "the overall framework we have employed should still function , although more appropriate local covariance kernels may need to be developed to capture the different nature of these outliers .",
    "for example , one might employ hybrid kernels ( like the product of a truncated exponential and a matrn  kernel ) or empirically - motivated parametric shapes ( e.g. , a saw - tooth pattern ) to provide a better representation than a simple gaussian feature .",
    "the results of the sequence of tests in the previous two sections illustrate some key issues in the spectroscopic inference of stellar parameters .",
    "first , the residual spectra derived from ( typically ) imperfect models exhibit correlated structure ( e.g. , see fig .  [",
    "fig : class0 ] ) that can not be explained well with a trivial ( diagonal ) covariance matrix .",
    "if that naive assumption is made ( as is usually the case ) , the resulting posteriors are unrealistically narrow and may end up being biased ( particularly for @xmath4}$ ] or for cases influenced by prominent ` outlier ' lines ) .",
    "this issue of implausibly small formal uncertainties has long been recognized in the stellar spectroscopy community .",
    "the standard solution has been to add ( in quadrature ) a ` floor ' contribution , imposed independently on each parameter and meant to be representative of the systematics ( e.g. , see @xcite or @xcite for clear and open discussions of this approach ) .",
    "the key problems with this tactic are that these systematics are in reality degenerate ( and so should not be applied independently ) and that they dominate the uncertainty budget , but are in a large sense arbitrary  they are not self - consistently derived in the likelihood framework .",
    "our goal here has been to treat one aspect of this systematic uncertainty budget internal to the forward - modeling process , by employing a non - trivial covariance matrix that accounts for generic issues in the pixel - by - pixel inference problem . given the results above , we have demonstrated that this procedure successfully accounts for a substantial fraction of the ( empirically motivated ) _ ad hoc _ systematic ` floor ' contribution typically adopted in inference studies .    however , although a likelihood function that can properly account for the character of the residuals is important , it does not by itself treat _ all _ of the important kinds of systematics in the general spectroscopic inference problem . in future work that can build on the flexible likelihood formalism we have advocated here ,",
    "there are three other important sources of systematic uncertainty that should be considered : ( 1 ) data calibration ; ( 2 ) optimized parameter sensitivity ; and ( 3 ) model assumptions , or flexibility .",
    "we discuss each of these issues briefly , with attention paid to potential remedies that fit within the likelihood framework developed here .",
    "perhaps the most familiar source of systematics lies with issues in the data calibration . in the idealized case of perfect calibration",
    ", the physical parameters inferred from different observations of the same ( static ) source should be indistinguishable . but given the complexity of a detailed spectroscopic calibration , that is not typically the case in practice . the common approach to quantify the systematic uncertainties contributed by calibration issues",
    "is to compare the inferences made using different spectra ( e.g. , from different instruments and/or observations ) .",
    "the final parameter values are usually presented as an average of these separate inferences , with the uncertainties inflated by adding in quadrature some parameter - independent terms that account for their dispersion .",
    "the more appropriate way of combining these inferences is to model the individual spectra simultaneously in a hierarchical framework like the one discussed in section  [ sec : method ] : in that way , the dispersion is appropriately propagated into the parameter uncertainties while any intrinsic degeneracies are preserved ( which is not possible in the standard ` weighted average ' approach ) .",
    "ultimately , one could also introduce some empirically - motivated nuisance parameters that are capable of forward - modeling imperfections in the data calibration , similar to the approach adopted in section  [ subsec : postprocess ] ( e.g. , see fig .  [",
    "fig : chebyshev ] ) .",
    "another important source of systematic _ bias _ comes from the fact that certain physical parameters have only a relatively subtle effect on the spectrum .",
    "stellar spectroscopists are familiar with this being an issue when inferring the surface gravity , @xmath3 , since it is primarily manifested as low - level modifications to the wings of certain spectral lines like mg  b and in the equivalent widths of lines from singly - ionized elements like and .",
    "when modeling data with a large spectral range , the effects of varying @xmath3 are small compared to the residuals introduced by the many other model imperfections .",
    "consequently , the surface gravity will not be constrained well , and inferences on @xmath3 ( and therefore other degenerate parameters ) can be substantially biased . as an example , when fitting the wasp-14 data in section  [ subsec : wasp ] without prior information on the surface gravity , we find a shift of @xmath620.9 dex to lower @xmath3 ( and accompanying shifts in @xmath2 and @xmath4}$ ] ) .",
    "if we instead use a customized version of the @xcite models designed to more accurately reproduce this part of the optical spectrum for sun - like stars ( as employed by spc ; @xcite ) , there is still a 0.2 dex shift compared to the independent , accurate constraints from the transiting planet @xcite",
    ". similar work with larger samples indicate a typical scatter in the @xmath3 values inferred solely from spectra relative to independent , accurate constraints from other data ( @xmath620.5 dex ; @xcite ) .",
    "there are two commonly utilized , and not mutually exclusive , approaches to mitigating this kind of bias .",
    "first is the judicious use of a prior , based on either independent and accurate measurements ( e.g. , from asteroseismology , dynamical masses and distances , etc . ) or stellar evolution models ( as is demonstrated here ) . of course , such information is unfortunately not always readily available for the target of interest .",
    "a second approach is to severely limit the spectral range of the data being modeled , focusing primarily on those spectral features especially sensitive to the parameter of interest . but that carries its own risk , since the models derived from the inferred posteriors might well be wildly inconsistent with the rest of the spectrum .",
    "recently , @xcite proposed a sophisticated , iterative approach that apparently resolves this issue , employing a sequence of conditional inferences based on sets of specific spectral features that are especially sensitive to individual physical parameters .",
    "this seems like a promising component for future inclusion in the likelihood framework we have developed in section  [ sec : method ] .    finally , and perhaps most significant , there are also sources of systematic bias and uncertainty introduced by limitations in the synthetic stellar models themselves .",
    "different models make varied assumptions in their treatments of the atmosphere structures , boundary conditions ( e.g. , convection ) , fundamental atomic / molecular data ( e.g. , opacities ) , and radiative transfer . taken together , these variations produce notably different model spectra for the same values of the physical parameters . as a benchmark for estimating the scope of this source of bias ,",
    "we re - performed the inference described in the fourth test of section  [ subsec : wasp ] , but using the customized @xcite model library instead of the @xcite library .",
    "the resulting inferences for @xmath2 and @xmath4}$ ] are in excellent agreement with those derived by @xcite using the spc method , but are shifted by 150k ( higher ) and 0.15dex ( higher ) , respectively , compared to the phoenix results . while the relevant physics included in these models is very similar for these temperatures and the inferred stellar parameters are similar in an absolute sense , it is still striking that the systematic shift between models is several times larger than the statistical uncertainties derived from our likelihood function . at this point",
    ", there is little to be done to rectify these model - dependent differences ; in the future , one hopes that the model inputs can be refined based on feedback from the data ( see sect .  [ sec : discussion ] ) .",
    "any inferences of physical parameters should only be considered in the context of the assumed models .    aside from these different assumptions and inputs ,",
    "the limited _ flexibility _ of these models certainly also contributes to the systematic uncertainty budget , and is possibly also a source of systematic bias .",
    "model spectral libraries typically have neglected dimensions in parameter - space that , if made available , would be expected to broaden and possibly shift the posteriors for the primary physical parameters .",
    "one typical example lies with element - specific abundance patterns , often distilled to the enhancement of @xmath148-elements ( i.e. , [ @xmath148/fe ] ) . if the target star has a non - zero [ @xmath148/fe ] ( an enhancement or deficit relative to the solar ratios ) , but is fit with a single , global metallicity pattern , it is not clear that the sophisticated covariance formalism developed here would be capable of appropriately capturing such residual behavior .",
    "another prominent example of an important hidden parameter dimension is the microturbulence , which for some spectral types and spectral resolution may impact the spectrum in a similar way as the surface gravity ( and may be partly responsible for the @xmath3 bias discussed above ; * ? ? ?",
    "* ) . to mitigate the resulting deficiencies in precision ( and potentially accuracy ) on the inference of other parameters , we would ideally employ libraries or modeling front - ends that can incorporate some flexibility in these hidden ( i.e. , ignored ) dimensions of parameter - space ( e.g. , individual elemental or group - based abundance patterns , microturbulence , etc . )",
    "astronomers exploit spectroscopy to retrieve physical information about their targets .",
    "ideally , such inferences are made with the maximal precision afforded by the measurement noise , and accurately reflect the uncertainties with minimal systematic bias .",
    "but in practice , the spectral models used as references are never perfect representations . even modest mismatches between data and model can propagate substantial systematic uncertainty into the inference problem . in high - sensitivity applications ( e.g. , stellar and exoplanetary astrophysics ) , ignoring these systematics",
    "can give a false sense of both precision and accuracy in the inferences of key parameters .",
    "typically , the more egregious of these imperfections are  mitigated \" by dismissal ( explicitly not considering a subset of the data ; e.g. , masking , clipping ) .",
    "rarely , they are confronted directly with painstaking , computationally expensive fine - tuning of more general ( nuisance ) parameters in the model ( e.g. , oscillator strengths , opacities ) , albeit only over a very limited spectral range and region of physical parameter - space .",
    "we have presented an alternative approach to dealing with this fundamental issue , grounded in a generative bayesian framework .",
    "the method advocated here constructs a sophisticated likelihood function , employing a non - trivial covariance matrix to treat the correlated pixel - to - pixel residuals generated from intrinsically imperfect models . that matrix is composed of a linear combination of _ global _ ( stationary ) and _ local _ ( non - stationary ) gaussian process kernels , which parameterize an overall mild covariance structure as well as small patches of highly discrepant outlier features .",
    "in the context of a given model parameterization ( i.e. , synthetic spectral library , or a more complex and flexible model generator ) , the framework we have developed provides a better inference than the standard @xmath149 ( or cross - correlation ) comparison .",
    "we have built up a series of tests that demonstrates how the emulator , global kernels , and local kernels affect the nature of the inference on the stellar parameters . to demonstrate how the framework is used , we determined the surface parameters of main - sequence stars with mid - f and mid - m spectral types from high - s / n optical and near - infrared data , with reference to pre - computed model libraries ( sect .",
    "[ sec : examples ] ) .",
    "the source code developed here is open and freely available for use : see http://iancze.github.io/starfish .",
    "the novelty of employing this kind of likelihood function in the spectroscopic inference problem is that the treatment of data  model mismatches ( in essence , the fit quality ) is explicitly built into the forward - modeling framework .",
    "this offers the unique advantage that discrepant spectral features ( outliers ) , which may contain substantial ( even crucial ) information about the parameters of interest , can still effectively propagate their useful information content into the posteriors with a weighting that is determined self - consistently . from a practical standpoint ,",
    "this means that a larger spectral range can be used and model imperfections can be downweighted by the usage of covariance kernels .",
    "the global covariance framework provides more appropriate estimates of the posterior probability distribution functions ( i.e. , the precision or uncertainty estimates ) for the model parameters . the automated identification and disciplined downweighting of problematic  outlier \" spectral lines ( those that can not be reproduced with any combination of the model parameters ) with local covariance kernels can prevent them from overly influencing ( and possibly biasing , especially in cases with few spectral features available ) the inferences . in many cases ,",
    "the underlying physical problem lies with incorrect ( or inaccurate ) atomic and/or opacity data used in the models . in this sense",
    ", the posteriors of the hyperparameters of the local covariance kernels can actually indicate in what sense and scale these inputs need to be modified to better reproduce observational reality .",
    "the approach we describe is generally applicable to any spectroscopic inference problem ( e.g. , population synthesis in unresolved star clusters or galaxies , physical / chemical models of emission line spectra in star - forming regions , etc . ) .",
    "moreover , it has the flexibility to incorporate additional information ( as priors ) or parametric complexity ( if desired ) , and could be deployed as a substitute for a simplistic @xmath149 metric in already - established tools ( e.g. , sme )",
    ". another potential application might be in the estimation of radial velocities using traditional doppler - tracking pipelines for exoplanet or binary star research .",
    "poorly modeled micro - tellurics can lead to incorrect measurements of radial velocities for certain contaminated chunks of the spectrum , causing them to give unrealistically precise but biased velocity measurements .",
    "a flexible noise model would broaden the posteriors on these points and allow them to be combined into a more accurate systemic velocity .",
    "ultimately , the benefits of employing covariance kernels to accommodate imperfect models could be extended well beyond modeling the spectra of individual targets . in principle , the approach we have described here can be used to systematically discover and quantify imperfections in spectral models and eventually to build data - driven improvements of those models that are more appropriate for spectroscopic inference . by fitting many stellar spectra with the same family of models",
    ", we can catalog the covariant structure of the fit residuals  especially the parameters of the local covariance kernels  to collate quantitative information about where and how the models tend to deviate from observational reality . that information can be passed to the spectral synthesis community , in some cases enabling modifications that will improve the quality of the spectral models . on a large enough scale",
    ", this feedback between observers and modelers could be used to refine inputs like atomic and molecular data ( oscillator strengths , opacities ) , elemental abundance patterns , and perhaps the stellar atmosphere structures .",
    "if one has access to the radiative synthesis process that generates the model spectra , there are many possible means to improve their quality .",
    "in particular , a process of history matching can be used to rule out regions of parameter space where the models do not fit well ( e.g. , for a use in galaxy formation simulations , see @xcite ) .",
    "for example , if one had full control over the radiative synthesis code , stellar structure code , and atomic line database , one could improve the performance of the spectral emulator by ruling out regions of parameter space for these separate components that are inconsistent with a collection of observed spectra , such as a set of standard stars spanning the full range of spectral classifications .    in a similar vein",
    ", we could also simultaneously use several synthetic spectral libraries to infer the stellar parameters while also identifying discrepant regions of the spectrum .",
    "a treatment using multiple synthetic libraries would likely reveal interesting correlations between model discrepancies , such as a specific signature among many lines ( e.g. deviations in spectral line shape that can not be explained by variations in @xmath150 ) .",
    "conversely , if a discrepant feature is seen for all models , it could be due to either an anomaly with the given star ( e.g. , a chromospheric line due to activity or perhaps an intervening interstellar absorption line ) or a correlated difficulty among all models ( e.g. , an incorrect atomic constant ) .",
    "alternatively , this kind of feedback could be used to make data - driven modifications to the already existing models , creating a new semi - empirical model library .",
    "this could be accomplished by linking the parameters of the covariance kernels while fitting many stars of similar spectral type in a hierarchical bayesian model , which would add confidence to the assessment that certain spectral features are _",
    "systematic _ outliers and offer general quantitative guidance on how to weight them in the likelihood calculation . rather than simply assembling an empirical spectral library using only observations , this combined machine - learning approach would naturally provide a physical anchoring for the key physical parameters , since they are reflected in the spectra based on the physical assumptions in the original models .",
    "this kind of large - scale analysis holds great promise in the ( ongoing ) era of large , homogeneous high resolution spectroscopic datasets ( e.g. , like those being collected in programs like the apogee and hermes surveys ; @xcite ) , since they provide enormous leverage for identifying and improving the underlying model systematics .",
    "+ the authors would like to acknowledge the following people for many extraordinarily helpful discussions and key insights : daniel foreman - mackey , guillermo torres , david latham , lars buchhave , john johnson and the exolab , daniel eisenstein , rebekah dawson , tom loredo and the exostat group , allyson bieryla , and maxwell moe .",
    "two anonymous reviewers provided encouraging and very useful comments on the manuscript draft that greatly improved its clarity and focus .",
    "ic is supported by the nsf graduate fellowship and the smithsonian institution .",
    "is supported at harvard by nsf grant ast-1211196 .",
    "this research made extensive use of astropy @xcite and the julia language @xcite .",
    "the spectral emulator is designed to serve as an improved interpolator for the synthetic spectral library . rather than a ( tri-)linear interpolator , which would deliver a single spectrum for a given @xmath151 , the spectral emulator delivers a probability distribution of possible interpolated spectra .",
    "in this manner , it is possible to incorporate realistic uncertainties about the interpolation process into the actual likelihood calculation . in the limit of moderate to high signal - to - noise spectra",
    ", these interpolation uncertainties can have a significant effect on the posterior distribution of @xmath151 .",
    "a schematic of the emulator is shown in figure  [ fig : flowchart_appendix ] , which is a continuation of figure  [ fig : flowchart ] .",
    "briefly , the emulator consists of a set of eigenspectra , representing the synthetic spectral library , that can be summed together with different weights to reproduce any spectrum originally in the library . to produce spectra that have @xmath151 in between @xmath152 ,",
    "the weights are modeled with a smooth gaussian process ( gp ) .",
    "this gp delivers a probability distribution over interpolated spectra , which can then be incorporated into the covariance matrix introduced in section  [ subsec : likelihood ] .",
    "here we describe the design and construction of our spectral emulator .",
    "model library spectra are stored as ( 1-dimensional ) arrays of fluxes , sampled on high resolution wavelength grids . in the case of interest here ,",
    "the sets of model parameters @xmath153}\\}]$ ] define the dimensions of the library grid . the full spectral library , @xmath154 , is therefore encapsulated in a 4-dimensional array .",
    "the libraries used here have grid spacings of 0.5dex in @xmath3 and 0.5 dex in @xmath4}$ ] ; the @xcite library steps by 250k in @xmath139 , but the phoenix library has finer coverage in 100k increments .",
    "the first step in designing a spectral emulator is to break down the library into an appropriate basis @xcite .",
    "we chose the principal component basis to decompose the library into a set of  eigenspectra \" , following the techniques of @xcite . prior to this decomposition ,",
    "we isolate a subset of the library ( containing @xmath155 spectra ) with parameter values that will be most relevant to the target being considered ( e.g. , for gl  51 , this means considering only effective temperatures below @xmath623800k ) .",
    "we then standardize these spectra by subtracting off their mean spectrum and then  whiten \" them by dividing off the standard deviation spectrum measured in each pixel across the grid .",
    "the mean spectrum is @xmath156 and the standard deviation spectrum is @xmath157 ^ 2 } , \\ ] ] where @xmath158 denotes the full collection of the @xmath155 sets of stellar parameters under consideration in the library and @xmath159 denotes a single set of those parameters drawn from this collection .",
    "both @xmath160 and @xmath161 are vectors with length @xmath162 , the same size as a raw synthetic spectrum ( @xmath163 ) . in effect",
    ", all library spectra are standardized by subtracting the mean spectrum and dividing by the standard deviation spectrum @xmath164    the eigenspectra are computed from this standardized grid using principal component analysis ( pca ; * ? ? ?",
    "each eigenspectrum is a vector with length @xmath162 , denoted as @xmath165 , where @xmath80 is the principal component index @xmath166 .",
    "we decided to truncate our basis to the first @xmath167 eigenspectra , where @xmath167 is decided by the minimum number of eigenspectra required to reproduce any spectrum in the grid to better than 2% accuracy for all pixels ( the typical error for any given pixel is generally much smaller than this , @xmath168 ) .",
    "as an example , the eigenspectra basis computed for gl  51 using the phoenix library is shown in the top panel of figure  [ fig : pca_reconstruct ] .      using the principal component basis",
    ", we can lossily reconstruct any spectrum from the library with a linear combination of the eigenspectra @xmath169 where @xmath170 is the weight of the @xmath171 eigenspectrum .",
    "these weights are 3-dimensional scalar functions that depend on the stellar parameters @xmath151 . any given weight , which is generally a smooth function of the stellar parameters ( see the left panel of figure  [ fig : gp_interp ] ) , can be determined at any grid point in the library by taking the dot product of the standardized synthetic spectrum with the eigenspectrum @xmath172 to simplify notation , we can write the collection of eigenspectra weights in a length-@xmath167 column vector @xmath173 and horizontally concatenate the eigenspectra into a matrix with @xmath27 rows and @xmath167 columns @xmath174 then , we can rewrite eq .",
    "( [ eqn : reconstruct ] ) as @xmath175 where @xmath176 represents the element - wise multiplication of two vectors .    to recapitulate",
    ", the framework described above can be used to decompose the synthetic spectra in a model library into a principal component basis , allowing us to reconstruct any spectrum in the library as a ( weighted ) linear combination of @xmath167 eigenspectra .",
    "the weights corresponding to each eigenspectrum are moderately - smooth scalar functions of the three stellar parameters , @xmath10 .",
    "therefore , to create a spectrum corresponding to an arbitrary set of these parameters that is not represented in the spectral library , we must interpolate the weights to this new set . in practice",
    ", it may be possible to use a traditional scheme like spline interpolation to do this directly .",
    "however , we found that with sensitive spectra ( e.g. , for gl  51 the s / n is @xmath128400 ) , the uncertainty in the interpolated representation of the spectrum can constitute a significant portion of the total uncertainty budget .",
    "this , combined with the under - sampling of the synthetic grid can cause artificial  noding \" of the posterior near grid points in the synthetic library , because the interpolated spectrum is not as good as the raw spectrum at the grid point . even explicitly accounting for interpolation error by doing",
    " drop - out \" interpolation tests and empirically propagating it forward does not relieve this noding issue .",
    "so instead , we address this problem by employing a gaussian process to model the interpolation of the eigenspectra weights over @xmath10 , thereby encapsulating the range of possible interpolated spectra .",
    "each weight is modeled by a gaussian process for each eigenspectrum . for a single eigenspectrum with index @xmath80",
    ", we denote the collection of @xmath177 evaluated for all the spectra in the library as a length @xmath155 vector @xmath178 .",
    "the gaussian process treats @xmath178 as a collection of random variables drawn from a joint multi - variate gaussian distribution @xcite , @xmath179 with @xmath180 denoting the covariances .",
    "the kernel that describes the covariance matrix for this distribution is assumed to be a 3-dimensional squared exponential , @xmath181 \\nonumber \\\\    & & \\times \\exp \\left [ -\\frac{(\\log g_i - \\log g_j)^2}{2 \\ , \\ell_{{\\log g}}^2 } \\right ] \\\\    & & \\times \\exp \\left [ -\\frac { ( { [ { \\rm fe}/{\\rm h}]}_i - { [ { \\rm fe}/{\\rm h}]}_j)^2}{2 \\ ,",
    "\\ell_{{[{\\rm fe}/{\\rm h}]}}^2 } \\right ] \\nonumber ,    \\label{eqn : emulator_kernel}\\end{aligned}\\ ] ] with hyperparameters @xmath182 , @xmath183 , @xmath184 , @xmath185}}$ ] } representing an amplitude and length scale for each dimension of @xmath10 .",
    "unlike the matrn  kernel used in section  [ sec : method ] ( which produces a more structured behavior reminiscent of the spectral residuals ) , this squared exponential kernel has a smooth functional form that is more appropriate to represent the behavior of the eigenspectra weights across the library grid , as demonstrated in figure  [ fig : gp_interp ] .",
    "the @xmath186-dimensional covariance matrix is @xmath187 the evaluation of the covariance kernel for all pairings of stellar parameters at library gridpoints .",
    "once the gaussian processes for each @xmath80 are specified , we can construct the joint distribution .",
    "@xmath188 we use @xmath189 to denote the concatenation of @xmath178 vectors into a single length @xmath190 vector , and @xmath191 as the @xmath192 covariance matrix , @xmath193    although we could optimize the hyperparameters of each gaussian process independently based upon how well it reproduces the collection of weights for that eigenspectrum , ideally we would like to optimize the hyperparameters according to a metric that describes how well the emulator actually reproduces the original library of synthetic spectra .",
    "following @xcite , we write down a likelihood function describing how well the reconstructed spectra match the entirety of the original synthetic grid @xmath194 \\label{eqn : em_data_likelihood}\\end{gathered}\\ ] ] here , @xmath195 represents a length @xmath196 vector that is the collection of all of the synthetic flux vectors concatenated end to end . the precision of the eigenspectra basis representation , or the statistical error in the ability of the emulator to reproduce the known eigenspectra",
    "is represented by @xmath197 . because we have truncated the eigenspectra basis to only @xmath167 components , where @xmath198 is much smaller than the number of raw spectra in the library",
    ", the emulator will not be able to reproduce the synthetic spectra perfectly . by including this  nugget \" term in the emulator , we are also forward propagating the interpolation uncertainty for @xmath151 near or at values of @xmath152 .",
    "we specify a broad @xmath199 function prior on @xmath197 because we expect it to be well constrained by the data .",
    "@xmath200 where shape @xmath201 and rate @xmath202 . to facilitate the manipulation of eqn  [ eqn : em_data_likelihood ]",
    ", we create a large @xmath203 matrix that contains the all of the eigenspectra @xmath204\\ ] ] where @xmath205 is the kronecker product .",
    "this operation creates a matrix , which , when multiplied by the vector @xmath189 , enables ( lossy ) reconstruction of the entire synthetic library @xmath206 up to truncation error in the eigenspectrum basis ( @xmath197 ) . for a given @xmath197 ,",
    "the maximum likelihood estimate for eqn  [ eqn : em_data_likelihood ] is @xmath207 .",
    "using @xmath208 , we can factorize eqn  [ eqn : em_data_likelihood ] into @xmath209\\\\ \\times \\lambda_\\xi^{m ( n_\\textrm{pix } - m ) /2 } \\exp \\left [ -\\frac{\\lambda_\\xi}{2 } { \\cal f}^{\\mathsf{t}}\\left ( i - \\phi(\\phi^{\\mathsf{t}}\\phi)^{-1 } \\phi^t \\right ) { \\cal f } \\right ] \\\\\\end{gathered}\\ ] ] now , only the middle line of this distribution depends on @xmath208 , so we can reformulate this equation into a dimensionality reduced likelihood function and absorb the other terms into a modified prior on @xmath197 .",
    "@xmath210\\end{gathered}\\ ] ] to summarize , we have reduced the dimensionality of the distribution from @xmath211 to @xmath212 although we introduced the likelihood function in eqn  [ eqn : em_data_likelihood ] , we have yet to include the gaussian processes or the dependence on the emulator parameters @xmath213 .",
    "we do this by multiplying eqn  [ eqn : dimensionality_reduced ] with our prior distribution on the weights ( eqn  [ eqn : weight_prior ] ) , @xmath214 and integrate out the dependence on @xmath189 .",
    "we perform this integral using eqn  a.7 of @xcite for the product of two gaussians , which yields @xmath215 \\end{gathered}\\ ] ] the dimensionality reduction operation changes the priors on @xmath197 ( eqn  [ eqn : gamma_priors ] ) to @xmath216 @xmath217 to complete the posterior distribution for the emulator , we specify @xmath199 function priors on the gaussian process length scale kernel parameters @xmath213 .",
    "typically , these priors are broad and peak at lengths corresponding to a few times the spacing between grid points , which helps the gaussian process converge to the desired emulation behavior .",
    "the full posterior distribution is given by @xmath218 where the prior is given by @xmath219 } } , b_{{[{\\rm fe}/{\\rm h}]}}).\\end{gathered}\\ ] ]    now that we have fully specified a posterior probability distribution , we can sample it and find the joint posteriors for the parameters @xmath197 and the @xmath213 for all @xmath80 simultaneously .",
    "once we have identified the best - fit parameters for the emulator , we fix these parameters for the remainder of the spectral fitting .    now , the emulator is fully specified and can be used to predict the values of the weights at any arbitrary set of stellar parameters @xmath151 by considering them drawn from the joint distribution @xmath220 where @xmath221 is an augmented covariance matrix that includes the point @xmath151 . to simplify notation",
    ", we let @xmath222\\ ] ] with this notation , the @xmath223 matrix @xmath224 is the region of the covariance matrix that describes the relations between the set of parameters in the grid , @xmath225 .",
    "the @xmath226 matrix @xmath227 ( and its transpose @xmath228 ) describe the relations between the set of parameters in the grid and the newly chosen parameters to interpolate at @xmath151 .",
    "the structure of @xmath227 is set by evaluating @xmath229 ( eqn  [ eqn : emulator_kernel ] ) across a series of rows of @xmath225 like in @xmath191 , for @xmath230 , and across @xmath167 columns of @xmath151 .",
    "@xmath231 is a @xmath232 diagonal matrix that represents @xmath229 evaluated at the zero - spacing parameter pair ( @xmath233 ) , @xmath230 .",
    "then , to predict a vector of weights at the new location , we use the conditional probability @xmath234 where @xmath235 @xmath236 these equations are also commonly referred to as kriging equations @xcite . though the notation is complex , the interpretation is straightforward : the probability distribution of a set of eigenspectra weights @xmath237 is a @xmath167-dimensional gaussian distribution whose mean and covariance are a function of @xmath151 , conditional upon the ( fixed ) values of @xmath208 and the squared exponential hyperparameters ( an example for a single @xmath170 is shown in figure  [ fig : gp_interp ] , right panel ) .    if we desired actual values of the interpolated weights , for example to reconstruct a model spectrum , we could simply draw a gaussian random variable @xmath237 from the probability distribution in eq .",
    "( [ eqn : weight_conditional ] ) .",
    "however , because we now know the probability distribution of the weight as a function of @xmath151 , we can rewrite our data likelihood function ( eq .  [ eqn : lnlikelihood ] ) in such a way that it is possible to analytically marginalize over all possible values of @xmath237 , and thus all probable spectral interpolations .",
    "up until this point , we have described the reconstruction of a spectrum as a linear combination of the eigenspectra that characterize the synthetic library ( figure  [ fig : pca_reconstruct ] ) .",
    "but in practice , that reconstructed spectrum must be further post - processed as detailed in section  [ subsec : postprocess ] . fortunately , because convolution is a linear operation , we can first post - process the raw eigenspectra according to @xmath14 , and then represent the reconstructed spectrum as a linear combination of these modified eigenspectra without loss of information . unfortunately , the doppler shift and resampling operations are not linear operations , and there will be some loss of information when trying to approximate them in this manner .",
    "however , we find that in practice when the synthetic spectra are oversampled relative to the instrument resolution by a reasonable factor , the flux error due to resampling is smaller than 0.2%  across all pixels , and thus any effect of that information loss is negligible . for notational compactness , we let @xmath238 , @xmath239 , and @xmath240 represent the post - processed eigenspectra , with an implied dependence on the current values of the extrinsic observational parameters ( @xmath241 ) and the polynomial nuisance parameters @xmath242 . now , the model spectrum is a function of the vector of eigenspectra weights @xmath243 where @xmath244 because the gaussian process describes a probability distribution of the weights , we now have a distribution of possible ( interpolated ) models and the likelihood function ( eq .  [ eqn : likelihood ] ) is specified conditionally on the weights , @xmath245    the final task of designing the spectral emulator is to combine this data likelihood function with the posterior predictive distribution of the eigenspectra weights ( eq .  [ eqn : weight_conditional ] ) and then marginalize over the weights @xmath246 such that we are left with a modified posterior distribution of the data that incorporates the range of probable interpolation values for the model . to perform this multidimensional integral",
    ", we use a convenient lemma found in ( * ? ? ?",
    "* their appendix a ) : if the probability distributions of @xmath237 and @xmath247 are specified conditionally as in eq .",
    "[ eqn : weight_conditional ] and [ eqn : likelihood_conditional ] , respectively , then the marginal distribution ( eq .  [ eqn : marginal ] ) is @xmath248 where the dependence on the model parameters is now made explicit .",
    "we can couch this modified likelihood function in the form of eqn  [ eqn : lnlikelihood ] by rewriting @xmath249 @xmath250 @xmath251 where @xmath252 can be thought of as the  mean model spectrum \" given the model parameters , and the covariance matrix has been modified to account for the various probable manifestations of the model spectrum about that mean spectrum .",
    ", a.  s. , reyl , c. , schultheis , m. , & allard , f. 2010 , in sf2a-2010 : proceedings of the annual meeting of the french society of astronomy and astrophysics , ed . s.  boissier , m.  heydari - malayeri , r.  samadi , & d.  valls - gabaud , 275                                  , d.  b. , de silva , g. , freeman , k. , bland - hawthorn , j. , & hermes team .",
    "2012 , in astronomical society of the pacific conference series , vol .",
    "458 , galactic archaeology : near - field cosmology and the formation of the milky way , ed .",
    "w.  aoki , m.  ishigaki , t.  suda , t.  tsujimoto , & n.  arimoto , 421"
  ],
  "abstract_text": [
    "<S> we present a modular , extensible likelihood framework for spectroscopic inference based on synthetic model spectra . </S>",
    "<S> the subtraction of an imperfect model from a continuously sampled spectrum introduces covariance between adjacent datapoints ( pixels ) into the residual spectrum . for the high signal - to - noise data with large spectral range that is commonly employed in stellar astrophysics , that covariant structure can lead to dramatically underestimated parameter uncertainties ( and , in some cases , biases ) . </S>",
    "<S> we construct a likelihood function that accounts for the structure of the covariance matrix , utilizing the machinery of gaussian process kernels . </S>",
    "<S> this framework specifically address the common problem of mismatches in model spectral line strengths ( with respect to data ) due to intrinsic model imperfections ( e.g. , in the atomic / molecular databases or opacity prescriptions ) by developing a novel local covariance kernel formalism that identifies and self - consistently downweights pathological spectral line  outliers . \" by fitting many spectra in a hierarchical manner , these local kernels provide a mechanism to learn about and build data - driven corrections to synthetic spectral libraries . </S>",
    "<S> an open - source software implementation of this approach is available at http://iancze.github.io/starfish , including a sophisticated probabilistic scheme for spectral interpolation when using model libraries that are sparsely sampled in the stellar parameters . </S>",
    "<S> we demonstrate some salient features of the framework by fitting the high resolution @xmath0-band spectrum of wasp-14 , an f5 dwarf with a transiting exoplanet , and the moderate resolution @xmath1-band spectrum of gliese  51 , an m5 field dwarf . </S>"
  ]
}