{
  "article_text": [
    "the evolution of many complex systems in natural , economical , and social sciences is usually presented in the form of time series . in order to analyze time series , several of statistical measures have been introduced in the literature .",
    "these include such concepts as probability distributions  @xcite , autocorrelations  @xcite , multi - fractals  @xcite , complexity  @xcite , or entropy densities  @xcite .",
    "recently , it has been pointed out that the _ transfer entropy _ ( te ) is a very useful instrument in quantifying statistical coherence between time evolving statistical systems  @xcite . in particular , in schreiber s paper  @xcite it was demonstrated that te is especially expedient when global properties of time series are analyzed .",
    "prominent applications are in multivariate analysis of time series , including e.g. , study of multichannel physiological data or bivariate analysis of historical stock exchange indices .",
    "methods based on te have substantial computational advantages which are particularly important in analyzing a large amount of data . in all past works , including  @xcite , the emphasis has been on various generalizations of transfer entropies that were firmly rooted in the framework of shannon s information theory .",
    "these , so called shannonian transfer entropies are , indeed , natural candidates due to their ability to quantify in a non - parametric and in explicitly non - symmetric way the flow of information between two time series .",
    "an ideal testing ground for various te concepts are financial - market time series because of the immense amount of electronically recorded financial data .",
    "recently , economy has become an active research area for physicists .",
    "they have investigated stock markets using statistical - physics methods , such as the percolation theory , multifractals , spin - glass models , information theory , complex networks , path integrals , etc .. in this connection the name econophysics has been coined to denote this new hybrid field on the border between statistical physic and ( quantitative ) finance . in the framework of econophysics",
    "it has became steadily evident that the market interactions are highly nonlinear , unstable , and long - ranged .",
    "it has also became apparent that all agents ( e.g. , companies ) involved in a given stock market exhibit interconnectedness and correlations which represent important internal force of the market .",
    "typically one uses correlation functions to study the internal cross - correlations between various market activities .",
    "the correlation functions have , however , at least two limitations : first , they measure only linear relations , although it is clear that linear models do not faithfully reflect real market interactions .",
    "second , all they determine is whether two time series ( e.g. , two stock - index series ) have correlated movement .",
    "they , however , do not indicate which series affects which , or in other words , they do not provide any directional information about cause and effect .",
    "some authors use such concepts as time - delayed correlation or time - delayed mutual information in order to construct asymmetric `` correlation '' matrices with inherent directionality .",
    "this procedure is in many respects _ ad hoc _ as it does not provide any natural measure ( or quantifier ) of the information flow between involved series .    in the present paper we study multivariate properties of stock - index time with the help of econophysics paradigm . in order to quantify the information flow between two or more stock indices we generalize schreibers shannonian transfer entropy to rnyi s information setting . with this",
    "we demonstrate that the corresponding new transfer entropy provides more detailed information concerning the excess ( or lack ) of information in various parts of the underlying distribution resulting from updating the distribution on the condition that a second time series is known .",
    "this is particularly relevant in the context of financial time series where the knowledge of tale - part ( or marginal ) events such as spikes or sudden jumps bears direct implications , e.g. , in various risk - reducing formulas in portfolio theory .",
    "the paper is organized as follows : in section  [ sec2 ] we provide some information - theoretic background on shannon and rnyi entropies ( re s ) . in particular , we identify the _ conditional _ rnyi s entropy with the information measure introduced in ref .",
    "apart from satisfying the chain rule ( i.e. , rule of additivity of information ) the latter has many desirable properties that are to be expected from a conditional information measure .",
    "another key concept  the _ mutual _ rnyi entropy , is then introduced in a close analogy with shannon s case .",
    "the ensuing properties are also discussed .",
    "transfer _ entropy of schreiber is briefly reviewed in section  [ sec4 ] .",
    "there we also comment on effective transfer entropy of marchinski _",
    "et all_. the core quantity of this work  the rnyian _ transfer _ entropy ( rte ) , is motivated and derived in section  [ sec7 ] .",
    "in contrast to shannonian case , the rnyian transfer entropy is generally not positive semi - definite .",
    "this is because re non - linearly emphasizes different parts of involved probability density functions ( pdf s ) . with the help of campbell s coding theorem we show that the rte rates a gain / loss in risk involved in a next - time - step behavior in a given stochastic process , say @xmath2 , resulting from learning a new information , namely historical behavior of another ( generally cross - correlated ) process , say @xmath3 . in this view",
    "the rte can serve as a convenient _ rating _ factor of a _ riskiness _ in inter - connected markets .",
    "we also show that rnyian transfer entropy allows to amend spurious effects caused by a finite size of a real data set which in shannon s context must be , otherwise , solved by means of the surrogate data technique and ensuing effective transfer entropy . in section  [ sec10 ] we demonstrate the usefulness and formal consistency of rte by analyzing cross - correlations in various international stock markets . on a qualitative level we use",
    "@xmath4 simultaneously recorded data points of the eleven stock exchange indices , sampled at a daily ( end - of - trading day ) rate to construct the _ heat maps _ and _ net flows _ for both shannon s and rnyi s information flows . on a quantitative level",
    "we explicitly discuss time series from the dax and s&p500 market indices gathered on a minute - tick basis in the period from december 1990 till november 2009 in the german stock exchange market ( deutsche brse ) . presented calculations of rnyi and shannon transfer entropies",
    "are based on symbolic coding computation with the open source software @xmath5 .",
    "our numerical results imply that rte s among world markets are typically very asymmetric .",
    "for instance , we show that there is a strong surplus of an information flow from the asia - pacific region to both europe and the u.s .",
    "a surplus of the information flow can be also observed to exists from europe to the u.s . in this last case",
    "the substantial volume of transferred information comes from tail - part ( i.e. , risky part ) of underlying asset distributions .",
    "so , despite the fact that the u.s . contributes more than half of the world trading volume , this is not so with information flow .",
    "further salient issues , such as dependence of rte on rnyi s @xmath0 parameter or on the data block length are numerically also investigated . in this connection",
    "we find that the cross - correlation between dax and s&p500 has a long - time memory which is around 200 - 300 mins .",
    "this should be contrasted with typical memory of stock returns which are of the order of seconds or maximally few minutes .",
    "various remarks and generalizations are proposed in the concluding section  [ sec12 ] . for reader",
    "s convenience we give in appendix  a a brief dictionary of market indices used in the main text and in appendix  b we tabulate an explicit values of effective transfer entropies used in the construction of heat maps and net information flows .",
    "in order to express numerically an amount of information that is shared or transferred between various data sets ( e.g. , two or more random processes ) , one commonly resorts to information theory and especially to the concept of entropy . in this section",
    "we briefly review some essentials of shannon s and rnyi s entropy that will be needed in following sections .",
    "the entropy concept was originally introduced by clausius  @xcite in the framework of thermodynamics . by analyzing a carnot engine he was able to identify a new state function which never decreases in isolated systems .",
    "the microphysical origin of clausius phenomenological entropy was clarified more than @xmath6 years later in works of boltzman and ( yet later ) gibbs who associated clausius entropy with the number of allowed microscopic states compatible with a given observed macrostate .",
    "the ensuing _ boltzmann  gibbs entropy _ reads @xmath7 where @xmath8 is boltzmann s constant , @xmath2 is the set of all accessible microstates compatible with whatever macroscopic observable ( state variable ) one controls and @xmath9 denotes the number of microstates .",
    "it should be said that the passage from boltzmann  gibbs to clausius entropy is established only when the conditional extremum @xmath10 of @xmath11 subject to the constraints imposed by observed state variables is inserted back into @xmath11 .",
    "only when this _ maximal entropy prescription _  @xcite is utilized @xmath11 turns out to be a thermodynamic state function and not mere functional on a probability space .    in information theory , on the other hand , the interest was in an optimal coding of a given source data . by _",
    "optimal code _ is meant the shortest averaged code from which one can uniquely decode the source data .",
    "optimality of coding was solved by shannon in his 1948 seminal paper  @xcite . according to shannon s _",
    "source coding theorem _",
    "@xcite , the quantity @xmath12 corresponds to the averaged number of bits needed to optimally encode ( or  zip \" ) the source dataset @xmath2 with the source probability distribution @xmath13 . on a quantitative level ( [ ii.2.a ] )",
    "represents ( in bits ) the minimal number of binary ( yes / no ) questions that brings us from our present state of knowledge about the system @xmath2 to the one of certainty  @xcite .",
    "it should be stressed that in shannon s formulation @xmath2 represents a discrete set ( e.g. , processes with discrete time ) , and this will be also the case here .",
    "apart from the foregoing _ operational _ definitions , eq .",
    "( [ ii.2.a ] ) has also several axiomatic underpinnings .",
    "axiomatic approaches were advanced by shannon  @xcite , khinchin  @xcite , fadeev  @xcite an others  @xcite . the quantity ( [ ii.2.a ] )",
    "has became known as shannon s entropy ( se ) .",
    "there is an intimate connection between boltzmann ",
    "gibbs entropy and shannon s entropy .",
    "in fact , thermodynamics can be viewed as a specific application of shannon s information theory : the thermodynamic entropy may be interpreted ( when rescaled to `` bit '' units ) as the amount of shannon information needed to define the detailed microscopic state of the system , that remains  uncommunicated \" by a description that is solely in terms of thermodynamic state variables  @xcite .    among important properties of se is its concavity in @xmath14 , i.e. for any pair of distributions @xmath14 and @xmath15 , and a real number @xmath16 holds @xmath17 eq .  ( [ i.19.a ] ) follows from jensen s inequality and a convexity of @xmath18 for @xmath19 .",
    "concavity is an important concept since it ensures that any maximizer found by the methods of the differential calculus yields an absolute maximum rather than a relative maximum or minimum or saddle point . at the same time it is just a sufficient ( i.e. , not necessary ) condition guarantying a unique maximizer .",
    "it is often customary to denote se of the source @xmath2 as @xmath20 rather than @xmath21 .",
    "note that se is generally not convex in @xmath2 !",
    "it should be stressed that the entropy ( [ ii.2.a ] ) really represents a self - information : the information yielded by a random process about itself .",
    "a step further from a self - information offers the _",
    "joint entropy _ of two random variables @xmath2 and @xmath3 which is defined as @xmath22 and which represents the amount of information gained by observing jointly two ( generally dependent or correlated ) statistical events .",
    "a further concept that will be needed here is the _ conditional entropy _ of @xmath2 given @xmath3 , which can be motivated as follows : let us have two statistical events @xmath2 and @xmath3 and let event @xmath3 has a sharp value @xmath23 , then the gain of information obtained by observing @xmath2 is @xmath24 here the conditional probability @xmath25 . for general random @xmath3 one defines the conditional entropy as the averaged shannon entropy yielded by @xmath2 under the assumption that the value of @xmath3 is known , i.e. @xmath26 from ( [ ii.a.5a ] ) , in particular , follows that @xmath27 identity ( [ ii.2.6.a ] ) is known as additivity ( or chain ) rule for shannon s entropy . in statistical thermodynamics",
    "this rule allows to explain , e.g. , gibbs paradox .",
    "applying eq .",
    "( [ ii.2.6.a ] ) iteratively , we obtain : @xmath28 another relevant quantity that will be needed is the _ mutual information _ between @xmath2 and @xmath3 .",
    "this is defined as : @xmath29 and can be equivalently written as @xmath30 this shows that the mutual information measures the average reduction in uncertainty ( i.e. , gain in information ) about @xmath2 resulting from observation of @xmath3 .",
    "of course , the amount of information contained in @xmath2 about itself is just the shannon entropy : @xmath31    notice also that from eq .",
    "( [ eq : mi ] ) follows @xmath32 and so @xmath2 provides the same amount of information on @xmath3 as @xmath3 does on @xmath2 . for this reasons",
    "the mutual information is not a useful measure to quantify a flow of information .",
    "in fact , the flow of information should be by its very definition directional .    in the following we will also find useful the concept of _ conditional mutual entropy _ between @xmath2 and @xmath3 given @xmath33 which is defined as",
    "@xmath34 the latter quantifies the averaged mutual information between @xmath2 and @xmath3 provided that @xmath33 is known .",
    "applying ( [ eq : condmii ] ) and ( [ eq : mi2 ] ) iteratively we may write @xmath35 & = & \\ i(x;y_1\\cap \\cdots",
    "\\cap y_n \\cap z_1\\cap \\cdots \\cap z_m)\\nonumber \\\\ & - &   \\ i(x;z_1\\cap \\cdots",
    "\\cap z_m ) \\ , .",
    "\\label{eq : condmiiii}\\end{aligned}\\ ] ]    for further details on the basic concepts of shannon s information theory , we refer the reader to classical books , e.g. , ash  @xcite and , more recently , csiszr and shields  @xcite .",
    "rnyi introduced in refs .",
    "@xcite a one - parameter family of information measures presently known as _ rnyi entropies _  @xcite . in practice , however , only a singular name  rnyi s entropy  is used .",
    "re of order @xmath0 @xmath36 of a distribution @xmath37 on a finite set @xmath38 is defined as @xmath39 for re ( [ renyi ] ) one can also formulate source coding theorem .",
    "while in the shannon case the cost of a code - word is a linear function of the length  so the optimal code has a minimal cost out of all codes , in the rnyi case the cost of a code - word is an exponential function of its length  @xcite .",
    "this is , in a nutshell , an essence of the so - called campbell s coding theorem ( cct ) . according to this re corresponds to the averaged number of bits needed to optimally encode the discrete source @xmath2 with the probability @xmath13 , provided that the codeword - lengths are exponentially weighted  @xcite . from the form ( [ renyi ] ) one can easily see that for @xmath40 re depends more on the probabilities of the more probable values and less on the improbable ones .",
    "this dependence is more pronounced for higher @xmath0 . on the other hand , for @xmath41 marginal events",
    "are accentuated with decreasing @xmath0 . in this connection",
    "we should also point out that campbell s coding theorem for re is equivalent to shannon s coding theorem for se provided one uses instead of @xmath42 the _ escort distribution _",
    "@xcite : @xmath43 the pdf @xmath44 was first introduced by rnyi  @xcite and in the physical context brought by beck , schlgl , kadanoff and others ( see , e.g. , refs .",
    "@xcite ) . note ( cf .",
    "[ fig2 ] )     for event probability @xmath45 and varying rnyi s parameter @xmath0 .",
    "arrows indicate decreasing values of @xmath0 for @xmath41 ( dark arrow ) or increasing values of @xmath0 for @xmath40 ( lighter arrow).,width=377 ]    ( 20,7 ) ( 105,65 ) @xmath45 ( -135,185)@xmath46    that for @xmath40 the escort distribution emphasizes the more probable events and de - emphasizes more improbable ones .",
    "this trend is more pronounced for higher values of @xmath0 .",
    "for @xmath41 the escort distribution accentuates more improbable ( i.e. , marginal or rare ) events .",
    "this dependence is more pronounced for decreasing @xmath0 . this fact is clearly seen on fig .  [ fig1 ] .    :",
    "@xmath47 . , width=302 ]    ( 20,7 ) ( 90,80 ) @xmath0 ( -35,55 ) @xmath45 ( 125,155)@xmath48    so by choosing different @xmath0 we can  scan \" or  probe \" different parts of the involved pdf s .",
    "it should be stressed that apart from cct , re has yet further operational definitions , e.g. , in the theory of guessing  @xcite , in the buffer overflow problem  @xcite or in the theory of error block coding  @xcite .",
    "re is also underpinned with various axiomatics  @xcite . in particular",
    ", it satisfies identical khinchin axioms  @xcite as shannon s entropy save for the additivity axiom ( chain rule )  @xcite : @xmath49 where the conditional entropy @xmath50 is defined with the help of the escort distribution ( [ ii.15.a ] ) ( see , e.g. , refs .",
    "@xcite ) . for @xmath51 re reduces to the shannon entropy : @xmath52 as one can easily verify with lhospital s rule .",
    "we define the _ joint rnyi entropy _ ( or the _ joint entropy _ of order @xmath0 ) for two random variables @xmath38 and @xmath53 in a natural way as : @xmath54 the _ conditional entropy _ of order @xmath0 of @xmath2 given @xmath3 is similarly as in the shannon case defined as the averaged rnyi s entropy yielded by @xmath2 under the assumption that the value of @xmath3 is known . as shown in refs .",
    "@xcite this has the form @xmath55 & = & \\",
    "\\frac{1}{1-q}\\log_2 \\frac{\\sum_{x\\ \\ !",
    "\\in \\ \\ ! { x } , \\ \\ ! y \\ \\ !",
    "{ y}}p^{\\,q}(x , y)}{\\sum_{y\\ \\ !",
    "\\in \\ \\ ! { y } } q^{\\,q}(y)}\\ , .",
    "\\label{re.condentropy}\\end{aligned}\\ ] ] in this connection it should be mentioned that several alternative definitions of the conditional re exist ( see , e.g. , refs .",
    "@xcite ) , but the formulation ( [ re.condentropy ] ) differs from other versions in a few important ways that will show up to be desirable in the following considerations .",
    "the conditional entropy defined in ( [ re.condentropy ] ) has the following important properties , namely  @xcite     : :    @xmath56 , where    @xmath57 is a number of elements in @xmath2 ,  : :    @xmath58 only when @xmath3 uniquely    determines @xmath2 ( i.e. , no gain in information ) ,  : :    @xmath59 ,  : :    when @xmath2 and @xmath3 are independent then    @xmath60 .    unlike shannon s case one can",
    "not , however , deduce that the equality @xmath61 implies independency between event @xmath2 and @xmath3 .",
    "also the inequality @xmath62 ( i.e. , an extra knowledge about @xmath3 lessens our ignorance about @xmath2 ) does not hold here in general  @xcite .",
    "the latter two properties may seem as a serious flaw .",
    "we will now argue that this is not the case and , in fact , it is even desirable .",
    "first , in order to understand why @xmath61 does not imply independency between @xmath2 and @xmath3 we define the information - distribution function @xmath63 which represents the total probability caused by events with information content @xmath64 . with this",
    "we have @xmath65 and thus @xmath66 taking the inverse laplace transform with the help of the so - called _",
    "post s inversion formula _",
    "@xcite we obtain @xmath67\\right|_{q \\ = \\",
    "k/(x \\ln 2 ) + 1 } .",
    "\\label{iii.b.20a}\\end{aligned}\\ ] ] analogous relation holds also for @xmath68 and associated @xmath50 . as a result",
    "we see that when working with @xmath69 of different orders we receive much more information on underlying distribution than when we restrict our investigation to only one @xmath0 ( e.g. , to only shannon s entropy ) .",
    "in addition , eq .  ( [ iii.b.20a ] ) indicates that we need all @xmath70 ( or equivalently all @xmath71 , see  @xcite ) in order to uniquely identify the underlying pdf .    in view of eq .",
    "( [ iii.b.20a ] ) we see that the equality between @xmath50 and @xmath72 at some neighborhood of @xmath0 merely implies that @xmath73 for some @xmath74 .",
    "this naturally does not ensure independency between @xmath2 and @xmath3 .",
    "we need equality @xmath75 for all @xmath40 ( or for all @xmath71 ) in order to secure that @xmath73 holds for all @xmath74 which would in turn guarantee that @xmath76 .",
    "therefore , all re with @xmath40 ( or all with @xmath71 ) are generally required to deduce from @xmath77 an independency between @xmath2 and @xmath3 . in order to understand the meaning of the inequality @xmath78 we first introduce the concept of mutual information .",
    "the _ mutual information of order _",
    "@xmath0 between @xmath2 and @xmath3 can be defined as ( cf .",
    "( [ eq : mi2 ] ) ) @xmath79 & = & \\ s_q^{(r)}(x)\\ + \\",
    "s_q^{(r)}(y ) \\ - \\ s_q^{(r)}(x\\cap y ) \\",
    ", , \\label{re.mutinf}\\end{aligned}\\ ] ] which explicitly reads @xmath80 & = & \\ \\frac{1}{1-q}\\log_2 \\frac{\\sum_{x\\ \\ !",
    "\\in \\ \\ ! { x } , \\ \\ ! y \\ \\ !",
    "\\in \\ \\ ! { y } } q^{\\,q}(y ) p^{\\,q}(x)}{\\sum_{x\\ \\ ! \\in \\ \\ ! { x } , \\ \\ !",
    "\\in \\ \\ ! { y}}q^{\\,q}(y)p^{\\,q}(x|y ) } \\ , .",
    "\\label{ii.20a}\\end{aligned}\\ ] ] note that we have again the symmetry relation @xmath81 as well as the consistency condition @xmath82 .",
    "so similarly as in the shannon case , rnyi s mutual information formally quantifies the average reduction in uncertainty ( i.e. , gain in information ) about @xmath2 that results from learning the value of @xmath3 , or vice versa .    from eq .",
    "( [ re.mutinf ] ) we see that the inequality in question , i.e. , @xmath83 implies @xmath84 .",
    "according to ( [ ii.20a ] ) this can be violated only when @xmath85 & & \\sum_{x\\ \\ !",
    "\\in \\ \\ ! { x } }",
    "p^{\\,q}(x ) \\ < \\sum_{x\\ \\ ! \\in \\ \\ ! { x } } \\langle { \\mathcal{p}}^{\\,q}(x|y)\\rangle_q \\;\\;\\;\\;\\;\\ ; \\mbox{for } \\;\\;\\;\\ ; 0 < q < 1\\ , . \\label{2.b.26a}\\end{aligned}\\ ] ] here @xmath86 is an average with respect to the escort distribution @xmath87 ( see eq .",
    "( [ ii.15.a ] ) ) .    by taking into account properties of the escort distribution",
    ", we can deduce that @xmath88 when a larger probability events of @xmath2 receive by learning @xmath3 a lower value .",
    "as for the marginal events of @xmath2 , these are by learning @xmath3 indeed enhanced , but the enhancement rate is smaller than the suppression rate of large probabilities . for instance , this happens when @xmath89 for @xmath90 the inequality ( [ 2.b.28a ] ) ensures that @xmath91 holds . the left inequality in ( [ 2.b.28a ] )",
    "saturates when @xmath92 , see also fig .",
    "[ fig2a ] .    for @xmath41",
    "is the situation analogous . here",
    "properties of the escort distribution imply that @xmath93 when marginal events of @xmath2 get by learning @xmath3 a higher probability .",
    "the suppression rate for large ( i.e. close - to - peak ) probabilities is now smaller than the enhancement rate of marginal events .",
    "this happens , for example , for distributions , @xmath94 with @xmath95 fulfilling again the inequality ( [ 2.b.28a ] ) .",
    "this can be also directly seen from fig .",
    "[ fig2a ] when we revert the sign of @xmath96 .",
    "when we set @xmath97 then both inequalities ( [ 2.b.26a ] ) are simultaneously satisfied yielding @xmath98  as it should .",
    "in contrast to a shannonian case where the mutual information quantifies the average reduction in uncertainty resulting from observing / learning a further information , in the rnyi case we should use campbell s coding theorem in order to properly understand the meaning of @xmath96 .",
    "( 20,7 ) ( 118,235)@xmath0 ( -100,85)@xmath99    according to the cct @xmath72 corresponds to the minimal average cost of a coded message with a non - linear ( exponential ) weighting / pricing of codeword - lengths . while according to shannon we never increase ignorance by learning @xmath3 ( i.e. , possible correlations between @xmath2 and @xmath3 can only reduce the entropy ) , in rnyi s setting extra knowledge about @xmath3 might easily increase the minimal price of coding @xmath2 because of the nonlinear pricing . since the cct penalizes long codewords which in shannon s coding have low probability , the price of the @xmath100 code may easily increase , as we have seen in examples ( [ ii.20aaa ] ) and ( [ ii.20aaaa ] ) .    in the key context of financial time",
    "series , the risk valuation of large changes such as spikes or sudden jumps is of a crucial importance , e.g. , in various risk - reducing formulas in portfolio theory .",
    "the rle of campbell s pricing can in these cases be interpreted as a risk - rating method which puts an exponential premium on rare ( i.e. , risky ) asset fluctuations . from this point of view the mutual information @xmath99 represents a _ rating factor _ which rates a gain / loss in risk in @xmath2 resulting from learning a new information , namely information about @xmath3 .",
    "the _ conditional mutual information _ of order @xmath0 between @xmath2 and @xmath3 given _ z _ is defined as @xmath101 note that because of a validity of the chain rule ( [ iii.b.15a ] ) , relations ( [ ii.a.8a ] ) and ( [ eq : condmiiii ] ) also hold true for the re .    to close this section",
    ", we shall stress that information entropies are primarily important because there are various coding theorems which endow them with an operational ( that is , experimental ) meaning , and not because of intuitively pleasing aspects of their definitions . while coding theorems do exist both for the shannon entropy and the rnyi entropy",
    "there are ( as yet ) no such theorems for tsallis , kaniadakis , naudts and other currently popular entropies .",
    "the information - theoretic significance of such entropies is thus not obvious . since the information - theoretic aspect of entropies is of a crucial importance here , we will in the following focus only on the se and the re .",
    "as seen in section  [ sec2b ] , the mutual information @xmath102 quantifies the decrease of uncertainty about @xmath2 caused by the knowledge of @xmath3 .",
    "one could be thus tempted to use it as a measure of an informational transfer in general complex systems . a major problem , however , is that shannon s mutual information contains no inherent directionality since @xmath103 .",
    "some early attempts tried to resolve this complication by artificially introducing the directionality via time - lagged random variables . in this way",
    "one may define , for instance , the _ time - lagged mutual _ ( or _ directed kullback  leibler _ ) _ information _ as @xmath104 the later describes the average gain of information when replacing the product probability @xmath105 by the joint probability @xmath106 .",
    "so the information gained is due to cross - correlation effect between random variables @xmath107 and @xmath108 ( respectively , @xmath109 ) .",
    "it was , however , pointed out in ref .",
    "@xcite that prescriptions such as ( [ iii.a.1.a ] ) , though directional , also take into account some part of the information that is statically shared between the two random processes @xmath2 and @xmath3 . in other words , these prescriptions do not produce statistical dependences that truly originate only in the stochastic random process @xmath3 , but they do include the effects of a common history ( such as , for example , in the case of a common external driving force ) .",
    "for this reason , schreiber introduced in ref .",
    "@xcite the concept of ( shannonian ) transfer entropy ( ste ) .",
    "the latter , apart from directionality , accounts only for the cross - correlations between statistical time series @xmath2 and @xmath3 whose genuine origin is in the  source \" process @xmath3 .",
    "the essence of the approach is the following .",
    "let us have two time sequences described by stochastic random variables @xmath107 and @xmath108 .",
    "let us assume further that the time steps ( data ticks ) are discrete with the size of an elementary time lag @xmath110 and with @xmath111 ( @xmath112 is a reference time ) .",
    "the transfer entropy @xmath113 can then be defined as @xmath114 & = & i(x_{t_{m+1 } } ; x_{t_1}\\cap \\cdots \\cap",
    "x_{t_{m}}\\cap y_{t_{m - l+1}}\\cap \\cdots \\cap y_{t_{m } } ) - i(x_{t_{m+1}};x_{t_1 } \\cap \\cdots \\cap",
    "x_{t_{m } } ) \\ , .\\nonumber \\\\ & & \\mbox{\\hspace{-15 mm } } \\label{iii.a.23a}\\end{aligned}\\ ] ] the last line of ( [ iii.a.23a ] ) indicates that @xmath113 represents the following .",
    "[ cols= \" < , < \" , ]",
    "in this appendix we specify explicit values of effective transfer entropies that are employed in section  [ sec10 ] .",
    "these are calculated for alphabet with @xmath115 .            j .- s .",
    "yang , w.  kwak , t.  kaizoji and i .-",
    "kim , _ eur .",
    "j.  _ * b61 * ( 2008 ) 389 ; k.  matal , m.  pal , h.  salunkay and h.e .",
    "stanley , _ europhys . lett .",
    "_ * 66 * ( 2004 ) 909 ; h.e .",
    "stanley , l.a.n .",
    "amaral , x.  gabaix , p.  gopikrishnan and v.  plerou , _",
    "physica  _ * a299 * ( 2001 ) 1 .",
    "chaudy and j.b .",
    "edinburgh mathematical notes _  * 43 * ( 1960 ) 7 ; for h. tverberg s , p.m. lee s or d.g .",
    "kendall s axiomatics of shannon s entropy see e.g. , s. guias , _ information theory with applications _ ,",
    "( mcgraw hill , new york , 1977 ) .",
    "this exponential weighting is also known as a kolmogorov ",
    "nagumo averaging . while the linear averaging is given by @xmath118 , the exponential weighting ia defined as @xmath119 with @xmath120 .",
    "the @xmath121 factor is known as campbell exponent .",
    "one can map @xmath69 with @xmath122 to @xmath69 with @xmath123 via duality @xmath124 that exists between @xmath48 and @xmath37 .",
    "in fact , we can observe that @xmath125 and @xmath126 .",
    "so @xmath69 with @xmath122 and @xmath69 with @xmath127 carry equal amount of information .",
    "y.  liu , p.  cizeau , m.  meyer , c .- k .",
    "peng , h.e .",
    "stanley , _ physica _ * a245 * ( 1997 ) 437 ; _ physica _ * a245 * 441 ; y.  liu , p.  gopikrishnan , p.  cizeau , m.  mayer , c .- k .",
    "peng , h.e .",
    "stanley , _ phys .",
    "* e60 * ( 1999 ) 1390 ."
  ],
  "abstract_text": [
    "<S> in this paper , we quantify the statistical coherence between financial time series by means of the rnyi entropy . with the help of campbell s coding theorem </S>",
    "<S> we show that the rnyi entropy selectively emphasizes only certain sectors of the underlying empirical distribution while strongly suppressing others . </S>",
    "<S> this accentuation is controlled with rnyi s parameter @xmath0 . to tackle the issue of the information flow between time series we formulate the concept of rnyi s transfer entropy as a measure of information that is transferred only between certain parts of underlying distributions . </S>",
    "<S> this is particularly pertinent in financial time series where the knowledge of marginal events such as spikes or sudden jumps is of a crucial importance . </S>",
    "<S> we apply the rnyian information flow to stock market time series from @xmath1 world stock indices as sampled at a daily rate in the time period 02.01.1990 - 31.12.2009 . corresponding _ heat maps _ and _ net information flows _ are represented graphically . </S>",
    "<S> a detailed discussion of the transfer entropy between the dax and s&p500 indices based on minute tick data gathered in the period from 02.04.2008 to 11.09.2009 is also provided . </S>",
    "<S> our analysis shows that the bivariate information flow between world markets is strongly asymmetric with a distinct information surplus flowing from the asia  pacific region to both european and us markets . </S>",
    "<S> an important yet less dramatic excess of information also flows from europe to the us . </S>",
    "<S> this is particularly clearly seen from a careful analysis of rnyi information flow between the dax and s&p500 indices .    </S>",
    "<S> pacs numbers : 89.65.gh , 89.70.cf , 02.50.-r    keywords : econophysics ; rnyi entropy ; information transfer ; financial time series    # 1 # 1 </S>"
  ]
}