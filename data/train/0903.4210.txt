{
  "article_text": [
    "during the last decade , cosmology has advanced from an era of largely qualitative questions  is the universe flat , open or closed ?",
    ", does dark energy exist in the universe ? , etc .",
    " to one where precision determinations of many of the universe s properties are possible .",
    "we have cosmological models capable of explaining the detailed observations available , and whose parameters are beginning to be pinned down at the ten percent , and in some cases one percent , level @xcite . nevertheless , quality cosmological data are an expensive resource and it is imperative to make the best possible use of them .",
    "this implies use of the best available statistical tools in order to obtain accurate and robust conclusions .    for around a decade now",
    ", the established leading cosmological model considers five material constituents : baryons ( taken , imprecisely , to include electrons ) , photons , neutrinos , cold dark matter ( cdm ) , and dark energy .",
    "the simplest model for dark energy , a cosmological constant @xmath0 , is in excellent agreement with observations , and the model is then known as a @xmath0cdm model .",
    "the most important constraints come from the evolution of cosmic structures .",
    "these are seeded by small initial density perturbations , which in the standard cosmological model are taken as adiabatic , gaussian , and nearly scale - invariant , as predicted by the simplest models of cosmological inflation @xcite .    this model is supported and constrained by a series of cosmological observations .",
    "most important are measurements of cosmic microwave background ( cmb ) anisotropies , particularly by the wilkinson microwave anisotropy probe ( wmap ) as shown in figure  [ f : liddle1 ] .",
    "typical analyses also incorporate other data , such as galaxy clustering data , the luminosity distance ",
    "redshift relation of type ia supernovae , and direct measures of the hubble constant .",
    "the region of parameter space where the @xmath0cdm model matches all those data is often referred to as the concordance model .    in its very simplest incarnation ,",
    "the photon density is taken to be well measured by the cmb temperature , neutrinos to be nearly massless , and the universe spatially flat .",
    "the model then features only four fundamental parameters : the hubble parameter @xmath1 , the densities of baryons @xmath2 and cdm @xmath3 , and the amplitude of primordial density perturbations @xmath4 .",
    "in addition , a comparison with data will usually include extra phenomenological parameters , which in principle can be computed from the above but which in practice can not be reliably .",
    "for cosmic microwave background studies , the optical depth @xmath5 , measuring the fraction of cmb photons scattering from ionized gas at low redshift , is needed , while use of galaxy clustering data may require inclusion of the galaxy bias parameter @xmath6 which relates galaxy clustering to dark matter clustering .    beyond those basic parameters ,",
    "cosmologists hope that future investigations will uncover new physical processes , permitting extra parameters to be incorporated and measured . in some cases , it is more or less certain that the parameter is relevant and only a matter of time before observational sensitivity becomes sufficient .",
    "examples here are neutrino masses and the cosmic helium fraction ( though the latter is again in principle computable from other parameters , independent verification of its value would be an important consistency check ) .    much more numerous , though , are parameters describing effects which may or may not be relevant to our universe .",
    "an extensive list is given , for instance , in ref .",
    "are the primordial perturbations precisely scale invariant , or do they have a scale dependence quantified by the spectral index @xmath7 ? do primordial gravitational waves exist , as predicted by inflation ? does the dark energy density evolve with time ? are there cosmic strings in the universe ?",
    "are the initial perturbations really adiabatic and gaussian ?",
    "a fuller account of these parameters can be found for instance in ref .",
    "@xcite .    in summary , creation of precision cosmological models",
    "is an ongoing process with two distinct goals .",
    "one is to determine the set of parameters , i.e.  physical processes , necessary to describe the available observations .",
    "the second is to determine the preferred values of these parameters .",
    "we can then pursue the ultimate aim of relating cosmological observations to underlying fundamental physics .",
    "inference is the method by which we translate experimental / observational information into constraints on our mathematical models .",
    "the model is a representation of the physical processes that we believe are relevant to the quantities we plan to observe . to be useful",
    ", the model must be sufficiently sophisticated as to be able to explain the data , and simple enough that we can obtain predictions for observational data from it in a feasible time . at present",
    "these conditions are satisfied in cosmology , with the best models giving an excellent representation of that data , though the computation of theoretical predictions for a representative set of models is a supercomputer class problem .",
    "particularly valued are models which are able to make distinctive predictions for observations yet to be made , though nature is under no obligation to behave distinctively .",
    "the data which we obtain may be subject only to experimental uncertainty , or they may also have a fundamental statistical uncertainty due to the random nature of underlying physical processes . both types of data arise in cosmology .",
    "for instance , the present expansion rate of the universe ( the hubble constant ) , could in principle be measured to near - arbitrary accuracy with sufficiently advanced instrumentation . by contrast , the detailed pattern of cosmic microwave anisotropies , as measured by wmap , is not believed to be predictable even in principle , being attributed to a particular random realization of quantum processes occurring during inflation @xcite .",
    "observers at different locations in the universe see different patterns in the cmb sky , the cosmological information being contained in statistical measures of the anisotropies such as the power spectrum .",
    "observers at any particular location , such as ourselves , can witness only our own realization and there is an inherent statistical uncertainty , _ cosmic variance _ , that we can not overcome , but which fortunately can be modelled and incorporated in addition to measurement uncertainty .",
    "a model will typically not make unique predictions for observed quantities ; those predictions will instead depend on some number of _ parameters _ of the model .",
    "examples of cosmological parameters are the present expansion rate of the universe , and the densities of the various constituents such as baryons , dark matter , etc .",
    "such parameters are not ( according to present understanding , anyway ) predictable from some fundamental principle ; rather , they are to be determined by requiring that the model does fit the data to hand . indeed , determining the values of such parameters is often seen as the primary goal of cosmological observations , and chapter  [ c : cospar ] is devoted to this topic .    at a more fundamental level",
    ", several different models might be proposed as explanations of the observational data .",
    "these models would represent alternative physical processes , and as such would correspond to different _ sets _ of parameters that are to be varied in fitting to the data .",
    "it may be that the models are nested within one another , with the more complex models positing the need to include extra physical processes in order to explain the data , or the models may be completely distinct from one another .",
    "an example of nested models in cosmology is the possible inclusion of a gravitational wave contribution to the observed cmb anisotropies .",
    "an example of disjoint models would be the rival explanations of dark energy as caused by scalar field dynamics or by a modification to the gravitational field equations .",
    "traditionally , the choice of model to fit to the data has been regarded as researcher - driven , hopefully coming from some theoretical insight , with the model to be validated by some kind of goodness - of - fit test .",
    "more recently , however , there has been growing interest in allowing the data to distinguish between competing models .",
    "this topic , _ model selection _ or model comparison , is examined in chapter  [ c : modsel ] .",
    "the comparison of model prediction to data is , then , a statistical inference problem where uncertainty necessarily plays a role . while a variety of techniques exist to tackle such problems , within cosmology one paradigm dominates",
    "bayesian inference_. this article will therefore focus almost exclusively on bayesian methods , with only a brief account of alternatives at the end of this section .",
    "the dominance of the bayesian methodology in cosmology sets it apart from the traditional practice of particle physicists , though there is now increasing interest in applying bayesian methods in that context ( e.g.  ref .",
    "@xcite ) .",
    "the bayesian methodology goes all the way back to thomas bayes and his theorem , posthumously published in 1763 @xcite , followed soon after by pioneering work on probability by laplace .",
    "the technical development of the inference system was largely carried out in the first half of the 20th century , with jeffreys textbook @xcite the classic source . for several decades afterwards progress was held up due to an inability to carry out the necessary calculations , and only in the 1990s did use of the methodology become widespread with the advent of powerful multiprocessor computers and advanced calculational algorithms .",
    "initial applications were largely in the fields of social science and analysis of medical data , the volume edited by gilks et al .",
    "@xcite being a particularly important source .",
    "the publication of several key textbooks in the early 21st century , by jaynes @xcite , mackay @xcite and gregory @xcite , the last of these being particularly useful for physical scientists seeking to apply the methods , cemented the move of such techniques to the mainstream .",
    "an interesting history of the development of bayesianism is given in ref .",
    "@xcite .",
    "the essence of the bayesian methodology is to assign probabilities to all quantities of interest , and to then manipulate those probabilities according to a series of rules , amongst which bayes theorem is the most important .",
    "the aim is to update our knowledge in response to emerging data .",
    "an important implication of this set - up is that it requires us to specify what we thought we knew _ before _",
    "the data was obtained , known as the _",
    "prior probability_. while all subsequent steps are algorithmic , the specification of the prior probability is not , and different researchers may well have different views on what is appropriate .",
    "this is often portrayed as a major drawback to the bayesian approach .",
    "i prefer , however , to argue the opposite  that the freedom to choose priors is the opportunity to express physical insight . in any event , one needs to check that one s result is robust under reasonable changes to prior assumptions .",
    "an important result worth bearing in mind is a theorem of cox @xcite , showing that bayesian inference is the unique consistent generalization of boolean logic in the presence of uncertainty .",
    "jaynes in particular sees this as central to the motivation for the bayesian approach @xcite .    in abstract form",
    ", bayes theorem can be written as @xmath8 where a vertical line indicates the conditional probability , usually read as ` the probability of b given a ' . here",
    "@xmath9 and @xmath10 could be anything at all , but let s take @xmath9 to be the set of data @xmath11 and @xmath10 to be the parameter values @xmath12 ( where @xmath12 is the @xmath13-dimensional vector of parameters being varied in the model under consideration ) , hence writing @xmath14    in this expression , @xmath15 is the prior probability , indicating what we thought the probability of different values of @xmath12 was before we employed the data d. one of our objectives is to use this equation to obtain the _ posterior probability _ of the parameters given the data , @xmath16 .",
    "this is achieved by computing the _ likelihood _",
    "@xmath17 , often denoted @xmath18 with the dependence on the dataset left implicit .",
    "the principal alternative to the bayesian method is usually called the _ frequentist approach _ , indeed commonly a dichotomy is set up under which any non - bayesian method is regarded as frequentist .",
    "the underpinning concept is that of sampling theory , which refers to the frequencies of outcomes in random repeatable experiments ( often caricatured as picking coloured balls from urns ) . according to mackay @xcite , the principal difference between the systems is that frequentists apply probabilities only to random variables , whereas bayesians additionally use probabilities to describe inference .",
    "frequentist analyses commonly feature the concepts of _ estimators _ of statistical quantities , designed to have particular sampling properties , and _",
    "null hypotheses _ which are set up in the hope that data may exclude them ( though without necessarily considering what the preferred alternative might be ) .",
    "an advantage of frequentist methods is that they avoid the need to specify the prior probability distribution , upon which different researchers might disagree .",
    "notwithstanding the bayesian point - of - view that one should allow different researchers to disagree on the basis of prior belief , this means that the frequentist terminology can be very useful for expressing results in a prior - independent way , and this is the normal practice in particle physics .",
    "a drawback of frequentist methods is that they do not normally distinguish the concept of a model with a fixed value of a parameter , versus a more general model where the parameter happens to take on that value ( this is discussed in greater detail below in section  [ c : modsel ] ) , and they find particular difficulties in comparing models which are not nested .",
    "in cosmological parameter estimation , we take for granted that we have a dataset d , plus a model with parameter vector @xmath12 from which we can extract predictions for those data , in the form of the likelihood @xmath19 .",
    "additionally , we will have a prior distribution for those parameters , representing our knowledge before the data was acquired . while this could be a distribution acquired from analyzing some previous data , more",
    "commonly cosmologists take the prior distribution to be flat , with a certain range for each parameter , and reanalyze from scratch using a compilation of all data deemed to be useful .",
    "our aim is then to figure out the parameter values which give the best fit to the data , or , more usefully , the region in parameter space within which the fit is acceptable , i.e.  to find the maximum likelihood value and explore the shape of the likelihood in the region around it . in many cases",
    "one can hope that the likelihood takes the form of a multi - variate gaussian , at least as long as one does nt stray too far from the maximum .",
    "the task then is to find the high - likelihood regions of the function @xmath18 , which sounds straightforward .",
    "however , there are various obstacles    * the likelihood function may be extremely sharply peaked , and it may have several maxima masquerading as the true maximum . * the parameter space may have a high dimensionality , cosmological examples often having 6 to 10 parameters independently varying .",
    "* there may be parameter degeneracies , where likelihood varies only weakly , or not at all , along some direction in parameter space . * the evaluations of the likelihood may be computationally demanding , either in generating the theoretical predictions from the model , or in computing the likelihood of those predictions .",
    "a typical likelihood evaluation in a cosmological calculation involving cmb anisotropies is a few seconds of cpu time .    in combination , these seriously obstructed early data analysis efforts , even when the dataset was fairly limited , because available computer power restricted researchers to perhaps @xmath20 to @xmath21 likelihood evaluations .",
    "once beyond five or six parameters , which is really the minimum for an interesting comparison , brute - force mapping of the likelihood on a grid of parameters becomes inefficient , as the resolution in each parameter direction becomes too coarse , and anyway too high a fraction of computer time ends up being used in regions where the likelihood turns out to be too low to be of interest .",
    "this changed with a paper by christensen and meyer @xcite , who pointed out that problems of this kind are best tackled by monte carlo methods , already extensively developed in the statistics literature , e.g.  ref .",
    "subsequently , lewis and bridle wrote the cosmomc package @xcite , implementing a class of monte carlo methods for cosmological parameter estimation .",
    "the code has been very widely adopted by researchers , and essentially all cosmological parameter estimation these days is done using one of a variety of monte carlo methods .",
    "monte carlo methods are computational algorithms which rely on random sampling , with the algorithm being guided by some rules designed to give the desired outcome .",
    "an important subclass of monte carlo methods are _ markov chain monte carlo ( mcmc ) methods _ , defined as those in which the next ` step ' in the sequence depends only upon the previous one .",
    "the sequence of steps is then known as a markov chain .",
    "each step corresponds to some particular value of the parameters , for which the likelihood is evaluated .",
    "the markov chain can therefore be viewed as a series of steps ( or jumps ) around the parameter space , investigating the likelihood function shape as it goes .",
    "you may find it convenient to visualize the likelihood surface as a mountainous landscape with one dominant peak .",
    "the simplest task that such a process could carry out would be to find the maximum of the likelihood : choose a random starting point , propose a random jump to a new point , accept the jump only if the new point has a higher likelihood , return to the proposal step and repeat until satisfied that the highest point has been found .",
    "even this simple algorithm obviously needs some tuning : if the steps are too large , the algorithm may soon find it difficult to successfully find a higher likelihood point to jump to , whereas if they are small the chain may get stuck in a local maximum which is not the global maximum .",
    "that latter problem may perhaps be overcome by running a series of chains from different starting points .",
    "anyway , the maximum itself is not of great interest ; what we want to know is the region around the maximum which is compatible with the data . to do this , we desire an algorithm in which the markov chain elements correspond to random samples from the posterior parameter distribution of the parameters , i.e.  that each chain element represents the probability that those particular parameter values are the true ones .",
    "the simplest algorithm which achieves this is the metropolis  hastings algorithm , which is a remarkably straightforward modification of the algorithm described in the previous paragraph .      the metropolis ",
    "hastings algorithm is as follows :    1 .",
    "choose a starting point within the parameter space .",
    "2 .   propose a random jump .",
    "any function can be used to determine the probability distribution for the length and direction of the jump , as long as it satisfies the ` detailed balance ' condition that a jump back to the starting point is as probable as the jump away from it .",
    "this is most easily done using a symmetric proposal function , e.g.  a multivariate gaussian about the current point .",
    "evaluate the likelihood at the new point , and hence the probability by multiplying by the prior at that point .",
    "[ if the prior is flat , the probability and likelihood become equivalent . ]",
    "if the probability at the new point is higher , accept the jump .",
    "if it is lower , we accept the jump with a probability given by the ratio of the probabilities at the new and old point . if the jump is not accepted , we stay at the same point , creating a duplicate in the chain .",
    "4 .   repeat from step 2 , until satisfied that the probability distribution is well mapped out .",
    "this may be done for instance by comparing several chains run from different starting points , and/or by using convergence statistics amongst which the gelman ",
    "rubin test @xcite is the most commonly used .    by introducing a chance of moving to a lower probability point",
    ", the algorithm can now explore the shape of the posterior in the vicinity of the maximum .",
    "the generic behaviour of the algorithm is to start in a low likelihood region , and migrate towards the high likelihood ` mountains ' . once near the top , most possible jumps are in the downwards direction , and the chain meanders around the maximum mapping out its shape . accordingly , all the likelihood evaluations , which is where the cpu time is spent , are being carried out in the region where the likelihood is large enough to be interesting .",
    "the exception is the early stage , which is not representative of the posterior distribution as it maintains a memory of the starting position .",
    "this ` burn - in ' phase is then deleted from the list of chain points .",
    "although any choice of proposal function satisfying detailed balance will ultimately yield a chain sampling from the posterior probability distribution , in practice , as with the simple example above , the algorithm needs to be tuned to work efficiently .",
    "this is referred to as the convergence of the chain ( to the posterior probability ) .",
    "the proposal function should be tuned to the scale of variation of the likelihood near its maximum , and if the usual choice of a gaussian is made its axes should ideally be aligned to the principal directions of the posterior ( so as to be able to navigate quickly along parameter degeneracies ) . usually , a short initial run is carried out to roughly map out the posterior distribution which is then used to optimize the proposal function for the actual computation . the resulting acceptance rate of new points tends to be around 25% .",
    "the upshot of this procedure is a markov chain , being a list of points in parameter space plus the likelihood / posterior probability at each point",
    ". a typical cosmological example may contain @xmath22 to @xmath20 chain elements , and some collaborations including wmap make their chains public .",
    "there is usually some short - scale correlation of points along the chain due to the proposal function ; some researchers ` thin ' the chain by deleting elements to remove this correlation though this procedure appears unnecessary . by construction ,",
    "the elements correspond to random samples from the posterior , and hence a plot of the point density maps it out .",
    "the joy of having a chain is that marginalization ( i.e.  figuring out the allowed ranges of a subset of the full parameter set , perhaps just one parameter ) becomes trivial ; you just ignore the other parameters and plot the point density of the one you are interested in .",
    "by contrast , a grid evaluation of the marginalized posterior requires an integration over the uninteresting directions .",
    "figure  [ f : liddle2 ] shows a typical outcome of a simple mcmc calculation .    in mapping the posterior using the point density of the chains ,",
    "one in fact ignores the actual values of the probability in the chains , since the chain point locations themselves encode the posterior .",
    "however , one can also plot the posterior by computing the average likelihood in a parameter - space bin ( marginalized or not ) , which should of course agree . whether it does or not is an additional guide to whether the chain is properly converged .",
    "in addition to analyzing the posterior probability from the chains , both to plot the outcome and extract confidence levels to quote as headline results , there are a number of other ways of using them .",
    "two examples are    * importance sampling .",
    "if new data becomes available , rather than computing new chains from scratch one can importance sample existing chains , by reweighting the elements according to the new likelihood @xcite .",
    "one can also use importance sampling to study the effect of varying the prior .",
    "these operations mean that the points now have non - integer weights , but this creates no new issue of principle",
    ". however importance sampling may result in an insufficient density of points in the new preferred region , making the sampling of the posterior noisier than would be possible with new chains .",
    "* bayesian complexity .",
    "this quantity measures the number of parameters actually constrained by the data in hand @xcite , which may be less than the number of parameters of the model , either because overall the data are poorly constraining , or because of specific parameter degeneracies leaving some parameter combinations unconstrained .",
    "it can also be used to determine a semi - bayesian model selection statistic known as the deviance information criterion , discussed briefly in section  [ ss : ic ]      metropolis ",
    "hastings achieves the desired goal , but may struggle to do so efficiently if it is difficult to find a good proposal function or if the assumption of a fixed proposal function proves disadvantageous .",
    "this has tended not to be a problem in cosmological applications to date , but it is nevertheless worthwhile to know that there are alternatives which may be more robust .",
    "some examples , all discussed in mackay s book @xcite , are    * slice sampling : this method allows the proposal function to change during the calculation , tuning itself to an appropriate scale , though there is an additional computational cost associated with enforcing the detailed balance condition .",
    "the steps are made in a single parameter direction at a time , hence the name , and cycle through the parameter directions either sequentially or randomly .",
    "slice sampling is implemented in cosmomc as an alternative to metropolis ",
    "hastings , as are some other more specialized sampling algorithms .",
    "* gibbs sampling : this relies on obtaining a proposed step by sampling from conditional probability distributions , e.g.  to step in the @xmath23 direction we sample from @xmath24 and vice versa .",
    "it turns out that such proposals are always accepted , enhancing the efficiency .",
    "the method can however struggle to make progress along highly correlated parameter directions , traversing the diagonal through a series of short steps parallel to the axes .",
    "* hamiltonian sampling : this more sophisticated approach uses an analogy with hamiltonian dynamics to define a momentum from derivatives of the likelihood .",
    "the momentum associated with a point enables large proposal steps to be taken along trajectories of constant ` energy ' and is particularly well adapted to very high dimensionality problems .",
    "see refs .",
    "@xcite for cosmological applications .      the slow likelihood evaluations , stemming mainly from the time needed to predict observational quantities such as the cmb power spectra from the models , remain a significant stumbling block in cosmological studies .",
    "one way around this may be to use machine learning to derive accurate theoretical predictions from a training set , rather than carry out rigorous calculations of the physical equations at each parameter point .",
    "two recent attempts to do this are pico @xcite and cosmonet @xcite , the former also allowing direct estimation of the wmap likelihood from the training set .",
    "this is a promising method , though validation of the learning output when new physical processes are included may still mean that many physics - based calculations need to be done .      in conclusion ,",
    "cosmological parameter estimation from data is increasingly regarded as a routine task , albeit one that requires access to significant amounts of computing resource .",
    "this is principally attributed to the public availability of the cosmomc package , and the continued work by its authors and others to enhance its capabilities . on the theoretical side , an impressive range of physical assumptions",
    "are supported , and many researchers have the expertise to modify the code to further broaden its range . on the observational side ,",
    "researchers have recognized the importance of making their data available in formats which can readily be ingested into mcmc analyses .",
    "the wmap team were the first to take this aspect of delivery very seriously , by publically releasing the ` wmap likelihood code ' at the same time as their science papers , allowing cosmologists everywhere to immediately explore the consequences of the data for their favourite models .",
    "indeed , i would argue that by now the single most important product of an observational programme would be provision of a piece of software calculating the likelihood as a function of input quantities ( e.g.  the cmb power spectra ) computable by cosmomc .",
    "the significance with which results are endowed appears to be strongly dependent on the science community obtaining them . at one",
    "extreme lies particle physics experimentalists , who commonly set a ` five - sigma ' threshold to claim a detection ( in principle , for gaussian uncertainties , corresponding to 99.99994% probability that the result is not a statistical fluke ) .",
    "by contrast , astrophysicists have been known to get excited by results at or around the ` two - sigma ' level , corresponding to 95.4% confidence for a gaussian . at the same time",
    ", there is clearly a lot of skepticism as to the accuracy of confidence limits ; some possibly apocryphal quotes circulating in the data analysis community include `` once you get to three - sigma you have about a half chance of being right . '' and `` 95% of 95% confidence results do not turn out to be right ; if anything 95% of them turn out to be wrong '' .",
    "there are certainly good reasons to think that results are less secure than the stated confidence from a likelihood analysis would imply . amongst these",
    "are    * in realistic cases , the probability may not fall as fast as a gaussian in the tails of the distribution even if accurately gaussian near the peak .",
    "* there may be unmodelled systematic errors .",
    "the natural trend in a maturing observational field is to be initially dominated by statistical uncertainty , but as instrumental accuracy improves to then reach a systematic floor where it becomes difficult or impossible to model extraneous physical effects ( e.g.  population evolution in supernovae which one is planning to use as standard candles over cosmic epochs ) .",
    "* the likelihood function may be uncertain in a way not included in the quoted uncertainty .",
    "for instance there are now several different treatments of the wmap likelihood , differing in the way the beam profiles , source subtraction , or low multipole likelihoods are calculated . *",
    "the researchers may , consciously or otherwise , have adopted a model motivated by having seen the data , and then attempted to verify it from the same data .",
    "it has recently been claimed that this problem is widespread in the neuroscience field @xcite , with the authors of that study suspecting the issue extends to other disciplines .",
    "an example of this would be to spot an unusual feature in , say , a cosmic microwave background map , and then attempt to assess the probability of such a feature using monte carlo simulations .",
    "this ignores the fact that there may have been any number of other no more unlikely features , that _ were nt seen in the data_. consider the well - publicized appearance of stephen hawking s initials in the wmap maps , obviously massively improbable _ a priori _ but nonetheless visible in fig .",
    "[ f : liddle1 ] ( in blue , just above the middle axis , somewhat left of center ) . * publication bias : positive results are more likely to get published than negative ones , e.g.  the 95% confidence result you just read about arose from one of 20 studies , the other 19 of which generated null results .",
    "this is widely recognized in the medical statistics community , leading to introduction of costly treatments that may be ineffective or even harmful . in this context",
    ", the additional problem may exist that trials are funded by large companies whose profitability depends on the outcome , and who may be in a position to influence whether they are published . in cosmology , this may be a particular problem for cosmic non - gaussianity studies , where many different independent tests can be carried out .",
    "* model uncertainty : the possibility of different models , rather than just parameter values , describing the data has not be consistently allowed for .",
    "i m aware of situations where all of these have been important , and in my view 95% confidence is not sufficient to indicate a robust result .",
    "this appears to increasingly be the consensus in the cosmology community , perhaps because too much emphasis was put on two 95% confidence level results in the first - year wmap data ( a high optical depth and running of the spectral index ) which lost support in subsequent data releases .    on the other hand , ` five - sigma ' may be rather too conservative , intended to give a robust result in all circumstances . in reality , the confidence level at which a result becomes significant should depend on the nature of the model(s ) under consideration , and the nature of the data obtained .",
    "some guidance on where to draw the line may be obtained by exploring the issue of model uncertainty , the topic of the next section .",
    "estimation of cosmological parameters , as described in the previous section , assumes that we have a particular model in mind to explain the data .",
    "more commonly , however , there tends to be competing models available to describe the data , invoking parametrizations of different physical effects .",
    "each model corresponds to a different choice of variable parameters , accompanied by a prior distribution for those parameters .",
    "indeed , the most interesting questions in cosmology tend to be those about models , because those are the qualitative questions .",
    "is the universe spatially flat or not ?",
    "does the dark energy density evolve ?",
    "do gravitational waves contribute to cmb anisotropies ?",
    "we therefore need techniques not just for estimating parameters within a model , but also for using data to discriminate between models .",
    "bayesian tools are particularly appropriate for such a task , though i also describe some non - bayesian alternatives at the end of this section .",
    "a comprehensive review of bayesian model selection as applied to cosmology was recently given by trotta @xcite .",
    "an important implication of model - level bayesian analysis is that there is a clear distinction between a model where a quantity is fixed to a definite value , versus a more general model where that parameter is allowed to vary but happens to take on that special value .",
    "a cosmological example is the dark energy equation of state , @xmath25 , which the cosmological constant model predicts ( in good agreement with current observations ) to be precisely @xmath26 , and which other models such as quintessence leave as a free parameter to be fit from data .",
    "even if it is the cosmological constant which is the true underlying model , a model in which the equation of state can vary will be able to fit any conceivable data just as well as the cosmological constant model ( assuming of course that its range of variation includes @xmath27 ) .",
    "what distinguishes the models is _",
    "predictiveness_.    as an example , consider a magician inviting you to `` pick a card , any card '' .",
    "since you do nt know any better , your ` model ' tells you that are equally likely to pick any card .",
    "the one you actually do pick is not particularly surprising , in the sense that whatever it was , it was compatible with your model . the magician , however , has a much more predictive model ; by whatever means , they know that you will end up picking the queen of clubs .",
    "both models are equally capable of explaining the observed card , but the magician s much more predictive model lets them earn a living from it .",
    "note in this example , any surprise that you might feel comes not from the card itself , but from the magician s ability to predict it .",
    "likewise , a scientist might find themselves surprised , or dismayed , as incoming data continue to lie precisely where some rival s model said they would .",
    "model selection / comparison is achieved by choosing a ranking statistic which can be computed for each model , allowing them to be placed in rank order . within the bayesian context , where everything is governed by probabilities ,",
    "the natural choice is the model probability , which has the advantage of having a straightforward interpretation .",
    "the extension of the bayesian methodology to the level of models is both unique and straightforward , and exploits the normalizing factor @xmath28 in equation ( [ e : bayes2 ] ) which is irrelevant to and commonly ignored in parameter estimation .",
    "we now assume that there are several models on the table , each with their own probability @xmath29 and explicitly acknowledge that our probabilities are conditional not just on the data but on our assumed model @xmath30 , writing @xmath31 this is just the previous equation with a condition on m written in each term .",
    "the denominator , the probability of the data given the model , is by definition the model likelihood , also known as the _ bayesian evidence_. note that , unlike the other terms in this equation , it does not depend on specific values for the parameters @xmath12 of the model .",
    "the evidence is key because it appears in yet another rewriting of bayes theorem , this time as @xmath32 the left - hand side is the posterior model probability ( i.e.  the probability of the model given the data ) , which is just what we want for model selection . to determine it , we need to compute the bayesian evidence @xmath33 , and we need to specify the prior model probability @xmath34 .",
    "it is a common convention to take the prior model probabilities to be equal ( the model equivalent of a flat parameter prior ) , but this is by no means essential .    to obtain an expression for",
    "the evidence , consider eq .",
    "( [ e : evparm ] ) integrated over all @xmath12 .",
    "presuming we have been careful to keep our probabilities normalized , the left - hand side integrates to unity , while the evidence on the denominator is independent of @xmath12 and comes out of the integral .",
    "hence @xmath35 or , more colloquially , @xmath36 in words , the evidence @xmath37 is the average likelihood of the parameters averaged over the parameter prior .",
    "for the distribution of parameter values you thought reasonable before the data came along , it is the average value of the likelihood .    the bayesian evidence rewards model predictiveness . for a model to be predictive , observational quantities derived from it should not depend very strongly on the model parameters .",
    "that being the case , if it fits the actual data well for a particular choice of parameters , it can be expected to fit fairly well across a significant fraction of its prior parameter range , leading to a high average likelihood .",
    "an unpredictive model , by contrast , might fit the actual data well in some part of its parameter space , but because other regions of parameter space make very different predictions it will fit poorly there , pulling the average down .",
    "finally , a model , predictive or otherwise , that can not fit the data well anywhere in this parameter space will necessarily get a poor evidence .",
    "often predictiveness is closely related to model simplicity ; typically the fewer parameters a model has , the less variety of predictions it can make .",
    "consequently , model selection is often portrayed as tensioning goodness of fit against the number of model parameters , the latter being thought of as an implementation of ockham s razor .",
    "however the connection between predictiveness and simplicity is not always a tight one .",
    "consider for example a situation where the predictions turn out to have negligible dependence on one of the parameters ( or a degenerate combination of parameters ) .",
    "this is telling us that our observations lack the sensitivity to tell us anything about that parameter ( or parameter combination ) .",
    "the likelihood will be flat in that parameter direction and it will factorize out of the evidence integral , leaving it unchanged .",
    "hence the evidence will not penalize the extra parameter in this case , because it does not change the model predictiveness .",
    "the ratio of the evidences of two models @xmath38 and @xmath39 is known as the bayes factor @xcite : @xmath40 which updates the prior model probability ratio to the posterior one .",
    "some calculational methods determine the bayes factor of two models directly .",
    "usual convention is to specify the logarithms of the evidence and bayes factor",
    ".      equation ( [ e : evidence ] ) tells us that to get the evidence for a model , we need to integrate the likelihood throughout the parameter space . in principle",
    "this is a very standard mathematical problem , but it is made difficult because the integrand is likely to be extremely highly peaked and we do not know in advance where in parameter space the peak might be .",
    "further , the parameter space is multi - dimensional ( between about 6 and 10 dimensions would be common in cosmological applications ) , and as remarked in section  [ c : cospar ] the individual likelihood evaluations of the integrand at a point in parameter space are computationally expensive ( a few cpu seconds each ) , limiting practical calculations to @xmath20 to @xmath21 evaluations .",
    "successful bayesian model selection algorithms are therefore dependent on efficient algorithms for tackling this type of integral .",
    "model probabilities are meaningful in themselves and do nt require further interpretation , but it is useful to have a scale by which to judge differences in evidence .",
    "the usual scale employed is the jeffreys scale @xcite which , given a difference @xmath41 between the evidences @xmath37 of two models , reads"
  ],
  "abstract_text": [
    "<S> the estimation of cosmological parameters from precision observables is an important industry with crucial ramifications for particle physics . </S>",
    "<S> this article discusses the statistical methods presently used in cosmological data analysis , highlighting the main assumptions and uncertainties . </S>",
    "<S> the topics covered are parameter estimation , model selection , multi - model inference , and experimental design , all primarily from a bayesian perspective .    </S>",
    "<S> = 1    epsf.tex epsf.def psfig.sty </S>"
  ]
}