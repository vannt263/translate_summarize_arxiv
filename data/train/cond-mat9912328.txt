{
  "article_text": [
    "the collective properties of neural networks , such as the storage capacity and the overlap with the memorized patterns , have been a subject of intensive research in the last decade@xcite,@xcite .",
    "however , more precise measures of their performance as an associative memory , as the information capacity and the basins of attraction of their retrieval states , have received comparatively less attention@xcite-@xcite .",
    "for some models as the sparse - code networks@xcite-@xcite , or the three - state networks@xcite-@xcite , where the patterns are not uniformly distributed , an information - theoretical approach@xcite-@xcite seems crucial .",
    "calculations of the shannon mutual information ( @xmath0 ) for the sparse - code network were made@xcite-@xcite . for low storage of patterns ,",
    "a few time steps are need to retrieve them@xcite,@xcite .",
    "however , for large storage , only @xmath2perfect retrieval is possible .",
    "the closer to saturation , the larger the time steps required to dynamical retrieval .",
    "so , first - time retrieval is not enough and it is interesting to study the information capacity of recurrent networks . to improve @xmath0 for this recurrent network ,",
    "a scheme , based on a self - control threshold mechanism , was proposed@xcite .",
    "this self - control neural network ( scnn ) is an adaptive scheme induced by the dynamics itself instead of imposing any external constraint on the activity of the neurons .",
    "such procedure successfully increases both @xmath0 and the basins of attraction of the patterns .",
    "similar mechanisms can improve @xmath0 for three - state low - activity networks@xcite , with diluted and fully - connected architectures .    here",
    "we propose a new method , based on direct use of the @xmath0 calculated in the mean - field approximation , to obtain a hamiltonian which maximizes @xmath0 within a large range of values for the activity of the network .",
    "a three - state neural network is defined by the use of a set of @xmath3 @xmath4 patterns , @xmath5 , which are independent random variables given by the probability distribution    @xmath6    where @xmath7 is the @xmath8 of the patterns ( @xmath9 are the inactive states ) .",
    "a low - activity three - state neural network corresponds to the case where the distribution is not uniform , @xmath10 , @xmath11 . in the limit @xmath12 the binary hopfield model is reproduced .",
    "the information enclosed in a simple unit @xmath13 is given by the entropy of its probability ,    @xmath14    one can define as sparse a code whose fraction of active neurons is very small and tends to zero in the thermodynamic limit@xcite .",
    "sparse - code binary patterns can have a large load rate @xmath15^{-1}$ ] , where @xmath16 is the ratio between the number of patterns @xmath17 and number of connection per neuron @xmath18 @xcite,@xcite .",
    "however , the information per unit for the sparse code is @xmath19 , and it is not clear if the total information per connection , @xmath20 of such network is larger than the uniform ( non - sparse ) one .    although ternary patterns with low - activity have not been studied in the same proportion , they present a similar behavior@xcite .",
    "besides of the fact that ternary patterns are a step towards an analog neural model , they have the advantage that they can be generated with a bias but keeping their symmetric distribution ( both @xmath21 states are considered active ) . an important question related to the three - state model",
    "is the measurement of the retrieval quality in the cases where this is imperfect .",
    "the overlap alone is not anymore a good measure because it accounts only for the active states . for the homogeneous ternary patterns ,",
    "the hamming distance can be considered a good measure , since it takes equally into account all errors in retrieving , the active and the inactive one .",
    "for the low - activity case , however , also the hamming distance is not a good parameter of the retrieval quality because the errors in retrieving the active states are much more relevant ( they contain much more information ) than the errors in retrieving the inactive states . to solve this problem",
    ", we use the conditional probability of neuron states given the pattern states@xcite , to obtain the mutual information @xmath0 of the attractor neural network ( ann ) .",
    "this quantity measures directly the amount of dependence between the random variables , neurons and patterns . to accomplish that",
    ", we must use a new variable , we call @xmath8-@xmath22 , which is the overlap between the active states of the ternary neurons and the active states of the ternary patterns , taken with the absolute value",
    ".    this @xmath0 is thus a function of three parameters : the overlap @xmath23 , the neural - activity @xmath24 , and the activity - overlap @xmath25 .",
    "we then expand the @xmath0 around the values of the parameters when the neurons are independent on the patterns .",
    "this expansion gives us an expression that can be interpreted as a hamiltonian , a function only of the neuron states and the synaptic couplings .",
    "this hamiltonian is similar to the blume - emery - griffiths@xcite-@xcite spin-1 model ( beg ) , but with random interactions .",
    "the beg model , originally proposed to study @xmath26 mixtures , was latter used to describe several systems , like memory alloys , fluid mixtures , micro - emulsions , etc . , and displays a variety of new thermodynamic phases .",
    "some disordered beg models have been recently studied @xcite-@xcite , where either the exchange - interactions or the crystal - field are random variables . however , from our knowledge , no random biquadratic - interactions model has been treated up to this date .",
    "we describe our model in section ii . in section iii",
    "we describe the @xmath0 measures used to evaluate the performance of the ann , and derive the beg hamiltonian from @xmath0 . after solving the thermodynamics for this model in section iv",
    ", we present some results for the dynamics and the phase diagrams in the section v , comparing the results with previous works .",
    "we conclude in the last section with some comments about possible improvements of the network .",
    "as well as the pattern states , the neuron states at time @xmath27 are three - state variables , defined as    @xmath28    they are updated according to a stochastic dynamics which depends on the previous states @xmath29 and on synaptic interactions between different neurons .",
    "the specific form of the synapses will be obtained latter , by construction .",
    "we will see they are of the hebbian type , that is , the learning is local ( the synapses depend only on the two neurons interacting ) .",
    "moreover , the updating rule will be also obtained by construction , no supposition being done here except that the patterns have the same three - state symmetry as the neuron states .",
    "the three - state patterns @xmath30 , @xmath3 , are independent identically distributed random variables ( @xmath31 ) chosen according to the probability distribution in eq.([1.px ] ) .",
    "there is no bias ( @xmath32 ) neither correlation between patterns ( @xmath33 ) , and @xmath34 is the activity of the patterns .",
    "the mean - field networks have the property of being site - independent , that means , the correlations between different sites are negligeable in the thermodynamic limit , @xmath35 .",
    "this implies that every macroscopic quantity satisfies the conditions of the law of large numbers ( @xmath36 ) , so they can be defined as an average on the probability distribution of a state in a single site . if @xmath37 is the thermodynamic limit of the variable @xmath38 , we have    @xmath39    where the brackets represent averages over the distribution of a single , typical state @xmath40 ( we can drop the index @xmath41 ) .",
    "an example of this is the special case of the overlap , but this property is valid also for every function @xmath42 , since it comes from a property of the probability distribution of the states itself ,    @xmath43    the task of retrieval is successful if the distance between the state of the neuron @xmath44 and the pattern @xmath45 , defined as    @xmath46    becomes small after some time @xmath27 .",
    "this is the so - called hamming distance ( an euclidean quadratic measure for discrete sets ) .",
    "the overlap of the @xmath47th pattern with the neuron - state is defined as :    @xmath48    while the neural activity is    @xmath49    the @xmath50 are called the @xmath51 @xmath52 , and they are normalized parameters within the interval @xmath53 $ ] , which attain the extreme values @xmath54 whenever @xmath55 , as we see from eq.([1.px ] ) .",
    "another parameter is need to define completely the macroscopic state of the ann .",
    "this is    @xmath56    we call this quantity the @xmath8-@xmath22@xcite , as long as @xmath57 represents the overlap between the sites , where the neurons are active , @xmath58 , and the sites where the patterns are active , @xmath59 . for the dynamics used in most",
    "work found in the literature@xcite,@xcite,@xcite , where the synapses used are of the hopfield form @xmath60 , this parameter @xmath57 does not seem to play any role in the evolution of the network , independent of the architecture considered ( diluted , layered or fully - connected for instance ) .",
    "however @xmath57 is necessary to define the mutual information of the network , as well as this is necessary in computing the network s performance @xcite,@xcite .    for this hopfield three - state network a self - control ( sc )",
    "mechanism was recently introduced with the following threshold dynamics @xcite : @xmath61 , where @xmath62 is a function only of the pattern activity , while the variance of the cross - talk noise ( due to the @xmath63 non - retrieved patterns ) has the simple form @xmath64 for the diluted architecture@xcite .",
    "here we take the alternative approach of starting from the mutual information for the model , which we describe in the next section .",
    "compared to the binary neural network ( nn ) , where the natural parameter is the overlap @xmath65 , to describe the statistical macro - dynamics of the three - state nn , there should be two additional parameters .",
    "although the only variables appearing in the usual hopfield dynamics are the overlap @xmath50 and the neural activity @xmath66 , the activity - overlap @xmath57 is an independent parameter which complete the macroscopic description . for a long - range system , as the one we are considering ,",
    "it is enough to observe the distribution of a single typical neuron in order to know the global distribution .",
    "the conditional probability of having a neuron in a state @xmath67 in a time @xmath27 , given that in the same site the pattern being retrieved is @xmath68 , is :    @xmath69    one can verify that this probability satisfies the averages :    @xmath70    these are the thermodynamic limits ( @xmath35 ) of eqs.([2.mm],[2.qn],[2.nn ] ) , for a given time @xmath27 and pattern @xmath47 . due to the symmetry of the patterns , we have also @xmath71 .",
    "the averages are over the pattern distribution , eq.([1.px ] ) , and over the conditional distribution , eq.([3.ps ] ) :    @xmath72    together with the distribution of the patterns , the conditional probability leads also to the probability    @xmath73    with the above expressions we can calculate the @xmath74 @xmath75 @xmath0@xcite,@xcite , a theoretical information quantity used to measure the average amount of information that can be received by the user by observing the symbol ( or the signal ) at the output of a channel .",
    "we can regard all the dynamical process , or rather each time step of it , as a channel , and write the @xmath0 as :    @xmath76&&= s[\\sigma]-\\langle s[\\sigma|\\xi]\\rangle_{\\xi};\\nonumber\\\\ s[\\sigma ] & & \\equiv -\\sum_{\\sigma}p(\\sigma)\\ln[p(\\sigma)],\\nonumber\\\\ s[\\sigma|\\xi ] & & \\equiv -\\sum_{\\sigma}p(\\sigma|\\xi)\\ln[p(\\sigma|\\xi ) ] .",
    "\\label{3.is}\\end{aligned}\\ ] ]    @xmath77 $ ] and @xmath78 $ ] are the entropy of the output and the conditional entropy of the output , respectively .",
    "the quantity @xmath79\\rangle_{\\xi}$ ] is also called the @xmath80 @xmath81 of the @xmath82 $ ] .    for an ann of homogeneous distributed patterns",
    "the @xmath0 is not a necessary measure , because the hamming distance is enough to quantify the quality of the retrieval .",
    "the latter distinguishes well between a situation where most of the wrong neurons were turned off ( @xmath83 ) and another situation where they were flipped ( @xmath84 ) .",
    "however , for the low - activity ann , @xmath0 can be a very useful measure , since the hamming distance is not so good in distinguishing the cases where neurons were turned off from the cases where they were turned on ( @xmath83 ) .",
    "this distinction is critical in the sparse coded three - state @xmath85 , where @xmath86 , because the inactive states have less information than the active ones .",
    "for instance , let be an ann with pattern activity @xmath7 and denote the active ( inactive ) sites as @xmath87 ( @xmath88 ) , such that @xmath89 ( @xmath90 ) .",
    "now suppose that all neurons where turned off , @xmath91 , then @xmath92 , @xmath93 and @xmath94 , so the hamming distance is @xmath95 and there is no information transmitted , @xmath96 .",
    "if instead of turning off the @xmath97 active neurons @xmath98 , one had turned on @xmath99 neurons among the inactive @xmath100 , one get @xmath101 , @xmath102 and @xmath103 .",
    "so the hamming distance is still @xmath95 , but now there is some transmitted information .",
    "it is intuitive that the first kind of errors have erased all the meaningful bits , while the second situation have not affected essentially the code , and obviously have much less important errors .",
    "the expressions for the entropies defined above are :    @xmath104= - q\\ln{q\\over 2 } - ( 1-q)\\ln(1-q ) , \\nonumber\\\\ & & \\langle s[\\sigma|\\xi]\\rangle_{\\xi}= a s_{a } + ( 1-a ) s_{1-a } ,   \\nonumber\\\\ & & s_{a}=   - { n+m\\over 2}\\ln{n+m\\over 2 }   - { n - m\\over 2}\\ln{n - m\\over 2 } \\nonumber\\\\ & & - ( 1-n)\\ln(1-n ) ,   \\nonumber\\\\ & & s_{1-a}= - s\\ln{s\\over 2 } - ( 1-s)\\ln(1-s ) . \\label{3.hs}\\end{aligned}\\ ] ]    applying to the second case cited above , the entropy of the output is @xmath77=-2a\\ln a-(1 - 2a)\\ln(1 - 2a)$ ] , while the equivocation is @xmath105>= -a[\\ln a-\\ln 2-\\ln(1-a)]-(1 - 2a)[\\ln(1 - 2a)-\\ln(1-a)]$ ] , so that the mutual information is @xmath106 - 2a\\ln(2)$ ] which is not so smaller than the entropy of the original patterns , @xmath107 $ ] , eq.([1.hm ] ) .",
    "it is easy to understand why we must use @xmath0 for the sparse code case , instead of the hamming distance .",
    "we search for a hamiltonian which is symmetric in any permutations of the patterns @xmath108 , since they are not known during the retrieval process .",
    "this imposes that the retrieval of any pattern @xmath108 is week , i.e. , @xmath109 is almost independent of it .",
    "then obviously the overlap @xmath110 .",
    "an expansion of @xmath0 with @xmath111 around @xmath110 yields the hopfield hamiltonian .",
    "if afterwards some particular overlap becomes eventually large , this should be a consequence of the network evolution",
    ".    however , for general @xmath112 , this is not the only quantity which vanishes in this limit .",
    "the variable @xmath113 is also almost independent of @xmath114 , so that @xmath115 .",
    "hence , the parameter    @xmath116    also vanishes when the states of the neurons and the patterns are independent .",
    "we use this fact to look at the information close to the non - retrieval regime .",
    "an expansion of the expression for the @xmath0 around @xmath117 gives    @xmath118    since this expression gives the information for a single site @xmath41 of a single pattern @xmath47 , @xmath119 , it should be summed @xmath120 to give the total information of the network .",
    "it is natural to associate this quantity with the opposite of the hamiltonian , because the maximum of the information gives the minimal energy .",
    "we suppose , as a further simplification of the model , that the neural activity is of the same order of the pattern activity , @xmath121 . with this assumption ,",
    "@xmath0 from eq.([3.i1 ] ) depends on the same way on @xmath65 and @xmath122 .",
    "substituting the expressions for these parameters , given by the definitions ( [ 2.mm]),([2.qn ] ) and ( [ 2.nn ] ) ( @xmath1 , eqs.([3.ma ] ) before the thermodynamic limit ) , we obtain the following expression for the @xmath0 :    @xmath123    where    @xmath124    and    @xmath125    are the quadratic and the biquadratic terms , respectively .",
    "the above expression for the hamiltonian , obtained from the mutual information close to the non - retrieval regime , has the same form as of the beg model@xcite .",
    "we call our model the beg neural network ( begnn ) .",
    "the interactions are randomly distributed , given by    @xmath126    and    @xmath127    the first term of the hamiltonian is the usual hopfield model with the hebbian rule given by eq.([3.ji ] ) .",
    "the second term , arising from the term depending on @xmath122 in eq.([3.i1 ] ) , related to the activity - overlap , is also hebbian - like , but is associated , as will be seen latter , with the quadrupolar order of the system .",
    "note that the hamiltonian formulation of the problem is only possible in the case of fully - connected neural network , where the interaction matrix is symmetric . in the next section",
    "we will present the dynamical formulation of the problem , which can be applied to the cases of asymmetric couplings@xcite .",
    "as is well known , the phase diagram of the usual beg model is very rich , showing different phases , depending on the sign and the strength of the biquadratic coupling constant . without any disorder and for very negative biquadratic coupling constant ,",
    "a quadrupolar phase , related to the quadrupolar moment @xmath128 also appear , apart of the usual disordered and ferromagnetic phases @xcite-@xcite .",
    "however , our variables @xmath129 are quenched , so we have a disordered system .",
    "beg models with disordered quadratic coupling have been recently studied@xcite-@xcite , showing some new phases ( spin - glass , quadrupolar spin - glass phases , etc ) , but , from our knowledge , no disordered biquadratic beg model has been studied up to this date .",
    "for the derivation of the asymptotic macro - dynamics we will use a naive mean - field ( mf ) approach using the hamiltonian eqs.([3.h1])-([3.h2 ] ) .",
    "since the hamiltonian is quadratic in the overlaps , we can linearize it , using gaussian transformation , to obtain the partition function :    @xmath130 \\prod_{i}\\sum_{\\sigma=\\pm1,0}e^{\\tilde{{\\cal h}_i } } \\nonumber , \\label{4.zt}\\end{aligned}\\ ] ]    where @xmath131 , and @xmath132 .",
    "the effective hamiltonian is    @xmath133    where the local fields are    @xmath134    after taking the trace over the spin variables , we apply a saddle point integration and use eq.([2.fn ] ) for the thermodynamic limit , to get the free energy in terms of the parameters @xmath23 , @xmath135 and @xmath24 :    @xmath136    where the effective partition function is :    @xmath137    the fields @xmath138 are defined in eq.([4.hi ] ) , but the indices @xmath41 can be dropped out .",
    "the saddle - point equations @xmath139 and @xmath140 . leads to the following expressions for the stationary states :    @xmath141    where the angular brackets mean the average over the patterns , and the thermal averages of the states are :    @xmath142    for zero - temperature , the behavior of the averages are :    @xmath143    where @xmath144 is the step function .",
    "this result , obtained from the naive mf theory , can be easily understood if we write the hamiltonian in eqs.([3.h1],[3.h2 ] ) in the form :    @xmath145    so , the deterministic parallel dynamics , which leads to the minimization of the hamiltonian , is    @xmath146    where the local fields @xmath147 ( associated to the variables @xmath148 respectively ) , are given in the time step @xmath27 .",
    "such dynamics has the same form as the zero - temperature function in eq.([4.fg ] ) .    alternatively to the thermodynamic approach , in the noise case",
    ", we can also start from the stochastic parallel dynamics@xcite,@xcite :    @xmath149/\\tilde{z } , \\label{4.ps}\\end{aligned}\\ ] ]    where @xmath150 is given by eq.([4.hi ] ) ( in the time step @xmath27 ) , and @xmath151 by eq.([4.z1 ] ) .",
    "differently from the dynamics for the ( q=3)-ising model@xcite@xcite , here the field @xmath152 in the effective hamiltonian is a function of the states in the previous time steps .",
    "the resulting noise - averaged states coincide with eqs.([4.sf ] ) in the stationary regime .    because we are mainly interested on the retrieval properties of our network , we take an initial configuration whose retrieval overlaps are only macroscopic of order @xmath153 for a given pattern , let say the first one .",
    "we singled out the term @xmath154 in the local fields of eq.([4.hi ] ) in order to study the retrieval of the first pattern .    supposing an initial configuration @xmath155 as a collection of iidrv with zero - mean and variance @xmath156 , the fields @xmath157 and @xmath158 in the zeroth time step are given by :    @xmath159    where the indices @xmath154 where dropped , and the rest of the patterns is regarded as some additive noise . according to the central limit theorem ( clt ) , they are independent gaussian distributed@xcite,@xcite , with zero mean and variance    @xmath160=   \\frac{1}{a^2}\\alpha q_{t=0 } \\equiv\\delta^2 \\nonumber\\\\ & & var[\\omega_{t=0}]= \\frac{\\delta^2}{(1-a)^2 } .",
    "\\label{4.on}\\end{aligned}\\ ] ]    although the dynamics for the parameters @xmath161 , @xmath162 and @xmath163 in the first time step is a function of the initial step , the expression for the noises in the next steps evolves with time in more complicated way then eqs.([4.on ] ) . in the extremely diluted synaptic case@xcite",
    ", however , the first time step describes the dynamics for every time step @xmath27 . from now on we will adopt this limiting case .",
    "thus , in the asymptotic limit @xmath35 , the expression for the overlap @xmath164 becomes , after averaging over the pattern @xmath165 :    @xmath166    where the averages over @xmath167 on the brackets should be done with the gaussian distributions , eq([4.on ] ) .",
    "the neural activity is the thermodynamic limit of eq.([2.qn ] ) , which reads    @xmath168    here @xmath169 is the variable defined in eq.([3.ps ] ) and the activity - overlap is given by    @xmath170    the equation for @xmath171 is obtained using the definition in eq.([3.lm ] ) , @xmath172 .",
    "it is worth to note that the definitions of the parameters @xmath173 in eqs.([3.ma ] ) are the same as that in eqs.([4.mt]-[4.nt ] ) , since the average over the conditional probability @xmath174 is equivalent to the average over the noise due the @xmath63 remaining patterns @xmath175 .",
    "eqs.([4.mt]-[4.nt ] ) describe the macro - dynamics of the diluted begnn by adapting self - consistently the threshold during the time - evolution of the system .",
    "with these equations we can calculate the mutual information from eqs.([3.is]-[3.hs ] ) .",
    "in this section we present some explicit results for the begnn model .",
    "we first calculated the stable fixed - points of the eqs.([4.mt]-[4.nt ] ) for the asymptotic @xmath35 network , and obtained the curves for the order parameters @xmath173 and the _ information _ @xmath176 as a function of the load parameter @xmath16 for two values of the activity @xmath7 ( fig.1 ) . for small load ( @xmath177 ) , the overlap remains close to @xmath178 and the neural activity is @xmath179 .",
    "when more patterns are stored in the network , @xmath41 increases almost linearly , up to an optimal value , @xmath180 , after which @xmath41 decreases to zero in @xmath181 .",
    "the comparison is done with the self - control neural network ( @xmath182 ) model@xcite,@xcite .",
    "it is seen that for small activities ( @xmath183 ) , the begnn model gives worst results compared with the scnn model , with a smaller value for @xmath41 , while for @xmath184 ( close to the uniform distribution of patterns , @xmath185 ) , the begnn performs better , with an optimal value of the information @xmath186 , although it is attained for a smaller value of load , @xmath187 .",
    "the reason for this behavior is that the third order parameter ( related to the activity - overlap ) , is @xmath188 for the begnn ( scnn ) and @xmath189 for the scnn ( begnn ) with @xmath184 ( @xmath183 ) .",
    "the behavior of the order parameters and the @xmath41 with load , for the zero - temperature case is presented for three different values of the activities ( fig.2 ) .",
    "the initial conditions used where @xmath190 , such that there is almost no initial overlap . in this case",
    "there is always a sharp fall on the information for @xmath16 not so larger than @xmath191 .",
    "we see different behaviors depending on the activities .",
    "the corresponding dynamical phase diagram is drawn in fig.3 .",
    "four possible phases are present : the retrieval r ( @xmath192 ) and m ( @xmath193 ) phases , the quadrupolar phase q ( @xmath194 ) and the zero phase z ( @xmath195 ) .",
    "the last phase z , so called because there is no information transmitted , is analogue to the self - sustained ( s ) activity phase of the ( q=3)-ising ann@xcite@xcite , since the parameter related to the spin - glass order is @xmath196 .",
    "we have not find any paramagnetic ( p ) phase , with all ( @xmath197 ) for the begnn .",
    "note that the quadrupolar phase is a quite new phase , compared to the other nn models and is a special one for the begnn - model .",
    "this phase is also present in the original beg - model@xcite , as well as in all its generalizations including disorder@xcite-@xcite .",
    "it is seen in fig.2 , for @xmath198 , where the overlap goes to @xmath92 at @xmath199 ( much before @xmath135 , which goes to zero at @xmath200 ) ; this phase corresponds to non - zero information , although there is no retrieval overlap .",
    "the phase r appears for @xmath201 , where both @xmath23 and @xmath135 are large and so is @xmath41 . on the other hand ,",
    "the phase m is observed for @xmath202 where the parameter @xmath135 is much smaller than @xmath23 .",
    "the phase transitions from @xmath203 or @xmath204 to @xmath205 are usually sharp .",
    "the behavior of the order parameters and the information with the temperature @xmath206 for fixed activity @xmath201 is shown on fig.4 .",
    "we observe an increase of @xmath41 with the temperature , showing an optimal value for @xmath207 .",
    "such an improvement of a feeble signal with noise , similar to the stochastic resonance phenomena , appears also in other physical systems@xcite .",
    "a further increase in temperature leads to decreasing the information of the model .",
    "we note that this behavior does nt hold for @xmath208 , nevertheless there is still an increase of the storage capacity @xmath181 .",
    "the last result is in agreement with other investigations of dynamical activity of real and model neurons , where the observed stochastic resonance disappears by increasing the amplitude of the external stimulus .",
    "a cut of the phase diagram in the plane @xmath209 for a fixed value of the activity @xmath210 is shown in fig.5 . the dashed line , which corresponds to the optimal case , @xmath211 , is within either the phase r or q. it is also interesting to observe that there are two separate q - phase islands , for either small temperature @xmath206 and large load @xmath16 or large temperature @xmath206 and small load @xmath16 .",
    "the phase transitions become smoother with the temperature .    finally on fig.6",
    "we present the evolution of the information and of the order parameters with the time @xmath27 , for a given temperature @xmath212 and activity @xmath210 , for two values of the load parameter @xmath16 .",
    "as can be seen from this figure , for @xmath213 , which is close to the transition r - m , the change to the behavior of the order parameters needs more time steps than for @xmath214 .",
    "this is not strange due to the critical slowing down near the transition .",
    "however , an interesting new fact appears here : the parameters @xmath171 and @xmath163 have a fast felt down to a much smaller value , after which the network stays a long while with an almost zero overlap , and finally the begnn is able to retrieve quite well the pattern .",
    "for instance , for @xmath214 , @xmath135 falls to @xmath215 and @xmath23 stays near @xmath216 during the first @xmath217 time steps , then they jump up to @xmath218 , @xmath219 , which means the memory pattern was ( partially ) attained .",
    "this result , caused by the instability of the z - phase in this region , makes the begnn capacity much larger than that of the usual hopfield model , in all its versions so far as we know .",
    "the behavior of the continuous phase transitions can be analytically studied within the mean - field approximation by expanding eqs.([4.mt]-[4.nt ] ) for small values of the order parameters .",
    "a standard calculation , for example , for the transition line qz ( @xmath220 ) leads to the following expression :    @xmath221    where the transition temperature between the phases q and z is :    @xmath222    with @xmath223    expanding the above expressions for small value of the load rate and large temperatures , @xmath224 , and calculating the averages over the noise up to the leading terms , one obtains the following equation for the transition line :    @xmath225    the last expression for @xmath226 is in qualitative agreement with the previous results shown on fig.5 .    regarding the equation for the order parameter @xmath135",
    ", one can verify that in leading order :    @xmath227    by use of eq.([t_c ] ) , it is seen that the quadratic term of the above expansion changes sign when the activity @xmath201 , thus defining a tricritical line between the transition of second order ( @xmath228 ) and of first order ( @xmath229 ) .",
    "note that similar tricritical behavior has been described also in the other versions of the beg model @xcite-@xcite .",
    "similar analysis can be also performed for the other continuous transition between the different phases .",
    "in this paper we proposed a beg - like hamiltonian for a ternary neural network , which couplings arise from an expansion of its mean - field mutual information , @xmath0@xcite , resulting in a system evolving with a self - consistently adapting threshold .",
    "the stationary and dynamical equations for this model were obtained as functions of three order parameters , the overlap @xmath23 , the neural activity @xmath24 , and the activity - overlap @xmath25 .",
    "their solutions were explicitly calculated as functions of the variables : the pattern activity @xmath7 , the load @xmath16 and the temperature @xmath206 .",
    "when the activity is near @xmath185 , corresponding to the uniform ternary patterns , the begnn improves the information , compared with a previous scnn model@xcite,@xcite .",
    "improvement of the information content by increasing the noise , effect similar to the stochastic resonance , is also observed for activities @xmath11 .",
    "there are four possible phases for the begnn , which were displayed in phase diagrams @xmath230 and @xmath209 .",
    "in particular , a quadrupolar phase , q , with @xmath231 , holds whenever the activity is large enough .",
    "this phase , known in the beg literature , but new in an ann context , carries out some nonzero information about the patterns even without any overlap @xmath23 .    as the main result we obtained that ,",
    "while the phase z is not stable in a large range of the variables , the basin of attraction of the retrieval phase is increased with respect to the usual ternary neural network models .",
    "states with initial conditions having very small overlap flow to final states with large overlap .",
    "we believe that the begnn has a quite large range of applications for real systems .",
    "we also think that this way to obtain an hamiltonian starting from a mean - field calculation of @xmath0 , which yields an almost optimal retrieval dynamics , can be generalized to other spin systems , as the @xmath232-ising with @xmath233 or the potts models , for instances .",
    "such method , based on the maximization of the entropy , can be an universal approach to information systems .",
    "then , we expect that the same improvement should happen for analogue neurons and for networks of binary _",
    "synapses_. it would be also interesting to investigate the case of local field for multi - neuron synapses , which comes up from higher order terms in the expansion of the mutual information , such that a better use of a network with fixed size is expected .",
    "we thank the workshop on `` statistical mechanics of neural networks '' , max - planck institute , dresden99 , for useful discussions .",
    "is financially supported by spanish dges grant pb97 - 0076 and partly by contract f-608 with bulgarian scientific foundation ."
  ],
  "abstract_text": [
    "<S> the mutual information , @xmath0 , of the three - state neural network can be obtained exactly for the mean - field architecture , as a function of three macroscopic parameters : the overlap , the neural activity and the _ activity - overlap _ , @xmath1 , the overlap restricted to the active neurons . </S>",
    "<S> we perform an expansion of @xmath0 on the overlap and the activity - overlap , around their values for neurons almost independent on the patterns . from this expansion </S>",
    "<S> we obtain an expression for a hamiltonian which optimizes the retrieval properties of this system . </S>",
    "<S> this hamiltonian has the form of a disordered blume - emery - griffiths model . </S>",
    "<S> the dynamics corresponding to this hamiltonian is found . as a special characteristic of such network </S>",
    "<S> , we see that information can survive even if no overlap is present . </S>",
    "<S> hence the basin of attraction of the patterns and the retrieval capacity is much larger than for the hopfield network . </S>",
    "<S> the extreme diluted version is analized , the curves of information are plotted and the phase diagrams are built .    </S>",
    "<S> epsf    pacs numbers : 87.10 , 64.60c keywords : statistical physics , neural network , multi - state neuron , spin-1 model , sparse code , information theory , dynamical systems , blume - emery - griffith </S>"
  ]
}