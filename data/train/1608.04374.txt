{
  "article_text": [
    "machine learning algorithms have long worked with multi - dimensional vector data and parameters , but have not exploited the underlying inner product space structure . a recent paper on deep learning in _ nature _ called for `` new paradigms '' involving `` operations on large vectors '' @xcite to propel the field forward .",
    "this approach is taken to describe the convolutional neural network ( cnn ) in this paper .",
    "in particular , the layers are described as vector - valued maps , and gradients of these maps with respect to the parameters at each layer are taken in a coordinate - free manner .",
    "this approach promotes a greater understanding of the network than a coordinate - based approach , and allows for loss function gradients to be calculated compactly using coordinate - free backpropagation of error .",
    "this paper also considers a higher - order loss function , as in @xcite and @xcite .",
    "algorithms to compute one iteration of gradient descent are provided for both types of loss functions to clarify the application of the developed theory .",
    "the precise notation developed throughout this paper provides a mathematical standard upon which deep learning can be researched , overcoming the inconsistent notation currently employed across the field .",
    "the framework developed in this paper is flexible , and can be extended to cover other types of network structures , and even inspire further developments in deep learning .",
    "some prerequisite notation and concepts are introduced here before cnns can be fully described .",
    "every individual vector space is assumed to be an inner product space , with the inner product represented by @xmath0 .",
    "the inner product naturally extends to the direct product @xmath1 of inner product spaces @xmath2 and their tensor product @xmath3 as follows @xcite : @xmath4 where @xmath5 , @xmath6 . the symbol @xmath7 is exclusively used to denote the tensor product operator in this paper .",
    "an inner product space @xmath8 is canonically identified here with its dual space @xmath9 using the inner product on @xmath8 , so dual spaces will rarely be used in this paper .",
    "the set of @xmath10-linear maps from @xmath11 to a vector space @xmath12 is denoted by @xmath13 . for a linear map @xmath14 , its adjoint map , denoted by @xmath15 , is a linear map in @xmath16 defined by the relationship @xmath17 for all @xmath18 and @xmath19 . for each vector @xmath20 and any bilinear map @xmath21 ,",
    "define a linear map @xmath22 by @xmath23 for all @xmath24 .",
    "likewise , for each vector @xmath25 and any bilinear map @xmath21 , define a linear map @xmath26 by @xmath27 for all @xmath28 .",
    "now , notation for derivatives in accordance with @xcite is presented .",
    "consider a map @xmath29 .",
    "the ( first ) derivative @xmath30 of @xmath31 at a point @xmath32 is a linear map from @xmath33 to @xmath34 , i.e. @xmath35 , and it can be defined as @xmath36 for any @xmath37 .",
    "the derivative @xmath38 can be viewed as a map from @xmath33 to @xmath39 , defined by @xmath40 .",
    "let @xmath41 denote the adjoint of @xmath30 so that @xmath42 for all @xmath37 and @xmath43",
    ".    now consider a map @xmath44 written as @xmath45 for @xmath32 and @xmath46 , where the semi - colon is inserted between @xmath47 and @xmath48 to distinguish the state variable @xmath47 from the parameters @xmath48 .",
    "let @xmath49 denote the derivative of @xmath31 with respect to @xmath47 evaluated at @xmath50 , and let @xmath51 denote the derivative of @xmath31 with respect to @xmath48 evaluated at @xmath50 .",
    "it is easy to verify that @xmath52 and @xmath53 and that @xmath54 for all @xmath55 and @xmath56 .",
    "the adjoints of @xmath49 and @xmath57 are denoted by @xmath58 and @xmath59 , respectively .",
    "sometimes , @xmath60 is written instead of @xmath61 , to emphasize differentiation of @xmath31 with respect to the parameter variable @xmath48 .",
    "the second derivative @xmath62 of @xmath31 with respect to @xmath47 evaluated at @xmath50 is a bilinear map in @xmath63 defined as follows : for any @xmath64 , @xmath65 it is assumed that every function that appears in this paper is ( piecewise ) twice continuously differentiable .",
    "the second derivative @xmath66 is symmetric , i.e. @xmath67 for all @xmath64 .",
    "the second derivative @xmath68 of @xmath31 with respect to @xmath47 and @xmath48 at the point @xmath50 is a bilinear map in @xmath69 defined as follows : for any @xmath55 and @xmath56 , @xmath70 on the other hand , the second derivative @xmath71 of @xmath31 with respect to @xmath48 and @xmath47 at the point @xmath50 denotes the bilinear map in @xmath72 defined as follows : for any @xmath56 and @xmath55 , @xmath73 note that for all @xmath28 and @xmath56 , it is easy to verify that @xmath74      now , backpropagation will be presented in a coordinate - free form . given two maps @xmath75 for @xmath76 and @xmath77 for @xmath78 , the composition @xmath79 is the map defined as follows : @xmath80 for @xmath81 .",
    "in this framework , functions are composed with respect to the state variables . by the chain rule , @xmath82 which are evaluated at a point @xmath83 as follows : @xmath84 where the dependency on the parameters @xmath85 and @xmath86 is suppressed for brevity , which shall be understood throughout the paper .",
    "in particular , taking the adjoint of @xmath87 produces @xmath88 which is _ backpropagation _ in a nutshell .",
    "this can be seen by the following : consider a loss function @xmath89 defined by @xmath90 for some vector @xmath91 that may depend on @xmath47 , along with @xmath31 as in ( [ eqn : f_comp ] ) .",
    "then , for any @xmath56 , with @xmath92 representing the parameters , @xmath93 since this holds for any @xmath56 , the canonical identification of an inner product space with its dual is used to obtain @xmath94 where ( [ backprop : nutshell ] ) is used for the second equality .",
    "this shows that the error @xmath95 propagates backward from layer 2 to layer 1 through multiplication by @xmath96 .",
    "the adjoint operator reverses the direction of composition , i.e. @xmath97 , which is the key to backpropagating the error .",
    "the second derivative @xmath98 of @xmath99 is given by @xmath100 for all @xmath101 .",
    "the second derivative @xmath102 is given by @xmath103 for all @xmath28 and @xmath104 , which is equivalent to the following : for any fixed @xmath28 @xmath105 which is a linear map from @xmath106 to @xmath107 , or by ( [ mixed : partials ] ) @xmath108 the adjoint of ( [ backprop2:nabd ] ) or ( [ backprop2:dnab ] ) yields higher - order backpropagation of error , say for a loss function @xmath109 with some @xmath28 and @xmath91 that may depend on @xmath47 , but not on the parameters .",
    "higher - order backpropagation will be studied in more detail in the next section .",
    "backpropagation can be expressed recursively for the composition of more than two functions .",
    "consider @xmath110 functions @xmath111 for @xmath112 , @xmath113 .",
    "define the composition @xmath114 .",
    "let @xmath115 and @xmath116 for @xmath117 so that @xmath118 for all @xmath119 .",
    "the first and second derivatives of ( [ eqn : recursive_rel ] ) and their adjoints can be easily obtained .",
    "this section will describe how the above framework can be applied to convolutional neural networks ; refer to @xcite or @xcite , for example , for more on the theory of cnns .",
    "first , the actions of one layer of a generic cnn will be described , and then this will be extended to multiple layers . a coordinate - free gradient descent algorithm will also be described .",
    "note that in this section , all bases of inner product space will be assumed to be orthonormal .",
    "the actions of one layer of the network will be denoted @xmath120 , where @xmath121 is the state variable , and @xmath122 and @xmath123 are the parameters . throughout this section ,",
    "let @xmath124 ( resp .",
    "@xmath125 ) be a basis for @xmath126 ( resp .",
    "@xmath127 ) , and let @xmath128 ( resp .",
    "@xmath129 ) be a basis for @xmath130 ( resp .",
    "@xmath131 ) .",
    "then @xmath132 , @xmath133 and @xmath134 can be written as follows : @xmath135 each @xmath136 is called a _ feature map _ , which corresponds to an abstract representation of the input for a generic layer .",
    "each @xmath137 is a _",
    "filter _ used in convolution , and each @xmath138 is a _ bias _ term .",
    "the actions of the layer are then a new set of feature maps , @xmath139 , with explicit form given by : @xmath140 where @xmath141 is a pooling operator , @xmath142 is an elementwise nonlinear function , and @xmath143 is the convolution operator , all of which will be defined in this section .",
    "the _ cropping _ and _ mixing _ operators will be used to define the convolution operator @xmath143 that appears in ( [ eqn : single_layer ] ) .",
    "the cropping operator , @xmath144 , is defined as : @xmath145 where @xmath146 is defined as : @xmath147 define the _ embedding _ operator @xmath148 by @xmath149 for @xmath150 , which corresponds to embedding @xmath151 into the zero matrix when @xmath128 is the standard basis .",
    "the adjoints of @xmath152 and @xmath153 are calculated as follows :    for any @xmath154 , @xmath155 where , for any @xmath156 , @xmath157    let @xmath158 . then , for any @xmath156 ,",
    "@xmath159 which proves .",
    "furthermore , let @xmath160 .",
    "then , @xmath161 which completes the proof    for @xmath162 , the _ mixing _ operator @xmath163 defines how the cropped feature maps are combined into the next layer of feature maps , which is useful in a framework such as @xcite . it can be explicitly represented as : @xmath164 where @xmath165 .",
    "the adjoint operator @xmath166 has a compact form , as the following lemma describes .    for any @xmath167 and @xmath162 , @xmath168    let @xmath169 .",
    "then , @xmath170 this implies that @xmath171 since the above equations are true for any @xmath132 .      the @xmath143 operator in ( [ eqn : single_layer ] )",
    "is known as the _ convolution _ operator .",
    "the convolution @xmath172 is defined as : @xmath173 where @xmath174 is a bilinear operator that defines the mechanics of the convolution .",
    "the specific form of @xmath175 is defined using ( [ eqn : bigcrop ] ) and ( [ eqn : phi ] ) as follows : @xmath176 with @xmath177 .",
    "the fixed vectors @xmath178 , where @xmath179 for each @xmath180 , define the action of @xmath181 and thus the mixing of feature maps .",
    "the choice of @xmath182 defines the _ stride _ of the convolution .",
    "the adjoints of the operators @xmath183 , @xmath184 , and @xmath185 will be used in gradient calculations .",
    "the following theorems describe how to calculate them :    let @xmath186 and @xmath160 . then , @xmath187    let @xmath188 . then , @xmath189 since this is true for any @xmath190 , the proof is complete .",
    "[ thm : w_lh_c ] let @xmath191 and @xmath192 .",
    "then , @xmath193 furthermore , for any @xmath194 , @xmath195    let @xmath160 .",
    "then , @xmath196 also , @xmath197 both of the above results are true for a generic @xmath121 , which completes the proof .      the @xmath142 operator in ( [ eqn : single_layer ] ) is an elementwise nonlinear function , @xmath198 , that operates as follows : @xmath199 where @xmath200 is some elementwise nonlinear function , which can be written as @xmath201 .",
    "the map @xmath202 defines the nonlinear action .",
    "common choices for @xmath203 include the ramp function @xmath204 ( also known as the _ rectifier _ ) , the sigmoidal function , or hyperbolic tangent , for example .    some more maps are defined to assist in the calculation of the derivative @xmath205 of @xmath142 .",
    "the elementwise _ first _ and _ second _ derivatives , @xmath206 and @xmath207 , are maps of the same dimension as @xmath142 , defined with @xmath203 replaced by @xmath208 and @xmath209 in the above formulation , respectively . furthermore , consider a bilinear map @xmath210 that operates on @xmath211 and @xmath212  both in @xmath213  according to : @xmath214 this is an extension of the hadamard product to the tensor product space .",
    "the map @xmath205 and its adjoint are now easy to calculate .",
    "for any @xmath215 and @xmath216 , @xmath217 furthermore , @xmath218 is self - adjoint , i.e. @xmath219 .",
    "let @xmath220 and @xmath211 , where @xmath221 for each @xmath180 .",
    "then , @xmath222 where the final line follows from the definition of the hadamard product and the elementwise first derivative @xmath223 . to prove that @xmath218 is self - adjoint , first note that it is not hard to show that @xmath224 , for any @xmath225 and @xmath226 in the same space .",
    "thus , for any @xmath227 , @xmath228 this proves that @xmath229 .",
    "the @xmath141 operator in ( [ eqn : single_layer ] ) is known as the _ pooling _ operator , and its purpose is to reduce the size of the feature maps at each layer . only _ linear",
    "_ pooling is considered in this paper ( the framework does extend to the nonlinear case though ) , so that @xmath230 operates as : @xmath231 for @xmath232 . here",
    "@xmath233 operates in the same way for each feature map @xmath234 .",
    "the operator @xmath235 acts on disjoint @xmath236 neighbourhoods that form a partition of the input @xmath234 , with one output from each neighbourhood .",
    "this implies that @xmath237 and @xmath238 ( assuming that @xmath239 and @xmath240 ) .",
    "one type of linear pooling is _ average pooling _ , which involves taking the average over all elements in the @xmath236 neighbourhoods .",
    "this can be represented using ( [ eqn : smallcrop ] ) as : @xmath241 where the operator @xmath242 is defined in ( [ eqn : smallcrop ] ) with @xmath243 and @xmath244 if @xmath245 is the standard basis , @xmath246 is the all - ones matrix .",
    "the adjoint @xmath247 of the average pooling operator @xmath248 can be computed using the following theorem .",
    "[ thm : bigpool ] let @xmath249 . then , using _",
    "( [ eqn : embed ] ) _ with @xmath250 , @xmath251    first , let @xmath252 for notational convenience . then , for any @xmath186 , @xmath253 since this is true for any @xmath151 , the proof is complete .",
    "the derivatives of a generic layer @xmath254 , as described in ( [ eqn : single_layer ] ) , with respect to @xmath132 , @xmath133 , and @xmath134 are presented in the following theorem .",
    "[ thm : df ]    1 .",
    "@xmath255 2 .",
    "@xmath256 3 .",
    "@xmath257    these are all direct consequences of the chain rule and linearity of the derivative for the function @xmath31 given in .",
    "the adjoints of the above operators can be calculated using the reversing property of the adjoint operator @xmath258 .",
    "[ thm : df : star ]    1 .",
    "@xmath259 2 .",
    "@xmath260 3 .",
    "@xmath261      suppose now that the network consists of @xmath110 layers .",
    "denote the actions of the @xmath262 layer as @xmath263 , where @xmath264 and @xmath265 is one point in the input data .",
    "the layer map @xmath266 can be given explicitly as : @xmath267 here , @xmath268 and @xmath269 .",
    "note that the pooling operator @xmath270 , the nonlinearity @xmath271 , and the convolution operator @xmath272 are layer - dependent .",
    "the entire network s actions can be denoted as : @xmath273 where @xmath274 is the parameter set and @xmath275 is the input data .",
    "classification is often the goal of a cnn , thus assume that there are @xmath276 classes .",
    "this implies the following : @xmath277 , @xmath278 , and @xmath279 .",
    "the final layer is assumed to be fully connected , which aligns with the form given in ( [ eqn : f_t ] ) if the cropping operator ( [ eqn : bigcrop ] ) and pooling operator ( [ eqn : bigpool ] ) for the final layer  @xmath280 and @xmath281 , respectively  are identity maps .",
    "also , @xmath282 defining the mixing operator @xmath283 in ( [ eqn : conv ] ) is @xmath284 for each @xmath180 .",
    "then , the final layer is given as : @xmath285 where @xmath286 is a basis for @xmath287 , and @xmath288 .",
    "note that @xmath289 .",
    "it is also important to note that this shows that simpler , fully - connected neural networks are just a special case of convolutional neural networks .",
    "while training a cnn , the goal is to optimize some loss function @xmath89 with respect to the parameters @xmath48 .",
    "for example , consider @xmath290 where @xmath226 represents the given data and @xmath291 is the prediction .",
    "gradient descent is used to optimize the loss function , so it is important to calculate the gradient of @xmath89 with respect to each of the parameters . for this , define maps @xmath292 and @xmath293 as : @xmath294 for @xmath295 , which satisfy ( [ eqn : recursive_rel ] ) .",
    "assume @xmath296 and @xmath297 are identity maps for the sake of convenience .",
    "then , for any @xmath298 , @xmath299 since this holds for any @xmath300 , @xmath301 by the same logic used to derive ( [ eqn : nabla_j ] ) from ( [ eqn : nabla_j_u ] )",
    ". differentiating @xmath302 with respect to @xmath303 produces @xmath304 where @xmath305 and @xmath306 . taking the adjoint of ( [ eqn : dfdw ] )",
    "yields @xmath307 which can be substituted into ( [ eqn : djdw ] ) .",
    "then , the final step in computing ( [ eqn : djdw ] ) involves computing @xmath308 in ( [ eqn : dfdw : adjoint ] ) , which can be done recursively : @xmath309 this comes from taking the derivative and then the adjoint of the relationship @xmath310 .",
    "note that @xmath311 and @xmath312 in ( [ eqn : dfdw : adjoint ] ) and ( [ domega : adjoint : recursion ] ) are calculated using theorem [ thm : df : star ] .",
    "since @xmath313 can be calculated , gradient descent can be performed .",
    "one iteration of a gradient descent algorithm to update @xmath314 and @xmath303 for all @xmath315 is given in algorithm [ alg : cnn_first_grad_desc ] .",
    "the method for calculating @xmath316 is not explicitly shown in the derivation , but is a simpler version of @xmath317 and is included in the algorithm .",
    "the algorithm can be extended to a batch of points by summing the contribution to @xmath318 from each input point @xmath132 .",
    "note that @xmath319 is the learning rate .",
    "@xmath320 @xmath321 @xmath322 @xmath323 @xmath324 from @xmath325 store old @xmath303 for updating @xmath326 @xmath327 @xmath328 @xmath329 @xmath330 & thm [ thm : df : star ] , update with @xmath331 @xmath332 @xmath333 & thm [ thm : df : star ] @xmath334 @xmath335      suppose that another term is added to the loss function to penalize the first - order derivative of @xmath291 , as in @xcite or @xcite for example .",
    "this can be represented using @xmath336 for some @xmath337 and @xmath338 .",
    "when @xmath339 , minimizing @xmath340 promotes invariance of the network in the direction of @xmath341 .",
    "this can be useful in image classification , for example , where the class of image is expected to be invariant with respect to rotation . in this case",
    ", @xmath341 would be an infinitesimal generator of rotation .",
    "this new term @xmath342 can be added to @xmath89 to create a new loss function @xmath343 where @xmath344 determines the amount that the higher - order term contributes to the loss function .",
    "note that @xmath342 could be extended to contain multiple terms as : @xmath345 where @xmath346 is a finite set of pairs @xmath347 for each @xmath132 .",
    "the gradient of @xmath342 with respect to the parameters must now be taken .",
    "this can calculated for a generic parameter @xmath348 , which is one of @xmath303 or @xmath314 : @xmath349 for all @xmath300 in the same space as @xmath348 .",
    "again , in the same way that was derived from , @xmath350 before can be computed , however , some preliminary results will be given .",
    "[ thm : d_nab_f ]",
    "let @xmath31 be defined as in , and @xmath351 .",
    "let @xmath352 .",
    "then , @xmath353    let @xmath354 . then , prove directly : @xmath355 \\cdot v \\\\ & = { \\psi}\\cdot { \\mathrm{d}}^2{s}(z ) \\cdot ( c(w , v ) , c(u , x ) ) + { \\psi}\\cdot { \\mathrm{d}}{s}(z ) \\cdot c(u , v ) \\\\ & = { \\psi}\\cdot \\left [ \\left(c(w , v ) { \\righthalfcup}{\\mathrm{d}}^2 { s}(z)\\right ) \\cdot ( c { \\lefthalfcup}x ) + { \\mathrm{d}}{s}(z ) \\cdot ( c { \\lefthalfcup}v ) \\right ] \\cdot u.\\end{aligned}\\ ] ] this is true for any @xmath190 , so equation is proven .",
    "equation can be proven similarly , so its proof is omitted .",
    "also , let @xmath356 .",
    "then , equation can also be proven directly : @xmath357 this is true for any @xmath358 , so the proof is completed .",
    "the next lemma shows how to actually calculate @xmath359 so that the above equations can be computed .",
    "[ lem : d2s ] for any @xmath360 and @xmath361 with @xmath142 defined in , @xmath362 where @xmath207 is defined similarly to @xmath142 , but with @xmath209 replacing @xmath203 .",
    "furthermore , @xmath363 is self - adjoint , i.e. @xmath364 .    from the definition of the second derivative , @xmath365 where the last equality follows from viewing @xmath366 as an elementwise function in @xmath132 . as for the adjoint ,",
    "let @xmath367 .",
    "then , @xmath368 this proves that @xmath369 is self - adjoint .",
    "the adjoints of the equations in theorem [ thm : d_nab_f ] can now easily be calculated using the above lemma and the reversing property of the adjoint operator .",
    "[ thm : d_nab_f : star ] let @xmath31 be defined as in , and @xmath351 .",
    "let @xmath352 .",
    "then , @xmath370    now , propagation through the tangent network can be described in the spirit of @xcite .",
    "_ forward _ propagation through the network can be computed recursively , using @xmath371 : @xmath372 for any @xmath315 and @xmath121 .",
    "_ backward _ propagation through the tangent network is described in the next theorem .",
    "[ theorem : recursive : update : varphi : omega ] let @xmath324 be defined as in _ ( [ eqn : f_t ] ) _ and @xmath292 and @xmath293 be defined as in _ ( [ eqn : omega_phi])_. then , for any @xmath373 , and @xmath315 , @xmath374 where @xmath305 . also , @xmath375 is the zero operator .",
    "since @xmath376 is the identity , its second derivative is the zero operator .",
    "now consider the case when @xmath315 .",
    "take any @xmath377 and @xmath378 .",
    "then , @xmath379 where the third equality follows from the product rule . removing the trailing @xmath151 from both sides , and setting @xmath380 and @xmath381 , @xmath382 since @xmath383 and @xmath306 .",
    "taking the adjoint of this result completes the proof .    note",
    "that calculating involves taking the adjoint of , which can be done using theorem [ thm : d_nab_f : star ] along with theorems [ thm : w_lh_c ] and [ thm : bigpool ] and lemma [ lem : d2s ] .",
    "the above results are crucial for the next theorem , which is the main result .",
    "[ thm : main ] suppose @xmath384 and @xmath121 , @xmath385 , and @xmath12 , @xmath293 , and @xmath386 are defined as in . then , for a generic parameter @xmath387 , @xmath388 where @xmath305 .    for any @xmath190 in the same space as @xmath348 , @xmath389 where the final equality follows since @xmath305 for all @xmath390 . removing the trailing @xmath190 from both sides and taking",
    "the adjoint produces equation .",
    "note that in equation , @xmath391 and @xmath392 can be replaced by their corresponding expressions in theorem [ thm : df ] and [ thm : d_nab_f ] , respectively , once @xmath393 is replaced by one of @xmath394 or @xmath395 .",
    "then , can be computed with theorem [ thm : main ] , where @xmath396 is computed recursively by .",
    "algorithm [ alg : cnn_second_grad_desc ] shows one iteration of a gradient descent algorithm to optimize @xmath397 defined in for one point @xmath132 .",
    "this algorithm extends to a batch of updates , and for @xmath342 defined with multiple @xmath347 pairs as in .",
    "@xmath320 @xmath398 @xmath399 @xmath400 @xmath322 @xmath323 @xmath324 from @xmath401 with thm .",
    "[ thm : df ] @xmath325 store old @xmath303 for updating @xmath326 @xmath329 @xmath402 @xmath403 @xmath404 @xmath405 @xmath406 @xmath407 update these with @xmath331 @xmath408 with thm .",
    "[ thm : df : star ] @xmath409 @xmath410 with thms .",
    "[ thm : df : star ] & [ thm : d_nab_f : star ] , use old @xmath411 to update @xmath412 with thm .",
    "[ thm : df : star ] @xmath413 @xmath414 with thm .",
    "[ thm : df : star ] @xmath415 @xmath416 @xmath417 both @xmath418 and @xmath419 can be computed via thm .",
    "[ thm : main ] , along with thms .",
    "[ thm : df : star ] and [ thm : d_nab_f : star ] @xmath420 @xmath421",
    "this work has developed a geometric framework for convolutional neural networks .",
    "the input data and parameters are defined over a vector space equipped with an inner product .",
    "the parameters are learned using a gradient descent algorithm that acts directly over the inner product space , avoiding the use of individual coordinates .",
    "derivatives for higher - order loss functions are also explicitly calculated in a coordinate - free manner , providing the basis for a gradient descent algorithm .",
    "this mathematical framework can be extended to other types of deep networks , including recurrent neural networks , autoencoders and deep boltzmann machines .",
    "another interesting future direction is to expand the capabilities of automatic differentiation ( ad ) into this coordinate - free realm , strengthening the hierarchical approach to ad @xcite .",
    "this paper has shown how to express a particular deep neural network , end - to - end , in a precise format . however",
    ", this framework should not be limited to only expressing previous results , and it should not be written off as simply a derivative calculation method . the stronger mathematical understanding of neural networks provided by this work should promote expansion into new types of networks .          f.  huang and y.  lecun .",
    "large - scale learning with svm and convolutional networks for generic object recognition . in _",
    "2006 ieee computer society conference on computer vision and pattern recognition _ , 2006 .",
    "p.  simard , b.  victorri , y.  lecun , and j.  denker .",
    "tangent prop  a formalism for specifying selected invariances in an adaptive network . in _ advances in neural information processing systems _ , pages 895903 , 1992 ."
  ],
  "abstract_text": [
    "<S> in this paper , a geometric framework for neural networks is proposed . </S>",
    "<S> this framework uses the inner product space structure underlying the parameter set to perform gradient descent not in a component - based form , but in a coordinate - free manner . </S>",
    "<S> convolutional neural networks are described in this framework in a compact form , with the gradients of standard  and higher - order  loss functions calculated for each layer of the network . </S>",
    "<S> this approach can be applied to other network structures and provides a basis on which to create new networks . </S>"
  ]
}