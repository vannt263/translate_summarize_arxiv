{
  "article_text": [
    "when we consider the problem of an agent ( artificial or biological ) interacting with its environment , its signal processing is naturally embedded in time . in such a scenario , predictiveness is a necessary condition for a feature to be useful in any behaviorally relevant way : a feature that does not hold information about the future is out - dated the moment it is processed and any action based on such a feature can only be expected to have random effects .",
    "similarly are we interested in predictive features of time - series like weather or business data because we want them to support informed decisions about the future .",
    "thus , the task of extracting information from the past that holds information about the future is crucial for neural signal processing as well as machine learning .",
    "accordingly , it has been addressed by different theoretical frameworks before . to name a few",
    ", @xcite introduced the framework of _ past - future information bottlenecks _ as a specific case of _ information bottlenecks _",
    "@xcite where the amount of information kept about the future by some state variable is traded against the simplicity of its encoding .",
    "the minimal set of state variables that preserves all available information about the future is called _ causal states _ and described in @xcite . for interactive settings , the trade - off between complexity and predictiveness of state representations",
    "was explored in @xcite .    in the context of machine learning and dimensionality reduction ( dr ) ,",
    "only few algorithms have addressed the task of extracting features whose future is most predictable , i.e. , the features that preserve most information about the future .",
    "recently , two methods have been proposed : _",
    "forecastable component analysis _ ( foreca )",
    "@xcite and _ predictable feature analysis _ ( pfa ) @xcite .",
    "foreca is based on the idea that predictable signals can be recognized by their low entropy in the power spectrum while white noise in contrast would result in a power spectrum with maximal entropy .",
    "pfa focuses on signals that are well predictable through autoregressive processes .",
    "advantages of foreca are its minimal assumptions about the signals to extract as well as being free of any hyper - parameters .",
    "its computational complexity though prevents its application to high - dimensional data .",
    "pfa on the other hand , while being much more efficient , makes much stronger assumption about the signals to extract .    in this work we present _ graph - based predictable feature analysis _ ( gpfa ) , a new algorithm for predictable feature extraction that ( like foreca ) makes little assumptions about the extracted signals but ( like pfa ) is efficient enough to be applied to high - dimensional data .",
    "gpfa also can be described in the framework of graph embedding .",
    "graph embedding provides us with a powerful framework that not only unifies more recent , mostly geometrically motivated dr algorithms like isomap @xcite , locally linear embedding ( lle ) @xcite , laplacian eigenmaps @xcite , locality preserving projections ( lpp ) @xcite , and graph - based slow feature analysis ( gsfa ) @xcite , but also classic ones like pca and fisher - lda @xcite .",
    "the framework also allows for kernel - based extension ( see @xcite and @xcite ) and it is applicable to non - vectorial data as well .    in the following section we introduce the gpfa algorithm .",
    "this includes a measure for predictability as well as a consistent estimate for it ( section  [ sec : measuring ] ) , establishing a link to the graph embedding framework ( section  [ sec : pfa - graphs ] ) , and the objective s relation to excess entropy ( section  [ sec : excess_entropy ] ) . after describing two useful heuristics in section  [ sec : heuristics ] ,",
    "the core algorithm is summarized in section  [ sec : algorithm ] and the final iterated algorithm in section  [ sec : iterated_gpfa ] .",
    "section  [ sec : related_methods ] briefly introduces the objectives of foreca and pfa and section  [ sec : experiments ] describes empirical experiments on three synthetic data sets and one real - world data set .",
    "limitations and future work are discussed in section  [ sec : discussion ] , followed by a conclusion in section[sec : conclusion ] .",
    "given a time series @xmath0 , @xmath1 , as training data that is assumed to be generated by a stationary stochastic process @xmath2 of order @xmath3 , the goal of gpfa is to find an orthogonal transformation @xmath4 leading to projected random variables @xmath5 , that are most predictable given the state of the @xmath3 previous time steps .",
    "we can understand predictability as the ability to predict the state in the next time step @xmath6 given the previous @xmath3 states with as little error as possible .",
    "we measure this prediction error through the average covariance matrix of @xmath7 given @xmath8 and minimize it in terms of its trace , i.e. , we minimize the sum of variances in all principal directions .",
    "formally , we look for the projection matrix @xmath9 with minimum @xmath10 where @xmath11 serves as a short - hand notation for @xmath8 and @xmath12 denotes the average over @xmath13 . for simplicity , we refer to this as `` minimizing the covariance '' in the following .",
    "when estimating the predictability of a given time series @xmath14 in practice , we can not simply rely on the corresponding sample covariances because the sample size for each conditional probability distribution @xmath15 is likely to be just one .",
    "therefore we use the @xmath16-nearest neighbor estimate instead , that is , we minimize @xmath17 with the set @xmath18 containing the indices of the @xmath16 nearest neighbors of @xmath19 plus @xmath13 itself .",
    "note , that the distance measure used for the @xmath16 nearest neighbors does not necessarily need to be euclidean .",
    "think for instance of `` perceived similarities '' of sounds or faces .",
    "finding the transformation @xmath9 that leads to the most predictable @xmath20 in the sense of becomes difficult through the circumstance that the predictability can only be evaluated _",
    "@xmath9 has been fixed .",
    "the circular nature of this optimization problem motivates the iterated algorithm described in section  [ sec : iterated_gpfa ] . as a helpful intermediate step",
    "we define a weaker objective for predictability that is conditioned on @xmath21 instead of @xmath22 and has a closed - form solution , namely minimizing @xmath23 via its @xmath16-nearest neighbor estimate @xmath24 analogous to @xmath25 , the set @xmath26 contains the indices of the @xmath16 nearest neighbors of @xmath27 plus @xmath13 itself . under certain mild mixing assumptions for the stochastic process",
    ", the text - book results on @xmath16-nearest neighbor estimates can be applied to auto - regressive time series as well @xcite .",
    "thus , in the limit of @xmath28 , @xmath29 , @xmath30 , the estimated covariance @xmath31 converges to @xmath32 - e[{\\boldsymbol{y}}_{t+1 } | { \\boldsymbol{x}}_t^{(p ) } = { \\mathbf{x}}_t^{(p ) } ] e[{\\boldsymbol{y}}_{t+1 } | { \\boldsymbol{x}}_t^{(p ) } = { \\mathbf{x}}_t^{(p)}]^t$ ] , i.e. , it is a consistent estimator of @xmath33 .    when measuring predictability , one assumption made about the process @xmath2 in the following is that it is already white , i.e. , @xmath34 = { \\mathbf{0}}$ ] and @xmath35 for all @xmath13 .",
    "otherwise components with lower variance would tend to have higher predictability _ per se_.      instead of optimizing the above objective directly , we reformulate it such that it can be interpreted as embedding of an undirected graph on the set of training samples .",
    "consider the graph to be represented by a symmetric connection matrix @xmath36 with weights @xmath37 whenever two nodes corresponding to vectors @xmath38 and @xmath39 from the training sequence are connected by an edge @xmath40 .",
    "further assume an orthogonal transformation @xmath41 for that graph with @xmath42 that minimizes @xmath43 intuitively , this term becomes small if the projections of points connected in the graph ( i.e. , nodes for which @xmath44 ) are close to each other , while there is no penalty for placing the projections of unconnected points far apart .    through a proper selection of the weights @xmath45 , the transformation @xmath9 can be used to maximize predictability in the sense of minimizing .",
    "this becomes clear by noting that the trace of the sample covariance @xmath46 , with @xmath47 being the sample mean , can always be formulated via pairwise differences of samples , since @xmath48 thus , by creating edges @xmath49 for all @xmath50 , @xmath51 , minimizing directly leads to the minimization of .",
    "note that for the construction of the graph , the data actually does not need to be represented by points in a vector space .",
    "data points also could , for instance , be words from a text corpus as long as there are either enough samples per word or there is an applicable distance measure to determine `` neighboring words '' for the @xmath16-nearest neighbor estimates .      to find the orthogonal transformation @xmath52 that minimizes ,",
    "let the training data be concatenated in @xmath53 , and let @xmath54 be a diagonal matrix with @xmath55 being the sum of edge weights connected to node @xmath56 .",
    "let further @xmath57 be the graph laplacian .",
    "then can be reduced to    @xmath58    it is beneficial to minimize the terms @xmath59 subject to the additional constraint @xmath60 . through this constraint",
    "the projected data points are normalized with respect to their degree of connectivity in every component @xmath61 , i.e. , @xmath62 .",
    "objective functions and constraints can be combined in the lagrange functions @xmath63 , which are minimized by the first ( `` smallest '' ) @xmath64 eigenvectors of the generalized eigenvalue problem    @xmath65    see @xcite for the analogous derivation of the one - dimensional case that was largely adopted here as well as for a kernelized version of the same problem .",
    "there are two heuristics that proved to be useful in improving the results in practice .",
    "first , while not being directly linked to predictability , results benefit significantly when the variance of the past is minimized simultaneously to that of the future .",
    "our intuition here is that having a compact representation of the past makes more efficient use of the limited data available .",
    "therefore additional edges @xmath66 are added to the graph for all @xmath50 , @xmath67 .",
    "note that this additional objective is related to the concept of _ _ causal states__@xcite , where all ( discrete ) states that share the same conditional distribution over possible futures are mapped to the same causal state .",
    "the proposed edges here have the effect of mapping states with _ similar _ futures to _ similar _ features .    as a second heuristic ,",
    "the graph above can be simplified by replacing the sample mean @xmath68 in @xmath69 by @xmath70 .",
    "this leads to @xmath71 inducing a graph with star - like structures .",
    "it is constructed by adding ( undirected ) edges @xmath72 for all @xmath73 .",
    "analogously , edges for reducing the variance of the past are given by @xmath74 for @xmath73 .",
    "we refer to the resulting algorithms as gpfa ( 1 ) and gpfa ( 2 ) , corresponding to the graphs defined through and , respectively . see figure  [ fig : graph_construction ] for an illustration of both graphs .",
    "the differences in performance are experimentally analyzed in section  [ sec : experiments ] .      in the following ,",
    "the core algorithm is summarized step by step , where training data @xmath75 is assumed to be white already or pre - processed accordingly ( in that case the same transformation has to be taken into account during subsequent feature extractions ) .",
    "lines starting with ( 1 ) and ( 2 ) indicate the steps for gpfa  ( 1 ) and gpfa  ( 2 ) , respectively .    1 .",
    "* calculate neighborhood * + for every @xmath76 , @xmath77 , calculate index set @xmath26 of @xmath16 nearest neighbors ( plus @xmath13 itself ) .",
    "2 .   * construct graph ( future ) * + initialize connection matrix @xmath78 to zero .",
    "for every @xmath79 , add edges , according to either 1 .",
    "@xmath80 or 2 .",
    "@xmath81 and + @xmath82 .",
    "3 .   * construct graph ( past ) * + for every @xmath67 , add edges , according to either 1 .",
    "@xmath83 or 2 .",
    "@xmath84 and + @xmath85 .",
    "* linear graph embedding * + calculate @xmath86 and @xmath87 as defined in section [ sec : graph_embedding ] .",
    "+ find the first ( `` smallest '' ) @xmath64 solutions to @xmath88 and normalize them , i.e. , @xmath89 .     and its @xmath16 nearest neighbors together with their successors in time .",
    "the distribution of the successors indicates that the first axis can be predicted with less uncertainty than the second axis .",
    "the dotted lines depict edges that are added to the graph according to the two variants of the algorithm .",
    "edges for minimizing the variance of the past are constructed analogously.,title=\"fig : \" ]   and its @xmath16 nearest neighbors together with their successors in time .",
    "the distribution of the successors indicates that the first axis can be predicted with less uncertainty than the second axis .",
    "the dotted lines depict edges that are added to the graph according to the two variants of the algorithm .",
    "edges for minimizing the variance of the past are constructed analogously.,title=\"fig : \" ]      as shown in section [ sec : pfa - graphs ] , the algorithm above produces features with a low average ( estimated ) covariance of @xmath90 . in many cases these features may already be predictable in themselves , i.e. , have a low average covariance of @xmath91 .",
    "there are , however , cases where the results of both objectives can differ significantly ( see figure  [ fig : pfa_problem ] for an example of such a case ) .",
    "also , the @xmath16-nearest neighbor estimates of the covariances become increasingly unreliable in higher - dimensional spaces .",
    "we propose an iterated version of the core algorithm as a heuristic to address these problems .",
    "first , an approximation of the desired covariances of @xmath92 can be achieved by rebuilding the graph according to neighbors of @xmath93 , not @xmath94 .",
    "this in turn may change the whole optimization problem , which is the reason to repeat the whole procedure several times .",
    "second , calculating the sample covariance matrices based on the @xmath16 nearest neighbors of @xmath95 instead of @xmath96 counteracts the problem of unreliable @xmath16-nearest neighbor estimates in high - dimensional spaces , since @xmath97 .",
    "the resulting ( iterated ) gpfa algorithm works like this :    * calculate neighborhoods @xmath26 of @xmath94 for @xmath98 . * perform steps 2 and 3 of gpfa as described in section[sec : algorithm ] .",
    "* calculate projections @xmath99 for @xmath100 . *",
    "calculate neighborhoods @xmath25 of @xmath93 for @xmath98 .",
    "* start from step b ) , using @xmath25 instead of @xmath26 .    where steps b ) to e )",
    "are either repeated for r iterations or until convergence .",
    "note that in general there is no need for the dimensionality @xmath64 of the intermediate projections @xmath101 to be the same as for the final feature space .",
    "see figure  [ fig : results_kai ] in section  [ sec : experiments ] on how results improve with the number of iterations during an experiment .     and",
    "@xmath102 differ significantly .",
    "points from two neighborhoods are shown together with their immediate successors in time .",
    "the distributions of the successors indicate that the first axis would be the most predictable direction .",
    "however , projecting all points on the first axis would result in a feature that is highly unpredictable.,scaledwidth=40.0% ]      the mutual information between past and future has been used as a natural measure of complexity for stochastic processes and has been referred to  among others  as _ excess entropy _ or _ predictive information _",
    "( see for instance @xcite , or @xcite , and references therein ) . in this section",
    "we show that in some special cases the objective of gpfa corresponds to extracting features with maximal predictive information .",
    "consider an one - dimensional stationary stochastic process @xmath103 with markov property .",
    "its predictive information is given as @xmath104 where @xmath105 denotes the entropy and @xmath106 denotes the entropy rate of @xmath103 .",
    "if we assume @xmath107 to be normally distributed  which can be justified by the fact that it corresponds to the mixture of a potentially high number of distributions from the original high - dimensional space  then its differential entropy is given by @xmath108 and is thus a strictly increasing function of its variance @xmath109 .",
    "now , recall that @xmath2 is assumed to have zero mean and covariance @xmath110 .",
    "thus , @xmath111 holds independently of the selected transformation @xmath9 which makes @xmath105 independent of @xmath9 too .",
    "let us further assume the distribution of predictions @xmath112 , @xmath113 , to be normally distributed as well and their variances @xmath114 to be the same for all @xmath13 .",
    "minimizing then minimizes @xmath115 and thus the whole entropy rate @xmath116 .",
    "therefore , under the stated assumptions , minimizing predictability gets equivalent to maximizing the predictive information of the extracted feature @xmath117 ( to the extend that the heuristics yield the optimal solution ) .",
    "this equality of predictability and predictive information does not , however , generalize to the case of multivariate feature extraction .",
    "the differential entropy @xmath118 of a multivariate normal distribution is a strictly increasing function of the determinant of the covariance but from a minimal trace in does not ( in general ) follow a minimal determinant .",
    "in this section we briefly summarize the objectives of the previously mentioned algorithms foreca and pfa that are also designed to extract predictable features .      in case of foreca",
    ", @xmath2 is assumed to be a stationary second - order process and the goal of the algorithm is finding an extraction vector @xmath119 such that the projected random variables @xmath120 are as _ forecastable _ as possible .",
    "for the definition of _ forecastability _ , first consider the signal s autocovariance function @xmath121 , with @xmath122 being the mean value and the corresponding autocorrelation function @xmath123 .",
    "the spectral density of the process can be calculated as the fourier transform of the autocorrelation function , i.e. , as @xmath124 with @xmath125 being the imaginary unit .    since @xmath126 and @xmath127",
    ", the spectral density can be interpreted as a probability density function and thus its entropy calculated as @xmath128 for white noise the spectral density becomes uniform with entropy @xmath129 .",
    "this motivates the definition of _ forecastability _ as @xmath130 with values between @xmath131 ( white noise ) and @xmath132 ( most predictable ) . since @xmath133 ) is invariant to scaling and shifting , @xmath21 can be assumed to be white , without loss of generality .",
    "the resulting optimization problem @xmath134 then is solved by an em - like algorithm that uses weighted overlapping segment averaging ( wosa ) to estimate the spectral density of a given ( training ) time series . by subsequently finding projections which are orthogonal to the already extracted ones ,",
    "the approach can be employed for finding projections to higher dimensional subspaces as well . for details about foreca",
    "see @xcite .",
    "the motivation behind pfa is finding an orthogonal transformation @xmath4 as well as coefficient matrices @xmath135 , with @xmath136 , such that the autoregressive prediction error of order @xmath137 , @xmath138 is minimized .",
    "however , this is a difficult problem to optimize because the optimal values of @xmath9 and @xmath139 mutually depend on each other .",
    "therefore the solution is approached via a related but easier optimization problem : let @xmath140 be a vector containing the @xmath137-step history of @xmath141 .",
    "let further @xmath142 contain the coefficients that minimize the error of predicting @xmath141 from its own history , i.e. , @xmath143 .",
    "then minimizing @xmath144 with respect to @xmath9 corresponds to a pca ( in the sense of finding the directions of smallest variance ) on that prediction error . minimizing this prediction error however does not necessarily lead to features @xmath99 that are best for predicting their own future because the calculated prediction was based on the history of @xmath141 , not @xmath145 alone .",
    "therefore an additional heuristic is proposed that is based on the intuition that the inherited errors of @xmath146 times repeated autoregressive predictions create an even stronger incentive to avoid unpredictable components .",
    "finally , @xmath147 is minimized with respect to @xmath9 , where @xmath148 contains the coefficients that minimize the prediction error @xmath149 .",
    "like gpfa , pfa includes a pre - processing step to whiten the data . for further details about pfa",
    "see @xcite .",
    "we conducted a set of experiments to analyze different aspects of the proposed algorithm and to compare it to foreca and pfa .",
    "all experiments have in common that @xmath64 predictable signals are extracted from the test set and their predictability determined according to . for the first three data sets ,",
    "the choice of @xmath150 is given by how the data is generated .",
    "for the last experiment , best results where produced with @xmath151 .",
    "optimal parameters @xmath137 and @xmath146 for pfa were determined in each experiment by searching through all combinations of @xmath152 and @xmath153 .      in a first experiment",
    "a simple agent in a two - dimensional room was simulated .",
    "the room was restricted to values between @xmath154 and @xmath155 in both directions and the agent started at position @xmath156 .",
    "each time step , noise with variance @xmath157 was added to its @xmath158 and its @xmath159 position .",
    "when the agent reached one of the corners of the room ( with a tolerance of @xmath160 in both directions ) , it was teleported back to @xmath156 in the next step . the two generated signals ( one for each dimension of the room )",
    "were augmented with several dimensions containing uniformly distributed noise to create a data set to test the signal extraction on .",
    "@xmath161 signals were extracted by foreca , pfa , gpfa , and  for comparison  by random orthogonal extraction vectors .",
    "different experiments were performed to test the effects of varying choices for the number of additional noisy dimensions , the size @xmath162 of the training ( and test ) set , the number of iterations @xmath163 of gpfa , and the neighborhood size @xmath16 .",
    "if not varied , we used @xmath164 noisy dimensions , a training and test set of size @xmath165 , @xmath166 iterations , and @xmath167 .",
    "for pfa , @xmath168 and @xmath169 were used .",
    "the second experiment was designed to challenge the heuristics used by the different algorithms .",
    "data was generated through a process @xmath170 where @xmath171 , @xmath172 , was drawn from a normal distribution .",
    "thus , half of the variance in this process is completely predictable by knowing the previous state . as for the first experiment ,",
    "varying numbers of additional noisy dimensions ( normally distributed ) as well as varying values for @xmath162 , @xmath163 , and @xmath16 were tested .",
    "the default values were @xmath164 noisy dimensions , @xmath165 , @xmath166 , and @xmath173 .",
    "for pfa , @xmath168 and @xmath169 were used .",
    "one predictable feature was extracted ( @xmath174 ) .      for a third experiment ,",
    "a signal was generated through a process @xmath175 , where @xmath171 was drawn from a discrete uniform distribution of values between one and four ( see the left of figure  [ fig : examples ] for an example of subsequent samples generated by this process ) .",
    "the motivation behind this time series is modeling an idealized system consisting of @xmath176 different states with random transitions to a set of four successor states ( @xmath177 ) . as before , different experiments were performed with varying values for additional noisy dimensions ( uniform noise ) as well as varying values for @xmath162 , @xmath163 , and @xmath16 .",
    "defaults were set to @xmath178 noisy dimensions , @xmath165 , @xmath166 , and @xmath179 .",
    "for pfa , @xmath180 and @xmath169 were used .",
    "successive data points generated for the second experiment ( markov chain ) .",
    "the right plot shows an example frame generated by modified simulator from the mario ai challenge .",
    "the highlighted square indicated the @xmath181 pixels extracted for the fourth experiment ( super mario).,title=\"fig : \" ]   successive data points generated for the second experiment ( markov chain ) .",
    "the right plot shows an example frame generated by modified simulator from the mario ai challenge .",
    "the highlighted square indicated the @xmath181 pixels extracted for the fourth experiment ( super mario).,title=\"fig : \" ]      as a more realistic fourth problem , we generated data from the _ mario ai challenge _ @xcite .",
    "we modified the simulator such to return raw visual input in gray - scale without text labels .",
    "the raw input was scaled from @xmath182 down to @xmath183 dimensions and then the final data points were taken from a small window of @xmath184 pixels at a position where much of the game dynamics happened ( see the right of figure  [ fig : examples ] for an example ) .",
    "the dimensionality of the problem was varied between @xmath178 and @xmath181 dimensions through a preceding pca step .",
    "experiments were repeated for different numbers of predictable features ( @xmath185 ) .",
    "other parameters were set to @xmath165 , @xmath166 , and @xmath186 ( which was the best choice for all configurations ) .",
    "we conclude from the experiments that gpfa has advantages over foreca and pfa in several settings . from the start",
    ", foreca had to be excluded from many experimental settings due to its high cpu and memory demands .",
    "therefore the following discussion focuses on the comparison of pfa and gpfa .    in the first experiment ( _ teleporter room _ ) , the most predictable signals were found by gpfa  ( 2 ) for most configurations of parameters @xmath163 and @xmath16 .",
    "gpfa  ( 1 ) extracted better predictable signals than pfa as long as the number of iterations was large enough ( @xmath187 ) and @xmath16 was in the right range ( @xmath188 ) .",
    "compared to other algorithms , gpfa ( 1 ) tended to need more training data ( see the upper - right of figure  [ fig : results_teleporter ] ) .     of training and test , iterations @xmath163 of gpfa , neighborhood size @xmath16 ) .",
    "experiments were repeated @xmath176 times .",
    "note that foreca is included in the first plot for up to @xmath189 noisy dimensions.,title=\"fig : \" ]   of training and test , iterations @xmath163 of gpfa , neighborhood size @xmath16 ) .",
    "experiments were repeated @xmath176 times .",
    "note that foreca is included in the first plot for up to @xmath189 noisy dimensions.,title=\"fig : \" ]   of training and test , iterations @xmath163 of gpfa , neighborhood size @xmath16 ) .",
    "experiments were repeated @xmath176 times .",
    "note that foreca is included in the first plot for up to @xmath189 noisy dimensions.,title=\"fig : \" ]   of training and test , iterations @xmath163 of gpfa , neighborhood size @xmath16 ) .",
    "experiments were repeated @xmath176 times .",
    "note that foreca is included in the first plot for up to @xmath189 noisy dimensions.,title=\"fig : \" ]    in the second experiment ( _ predictable noise _ ) , it turns out that pfa has difficulties extracting the two predictable signals at all from the data ( see figure  [ fig : results_kai ] ) .",
    "since the auto - regressive prediction model used by pfa actually is able to describe the two predictable signals , it seems to be its heuristics that have problems in this setting .",
    "gpfa would also fail to extract the signals without the additional step that minimizes the variance of the past ( data not shown ) .",
    "as in the first experiment , gpfa  ( 2 ) turned out to be more robust with respect to the selection of @xmath163 and @xmath16 .    [",
    "cols=\"^,^ \" , ]     with an increasing number of training samples , the repeated nearest - neighbor search may also become a bottleneck .",
    "however , all but the first search are performed in only @xmath64 dimensions , which can be implemented efficiently through methods like k - d trees ( see @xcite for an overview of multi - dimensional access methods ) .",
    "for the first search in high - dimensional space , sub - sampling the data set may be sufficient .",
    "we presented gpfa , a new algorithm for unsupervised extraction of predictable features from high - dimensional time - series .",
    "we quantified predictability very generically as low variance in the next time step given the current one and re - formalized it to fit into the framework of graph embedding .",
    "experimentally we compared gpfa with two existing dr methods ( foreca and pfa ) that also aim for predictable features and conclude that the results are very competitive .",
    "compared to foreca , gpfa scales to much higher dimensions and it performed better than pfa in most of the tested settings .",
    "we believe that gpfa also makes a theoretical contribution since it is easy to link to other approaches due to its very general , probabilistic formulation , as exemplified here by its link to the information theoretic measure of excess entropy .",
    "collomb , g. : non parametric time series analysis and prediction : uniform almost sure convergence of the window and k - nn autoregression estimates .",
    "statistics : a journal of theoretical and applied statistics * 16*(2 ) , 297307 ( 1985 )      escalante , a.n . , wiskott , l. : how to solve classification and regression problems on high - dimensional data with a supervised extension of slow feature analysis . journal of machine learning research ( jmlr ) * 14 * , 36833719 ( 2013 )                      yan , s. , xu , d. , zhang , b. , zhang , h.j . , yang , q. , lin , s. : graph embedding and extensions : a general framework for dimensionality reduction .",
    "pattern analysis and machine intelligence , ieee transactions on * 29*(1 ) , 4051 ( 2007 ) ."
  ],
  "abstract_text": [
    "<S> we propose a new method for the unsupervised extraction of predictable features from high - dimensional time - series , where high predictability is understood very generically as low variance in the distribution of the next data point given the current one . </S>",
    "<S> we show how this objective can be understood in terms of graph embedding as well as how it corresponds to the information - theoretic measure of excess entropy in special cases . </S>",
    "<S> experimentally , we compare the approach to two other algorithms for the extraction of predictable features , namely foreca and pfa , and show how it is able to outperform them in certain settings . </S>"
  ]
}