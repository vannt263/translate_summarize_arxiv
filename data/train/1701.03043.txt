{
  "article_text": [
    "suppose that @xmath0 distributed agents constitute a bidirectionally connected network and sense correlated signals under sparse measurement errors .",
    "the measurement equation of agent @xmath1 is @xmath2 where @xmath3 is the measurement vector , @xmath4 is the sensing matrix , @xmath5 is the unknown signal vector , and @xmath6 is the unknown sparse error vector .",
    "we are particularly interested in a certain correlation pattern of the signal vectors , where the signal matrix @xmath7 \\in \\mathcal{r}^{n \\times l}$ ] is group sparse , meaning that @xmath8 is sparse and its nonzero entries appear in a small number of common rows .",
    "define @xmath9 as the measurement matrix and @xmath10 as the sparse error matrix , the matrix form of the agents measurement equations is @xmath11 +      \\mathbf{s}.\\ ] ] given @xmath12 and @xmath13 s , the goal of the network is to recover @xmath8 and @xmath14 from the linear measurement equation .",
    "the recovery of group sparse ( also known as block sparse @xcite or jointly sparse @xcite ) signals finds a variety of applications such as direction - of - arrival estimation @xcite , collaborative spectrum sensing @xcite and motion detection @xcite .",
    "a well - known model to recover group sparse signals is group lasso ( least absolute shrinkage and selection operator ) @xcite , which solves @xmath15\\|_f^2.\\end{aligned}\\ ] ] here @xmath16 is a nonnegative trade - off parameter .",
    "a key assumption leading to the success of such model is the sub - gaussianity of errors .",
    "however , in many applications , the measurements of the agents may be seriously contaminated or even missing due to uncertainties such as sensor failure or transmission errors .",
    "this kind of measurement errors are often sparse @xcite .",
    "hence , a natural extension of is to exploit the structures of both the signal matrix @xmath8 and the sparse error matrix @xmath14 by solving @xmath17 + \\mathbf{s}. \\nonumber\\end{aligned}\\ ] ] this model is termed as robust group lasso , whose performance guarantee is given in @xcite .",
    "under mild conditions , the robust group lasso model is able to simultaneously recover the true values of @xmath8 and @xmath14 with high probability .",
    "this paper develops efficient algorithms to solve the robust group lasso model .",
    "our contributions are as follows .    1 .",
    "we propose a centralized algorithm that is based on the alternating direction method of multipliers ( admm ) , a powerful operator - splitting technique .",
    "one subproblem of the centralized algorithm is the traditional group lasso model , which is approximately solved by a block coordinate descent ( bcd ) approach through successively estimating the row - support of the signal matrix @xmath8 .",
    "we develop decentralized versions of the above algorithm that are suitable for autonomous computation over large - scale networks .",
    "since estimating the row - support of the signal matrix @xmath8 requires collaborative information fusion of all the agents , we propose to achieve inexact information fusion through dynamic average consensus techniques , which only require information exchange among neighboring agents .",
    "matrices are denoted by bold uppercase letters and vectors are denoted by bold lowercase letters . for a matrix @xmath18 , @xmath19 denotes its @xmath20-th row , @xmath21 denotes its @xmath22-th column , while @xmath23 denotes its @xmath24-th element .",
    "the @xmath25-norm of @xmath18 is @xmath26 , the @xmath27-norm is @xmath28 , and the frobenius norm is @xmath29 .",
    "the multi - agent network is described as a bidirectional graph @xmath30 .",
    "if two agents @xmath31 are neighbors , then they can communicate with each other within one hop , and @xmath32 is a bidirectional communication edge .",
    "optimally solving ( [ ee3 ] ) is nontrivial since the objective function is a weighted summation of two nonsmooth functions @xmath33 and @xmath34 , where @xmath8 and @xmath14 are entangled in the constraint .",
    "therefore we resort to the alternating direction method of multipliers ( admm ) to split the two entangled variables @xmath8 and @xmath14 such that the resulting subproblems are easier to solve .      the augmented lagrangian function of ( [ ee3 ] ) is @xmath35+\\mathbf{s}-\\mathbf{m } \\rangle \\nonumber \\\\      & + \\frac{\\beta}{2}\\|[\\mathbf{a}_{(1)}\\mathbf{y}_{1 } , \\cdots , \\mathbf{a}_{(l)}\\mathbf{y}_{l}]+\\mathbf{s}-\\mathbf{m}\\|_{f}^{2 } ,      \\nonumber\\end{aligned}\\ ] ] where @xmath36 is the lagrange multiplier and @xmath37 is a positive penalty parameter .",
    "the admm alternatingly minimizes the augmented lagrangian function with respect to @xmath8 and @xmath14 , and then updates the lagrange multiplier @xmath38 @xcite . at time @xmath39",
    ", the admm works as follows .",
    "first , fixing @xmath40 and @xmath41 , we minimize the augmented lagrangian function respect to @xmath8 to get @xmath42 .",
    "simple manipulation shows that it is equivalent to @xmath43+\\mathbf{s}(t)-\\mathbf{m}-\\frac{\\mathbf{z}(t)}{\\beta}\\|_{f}^{2}. \\nonumber\\end{aligned}\\ ] ] note that ( [ ee12 ] ) is a standard group lasso problem that generally does not have a closed - form solution .",
    "we will develop an efficient algorithm to solve ( [ ee12 ] ) later in this section .",
    "second , fixing @xmath44 and @xmath41 , we minimize the augmented lagrangian function respect to @xmath14 to get @xmath45 . again , combining the linear term with the quadratic term of @xmath14 yields @xmath46+\\mathbf{s}-\\mathbf{m}-\\frac{\\mathbf{z}(t)}{\\beta}\\|_{f}^{2}.      \\nonumber\\end{aligned}\\ ] ] denoting @xmath47-\\mathbf{z}(t)/\\beta$ ] , ( [ ee10 ] ) has a closed - form solution given by @xmath48 where @xmath49 is the sign function ; @xmath50 and @xmath51 denote the @xmath52-th entries of @xmath45 and @xmath53 , respectively .",
    "note that the term @xmath54 can be viewed as the support detector of the @xmath52-th element of @xmath14 .",
    "if @xmath54 is smaller than the threshold @xmath55 , then @xmath50 is set to be zero .",
    "finally , given @xmath44 and @xmath56 , the lagrange multiplier @xmath38 is updated according to the following formula @xmath57+\\mathbf{s}(t+1)-\\mathbf{m}\\big ) .",
    "\\nonumber\\end{aligned}\\ ] ]    since the update of @xmath14 in ( [ ee11 ] ) and the update of @xmath38 in ( [ ee19 ] ) are both simple , now we focus on the update of @xmath8 in ( [ ee12 ] ) that is the bottleneck of the admm .",
    "observe that in ( [ ee12 ] ) the @xmath25-norm term is separable with respect to @xmath58 s but nonsmooth , while the frobenius term is smooth but nonseparable with respect to @xmath58 s .",
    "therefore , in this paper we solve ( [ ee12 ] ) with the block coordinate descent ( bcd ) algorithm that has shown to be an efficient tool to handle this special problem structure @xcite .      to set up the iterative bcd algorithm that solves ( [ ee12 ] ) at time @xmath39 , we divide time @xmath39 into @xmath59 slots . at time",
    "@xmath39 slot @xmath60 ( @xmath61 ) , we linearize the frobenius norm term in ( [ ee12 ] ) with respect to @xmath62 and add an extra quadratic regularization term , which gives @xmath63 where @xmath64 is a positive proximal parameter and the @xmath1-th column of @xmath65 is defined as @xmath66 note that ( [ ee13 ] ) is equivalent to @xmath67 which has a closed - form solution given by the soft - thresholding operator @xcite . denote @xmath68 whose @xmath69-th row is given by @xmath70 .",
    "also denote @xmath71 as the solution of ( [ ee13_tmp ] ) .",
    "the @xmath69-th row of @xmath72 is @xmath73    again , note that the term @xmath74 can be viewed as the row - support detector of the @xmath69-th row of @xmath8 . if @xmath74 is smaller than the threshold @xmath75 , then @xmath76 is set to be zero .      the centralized admm to solve the robust group lasso model ( [ ee3 ] ) is summarized in table i. each iteration of the admm includes an inner - loop bcd subroutine that updates @xmath8 through solving ( [ ee12 ] ) , the update of @xmath14 that has a closed - form solution ( [ ee11 ] ) , and the update of @xmath38 in ( [ ee19 ] ) .",
    "the admm parameter @xmath37 can be any positive value , though its choice may influence the convergence rate .",
    "the bcd parameter @xmath64 is set to be the minimum of largest eigenvalues of @xmath77 that guarantees the convergence of the bcd subroutine @xcite .",
    "as long as @xmath64 is properly chosen and @xmath59 is large enough , the bcd subroutine is able to solve the subproblem ( [ ee12 ] ) with enough accuracy such that the admm converges to the global minimum of the convex program ( [ ee3 ] ) .",
    "the algorithm outlined in table i is centralized , which means that a fusion center is necessary to gather information from all the agents and conduct optimization .",
    "this centralized scheme is sensitive to the failure of the fusion center , requires multi - hop communication within the network , and is hence unscalable with respect to the networks size . in view of the need of decentralized optimization for large - scale networks ,",
    "we discuss how to implement it in a decentralized manner , as shown in the next section .",
    "[ tab1 ]    .algorithm 1 : centralized robust group lasso [ cols= \" < \" , ]                  the decentralized group lasso algorithm is outlined in table ii .",
    "it is very close to the centralized algorithm in table i , except that the row - support detector is successively approximated through static and dynamic average consensus strategies .",
    "if the static average consensus strategy is adopted , then at time @xmath39 slot @xmath60 , the network needs @xmath78 rounds of information exchange .",
    "the number of round reduces to one in the two dynamic average consensus strategies .",
    "observe that in each round of first - order dynamic average consensus , agent @xmath1 requires @xmath79 from all of its neighbors @xmath80 .",
    "however , in each round of second - order dynamic average consensus , agent @xmath1 requires both @xmath79 and @xmath81 from all of its neighbors @xmath80 .",
    "therefore , the second - order strategy doubles the communication cost per time slot , compared to its first - order counterpart .    with particular note ,",
    "when @xmath78 is set to be large enough in the static average consensus strategy , the average consensus is exact .",
    "therefore , the resulting decentralized algorithm enjoys the same convergence guarantee as the centralized one , at the cost of unaffordable communication cost .",
    "embedding the two dynamic average consensus strategies saves remarkable communication cost , but makes convergence analysis a challenging task .",
    "we will leave it as our future work .",
    "in addition , to avoid possible computational instability , we also set safeguards to the value of @xmath82 .",
    "if going beyond the region of @xmath83 $ ] , its value is set to the nearest boundary .",
    "in the numerical experiments , we consider a network of @xmath84 agents .",
    "the dimension of every signal vector is @xmath85 , while the dimension of every measurement vector is @xmath86 .",
    "the group sparse signal matrix @xmath87 has @xmath88 nonzero rows ( row sparsity ratio is @xmath89 ) , whose positions are uniformly randomly chosen .",
    "the amplitudes of the nonzero elements follow i.i.d",
    ". uniform distribution within @xmath90 $ ] .",
    "elements of every sensing matrix @xmath91 follow i.i.d .",
    "standard normal distribution .",
    "the sparse error matrix @xmath92 has @xmath93 nonzero elements ( sparsity ratio is @xmath94 ) , whose positions are uniformly randomly chosen and the amplitudes follow i.i.d .",
    "uniform distribution within @xmath90 $ ] .    in the robust group lasso model , the weight parameter @xmath95 .",
    "the admm parameter @xmath37 is also set as @xmath96 .",
    "the bcd parameter @xmath64 is set to be the minimum of largest eigenvalues of @xmath77 .",
    "every iteration of the admm algorithm is divided into @xmath97 slots so as to run the bcd subroutine . for the static average consensus strategy",
    ", we let @xmath98 , meaning that each slot requires @xmath99 rounds of communication . for the dynamic average consensus strategies , we let the safeguards @xmath100 and @xmath101 .",
    "the performance metric is relative error , defined as the frobenius distance between the true @xmath102 $ ] solving and the estimated one by admm , normalized by the frobenius norm of @xmath102 $ ] .",
    "we first compare the centralized algorithm and the three decentralized ones , as depicted in fig .",
    "[ eps : fig1 ] . the connectivity ratio of the network ( the percentage of randomly connected edges out of all possible ones ) is @xmath103 .",
    "the curve of the centralized algorithm coincides with that using static average consensus .",
    "recall that static average consensus incurs @xmath99 round of communications at every time slot , and is hence expensive .",
    "in contrast , the dynamic average consensus strategies demonstrate satisfactory convergence properties , though yielding slightly degraded estimates . particularly , the second - order dynamic average consensus is close to the centralized one in terms of the relative error .    in the second set of numerical experiments , we vary the connectivity ratio to observe its impact on the decentralized algorithms , as shown in fig . [ eps : fig2 ] .",
    "when the connectivity ratio decreases , the performance of the static average consensus degrades significantly .",
    "the reason is that a lower connectivity ratio reduces the speed of network information fusion , and hence makes the static average consensus less accurate under a given @xmath78 .",
    "the two dynamic average consensus strategies , on the other hand , are not very sensitive to the variation of connectivity ratio .",
    "the numerical experiments validate the effectiveness of using dynamic average consensus to decentralize computation over networks .",
    "though its theoretical properties in tracking problems have been investigated @xcite , its interplay with the overall optimization scheme is still unclear , and shall be our future research focus .",
    "d.  malioutov , m.  etin , and a.  s.  willsky , `` a sparse signal reconstruction perspective for source localization with sensor arrays , '' _ ieee transactions on signal processing _ , vol .",
    "53 , no .  8 , pp .  30103022 , 2005 .",
    "f.  zeng , c.  li and z.  tian , `` distributed compressive spectrum sensing in cooperative multihop cognitive networks , '' _ ieee journal of selected topics in signal processing _ , vol .  5 , no .  2 , pp .  3748 , 2011 .",
    "j.  meng , w.  yin , h.  li , e.  hossain , and z.  han , `` collaborative spectrum sensing from sparse observations in cognitive radio networks , '' _ ieee journal on selected areas in communications _ ,",
    "29 , no .  2 , pp .",
    "327337 , 2011 .",
    "e.  dallanese , j.  a.  bazerque , and g.  b.  giannakis , `` group sparse lasso for cognitive network sensing robust to model uncertainties and outliers , '' _ physical communication _ ,",
    "vol . 5 , no .",
    "2 , pp . 161172 , 2012 .",
    "m.  razaviyayn , m.  hong and z.  luo , `` a unified convergence analysis of block successive minimization methods for nonsmooth optimization , '' _ siam journal on optimization _ ,",
    "32 , no .  2 , pp .",
    "11261153 , 2013 ."
  ],
  "abstract_text": [
    "<S> this paper considers the recovery of group sparse signals over a multi - agent network , where the measurements are subject to sparse errors . </S>",
    "<S> we first investigate the robust group lasso model and its centralized algorithm based on the alternating direction method of multipliers ( admm ) , which requires a central fusion center to compute a global row - support detector . to implement it in a decentralized network environment , </S>",
    "<S> we then adopt dynamic average consensus strategies that enable dynamic tracking of the global row - support detector . </S>",
    "<S> numerical experiments demonstrate the effectiveness of the proposed algorithms .    </S>",
    "<S> decentralized optimization , dynamic average consensus , group sparsity , alternating direction method of multipliers ( admm ) </S>"
  ]
}