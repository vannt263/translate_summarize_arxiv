{
  "article_text": [
    "what makes a star look different from a galaxy in a deep image ?",
    "this seemingly very simple question hides the much more complicated issue of allocating a size and a scale to objects observed in the sky , which has concerned observers and theorists throughout the 20th century .",
    "the problem of classifying stars and galaxies in large scale surveys is a long - standing one .",
    "it has been encountered back in the early 1990 s ( e.g. the apm survey , @xcite ) and poses a major challenge for all recent and large imaging cosmological surveys , including the dark energy survey ( des ) ( http://www.darkenergysurvey.org/ ) and euclid ( http://sci.esa.int/euclid ) , which have been designed to uncover the nature of dark energy ( de ) .",
    "one common denominator of the wide variety of observational probes constraining de is the necessity to select pure samples of galaxies .",
    "more specifically , all the surveys must differentiate galaxies at cosmological distances from local objects , to obtain pure , or at least well - understood , samples .    in the area of ",
    "precision cosmology \" , any source of systematic error is likely to play a decisive role and needs to be taken into account in order to refine the standard inflationary big bang picture .",
    "an example of a scientific question for which star / galaxy separation is a potentially critical systematic is the precision measurement of primordial non - gaussianities ( png ) .",
    "these manifest themselves by making the bias of a given type of tracers of dark matter halos strongly scale - dependent .",
    "this effect can easily be mimicked by any local systematic effect adding power at large scales and correlated with the galaxies . as the stellar distribution in the milky way is across large angular scales ,",
    "star / galaxy separation is likely to introduce systematic errors in the measurement of png .",
    "another example is the effect of occultation of galaxies by stars of comparable magnitudes .",
    "@xcite showed that this effect constitutes a source of systematic error in the measurement of angular and photometric distributions of luminous red galaxies .",
    "photometric effects associated with faint stars could therefore partially account for the excess power seen in @xcite for the megaz luminous red galaxy survey .",
    "this paper gives two other examples , in the case of weak lensing ( wl ) and large scale structures ( lss ) measurements , where star / galaxy separation is a key systematic , which needs to be taken into account in order to properly constrain de .",
    "the outline of this paper is as follows . in section 2",
    ", we present the dark energy survey ( des ) and the `` des - like '' simulations which we base our analysis on . in section 3 ,",
    "we study the impact of star / galaxy misclassification on the measurement of the cosmological parameters , in the case of the wl and lss probes , and show how the requirements on the statistical and systematic errors propagate into new requirements on the quality of star / galaxy separation . in section 4 we summarise the current methods for star / galaxy classification and the motivations for our multi - parameter approach .",
    "the details of the method are presented in section 5 . in section 6",
    ", we compare our star / galaxy classification tool to the ones provided by other methods and confront these results to the science requirements derived in section 3 .",
    "finally , we summarise our main conclusions in section 7 .",
    "the dark energy survey ( des ) is an imaging survey of @xmath0 sq - degrees on southern sky , utilising the four meter blanco telescope in chile",
    ". it will provide imaging of @xmath1 million galaxies in five filters ( _ g _ , _ r _ , _ i _ , _ z _ and _ y _ ) .",
    "photometric redshifts will be obtained from the colour information to produce a three dimensional survey .",
    "the main goal of des is to determine the dark energy equation of state parameter , @xmath2 , and other key cosmological parameters to high precision .",
    "des will measure @xmath2 using four complementary techniques in a single survey : counts of galaxy clusters ( gc ) ( with synergy with clusters detected by the sunyaev - zeldovich effect with the south pole telescope ) , weak gravitational lensing ( wl ) , galaxy power spectra and type ia supernovae ( sne ) .",
    "it is expected that the uncertainty on @xmath2 will be only a few percent for each probe ( see @xcite , for detailed parameterisations and statistics ) .",
    "the science requirements of des drove the construction of a new camera , the dark energy camera ( decam ) , which had its first light in september , 2012 , and the survey has started in september , 2013 .",
    "as part of the process of testing and validation of the des data management ( desdm ) system @xcite , a series of detailed simulations have been designed to serve as a test - bench for the development of the pipelines and for verifying the scientific reach of the experimental channels . each of these iterations of the simulations are dubbed  data challenges \" ( dc ) .",
    "the simulation starts with the creation of galaxy catalogs stemming from an n - body simulation ( @xcite ) and detailed models of the milky way galaxy @xcite for the star component .",
    "these are merged and fed to an image simulator which includes atmospheric and instrumental effects .",
    "the resulting images serve as inputs for desdm and are processed as the data will be : the code _ sextractor _",
    "@xcite produces a catalogue of more than 300 parameters encapsulating information about each detected object .",
    "the most relevant features of these simulations for our study are :    * the seeing is introduced as a function of observing time ; * the galaxy shapes have been implemented using a sersic profile which matches the observed profile ; * the point spread function ( psf ) takes into consideration the seeing for that time , the optics and the distortion as a function of separation from the optical axis .",
    "the results shown in this paper are based on the latest release ( internal to the des collaboration ) of simulated data , dc6 , which covers approximately 140 square degrees to the full des depth , corresponding to about 10 nights of observations .",
    "we select from it the objects with a model magnitude in the _ i _ band brighter than @xmath3 , as they are the ones most likely to be detected with des .",
    "des will be among the first surveys to combine in a single project the observation of the four preferred dark energy probes , as identified by the dark energy task force ( detf ) @xcite .",
    "sne and baryonic acoustic oscillation ( bao ) constrain the expansion of the universe as a whole and are therefore referred to as _",
    "purely geometric_. wl and gc constrain both the expansion on the universe and the growth of large scale structures ( lss ) ( see @xcite for a complete review ) .",
    "in order to properly constrain de , the broad variety of measures carried out within each probe must meet certain requirements defined by des science teams .",
    "while there is no unique way to specify the constraints on dark energy experiments and probes , the figure of merit ( fom ) , defined by the detf , provides a useful metric .",
    "if we parameterise the time evolution of de by the equation of state @xmath4 , where @xmath5 is the cosmic scale factor and @xmath6 is the redshift of an object emitting at time t , the fom is defined as the reciprocal of the area of the error ellipse enclosing @xmath7 confidence limit in the @xmath8-@xmath9 plane .",
    "larger fom indicates smaller errors and therefore greater accuracy on the measurement of the parameters .    in other words , reaching the fom goals requires to minimise the error on @xmath8 and @xmath9 .",
    "since the total error is the sum of the _ statistical _ error and the _ systematic _ error , we can derive two types of science requirements . more concretely , the total mean square error ( mse ) on a cosmological parameter @xmath10 can be decomposed as @xmath11=\\sigma^2[p_{\\alpha}]+\\delta^2[p_\\alpha]\\;,\\ ] ] where @xmath12 $ ] is the statistical error variance and @xmath13 $ ] is the parameter shift due to the systematic signals . for each probe ,",
    "both of these terms needs to be controlled in order to minimise the total error .",
    "star / galaxy misclassification is an interesting effect because it contributes to both the statistical and systematic part of the total error , for the wl and lss probes .",
    "this allows us to translate separately the requirement on the statistical term ( section @xmath14 ) and the requirements on the systematic term ( section 3.3 ) into requirements on the quality of the star / galaxy separation .",
    "additional requirements are specific to each probe , e.g. psf calibration for wl ( section @xmath15 ) .",
    "we outline below a formalism to derive these requirements .        in the following ,",
    "we define the parameters used to quantify the quality of a star / galaxy classifier . for a given class of objects , @xmath16 ( stars or galaxies ) ,",
    "we distinguish the surface density of well classified objects , @xmath17 , and the density of misclassified objects , @xmath18 . +   +    [ cols=\"<,^,>\",options=\"header \" , ]     ideally , we could run an ann with this full set of relevant inputs . in practice ,",
    "training the ann is a non - linear iterative process , which becomes more time consuming and less robust as the number of input parameters increases .",
    "in fact , defining an optimal set of input parameters consists of minimising its size while maximising the amount of relevant information it contains .",
    "our initial set of parameter is redundant , as many of the parameters within each sub - group are dependent variables .",
    "for example , we show in figure  [ fig : scatter ] the dependencies between four types of magnitudes parameters measured in a given band . in order to reveal the redundancies within the data and compress it",
    ", we use a principal component analysis ( pca ) .",
    "this statistical method , which comes down to diagonalising the covariance matrix of the data , allows us to re - express the pre - selected parameters detailed above in a more meaningful basis of orthogonal , i.e. uncorrelated variables called _",
    "principal components_. the first principal component is chosen to account for most of the data variability and thus to have the highest possible variance .",
    "then each succeeding principal component has the highest possible variance under the constraint of being orthogonal - that is uncorrelated - to the preceding one .",
    "we run several `` well - informed '' pcas on sub - ensembles of parameters , rather than a `` blind '' pca on the full set of initial parameters . we choose to group in these sub - ensembles parameters which have the same units ( or measure ) and which are linearly dependent on each other ( such as the magnitudes in a given band , as shown in figure  [ fig : scatter ] ) .",
    "indeed , when the parameters are linearly dependent , pca is successful at finding a new basis of meaningful independent variables .",
    "our new set of parameters includes uni - band parameters from the initial set ( such as the photometric redshift or the ellipticity ) , as well as the principal components from the pcas listed below :    * pca on the five bands of each multi - band parameter ; * pca on the six fixed - aperture magnitudes in each band ; * pca on the six other types of magnitudes in each band ( i.e. _ mag_auto _ , _ mag_iso _ , _ mag_model _ , _",
    "mag_petro _ , _",
    "mag_spheroid _ and _ mag_psf _ ) .",
    "figure  [ fig : variance ] shows the variances of the principal components of these six types of magnitudes in each band as a function of their index .",
    "each of these pcas shows that most of the variance of the data is encapsulated in a reduced number of principal components . in many cases , using pca for data reduction consists of selecting only the principal components with the highest variance and approximating the data by its projection on this smaller set of variables .",
    "this encompasses the assumption that the important information is represented by the components with the highest variances . in the case of star / galaxy separation , this assumption is too simplistic .",
    "indeed , the class of an object is only one possible source of variance and high variance could also be due to differences between objects in a given class .",
    "therefore , when looking for the most relevant components for star / galaxy separation , we need another criterion to quantify their aptitude to separate between the classes .",
    "we calculate the _ fisher discriminant _",
    "@xcite for each of the new parameters , defined as the inter - class variance over the intra - class variance , @xmath19 where @xmath20 is the empirical mean value of the @xmath21 parameter for class a and @xmath22 is its empirical variance .",
    "figure  [ fig : best_fish ] shows the classification performed by the three parameters with the highest fisher discriminant .",
    "the fifteen parameters with the highest fisher discriminant form our final set of input parameters for the ann ( as dicussed in section @xmath23 , more than twenty input parameters make the ann less robust , so we limit the basic set to fiftenn parameters , in anticipation of the other five that will be added in section @xmath24 ) .",
    "the set of parameters with the highest fisher discriminant is data - specific . in our analysis of dc6 ,",
    "the most discriminative combinations of parameters are those of the class_star different bands , ( group ( vi ) of table  [ presel ] ) , morphometric parameters such as the ellipticity ( group ( ii ) of table  [ presel ] ) , followed by photometric parameters such as the photometric redshift and the magnitudes ( group ( i ) of table  [ presel ] ) .",
    "this being said , when generalising the method to other data sets , the fisher discriminant should be recalculated .",
    "once a set of optimal parameters is defined , the next step consists of mapping these parameters to the class of the objects .",
    "this mapping is performed by training an ann .",
    "in essence , an ann is a highly - flexible , fully non - linear fitting algorithm . during the training phase",
    ", it receives a set of input patterns and a given property ( in our case the class of the object ) , which needs to be fitted to them .",
    "the training consists of several iterations during which a number of free parameters known as _ weights _ are adjusted so as to minimise the difference between the outputs of the neural network for each pattern and the desired property .",
    "the algorithm then learns how to link the inputs to the desired property . after the training phase",
    ", the ann can be used to infer this property from a set of input objects for which it is unknown . for our analysis , we train an ann to map the set of optimal input parameters selected in section @xmath25 to the class of the object ( star or galaxy ) on a sample of objects for which the answer is known ( the training is made on the dc6 simulations for which we know the true class of each object ) .",
    "the ann is then used to deduce the class of a distinct set of objects .",
    "an ann is made of computing units called _ neurons _ , arranged in several layers and connected by synapses in which the information flows in a single direction .",
    "the complexity of the network depends on the number of layers and neurons in each layer .",
    "we chose to use the annz photometric redshift code @xcite , which was originally designed for photometric redshift measurements , but can be effectively and straightforwardly applied to our classification problem .",
    "the trade - off between the complexity of the network and its performance has been investigated by @xcite .",
    "for the same number of parameters , adding extra hidden layers is found to give greater gains than widening existing layers .",
    "as the network complexity is increased , the accuracy eventually converges so that no further improvement is gained by adding additional nodes .",
    "we chose a network architecture with an input layer of fifteen parameters ( or twenty , as explained in the next section ) and two hidden layers of twenty nodes , which turns out to be sufficiently complex for such convergence to be achieved .",
    "training on real data , as opposed to simulations , is preferable , yet more challenging .",
    "one option would be to use data from space - based surveys , as in space the psf is not affected by the seeing .",
    "data from the hubble space telescope could be used to train our tool for the real des survey data .",
    "using an ann brings flexibility to the training approach .",
    "it allows us not only to choose which inputs to use , but also in what number .",
    "in particular , we can take the output of other classifiers as inputs to our method .",
    "we run a pca on the five @xmath26 ( in the five bands ) .",
    "not surprisingly , the first principal component has a high fisher discriminant ( as shown in figure  [ fig : best_fish ] ) and is therefore included in the 15 input parameters selected in section @xmath25 . as the the five bands of @xmath27 are less clearly linearly dependent , we choose not to run a pca on them and add the five @xmath27 to the set of fifteen input parameters , which amounts to twenty input parameters .",
    "figure  [ fig : emmanuel ] presents the purity level at a given completeness for these two different configurations of our method .",
    "the performance of our method with fifteen input parameters ( orange curve ) can be compared to the performance when plugging in @xmath27 ( pink curve ) .",
    "including @xmath27 in the inputs allows an increase in the level of the purity by @xmath28 at faint magnitudes . running the ann on the fifteen preselected parameters ( orange curve ) already gives better results than spread_model ( blue curve ) for most of the magnitude range ( except for the very faint magnitudes , in the galaxies case ) .",
    "however , the best results are obtained by combining the two , i.e by running the ann on a hybrid input space combining the 15 selected parameters and @xmath27 .",
    "we showed that we can optimise our classifier performance by using a `` well - informed '' pca strategy ( section @xmath25 ) , and by incorporating @xmath27 into the method ( section @xmath24 ) .",
    "we now compare our classifier performance to the one of the other classifiers .",
    "we will focus on comparing multi_class to spread_model , as the performance of class_star is widely surpassed by both spread_model and multi_class for most of the magnitude range ( as shown in figure  [ fig : emmanuel ] ) .    for lss , our new classifier allows us to achieve requirements which can not be fulfilled by spread_model .",
    "figure  [ fig : black_gal ] shows that the @xmath29 limit on @xmath30 ( derived in section @xmath31 and shown in purple on the figure ) can not be reached by spread_model , whereas multi_class allows us to reach it up to magnitudes of @xmath32 ( at the required @xmath33 completeness level , derived in section @xmath34 ) .    for wl",
    ", multi_class allows us to increase the magnitude limit below which the science requirements are achieved .",
    "figure  [ fig : emmanuel ] shows that this magnitude limit increases from @xmath35 to @xmath36 for the requirement on the stars purity @xmath37 , and from @xmath38 to @xmath32 for the requirement on the galaxy purity @xmath30 .",
    "figure  [ fig : black_gal ] and figure  [ fig : black_star ] generalise this to a broad range of completenesses . in figure",
    "[ fig : impr ] , we consider the improvement in the purity of a sample of stars and a sample of galaxies , as a function of magnitude , for a large range of completenesses . at faint magnitudes - typically fainter than 23 - multi_class",
    "improves the purity achieved by spread_model by up to @xmath39 for galaxies and by up to @xmath40 for stars .    , for different magnitudes and values of the completeness .",
    "the @xmath41 science requirement ( from wl , derived in section @xmath15 ) is shown in black .",
    "higher purity levels are shown in purple and light purple .",
    "our new estimator , multi_class , allows us to widen the range of both magnitude and completeness where this requirement is achieved.,width=294 ]    , for different magnitudes and values of the completeness .",
    "the @xmath41 science requirement ( from wl , derived in section @xmath15 ) is shown in black .",
    "higher purity levels are shown in purple and light purple .",
    "our new estimator , multi_class , allows us to widen the range of both magnitude and completeness where this requirement is achieved.,width=294 ]     for stars ( left ) and galaxies . at faint magnitudes ( ranging from @xmath42 to @xmath3 ) ,",
    "multi_class allows us to increase the level of @xmath37 achieved by spread_model by up to @xmath40 , and @xmath30 by up to @xmath39.,width=321 ]",
    "we showed that star / galaxy misclassification contributes to both the statistical and systematic error on the measurement of cosmological parameters .",
    "in particular , it affects the measurement of the de equation of state parameters , @xmath8 and @xmath9 , which future large photometric surveys such as des aim to measure accurately . in the case of wl and lss measurements , we translated the detf fom requirements on the statistical and systematic errors and the constraints from psf calibration into the corresponding science requirements on the quality of star / galaxy separation .",
    "we formulated these requirements using two parameters : the purity and completeness of classified samples of stars and galaxy .    in order to meet these new requirements",
    ", we built an efficient method for star / galaxy classification , called multi_class , which combines a pca with a learning algorithm .",
    "our multi - parameter approach allows us to make use of the huge amount of information provided by _",
    "sextractor_. in particular , the use of pca allows us to better understand the correlations in the data , and to implement this physical knowledge in the classifier .    in ground - based surveys such as des ,",
    "the image quality is not constant with sky position and therefore any purely morphometric method gives limited performance , especially at faint magnitudes .",
    "the flexibility of using an ann allows us to consider the morphometry as one input parameters among many others and to integrate the performance of other classifiers to our new tool .",
    "our new classifier , multi_class , significantly improves the performance of the morphometric classifier implemented in _",
    "sextractor _ ( spread_model ) , which can not achieve the lss science requirements on star / galaxy separation . for both the lss and wl probes",
    ", it allows us to widen the range of both magnitude and completeness where the derived science requirements are achieved . for magnitudes fainter than 23",
    ", multi_class improves the purity achieved by spread_model by up to @xmath39 for galaxies and by up to @xmath40 for stars .",
    "des began survey operations in september , 2013 , and will be running for five years .",
    "therefore , we should be able to test the results shown in this paper on real data in the near future .",
    "the faint magnitudes reached by this new classifier constitute an important asset , which should allow to achieve the science requirements on star / galaxy separation in the next generation of wide - field photometric surveys .",
    "mts would like to thank gary bernstein , benjamin joachimi and alan heavens for very usefull comments and advice , alexandre refregier for a very useful discussion , and ashley ross , adam hawken , manda banerji , alex merson , foteini oikonomou , boris leistedt , sreekumar balan and iftach sadeh for their input to the project .",
    "mts is grateful for the support from the university college london perren and impact studenships .",
    "fba acknowledges the support of the royal society via a university research fellowship .",
    "ol acknowledges a royal society wolfson research merit award , a leverhulme senior research fellowship and an advanced grant from the european research council .",
    "br acknowledges support from the european research council in the form of a starting grant with number 240672 .",
    "we acknowledge uk s stfc for supporting des optics and science .",
    "funding for the des projects has been provided by the u.s .",
    "department of energy , the u.s",
    ". national science foundation , the ministry of science and education of spain , the science and technology facilities council of the united kingdom , the higher education funding council for england , the national center for supercomputing applications at the university of illinois at urbana - champaign , the kavli institute of cosmological physics at the university of chicago , financiadora de estudos e projetos , fundao carlos chagas filho de amparo  pesquisa do estado do rio de janeiro , conselho nacional de desenvolvimento cientfico e tecnolgico and the ministrio da cincia e tecnologia , the deutsche forschungsgemeinschaft and the collaborating institutions in the dark energy survey .",
    "the collaborating institutions are argonne national laboratories , the university of california at santa cruz , the university of cambridge , centro de investigaciones energeticas , medioambientales y tecnologicas - madrid , the university of chicago , university college london , des - brazil , fermilab , the university of edinburgh , the university of illinois at urbana - champaign , the institut de ciencies de lespai ( ieec / csic ) , the institut de fisica daltes energies , the lawrence berkeley national laboratory , the ludwig - maximilians universitt and the associated excellence cluster universe , the university of michigan , the national optical astronomy observatory , the university of nottingham , the ohio state university , the university of pennsylvania , the university of portsmouth , slac , stanford university , the university of sussex , texas a&m university , and the institute of astronomy at eth - zurich .",
    "99 albrecht a. et al . 2006 , arxiv : astro - ph/0609591 amara a. , refregier a. , 2008 , mnras , 391 , 228 audren b. , lesgourgues j. , bird s. , haehnelt m. g. and viel m. , 2013 , jcap , 1 , 26 bartelmann m. , schneider p. , 2001",
    ", phys.rept , 340 , 291 bertin e. , arnouts s. , 1996 , a&a , 393 , 404 bertin e. , in preparation .",
    "busha m. t. , wechsler r. h. , becker m. r. , erickson b. , evrard a. e. , 2013 , am .",
    "aas meeting 221 , 341.07 collister a. , lahav o. , 2004 , pasp , 345 , 351 das s. et al , 2011 , arxiv:1102.5090v1 debono i. , rassat a. , refregier a. , amara a. , kitching t. , 2010 , mnras , 404 , 110 des collaboration , 2005 astro - ph/0510346 desai s. et al .",
    ", 2012 , arxiv:1204.1210 fadely r. , hogg d. w. , willman b. , 2012 , arxiv:1206.4306v2 firth a. e. et al . , 2003 ,",
    "mnras , 339,1202 fisher r. a. , 1936 , annals of eugenics , 179 , 1936 gorski k. m. et al . , 2005 ,",
    "aj , 622 , 759 henrion m. , mortlock d. j. , hand d. j. , gandy a. , 2011 , mnras , 412 , 2286 huterer d. et al . , 2006 , mnras , 366 , 101 jarvis m. , bernstein g. , jain n. , 2004 , mnras , 352 , 338 a&a , 523 : a1 kirk d. , laszlo i. , bridle s. , bean r. , 2011 , preprint ( arxiv:1109.4536v2 ) kirk d. , et al",
    ". , 2012 , mnras , 424 , 1647 kron r. g. , 1980 , apjs , 43 , 305 laszlo i. , bean r. , kirk d. , bridle s. , 2012 , mnras , 423 , 1750 maddox s. j. , efstathiou g. , sutherland w. j. , 1990 , mnras , 246 , 433 mellier y. , 1999 , ara&a , 37 , 127 mohr j. j. et al . , 2012 , arxiv:1207.3189v1 naim , a. , phd thesis `` the application of artificial neural network to astronomical classification '' , university of cambrige , 1995 . odewahn s. c. , stockwell e. b. , pennington r. l. , humphreys r. m. , zumach w. a. , 1992 , aj , 103 , 318 oyaizu h. et al . , 2008 , apj , 674 , 768 rassat a. et al , 2008 , arxiv:0810.0003 refregier a. , 2003 , ara&a , 41 , 645 a&a , 528 ross a. et al . , 2011 , mnras , 417 , 1350 rossetto b. m. et al .",
    ", 2011 , aj , 141 , 185 sebok w. l. , 1979 , aj , vol",
    ". 84,1526 , 1536 takad m. , jain b. , 2004 , mnras , 348 , 897 tegmark m. , taylor a. n. , heavens a. f. , 1997 , apj , 480 , 22 thomas s. , abdalla f. b. , lahav o. , 2010 , mnras , 1669,1685 thomas s. , abdalla f. b. , lahav o. , 2011 , phys . rev .",
    "lett . , 106.241301 valdes f. , 1982 , spie , 465 , 472 vasconcellos e. c. et al .",
    ", 2011 , aj , 141 , 189 viola , m. , kitching , t. d. and joachimi , b. 2014 , mnras , 439 , 1909 weinberg et al . , 2013 , arxiv:1201.2434v2 yee h. k. c. , 1991 , pasp , 103 , 396",
    "in the following , we show the marginalised statistical errors on \\{@xmath43 , @xmath44 , @xmath45 , @xmath46 , @xmath47 , @xmath48 } from the wl probe ( figure  [ fig : neff_sigma_wl_others ] ) and the lss probe ( figure  [ fig : neff_sigma_lss_others ] ) , for different values of the density of galaxies with reliable shape measurement @xmath49 and of the density of detected galaxies @xmath50 respectively .",
    "the errors are marginalised and computed using the assumptions and setup described in section @xmath51 , with @xmath52 $ ] in the wl case and with @xmath53 $ ] in the lss case .",
    "the red curve shows the errors computed with a non - informative prior whereas the blue curve is obtained assuming a planck prior ."
  ],
  "abstract_text": [
    "<S> we address the problem of separating stars from galaxies in future large photometric surveys . </S>",
    "<S> we focus our analysis on simulations of the dark energy survey ( des ) . in the first part of the paper , we derive the science requirements on star / galaxy separation , for measurement of the cosmological parameters with the gravitational weak lensing and large scale structure probes . </S>",
    "<S> these requirements are dictated by the need to control both the statistical and systematic errors on the cosmological parameters , and by point spread function calibration . </S>",
    "<S> we formulate the requirements in terms of the _ completeness _ and _ purity _ provided by a given star / galaxy classifier . in order to achieve these requirements at faint magnitudes , </S>",
    "<S> we propose a new method for star / galaxy separation in the second part of the paper . </S>",
    "<S> we first use principal component analysis to outline the correlations between the objects parameters and extract from it the most relevant information . </S>",
    "<S> we then use the reduced set of parameters as input to an artificial neural network . </S>",
    "<S> this multi - parameter approach improves upon purely morphometric classifiers ( such as the classifier implemented in _ sextractor _ ) , especially at faint magnitudes : it increases the purity by up to 20% for stars and by up to 12% for galaxies , at i - magnitude fainter than 23 .    </S>",
    "<S> [ firstpage ]    gravitational lensing : weak  methods : data analysis  </S>",
    "<S> surveys  cosmology : observations  dark energy  large - scale structure of universe . </S>"
  ]
}