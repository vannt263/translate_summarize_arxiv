{
  "article_text": [
    "the goal of _ compressed sensing ( cs ) _  @xcite is to recover a sparse signal @xmath1 from a small number of non - adaptive linear measurements @xmath15 , ( typically ) by convex optimization ( e.g. , linear programming ) . here , @xmath16 is the vector of measurements and @xmath7 is the design matrix ( also called the measurement matrix ) . in classical settings , entries of @xmath8",
    "are i.i.d .",
    "samples from the gaussian distribution @xmath17 , or a gaussian - like distribution ( e.g. , a distribution with finite variance ) .    in this paper , we sample @xmath8 from a heavy - tailed distribution which only has the @xmath18-th moment with @xmath19 and we will choose @xmath10 . strikingly , using such a design matrix turns out to result in a simple and powerful solution to the problem of exact @xmath0-sparse recovery , i.e. , @xmath20",
    ".      sparse recovery ( compressed sensing ) , which has been an active area of research , can be naturally suitable for : ( i ) the `` single pixel camera '' type of applications ; and ( ii ) the  data streams  type of applications .",
    "the idea of compressed sensing may be traced back to many prior papers such as  @xcite .",
    "it has been realized ( and implemented by hardware ) that collecting a linear combination of a sparse vector , i.e. , @xmath21 , can be more advantageous than sampling the vector itself .",
    "this is the foundation of the `` single pixel camera '' proposal .",
    "see the site https://sites.google.com/site/igorcarron2/ + compressedsensinghardware for a list of implementations of single - pixel - camera type of applications .",
    "figure  [ fig_zip54 ] provides an illustrative example .",
    "natural images are in general not as sparse as the example in figure  [ fig_zip54 ] .",
    "we nevertheless expect that in many practical scenarios , the sparsity assumption can be reasonable .",
    "for example , the differences between consecutive image / video frames taken by surveillance cameras are usually very sparse because the background remains still . in general , anomaly detection problems are often very sparse .    another line of applications concerns data streams , which can be conceptually viewed as a long sparse dynamic vector with entries rapidly varying over time .",
    "because of the dynamic nature , there may not be an easy way of knowing where the nonzero coordinates are , since the history of streaming is usually not stored . perhaps surprisingly , many problems can be formulated as sparse data streams .",
    "for example , video data are naturally streaming . a common task in databases",
    "is to find the `` heavy - hitters ''  @xcite , e.g. , finding which product items have the highest total sales . in networks",
    "@xcite , this is often referred to as the `` elephant detection '' problem ; see some recent papers on compressed sensing for network applications  @xcite",
    ".    for data stream applications , entries of the signals @xmath4 are ( rapidly ) updated over time ( by addition and deletion ) . at a time @xmath22 ,",
    "the @xmath23-th entry is updated by @xmath24 , i.e. , @xmath25 .",
    "this is often referred to as the _ turnstile model _  @xcite .",
    "as the projection operation is linear , i.e. , @xmath26 , we can ( re)generate corresponding entries of @xmath8 on - demand whenever one entry of @xmath4 is altered , to update all entries of the measurement vector @xmath27 .",
    "the use of stable random projections for estimating the @xmath9-th frequency moment @xmath28 ( instead of the individual terms @xmath29 ) was studied in  @xcite . @xcite",
    "proposed the use of geometric mean estimator for stable random projections , for estimating @xmath28 as well as the harmonic mean estimator for estimating @xmath28 when @xmath10 .",
    "when the streaming model is not the turnstile model ( for example , the update mechanism may be nonlinear ) , @xcite developed the method named _ conditional random sampling ( crs ) _ , which has been used in network applications  @xcite . at this point",
    ", our work focuses on the turnstile data stream model .    in this paper ,",
    "our goal is to use stable random projections to recover the individual entries @xmath29 s , not just the summary statistics such as the frequency moments .",
    "we first provide a review of @xmath9-stable distributions .",
    "a random variable @xmath30 follows an @xmath9-stable distribution with unit scale , denoted by @xmath31 , if its characteristic function can be written as  @xcite @xmath32 when @xmath33 , @xmath34 is equivalent to the normal distribution with variance 2 , i.e. , @xmath35 .",
    "when @xmath36 , @xmath37 is the standard cauchy distribution centered at zero with unit scale .",
    "to sample from @xmath31 , we use the cms procedure  @xcite .",
    "that is , we sample independent exponential @xmath38 and uniform @xmath39 variables , and then compute @xmath40 by @xmath41^{(1-\\alpha)/\\alpha}\\sim s(\\alpha,1)\\end{aligned}\\ ] ] if @xmath42 i.i.d .",
    ", then for any constants @xmath43 , we have @xmath44 , where @xmath45 .",
    "more generally , @xmath46 .    in this paper",
    ", we propose using @xmath10 . in our numerical experiments with matlab",
    ", the value of @xmath9 is taken to be @xmath47 and no special data storage structure is needed . while the precise theoretical analysis based on a particular choice of ( small ) @xmath9 is technically nontrivial , our algorithm is intuitive , as illustrated by a simpler analysis of an `` idealized '' algorithm using the limit of @xmath9-stable distributions as @xmath48 .      * input : * @xmath0-sparse signal @xmath1 , threshold @xmath49 ( e.g. , @xmath50 ) , design matrix @xmath7 sampled from @xmath31 with @xmath10 ( e.g. , 0.03 ) .",
    "@xmath8 can be generated on - demand when the data arrive at a streaming fashion . +",
    "* output : * the recovered signal , denoted by @xmath51 , @xmath52 to @xmath13 . + * linear measurements : * @xmath53 , which can be conducted incrementally if entries of @xmath4 arrive in a streaming fashion .",
    "+ * detection : * for @xmath54 to @xmath13 , compute @xmath55 , where @xmath56 .",
    "if @xmath57 , set @xmath58 .",
    "+ * estimation : * if @xmath59 , compute the gaps for the sorted observations @xmath60 and estimate @xmath29 using the gap estimator @xmath61 .",
    "let @xmath62 .",
    "see the details below .",
    "+ * iterations : * if @xmath59 and the minimum gap length @xmath63 , we call this @xmath64 an `` undetermined '' coordinate and set @xmath65 . compute the residuals : @xmath66 , and apply the gap estimator using the residual @xmath67 , only on the set of `` undetermined '' coordinates . repeat the iterations a number of times ( e.g. , 2 to 4 ) until no changes are observed .",
    "this iteration step is particularly helpful when @xmath68 where @xmath69 or 0.01 .",
    "we assume @xmath1 is @xmath0-sparse and we do not know where the nonzero coordinates are .",
    "we obtain @xmath5 linear measurements @xmath70 , where entries of @xmath7 , denoted by @xmath71 , are i.i.d .",
    "samples from @xmath31 with a small @xmath9 ( e.g. , 0.03 ) .",
    "that is , each measurement is @xmath72 .",
    "our algorithm , which consists of two estimators , utilizes the ratio statistics @xmath73 , @xmath74 , to recover @xmath29 .",
    "the * absolute minimum estimator * is defined as @xmath75 which is effective for detecting whether any @xmath76 .",
    "in fact we prove that essentially @xmath77 measurements are sufficient for detecting all zeros with at least probability @xmath78 .",
    "the actual required number of measurements will be significantly lower than @xmath79 if we use the minimum algorithm together with the gap estimator and the iterative process .    when @xmath59 , in order to estimate the magnitude of @xmath29 , we resort to the * gap estimator * defined as follows .",
    "first we sort @xmath80 s and write them as order statistics : @xmath81",
    ". then we compute the gaps : @xmath82 .",
    "the gap estimator is simply @xmath83 we have also derived theoretical error probability bound for @xmath84 .",
    "when @xmath85 , we discover that it is better to apply the gap estimator a number of times , each time using the residual measurements only on the `` undetermined '' coordinates ; see alg .  [ alg_recovery ] .",
    "the iteration procedure will become intuitive after we explain the `` idealized algorithm '' in sec .",
    "[ sec_intuition ] and sec .",
    "[ sec_ideal ] .",
    "+ note that our algorithm does not directly utilize the information of @xmath0 .",
    "in fact , for small @xmath9 , the quantity @xmath86 ( which is very close to @xmath0 ) can be reliably estimated by the following harmonic mean estimator  @xcite : @xmath87 ^ 2}-1\\right)\\right)\\end{aligned}\\ ] ]",
    "while a precise analysis of our proposed method is technical , our procedure is intuitive from the ratio of two independent @xmath9-stable random variables , in the limit when @xmath48 .",
    "recall that , for each coordinate @xmath64 , our observations are @xmath88 , @xmath89 to @xmath5 .",
    "naturally our first attempt was to use the joint likelihood of @xmath88 .",
    "however , in our proposed method , the observations are utilized only through the ratio statistics @xmath90 .",
    "we first explain why .      for convenience , we first define @xmath91    denote the density function of @xmath31 by @xmath92 . by a conditional probability argument , the joint density of @xmath93 can be shown to be @xmath94 , from which we can derive the joint log - likelihood of @xmath88 , @xmath95 to @xmath5 , as @xmath96 here we can treat @xmath29 and @xmath97 as the parameters to be estimated .",
    "closed - form density functions of @xmath98 are in general not available ( except for @xmath33 and @xmath99 ) .",
    "interestingly , when @xmath10 , we can obtain a convenient approximation .",
    "recall , for two independent variables @xmath38 and",
    "@xmath39 , we have @xmath100^{(1-\\alpha)/\\alpha}\\sim s(\\alpha,1),\\end{aligned}\\ ] ] thus , it is intuitive that @xmath101 is approximately @xmath38 when @xmath102 . as rigorously shown by @xcite , @xmath103 in distribution . using this limit ,",
    "the density function @xmath92 is approximately @xmath104 , and hence the joint log - likelihood @xmath105 is approximately @xmath106 which approaches infinity ( i.e. , the maximum likelihood ) at the poles : @xmath107 , @xmath89 to @xmath5 .",
    "this is the reason why we use only the ratio statistics @xmath73 for recovering @xmath29 .",
    "since our procedure utilizes the statistic @xmath90 , we need to know its distribution , at least approximately .",
    "note that @xmath108 , where @xmath109 and @xmath110 are i.i.d .",
    "@xmath31 variables .",
    "recall the definition @xmath111 .",
    "thus , @xmath112 and the problem boils down to finding the distribution of the ratio of two @xmath9-random variables with @xmath10 . using the limits : @xmath113 and @xmath114 , as @xmath48 ,",
    "it is not difficult to show an approximate cumulative distribution function ( cdf ) of @xmath90 : @xmath115 the cdf of @xmath116 is the also given by ( [ eqn_appr_cdf ] ) by letting @xmath117 and @xmath118 .",
    "+    figure  [ fig_appr_cdf ] plots the approximate cdfs ( [ eqn_appr_cdf ] ) for @xmath116 ( left panel ) and @xmath90 ( right panel , with @xmath117 and three values of @xmath86 ) .",
    "while the distribution of @xmath116 is extremely heavy - tailed , about half of the probability mass concentrated near 0 .",
    "this means , as @xmath48 , samples of @xmath119 are equal likely to be either very close to zero or very large .",
    "since ( [ eqn_appr_cdf ] ) is only approximate , we also provide the simulations of @xmath116 in figure  [ fig_pmfs2s1 ] to help verify the approximate cdf in figure  [ fig_appr_cdf ] .      to illustrate our algorithm , in particular the iterative procedure in alg .",
    "[ alg_recovery ] , we consider the simplest example of @xmath11 . without loss of generality ,",
    "we let @xmath120 and @xmath121 . this way , our observations are @xmath122 for @xmath95 to @xmath5 .",
    "the ratio statistics are @xmath123    we assume an `` idealized '' algorithm , which allows us to use an extremely small @xmath9 .",
    "as @xmath48 , @xmath124 is either ( virtually ) 0 or @xmath125 .",
    "note that @xmath126 is the reciprocal of @xmath124 , i.e. , @xmath127 .",
    "suppose , with @xmath128 observations , the ratio statistics , for @xmath129 , are : @xmath130 then we have seen @xmath131 twice and we assume this `` idealized '' algorithm is able to correctly estimate @xmath132 , because there is a `` cluster '' of 1 s .",
    "after we have estimated @xmath133 , we compute the residual @xmath134 . in the second iteration ,",
    "the ratio statistics become @xmath135 this means we know @xmath136 .",
    "next , we again update the residuals , which become zero .",
    "therefore , in the third iteration , all zero coordinates can be recovered .",
    "the most exciting part of this example is that , with @xmath128 measurements , we can recovery a signal with @xmath11 , regardless of @xmath13 .",
    "when @xmath137 , the analysis of the `` idealized '' algorithm requires a bit more work , which we present in sec .",
    "[ sec_ideal ] .",
    "we summarize the `` idealized '' algorithm ( see more details in sec .  [ sec_ideal ] ) as follows :    1 .",
    "the algorithm assumes @xmath48 , or as small as necessary .",
    "as long as there are two observations @xmath90 in the extremely narrow interval @xmath138 with @xmath139 very close to 0 , the algorithm is able to correctly recover @xmath29 .",
    "we assume @xmath139 is so small that it is outside the required precision range of @xmath29 . here",
    "we purposely use @xmath139 instead of @xmath140 to differentiate it from the parameter @xmath140 used in our practical procedure , i.e. , alg .",
    "[ alg_recovery ] .    clearly , this `` idealized '' algorithm can not be strictly faithfully implemented .",
    "if we have to use a small @xmath9 instead of @xmath141 , the observations @xmath142 will be between 0 and @xmath143 , and we will not be able to identify the true @xmath29 with high confidence unless we see two essentially identical observations .",
    "this is why we specify that an algorithm must see at least two repeats in order to exactly recover @xmath29 . as analyzed in sec .",
    "[ sec_theory ] , the proposed gap estimator is a practical surrogate , which converges to the `` idealized '' algorithm when @xmath48 .      as shown in figure  [ fig_appr_cdf ] ( right panel ) , while the distribution of @xmath90 is heavy - tailed , its cdf has a significant jump very near the true @xmath29 .",
    "this means more than one observations ( among @xmath5 observations ) will likely lie in the extremely narrow interval around @xmath29 , depending on the value of @xmath86 ( which is essentially @xmath0 ) .",
    "we are able to detect whether @xmath117 if there is just one observation near zero . to estimate the magnitude of @xmath29 , however , we need to see a `` cluster '' of observations , e.g. , two or more observations which are essentially identical .",
    "this is the intuition for the minimum estimator and the gap estimator .",
    "also , as one would expect , figure  [ fig_appr_cdf ] shows that the performance will degrade ( i.e. , more observations are needed ) as @xmath144 increases .",
    "the gap estimator is a practical surrogate for the `` idealized '' algorithm .",
    "basically , for each @xmath64 , if we sort the observations : @xmath145 , the two neighboring observations corresponding to the minimum gap will be likely lying in a narrow neighborhood of @xmath29 , provided that the length of the minimum gap is very small , due to the heavy concentration of the probability mass about @xmath29 .",
    "if the observed minimum gap is not small , we give up estimating this ( `` undermined '' ) coordinate in the current iteration .",
    "after we have removed the ( reliably ) estimated coordinates by computing the residuals , we may have a better chance to successfully recover some of these undermined coordinates because the effective `` @xmath0 '' and the effective `` @xmath13 '' are significantly reduced .",
    "+ finally , to better understand the difference between the minimum estimator and the gap estimator , we compute ( assuming @xmath146 ) the probability @xmath147 , which is related to the probability that the two estimators differ , i.e. , @xmath148 . using the approximate cdf of @xmath90 ( [ eqn_appr_cdf ] ) , we have @xmath149 when @xmath150 , @xmath151 , @xmath152 , @xmath153 , this probability is about 0.002 , which is not small considering that we normally have to use at least @xmath154 measurements . given enough measurements , almost certainly the minimum estimator will be smaller than the gap estimator in absolute values , as verified in the simulations in sec .",
    "[ sec_simu ] . in other words",
    ", the minimum estimator will not be reliable for exact recovery .",
    "this also explains why we need to see a clustered observations instead of relying on the absolute minimum observation to estimate the magnitude .",
    "since the distribution of @xmath90 is symmetric about the true @xmath29 , it is also natural to consider using the sample median to estimate @xmath29 .",
    "we have found , however , at least empirically , that the median estimator will require significantly more measurements , for example , @xmath155 or more .",
    "this is why we develop the gap estimator to better exploit the special structure of the distribution as illustrated in figure  [ fig_appr_cdf ] .",
    "both linear programming ( lp ) and orthogonal matching pursuit ( omp ) utilize a design matrix sampled from gaussian ( i.e. , @xmath9-stable with @xmath33 ) or gaussian - like distribution ( e.g. , a two - point distribution on @xmath156 with equal probabilities ) . here",
    ", we use @xmath157 to denote such a design matrix .",
    "the well - known lp algorithm recovers the signal @xmath4 by solving the following @xmath158 optimization problem : @xmath159 which is also commonly known as _ basis pursuit _",
    "it has been proved that lp can recover @xmath4 using @xmath160 measurements  @xcite , although the exact constant is unknown .",
    "this procedure is computationally prohibitive for large @xmath13 ( e.g. , @xmath161 ) .",
    "when there are measurement noises , the lp algorithm can be modified as other convex optimization problems , for example , the _ lasso _ algorithm  @xcite .",
    "the orthogonal matching pursuit ( omp ) algorithm  @xcite is a popular greedy iterative procedure .",
    "it typically proceeds with @xmath0 iterations . at each iteration",
    ", it conducts univariate least squares for all the coordinates on the residuals , and chooses the coordinate which maximally reduces the overall square errors . at the end of each iteration",
    ", all the chosen coordinates are used to update the residuals via a multivariate least square .",
    "the algorithm can be coded efficiently ( e.g. , in matlab ) ( but we find omp is still significantly slower than our method especially when @xmath0 is not small . ) @xcite showed that , under appropriate conditions , the required number of measurements of omp is essentially @xmath162 , which improved the prior result in  @xcite .",
    "there are also modified omp algorithms , e.g. , _ cosamp _  @xcite .    in this paper ,",
    "our experimental study will focus on the comparisons with omp and lp , as these two methods are the most basic and still strong baselines .",
    "of course , we recognize that compressed sensing is a rapidly developing area of research and we are aware that there are other promising sparse recovery methods such as the `` message - passing '' algorithm  @xcite and the `` sparse matrix '' algorithm  @xcite .",
    "we plan to compare our algorithm with those methods in separate future papers .",
    "to validate the procedure in alg .",
    "[ alg_recovery ] , we provide some simulations ( and comparisons with lp and omp ) , before presenting the theory . in each simulation , we randomly select @xmath0 coordinates from a total of @xmath13 coordinates .",
    "we set the magnitudes ( and signs ) of these @xmath0 coordinates according to one of the two mechanisms .",
    "( i ) * gaussian signals * : the values are sampled from @xmath163 .",
    "( ii ) * sign signals * : we simply take the signs , i.e. , @xmath164 , of the generated gaussian signals .",
    "the number of measurements @xmath5 is chosen by @xmath165 where @xmath166 and @xmath167 .",
    "when @xmath168 , all methods perform well in terms of accuracies ; and hence it is more interesting to examine the results when @xmath85 . also , we simply fix @xmath152 in alg  [ alg_recovery ] .",
    "our method is not sensitive to @xmath140 ( as long as it is small ) .",
    "we will explain the reason in the theory section .",
    "figures  [ fig_recsignb1 ] to  [ fig_recgausb5 ] present several instances of simulations , for @xmath169 and @xmath170 . in each simulation",
    "( each figure ) , we generate the heavy - tailed design matrix @xmath8 ( with @xmath150 ) and the gaussian design matrix @xmath157 ( with @xmath171 ) , using the same random variables ( @xmath172 s and @xmath173 s ) as in ( [ eqn_stable_sample ] ) .",
    "this provides shoulder - by - shoulder comparisons of our method with lp and omp .",
    "we use the @xmath158-magic package  @xcite for lp , as we find that @xmath158-magic produces very similar recovery results as the matlab build - in lp solver and is noticeably faster . since @xmath158-magic is popular",
    ", this should facilitate reproducing our work by interested readers .",
    "+ in figures  [ fig_recsignb1 ] and  [ fig_recgausb1 ] , we let @xmath174 ( i.e. , @xmath175 ) . for this @xmath5 ,",
    "all methods perform well , for both sign signal and gaussian signal .",
    "the * left - top * panels of figures  [ fig_recsignb1 ] and  [ fig_recgausb1 ] show that the _ minimum estimator _",
    "@xmath176 can precisely identify all the nonzero coordinates .",
    "the * right - top * panels show that the _ gap estimator _ @xmath61 applied on the coordinates identified by @xmath176 , can accurately estimate the magnitudes .",
    "the label `` * * min+gap(1 ) * * '' means only one iteration is performed ( which is good enough for @xmath177 ) .    the * bottom * panels of figures  [ fig_recsignb1 ] and  [ fig_recgausb1 ] show that both omp and lp also perform well when @xmath177 .",
    "omp is noticeably more costly than our method ( even though @xmath0 is small ) while lp is significantly much more expensive than all other methods .",
    "+ we believe these plots of sample instances provide useful information , especially when @xmath178 .",
    "if @xmath5 is too small , then all methods will ultimately fail , but the failure patterns are important , for example , a `` catastrophic '' failure such that none of the reported nonzeros is correct will be very undesirable . figure  [ fig_recsignb5 ] and figure  [ fig_recgausb5 ] will show that our method does not fail catastrophically even with @xmath179 .",
    "simulations in figures  [ fig_recsignb3 ] and  [ fig_recgausb3 ] use @xmath180 ( i.e. , @xmath181 ) .",
    "the minimum estimator @xmath176 outputs a significant number of false positives but our method can still perfectly reconstruct signal using the gap estimator with one additional iteration ( i.e. , * min+gap(2 ) * ) . in comparisons , both lp and omp perform poorly .",
    "furthermore , figures  [ fig_recsignb5 ] and  [ fig_recgausb5 ] use @xmath179 ( i.e. , @xmath182 ) to demonstrate the robustness of our algorithm . as @xmath5 is not large enough , a small fraction of nonzero coordinates",
    "are not recovered by our method , but there are no catastrophic failures .",
    "this point is of course already illustrated in figure  [ fig_zip54 ] . in comparison , when @xmath183 , both lp and omp perform very poorly .",
    "note that the decoding times for our method and omp are fairly consistent in that smaller @xmath5 results in faster decoding .",
    "however , the run times of lp can vary substantially , which should have to do with the quality of measurements and implementations .",
    "also , note that we only display the top-@xmath0 ( in absolute values ) reconstructed coordinates for all methods .",
    "finally , figure  [ fig_zip54gap123 ] supplements the example in figure  [ fig_zip54 ] ( which only displays `` min+gap(3 ) '' ) by presenting the results of `` min+gap(1 ) '' and `` min+gap(2 ) '' , to illustrate that our proposed iterative procedure improves the quality of reconstructions .",
    "we repeat the simulations many times to compare the aggregated reconstructed errors and run times . in this set of experiments ,",
    "we choose @xmath184 from @xmath185 combinations .",
    "we choose @xmath186 with @xmath167 .",
    "we again experiment with both gaussian @xmath163 signals and sign signals .    for each setting , we repeat the simulations 1000 times , except @xmath187 , for which we only repeat 100 times as the lp experiments take too long .      for sparse recovery , it is crucial to correctly recover the nonzero locations .",
    "here we borrow the concept of precision and recall from the literature of information retrieval ( ir ) : @xmath188 to compare the proposed absolute minimum estimator with lp decoding . here , we view nonzero coordinates as `` positives '' ( p ) and zero coordinates as `` negatives '' ( n ) .",
    "ideally , we hope to maximize `` true positives '' ( tp ) and minimize `` false positives '' ( fp ) and `` false negatives '' ( fn ) . in reality",
    ", we usually hope to achieve at least perfect recalls so that the retrieved set of coordinates contain all the true nonzeros .",
    "figure  [ fig_pr ] presents the ( median ) precision - recall curves .",
    "our minimum estimator always produces essentially @xmath189 recalls , meaning that the true positives are always included for the next stage of reconstruction . in comparison , as @xmath5 decreases",
    ", the recalls of lp decreases significantly .",
    "the reconstruction accuracy is another useful measure of quality .",
    "we define the reconstruction error as @xmath190 note that , since errors are normalized , a value @xmath191 should indicate a very bad reconstruction outcome .",
    "figure  [ fig_err ] presents the median reconstruction errors . at @xmath168 ( i.e. , @xmath177 )",
    ", all methods perform well . for sign signals , both omp and lp perform poorly as soon as @xmath192 or 2 and omp results are particularly bad . for gaussian signals",
    ", omp can produce good results even when @xmath181 . +",
    "our method performs well , and 2 or 3 iterations of the gap estimation procedure help noticeably .",
    "one should keep in mind that errors defined by ( [ eqn_error ] ) may not always be as informative .",
    "for example , with @xmath183 , figures  [ fig_recsignb5 ] and  [ fig_recgausb5 ] show that , even though our method fails to recover a small fraction of nonzero coordinates , the recovered coordinates are accurate . in comparison , for omp and lp ,",
    "essentially none of the nonzero coordinates in figures  [ fig_recsignb5 ] and  [ fig_recgausb5 ] could be accurately identified when @xmath179 .",
    "this confirms that our method is stable and reliable .",
    "of course , we have already seen this behavior in the example in figure  [ fig_zip54 ] . in that example , even with @xmath193 , the reconstructed signal by our method is still quite informative .",
    "figure  [ fig_time ] confirms that lp is computationally expensive , using the @xmath158-magic package  @xcite .",
    "we have also found that the matlab build - in l1 solver can take significantly more time ( and consume more memory ) than the @xmath158-magic package . in comparison , omp is substantially more efficient than lp , although it is still much more costly than our proposed algorithm , especially when @xmath0 is not small . in our experiments with the data for generating figure  [ fig_zip54 ] , omp was more than 100 times more expensive than our method .",
    "our proposed algorithm is robust against measurement noise .",
    "recall @xmath194 . with additive noise @xmath195",
    ", we have @xmath196 .",
    "since the observations are only useful when @xmath116 is essentially 0 , i.e. , @xmath109 is large , @xmath197 does not really matter for our procedure .",
    "we will provide more results about measurement noise in sec .",
    "[ sec_noise ] .",
    "the analysis of our practical algorithm , i.e. , the gap estimator and iterative procedure , is technically nontrivial . to better understand our method",
    ", we first provide the analysis of the `` idealized '' algorithm .",
    "the `` idealized '' algorithm makes the following three major assumptions :    1 .",
    "the coordinate @xmath29 is perfectly estimated ( i.e. , effectively zero error ) if the estimate @xmath51 satisfies @xmath198 , for small @xmath139 .",
    "this assumption turns out to be realistic in practice .",
    "for example , if @xmath29 can only be either 0 or integers , then @xmath199 ( or even @xmath200 ) is good enough .",
    "if we know that the signal comes from a system with only 15 effective digits , then @xmath201 is good enough ( for example , in matlab `` ( 1 + 1e-16)-1==0 '' ) . here",
    ", we do not try to specify how @xmath139 is determined ; we can just think of it as a small number which physically exists in every application .",
    "the algorithm is able to use @xmath48 for generating stable random variables and no numerical errors occur during calculations .",
    "this convenient assumption is of course strong and not truly necessary .",
    "basically , we just need @xmath9 to be small enough so that @xmath198 . for the sake of simplifying the analysis",
    ", we just assume @xmath48 .",
    "3 .   as long as there are at least two observations @xmath90 in the interval @xmath138 ,",
    "the algorithm is able to perfectly recover @xmath29 .",
    "this step is subtly different from the gap estimator in our practical procedure .",
    "because we do not know @xmath29 in advance , we have to rely on gaps to guess @xmath29 .",
    "although the distribution of @xmath90 is extremely heavy - tailed ( as shown in figure  [ fig_appr_cdf ] ) , there is always an extremely small chance that two nearly identical observations reside outside @xmath138 . of course , with @xmath9 decreases , the difference between the gap estimator and the `` idealized '' algorithm diminishes .      with the above assumptions , we are able to analyze the `` idealized '' algorithm .",
    "first , we define the success probability for each observation : @xmath202 as shown in sec .",
    "[ sec_intuition ] , @xmath203 , where @xmath42 i.i.d . and @xmath204 .",
    "thus , @xmath205 . recall @xmath206 and @xmath207 , as @xmath208 . in this section , for notational convenience ,",
    "we simply write @xmath209 and @xmath210 .",
    "therefore @xmath211 this means the number of observations falling in @xmath138 follows a binomial distribution @xmath212 .",
    "we can bound the failure probability ( i.e. , the probability of having at most one success ) by @xmath213 , as @xmath214 to make sure that @xmath29 can be perfectly recovered , we need to choose @xmath5 large enough such that @xmath215 clearly , @xmath216 is sufficient for any @xmath217 .",
    "but we can do better for small @xmath213 : @xmath218 which is @xmath219 when @xmath69 , and about @xmath220 when @xmath166 .",
    "there are @xmath13 coordinates .",
    "if we stop the algorithm with only one iteration , then we have to use the union bound which will result in an additional @xmath221 multiplicative term .",
    "however , under our idealistic setting , this @xmath221 term is actually not needed if we perform iterations based on residuals .",
    "each time , after we remove a nonzero coordinate which is perfectly recovered , the remaining problem only becomes easier , i.e. , the success probability becomes @xmath222 and @xmath5 remains the same .",
    "+ suppose , @xmath223 and @xmath76 if @xmath224 .",
    "the ratio statistics are , for @xmath89 to @xmath5 , @xmath225    suppose in the first iteration , @xmath133 is perfectly recovered .",
    "that is , among @xmath5 observations , for at least two observations , @xmath226 is very close 1 , i.e. , @xmath227 , for at least @xmath228 values .",
    "after we recover @xmath133 , we compute the residual and move to the second iteration : @xmath229 we need to check to make sure that we have an easier problem in that the success probability is at least @xmath230 .",
    "it turns out this probability is at least @xmath231 , which is even better of course . to see this , we consider two cases , depending on whether in the first iteration @xmath226 is a success . + * case 1 : * in the first iteration , the observation @xmath226 is a success .",
    "@xmath232    * case 2 : * in the first iteration , the observation @xmath226 is not a success .",
    "@xmath233    in both cases , the success probability in the second iteration increases ( i.e. , larger than @xmath230 ) .",
    "note that , assuming @xmath234 is for notational convenience .",
    "as long as @xmath48 , the same result holds .",
    "the above analysis provides the good intuition but it is not the complete analysis for the iterative procedure .",
    "note that the total number of iterations @xmath235 satisfies @xmath236 , i.e. , @xmath237 .",
    "a precise analysis of the iterative procedure involves calculating the conditional probability .",
    "for example , after the first iteration , @xmath195 ( out of @xmath13 ) nonzero coordinates are recovered . a rigorous analysis of the success probability for the second iteration will require conditioning on all these @xmath195 recovered coordinates and the remaining @xmath238 unrecovered coordinates .",
    "such an analysis may add complication for understanding our method .    here",
    ", for simplicity , we assume that in each iteration , we use `` fresh '' projections so that the iterations become independent . this way ,",
    "the total number of required measurements becomes ( assuming @xmath239 ) : @xmath240    the additional multiplicative factor @xmath241 has little impact on the overall complexity .",
    "when @xmath69 , the required number of measurements becomes @xmath242 instead of @xmath219 .",
    "the result is still highly encouraging .",
    "this section will develop a more formal theoretical analysis of our procedure in alg .",
    "[ alg_recovery ] , including the minimum estimator and the gap estimator .",
    "the gap estimator is a surrogate for the `` idealized '' algorithm .",
    "the minimum estimator is not crucial once we have the gap estimator and the iterative process .",
    "we keep it in our proposed procedure for two reasons .",
    "firstly , it is faster than the gap estimator and is able to identify a majority of the zero coordinates in the first iteration . secondly , even if we just use one iteration , the required sample size for the minimum estimator @xmath5 is essentially @xmath243 , which already matches the known complexity bounds in the compressed sensing literature .",
    "our analysis uses the distribution of the ratio of two independent stable random variables , @xmath244 . as a closed - form expression is not available ,",
    "we compute the lower and upper bounds .",
    "first , we define @xmath245 where @xmath246^{(1-\\alpha)/\\alpha}\\end{aligned}\\ ] ] based on the cms procedure ( [ eqn_stable_sample ] ) for generating @xmath9-stable random variables .",
    "the following lemmas provide several useful bounds for @xmath247 .",
    "[ lem_f_lower ] for all @xmath248 , @xmath249 in particular , when @xmath250 , we have @xmath251 in addition , for any fixed @xmath252 , @xmath253    * proof : *  see appendix  [ app_f_lower ] .",
    "@xmath254 +    [ lem_f_upper ] if @xmath255 , then @xmath256 @xmath257 @xmath258 as @xmath208 , @xmath259 if @xmath260 , and @xmath261 if @xmath262 .",
    "+ * proof : *  see appendix  [ app_f_upper ] .",
    "figure  [ fig_constc ] plots @xmath263 for @xmath264$].@xmath254     as defined in ( [ eqn_constc]).,width=240 ]    for all @xmath265 , @xmath266 * proof : *  for all @xmath265 , we have @xmath267 .",
    "@xmath254    figure  [ fig_f ] plots the simulated @xmath247 curves together with the upper and lower bounds .     obtained by simulations ( solid curves ) , for @xmath150 and @xmath268 .",
    "in each panel , the bottom and top curves are the lower bound ( [ eqn_f_lower ] ) and upper bound ( [ eqn_f_upper ] ) , respectively .",
    "the lower bound is sharp , especially for small @xmath9.,title=\"fig:\",width=240 ]   obtained by simulations ( solid curves ) , for @xmath150 and @xmath268 . in each panel ,",
    "the bottom and top curves are the lower bound ( [ eqn_f_lower ] ) and upper bound ( [ eqn_f_upper ] ) , respectively .",
    "the lower bound is sharp , especially for small @xmath9.,title=\"fig:\",width=240 ]      recall the definition of the absolute min estimator : @xmath269 if @xmath59 , then we consider the @xmath64-th coordinate is a ( candidate of ) nonzero entry .",
    "our task is to analyze the probability of false positive , i.e. , @xmath270 , and the probability of false negative , i.e. , @xmath271 .",
    "again , we should keep in mind that , in the proposed method , i.e. , alg .",
    "[ alg_recovery ] , the minimum estimator is merely the first crude step for filtering out many true zero coordinates .",
    "false positives will have chance to be removed by the gap estimator and iterative process .",
    "[ thm_fp ] assume @xmath272 , where @xmath273 .",
    "then @xmath274 * proof : *  @xmath275 , where @xmath109 and @xmath110 are i.i.d .",
    "@xmath31 variables . when @xmath117 , @xmath276 .",
    "using the probability bound in lemma  [ lem_f_lower ] , we obtain @xmath277^m =   \\left[\\mathbf{pr}\\left(|s_2/s_1| > \\epsilon/\\theta\\right)\\right]^m\\\\\\notag = & \\left[1-\\mathbf{pr}\\left(|s_2/s_1|^{\\alpha/(1-\\alpha)}\\leq \\left(\\epsilon/\\theta\\right)^{\\alpha/(1-\\alpha)}\\right)\\right]^m = \\left(1-f_\\alpha(\\psi)\\right)^m \\leq",
    "\\left(1-\\frac{1}{1 + 1/\\psi}\\right)^m = \\frac{1}{(1+\\psi)^m}\\end{aligned}\\]]@xmath254    the assumption @xmath278 is reasonable for small @xmath9 because @xmath279 .",
    "we derive the required @xmath5 , number of measurements , based on the false positive probability in theorem  [ thm_fp ] .",
    "this complexity result is useful if we just use one iteration , which matches the known complexity bounds in the compressed sensing literature .    to ensure that the total number of false positives is bounded by @xmath213",
    ", it suffices to let @xmath280@xmath254    since @xmath281 and @xmath282 , we define @xmath283 as a convenient approximation .",
    "note that the parameter @xmath140 affects the required @xmath5 only through @xmath284 .",
    "this means our algorithm is not sensitive to the choice of @xmath140 .",
    "for example , when @xmath150 , then @xmath285 , @xmath286 .",
    "if we can afford to use very small @xmath9 like 0.001 , then @xmath287 and @xmath288 .",
    "[ thm_fn ] if @xmath289 , @xmath290 , then @xmath291^m\\right\\}%\\\\ % \\leq&m\\left|\\frac{|x_i|+\\epsilon}{\\theta_i}\\right|^{\\alpha } % \\left(1-\\left|\\frac{|x_i|-\\epsilon}{|x_i|+\\epsilon}\\right|^{\\alpha/(1-\\alpha)}\\right)\\end{aligned}\\ ] ] * proof : *  @xmath292^m\\end{aligned}\\ ] ] again , we can write @xmath293 . by symmetry",
    ", @xmath294 the result follows from the probability bounds , @xmath295 and @xmath296 .",
    "@xmath254      we can better understand the choice of @xmath140 from the false negative probability as shown in theorem  [ thm_fn ] .",
    "assume @xmath297 and @xmath298 , the probability @xmath271 upper bound is roughly @xmath299^m\\approx 1-\\left[1-\\frac{3/4}{k}\\frac{2\\alpha}{h_i}\\right]^m \\approx1- e^{-\\frac{3/2\\alpha m}{kh_i } } \\approx \\frac{3/2\\alpha m}{kh_i}\\end{aligned}\\ ] ] as we usually choose @xmath300 , we have @xmath301 . to ensure that all the @xmath0 nonzero coordinates can be safely detected by the minimum estimator ( i.e.",
    ", the total false negatives should be less than @xmath213 ) , we need to choose @xmath302 for sign signals , i.e. , @xmath303 if @xmath297 , we need to have @xmath304 , or equivalently @xmath305 . if @xmath153 ( or 1000 ) , it is sufficient to let @xmath306 ( or @xmath50 ) . note that even with @xmath307 ( and @xmath166 ) , @xmath308 is still not large .    for general signals , when the smallest @xmath309 dominate @xmath310 , we essentially just need the smallest @xmath311 , without the @xmath0 term . in our experiments , for simplicity , we let @xmath312 , for both sign signals and gaussian signals . +",
    "again , we emphasize that this threshold analysis is based on the minimum estimator , for the first iteration only . with the gap estimator and the iterative process , we find the performance is not sensitive to @xmath140 as long as it is small , e.g. , @xmath313 to @xmath314 .",
    "the absolute minimum estimator only detects the locations of nonzero coordinate ( in the first iteration ) . to estimate",
    "the magnitudes of these detected coordinates , we resort to the gap estimator , defined as follows : @xmath315    although the gap estimator is intuitive from figure  [ fig_appr_cdf ] , the precise analysis is not trivial . to analyze the recovery error probability bound of @xmath84",
    ", we first need a bound of the gap probability .",
    "[ lem_eta ] let @xmath316 , @xmath317 , @xmath318 , @xmath73 , @xmath319 , and @xmath320 , [ 2 ] , ... , [ m]\\}$ ] a permutation of @xmath321 giving @xmath322 } \\leq t_{i,[2]}\\leq ... \\leq t_{i,[m]}$ ] .",
    "then @xmath323}| - |z_{i,[k+1]}|}{|z_{i,[2 ] } - z_{i,[1]}|}\\leq 1 , \\ \\frac{f_\\alpha(t_{i,[1]})}{f_\\alpha(t_{i,[2]})}\\leq ( c_0 - 1)^{1/\\gamma}\\right ) \\leq \\eta_{k,\\gamma , c_0}\\left(1 + \\frac{1}{2k}\\right)\\\\\\label{eqn_eta } & \\eta_{k,\\gamma , c_0}= \\min\\left\\{u\\in(0,1 ) : c_0\\left(1-\\left(\\frac{u}{2k}\\right)^{1/k}\\right)^{\\gamma } + \\left(1-\\frac{u}{2k}\\right)^{\\gamma}\\leq 1\\right\\}\\end{aligned}\\ ] ] * proof : *  see appendix  [ app_lem_eta].@xmath254    although in this paper we only use @xmath324 ( i.e. , @xmath325 and @xmath326})}{f_\\alpha(t_{i,[2]})}\\leq ( c_0 - 1)^{1/\\gamma}$ ] always holds ) , we keep a more general bound which might improve the analysis of the gap estimator ( or other estimators ) in future study . also , we should notice that as @xmath327 ( i.e. , @xmath48 ) , @xmath328 .    as shown in figure  [ fig_eta ] , for small @xmath329 , the constant @xmath330 can be numerically evaluated . for larger @xmath329 values , we resort to an upper bound which is numerically stable for any @xmath329 , in the next lemma .",
    "@xmath331    * proof : *  note that @xmath332 is close to 1 and @xmath333 close to 0 .",
    "we apply the inequalities : @xmath334 , @xmath335 , @xmath336 , to obtain @xmath337@xmath254      [ thm_gap ] let @xmath317 .",
    "suppose the existence of @xmath338 and positive integer @xmath339 satisfying @xmath340 , i.e. , @xmath341 , where @xmath342 .",
    "then @xmath343 where @xmath344 is the binomial cdf . +",
    "* proof : *  the idea is that we can start counting the gaps from @xmath339-th gap , because we only need to ensure that the estimated @xmath29 is within a small neighborhood of the true @xmath29 , not necessarily from the first gap .",
    "we choose @xmath345 .",
    "if @xmath346}\\right ) < q$ ] , then @xmath347}\\right ) < q$ ] , or equivalently , @xmath348 } < q/(1-q)$ ] .",
    "thus , @xmath349}^\\gamma < \\theta \\left(q/(1-q)\\right)^\\gamma\\leq \\epsilon$ ] when @xmath346}\\right)<q$ ] and + @xmath350 } - z_{i,[k+1]}|\\right ) > |z_{i,[2 ] } - z_{i,[1]}|$ ] .",
    "thus , the result follows from lemma  [ lem_eta ] and the fact that @xmath351}\\right)\\geq q\\right ) \\leq \\mathbf{pr}\\left(binomial(m , q ) < k_0\\right)$ ] .",
    "@xmath254 +    since @xmath339 in theorem  [ thm_gap ] only takes finite values , we can basically numerically evaluate @xmath352 to obtain the upper bound for @xmath353 .",
    "it turns out that , once @xmath9 and @xmath140 are fixed , @xmath354 is only a function of @xmath355 and the ratio @xmath356 . also , note that @xmath357 .     as computed in ( [ eqn_upper_g ] ) .",
    "the labels on the curves are the values of @xmath358 .",
    "for example , when using @xmath359 and @xmath360 0.005 / 0.01 / 0.03 / 0.05 , the error probabilities are 0.042 / 0.046 / 0.084 / 0.163 .",
    "in other word , in order for the error probability to be @xmath361 , it suffices to use @xmath359 if @xmath362 or 0.01 .",
    "however , if @xmath150 or @xmath363 , we will have to use respectively @xmath364 and @xmath365 measurements in order to achieve error probability @xmath366 .",
    ", title=\"fig:\",width=240 ]   as computed in ( [ eqn_upper_g ] ) .",
    "the labels on the curves are the values of @xmath358 .",
    "for example , when using @xmath359 and @xmath360 0.005 / 0.01 / 0.03 / 0.05 , the error probabilities are 0.042 / 0.046 / 0.084 / 0.163 .",
    "in other word , in order for the error probability to be @xmath361 , it suffices to use @xmath359 if @xmath362 or 0.01 .",
    "however , if @xmath150 or @xmath363 , we will have to use respectively @xmath364 and @xmath365 measurements in order to achieve error probability @xmath366 .",
    ", title=\"fig:\",width=240 ]   as computed in ( [ eqn_upper_g ] ) .",
    "the labels on the curves are the values of @xmath358 .",
    "for example , when using @xmath359 and @xmath360 0.005 / 0.01 / 0.03 / 0.05 , the error probabilities are 0.042 / 0.046 / 0.084 / 0.163 .",
    "in other word , in order for the error probability to be @xmath361 , it suffices to use @xmath359 if @xmath362 or 0.01 .",
    "however , if @xmath150 or @xmath363 , we will have to use respectively @xmath364 and @xmath365 measurements in order to achieve error probability @xmath366 .",
    ", title=\"fig:\",width=240 ]   as computed in ( [ eqn_upper_g ] ) .",
    "the labels on the curves are the values of @xmath358 .",
    "for example , when using @xmath359 and @xmath360 0.005 / 0.01 / 0.03 / 0.05 , the error probabilities are 0.042 / 0.046 / 0.084 / 0.163 . in other word , in order for the error probability to be @xmath361 , it suffices to use @xmath359 if @xmath362 or 0.01 .",
    "however , if @xmath150 or @xmath363 , we will have to use respectively @xmath364 and @xmath365 measurements in order to achieve error probability @xmath366 .",
    ", title=\"fig:\",width=240 ]    figure  [ fig_g ] plots the upper bound @xmath352 for @xmath362 , 0.01 , 0.03 , and 0.05 , in terms of @xmath367 and @xmath368 . for example , when using @xmath359 and @xmath360 0.005 / 0.01 / 0.03 / 0.05 , the error probabilities are 0.042 / 0.046 / 0.084 / 0.163 . in other word , in order for the error probability to be @xmath361 , it suffices to use @xmath359 if @xmath362 or 0.01 .",
    "when @xmath150 or @xmath363 , we will have to use respectively @xmath364 and @xmath365 measurements in order to achieve error probability @xmath366 .",
    "this way , the required sample size can be at least numerically computed from theorem  [ thm_gap ] . of course",
    ", we should keep in mind that the values are merely the ( possibly conservative ) upper bounds .      the `` idealized '' algorithm analyzed in sec .",
    "[ sec_ideal ] assumes @xmath48 and that , as long as there are two or more observations within @xmath138 , there will be an algorithm which could perfectly recover @xmath29 ( provided @xmath139 is small enough ) .",
    "the gap estimator is a surrogate for implementing the `` idealized '' algorithm .    in the `` idealized '' algorithm ,",
    "the probability that @xmath29 can not be recovered is @xmath369 which is basically the limit of @xmath353 in theorem  [ thm_gap ] , as @xmath48 .",
    "recall @xmath370 if @xmath48 .",
    "we can see from ( [ eqn_upper_g ] ) that @xmath371 , which is exactly @xmath372 .",
    "the gap estimator is practical in that the error probability bound ( [ eqn_upper_g ] ) holds for any @xmath9 and @xmath140 .",
    "this property allows us to use a finite @xmath9 ( e.g. , 0.03 ) and very small @xmath140 .",
    "note that @xmath373 is basically @xmath374 .",
    "even if we have to choose @xmath375 , i.e. , @xmath376 , we can still recover @xmath29 by using twice as many examples compared to using @xmath48 .",
    "the analysis of the `` idealized '' algorithm reveals that @xmath154 to @xmath377 might be sufficient for achieving perfect recovery . in our simulation study in sec .",
    "[ sec_simu ] , we find @xmath180 is good enough for our practical algorithm for a range of @xmath378 values . perhaps not surprisingly , one can verify that the values of @xmath379 roughly fall in the @xmath380 range for those values of @xmath378 .",
    "this , to an extent , implies that the performance of our practical procedure , i.e. , alg .",
    "[ alg_recovery ] can be close to what the `` idealized '' algorithm could achieve , despite that the theoretical probability upper bound @xmath381 might be too conservative when @xmath9 is away from zero .",
    "it is intuitive that our method is robust against measurement noise . in this paper , we focus on exact sparse recovery . in the compressed sensing literature",
    ", the common model is to assume additive measurement noise @xmath382 , where each component @xmath383 is the random noise , which is typically assumed to be @xmath384 .",
    "a precise analysis will involve a complicated calculation of convolution .      to provide the intuition ,",
    "we first present a set of experiments with additive noise in figures  [ fig_recsignb1sig01 ] to  [ fig_recgausb1sig05 ] .",
    "with @xmath385 , @xmath170 , and @xmath168 ( i.e. , @xmath386 ) , we have seen in the simulations in sec .",
    "[ sec_simu ] that all methods perform well , in both sign and gaussian signals .",
    "when we add additive noises with @xmath387 to the measurements , figure  [ fig_recsignb1sig01 ] and figure  [ fig_recgausb1sig01 ] show that our proposed method still achieves perfect recovery while lp and omp fail .",
    "when we add more measurement noise by using @xmath388 in figure  [ fig_recsignb1sig05 ] and figure  [ fig_recgausb1sig05 ] , we observe that our method again achieves perfect recovery while both omp and lp fail .    to understand why our method is insensitive to measurement noise , we can examine @xmath389 without measurement noise , our algorithm utilizes observations with @xmath390 to recover @xmath29 , i.e. , either @xmath109 is absolutely very large , or @xmath109 is large only relative to @xmath110 .",
    "because @xmath109 is extremely heavy - tailed , when @xmath390 , it is most likely @xmath391 is extremely large in the absolute scale .",
    "when @xmath109 is small , @xmath392 will be large but likely @xmath393 will be large as well ( i.e. , the observation would not be useful anyway ) .",
    "this intuition explains why our method is essentially indifferent to measurement noise .",
    "an analysis for multiplicative noise turns out to be easy and should also provide a good insight why our method is not sensitive to measurement noise . for convenience , we consider the following model : @xmath394 where @xmath395 s are assumed to be constants , to simplify the analysis .",
    "for example , when @xmath396 ( or @xmath397 ) , this means the measurement @xmath398 is magnified ( or shrunk ) by a factor of 5 .",
    "we assume that we still use the same minimum estimator @xmath399 , where @xmath400 .",
    "[ lem_fpn ] assume @xmath401 , where @xmath402 and @xmath273 .",
    "then @xmath403 * proof : *  the proof is analogous to the proof of theorem  [ thm_fp].@xmath254    note that @xmath404 even for large ( or small ) @xmath395 values . in other words , the false positive error probability is virtually not affected by the measurement noise .",
    "there have been abundant of studies of compressed sensing using the gaussian design matrix .",
    "it is a natural idea to combine these two types of projections .",
    "there are several obvious options .",
    "for example , suppose we can afford to use @xmath405 measurements to detect all the nonzero coordinates with essentially no false positives .",
    "we can then use additional @xmath0 ( or slightly larger than @xmath0 ) gaussian measurements to recover the magnitudes of the ( candidates of ) nonzero coordinates via one least square .",
    "this option is simple and will require @xmath406 total measurements .",
    "+ we have experimented with another idea .",
    "first , we use @xmath407 measurements and alg .",
    "[ alg_recovery ] with gap estimators and iterations .",
    "because the number of measurements may not be large enough , there will be a small number of undetermined coordinates after the procedure .",
    "we can apply additional @xmath408 gaussian measurements and the lp decoding on the set of the undetermined coordinates .",
    "we find this approach also produces excellent recovery accuracy , with about @xmath409 total measurements . + here , we present some interesting experimental results on one more idea . that is , we use @xmath410 measurements and hence there will be a significant number of false positives detected by the minimum estimator @xmath176 . instead of choosing a threshold @xmath140 , we simply take the top-@xmath411 coordinates ranked by @xmath412 .",
    "we then use additional @xmath411 gaussian measurements and lp decoding . in figure",
    "[ fig_errt ] , we let @xmath413 .",
    "when @xmath414 and @xmath415 , even using only @xmath416 additional gaussian measurements produces excellent results . when @xmath169 , we need more ( in this case @xmath417 ) additional measurements , as one would expect .",
    "we anticipate that this paper is a start of a new line of research .",
    "for example , we expect that the following research projects ( among many others ) will be interesting and useful",
    ".    1 .   *",
    "very sparse l0 projections . * instead of using a dense design matrix @xmath8 with entries sampled from i.i.d @xmath31 , we can use an extremely sparse matrix by making ( e.g. , ) @xmath418 or more entries be zero .",
    "furthermore , we can sample nonzero entries from a symmetric @xmath9-pareto distribution , i.e. , a random variable @xmath30 with @xmath419 .",
    "these efforts will significantly speed up the processing and simplify the hardware design .",
    "this is inspired by the work on _ very sparse stable random projections _",
    "* correlated projections .",
    "* it might be possible to use multiple projections with different @xmath9 values to further improve the performance of sparse recovery . recall that we can generate different ( and highly `` correlated '' ) @xmath9-stable variables with the same set of uniform @xmath173 and exponential @xmath172 variables as in ( [ eqn_stable_sample ] ) .",
    "for example , there is a recent work on using correlated stable projections for entropy estimation  @xcite .",
    "compressed sensing has been a highly active area of research , because numerous important applications can be formulated as sparse recovery problems , for example , anomaly detections . in this paper , we present our first study of using l0 projections for exact sparse recovery . our practical procedure , which consists of the minimum estimator ( for detection ) , the gap estimator ( for estimation ) , and the iterative process , is computationally very efficient .",
    "our algorithm is able to produce accurate recovery results with smaller number of measurements , compared to two strong baselines ( lp and omp ) using the traditional gaussian ( or gaussian - like ) design matrix .",
    "our method utilizes the @xmath9-stable distribution with @xmath10 . in our experiments with matlab , in order for interested readers to easily reproduce our results , we take @xmath420 and find no special storage structure is needed at this value of @xmath9 .",
    "our algorithms are robust against measurement noises .",
    "in addition , our algorithm produces stable ( partial ) recovery results with no catastrophic failure even when the number of measurements is very small ( e.g. , @xmath193 ) .",
    "we also analyze an `` idealized '' algorithm by assuming @xmath48 .",
    "for a signal with @xmath11 nonzero coordinates , merely 3 measurements are sufficient for exact recovery . for general @xmath0",
    ", our analysis reveals that about @xmath14 measurements are sufficient regardless of the length of the signal vector .",
    "finally , we anticipate this work will lead to interesting new research problems , for example , very sparse l0 projections , correlated projections , etc , to further improve the algorithm .",
    "10    emmanuel cands and justin romberg .",
    "@xmath158-magic : reocvery of sparse signals via convex programming .",
    "technical report , calinfornia institute of technology , 2005 .",
    "emmanuel cands , justin romberg , and terence tao .",
    "robust uncertainty principles : exact signal reconstruction from highly incomplete frequency information . , 52(2):489509 , 2006 .",
    "john  m. chambers , c.  l. mallows , and b.  w. stuck . a method for simulating stable random variables .",
    ", 71(354):340344 , 1976 .",
    "scott  shaobing chen , david  l. donoho , michael , and a.  saunders .",
    "atomic decomposition by basis pursuit .",
    ", 20:3361 , 1998 .",
    "graham cormode and s.  muthukrishnan .",
    "an improved data stream summary : the count - min sketch and its applications .",
    ", 55(1):5875 , 2005 .",
    "n.  cressie . a note on the behaviour of the stable distributions for small index .",
    ", 31(1):6164 , 1975 .",
    "daivd  l. donoho and xiaoming huo .",
    "uncertainty principles and ideal atomic decomposition .",
    ", 40(7):28452862 , nov .",
    "david  l. donoho .",
    "compressed sensing .",
    ", 52(4):12891306 , 2006 .",
    "david  l. donoho , arian maleki , and andrea montanari .",
    "message - passing algorithms for compressed sensing . , 106(45):1891418919 , 2009 .",
    "david  l. donoho and philip  b. stark .",
    "uncertainty principles and signal recovery . , 49(3):906931 , 1989 .",
    "david  l. donoho and jared tanner .",
    "counting faces of randomly projected polytopes when the projection radically lowers dimension . , 22(1 ) , jan .",
    "alyson  k. fletcher and sundeep rangan .",
    "orthogonal matching pursuit from noisy measurements : a new analysis . in _",
    "nips_. 2009 .",
    "a.  gilbert and p.  indyk .",
    "sparse recovery using sparse matrices . , 98(6):937 947 , june 2010 .    izrail  s. gradshteyn and iosif  m. ryzhik . .",
    "academic press , new york , seventh edition , 2007 .",
    "piotr indyk .",
    "stable distributions , pseudorandom generators , embeddings , and data stream computation . , 53(3):307323 , 2006",
    ".    ping li .",
    "very sparse stable random projections for dimension reduction in @xmath421 ( @xmath422 ) norm . in _",
    "kdd _ , san jose , ca , 2007 .",
    "estimators and tail bounds for dimension reduction in @xmath421 ( @xmath422 ) using stable random projections . in _ soda _ , pages 10  19 , san francisco , ca , 2008 .",
    "ping li , kenneth  w. church , and trevor  j. hastie .",
    "one sketch for all : theory and applications of conditional random sampling . in _ nips _ ,",
    "vancouver , bc , canada , 2008 ( preliminary results appeared in nips 2006 ) .    ping li and cun - hui zhang .",
    "entropy estimations using correlated symmetric stable random projections . in _ nips _ , lake tahoe , nv , 2012 .",
    "tsung - han lin and h.  t. kung .",
    "compressive sensing medium access control for wireless lans . in _",
    "globecom _ , 2012 .",
    "mallat and zhifeng zhang . matching pursuits with time - frequency dictionaries .",
    ", 41(12):3397 3415 , 1993 .",
    "s.  muthukrishnan .",
    "data streams : algorithms and applications . , 1:117236 , 2 2005 .",
    "d.  needell and j.a .",
    "cosamp : iterative signal recovery from incomplete and inaccurate samples . , 26(3):301321 , 2009 .",
    "gennady samorodnitsky and murad  s. taqqu . .",
    "chapman & hall , new york , 1994 .",
    "robert tibshirani .",
    "regression shrinkage and selection via the lasso .",
    ", 58(1):267288 , 1996 .",
    "greed is good : algorithmic results for sparse approximation .",
    ", 50(10):2231  2242 , oct .",
    "jun wang , haitham hassanieh , dina katabi , and piotr indyk .",
    "efficient and reliable low - power backscatter networks . in _ sigcomm _ , pages 6172 , helsinki , finland , 2012 .",
    "meng wang , weiyu xu , enrique mallada , and ao  tang .",
    "sparse recovery with graph constraints : fundamental limits and measurement construction . in _ infomcom _ , 2012 .",
    "tong zhang .",
    "sparse recovery with orthogonal matching pursuit under rip .",
    ", 57(9):6215 6221 , sept . 2011 .",
    "haiquan  ( chuck ) zhao , nan hua , ashwin lall , ping li , jia wang , and jun xu . towards a universal sketch for origin - destination network measurements . in _ npc _",
    ", 2011 .",
    "haiquan  ( chuck ) zhao , ashwin lall , mitsunori ogihara , oliver spatscheck , jia wang , and jun xu .",
    "a data streaming algorithm for estimating entropies of od flows . in _",
    "imc _ , san diego , ca , 2007 .",
    "[ app_f_lower ] to show , @xmath423 , @xmath424 where @xmath425^{(1-\\alpha)/\\alpha}\\end{aligned}\\ ] ]    * proof : *  firstly , since @xmath426 and @xmath427 are independent @xmath428 variables , we have @xmath429 note that @xmath430 , and @xmath431 for the other bound , we let @xmath432 , which is symmetric about 0 . this way , we can write @xmath433 . note that @xmath434 is convex when @xmath435 ( and hence jensen s inequality applies ) . for",
    "now , we assume @xmath436 ( i.e. , @xmath437 ) and obtain @xmath438 in particular , when @xmath439 , we have @xmath440 .",
    "also , note that when @xmath441 , @xmath442 is sharper than the other bound .",
    "to prove , for any fixed @xmath443 , @xmath444 , we just need to use dominated convergence theorem and the fact that @xmath445 point - wise .",
    "this completes the proof .",
    "to show , if @xmath446 , then @xmath447 where @xmath448    * proof : *  we need to first find a good lower bound of @xmath449 , where @xmath450 we will make use of the following inequalities , when @xmath451 , @xmath452 to see @xmath453 , consider , @xmath454 $ ] , @xmath455 .",
    "we can bound @xmath449 as follows : @xmath456    because @xmath457 and @xmath458 are i.i.d .",
    "@xmath459 , we know that @xmath460 and @xmath461 are i.i.d .",
    "standard cauchy variables .",
    "therefore , @xmath462 where @xmath463 .",
    "note that @xmath464 is concave in @xmath465 .",
    "furthermore , for any @xmath466 , @xmath467 where @xmath468 .",
    "the next task is to find the @xmath469 which minimizes this upper bound .",
    "note that @xmath470 , attained at @xmath471 .",
    "thus , we obtain @xmath472 where , using integral formulas  @xcite ( assuming @xmath473 ) @xmath474    therefore , we can write @xmath447 where @xmath475 moreover , @xmath258 as @xmath208 , @xmath259 if @xmath260 , and @xmath261 if @xmath476 .",
    "this completes the proof .",
    "let @xmath316 , @xmath317 , @xmath318 , @xmath73 , @xmath319 , and @xmath320 , [ 2 ] , ... , [ m]\\}$ ] a permutation of @xmath321 giving @xmath322 } \\leq t_{i,[2]}\\leq ... \\leq t_{i,[m]}$ ] .",
    "to show @xmath477}| - |z_{i,[k+1]}|}{|z_{i,[2 ] } - z_{i,[1]}|}\\leq 1 , \\ \\frac{f_\\alpha(t_{i,[1]})}{f_\\alpha(t_{i,[2]})}\\leq ( c_0 - 1)^{1/\\gamma}\\right ) \\leq \\eta_{k,\\gamma , c_0}\\left(1 + \\frac{1}{2k}\\right)\\\\\\notag & \\eta_{k,\\gamma , c_0}= \\min\\left\\{u\\in(0,1 ) : c_0\\left(1-\\left(\\frac{u}{2k}\\right)^{1/k}\\right)^{\\gamma } + \\left(1-\\frac{u}{2k}\\right)^{\\gamma}\\leq 1\\right\\}\\end{aligned}\\ ] ]    * proof : *  suppose @xmath478})/f_\\alpha(t_{i,[2 ] } ) \\leq ( c_0 - 1)^{1/\\gamma}$ ] . recall that",
    "@xmath479 with @xmath480 . since @xmath481 for @xmath482",
    ", we know that @xmath483}^\\gamma$ ] ; and hence @xmath484}| - |z_{i,[k+1]}|\\leq |z_{i,[k+2]}-z_{i,[k+1]}| \\leq |z_{i,[k+2]}-x_i|+|z_{i,[k+1]}-x_i|$ ] , it follows that latexmath:[\\ ] ]"
  ],
  "abstract_text": [
    "<S> many applications concern sparse signals , for example , detecting anomalies from the differences between consecutive images taken by surveillance cameras . </S>",
    "<S> this paper focuses on the problem of recovering a @xmath0-sparse signal @xmath1 , i.e. , @xmath2 and @xmath3 . in the mainstream framework of compressed sensing ( cs ) , </S>",
    "<S> the vector @xmath4 is recovered from @xmath5 non - adaptive linear measurements @xmath6 , where @xmath7 is typically a gaussian ( or gaussian - like ) design matrix , through some optimization procedure such as linear programming ( lp ) .    in our proposed method , </S>",
    "<S> the design matrix @xmath8 is generated from an @xmath9-stable distribution with @xmath10 . </S>",
    "<S> our decoding algorithm mainly requires one linear scan of the coordinates , followed by a few iterations on a small number of coordinates which are `` undetermined '' in the previous iteration . </S>",
    "<S> our practical algorithm consists of two estimators . in the first iteration , the _ ( absolute ) minimum estimator _ is able to filter out a majority of the zero coordinates . </S>",
    "<S> the _ gap estimator _ , which is applied in each iteration , can accurately recover the magnitudes of the nonzero coordinates . </S>",
    "<S> comparisons with two strong baselines , linear programming ( lp ) and orthogonal matching pursuit ( omp ) , demonstrate that our algorithm can be significantly faster in decoding speed and more accurate in recovery quality , for the task of exact spare recovery . </S>",
    "<S> our procedure is robust against measurement noise . </S>",
    "<S> even when there are no sufficient measurements , our algorithm can still reliably recover a significant portion of the nonzero coordinates .    to provide the intuition for understanding our method </S>",
    "<S> , we also analyze the procedure by assuming an idealistic setting . </S>",
    "<S> interestingly , when @xmath11 , the `` idealized '' algorithm achieves exact recovery with merely @xmath12 measurements , regardless of @xmath13 . for general @xmath0 , </S>",
    "<S> the required sample size of the `` idealized '' algorithm is about @xmath14 . </S>",
    "<S> the gap estimator is a practical surrogate for the `` idealized '' algorithm . </S>"
  ]
}