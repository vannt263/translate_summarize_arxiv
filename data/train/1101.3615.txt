{
  "article_text": [
    "this paper considers the imaging problem of determining physical characteristics in a region of space given surface measurements of scattered waves .",
    "several imaging modalities fall under this umbrella ( ground - penetrating radar , nondestructive acoustic testing , remote personnel assessment ) , but in the sequel we focus exclusively on the example of reflection seismology . throughout this paper",
    "we let @xmath0 for the physical parameters in the subsurface , and @xmath1 for the recorded waveforms ( seismograms ) . here",
    "@xmath2 are the space coordinates in the volume , @xmath3 is receiver position , @xmath4 is source position , and @xmath5 is time .    a most popular way of treating the inversion problem of recovering @xmath6 from @xmath7 is through the minimization of the output least - squares functional @xmath8 = \\frac{1}{2 } \\| d - \\mathcal{f}[m ] \\|^2_2,\\ ] ] where @xmath9 is the nonlinear map for predicting data from the model @xmath6 .",
    "in this paper we restrict ourselves to the setup of constant - density acoustics , and let @xmath0 be a variable wave speed .",
    "the prediction @xmath10 $ ] then consists of the solutions @xmath11  sampled at the receivers @xmath12  of the acoustic wave equations @xmath13 with different right - hand sides @xmath14 index by @xmath4 ( the source ) .",
    "the notation @xmath15 refers to the sum of the squares of the components .",
    "the quantity @xmath0 is the inverse of the square of the local wave speed .",
    "whether data is considered all at once , or by frequency increments as in full waveform inversion , the procedure for minimizing @xmath16 $ ] is usually some variant of the gauss - newton method , which consists in linearizing @xmath16 $ ] about some current vector @xmath17 . specifically , if a new vector @xmath18 is sought so that @xmath19 $ ] is closer to the minimum than @xmath20 $ ] is",
    ", then we first write @xmath21 = j[m_0 ] + { \\langle}\\frac{\\delta j}{\\delta m}[m_0 ] , \\ , \\delta m { \\rangle}+ \\frac{1}{2 } { \\langle}\\delta m , \\ ,   \\frac{\\delta^2 j}{\\delta m^2}[m_0 ] \\ , \\delta m { \\rangle}+ ... \\ ] ] where @xmath22 , and find @xmath23 as the minimum of the quadratic form above .",
    "the solution is @xmath24 + \\frac{\\delta^2 j}{\\delta m^2}[m_0 ] \\ , \\delta m \\qquad \\rightarrow \\qquad \\delta m = - \\left ( \\frac{\\delta^2 j}{\\delta m^2}[m_0 ] \\right)^{-1 } \\frac{\\delta j}{\\delta m}[m_0].\\ ] ] this equation is a newton descent step : it is then applied iteratively to obtain a new @xmath25 from @xmath18 , etc .",
    "the hessian is the operator @xmath26 $ ] .",
    "if @xmath16 $ ] is the least - squares misfit functional above , then by denoting @xmath27,\\ ] ] we obtain the first and second variations of @xmath28 as @xmath29 = - f^ * ( d - \\mathcal{f}[m_0 ] ) , \\ ] ] @xmath30 = f^ * f - { \\langle}\\frac{\\delta^2 \\mathcal{f}}{\\delta m^2}[m_0 ] , d - \\mathcal{f}[m_0 ] { \\rangle}.\\ ] ] the migration operator @xmath31 acts from data space to model space , and is most accurately computed by reverse - time migration .",
    "the demigration operator @xmath32 acts from model space to data space , and can be computed by solving a forward  modeling \" wave equation .",
    "the term involving the second variation of @xmath9 in the expression of the hessian is routinely discarded on the basis that @xmath9 is  locally well - linearized \"  a heuristically plausible claim when @xmath17 is smooth in comparison to @xmath23  but which has so far eluded rigorous analysis . with this simplification in mind",
    ", we refer to the ( reduced ) hessian @xmath33 as the leading - order contribution @xmath34 this linear operator is also called the normal operator , and acts within model space .",
    "the newton descent step then calls for computing the pseudoinverse @xmath35 , well - known to arise in the solution of the overdetermined linearized least - squares problem .",
    "physically , inversion of the hessian corresponds to the idea of correcting for low levels of illumination of the medium by the forward ( physical ) wavefield .",
    "although illumination seems to make good sense as a function of space @xmath2 , it is in fact unclear how to define it as such .",
    "rather , it is more appropriate to define illumination as a function in _ phase - space _ , i.e. , the set of @xmath2 and @xmath36 ( wave vectors ) . in the words of nammour and symes @xcite",
    ", illumination is not just a scaling , but a dip - dependent scaling .",
    "this paper follows this idea by considering the pseudodifferential symbol of the hessian .    while reasonably efficient methods of applying the operators @xmath32 and @xmath31 to vectors are common knowledge , little is currently known about the structure of the inverse hessian @xmath37 .",
    "direct linear algebra methods for computing a matrix inverse are out of the question , because the matrix @xmath33 is too large to be formed in practice .",
    "this also prevents the immediate application of methods such as bfgs .",
    "iterative linear algebra methods such as gmres or lsqr can be set up , but need a very large number of iterations to converge due to the poor conditioning of @xmath33 .",
    "the problem of slow convergence is particularly acute since the full prestack data space ( after the application of @xmath32 ) is much larger than poststack model space  hence each application of @xmath38 is very costly .",
    "the obvious alternative to the gauss - newton iteration , namely straight gradient descent without considering the hessian , is even less attractive than gmres for solving the ill - conditioned linearized least - squares problem .",
    "preconditioning is needed to properly guide the inversion iterations .",
    "a preconditioner for a matrix @xmath33 is a matrix @xmath39 that approximates the inverse @xmath37 .",
    "it can be used to rewrite @xmath40 as @xmath41 where now only the matrix @xmath42 needs to be inverted . an alternative formulation",
    "where @xmath43 postmultiplies @xmath33 is also possible .",
    "several preconditioners for the wave equation hessian have already been proposed in the literature : they are reviewed in context in section [ sec : review ] .",
    "this paper solves the preconditioning problem by  probing \" , or testing the hessian by applying it to a small number of randomized vectors , followed by a fit of the inverse hessian in a special expansion scheme in phase - space .",
    "our work is closest in spirit to that of nammour and symes @xcite and herrmann et al .",
    "@xcite ( which in turn follows from a legacy of so - called scaling preconditioners reviewed below ) but departs from it in that the trial space is randomized instead of being the krylov subspace of the migrated model , for a matrix @xmath33 , is the space spanned by @xmath44 , etc . ] .",
    "randomness of the trial functions guarantees recovery of the action of the inverse hessian on a much larger linear subspace than is normally the case with a deterministic method .",
    "this claim is backed both by numerical experiments ( section [ sec : num ] ) and by a theoretical justification ( section [ sec : theory ] ) .",
    "the proposed approach bridges a gap in the literature , in that we obtain quantitative results  hence finally a rationale  for the probing methods to precondition the wave - equation hessian .",
    "we found that randomization is an important step to achieve such guarantees , and may be an attractive numerical choice in its own right .      to provide an expansion scheme for the inverse hessian , it is important to understand its structure as a pseudodifferential operator . in the sequel",
    "we consider only two spatial dimensions @xmath45 , but the main ideas do not depend on this assumption .",
    "it is well - known that migration @xmath31 is a ",
    "kinematic \" inverse of the modeling operator @xmath32 in the sense that the mapping of singularities generated by @xmath31 generically undoes that of @xmath32 . putting technical pathologies aside",
    ", this claim means that @xmath46 does not change the location of singularities in model space .",
    "hence the hessian is  microlocally equivalent \" to the identity , or  microlocal \" for short .",
    "this property was understood and made precise by at least the following people .    * in 1985 , beylkin showed that the hessian is pseudodifferential in the absence of caustics , and in the context of generalized radon transforms @xcite .",
    "* in 1988 , rakesh removed the no - caustic assumption , but considers a point source and full - aperture ( whole - earth ) data @xcite . * in 1998 , ten kroode , smit and verdel showed that beylkin s result still holds if a less restrictive  traveltime injectivity condition \" is satisfied @xcite . * in 2000 , stolk refined these results by showing that the hessian is generically invertible : if a @xmath47 wave speed does not give rise to a pseudodifferential hessian , an arbitrarily small @xmath47 perturbation of it will @xcite .",
    "the consequence of this body of theory for the problem of designing a compressed numerical representation of the hessian is the following .",
    "we will consider a representation of the hessian as a pseudodifferential operator : @xmath48 where hat denotes fourier transformation in the spatial variables .",
    "the amplitude , or symbol @xmath49 plays the role of illumination in phase - space @xmath50 as alluded to earlier .",
    "there is nothing special about writing an integral sign instead of a sum : interpolation and sampling allow to transform number arrays into functions and vice - versa . keeping @xmath2 and @xmath36 continuous for the time being however offers the opportunity to discuss the important point : _ smoothness _ of the symbol @xmath49 .",
    "indeed , while the symbol representation ( [ eq : symbol ] ) is always available whichever the linear operator considered , the symbol will be smooth in a very specific way for  microlocal \" operators as discussed above .",
    "we say that the symbol @xmath49 is of order @xmath51 ( and type @xmath52 ) if it obeys the condition @xmath53 , @xmath54 , @xmath55 , and similarly for @xmath56 .",
    "notice that since the bound",
    "_ decreases _ by one power of @xmath57 for every derivative in @xmath36 , it means that the larger @xmath58 the smoother the symbol @xmath49 .",
    "if we had considered the symbol of either @xmath32 or @xmath31 instead , each derivative in @xmath36 space would have _ increased _ the value of the symbol by a quantity proportional to @xmath36 .",
    "physically , illumination is a phase - space concept , but it is  not too far \" from being purely a function of @xmath2 since the @xmath36 dependence of @xmath59 is extremely smooth for large @xmath58 .",
    "there is one very idealized scenario in which the hessian obeys the condition ( [ eq : type10 ] ) with order @xmath60 .",
    "the assumptions are the following : 1 ) sufficiently fine cartesian sampling of the data in time and receiver coordinate ( so that the sum can easily be written as an integral ) , 2 ) full aperture acquisition , 3 ) a point - impulse wavelet @xmath61 in time , and 4 ) smooth and generic background physical parameters such as wave speed . if all these conditions are met , it is known at least from @xcite that the hessian has a symbol that obeys ( [ eq : type10 ] ) .    in turn , if a symbol obeys ( [ eq : type10 ] ) , it is by now well - known that it is in fact extraordinarily compressible numerically .",
    "bao and symes @xcite show that the asymptotic behavior of @xmath49 as @xmath62 , i.e. the action of the hessian at very small scales , can be encoded using only a few fourier series coefficients in @xmath2 and in @xmath63 : @xmath64 recent work by demanet and ying @xcite has shown how to add degrees of freedom in the radial wave number variable @xmath58 to obtain an @xmath65-accurate expansion of @xmath49 : @xmath66 where the @xmath67 are rational chebyshev functions .",
    "the number of terms in the sum is a @xmath68 for all @xmath69 .",
    "other expansion schemes exist , such as the hierarchical spline grids in @xmath36 space , considered in @xcite . in practice , symbols are considered for values of @xmath36 that obey @xmath70 for some large @xmath71 . in view of the shannon sampling theorem , this restriction corresponds to sampling ( 2d ) functions on a square grid as vectors of length @xmath71 , and operators such as the hessian as matrices of size @xmath71-by-@xmath71 .",
    "both ( [ eq : compr1 ] ) and ( [ eq : compr2 ] ) are good approximations of the symbol @xmath49 in the sense that they each contain a number of terms _ independent of the size @xmath71 of the matrix _ that eventually realizes the hessian .    in three spatial dimensions , spherical harmonics would be used in place of complex exponentials in angle .",
    "otherwise , the symbol expansion scheme needs not be changed .",
    "equation ( [ eq : compr2 ] ) provides a decomposition of @xmath33 into  elemetary operators \" @xmath72 , each with symbol @xmath73 .",
    "the index @xmath74 is a shorthand for @xmath75 , and accordingly we let @xmath76 for the coefficients @xmath77 . in this more compact notation",
    "we have the fast - converging expansion @xmath78 for the hessian .",
    "it is not the hessian that is of interest , but rather the inverse hessian .",
    "fortunately , it is a result of shubin @xcite that if the symbol @xmath49 of an operator obeys ( [ eq : type10 ] ) , and if this operator is assumed to be invertible , then the symbol @xmath79 of the inverse of the operator also obeys ( [ eq : type10 ] ) , namely @xmath80 that are possibly different from @xmath81 . notice that the order is now @xmath82 . in other words ,",
    "smoothness of the symbol is preserved , or closed , under inversion .",
    "if the operator is invertible but only barely so ( small singular values which are not regularized ) , then the constants @xmath83 may become large , but the behavior under differentiations in @xmath36 space is still controlled by ( [ eq : type10_b ] ) . note in passing that @xmath79 is not exactly given by @xmath84 , but the latter is an approximation of @xmath79 that mathematicians find satisfying when @xmath58 is large .    using the same expansion scheme as above we write @xmath85 with different coefficients @xmath86 .",
    "the four assumptions on the sampling , aperture , wavelet , and medium enumerated earlier are of course far from being realistic in practice .",
    "their violation invariably creates ill - conditioning in the form of a linear subspace in model space where applying the hessian will return very small values .",
    "this issue manifests itself as small values of the symbol @xmath49 . for instance",
    "( and this may not be an exhaustive list ) ,    * limiting the sampling and the aperture will create an angular deficiency in the sense that reflectors with certain orientations will not be visible in the dataset .",
    "the symbol @xmath49 will take on small values for the kinematically ",
    "invisible \" @xmath2 and @xmath36 .",
    "* restricting the wavelet in @xmath87 space ( frequency ) will have the effect to remove low and high wavenumbers from the data .",
    "this will have the effect of restricting the symbol @xmath49 in wave number @xmath58 .",
    "* finally , complicated kinematics of the background wave speed(s ) may create shadow zones in which there is very poor illumination .",
    "such is the region behind an impenetrable sphere . in that case",
    "the symbol @xmath49 becomes very small in those inaccessible regions .",
    "the subspace of model space in which the hessian produces small values is a numerical version of its nullspace . because this subspace is nonempty , not all vectors in model space are accessible from applying the hessian to some other vector",
    ": the range space does not have full dimension .",
    "in other words , the hessian does not have full rank .",
    "it is well - known from linear algebra that the dimension of the ( numerical ) nullspace is equal to the codimension of the ( numerical ) range space . because the hessian is symmetric , the range space is in fact orthogonal to the nullspace  and ditto of their numerical versions .",
    "figure [ fig:2spaces ] depicts the fundamental subspaces of the hessian .    .",
    "the blue arrows indicate that , under the action of @xmath33 , the whole space gets mapped to the range space , while the nullspace gets mapped to the origin.,width=340 ]    in spite of these complications , this paper speculates that for models well _ inside the range space _ of the hessian , an estimate like ( [ eq : type10_b ] ) for the inverse hessian holds .",
    "it is not currently known whether this is true theoretically , but we show numerical evidence that supports the claim .",
    "we now address the question of fitting the coefficients in an expansion scheme for the symbol of the inverse hessian , from application of the hessian on randomized trial functions . for the time being we assume that the hessian is invertible and well - conditioned ; we return to the discussion of the nullspace in the next section .",
    "assume that the inverse hessian is an @xmath71-by-@xmath71 matrix that can be expanded as @xmath88 where @xmath72 are themselves matrices , and @xmath89 counts the number of terms .",
    "one possible choice for the @xmath72 was given in section [ sec : psido ] ( up to discretization ) , but here the discussion is general .",
    "denote by @xmath90 a vector of independent and identically distributed ( i.i.d . )",
    "gaussian random variables , in model space  a  noise \" vector . the application of the hessian to @xmath90 is available : @xmath91 given this information , we may now solve for the coefficients @xmath86 in @xmath92 this linear system can be overdetermined only if @xmath93 ; in that case the least - squares solution is @xmath94 where @xmath95 the coefficients @xmath86 can therefore be solved for , in a unique and stable manner , provided the matrix @xmath39 is invertible and well - conditioned .",
    "as we show in the sequel , the invertibility of @xmath39 hinges on two important assumptions on the elementary matrices @xmath72 :    1 .",
    "the @xmath72 obey an @xmath33-dependent near - orthogonality relation : @xmath96 which we express more precisely as requiring that @xmath97 be positive definite .",
    "the symbol @xmath98 stands for mathematical expectation , or ",
    "average over an infinite number of random realizations \" .",
    "2 .   each @xmath72 is a full - rank ( invertible ) , well - conditioned matrix .    when those two conditions are met , we show in section [ sec : theory ] that @xmath99 is an invertible matrix , with high probability , provided @xmath89 is large enough , on the order of the square root @xmath100 of the rank @xmath3 of @xmath39",
    ". this result may not be tight but has the advantage of motivating the two assumptions above .",
    "we suspect that the number @xmath89 of coefficients @xmath86 that can be fitted with this method is in fact closer to a constant times @xmath101  this will be the subject of a separate study .",
    "the expansion schemes in equations ( [ eq : compr1 ] ) and ( [ eq : compr2 ] ) correspond to matrices @xmath72 that obey the above conditions .",
    "notice that if the expansion ( [ eq : expand ] ) is accurate , i.e. that @xmath37 is determined as a linear combination of the @xmath72 , then the proposed method recovers the _ whole matrix _",
    "@xmath37 in compressed form , not just the action of the matrix @xmath37 on the trial vector @xmath102 .",
    "this property is important : we call it generalizability",
    ". the action of @xmath37 can be reliably  generalized \" from its knowledge on @xmath102 , to other vectors .",
    "the randomness of the vector @xmath90 is essential in this regard : it would be much harder to argue generalizability if the vector @xmath90 had been chosen deterministically .",
    "the numerical experiments in section [ sec : num ] confirm this observation .",
    "finally , it is worth noting that @xmath37 needs not be given exactly by a sum of @xmath89 terms of the form @xmath103 .",
    "if the series converges fast instead of terminating exactly , it is possible to show that the coefficients @xmath86 are determined up to an error commensurate with the truncation error of the series .",
    "as mentioned earlier , inversion of the wave - equation hessian is complicated by various factors that create ill - conditioning .",
    "the lack of invertibility not only prevents randomized fitting to work as presented in the previous section , but it also adds to the numerical complexity of the inverse hessian itself . just being able to specify the numerical nullspace  the subspace in which the hessian erases information  is at least as complex as specifying the action of the inverse hessian away from it . as a consequence , it may be advantageous for a coarse preconditioner not to explicitly try and invert the hessian in the neighborhood of the numerical nullspace .",
    "our solution to the ill - conditioning problem is to consider noise realizations @xmath104 that avoid the nullspace , i.e. , belong to the range space of @xmath33 .",
    "the relation @xmath105 then makes sense if we understand @xmath37 as the pseudo - inverse of @xmath33 .",
    "the numerical nullspace of @xmath33 is best described in phase space : it corresponds to the points @xmath50 where the symbol @xmath49 of @xmath33 is small .",
    "this calls for considering an illumination mask , i.e. , a simple 0 - 1 function which indicates whether a point @xmath50 is in the essential support of the symbol ( value 1 ) or not ( value 0 ) . this piece of a priori information is then used to filter out components of the noise vector ( in @xmath2 space ) which would otherwise intersect the nullspace of the hessian .    an explicit expression for the pseudo - differential operator",
    "@xmath33 can be obtained in the idealized case of densely sampled data with idealized sources and receivers .",
    "the process involves the asymptotic expansion(stationary phase analysis ) of a generalized radon transform and is described in @xcite .",
    "we use this expansion as a way of isolating the null space of @xmath33 .",
    "concretely , we built this illumination indicator function in curvelet - transformed model space .",
    "curvelets are directional generalizations of wavelets which are efficient at representing bandlimited wavefronts in a sparse manner @xcite , and have had applications for regularizing the inversion in seismic imaging @xcite .",
    "they also provide a sparse representation of wave propagators @xcite .",
    "each curvelet @xmath106 is indexed by a position vector @xmath107 and a wave vector @xmath108 .",
    "any ( square - integrable ) function @xmath109 can be expanded in curvelets as @xmath110 as explained in section [ sec : theory2 ] , curvelets efficiently discriminate between different regions of phase - space where the symbol of the hessian takes on different values .",
    "consider @xmath111 , the set of curvelets @xmath112 whose center @xmath113 belongs to the essential support of the symbol @xmath49 of the hessian .",
    "the stationary phase analysis mentioned above @xcite reveals the geometric interpretation of these phase - space points : they are _ visible _ , in the ( microlocal ) sense that there is a ray linking some source @xmath4 to the point @xmath2 , reflecting at @xmath2 in a specular fashion about the normal vector @xmath36 , and then linking @xmath2 back to some receiver @xmath3 .",
    "when a curvelet is visible , it means that it acts like a  local reflector \" for some waves that end up being observed in the dataset .",
    "more precisely , a phase - space point @xmath113 belongs by definition to @xmath111 if there exist two rays @xmath114 originating from @xmath107 such that :    * @xmath115 links @xmath107 to some source in the source manifold ; * @xmath116 links @xmath107 to some receiver in the receiver manifold ; and * @xmath116 is a reflected ray for @xmath115 at @xmath107 , i.e. , the angle of incidence is equal to the angle of reflection and the two rays form a plane with the normal direction @xmath108 .",
    "the rays are obtained by ray - tracing from the hamiltonian system of geometrical optics .",
    "the illumination mask is then the sequence equal to @xmath117 if @xmath118 , and zero otherwise .",
    "a noise realization @xmath90 in curvelet space , filtered by the illumination mask , is simply @xmath119 the sequence @xmath120 is then inverted to yield @xmath121 the rest of the algorithm for determining the inverse hessian then proceeds as in the previous section .",
    "once the inverse hessian is available as a series ( [ eq : expand ] ) , the algorithm for applying it to a vector like the migrated model is well - known and very fast @xcite .",
    "being able to extract information on the inverse hessian from a single application of the hessian is a very good idea which perhaps first appeared , in seismology , in the work of claerbout and nichols @xcite .",
    "there , a single scalar function of @xmath2 is sought to represent inverse illumination . in our notations , they seek to fit a symbol @xmath79 which is not a function of @xmath36 .",
    "this work generated refinements that w. symes puts under the umbrella of  scaling methods \" . in 2003 ,",
    "rickett @xcite offers a solution similar to that of claerbout and nichols . in 2004",
    ", guitton @xcite proposes a solution based on  nonstationary convolutions \" which corresponds to considering a symbol @xmath79 which is essentially only a function @xmath36 . in 2008 , symes @xcite proposes to consider symbols of the form @xmath122 i.e. which have the proper homogeneity behavior in @xmath58 . in 2009 ,",
    "nammour and symes @xcite upgrade to the bao - symes expansion scheme given in equation ( [ eq : compr1 ] ) . in 2009 , herrmann et al .",
    "@xcite propose to realize the scaling as a diagonal operator in curvelet space .",
    "in all these papers , it is the remigrated image to which the inverse hessian is applied ; in contrast , our paper uses randomized curvelet trial functions . for the representation of the inverse hessian",
    ", we use both ( [ eq : compr1 ] ) and ( [ eq : compr2 ] ) for its symbol .",
    "it should also be noted that herrmann et al .",
    "@xcite already proposed in 2003 to realize a curvelet - diagonal approximation of the hessian , obtained by randomized testing of the hessian .",
    "the idea of recovering a matrix that has a given sparsity pattern or some other structure from a few applications on well - chosen vectors (  probing \" ) also appeared in the 1990 work of chan and keyes on domain - decomposition preconditioning for convection - diffusion problems @xcite .",
    "see also the 1991 work of chan and mathew @xcite .",
    "the related idea of computing a low - rank approximation or  skeleton \" of a matrix by means of randomized testing , albeit without a priori knowledge of the row and column spaces , was extensively studied in recent work of rokhlin et al .",
    "@xcite , and martinsson and tropp @xcite .",
    "the classical marmousi benchmark example is the basis of all our numerical experiments .",
    "the forward model is taken to be the linearized wave equation @xmath123 where the incident field @xmath124 obeys @xmath125 with @xmath126 .",
    "the wavelet @xmath127 is taken to be the second derivative of a gaussian ( ricker wavelet ) .",
    "the background medium @xmath17 is either taken to be constant ( in sections [ sec : numbasic ] , [ sec : numgen ] ) , or a smoothed version of the original marmousi model with various degrees of smoothing ( in section [ sec : numvariable ] ) .",
    "the data @xmath1 are then collected as the samples of @xmath11 at receiver positions @xmath3 and source positions @xmath4 at the surface @xmath128 , and all adequate times @xmath5 .",
    "the same equations are then used for the imaging , with @xmath17 and @xmath129 assumed known , but not @xmath130 .",
    "this is known as the  inversion crime \" , as any real - life imaging application would also require to solve for @xmath17 and @xmath129  problems that we leave aside in this paper .",
    "notice also that the forward model is _ linear _ in @xmath130 , a clearly uncalled - for assumption in practice since it neglects multiple scattering .",
    "a better wave equation for @xmath11 would have @xmath131 in place of @xmath132 in the right - hand - side .",
    "we nevertheless made this assumption so as not to obscure the fact that the hessian is intrinsically present to correct the solution of the linearized inverse problem .",
    "for the convenience of being able to run hundreds of simulations in a matter of hours , we choose to consider a 2d problem on a square domain with @xmath133 points , @xmath134 for most of the results shown . a perfectly matched layer ( pml ) of width @xmath135 surrounds the domain of interest .",
    "the numerical method has spectral differences in space , and second - order differences in time .",
    "the poststack imaging operator @xmath31 performs a stack on three sources maximally spaced from each other ( albeit not in the pml ) .",
    "more sources were used in some of the numerical experiments , but this did not significantly affect the inverse hessian . as is well - known , the main advantage of using more sources is the robustness to noise .",
    "( all the imaging results are robust to additive gaussian white noise , but not to purely multiplicative gaussian white noise . )",
    "two types of preconditioners are compared :    * * rn * : fitting of the inverse hessian from randomized curvelet trial functions .",
    "this preconditioner is denoted as rn where @xmath71 is the number of trial functions used for the fitting , e.g. r4 is four functions were used . * * kn * : fitting the inverse hessian from trial functions taken in the krylov subspace of the migrated image .",
    "this preconditioner is denoted kn where @xmath71 is the number of trial functions used for the fitting , e.g. k2 if both the migrated image and the remigrated image were used .",
    "this is essentially the method of nammour and symes @xcite , with the slight improvement of using the full expansion ( [ eq : compr2 ] ) in place of ( [ eq : compr1 ] )  a minor point .    in both cases",
    "the @xmath72 are the elementary symbols of equation ( [ eq : compr2 ] ) .",
    "different numbers of terms are tested in this pseudodifferential expansion : in order of decreasing importance , the parameters are 1 ) number of fourier modes in @xmath2 , 2 ) number of fourier modes in the wavevector argument @xmath136 , and 3 ) number of chebyshev modes in the wavenumber @xmath58 . the right balance of parameters in each dimension was obtained manually for best accuracy ; only their total number ( their product ) is reported .",
    "the action of the preconditioners on the migrated image @xmath137 is compared to the image obtained after 200 gradient descent steps for the ( linearized ) least - squares functional .",
    "the refinement of this brute force method to an iterative solver such as gmres or lsqr is important in practice , but was not investigated in the scope of this paper .",
    "errors between models are measured in the relative mean - squared sense , i.e. if @xmath138 is a reference model and @xmath139 another model , then @xmath140      the action of the preconditioners on the migrated image is satisfactory : as the figures below show it is visually closer to the image obtained after 200 gradient steps than the migrated image .",
    "the krylov preconditioner k1 usually works well on the migrated image .",
    "the randomized preconditioner r1 is often a notch worse than k1 , but when going up to r4 and higher the performance becomes very comparable to k1 .",
    "we did not find an instance where any rn , regardless of @xmath71 , would significantly outperform k1 ( a puzzling observation ) .",
    "however , we notice in figure [ fig : num4 ] that as the dimension of the krylov subspace increases , the performance of k2 , k3 , etc .",
    "deteriorates very quickly .",
    "this is in contrast to what was advocated in @xcite .",
    "there is a sweet spot in the number of parameters in the symbol expansion of the inverse hessian , around 500 to 1000 for the numerical scenario considered .",
    "see figure [ fig : num3 ] . if the number of parameters is too small , the inverse hessian is not properly represented .",
    "if the number of parameters is too large , they are either not used to improve the representation of the hessian , or their large number leads to ill - conditioning of the fitting problem ( hence large numerical errors . )    , left . by ",
    "slightly modified \" , we mean that a curvelet mask is taken to only measure the components of those images in the set @xmath111 ( see section [ sec : curvelettrial ] ) .",
    "right : preconditioned migrated image vs. recovered image of figure [ fig : num1 ] , right .",
    "no curvelet mask is taken here .",
    "any mse below 1 ( 100 percent relative error ) indicates that the preconditioning is working.,title=\"fig:\",width=302 ] , left . by ",
    "slightly modified \" , we mean that a curvelet mask is taken to only measure the components of those images in the set @xmath111 ( see section [ sec : curvelettrial ] ) .",
    "right : preconditioned migrated image vs. recovered image of figure [ fig : num1 ] , right .",
    "no curvelet mask is taken here .",
    "any mse below 1 ( 100 percent relative error ) indicates that the preconditioning is working.,title=\"fig:\",width=302 ]    note that in this experiment the hessian is a @xmath141-by-@xmath141 matrix .",
    "its numerical rank hovers in the few thousands ; more precisely , for a top singular value normalized to unity , the @xmath142-rank as a function of @xmath142 is given by the following table .",
    "we attribute the rank deficiency mostly to the perfectly matched layer ( pml ) and other windows applied .    [ cols=\"^,^\",options=\"header \" , ]      the rn preconditioners show their true potential when the inverse hessian is applied to another randomized trial function , drawn independently from those used for fitting the symbol , see figure [ fig : num4 ] , right .",
    "generalizability to a large linear subspace of models is as the theory predicts .",
    "the krylov preconditioners , on the other hand , show some fragility here .",
    "they are not designed to work when applied on images far from the remigrated image , and indeed , the error level is higher for k1 than for any rn .",
    "the degradation of the kn preconditioners as @xmath71 increases is understandable . in applying the normal operator @xmath143 times to the migrated image",
    ", information is lost in all but the eigenspaces corresponding to leading eigenvalues .",
    "this is well - known from the analysis of the power method in linear algebra . as a result , the disproportionate weight lent to those subspaces",
    " hijacks \" most of the degrees of freedom of the symbol expansion and prevents a good fit",
    ".    the robustness of the rn preconditioners offered by generalizability may be useful in the scope of preconditioned gradient descent iterations .",
    "while @xmath37 is applied to @xmath144 ( migrated image ) in the first iteration , it is subsequently applied to @xmath145 ( migrated residual ) .",
    "the latter will deviate from @xmath137 in the course of the iterations , resulting in a weaker k1 preconditioner .",
    "the curvelet mask used in the definition of the randomized trial functions is a set @xmath111 in curvelet space indicating whether the curvelet is  visible in the dataset \" or not . in the case of a uniform medium ,",
    "this information is obtained by considering the fan of couples of lines originating from each curvelet s center point , for which the angle of incidence equals the angle of reflection . for a given curvelet",
    "the test is whether one of the lines joins the curvelet to a source while the other line joins the curvelet to a receiver .",
    "if this test returns a positive match for one couple of lines , we declare that the curvelet is active and its index belongs to the set @xmath111 .    in the case of smooth variable media ,",
    "the test is similar but now involves ray tracing , i.e. , computing the trajectories of the hamiltonian system of geometrical optics .",
    "this is performed ray - by - ray using the high - order adaptive runge - kutta time integrator ode45 built in matlab .",
    "ray - tracing is normally not a computational bottleneck ; if solving for the rays one - by - one is too slow , a fast algorithm such as the phase - flow method of ying and cands @xcite can be set up to speed up the process .    for the numerical experiment",
    "we take the smooth part of the marmousi model @xmath146 and smooth it further by convolution with a radial bump .",
    "this operation is realized in the wavevector domain , by multiplying the fourier transform of @xmath146 by the indicator function of a disk of radius @xmath147 ( the whole wavevector space is a square of sidelength @xmath148 ) .",
    "we let @xmath149 and consider @xmath150 the further - smoothed marmousi background model velocity",
    ". then we set @xmath151 if @xmath152 we recover a uniform medium .",
    "the mse of the r5 preconditioner as a function of @xmath149 is shown below .",
    "most of the numerical tests performed in the earlier sections were repeated in variable media : we did not find that any particular plot was worth reporting , as the performance systematically degrades in a predictable manner as @xmath153 increases .",
    ", right ) .",
    "the x axis shows the number of parameters .",
    "the different curves refer to different smoothness levels of the model velocity , as explained in the text.,width=302 ]      other sizes , from @xmath154 to @xmath155 were tested and showed similar performance levels .",
    "other randomized trial functions than  curvelet - masked noise \" were attempted , such as    * gaussian white noise in model space , which failed badly because it contains too much energy in the nullspace , with high probability .",
    "* gaussian white noise in data space , migrated to model space .",
    "such trial functions still have too much energy in the nullspace and led to unequivocally poor results .",
    "* gaussian white noise in model space , to which the normal operator is applied .",
    "these trial functions work well , and show error levels comparable ( at times slightly worse ) than the curvelet trial functions .",
    "they have the advantage of being simple to define  no need for curvelets  but more complicated to compute as each randomized trial function requires one application of the expensive hessian .",
    "* gaussian white noise in model space , to which the normal operator is applied , followed by a diagonal operation in curvelet space where the coefficient magnitudes are either put to 1 or to zero if they are under a small threshold .",
    "coefficient phases are unchanged .",
    "these trial functions are comparable to the simpler ones defined directly in curvelet space .",
    "* other distributions than gaussian for the noise : this did not give rise to any noticeable difference in our numerical experiments .",
    "lemmas are indeed often available to pass from one distribution to the other in large deviation theory .",
    "the fitting of the inverse hessian was also realized from an application of the hessian to the desired unknown model that served to create the data .",
    "this operation can of course not be performed in practice since we are precisely trying to invert for this unknown model . but the numerical experiment is very instructive : it shows that the relative mse of the rn preconditioner applied to the migrated image decays to such small values as 0.1 when the number of parameters is large enough ; the mse does not stall on a plateau at 0.3 like it does in all the figures above .",
    "this goes to show that the pseudodifferential expansion is instrinsically good , but that neither the krylov fit nor the randomized fit is fine enough to predict the right coefficients .",
    "this leaves exciting room for improvement of the method .",
    "to carry out the least squares minimization in section [ sec : randfit ] , the @xmath71 by @xmath71 matrix @xmath39 has to be well - conditioned . in this section",
    ", we will show that this happens with high probability ( whp ) when the number of parameters @xmath89 is related to the ( numerical ) rank @xmath3 of @xmath33 through @xmath156 if @xmath33 were an invertible matrix , we would simply let @xmath157 , independent and identically distributed ( iid ) .",
    "but in the general case , and as mentioned earlier , we should make sure that @xmath104 is properly  colored \" to avoid the nullspace of @xmath33 .",
    "while our numerical solution to this problem is approximate , we will assume for simplicity that we can exactly project @xmath104 onto the range space of @xmath33 , @xmath158 where @xmath43 is the orthogonal projector onto ran@xmath159 .",
    "the random matrix @xmath39 to invert for the fitting step is then @xmath160 it holds that @xmath161 without the tildes , hence @xmath162 it is assumed that @xmath163 is positive definite and well - conditioned ; our argument consists in showing that @xmath39 does not depart too much from its expectation whp .",
    "let @xmath164 and @xmath165 denote the spectral and frobenius norms respectively .",
    "we denote by @xmath166 the condition number of @xmath163 , @xmath167 we also need to consider @xmath168 , the smallest number such that @xmath169 uniformly over @xmath74 .",
    "we may call @xmath170 the  weak condition number \" of the collection of @xmath171 .    both @xmath166 and @xmath170 are greater than 1 , but it will be manifest from the way they enter the estimates below that they ought to be small ( close to 1 ) . if @xmath170 is small , then @xmath172 has approximate numerical rank @xmath3 , i.e. , the largest @xmath3 singular values are comparable in size .    the following result is a perturbative analysis quantifying the size of @xmath173 in relation to @xmath174 .    [",
    "thm : psquare ] assume that @xmath33 is a symmetric rank-@xmath3 matrix that can be written as @xmath175 .",
    "define @xmath99 and @xmath170 as above .",
    "for all @xmath176 , there exists a number @xmath177 such that , if @xmath178 then with high probability @xmath179 explicitly , @xmath180 , and the  high probability \" is at least @xmath181 .",
    "before we prove this theorem , let us explain how invertibility of @xmath39 follows at once .",
    "since the condition number of @xmath163 is @xmath166 , its minimum eigenvalue obeys @xmath182 when a matrix is perturbed , the change in eigenvalues is controlled by the spectral norm of the perturbation , so @xmath183 it suffices therefore to apply the theorem above with @xmath184 to ensure invertibility of @xmath39 .",
    "let us first settle that @xmath161 , without the tildes .",
    "it suffices to argue that @xmath185 . by transposition , and symmetry of both @xmath33 and @xmath43",
    ", it suffices to show that @xmath186 .",
    "this latter equation is obviously true since @xmath43 acts as the identity on the range space of @xmath33 .",
    "now let @xmath187 .",
    "our proof considers the statistics of @xmath99 element - wise as a quadratic form of the gaussian random vector @xmath90 .",
    "we will show that @xmath99 is highly unlikely to be more than @xmath188 away from @xmath97 . in what follows we use the @xmath189 and",
    "@xmath190 induced matrix norms  the maximum absolute column and row sums respectively .",
    "if we can show that @xmath191 for all @xmath192 , then the following inequality completes the proof : @xmath193    the statistics of quadratic forms @xmath194 were perhaps first completely studied by grenander , pollak and slepian @xcite . in a nutshell ,",
    "the variance of the quadratic form @xmath195 is known to be proportional to @xmath196 .",
    "we seek to bound these variances using the fact that the @xmath172 are  weakly well - conditioned \" .",
    "fix @xmath74 .",
    "we know that @xmath197    using the definition of @xmath170 we obtain a stronger bound on the spectral norm , namely @xmath198 the implication is that for all @xmath192 , @xmath199 as for the frobenius norm of @xmath200 , we make use of the fact that @xmath33 has rank @xmath3 to bound @xmath201    we are now ready to bound @xmath202 . for clarity , fix @xmath192 and let @xmath203 . the standard deviation of @xmath204 is proportional to @xmath205 , which by eq . ( [ eq : varf ] ) is roughly on the order of @xmath206 or @xmath207 .",
    "this is qualitatively correct .",
    "for an explicit bound , we refer to bechar @xcite , who builds on the work of @xcite to state the following .",
    "[ lemma : bechar ] let @xmath208 and @xmath209 iid . then for any @xmath210 ,",
    "@xmath211    we pick @xmath212 .",
    "it is straightforward to verify that with this choice of @xmath213 , with the definition of @xmath214 , and with equations ( [ eq : var2 ] ) and ( [ eq : varf ] ) , we have @xmath215 it follows that @xmath216    an union bound over @xmath217 pairs of @xmath192 s concludes the proof .",
    "note in passing that we made no effort to minimize @xmath214 .    finally , we sketch a standard procedure to handle complex - valued matrices",
    ". instead of taking the symmetric part of @xmath204 by @xmath218 , decompose it into hermitian and anti - hermitian components , that is @xmath219 where @xmath220 and @xmath221 are both hermitian . then bound the deviations from their expectations separately by @xmath222 .",
    "repeat similar arguments and invoke lemma [ lemma : bechar ] to show that each term is less than @xmath223 whp .      the success of the proposed method for inverting the hessian depends on the property of phase - space localization of curvelets .",
    "good localization of a basis function like a curvelet near a point @xmath50 implies that it will only  see \" values of the symbol @xmath49 near that point , when acted upon by the hessian .",
    "the following result makes this heuristic precise ; it is a minor modification of a theorem of stolk @xcite so the proof is omitted .",
    "( stolk , 2008 ) .",
    "let @xmath49 be the pseudodifferential symbol of the wave equation hessian @xmath33 , as in equation ( [ eq : symbol ] ) , and assume that it obeys ( [ eq : type10 ] ) with @xmath224 .",
    "consider the zeroth - order symbol @xmath225 of the operator @xmath226 .",
    "denote by @xmath227 the diagonal approximation of @xmath226 in curvelet space , with the sampled symbol as multiplier , @xmath228 if @xmath109 obeys @xmath229 for @xmath230 , then there exists @xmath231 such that @xmath232    in other words , the more oscillatory the model @xmath233 the better the diagonal approximation of the hessian via curvelets .",
    "hence the larger @xmath36 the better the  probing \" character of a curvelet near its center in phase - space .",
    "the theorem above is also true for another frame of functions , the wave atoms of demanet and ying @xcite , but would not be true for wavelets , directional wavelets , gabor functions , or ridgelets .",
    "this paper presents a preconditioner for the wave equation hessian based on ideas of randomized testing , pseudodifferential symbols , and phase - space localization .",
    "numerical experiments show that the proposed solution belongs to a class of effective  probing \" preconditioners .",
    "the precomputation only requires applying the wave equation hessian once , or a small number of times .    fitting the inverse hessian involves solving a small least - squares problem , of size @xmath89-by-@xmath89 , where @xmath89 is much smaller than @xmath71 and",
    "the hessian is @xmath71-by-@xmath71 .",
    "even if @xmath89 were on the order of @xmath71 the proposed method would be very advantageous since constructing each row of the hessian requires going back to the much higher dimensional data space .",
    "it is anticipated that the techniques developed in this paper will be of particular interest in 3d seismic imaging and with more sophisticated physical models that require identifying a few different parameters ( elastic moduli , density ) . in that setting , properly inverting the hessian with low complexity algorithms to unscramble the multiple parameters will be particularly desirable .",
    "t. f. chan and d. e. keyes interface preconditioning for domain - decomposed convection - diffusion operators , in _ third international symposium on domain decomposition methods for partial differential equations _ ,",
    "siam , philadelphia , pa , 1990 .",
    "t. f. chan and t. p. mathew , an application of the probing technique to the vertex space method in domain decomposition , in _ fourth international symposium on domain decomposition methods for partial differential equations _ , siam , philadelphia , pa , 1991 , pp .",
    "101 - 111 ."
  ],
  "abstract_text": [
    "<S> this paper considers the problem of approximating the inverse of the wave - equation hessian , also called normal operator , in seismology and other types of wave - based imaging . </S>",
    "<S> an expansion scheme for the pseudodifferential symbol of the inverse hessian is set up . </S>",
    "<S> the coefficients in this expansion are found via least - squares fitting from a certain number of applications of the normal operator on adequate randomized trial functions built in curvelet space . </S>",
    "<S> it is found that the number of parameters that can be fitted increases with the amount of information present in the trial functions , with high probability . </S>",
    "<S> once an approximate inverse hessian is available , application to an image of the model can be done in very low complexity . </S>",
    "<S> numerical experiments show that randomized operator fitting offers a compelling preconditioner for the linearized seismic inversion problem .    </S>",
    "<S> * acknowledgments*. ld would like to thank rami nammour and william symes for introducing him to their work . </S>",
    "<S> ld , pdl , and nb are supported by a grant from total sa . </S>"
  ]
}