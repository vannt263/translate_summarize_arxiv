{
  "article_text": [
    "a fundamental limitation of the common machine learning methods is the cost incurred by the preparation of the large training samples required for good generalization .",
    "multi - task learning ( mtl ) offers a potential remedy .",
    "unlike common single task learning , mtl accomplishes tasks simultaneously with other related tasks , using a shared representation .",
    "one general assumption of multi - task learning is that all tasks should share some common structures , including a similarity metric matrix @xcite , a low rank subspace @xcite , parameters of bayesian models @xcite or a common set of features @xcite .",
    "improved generalization is achieved because what is learned from each task can help with the learning of other tasks @xcite .",
    "mtl has been successfully applied to many applications such as stock selection @xcite , speech classification @xcite and medical diagnoses @xcite .",
    "while the majority of existing multi - task feature learning algorithms assume that the relevant features are shared by all tasks , some studies have begun to consider a more general case where features can be commonly shared only among most , but not necessarily all of them . in other word",
    ", they try to learn the features specific to each task as well as the common features shared among tasks @xcite .",
    "in addition , mtl is commonly formulated as a convex regularization problem . thus the resultant models are restrictive and suboptimal . in order to remedy the above two shortcomings , a specific non - convex formulation based on the capped-@xmath0 regularized formulation for multi - task sparse feature learning ,",
    "was proposed in @xcite .",
    "then a corresponding multi - stage multi - task feature learning ( msmtfl ) algorithm is presented . this non - convex model and",
    "corresponding algorithm usually achieves a better solution than the corresponding convex models .",
    "however , like many non - convex algorithms , it may not get a globally optimal solution which is often computationally prohibitive to obtain in practice .",
    "notice that the above capped-@xmath0 regularized formulation employs a prescribed fixed threshold value in its definition , and the msmtfl employs the same fixed threshold value .",
    "this threshold value is used to determine the large rows of the unknown weight matrix . however , the result is difficult to prescribe beforehand , because this value is data - dependent .",
    "thus a natural idea is to adaptively determine it in practice , in order to achieve even better performance . in this paper",
    ", we propose to incorporate an adaptive threshold estimation scheme into the msmtfl .",
    "in particular , we consider the structure information of the intermediate estimated solution of the current stage and make use of this information to estimate the threshold value , which will be used in the next stage .",
    "this way , the threshold value and the resultant solution will be updated in an alternative way . as for the adopted threshold value estimation scheme ,",
    "we are motivated by the iterative support detection method first proposed by wang et al @xcite which employed the  first significant jump \" heuristic rule to estimate the adaptive threshold value .",
    "we leverage the rule to the msmtfl in order to solve the non - convex feature learning formulation with the adaptive threshold , though other methods of determining the adaptive threshold could be applied . with the help of the adaptive threshold value , rather than a prescribed fixed value ,",
    "an even better feature learning result is expected to be obtained for both synthetic and real data tests .",
    "* organization * : first , we will review the original msmtfl algorithm in section @xmath1 . in section @xmath2 , we will present the method to adaptively set the threshold in msmtfl . in section @xmath3 ,",
    "several numerical experiments verify the effectiveness of our new algorithm . in the conclusion",
    "we will describe future work .",
    "* notation * : scalars and vectors are denoted by lower case letters .",
    "matrices and sets are denoted by capital letters . the euclidean norm , @xmath4 norm , @xmath5 norm , @xmath6 norm , @xmath7 norm and frobenius norm are denoted by @xmath8,@xmath9 , @xmath10 , @xmath11 , @xmath12 and @xmath13 , respectively . @xmath14 and @xmath15 denote the @xmath16-th column and @xmath17-th row of matrix @xmath18 , respectively .",
    "@xmath19 denotes the absolute value of a scalar or the number of elements in a set",
    ". then we define @xmath20 as the set @xmath21 and @xmath22 as a gaussian distribution with mean @xmath23 and standard deviation @xmath24 .",
    "in this section , we first give a brief introduction to multi - task feature learning ( mtfl ) exploiting the shared features among tasks .",
    "then we review the capped-@xmath0 regularized feature learning model and corresponding msmtfl algorithm proposed in @xcite .",
    "consider @xmath25 learning tasks associated with the given training data @xmath26,@xmath27 , @xmath28 , where @xmath29 is the data matrix of the @xmath30th task with each row being a sample , @xmath31 is the response of the @xmath30th task , and @xmath32 is the number of samples for the @xmath30th task .",
    "each column of @xmath33 represents a feature and @xmath34 is the number of features . in the scenario of mtl",
    ", it is assumed that the different tasks share the same set of features , i.e. the columns of different @xmath33 represent the same features .",
    "the purpose of feature selection is to learn a weight matrix @xmath35\\in r^{d\\times m}$ ] , consisting of the weight vectors for @xmath25 linear predictors : @xmath36 , @xmath37 .",
    "the quality of prediction is measured through a loss function @xmath38 and we assume that @xmath38 is convex in @xmath18 throughout the paper . for each @xmath39 , the magnitude of the components of @xmath40 reflects the importance of the corresponding column ( feature ) of @xmath39 . in mtl ,",
    "the important features among different tasks are assumed to be almost the same and should be used when establishing an mtl model .",
    "we consider mtl in the scenario that @xmath41 , i.e there are many more features than tasks , because this situation commonly arises in many applications . in such cases ,",
    "an unconstrained empirical risk minimization is inadequate to obtain a reliable solution because overfitting of the training data is likely to occur . a standard remedy for this problem is to impose a constraint on @xmath18 to obtain a regularized feature learning model .",
    "this constraint or regularization reflects our prior knowledge of the desired solution .",
    "for example , we have learned that one important aspect of @xmath18 is sparsity , because only a small portion of the features are really relevant to the given tasks .",
    "it is well known that the @xmath42 regularization is a natural choice for the sparsity regularization .",
    "if the sparsity parameter @xmath43 for the target vector is known , a feature learning model based on @xmath42 regularization is as follows : @xmath44 where @xmath38 is usually the quadratic loss function of @xmath18 , i.e. @xmath45 however , the sparsity parameter @xmath43 is often unknown in practice and the following unconstrained formulation can be considered instead .",
    "@xmath46 where @xmath47 is the sparsity regularization term and @xmath48 is a parameter balancing the empirical loss and the regularization term .",
    "however , either ( [ eq:1 ] ) or ( [ eq:2 ] ) seems to consider the sparsity of @xmath40 individually and fails to make use of the fact that different tasks share almost the same relevant features , i.e. the solution of mtl has a joint sparsity structure .",
    "in addition , another fundamental difficulty for this model is the prohibitive computational burden since the @xmath42 norm minimization is an np - hard problem .    in order to make use of the joint sparsity structure of the solution of mtl",
    ", ones often turn to the mixed @xmath49 norm based joint sparsity regularization @xcite .",
    "@xmath50 where @xmath51 is the @xmath17-th row of @xmath18 , @xmath52 and @xmath53 .",
    "one common choice of @xmath54 is that @xmath55 and @xmath56 @xcite .",
    "it assumes that all these tasks share exactly the same set of the features , i.e. the relevant features to be shared by all tasks .",
    "however , it is too restrictive in real - world applications to require the relevant features to be shared by all tasks . in order for a certain feature to be shared by some but not all tasks ,",
    "many efforts have been made .",
    "@xcite proposed to use @xmath57 + @xmath58 regularized formulation to leverage the common features shared among tasks .",
    "however , @xmath57 + @xmath58 regularizer is a convex relaxation of an @xmath59-type one , for example , @xmath60 .",
    "the convex regularizer is known to be too loose to approximate the @xmath59-type one and often achieves suboptimal performance .",
    "with @xmath61 , the regularization @xmath62 is non - convex and expects to achieve a better solution theoretically .",
    "however , it is difficult to solve in practice .",
    "moreover , the solution of the non - convex formulation heavily depends on the specific optimization algorithms employed and may result in different solutions .    in @xcite , a non - convex formulation , based on a capped-@xmath63 regularized model for multi - task feature learning",
    "is proposed , and a corresponding multi - stage multi - task feature learning ( msmtfl ) algorithm is also given .",
    "the capped-@xmath63 regularization is defined as @xmath64 .",
    "this proposed model aims to simultaneously learn the features specific to each task and the common features shared among tasks .",
    "the advantage of msmtfl is that the convergence , reproducibility analysis , and theoretical analysis for better performance over the convex models are given .",
    "in addition , the capped-@xmath63 regularization is a good approximation to @xmath7-type norm because as @xmath65 , @xmath66 , which is mostly preferred for its theoretically optimal sparsity enforcement .    specifically , the model for multi - task feature learning with the capped-@xmath63 regularization is as follows : @xmath67 where @xmath68 is a thresholding parameter , which distinguishes nonzeros and zero components ; @xmath15 is the @xmath17-th row of the matrix @xmath18 . obviously , due to the capped-@xmath0 penalty",
    ", the optimal solution of problem ( 3 ) denoted as @xmath69 has many zero rows . due to the @xmath4 penalty on each row of @xmath18",
    ", some entries of the nonzero row may be zero .",
    "therefore , a certain feature can be shared by some but not necessarily all the tasks under the formulation ( 3 ) .",
    "the multi - stage multi - task feature learning ( msmtfl ) algorithm ( see algorithm @xmath70 ) based on the work @xcite is proposed in @xcite to solve problem ( 3 ) .",
    "note that for the first step of msmtfl ( @xmath71 ) , the msmtfl algorithm is equivalent to solving the @xmath72 regularized multi - task feature learning model ( lasso for mtl ) .",
    "therefore , the final solution of the msmtfl algorithm can be considered as a refinement of lasso s for mtl .",
    "although msmtfl may not find a globally optimal solution , @xcite has theoretically shown that the solution obtained by algorithm 1 improved the performance of the parameter estimation error bound as the multi - stage iteration proceeds , under certain circumstances .",
    "more details about intuitive interpretations , parameter settings and convergence analysis of the msmtfl algorithm are provided in @xcite .",
    "in addition , we need to point out that the key thresholding parameter @xmath73 is bounded below , that is @xmath74 , where @xmath75 is a constant .",
    "an exact solution could be obtained from the algorithm 1 with an appropriate thresholding parameter . +   +    [",
    "cols=\"<\",options=\"header \" , ]      + as shown in @xcite , the msmtfl achieves a better estimation error bound than the convex alternative such as the @xmath72 regularized model , though in general it only leads to a local critical point for the non - convex problem ( [ eq : nonconvex ] ) .",
    "this enhancement is achieved because this local critical point is a refinement based on the solution of the initial convex model , which is the @xmath72 regularized model .",
    "furthermore , our proposed msmtfl - at is expected to achieve an even better estimation accuracy than the original msmtfl because of the incorporated adaptive threshold estimation scheme .",
    "here we only present an intuitive explanation , which is partially borrowed from the idea of the analysis of the iterative reweighted @xmath76 algorithm for @xmath77 ( @xmath78 ) based non - convex compressive sensing in @xcite .",
    "we extended it from the common sparsity regularization to joint sparsity regularized considered in this paper .",
    "note that the non - convex formulations ( [ eq : nonconvex ] ) or ( [ eq : at ] ) based on capped-@xmath0 regularization can be a good approximation to @xmath42 regularization if @xmath73 or @xmath79 is small , because @xmath80}$ ] , @xmath81 is close to @xmath82 , where @xmath83 } , t_{[2 ] } ,   \\ldots , t_{[d]}]$ ] .",
    "indeed , in practice , the @xmath84 of algorithm 2 usually decreases gradually as the iteration proceeds , though not necessarily always monotonically , due to the inherent estimation errors .",
    "based on this observation , an intuitive explanation why msmtfl - at could achieve a better performance than msmtfl , is that an adaptive but relatively large @xmath84 at the beginning of msmtfl - at results in undesirable local minima being  filled in \" .",
    "once @xmath85 is in the correct basin , decreasing @xmath84 allows the basin to deepen and @xmath85 is expected to approach the true @xmath18 more closely .",
    "we expect to turn these notions into a rigorous proof in the future work .",
    "in addition , suppose that @xmath86 exists and we denote it as @xmath87 , then we intuitively expect that @xmath85 of msmtfl - at converges to a critical point of the problem ( [ eq : at ] ) with @xmath79 being @xmath87 .",
    "the rigorous proof will also constitute an important future research topic .",
    "the reproducibility of the msmtfl - at algorithm can be guaranteed , as can the original msmtfl .",
    "theorem 2 in @xcite proves that the optimization problem 2(a ) in msmtfl ( algorithm 1 ) has a unique solution if @xmath88 has entries drawn from a continuous probability distribution on @xmath89 . considering that problem 2(a ) in msmtfl - at ( algorithm 2 ) has the same formula as the one in msmtfl ,",
    "the solution @xmath85 of the msmtfl - at is also unique , for any @xmath90 .",
    "the main difference between msmtfl - at and msmtfl is step 2(b ) , which has a unique output .",
    "therefore the solution generated by algorithm 2 is also unique , since steps 2(a ) , 2(b ) and 2(c ) all return unique results .",
    "while there could be many different rules for @xmath84 , our choice is based on locating the  first significant jump \" in the increasingly sorted sequence @xmath91}^{(\\ell)}|$ ] .",
    "for simplicity , after sorting , we still use the same notation , where @xmath91}^{(\\ell)}|$ ] denotes the @xmath17-th smallest among all @xmath92}^{(\\ell)}\\}_{j=1}^d$ ] by magnitude , as used in @xcite . denote @xmath93}^{(\\ell ) } , \\hat{t}_{[2]}^{(\\ell ) } ,   \\ldots , \\hat{t}_{[d]}^{(\\ell)}]$ ] .",
    "this rule looks for the smallest @xmath17 such that @xmath94}^{(\\ell)}|- |\\hat{t}_{[j]}^{(\\ell)}|>\\tau^{(\\ell)}.\\ ] ] this amounts to sweeping the increasing sequence @xmath91}^{(\\ell+1)}|$ ] and looking for the first jump larger than @xmath95",
    ". then we set @xmath84= @xmath91}^{(\\ell)}|$ ] .",
    "this rule has proved capable of detecting many true nonzeros with few false alarms if the sequence @xmath91}^{(\\ell)}|$ ] has the fast - decaying property @xcite .",
    "in addition , several simple and heuristic methods have been adopted to define @xmath95 for different kinds of data matrix , as suggested in @xcite . in this paper , we adopt the following method for following both synthetic and real data experiments , though other heuristic formulas could be proposed and tried .",
    "@xmath96 where @xmath97 is the number of all samples .",
    "an excessively large @xmath95 results in penalizing too many true nonzeros , while an excessively small @xmath95 results in ignoring too many false nonzeros and leads to low quality of solution .",
    "msmtfl - at will be quite effective with an appropriate @xmath95 , though the proper range of @xmath95 might be case - dependent @xcite , due to the no free lunch theorem for the nonconvex optimization or learning @xcite .",
    "however , numerical experiments have shown that the practical performance of msmtfl - at is less sensitive to the choice of @xmath95 .",
    "in addition , we need to point out that the tuning parameter @xmath98 is a key parameter , which typically decreases from a large value to a small value to detect more correct nonzeros from the gradually improved intermediate learning results as the iteration proceeds ( as @xmath99 increases ) .    intuitively , the ",
    "first significant jump \" rule works well partly because the true nonzeros of @xmath100 are large in magnitude and small in number , while the false ones are large in number and small in magnitude .",
    "therefore , the magnitudes of the true nonzeros are spread out , while those of the false ones are clustered .",
    "the phenomenon of  first significant jump \" was first observed in compressive sensing @xcite and has been observed in other related sparse pattern recognition problems @xcite . while it is only a heuristic rule , it is still of practical importance , because a theoretically rigorous way to set the threshold value while keeping control of the false detections is still a challenging task in statistics @xcite .",
    "we present a demo to show the effectiveness of the  first significant jump \" rule in multi - stage multi - task feature learning in figure 1 .",
    "we generate a sparse weight matrix @xmath101 and randomly set @xmath102 rows of it as zero vectors and @xmath103 elements of the remaining nonzero entries as zeros .",
    "we set @xmath104 and generate a data matrix @xmath105 from the gaussian distribution @xmath106 .",
    "the noise @xmath107 is sampled i.i.d from the gaussian distribution @xmath108 with @xmath109 .",
    "the responses are computed as @xmath110 .",
    "the parameter estimation error is defined as @xmath111 . in figure 1 , @xmath112 ( a column vector )",
    "corresponds to the true weight matrix @xmath113 and @xmath100 corresponds to the learning weight matrix @xmath114 in the @xmath99-th iteration .",
    "subgraphs ( a)-(d ) plot @xmath115 , @xmath116 , @xmath117 , @xmath118 , respectively , in comparison with @xmath112 . from subgraph ( a )",
    ", it is clear that @xmath115 , which represents the solution of a lasso - like model , contains a large number of false nonzeros and has a large recovery error , as expected .",
    "as iteration proceeds , the intermediate learning result becomes more accurate .",
    "for example , @xmath116 has a smaller error , and we can see that most of true nonzeros with large magnitude have been correctly identified . then , @xmath117 well matches the true learning matrix @xmath112 even better , with only a tiny number of false nonzero components .",
    "finally , @xmath118 exactly have the same nonzero components as the true @xmath112 , and the error is quite small .",
    "in short , figure 1 shows that our proposed algorithm is insensitive to a small false number in @xmath119 and has an attractive self - correction capacity .",
    "in this section , we demonstrate the better performance of the proposed msmtfl - at in terms of smaller recovery errors .",
    "we compare the msmtfl - at with a few competing multi - task feature learning algorithms : the msmtfl , @xmath4-norm multi - task feature learning algorithm ( lasso ) , @xmath120-norm multi - task feature learning algorithm ( l2,1 ) , an efficient @xmath120-norm multi - task feature learning algorithm ( efficient - l2,1 ) @xcite and a robust multi - task feature learning algorithm ( rmtfl ) @xcite .",
    "as above , we denote the number of tasks as @xmath25 and each task has @xmath121 samples .",
    "the number of features is denoted as @xmath34 .",
    "each element of the data matrix @xmath122 for the @xmath16-th task is sampled i.i.d . from the gaussian distribution @xmath106 and each entry of the true weight @xmath123",
    "is sampled i.i.d . from the uniform distribution defined in the interval [ -10 , 10 ] . here",
    "we randomly set @xmath102 rows of @xmath113 as zero vectors and @xmath103 elements of the remaining nonzero entries as zeros .",
    "each entry of the noise @xmath124 is sampled i.i.d . from the gaussian distribution @xmath125 .",
    "the responses are computed as @xmath126 .",
    "the quality of the recovered weight matrix is measured by the averaged parameter estimation error @xmath127 , which has a theoretical bound referring to @xcite .",
    "we present the averaged parameter estimation error @xmath127 vs. stage @xmath99 in comparison with the msmtfl and the msmtfl - at in figure 2 .",
    "it is clear that the parameter estimation errors of all tested algorithms decrease as the stage number @xmath99 increases , which shows the advantage of msmtfl and msmtfl - at over the plain lasso - like model ( @xmath71 ) .",
    "it is worth noting that the msmtfl - at is always superior to the msmtfl under the same settings .",
    "moreover , the parameter estimation error of our memtfl - at decreases more rapidly and is more stable in most stages .    in order to show that the performance of msmtfl - at is insensitive to the settled value of the parameter @xmath128 to certain degree",
    ", we depict the averaged parameter estimation error @xmath127 corresponding to different @xmath128 values . the default value of @xmath128 is based on the heuristic formula ( [ eq : tau ] ) and we also tried other values such as several times the default value .",
    "the resultant recovery errors are listed in table 1 .",
    "we can see that , with different @xmath128 values , our msmtfl - at achieves almost the same recovery accuracy and demonstrates the insensitiveness to the choice of the @xmath128 value .",
    "thus , in following numerical experiments we just select @xmath128 based on the heuristic formula ( [ eq : tau ] ) .    in figure 3 , we present the averaged parameter estimation error @xmath127 vs. lambda @xmath129 in comparison with the msmtfl with different @xmath73 ( @xmath130 , @xmath131 , @xmath132 and @xmath133 ) , the msmtfl - at , the @xmath4-norm multi - task feature learning algorithm ( lasso ) and the @xmath120-norm multi - task feature learning algorithm ( l2,1 )",
    ". the settings of the parameters of these involved alternative algorithms follow the same settings in their original papers .",
    "we compared the averaged parameter estimation errors of all the tested algorithms .",
    "as expected , the error of our msmtfl - at was the smallest among them .",
    "specifically , our proposed msmtfl - at significantly outperformed the plain msmtfl algorithm with the same settings suggested in @xcite .    in order to further demonstrate the advantage of the msmtfl - at , we compared it with an efficient @xmath120-norm multi - task feature learning algorithm ( efficient - l2,1 ) proposed in @xcite and a robust multi - task feature learning algorithm ( rmtfl ) proposed in @xcite .",
    "the efficient - l2,1 solves the @xmath120-norm regularized regression model via nesterov s method .",
    "the rmtfl employs the accelerated gradient descent to efficiently solve the corresponding multi - task learning problem , and has shown the scalability to large - size problems .",
    "the results are plotted in figure 4 .",
    "we see that our msmtfl - at outperforms these two alternative algorithms in terms of achieving the smallest recovery error .",
    "in short , these empirical results demonstrate the effectiveness of our proposed msmtfl - at . in particular , we have observed that ( a ) when noise level ( @xmath134 ) is relatively large , the msmtfl - at also outperforms other tested algorithms and shows its robustness to the noise .",
    "( b ) when @xmath129 exceeds to a certain degree , the sparsity regularization weighs too much and the solutions @xmath114 obtained by the involved algorithms will be too sparse .",
    "in such cases , the errors of all tested algorithms increase . therefore , proper choice of @xmath129 value is also very important .",
    "we conduct a typical real - world data set , i.e. the isolte data set ( this data can be found at www.zjucadcg.cn/dengcai/data/data.html ) in this experiment .",
    "we aim to demonstrate the high efficiency of our algorithm in practical problems .",
    "this isolet data set is collected from @xmath135 speakers who speak the name of each english letter of the alphabet twice .",
    "hence , we have @xmath136 training samples from each speaker .",
    "the speakers are grouped into @xmath137 subsets , each of which has @xmath138 similar speakers .",
    "these subsets are referred to as isolet1 , isolet2 , isolet3 , isolet4 , and isolet5 , respectively .",
    "thus , there are @xmath137 tasks and each task corresponds to a subset .",
    "the @xmath137 tasks have @xmath139 , @xmath139 , @xmath139 , @xmath140 and @xmath141 samples , respectively ( three samples historically are missing ) , where each sample has @xmath142 features and the response is the english letter label ( 1 - 26 ) .    for our experiments , the letter labels are treated as the regression values for the isolet sets .",
    "we randomly extract the training samples from each task with different training ratios ( @xmath143 , @xmath144 and @xmath145 ) and use the rest of the samples to form the test set .",
    "we evaluate four multi - task feature learning algorithms according to normalized mean squared error ( nmse ) and averaged means squared error ( amse ) , whose definitions are as follows : @xmath146 @xmath147 where @xmath148 is the predictive value from tested algorithms and @xmath149 is the referent true value from the whole isolet data set .",
    "both nmse and amse are commonly used in multi - task learning problems @xcite .",
    "the experimental results are shown in figure 5 .",
    "we can see that our proposed msmtfl - at is superior to the efficient - l2,1 , rmtfl and msmtfl algorithms in terms of achieving the smallest nmse and amse .",
    "the msmtfl - at performs especially well even in the case of a small training ratio .",
    "all of the experimental results above suggest that the proposed msmtfl - at algorithm is a promising approach .",
    "this paper proposes a non - convex multi - task feature learning formulation with an adaptive threshold parameter and introduces a corresponding msmtfl - at algorithm .",
    "the msmtfl - at is a combination of an adaptive threshold learning via  first significant jump \" rule proposed in @xcite with the msmtfl algorithm proposed in @xcite .",
    "the intuition is to refine the estimated threshold by using intermediate solutions obtained from the recent stage .",
    "this alternative procedure between threshold value estimation and feature learning , leads to a gradually appropriate threshold value , and a gradually improved solution .",
    "the experimental results on both synthetic data and real - world data demonstrate the effectiveness of our proposed msmtfl - at in comparison with several state - of - the - art multi - task feature learning algorithms . in the future",
    ", we will give a theoretical analysis about the convergence of msmtfl - at .",
    "in addition , we expect to perform a rigorous analysis about why msmtfl - at could achieve a better practical performance than the original msmtfl .",
    "this work was supported by the national key basic research program of china ( no .",
    "2015cb856000 ) , the natural science foundation of china ( nos . 11201054 , 91330201 ) and by the fundamental research funds for the central universities ( no .",
    "zygx2013z005 ) .",
    "we would also like to thank the anonymous reviewers for their many constructive suggestions , which have greatly improved this paper .    00",
    "p. gong , j. ye and c.zhang .",
    ": multi - stage multi - task feature learning . the journal of machine learning research archive .",
    "* 14 * , 2979 - 3010(2013 ) .",
    "y. wang and w. yin . : sparse signal reconstruction via iteration support detection .",
    "siam journal on imaging sciences .",
    "* 3 * , 462 - 491(2010 ) .",
    "j. ghosn and y. bengio .",
    ": multi - task learning for stock selection . advances in neural information processing systems(1997 ) .",
    "j. chen , j. liu , and j. ye .",
    ": learning incoherent sparse and low - rank patterns from multiple tasks .",
    "acm transactions on knowledge discovery from data . * 5 * , 1 - 31(2012 ) .",
    "s. negahban and m.wainwright .",
    ": estimation of ( near ) low - rank matrices with noise and high - dimensional scaling .",
    "the annals of statistics . * 39 * , 1069 - 1097(2011 ) .",
    "k. yu , v. tresp , and a. schwaighofer .",
    ": learning gaussian processes from multiple tasks .",
    "icml 05 proceedings of the 22nd international conference on machine learning . 1012 - 1019(2005 ) .",
    "a. argyriou , t. evgeniou , and m. pontil .",
    ": convex multi - task feature learning .",
    "machine learning . * 73 * , 243 - 272(2008 ) .",
    "s. kim and e. xing .",
    ": tree - guided group lasso for multi - response regression with structured sparsity , with an application to eqtl mapping .",
    "the annals of applied statistics . * 6 * , 1095 - 1117(2012 ) .",
    "k. lounici , m. pontil , a. tsybakov , and s. van de geer . : taking advantage of sparsity in multi - task learning . in colt.73 - 82(2009 ) .",
    "p. gong , j. zhou , w. fan and j. ye .",
    ": efficient multi - task feature learning with calibration .",
    "kdd 14 proceedings of the 20th acm sigkdd international conference on knowledge discovery and data mining .",
    "761 - 770(2014 )",
    ". s. parameswaran and k. weinberger . : large margin multi - task metric learning . in nips .",
    "1867 - 1875(2010 ) .",
    "j. bi , t. xiong , s. yu , m. dundar , and r. rao . : an improved multi - task learning approach with applications in medical diagnosis .",
    "machine learning and knowledge discovery in databases lecture notes in computer science . *",
    "5211 * , 117 - 132(2008 ) .",
    "kowalski , matthieu .",
    ": sparse regression using mixed norms . applied and computational harmonic analysis . *",
    "27 * , 303 - 324(2009 ) .",
    "m. yuan , y. lin .",
    ": model selection and estimation in regression with grouped variables .",
    "journal of the royal statistical society serie b. * 68 * , 49 - 67(2006 ) .",
    "jalali , a. , ravikumar , p. , sanghavi , s. : a dirty model for multiple sparse regression .",
    "ieee transactions on information theory .",
    "* 59 * , 7947 - 7968(2013 ) .",
    "t. zhang . :",
    "analysis of multi - stage convex relaxation for sparse regularization .",
    "jmlr . * 11 * , 1081 - 1107(2010 ) .",
    "t. zhang . :",
    "multi - stage convex relaxation for feature selection .",
    "* 19 * , 2277 - 2293(2013 ) .",
    "r. chartrand and w. yin . :",
    "iteratively reweighted algrithms for compressive sensing .",
    "ieee international conference on acoustics , speech and signal processing .",
    "3869 - 3872(2008 ) .",
    "wolpert , david h. , and william g. macready . : no free lunch theorems for optimization .",
    "ieee transactions on evolutionary computation .",
    "* 1 * , 67 - 82(1997 )",
    ". y. wang , j. zheng , s.zhang , x. duan , and h. chen . : randomized structural sparsity via constrained block subsampling for improved sensitivity of discriminative voxel identification .",
    "neuroimage , in press ( 2015 ) .",
    "y. wang , s. zhang , j. zheng and h. chen . : randomized structural sparsity based feature selection with applications to brain mapping of autism spectrum disorder : a multimodal and multi - center reproducibility study .",
    "ieee transactions on autonomous mental development , in press(2015 ) .",
    "barbera , rina foygel , and emmanuel candes .",
    ": controlling the false discovery rate via knockoffs .",
    "arxiv preprint .",
    "arxiv:1404.5609(2014 ) .",
    "j. liu , s. ji , and j. ye .",
    ": multi - task feature learning via efficient @xmath120-norm minimization . in uai",
    "09 proceedings of the twenty - fifth conference on uncertainty in artificial intelligence .",
    "339 - 348(2009 ) .",
    "p. gong , j. ye , and c. zhang . :",
    "robust multi - task feature learning .",
    "kdd 12 proceedings of the 18th acm sigkdd international conference on knowledge discovery and data mining . *",
    "12 * , 895 - 903(2012 ) .",
    "t. zhang . : some sharp performance bounds for least squares regression with @xmath4 regularization .",
    "the annals of statistics , * 37 * , 2109 - 2144(2009 ) .",
    "y. zhang and d. yeung .",
    ": multi - task learning using generalized t process .",
    "jmlr w@xmath150cp .",
    "* 9 * , 964 - 971(2010 ) ."
  ],
  "abstract_text": [
    "<S> multi - task feature learning aims to identity the shared features among tasks to improve generalization . </S>",
    "<S> it has been shown that by minimizing non - convex learning models , a better solution than the convex alternatives can be obtained . </S>",
    "<S> therefore , a non - convex model based on the capped-@xmath0 regularization was proposed in @xcite , and a corresponding efficient multi - stage multi - task feature learning algorithm ( msmtfl ) was presented . however , this algorithm harnesses a prescribed fixed threshold in the definition of the capped-@xmath0 regularization and the lack of adaptivity might result in suboptimal performance . in this paper </S>",
    "<S> we propose to employ an adaptive threshold in the capped-@xmath0 regularized formulation , where the corresponding variant of msmtfl will incorporate an additional component to adaptively determine the threshold value . </S>",
    "<S> this variant is expected to achieve a better feature selection performance over the original msmtfl algorithm . </S>",
    "<S> in particular , the embedded adaptive threshold component comes from our previously proposed iterative support detection ( isd ) method @xcite . </S>",
    "<S> empirical studies on both synthetic and real - world data sets demonstrate the effectiveness of this new variant over the original msmtfl . </S>"
  ]
}