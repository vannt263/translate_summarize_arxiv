{
  "article_text": [
    "during the last number of years , the treatment of dynamics using generating functional techniques ( gfa ) has received a lot of attention in the field of statistical mechanics of disordered systems , in particular neural networks ( see , e.g. , @xcite@xcite and references therein ) .",
    "such a treatment allows for the exact solution of the dynamics and finds all relevant physical order parameters at any time step via the derivatives of a generating functional @xcite@xcite . an alternative method to study dynamics of neural networks",
    "is the so called signal - to - noise analysis ( sna ) or statistical neurodynamics ( see , e.g. , @xcite@xcite ) where the idea is to start from a splitting of the local field into a signal part originating from the pattern to be retrieved and a noise part arising from the other patterns .",
    "the differences in the existing versions of this approach consist out of the different treatments of this noise term , ranging from the assumption that it is gaussian with various approximations for its variance @xcite to a supposedly exact treatment @xcite .",
    "in two recent papers some comparisons have been made between the two methods for a sequence processing network @xcite respectively the fully connected blume - emery - griffiths network @xcite .",
    "for the first system , surprisingly , the order parameter equations obtained through the exact gfa solution are shown to be completely equivalent to those of statistical neurodynamics , known to be an approximation that assumes gaussian noise .",
    "these theoretical results are verified by computer simulations .",
    "we recall that this system contains no feedback correlations . for the second system that is fully connected and , hence , does contain feedback correlations",
    ", it has been shown that the results of the gfa and sna coincide up to the third time step .",
    "some numerical experiments then indicated that they may differ for further time steps , certainly for those parameters of the system corresponding to spin - glass behaviour .",
    "the idea of the present work is to perform a systematic analytical study of the relationship between both techniques using the fully connected little - hopfield model . in order to do so , we take the gfa one step further by deriving a recursion relation for the effective local field . to our knowledge",
    "such a recursion relation has not yet been reported upon in the literature .",
    "furthermore , it is precisely this relation that we use as basis for studying the correspondence between both methods .    for the fully connected model",
    "we show that the sna as it has been applied up to now in fact approximates the exact dynamics because it forgets about a part of the correlations .",
    "we discuss the physics behind such a short - memory approximation and explain why it leads to very good results in the case of retrieval .",
    "moreover , we show how to apply the sna correctly leading to a complete equivalence with the gfa .",
    "the results obtained are applied to other architectures and to sequence processing networks .",
    "the paper is organized as follows . in section 2",
    "we recall the fully connected little - hopfield model and discuss the sna approach to solve its dynamics .",
    "section 3 shortly reviews the gfa and derives recursion relations for the effective local field in this framework .",
    "section 4 introduces the short - memory approximation and shows how this reduces the results of the gfa to those of the sna .",
    "it also explains the physics behind this approximation and discusses some numerical results .",
    "section 5 presents a scheme how to apply exactly the sna . in section 6",
    "we apply our findings to the extremely diluted symmetric and asymmetric architectures and to sequence processing models .",
    "finally , section 7 contains some concluding remarks .",
    "by now , the little - hopfield model is a standard model for associative memory and can be found in many textbooks ( see , e.g , @xcite ) . consider a system of @xmath0 ising spins @xmath1 , @xmath2 .",
    "we want to store @xmath3 patterns @xmath4 , @xmath5 , independent and identically distributed with respect to @xmath6 and @xmath7 .",
    "the local field in neuron @xmath6 is defined by @xmath8 with the couplings given by the hebb rule @xmath9 all neurons are updated in parallel according to the glauber dynamics @xmath10",
    "=           \\frac{e^{\\beta s h_i(t)}}{2",
    "\\cosh(\\beta h_i(t ) ) } \\ ,       \\quad s=\\pm 1 \\ , , \\ ] ] which becomes , in the limit @xmath11 , equivalent to the gain function formulation @xmath12 in general , we write @xmath13    the long time behaviour of the system is governed by the hamiltonian @xcite @xmath14 \\,.\\ ] ] the thermodynamic and retrieval properties are visualized in the @xmath15 phase diagram @xcite .",
    "there is a transition curve starting at @xmath16 and ending at @xmath17 beyond which the system does not retrieve any patterns anymore and behaves like a spin - glass . for higher temperatures the system undergoes a transition to the paramagnetic phase .",
    "the parallel dynamics of this model using both the sna and gfa and , especially , the comparison between these approaches is the subject of the following sections .      in order to keep this paper self - contained we shortly review the signal - to - noise analysis .",
    "we assume that the system has an initial finite overlap with one of the patterns , say the first one , which we call condensed . the other patterns ( non - condensed ) act as noise , making it harder for the network to retrieve the condensed pattern .",
    "we first focus on zero temperature .",
    "the key idea is to separate the signal from the noise in the local field @xmath18 whereby it is technically convenient to include the self - interaction and subtract it again leading to the term @xmath19 .",
    "quantities that have a site - index , or an index @xmath0 are quantities for a finite system .",
    "we define the overlap , respectively the residual overlap by @xmath20 the local field ( [ eq : h : signal - noise ] ) then becomes @xmath21    our aim is to determine the form of the local field in the thermodynamic limit @xmath22 . to the signal term we apply the law of large numbers ( lln ) @xmath23 where the convergence is in probability .",
    "taking a closer look at the second term in ( [ eq : h : signal - noise:1 ] ) , we could apply the central limit theorem ( clt ) if all the terms in the sum would be independent .",
    "this , of course , depends on the architecture and it is true , e.g. , for asymmetrically extremely diluted models @xcite . in general , there are non - trivial correlations between the terms and the different implementations of the sna mentioned before treat these correlations in a different way .    for the fully connected architecture at hand , the most naive and simple approach @xcite is to keep the assumption that all terms in the sum are uncorrelated , and that one can simply apply the clt theorem to the second term in ( [ eq : h : signal - noise:1 ] ) .",
    "the local field becomes normally distributed with mean @xmath24 and variance @xmath25 .",
    "this approach has then been refined in the theory of statistical neurodynamics @xcite .",
    "there one also assumes that the noise part of the local field is normally distributed but one calculates explicitly its variance starting from its definition and taking into account part of the correlations between the embedded pattern and neuron states in the dynamics .",
    "this results in a more complex structure of the variance of the local field . for more technical details",
    "we refer to @xcite .",
    "detailed simulations show that the assumption of gaussian noise is approximately valid and close to reality as long as retrieval is successful .",
    "in particular , it succeeds in explaining qualitatively the dynamical behaviour of retrieval in the associative memory model .",
    "however , in addition to a different critical capacity at @xmath26 , the basin of attraction calculated in this scheme is larger than the one obtained by computer simulations @xcite .",
    "this is attributed to the fact that the correlations of the local fields at successive time steps are neglected .",
    "more specific , the average over the gaussian distribution of the local field at time @xmath27 is taken independently of the average of the neuron state at time @xmath28 , which clearly depends on the local field at time @xmath29 .",
    "later on these correlations between the local field at successive time steps ( and therefore also correlations between neuron states at different time steps ) have been taken into account . in this treatment one",
    "rewrites the noise part of the local field as the sum of two correlated terms to which one can apply the clt but one keeps the gaussian assumption throwing away possible non - gaussian noise . for more details we refer to @xcite .",
    "this further refined theory gives a better explanation of the dynamics and the basin of attraction .",
    "moreover , the storage capacity resulting from this method is in good agreement with the results from computer simulations .",
    "however , we recall that the a priori gaussian approximation is only valid when retrieval is successful .",
    "an improvement of the approximation is obtained by taking into account explicitly the feedback : the network state at time @xmath27 , @xmath30 , depends on its previous states up to @xmath29 namely @xmath31 .",
    "one proposal has been to assume two gaussian peaks with variance calculated from the residual noise and separated by an appropriately chosen distance @xcite but also here the correlations between the network states @xmath32 are only partially taken into account .",
    "more recently , one has studied the distribution of the local field without any a priori assumption on the residual noise , trying to take into account all correlations of the different terms of the sum ( [ eq : h : signal - noise:1 ] ) using the insight gained before @xcite ( see also @xcite and references therein ) .    in the treatment of the correlations between the variables appearing in this expression for the local field ,",
    "one has to be very careful , because even a small dependence of order @xmath33 may give rise to a macroscopic contribution after summation .",
    "therefore , we rewrite the local field @xmath34 where we have split of the part of @xmath35 that depends strongly on @xmath36 , such that @xmath37 only depends weakly on @xmath38 .",
    "remark that the second term is small for large @xmath0 , so @xmath39 using this and the definition of the residual overlap in ( [ eq : op : def ] ) , we get @xmath40 where we have defined @xmath41 now , we take the limit @xmath42 . in this limit",
    "the density distributions of @xmath43 and @xmath44 are equal since they only differ upto a factor @xmath45 .",
    "since @xmath46 only depends weakly on @xmath38 , we assume that in the limit we can use the clt for the first term . to the second term in ( [ eq : r : recur : n ] ) , we apply the lln arriving at @xmath47 with @xmath48 a normally distributed random variable @xmath49 and @xmath50 where the average denoted by @xmath51 is over the distribution of the local field with probability density @xmath52 and where @xmath53 denotes the average over the initial conditions and the condensed pattern .",
    "next , in order to find the distribution of the local field in the thermodynamic limit , we start from ( [ eq : h : signal - noise ] ) and use ( [ eq : op : def ] ) and ( [ eq : r : recur : n ] ) @xmath54 the first term on the r.h.s . ,",
    "using ( [ eq : blabla ] ) is clear . in the second term",
    ", we can replace @xmath46 by @xmath55 leading to a contribution @xmath56 .",
    "the third term is a sum of independent random variables and by the clt it converges to @xmath57 . in the fourth term ,",
    "we employ ( [ eq : h : signal - noise:1 ] ) . from this",
    "we obtain , omitting the site index @xmath6 @xmath58          + { \\ensuremath{{\\cal n}}}(0,\\alpha ) \\,.\\ ] ] in this way we arrive at the recursion relation ( [ eq : h : recur ] ) for the local field and ( [ eq : r : recur ] ) for the residual overlap .",
    "we still want a recursion relation for the overlap by using the dynamics ( [ eq : dyn : det : gen ] ) starting from ( [ eq : op : def ] ) @xmath59    finally , from ( [ eq : h : recur ] ) it is clear that the local field consist out of a discrete part and a normally distributed part @xmath60 where @xmath61 can be found by iterating the recursion relation for the local field , i.e. @xmath62 the variance of the noise in ( [ eq : m : def ] ) can be calculated by using ( [ eq : r : recur ] ) @xmath63    we still have to write out the probability density of the local field used to define @xmath64 in ( [ eq : chi : def ] ) .",
    "the evolution equation tells us that @xmath65 can be replaced by @xmath66 , such that the second term of @xmath61 in ( [ eq : m : def ] ) is the sum of step functions of correlated variables .",
    "these variables are also correlated with the gaussian part of the local field through the dynamics .",
    "therefore , the local field can be seen as a transformation of a set of correlated gaussian variables @xmath67 which we choose to normalize . defining the correlation matrix by @xmath68 , @xmath69 we arrive at the following expression for this probability density @xmath70",
    "this concludes the sna treatment of the little - hopfield model at zero temperature .",
    "the above equations form a recursive scheme in order to calculate the dynamical properties of the system up to an arbitrary time step .",
    "the practical difficulty which remains , certainly after a few time steps is the explicit calculation of the correlations .",
    "it is possible to extend the method to arbitrary temperatures by introducing auxiliary thermal fields to express the stochastic dynamics within the gain function formulation of the deterministic dynamics @xcite @xmath71 where the @xmath72 are independent and identically distributed with probability density @xmath73 one then averages the zero temperature results over the auxiliary field @xmath72 .",
    "this alters the equations in a non - trivial way but such that the idea of the derivation can be completely retained . at this point",
    "we remark that in the derivation of the local field distribution one has to replace @xmath65 by @xmath66 with @xmath74 given by ( [ eq : m : def ] ) . for arbitrary temperatures",
    "one has to replace @xmath65 by @xmath75 and the average over the @xmath72 then enters at the same level as the average over the noise .",
    "for more details we refer to the literature @xcite .      in order to compare with the results of the gfa method to be explained in the following section it is useful to recall the sna results for the first few time steps as found in @xcite ( and references therein ) .",
    "we only write down explicitly the expressions for the overlap @xmath76 at zero temperature @xmath77 + \\sqrt{\\alpha d(2 ) } y )                       \\big\\rangle\\!\\!\\big\\rangle                        } } \\\\",
    "m(4 ) & = &   { \\ensuremath{\\big\\langle\\!\\!\\big\\langle                                                \\xi \\int { { \\cal d } w^{0,1,3}}{(x , y , z ) } \\ , g\\big [ \\xi m(3 )                            } }            \\nonumber   \\\\      & & \\qquad   + \\alpha \\chi(2 ) \\ ,           g ( \\xi m(1 ) + \\alpha \\chi(0 ) \\sigma(0 )                                 + \\sqrt{\\alpha d(1)}y                                                     )                  \\nonumber \\\\     & & \\qquad   + \\alpha \\chi(2)\\chi(1 ) \\ ,             g ( \\xi m(0 ) + \\sqrt{\\alpha d(0)}x                                                     )                         \\nonumber   \\\\      & & \\qquad             + \\alpha \\chi(2)\\chi(1)\\chi(0 ) \\sigma(0 )   \\nonumber",
    "\\\\      & & \\qquad   { \\ensuremath {                                           + \\sqrt{\\alpha d(3 ) } z                                          \\big ]                             \\big\\rangle\\!\\!\\big\\rangle                        } }                       \\ , .",
    "\\label{eq : m(4):sna}\\end{aligned}\\ ] ] here @xmath78 is the gaussian measure with variable @xmath79 while @xmath80 is the multidimensional gaussian measure with correlation matrix @xmath81 as it appears in ( [ eq : h : sna ] ) . for the explicit form of the expressions for the variance of the residual overlap , the function @xmath82 and the correlations we refer to the literature @xcite",
    "the idea of the gfa aproach to study dynamics @xcite is to look at the probability to find a certain microscopic path in time .",
    "the basic tool to study the statistics of these paths is the generating functional @xmath83 = \\sum_{{\\ensuremath{\\boldsymbol{\\sigma}}}(0 ) , \\dots , { { \\ensuremath{\\boldsymbol{\\sigma}}}}(t ) }               p[{{\\ensuremath{\\boldsymbol{\\sigma}}}}(0 ) , \\dots , { { \\ensuremath{\\boldsymbol{\\sigma}}}}(t ) ]      \\ , \\prod_{i=1}^n \\prod_{s=0}^t e^{-i \\ , \\psi_i(s ) \\ , \\sigma_i(s ) } \\ ] ] with @xmath84 $ ] the probability to have a certain path in phase space @xmath85   = p[{{\\ensuremath{\\boldsymbol{\\sigma}}}(0 ) } ] \\prod_{s=0}^{t-1 } w[{{\\ensuremath{\\boldsymbol{\\sigma}}}}(s+1)|{{\\ensuremath{\\boldsymbol{\\sigma}}}}(s)]\\ ] ] and @xmath86 $ ] the transition probabilities from @xmath87 to @xmath88 .",
    "looking back at ( [ eq : dyn : glauber ] ) and assuming parallel dynamics we have @xmath89    = \\left .",
    "\\prod_{i } \\frac{e^{\\beta \\sigma_i(s+1 ) h_i(s ) } }                         { 2\\cosh(\\beta h_i(s ) ) }      \\right|_{h_i(s ) = \\sum_j j_{ij } \\sigma_j(s ) + \\theta_i(s ) }      \\label{transprob}\\ ] ] where we have introduced a time - dependent external field @xmath90 in order to define a response function .",
    "one can find all the relevant order parameters , i.e. , the overlap @xmath76 , the correlation function @xmath91 and the response function @xmath92 , by calculating appropriate derivatives of the above functional and letting @xmath93 tend to zero afterwards @xmath94}{\\delta\\psi_i(t ) } \\\\",
    "g(t , t ' ) & = & i \\lim_{{\\ensuremath{\\boldsymbol{\\psi}}}\\to0 }             \\frac{1}{n } \\sum_{i }             \\frac{\\delta^2 z[{\\ensuremath{\\boldsymbol{\\psi}}}]}{\\delta\\psi_i(t ) \\delta\\theta_i(t ' ) } \\\\ c(t , t ' ) & = & - \\lim_{{\\ensuremath{\\boldsymbol{\\psi}}}\\to0 }             \\frac{1}{n } \\sum_{i }             \\frac{\\delta^2 z[{\\ensuremath{\\boldsymbol{\\psi}}}]}{\\delta\\psi_i(t ) \\delta\\psi_i(t')}\\ ,         .\\end{aligned}\\ ] ]    in the thermodynamic limit one expects the physics of the problem to be independent of the quenched disorder and , therefore , one is interested in derivatives of @xmath95}$ ] with the overline denoting the average over this disorder , i.e. , all pattern realizations .",
    "this results in an effective single spin local field given by @xmath96 with @xmath97 temporally correlated noise with zero mean and correlation matrix @xmath98 and the retarded self - interaction @xmath99 we refer to @xcite for further details .",
    "the order parameters defined above can be written as @xmath100 the average over the effective path measure is given by @xmath101 where @xmath102 and with @xmath103 the average denoted by the double brackets is again ( as in the sna ) over the condensed pattern and initial conditions .",
    "we remark that @xmath104 for @xmath105 and @xmath106 , and that for all @xmath107 @xmath108 \\right\\rangle}}_\\star                   \\right\\rangle\\!\\right\\rangle\\ ] ] where @xmath109 is given by ( [ eq : h ] ) .",
    "the gfa at arbitrary temperatures starts from the transition probabilities ( [ transprob ] ) following from the glauber dynamics ( [ eq : dyn : glauber ] ) . in order to make the comparison with the sna later on",
    "we want to introduce an alternative description starting from the deterministic dynamics as in ( [ eq : dyn : det])-([distribgamma ] ) .",
    "starting from ( [ eq : s|eta ] ) and ( [ eq : eta ] ) , taking the limit of zero temperature and introducing the auxiliary thermal field @xmath72 both formulations are equivalent . indeed , suppose that we want to calculate the effective path average of a general function @xmath110 using the alternative description . writing out the expression for this average ( the average over the condensed pattern and initial conditions is not relevant here )",
    "@xmath111         f({{\\ensuremath{\\boldsymbol{\\sigma}}}})\\end{aligned}\\ ] ] where @xmath112 is the thermal field .",
    "since the latter only occurs inside the @xmath113-function and the probability density , we can evaluate the integral over @xmath112 .",
    "then the integration over the local field becomes @xmath114 \\ , .\\end{gathered}\\ ] ] this integral can be evaluated , and yields @xmath115 which is exactly the same as ( [ eq : s|eta ] ) showing that we can use both representations for the dynamics .    using this alternative description for the thermal dynamics",
    "it is straightforward to write the distribution of the local field as @xmath116 where we denote by @xmath117 the substitutions @xmath118 and @xmath119 for all @xmath120 .",
    "the above discussion makes it trivial to perform the zero temperature limit in the gfa and also shows that it is enough to look at the relation between both methods at zero temperature in order to be able to compare them in the whole phase diagram , because the way to extend the methods to finite temperature is completely equivalent .",
    "the aim of this subsection is to derive recursion relations for the local field and also for the noise appearing in the local field starting from the gfa . in this way",
    "we want to gain more insight into the equations at hand and make a detailed comparison between the sna and gfa .    from expression ( [ eq : h ] ) for the effective local field , it is not immediately obvious how @xmath121 depends on @xmath109 , @xmath122 .",
    "first , we want an expression for the retarded self - interaction @xmath123 as a function of the previous time step(s ) . to this end",
    "we write the response matrix at time @xmath124 in the following way @xmath125 where @xmath126 is the following vector of dimension @xmath28 @xmath127 and @xmath128 is the response matrix at time @xmath28 .",
    "it is clear that adding a time step adds a row and a column to the matrix while it leaves the other elements unaltered . from the decomposition of the response matrix",
    ", we calculate @xmath129 ( see , e.g. , @xcite ) @xmath130 and use this expression in ( [ eq : h ] ) to arrive at @xmath131    + \\sqrt{\\alpha } \\sum_{s=0}^{t } ( { { \\boldsymbol{i}}}- { \\ensuremath{\\boldsymbol{g}}})(t , s ) \\ , { \\ensuremath{\\eta}}(s ) \\ , .\\ ] ] this is one of the main results of this section and will be used as the basis of comparison between the gfa and sna approaches .",
    "it leads naturally to the definition of a modified noise variable @xmath132 @xmath133 and it follows that the covariance matrix of the noises , @xmath134 , is given by @xmath135(s , s')\\ , .\\ ] ] the correlation matrix of this transformed noise variable is clearly simpler than the original one and is in fact the correlation matrix of the spins .",
    "next , we derive recursion relations for the noise in a similar way .",
    "analogous to ( [ eq : g : decomp ] ) we write @xmath136 where @xmath137 is the following vector of dimension @xmath28 @xmath138 one then finds @xmath139 again , going one time step further implies adding one row and column to the matrix while the rest of the matrix remains unchanged . from ( [ eq : d : recursion ] ) we find a relation for the variance of the noise at time @xmath27 @xmath140 as a function of the variance of the noise at time @xmath107 and other quantities . apparently ,",
    "the right - hand side of this equation does still depend on @xmath27 and , therefore , does not seem to be really a recursion relation .",
    "however , as we will show , the quantities on the right - hand side can be calculated without having full information about time @xmath27 , e.g. , @xmath141 can be obtained without knowing @xmath142 .",
    "comparing the expressions for the effective local field ( [ eq : h : recursion ] ) and ( [ eq : h : recur ] ) we notice that the first one contains a sum over all time steps up to @xmath29 while the second one only contains @xmath29 itself .",
    "therefore , we introduce the following approximation @xmath143 where we have defined @xmath144 , @xmath145 and @xmath146 in this way .",
    "the approximated matrix @xmath146 now has a simple form @xmath147 this approximation reduces the expression for the local field ( [ eq : h : recursion ] ) to @xmath148    + \\sqrt{\\alpha } { \\ensuremath{\\hat{{\\ensuremath{\\phi}}}}}(t+1 ) \\ , .\\ ] ] furthermore , the modified noise equation ( [ eq : etat ] ) simplifies to @xmath149 and the variance of the noise itself ( [ eq : eta : recur ] ) becomes @xmath150 another consequence of this approximation is that the discrete part in the local field ( [ eq : m : def ] ) can be written down or , in other words , the retarded self - interaction @xmath123 can be calculated explicitly to be @xmath151    comparing the recursion relations ( [ eq : h : recur ] ) and ( [ eq : h : ma ] ) for the local field , those for the residual overlap , ( [ eq : r : recur ] ) and ( [ eq : etat : ma ] ) , and the ones for the variance of the residual overlaps , ( [ eq : v : recur ] ) and ( [ eq : eta : rec : ma ] ) , we find that they are formally the same by taking @xmath152    next , we would like to know what is the physics behind this approximation .",
    "in fact , all solutions to ( [ eq : ma ] ) can be called k - th order short - memory approximations because they approximate the feedback @xmath153 these approximations take into account that responses , in general , decrease very fast as a function of time .",
    "as we will show , the first order approximation obtained for @xmath154 and simply called the short - memory approximation , corresponds to the sna @xmath155 we remark that these approximations all have a different @xmath64 so that we can use the latter in order to distiguish between them .",
    "we calculate the @xmath64 corresponding to @xmath154 , starting from the alternative stochastic description of the gfa ( forgetting about the average over the initial conditions and condensed pattern ) .",
    "we find @xmath156 recalling the distribution of the local field at time @xmath28 ( [ eq : h : alt : t ] ) , and taking the temperature to be zero ( which means vanishing @xmath112 ) , this expression already resembles the analogous expression ( [ eq : h : sna ] ) in the sna approach .",
    "furthermore , one sees that @xmath157 does not occur in the integrand of this expression . as a consequence",
    ", one can show that the matrix @xmath158 can be replaced by @xmath159 or in more detail @xmath160 one can then verify that @xmath161 , for zero temperature , is then exactly the same as the expression for @xmath64 in the sna approach , viz .",
    "@xmath162 in conclusion , compared with the exact gfa method we find that the sna approach is a short - memory approximation , in which the response from earlier times is not taken into account .      the origin of the approximation inherent in the sna approach as discussed above lies in the treatment of the residual noise @xmath163 ( recall ( [ eq : r : n ] ) ) .",
    "we have assumed that after taking out the term @xmath164 of the local field , @xmath36 and @xmath165 are only weakly correlated , and therefore we have applied the clt to find that @xmath163 converges to a normal distribution . comparing with the gfa it appears that we took out only part of the correlations between @xmath166 and @xmath36 , those coming from the previous time step . in a fully connected system however , there are not only feedback loops of length 1 and 2 , but of arbitrary length , although they may be less probable @xcite .",
    "therefore , one would expect the sna method to approximate the dynamics already from the second time step onwards .",
    "however , we can show that @xmath167 is zero such that the second time step is still exact . moreover , some of the order parameters in the third time step , e.g.",
    ", the overlap only involve noises up to the second time step , so that they are also correct for the third time step .    in order to show that @xmath168 we proceed as follows ( recall ( [ eq : g : t>0 ] ) ) @xmath169       \\right\\rangle_{\\star }       } } \\ , .\\end{aligned}\\ ] ] considering the effective path average the sum over @xmath65 can be done explicitly @xmath170",
    "moreover , the expression for the distribution only contains summations over @xmath171 up to @xmath172 and there is only one term left that contains @xmath172 so @xmath173 remark that further sums over spins can not be done since @xmath74 contains the spins at times @xmath174 .",
    "this shows that @xmath175 .",
    "first , for @xmath176 , one can easily verify that the gfa analysis yields @xmath177 implying that the short - memory approximation , i.e. , the sna is exact .",
    "the reason is that the local field at time @xmath28 does not depend on previous times because the terms in the retarted self - intereaction and the noise are proportional to @xmath25 .",
    "the only correlation that remains in the system comes from the dependence of the spins on the local field at the previous time step .",
    "consequently , we expect the sna to be a very good approximation to the exact dynamics for small loading capacities .",
    "some extra reasons to do so is that for small loading capacities , the convergence time to the attractor is very small , only a few time steps , and the sna is exact up to time step @xmath178 . moreover , to write down the evolution equations for the order parameters like the overlap one performs averages over the noise .",
    "therefore , not surprisingly , the sna results for the first few time steps coincide with numerical simulations as has been reported in the literature before .",
    "only discrepancies of the order @xmath179 that are of the same magnitude as the finite size effects in the simulations , and hence not conclusive , have been observed ( see , e.g. , @xcite and references therein ) .    analytically , of course , one notices the difference from the fourth time step onwards .",
    "starting from the exact gfa approach and writing the expression in a convenient form for comparison , we get at zero temperature @xmath180                   \\big\\rangle\\!\\!\\big\\rangle                        } } \\,\\end{aligned}\\ ] ] the only difference with the sna result ( [ eq : m(4):sna ] ) is in the term that is underlined .",
    "it is present due to the fact that beyond @xmath181 , @xmath123 is not simply given by ( [ eq : m : approx ] ) .",
    "we will show numerically in the next subsection that this difference is small ( e.g. , of the order of 0.3% upto 3% for @xmath182 , @xmath183 respectively @xmath184 )",
    ". it will be interesting to see how this difference behaves for further time steps .",
    "now that we know precisely what the origin is of the approximation inherent in the sna method as usually applied , we can check numerically how accurate it is for retrieval and also for spin - glass behaviour , although from the point of view of neural networks one is not primarily interested in the latter .",
    "we first want to compare the limiting normal distribution of @xmath185 in the sna approach with simulations for different time steps .",
    "this is done in fig .  1 for two representative values of the capacity , i.e. @xmath186 which lies in the retrieval phase and @xmath187 which is above the critical capacity and thus within the spinglass phase .    [ fig1 ]    , for different time steps compared with their theoretical distribution as calculated using the sna ( full curve ) . the picture on the left is for @xmath188 while the one on the right is for @xmath189 corresponding to retrieval respectively spinglass behaviour .",
    "both pictures where made for @xmath190 , @xmath191 using a finite size simulation with @xmath192 ( @xmath193 ) for the left ( right ) figure and averaging over 200 samples .",
    ", title=\"fig : \" ] , for different time steps compared with their theoretical distribution as calculated using the sna ( full curve ) .",
    "the picture on the left is for @xmath188 while the one on the right is for @xmath189 corresponding to retrieval respectively spinglass behaviour .",
    "both pictures where made for @xmath190 , @xmath191 using a finite size simulation with @xmath192 ( @xmath193 ) for the left ( right ) figure and averaging over 200 samples .",
    ", title=\"fig : \" ] +    we conclude that in the retrieval region ( @xmath194 for @xmath190 ) the simulation results coincide quite good with the limiting sna distribution , while in the spin - glass region , the results for @xmath181 start to divert systematically .",
    "this is consistent with the results obtained in the fully connected blume - emery - griffiths neural network  @xcite .",
    "remark that the distribution of @xmath195 starts to divert for @xmath181 , while @xmath196 is still exact as discussed before .",
    "we also want to compare the evolution of the order parameters . as a typical result",
    "we show in fig .",
    "2 the overlap @xmath76 as a function of time using the method in @xcite , precisely in order to avoid finite size effects .    , @xmath197 for retrieval behaviour @xmath198 and spin - glass behaviour @xmath199 .",
    "the thin line represents the stationary limit for retrieval . ]",
    "[ fig2 ]    we see that the sna results coincide with those of the gfa up to the third time step , as shown analytically . for further time",
    "steps the results are strongly dependent upon the parameters of the network determining its behaviour : retrieval or spin - glass behaviour .",
    "the parameters chosen in fig .  2 are @xmath200 , @xmath197 , and @xmath198 respectively @xmath201 .",
    "for the first choice ( the two upper curves ) the system evolves to the retrieval attractor and one observes that there is only a marginal difference between the sna and gfa method .",
    "this confirms the previous observations made in the literature ( @xcite and references therein ) . for the second choice ( the two lower curves )",
    "the initial overlap is too small such that we are outside the basin of attraction for retrieval and , hence , we do not evolve to the attractor ( spin - glass behaviour ) . here",
    "the sna method , as used in the literature , does not give good results for further time steps .",
    "we remark that we have also compared these results with finite size simulations .",
    "these simulations are not indicated on the figure because they lie almost exactly on the gfa lines .",
    "in this section we show how to treat the feedback correlations in the sna approach correctly . looking at the definition of the residual overlap at time @xmath28 , i.e. ,",
    "@xmath202 we want to know how the dependence between spins and patterns evolves in the course of the dynamics .",
    "in general , the local field at a certain time step @xmath35 depends on @xmath36 in two ways .",
    "first , there is the correlation through the occurrence of @xmath36 in the coupling matrix ( @xmath203 ) .",
    "we call this type of correlations first order correlations because they are the most obvious ones .",
    "second , there may be so - called second order correlations , being correlations with pattern @xmath36 through the dynamics as a result of feedback .",
    "second order correlations can only appear due to first order correlations earlier in the dynamics .",
    "therefore , extracting the first order correlations at every time step results in a system that does not depend on @xmath36 anymore .",
    "we define @xmath204 by @xmath205 in this way , adding @xmath206 removes the first order correlations from the local field at time @xmath28 , i.e. , @xmath207 such that @xmath208 only depends on @xmath36 in second order .",
    "moreover , @xmath204 tends to zero as @xmath45 in the thermodynamic limit , and can therefore be regarded as a perturbation to the field @xmath209 .",
    "we apply this perturbation for all time steps @xmath107 and find @xmath210 where @xmath211 is now completely independent of @xmath36 .",
    "we rewrite the derivatives by introducing an external field @xmath90 , @xmath212 inserting ( [ eq : delta - h ] ) in ( [ eq : s - recur ] ) and using ( [ eq : part - der ] ) yields @xmath213 we multiply both sides by @xmath214 and sum over @xmath6 , so that @xmath215 by using the definition of @xmath216 .",
    "we now take the limit @xmath22 . by construction , the first term on the r.h.s .",
    "is a sum of independent terms and thus converges to a normal distribution with zero mean and variance 1 . to the second term",
    "we apply the lln .",
    "the result of the limit reads @xmath217    this recursion relation for @xmath218 corresponds to ( [ eq : etat ] ) by using the substitution @xmath219 for @xmath220 and @xmath195 for @xmath132 as in ( [ eq : bla ] ) . finally , we insert this relation into the expression ( [ eq : h : recur ] ) of the local field at time @xmath27 leading to , after some algebra , the expression for the exact local field as in equation ( [ eq : h : recursion ] ) for the gfa .",
    "this shows that we have found all feedback correlations .",
    "in this section we shortly discuss the application of the sna approach to other architectures .    for the extremely diluted symmetric architecture one has found an exact solution for the dynamics up to time step @xmath178 by using a probabilistic method analogous to the sna @xcite and comparing it with the generating functional approach @xcite . in this case , the effective local field , starting from the gfa is given by @xmath221 with @xmath97 a temporally correlated noise with zero mean and correlation matrix @xmath158 . hereby , @xmath222 it is clear that , besides the simplification in both the correlations and retarded self - interaction , feedback correlations of arbitrary length survive and make the dynamics as hard to solve as the one of the fully connected model .",
    "hence , a completely analogous discussion as the one before can be made in this case .",
    "for the extremely diluted asymmetric architecture the effective local field in the gfa approach is given by ( [ eq : h : gen ] ) , where now @xmath223 hence , the retarded self - interaction is zero , telling us that the local field at time @xmath28 does not directly depend on the spins at previous times but only indirectly via the noise .",
    "this is precisely the reason why for @xmath176 , the short - memory approximation gives the exact answer ( see above ) .",
    "it amounts to having a response function of the form ( [ eq : g : alpha=0 ] ) .",
    "therefore , the sna describes the correct dynamics .",
    "the reason is that for asymmetric dilution the probability to have a loop of finite length tends to zero in the thermodynamic limit @xcite .    for sequence processing networks",
    "the gfa effective local field is given by ( [ eq : h : gen ] ) with @xmath224 and the situation is analogous to the one for the asymmetrically diluted model in the sense that the retarded self - interaction is zero .",
    "again we have a response function of the form ( [ eq : g : alpha=0 ] ) and , hence , the short - memory approximation is exact .",
    "this is consistent with and further explains the results in @xcite where it has been shown through explicit calculation that the order parameter equations obtained through the gfa are equivalent to those of statistical neurodynamics .",
    "in this paper we have revisited the signal - to - noise approach for solving the dynamics of the fully connected little - hopfield model by comparing it with the exact generating functional analysis . in order to do so",
    "we have derived a recursion relation for the effective local field in the generating functional approach .",
    "we have shown that the signal - to - noise analysis is a short - memory approximation that is exact up to the third time step . for further time steps",
    ", it stays very accurate in the retrieval region but not in the spin - glass region .",
    "these results are confirmed by numerical simulations .",
    "the application of these methods to other architectures and to sequence processing models has also been discussed .",
    "we would like to thank t. coolen and g.m .",
    "shim for informative discussions .",
    "this work has been supported in part by the fund of scientific research , flanders - belgium .",
    "galla t , coolen a c c and sherrington d 2003 _ preprint _ cond - mat/0303615 boll d , busquets blanco j , shim g m and verbeiren t 2003 _ preprint _ cond - mat/0304553 ( physica a at press ) coolen a c c 2001 in _ handbook of biological physics vol 4 _ , ed . by moss f and gielen s ( elsevier science ) 597"
  ],
  "abstract_text": [
    "<S> using the generating functional analysis an exact recursion relation is derived for the time evolution of the effective local field of the fully connected little - hopfield model . </S>",
    "<S> it is shown that , by leaving out the feedback correlations arising from earlier times in this effective dynamics , one precisely finds the recursion relations usually employed in the signal - to - noise approach . </S>",
    "<S> the consequences of this approximation as well as the physics behind it are discussed . </S>",
    "<S> in particular , it is pointed out why it is hard to notice the effects , especially for model parameters corresponding to retrieval . </S>",
    "<S> numerical simulations confirm these findings . </S>",
    "<S> the signal - to - noise analysis is then extended to include all correlations , making it a full theory for dynamics at the level of the generating functional analysis . </S>",
    "<S> the results are applied to the frequently employed extremely diluted ( a)symmetric architectures and to sequence processing networks . </S>"
  ]
}