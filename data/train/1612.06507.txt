{
  "article_text": [
    "cloud computing has emerged as a trending platform for hosting various services for the public . among different cloud computing platforms ,",
    "the infrastructure - as - a - service ( iaas ) clouds offer virtualized computing infrastructures to public tenants in order to host tenant services . enabled by advanced resource virtualization technologies ,",
    "the clouds support intelligent resource sharing among multiple tenants , and can provision resources per tenant demand .",
    "however , due to the massive migration of services to the cloud , there is increasing concern about the unpredictable performance of cloud - based services . one major cause is the lack of network performance guarantee .",
    "all the tenants have to compete in the congested cloud network in an unorganized manner .",
    "this has motivated recent efforts on cloud resource sharing with network bandwidth guarantee , for which a novel cloud service abstraction has been proposed , named virtual cluster ( vc )  @xcite .",
    "the vc abstraction allows each tenant to specify both the virtual machines ( vms ) and per - vm bandwidth demand of its service .",
    "the cloud then realizes the request by allocating vms on physical machines ( pms ) , as well as reserving sufficient bandwidth in the network to guarantee the bandwidth demand in the _ hose model _  @xcite .",
    "the process of resource allocation for virtual clusters is called _ virtual cluster embedding _ ( vce ) .",
    "algorithms have been developed for vce with various objectives and constraints  @xcite .",
    "one missing perspective in existing vce solutions is the _ availability _ of tenant services . due to the large - scale nature of cloud data centers",
    ", pm failures can happen frequently in the cloud  @xcite .",
    "when such failure happens , all services who have their vcs fully or partly embedded on the failed pms will be affected , possibly receiving degraded service performance or even interruption of operation .",
    "this not only impairs the tenants interests , but also incurs additional cost to the cloud due to violation of service - level agreements .    to achieve the high availability goal of tenant services ,",
    "one common practice is to enable service _ survivability _ , by utilizing extra resources to help services recover quickly when actual failures happen . a survivability mechanism can be either _ pro - active _ or _ reactive_. a pro - active mechanism provisions backup resources at the time of service provisioning , prior to the actual happening of failures . due to this , it can offer guaranteed recovery against a certain level of failures in the substrate , at the cost of underutilized resources when no failure is present . on the contrary ,",
    "a reactive mechanism only looks for backups as a reaction to actual failures .",
    "while this means less reserved resources in the normal operation , a reactive mechanism may not always find a feasible recovery during the failure , and thus can not guarantee the survivability of the service .    in this paper",
    ", we study how to efficiently provide pro - active protection for tenant services under the vc model .",
    "in particular , we aim at embedding tenant vc requests survivably such that they can recover from any single - pm failure in the data center , meanwhile minimizing the total amount of resources reserved for each tenant .",
    "we formally define survivable vc embedding as a joint resource optimization problem of both primary and backup embeddings of the vc .",
    "following existing work  @xcite , we assume the data center has a tree structure , which abstracts many widely - adopted data center architectures .",
    "we then propose an algorithm to optimally solve the embedding problem , within time bounded by a polynomial of the network size and the number of requested vms ( pseudo - polynomial time to input size ) .",
    "the algorithm is based on the observation that the embedding decisions are independent for each subtree in the same level .",
    "since the optimal approach is time - consuming , we further propose a faster heuristic algorithm , whose performance is comparable to the optimal in practical settings .",
    "we conduct both theoretical analysis and simulation - based performance evaluation , which have validated the effectiveness of our proposed algorithms .",
    "our main contributions are summarized as follows :    to the best of our knowledge , we are the first to study the survivable and bandwidth - guaranteed vc embedding problem with joint primary and backup optimization .",
    "we propose a pseudo - polynomial time algorithm that finds the most resource - efficient survivable vc embedding for each tenant request .",
    "we further propose a heuristic algorithm that reduces the time complexity of the optimal algorithm by several orders , yet has similar performance in the online scenario .",
    "we use extensive experiments to evaluate the performance of our proposed algorithms .",
    "the rest of this paper is organized as follows .",
    "section  [ sec : rw ] presents the background and related work on vc embedding , as well as survivable cloud service provisioning .",
    "section  [ sec : model ] describes the network service model , introduces our pro - active survivability mechanism , and formally defines the survivable vc embedding problem .",
    "section  [ sec : a ] presents our optimal algorithm , and theoretical analysis for the proposed algorithm .",
    "section  [ sec : heu ] presents our efficient heuristic algorithm and proves its feasibility .",
    "section  [ sec : eval ] shows the evaluation results of our proposed algorithms , compared to a baseline algorithm .",
    "section  [ sec : conclusions ] concludes this paper .",
    "virtual cluster ( vc ) is a newly proposed cloud service abstraction , which offers bandwidth guarantee over existing vm - based abstractions  @xcite . in the vc model",
    ", the tenant submits its service request in terms of both the number of vms and the per - vm bandwidth demand .",
    "a tenant request , defined as a tuple @xmath0 , specifies a virtual topology where @xmath1 uniform vms are connected to a central virtual switch , each via a virtual link with bandwidth of @xmath2 , as shown in fig .",
    "[ fig : vc ] . to fulfill the request ,",
    "the cloud should provision @xmath1 vms in the substrate data center , with bandwidth guaranteed in the _ hose model _ ( to be detailed in section  [ sec : model ] ) . in short words ,",
    "hose model brings two major benefits : reduced model complexity ( user specifies per - vm bandwidth instead of per - vm pair bandwidth as in the traditional _ pipe model _",
    "@xcite ) , and simple characterization of the minimum bandwidth requirement on each link  @xcite ; interested readers are referred to  @xcite and  @xcite for details .",
    "ballani  _ et  al . _",
    "@xcite first proposed the vc abstraction for cloud services with hose - model bandwidth guarantee .",
    "they characterized the minimum bandwidth required on each link to satisfy the hose - model bandwidth guarantee , and developed a recursive heuristic for computing the vc embedding with minimum bandwidth consumption .",
    "based on it , zhu  _ et  al . _",
    "@xcite proposed an optimal dynamic programming algorithm to embed vc in the lowest subtree in tree - like data center topologies .",
    "they also proposed a heuristic algorithm for vcs with heterogeneous bandwidth demands of their vms .",
    "tivc  @xcite extends oktopus with a time - related vc model that takes into consideration the dynamic traffic patterns of cloud services .",
    "svc  @xcite also extends oktopus , and considers the statistical distribution of bandwidth demands between vms .",
    "it proposes another dynamic programming algorithm to tackle the uncertain bandwidth demands .",
    "dcloud  @xcite incorporates deadline constraints into the vc abstraction . instead of guaranteeing per - vm bandwidth",
    ", it guarantees that each accepted job will finish execution within its specified deadline .    in a recent work ,",
    "rost  _ et  al . _",
    "@xcite proposed that the vc embedding problem could be solved in polynomial time . yet",
    ", their model does not capture the minimum bandwidth required on each link to satisfy vm bandwidth requirements , which was characterized originally in  @xcite ; as a result , their solution may over - provision bandwidth for vcs .",
    "recently , elastic bandwidth guarantee has drawn attention  @xcite .",
    "yu  _ et  al . _",
    "@xcite proposed dynamic programming algorithms for dynamically scaling vcs , optimizing virtual cluster locality and vm migration cost .",
    "fuerst  _ et  al . _",
    "@xcite also studied vc scaling minimizing migrations and bandwidth ; their approach relies on the concept of center - of - gravity , which is determined by the location of the central switch .",
    "none of the above has considered survivability in vc embedding .",
    "existing survivability mechanisms , such as those shown in the next subsection , do not lead to satisfactory solutions when directly applied to vc embedding , due to their lack of consideration for bandwidth requirement and/or lack of performance guarantee .",
    "this paper focuses on deriving theoretically guaranteed solutions for the survivable vc embedding problem , as well as promising ( low - complexity ) heuristics .",
    "other problems similar to vc embedding include bandwidth - guaranteed vm embedding  @xcite and virtual network / infrastructure embedding  @xcite .",
    "the former problem is topology - agnostic , and only considers bandwidth on edge links ; the latter considers a more general model where the virtual topology can be arbitrary graphs , hence it commonly suffers from high model complexity ( to be detailed below ) .      providing",
    "survivability guarantee for vcs has been studied by alameddine  _ et  al . _",
    "given a fixed primary embedding of a vc , they proposed a heuristic solution to ensure 100% survivability for the vc with minimum backup vms and bandwidth  @xcite .",
    "they further considered inter - vc bandwidth sharing to reduce backup bandwidth in@xcite .",
    "however , their solutions did not consider the impact of the primary embedding to backup resource consumption . in this paper",
    ", we propose to jointly optimize both primary and backup resources of a vc , which can result in reduced backup resource consumption .",
    "also , we propose an optimal solution , rather than heuristic solutions in@xcite .    beyond the vc abstraction , many have studied offering survivable virtual cloud services under various computing and network models  @xcite .",
    "a first line of research focuses on providing survivable vm hosting in the cloud .",
    "nagarajan  _ et  al . _",
    "@xcite proposed the first pro - active vm protection method , which leverages the live migration capability of the xen hypervisor to protect vms from detected failures .",
    "based on this , machida  _ et  al . _",
    "@xcite studied redundant vm placement to protect a service from @xmath3 pm failures .",
    "bin  _ et  al . _",
    "@xcite also studied vm placement for @xmath3-survivability , and proposed a shadow - based solution for vms with heterogeneous resource demands .",
    "the above papers do not offer bandwidth guarantee for vms .",
    "our work utilizes a similar survivability mechanism as in  @xcite , where a predicted physical failure will trigger migration of the affected vms to its backup location .",
    "yet we consider bandwidth guarantee in addition to vm placement , which complicates the problem and differentiates our work from the above .    along another line ,",
    "many solutions focus on survivable service hosting using the virtual infrastructure ( vi ) abstraction  @xcite .",
    "a vi is a general graph , where each node or link may have a different resource demand , and an embedding is defined as two mappings : virtual node ( vm ) mapping and virtual link mapping ; the _ pipe model _",
    "is used in the vi abstraction instead of the hose model as in the vc abstraction .",
    "for example , yeow  _ et  al . _",
    "@xcite , yu  _ et  al . _",
    "@xcite and xu  _ et  al . _",
    "@xcite investigated survivable vi embedding through redundancy .",
    "they formulated the problem with various objectives and constraints , and designed heuristic algorithms . from a different angle , zhang  _ et  al . _",
    "@xcite proposed heuristic algorithms to embed vis based on the availability statistics of the physical components .",
    "the vi abstraction is more general than the vc abstraction , however , it is both hard to analyze theoretically and difficult to implement in large - scale networks due to its intrinsic model complexity  @xcite .",
    "an even more general model was proposed by guo  _ et  al . _",
    "@xcite , where a bandwidth requirement matrix is used to describe the bandwidth demand between each and every pair of virtual nodes ; it suffers from even higher model complexity than the vi abstraction .",
    "cloudmirror  @xcite proposes a tenant application graph model for bandwidth guarantee , and discusses a heuristic opportunistic solution for high - availability that balances between bandwidth saving and availability .",
    "bodik  _ et  al . _",
    "@xcite studied general service survivability in bandwidth - constrained data centers .",
    "based on the service characteristics of bing , they proposed an optimization framework and several heuristics to maximize fault - tolerance meanwhile minimizing bandwidth under the pipe model ( per - vm pair bandwidth demand ) .",
    "survivability has been studied extensively in conventional communication networks and optical networks  @xcite . existing work focuses on providing connectivity guarantee against network link and switch failures .",
    "the problem studied in this paper focuses on protecting tenant services from pm failures , which are different from link and switch failures .",
    "we study service provisioning in an iaas cloud environment , where the cloud offers services in the form of inter - connected vms .",
    "to request a service , the tenant submits its request in terms of both vms and network bandwidth .",
    "a cloud hypervisor processes requests in an online manner . for each request , the hypervisor first attempts to allocate enough resources in the data center .",
    "if the allocation succeeds , it then reserves the allocated resources and provisions the vc for the tenant .",
    "the vc will exclusively use all reserved resources until the end of its usage period , when the hypervisor will then revoke all allocated resources of the vc .",
    "if the allocation fails due to lack of resources , the hypervisor rejects the request .",
    "formally , each tenant request is defined as @xmath4 , where @xmath1 is the number of requested vms , and @xmath5 is the per - vm bandwidth demand .    following existing work  @xcite",
    ", we assume that the data center has a tree - structure topology .",
    "in fact , many commonly used data center architectures have tree - like structures ( fattree  @xcite , vl2  @xcite , etc . ;",
    "see section  [ sec : disc ] ) , where our proposed algorithms can be adopted with simple abstraction of the substrate .",
    "the substrate data center is defined as an undirected tree @xmath6 , where @xmath7 is the set of nodes , and @xmath8 is the set of physical links .",
    "the node set is further partitioned into two subsets @xmath9 , where @xmath10 is the set of pms that host vms , and @xmath11 is the set of abstract switches which perform networking functions . note that each abstract switch can represent a group of physical switches in the data center .",
    "each pm is a leaf node , while each switch is an intermediate node in the topology . without loss of generality",
    ", the substrate can be viewed as a rooted tree , and we pick a specific node @xmath12 as its root , which generally represents all core switches . for each node @xmath13 , we use @xmath14 to denote the subtree rooted at @xmath13 .",
    "we use @xmath15 to denote the out - bound link of @xmath14 , _",
    "i.e. _ , the link adjacent to node @xmath13 and on the shortest path from @xmath13 to global root @xmath16 .",
    "we use @xmath17 to denote the number of children of @xmath13 in the tree .    for each pm @xmath18 , we define @xmath19 as the number of available vm slots on @xmath20 . for each node @xmath21",
    ", we define @xmath22 as the available bandwidth on its out - bound link @xmath23 .",
    "to fulfill a request @xmath4 , the cloud needs to allocate @xmath1 vms with bandwidth guarantee in _ the hose model _",
    "given subtree @xmath14 , let @xmath24 be the number of vms allocated in @xmath14 , then the minimum bandwidth required on link @xmath23 is given by @xmath25 _",
    "i.e. _ , the bandwidth demand of vms either inside @xmath14 or outside @xmath14 , whichever is smaller . in other words , given link bandwidth @xmath22 , the number of vms allocated within @xmath14 , @xmath24 , must satisfy @xmath26 \\cup   [ n - { b_v \\over b } , n]\\ ] ] for simplicity of illustration , we define @xmath27 \\cup   [ n - { b_v \\over b } , n ] \\right ) \\cap [ 0 , n]$ ] as the _ feasible range of vms w.r.t .",
    "the bandwidth of node @xmath13 _ , and @xmath28 as its complement set over the universe @xmath29 $ ] .",
    "we also define @xmath30 as the lower bound of the upper feasible range , if @xmath31 .    given substrate @xmath6 and request @xmath4 , a * virtual cluster embedding ( vce ) * is defined by a vm allocation function , @xmath32 denoting the number of vms allocated on each host , which satisfies the following properties : + 1 ) @xmath33 for any @xmath18 , + 2 ) @xmath34 for any @xmath15 , where @xmath35 , and + 3 ) @xmath36 . @xmath37    note that bandwidth allocation @xmath38 is implicitly defined , as it can be computed based on vm allocation @xmath39 as in eq .  .",
    "[ fig : vcea ] shows an embedding of the tenant vc request @xmath40 on a @xmath41-level @xmath42-ary tree topology rooted at node @xmath43 , where pm @xmath44 has @xmath45 vm slots and pms @xmath42@xmath45 each has @xmath41 vm slots , and each link has bandwidth of @xmath46mbps .",
    "@xmath47 vms are allocated on pms @xmath44 , @xmath42 and @xmath41 to fulfill the request .",
    "note that link bandwidth constraints are satisfied based on eq .",
    ", as shown . for example",
    ", although the subtree rooted at switch @xmath48 contains @xmath49 working vms , only @xmath50mbps bandwidth is required on its out - bound link as shown .",
    "if a tenant request is accepted , it then exclusively uses all its allocated resources , including both vm slots @xmath51 for @xmath52 and bandwidth @xmath53 for @xmath54 .",
    "this ensures guaranteed resources to the tenant service , leading to predictable service performance . finding vce has been addressed in  @xcite .      existing work for vce does not consider service availability against physical failures .",
    "for example , when a pm fails , all services with vms hosted on it will be interrupted . moreover , due to lack of pre - provisioned backup resources , the cloud may not be able to recover the affected services in a short period of time . this will lead to violated service - level agreements , and further economic losses to both the tenants and the cloud .",
    "we use a pro - active survivability mechanism to improve service availability .",
    "the idea is to pre - reserve dedicated backup resources for each service , and pre - compute the recovery plan against any possible failure scenario , during the initial embedding process . during the life cycle of the service , a predicted physical failure will trigger the pre - determined automatic failover process , which will migrate the affected vms to their backups.this way , the interruption period of the service is minimized . note that while failures are frequent in the cloud , simultaneous pm failures are relatively rare  @xcite .",
    "hence we only focus on the single - pm failure scenario , where each failure is defined by the failed pm alone : @xmath55 .",
    "link and switch failures are not considered , as modern data centers typically have rich path diversity between any pair of pms  @xcite , which can effectively protect over these failures .",
    "to realize this mechanism , the key point is to reserve sufficient backup resources during the initial embedding process . specifically , the cloud needs to reserve both backup vm slots and backup bandwidth for the service . to characterize the total vm and bandwidth consumption , and the survivability guarantee of a vc",
    ", we define the following concept :    given substrate @xmath6 and request @xmath4 , a * survivable virtual cluster embedding ( svce ) * is defined by a tuple of allocation functions @xmath56 , with @xmath57 denoting the total number of vms allocated on each pm , and @xmath58 denoting the total bandwidth allocated on each link , such that during any single - pm failure @xmath55 , there still exists a _ vce _ of @xmath59 , in the auxiliary topology @xmath60 with resources on nodes and links defined as the remaining allocated resources , _",
    "i.e. _ , @xmath61 for @xmath62 and @xmath63 for @xmath64 .",
    "@xmath37    the above definition does not explicitly require a vce in the normal operation ( when no failure happens ) .",
    "such requirement is implicit , because after allocation , the vce for any failure scenario can be used in the normal operation .",
    "we call the vce used in the normal operation as the _ primary working set _ ( pws ) , and the vce used in failure @xmath65 as the _ recovery working set _ ( rws ) regarding @xmath65",
    ". we also use _ working vms _ to denote the set of vms that are used ( active ) in a specific scenario , compared to the set of _ backup vms _ that remain inactive .",
    "note that given the svce , both working sets can be easily computed using existing vce algorithms  @xcite .",
    "the cloud pre - computes these vces in all possible scenarios , hence when failure happens , the recovery process can quickly find the backup resources needed for each affected vm .    fig .",
    "[ fig : vcea ] shows an svce of the tenant request @xmath40 .",
    "compared to the vce which allocates exactly @xmath66 vms , in total @xmath67 vms are provisioned in the svce . during the failure of any pm , 1 ) the number of remaining vms is always no less than @xmath66 , and 2 ) a vce exists under the hose model ( with no more than @xmath45 , @xmath42 and @xmath41 vms on one side of the link pm @xmath44@xmath48 , the link pm @xmath41@xmath68 and any other link respectively ) .",
    "hence the given svce can always recover the requested vc during any failure .",
    "note that we can assign the rws during arbitrary failure as the pws . in this example , the dark blue vm slots on pm @xmath44 to @xmath41 are assigned as the pws , while the green vm slots are backups .",
    "the problem we study is to find the svce that uses minimum resources in the cloud .",
    "moreover , we are interested in finding the svce that occupies the minimum number of vm slots , in order to accommodate as many future requests as possible .",
    "formally , we study the following optimization problem :    given substrate @xmath6 and request @xmath4 , the * survivable virtual cluster embedding problem ( svcep ) * is to find an svce of request @xmath59 that consumes the minimum number of vm slots in the substrate @xmath69 .",
    "@xmath37    the necessity of resource optimization is illustrated in figs .",
    "[ fig : vcea ] and  [ fig : vcea ] . while fig .",
    "[ fig : vcea ] indeed shows an svce of @xmath40 , it consumes @xmath45 backup vms , due to that a single failure at pm @xmath44 will affect @xmath45 vms . on the contrary , fig .",
    "[ fig : vcea ] shows a different svce that consumes only @xmath41 backup vm slots , and is optimal regarding total vm slots consumption . with",
    "less consumed resources , the data center can accept more tenant requests in the future .",
    "note that although we focus on minimizing vm consumption , our proposed algorithms can be extended to minimize bandwidth as well ; see section  [ sec : disc ] . in the next two sections",
    ", we will present our proposed algorithms for solving svce .",
    "we start from designing an algorithm that solves svcep optimally .",
    "the algorithm works in a bottom - up manner : starting from the leaf nodes up to the root , the algorithm progressively determines the minimum number of total vms needed in the subtree rooted at each node , each time solving a generalized problem of svcep .",
    "formally , define the following generalization of svcep :    given substrate @xmath6 , request @xmath4 , an arbitrary node @xmath21 , and two nonnegative integers @xmath70 $ ] and @xmath71 $ ] , the generalized problem * svcep - gp * seeks to find the minimum number of vms needed in @xmath14 , to ensure that @xmath14 can provide _ at least _",
    "@xmath72 working vms when no failure happens in @xmath14 , and _ at least _",
    "@xmath73 working vms during arbitrary ( single - pm ) failure in @xmath14 .",
    "@xmath37    both @xmath72 and @xmath73 concern not only the vm slots that can be offered by each child subtree of node @xmath13 , but also the node s out - bound bandwidth @xmath22 . in other words",
    ", there exists a feasible solution to svcep - gp with @xmath13 , @xmath72 and @xmath73 if and only if there exist two integers @xmath74 and @xmath75 , such that @xmath76 and all child subtrees of @xmath13 can jointly offer exactly @xmath77 and @xmath78 vms in the normal and worst - case failure scenarios ( failure resulting in minimum number of available vms ) respectively .    note that given @xmath13 , @xmath72 and @xmath73 , a feasible solution of svcep - gp does not guarantee that the subtree @xmath14 can provide _ exactly _ @xmath72 vms if no failure happens in @xmath14 , or @xmath73 vms if arbitrary failure happens in @xmath14 .",
    "it only requires that @xmath14 can offer _ at least _ @xmath72 or @xmath73 vms in either scenario respectively .",
    "for example , a subtree with out - bound bandwidth of @xmath79mbps can offer @xmath80 vms for request @xmath40 if its child subtrees can jointly offer @xmath80 vms , but can not offer exactly @xmath81 vms due to lack of bandwidth in the hose model .",
    "however , as we will prove in the next subsection , the optimal solution to svcep - gp with @xmath82 and @xmath83 yields an optimal solution to the original svcep , and vice versa .    utilizing the above subproblem structure , we propose the following dynamic programming ( dp ) algorithm to compute the optimal solution by solving a sequence of svcep - gp instances . define @xmath84 $ ] as the optimal solution to svcep - gp on node @xmath13 , with non - negative integers @xmath72 and @xmath73 .",
    "the values are computed for pms and switches as follows : * pm computation : * for leaf node @xmath18 , we have @xmath85 =   \\left\\ { { \\begin{array}{*{20}{l } }      { n_0 }        & { \\text{if } n_1 = 0 , n_0 \\in [ 0 , c_h ] \\cap \\lambda_h}\\\\      { \\lambda_h } & { \\text{if } n_1 = 0 , n_0 \\in \\overline \\lambda_h , \\lambda_h \\le c_h }",
    "\\\\      \\infty       & { \\text{otherwise } } \\end{array } } \\right.\\ ] ] _ explanation _ : the number of working vms that a pm can offer is bounded by three factors : the number of available slots @xmath19 , the out - bound bandwidth @xmath86 , and the requested vm number @xmath1 . in the normal operation ,",
    "the minimum number of vms to offer @xmath72 working vms is equal to @xmath72 , if @xmath72 is in the feasible range bounded by bandwidth @xmath86 .",
    "recall that @xmath87 ( @xmath28 ) defines the feasible ( infeasible ) range of working vms in @xmath14 w.r.t .",
    "bandwidth , and @xmath88 is the lower bound of the upper range of @xmath87 . if @xmath89 , the pm can not offer exactly @xmath72 vms due to the bandwidth limit ; however , if there are at least @xmath90 available slots , the pm can offer @xmath90 vms , which guarantees _ at least _",
    "@xmath72 vms in the pws with the minimum total vms .",
    "note that @xmath73 always equals @xmath91 , as when this pm fails , no vm can be offered .",
    "all other entries are @xmath92 , meaning such instances are infeasible .",
    "* switch computation : * the computation for a switch node is more complicated , as there are exponential number of ways to write an integer value ( @xmath72 or @xmath73 ) as the sum of @xmath93 integer values , where @xmath93 is the number of children of node @xmath13 .",
    "however , we observe that the allocation in each subtree is independent from the other subtrees .",
    "hence we employ another level of dynamic programming to aggregate results from child subtrees .",
    "define @xmath94 $ ] as the minimum number of total vms in @xmath14 , to ensure that @xmath14 can provide at least @xmath72 working vms in the normal operation , and at least @xmath73 working vms during arbitrary failure in @xmath14 , _ using the first @xmath3 subtrees of @xmath14 _ , where @xmath95 .",
    "note that @xmath94 $ ] does not consider the out - bound bandwidth @xmath22 of node @xmath13 .",
    "we first establish the relationship between @xmath96 and @xmath97 as : @xmath98 =   \\left\\ { { \\begin{array}{*{20}{l } }      { n'_v[n_0 , n_1 , d_v ] }                & { \\text{if } n_0 , n_1 \\in \\lambda_h } \\\\      { n'_v[\\lambda_v , n_1 , d_v ] }      & { \\text{if } n_0 \\in \\overline \\lambda_h , n_1 \\in \\lambda_h } \\\\      { n'_v[n_0 , \\lambda_v , d_v ] }      & { \\text{if } n_0 \\in \\lambda_h , n_1 \\in \\overline \\lambda_h } \\\\      { n'_v[\\lambda_v , \\lambda_v , d_v ] }    & { \\text{if } n_0 , n_1 \\in \\overline \\lambda_h } \\\\ \\end{array } } \\right.\\ ] ] for each switch node @xmath99 and @xmath100 .    _ explanation _ : based on their definitions , @xmath101 $ ] and @xmath102 $ ] only differ in that the latter does not consider the out - bound bandwidth at node @xmath13 .",
    "hence we apply the bandwidth constraints to obtain @xmath101 $ ] from @xmath97 : if @xmath22 can not support @xmath72 ( @xmath73 ) working vms in @xmath14 , then we take the minimum value @xmath88 that both can be supported by @xmath22 and is at least @xmath72 ( @xmath73 ) as desired .",
    "note that both @xmath84 $ ] and @xmath103 $ ] are non - decreasing in either @xmath72 or @xmath73 based on definition .",
    "hence the above defined @xmath84 $ ] is optimal given the optimality of all its dependent @xmath104 $ ] values , for @xmath105 and @xmath106 .    *",
    "inner dp : * the value of @xmath107 $ ] is computed from @xmath108 to @xmath93 .",
    "the initial iteration where @xmath108 is computed as : @xmath109 =   \\left\\ { { \\begin{array}{*{20}{l } }      { 0 } & { \\text{if } n_0 = n_1 = 0}\\\\      \\infty & { \\text{otherwise } } \\end{array } } \\right.\\ ] ] since only @xmath91 vms can be offered using @xmath91 subtrees . based on this ,",
    "each of the other iterations computes @xmath107 $ ] based on the values @xmath110 $ ] computed in the @xmath111-th iteration as well as the values @xmath112 $ ] computed for the @xmath3-th child node @xmath113 of node @xmath13 , as shown in eq .  .",
    "@xmath114 = { \\min\\limits_{\\substack{n_0 ' , n_0 '' , \\\\ n_1 ' , n_1 '' } } \\left\\ { n'_v[n_0 ' , n_1 ' , k-1 ] + n_{u_k}[n_0 '' , n_1 '' ]   \\left|\\ , \\begin{array}{*{20}{l } } { \\ }     \\\\ { \\ } \\end{array } \\right .",
    "\\right . }",
    "\\\\   { \\left .",
    "\\begin{array}{*{20}{l } } { n_0 ' + n_0 '' \\ { \\ge } \\",
    "{ \\min \\ { n_0 ' + n_1'',n_0 '' + n_1 ' \\ }   \\ { \\ge } \\   { n_1 } }",
    "\\end{array } \\right\\ } } \\ ] ]    _ explanation _ : the computation of @xmath103 $ ] iterates over four variables : @xmath115 , @xmath116 , @xmath117 and @xmath118 .",
    "the rationale is that , either in the normal operation or during arbitrary failure , the required working vms in the first @xmath3 subtrees of @xmath14 can be split into two parts : the vms in the first @xmath111 subtrees , and the vms in the @xmath3-th subtree .",
    "moreover , since we assume that only single pm failure can happen at any time , once the failure happens within one part , the other part is assured to work in the normal operation .",
    "hence we know how the required working vms @xmath72 and @xmath73 should be split between the two parts .",
    "specifically , we have @xmath119 in the normal operation ; the failure can happen in either the first or the second part , and the worst - case failure is the one that results in fewer working vms : @xmath120 .",
    "algorithm  [ a:1 ] shows the whole procedure of the dynamic programming .",
    "the algorithm first computes all the entries of @xmath84 $ ] in a bottom - up manner .",
    "the order of computation guarantees that during the computation of an entry in either @xmath96 or @xmath97 , all its depending entries have already been computed in previous iterations .",
    "after computation , if the value @xmath121 $ ] is feasible , the algorithm then backtracks the dp process to obtain the exact vm and bandwidth allocations in each subtree .",
    "vm allocation can be determined by recording the path towards each entry in the table during dp , while bandwidth can be determined based on @xmath72 and @xmath73 due to eq .  .",
    "the following theorems deliver the optimality and the running time of our proposed algorithm .",
    "for clarity of presentation , we refer readers to the appendix for rigorous proofs of the theorems and lemmas in this subsection .",
    "[ l:1 ] given an allocation of vms in any subtree @xmath14 for @xmath4 , if the subtree can offer @xmath122 $ ] working vms in a scenario , then it can offer any number of working vms less than or equal to @xmath123 in the same scenario without increasing bandwidth on any link",
    ". @xmath37    [ l:2 ] given an allocation of vms in any subtree @xmath14 for @xmath4 , if the subtree can offer _ more than _",
    "@xmath1 working vms in a scenario , then it can offer _ exactly _ @xmath1 vms in the same scenario , without increasing bandwidth on any link .",
    "@xmath37    [ th:1 ]",
    "given an instance of svcep , algorithm  [ a:1 ] returns the optimal solution if the instance is feasible , and returns `` infeasible '' otherwise .",
    "@xmath37    [ th:2 ] the worst - case time complexity of algorithm  [ a:1 ] is bounded by @xmath124 , where @xmath125 is the network size and @xmath1 is the request size ( the number of vms requested ) .",
    "@xmath37    based on theorem  [ th:1 ] , the solution is guaranteed to offer a feasible vce of @xmath4 using the allocated resources when facing any single pm failure . to find the rws for each failure",
    ", one can apply existing vce algorithms  @xcite in the auxiliary topology where vm slots and bandwidth are the same as allocated except for the failed pm . as mentioned in section  [ sec : surv ] , the rws of any failure can be used as the pws .",
    "the algorithm proposed in section  [ sec : a ] optimally solves svcep",
    ". however , its worst - case time complexity can be as high as @xmath126 , which may be too expensive when a tenant asks for many vms . in this section",
    ", we propose an efficient heuristic algorithm that runs in @xmath127 time .    before the algorithm",
    ", we first state the following lemma , whose proof is also detailed in the appendix :    [ l : heu ] given substrate @xmath69 , request @xmath4 , and an integer @xmath128 $ ] , a vce of the augmented request @xmath129 yields a feasible svce of @xmath59 as long as each pm is allocated with no more than @xmath130 vms in the vce .",
    "@xmath37    based on lemma  [ l : heu ] , we design a heuristic algorithm shown in algorithm  [ a:2 ] , based on the algorithm in  @xcite .",
    "the algorithm iterates from @xmath131 to @xmath1 , each time calling @xmath132 to find a feasible vce for the augmented request @xmath129 .",
    "if such feasible embedding is found at a specific value of @xmath130 , the total number of vms provisioned is exactly @xmath133 , hence yielding a solution with minimum vms under this algorithm .",
    "the algorithm stops when no solution is found in @xmath1 iterations , because the number of backup vms should not exceed the number of requested vms .",
    "the subroutine @xmath132 uses the algorithm in  @xcite to find a vce for @xmath134 .",
    "minor modification is done to enforce the per - pm vm limit @xmath130 .",
    "due to page limit , we only briefly introduce the idea of the algorithm .",
    "for each node , the algorithm uses a data structure called the _ allocation range _ ( ar ) to record the number of vms that can be allocated within its subtree .",
    "the ar consists of @xmath135 bits for request @xmath134 , where the @xmath136-th bit is @xmath44 if the subtree can offer @xmath137 vms , and @xmath91 otherwise .",
    "continuous @xmath44 bits are aggregated into sections defined by both end - points .",
    "for example , a section @xmath138 $ ] means that the subtree can offer @xmath81 , @xmath80 or @xmath49 vms .",
    "the algorithm progressively computes the ar of every node , from leaves to root .",
    "it then finds the lowest subtree that can offer @xmath133 vms , and makes allocation through backtracking .",
    "the algorithm in  @xcite finds a vce within @xmath139 time .",
    "algorithm  [ a:2 ] calls @xmath132 for at most @xmath1 times , hence it has time complexity @xmath140 .",
    "algorithm  [ a:2 ] does not guarantee optimality .",
    "in fact , we can construct simple examples for which it fails to find an svce , yet one with the optimal objective can be found by our optimal algorithm . due to page limit , we omit the examples here .",
    "however , as shown in section  [ sec : eval ] , this heuristic algorithm has similar performance to the per - request optimal solution proposed in section  [ sec : a ] when working in the online manner , but is several orders more time - efficient .",
    "therefore it is practically important for providing fast response to tenants in the cloud .",
    "_ shadow - based solution _ ( sbs ) is a well - known failover provisioning solution for vm management  @xcite . in sbs , each primary vm is protected by a dedicated backup vm ( called _ shadow _ ) .",
    "different primary vms do not share any common backup vm . to employ sbs for vcs ,",
    "both vms and bandwidth need to be shadowed .",
    "we designed a heuristic bandwidth - aware algorithm for sbs as our baseline algorithm .",
    "it works as follows : given a request @xmath4 , the algorithm seeks to find one primary vce , as well as one shadow vce , on two disjoint sets of pms respectively .",
    "when making the primary vce , the algorithm seeks to minimize the pms used , using a modified algorithm as in  @xcite , therefore leaving more room for the shadow .",
    "a request is accepted only when the network accommodates both the primary vce and the shadow .",
    "we compared our proposed algorithms ( opt for the optimal algorithm and heu for the heuristic algorithm ) to this baseline algorithm ( sbs ) to show how resources are conserved to serve more requests by our optimization algorithms .      *",
    "_ acceptance ratio _ is the number of fulfilled requests over total requests , which directly reflects an algorithm s capability in serving as many requests as possible . * _ average vm consumption ratio _ is defined as the average ratio of actual vm slot consumption over the requested vms , namely @xmath141 , for each request .",
    "note that this only counts those requests accepted by all three algorithms , in order to make fair comparison . *",
    "_ average running time _ reflects how much time an algorithm spends in average to determine a solution ( or rejection ) of each incoming request .",
    "we developed a c++-based simulator to evaluate our proposed algorithms .",
    "the substrate was simulated as a @xmath45-layer @xmath47-ary tree , including the pms .",
    "each pm has @xmath81 vm slots , and @xmath47 pms are connected to a top - of - rack ( tor ) switch each via a @xmath44gbps link .",
    "tor switches are connected to one aggregation switch , and @xmath47 aggregation switches to the core , both via @xmath142gbps links .",
    "we conducted experiments in two scenarios : the static scenario and the dynamic scenario . in the static scenario",
    ", we used the same network information and the same tenant request in each experiment ; hence no resource was reserved after the acceptance of a request . to simulate realistic network states ,",
    "we randomly generated load on pms and links . specifically , given a load factor @xmath143 , we randomly occupied a fraction of the vm slots on each pm and bandwidth on each link , according to a normal distribution with mean of @xmath143 and standard deviation of @xmath144 .",
    "we then randomly generated @xmath145 tenant requests each requesting @xmath146 vms and @xmath79mbps per - vm bandwidth on average with a normal distribution , and tested each of them on the network with random load .",
    "in the dynamic scenario , we generated randomly arriving tenant requests , and embedded them in the initially unoccupied network in an online manner . in each experiment @xmath145",
    "tenant requests were generated , which arrive in a poisson process with mean arrival interval of @xmath146 and mean lifetime of @xmath147 .",
    "each request asks for @xmath146 vms and @xmath148mbps per - vm bandwidth on average , generated with a normal distribution .",
    "resources were reserved after the acceptance of a request , hence existing vcs in the system would have impact on the embedding of future incoming vcs .",
    "each experiment was repeated for 20 times in the same setting , and the results were averaged over all runs . in both scenarios , we varied one system parameter in each series of experiments , while keeping other parameters as default .",
    "experiments were run on a ubuntu linux pc with quad - core 3.4ghz cpu and 16 gb memory .",
    "[ fig : load ] shows the acceptance ratio and the vm consumption ratio with increasing network load ( overall bandwidth consumption is similar to fig .",
    "[ fig : load ] and is not shown due to page limit ) .",
    "we observed that the opt algorithm outperforms both heu and sbs in terms of both number of requests accepted and the per - request vm consumption ratio , due to its optimality . on the other hand ,",
    "[ fig : load ] shows that the heu algorithm performs less preferably than the sbs baseline in terms of acceptance ratio , when the network is loaded .",
    "further analysis reveals that heu commonly requires more bandwidth from upper layer links , which are heavily congested by the random load , hence heu s acceptance ratio is affected .",
    "however , as can be observed in fig .",
    "[ fig : load ] , heu consumes much less vms than sbs per accepted request . due to this , it is more likely for heu to receive better performance when employed as an online scheduler , due to its capability in conserving cloud resources .",
    "as will be shown next , heu indeed outperforms sbs greatly in the dynamic experiments .",
    "sbs always consumes @xmath149 the vms requested , as it provisions an entire duplicate of the primary vms . per - request",
    "vm consumption increases slightly with the increasing load , due to that it is harder to find a survivable embedding with few backup vms when the network is short of bandwidth .",
    "[ fig : vms ] shows the experiment results with varying average number of requested vms per tenant request .",
    "opt obviously achieves the best acceptance ratio in all scenarios , while heu s acceptance ratio is only slightly lower than opt .",
    "both algorithms have much higher acceptance ratio compared to the sbs baseline , due to their capability to conserve vm ( and bandwidth ) resources per tenant request .",
    "meanwhile , heu has much shorter running time than opt in all cases due to its low time complexity , and is only a little worse than sbs in most scenarios . as each tenant asking for more vms , acceptance ratio drops while running time increases",
    "this is due to that the running time of all algorithms are related to the per - tenant request size @xmath1 .",
    "[ fig : dmd ] shows the experiment results with varying average per - vm bandwidth .",
    "the acceptance ratio results show similar pattern as in fig .",
    "[ fig : vms ] , where opt performs the best , heu performs slightly worse , and sbs performs much worse compared to the former two .",
    "acceptance ratio drops as per - vm bandwidth increases . as for running time",
    ", clearly heu and sbs are both much better than opt due to their low complexity . unlike fig .",
    "[ fig : vms ] , running time drops as per - vm bandwidth increases .",
    "it has two reasons .",
    "first , the worst - case time complexity of each algorithm is not related to the per - vm bandwidth .",
    "second , as per - vm bandwidth increases , the search spaces decrease due to more consumed network resources .    in the last set of experiments",
    ", we varied the network size .",
    "each topology is a @xmath45-level @xmath3-ary tree , which has @xmath150 pms , and @xmath151 switches .",
    "we varied @xmath3 from @xmath81 to @xmath142 . with a larger network size ( and thus more vms and bandwidth ) ,",
    "the acceptance ratio of all algorithms increases in fig .  [ fig : ary ] . opt and heu both outperform sbs due to resource conservation .",
    "the running time of all algorithms increase with the tree ary number in fig .",
    "[ fig : ary ] .",
    "as the network itself grows linearly in the logarithmic scale , all algorithms show linear or nearly linear running time growth .",
    "we summarize our findings as follows :    1 .",
    "opt guarantees per - request optimality , and has the best performance in both static and dynamic scenarios ; heu shows low acceptance ratio in the static case , but has much higher acceptance ratio in the dynamic case due to resource conservation ; sbs consumes too much resources and hence performs the worst in the dynamic case .",
    "2 .   compared to opt , heu has much better time efficiency , which is a great advantage in practice ; however , opt is still important when 1 ) tenant requests are small in general , 2 ) cloud resources are very scarce , or 3 ) future researches along the same line need to compare with a theoretically ( per - request ) optimal solution .",
    "* resource optimization : * our current solutions focus on minimizing the number of backup vms .",
    "however , they can be extended to other objectives , such as minimization of total bandwidth .",
    "specifically , instead of the minimum number of vms , the minimum bandwidth to achieve a specific @xmath152 pair is computed for each node .",
    "the aggregation process incorporates the bandwidth consumed both in lower levels and in the current level .",
    "we omit more details due to page limit .",
    "* simultaneous pm failures : * our proposed algorithms protect from any single pm failure in the substrate .",
    "they can be extended to cover multiple simultaneous pm failures , at the cost of exponentially increased time complexity regarding the number of failures to be covered .",
    "specifically , the extension involves adding @xmath153 dimensions into the dynamic programming , where @xmath154 is the number of covered simultaneous failures . as our future work , more efficient algorithms for covering multiple simultaneous pm failures are to be developed .",
    "-ary fattree ( top).,scaledwidth=48.0% ]    * data center topologies : * as aforementioned , our solutions can be applied to generic tree - like topologies with simple abstractions . to support our argument ,",
    "an example is shown for the widely adopted fattree topology  @xcite in fig .",
    "[ fig : fattree ] .",
    "a @xmath45-ary fattree topology is shown on the top , which is then abstracted as the virtual tree topology on the bottom .",
    "switches or links connected to the same set of lower layer nodes are aggregated into a single abstract switch or link ; link capacities are also aggregated .",
    "as the majority of data center traffic consists of small flows , we can assume arbitrary splitting of traffic between different vm pairs ; hence any bandwidth allocation feasible on the aggregated topology can be successfully configured on the original topology as well .",
    "other topologies feasible for adopting such abstraction include vl2  @xcite and other multi - rooted tree - based topologies .",
    "survivable vce for more general data center topologies is among our future directions .",
    "in this paper , we studied survivable vc embedding with hose model bandwidth guarantee .",
    "we formally defined the problem of minimizing vm consumption for providing survivability guarantee . to solve the problem , we proposed a novel dynamic programming - based algorithm , with worst - case time complexity polynomial in the network size and the number of requested vms .",
    "we proved the optimality of our algorithm and analyzed its time complexity .",
    "we also proposed an efficient heuristic algorithm , which is several orders faster than the optimal algorithm .",
    "simulation results show that both proposed algorithms can achieve much higher acceptance ratio compared to the baseline solution in the online scenario , and our heuristic algorithm can achieve similar performance as the optimal with much faster computational speed .",
    "p.  yalagandula , s.  nath , h.  yu , p.",
    "b. gibbons , and s.  seshan , `` beyond availability : towards a deeper understanding of machine failure characteristics in large distributed systems , '' in _ usenix worlds _ , 2004 .",
    "yeow , c.  westphal , and u.  c. kozat , `` designing and embedding reliable virtual infrastructures , '' in _ acm visa _ , 2010 .",
    "w.  zhang , g.  xue , j.  tang , and k.  thulasiraman , `` faster algorithms for construction of recovery trees enhancing qop and qos , '' _ ieee / acm trans .",
    "_ , 16(3 ) : 642655 , 2008 .",
    "q.  zhang , m.  f. zhani , m.  jabri , and r.  boutaba , `` venice : reliable virtual data center embedding in clouds , '' in _ ieee infocom _ , 2014 .        preliminarily , according to eq .",
    ", a subtree offering at most @xmath155 working vms can reduce its working vms without increasing bandwidth on its out - bound link , and since such subtree only has child offering at most @xmath155 working vms , such reduction does not increase link bandwidth within the subtree as well .",
    "first , as @xmath14 can offer @xmath156 vms , its bandwidth allocation satisfies @xmath157 . hence offering @xmath123 or less working vms will not increase bandwidth on @xmath14 s out - bound link .",
    "if @xmath13 is a pm , then we can directly reduce the number of working vms on @xmath13 from @xmath24 to @xmath158 . then , assume the lemma holds for all nodes lower than level @xmath159 in the tree , and let @xmath13 be a switch on level @xmath159 .",
    "if for any child @xmath160 of @xmath13 , the number of working vms that it offers @xmath161 , then we can reduce the number of working vms on any pm in @xmath14 without increasing bandwidth needed on any link . now if some child @xmath160 of @xmath13 has @xmath162 working vms . since @xmath163 , there is at most one such child .",
    "first , we reduce working vms in all other child subtrees to none .",
    "then , by induction from the @xmath164-th level , the number of working vms on @xmath160 can be reduced from @xmath165 to @xmath166 , which is no less than @xmath167 , without increasing bandwidth on any link .",
    "now , since @xmath168 , we can reduce the number of working vms in @xmath169 without increasing bandwidth on any link , which completes the proof .    to start with",
    ", this always holds for any pm node @xmath170 by simply reducing the number of vms .",
    "if subtree @xmath14 can offer more than @xmath1 working vms , each child subtree @xmath169 of @xmath13 is one of the three cases : 1 ) @xmath169 can offer more than @xmath1 working vms , 2 ) @xmath169 can offer working vms in @xmath171 $ ] , or 3 ) @xmath169 can offer no more than @xmath155 working vms . if there is any subtree @xmath169 in case 1 , then we let all pms not in @xmath169 to have @xmath91 working vms , and continue to prove the lemma on @xmath169 , until we reach any pm node . otherwise ,",
    "if there are at least two subtrees @xmath169 and @xmath172 both in case 2 , then we also let all pms not in these two subtrees to have @xmath91 working vms . without loss of generality ,",
    "assume @xmath169 can offer @xmath165 working vms , @xmath172 can offer @xmath173 working vms , and @xmath174 . by lemma  [ l:1 ] , @xmath172 can also offer @xmath175 working vms .",
    "hence we can let @xmath172 offer @xmath176 working vms , and let @xmath169 offer @xmath165 , the sum of which is exactly @xmath1 .",
    "finally , if there is at most one subtree @xmath169 in case 2 , we can always reduce the total number of working vms not hosted in @xmath169 ( or in all subtrees if @xmath169 does not exist ) to @xmath177 , without increasing bandwidth needed on any link .",
    "this completes the proof .",
    "first , we show that any feasible solution to svcep is feasible to svcep - gp with @xmath82 and @xmath83 , and vice versa .",
    "the forward direction is true based on their problem definitions . on the reverse direction , by lemma  [ l:2 ] , a feasible solution to svcep - gp with @xmath82 and @xmath83 is always able to provide exactly @xmath1 working vms in any scenario .",
    "this proves the equivalence between svcep and svcep - gp with @xmath82 and @xmath83 .",
    "we then prove by induction that svcep - gp has been solved optimally in algorithm  [ a:1 ] .",
    "for pms , each entry is computed directly and is optimal based on definition . for each switch",
    "node @xmath13 , @xmath84 $ ] is dependent on the four possible values computed in the @xmath97 table as in eq .  .",
    "based on definition , if @xmath102 $ ] is correctly computed for every @xmath72 and @xmath73 , @xmath101 $ ] is correct in applying the bandwidth bound of @xmath14 based on eq .  .",
    "as for @xmath94 $ ] , initially when @xmath108 , the values are computed directly based on definition .",
    "then , assume that @xmath178 $ ] and @xmath179 $ ] are correctly computed for any @xmath180 $ ] respectively , where @xmath113 is the @xmath3-th child of node @xmath13 .",
    "the corresponding optimal vm allocation to the problem of @xmath94 $ ] can be split into two parts : @xmath181 in the first @xmath111 subtrees of @xmath13 , and @xmath182 in the @xmath3-th subtree .",
    "assume that the first part can offer @xmath183 and @xmath184 vms in the normal operation and the worst - case failure scenario respectively , and the second can offer @xmath185 and @xmath186 respectively .",
    "based on definition , @xmath181 and @xmath182 are feasible solutions to the problems @xmath187 $ ] and @xmath188 $ ] respectively , and @xmath189 as well as @xmath190",
    ". now assume @xmath94 $ ] is not optimal .",
    "this means that @xmath191 \\le n_v ' [ n_0^ * , n_1^ * , k-1 ] + n_{u_k } [ n_0^ { * * } , n_1^{**}]$ ] , the righthand inequality due to the algorithm .",
    "hence we have either @xmath192 $ ] or @xmath193 $ ] , contradicting the optimality of @xmath187 $ ] or @xmath188 $ ] .",
    "this completes the proof .",
    "each node in the tree is traversed once during the dp process . at each pm node",
    ", each entry indexed by @xmath72 and @xmath73 needs @xmath194 time due to eq .",
    ", hence each pm takes @xmath195 times . for each switch node ,",
    "its computation iterates over its every child @xmath13 , and each entry indexed by @xmath13 , @xmath72 and @xmath73 needs to iterate over up to @xmath45 variables , @xmath196 , hence each switch node @xmath13 takes @xmath197 complexity .",
    "summing them all up , the final complexity of the dp computation process is @xmath124 .",
    "the time for backtracking is bounded by the dp computations .",
    "this completes the proof , which shows that the time complexity of algorithm  [ a:1 ] is polynomial to the network size @xmath125 and the request size @xmath1 .",
    "we need to prove that the vce of @xmath134 can provide exactly @xmath1 bandwidth - guaranteed vms in any failure @xmath55 .",
    "let @xmath24 be the number of vms allocated in subtree @xmath14 in the vce ; also let @xmath53 be the hose model bandwidth of the vce on link @xmath23 as specified in eq .  .",
    "first , since each pm hosts at most @xmath130 vms , at least @xmath1 vms remain during the failure .",
    "now , let @xmath198 be the path from root @xmath16 to pm @xmath65 , with @xmath199 and @xmath200 .",
    "by cutting the @xmath201 links @xmath202 for @xmath203 , the topology is partitioned into @xmath204 components , and we denote the component containing node @xmath205 as @xmath206 .",
    "@xmath207 contains only the failed pm @xmath65 , hence is discarded . for any link @xmath23 within each component ,",
    "its allocated bandwidth satisfies @xmath208 , hence each component can offer all the allocated vms within it for request @xmath59 , if not considering bandwidth on the cut links .",
    "if any component @xmath206 ( viewed as a subtree rooted at @xmath205 ) can offer at least @xmath1 vms , by lemma  [ l:2 ] it can offer exactly @xmath1 vms in this scenario .",
    "otherwise , let @xmath11 be an initially empty set of vms , we then proceed with each component @xmath206 from @xmath108 to @xmath209 , each time adding all allocated vms within @xmath206 to @xmath11 , until _ at least _",
    "@xmath1 vms are added .",
    "assume we stop at @xmath210 , where @xmath211 $ ] .",
    "@xmath11 now contains @xmath212 vms , as both the first @xmath213 components and the @xmath154-th component contain less than @xmath1 vms . for node @xmath205 where @xmath214 , let @xmath215 be the number of vms added before @xmath206 , then its out - bound link @xmath216 has bandwidth allocation @xmath217 .",
    "hence each link @xmath218 is sufficient to offer all vms in the first @xmath111 components , for @xmath214 .",
    "then we form a new tree @xmath219 by pruning all components after @xmath210 , and assigning @xmath220 as the root of @xmath219 .",
    "every link in @xmath219 is either within some component @xmath206 , or one of the links @xmath218 , where @xmath214 , hence its bandwidth is sufficient as above .",
    "now , @xmath219 can offer @xmath221 working vms , hence by lemma  [ l:2 ] it can offer exactly @xmath1 working vms . therefore , there is a feasible vce for @xmath59 in the given vce of @xmath134 in any scenario , and the lemma follows ."
  ],
  "abstract_text": [
    "<S> cloud computing has emerged as a powerful and elastic platform for internet service hosting , yet it also draws concerns of the unpredictable performance of cloud - based services due to network congestion . to offer predictable performance , </S>",
    "<S> the virtual cluster abstraction of cloud services has been proposed , which enables allocation and performance isolation regarding both computing resources and network bandwidth in a simplified virtual network model . </S>",
    "<S> one issue arisen in virtual cluster allocation is the survivability of tenant services against physical failures . </S>",
    "<S> existing works have studied virtual cluster backup provisioning with fixed primary embeddings , but have not considered the impact of primary embeddings on backup resource consumption . to address this issue , </S>",
    "<S> in this paper we study how to embed virtual clusters survivably in the cloud data center , by jointly optimizing primary and backup embeddings of the virtual clusters . </S>",
    "<S> we formally define the survivable virtual cluster embedding problem . </S>",
    "<S> we then propose a novel algorithm , which computes the most resource - efficient embedding given a tenant request . since the optimal algorithm has high time complexity , we further propose a faster heuristic algorithm , which is several orders faster than the optimal solution , yet able to achieve similar performance . </S>",
    "<S> besides theoretical analysis , we evaluate our algorithms via extensive simulations .    </S>",
    "<S> = 1    virtual cluster , survivability , bandwidth guarantee </S>"
  ]
}