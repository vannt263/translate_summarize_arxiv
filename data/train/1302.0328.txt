{
  "article_text": [
    "shannon s discrete entropy appears as a basic statistic in many fields , from probability theory to engineering and even ecology .",
    "while entropy may best be known as a theoretical quantity , its accurate estimation from data is an important step in disparate applications .",
    "entropy is employed in the study of information processing in neuroscience  @xcite .",
    "it is also used in statistics and machine learning for estimating dependency structure and inferring causal relations  @xcite , for example in molecular biology  @xcite ; as a tool in the study of complexity and dynamics in physics @xcite ; and as a measure of diversity in ecology @xcite and genetics  @xcite .",
    "each of these studies , confronted with data arising from an unknown discrete distribution , seeks to estimate the entropy rather than the distribution itself . estimating the entropy is much easier than estimating the full distribution .",
    "in fact , in many cases , entropy can be accurately estimated with fewer samples than the number of distinct symbols .",
    "however , entropy estimation remains a difficult problem : there is no unbiased estimator for entropy , and the maximum likelihood estimator is severely biased for small datasets .",
    "many previous studies have focused upon methods for computing and reducing this bias @xcite . in this paper",
    "we instead take a bayesian approach , building upon the work of @xcite .",
    "our basic strategy is to place a prior over the space of discrete probability distributions , and then perform inference using the induced posterior distribution over entropy .",
    "( see fig .",
    "[ fig : graphicalmodel ] ) .",
    "we focus here on the under - sampled regime , where the number of unique symbols observed in the data is small in comparison with the unknown ( perhaps countably infinite ) number of possible symbols .",
    "the pitman - yor process ( pyp ) , a two - parameter generalization of the dirichlet process ( dp ) @xcite , provides an attractive family of priors in this setting , since : ( 1 ) the posterior distribution over entropy has analytically tractable moments ; and ( 2 ) distributions drawn from a pyp can exhibit power - law tails , a feature commonly observed in data from social , biological , and physical systems  @xcite . we show that a pyp prior with fixed hyperparameters imposes a narrow prior distribution over entropy , leading to severe bias and overly narrow posterior credible intervals given a small dataset .",
    "our approach , inspired by  @xcite , is to introduce a family of mixing measures over pitman - yor processes such that the resulting pitman - yor mixture ( pym ) prior provides an approximately non - informative ( i.e. , flat ) prior over entropy .",
    "the remainder of the paper is organized as follows . in section [ sec :",
    "basics ] , we introduce the entropy estimation problem and review prior work . in section 3 , we introduce the dirichlet and pitman - yor processes and discuss key mathematical properties relating to entropy . in section 4 , we introduce a novel entropy estimator based on pym priors and derive several of its theoretical properties . in section 5 ,",
    "we show compare various estimators with applications to data",
    ".     indicated at bottom ) . for entropy estimation , the joint probability distribution over entropy @xmath0 , data @xmath1 , discrete distribution @xmath2 , and parameter @xmath3 factorizes as : @xmath4 .",
    "entropy is a deterministic function of @xmath5 , so @xmath6 the bayes least squares estimator corresponds to the posterior mean : @xmath7 = \\iint p(h|{{\\bm{\\pi } } } ) p({{\\bm{\\pi}}},\\theta|{{\\mathbf{x } } } )    d { { \\bm{\\pi}}}\\ , d\\theta$ ] . , width=192 ]",
    "consider samples @xmath8 drawn _ iid _ from an unknown discrete distribution @xmath9 on a finite or ( countably ) infinite alphabet @xmath10 , with cardinality @xmath11 , that is , @xmath12 .",
    "we wish to estimate the entropy of @xmath5 , @xmath13 we are interested in the under - sampled regime , @xmath14 , where many of the symbols remain unobserved .",
    "we will see that a naive approach to entropy estimation in this regime results in seriously biased estimators , and briefly review approaches for correcting this bias .",
    "we then consider bayesian techniques for entropy estimation in general before introducing the nsb method upon which the remainder of the article will build .",
    "perhaps the most straightforward entropy estimation technique is to estimate the distribution @xmath5 and then use the plugin formula to evaluate its entropy .",
    "the empirical distribution @xmath15 is computed by normalizing the observed counts @xmath16 of each symbol , @xmath17 for each @xmath18 . plugging this estimate for @xmath5 into , we obtain the so - called `` plugin '' estimator : @xmath19 which is also the maximum - likelihood estimator under discrete ( or multinomial ) likelihood . despite its simplicity and desirable asymptotic properties , @xmath20 exhibits substantial negative bias in the undersampled regime .",
    "there exists a large literature on methods for removing this bias , much of which considers the setting in which @xmath11 is known and finite .",
    "one popular and well - studied method involves taking a series expansion of the bias  @xcite and then subtracting it from the plugin estimate .",
    "other recent proposals include minimizing an upper bound over a class of linear estimators  @xcite , and a james - stein estimator  @xcite .",
    "recent work has also considered countably infinite alphabets .",
    "the coverage - adjusted estimator ( cae )  @xcite addresses bias by combining the horvitz - thompson estimator with a nonparametric estimate of the proportion of total probability mass ( the `` coverage '' ) accounted for by the observed data @xmath21 . in a similar spirit , @xcite proposed an estimator based on the good - turing estimate of population size .",
    "the bayesian approach to entropy estimation involves formulating a prior over distributions @xmath5 , and then turning the crank of bayesian inference to infer @xmath0 using the posterior distribution .",
    "bayes least squares ( bls ) estimators take the form : @xmath22 = \\int h({{\\bm{\\pi } } } ) p(h|{{\\bm{\\pi } } } ) p({{\\bm{\\pi}}}|{{\\mathbf{x}}})\\ , \\mathrm{d}{{\\bm{\\pi}}},\\ ] ] where @xmath23 is the posterior over @xmath5 under some prior @xmath24 and discrete likelihood @xmath25 , and @xmath26 since @xmath0 is deterministically related to @xmath5 . to the extent that @xmath24 expresses our true prior uncertainty over the unknown distribution that generated the data , this estimate is optimal ( in a least - squares sense ) , and the corresponding credible intervals capture our uncertainty about @xmath0 given the data . for distributions with",
    "known finite alphabet size @xmath11 , the dirichlet distribution provides an obvious choice of prior due to its conjugacy with the categorical distribution .",
    "it takes the form @xmath27 for @xmath5 on the @xmath28-dimensional simplex ( @xmath29 , @xmath30 ) , where @xmath31 is a `` concentration '' parameter @xcite .",
    "many previously proposed estimators can be viewed as bayesian under a dirichlet prior with particular fixed choice of @xmath32 .",
    "see @xcite for a historical overview of entropy estimators arising from specific choices of @xmath32 .      in a seminal paper",
    ", @xcite showed that for finite distributions with known @xmath11 , dirichlet priors with fixed @xmath32 impose a narrow prior distribution over entropy . in the undersampled regime ,",
    "bayesian estimators based on such priors are severely biased .",
    "moreover , they have undesirably narrow posterior credible intervals , reflecting narrow prior uncertainty rather than strong evidence from the data .",
    "( these estimators generally give incorrect answers with high confidence ! ) .",
    "to address this problem , @xcite suggested a mixture - of - dirichlets prior : @xmath33 where @xmath34 denotes a @xmath35 prior on @xmath5 , and @xmath36 denotes a set of mixing weights , given by @xmath37      = { \\ensuremath{\\mathcal{a}}}\\psi_1({\\ensuremath{\\mathcal{a}}}\\alpha+1 ) - \\psi_1(\\alpha+ 1),\\ ] ] where @xmath38 $ ] denotes the expected value of @xmath0 under a @xmath35 prior , and @xmath39 denotes the tri - gamma function . to the extent that @xmath40 resembles a delta function , and",
    "imply a uniform prior for @xmath0 on @xmath41 $ ] .",
    "the bls estimator under the nsb prior can be written : @xmath42 = \\iint h({{\\bm{\\pi } } } ) p({{\\bm{\\pi}}}|{{\\mathbf{x}}},\\alpha)\\ , p(\\alpha|{{\\mathbf{x } } } ) { \\ensuremath{\\,\\mathrm{d}{{{\\bm{\\pi } } } } } } { \\ensuremath{\\,\\mathrm{d}{\\alpha } } }    \\nonumber\\\\        & =   \\int { \\mathbb{e}}[h|{{\\mathbf{x}}},\\alpha ] \\frac{p({{\\mathbf{x}}}|\\alpha ) p(\\alpha)}{p({{\\mathbf{x}}})}\\,d\\alpha,\\end{aligned}\\ ] ] where @xmath43 $ ] is the posterior mean under a @xmath35 prior , and @xmath44 denotes the evidence , which has a plya distribution  @xcite : @xmath45 the nsb estimate @xmath46 and its posterior variance are fast to compute via 1d numerical integration in @xmath32 using closed - form expressions for the first two moments of the posterior distribution of @xmath0 given @xmath32 .",
    "the forms for these moments are discussed in  @xcite , but the full formulae are not explicitly shown . here",
    "we state the results : @xmath47 & = \\psi_0(\\tilde n+1 ) -",
    "\\sum_i \\frac{\\tilde      n_i}{\\tilde n } \\psi_0(\\tilde",
    "n_i+ 1 )      \\label{eq : dir : posterior : mean }      \\\\     { \\mathbb{e}}[h^2|{{\\mathbf{x}}},\\alpha ] & =       \\sum_{i\\neq k }      \\frac { \\tilde n_i \\tilde n_k } { ( \\tilde n+1)\\tilde n }          i_{i , k }      +      \\sum_i       \\frac{(\\tilde n_i+1)\\tilde n_i}{(\\tilde n+1)\\tilde n }          j_i      \\label{eq : dir : posterior : var }        \\\\",
    "i_{i , k } & =      \\left (        \\psi_0(\\tilde n_k+1)-\\psi_0(\\tilde n + 2 )      \\right )       \\left (          \\psi_0(\\tilde n_i+1)-\\psi_0(\\tilde n + 2 )      \\right )   -\\psi_1(\\tilde n + 2 )       \\nonumber \\\\",
    "j_i & =       ( \\psi_0 ( \\tilde n_i + 2 ) -         \\psi_0 ( \\tilde",
    "n + 2))^2 +        \\psi_1 ( \\tilde n_i + 2 ) - \\psi_1(\\tilde n + 2 ) ,      \\nonumber\\end{aligned}\\ ] ] where @xmath48 are counts plus prior `` pseudocount '' @xmath32 , @xmath49 is the total of counts plus pseudocounts , and @xmath50 is the polygamma of @xmath51-th order ( i.e. , @xmath52 is the digamma function ) .",
    "finally , @xmath53 = { \\mathbb{e}}[h^2|{{\\mathbf{n}}},\\alpha ] - { \\mathbb{e}}[h|{{\\mathbf{n}}},\\alpha]^2 $ ] .",
    "we derive these formulae in the appendix , and in addition provide an alternative derivation using a size - biased sampling formulae discussed in section [ sec : prior : dppy ] .",
    "et al._have proposed an extension of the nsb estimator to countably infinite distributions ( or distributions with unknown cardinality ) , using a zeroth order approximation to @xmath46 in the limit @xmath54 which we refer to as @xmath55 @xcite , @xmath56 where @xmath57 is the number of distinct symbols in the sample . unfortunately , @xmath55 increases unboundedly with @xmath58 ( as noted by @xcite ) , and performs poorly for the examples we consider .",
    "to construct a prior over unknown or countably infinite discrete distributions , we borrow tools from nonparametric bayesian statistics . the dirichlet process ( dp ) and pitman - yor process ( pyp )",
    "define stochastic processes whose samples are countably infinite discrete distributions  @xcite .",
    "a sample from a dp or pyp may be written as @xmath59 , where @xmath60 denotes a countably infinite set of ` weights ' on a set of atoms @xmath61 drawn from some base probability measure , where @xmath62 denotes a delta function on the atom @xmath63 s are distinct with probability one .",
    "this allows us to ignore the base measure , making entropy of the distribution equal to the entropy of the weights @xmath5 .",
    "] we use dp and pyp to define a prior distribution on the infinite - dimensional simplex . the prior distribution over @xmath5 under the dp or pyp",
    "is technically called the gem distribution or the two - parameter poisson - dirichlet distribution , but we will abuse terminology by referring to both the process and its associated weight distribution by the same symbol , dp or py  @xcite . the dp distribution over @xmath5 results from a limit of the ( finite ) dirichlet distribution where alphabet size grows and concentration parameter shrinks : @xmath64 and @xmath65 s.t .",
    "the pyp distribution over @xmath5 generalizes the dp to allow power - law tails ( and includes dp as a special case )  @xcite . for @xmath67 with @xmath68 ,",
    "the tails approximately follow a power - law : @xmath69  ( pp .  867 ,  @xcite ) many natural phenomena such as city size , language , spike responses , etc . , also exhibit power - law tails @xcite . fig .  [",
    "fig : powerlaw ] shows two such examples , along with a sample drawn from the best - fitting dp and pyp distributions .",
    "let @xmath67 denote the pyp with _ discount _",
    "parameter @xmath70 and _ concentration _",
    "parameter @xmath32 ( also called the `` dirichlet parameter '' ) , for @xmath71 .",
    "when @xmath72 , this reduces to the dirichlet process , @xmath73 .",
    "we can draw samples @xmath74 from an infinite sequence of independent beta random variables in a process known as `` stick - breaking ''",
    "@xcite : @xmath75 where @xmath76 is known as the @xmath77th _ size - biased permutation _ from @xmath5  @xcite .",
    "the @xmath76 sampled in this manner are not strictly decreasing , but decrease on average such that @xmath78 with probability 1 @xcite .     and @xmath79 , respectively .",
    "for both datasets , pyp better captures the heavy - tailed behavior of the data . *",
    "top : * frequency of @xmath80 words in the novel moby dick by herman melville . *",
    "bottom : * frequencies among @xmath81 neural spike words from @xmath82 simultaneously - recorded retinal ganglion cells , binarized and binned at 10ms . ,",
    "width=288 ]      a key virtue of pyp priors for our purposes is a mathematical property called _ invariance under size - biased sampling _ , which allows us to convert expectations over @xmath5 on the infinite - dimensional simplex ( which are required for computing the mean and variance of @xmath0 given data ) into one- or two - dimensional integrals with respect to the distribution of the first two size - biased samples @xcite .",
    "[ prop : sizebiased : mean ] for @xmath74 , @xmath83       & = { \\mathbb{e}}_{(\\tilde \\pi_1|d,\\alpha ) } \\left [ \\frac{f(\\tilde          \\pi_1)}{\\tilde \\pi_1 } \\right ] , \\\\",
    "\\label{eq : sizebiasedexpp } { \\mathbb{e}}_{({{\\bm{\\pi}}}|d,\\alpha ) } \\left [ \\sum_{i , j \\neq i } g(\\pi_i , \\pi_j ) \\right ]        & = { \\mathbb{e}}_{(\\tilde \\pi_1,\\tilde \\pi_2|d,\\alpha ) }       \\left [        \\frac{g(\\tilde \\pi_1,\\tilde \\pi_2)}{\\tilde \\pi_1 \\tilde \\pi_2 } ( 1 - \\tilde\\pi_1 )      \\big| { { \\bm{\\pi}}}\\right ] ,    \\end{aligned}\\ ] ] where @xmath84 and @xmath85 are the first two size - biased samples from @xmath5 .",
    "the first result appears in  @xcite , and an analogous proof can be constructed for ( see appendix ) .",
    "the direct consequence of this lemma is that the first two moments of @xmath86 under the pyp and dp priors have closed forms , @xmath87 & = { { \\psi_0}}(\\alpha+1 ) - { { \\psi_0}}(1-d ) , \\label{eq : py : prior : entropy : mean } \\\\",
    "\\operatorname*{var}[h|d,\\alpha ] & =      \\frac{\\alpha+d}{(\\alpha+1)^2 ( 1-d ) }       + \\frac{1-d}{\\alpha+1}\\psi_1(2-d ) - \\psi_1(2+\\alpha ) .",
    "\\label{eq : py : prior : entropy : var}\\end{aligned}\\ ] ] the derivation can be found in the appendix .      a useful property of pyp priors ( for multinomial observations )",
    "is that the posterior @xmath88 takes the form of a mixture of a dirichlet distribution ( over the observed symbols ) and a pitman - yor process ( over the unobserved symbols )  @xcite .",
    "this makes the integrals over the infinite - dimensional simplex tractable , and as a result we obtain closed form solutions for the posterior mean and variance of @xmath0 .",
    "let @xmath57 be the number of unique symbols observed in @xmath58 samples , i.e. , @xmath89 .",
    "further , let @xmath90 , @xmath91 , and @xmath92 .",
    "now , following  @xcite we write the posterior as an infinite random vector @xmath93 , where @xmath94 the posterior mean @xmath95 $ ] is given by , @xmath96 = \\psi_0(\\alpha+n+1 )     - \\frac{\\alpha+kd}{\\alpha+n}\\psi_0(1-d )     - \\frac{1}{\\alpha+n } \\left[\\sum_{i=1}^k      ( n_i - d ) \\psi_0(n_i - d+1)\\right].\\ ] ] the variance , @xmath97 $ ] , also has an analytic closed form which is fast to compute . as we discuss in detail in appendix  [",
    "sec : appendix : posterior ] , @xmath97 $ ] may be expressed in terms of the first two moments of @xmath98 , @xmath5 , and @xmath99 appearing in the posterior .",
    "applying the law of total variance and using the independence properties of the posterior , we find : @xmath100 & =   { \\mathbb{e}}_{p_{\\ast}}[(1-p_\\ast)^2]\\operatorname*{var}_{{\\mathbf{p}}}[h({\\mathbf{p } } ) ] + { \\mathbb{e}}_{p_\\ast } [   p_{\\ast}^2]\\operatorname*{var}_{{{\\bm{\\pi}}}}[h({{\\bm{\\pi } } } ) ]     \\nonumber\\\\   & \\quad   + { \\mathbb{e}}_{p_\\ast}[\\omega^2(p_\\ast ) ]   - { \\mathbb{e}}_{p_\\ast}[\\omega(p_\\ast)]^2,\\end{aligned}\\ ] ] where @xmath101 + p_\\ast{\\mathbb{e}}_{{{\\bm{\\pi}}}}\\left[h({{\\bm{\\pi}}})\\right ] + h(p_\\ast)$ ] . to specify @xmath102 ,",
    "we let @xmath103 $ ] , @xmath104 $ ] so that , @xmath105 & : =    { \\mathbb{e}}_{p_\\ast } [ 1-p_\\ast ] { \\mathbb{e}}_{{\\mathbf{p}}}\\left [ h({\\mathbf{p } } ) \\right ] +    { \\mathbb{e}}_{p_\\ast } [ p_\\ast]{\\mathbb{e}}_{{{\\bm{\\pi}}}}\\left [    h({{\\bm{\\pi}}})\\right ] + h(p_\\ast ) , \\\\ { \\mathbb{e}}[\\omega^2 ] & : = 2   { \\mathbb{e}}_{p_\\ast}[p_\\ast h(p_\\ast)][\\mathbf{b}-\\mathbf{a } ]   + 2\\mathbf{a } { \\mathbb{e}}_{p_\\ast } [ h(p_\\ast ) ] + { \\mathbb{e}}_{p_\\ast } [ h^2(p_\\ast ) ] \\\\ & + { \\mathbb{e}}_{p_\\ast } [ p_\\ast^2]\\left[\\mathbf{b}^2 - 2\\mathbf{a}\\mathbf{b}\\right ] +   2{\\mathbb{e}}_{p_\\ast}[p_\\ast]\\mathbf{a}\\mathbf{b } +   { \\mathbb{e}}_{p_\\ast } [ ( 1-p_\\ast)^2]\\mathbf{a}^2.\\end{aligned}\\ ] ]",
    "the posterior expectations computed in section [ sec : posterior ] provide a class of entropy estimators for distributions with countably - infinite support . for each choice of @xmath79 , @xmath106 $ ]",
    "is the posterior mean under a @xmath107 prior , analygous to the fixed-@xmath32 dirichlet priors discussed in section [ sec : bayesian : estimation ] .",
    "unfortunately , fixed @xmath107 priors also carry the same difficulties as fixed dirichlet priors .",
    "a fixed - parameter @xmath67 prior on @xmath5 results in a highly concentrated prior distribution on entropy ( fig .",
    "[ fig : hpriorvsad ] ) .",
    "we address this problem by introducing a mixture prior @xmath108 on @xmath67 under which the implied prior on entropy is flat , but rather in estimating a single statistic .",
    "an objective prior on the model parameters is not necessarily optimal for estimating entropy : entropy is not a parameter in our model .",
    "in fact , jefferys prior for multinomial observations is exactly a dirichlet distribution with a fixed @xmath109 . as mentioned in the text , such bayesian priors heavily bias entropy estimates . ]",
    "we then define the bls entropy estimator under this mixture prior , the pitman - yor mixture ( pym ) estimator , and discuss some of its theoretical properties .",
    "finally , we turn to the computation of pym , discussing methods for sampling , and numerical quadrature integration .     as a function of @xmath32 and @xmath70 .",
    "note that entropy is approximiately linear in @xmath110 .",
    "for large values of @xmath32 , @xmath111 is highly concentrated around the mean . ]",
    "one way of constructing a flat mixture prior is to follow the approach of @xcite by setting @xmath108 proportional to the derivative of the expected entropy . unlike nsb",
    ", we have two parameters through which to control the prior expected entropy . for instance , large prior ( expected ) entropies can arise either from large values of @xmath32 ( as in the dp ) or from values of @xmath70 near 1 ( see fig .",
    "[ fig : hpriorvsad]a ) . we can explicitly control this trade - off by reparametrizing pyp as follows , @xmath112 where @xmath113 is equal to the expected prior entropy , and @xmath114 captures prior beliefs about tail behavior ( fig .  [ fig : pyparam]a ) . for @xmath115",
    ", we have the dp ( i.e. , @xmath116 , giving @xmath5 with exponential tails ) , while for @xmath117 we have a @xmath118 process ( i.e. , @xmath119 , yielding @xmath5 with power - law tails ) . where required , the inverse transformation to standard py parameters is given by : @xmath120 , @xmath121 where @xmath122 denotes the inverse digamma function .",
    "we can construct an ( approximately ) flat improper distribution over @xmath0 on @xmath123 $ ] by setting @xmath124 for all @xmath125 , where @xmath126 is any density on @xmath127 .",
    "we call this the pitman - yor process mixture ( pym ) prior .",
    "the induced prior on entropy is thus : @xmath128 where @xmath129 denotes a pyp on @xmath5 with parameters @xmath130 .",
    "the ability to adapt @xmath131 to a given problem greatly enhances pym s flexibility .",
    "the pym mixture priors resulting from two different choices of @xmath131 are both approximately flat on @xmath0 , but each favors distributions with different tail behavior .",
    "[ fig : pyparam]b shows samples from this prior under three different choices of @xmath131 , for @xmath125 uniform on @xmath132 $ ] . for the experiments",
    ", we use @xmath133 which yields good results by weighting less on extremely heavy - tailed distributions . combined with the likelihood",
    ", the posterior @xmath134 quickly concentrates as more data are given , as demonstrated in fig .",
    "[ fig : posterior : evidence : convergence ] .    .",
    "right : expected entropy as a function of transformed parameters @xmath135 . * ( b ) * sampled prior distributions ( @xmath136 ) over entropy implied by three different pyp priors : @xmath118 ( red ) , @xmath137 ( grey ) , and @xmath138 ( blue ) .",
    "we show the distributions only on the range from @xmath139 to @xmath140 nats ; sampling from @xmath141 becomes prohibitively expensive with increasing expected entropy , especially as @xmath142 .",
    ", width=384 ]          now that we have determined a prior on the infinite simplex , we turn to the problem of inference given observations @xmath21 .",
    "the bayes least squares entropy estimator under the mixture prior @xmath108 , the pitman - yor mixture ( pym ) estimator , takes the form @xmath143 = \\int       { \\mathbb{e}}[h|{{\\mathbf{x}}},d,\\alpha ] \\frac{p({{\\mathbf{x}}}|d,\\alpha )      p(d,\\alpha)}{p({{\\mathbf{x}}})}{\\ensuremath{\\,\\mathrm{d}{(d,\\alpha)}}},\\ ] ] where @xmath144 $ ] is the expected posterior entropy for a fixed @xmath79 ( see section [ sec : posterior ] ) .",
    "the quantity @xmath145 is the evidence , given by @xmath146 we can obtain posterior credible intervals regions for @xmath147 by estimating the posterior variance @xmath148 $ ] .",
    "the estimate takes the same form as , except that we replace @xmath149 $ ] with @xmath97 $ ] in the integrand .      due to the improperness of the prior @xmath108 and the requirement of integrating over all @xmath31 ( eq .  ) , it is not obvious that the pym estimate @xmath147 is computationally tractable . in this section",
    "we discuss techniques for efficient and accurate computation of @xmath147 .",
    "first , we outline a compressed data representation we call the `` multiplicities '' representation , which substantially reduces computational cost",
    ". then , we outline a fast method for performing the numerical integration over a suitable range of @xmath32 and @xmath70 .",
    "computation of the expected entropy @xmath144 $ ] can be carried out more efficiently using a representation in terms of _ multiplicities _ ( also known as the _ empirical histogram distribution function _",
    "@xcite ) , the number of symbols that have occurred with a given frequency in the sample .",
    "letting @xmath150 denote the total number of symbols with exactly @xmath151 observations in the sample gives the compressed statistic @xmath152{^\\top}$ ] , where @xmath153 is the largest number of samples for any symbol .",
    "note that the inner product @xmath154{^\\top}{{\\mathbf{m}}}= n$ ] , is the total number of samples .",
    "the multiplicities representation significantly reduces the time and space complexity of our computations for most datasets , as we need only compute sums and products involving the number symbols with distinct frequencies ( at most @xmath153 ) , rather than the total number of symbols @xmath57 . in practice",
    "we compute all expressions not explicitly involving @xmath5 using the multiplcities representation .",
    "for instance , in terms of the multiplicities the evidence takes the compressed form , @xmath155      in principle the pym integral over @xmath32 is supported on the range @xmath127 . in practice , however , the posterior is concentrated on a relatively small region of parameter speace .",
    "it is generally unncessary to consider the full integral over a semi - infinite domain . instead",
    ", we select a subregion of @xmath156\\times[0,\\infty)$ ] which supports the posterior up to @xmath157 probability mass . we illustrate the concentration of the posterior visually in figure  [ fig : posterior : evidence : convergence ] .",
    "we compute the hessian at the map parameter value , @xmath158 . using the inverse hessian as the covariance of a gaussian approximation to the posterior , we select the grid which spans @xmath159 .",
    "we use numerial integration ( gauss - legendre quadrature ) on this region to compute the integral . when the hessian is rank - deficient ( which may occur , for instance , when the @xmath160 or @xmath161 ) , we use gauss - legendre quadature perform the integral in @xmath70 over @xmath162 , but employ a fourier - chebyshev numerical quadrature routine to integrate @xmath32 over @xmath127 @xcite .      the closed - form expressions for the conditional moments derived in the previous section allow us to compute pym and its variance by 2-dimensional numerical integraton .",
    "pym s posterior mean and variance provide essentially a gaussian approximation to the posterior , and corresponding credible regions .",
    "however , in some situations ( see fig .  [",
    "fig : sampling ] ) , variance - based credible intervals are a poor approximation to the true posterior credible intervals . in such situations we may wish to examine the full posterior distribution over @xmath0 . in what follows",
    "we describe methods for exactly sampling the posterior and argue that the posterior variance provides a good approximation to the true credible interval in many situations .",
    "stick - breaking , as described by , provides a straightforward algorithm for sampling distributions @xmath74 . with large enough @xmath163 , stick - breaking samples @xmath164 approximate @xmath165 to arbitrary accuracy , the number of samples ( i.e. , sticks ) , necessary to reach @xmath157 on average are provided in @xcite . ] .",
    "even so , in practice sampling @xmath5 is difficult when it is heavy - tailed .",
    "when sampling @xmath166 for @xmath70 near @xmath167 , where @xmath5 is likely to be heavy - tailed , @xmath163 may need to be intractably large to assure @xmath168 .",
    "in such sitations , truncation may result in serverely biased samples of entropy .",
    "we address this problem by directly estimating the entropy of the tail , @xmath169 , using . as shown in fig .  [",
    "fig : hpriorvsad ] , the prior variance of becomes arbitrarily small as for large @xmath32 . for sampling ,",
    "@xmath163 need only be large enough to make the variance of the tail entropy small .",
    "the resulting sample is the entropy of the ( finite ) samples plus the expected entropy of the tail , @xmath170 $ ] .. ] sampling entropy is most useful for very small amounts of data drawn from distributions with low expected entropy . in fig .",
    "[ fig : posterior : evidence : convergence ] we illustrate the posterior distributions of entropy in two simulated experiments . in general , as the expected entropy and sample size increase , the posterior becomes more approximately gaussian .",
    "having defined pym and discussed its practical computation , we now consider first establish conditions under which   is defined ( that is , finite ) , and also prove some basic facts about its asymptotic properties . while @xmath147 is a bayesian estimator , we wish to build connection to the literature by showing frequentist properties .",
    "note that the prior expectation @xmath171 $ ] does not exist for the improper prior defined above , since @xmath172 ) \\propto 1 $ ] on @xmath173 $ ] .",
    "it is therefore reasonable to ask what conditions on the data are sufficient to obtain finite posterior expectation @xmath174 $ ] .",
    "we give an answer to this question in the following short proposition ( proofs of all statements may be found in the appendix ) ,    [ finiteposterior ] given a fixed dataset @xmath21 of @xmath58 samples , @xmath175 for any prior distribution @xmath108 if @xmath176 .",
    "in other words , we require @xmath177 coincidences in the data for @xmath147 to be finite .",
    "when no coincidences have occurred in @xmath21 , we have no evidence regarding the support of the @xmath5 , and our resulting entropy estimate is unbounded . in fact , in the absence of coincidences , no entropy estimator can give a reasonable estimate without prior knowledge or assumptions about @xmath11 .",
    "concerns about inadequate numbers of coincidences are peculiar to the undersampled regime ; as we collect more data , we will almost surely observe each letter infinitely often .",
    "we now turn to asymptotic considerations , establishing consistency of @xmath147 in the limit of large @xmath58 for a broad class of distributions .",
    "it is known that the plugin is consistent for any distribution ( finite or countably infinite ) , although the rate of convergence can be arbitrarily slow  @xcite .",
    "therefore , we establish consistency by showing asymptotic convergence to the plugin estimator . for clarity , we explicitly denote a quantity s dependence upon sample size @xmath58 by a introducing a subscript .",
    "thus , @xmath21 and @xmath57 become @xmath178 and @xmath179 , respectively . as a first step ,",
    "we show that @xmath180 $ ] converges to the plugin estimator .",
    "[ fixedhconv ] assuming @xmath178 drawn from a fixed , finite or countably infinite discrete distribution @xmath5 such that @xmath181 , @xmath182 - { \\mathbb{e}}[h_\\mathrm{plugin}| { { \\mathbf{x}}}_n ] \\right| \\xrightarrow{p } 0\\ ] ]    the assumption @xmath183 is more general than it may seem . for any discrete distribution it holds that @xmath184 $ ] a.s . , and @xmath185/n \\to 0 $ ] a.s .",
    "@xcite , and so @xmath186 in probability for an arbitrary distribution . as a result ,",
    "shares its asymptotic behavior with @xmath20 , in particular consistency .",
    "as is consistent for each value of @xmath32 and @xmath70 , it is intuitively plausible that pym , as a mixture of such values , should be consistent as well .",
    "however , while alone is well - behaved , it is not clear that pym should be . since @xmath144 \\to \\infty$ ] as @xmath187",
    ", care must be taken when integrating over @xmath188 .",
    "our main consistency result is ,    [ consistencyproof ] for any proper prior or bounded improper prior @xmath189 , if data @xmath178 are drawn from a fixed , countably infinite discrete distribution @xmath5 such that for some constant @xmath190 , @xmath191 in probability , then @xmath192 - { \\mathbb{e}}[h_\\mathrm{plugin}| { { \\mathbf{x}}}_n]| \\xrightarrow{p } 0\\ ] ]    intuitively , the asymptotic behavior of @xmath193 is tightly related to the tail behavior of the distribution  @xcite . in particular , @xmath194 with @xmath195 if and only if @xmath196 where @xmath197 and @xmath198 are constants  @xcite .",
    "the class of distributions such that @xmath191 a.s . includes the class of power - law or thinner tailed distributions , i.e. , @xmath199 for some @xmath200 .",
    "we conclude this section with some remarks on the role of the prior in theorem  [ consistencyproof ] as well as the significance of asymptotic results in general . while consistency is an important property for any estimator , we emphasize that pym is designed to address the undersampled regime .",
    "indeed , since @xmath20 is consistent and has an optimal rate of convergence for a large class of distributions @xcite , asymptotic properties provide little reason to use @xmath147 . nevertheless , notice that theorem  [ consistencyproof ] makes very weak assumptions about @xmath108 .",
    "in particular , the result is not dependant upon the form of the pym prior introduced in the previous section : it holds for any probability distribution @xmath201 , or even a bounded improper prior .",
    "thus , we can view theorem  [ consistencyproof ] as a statement about a class of pym estimators .",
    "almost any prior we choose on @xmath202 results in a consistent estimator of entropy .",
    "we compare @xmath147 to other proposed entropy estimators using several example datasets .",
    "each plot in figs  [ fig : convergence : sb ] , [ fig : convergence : powerlaw ] , [ fig : convergence : real ] , and [ fig : convergence : finite ] shows convergence as well as small sample performance .",
    "we compare our estimators , dpm ( @xmath72 only ) and pym ( @xmath147 ) , with other enumerable - support estimators : coverage - adjusted estimator ( cae )  @xcite , asymptotic nsb ( ansb , section  [ sec : ansb ] )  @xcite , james - stein ( js )  @xcite , grassberger s asymptotic bias correction ( gr08 )  @xcite , and good - turing estimator  @xcite .",
    "note that like ansb , dpm is an asymptotic ( poisson - dirichlet ) limit of nsb , and hence behaves close to nsb assuming a large number of symbols .",
    "we also compare with plugin   and a standard bias correction methods assuming finite support : miller - maddow bias correction ( mima )  @xcite . to make comparisons more straightforward",
    ", we do not apply jackknife - based bias correction to any of the estimators .",
    "( top ) , @xmath203 ( middle ) , @xmath204 ( bottom ) .",
    "we compare our estimators ( dp , pym ) with other enumerable support estimators ( cae , ansb , js , zhang , gr08 ) , and finite support estimators ( plugin , mima ) .",
    "solid lines are averaged over 10 realizations .",
    "shaded area represent two standard deviations credible intervals averaged over 10 realizations . ]",
    "pym performs well as expected when the data are truly generated by a pitman - yor process ( fig .",
    "[ fig : convergence : sb ] ) .",
    "credible intervals for dpm tend to be smaller than pym , although both shrink quickly ( indicating high confidence ) . when the tail of the distribution is exponentially decaying , ( @xmath116 case ; fig .",
    "[ fig : convergence : sb ] top ) , dpm shows slightly improved performance . when the tail has a strong power - law decay , ( fig .",
    "[ fig : convergence : sb ] bottom ) , pym performs better than dpm .",
    "most of the other estimators are consistently biased down , with the exception of js and ansb .        although pitman - yor process @xmath137 has a power - law tail controlled by @xmath70 , the high probability portion is modulated by @xmath32 , and does not strictly folllow a power - law distribution as a whole . in fig .",
    "[ fig : convergence : powerlaw ] , we evaluate the performance for @xmath205 and @xmath206 .",
    "pym and dpm has slight negative bias , but the credible interval covers the true entropy for all sample sizes . for small sample sizes , most estimators are negatively biased , again except for js and ansb ( which does not show up in the plot since it is severely biased upwards ) .",
    "notably cae performs very well in moderate sample sizes .        in fig .",
    "[ fig : convergence : real ] , we compute the entropy per word of in the novel _ moby dick _ by herman melville , and entropy per time bin of a population of retinal ganglion cells from monkey retina  @xcite . these real - world datasets have heavy , approximately power - law tails as pointed out earlier in fig .",
    "[ fig : powerlaw ] . for moby dick ,",
    "pym slightly overestimates , while dpm slightly underestimates , yet either method is closer to the entropy estimated by the full data available than other estimators .",
    "dpm is overly confident ( its credible interval is too narrow ) , while pym becomes overly confident with more data .",
    "the neural data were preprocessed to be a binarized response ( 10 ms time bins ) of 8 simultaneously recorded off - response retinal ganglion cells .",
    "pym , dpm , and cae all perform well on this dataset , with both pym and dpm bracketing the asymptotic value with their credible intervals .",
    "finally , we applied the denumerable support estimators to finite support distributions ( fig .",
    "[ fig : convergence : finite ] ) .",
    "the power - law @xmath207 has the heaviest tail among the simulations we consider , but notice that it does not define a proper distribution ( the probability mass does not integrate ) , and so we use a truncated @xmath208 distribution with the first @xmath209 symbols ( fig .  [",
    "fig : convergence : finite ] top ) .",
    "initially pym shows the least bias , but dpm provides a better estimate for increasing sample size .",
    "notice , however , that for both estimates the credible intervals consistently cover the true entropy .",
    "interestingly , the finite support estimators perform poorly compared to dpm , cae and pym . for the uniform distribution over @xmath209 symbols , both dpm and pym have slight upward bias , while cae shows almost perfect performance ( fig .",
    "[ fig : convergence : finite ] middle ) . for poisson distribution , a theoretically enumerable support distribution on the natural number",
    ", the tail decays so quickly that the effective support ( due to machine precision ) is very small ( @xmath210 in this case ) .",
    "all the estimators , with the exception of js and ansb , work quite well .",
    "note that js performs poorly for both uniform and poisson distribution ( it shows severe upward biased ) .",
    "the novel moby dick provides the most challenging data : no estimator seems to have converged , even with the full data .",
    "surprisingly , the good - turing estimator  @xcite tends to perform similarly to the grassberger and miller - maddow bias - correction methods . among such the bias - correction methods , grassberger s method tended to show the best performance , outperforming zhang s method .    ) has a countably infinite tail , but a very thin one  all probability mass was concentrated in 26 symbols within machine precision . ]",
    "in this paper we introduced pym , a novel entropy estimator for distributions with unknown support .",
    "we derived analytic forms for the conditional mean and variance of entropy under a and prior for fixed parameters .",
    "inspired by the work of @xcite , we defined a novel mixture prior , pym , which implies an approximately flat prior on entropy .",
    "pym addresses two major issues with nsb : its dependence on knowledge of @xmath11 and its inability ( inherited from the dirichlet distribution ) to account for the heavy - tailed distributions which abound in biological and other natural data .",
    "we have shown that pym performs well in comparison to other entropy estimators , and indicated its practicality in example applications to data .",
    "a matlab implementation of the pym estimator is available at https://github.com/pillowlab/pymentropy .",
    "in this appendix we present as propositions a number of technical moment derivations used in the text .      for @xmath211 , such that @xmath212 , and letting @xmath213 , we have @xmath214 & = & \\psi_0(a+1 ) - \\sum_{i=1}^{{\\ensuremath{\\mathcal{a } } } }      \\frac{\\alpha_i}{a } \\psi_0(\\alpha_i+1)\\label{eq : dirmeanentropy }    \\end{aligned}\\ ] ]    first , let @xmath197 be the normalizer of dirichlet , @xmath215 and let @xmath216 denote the laplace transform ( on @xmath165 to @xmath217 ) .",
    "now , @xmath218 & = \\int \\left ( -\\sum_i \\pi_i \\log_2        \\pi_i      \\right ) \\delta({\\scriptstyle \\sum_i } \\pi_i - 1 ) \\prod_j \\pi_j^{\\alpha_j - 1 } d{{\\bm{\\pi}}}\\\\      & = -\\sum_i \\int \\left ( \\pi_i^{\\alpha_i } \\log_2 \\pi_i \\right )      \\delta({\\scriptstyle \\sum_i } \\pi_i - 1 ) \\prod_{j \\neq i }      \\pi_j^{\\alpha_j - 1 } d{{\\bm{\\pi}}}\\\\      & = - \\sum_i \\int \\left (        { \\frac{d } { d ( \\alpha_i)}}\\pi_i^{\\alpha_i } \\right )      \\delta({\\scriptstyle \\sum_i } \\pi_i - 1 ) \\prod_{j \\neq i }      \\pi_j^{\\alpha_j - 1 } d{{\\bm{\\pi}}}\\\\      & = - \\sum_i { \\frac{d } { d ( \\alpha_i ) } } \\int      \\pi_i^{\\alpha_i } \\delta({\\scriptstyle \\sum_i } \\pi_i - 1 )      \\prod_{j \\neq i } \\pi_j^{\\alpha_j - 1 } d{{\\bm{\\pi}}}\\\\      & = - \\sum_i { \\frac{d } { d ( \\alpha_i ) } } { \\mathcal{l}}{^{-1}}\\left [ { \\mathcal{l}}(\\pi_i^{\\alpha_i } ) \\prod_{j \\neq i }        { \\mathcal{l}}(\\pi_j^{\\alpha_j - 1 } ) \\right](1 )      \\\\      & = - \\sum_i { \\frac{d } { d ( \\alpha_i ) } } { \\mathcal{l}}{^{-1}}\\left [ \\frac { \\gamma(\\alpha_i + 1 ) \\prod_{j \\neq i }          \\gamma({\\alpha_j } ) } { s^{\\sum_k ( \\alpha_k ) + 1 } }      \\right](1 )      \\\\      & = - \\sum_i { \\frac{d } { d ( \\alpha_i ) } } \\left [ \\frac {          \\gamma(\\alpha_i + 1 ) } { \\gamma(\\sum_k ( \\alpha_k ) + 1 ) }      \\right ] \\prod_{j \\neq i } \\gamma({\\alpha_j } )      \\\\      & = - \\sum_i \\frac { \\gamma(\\alpha_i + 1 ) } {          \\gamma(\\sum_k \\alpha_k + 1 ) } \\left [ \\psi_0(\\alpha_i+ 1 )   - \\psi_0(a+ 1 ) \\right ]      \\prod_{j \\neq i } \\gamma({\\alpha_j } )      \\\\      & =   \\left [ \\psi_0(a+1 ) -       \\sum_{i=1}^{{\\ensuremath{\\mathcal{a } } } } \\frac { \\alpha_i } { a }       \\psi_0(\\alpha_i+ 1 ) \\right ]      \\frac { \\prod_{j } \\gamma({\\alpha_j } ) } { \\gamma(a ) } .",
    "\\end{aligned}\\ ] ]      we derive @xmath219 $ ] . in practice",
    "we compute @xmath220 = { \\mathbb{e}}[h^2({{\\bm{\\pi}}})|{\\vec{\\alpha } } ] - { \\mathbb{e}}[h({{\\bm{\\pi}}})|{\\vec{\\alpha}}]^2 $ ] .    for @xmath211 ,",
    "such that @xmath212 , and letting @xmath213 , we have @xmath221 & =   \\sum_{i\\neq k }    \\frac { \\alpha_i \\alpha_k   } {    ( a+ 1)(a ) }   i_{ik }   + \\sum_i   \\frac {    \\alpha_i(\\alpha_i+1 ) } {    ( a + 1)(a ) } j_i \\\\ \\nonumber i_{ik } & =   \\left (    \\psi_0(\\alpha_k+1)-\\psi_0(a + 2 ) \\right )     \\left (      \\psi_0(\\alpha_i+1 )      \\right .",
    "\\\\ & \\qquad\\left.-\\psi_0(a + 2 )    \\right ) -\\psi_1(a + 2 ) \\nonumber \\\\",
    "\\nonumber j_i & = ( \\psi_0 ( \\alpha_i+2 ) -     \\psi_0 ( a + 2))^2 +    \\psi_1(\\alpha_i+2 ) \\\\&\\qquad- \\psi_1(a + 2 ) \\nonumber    \\end{aligned}\\ ] ]        we derive the prior entropy mean and variance of a @xmath141 distribution with fixed parameters @xmath32 and @xmath70 , @xmath227 $ ] and @xmath228 $ ] .",
    "we first prove our proposition  [ prop : sizebiased : mean ] ( mentioned in @xcite ) .",
    "this proposition establishes the identity @xmath229 = \\int_0 ^ 1 \\frac{f(\\tilde \\pi_1 ) } { \\tilde    \\pi_1 } p(\\tilde",
    "\\pi_1 | \\alpha ) d\\tilde \\pi_1 $ ] which will allow us to compute expectations over @xmath141 using only the distribution of the first size biased sample , @xmath230 .",
    "first we validate .",
    "writing out the general form of the size - biased sample , @xmath231 we see that @xmath232 & =    \\int_0 ^ 1 \\frac{f(x)}{x }      p(\\tilde \\pi_1 = x)dx \\\\ & = \\int_0 ^ 1    { \\mathbb{e}}_{{\\bm{\\pi}}}\\left[\\frac{f(x)}{x } p(\\tilde \\pi_1 = x | { { \\bm{\\pi } } } )    \\right ] dx \\\\ & = \\int_0 ^ 1    { \\mathbb{e}}_{{\\bm{\\pi}}}\\left [ \\sum_{i=1}^\\infty \\frac{f(x)}{x}\\pi_i \\delta(x-\\pi_i ) \\right ] dx \\\\ & = { \\mathbb{e}}_{{\\bm{\\pi}}}\\left[\\int_0 ^ 1    \\sum_{i=1}^\\infty \\frac{f(x)}{x}\\pi_i \\delta(x-\\pi_i ) dx\\right ] \\\\ & = { \\mathbb{e}}_{{\\bm{\\pi}}}\\left [    \\sum_{i=1}^\\infty \\int_0 ^ 1 \\frac{f(x)}{x}\\pi_i \\delta(x-\\pi_i ) dx\\right ] \\\\ & = { \\mathbb{e}}_{{\\bm{\\pi}}}\\left [ \\sum_{i=1}^\\infty f(\\pi_i ) \\right],\\end{aligned}\\ ] ] where the interchange of sums and integrals is justified by fubini s theorem .",
    "a similar method validates .",
    "we will need the second size - biased sample in addition to the first .",
    "we begin with the sum inside the expectation on the left hand side of , @xmath233\\end{aligned}\\ ] ] where the joint distribution of size biased samples is given by , @xmath234    as this identity is defined for any additive functional @xmath235 of @xmath5 ; we can employ it to compute the first two moments of entropy .",
    "for pyp ( and dp when @xmath116 ) , the first size - biased sample is distributed according to : @xmath236 proposition  [ prop : sizebiased : mean ] gives the mean entropy directly .",
    "taking @xmath237 we have , @xmath238 = -{\\mathbb{e}}_\\alpha [ \\log(\\pi_1 ) ] = \\psi_0(\\alpha+1 ) - \\psi_0(1-d),\\ ] ] the same method may be used to obtain the prior variance , although the computation is more involved . for the variance , we will need the second size - biased sample in addition to the first .",
    "the second size - biased sample is given by , @xmath239 we will compute the second moment explicitly , splitting @xmath240 into square and cross terms , @xmath241      & = { \\mathbb{e}}\\left[\\left.\\left ( - \\sum_i \\pi_i \\log(\\pi_i ) \\right)^2\\right|d,\\alpha\\right ]      \\\\     & = { \\mathbb{e}}\\left[\\left.\\sum_i \\left(\\pi_i \\log(\\pi_i )         \\right)^2\\right|d,\\alpha\\right ]     \\\\     & + { \\mathbb{e}}\\left[\\left.\\sum_i \\sum_{j \\neq i } \\pi_i \\pi_j \\log(\\pi_i ) \\log(\\pi_j)\\right|d,\\alpha\\right ]      \\label{eq : py : prior : entropy:2nd}\\end{aligned}\\ ] ] the first term follows directly from , @xmath242       = \\int_0 ^ 1 x ( -\\log(x))^2 p(x|d,\\alpha ) { \\ensuremath{\\,\\mathrm{d}{x } } }      \\nonumber \\\\      & =       b^{-1}(1-d,\\alpha+d ) \\int_0 ^ 1 x \\log^2(x ) x^{1-d } ( 1-x)^{\\alpha+d-1 } { \\ensuremath{\\,\\mathrm{d}{x } } }      \\nonumber \\\\      & =       \\frac{1-d}{\\alpha+1 } \\left[(\\psi_0(2-d ) - \\psi_0(2+\\alpha))^2 + \\psi_1(2-d ) - \\psi_1(2+\\alpha)\\right]\\end{aligned}\\ ] ] the second term of , requires the first two size biased samples , and follows from with @xmath243 . for the pyp",
    "prior , it is easier to integrate on @xmath244 and @xmath245 , rather than the size biased samples .",
    "the second term is then ( note that we let @xmath246 and @xmath247 ) , @xmath248      | \\alpha      \\right ] \\\\      & =      { \\mathbb{e}}\\left [      { \\mathbb{e}}\\left [          \\log(v_1 ) \\log((1-v_1 ) v_2 ) ( 1 - v_1 )          | { { \\bm{\\pi}}}\\right ]      | \\alpha      \\right ]      \\\\      & =      \\zeta      \\gamma      \\int_0 ^ 1 \\int_0 ^ 1      \\log(v_1 ) \\log((1-v_1 ) v_2 ) ( 1 - v_1 )      v_1^{1 - d }      ( 1 - v_1)^{\\alpha+d-1 }          \\\\      & \\qquad \\qquad \\qquad \\qquad          \\times v_2^{1 - d }      ( 1 - v_2)^{\\alpha+2d-1 }      { \\ensuremath{\\,\\mathrm{d}{v_1 } } } { \\ensuremath{\\,\\mathrm{d}{v_2 } } }      \\\\      & =      \\zeta      \\left [      \\int_0 ^ 1      \\log(v_1 ) \\log(1-v_1 ) ( 1 - v_1 ) v_1^{1 - d }           ( 1 - v_1)^{\\alpha+d-1 }          { \\ensuremath{\\,\\mathrm{d}{v_1 } } }        \\right .      \\\\      & \\quad+\\left .",
    "\\gamma      \\int_0 ^ 1       \\log(v_1 ) ( 1 - v_1 ) v_1^{1 - d } ( 1 - v_1)^{\\alpha+d-1 }      \\right .      \\\\      & \\qquad\\qquad\\qquad\\left",
    "\\times\\int_0 ^ 1      \\log(v_2 ) v_2^{1 - d } ( 1 - v_2)^{\\alpha+2d-1 }      { \\ensuremath{\\,\\mathrm{d}{v_1 } } } { \\ensuremath{\\,\\mathrm{d}{v_2 } } }      \\right ]      \\\\      & =      \\frac{\\alpha+d}{\\alpha+1}\\left [ ( \\psi_0(1-d)-\\psi_0(2+\\alpha))^2 - \\psi_1(2+\\alpha)\\right ]      \\label{eq : py : prior : entropy : var : derivation}\\end{aligned}\\ ] ] finally combining the terms , the variance of the entropy under pyp prior is @xmath249 = \\\\ & \\frac{1-d}{\\alpha+1 } \\left[(\\psi_0(2-d ) - \\psi_0(2+\\alpha))^2 + \\psi_1(2-d ) - \\psi_1(2+\\alpha)\\right ] \\nonumber\\\\ & \\quad\\quad + \\frac{\\alpha+d}{\\alpha+1}\\left [ ( \\psi_0(1-d)-\\psi_0(2+\\alpha))^2 - \\psi_1(2+\\alpha)\\right ] \\nonumber\\\\ & \\qquad -(\\psi_0(1+\\alpha ) - \\psi_0(1-d))^2 \\nonumber\\\\ & = \\frac{\\alpha+d}{(\\alpha+1)^2 ( 1-d ) } + \\frac{1-d}{\\alpha+1}\\psi_1(2-d ) - \\psi_1(2+\\alpha)\\end{aligned}\\ ] ] we note that the expectations over the finite dirichlet may also be derived using this formula by letting the @xmath250 be the first size - biased sample of a finite dirichlet on @xmath251 .",
    "first , we discuss the form of the pyp posterior , and introduce independence properties that will be important in our derivation of the mean .",
    "we recall that the pyp posterior , @xmath252 , of has three stochastically independent components : bernoulli @xmath98 , py @xmath5 , and dirichlet @xmath253 .",
    "* component expectations : * from the above derivations for expectations under the pyp and dirichlet distributions as well as the beta integral identities ( see e.g. , @xcite ) , we find expressions for @xmath254 $ ] , @xmath255 $ ] , and @xmath256 $ ] .",
    "@xmath257 & = \\psi_0(\\alpha+1 ) - \\psi_0(1-d ) \\\\    { \\mathbb{e}}_{p_\\ast } [ h(p_\\ast ) ]       & = \\psi_0 ( \\alpha + n + 1 )         - \\frac{\\alpha + kd}{\\alpha+n } \\psi_0(\\alpha+kd + 1 )   \\\\ & \\qquad\\quad        - \\frac{n - kd}{\\alpha+n } \\psi_0(n - kd+1 ) \\\\ { \\mathbb{e}}_{{\\mathbf{p}}}[h({\\mathbf{p}})|d,\\alpha ]       & = \\psi_0(n - kd+1 ) - \\sum_{i=1}^k   \\frac{n_i - d}{n - kd } \\psi_0(n_i - d+1)\\end{aligned}\\ ] ] where by a slight abuse of notation we define the entropy of @xmath98 as @xmath258 .",
    "we use these expectations below in our computation of the final posterior integral . *",
    "derivation of posterior mean : * we now derive the analytic form of the posterior mean , .",
    "@xmath259 = { \\mathbb{e}}\\left [ - \\sum_{i=1}^k p_i    \\log{p_i } - p_\\ast \\sum_{i=1}^\\infty \\pi_i \\log { p_\\ast \\pi_i } \\big|    d,\\alpha \\right]\\nonumber \\\\\\nonumber & = { \\mathbb{e}}\\left [   - ( 1-p_\\ast)\\sum_{i=1}^k \\frac{p_i}{1-p_\\ast } \\log{\\left(\\frac{p_i}{1-p_\\ast}\\right ) }   \\right.\\\\&\\quad \\left .",
    "- ( 1-p_\\ast)\\log(1-p_\\ast ) - p_\\ast \\sum_{i=1}^\\infty \\pi_i \\log{\\pi_i } - p_\\ast \\log { p_\\ast } \\big|    d,\\alpha \\right ] \\\\\\nonumber & = { \\mathbb{e}}\\left [   - ( 1-p_\\ast)\\sum_{i=1}^k \\frac{p_i}{1-p_\\ast } \\log{\\left(\\frac{p_i}{1-p_\\ast}\\right ) }   \\right.\\\\&\\qquad\\qquad\\qquad \\left .   - p_\\ast \\sum_{i=1}^\\infty \\pi_i \\log{\\pi_i }     + h(p_\\ast ) \\big|    d,\\alpha \\right ] \\\\\\nonumber & = { \\mathbb{e}}\\left [ { \\mathbb{e}}\\left [ - ( 1-p_\\ast)\\sum_{i=1}^k \\frac{p_i}{1-p_\\ast } \\log{\\left(\\frac{p_i}{1-p_\\ast}\\right ) }   \\right.\\right.\\\\&\\qquad\\qquad\\qquad \\left .",
    "\\left . - p_\\ast \\sum_{i=1}^\\infty \\pi_i \\log{\\pi_i }     + h(p_\\ast )   ~\\big| ~ p_\\ast \\right ] \\big|    d,\\alpha \\right ] \\\\\\nonumber & = { \\mathbb{e}}\\left [ { \\mathbb{e}}\\left[(1-p_\\ast ) h({\\mathbf{p}})+ p_\\ast h({{\\bm{\\pi } } } ) + h(p_\\ast )   ~\\big| ~ p_\\ast    \\right ] \\big|    d,\\alpha \\right]\\nonumber \\\\ & = { \\mathbb{e}}_{p_\\ast}\\left [ ( 1-p_\\ast ) { \\mathbb{e}}_{{\\mathbf{p}}}\\left [ h({\\mathbf{p } } ) | d,\\alpha \\right ] +   p_\\ast{\\mathbb{e}}_{{{\\bm{\\pi}}}}\\left [ h({{\\bm{\\pi } } } )    |d,\\alpha\\right ] + h(p_\\ast )     \\right]\\nonumber\\end{aligned}\\ ] ] using the formulae for @xmath254 $ ] , @xmath260 $ ] , and @xmath261 $ ] and rearranging terms , we obtain , @xmath262       = \\frac{a}{\\alpha+n }   { \\mathbb{e}}_{{\\mathbf{p } } } [ h({\\mathbf{p } } ) ]       \\\\   & \\qquad\\qquad\\qquad       + \\frac{\\alpha + kd}{\\alpha+n } { \\mathbb{e}}_{{{\\bm{\\pi } } } } [ h({{\\bm{\\pi } } } ) ]        + { \\mathbb{e}}_{p_\\ast } [ h(p_\\ast ) ] \\nonumber \\\\",
    "\\nonumber      & = \\frac{a}{\\alpha+n } \\left[\\psi_0(a+1 ) - \\sum_{i=1}^k        \\frac{\\alpha_i}{a } \\psi_0(\\alpha_i+1)\\right ]       \\\\&\\quad      + \\frac{\\alpha + kd}{\\alpha+n }      \\left [ \\psi_0(\\alpha+kd+1 )         - \\psi_0(1-d)\\right ] +      \\\\\\nonumber      & \\quad \\psi_0 ( \\alpha + n + 1 ) - \\frac{\\alpha + kd}{\\alpha+n }      \\psi_0(\\alpha+kd + 1 ) - \\frac{a}{\\alpha+ n } \\psi_0(a+1 ) \\\\\\nonumber & = \\psi_0(\\alpha+n+1 ) - \\frac{\\alpha+kd}{\\alpha+n}\\psi_0(1-d ) -",
    "\\\\&\\qquad \\qquad \\qquad \\qquad \\qquad \\frac{a}{\\alpha+n } \\left[\\sum_{i=1}^k      \\frac{\\alpha_i}{a } \\psi_0(\\alpha_i+1)\\right ] \\nonumber \\\\ & = \\psi_0(\\alpha+n+1 ) - \\frac{\\alpha+kd}{\\alpha+n}\\psi_0(1-d ) -",
    "\\\\&\\qquad \\qquad \\qquad \\qquad \\qquad      \\frac{1}{\\alpha+n } \\left [ \\sum_{i=1}^k      ( n_i - d ) \\psi_0(n_i - d+1)\\right ] \\nonumber\\end{aligned}\\ ] ] * derivation of posterior variance :",
    "* we continue the notation from the subsection above . in order to exploit the independence properties of @xmath263 we first apply the law of total variance to obtain , @xmath264 & = \\operatorname*{var}_{p_\\ast}\\left [      { \\mathbb{e}}_{{{\\bm{\\pi } } } , { \\mathbf{p}}}[h(\\pi_{\\mathrm{post } } ) ]   \\big| d,\\alpha\\right ] \\nonumber        \\\\&\\qquad     + { \\mathbb{e}}_{p_\\ast}\\left [      \\operatorname*{var}_{{{\\bm{\\pi } } } , { \\mathbf{p}}}[h(\\pi_{\\mathrm{post } } ) ] \\big| d,\\alpha\\right]\\end{aligned}\\ ] ] we now seek expressions for each term in in terms of the expectations already derived .",
    "_ step 1 : _ for the right - hand term of , we use the independence properties of @xmath263 to express the variance in terms of pyp , dirichlet , and beta variances , @xmath265 \\big|    d,\\alpha\\right ] \\\\&= { \\mathbb{e}}_{p_\\ast}\\left [   ( 1-p_\\ast)^2\\operatorname*{var}_{{\\mathbf{p}}}[h({\\mathbf{p } } ) ]     + p_\\ast^2 \\operatorname*{var}_{{{\\bm{\\pi}}}}[h({{\\bm{\\pi } } } ) ]   \\big|d,\\alpha\\right ] \\nonumber \\\\ & = \\frac{(n - kd)(n - kd+1)}{(\\alpha+n)(\\alpha+n+1)}\\operatorname*{var}_{{\\mathbf{p}}}[h({\\mathbf{p}})]\\nonumber \\\\ & \\qquad \\qquad + \\frac{(\\alpha+kd)(\\alpha+kd+1)}{(\\alpha+n)(\\alpha+n+1 ) } \\operatorname*{var}_{{{\\bm{\\pi}}}}[h({{\\bm{\\pi } } } ) ]   \\end{aligned}\\ ] ] _ step 2 : _ in the left - hand term of the variance is with respect to the @xmath266 distribution , while the inner expecation is precisely the posterior mean we derived above .",
    "expanding , we obtain , @xmath267   \\big| d,\\alpha\\right]\\\\&= \\operatorname*{var}_{p_\\ast}\\left [ ( 1-p_\\ast ) { \\mathbb{e}}_{{\\mathbf{p}}}\\left [ h({\\mathbf{p } } ) \\right ] +   p_\\ast{\\mathbb{e}}_{{{\\bm{\\pi}}}}\\left [ h({{\\bm{\\pi } } } )      | p_\\ast \\right ] + h(p_\\ast ) \\big|",
    "d,\\alpha \\right ] \\end{aligned}\\ ] ] to evaluate this integral , we introduce some new notation , @xmath268 \\\\",
    "\\mathbf{b } & : = { \\mathbb{e}}_{{{\\bm{\\pi}}}}\\left [ h({{\\bm{\\pi}}})\\right ]   \\\\",
    "\\omega(p_\\ast ) & : =   ( 1-p_\\ast ) { \\mathbb{e}}_{{\\mathbf{p}}}\\left [ h({\\mathbf{p } } ) \\right ] +   p_\\ast{\\mathbb{e}}_{{{\\bm{\\pi}}}}\\left [    h({{\\bm{\\pi}}})\\right ] + h(p_\\ast ) \\\\ & = ( 1-p_\\ast ) \\mathbf{a } +   p_\\ast\\mathbf{b } + h(p_\\ast)\\end{aligned}\\ ] ] so that @xmath269   + 2\\mathbf{a}h(p_\\ast ) + h^2(p_\\ast ) \\\\ & + p_\\ast^2[\\mathbf{b}^2 - 2\\mathbf{a}\\mathbf{b } ] + 2p_\\ast\\mathbf{a}\\mathbf{b } + ( 1-p_\\ast)^2\\mathbf{a}^2\\label{eq : kpstarsquared}\\end{aligned}\\ ] ] and we note that @xmath270 \\big| d,\\alpha\\right ] = { \\mathbb{e}}_{p_\\ast}[\\omega^2(p_\\ast ) ]   - { \\mathbb{e}}_{p_\\ast}[\\omega(p_\\ast)]^2\\ ] ] the components composing @xmath271 $ ] , as well as each term of can be found in  @xcite .",
    "although less elegant than the posterior mean , the expressions derived above permit us to compute numerically from its component expecatations , without sampling .",
    "in this appendix we give a proof for proposition [ finiteposterior ] .",
    "pym is given by @xmath272 where we have written @xmath273 $ ] .",
    "note that @xmath145 is the evidence , given by .",
    "we will assume @xmath274 for all @xmath32 and @xmath70 to show conditions under which @xmath275 is integrable for any prior .",
    "using the identity @xmath276 and the log convexity of the gamma function we have , @xmath277 since @xmath278 , we have from the properties of the digamma function , @xmath279 and thus the upper bound , @xmath280.\\end{aligned}\\ ] ] although second term is unbounded in @xmath70 notice that @xmath281 ; thus , so long as @xmath282 , @xmath283 is integrable in @xmath70 . for the integral over alpha , it suffices to choose @xmath284 and consider the tail , @xmath285 . from and",
    "the asymptotic expansion @xmath286 as @xmath287 we see that in the limit of @xmath288 , @xmath289 where @xmath197 is a constant depending on @xmath57 , @xmath58 , and @xmath70 .",
    "thus , we have @xmath290 and so @xmath275 is integrable in @xmath32 so long as @xmath176 .",
    "we have , @xmath291\\\\ & = \\lim_{n\\to\\infty } \\left [ \\psi_0(\\alpha+n+1 ) -      \\frac{\\alpha+k_nd}{\\alpha+n}\\psi_0(1-d ) -       \\right . \\\\ & \\qquad\\qquad\\left .",
    "\\frac{1}{\\alpha+n }      \\left[\\sum_{i=1}^{k_n }   ( n_i - d ) \\psi_0(n_i - d+1)\\right ] \\right ] \\\\ & = \\lim_{n\\to\\infty } \\left [ \\psi_0(\\alpha+n+1 ) - \\sum_{i=1}^{k_n }      \\frac{n_i}{n } \\psi_0(n_i - d+1 ) \\right ] \\\\ & = -\\lim_{n\\to\\infty } \\sum_{i=1}^{k_n }      \\frac{n_i}{n } \\left[\\psi_0(n_i - d+1 ) - \\psi_0(\\alpha+n+1 ) \\right ]    \\end{aligned}\\ ] ] although we have made no assumptions about the tail behavior of @xmath5 , so long as @xmath292 , @xmath293 = { \\mathbb{e}}[\\sum_{i=1}^\\infty { \\mathbf{1}_{\\left\\ { x_i = k\\right\\ } } } ] = \\sum_{i=1}^\\infty p\\ { x_i = k\\ } = \\lim_{n\\to\\infty } n\\pi_k \\to \\infty$ ] , and we may apply the asymptotic expansion @xmath286 as @xmath287 to find , @xmath294   = h_{\\mathrm{plugin}}\\ ] ]    we now turn to the proof of consistency for pym .",
    "although consistency is an intuitively plausible property for pym , due to the form of the estimator our proof involves a rather detailed technical argument .",
    "because of this , we break the proof of theorem [ consistencyproof ] into two parts .",
    "first , we prove a supporting lemma .",
    "[ lem : convergence : head ] if the data @xmath178 have at least two coincidences , and are sampled from a distribution such that , for some constant @xmath190 , @xmath295 in probability , the following sequence of integrals converge .",
    "@xmath296       \\frac{p({{\\mathbf{x}}}_n | \\alpha , d ) p(\\alpha , d)}{p({{\\mathbf{x}}}_n ) }      \\mathrm{d}\\alpha \\mathrm{d}d      \\xrightarrow{p } { \\mathbb{e}}[{\\hat h_{\\text{plugin}}}| { { \\mathbf{x}}}_n]\\end{aligned}\\ ] ] where @xmath297 is an arbitrary constant .    notice",
    "first that @xmath298 $ ] is monotonically increasing in @xmath32 , and so @xmath299    \\frac{p({{\\mathbf{x}}}_n|\\alpha , d)}{p({{\\mathbf{x}}}_n ) } { \\ensuremath{\\,\\mathrm{d}{\\alpha}}}{\\ensuremath{\\,\\mathrm{d}{d } } } \\\\&\\leq    \\int_{\\alpha=0}^{k_n+c}\\int_{d=0}^1    { \\mathbb{e}}[h|k_n+c , d,{{\\mathbf{x}}}_n]\\frac{p({{\\mathbf{x}}}_n|\\alpha , d)}{p({{\\mathbf{x}}}_n ) } { \\ensuremath{\\,\\mathrm{d}{\\alpha}}}{\\ensuremath{\\,\\mathrm{d}{d}}}\\end{aligned}\\ ] ] we have that , @xmath300 = \\psi_0(k_n+c+n+1 ) \\\\&\\qquad\\qquad- \\frac {    ( 1+d)k_n+c}{k_n+n+c}\\psi_0(1-d ) \\nonumber      \\\\&\\qquad\\qquad- \\frac{1}{k_n+c+n } \\left(\\sum_{i=1}^{k_n } ( n_i - d ) \\psi_0(n_i -        d+1)\\right)\\nonumber\\end{aligned}\\ ] ] as a consequence of proposition [ finiteposterior ] , @xmath301 , and so the second term is bounded and controlled by @xmath193 .",
    "we let @xmath302 and , since @xmath303 , we focus on the remaining terms of .",
    "we also let @xmath304 , and note that @xmath305 .",
    "we find that , @xmath306 \\\\ & \\leq   \\log(n+k_n+c+1 )     + a(d , n )      \\\\ &     - \\sum_{i=1}^{k_n}\\left(\\frac{n_i-1}{k_n+n + c }       \\log(n_i)\\right ) \\\\ & = \\log(n+k_n+c+1 ) + a(d , n ) - \\\\&\\frac{n}{k_n+n + c } \\left[\\sum_{i=1}^{k_n}\\left(\\frac{n_i-1}{n }      \\log\\left(\\frac{n_i}{n}\\right)\\right ) +    \\frac{n - k_n}{n}\\log(n)\\right ] \\\\ & = \\log\\left(1 + \\frac{k_n+c+1}{n}\\right ) + a(d , n )   \\\\ & \\quad + \\log(n ) \\left [ \\frac{2k_n+c}{n+k_n+c } \\right]+ \\frac{n}{k_n+n + c } b \\\\ &",
    "= \\log\\left(1 + \\frac{k_n+c+1}{n}\\right ) + a(d , n )    \\\\ & \\quad+ \\frac{1}{1+(k_n+c)/n}\\frac{2k_n+c}{n^{1 - 1/c}}\\frac{\\log(n)}{n^{1/c}}+ \\frac{n}{k_n+n",
    "+ c } b \\\\ & \\to   { \\hat h_{\\text{plugin}}}+ o(1)\\end{aligned}\\ ] ] as a result , @xmath307    \\frac{p({{\\mathbf{x}}}_n|\\alpha , d)}{p({{\\mathbf{x}}}_n)}{\\ensuremath{\\,\\mathrm{d}{d } } } { \\ensuremath{\\,\\mathrm{d}{\\alpha } } } \\\\&\\leq    \\left [ { \\hat h_{\\text{plugin}}}\\int_{\\alpha=0}^{k_n+c } \\int_{d=0}^1 \\frac{p({{\\mathbf{x}}}_n|\\alpha , d)}{p({{\\mathbf{x}}}_n ) }      { \\ensuremath{\\,\\mathrm{d}{d}}}{\\ensuremath{\\,\\mathrm{d}{\\alpha } } } + o(1)\\right ] \\\\ & \\to { \\hat h_{\\text{plugin}}}\\end{aligned}\\ ] ] for the lower bound , we let @xmath308{\\mathbf{1 } _ { [ 0,k_n+c]}}(\\alpha)$ ] . notice that @xmath309 , so by dominated convergence @xmath310 = \\exp(-{\\hat h_{\\text{plugin}}})$ ] by proposition [ finiteposterior ] . and",
    "so by jensen s inequality , @xmath311 ) & \\leq    \\lim_{n\\to\\infty}{\\mathbb{e}}[\\exp(-h_{(\\alpha , d , n ) } ) ] = \\exp(-{\\hat h_{\\text{plugin } } } ) \\\\ \\implies & \\lim_{n\\to\\infty } { \\mathbb{e}}[h_{(\\alpha , d , n ) } ]   \\geq { \\hat h_{\\text{plugin}}},\\end{aligned}\\ ] ] and the lemma follows .",
    "+ we now turn to the proof of our primary consistency result",
    ". +    @xmath312       \\frac{p({{\\mathbf{x}}}_n | \\alpha , d ) p(\\alpha , d)}{p({{\\mathbf{x}}}_n ) }      \\mathrm{d}\\alpha \\mathrm{d}d      \\\\      & =       \\int_0^{\\alpha_0 } \\int_0 ^ 1      { \\mathbb{e}}[h|\\alpha , d , { { \\mathbf{x}}}_n ]       \\frac{p({{\\mathbf{x}}}_n | \\alpha , d ) p(\\alpha , d)}{p({{\\mathbf{x}}}_n ) }      \\mathrm{d}\\alpha \\mathrm{d}d      \\\\      & \\quad+      \\int_{\\alpha_0}^\\infty \\int_0 ^ 1      { \\mathbb{e}}[h|\\alpha , d , { { \\mathbf{x}}}_n ]       \\frac{p({{\\mathbf{x}}}_n | \\alpha , d ) p(\\alpha , d)}{p({{\\mathbf{x}}}_n ) }      \\mathrm{d}\\alpha \\mathrm{d}d\\end{aligned}\\ ] ]    if we let @xmath313 , by lemma [ lem : convergence : head ] , @xmath314       \\frac{p({{\\mathbf{x}}}_n | \\alpha , d ) p(\\alpha , d)}{p({{\\mathbf{x}}}_n ) }      \\mathrm{d}\\alpha \\mathrm{d}d      \\to { \\mathbb{e}}[h_\\mathrm{plugin}| { { \\mathbf{x}}}_n].\\end{aligned}\\ ] ] therefore , it remains to show that @xmath315       \\frac{p({{\\mathbf{x}}}_n | \\alpha , d ) p(\\alpha , d)}{p({{\\mathbf{x}}}_n ) }      \\mathrm{d}\\alpha \\mathrm{d}d       \\to 0\\end{aligned}\\ ] ] for finite support distributions where @xmath316 , this is trivial .",
    "hence , we only consider infinite support distributions where @xmath317 . in this case , there exists @xmath318 such that for all @xmath319 , @xmath320 , [ 0 , 1 ) ) \\neq 0 $ ] . since @xmath321 has a decaying tail as @xmath187 , @xmath322 , @xmath323 , thus , it is sufficient demonstrate convergence under an improper prior @xmath324 . using , @xmath325   \\leq \\psi_0(n + \\alpha + 1 ) \\leq n + \\alpha\\end{aligned}\\ ] ] we bound @xmath315       & \\frac{p({{\\mathbf{x}}}_n | \\alpha , d)}{p({{\\mathbf{x}}}_n ) }      \\mathrm{d}\\alpha \\mathrm{d}d       \\\\&\\leq      \\frac {      \\int_{\\alpha_0}^\\infty \\int_0 ^ 1      ( n + \\alpha - 1 ) p({{\\mathbf{x}}}_n | \\alpha , d )      \\mathrm{d}\\alpha \\mathrm{d}d       } { p({{\\mathbf{x}}}_n ) }      \\\\&\\qquad+      \\frac {      \\int_{\\alpha_0}^\\infty \\int_0 ^ 1      p({{\\mathbf{x}}}_n | \\alpha , d )      \\mathrm{d}\\alpha \\mathrm{d}d       } { p({{\\mathbf{x}}}_n)}\\end{aligned}\\ ] ] we focus upon the first term on the rhs since its boundedness implies that of the smaller second term .",
    "recall , that @xmath326 .",
    "we seek an upper bound for the numerator and a lower bound for @xmath327 .",
    "_ upper bound : _ first we integrate over @xmath70 to find the upper bound of the numerator .",
    "( for the following display only we let @xmath328 .",
    "@xmath329 fortunately , the first integral on @xmath70 will cancel with a term from the lower bound of @xmath327 .",
    "using rather than @xmath179 for concision .",
    "] , @xmath330 , @xmath331 _ lower bound : _ again , we first integrate @xmath70 , @xmath332 so , since @xmath333 , then @xmath334 where we ve used the fact that @xmath335 .",
    "finally , we obtian the bound , @xmath336 now , we apply the upper and lower bounds to bound pym .",
    "we have , @xmath337 where we have applied the asymptotic expansion for the beta function , @xmath338a consequence of stirling s formula .",
    "finally , we take @xmath339 so that the limit becomes , @xmath340 which tends to @xmath139 with increasing @xmath58 since , by assumption , @xmath341 .",
    "we thank e.  j.  chichilnisky , a.  m.  litke , a.  sher and j.  shlens for retinal data , and y.  .w .",
    "teh for helpful comments on the manuscript .",
    "this work was supported by a sloan research fellowship , mcknight scholar s award , and nsf career award iis-1150186 ( jp ) .",
    "parts of this manuscript were presented at the advances in neural information processing systems ( nips ) 2012 conference .",
    "e.  archer , i.  m. park , and j.  pillow .",
    "bayesian estimation of discrete entropy with mixtures of stick - breaking priors . in p.",
    "bartlett , f.  pereira , c.  burges , l.  bottou , and k.  weinberger , editors , _ advances in neural information processing systems 25 _ , pages 20242032 .",
    "nips , 2012 .",
    "m.  farach , m.  noordewier , s.  savari , l.  shepp , a.  wyner , and j.  ziv . on the entropy of dna : algorithms and measurements based on memory and rapid convergence . in _ proceedings of the sixth annual acm - siam symposium on discrete algorithms _ ,",
    "pages 4857 .",
    "society for industrial and applied mathematics , 1995 .",
    "j.  w. pillow , l.  paninski , v.  j. uzzell , e.  p. simoncelli , and e.  j. chichilnisky .",
    "prediction and decoding of retinal ganglion cell responses with a probabilistic spiking model .",
    "_ the journal of neuroscience _ , 25:0 1100311013 , 2005 .",
    "y.  teh . a hierarchical bayesian language model based on pitman - yor processes . _ proceedings of the 21st international conference on computational linguistics and the 44th annual meeting of the association for computational linguistics _ , pages 985992 , 2006 ."
  ],
  "abstract_text": [
    "<S> we consider the problem of estimating shannon s entropy @xmath0 from discrete data , in cases where the number of possible symbols is unknown or even countably infinite . </S>",
    "<S> the pitman - yor process , a generalization of dirichlet process , provides a tractable prior distribution over the space of countably infinite discrete distributions , and has found major applications in bayesian non - parametric statistics and machine learning . </S>",
    "<S> here we show that it also provides a natural family of priors for bayesian entropy estimation , due to the fact that moments of the induced posterior distribution over @xmath0 can be computed analytically . </S>",
    "<S> we derive formulas for the posterior mean ( bayes least squares estimate ) and variance under dirichlet and pitman - yor process priors . </S>",
    "<S> moreover , we show that a fixed dirichlet or pitman - yor process prior implies a narrow prior distribution over @xmath0 , meaning the prior strongly determines the entropy estimate in the under - sampled regime . we derive a family of continuous mixing measures such that the resulting mixture of pitman - yor processes produces an approximately flat prior over @xmath0 . </S>",
    "<S> we show that the resulting pitman - yor mixture ( pym ) entropy estimator is consistent for a large class of distributions . </S>",
    "<S> we explore the theoretical properties of the resulting estimator , and show that it performs well both in simulation and in application to real data .    </S>",
    "<S> entropy , information theory , bayesian estimation , bayesian nonparametrics , dirichlet process , pitman  yor process , neural coding </S>"
  ]
}