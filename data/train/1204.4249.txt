{
  "article_text": [
    "in his early work [ 9 ] , shannon proved that feedback could not increase the capacity of a point - to - point memoryless channel .",
    "however , feedback could improve error performance and simplify the transmission scheme for this kind of channel . in [ 10 ]",
    ", horstein proposed a simple sequential transmission scheme , which achieves the capacity of binary symmetric channel ( bsc ) and provides larger error exponents than traditional fixed length block coding .",
    "besides , schalkwijk and kailath also showed that feedback could improve error performance and/or simplify the transmission scheme for the point - to - point gaussian channel [ 7 ] , [ 8 ] . for gaussian multiuser channels ,",
    "the situation is more interesting . in",
    "[ 12 ] , gaarder and wolf proved that feedback can enlarge the capacity region of the multiple access channel , and ozarow [ 3 ] successfully constructed a simple coding scheme for the two user gaussian mac with feedback and reaffirmed that feedback could increase the capacity of the channel .",
    "furthermore , kramer devised a code for complex gaussian channel based on a beautiful property of the circulant matrix that has all columns of the dft ( discrete fourier transform ) matrix as its eigenvectors [ 14 ] .",
    "this code was proved to obtain the linear - feedback sum - capacity of the symmetric gaussian channel with feedback in [ 16 ] . by using the control - theoretic approach to communications with feedback , ardestanizadeh and fraceschetii [ 17 ] also proposed a linear code that has the same performance as kramer s code for symmetric gaussian complex channels .",
    "recently , shayevitz and feder [ 1 ] , [ 2 ] , and [ 4 ] have discovered an underlying principle between the horstein and schalkwijk - kailth schemes in a simple encoding scheme called posterior matching scheme for general point - to - point memoryless channels .",
    "the idea of posterior matching is that the transmitter encapsulates the information the receiver does not know up to present time in one random variable and then transmits that random variable to the receiver in the next transmission to refine the receiver s knowledge .",
    "the distribution of that variable will be selected in a way such that the input constraint is satisfied .",
    "later , bae and anastasopolous extended this scheme for the finite - state channel with feedback by using another approach [ 11 ] .",
    "ma and coleman provided a viewpoint on posterior matching from stochastic control perspectives [ 18 ] and generalized this encoding scheme to higher dimension via optimal transportation [ 19 ] .",
    "one interesting open problem is to extend the shayevitz and feder posterior matching scheme for multiuser cases . in this paper , using the same approach as shayevitz and feder used for point - to - point memoryless channels , we propose a posterior matching based encoding and decoding strategy for real gaussian macs , referred to as _ a time - varying posterior matching scheme _ , and analyze the error probabilities for all encoding - decoding schemes designed by using these strategies .",
    "we analyze the achievable rate region and error performance of encoding and decoding schemes using these strategies by defining a generalized iterated function systems ( gifs ) which has the _ generalized average contractive _ property ( asymptotically average contractive ) .",
    "refer to our theorem i for more details .",
    "note that our imposed constraint is less strict than the constraint that shayevitz and feder imposed to analyze the point - to - point memoryless channels . specifically , in theorem 6 in [ 4 ] , shayevitz and feder used the relations between the information rates and contraction properties of the iterated function system ( ifs ) to analyze the error probability for point - to - point cases . for the continuous cases , they assumed that the reverse iterated function system ( rifs ) , generated by the kernel @xmath0 and controlled by the identically distributed output sequence @xmath1 , has the average contractive property to analyze the error performance of their posterior matching schemes .",
    "that assumption requires the distribution at the output of the point - to - point memoryless channel be identically distributed when using their proposed encoding schemes .",
    "this also means that if the output distribution is not identically distributed , the error analysis in theorem 6 in [ 4 ] can not apply . for example , this situation happens with our proposed matching schemes for the gaussian mac in this paper .    finally , we illustrate our strategies by designing an encoding scheme that obtains optimal performance for the gaussian mac .",
    "specifically , our proposed code obtains the same performance as ozarow s code [ 3 ] for the general two - user gaussian channel , so it achieves the capacity of this channel . for the case",
    "when the number of users is greater than 3 , our proposed code obtains the same performance as the kramer s code in the sense of sum - rate , so it is optimal among linear code with respect to sum rate capacity . to the best of our knowledge ,",
    "the _ time - varying posterior matching _ in this paper is the first code designed for the real symmetric gaussian mac to achieve the linear - feedback sum - capacity when the number of users is greater than 3 .",
    "the rest of this paper is organized as follows .",
    "section ii presents the channel model and some mathematical preliminaries .",
    "sections iii , iv introduce the time - varying posterior matching idea , and perform the error analysis of an encoding - decoding scheme for the gaussian mac with feedback constructed by using that idea . a time - varying encoding - decoding strategy and error analysis for the general two - user white gaussian mac and the multiuser symmetric white gaussian mac",
    "are placed in section v. finally , section vi concludes this paper .",
    "upper - case letters , their realizations by corresponding lower - case letters , denote random variables . a real - valued random variable @xmath2 is associated with a distribution @xmath3 defined on the usual borel @xmath4-algebra over @xmath5 , and we write @xmath6 . the cumulative distribution function ( c.d.f . ) of @xmath2 is given by @xmath7)$ ] , and their inverse c.d.f is defined to be @xmath8 . the uniform probability distribution over @xmath9 is denoted through @xmath10 . the composition function @xmath11 . in this paper",
    ", we use the following lemma :    * lemma i : * let @xmath2 be a continuous random variable with @xmath12 and @xmath13 be an uniform distribution random variable , i.e. @xmath14 be statistical independent . then @xmath15 and @xmath16",
    ".    refer to [ 4 ] for the proof .",
    "big o notation ( with a capital letter o , not a zero ) , also called landau s symbol , is a symbolism used in complexity theory , computer science , and mathematics to describe the asymptotic behavior of functions",
    ". basically , it tells you how fast a function grows or declines . for the formal definition , suppose @xmath17 and @xmath18 are two functions defined on positive integer number .",
    "we write @xmath19 ( or @xmath20 for @xmath21 to be more precise ) if and only if there exists constants @xmath22 and @xmath23 such that @xmath24 intuitively , this means that @xmath25 does not grow faster than @xmath26 .",
    "in addition to big o notations , another landau symbol is used in mathematics : the little o. formally , we write @xmath27 for @xmath21 if and only if for every @xmath28 there exists a real number @xmath22 such that for all @xmath29 we have @xmath30 . if @xmath31 , this is equivalent to @xmath32 .",
    "a hadamard matrix [ 15 ] of order @xmath33 is an @xmath34 matrix of @xmath35s and @xmath36s such that @xmath37 .",
    "in fact , it is not yet known for which values of @xmath33 an @xmath38 does exists .",
    "however , we know that if a hadamard matrix of order @xmath33 exists , then @xmath33 is @xmath39 , or a multiple of @xmath40 . moreover , if @xmath33 is of the form @xmath41 , @xmath42 a positive integer , we can construct @xmath38 by using the sylvester method .",
    "besides , the paley construction , which uses quadratic residues , can be used to construct hadamard matrices of order @xmath33 , where @xmath33 is of the form @xmath43 , @xmath44 is a prime , and @xmath33 is a multiple of @xmath40 .",
    "consider the communication problem between @xmath45 senders and a receiver over a multiple access channel with additive gaussian noise ( awgn - mac ) when channel outputs are noiselessly fed back to all the senders ( figure 1 ) .",
    "each sender @xmath46 wishes to reliably transmit a random message point @xmath47 , which is uniformly distributed over the unit interval with its binary expansion representing an infinite independent - identically - distributed ( i.i.d . )",
    "bernoulli(1/2 ) sequence , to the receiver . at each time",
    "@xmath33 , the output of the channel is @xmath48 where @xmath49 is the transmitted symbol by sender @xmath42 at time @xmath33 , @xmath50 is the output of the channel , and @xmath51 is a discrete - time zero mean white gaussian noise process with unit average power , i.e. , @xmath52=1 $ ] and is independent of @xmath53 .",
    "we assume that output symbols are casually fed back to the sender and the transmitted symbol @xmath54 for sender @xmath42 at time @xmath33 can depend on both the message @xmath47 and the previous channel output sequence @xmath55 .",
    "a _ transmission scheme _ for a gaussian mac is a set of @xmath45 sequences of transmission functions @xmath56 for @xmath57 , so that the input to the channel generated by the transmitter is given by @xmath58    a _ decoding rule _ for a mac is set of sequences of measurable mappings @xmath59 , where @xmath60 is the set of all open intervals in @xmath9 and @xmath57 . here ,",
    "@xmath61 , refers as to the decoded interval for the user @xmath42 .",
    "the error probabilities at time @xmath33 associated with a transmission scheme and a decoding rule , is defined as @xmath62 and the corresponding achievable rate vector at time @xmath33 is defined to be @xmath63    we say that a transmission scheme together with a decoding rule achieve a rate vector @xmath64 over a gaussian mac",
    "if for all @xmath46 we have @xmath65    the rate vector is achieved within input power constraints @xmath66 , if in addition @xmath67    an optimal fixed rate decoding rule for a mac with rate region @xmath68 is one that decodes a vector of fixed length intervals @xmath69 , whose marginal posteriori probabilities are maximal , i.e. , @xmath70    an optimal variable rate decoding rule with target error probabilities @xmath71 is one that decodes a vector of minimal - length intervals @xmath72 with accumulated marginal posteriori probabilities exceeds corresponding targets , i.e. , @xmath73    both decoding rules make use of the marginal posterior distribution of the message point @xmath74 which can calculate online at the transmitter @xmath42 and the receiver",
    ". refer [ 4 ] for more details .",
    "a proof that the achievability in the sense of @xmath75 and @xmath76 implies that the achievability in the standard framework are in the appendix . + * lemma ii : * the achievability in the definition @xmath75 and @xmath76 implies the achievability in the standard framework .",
    "refer to the appendix .",
    "in this part , we firstly review the posterior matching scheme proposed by ofer shayevitz and meir feder for point - to - point channel in [ 4 ] .",
    "specifically , the authors argued that after the receiver observed the output sequence @xmath77 , there is still some `` missing information '' that can be encapsulated in a random variable @xmath78 with the following properties : + ( i ) @xmath78 is statistically independent of @xmath77 .",
    "+ ( ii ) the message point @xmath79 can be a.s .",
    "uniquely recovered from @xmath80 + with that line of thought , they proposed a principle for generating the next channel input as follow : + _ the transmission function @xmath81 should be selected so that @xmath82 is @xmath83-distributed , and is a fixed function of some random variable @xmath78 satisfying properties @xmath84 and @xmath85 . _ + * lemma iii : * ( _ posterior matching scheme [ 4 ] _ ) . the following transmission scheme satisfies the posterior matching principle for any @xmath33 : @xmath86 based on the transmission functions defined in section ii - b [ 4 ] , the input to the channel is a sequence of random variables given by @xmath87    refer to [ 4 ] for the proof .      in this section ,",
    "we propose a posterior matching scheme for additive gaussian multiple access channel ( mac ) with feedback , called _ time - varying posterior matching_. our encoding proposal is based on the following lemma : + * lemma iv : * for an additive white gaussian mac with feedback having @xmath45 inputs and one output , let the output signal be a linear combination with known coefficients of the input signals , i.e. @xmath88 where @xmath89 is the additive white gaussian noise and the coefficients @xmath90 are part of the coding scheme , but are viewed as part of the channel for the purpose of deriving the posterior matching rule .",
    "also assume that the covariance matrix among transmitted symbols at each time slot @xmath33 defined as @xmath91 & \\cdots&e[x_n^{(1)}x_n^{(m)}]\\\\e[x_n^{(2)}x_n^{(1 ) } ] & \\cdots&e[x_n^{(2)}x_n^{(m)}]\\\\ \\vdots&\\ddots&\\vdots\\\\e[x_n^{(m)}x_n^{(1 ) } ] & \\cdots&e[x_n^{(m)}x_n^{(m)}]\\end{array}\\right]\\ ] ] then the posterior matching scheme in lemma iii at each transmitter @xmath42 , i.e. @xmath92 becomes a _ time - varying posterior matching _",
    "given by @xmath93 where @xmath94 is a random variable encapsulating the input power constraint , @xmath95 , and @xmath47 is the intended transmitted message at the transmitter @xmath42 .",
    "in addition , the correlation matrix among transmitted symbols at each time slot @xmath33 , i.e. @xmath96 , can be calculated online at the transmitters and the receiver .",
    "especially , the posterior distribution @xmath97 can be calculated online at both the transmitter @xmath42 and the receiver .    to begin with ,",
    "we show that @xmath98 constitutes a markov chain . indeed ,    * from the lemma i , we know that @xmath99 and this holds for all @xmath100 .",
    "then , @xmath101 and is statistically independent of @xmath102 .",
    "therefore , @xmath54 is independent of @xmath102 . *",
    "since the resulting scheme is linear by our assumption , and the channel is additive gaussian , @xmath103 are jointly gaussian .",
    "the two observations imply that @xmath104 are mutually independent of @xmath102 , which together with the memoryless property of the channel , implies the markov chains .",
    "moreover , by our construction of the multi - letter posterior matching formula , we have @xmath105 since @xmath106 are continuous random variables , their c.d.f . and inverse c.d.f .",
    "functions are continuous , hence the composite function @xmath107 is continuous on @xmath108 . besides",
    ", the monotonicity of this function is originated from the monotonicity of the c.d.f . and",
    "the inverse c.d.f .",
    "now , let s return to prove the lemma iv .",
    "note that @xmath109 @xmath110 @xmath111 @xmath112 @xmath113 where ( a ) follows from the fact that the transmission function is continuous and monotone , and ( b ) follows from the aforementioned fact that @xmath114 forms a markov chain for any @xmath42 .",
    "finally , we have @xmath115 @xmath116 @xmath117    moreover , with _ time - varying posterior matching _ transmission applied for gaussian mac , @xmath118 is a linear combination of @xmath119 ( as we will see in the proof of the theorem ii below ) , so @xmath118 is a linear combination of @xmath120 .",
    "therefore , the correlation between @xmath118 and @xmath121 only depends on the correlations among transmitted symbols at time @xmath33 , i.e. @xmath122 . in other words",
    ", @xmath123 is a function of @xmath96 , so the transmitters and receiver can calculate the matrix @xmath96 online at both transmitters and receiver .",
    "moreover , from the relation @xmath88 , we see that the distribution of @xmath124 is a function of all elements in the correlation matrix @xmath125 .",
    "that concludes the proof .    _",
    "remark : _ we refer the transmission scheme in lemma iv to as _ time - varying posterior matching scheme_. for a continuous point - to - point memoryless channel , the distribution @xmath126 does nt depend on @xmath33 , hence we have the posterior matching scheme for this case like the formula ( 16 ) in [ 4 ] .",
    "however , in a mac ( for example additive white gaussian mac ) , where each received signal is a linear combination of all transmitted signals and gaussian noise , the distribution @xmath127 between the input @xmath42 and the output may be dependent on @xmath33 .",
    "therefore , _ time - varying posterior matching scheme _ may be the solution to overcome this problem .",
    "however , we will see from our proof in the theorem i below the variable rate decoding rule , or _ generalized reverse iterated function system ( grifs ) _ , can be applied at receiver to decode signals if and only if all the distributions @xmath127 can be calculated online at the corresponding transmitters and receiver . with the result in the lemma iv , this condition is always satisfied when _ time - varying posterior matching encoding schemes _ used at transmitters .",
    "we will show in the next parts that using _ time - varying posterior matching scheme _ at transmitters can obtain optimal performances for some known cases .",
    "in this section , we analyze error performance for gaussian mac with feedback employing the _ time - varying posterior matching scheme _ at the transmitters and variable - decoding rule at the receiver . +",
    "* theorem i : * consider a real gaussian mac with @xmath45 transmitters and one receiver without input power constraints . assuming that at each transmitter @xmath42 , transmitted sequence @xmath54 conforms to the _ time - varying posterior matching rule _",
    ", as following : @xmath128 where @xmath129 is a gaussian random variable , and @xmath130 is the output of the gaussian mac which is a linear combination of all these transmitted signals at time @xmath33 .",
    "let @xmath131 and @xmath132 as the global lipschitz operator . under the conditions that @xmath133\\right )",
    "< 1\\ ] ] define : @xmath134\\right ) $ ] then the rate region @xmath135 is achievable and the error probabilities @xmath136 decay to zero as @xmath137    first , observe that since the distributions @xmath138 can be calculated online at both transmitters and receiver by lemma iv , therefore @xmath139 can be calculated online at both the transmitters and receiver . denote @xmath140 , referred as to a _ generalized iterated function system ( gifs ) _ generated by the _ kernel _ sequence @xmath141 . for each @xmath142 , select a fixed interval @xmath143 as the decoded interval with respect to @xmath118 .",
    "define the corresponding interval at the origin to be @xmath144 and set them to be the decoded interval for @xmath145 , and so the decoded interval for @xmath47 are set to be @xmath146 .",
    "it is easy to see that @xmath147 where @xmath148 for any fixed rate @xmath149 , we can find an @xmath150 such that @xmath151 .",
    "observe that : @xmath152 @xmath153\\ ] ] @xmath154 where @xmath155 . here , ( a ) follows from the definition of the instant corresponding achievable rate vector at time @xmath33 in the section ii above , and ( b ) follows from the fact that we set the decoded interval @xmath156 .",
    "+ on the other hand , since @xmath157 , there exists an @xmath158 such that @xmath159",
    ". let @xmath160 .",
    "from ( 7 ) we have @xmath161 @xmath162 @xmath163\\ ] ] @xmath164 @xmath165 @xmath166 @xmath167 @xmath168 where @xmath169 follows from the markov s inequality , @xmath170 follows from @xmath160 , and @xmath171 is a recursive application of the preceding transitions , @xmath172 follows from @xmath173 above and recursive applications of the preceding transitions .    from ( 8) ,",
    "it is easy to see that a sufficient condition for both @xmath174 is given by choosing @xmath175 .",
    "since @xmath176 depends only on the length of @xmath177 , so we can choose @xmath178 .",
    "furthermore , from the lemma i , we are easy to come to conclusion that @xmath179 .",
    "denote @xmath180 as the well - known tail function of the standard normal distribution and use the chernoff bound of this function we obtain @xmath181 @xmath182 @xmath183 here , ( a ) follows from the fact that @xmath184 is symmetric @xmath185 , and ( b ) follows from the chernoff bound for the q - function @xmath186 .    to put it simply",
    ", any rate vector @xmath187 is achievable , where @xmath188\\right)\\ ] ] the error probabilities decay to zero as @xmath189    _ remark : _ since we can estimate @xmath190 and know our desired rate @xmath191 in advance , it is possible to choose @xmath192 by target .",
    "this means that the decoding algorithm is technically realizable .",
    "however , there is a tradeoff between the transmission rate @xmath191 ( the possible values of @xmath192 ) and the code length @xmath33 .",
    "if we transmit at the rate @xmath191 very close to @xmath190 , we need to choose @xmath192 to be very small . as a result",
    ", the required @xmath193 may be very big .",
    "furthermore , the fact that @xmath194 is very lose to @xmath191 also makes the error probabilities slowly decayed to zero . to put it",
    "simply , the code length @xmath33 may be very large if we transmit at the rate @xmath191 is nearly @xmath190 .",
    "on the contrary , being able to choose quite large @xmath192 makes the required @xmath193 smaller and the decay of error probabilities faster .",
    "in this section , we consider a real gaussian mac with @xmath45 receivers and input power constraints @xmath195 at the transmitters @xmath196 , respectively as defined in the section ii . our encoding scheme for this channel as following :      * at the time interval @xmath33",
    ", each transmitter @xmath142 creates a random variable @xmath54 following the posterior matching rule : @xmath197 @xmath198 where @xmath129 is a gaussian random variable , and @xmath130 is the output of the mac . * after that , the transmitter @xmath42 sends the signal @xmath199 where @xmath200^t$ ] is the column ( @xmath33 mod @xmath201 ) of the hadamard matrix @xmath45 by @xmath45 .",
    "* at each time slot @xmath33 , the receiver selects a fixed interval @xmath143 as the decoded interval with respect to @xmath118 . *",
    "then , set the decoded interval @xmath202 as the decoded interval with respect to @xmath203 , where @xmath204 and @xmath131 .",
    "* the receiver sets the decoded interval for the message @xmath47 is @xmath205    we refer this encoding strategy as _ gaussian mac posterior matching feedback coding and decoding strategy_. + * theorem ii * : using the _ gaussian mac posterior matching feedback coding and decoding strategy _ above , the rate region @xmath135 is achievable for gaussian mac with feedback , where @xmath206 by setting the target error probabilities @xmath207 where @xmath208 with @xmath209}{\\sqrt{p_t p_l}},\\hspace{8 mm } t \\neq l \\\\ 1 , \\hspace{10 mm } t = l \\end{array}\\right.\\ ] ]    applying the lemma i , we see that for any @xmath142 then : @xmath210 observe that , by this transmission strategy , each transmitter @xmath211 transmits @xmath212 at time @xmath33 with @xmath213 , thus the input power constraints at all transmitters are always satisfied at each transmission time @xmath214 moreover , the output at receiver at time @xmath33 will be @xmath215 thus @xmath216 @xmath217 then , we have @xmath218= \\frac{\\mbox{cov}(x_n^{(m)},y_n)}{\\mbox{var}(y_n)}y_n\\ ] ] @xmath219 and @xmath220 ^ 2}{\\mbox{var}(y_n)}\\ ] ] @xmath221 where @xmath222 and @xmath223 finally , we obtain @xmath224 where @xmath225 moreover , from @xmath226 we have : @xmath227 thus , @xmath228 combining with @xmath229 , we obtain : @xmath230 hence , @xmath231 finally , we have : @xmath232\\right)=\\limsup_{n \\rightarrow \\infty } \\sqrt{\\frac{b_n^{(m ) } } { p_m } } \\ ] ] @xmath233^{1/2}\\ ] ] if we can achieve @xmath234 then @xmath235 < 1\\ ] ]    this means that the condition in the theorem i is satisfied , which leads to the rate region @xmath135 is achievable , where @xmath236\\right)\\ ] ] @xmath237    note that this capacity region is obtained by setting the target error probabilities @xmath238 under the constraints @xmath239 which have the well - known double - exponential behavior .    in the following ,",
    "we will specify achievable rate regions , and error probabilities for two cases : the general two - user gaussian mac , and the real symmetric gaussian mac when the number of users is arbitrary .",
    "+ from the theorem ii , we see that the strategy to design posterior encoding scheme for multiple access channels with feedback is to find the sequences @xmath90 such that @xmath234 for all @xmath240 .",
    "+ _ case 1 : two - user gaussian mac with feedback_. + to show that ozarow s coding scheme [ 3 ] is a special case of our posterior matching framework , we can set @xmath241 and later prove that @xmath242 . observe that the constraint @xmath243 can be also checked to be satisfied by this setting since @xmath244 ^ 2}{p_1+p_2 + 2|\\rho_n|\\sqrt{p_1p_2}+1}<1\\ ] ] for @xmath245 and @xmath246 ^ 2}{p_1+p_2 + 2|\\rho_n|\\sqrt{p_1p_2}+1}<1\\ ] ] for @xmath247 .",
    "+ now we need to find the recursion of @xmath248 and @xmath249 in this case .",
    "observe that the output sequence : @xmath250 where @xmath89 is noise process and @xmath251 .",
    "+ from @xmath252 we have @xmath253 @xmath254 and @xmath255 @xmath256 therefore , we obtain : @xmath257 @xmath258 and @xmath259 @xmath260 @xmath261}{\\sqrt{\\mbox{var}(x_{n+1}^{(1)})\\mbox{var}(x_{n+1}^{(2})}}\\ ] ] @xmath262[p_1(1-\\rho_n^2)+1]}}\\ ] ] + finally , the _ time - varying posterior matching encoding _",
    "scheme for gaussian mac with feedback in this special case as following : +    * step 1 : @xmath263 = 0\\ ] ] transmitter 1 sends : @xmath264 transmitter 2 sends : @xmath265 * step @xmath266 , + both transmitters estimate : @xmath267[p_1(1-\\rho_n^2)+1]}}\\ ] ] transmitter 1 sends : @xmath257 @xmath268 transmitter 2 sends : @xmath269 @xmath270    * achievable rate region and error analysis : * + for this special case , we have @xmath271 and @xmath272    apply the theorem ii above , we obtain the achievable rate region for gaussian mac with two users as @xmath273 where @xmath274    similarly , @xmath275 by setting the target error probability to @xmath276 and @xmath277    much like ozarow in @xmath278 $ ] , at the reception @xmath279 , the receiver adds an independent random variable @xmath280 before feeding back the first receiver signal to the transmitters 1 and 2 to set @xmath281 , where @xmath282 is the biggest solution in @xmath9 of the following equation : @xmath283[p_1(1-\\rho^2)+1]}}=0\\ ] ]    by this changing , from @xmath284 we see that @xmath285 , so @xmath242 as mentioned above .",
    "we also have @xmath286 , where @xmath282 is a positive solution of the equation @xmath287 .",
    "replace this result to @xmath288 and combine with @xmath287 , we have : @xmath289\\ ] ] @xmath290\\ ] ] @xmath291 where @xmath282 is defined above .",
    "we see that , all the results are the same as ozarow s results in [ 3 ] .",
    "so our posterior matching encoding scheme is optimal for gaussian channel mac with two users .",
    "+ _ case 2 : m - user symmetric gaussian mac with feedback_. we consider symmetric case , where @xmath292 . + * achievable rate region and error analysis : *    assuming that all the transmitted messages are statistically independent .",
    "define the normalized covariance matrix by @xmath293\\ ] ] then @xmath294 & \\cdots&e[x_n^{(1)}x_n^{(m)}]\\\\e[x_n^{(2)}x_n^{(1 ) } ] & \\cdots&e[x_n^{(2)}x_n^{(m)}]\\\\ \\vdots&\\ddots&\\vdots\\\\e[x_n^{(m)}x_n^{(1 ) } ] & \\cdots&e[x_n^{(m)}x_n^{(m)}]\\end{array}\\right]\\ ] ] @xmath295\\ ] ] where @xmath296}{\\sqrt{\\mbox{var}(x_n^{(m)})\\mbox{var}(x_n^{(k)}})}=\\frac{e[x_n^{(m)}x_n^{(k)}]}{p}\\ ] ] is the correlation coefficient between @xmath54 and @xmath297 . + we will prove by induction that the normalized covariance has all the columns of the hadamard matrix @xmath298 by @xmath299 as its eigenvectors and that @xmath96 is symmetric positive definite for all @xmath300    indeed , with the assumption all the transmitted information messages are statistically independent , we will have @xmath301 , which is an identity matrix of size @xmath45 .",
    "therefore , it is obvious that all the columns of the hadamard matrix @xmath298 by @xmath299 are eigenvectors of the matrix @xmath302 and that @xmath301 is a positive definite matrix .",
    "now , assume that @xmath96 has all columns of the hadamard matrix @xmath298 by @xmath299 as its eigenvectors and that @xmath96 is positive definite for some @xmath303 . since we assumed that @xmath96 is symmetric positive definite matrix , all its eigenvalues are positive .",
    "denote by @xmath304 the @xmath45 columns of the hadamard matrix @xmath305 . by this encoding scheme",
    ", we set the vector @xmath306 .",
    "assume that @xmath307 is the eigenvalue of @xmath96 associated with the @xmath308 eigenvector .",
    "we have @xmath309 on the other hand , we also have @xmath310 where @xmath311 is the @xmath42th column of the matrix @xmath96 .",
    "note that @xmath312 , this means that @xmath313 hence , @xmath314 moreover , observe that @xmath315 combining these results , we obtain : @xmath316 and @xmath317 substitute @xmath318 and @xmath319 into @xmath320 , we obtain : @xmath321 = \\mbox{cov}\\left(x_n^{(m)},y_n\\right)\\ ] ] @xmath322 @xmath323 = \\mbox{var}(y_n)=p \\sum_{t=1}^m \\sum_{l=1}^m \\alpha_n^{(t ) } \\alpha_n^{(l ) } \\rho_n^{(t , l ) } + 1\\ ] ] @xmath324 @xmath325 @xmath326\\ ] ] observe that from the proof of the theorem ii above , then @xmath327 then , we have @xmath328}{p}=\\ ] ] @xmath329}{\\sqrt{b_n^{(m)}b_n^{(k)}}}\\ ] ] @xmath330+a_n^{(k ) } e[x_n^{(m ) } y_n ] } { \\sqrt{b_n^{(m)}b_n^{(k)}}}\\ ] ] moreover , from ( 13 ) we know that @xmath331=a_n^{(m ) } e[y_n^2]\\ ] ] therefore , @xmath332}{\\sqrt{b_n^{(m)}b_n^{(k)}}}\\ ] ] replacing the results in @xmath333 , and @xmath334 to this expression , we obtain @xmath335 @xmath336 for all @xmath337 .",
    "+ since we assumed that @xmath96 is symmetric positive definite matrix , hence @xmath338 , hence @xmath339 . therefore , from @xmath340",
    ", it is easy to see that @xmath341 . in other words",
    ", @xmath123 is also a symmetric matrix .",
    "+ moreover , from @xmath340 , we also have @xmath342 denote @xmath343 $ ] .",
    "we see that the columns of @xmath38 creates @xmath45 linearly independent eigenvectors of the matrix @xmath96 . moreover , we also have @xmath344 @xmath345    note that since all columns of @xmath38 are eigenvectors of the matrix @xmath96 , so all the columns of the matrix @xmath346 are also eigenvectors of the matrix @xmath96 , hence we has the following eigenvalue decomposition @xmath347 where @xmath348 is a diagonal matrix .",
    "+ moreover , we have @xmath349^t { \\bf \\alpha}_n^t{\\bf h}_{n+1}\\ ] ] where @xmath350\\ ] ] @xmath351\\ ] ]    from @xmath352 , the matrix @xmath353 must be a diagonal one since the right side of @xmath354 is a diagonal matrix .",
    "hence , all columns of the matrix @xmath346 are eigenvectors of the matrix @xmath123 .    assuming @xmath355 are @xmath45 eigenvalues corresponding to eigenvectors which are columns of the matrix @xmath38 . by this notation",
    ", we see that @xmath356 .",
    "+ combining @xmath357 , and @xmath358 we obtain @xmath359 for all @xmath360 .    since we assumed that all eigenvalues of @xmath96 are positive ( @xmath96 symmetric positive definite ) , from @xmath361 we see that all eigenvalues of @xmath123 are also positive .",
    "note that we also confirmed that @xmath123 is symmetric above , therefore @xmath123 is a symmetric positive definite matrix . in short ,",
    "if @xmath96 is a symmetric positive definite matrix and has all columns of the hadamard matrix as its eigenvectors , then @xmath123 has all these properties .",
    "this concludes our proof by induction .    according to the lemma 1 [ 12 ] ( or see the appendix below ) ,",
    "the sequence @xmath362 converges ( or can be forced to converge ) to a fixed point @xmath363 , which is the biggest positive solution in @xmath364 $ ] of the following equation @xmath365^m\\ ] ] from ( 27 ) , ( 28 ) we obtain @xmath366 @xmath367 since @xmath368 , we have @xmath369 therefore , the constraints in the theorem ii is satisfied .",
    "applying the result of this theorem , we have any rate less than @xmath370\\ ] ] @xmath371\\ ] ] is achievable , for all @xmath240 .",
    "hence , any sum rate which is less than @xmath372 @xmath373^m\\right)=\\frac{1}{2}\\log\\left(1+pm\\lambda^*\\right)\\ ] ] is achievable , where @xmath363 is solution in the @xmath364 $ ] of the equation @xmath374 .",
    "this result coincides with the formula ( 68 ) in [ 14 ] .",
    "the paper [ 16 ] proves that this achievable sum rate is optimal for the class of linear feedback coding .",
    "a posterior matching based encoding - decoding strategy for general gaussian mac with feedback was proposed , and achievable rate region , error performance were drawn .",
    "finally , we analyzed error performance of the proposed posterior encoding scheme and showed that the _ time - varying posterior matching scheme _ and variable rate decoding ideas can be applied to gaussian mac and obtain optimal performances . specifically , the proposed encoding scheme achieves the capacity of two - user feedback gaussian mac as well as linear - feedback sum - rate for symmetric gaussian mac with feedback where the number of users is arbitrary .",
    "moreover , by the encoding scheme s structure , which uses the spreading codes like the hadamard matrix , our encoding scheme can be directly applied to cdma systems with feedback . finally , by analyzing all arguments in the theorem i , the _ time - varying posterior matching scheme _",
    "approach in this paper might be applied for other gaussian and non - gaussian multiuser channels to achieve optimal performances .",
    "we use the same line argument as lemma 1 [ 4 ] .",
    "assume we are given a transmission scheme with @xmath45 transmission functions @xmath375 and a decoding rule which are known to achieve the rate vector @xmath376 .",
    "for simplicity , we assume that the decoding rule is fixed rate @xmath377 for all @xmath100 ) , since any variable rate decoding rule can be easily mapped into a fixed rate rule that achieves the same rate vector .",
    "it is easy to see that in order to prove that the above translates into achievability for some rate vector @xmath378 in the standard framework , it is enough to show we can find @xmath45 sequences @xmath379 and such that we have the uniform achievability over @xmath380 , i.e. , @xmath381 we now show how @xmath380 can be constructed for any @xmath149 .",
    "let @xmath382 be the ( average ) error probability associated with our scheme and the fixed rate vector @xmath383 .",
    "define @xmath384 @xmath385 and write @xmath386 @xmath387 and so we have that @xmath388 .",
    "it is now easy to see that if we want to select @xmath380 such that @xmath389 , and also @xmath390 , then a sufficient condition is that @xmath391 for some positive @xmath392 .",
    "this condition can be written as @xmath393 @xmath394 at the same time , we also have by definition @xmath395 @xmath396    we use the similar arguments as the appendix a [ 14 ] .",
    "it is difficult to prove that the recursion ( 36 ) is convergence .",
    "however , we know that for @xmath397 @xmath398 \\lambda_n^{(2 ) } = \\ ] ] @xmath399 \\left[\\frac{1+pm\\lambda_{n-1}^{(1)}}{1+p\\lambda_n^{(1)}(m-\\lambda_{n-1}^{(1)})}\\right]\\lambda_{n-1}^{(2)}\\ ] ] @xmath400\\ ] ] @xmath401\\ ] ] therefore , if the sequence @xmath362 is convergent , it will converges to the solution of the following equation : @xmath402^m\\ ] ] it is easy to show that this equation has at least one solution in the @xmath403 $ ]",
    ". set @xmath404 to be the biggest solution of this equation .",
    "we also set @xmath405 we will force the recursion ( 36 ) to yield the desired values @xmath406 at time @xmath407 .    to perform this forcing ,",
    "we replace @xmath408 by @xmath409 at time @xmath33 and apply ( 36 ) @xmath410 times to get @xmath411\\ ] ] for all @xmath42 , where @xmath412\\ ] ] we thus have , for @xmath413 @xmath414 solving for @xmath409 , we have @xmath415 where @xmath362 can be computed recursively via @xmath416 and @xmath417",
    "the author would like to thank david j. love , purdue university , west lafayette , united states for introducing me to the communications with feedback .",
    "the author also would like to thank anonymous reviewers for their useful suggestions to improve the manuscript .    1 o.shayevitz and m.feder , `` communication with feedback via posterior matching , '' in _ proc .",
    "information theory _ , jun .",
    "o.shayevitz and m.feder , `` the posterior matching feedback scheme : capacity achieving and error analysis , '' in _ proc . int .",
    "information theory _ , jul .",
    "l.h.ozarow , `` the capacity of the white gaussian multiple access channel with feedback , '' _ ieee trans .",
    "inf . theory _",
    "it-30 , no .",
    "623 - 639 , jul .",
    "o.shayevitz and m.feder , `` optimal feedback communication via posterior matching , '' _ ieee trans .",
    "inf . theory _ ,",
    "it-57 , no.3 , pp.1186 - 1221 , mar .",
    "p.diaconis and d.freedman , `` iterated random functions , '' _ siam rev .",
    "_ , vol.41 , no.1 , pp.45 - 76 , 1999 .",
    "d.steinsaltz , `` locally contractive iterated function systems , '' _ ann . of .",
    "_ , vol.27 , no.4 , pp.1952 - 1979 , oct . 1999 .",
    "j.p.m.schalkwijk and t.kailath , `` a coding scheme for additive noise channels with feedback part i : no bandwidth constraint , '' _ ieee trans .",
    "inf . theory _ , vol , it-12 , pp.172 - 182 , apr .",
    "j.p.m.schalkwijk , `` a coding scheme for additive noise channels with feedback part ii : band - limited signal , '' _ ieee trans .",
    "inf . theory _ , vol.it-12 , pp.183 - 189 , apr .",
    "c.e.shannon , `` the zero - error capacity of a noisy channel , '' _ ire trans .",
    "inf . theory _ ,",
    "it-2 , pp .",
    "8 - 19 , sep . 1956 .",
    "m.horstein , `` sequential transmission using noiseless feedback , '' _ ire trans .",
    "inf . theory _ ,",
    "it-9 , pp .",
    "136 - 143 , jul .",
    "j.h.bae and a.anastasopolous , `` a posterior matching scheme for finite - state channels with feedback , '' in _ proc .",
    "information theory _ , jun .",
    "n.t.gaarder and j.k.wolf , `` the capacity region of a multiple access discrete memoryless channel can increase with feedback , '' _ ire trans .",
    "inf . theory _ ,",
    "it-21 , pp.100 - 102 , jan .",
    "thomas m.cover and joy a. thomas , _ elements of information theory , _",
    "2nd ed.,new jersey : john wiley @xmath418 sons , inc . , 2006 .",
    "gerhard kramer , `` feedback strategies for white gaussian interference networks , '' _ ire trans .",
    "inf . theory _ ,",
    "it-48 , pp.1423 - 1438 , jan .",
    "stephen b. wicker , _ error control systems for digital communication and storage , _ prentice hall , inc .",
    "ehsan ardestanizadeh , michele a.wigger , y.h .",
    "kim , and tara javidi , `` linear - feedback sum - capacity for gaussian multiple access channels with feedback , '' in _ proc .",
    "information theory _ , jun .",
    "ehsan ardestanizadeh , massimo franceschetti , `` control - theoretic approach to communication with feedback : fundamental limits and code design , '' in _",
    "48th annual allerton conference _ , sep .",
    "29 - oct . 1 , 2010 .",
    "todd p. coleman , `` a stochastic control viewpoint on posterior matching - style communication schemes , '' in _ proc .",
    "inf . theory _",
    ", june . , 2009 .",
    "rui ma , todd p. coleman , `` generalizing the posterior matching scheme to higher dimensions via optimal transportation , '' in _",
    "49th annual allerton conference _ , sep ."
  ],
  "abstract_text": [
    "<S> posterior matching is a method proposed by ofer shayevitz and meir feder to design capacity achieving coding schemes for general point - to - point memoryless channels with feedback . in this paper </S>",
    "<S> , we present a way to extend posterior matching based encoding and variable rate decoding ideas for gaussian mac with feedback , referred to as _ time - varying posterior matching scheme _ , analyze the achievable rate region and error probabilities of the extended encoding - decoding scheme . the _ time - varying posterior matching scheme _ is a generalization of the shayevitz and feder s posterior matching scheme when the posterior distributions of the input messages given output are not fixed over transmission time slots . </S>",
    "<S> it turns out that the well - known ozarow s encoding scheme , which obtains the capacity of two - user gaussian channel , is a special case of our extended posterior matching framework as the schalkwijk - kailath s scheme is a special case of the point - to - point posterior matching mentioned above . </S>",
    "<S> furthermore , our designed posterior matching also obtains the linear - feedback sum - capacity for the symmetric multiuser gaussian mac . besides </S>",
    "<S> , the encoding scheme in this paper is designed for the real gaussian mac to obtain that performance , which is different from previous approaches where encoding schemes are designed for the complex gaussian mac . </S>",
    "<S> more importantly , this paper shows potential of posterior matching in designing optimal coding schemes for multiuser channels with feedback .    </S>",
    "<S> gaussian multiple access channel , feedback , posterior matching , iterated function systems . </S>"
  ]
}