{
  "article_text": [
    "while there has been significant progress on supervised large - scale classification in recent years @xcite , the lack of sufficient annotated training data uniformly across all classes @xcite has been a bottleneck in achieving acceptable performance . at a basic level , in these cases we encounter situations where we may have sufficient annotated training data for some of the classes and little or even no annotated data to train supervised classifiers for the other interesting classes . in this context , a fundamental question that arises is as to how to leverage training data for observed classes for recognition of rare or unobserved classes .",
    "one possible scenario is when we have data from a different domain that can be collected easily , as is assumed to be the case in zero - shot recognition ( zsr ) . in zsr",
    "we are given _ source _ and _ target _ domains as training data belonging to a sub - collection of classes , forming _ seen _ or observed classes .",
    "no training data is available for other _ unseen _ classes .",
    "the class information in source domain is described in a variety of ways such as attribute vectors @xcite , language words / phrases @xcite , or even learned classifiers @xcite .",
    "target domain is described by a joint distribution of data ( images or videos ) and labels @xcite .     and",
    "@xmath0 denote source and target domain data embeddings , respectively , colors denote different classes , filled / empty shapes denote original / adapted feature embeddings . using the direct matching in ( a )",
    "the image will be mis - classified as `` face '' ( based on distance measure ) , while using the adaptive matching in ( b ) it will be classified correctly as `` dog '' . ]",
    "the challenge in zsr lies in learning models based on seen - class data that can generalize to unseen classes . in this context , many perspectives for zero - shot learning ( zsl ) have been proposed , including unseen classifier prediction from source domain data based on learning attribute classifiers  @xcite , learning similarity functions between source and target domains to score similarity for unseen classes  @xcite , and manifold embedding methods based on identifying inter - class relationships in source and target data that can be aligned during test time  @xcite .",
    "nevertheless , the challenge posed by the relative sparseness of source domain descriptions in recognition has not been fully considered . in particular , the target - domain data exhibits significant intra - class variation ( appearance and poses ) . on the other hand",
    "the source domain information is relatively sparse and typically amounting to a single attribute vector .",
    "this is generally insufficient to account for all of the intra - class variation .",
    "[ fig : idea ] illustrates this point . in the joint embedded feature space ,",
    "the target - domain data distributions of certain classes ( `` dog '' class in the figure ) are relatively flat and consist of data instances with large variation .",
    "this issue leads us to view the presented source domain vector as a `` mean - value '' over all candidate ( or alternative ) source vectors .",
    "during test time for a given target instance we optimize the matching over all possible source and target candidates in a neighborhood of the presented source and target instances . during training",
    "we propose learning a data - dependent feature transform chosen from a parameterized family of displacement functions that maximizes the similarity between an arbitrary source and target instances .",
    "we learn our similarity functions from training data using _ latent structural svms_. as demonstration we design a specific algorithm under the proposed framework involving bilinear similarity functions and regularized least squares as penalties for displacement .    to illustrate how this would work , consider again fig .",
    "[ fig : idea ] . in test time",
    "our proposed approach manifests as new features ( empty `` @xmath1 '' in the figure ) that adapt to the potential contents in target data instance .",
    "this leads to significantly richer representations than the provided source - domain vectors .",
    "our proposed approach also induces displacements in the target domain data instances . these displaced features ( empty `` @xmath0 '' in the figure ) in turn",
    "adapt to source domain features .",
    "this process is akin to de - noising of presented target - domain data / features . as we see , by using the new features ,",
    "the dog face image is correctly classified based on similarity measure between the new data - dependent adapted features , as illustrated in fig .",
    "[ fig : idea](b ) .",
    "* contributions : * in this paper we introduce a novel _ adaptive similarity function _ for comparing an arbitrary pair of source and target domain data instances .",
    "this function , in test time and adaptively in a data - dependent way , determines the similarity between presented source and target instances .",
    "we propose considering optimizing over a parameterized bilinear family of functions for our cross - domain similarity measure .",
    "alternating optimization is utilized to efficiently estimate ( globally ) best adapted features within a constrained family of displacements . in this context",
    "we show that the compatibility function defined in @xcite is indeed a special case of our similarity function .",
    "to learn the parameters in our adaptive similarity function , we further propose formulating the zsl problem using _ latent structural svms_. the latent part comes from the adapted ( latent ) features , which are considered as the latent variables in the formulation .",
    "the structural part arises from the structures of label embeddings as did in  @xcite .",
    "we test our approach on four benchmark image datasets for zsl / zsr , namely , ap&y , awa , cub and sun - attribute .",
    "under both standard and transductive settings , our approach outperforms the state - of - the - art significantly .      in general zsl / zsr approaches",
    "can be divided into two categories : standard setting and transductive setting . recently",
    "zero - shot approaches have been successfully applied to several visual tasks such as event detection @xcite , action recognition @xcite , and image tagging @xcite .",
    "below we primarily describe learning approaches in this context .",
    "* standard setting : * in test time , the source - domain descriptors for unseen classes are all given at once .",
    "our task is to sequentially recognize target - domain instances as they are revealed _ one at a time_. in this context , several works in the literature are based on training attribute classifiers which directly map target - domain data into source - domain attribute space @xcite .",
    "the resulting attribute classifiers do not fully account for data noise in source ( ambiguity or mislabeling in attributes ) and target ( large variation because of the changes of appearance , poses , ) domains .",
    "linear and nonlinear embedding approaches @xcite have attracted attention recently .",
    "the basic idea of these methods is to embed the source and target domain features into a kronecker product embedding space .",
    "for instance , akata @xcite proposed label embedding to map class labels into a high dimensional vector space ( source - domain attribute space ) , and measure cross - domain similarities using a bilinear function whose parameters are learned using structured svms .",
    "zhang and saligrama @xcite proposed a joint learning framework to learn the latent embeddings for both domains and utilized them for similarity measure .",
    "changpinyo @xcite proposed a learning method to generate synthesized classifiers for unseen classes .",
    "bucher @xcite proposed a metric learning based formulation to improve semantic embedding consistency , achieving the best performance on the four benchmark datasets under the standard setting in the current literature , to our best knowledge .",
    "the underlying assumption behind such approaches is that there exist ( hidden ) corresponding matches between source - domain feature vectors and target - domain data distributions , one - to - one match @xcite or one - to - many match @xcite . in this context",
    "there are other related proposed methods such as semantic transfer propagation @xcite , random forest based approaches @xcite , semantic manifold distance @xcite approaches , and similarity calibration method @xcite .",
    "nevertheless , the issue of source - domain sparsity and the resulting imbalance with target - domain data is not fully accounted for in these methods .",
    "our proposed method explicitly focuses on handling the scarcity issue of source - domain data by learning data - dependent latent features .",
    "this in turn accounts for the large data variation in target domain implicitly so that the cross - domain matches can be improved .",
    "* transductive setting : * recently researchers have begun to incorporate test - time unseen - class data in target domain into zsl / zsr as unlabeled data analogous to the transductive setting .",
    "this has led to approaches that attempt to account for domain shift @xcite . in this setting , during test time , we are given a list of all unlabelled target instances in addition to unseen - class source - domain descriptions .",
    "potentially these methods can be used in conjunction with any similarity learning procedure trained on seen - class data , as demonstrated in @xcite , to score similarity between unseen classes and target domain data instances .",
    "while much of the focus of this paper is on the standard setting , in our experimental section we also test our learning algorithm in the transductive mode to benchmark our performance in the transductive setting .",
    "in the training stage , we are given a set of observed classes @xmath2 .",
    "for source domain , attribute vectors ( or label embeddings ) in the form of @xmath3 , are provided .",
    "typically there exists only one vector per class .",
    "corresponding target domain data instances @xmath4 and feature embeddings @xmath5 associated with the observed source labels are also provided for training .",
    "we aggregate training data as @xmath6 , where @xmath7 denotes the @xmath7-th training data instance in target domain .",
    "our goal is to learn a prediction model , by leveraging observed training data , @xmath8 , such that it generalizes well to unobserved data instances and classes during test time .    in the testing stage ,",
    "a set of source vectors corresponding to unobserved classes @xmath9 are revealed .",
    "for a given unobserved data instance @xmath10 from target domain , the task is to identify the source vector among those unobserved classes that corresponds to @xmath10 .",
    "abstractly , our decision rule is based on maximizing a posterior probability ( map ) conditioned on all the available data : @xmath11 where @xmath12 denotes the posterior probability tuned to the embeddding functions @xmath13 . in what follows",
    "we drop the parameter dependence on @xmath13 for notational simplicity , since we assume that these embedding functions are provided a priori .",
    "the posterior probability is unknown and must be learned from training data .",
    "we describe our proposed approach in the following section .",
    "we face two fundamental challenges in zsr .",
    "first , target instances and labels for unobserved classes are not known during training .",
    "therefore , proposed methods must base its recognition on scoring the similarity between an arbitrary source descriptor and a target instance .",
    "second , source vectors in zsr are sparse and typically we only observe a single source vector per class . on the other hand there is significant variability in the target domain . consequently , the source vectors serve only as `` average '' attribute descriptors across the target domain instances .",
    "the source descriptor that best matches a target instance is a vector that is typically close to but not necessarily equal to the given source domain vector .",
    "we propose optimizing over all such vectors in both learning and test time to determine the optimal matching source descriptors .    in this context",
    "we propose a family of posterior distributions : to account for relative sparseness of source domain descriptors and large variability of target domain instances we introduce new data - dependent feature vectors @xmath14 corresponding to source and target domains , respectively . to ensure that these feature vectors are `` typically '' close to the given source and target data pair we introduce a displacement penalty term @xmath15 parametrized by @xmath16 .",
    "to score similarity between source and target domain data we propose a scoring function , @xmath17 , that scores similarity between the new data - dependent feature vectors parameterized by a matrix @xmath18 .",
    "this leads to the following posterior probability : @xmath19    in order to compute the posterior @xmath20 we can marginalize @xmath21 over variables @xmath22 , where @xmath23 denote their corresponding feasible domains ( simplex ) .",
    "however , in general this calculation will be very difficult given arbitrary parameter spaces , and typically bayesian parametrization is often involved ( @xcite ) to simplify the calculation .",
    "alternatively the posterior can be upper - bounded by the maximum value over the variables , as did in @xcite , which can be very computationally efficient and demonstrated with good performance for zsr as well .",
    "therefore , here we adopt the strategy in @xcite and take the maximum for posterior approximation purpose .",
    "this leads naturally to our _ adaptive similarity function _ as below for scoring each target data instance with a class label : @xmath24 intuitively our similarity function allows the features to move from their original locations in the feature space ( adaptation ) to achieve a higher similarity score within a neighborhood ( feature displacements incur penalties ) .",
    "our function in eq .",
    "[ eqn : f ] thus attempts to achieve a balance between these two objectives .",
    "in fact similar strategy has been widely used in deformable part models ( dpm ) @xcite , where 2d locations for parts are considered as adapted features .",
    "the parameters @xmath14 in eq .",
    "[ eqn : f ] play the role of latent variables for given values of @xmath25 .",
    "consequently , we can pose the problem as a latent structural svm problem by viewing the label variable @xmath26 as taking values from a structured output space : @xmath27 where @xmath28 denote two regularization functions ( @xmath29-norm regularizers ) for parameters @xmath25 , respectively , @xmath30 denotes a penalty term measuring the difference between the ground - truth label @xmath31 and an arbitrary label @xmath26 , @xmath32 denote the feasible domains for @xmath25 , respectively , and @xmath33 is a slack variable .",
    "the cutting - plane algorithm @xcite can be used for general training purpose .    in test time",
    ", we replace the probability term in eq .",
    "[ eqn : y * ] with our adaptive similarity function in eq .",
    "[ eqn : f ] to rewrite the decision rule for zsr as follows : @xmath34      for the purpose of demonstration we describe one instance of an adaptive similarity function that can be utilized in our general learning framework .",
    "specifically we design the similarity term @xmath17 in eq .",
    "[ eqn : f ] as a bilinear function .",
    "these type of functions have been widely used in recent zsl literature , @xcite , and has been shown to achieve state - of - the - art performance . for the penalty term , we simply adopt the regularized least square loss for the displacement .",
    "putting these together , we propose the following adaptive similarity function : @xmath35 where @xmath36 is a weighting matrix between @xmath37 and @xmath38 , @xmath39 $ ] is a 4d vector controlling the trade - off between similarity and penalty , and @xmath40 denotes the @xmath29 norm of a vector . in general we can utilize _ alternating optimization _ ( ao ) to solve eq .",
    "[ eqn : ff ] as follows : @xmath41 where @xmath42 and @xmath43 .",
    "ideally , we would like to have a decision rule that during test time using eq .",
    "[ eqn : ff ] converges to a ( unique ) global solution for an arbitrary pair of source and target data instances .",
    "this is because we can then be certain that the similarity scores are unique and reliable .",
    "therefore , below we provide some general and useful properties about of the similarity function in eq .",
    "[ eqn : ff ] .",
    "[ prop : go ] let us define a new matrix @xmath44 ,      \\end{aligned}\\ ] ] where @xmath45 and @xmath46 denote two identity matrices with sizes of @xmath47 and @xmath48 entries , respectively",
    ". then if @xmath49 is positive definite ( pd ) and @xmath23 are nonempty closed convex sets , there exists a unique global solution for eq .",
    "[ eqn : ff ] .",
    "[ eqn : ff ] can be rewritten with @xmath49 in eq .",
    "[ eqn : h ] as follows : @xmath50 where @xmath51 $ ] , @xmath52 $ ] , and @xmath53 .",
    "since @xmath23 are nonempty closed convex sets , the feasible domain for @xmath54 is nonempty closed convex as well .",
    "based on @xcite , we can prove this property .",
    "[ prop : global ] under the conditions in property [ prop : go ] , the alternating optimization in eq .",
    "[ eqn : zt ] and eq .",
    "[ eqn : zs ] can guarantee global convergence .    due to matrix @xmath49 being pd",
    ", we can have @xmath55 and @xmath56 .",
    "further since @xmath23 are nonempty closed convex sets , both eq .",
    "[ eqn : zt ] and eq .",
    "[ eqn : zs ] define convex optimization problems ( see @xcite ) , respectively .",
    "now based on property [ prop : go ] we can prove this property .    [ prop : local ] if @xmath55 , @xmath56 , and @xmath23 are nonempty closed convex sets , then the alternating optimization in eq .",
    "[ eqn : zt ] and eq .",
    "[ eqn : zs ] can guarantee to converge to local optima .",
    "please refer to the proof for property [ prop : global ] .",
    "[ prop:2 ] suppose that all the vectors and matrix in eq .",
    "[ eqn : ff ] are upper - bounded .",
    "then we have @xmath57    from property [ prop:2 ] we can easily see that our adaptive similarity function in eq .",
    "[ eqn : ff ] can be taken as the generalization of the bilinear compatibility function defined in @xcite , and so does our learning framework in eq .",
    "[ eqn : general_learning ] accordingly",
    ". +      with various feasible domains @xmath23 , we can design different adaptive similarity functions accordingly . particularly here",
    "we define @xmath58 that is , we define @xmath23 to be sufficiently large sets which contain _",
    "any possible _ source or target adapted feature embedding , respectively , very large real numbers . ] . our reasoning for",
    "this choice is its simplicity and our need for high computational efficiency .    then by setting the first derivative of @xmath59 over @xmath54 to 0 ,",
    "@xmath60 , we can easily get the close - form solution for @xmath54 , equivalently for @xmath14 , as follows : @xmath61 where @xmath62 denotes the pseudo - inverse operation .",
    "* discussion : * eq . [ eqn : z ] suggests a linear transform function of combining source and target information to generate the adapted features . since @xmath49 is pd ( and thus so is @xmath63 ) , the target domain data structures are fully preserved in @xmath64 while matching with a single source domain vector .",
    "correspondingly the target data structures will have a larger impact in generating @xmath65 as well .",
    "[ fig : data ] illustrates the distributions of different features using the test data in ap&y dataset , which conform with our analysis .",
    "next we substitute eq .",
    "[ eqn : ff ] , [ eqn : zs ] and [ eqn : zt ] into eq .  [ eqn : general_learning ] to learn the parameters in @xmath59 .",
    "note that in order to achieve global optimality in property [ prop : go ] , the learned parameters must guarantee that matrix @xmath49 in eq .",
    "[ eqn : h ] is pd .",
    "this leads us to the following learning problem : @xmath66 where @xmath67 in eq .",
    "[ eqn : h ] with @xmath25 as parameters , `` @xmath68 '' denotes the pd constraint which makes it very difficult to solve the problem , and @xmath69 are two predefined regularization parameters .    as a relaxation we tried to solve eq .",
    "[ eqn : hard - to - solve ] without considering the pd constraint . however , we observed empirically that the learned parameters do not always satisfy the pd constraint using ao procedure .",
    "this leads to poor recognition performance . on the other hand ,",
    "if we assume that the maximum @xmath70 norm of the row vectors @xmath71 or column vectors @xmath72 in matrix @xmath18 , denoted by @xmath73 is non - zero and upper - bounded ( which is always the case ) , we can obtain global optimality at least by manually setting parameter @xmath16 so that @xmath74 and @xmath75 .",
    "this creates a diagonally dominant matrix for @xmath49 and guarantees that pd is satisfied .",
    "based on this consideration , we chose not to learn parameter @xmath76 but instead set it manually to guarantee the pd constraint during training .",
    "we thus only learn parameter @xmath18 .",
    "our learning formulation can now be rewritten as follows : @xmath77 where @xmath78 denotes the predefined parameter vector and @xmath79 is a predefined constant . since in our experiments the current zsr problem is essentially equivalent to a multi - class prediction problem , we simply set @xmath80 if @xmath81 , otherwise 0 . in test time , by substituting eq .",
    "[ eqn : ff ] , [ eqn : h ] and [ eqn : z ] into eq .",
    "[ eqn : y * * ] we can rewrite the our decision function for zsr as follows : @xmath82    * discussion : * learning based on eq . [",
    "eqn : learning1 ] has convergence issues due to the nature of latent structural svms . on the other hand , since our decisions are based on @xmath63 explicitly as in eq .",
    "[ eqn : yy ] , it might be possible to learn @xmath63 approximately and efficiently by substituting similarity function @xmath83 in eq .",
    "[ eqn : yy ] into structural svms @xcite .",
    "it turns out that this learning strategy is equivalent to @xcite with source - domain feature augmentation , and thus leads to global convergence ( under the multi - class prediction setting for zsr ) .",
    "empirically we tested this learning strategy and found marginal differences from @xcite in terms of recognition performance .",
    "therefore we do not report these results in our experimental section .",
    "[ cols=\"<,<,<,<\",options=\"header \" , ]     for transductive setting , we list our comparison results in table [ tab : transductive_recognition ] . overall , our method outperforms the state - of - the - art @xcite significantly by 2.26% on average .",
    "it is worth mentioning that on ap&y by substituting our similarity scores in @xcite we can achieve 80.89% in terms of accuracy and outperform the state - of - the - art significantly by 11.15% .",
    "analogous to the results of the traditional setting , we observe that the standard deviations of our results are slightly higher than those of the competitors .    to better compare our results , we further measure the class - level performance on the datasets in terms of precision and recall ( equivalent to accuracy per class ) .",
    "the detailed comparisons are illustrated in fig .",
    "[ fig : recog - pre ] without the cub dataset due to the space limit .",
    "we summarize the averaged performance across different classes on each dataset in table  [ tab : pre - rec ] .",
    "overall at the class level our method behaves similar to @xcite with the same inputs .",
    "however , as we see there exists no single dominant method over all the datasets and uniformly over all classes on each dataset .",
    "better similarity measure does not necessarily lead to better performance under either standard or transductive setting .",
    "it could be interesting as future work to see whether we can improve the zsr performance further by integrating different similarity metrics .",
    "in this paper we solve the relative sparseness issue of source - domain attribute vectors in zsr problems .",
    "we formulate zsl as a latent structural svms . to account for the rich data variability in target domain",
    ", we propose a novel data - dependent adaptive similarity function that adapts to test - time source and target data instances .",
    "our similarity function searches for latent features from both domains by maximizing the latent similarities as well as minimizing the penalties incurred by feature displacements . to parameterize our adaptive similarity function , we propose a family of bilinear based similarity functions with regularized least squares to penalize displacements .",
    "we design a specific function with closed - form solutions and propose its corresponding learning algorithm for zsr . to demonstrate the effectiveness of our proposed method , we test it on four benchmark datasets for zsr with comprehensive comparison , and show significant improvement over the state - of - the - art under both standard and transductive settings ."
  ],
  "abstract_text": [
    "<S> zero - shot recognition ( zsr ) aims to recognize target - domain data instances of unseen classes based on the models learned from associated pairs of seen - class source and target domain data . </S>",
    "<S> one of the key challenges in zsr is the relative _ scarcity _ of source - domain features ( one feature vector per class ) , which do not fully account for wide variability in target - domain instances . in this paper </S>",
    "<S> we propose a novel framework of learning data - dependent feature transforms for scoring similarity between an arbitrary pair of source and target data instances to account for the wide variability in target domain . </S>",
    "<S> our proposed approach is based on optimizing over a parameterized family of local feature displacements that maximize the source - target adaptive similarity functions . </S>",
    "<S> accordingly we propose formulating zero - shot learning ( zsl ) using _ latent structural svms _ to learn our similarity functions from training data . as demonstration </S>",
    "<S> we design a specific algorithm under the proposed framework involving bilinear similarity functions and regularized least squares as penalties for feature displacement . </S>",
    "<S> we test our approach on several benchmark datasets for zsr and show significant improvement over the state - of - the - art . </S>",
    "<S> for instance , on ap&y dataset we can achieve 80.89% in terms of recognition accuracy , outperforming the state - of - the - art by 11.15% . </S>"
  ]
}