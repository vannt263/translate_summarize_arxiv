{
  "article_text": [
    "suppose a data string @xmath0 is generated by a stationary memoryless source @xmath1 with unknown marginal distribution @xmath2 on a discrete alphabet @xmath3 . in many theoretical and practical problems arising in a wide variety of scientific contexts ,",
    "it is desirable  and often important  to obtain accurate estimates of the entropy @xmath4 of the source , based on the observed data @xmath5 ; see , for example , @xcite@xcite@xcite@xcite@xcite@xcite@xcite and the references therein .",
    "perhaps the simplest method is via the so - called * plug - in estimator * , where the entropy of @xmath2 is estimated by @xmath6 , namely , the entropy of the empirical distribution @xmath7 of @xmath5 . the plug - in estimator satisfies the basic statistical requirement of consistency , that is , @xmath8 in probability as @xmath9 .",
    "in fact , it is strongly consistent ; the convergence holds with probability one@xcite .",
    "a natural generalization is the problem of estimating the rate - distortion function @xmath10 of a ( not necessarily discrete - valued ) source .",
    "motivation for this comes in part from lossy data compression , where we may need an estimate of how well a given data set could potentially be compressed , cf .",
    "@xcite , and also from cases where we want to quantify the `` information content '' of a particular signal , but the data under examination take values in a continuous ( or more general ) alphabet , cf .",
    ".    the rate - distortion function estimation question appears to have received little attention in the literature .",
    "here we present some basic results for this problem .",
    "first , we consider the simple * plug - in estimator * @xmath11 , and determine conditions under which it is strongly consistent , that is , it converges to @xmath10 with probability 1 , as @xmath12 we call this the * nonparametric estimation problem * , for reasons that will become clear below .    at first glance",
    ", consistency may seem to be a mere continuity issue : since the empirical distribution @xmath13 converges , with probability 1 , to the true distribution @xmath2 as @xmath9 , a natural approach to proving that @xmath11 also converges to @xmath10 would be to try and establish some sort of continuity property for @xmath10 as a function of @xmath2 .",
    "but , as we shall see , @xmath11 turns out to be consistent under rather mild assumptions , which are in fact _ too mild _ to ensure continuity in any of the usual topologies ; see section  [ ex : dc ] for explicit counterexamples .",
    "this also explains our choice of the empirical distribution @xmath7 as an estimate for @xmath2 : if @xmath10 was continuous in @xmath2 , then any consistent estimator @xmath14 of @xmath2 could be used to make @xmath15 a consistent estimator for @xmath10 .",
    "some of the subtleties in establishing regularity properties of the rate - distortion function @xmath10 as a function of @xmath2 are illustrated in @xcite@xcite .",
    "another advantage of a plug - in estimator is that @xmath7 has finite support , regardless of the source alphabet .",
    "this makes it possible ( when the reproduction alphabet is also finite ) to actually compute @xmath16 by approximation techniques such as the blahut - arimoto algorithm @xcite@xcite@xcite .",
    "when the reproduction alphabet is continuous , the blahut - arimoto algorithm can still be used after discretizing the reproduction alphabet ; the discretization can , in part , be justified by the observation that it can be viewed as an instance of the _ parametric estimation problem _ described below .",
    "other possibilities for continuous reproduction alphabets are explored in @xcite@xcite .",
    "the consistency problem can be framed in the following more general setting . as has been observed by several authors recently ,",
    "the rate - distortion function of a memoryless source admits the decomposition , r(p , d)=_q r(p , q , d ) , [ eq : decomp ] where the infimum is over all probability distributions @xmath17 on the reproduction alphabet , and @xmath18 is the rate achieved by memoryless random codebooks with distribution @xmath17 used to compress the source data to within distortion @xmath19 ; see , e.g. , @xcite@xcite .",
    "therefore , @xmath10 is the best rate that can be achieved by this family of codebooks .",
    "but in the case where we only have a restricted family of compression algorithms available , indexed , say , by a family of probability distributions @xmath20 on the reproduction alphabet , then the best achievable rate is : r^(p , d):= _ r(p , q_,d ) .",
    "[ eq : decomp2 ] we also consider the * parametric estimation problem * , namely , that of establishing the strong consistency of the corresponding * plug - in estimator * @xmath21 as an estimator for @xmath22 .",
    "it is important to note that , when @xmath23 indexes the set of all probability distributions on the reproduction alphabet , then the parametric and nonparametric problems are identical , and this allows us to treat both problems in a common framework .    our two main results , theorems  [ t : ub ] and  [ t : lb ] in the following section , give regularity conditions for both the parametric and nonparametric estimation problems under which the plug - in estimator is strongly consistent .",
    "it is shown that consistency holds in great generality for all distortion values @xmath19 such that @xmath22 is continuous from the left .",
    "an example illustrating that consistency may actually fail at those points is given in section  [ ex : fail ] .",
    "in particular , for the nonparametric estimation problem we obtain the following three simple corollaries , which cover many practical cases .",
    "1ex    [ c : finite ] if the reproduction alphabet is finite , then for any source distribution @xmath2 , @xmath11 is strongly consistent for @xmath10 at all distortion levels @xmath24 except perhaps at the single value where @xmath10 transitions from being finite to being infinite .",
    "1ex    [ c : mse ] if the source and reproduction alphabets are both equal to @xmath25 and the distortion measure is squared - error , then for any source distribution @xmath2 and any distortion level @xmath24 , @xmath11 is strongly consistent for @xmath10 .",
    "1ex    [ c : sc ] assume that the reproduction alphabet is a compact , separable metric space , and that the distortion measure @xmath26 is continuous for each @xmath27 .",
    "then ( under mild additional measurability assumptions ) , for any source distribution @xmath2 , @xmath11 is strongly consistent for @xmath10 at all distortion levels @xmath24 except perhaps at the single value where @xmath10 transitions from being finite to being infinite .",
    "1ex    corollaries  [ c : finite ] and  [ c : sc ] are special cases of corollary  [ c : sc2 ] in section  [ s : results ] .",
    "corollary  [ c : mse ] is established in section  [ s : examples ] , which contains many other explicit examples illustrating the consistency results and cases where consistency may fail .",
    "section  [ s : proofs ] contains the proofs of all the main results in this paper .",
    "we also consider extensions of these results in two directions . in section",
    "[ s : optimal ] we examine the problem of estimating the optimal reproduction distribution  namely , the distribution that actually achieves the infimum in equations ( [ eq : decomp ] ) and ( [ eq : decomp2 ] )  from empirical data .",
    "consistency results are given , under conditions identical to those required for the consistency of the plug - in estimator .",
    "finally , in section  [ s : general ] we show that consistency holds for a more general class of estimators , which arise as modifications of the plug - in .",
    "these include , in particular , penalized versions of the plug - in , analogous to the standard penalized maximum likelihood estimators used in statistics .",
    "the analysis of the plug - in estimator presents some unexpected technical difficulties .",
    "one way to explain the source of these difficulties is by noting that there is a very close analogy , at least on the level of the mathematics , with the problem of maximum likelihood estimation [ see also section  [ s : general ] for another instance of this connection ] . beyond the superficial observation that they are both extremization problems over a space of probability distributions , a more accurate , albeit heuristic",
    ", illustration can be given as follows : suppose we have a memoryless source with distribution @xmath2 on some discrete alphabet , take the reproduction alphabet to be the same as the source alphabet , and look at the extreme case where no distortion is allowed .",
    "then the plug - in estimator of the rate - distortion function ( which now is simply the entropy ) can be expressed as a trivial minimization over all possible coding distributions , i.e. , @xmath28 = -\\frac{1}{n}\\,\\max_q \\big[\\log q^n(x_1^n)\\big],\\ ] ] where @xmath29 denotes the relative entropy , and @xmath30 is the @xmath31-fold product distribution of @xmath31 independent random variables each distributed according to @xmath17 .",
    "therefore , the computation of the plug - in estimate @xmath6 is exactly equivalent to the computation of the maximum likelihood estimate ( mle ) of @xmath2 over a class of distributions @xmath17 . alternatively , in csiszr s terminology , the minimization of the relative entropy above corresponds to the so - called `` reversed @xmath32-projection '' of @xmath7 onto the set of feasible distributions @xmath17 , which in this case consists of _ all _ distributions on the reproduction alphabet ; see , e.g. , @xcite@xcite .",
    "formally , this projection is exactly the same as the computation of the mle of @xmath2 based on @xmath5 .    in the general case of nonzero distortion @xmath33 , the plug - in estimator",
    "can similarly be expressed as , @xmath34 , cf .",
    "( [ eq : decomp ] ) above .",
    "this ( now highly nontrivial ) minimization is mathematically very closely related to the problem of computing an @xmath32-projection as before .",
    "the tools we employ to analyze this minimization are based on the technique of epigraphical convergence @xcite@xcite ( this is particularly clear in the proof of our main result , the lower bound in theorem  [ t : lb ] ) , and it is no coincidence that these same tools have also provided one of the most successful approaches to proving the consistency of mles . by the same token , this connection also explains why the consistency of the plug - in estimator involves subtleties similar to those cases where mles fail to be consistent @xcite .    in the way of motivation",
    ", we also mention that the asymptotic behavior of the plug - in estimator  and the technical intricacies involved in its analysis  also turn out to be important in extending some of rissanen s celebrated ideas related to the minimum description length ( mdl ) principle to the context of lossy data compression ; this direction will be explored in subsequent work .    throughout the paper we work with stationary and ergodic sources instead of memoryless sources , though we are still only interested in estimating the first - order rate - distortion function .",
    "one reason for this is that the full rate - distortion function can be estimated by looking at the process in sliding blocks of length @xmath35 and then estimating the `` marginal '' rate - distortion function of these blocks for large @xmath35 ; see section  [ ex : block ] .",
    "another reason for allowing dependence in the data comes from simulation : for example , suppose we were interested in estimating the rate - distortion function of a distribution @xmath2 that we can not compute explicitly ( as is the case for perhaps the majority of models used in image processing ) , but for which we have a markov chain monte carlo ( mcmc ) sampling algorithm .",
    "the data generated by such an algorithm is not memoryless , yet we care only about the rate - distortion function of the marginal distribution . in section  [ s : lln ]",
    "we comment further on this issue , and also give consistency results for data produced by sources that may not be stationary .",
    "we begin with some notation and definitions that will remain in effect throughout the paper .",
    "suppose the random source @xmath1 taking values in the source alphabet @xmath3 is to be compressed in the reproduction alphabet @xmath36 , with respect to the single - letter distortion measures @xmath37 arising from an arbitrary distortion function @xmath38 .",
    "we assume that @xmath3 and @xmath36 are equipped with the @xmath39-algebras @xmath40 and @xmath41 , respectively , that @xmath42 and @xmath43 are borel spaces , and that @xmath44 is @xmath45-measurable . as well as all polish spaces , and they allow us to avoid certain measure - theoretic pathologies while working with random sequences and conditional distributions @xcite .",
    "henceforth , all @xmath39-algebras and the various product @xmath39-algebras derived from them are understood from the context .",
    "we do not complete any of the @xmath39-algebras , but we say that an event @xmath46 holds _ with probability 1 _ ( w.p.1 )  if @xmath46 contains a measurable subset @xmath47 that has probability 1 .",
    "] suppose the source is stationary , and let @xmath2 denote its marginal distribution on @xmath3 .",
    "then the ( first - order ) rate - distortion function @xmath48 with respect to the distortion measure @xmath44 is defined as , @xmath49 where the infimum is over all @xmath50-valued random variables @xmath51 with joint distribution @xmath52 belonging to the set @xmath53",
    "\\leq d\\right\\ } , \\ ] ] and where @xmath54 denotes the marginal distribution of @xmath52 on @xmath3 , and similarly for @xmath55 ; the infimum is taken to be @xmath56 when @xmath57 is empty . as usual , the mutual information @xmath58 between two random variables @xmath59 with joint distribution @xmath52 , is defined as the relative entropy between @xmath52 and the product of its two marginals , @xmath60 .",
    "here and throughout the paper , all familiar information - theoretic quantities are expressed in nats , and @xmath61 denotes the natural logarithm . in particular , for any two probability measures @xmath62 on the same space , the relative entropy @xmath63 is defined as @xmath64 $ ] whenever the density @xmath65 exists , and it is taken to be @xmath56 otherwise .",
    "we write @xmath66 for the set of distortion values @xmath24 for which @xmath48 is continuous from the left , i.e. , @xmath67 by convention , this set always includes @xmath68 and any value of @xmath19 for which @xmath69 .",
    "but since @xmath48 is nonincreasing and convex in @xmath19 @xcite@xcite , @xmath66 actually includes _ all _ @xmath24 with the only possible exception of the single value of @xmath19 where @xmath48 transitions from being finite to being infinite .",
    "conditions guaranteeing that @xmath66 is indeed all of @xmath70 can be found in @xcite .      given a finite - length data string @xmath0 produced by a stationary source @xmath71 as above with marginal distribution @xmath2 , the * plug - in estimator * of the first - order rate - distortion function @xmath48 is @xmath72 , where @xmath7 is the _ empirical distribution",
    "_ induced by the sample @xmath5 on @xmath73 , namely , @xmath74 and where @xmath75 is the indicator function .",
    "our first goal is to obtain conditions under which this estimator is strongly consistent .",
    "we call this the * nonparametric estimation problem*.    we also consider the more general class of estimation problems mentioned in the introduction",
    ". suppose for a moment that our goal is to compress data produced by a _",
    "source @xmath71 with distribution @xmath2 on @xmath3 , and suppose also that we are restricted to using memoryless random codebooks with distributions @xmath17 belonging to some parametric family @xmath76 where @xmath23 indexes a subset of all probability distributions on @xmath36 . using a random codebook with distribution @xmath17 to compress the data to within distortion @xmath19 , yields ( asymptotically ) a rate of @xmath77 nats / symbol , where the rate - function @xmath77 is given by , @xmath78 see @xcite@xcite for details . from this",
    "it is immediate that the rate - distortion function of the source admits the decomposition given in ( [ eq : decomp ] ) .",
    "having restricted attention to the class of codebook distributions @xmath79 then the best possible compression rate is : @xmath80 when @xmath81 indexes certain nice families , say gaussian , the infimum @xmath82 can be analytically derived or easily computed , often for any distribution @xmath2 , including an empirical distribution .",
    "thus motivated , we now formally define the * parametric estimation problem*. suppose @xmath71 is a stationary source as above , and let @xmath76 be a family of probability distributions on the reproduction alphabet @xmath36 parameterized by an arbitrary parameter space @xmath23 .",
    "the * plug - in estimator * for @xmath82 is @xmath83 , and we seek conditions for its strong consistency .",
    "note that @xmath84 when @xmath76 includes all probability distributions on @xmath85 or if it simply includes the optimal reproduction distribution achieving the infimum in ( [ eq : decomp ] ) . otherwise , @xmath82 may be strictly larger than @xmath48 .",
    "therefore , the nonparametric problem is a special case of the parametric one , and we can consider the two situations in a common framework .    in the parametric scenario we write , @xmath86 unlike @xmath66 , @xmath87 can exclude more than a single point .",
    "we investigate conditions under which the plug - in estimator @xmath88 is strongly consistent , i.e. , if @xmath89 diverges to @xmath90 ( and similarly for @xmath91 ) . ]",
    "@xmath92 of course in the special case where @xmath23 indexes all probability distributions on @xmath36 , this reduces to the nonparametric problem , and ( [ e : sc ] ) becomes @xmath93 .",
    "we separately treat the upper and lower bounds that combine to give .",
    "the upper bound does not require _ any _ further regularity assumptions , although there can be certain pathological values of @xmath19 for which it is not valid . in the nonparametric situation ,",
    "the only potential problem point is the single value of @xmath19 where @xmath48 transitions from finite to infinite .",
    "1ex    [ t : ub ] if the source @xmath71 is stationary and ergodic with @xmath94 , then @xmath95 for all @xmath96 .",
    "1ex    as illustrated by a simple counterexample in section  [ ex : fail ] , the requirement that @xmath96 can not be relaxed completely .",
    "the proof of the theorem , given in section  [ s : proofs ] , is a combination of the decomposition in and the fact that @xmath97 quite generally . actually , from the proof we also obtain an upper bound on the @xmath98 , @xmath99 which provides some information even for those values of @xmath19 where the upper bound in theorem [ t : ub ] may fail .    for the corresponding lower bound in ,",
    "some mild additional assumptions are needed .",
    "we will always assume that @xmath23 is a metric space , and also that the following two conditions are satisfied :    * the map @xmath100 $ ] is continuous for each @xmath27 and @xmath101 , where @xmath102 denotes expectation w.r.t .",
    "* for each @xmath24 , there exists a ( possibly random ) sequence @xmath104 with @xmath105 and such that @xmath104 is relatively compact with probability 1 .",
    "1ex    [ t : lb ] if @xmath23 is separable , a1 and a2 hold , and @xmath71 is stationary and ergodic with @xmath94 , then @xmath106 for all @xmath24 .",
    "1ex    although a1 and a2 may seem quite involved , they are fairly easy to verify in specific examples : for a1 , we have the following sufficient conditions ; as we prove in section  [ s : proofs ] , either one implies a1 .",
    "* whenever @xmath107 , we also have that @xmath108 setwise .",
    "setwise if @xmath109 for all bounded , measurable functions @xmath110 , or equivalently , if @xmath111 for all measurable sets @xmath46 . ] * @xmath43 is a metric space with its borel @xmath39-algebra , @xmath26 is continuous for each @xmath27 and @xmath107 implies that @xmath108 weakly . weakly if @xmath109 for all bounded , continuous functions @xmath110 , or equivalently , if @xmath111 for all measurable sets @xmath46 with @xmath112 . ]    for a2 , we first note that a sequence @xmath104 satisfying exists and that the inequality in must always be an equality .",
    "the important requirement in a2 is that @xmath104 be relatively compact . in particular , a2 is trivially true if @xmath23 is compact . more generally , the following two conditions make it easier to verify a2 in particular examples . in section  [",
    "s : proofs ] we prove that either one implies a2 as long as the source is stationary and ergodic with marginal distribution @xmath2 . for any subset @xmath113 of the source alphabet @xmath3 , we write @xmath114 for the subset of @xmath36 which is the union of all the distortion balls of radius @xmath115 centered at points of @xmath113 .",
    "formally , @xmath116    * for each @xmath24 , there exists a @xmath117 and a @xmath118 such that @xmath119 and @xmath120 is relatively compact for each @xmath121 .",
    "* @xmath43 is a metric space with its borel @xmath39-algebra , @xmath23 is the set of all probability distributions on @xmath36 with a metric that metrizes weak convergence of probability measures , and for each @xmath122 and each @xmath123 there exists a @xmath118 such that @xmath124 and @xmath114 is relatively compact .",
    "can always be metrized in this way , and so that @xmath23 will be separable ( compact ) if @xmath36 is separable ( compact ) @xcite .",
    "[ f : np ] ]    in section  [ s : examples ] we describe concrete situations where these assumptions are valid .",
    "the proof of theorem  [ t : lb ] has the following main ingredients .",
    "the separability of @xmath23 and the continuity in a1 are used to ensure measurability and , in particular , for controlling exceptional sets .",
    "a1 is a local assumption that ensures @xmath125 is well behaved in small neighborhoods @xmath126 .",
    "a2 is a global assumption that ensures the final analysis can be restricted to a small neighborhood . combining theorems  [ t : ub ] and  [ t : lb ]",
    "gives conditions under which @xmath127 . in the nonparametric situation we have the following corollary , which is a generalization of corollary  [ c : sc ] in the introduction ; it follows immediately from the last two theorems .",
    "1ex    [ c : sc2 ] suppose @xmath43 is a compact , separable metric space with its borel @xmath39-algebra and @xmath26 is continuous for each @xmath27 .",
    "if @xmath71 is stationary and ergodic with @xmath94 , then @xmath128 for all @xmath129 .",
    "furthermore , the compactness condition can be relaxed as in n2 .",
    "in all of the examples we assume that the source @xmath71 is stationary and ergodic with @xmath94 .",
    "let @xmath3 and @xmath36 be at most countable and let @xmath44 be unbounded in the sense that for each fixed @xmath27 and each fixed @xmath130 there are only finitely many @xmath131 with @xmath132 .",
    "n1 and n2 are clearly satisfied in the nonparametric setting where @xmath23 is the set of all probability distributions on @xmath36 , so @xmath93 for all @xmath19 except perhaps at the single value of @xmath19 where @xmath48 transitions from finite to infinite .",
    "if , in addition , for each @xmath133 there exists a @xmath134 with @xmath135 , then @xmath136 regardless of @xmath2 @xcite , and the plug - in estimator is strongly consistent for all @xmath2 and all @xmath19 .",
    "this example also yields a different proof of the general consistency result mentioned in the introduction , for the plug - in estimate of the entropy of a discrete - valued source : if we map @xmath137 into the integers , let @xmath138 , and take @xmath139 , then we obtain the strong consistency of ( * ? ? ?",
    "* cor .  1 ) .      again in the nonparametric setting , let @xmath140 be finite dimensional euclidean space , and let @xmath141 for some function @xmath110 of euclidean distance where @xmath142 is continuous and @xmath143 as @xmath144 . as in the previous example , n1 and n2 are clearly satisfied , so @xmath93 for all @xmath19 except perhaps at the single value of @xmath19 where @xmath48 transitions from finite to infinite .",
    "if furthermore @xmath145 , then @xmath136 regardless of @xmath2 @xcite and the plug - in estimator is strongly consistent for all @xmath2 and all @xmath19 .",
    "this example includes the important special case of squared - error distortion : in the nonparametric problem , the plug - in estimator is always strongly consistent under squared - error distortion over finite dimensional euclidean space , as stated in corollary  [ c : mse ] .",
    "this example also generalizes as follows .",
    "the alphabets @xmath3 and @xmath36 can be ( perhaps different ) subsets of @xmath25 , as long as @xmath36 is closed .",
    "the use of euclidean distance is not essential and we can take any @xmath146 , so that @xmath44 is not required to be translation invariant , as long as @xmath44 is continuous over @xmath36 for each fixed @xmath27 .",
    "this is enough for consistency except perhaps at a single value of @xmath19 . to use the results in @xcite to rule out any pathological values of @xmath19 , that is , to show that @xmath147 we also need @xmath3 to be closed , @xmath44 to be continuous over @xmath3 for each fixed @xmath134 and @xmath148 for each @xmath133 .",
    "let @xmath149 , let @xmath44 satisfy the assumptions of example [ ex : np ] , let @xmath150 with the euclidean metric , and for each @xmath151 let @xmath103 be gaussian with mean @xmath152 and standard deviation @xmath39 [ the case @xmath153 corresponds to the point mass at @xmath152 ] .",
    "conditions n1 and p2 are clearly satisfied , so @xmath155 for all @xmath96 . in the special case where @xmath156 is squared - error distortion , then it is not too difficult @xcite to show that @xmath157 where @xmath158 denotes the ( possibly infinite ) variance of @xmath2 , so @xmath159 and the convergence holds for all @xmath19 .",
    "furthermore , if the source @xmath2 happens to also be gaussian , then @xmath84 and the plug - in estimator is also strongly consistent for the nonparametric problem .",
    "let @xmath161 , @xmath162 , and @xmath163 .",
    "since there is only one possible distribution on @xmath36 , it is easy to show that @xmath164 for any distribution @xmath165 on @xmath3 . if @xmath166 , the only possible trouble point for consistency is @xmath167 , which is not in @xmath66 .",
    "it is easy to see that convergence ( and therefore consistency ) might fail at this point because @xmath168 will jump back and forth between @xmath68 and @xmath90 as @xmath169 jumps above and below @xmath167 .",
    "the law of the iterated logarithm implies that this failure to converge happens with probability 1 when the source is memoryless .",
    "in general , when the source is stationary and ergodic , it turns out that convergence will fail with positive probability @xcite@xcite@xcite .",
    "this slightly modified example from csiszr @xcite illustrates that @xmath170 can be discontinuous at @xmath2 even though the plug - in estimator is consistent .",
    "let @xmath171 , let @xmath165 be any distribution on @xmath3 with infinite entropy and with @xmath172 for all @xmath133 , and let @xmath173 .",
    "note that @xmath174 for all @xmath19 .",
    ", because any pair of random variables @xmath51 with @xmath175 and @xmath176 < \\infty$ ] has @xmath177 . to see this , first note that @xmath176 < \\infty$ ] implies that @xmath178 as @xmath179 ; simply use the definition of @xmath44 and ignore the @xmath180 term",
    ". computing the mutual information and using the log - sum inequality gives @xmath181 , where @xmath182 and where @xmath183 is a finite constant that comes from all of the other terms in the definition of @xmath58 combined together with the log - sum inequality .",
    "since @xmath184 and since @xmath185 for any probability distribution @xmath17 , we see that @xmath177 .",
    "we can ignore @xmath186 because the finiteness of the sum only depends on the behavior for large @xmath133 , and for large enough @xmath133 we have @xmath187 , say .",
    "] this is a special case of example  [ ex : discrete ] so the plug - in estimator is always strongly consistent regardless of @xmath2 and @xmath19 .",
    "nevertheless , @xmath170 is discontinuous everywhere it is finite .    to see this ,",
    "let the source @xmath2 be any distribution on @xmath3 with finite entropy @xmath4 .",
    "note that @xmath188 .",
    "define the mixture distribution @xmath189 .",
    "then @xmath190 in the topology of total variation . ]",
    "( and also any weaker topology ) as @xmath191 , but @xmath192 because @xmath193 for all @xmath121 .",
    "see below for a proof of this last inequality.^{-1}$ ] converges if and only if @xmath194 ) is @xmath195 ( infinite entropy ) and @xmath196 ( finite entropy ) , @xmath197 , because the relative entropies @xmath198 and @xmath199 are both finite , so @xmath200 and @xmath201 as @xmath191 . ( from the convexity of relative entropy . )",
    "this counterexample thus shows that even closeness in relative entropy between two distributions ( which is stronger than closeness in total variation ) is not enough to guarantee the closeness of the rate - distortion functions of the corresponding distributions . ]",
    "the key property of @xmath44 in this example is that there exists a @xmath165 with @xmath202 for _ all _ @xmath19 .",
    "if such a @xmath165 exists , then @xmath170 will be discontinuous in the topology of total variation at any point @xmath2 where @xmath48 is finite for exactly the same reason as above .",
    "although this specific example is based on a rather pathological distortion measure , many unbounded distortion measures on continuous alphabets , including squared - error distortion on @xmath203 , have such a @xmath165 and are thus discontinuous in the topology of total variation .",
    "be any distribution over discrete points @xmath204 where @xmath205 and where @xmath206 .",
    "this is essentially the same as csiszr s example above because any pair of random variables @xmath51 with @xmath176 < \\infty$ ] must have @xmath207 as @xmath208 . ]",
    "suppose that we want to estimate the @xmath35th - order rate - distortion function of a stationary and ergodic source @xmath71 with @xmath35th order marginal distribution @xmath209 , namely , @xmath210 where the infimum is over all @xmath211-valued random variables , with joint distribution @xmath52 in the set @xmath212 of probability distributions on @xmath213 whose marginal distribution on @xmath214 equals @xmath215 , and which have @xmath216 \\leq d$ ] for @xmath217 all our results above immediately apply to this situation .",
    "we simply estimate the first - order rate - distortion function of the sliding - block process @xmath218 defined by @xmath219 with source alphabet @xmath214 , reproduction alphabet @xmath220 and distortion measure @xmath221 , and then divide the estimate by @xmath35 .",
    "so far , we concentrated on conditions under which the plug - in estimator is consistent ; these guarantee an ( asymptotically ) accurate estimate of the best compression rate @xmath222 that can be achieved by codes restricted to some class of distributions @xmath223 . now",
    "suppose this infimum is achieved by some @xmath224 , corresponding to the optimal reproduction distribution @xmath225 .",
    "here we use a simple modification of the plug - in estimator in order to obtain estimates @xmath226 for the optimal reproduction parameter @xmath224 based on the data @xmath5 .",
    "specifically , since we have conditions under which _",
    "r_1(p_x_1^n , q_,d ) _",
    "r_1(p , q_,d ) , [ eq : approx ] we naturally consider the sequence of estimators which achieve the infima on the left - hand - side of for each @xmath227 ; that is , we simply replace the @xmath228 by an @xmath229 .",
    "since these arg - infima may not exist or may not be unique , we actually consider any sequence of * approximate minimizers * @xmath104 that have @xmath230 in the sense that below holds .",
    "similarly , minimizers @xmath224 of the right - hand - side of may not exist or be unique , either .",
    "we thus consider the ( possibly empty ) set @xmath231 containing all the minimizers of @xmath232 and address the problem of whether the estimators @xmath233 converge to @xmath231 , meaning that @xmath233 is eventually in any neighborhood of @xmath231 .",
    "our proofs are in part based on a recent result from @xcite@xcite .",
    "1ex    @xcite@xcite [ t : genaep ] if the source @xmath71 is stationary and ergodic with @xmath94 , then @xmath234 for all @xmath24 and @xmath235 for all @xmath19 in the set @xmath236",
    "1ex    similar to @xmath66 , @xmath237 always contains @xmath68 and any point where @xmath238 .",
    "since the function @xmath77 is convex and nonincreasing in @xmath19 @xcite@xcite , @xmath237 is the entire interval @xmath70 , except perhaps the single point where @xmath77 transitions from finite to infinite .",
    "somewhat loosely speaking , the main point of this paper is to give conditions under which an infimum over @xmath17 can be moved inside the limit in the above theorem .",
    "it turns out that our method of proof works equally well for moving an arg - infimum inside the limit .",
    "the next theorem , proved in section  [ s : proofs ] , is a strong consistency result giving conditions under which the approximate minimizers @xmath104 converge to the optimal parameters @xmath239 corresponding to the optimal reproduction distributions @xmath240 .",
    "1ex    [ t : arginf ] suppose the source @xmath71 is stationary and ergodic with @xmath94 , the parameter set @xmath23 is separable , and a1 and a2 hold .",
    "then for all @xmath241 , the set @xmath242 is not empty and any ( typically random ) sequence @xmath104 of approximate minimizers , i.e. , satisfying , @xmath243 has all of its limit points in @xmath231 with probability 1 .",
    "furthermore , if @xmath244 and either p2 or n2 holds , then any sequence of approximate minimizers @xmath104 is relatively compact with probability 1 .",
    "hence , @xmath245 with probability 1 .",
    "1ex      the upper and lower bounds of theorems  [ t : ub ] and  [ t : lb ] can be combined to extend our results to a variety of estimators besides the ones considered already . for example , instead of the simple plug - in estimator , @xmath246 we may wish to consider mdl - style penalized estimators , of the form , _",
    "\\{r_1(p_x_1^n , q_,d ) + f_n ( ) } , [ eq : penalty ] for appropriate ( nonnegative ) penalty functions @xmath247 .",
    "the penalty functions express our preference for certain ( typically less complex ) subsets of @xmath23 over others .",
    "this issue is , of course , particularly important when estimating the optimal reproduction distribution as discussed in the previous section .",
    "note that in the case when no distortion is allowed , these estimators reduce to the classical ones used in lossless data compression and in mdl - based model selection @xcite . indeed ,",
    "if @xmath137 are discrete sets , @xmath44 is hamming distance and @xmath139 , then the estimator in ( [ eq : penalty ] ) becomes , @xmath248 which is precisely the general form of a penalized maximum likelihood estimator .",
    "[ as usual , @xmath30 denotes the @xmath31-fold product distribution on @xmath249 corresponding to the marginal distribution @xmath17 . ]    more generally , suppose we have a sequence of functions @xmath250 with the properties that ,    @xmath251    for all @xmath31 , @xmath5 , @xmath81 and @xmath19 . for each such sequence of functions @xmath252",
    ", we define a new estimator for @xmath82 by , @xmath253 condition implies that any lower bound for the plug - in estimator also holds here .",
    "also , by considering a single @xmath254 for which @xmath255 we see that similarly implies a corresponding upper bound .",
    "we thus obtain :    1ex    [ c : phi ] theorems  [ t : ub ] , [ t : lb ] and  [ t : arginf ] remain valid if @xmath83 is replaced by @xmath256 for any sequence of functions @xmath252 satisfying and .",
    "1ex    for example , the penalized plug - in estimators above satisfy the conditions of the corollary , as long as the penalty functions @xmath257 satisfy , for each @xmath81 , @xmath258 as @xmath9 .",
    "another example is the sequence of estimators based on the `` lossy likelihoods '' of @xcite , namely , @xmath259 where @xmath260 denotes the distortion - ball of radius @xmath19 centered at @xmath5 , @xmath261 cf .",
    "again , both conditions and are valid in this case @xcite@xcite .",
    "as mentioned in the introduction , part of our motivation comes from considering the problem of estimating the rate - distortion function of distributions @xmath2 which can not be computed analytically , but which can be easily simulated by mcmc algorithms , as is very often the case in image processing , for example . of course ,",
    "mcmc samples are typically not stationary . however",
    ", the distribution of the entire sequence of mcmc samples is dominated by ( i.e. , is absolutely continuous with respect to ) a stationary and ergodic distribution , namely , the distribution of the same markov chain started from its stationary distribution , which is of course the target distribution @xmath2 .",
    "therefore , all of our results remain valid : results that hold with probability 1 in the stationary case necessarily hold with probability 1 in the nonstationary case .",
    "the only minor technicality is that the initial distribution of the mcmc chain needs to be absolutely continuous with respect to @xmath2 .",
    "more generally ( for non - markov sources ) , the requirements of stationarity and ergodicity are more restrictive than necessary .",
    "an inspection of the proofs ( both here and in the proof of theorem  [ t : genaep ] in @xcite@xcite ) , reveals that we only need the source to have the following law - of - large - numbers property :    _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ there exists a random variable @xmath262 taking values in the source alphabet @xmath3 , such that , @xmath263,\\ ] ] for every nonnegative measurable function @xmath110 . _",
    "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _    1ex    [ t : ns ] theorems  [ t : ub ] , [ t : lb ] and  [ t : arginf ] , corollary [ c : phi ] and the alternative conditions for a2 remain valid if , instead of being stationary and ergodic with @xmath94 , the source merely satisfies the lln property for some random variable @xmath264 .",
    "if the distortion measure @xmath44 is bounded , then the lln property need only hold for bounded , measurable @xmath110 .",
    "1ex    every stationary and ergodic source satisfies this lln property as does any source whose distribution is dominated by the distribution of a stationary and ergodic source .",
    "this lln property is somewhat different from the requirement that the source be asymptotically mean stationary ( a.m.s . )  with an ergodic mean stationary distribution @xcite .",
    "the latter is a stronger assumption in the sense that @xmath110 can depend on the entire future of the process , i.e. , @xmath265 $ ] , where @xmath266 is now a random variable on the infinite sequence space .",
    "it is a weaker assumption in that this convergence need only hold for bounded @xmath110 .",
    "the final statement of theorem  [ t : ns ] implies that our consistency results hold for a.m.s .",
    "sources ( with ergodic mean stationary distributions ) as long as the distortion measure is bounded .",
    "we frequently use the alternative representation @xcite@xcite @xmath267\\bigr ]   \\label{e : rpqd}\\ ] ] which is valid for all choices of @xmath2 , @xmath17 and @xmath19 .",
    "this representation makes it easy to prove that @xmath268 for @xmath269 , which is used above in example  [ ex : dc ] .",
    "indeed , @xmath270   - ( 1-\\epsilon ) e_{x\\sim p}\\bigl[\\log e_{y\\sim q_\\theta } e^{\\lambda\\rho(x , y)}\\bigr]\\bigr ] \\\\ & \\quad \\geq \\sup_{\\lambda \\leq 0 } \\bigl[\\lambda d - \\epsilon   e_{x\\sim p'}\\bigl[\\log e_{y\\sim q_\\theta } [ e^{\\lambda\\rho(x , y)}]\\bigr ] \\bigr ] \\\\ & \\quad = \\epsilon \\sup_{\\lambda \\leq 0 } \\bigl[\\lambda d/\\epsilon -   e_{x\\sim p'}\\bigl[\\log e_{y\\sim q_\\theta } [ e^{\\lambda\\rho(x , y)}]\\bigr]\\bigr ] \\\\ & \\quad = \\epsilon r_1(p',q_\\theta , d/\\epsilon ) .",
    "\\end{aligned}\\ ] ] taking the infimum over @xmath271 on both sides gives .",
    "here we discuss the various measurability assumptions that are used throughout the paper .",
    "note that we do not always establish the measurability of an event if it contains another measurable event that has probability 1 .",
    "since @xmath44 is product measurable , @xmath272 $ ] is measurable .",
    "this implies that @xmath273\\big\\}$ ] is measurable .",
    "since this is concave in @xmath274 @xcite , we can evaluate the supremum over all @xmath101 in ( [ e : rpqd ] ) by considering only countably many @xmath101 , which means that @xmath275 is measurable .",
    "if @xmath23 is a separable metric space and @xmath276 is measurable for fixed @xmath271 and continuous for fixed @xmath277 , then @xmath278 is measurable for any subset @xmath279 .",
    "this is because @xmath280 for any ( at most ) countable dense subset @xmath281 , and the latter is measurable because @xmath282 is ( at most ) countable .",
    "since @xmath23 is separable , such a @xmath282 always exists , and since @xmath283 is continuous , @xmath282 can be chosen independently of @xmath5 .",
    "an identical argument holds for @xmath284 .",
    "we make use of this frequently in the lower bound , where the necessary continuity comes from a1 .",
    "the upper bound in theorem  [ t : ub ] is deduced from theorem  [ t : genaep ] as follows .",
    "if @xmath139 or @xmath285 , then choose @xmath286 , otherwise , choose @xmath287 such that @xmath288",
    ". we can always do this since @xmath96 .",
    "now pick @xmath271 with @xmath289 .",
    "this ensures that @xmath290 and theorem  [ t : genaep ] gives @xmath291 completing the proof .",
    "notice that if we switch the @xmath292 to a @xmath98 , we can remove any restrictions on @xmath19 since there are no restrictions in this case in theorem  [ t : genaep ] .",
    "this gives .",
    "here we prove the lower bound of theorem  [ t : lb ] .",
    "let @xmath293 denote the metric on @xmath23 and let @xmath294 denote the open ball of radius @xmath295 centered at @xmath81 .",
    "the main goal is to prove that @xmath296 for all @xmath271 simultaneously , that is , the exceptional set can be chosen independently of @xmath81 . to see how this gives the lower bound ,",
    "first choose a sequence @xmath104 according to a2 and a subsequence @xmath297 along which the @xmath98 on the left side of is actually a limit .",
    "let @xmath224 be a limit point of the subsequence @xmath298 .",
    "note that such a @xmath224 exists with probability 1 by assumption a2 and that it depends on @xmath299 .",
    "we have , @xmath300 for each @xmath121 .",
    "the first inequality is from and the last is valid because infinitely many elements of @xmath298 are in @xmath301 for any @xmath121 . letting @xmath191 in and using gives @xmath302 as desired .",
    "note that with this also implies that that @xmath224 achieves the infimum in the definition of @xmath82 .",
    "we need only prove .",
    "for any @xmath303 , @xmath271 and @xmath121 , the pointwise ergodic theorem gives @xmath304 { { \\stackrel{\\text{\\tiny w.p.1}}{= } } } e_p\\left[\\sup_{\\theta'\\in o(\\theta,\\epsilon ) } \\log   e_{\\theta ' } [ e^{\\lambda\\rho(x , y)}]\\right ] .",
    "\\label{e : epi2 } \\end{aligned}\\ ] ] ( see section  [ s : meas ] for measurability . ) fix an at most countable , dense subset @xmath305 .",
    "we can choose the exceptional sets in independently of @xmath306 and @xmath122 rational .",
    "for any @xmath271 and @xmath121 we can choose a @xmath307 and a rational @xmath308 such that @xmath309 .",
    "since the exceptional sets in do not depend on @xmath310 and @xmath311 , we have that @xmath312 \\leq   \\limsup_{n\\to\\infty } \\frac{1}{n}\\sum_{k=1}^n \\sup_{\\theta'\\in o(\\theta,\\epsilon ) } \\log e_{\\theta'}[e^{\\lambda\\rho(x_k , y ) } ] \\notag \\\\ & \\quad \\leq \\limsup_{n\\to\\infty } \\frac{1}{n}\\sum_{k=1}^n \\sup_{\\theta'\\in o(\\tilde\\theta,\\tilde\\epsilon ) } \\log e_{\\theta'}[e^{\\lambda\\rho(x_k , y ) } ] \\notag   { { \\stackrel{\\text{\\tiny w.p.1}}{= } } }   e_p\\left[\\sup_{\\theta'\\in o(\\tilde\\theta,\\tilde\\epsilon ) } \\log   e_{\\theta ' } [ e^{\\lambda\\rho(x , y)}]\\right ] \\notag \\\\   & \\quad \\leq e_p\\left[\\sup_{\\theta'\\in o(\\theta,2\\epsilon ) } \\log   e_{\\theta ' } [ e^{\\lambda\\rho(x , y)}]\\right ] \\label{e : epi3 } \\end{aligned}\\ ] ] simultaneously for all @xmath271 and @xmath121 , that is , the exceptional set can be chosen independently of @xmath81 and @xmath295 .",
    "the monotone convergence theorem and the continuity in a1 give @xmath313\\right ] =   e_p\\left[\\lim_{\\epsilon\\downarrow 0 } \\sup_{\\theta'\\in o(\\theta,2\\epsilon ) } \\log e_{\\theta ' } [ e^{\\lambda\\rho(x , y)}]\\right ]   =   e_p\\left[\\log e_\\theta [ e^{\\lambda\\rho(x , y)}]\\right ] .",
    "\\end{aligned}\\ ] ] combining this with and letting @xmath191 gives @xmath314 { { \\stackrel{\\text{\\tiny w.p.1}}{\\leq } } }   e_p\\left[\\log e_\\theta [ e^{\\lambda\\rho(x , y)}]\\right ] \\label{e : epi4 } \\end{aligned}\\ ] ] simultaneously for all @xmath271 .",
    "both sides of are nondecreasing with @xmath274 .",
    "furthermore , the right side of is continuous from above for @xmath315 .",
    "( to see this , use the dominated convergence theorem to move the limit through @xmath102 and the monotone convergence theorem to move the limit through @xmath316 . )",
    "these two facts imply that we can also choose the exceptional sets independently of @xmath101 ( by first applying for @xmath274 rational and then squeezing ) .",
    "applying to the representation in gives , for each @xmath303 , @xmath317 \\right ]   \\notag \\\\ &",
    "\\quad = \\lambda d - \\lim_{\\epsilon\\downarrow 0 } \\limsup_{n\\to\\infty } \\sup_{\\theta'\\in o(\\theta,\\epsilon ) } \\frac{1}{n}\\sum_{k=1}^n \\log   e_{\\theta ' } [ e^{\\lambda\\rho(x_k , y ) } ] { { \\stackrel{\\text{\\tiny w.p.1}}{\\geq } } } \\lambda d - e_p\\left[\\log e_\\theta [ e^{\\lambda\\rho(x , y)}]\\right ]   \\end{aligned}\\ ] ] simultaneously for all @xmath271 and @xmath101 .",
    "optimizing over @xmath101 on the right gives .",
    "here we discuss the various alternative assumptions that imply a1 and a2 .",
    "p1 implies a1 because @xmath318 is bounded and measurable for each @xmath27 and @xmath101 .",
    "n1 implies a1 because @xmath318 is bounded and continuous for each @xmath27 and @xmath101 .      here",
    "we prove that p2 implies a2 when @xmath71 is stationary and ergodic with @xmath94 .",
    "fix @xmath19 , @xmath319 and @xmath113 according to p2 , so that @xmath320 is relatively compact for each @xmath121 .",
    "we will first show that @xmath321 where @xmath322 denotes the complement of @xmath323 .",
    "if @xmath322 is empty for some @xmath121 , then follows from the convention that @xmath324",
    ". we can thus focus on the case where @xmath322 is not empty for all @xmath121 .",
    "define @xmath325 . since @xmath326 we have for any @xmath327 @xmath328   \\leq { \\mathds{1}}\\{x\\in k\\}\\log\\left[\\epsilon+e^{\\lambda_\\epsilon(d+\\delta)}\\right ] = { \\mathds{1}}\\{x\\in k\\}\\log(2\\epsilon ) .",
    "\\end{aligned}\\ ] ] this and the representation in imply that @xmath329 \\right ] \\\\ & \\quad   \\geq \\frac{d}{d+\\delta}\\log \\epsilon - \\frac{1}{n}\\sum_{k=1}^n   { \\mathds{1}}\\{x_k\\in k\\}\\log(2\\epsilon ) .\\end{aligned}\\ ] ] taking limits , the pointwise ergodic theorem gives @xmath330 letting @xmath331 ( @xmath295 rational ) and noting that @xmath119 by assumption gives .",
    "now we will show that implies a2 .",
    "fix a realization @xmath332 of @xmath299 for which holds .",
    "let @xmath297 be a subsequence for which @xmath333 if @xmath334 , we can simply take @xmath335 for any constant @xmath81 and all @xmath31 . if @xmath336 , choose @xmath337 so that @xmath338 then implies that there exists an @xmath121 for which @xmath337 must be in @xmath323 for all @xmath339 large enough .",
    "since @xmath323 has compact closure , the subsequence @xmath298 is relatively compact and it can always be embedded in a relatively compact sequence @xmath104 . since @xmath332 is ( with probability 1 ) arbitrary , the proof is complete .",
    "here we prove that n2 implies a2 when @xmath71 is stationary and ergodic with @xmath94 .",
    "for each @xmath121 and each @xmath130 , let @xmath340 be the set in n2 .",
    "the pointwise ergodic theorem gives , @xmath341 fix a realization @xmath332 of @xmath299 for which holds for all rational @xmath295 and @xmath342 .",
    "let @xmath297 be a subsequence for which @xmath343 if @xmath334 , we can simply take @xmath335 for any constant @xmath81 and all @xmath31 .",
    "if @xmath336 , for @xmath339 large enough both sides are finite and we can choose @xmath344 so that @xmath345 let @xmath346 and note that @xmath347 we will show that @xmath337 is relatively compact by showing that the sequence @xmath348 is tight .",
    "on @xmath349 is said to be _ tight _ if @xmath350 , where the supremum is over all compact ( measurable ) @xmath351 .",
    "if @xmath352 is tight , then prohorov s theorem states that it is relatively compact in the topology of weak convergence of probability measures @xcite . ]",
    "this will complete the proof just like in the previous section .",
    "fix @xmath121 rational and @xmath353 rational .",
    "let @xmath354 .",
    "we have @xmath355 \\geq mw_k(k\\times b(k , m)^c ) \\geq 2dw_k(k\\times b(k , m)^c)/\\epsilon . \\end{aligned}\\ ] ] this implies that @xmath356 and we can bound @xmath357 taking limits and applying gives @xmath358 since @xmath114 has compact closure and since @xmath295 was arbitrary , the sequence @xmath352 is tight .      here",
    "we prove the convergence - of - minimizers result given in theorem  [ t : arginf ] .",
    "the proof of theorem [ t : lb ] in section  [ s : lb ] shows that @xmath231 is not empty .",
    "the assumptions ensure that both the lower and upper bounds for consistency of the plug - in estimator hold , so that @xmath155 .",
    "this shows that any sequence @xmath104 satisfying also satisfies with probability 1 , and that the @xmath292 and the @xmath98 agree .",
    "let @xmath224 be any limit point of this sequence ( if one exists ) .",
    "following the steps at the beginning of the proof of theorem  [ t : lb ] in section  [ s : lb ] , we see that @xmath359 .",
    "now further suppose that @xmath82 is finite so that @xmath360 we want to show that the sequence @xmath104 is relatively compact with probability 1 .",
    "if p2 holds , then immediately implies that there exists an @xmath121 such that @xmath361 eventually , with probability 1 .",
    "since @xmath323 is relatively compact , so is @xmath104 .    alternatively , suppose n2 holds . to show that @xmath104 is relatively compact with probability 1 , we need only show that @xmath362 is tight w.p.1 .",
    "fix a realization @xmath332 where the convergence in holds , where holds for all rational @xmath295 and @xmath342 , and where @xmath363 . for @xmath31 large enough , the left side of is finite , so @xmath364 is not empty and we can choose a sequence @xmath365 with @xmath366 so that @xmath367 let @xmath368 .",
    "an inspection of the above proof that n2 implies a2 shows that the sequence @xmath369 is tight .",
    "we will show that @xmath370 , implying that @xmath362 is also tight ( because , for example , relative entropy bounds total variation distance ) .",
    "indeed , @xmath371 since @xmath89 and @xmath372 both converge to @xmath82 , which is finite , @xmath373 , as claimed .      here",
    "we prove the result of theorem  [ t : ns ] , based on the law - of - large - numbers property . inspecting",
    "all of the proofs in this paper reveals that the assumption of a stationary and ergodic source is only used to invoke the pointwise ergodic theorem .",
    "furthermore , the pointwise ergodic theorem is not needed in full generality , only the lln property is used .",
    "the relevant equations are , and  .",
    "note that if @xmath44 is bounded , then it is enough to have the lln property hold for bounded @xmath110 .",
    "equation   from theorem  [ t : genaep ] , which we used in the proof of the upper bound , also assumes a stationary and ergodic source .",
    "the proof of a more general result than theorem  [ t : genaep ] is in @xcite@xcite , but that result makes extensive use of the stationarity assumption .",
    "a careful reading reveals that only the lln property is needed for . for completeness",
    ", we will give a proof , referring only to @xcite for results that do not depend on the nature of the source .",
    "specifically , what we need to prove for the upper bound is that @xmath374 for all @xmath375 .",
    "if the source satisfies the lln property for a random variable @xmath262 with distribution @xmath2 , then @xmath376 { { \\stackrel{\\text{\\tiny w.p.1}}{= } } } e_{x\\sim p}\\left[\\log e_{y\\sim q }   [ e^{\\lambda\\rho(x , y)}]\\right ]    : = \\lambda(\\lambda ) .",
    "\\label{e : ns : lam } \\end{aligned}\\ ] ] furthermore , since both sides are monotone in @xmath274 , the exceptional sets can be chosen independently of @xmath274 .",
    "the lln property also implies that @xmath377 { { \\stackrel{\\text{\\tiny w.p.1}}{= } } } e_{x\\sim p}\\left[e_{y\\sim q } [ \\rho(x , y)]\\right ]   : = { d_{\\operatorname*{ave}}}\\label{e : ns : dave } .",
    "\\end{aligned}\\ ] ] note that if @xmath44 is bounded , then the lln property need only hold for bounded @xmath110 in both and .",
    "define @xmath378 $ ] and @xmath379 , with the convention that the infimum of the empty set equals @xmath56 .",
    "in @xcite it is shown that @xmath380 , that @xmath381 is convex , nonincreasing and continuous from the right , and that @xmath382 where some of these cases may be empty .",
    "notice that @xmath381 is continuous except perhaps at @xmath383 , where it will not be continuous from the left if @xmath384 .",
    "fix a realization @xmath332 of @xmath299 for which holds for all @xmath274 and for which holds .",
    "define the random variables @xmath385 for @xmath227 , where the sequence @xmath386 consists of independent and identically distributed ( i.i.d . )",
    "random variables with common distribution @xmath17 .",
    "then implies that @xmath387 = \\lambda(\\lambda ) .",
    "\\label{e : zlim}\\ ] ]    we will first show that @xmath388 for all @xmath24 except the special case when both @xmath389 and @xmath390 . the second equality in is always valid @xcite@xcite .",
    "if @xmath391 , or @xmath389 and @xmath392 , or @xmath393 , the first equality in follows from ( * ? ? ?",
    "* lemma 11 ) , which is a slight modification of the grtner - ellis theorem in the theory of large deviations . the aforementioned properties of @xmath381 and the convergence in are what we need to use ( * ? ? ?",
    "* lemma 11 ) .",
    "if @xmath394 , then @xmath395 and we need only show that @xmath396 .",
    "but this follows from chebychev s inequality and because @xmath397/d \\to 1 - { d_{\\operatorname*{ave}}}/d > 0 . \\end{aligned}\\ ] ] this proves , except for the special case when @xmath389 and @xmath390  which exactly corresponds to @xmath398 .",
    "finally , gives because @xcite@xcite @xmath399 and because @xmath332 is ( with probability 1 ) arbitrary .",
    "the authors wish to thank m.  madiman for many useful comments .",
    "h.  attouch and r.  j .- b .",
    "wets , `` epigraphical analysis , '' in _ analyse non linaire _ , ser .",
    "annales de linstitut henri poincar , h.  attouch , j .-",
    "aubin , f.  clarke , and i.  ekeland , eds.1em plus 0.5em minus 0.4em paris : gauthier - villars , 1989 , pp .",
    "73100 .",
    "t.  cover and r.  wesel , `` a gambling estimate of the rate - distortion function for images , '' in _ proc .",
    "data compression conf .",
    " dcc 94 _ , ieee .",
    "1em plus 0.5em minus 0.4emlos alamitos , california : ieee computer society press , 1994 .",
    "r.  dykstra and j.  lemke , `` duality of @xmath32 projections and maximum likelihood estimation for log - linear models under cone constraints , '' _",
    "_ , vol .",
    "83 , no . 402 , pp . 546554 , 1988 .",
    " , `` the first order asymptotics of waiting times between stationary processes under nonstandard conditions , '' brown university , division of applied mathematics , providence , ri , appts # 03 - 3 , apr . 2003 .",
    "i.  kontoyiannis , p.  algoet , y.  suhov , and a.  wyner , `` nonparametric entropy estimation for stationary processes and random fields , with applications to english text , '' _ ieee trans .",
    "inform . theory _",
    "44 , no .  3 , pp .",
    "13191327 , 1998 .",
    "o.  koval and y.  rytsar , `` about estimation of the rate distortion function of the generalized gaussian distribution under mean square error criteria , '' in _ proc . xvi open scientific and technical of young scientists and specialists of institute of physics and mechanics _ , los alamitos , california , may 2001 , pp .",
    "201204 , ysc-2001 .",
    "g.  salinetti , `` consistency of statistical estimators : the epigraphical view , '' in _",
    "stochastic optimization : algorithms and applications _ , s.  uryasev and p.  m. pardalos , eds.1em plus 0.5em minus 0.4emdordrecht : kluwer academic publishers , 2001 , pp . 365383 ."
  ],
  "abstract_text": [
    "<S> motivated by questions in lossy data compression and by theoretical considerations , we examine the problem of estimating the rate - distortion function of an unknown ( not necessarily discrete - valued ) source from empirical data . </S>",
    "<S> our focus is the behavior of the so - called `` plug - in '' estimator , which is simply the rate - distortion function of the empirical distribution of the observed data . </S>",
    "<S> sufficient conditions are given for its consistency , and examples are provided to demonstrate that in certain cases it fails to converge to the true rate - distortion function . </S>",
    "<S> the analysis of its performance is complicated by the fact that the rate - distortion function is not continuous in the source distribution ; the underlying mathematical problem is closely related to the classical problem of establishing the consistency of maximum likelihood estimators . </S>",
    "<S> general consistency results are given for the plug - in estimator applied to a broad class of sources , including all stationary and ergodic ones . a more general class of estimation problems is also considered , arising in the context of lossy data compression when the allowed class of coding distributions is restricted ; analogous results are developed for the plug - in estimator in that case . </S>",
    "<S> finally , consistency theorems are formulated for modified ( e.g. , penalized ) versions of the plug - in , and for estimating the optimal reproduction distribution .    </S>",
    "<S> rate - distortion function , entropy , estimation , consistency , maximum likelihood , plug - in estimator </S>"
  ]
}