{
  "article_text": [
    "the multiple target tracking ( mtt ) problem concerns the analysis of data from multiple moving objects which are partially observed in noise to extract accurate motion trajectories .",
    "the mtt framework has been traditionally applied to solve surveillance problems but more recently there has been a surge of interest in biological signal processing , e.g. see @xcite .    the mtt framework is comprised of the following ingredients . a set of multiple independent targets moving in the surveillance region in a markov fashion .",
    "the number of targets varies over time due to departure of existing targets ( known as death ) and the arrival of new targets ( known as birth ) .",
    "the initial number of targets are unknown and the maximum number of targets present at any given time is unrestricted . at each time",
    "each target may generate an observation which is a noisy record of its _ state_. targets that do not generate observations are said to be undetected at that time .",
    "additionally , there may be spurious observations generated which are unrelated to targets ( known as clutter ) .",
    "the observation set at each time is the collection of all target generated and false measurements recorded at that time , but without any information on the origin or association of the measurements .",
    "false measurements , unknown origin of recorded measurements , undetected targets and a time varying number of targets render the task of extracting the motion trajectory of the underlying targets from the observation record , which is known as _ tracking _ in the literature , a highly challenging problem .",
    "there is a large body of work on the development of algorithms for tracking multiple moving targets .",
    "these algorithms can be categorised by how they handle the data association ( or unknown origin of recorded measurements ) problem . among the main approaches",
    "are the multiple hypothesis tracking ( mht ) algorithm @xcite and the probabilistic mht ( pmht ) variant @xcite , the joint probabilistic data association filter ( jpdaf ) @xcite , and the probability hypothesis density ( phd ) filter @xcite . with the advancement of monte carlo methodology , sequential monte carlo ( smc ) ( or particle filtering ) and markov chain monte carlo ( mcmc ) methods have been applied to the mtt problem , e.g. smc and mcmc implementations of jpda @xcite , smc implementations of the mht and pmht @xcite , and phd filter @xcite .    compared to the huge amount of work on developing tracking algorithms , the problem of estimating the static parameters of the tracking model has been largely neglected , although it is rarely the case that these parameters are known .",
    "some exceptions include the work of @xcite where they extended the mht algorithm to simultaneously estimate the parameters of the mtt model . a full bayesian approach for estimating the model parameters using mcmc was presented in @xcite .",
    "@xcite presented an approximated maximum likelihood method derived by using a poisson approximation for the posterior distribution of the hidden targets which is also central to the derivation of phd filter in @xcite .",
    "additionally , versions of phd and cardinalised phd ( cphd ) filters that can learn the clutter rate and detection profile while filtering are proposed in @xcite .    in this paper , we present maximum likelihood estimation ( mle ) algorithms to infer _ all _ the static parameters of the mtt model when the individual targets move according to a linear gaussian state - space model and when the target generated observations are linear functions of the target state corrupted with additive gaussian noise ; we will henceforth call this a linear gaussian mtt model .",
    "we maximise the likelihood function using the expectation - maximisation ( em ) algorithm and we present both online and batch em algorithms . for a linear gaussian mtt model we are able to present the exact recursions for updating static parameter estimate . to the best of our knowledge ,",
    "this is a novel development in the target tracking field .",
    "we stress though that these recursions are not obvious by virtue of the model being linear gaussian .",
    "this is because the mtt model allows for false measurements , unknown origin of recorded measurements , undetected targets and a time varying number of targets with unknown birth and death times . to implement the proposed em algorithms , an estimate of the posterior distribution of the hidden targets given the observations",
    "is required , and in the linear gaussian setting , the continuous values of the target states can be marginalised out .",
    "but , because the number of possible association of observations to targets grows very quickly with time , we have to resort to approximation schemes that focus the computation in the expectation(e)-step of the em algorithms on the most likely associations ; that is , we approximate the e - step with a monte carlo method . for this",
    "we employ both smc and mcmc which give rise to the following different mle algorithms :    * smc - em and mcmc - em algorithms for offline estimation ; and * smc online em for online estimation .",
    "we implement these three algorithms for simulated examples under various tracking scenarios and provide recommendations for the practitioner on which one is to be preferred .",
    "the em algorithms we present in this paper can be implemented with any monte - carlo scheme for inferring the target states in mtt and reducing the errors in the approximation of the e - step can only be beneficial to the em parameter estimates .",
    "we do not fully explore the use of the various monte carlo target tracking algorithms that have been proposed in the literature and instead focus on the following two . when using smc to approximate the e - step , we compute the @xmath0-best assignments @xcite as the sequential proposal scheme of the particle filter .",
    "this @xmath0-best assignments approached has appeared previously in the literature in the context of tracking , e.g. see @xcite .",
    "the mcmc algorithm we use for the e - step is the mcmc - da algorithm proposed for target tracking in @xcite . for further assessment / comparison of the em algorithms",
    ", we also implement a full bayesian estimation approach which is essentially a gibbs like sampler for estimating the static parameters that alternates between sampling the target states and static parameter .",
    "note that the bayesian approach is not novel and as it been proposed by @xcite .",
    "it is implemented in this work for the purpose of comparison with the mle techniques .",
    "the remainder of the paper is organised as follows . in section [ sec : multiple target tracking model ] , we describe the mtt model and formulate the static parameter estimation problem . in section [ sec : em algorithms for mtt ] , we present the batch and online em algorithms .",
    "section [ sec : experiments and results ] contains the numerical examples and we conclude the paper with a discussion of our findings in section [ sec : conclusion ] .",
    "the appendix contains further details on the derivation of the mtt em algorithm , and details of the smc and mcmc algorithms we use in this paper .",
    "we introduce random variables ( also sets and mappings ) with capital letters such as @xmath1 and denote their realisations by corresponding small case letters @xmath2 . if a non - discrete random variable @xmath3 has a density @xmath4 , with all densities being defined w.r.t .",
    "the lebesgue measure ( denoted by @xmath5 ) , we write @xmath6 to make explicit the law of @xmath3 . we use @xmath7 $ ] for the ( conditional ) expectation operator ; for jointly distributed random variables @xmath8 and @xmath9 and a function @xmath10 , @xmath11 $ ] is the expectation of the random variable @xmath12 w.r.t .  the joint distribution of @xmath13 conditioned on @xmath14 . @xmath15 $ ]",
    "is the expectation of the function @xmath16 for a fixed @xmath17 given @xmath18 .",
    "consider first a _ single _",
    "target tracking model where a moving object ( or target ) is observed when it traverses in a surveillance region .",
    "we define the target state and the noisy observation at time @xmath19 to be the random variables @xmath20 and @xmath21 respectively .",
    "the statistical model most commonly used for the evolution of a target and its observations @xmath22 is the hidden markov model ( hmm ) . in a hmm",
    ", it is assumed that @xmath23 is a hidden markov process with initial and transition probability densities @xmath24 and @xmath25 , respectively , and @xmath26 is the observation process with the conditional observation density @xmath27 , i.e. @xmath28 here the densities @xmath24 , @xmath25 and @xmath27 are parametrised by a real valued vector @xmath29 . in this paper , we consider a specific type of hmm , the gaussian linear state - space model ( glssm ) , which can be specified as @xmath30 where @xmath31 denotes the probability density function for the multivariate normal distribution with mean @xmath32 and covariance @xmath33 . in this case ,",
    "@xmath34 .    in a mtt model ,",
    "the state and the observation at each time ( @xmath35 ) are random finite sets , @xmath36 and @xmath37 . here",
    "each element of @xmath38 is the state of an individual target and elements of @xmath39 are the distinct measurements of these targets at time @xmath19 .",
    "the number of targets @xmath40 under surveillance changes over time due to targets entering and leaving the surveillance region @xmath41 .",
    "@xmath38 evolves to @xmath42 as follows : with probability @xmath43 each target @xmath38 ` survives ' and is displaced according to the state transition density @xmath25 in , otherwise it dies .",
    "the random deletion and markov motion happens independently for all the elements of @xmath38 .",
    "in addition to the surviving targets , new targets are created .",
    "the number of new targets created per time follows a poisson distribution with mean @xmath44 and each of their states is initiated independently according to the initial density @xmath24 in .",
    "now @xmath42 is defined to be the superposition of the states of the surviving and evolved targets from time @xmath19 and the newly born targets at time @xmath45 .",
    "the elements of @xmath38 are observed through a process of random thinning and displacement : with probability @xmath46 , each point of @xmath38 generates a noisy observation in the observation space @xmath47 through the observation density @xmath27 in .",
    "this happens independently for each point of @xmath38 .",
    "in addition to these target generated observations , false measurements are also generated .",
    "the number of false measurements collected at each time follows a poisson distribution with mean @xmath48 and their values are uniform over @xmath47 .",
    "@xmath39 is the superposition of observations originating from the detected targets and these false measurements .    a series of random variables , which are essential for the statistical analysis to follow are now defined .",
    "let @xmath49 be a @xmath50 vector of @xmath51 s and @xmath52 s where @xmath51 s indicate survivals and @xmath52 s indicate deaths of targets from time @xmath53 . for @xmath54",
    ", @xmath55 the number of surviving targets at time @xmath19 is @xmath56 .",
    "we also define the @xmath57 vector @xmath58 containing the indices of surviving targets at time @xmath19 , @xmath59 note that @xmath60 will also denote the ancestor of target @xmath61 from time @xmath53 , i.e. @xmath62 evolves to @xmath63 for @xmath64 . denoting the number of ` births ' at time @xmath65 as @xmath66",
    ", we have @xmath67 . note that according to these definitions ,",
    "the surviving targets from time @xmath53 are re - labeled as @xmath68 , and the newly born targets are denoted as @xmath69 .",
    "next , given @xmath40 targets we define @xmath70 to be a @xmath71 vector of @xmath51 s and @xmath52 s where @xmath51 s indicate detections and @xmath52 s indicate non - detections .",
    "for @xmath72 , @xmath73 therefore , the number of detected targets at time @xmath19 is @xmath74 .",
    "similarly , we also define the @xmath75 vector @xmath76 showing the indices of the detected targets , @xmath77 @xmath78 denotes the label of the @xmath61-th detected target at time @xmath19 .",
    "so the detected targets at time @xmath19 are @xmath79 .",
    "finally , defining the number of false measurements at time @xmath19 as @xmath80 , we have @xmath81 and the association from the detected targets to the observations can be represented by a one - to - one mapping @xmath82 where at time @xmath19 the @xmath61th detected target is target @xmath78 with state value @xmath83 and generates @xmath84 .",
    "we assume that @xmath85 is uniform over the set of all @xmath86 possible one - to - one mappings .",
    "to summarise , we give the list of the random variables in the mtt model introduced in this section as well as a sample realisation of them in figure [ fig : mtt ] .        * complete list of random variables of the mtt model *     + @xmath87 , @xmath88 : @xmath89th target and @xmath89th observation at time @xmath19 .",
    "+ @xmath90 , @xmath91 : sets of targets and observations at time @xmath19 .",
    "+ @xmath92 : numbers of newborn targets and false measurements at time @xmath19 + @xmath93 : numbers of targets survived from time @xmath53 to time @xmath19 and detected at time @xmath19 .",
    "+ @xmath94 : numbers of alive targets and observations at time @xmath19 .",
    "@xmath67 , @xmath81 .",
    "+ @xmath49 : @xmath50 vector of @xmath52 s and @xmath51 s indicating surviving targets from time @xmath53 to time @xmath19 .",
    "+ @xmath70 : @xmath71 vector of @xmath52 s and @xmath51 s indicating detected targets at time @xmath19 .",
    "+ @xmath58 : @xmath57 vector of labels of surviving targets from time @xmath53 to time @xmath19 .",
    "+ @xmath76 : @xmath95 vector of labels of detected targets at time @xmath19 .",
    "+ @xmath96 : association from detected targets to observations at time @xmath19 .",
    "+    [ name = x11 , style = cdet ] @xmath97 & & [ name = x21 , style = cmisdet ] @xmath98 & & [ name = x31 , style = cdet ] @xmath99 & & [ name = x41 , style = cmisdet ] @xmath100 & & [ name = x51 , style = cdet ] @xmath101 + & [ name = y11 , mnode = r ] & & & & [ name = y31 , mnode = r ] & & & & [ name = y51 , mnode = r ] + [ name = x12 , style = cdet]@xmath102 & & [ name = x22 , style = cdet ] @xmath103 & & [ name = x32 , style = cdet]@xmath104 & & [ name = x42 , style = cdet ] @xmath105 & & [ name = x52 , style = cdet]@xmath106 + & [ name = y12 , mnode = r ] & & [ name = y22 , mnode = r ] & & [ name = y32 , mnode = r ] & & [ name = y42 , mnode = r ] & & [ name = y52 , mnode = r ] + [ name = x13 , style = cmisdet]@xmath107 & & [ name = x23 , style = cdet]@xmath108 & & [ name = x33 , style = cdet]@xmath109 & & [ name = x43 , style = cdet]@xmath110 & & [ name = x53 , style = cdet]@xmath111 + & [ name = y13 , mnode = r ] & & [ name = y23 , mnode = r ] & & [ name = y33 , mnode = r ] & & [ name = y43 , mnode = r ] & & [ name = y53 , mnode = r ] + & [ name = y15 , mnode = r ] & [ name = x24 , style = cdet]@xmath112 & & & [ name = y34 , mnode = r ] & [ name = x44 , style = cmisdet]@xmath113 & & [ name = x54 , style = cdet]@xmath114 + & [ name = y14 , mnode = r ] & & [ name = y24 , mnode = r ] & & [ name = y35 , mnode = r ] & & [ name = y44 , mnode = r ] & & [ name = y54 , mnode = r ]",
    "the main difficulty in an mtt problem is that in general we do not know birth - death times of targets , whether they are detected or not , and which observation point in @xmath39 is associated to which detected target in @xmath38 .",
    "let @xmath115 be the collection of the just mentioned unknown random variables at time @xmath19 , and @xmath116^{2 } \\times [ 0 , \\infty)^{2}\\ ] ] be the vector of the mtt model parameters .",
    "we can write the joint likelihood of all the random variables of the mtt model up to time @xmath65 given @xmath117 as @xmath118 where @xmath119 here @xmath120 denotes the probability mass function of the poisson distribution with mean @xmath121 , @xmath122 is the volume ( w.r.t .",
    "the lebesgue measure ) of @xmath47 and the term @xmath123 in ( [ eq : density of z ] ) corresponds to the law of @xmath124 the marginal likelihood of the observation sequence @xmath125 is @xmath126.\\ ] ] the main aim of this paper is , given @xmath127 , to estimate the static parameter @xmath128 where we assume the data is generated by some true but unknown @xmath129 .",
    "our main contribution is to present the em algorithms , both batch and online versions , for computing the mle of @xmath128 : @xmath130 for comparison sake we also present the bayesian estimate of @xmath128 . in the bayesian approach ,",
    "the static parameter is treated as random variable taking values @xmath117 in @xmath131 with a probability density @xmath132 and the aim is to evaluate the density of the posterior distribution of @xmath117 given @xmath125 , i.e.@xmath133 @xcite use mcmc to sample from @xmath134 which integrates both metropolis - hastings and gibbs moves .",
    "in this section we present the batch and online em algorithms for linear gaussian mtt models . the notation is involved and we provide a list of the important variables used in the derivation of the em algorithms in table [ table : em variables ] at the end of the section .      given @xmath127 , the em algorithm for maximising @xmath135 in",
    "is given by the following iterative procedure : if @xmath136 is the estimate of the em algorithm at the @xmath137th iteration , then at iteration @xmath138 the estimate is updated by first calculating the following intermediate optimisation criterion , which is known as the expectation ( e ) step , @xmath139   \\\\ & = \\mathbb{e}_{\\theta_{j } } \\left [ \\log p_{\\theta}(z_{1:n } ) + \\log p_{\\theta}(\\mathbf{x}_{1:n } , \\mathbf{y}_{1:n } | z_{1:n } ) | \\mathbf{y}_{1:n } \\right ]   \\\\ &   = \\mathbb{e}_{\\theta_{j } } \\left [ \\log p_{\\theta}(z_{1:n } ) \\right .",
    "\\\\ &   \\quad \\left .",
    "+ \\mathbb{e}_{\\theta_{j } } \\left\\ { \\log p_{\\theta}(\\mathbf{x}_{1:n } , \\mathbf{y}_{1:n } | z_{1:n } ) | \\mathbf{y}_{1:n } , z_{1:n }   \\right\\ } | \\mathbf{y}_{1:n}\\right ] \\end{split}\\end{aligned}\\ ] ] the updated estimate is then computed in the maximisation ( m ) step @xmath140 this procedure is repeated until @xmath136 converges ( or in practice ceases to change significantly ) . from equations - , it can be shown that the e - step at the @xmath137th iteration reduces to calculating the expectations of fifteen sufficient statistics of @xmath141 , @xmath142 and @xmath125 denoted by @xmath143 .",
    "( from now on , any dependancy on @xmath125 in these sufficient statistics and further variables arising from them will be omitted from the notation for simplicity . )",
    "sufficient statistics @xmath144 to @xmath145 are : @xmath146 these sufficient statistics are related to those used for estimating the static parameters of a linear gaussian single target tracking model , and this relation will be made more explicit later .",
    "the rest of the sufficient statistics @xmath147 to @xmath148 do not depend on @xmath141 .",
    "@xmath149(z_{1:n } ) \\nonumber \\\\ & \\quad = \\sum_{t = 1}^{n } \\left [ \\sum_{k = 1}^{k_{t}^{d } } y_{t , a_{t}(k ) } y_{t , a_{t}(k)}^{t } , k_{t}^{d } , k^{x}_{t } ,   k^{s}_{t } , k^{x}_{t-1 } , k^{b}_{t } , k^{f}_{t } , 1 \\right ] \\label{eq : mtt sufficient statistics}\\end{aligned}\\ ] ] let @xmath150 denote the expectation of the @xmath151th sufficient statistic w.r.t .",
    "the law of the latent variables @xmath152 and @xmath153 conditional upon the observation @xmath125 for a given @xmath117 , i.e. @xmath154 & 1 \\leq m \\leq 7 , \\\\        \\mathbb{e}_{\\theta}\\left [ \\left .",
    "s_{m , n } \\left ( z_{1:n } \\right )   \\right\\vert \\mathbf{y}_{1:n } \\right ] & 8 \\leq m \\leq 15 .",
    "\\end{cases}\\end{aligned}\\ ] ] then the solution to the m - step is given by a known function @xmath155 such that at iteration @xmath137 @xmath156 the explicit expression of @xmath157 depends on the parametrisation of the mtt model , in particular on the parametrisation of the matrices @xmath158 as in the following example .",
    "[ ex : the constant velocity model ] ( _ the constant velocity model : _ ) each target has a position and velocity in the @xmath159-plane and hence @xmath160^{t } \\in \\mathcal{x } =   \\mathbb{r}^{2 } \\times   [ 0 , \\infty)^{2},\\ ] ] where @xmath161 are the @xmath162 and @xmath163 coordinates and @xmath164 are the velocities in @xmath162 and @xmath163 directions .",
    "only a noisy measurement of the position of the target is available @xmath165 \\in \\mathcal{y } = [ -\\kappa , \\kappa]^{2}.\\ ] ] we assumed a bounded @xmath47 and regard observations that are not recorded due to being outside this interval as also a missed detection . with reference to , the single target state - space model is @xmath166^{t } , \\quad \\sigma_{b } = \\left ( \\begin{array}{cc } \\sigma_{bp}^{2 } i_{2 \\times 2 } & \\mathbf{0}_{2 \\times 2 } \\\\",
    "\\mathbf{0}_{2 \\times 2 } & \\sigma_{bv}^{2 } i_{2 \\times 2 } \\end{array } \\right)\\nonumber\\\\ f = \\left(\\begin{array}{cc } i_{2 \\times 2 } & \\delta i_{2 \\times 2 } \\\\ \\mathbf{0}_{2 \\times 2 } & i_{2 \\times 2 } \\end{array}\\right ) , \\quad g = \\left(\\begin{array}{cc } i_{2 \\times 2 } & \\mathbf{0}_{2 \\times 2 } \\end{array}\\right)\\nonumber\\\\ w = \\left(\\begin{array}{cc } \\sigma_{xp}^{2 } i_{2 \\times 2 }   &   \\mathbf{0}_{2 \\times 2 } \\\\",
    "\\mathbf{0}_{2 \\times 2 } & \\sigma_{xv}^{2 } i_{2 \\times 2 } \\end{array}\\right ) , \\quad v = \\sigma_{y}^{2 } i_{2 \\times 2}\\nonumber\\end{gathered}\\ ] ]    therefore , the parameter vector of this mtt model is @xmath167 the update rule @xmath157 for @xmath117 at the m - step of the em algorithm is @xmath168 where @xmath169 , and @xmath170 and @xmath171 are the upper and lower halves of @xmath172 , that is @xmath173 and @xmath174 for @xmath175 and @xmath176 .",
    "it is easy to calculate the expectation of the sufficient statistics in that do not depend on @xmath141 . noting that @xmath177 is discrete , we simply calculate @xmath178 for every @xmath142 with a positive mass w.r.t .  to the density @xmath179 and calculate the expectations as @xmath180 for those sufficient statistics in that depend on @xmath141 , consider the last expression in with the following factorisation of the posterior @xmath181 this factorisation suggests that we can write the required expectations as @xmath182 \\nonumber \\\\ & = \\mathbb{e}_{\\theta } \\left [ \\left .",
    "\\mathbb{e}_{\\theta } \\left [ \\left .",
    "s_{m , n}(\\mathbf{x}_{1:n } , z_{1:n } ) \\right\\vert z_{1:n } , \\mathbf{y}_{1:n } \\right ] \\right\\vert \\mathbf{y}_{1:n } \\right ] .",
    "\\label{eq : nested expectation for x dependant sufficient statistics}\\end{aligned}\\ ] ] let us define the integrand of the outer expectation in which is the conditional expectation @xmath183.\\ ] ] as a matrix - valued function with domain @xmath184 .",
    "then , we can obtain @xmath150 by calculating @xmath185 for every @xmath142 with a positive mass w.r.t .",
    "the density @xmath179 and then calculate @xmath186 the crucial point here is that it is possible to calculate @xmath185 for any given @xmath142 .",
    "in fact , the availability of this calculation is based on the following fact : _ conditional on @xmath187 , @xmath188 may be regarded as a collection of independent glssms ( with different starting and ending times , possible missing observations ) and observations which are not relevant to any of these glssms_. in the context of mtt , each glssm corresponds to a target and irrelevant observations correspond to false measurements .",
    "we defer details on how @xmath185 is calculated to section [ sec : online em for mtt ] .      for exact calculation of the e - step of the em algorithm",
    "we need @xmath179 which is infeasible to calculate due to the huge cardinality of @xmath184 .",
    "we thus resort to monte carlo approximations of @xmath179 which we then use in the e - step ; in literature this approach is generically known as the stochastic em algorithm @xcite ) .",
    "we know from the previous sections that given @xmath189 the posterior distribution @xmath190 is gaussian and conditional expectations can be evaluated .",
    "therefore , it is sufficient to have the monte carlo particle approximation for @xmath179 only , which is expressed as @xmath191 then , the corresponding particle approximations for the expectations of the sufficient statistics are @xmath192 when @xmath117 changes with each em iteration , the appropriate update scheme at iteration @xmath137 involves a stochastic approximation procedure where in the e - step one calculates a weighted average of @xmath193 ; the resulting algorithm is known as the stochastic approximation em ( saem ) @xcite .",
    "specifically , let @xmath194 , called the step - size sequence , be a positive decreasing sequence satisfying @xmath195 a common choice is @xmath196 for @xmath197 .",
    "the saem algorithm is given in algorithm [ alg : saem for the mtt model ] .",
    "[ alg : saem for the mtt model ] * the saem algorithm for the mtt model * + start with @xmath198 and @xmath199 for @xmath200 . for @xmath201    * * e - step : * calculate @xmath202 for each @xmath151 , and then calculate the weighted averages @xmath203 * * m - step * update the parameter estimate using @xmath204 as before @xmath205    in general , the monte carlo approximation @xmath206 in is performed either sampling @xmath207 samples from @xmath208 using a mcmc method ( in which case weights @xmath209 , @xmath210 ) or using a smc method with @xmath207 particles . depending on",
    "which method is used , we will call the resulting algorithm mcmc - em or smc - em , respectively . for mcmc",
    ", we use the mcmc - da algorithm of @xcite , but with some refinements of the mcmc proposals .",
    "( details are available from the authors . )",
    "we use smc to obtain the approximations @xmath211 sequentially as follows .",
    "assume that we have the approximation at time @xmath53 @xmath212 to avoid weight degeneracy , at each time one can resample from @xmath213 to obtain a new collection of @xmath207 particles and then proceed to the time @xmath19 .",
    "alternatively , this resampling operation can be done according to a criterion which measures the weight degeneracy ( e.g. see @xcite ) .",
    "we define the @xmath214 random mapping @xmath215 containing the indices of the resampled particles , i.e. @xmath216 if the @xmath61th resampled particle is @xmath217 .",
    "( if no resampling is performed at the end of time @xmath53 , then @xmath218 for all @xmath61 . )",
    "then , given @xmath219 and @xmath220 , the particle @xmath221 at time @xmath19 is sampled from a proposal distribution @xmath222 for @xmath210 . therefore , @xmath221 is connected to @xmath223 and the @xmath61th path particle at time @xmath19 is @xmath224 and its new weight is @xmath225 where , for @xmath210 , we take @xmath226 if resampling is performed and @xmath227 otherwise .    note",
    "that we also need to implement smc for the online em algorithm in order to obtain a monte carlo approximation of the e - step .",
    "our smc algorithm calculates the @xmath0-best linear assignments @xcite as the sequential proposal ; see appendix [ sec : smc algorithm for mtt ] for details .",
    "we showed in the previous section how to implement the batch em algorithm for mtt using monte carlo approximations .",
    "however , the batch em algorithm is computationally demanding when the data sequence @xmath125 is long since one iteration of the em requires a complete browse of the data . in these situations , the online version of the em algorithm which updates the parameter estimates as a new data record is received at each time can be a much cheaper alternative . in this section",
    ", we present a smc online em algorithm for linear gaussian mtt models .",
    "an important observation at this point is that the sufficient statistics of interest for the em algorithm have a certain additive form such that the difference of @xmath228 and @xmath229 only depends on @xmath230 .",
    "this enables us to compute the required expectations in the e - step of the em algorithm effectively in an online manner .",
    "we shall see in this section that , with a fixed amount of computation and memory per time , it is possible to update from @xmath231 to @xmath232 given @xmath219 and @xmath233 at time @xmath19 .",
    "to show how to handle the sufficient statistics in for the mtt model , we first start with a single glssm and then extend the idea to the mtt case by showing the relation between the sufficient statistics in a single glssm and in the mtt model .",
    "consider the hmm @xmath234 defined in .",
    "it is possible to evaluate expectations of additive functionals of @xmath235 of the form @xmath236 ( with possible dependancy on @xmath237 also allowed ) w.r.t .",
    "the posterior density @xmath238 in an online manner using only the filtering densities @xmath239 .",
    "the technique is based on the following recursion on the intermediate function @xcite @xmath240 \\nonumber \\\\",
    "= & \\mathbb{e}_{\\theta } \\left [ \\left .",
    "t_{t-1}^{\\theta}(x_{t-1 } ) + s(x_{t-1 } , x_{t } ) \\right\\vert y_{1:t-1 } , x_{t } \\right ]   \\label{eq : fsintermediate}\\end{aligned}\\ ] ] with the initial condition @xmath241 .",
    "note that the expectation required for the recursion is w.r.t .",
    "the backward transition density @xmath242 .",
    "the required expectation @xmath243 $ ] can then be calculated as the expectation of the intermediate function @xmath244 w.r.t .",
    "the filtering density @xmath245 , that is , @xmath246",
    "= \\mathbb{e}_{\\theta } \\left [ \\left .",
    "t_{n}^{\\theta}(x_{n } ) \\right\\vert y_{1:n } \\right].\\ ] ] consider now the glssm that is defined in , where , additionally , @xmath247 is possibly missing / undetected and @xmath248 is the indicator of detection at time @xmath19 .",
    "it is well known that , given @xmath249 , the prediction and filtering densities @xmath250 and @xmath251 are gaussians with means @xmath252 and covariances @xmath253 and are updated sequentially as follows : @xmath254 where @xmath255 and @xmath256 . also , letting @xmath257 , @xmath258 , and @xmath259 we can show that the backward transition density required for the forward smoothing recursion ( [ eq : fsintermediate ] ) is gaussian as well @xmath260 we define the matrix valued functions @xmath261 such that @xmath262 for @xmath263 are in the following form : @xmath264 ( so , @xmath265 and @xmath266 , else @xmath267 ) .",
    "these functions are actually the sufficient statistics in the mtt model corresponding to a single target .",
    "then it is possible to define the incremental functions @xmath268 where @xmath269 s are defined such that for @xmath270 @xmath271 for example , @xmath272 , @xmath273 , @xmath274 , @xmath275 , @xmath276 , etc .",
    "we observe that each sufficient statistic is a matrix valued quantity , hence its expectation can be calculated using forward smoothing by treating each element of the matrix separately .",
    "for example , for @xmath277 we perform forward smoothing for each @xmath278 it was shown in @xcite that , the intermediate function @xmath279\\ ] ] for the @xmath280th element is a quadratic in @xmath281 : @xmath282 where @xmath283 is a @xmath284 matrix , @xmath285 is a @xmath286 vector , and @xmath287 is a scalar .",
    "online smoothing is then performed via the following recursion over the variables @xmath288 . @xmath289 where @xmath290 is the @xmath61th column of the identity matrix of the size @xmath291 , and @xmath292 is the trace of the matrix @xmath293 . for the initial value of @xmath294 , @xmath295",
    "therefore , the @xmath280th element of the required expectation at time @xmath65 can be calculated as @xmath296 = \\\\ & \\quad\\quad \\text{tr } \\left(\\bar{p}_{1 , n , ij } \\left ( \\sigma_{n|n } + \\mu_{n|n } \\mu_{n|n}^{t } \\right ) \\right ) + \\bar{q}_{1 , n , ij}^{t } \\mu_{n|n } + \\bar{r}_{1 , n , ij}. \\nonumber\\end{aligned}\\ ] ] we can similarly obtain the recursions for the other sufficient statistics in terms of variables @xmath297 for the @xmath151th sufficient statistic ( see appendix [ sec : recursive updates for sufficient statistics in a single glssm ] ) @xcite .    note",
    "that @xmath298 ( similarly for @xmath285 ) and therefore need only be calculated for @xmath299 .",
    "note that the variables @xmath300 obviously depend on @xmath301 , @xmath302 and @xmath117 , but we made this dependancy implicit in our notation for simplicity .",
    "we will carry on with this simplification in the rest of the paper .",
    "we showed above how to calculate expectations of the required sufficient for a single glssm .",
    "we can extend that idea to the scenario in the mtt case , where there may be multiple glssms at a time , with different starting and ending times and possible missing observations . recall that at time @xmath19",
    "the targets which are alive are the @xmath303 surviving targets from @xmath53 and the @xmath304 newly born targets at time @xmath19 , so the number of targets is @xmath305 . for each alive target",
    ", we can calculate the moments of the prediction density @xmath306 for the state @xmath307 recall that @xmath308 appears above due to the relabelling of surviving targets from time @xmath53 .",
    "also , given the detection vector @xmath309 and the association vector @xmath310 , we calculate the moments of the filtering density @xmath311 for the targets using the prediction moments @xmath312 where @xmath313 and @xmath314 , where @xmath315 .",
    "note that if the @xmath89th alive target at time @xmath19 is detected , it will be the @xmath316th detected target , which explains @xmath316 in @xmath317 . in a similar manner",
    ", we calculate @xmath318 , @xmath319 , and @xmath320 using @xmath321 and @xmath322 for @xmath323 in analogy with @xmath324 , @xmath325 , and @xmath326 .    in the following",
    ", we will present the rules for one - step update of the expectations @xmath327\\ ] ] of the sufficient statistics @xmath228 that are defined in . observe that we can write for @xmath328 , @xmath329 where the functions @xmath330 can be written in terms of @xmath269 s ( [ eq : barsm ] ) as follows : @xmath331 where , again , @xmath315 .",
    "( notice that if @xmath332 this @xmath316 can still be used as a convention ; since the choice of the observation point in @xmath219 is irrelevant as it will have no contribution being multiplied by @xmath333 . ) therefore , the forward smoothing recursion for those sufficient statistics in at time @xmath19 @xmath334 \\label{eq : recursive update of t in mtt } \\end{split}\\ ] ] can be handled once we have the forward smoothing recursion rules for the sufficient statistics in . for @xmath323 ,",
    "let @xmath335 denote the forward smoothing recursion function for the @xmath151th sufficient statistic for @xmath89th alive target at time @xmath19 .",
    "for the surviving targets , @xmath89th target at time @xmath19 is a continuation of the @xmath336the target at time @xmath53 .",
    "therefore , we have the recursion update for @xmath335 for @xmath337 as @xmath338 . \\end{split}\\ ] ] for the targets born at time @xmath19 ( for @xmath339 ) , the recursion function is initiated as @xmath340 .",
    "therefore , the @xmath341th component of the recursion function can be written as @xmath342 similarly to the single glssm case , where this time we have the additional subscript @xmath89 . for surviving targets",
    "the recursion variables @xmath343 for each @xmath344 are updated from @xmath345 , by using @xmath346 , @xmath347 , @xmath348 , @xmath349 , @xmath350 , @xmath333 and , @xmath351 with @xmath352 . for the targets born at time @xmath19 ( for @xmath339 ) , the variables are set to their initial values in the same way as in section [ sec : online smoothing in a single glssm ] using @xmath333 and , if @xmath353 , @xmath351 .",
    "the conditional expectations of sufficient statistics @xmath354\\ ] ] can then be calculated by using the forward recursion variables and the filtering moments .",
    "let @xmath355\\ ] ] denote the expectation of the @xmath151th sufficient statistic for the @xmath89th alive target at time @xmath19 , where its @xmath341th component is @xmath356 then , the required conditional expectation for the @xmath151th sufficient statistic can be written as the sum of two quantities @xmath357 where the quantities are respectively the contributions of the alive targets at time @xmath19 and dead targets up to time @xmath19 to the conditional expectation @xmath232 @xmath358 as shows , we also need to calculate @xmath359 at each time and by this can easily be done by storing @xmath360 at time @xmath53 and using the recursion @xmath361 where the terms in the sum correspond to targets that terminate at time @xmath53 .    finally , the sufficient statistics @xmath362 can be calculated online since we can write for each @xmath363 @xmath364 for some suitable functions @xmath330 which can easily be constructed from .",
    "hence they can be updated online as @xmath365    we now present algorithm [ alg : one step update for sufficient statistics in the mtt model ] to show how these one - step update rules for the sufficient statistics in the mtt model can be implemented . for simplicity of the presentation , we will use a short hand notation for representing the forward recursion variables in a batch way .",
    "let @xmath366 where @xmath367 denote all the variables required for the forward smoothing recursion for the @xmath151th sufficient statistic for the @xmath89th alive target at time @xmath19 .",
    "we can now present the algorithm using this notation .",
    "[ alg : one step update for sufficient statistics in the mtt model ] * one step update for sufficient statistics in the mtt model * + we have @xmath368 , @xmath360 , @xmath369 , @xmath370 , @xmath371 at time @xmath53 . given @xmath233 and @xmath219 , + - set @xmath372 , @xmath373 , @xmath374 and @xmath375 for @xmath270 .",
    "+ - for @xmath376    * if @xmath377 and @xmath378 , ( the @xmath61th target at time @xmath53 survives ) , or if @xmath379 , ( a new target is born ) , set @xmath380 . * * in case of survival , use @xmath381 and @xmath382 to obtain the prediction moments @xmath383 and @xmath384 . in case of birth ,",
    "set the prediction distribution @xmath385 and @xmath386 .",
    "* * * if @xmath387 , @xmath388th target is detected : @xmath389 .",
    "use @xmath383 and @xmath384 and @xmath390 to update the filtering moments @xmath391 and @xmath392 . * * * if @xmath393 , @xmath388th target is not detected : set @xmath394 . * * for @xmath270 * * * in case of survival , update the recursion variables @xmath395 using @xmath396 , @xmath381 , @xmath382 , @xmath397 , @xmath398 , @xmath399 , @xmath400 and @xmath401 if @xmath402 . in case of birth , initiate @xmath395 using @xmath400 and @xmath401 if @xmath402 . * * * * _ ( optional ) _ * calculate @xmath403 using @xmath395 , @xmath391 and @xmath404 and update @xmath405 . * if",
    "@xmath377 and @xmath406 , the @xmath61th target at time @xmath53 is dead . for @xmath270 , * * calculate @xmath407 from @xmath408 , @xmath381 and @xmath382 .",
    "* * update @xmath409    - * _ ( optional ) _ * update @xmath410 for @xmath270 .",
    "+ - update @xmath411 for @xmath363 .",
    "notice that the lines of the algorithm labeled as  optional \" are not necessary for the recursion and need not to be performed at every time step .",
    "for example , we can use algorithm [ alg : one step update for sufficient statistics in the mtt model ] in a batch em to save memory , in that case we perform these steps only at the last time step @xmath65 to obtain the required expectations .",
    "notice also that we included the update rule for the sufficient statistics in for completeness .      in order to develop an online em algorithm",
    ", we exploit the availability of calculating @xmath412 and @xmath413 in an online manner as shown in section [ sec : application to mtt ] . in online em , running averages of sufficient statistics are calculated and then used to update the estimate of @xmath128 at each time @xcite .",
    "let @xmath198 be the initial guess of @xmath128 before having made any observations and at time @xmath19 , let @xmath414 be the sequence of parameter estimates of the online em algorithm computed sequentially based on @xmath415 .",
    "when @xmath219 is received , we first update the posterior density to have @xmath416 , and compute for @xmath417 @xmath418 \\label{eq : stochastic approximation of forward smoothing}\\end{aligned}\\ ] ] for the values @xmath419 for @xmath210 , where we have the same constraints on the step - size sequence @xmath420 as in the saem algorithm .",
    "this modification reflects on the updates rules for the variables in @xmath421 . to illustrate the change in the recursions with an example , the recursion rules for the variables for @xmath422 for the simple glssm case become ( see appendix [ sec : recursive updates for sufficient statistics in a single glssm ] ) @xmath423 so this time we have @xmath424 where @xmath425 and the conditional expectations @xmath426 can be calculated by using @xmath427 as in section [ sec : application to mtt ] . finally ,",
    "regarding those @xmath428 in , we calculate @xmath429 @xmath430 for the values @xmath419 for @xmath210 . in the maximisation step ,",
    "we update @xmath431 where the expectations are obtained @xmath432 in practice , the maximisation step is not executed until a burn - in time @xmath433 for added stability of the estimators ( e.g. see @xcite ) .",
    "notice that the smc online em algorithm can be implemented with the help of algorithm [ alg : one step update for sufficient statistics in the mtt model ] the only changes are and instead of and .",
    "algorithm [ alg : online em for the mtt model ] describes the smc online em algorithm for the mtt model .",
    "[ alg : online em for the mtt model ] * the smc online em algorithm for the mtt model *    * * e - step : * if @xmath434 , start with @xmath198 , obtain @xmath435 , and for @xmath210 initialise + @xmath436 , @xmath437 for @xmath270 and @xmath438 for @xmath439 , + if @xmath35 , + obtain @xmath440 from @xmath441 along with @xmath442 .",
    "+ for @xmath210 , set @xmath443 .",
    "use algorithm [ alg : one step update for sufficient statistics in the mtt model ] with the stochastic approximation to obtain + @xmath444 , @xmath445 for @xmath270 and @xmath446 for @xmath439 from + @xmath447 , @xmath448 for @xmath270 and @xmath449 for @xmath450 . *",
    "* m - step : * if @xmath451 , @xmath452 .",
    "else , for @xmath210 , @xmath270 calculate @xmath453 and @xmath454 ( ` * * optional * * ' lines in algorithm [ alg : one step update for sufficient statistics in the mtt model ] ) .",
    "calculate the expectations @xmath455 \\\\ & \\quad = \\sum_{i = 1}^{n } w_{n}^{(i ) } \\left [ \\widetilde{s}_{\\gamma , m , t}^{\\theta } , \\ldots , \\widetilde{s}_{\\gamma , 7 , t}^{\\theta_{1:t } } ,   s_{\\gamma , 8 , t } , \\ldots ,   s_{\\gamma , 15 , t }   \\right ] \\left ( z_{1:t}^{(i ) } \\right ) .",
    "\\end{split}\\ ] ] and update @xmath431 .    finally , before ending this section , we list in table [ table : em variables ] some important variables used to describe the em algorithms throughout the section .",
    ".the list of the em variables used in section [ sec : em algorithms for mtt ] [ cols= \" < \" , ]     [ table : em variables ]",
    "we compare the performance of the parameter estimation methods described in section [ sec : em algorithms for mtt ] for the constant velocity model in example [ ex : the constant velocity model ] , where the parameter vector is @xmath167 note that the constant velocity model assumes the position noise variance @xmath456 .",
    "all other parameters are estimated .",
    "we run two experiments using the constant velocity model in the batch setting . in the first experiment",
    ", we generate an observation sequence of length @xmath457 by using the parameter value @xmath458 and window size @xmath459 .",
    "this particular value of @xmath128 creates on average @xmath51 target every @xmath460 time steps , and the average life of a target is @xmath461 time steps .",
    "therefore we expect to see around @xmath462 targets per time .    using the generated data set , we compare the performance of the three different methods for batch estimation , which are smc - em and mcmc - em ( two different implementations of saem in algorithm [ alg : saem for the mtt model ] ) for mle , and mcmc for the bayesian estimation @xcite . for smc - em , we used @xmath463 particles to implement the smc method based on the @xmath0-best linear assignment to sample associations , where we set @xmath464 , the details of the smc method are in appendix [ sec : smc algorithm for mtt ] . for the mcmc - em , in each em iteration we ran @xmath460 mcmc steps and the last sample is taken to compute the sufficient statistics , i.e.  @xmath465 . for both the smc and mcmc implementations of saem ,",
    "@xmath466 is used as the sequence of step - sizes for all parameters to be estimated , with the exception that @xmath467 is used for estimating @xmath468 .",
    "that is to say , in the saem algorithm , @xmath469 , @xmath470 , and @xmath471 are calculated using @xmath467 , and @xmath472 is calculated twice by using @xmath467 and @xmath466 separately ( since it appears both in the estimation of @xmath468 and @xmath43 ) , and for the rest of @xmath473 @xmath466 is used . for bayesian estimation ,",
    "the following conjugate priors are used : @xmath474    figure [ fig : saem_estimates_for_mtt ] shows the results obtained using smc - em , mcmc - em and mcmc after @xmath475 , @xmath476 , @xmath476 iterations respectively . for the bayesian estimate",
    ", we consider only the last @xmath477 samples generated using mcmc as samples from the true posterior @xmath134 . for comparison",
    ", we also execute the em algorithm with the true data association and the resulting @xmath128 estimate will serve as the benchmark .",
    "note that given the true association , the em can be executed without the need for any monte carlo approximation , and it gave the estimate @xmath478 the @xmath17 in the superscript is to indicate that this value of @xmath117 maximises the joint probability density of @xmath125 and @xmath142 , i.e. @xmath479 which is different than @xmath480 . however , for a data size of @xmath481 , @xmath482 is expected to be closer to @xmath480 than @xmath128 is , hence it is useful for evaluating the performances of the stochastic em algorithms we present . from figure",
    "[ fig : saem_estimates_for_mtt ] , we can see that almost all mle estimates obtained using smc - em and mcmc - em converge to values around @xmath482 , except for @xmath483 from smc - em has not converged within the experiment running time .",
    "the histogram of the bayesian mcmc samples in fig [ fig : saem_estimates_for_mtt ] indicate that the modes of the posterior probabilities obtained using mcmc are around @xmath482 as well .",
    "the computational complexity of one mcmc move for updating @xmath142 , for a fixed parameter @xmath117 , is dominated by a term which is @xmath484 , where @xmath485 is the average number of targets per time . on the other hand ,",
    "the cost of the e - step of smc - em is dominated by a term which is @xmath486 , where @xmath487 and @xmath0 is the parameter used in @xmath0-best assignment .",
    "( for a more detailed computational analysis for smc based em algorithms see appendix [ sec : computational complexity of smc based em algorithms ] . ) in realistic scenarios , one expects the smc e - step , being power three in the number of targets and clutter , to be far more costly then the mcmc e - step , which results in the smc - em algorithm being far slower , as in our example .",
    "we observed , but not shown in figure [ fig : saem_estimates_for_mtt ] , that the @xmath117 samples of the mcmc bayesian estimate reached the true values after approximately @xmath488 iterations , earlier than mcmc - em s @xmath489 iterations .",
    "this is because mcmc - em forgets its past more slowly than mcmc bayesian due to dependance induced by the stochastic approximation step .",
    "although in this case mcmc bayesian seems preferable , we need to be careful when choosing the prior distribution for @xmath117 especially when data is scarce as it may unduly influence the results .",
    "the reason why smc - em is comparatively slow to converge is because of the costly smc e - step .",
    "often , the parameters can be updated without a complete browse through all the data .",
    "we may thus speed up convergence by applying smc online em ( algorithm [ alg : online em for the mtt model ] ) on the following sequence of concatenated data @xmath490,\\ ] ] figure [ fig : online vs batch smc - em ] shows both our previous smc - em estimates ( vs number of iterations ) in figure [ fig : saem_estimates_for_mtt ] and the smc online em estimates ( vs number of passes over the original data @xmath125 ) on the concatenated data ; and we note that both algorithms are started with the same initial estimate of @xmath128 . noting that the computational cost of one iteration of the smc - em algorithm and the computational cost of one pass of smc online em algorithm over the data are roughly the same , we observe that @xmath483 and the other parameters converge much quicker in this way .",
    "the caveat though is that there is now a bias introduced due to the discontinuity at the concatenation points , e.g. @xmath491 may correspond to the observations of many surviving targets whereas @xmath492 may be the observations of an initially target free surveillance region .",
    "this discontinuity will effect , especially , survival @xmath493 , detection @xmath494 , and any other parameter depending crucially on a correct @xmath495 estimate over time .",
    "however it will have little effect on the parameters @xmath496 which govern the dynamics of the hmm associated with a target . in conclusion , one way to estimate @xmath128 in a batch setting using smc - em is by ( i ) first running smc online em on @xmath497 $ ] until convergence to get an estimator @xmath498 of @xmath128 , ( ii ) and then run the batch smc - em initialised at @xmath498 .      in the second experiment",
    "we compare the batch estimation algorithms , mcmc - em and the bayesian method , with a larger data set which has more targets and observations .",
    "recall that the smc - em algorithm is based on a smc algorithm which uses the @xmath0-best linear assignments and its computational complexity is approximately polynomial of order @xmath499 in @xmath500 .",
    "therefore , the smc - em algorithm would take a long time to execute and is left out of the comparison in this experiment .",
    "we created a data set of @xmath501 time steps by using the parameter @xmath502 with window size @xmath503 for the surveillance region . with this choice ,",
    "we see approximately @xmath504 targets per time .",
    "figure [ fig : saem_estimates_for_mtt2 ] shows the results obtained from the mcmc - em and the bayesian method for estimating @xmath128 . when the true association is given , the em algorithm finds @xmath482 for this data set as @xmath505 we can see that both methods work well for this large data set .",
    "it is worth mentioning that mcmc bayesian converged to the stationary distribution after @xmath506 iterations ( not shown in the figure ) , while mcmc - em converged after @xmath507 iterations .",
    "we demonstrate the performance of the smc online em in algorithm [ alg : online em for the mtt model ] in two settings .      in the first experiment for online estimation ,",
    "we create a scenario where there are a constant but unknown number of targets that never die and travel in the surveillance region for a long time .",
    "that is , @xmath508 ( which is unknown and to be estimated ) , @xmath509 and @xmath510 .",
    "we also slightly modify our mtt model so that the target state is a stationary process .",
    "the modified model assumes that the state transition matrix @xmath172 is @xmath511 and @xmath512 and @xmath513 are the same as the mtt model in example [ ex : the constant velocity model ] .",
    "the change is to the diagonals of matrix @xmath172 which should be @xmath514 for a constant velocity model .",
    "however , @xmath515 will lead to non - divergent targets , i.e. having a stationary distribution ; see figure [ fig : path_of_a_target_in_the_online_setting ] for a sample trajectory .",
    "we create data of length @xmath516 with @xmath517 targets which are initiated by using @xmath518 .",
    "the other parameters to create the data are @xmath519 , and the window size @xmath459 .",
    "figure [ fig : online_em_for_mtt_fixed_num_of_targets ] shows the estimates for parameters @xmath520 using the smc online em algorithm described in algorithm [ alg : online em for the mtt model ] , when @xmath521 is known .",
    "we used @xmath464 and @xmath522 , and @xmath523 is taken for all of the parameters except @xmath468 , where we used @xmath524 .",
    "the burn - in time , until when the m - step is not executed , is @xmath525 .",
    "we can observe the estimates for the parameters quickly settle around the true values .",
    "note that @xmath526 are not estimated here because they are the parameters of the initial distribution of targets which have no effect on the stationary distribution of a mtt model with fixed number of targets , and thus they are not identifiable by an online em algorithm @xcite .",
    "note that the online mle procedure is based on the fact that the parameters of the initial distribution will have a negligible effect on the likelihood of observations @xmath219 for large @xmath19 . in practice ,",
    "the parameters of the initial distribution can be estimated by running a batch em algorithm for the sequence of the first few observations , such as @xmath527 , and fixing all other parameters to the values obtained by smc online em .    the particle filter in algorithm [ alg : online em for the mtt model ] , which we used to produce the results in figure [ alg : online em for the mtt model ] , has all its particles having the same number of targets , which is the true @xmath528 .",
    "however , @xmath528 can be estimated by running several smc online em algorithms with different possible @xmath528 s , and comparing the estimated likelihoods @xmath529 versus @xmath19 .",
    "figure [ fig : likelihoods for selection of number of targets ] shows how the estimates of @xmath529 for values @xmath530 compare with time .",
    "both the left and right figures suggest that @xmath529 favours @xmath517 starting from @xmath531 and the decision on the number of targets can be safely made after about @xmath532 time steps .",
    "we have also checked this comparison with different initial values for @xmath117 and found out that the comparison is robust to the initial estimate @xmath533 .      in the second experiment with online estimation , we consider the constant velocity model in example [ ex : the constant velocity model ] with a time - varying number of targets , i.e. @xmath534 and @xmath535 .",
    "we generated a set of data of length @xmath536 using parameters @xmath537 and we estimated all of them ( except @xmath456 ) .",
    "again , we used @xmath464 and @xmath463 , and @xmath523 is taken for all of the parameters except @xmath468 for which we used @xmath524 .",
    "the online estimates for those parameters are given in figure [ fig : smc online em vs online em with true data association ] ( solid lines ) .",
    "the initial values are taken to be @xmath538 which is not shown in the figure in order to zoom in around @xmath128 .",
    "we observe that the estimates have quickly left their initial values and settle around @xmath128 . also , the parameter estimates for the initial distribution of newborn targets have the largest oscillations around their true values which is in agreement with the results in the batch setting .",
    "another important observation from figure [ fig : smc online em vs online em with true data association ] is that there is bias in the estimates of some of the parameters , namely @xmath539 .",
    "this bias arises from the monte carlo approximation . to provide a clearer illustration of this monte carlo bias",
    ", we compared the smc online em estimates with the online em estimates we would have if we were given the true data association , i.e.  @xmath540 . the dashed lines in figure [ fig : smc online em vs online em with true data association ] show the results obtained when the true association is known ; for illustrative purposes we plot every @xmath541th estimate only , hence the sequence @xmath542 .",
    "the source of the bias in the results is undoubtedly due to the smc approximation of @xmath543 .",
    "however , we are able to pin down more precisely which components of @xmath142 are being poorly tracked .",
    "we ran the smc online em algorithm for the same data sequence , but this time by feeding the algorithm with the birth - death information , i.e.  @xmath544 .",
    "figure [ fig : smc online em vs online em with true data association ] shows that when @xmath544 is provided to the algorithm , the bias for some components drops .",
    "this indicates that ( i ) the bias in the mtt parameters is predominantly due to the poor tracking of the birth and death times by our smc mtt algorithm and ( ii ) with knowledge of the births and deaths , the unknown assignments of targets to observations seem to be adequately resolved by the @xmath0-best approach since the bias in the target hmm parameters diminishes .",
    "therefore , the bottle neck of the smc mtt algorithm is birth / death estimation and , generally speaking , a better smc scheme for the birth - death tracking may reduce the bias .",
    "note that when the number of births per time is limited by a finite integer , _ all _ the variables of @xmath177 i.e.  @xmath545 can be tracked within the @xmath0-best assignment framework , and we expect in this case the bias to be significantly smaller .",
    "however , since in our mtt model the number of births per time is unlimited ( being a poisson random variable ) , we can not include birth - death tracking in the @xmath0-best assignment framework ; see the smc algorithm in appendix [ sec : smc algorithm for mtt ] for details .",
    "it is expected that a reasonable accuracy of smc target tracker is necessary for good performance in parameter estimation .",
    "obviously , there is a trade off between accuracy of smc tracking and computational cost , and this trade off is a function of @xmath207 , the number of particles .",
    "this raises the following question : how do we identify if the number of particles is adequate for the smc online em algorithm for a real data set given that @xmath128 is unknown ?",
    "we propose a procedure to address this issue .",
    "for the chosen value @xmath207 :    1 .   run smc online em on the real data set with @xmath207 particles to obtain an estimate @xmath546 of the unknown @xmath128 .",
    "2 .   simulate the mtt model with @xmath546 for a small number of time steps to obtain a data set for verification .",
    "3 .   run the smc target tracker for the simulated data with @xmath547 known .",
    "if the target tracking accuracy is `` bad '' , increase @xmath207 and return to step 1 ; else stop .",
    "the tracking accuracy can roughly be measured by comparing @xmath495 with its particle estimate which is suggestive of the birth - death tracking performance , which we have identified to have a significant impact on the bias of the estimates as shown in figure [ fig : smc online em vs online em with true data association ] .",
    "we have presented mle algorithms for inferring the static parameters in linear gaussian mtt models . based on our comparisons of the offline and online em implementations , our recommendations to the practitioner",
    "are : ( i ) if batch estimation permissible for the application then it should always be preferred .",
    "( ii ) moreover , mcmc - em should be preferred as batch smc - em has the disadvantage of slow convergence of some parameters while online smc - em applied to concatenated data , although converges quicker then batch mcmc - em , induces some bias for certain parameters due to the discontinuity caused at the concatenation boundaries .",
    "furthermore , smc tracker does not scale well with the average number of targets per time and clutter rate ; see sec calculation in [ sec : batch setting ] .",
    "( iii ) for very long data sets ( i.e. large time ) and when there is a computational budget , then online smc - em seems the most appropriate since the it is easier to control computational demands by restricting the number of particles .",
    "we have seen that in online smc - em there will be biases in some of the parameter estimates if the birth and death times are not tracked accurately .",
    "the particle number should be verified for adequacy as recommended in section [ sec : choice of n ] .",
    "we have not considered other tracking algorithms that work well such as those based on the phd filter @xcite which could be used provided track estimates can be extracted .",
    "the linear gaussian mtt model can be extended in the following manner while still admitting an em implementation of mle .",
    "for example , split - merge scenarios for targets can be considered . moreover , the number of newborn targets per time and false measurements need not be poisson random variables ; for example the model may allow no births or at most one birth at a time determined by a bernoulli random variable .",
    "furthermore , false measurements need not be uniform , e.g. their distribution may be a gaussian ( or a gaussian mixture ) distribution . also , we assumed that targets are born close to the centre of the surveillance region ; however , different types of initiation for targets may be preferable in some applications .    for non - linear non - gaussian mtt models ,",
    "monte carlo type batch and online em algorithms may still be applied by sampling from the hidden states @xmath38 s provided that the sufficient statistics for the em are available in the required additive form @xcite . in those mtt models where sufficient statistics for em are not available , other methods such as gradient based mle methods can be useful ( e.g.  @xcite ) .        referring to the variables in section [ sec : online smoothing in a single glssm ] , the intermediate functions for the sufficient statistics in",
    "can be written as @xmath548 where @xmath549 for @xmath550 ; @xmath551 for @xmath552 ; and @xmath553 , @xmath554 for @xmath555 .",
    "all @xmath556 s , @xmath557 s and @xmath558 s are @xmath284 matrices , @xmath286 vectors and scalars , respectively .",
    "forward smoothing is then performed via recursions over these variables .",
    "start at time 1 with the initial conditions @xmath559 , @xmath560 , and @xmath561 for all @xmath151 except @xmath562 , @xmath563 , @xmath564 , and @xmath565 . at time @xmath566 , update @xmath567    for the online em algorithm , we simply modify the update rules by multiplying the terms on the right hand side containing @xmath568 or @xmath569 by @xmath570 and multiplying the rest of the terms by @xmath571 .",
    "an smc algorithm is mainly characterised by its proposal distribution . hence , in this section we present the proposal distribution @xmath572 , where we exclude the superscripts for particle numbers from the notation for simplicity .",
    "assume that @xmath573 is the ancestor of the particle of interest with weight @xmath574 .",
    "we sample @xmath575 and calculate its weight by performing the following steps :    * _ birth - death move : _ sample @xmath576 and @xmath577 for @xmath578 . set @xmath579 and construct the @xmath580 vector @xmath581 from @xmath582 . set @xmath305 and calculate the prediction moments for the state . for @xmath583 , * * if @xmath584 , set @xmath585 and @xmath586 . * * if @xmath587 , set @xmath588 and @xmath589 . + also , calculate the moments of the conditional observation likelihood : for @xmath583 , @xmath590 and @xmath591 . * _ detection and association _",
    "define the @xmath592 matrix @xmath593 as @xmath594 and an assignment is a _ one - to - one _ mapping @xmath595 .",
    "the cost of the assignment , up to an identical additive constant for each @xmath596 is @xmath597 find the set @xmath598 of @xmath0 assignments producing the highest assignment scores .",
    "the set @xmath599 can be found using the murty s assignment ranking algorithm @xcite .",
    "finally , sample @xmath600 with probability @xmath601}{\\sum_{j^{\\prime } = 1}^{l } \\exp[d(d_{t } , \\alpha_{t , j^{\\prime } } ) ] } , \\quad j = 1 , \\ldots , l\\ ] ] given @xmath596 , one can infer @xmath602 ( hence @xmath603 ) , @xmath604 , @xmath605 and the association @xmath310 as follows : @xmath606 then @xmath607 , @xmath608 , @xmath603 is constructed from @xmath602 , and finally @xmath609 * _ reweighting : _",
    "after we sample @xmath610 from @xmath611 , we calculate the weight of the particle as in , which becomes for this sampling scheme as @xmath612.\\ ] ]        for simplicity , assume the true parameter value is @xmath117 .",
    "the computational cost of smc filtering with @xmath117 and @xmath207 particles , at time @xmath19 , is @xmath613 \\end{aligned}\\ ] ] where @xmath614 to @xmath615 are constants and @xmath616 is for sampling from the poisson distribution .",
    "if we assume that smc tracks the number of births and deaths well on average then we can simplify the term above @xmath617 \\end{aligned}\\ ] ] where @xmath618 .",
    "the process @xmath619 is markov and its stationary distribution is @xmath620 where @xmath621 .",
    "also @xmath622 and for simplicity we write @xmath623 . therefore the stationary distribution for @xmath624 is approximately that of @xmath625 which is @xmath626 where @xmath627 .",
    "therefore , assuming stationarity at time @xmath19 and substituting @xmath628 , the expected cost will be @xmath629   & \\approx n \\big",
    "[ c_{1 , 3 } + \\left ( c_{2 } + d_{x}^{3 }   \\left [ c_{4 } + c_{5 } \\left ( p_{d } + \\lambda_{f } \\right )   \\right ]   \\right ) \\lambda_{x }   \\nonumber \\\\ & \\quad\\quad\\quad + c_{5 } p_{d } \\lambda_{x}^{2 }   + c_{6 } l \\left ( \\lambda_{y}^{3 } + 3 \\lambda_{y}^{2 } + \\lambda_{y } \\right ) \\big].\\end{aligned}\\ ] ]      the smc - em algorithm for the batch setting first runs the smc filter , stores all its path trajectories i.e.  @xmath630 and then calculates the estimates of required sufficient statistics for each @xmath631 by using a forward filtering backward smoothing ( ffbs ) technique , which is bit quicker then forward smoothing . therefore , the overall expected cost of batch smc - em applied to data of size @xmath65 is @xmath632 where @xmath633 is the cost of the m - step , i.e.  @xmath157 .",
    "let us denote the total number of targets up to time @xmath65 is @xmath634 and let @xmath635 be their life lengths .",
    "the computational cost of ffbs to calculate the smoothed estimates of sufficient statistics for a target of life length @xmath0 is @xmath636 .",
    "therefore , @xmath637 assume the particle filter tracks well and @xmath638 and @xmath639 , @xmath640 for particles @xmath210 are close enough to @xmath641 , and @xmath634 , the true values , for @xmath642 .",
    "then , we have @xmath643 the expected values of @xmath641 and @xmath634 are @xmath644 , @xmath645 , respectively .",
    "also assume stationarity at all times so that the expectations of the terms @xmath646 are the same and we have @xmath647 \\approx c_{8 } n n d_{x}^{3 } \\lambda_{b}(1 - p_{s})^{-1}.\\ ] ] as a result , given a data set of @xmath65 time points , the overall expected cost of smc - em for the batch setting per iteration is @xmath648   \\approx \\mathbb{e}_{\\theta } \\left [ c_{\\text{ffbs}}(\\theta , n , n ) \\right ]   +   n \\mathbb{e}_{\\theta } \\left [ c_{\\text{smc}}(\\theta , t , n ) \\right ] + c_{7}. \\nonumber   % \\\\ % & \\quad\\quad\\quad = n n \\big\\ { c_{1 , 3 } + \\left ( c_{2 } + d_{x}^{3 } \\left [ c_{4 } + c_{8 } + c_{5 } ( p_{d } + \\lambda_{f } ) \\right ] \\right ) \\lambda_{x } \\nonumber \\\\ % & \\quad\\quad\\quad\\quad\\quad\\quad\\quad + c_{5 } d_{x}^{3 } p_{d } \\lambda_{x}^{2 } + c_{6 } l \\left ( \\lambda_{y}^{3 } + 3 \\lambda_{y}^{2 } + \\lambda_{y } \\right ) \\big\\ } + c_{8 } .\\end{aligned}\\ ] ]      the overall cost of an smc online em for a data set of @xmath65 time points is @xmath649.\\ ] ] the forward smoothing recursion and maximisation used in the smc online em requires @xmath650 calculations at time @xmath19 for a constant @xmath651 , whose expectation is @xmath652   = c_{9 } n \\lambda_{b}(1 - p_{s})^{-1 } d_{x}^{5}.\\ ] ] at stationarity .",
    "the overall expected cost of an smc online em for a data of @xmath65 time steps , assuming stationarity , is @xmath653 \\nonumber \\\\ & \\quad\\quad\\quad\\quad \\approx n \\left ( \\mathbb{e}_{\\theta } \\left [ c_{\\text{fsr}}(\\theta , t , n ) \\right ] + \\mathbb{e}_{\\theta } \\left [ c_{\\text{smc}}(\\theta , t , n ) \\right ] + c_{7 } \\right ) .",
    "\\nonumber % \\\\ % & = n \\left [ n \\left\\ { c_{1 , 3 } + \\left ( c_{2 } + d_{x}^{3 } \\left [ c_{4 } + c_{5 } ( p_{d } + \\lambda_{f } ) \\right ] + c_{9 } d_{x}^{5 } \\right ) \\lambda_{x } + c_{5 } d_{x}^{3 } p_{d } \\lambda_{x}^{2 } + c_{6 } l \\left ( \\lambda_{y}^{3 } + 3 \\lambda_{y}^{2 } + \\lambda_{y } \\right )   \\right\\ } + c_{7 } \\right].\\end{aligned}\\ ] ]"
  ],
  "abstract_text": [
    "<S> we present both offline and online maximum likelihood estimation ( mle ) techniques for inferring the static parameters of a multiple target tracking ( mtt ) model with linear gaussian dynamics . </S>",
    "<S> we present the batch and online versions of the expectation - maximisation ( em ) algorithm for short and long data sets respectively , and we show how monte carlo approximations of these methods can be implemented . performance is assessed in numerical examples using simulated data for various scenarios and a comparison with a bayesian estimation procedure is also provided . </S>"
  ]
}