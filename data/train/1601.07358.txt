{
  "article_text": [
    "if we consider the development of new technologies as a collective learning process , we can distinguish between different interlaced processes . while basic research focuses on exploration characterised by a search for potential alternatives to established methods , the more promising an approach appears",
    ", the more likely it becomes subject to subsequent exploitation , where it is optimised and matured with the ultimate hope to supersede what is available .",
    "an example of explorative activity are early efforts to solve problems by artificial intelligence ( ai ) , such as inventing unconventional heuristic techniques .",
    "ai has recently regained interest @xcite , which may be a consequence of new approaches to computation @xcite as well as improved capacities of classical computing and networking .",
    "the present work aims at drawing a connection between the recently suggested scheme of ps @xcite and quantum control theory @xcite , restricting attention to example problems analogous to those considered in the basic classical ps schemes @xcite , rather than a treatment of practically relevant but large - scale applications of , e.g. , machine learning .",
    "a discussion of scalability , quantum speed up , or practical implementability @xcite is beyond the scope of this work .",
    "we consider a class of schemes , where a quantum agent learns from cyclic interactions with an external environment via classical signals .",
    "the learning can be considered as an _ internal _ quantum navigation process of the agent s `` hardware '' or `` substrate '' that forms its memory of past experience . for notational convenience",
    ", we describe the memory operation as a unitary @xmath0 involving ( information carrying and other ) controllable and uncontrollable degrees of freedom ( such as a `` bath '' ) , where the latter are not necessarily identical with the environment , on which the agent operates .",
    "while conceptually , the memory may hence be seen as an open quantum system @xcite , the numerical examples considered in the present work restrict to closed system dynamics .",
    "this navigation of agent memory @xmath0 must be distinguished from the evolution of quantum states in which , following external or internal stimulus , the memory is excited @xcite .",
    "learning as an internal navigation process corresponds to the colloquial notion of a learner desiring to quickly `` make progress '' rather than `` marking time '' .",
    "for the agent s internal dynamics , we talk of a navigation process rather than a navigation problem that is to be solved , since ultimately , the agent responds to its environment that is generally unknown and subject to unpredictable changes .    while the proposed ps - model is characterised by an episodic & compositional memory ( ecm ) , we here ignore the clip network aspect and restrict attention to a parameter updating that is motivated from the basic scheme @xcite , which we apply to simple learning tasks involving an agent equipped with a quantum memory .",
    "we specifically reconsider some of the examples discussed in @xcite in order to investigate to what extent the results can be reproduced .",
    "in contrast to the classical scheme , where the parameters are weights in a clip network , we here refrain from ascribing a particular role , they could play ( e.g. , in a quantum walk picture mentioned in @xcite ) . here , the parameters are simply controls , although in our examples , they are defined as interaction strengths in a stack of layers constituting the agent memory @xmath0 .",
    "this choice of construction is however not essential for the main principle . from the viewpoint of the network - based classical ps , drawing a connection to quantum control theory",
    "opens the possibility to apply results obtained in the latter field over the last years @xcite . on the other hand ,",
    "classical ps is similar to rl @xcite , which considers a type of problems , where an `` agent '' ( embodied decision maker or `` controller '' ) learns from interaction with an environment ( controlled system or `` plant '' ) to achieve a goal . the learning consists in developing a ( generally stochastic ) rule , the agent s `` policy '' , of how to act depending on the situation it faces , with the goal to accumulate `` reward '' granted by the environment . in rl ,",
    "the environment is anything outside of control of this decision making .",
    "the reward could describe for example pleasure or pain felt by an individual .",
    "it is generated within the individual s body but is beyond it s control , and therefore considered originating in the _ agent s _ environment .",
    "historically , rl , which must be distinguished from supervised learning , originates from merging a trait in animal psychology with a trait in control theory .",
    "although dynamic programming as the basis of the latter is well understood , limited knowledge of the environment along with a vast number of conceivable situations , an rl - agent may face , render a direct solution impossible in practice .",
    "analogous to rl growing out of dynamic programming by refining the updates of values , in a quantum context , one could think of refining quantum control schemes with algorithmic elements that enhance their resource efficiency .",
    "another aspect is embodiment .",
    "a historical example is application - specific classical optical computing with a 4f - optical correlator .",
    "a more recent effort is neuromorphic computing , which aims at a very - large - scale integration ( vlsi)-based physical implementation of neural networks , whose simulation with a conventional computer architecture is inefficient .",
    "this becomes even more crucial for quantum systems , which may be implemented as superconducting solid state devices , trapped ions or atoms , or wave guide - confined optical fields .",
    "given the availability of a controllable quantum system , it is hence tempting to transform quantum state - encoded sensory input and select actions based on measurement outcomes .",
    "while the parameter update is typically done by some standard linear temporal difference ( td)-rule , the selection of actions is in classical algorithms governed by a separate stochastic rule that tries to balance exploration and exploitation .",
    "this stochastic rule is described in terms of a policy function , that determines , how the probabilities for choosing the respective actions depend on the value functions in rl , edge strengths in ps , or controls in direct policy approaches .",
    "examples are the @xmath1-greedy and the softmax - rule .",
    "the quantum measurement here serves as a direct physical realisation of an action - selection , whose uncertainty allows to incorporate both exploration and exploitation @xcite . in our context , the resulting measurement - based ( and hence effectively quadratic ) policy forms an intermediate between the linear stochastic function used in @xcite and the exponential softmax - function applied in @xcite .",
    "a measurement - based policy can moreover be tailored on demand by the way in which classical input is encoded as a quantum state .",
    "one could , e.g. , apply mixtures of a pure state and a maximally mixed state to mimic an @xmath1-greedy policy function , or one could use thermal input states to mimic an exponential function .",
    "in contrast to the value function - based rl , our approach amounts to a direct policy search , where the agent - environment interaction employs a general state preparation @xmath2 transformation @xmath2 measurement scheme , that reflects the kinematic structure of quantum mechanics .      consider the specific task of mapping input states @xmath3 by means of a controllable unitary @xmath0 to outputs @xmath4 . under the ( restrictive ) assumption , that for each input there is exactly one correct output , the task is to learn this output from interaction with an environment . in our context ,",
    "the @xmath3 ( @xmath4 ) are regarded as encoded percepts ( actions ) , while @xmath0 acts as memory of the learned information and can finally accomplish the mapping as an embodied `` ad hoc '' computer or an `` oracle '' , which is similar to learning an unknown unitary @xcite .",
    "consider ( i ) the case where there is only one possible input state @xmath5 . if the task is the navigation of the output state @xmath6 @xmath7 @xmath8 by means of @xmath0 to a desired destination state @xmath9 , a learning agent has to realize the maximisation of the conditional probability @xmath10 @xmath7 @xmath11 by tuning @xmath0 .",
    "the intuition behind this is that @xmath12 is bounded and if @xmath13 depends analytically on some control vector @xmath14 , the gradient with respect to @xmath14 must vanish at the maximum of @xmath12 . to give a simple example",
    ", we assume that @xmath15 depends ( rather than on @xmath14 ) on a single real parameter @xmath16 in a continuous and differentiable way such that it obeys the schrdinger equation @xmath17 @xmath7 @xmath18 with the state boundary conditions",
    "@xmath19 @xmath7 @xmath20 and @xmath21 @xmath7 @xmath22 .",
    "this gives @xmath23 so that indeed @xmath24 any algorithm that results in a @xmath0 such that @xmath12 approaches 1 accomplishes this task .",
    "assume now that ( ii ) we are required to transform a given orthonormal basis ( onb ) @xmath25 into another given onb @xmath26 of a vector space of same dimension , but we are not told which state is to be transformed into which other state .",
    "we could build a quantum device that implements some unitary @xmath27 such that @xmath4 @xmath7 @xmath28 . preparing the system in state @xmath3 and measuring in the second basis",
    "gives outcome @xmath4 .",
    "one may consider the problem as a ( trivial ) learning task , namely that of an identical mapping of the state - indices @xmath29 .",
    "however , if we do not know from the beginning what kind of mapping the solution is , we have to learn it . in our quantum device",
    ", we would tune @xmath0 until it gives the desired measurement statistics .",
    "inspired by @xcite , we call this task `` invasion game '' . to solve it , we initialize the device in states @xmath3 chosen randomly from the given onb , while the measurement is done in the second onb formed by the @xmath4 .",
    "the algorithm will drive @xmath0 to some unitary @xmath30 , where @xmath31 ( @xmath32 ) are undetermined unitaries which are diagonal in the basis @xmath25 ( @xmath26 ) .",
    "if ( iii ) the percept states are random , this phase freedom is removed up to a global phase . in the simplest case , we draw the initial states of the device from an `` overcomplete '' basis , where the set of all possible states is linearly dependent . for a @xmath33-level system , this can be accomplished by ( randomly ) choosing @xmath33 su(@xmath33)-unitaries . during each state initialisation",
    ", we then take one @xmath34 from this set , a random @xmath3 from our first onb , and then prepare the device in a state @xmath35 .",
    "consequently , the measurement is done in a transformed basis formed by the @xmath36 rather than the @xmath4 themselves .    in this sense ,",
    "the navigation of ( i ) a given input state , ( ii ) a given onb , and ( iii ) random states can be described as a navigation of unitaries @xmath0 with a varying amount of freedom . while formally , all three cases ( i)-(iii ) can be considered as special cases of a navigation of @xmath13 to a point ( [ task ] ) , where a percept statistics - based fidelity ( [ f2 ] ) becomes maximum , practically they can be accomplished in rl by means of the mentioned reward signal , independently of the availability of analytic solutions . in what follows , we consider @xmath0 as a memory of an rl - agent , that solves tasks arising from its interaction with an environment .",
    "the scheme is depicted in fig .",
    "[ fig1 ] .     agent - environment interaction as a feedback scheme .",
    "the @xmath37 are percepts , which initialise the agent s memory in a quantum state @xmath38 .",
    "choice of an action @xmath39 is made by a measurement process as described by a given povm @xmath40 .",
    "depending on the rewards @xmath41 given by the environment , the memory is updated at the end of each cycle @xmath16 .",
    "the memory can also be modified by internal loops based on a numerical objective no ( dotted line ) or measurements ( dash - dotted line ) . , width=226 ]    the agent is equipped with some quantum channel that acts as its memory whose properties can be modified by control parameters denoted by a vector @xmath14 .",
    "examples of memory structures are listed in fig .",
    "[ fig2 ] .",
    "examples for the agent s memory as shown in fig .",
    "( a ) unitary evolution of the percept states , ( b ) open system evolution due to interaction with a bath b , ( c ) composite memory with coupled subsystems for percept ( s ) and action ( a ) variables , ( d ) extends ( c ) to an open system evolution analogous to ( b ) extending ( a ) .",
    "further ancilla systems may be added ( not shown ) , to account for , e.g. , emotion degrees of freedom introduced in @xcite .",
    ", width=226 ]    in fig .",
    "[ fig2 ] and in what follows , we refer to the memory operation by means of some unitary @xmath0 for notational simplicity .",
    "since any quantum process can be treated as unitary on an enlarged space , this is not a conceptual restriction .",
    "the agent interacts with an external environment in discrete cycles @xmath16 . at the beginning of a cycle , the agent receives ( via sensors ) some percept @xmath37 , which it encodes as a quantum state @xmath38 , in which its memory is prepared .",
    "after transformation of @xmath38 by the memory channel , a quantum measurement is performed , where we assume for simplicity that the positive operator valued measure ( povm ) @xmath42 describing this measurement is fixed .",
    "depending on the outcome of this measurement , an action @xmath39 is selected and performed on the environment ( via actuators ) , which completes the cycle .",
    "the environment reacts with a new percept and a reward @xmath41 , which are perceived by the agent during the following cycle .",
    "depending on the reward , some adjustments are made on the control parameters , which modify the properties of the memory channel ( i.e. , its `` hardware '' ) .",
    "this feedback loop is adapted from the classical schemes in @xcite and @xcite , where the percepts @xmath37 in fig .",
    "[ fig1 ] correspond to the states in @xcite .",
    "the agent s interaction with the environment is here considered classical in the sense that percepts , actions and rewards are classical signals .",
    "the environment itself is not specified , it could represent , e.g. , an experiment performed on a quantum system . note that the environment in fig .",
    "[ fig1 ] is not to be confused with the bath in fig .",
    "[ fig2 ] , which affects the memory channel but is not considered part of the agents `` habitat '' .",
    "in addition to the external loop , we may also equip the agent with two types of internal feedback loops , which allow the agent to undertake what corresponds to `` planning steps '' in rl and `` reflection '' in ps .",
    "one type is similar to the external loop in that it involves state initialisations and measurements on the memory channel , but exploits that percepts , actions and rewards can be recorded and reproduced as a consequence of their classicality .",
    "the second type of internal loop does not involve state evolutions but requires some mathematical model of the memory channel itself , which is used to directly calculate a numerical objective ( no ) , whose value is used to alter the control parameters . fig .",
    "[ fig1 ] does not imply that all of these loops need to be simultaneously present , they are rather thought of either subprocesses within an overall agent scheme or possible modes of its operation .",
    "the numerical examples in this work will exclusively apply the external loop .",
    "all three loops involve a parameter update @xmath43 . in a `` first - order '' update ,",
    "@xmath43 is proportional to some quantity that depends on the gradient @xmath44 of @xmath0 with respect to @xmath14 .",
    "this gradient can either be computed directly from a memory model @xmath13 ( i.e. , from some symbolic expression of @xmath44 if available ) or estimated from measurements .",
    "these `` measurements '' can be physical ( povm in fig .",
    "[ fig1 ] ) or numerical ( no in fig .  [ fig1 ] ) . for the estimation ,",
    "one varies the components of @xmath14 by a small amount and records the changes in the measured povm or computed no . here",
    "are some elementary examples : _ ( 1a ) _ a simulation of an external loop with a given model - based ( i.e. analytic ) @xmath44 is performed in sec .",
    "[ sec : ig22 ] ( fig .",
    "[ fig5 ] ) for the case fig .",
    "[ fig2](c ) , in sec .",
    "[ sec : ig44 ] ( figs .",
    "[ fig7]-[fig8 ] ) for the case fig .",
    "[ fig2](a ) , and in sec .",
    "[ sec : gw ] ( figs .",
    "[ fig11 ] and [ fig12 ] ) for the case fig .",
    "[ fig2](c ) . _",
    "( 1b ) _ a simulation of an external loop with a povm measurement - based @xmath44 is carried out in @xcite ( fig .",
    "6 ) for the case fig .",
    "[ fig2](b ) . _",
    "( 2 ) _ a no - based internal loop with a model - based @xmath44 is considered in @xcite for the case fig .",
    "[ fig2](b ) and in @xcite ( figs.2 - 4 ) for the case fig .",
    "[ fig2](a ) . _",
    "( 3 ) _ the povm - based internal loop in fig .",
    "[ fig1 ] can be used to estimate @xmath44 in the absence of a model @xmath13 of the agent memory . to this end ,",
    "one of the agent s possibilities consists in inserting a number of internal cycles between each external cycle , where it repeatedly prepares its memory in the latest percept state and observes how a variation @xmath43 affects the measurement statistics .",
    "a discussion of this will be given in sec .",
    "[ sec6 ] . beyond these examples ,",
    "all three loops can be interlaced with each other in various ways , analogous to the wealth of approaches reviewed in @xcite .",
    "for the cycle - wise update of the control parameters @xmath14 of the memory channel @xmath0 , we apply a rule @xmath45 inspired by the basic model of ps @xcite .",
    "the number of components @xmath46 can range from one ( @xmath14 scalar ) to infinity ( @xmath14 may represent a function or a vector of functions ) , and the @xmath46 can be assumed to be real - valued without loss of generality . in @xcite , the components of @xmath14 are the edge strengths of a directed graph representing a network of clips ( the graph s vertices ) . while these clips are considered sequences of remembered percepts and actions , the network itself abstracts from the clip s internal contents .",
    "our view of @xmath14 as a control vector is one further simplification and generalization that may allow for but does not require the view of the memory as a network .    in ( [ ur3 ] ) ,",
    "@xmath14 and @xmath47 are the control vectors before and after the update at cycle @xmath16 , respectively .",
    "@xmath48 @xmath49 @xmath50 is a ( typically small ) learning rate , and @xmath41 is the reward given at cycle @xmath16 .",
    "@xmath51 @xmath52 @xmath53 $ ] is a relaxation rate towards some equilibrium value @xmath54 in the absence of rewards .",
    "this allows for what corresponds to the `` forgetting '' process suggested in @xcite to account for dissipation in an embodied implementation and deal with time - dependent environments .",
    "a natural possibility is to identify the value @xmath55 , with which the memory is initialised before the first cycle , with @xmath54 .",
    "this could be the zero vector @xmath55 @xmath7 @xmath56 yielding , e.g. , the identity , @xmath57 @xmath7 @xmath58 @xmath7 @xmath59 .",
    "the learning process will then be a reward - driven and generally stochastic navigation in parameter space @xmath60 away from the zero vector @xmath61 .",
    "lifted to @xmath13 , this navigation starts at the identity @xmath57",
    "@xmath7 @xmath59 , that relaxes back to it in the prolonged absence of rewards . in this work",
    ", we consider static environments as in @xcite , and hence always set @xmath51 @xmath7 @xmath50 .",
    "@xmath62 is a difference vector . while some options for finite difference choices of @xmath63 are outlined in sec .",
    "[ sec6 ] , in all numerical examples within this work we restrict to the case , where @xmath62 @xmath7 @xmath64 is a short - hand notation for the gradient @xmath65    = \\bigl\\langle\\hat{u}^\\dagger\\hat{\\pi}({a})\\hat{u}\\bigr\\rangle,\\end{aligned}\\ ] ] with components @xmath66 at cycle @xmath16 .",
    "@xmath67 is the probability of the obtained measurement outcome @xmath68 under the condition of the respective cycle s percept state @xmath38 , where @xmath69 @xmath70 @xmath71 $ ] denotes the expectation value with respect to this state , and @xmath72 is the member of the povm that corresponds to measurement outcome @xmath39 .",
    "the latter determines the action performed by the agent , and we use the same symbol for both . @xmath73 @xmath74 @xmath75 describes a backward - discount rate , which we have defined via a parameter @xmath76 @xmath52 @xmath53 $ ] to allow comparison with the glow mechanism introduced in @xcite . as mentioned above , the unitary transformation of the respective percept states @xmath77 by the memory @xmath0",
    "@xmath7 @xmath78 at cycle @xmath16 in ( [ grada ] ) refers in general to a larger ( dilated ) space .",
    "the dynamical semigroup of cpt maps proposed in @xcite is included and can be recovered by referring to fig .",
    "[ fig2](d ) [ or alternatively fig .  [",
    "fig2](b ) ] and the assumption that @xmath79&=&\\mathrm{e}^{\\mathcal{l}_{\\mathrm{sa } }    \\delta{t}^{(\\mathrm{mem})}_t}\\hat{\\varrho}_{\\mathrm{sa}}({s}_t),\\end{aligned}\\ ] ] where the physical memory evolution time @xmath80 may depend on the cycle @xmath16 for a chosen parametrisation @xmath13 and must be distinguished from the agent response time that can additionally be affected by the potential involvement of internal loops in fig .  [ fig1 ] .",
    "the superoperator @xmath81 @xmath7 @xmath82 , whose effect on @xmath6 @xmath7 @xmath83 is defined as a sum @xmath84+l\\hat{\\varrho},\\ ] ] generates in @xcite a quantum walk and is given by a hamiltonian @xmath85 @xmath7 @xmath86 @xmath87 @xmath88 @xmath87 @xmath89 and a lindbladian @xmath90 @xmath7 @xmath91 @xmath74 @xmath92 , with @xmath93 @xmath7 @xmath94 performing transitions between clip states @xmath95 @xmath52 @xmath96 along a graph @xmath97 @xmath7 @xmath98 consisting of a set @xmath99 of vertices and a set @xmath100 of edges . since on the one hand , we here do not intend to necessarily represent @xmath0 by a clip network , and on the other hand do not want to exclude from the outset situations involving time - dependent or non - markovian bath effects @xcite , we use the dilated @xmath0 for simplicity instead .",
    "the set of all probabilities @xmath67 in ( [ grada ] ) , i.e , the whole conditional distribution then defines the agent s policy .    a reward given by the environment at time",
    "@xmath16 raises the question of the extent to which decisions made by the agent in the past have contributed to this respective reward .",
    "a heuristic method is to attribute all past decisions , but to a lesser degree the further the decision lies in the past .",
    "( considering the agent s life as a trajectory of subsequent percepts and actions , we could imagine the latest event trailing a decaying tail behind . ) a detailed description of this idea is presented in @xcite in form of the eligibility traces , which can be implemented as accumulating or replacing traces . in the context of ps , a similar idea has been introduced as glow mechanism that can be implemented as edge or clip glow @xcite . in our context ( [ ur3 ] ) , we realise it by updating the control vector by a backward - discounted sum of gradients @xmath101 referring to percepts and actions involved in cycles that happened @xmath102 steps in the past . a sole update by the present gradient is included as limit @xmath76 @xmath7 @xmath103 , for which ( [ ur3 ] ) reduces to @xmath47 @xmath7 @xmath104 .",
    "this special case is sufficient for the invasion game , which we will consider in sec .",
    "[ sec4 ] , because at each cycle , the environment provides a feedback on the correctness of the agent s decision by means of a non - zero reward .",
    "after that , we apply the general update ( [ ur3 ] ) to a grid world task , where the agent s goal can not be achieved by a single action , and where the long term consequences of its individual decisions can not be foreseen by the agent .",
    "in this section , we ignore the embodied implementation of our method as a quantum agent and briefly summarise and compare the update rules of the methods considered from a computational point of view .",
    "it should be stressed that rl is an umbrella term for problems that can be described as agent - environment interactions characterised by percepts / states , actions , and rewards .",
    "hence _ all _ methods considered here are approaches to rl - problems . for notational convenience however , we here denote the standard value function - based methods as `` rl '' in a closer sense , keeping in mind that alternatives such as direct policy search deal with the same type of problem .",
    "the standard rl - methods successively approximate for each state or state action pair the expected return @xmath105 @xmath7 @xmath106 , i.e. , a sum of future rewards @xmath41 , forward - discounted by a discount rate @xmath107 $ ] , that the agent is trying to maximise by policy learning .",
    "corrections to the current estimates can be done by shifting them a bit towards actual rewards observed during an arbitrarily given number @xmath33 of future cycles , giving rise to `` corrected @xmath33-step truncated returns '' @xmath108 @xmath7 @xmath109 @xmath87 @xmath110 @xmath87 @xmath111 @xmath87 @xmath112 @xmath87 @xmath113 , where @xmath99 is the value function of the respective future state @xmath114 ( analogous considerations hold for state action pairs ) .",
    "a weighted average of these gives the @xmath115-return @xmath116 @xmath7 @xmath117 , where @xmath118 $ ] is a parameter . in an equivalent `` mechanistic '' backward view , this gives rise to so - called eligibility traces .",
    "since the glow mechanism of ps is closely related to this , we base our comparison on the @xmath115-extension of one - step rl .",
    "@xmath115 @xmath7 @xmath50 describes the limit of shallow sample backups of single - step learning , whereas the other limit @xmath115 @xmath7 @xmath103 refers to the deep backups of monte carlo sampling , cf .",
    "fig . 10.1 in @xcite .",
    "it would be a futile task to try a mapping of the numerous variations , extensions , or combinations with other approaches that have been discussed or are currently developed for the methods mentioned , such as actor - critic methods or planning in rl , or emotion , reflection , composition , generalization , or meta - learning in ps . in particular , our notion of basic ps implies a restriction to clips of length @xmath119 @xmath7 @xmath103 , which reduces the edge strengths in the ecm clip network fig . 2 in @xcite to values @xmath120 of state - action pairs .",
    "furthermore , in this section , we restrict attention to the basic versions that are sufficient to treat the numerical example problems discussed in this work .",
    "we may think of tasks such as grid world , where actions lead to state transitions , until a terminal state has been reached , which ends the respective episode , cf .",
    "[ sec4 ] .      in tabular rl",
    ", the updates are performed according to @xmath121 ,    \\\\",
    "u&\\leftarrow&u+\\alpha\\left[r+\\gamma{u}^\\prime-{u}\\right]{e }    \\nonumber\\\\ \\label{trl2 }    & & = ( 1-\\alpha{e})u+\\alpha{e}r+\\alpha\\gamma{e}{u}^\\prime ,    \\\\",
    "\\label{trl3 }    { e}&\\leftarrow&\\gamma\\lambda{e},\\end{aligned}\\ ] ] where @xmath122 @xmath7 @xmath123 is a state value function in td(@xmath115 ) , cf .",
    "fig . 7.7 in @xcite , whereas @xmath122 @xmath7 @xmath124 is an action value function in sarsa(@xmath115 ) , cf .",
    "fig . 7.11 in @xcite .",
    "@xmath125 @xmath7 @xmath126 [ or @xmath125 @xmath7 @xmath127 refers to the value of the subsequent state or state action pair .",
    "@xmath128 @xmath7 @xmath129 [ @xmath128 @xmath7 @xmath130 denote the eligibility trace in td(@xmath115 ) [ sarsa(@xmath115 ) ] .",
    "they can be updated by accumulating ( @xmath128 @xmath131 @xmath132 @xmath87 @xmath103 ) or replacing ( @xmath128 @xmath131 @xmath103 ) them in ( [ trl1 ] ) ( ignoring other options such as clearing traces @xcite ) .",
    "@xmath48 is a learning rate , @xmath41 is the reward , and @xmath133 the discount rate .",
    "note that there are alternatives to ( [ trl1])-([trl3 ] ) .",
    "one of them is q - learning , which can be derived from sarsa = sarsa(@xmath134 ) by updating @xmath135 off - policy , which simplifies a mathematical analysis . since a q(@xmath115)-extension of q - learning is less straightforward , and there are convergence issues with respect to the gradient - ascent form discussed below ( cf .",
    "sec . 8.5 in @xcite ) , while the methods discussed here update on - policy , we restrict attention to ( [ trl1])-([trl3 ] ) .",
    "tabular rl is a special case of gradient - ascent rl , where @xmath136 is in the latter defined as in ( [ trl1])-([trl3 ] ) , except that it is given by a number of parameters @xmath137 , which are combined to a vector @xmath138 .",
    "this parametrisation can be done arbitrarily . in the linear case",
    ", the parameters could be coefficients of , e.g. , some ( finite ) function expansion , where the functions represent `` features '' . hence @xmath122",
    "@xmath7 @xmath139 , and the components of the gradient @xmath140 are @xmath141 , giving rise to a vector @xmath142 of eligibility traces .",
    "the updates ( [ trl1])-([trl3 ] ) now generalize to @xmath143\\bm{e},\\end{aligned}\\ ] ] cf .",
    "sec . 8 in @xcite .",
    "while the eligibility traces are initialised with zero , the value functions ( by means of their parameters ) can be initialised arbitrarily in tabular and gradient - ascent rl .",
    "[ [ ps ] ] ps ~~    classical ps is a tabular model . by tabular we mean that the percepts and actions ( and ultimately also clips of length @xmath119 @xmath144 @xmath103 in the ecm ) form discrete ( i.e. , countable ) sets , with the consequence , that the edge strengths @xmath145 can be combined to a table ( matrix )",
    "let us hence write the updates as summarised in app .",
    "[ app : a1 ] in a form allowing comparison with ( [ trl1])-([trl3 ] ) : @xmath146 ,    \\\\    { h}&\\leftarrow&{h}+\\lambda{g}+\\gamma(h^{\\mathrm{eq}}-h )    \\nonumber\\\\ \\label{ps2 }    & & = ( 1-\\gamma){h}+\\lambda{g}+\\gamma{h}^{\\mathrm{eq } } ,    \\\\",
    "\\label{ps3 }    { g}&\\leftarrow&(1-\\eta){g}.\\end{aligned}\\ ] ] in ( [ ps1])-([ps3 ] ) , we intentionally adopted the notation of ps .",
    "the glow parameter @xmath147 in ( [ ps1])-([ps3 ] ) corresponds to a replacing trace @xmath128 in ( [ trl1])-([trl3 ] ) , with @xmath73 @xmath74 @xmath75 in ( [ ps3 ] ) corresponding to @xmath148 in ( [ trl3 ] ) , and @xmath115 in ( [ ps2 ] ) corresponds to the reward @xmath41 in ( [ trl2 ] ) .",
    "the discount rate @xmath133 in ( [ trl1])-([trl3 ] ) must not be confused with the dissipation or damping rate @xmath107 $ ] in ( [ ps2 ] ) . to avoid confusion",
    ", we denote the former by @xmath149 and the latter by @xmath150 for the remainder of this paragraph . if we disregard the absence of a learning rate in ( [ ps1])-([ps3 ] )",
    "[ we may set @xmath48 @xmath7 @xmath103 in ( [ trl1])-([trl3 ] ) ] , we can obtain ps from tabular sarsa(@xmath115 ) by replacing the action value function @xmath135 with the connection weight @xmath120 , and the update of @xmath145 corresponding to the r.h.s . of ( [ trl2 ] ) , @xmath151 with the update of @xmath145 given by the r.h.s . of ( [ ps2 ] ) , @xmath152 in ( [ trl1])-([trl3 ] ) , rl is equipped with forward- and backward - discounting mechanisms , as becomes apparent in the product @xmath153 in ( [ trl3 ] ) . disabling the accumulation of forward - discounted future rewards",
    "( that give rise in rl to the return mentioned above ) by setting @xmath149 @xmath7 @xmath50 reduces ( [ update1 ] ) to @xmath73 @xmath74 @xmath154 @xmath87 @xmath155 , while setting @xmath156 @xmath7 @xmath50 reduces ( [ update2 ] ) to @xmath73 @xmath74 @xmath157 @xmath87 @xmath155 .",
    "these expressions are very similar , except that in ps , the constant @xmath150 has taken the place of @xmath158 in rl , so that @xmath73 @xmath74 @xmath159 determines the range of backward - discounting in ( [ happ ] ) .",
    "since in ( [ happ ] ) , it is the respective past excitations ( glowing rewards ) @xmath160 @xmath7 @xmath161 , rather than the rewards @xmath162 themselves , which is summed up , damping and glow play a similar role . on the other hand ,",
    "the factor @xmath73 @xmath74 @xmath75 takes in ps the place of @xmath153 in ( [ trl3 ] ) , which becomes zero together with @xmath149 , as mentioned .      the update rule ( [ ur3 ] ) is implemented as @xmath163 which can be obtained from gradient - ascent sarsa(@xmath115 ) by replacing in ( [ grl1])-([grl2 ] ) the action value function @xmath135 with the conditional probability @xmath10 , renaming @xmath138 as @xmath14 , replacing @xmath148 in ( [ grl1 ] ) with @xmath73 @xmath74 @xmath75 , and replacing in ( [ grl2 ] ) the term @xmath164 @xmath74 @xmath165\\bm{e}$ ] with @xmath166 @xmath167 @xmath168 .",
    "the latter replacement is similar to the change from ( [ update1 ] ) to ( [ update2 ] ) , where @xmath51 in ( [ gm2 ] ) corresponds to @xmath133 in ( [ ps2 ] ) .",
    "analogous to the comments following ( [ update1 ] ) and ( [ update2 ] ) , in the case @xmath133",
    "@xmath7 @xmath50 , the update corresponding to the r.h.s . of ( [ grl2 ] ) becomes @xmath169 @xmath74 @xmath170 @xmath87 @xmath171 , whereas for @xmath54 @xmath7 @xmath50 , the r.h.s .",
    "of ( [ gm2 ] ) reduces to @xmath73 @xmath74 @xmath172 @xmath87 @xmath171 .",
    "similar to the tabular case , the constant damping rate @xmath51 has in our method taken the place of @xmath173 in gradient - ascent rl .",
    "our method ( [ gm1])-([gm2 ] ) replaces a value function with a conditional probability ( [ grada ] ) , whereas the edge strengths in ps remain value function - like quantities .",
    "while tabular rl can be recovered from gradient - ascent rl , one hence can not expect to recover the basic ps update rule ( [ ps1])-([ps3 ] ) as a special case of our scheme , despite replacements analogous to ( [ update1])-([update2 ] ) . to understand the difference , we restrict attention to an invasion game - like case as shown in fig .",
    "[ fig3 ] as the simplest example , cf .",
    "also sec .",
    "[ sec4 ] for details .",
    "transition strengths @xmath174 for an invasion game - like task with states ( which are here synonymous to percept - clips ) @xmath175 and action clips @xmath176 .",
    "the strengthening of a rewarded transition ( here @xmath177 ) is in an update ( [ gmcomp ] ) based on the gradient of @xmath178 accompanied by a weakening of the respective transitions to the remaining actions ( here @xmath179 and @xmath180 ) , which is absent in ps , cf .",
    "( [ pscomp ] ) .",
    ", width=264 ]    since here , each episode lasts one cycle , we disable both the eligibility trace / glow mechanism by setting @xmath76 @xmath7 @xmath103 in ( [ gm1])-([gm2 ] ) and ( [ ps1])-([ps3 ] ) . as shown in fig .",
    "[ fig3 ] , we are given a set of states @xmath181 , each of which allows one out of a set of actions @xmath182 .",
    "consider a cycle , where from state @xmath183 , an action @xmath184 is selected .",
    "if the transition probabilities @xmath185 are given by some policy function @xmath186 , then the components of the r.h.s .",
    "of ( [ gm1 ] ) read @xmath187 , \\ ] ] with which the components of the update ( [ gm2 ] ) become @xmath188 where we have renamed @xmath54 as @xmath189 .",
    "an observer ignorant of the transitions @xmath29 @xmath190 @xmath191 and the corresponding probabilities @xmath192 notices no change , @xmath193 in the special case @xmath194 @xmath7 @xmath195 , we can simplify ( [ eq23 ] ) to @xmath196 from ( [ gmcomp ] ) we see that in the gradient method , the strengthening of the @xmath197-edge is accompanied with a weakening of those edges @xmath174 connecting the respective state @xmath102 @xmath7 @xmath198 with different actions @xmath199",
    "@xmath200 @xmath191 . as a consequence ,",
    "the @xmath174 may become negative , even if @xmath194 @xmath7 @xmath195 and the rewards are non - negative .",
    "this weakening is absent in basic ps ( [ ps2 ] ) , where the corresponding update is independent of the policy function @xmath186 and given by @xmath201 hence , @xmath174",
    "@xmath49 @xmath50 as long as the rewards are non - negative . in any case ,",
    "choice of a non - negative policy function @xmath194 renders the methods independent of a need of positive edge strengths . [",
    "note that a similar problem occurs if the parameters in a memory consisting of alternating layers such as @xmath0 @xmath7 @xmath202 , cf .",
    "[ app : fl ] , refer to non - negative physical quantities @xmath203 . in @xcite ,",
    "this has been solved by using an exponential function such as @xmath204 @xmath7 @xmath205 for parametrisation in terms of the controls @xmath46 . in this work ,",
    "we identify the @xmath46 directly with the @xmath203 for simplicity , which , if the @xmath46 are to be interpreted as non - negative quantities , doubles the set of physically applied hamiltonians from @xmath206 to @xmath207 . ]",
    "the relations between the different methods are summarised in fig .",
    "[ fig4 ] .",
    "tabular rl ( [ trl1])-([trl3 ] ) is a special case of gradient - ascent rl ( [ grl1])-([grl2 ] ) . replacing updates based on values of subsequent states with updates based on a physical damping term yields basic ps ( [ ps1])-([ps3 ] ) and the method presented here ( [ gm1])-([gm2 ] ) , which however uses a conditional probability ( [ grada ] ) instead of a value function",
    ", hence the basic ps update rule can not be recovered from our approach , as explained in sec .",
    "[ sec : ve ] .",
    ", width=264 ]    if one considers the ecm as the core element of ps rather than a specific update rule , one could alternatively adopt , e.g. , the tabular sarsa(@xmath115)-update rule .",
    "the picture of a random walk in clip space does not contradict the general framework of rl - problems .",
    "one may understand the clips as the agent s states ( which must be distinguished from the percepts ) . the same holds for the gradient - ascent generalization , which , in physical terms , could be considered as `` continuous variable rl '' . on the one hand",
    ", we could equally well apply , e.g. , the gradient - ascent sarsa(@xmath115)-update instead of our rule . on the other hand , before trying to create algorithmic extensions such as those mentioned at the beginning of this section for tabular rl and ps , one should first investigate whether and how such extensions are accomplished in any existing gradient - ascent rl variants .",
    "in what follows , we consider a simple invasion game as treated in @xcite . an attacker randomly chooses one out of two possible symbols @xmath208 which signals the direction in which it intends to move .",
    "the chosen symbol may represent , e.g. , a head turn and is visible to the defender , whose task is to learn to move in the same direction , which is required to block the attacker .",
    "we approach this learning task as an external loop in fig .",
    "[ fig1 ] with a closed system ( i.e. , bath - less ) memory [ cases ( a ) and ( c ) in fig .",
    "[ fig2 ] ] , described within a 4-dimensional hilbert space .",
    "the control parameters are updated according to ( [ ur3 ] ) in the absence of relaxation ( @xmath51 @xmath7 @xmath50 ) and gradient glow ( @xmath76 @xmath7 @xmath103 ) .",
    "the update is done with an analytic @xmath44 as described in app .",
    "[ app : fl ] , where the memory consists of alternating layers , @xmath0 @xmath7 @xmath209 , with a given number of controls @xmath210 . at the beginning of the first cycle ,",
    "the memory is initialised as identity .",
    "for the two hamiltonians @xmath211 and @xmath212 , we distinguish ( i ) a general case , where @xmath211 and @xmath212 are two given ( randomly generated ) 4-rowed hamiltonians acting on the total hilbert space and ( ii ) a more specialised case , in which they have the form @xmath213 where @xmath214 , @xmath215 , @xmath216 , @xmath217 are four given ( randomly generated ) 2-rowed hamiltonians acting on the percept ( s ) and action ( a ) subsystems , respectively , with @xmath218 denoting the identity .",
    "the latter case ( ii ) refers to a physical implementation of fig .",
    "[ fig2](c ) as a bath - mediated interaction of the s and a subsystems that is obtained from the setup fig .",
    "[ fig2](d ) by eliminating the bath @xcite .",
    "it has been included here to demonstrate that this special structure as considered in @xcite may be applied in the present context , but this is not mandatory . while the hamiltonians have been chosen in both cases ( i ) and ( ii ) at random to avoid shifting focus towards a specific physical realization , in an experimental setup , the respective laboratory hamiltonians will take their place ( assuming that they generate universal gates in the sense of @xcite , which is almost surely the case for a random choice ) .",
    "we start with a basic version of the game with 2 possible percepts ( the two symbols shown by the attacker ) and 2 possible actions ( the two moves of the defender ) . for each percept",
    ", there is hence exactly one correct action , which is to be identified . the memory applied is shown in fig .",
    "[ fig2](c ) , and the different input states are @xmath219 where @xmath220 @xmath7 @xmath221 @xmath7 @xmath222 is given by the number of actions .",
    "@xmath5 and @xmath9 can both be one of the two orthonormal states @xmath223 or @xmath224 of the s and a subsystem , respectively .",
    "the povm consists of the elements @xmath225 choosing the correct ( wrong ) action [ i.e. @xmath39 @xmath7 @xmath226 ( @xmath39 @xmath200 @xmath226 ) in ( [ pom ] ) and ( [ is ] ) ] returns a reward of @xmath41 @xmath7 @xmath227 ( @xmath41 @xmath7 @xmath228 ) .",
    "[ fig5 ] shows the average reward @xmath229 received at each cycle , where the averaging is performed over an ensemble of @xmath230 independent agents .",
    "average reward @xmath229 as a function of the number of cycles for an invasion game ( 2 symbols , 2 moves ) , learning rate @xmath48 @xmath7 @xmath231 , with a reward of + 1 ( -1 ) for a correct ( false ) move , averaged over @xmath230 agents .",
    "the input states and povm are given by eqs .",
    "( [ is ] ) and ( [ pom ] ) , respectively . ( a )",
    "the 6 graphs from bottom to top correspond to 1 , 2 , 3 , 4 , 8 , and 16 controls , respectively , with @xmath232 @xmath7 @xmath103 in ( [ pcoh ] ) .",
    "( b ) 16 controls , where after @xmath233 cycles the meaning of the symbols is reversed .",
    "the 5 graphs from bottom to top correspond in eq .",
    "( [ pcoh ] ) to @xmath232= 0 , 0.25 , 0.5 , 0.75 , and 1 , respectively .",
    "( c ) applies 32 controls and @xmath232 @xmath7 @xmath103 , but refers to case ( ii ) described by ( [ ha ] ) and ( [ hb ] ) , whereas figs .",
    "[ fig5](a , b ) refer to case ( i ) . ,",
    "title=\"fig:\",width=158 ]   average reward @xmath229 as a function of the number of cycles for an invasion game ( 2 symbols , 2 moves ) , learning rate @xmath48 @xmath7 @xmath231 , with a reward of + 1 ( -1 ) for a correct ( false ) move , averaged over @xmath230 agents .",
    "the input states and povm are given by eqs .",
    "( [ is ] ) and ( [ pom ] ) , respectively .",
    "( a ) the 6 graphs from bottom to top correspond to 1 , 2 , 3 , 4 , 8 , and 16 controls , respectively , with @xmath232 @xmath7 @xmath103 in ( [ pcoh ] ) .",
    "( b ) 16 controls , where after @xmath233 cycles the meaning of the symbols is reversed .",
    "the 5 graphs from bottom to top correspond in eq .",
    "( [ pcoh ] ) to @xmath232= 0 , 0.25 , 0.5 , 0.75 , and 1 , respectively .",
    "( c ) applies 32 controls and @xmath232 @xmath7 @xmath103 , but refers to case ( ii ) described by ( [ ha ] ) and ( [ hb ] ) , whereas figs .",
    "[ fig5](a , b ) refer to case ( i ) . , title=\"fig:\",width=158 ]   average",
    "reward @xmath229 as a function of the number of cycles for an invasion game ( 2 symbols , 2 moves ) , learning rate @xmath48 @xmath7 @xmath231 , with a reward of + 1 ( -1 ) for a correct ( false ) move , averaged over @xmath230 agents .",
    "the input states and povm are given by eqs .",
    "( [ is ] ) and ( [ pom ] ) , respectively . ( a )",
    "the 6 graphs from bottom to top correspond to 1 , 2 , 3 , 4 , 8 , and 16 controls , respectively , with @xmath232 @xmath7 @xmath103 in ( [ pcoh ] ) .",
    "( b ) 16 controls , where after @xmath233 cycles the meaning of the symbols is reversed .",
    "the 5 graphs from bottom to top correspond in eq .",
    "( [ pcoh ] ) to @xmath232= 0 , 0.25 , 0.5 , 0.75 , and 1 , respectively .",
    "( c ) applies 32 controls and @xmath232 @xmath7 @xmath103 , but refers to case ( ii ) described by ( [ ha ] ) and ( [ hb ] ) , whereas figs .",
    "[ fig5](a , b ) refer to case ( i ) . , title=\"fig:\",width=158 ]    @xmath234 is hence an estimate of the defender s probability to block an attack . referring to pure states in ( [ pcoh ] ) , fig .",
    "[ fig5](a ) shows the increase of learning speed with the number of controls .",
    "significant learning progress begins only after some initial period of stagnation . from the viewpoint of control theory ,",
    "the identity , in which the memory is initialised , may lie on a `` near - flat ground '' ( valley ) , which must first be left before progress can be made @xcite .",
    "asymptotically , perfect blocking can be achieved once the memory becomes controllable , i.e. , if the number of controls equals ( or exceeds ) the number of group generators .",
    "[ fig5](b ) demonstrates the need of a pure input state @xmath235 in ( [ pcoh ] ) of the action subsystem a rather than an incoherent mixture .",
    "after the agent in fig .",
    "[ fig5](b ) had some time to adapt to the attacker , the meaning of the symbols is suddenly interchanged , and the agent must now learn to move in the opposite direction .",
    "this relearning differs from the preceding learning period in the absence of the mentioned initial stagnation phase , which supports the above hypothesis of the proposed valley , the agent has left during the initial learning .",
    "this plot is motivated by fig . 5 in @xcite describing classical ps .",
    "although the different behaviour in the classical case suggests that this is an effect specific to quantum control , the phenomenon , that a dynamically changing environment can facilitate learning in later stages appears to be more general @xcite . while figs .",
    "[ fig5](a , b ) refer to case ( i ) described before ( [ ha ] ) , fig .",
    "[ fig5](c ) refers to the restricted case ( ii ) , which appears to impede learning . in the simulations of the following sec .",
    "[ sec : ig44 ] , which all refer to case ( ii ) , this is resolved by applying a 10 times larger negative reward for each wrong action .",
    "this demonstrates the flexibility in approaching rl problems offered by the freedom to allocate rewards in a suitable way .",
    "we now consider a version with 4 percepts , referring to an attacker presenting each of its two symbols in two colors at random .",
    "since we want to keep the hilbert space dimension unchanged ( rather than doubling it by adding the color category ) for better comparison of the effect of the number of controls on the learning curve , we must apply a memory as shown in fig .",
    "[ fig2](a ) .",
    "the 4 percepts are encoded as tensor products of orthonormal projectors @xmath236 where @xmath237",
    "@xmath7 @xmath238 ( @xmath102 @xmath7 @xmath238 ) refers to the symbol ( color ) .",
    "the povm operators are the 4 projectors @xmath239 where @xmath27 is a given ( randomly generated ) 4-rowed target unitary acting on the total system .",
    "the memory in fig .",
    "[ fig2](a ) is hence still composed of two subsystems referring to the two percept categories ` symbol ' and ` color ' , but both subsystem s initial state depends on the respective percept , and both are measured afterwards .",
    "the differences between the setup discussed in the previous sec .",
    "[ sec : ig22 ] and the two setups discussed in the present sec .",
    "[ sec : ig44 ] are summarised in fig .",
    "[ fig6 ] .",
    "setups for the invasion game as investigated numerically in sec .",
    "[ sec : ig ] .",
    "setup ( a ) involves 2 percepts ( symbols ) and 2 actions ( moves ) as discussed in sec .",
    "[ sec : ig22 ] ( fig .",
    "[ fig5 ] ) .",
    "setup ( b ) involves 4 percepts consisting of 2 two - colored symbols and 2 measurements yielding 4 different outcomes described by @xmath240 as discussed in sec .",
    "[ sec : ig44 ] and determining either 4 [ fig .",
    "[ fig7](a ) and fig .",
    "[ fig8 ] ] or  if outcome @xmath102 is ignored  2 [ fig .",
    "[ fig7](b ) ] possible actions ( moves ) . while setup ( a ) refers to fig .",
    "[ fig2](c ) , setup ( b ) must refer to fig .",
    "[ fig2](a ) , if we want to keep the same hilbert space dimension of 4 for both setups , which allows better comparison of the effect of the number of controls on the learning curve .",
    "setup ( c ) involves a continuum of percepts consisting of 2 arbitrary - colored symbols and 2 actions ( moves ) as discussed in sec .",
    "[ sec : nec ] ( fig .",
    "[ fig9 ] ) . in setup ( c ) , separate subsystems are used for all 3 categories , hence it refers to fig .",
    "[ fig2](c ) , and the hilbert space dimension becomes 8 .",
    ", title=\"fig:\",width=253 ] +   setups for the invasion game as investigated numerically in sec .",
    "[ sec : ig ] .",
    "setup ( a ) involves 2 percepts ( symbols ) and 2 actions ( moves ) as discussed in sec .",
    "[ sec : ig22 ] ( fig .",
    "[ fig5 ] ) .",
    "setup ( b ) involves 4 percepts consisting of 2 two - colored symbols and 2 measurements yielding 4 different outcomes described by @xmath240 as discussed in sec .",
    "[ sec : ig44 ] and determining either 4 [ fig .",
    "[ fig7](a ) and fig .",
    "[ fig8 ] ] or  if outcome @xmath102 is ignored  2 [ fig .",
    "[ fig7](b ) ] possible actions ( moves ) . while setup ( a ) refers to fig .",
    "[ fig2](c ) , setup ( b ) must refer to fig .",
    "[ fig2](a ) , if we want to keep the same hilbert space dimension of 4 for both setups , which allows better comparison of the effect of the number of controls on the learning curve .",
    "setup ( c ) involves a continuum of percepts consisting of 2 arbitrary - colored symbols and 2 actions ( moves ) as discussed in sec .",
    "[ sec : nec ] ( fig .  [ fig9 ] ) . in setup ( c ) , separate subsystems are used for all 3 categories , hence it refers to fig .",
    "[ fig2](c ) , and the hilbert space dimension becomes 8 .",
    ", title=\"fig:\",width=226 ] +   setups for the invasion game as investigated numerically in sec .",
    "[ sec : ig ] .",
    "setup ( a ) involves 2 percepts ( symbols ) and 2 actions ( moves ) as discussed in sec .",
    "[ sec : ig22 ] ( fig .",
    "[ fig5 ] ) .",
    "setup ( b ) involves 4 percepts consisting of 2 two - colored symbols and 2 measurements yielding 4 different outcomes described by @xmath240 as discussed in sec .",
    "[ sec : ig44 ] and determining either 4 [ fig .",
    "[ fig7](a ) and fig .  [ fig8 ] ] or ",
    "if outcome @xmath102 is ignored  2 [ fig .",
    "[ fig7](b ) ] possible actions ( moves ) . while setup ( a ) refers to fig .",
    "[ fig2](c ) , setup ( b ) must refer to fig .",
    "[ fig2](a ) , if we want to keep the same hilbert space dimension of 4 for both setups , which allows better comparison of the effect of the number of controls on the learning curve .",
    "setup ( c ) involves a continuum of percepts consisting of 2 arbitrary - colored symbols and 2 actions ( moves ) as discussed in sec .",
    "[ sec : nec ] ( fig .",
    "[ fig9 ] ) . in setup ( c ) , separate subsystems are used for all 3 categories , hence it refers to fig .",
    "[ fig2](c ) , and the hilbert space dimension becomes 8 .",
    ", title=\"fig:\",width=253 ]    fig .",
    "[ fig7 ] shows the average reward @xmath229 received at each cycle , where the averaging is performed over an ensemble of @xmath230 independent agents , analogous to fig .",
    "[ fig5 ] .",
    "average reward @xmath229 as a function of the number of cycles for an invasion game ( 2 symbols in 2 colors ) , learning rate @xmath48 @xmath7 @xmath241 , with a reward of + 1 ( -10 ) for a correct ( false ) move , averaged over @xmath230 agents .",
    "the input states and povm are given by eqs .",
    "( [ is4 ] ) and ( [ pom4 ] ) , respectively .",
    "the graphs correspond to 1 , 2 , 3 , 4 , 8 , 16 , and 32 controls , as marked on the right .",
    "( a ) out of 4 possible moves , the defender must learn the correct one for each symbol and color .",
    "after @xmath242 cycles , the meanings of the symbols as well as the colors are reversed .",
    "( b ) out of 2 possible moves , the defender must learn the correct one for each symbol , whereas the color is irrelevant .",
    "for the first @xmath242 cycles , only symbols in a single color are presented , whereas for the remaining cycles , they are shown randomly in both colors .",
    ", title=\"fig:\",width=158 ]   average reward @xmath229 as a function of the number of cycles for an invasion game ( 2 symbols in 2 colors ) , learning rate @xmath48 @xmath7 @xmath241 , with a reward of + 1 ( -10 ) for a correct ( false ) move , averaged over @xmath230 agents .",
    "the input states and povm are given by eqs .",
    "( [ is4 ] ) and ( [ pom4 ] ) , respectively .",
    "the graphs correspond to 1 , 2 , 3 , 4 , 8 , 16 , and 32 controls , as marked on the right .",
    "( a ) out of 4 possible moves , the defender must learn the correct one for each symbol and color .",
    "after @xmath242 cycles , the meanings of the symbols as well as the colors are reversed .",
    "( b ) out of 2 possible moves , the defender must learn the correct one for each symbol , whereas the color is irrelevant .",
    "for the first @xmath242 cycles , only symbols in a single color are presented , whereas for the remaining cycles , they are shown randomly in both colors . ,",
    "title=\"fig:\",width=158 ]    note that in this sec .",
    "[ sec : ig44 ] , all figures refer to case ( ii ) described by ( [ ha ] ) and ( [ hb ] ) , where s and a now denote symbol and color , respectively . to account for this [ cf .",
    "the comments on fig .",
    "[ fig5](c ) above ] , a reward of @xmath41 @xmath7 @xmath243 ( instead of -1 ) is now given for a wrong action .",
    "the estimate of the defender s probability to block an attack is hence now @xmath244 .    in fig .",
    "[ fig7](a ) , the defender can choose between 4 moves , where for each percept , there is exactly one correct action [ i.e. , detecting @xmath245 ( @xmath246 @xmath200 @xmath247 ) for @xmath248 in ( [ pom4 ] ) and ( [ is4 ] ) returns a reward of @xmath41 @xmath7 @xmath227 ( @xmath41 @xmath7 @xmath243 ) ] .",
    "after @xmath242 cycles , symbol @xmath237 and color @xmath102 are read as symbol @xmath249 and color @xmath250 , respectively , similar to the manipulations in fig .",
    "5 in @xcite . in fig .",
    "[ fig7](b ) , the defender can choose between 2 moves , where for each symbol ( relevant category ) , there is exactly one correct action , irrespective of its color ( irrelevant category ) [ i.e. , detecting @xmath251 @xmath7 @xmath252 ( @xmath253 @xmath200 @xmath254 ) for @xmath248 in ( [ pom4 ] ) and ( [ is4 ] ) returns a reward of @xmath41 @xmath7 @xmath227 ( @xmath41 @xmath7 @xmath243 ) ] .",
    "the second color is added only after @xmath242 cycles , analogous to fig . 6 in @xcite .",
    "[ note that the mentioned initial stagnation phase in fig .",
    "[ fig5 ] is not visible in fig .",
    "[ fig7 ] , which is attributed to the choice of parameters ( rewards ) , accelerating the initial learning . ]    figs .",
    "[ fig5](b ) and [ fig7 ] are all motivated by figs . 5 and 6 in @xcite and confirm that the agent s adaptation to changing environments is recovered in our quantum control context .",
    "in addition , figs .  [ fig5](a ) and [ fig7 ] show the behaviour of an underactuated memory , where the number of controls is insufficient for its full controllability .",
    "since a @xmath255-matrix is determined by @xmath256 real parameters , and a global phase can be disregarded ( so that we can restrict to su(@xmath33)-matrices ) , @xmath256 @xmath74 @xmath103 controls are sufficient , i.e. , 15 for our invasion game , as mentioned above .    in ( [ pom4 ] ) , the measurements are made in a basis rotated by a randomly given unitary @xmath27 , which serves two purposes .",
    "on the one hand , it is required to ensure that the agent starts at the beginning of the first cycle with a policy that does not give exclusive preference to certain actions that follow from symmetries of the ( identity- ) initialised memory .",
    "this is a flaw of fig .",
    "[ fig2](a ) and can be overcome by using fig .",
    "[ fig2](c ) instead ( cf . a more detailed discussion in the grid world example below ) . on the other hand",
    ", @xmath27 serves as a given target in our discussion sec .",
    "[ sec3 ] , where we consider the agent learning as a navigation of its memory @xmath0 , cf . also fig .",
    "[ fig6](b ) .",
    "[ fig8 ] compares the case , where the agent is always fed with percept states drawn from one single onb defined via ( [ is4 ] ) with the case , where the percept states are drawn randomly , i.e. , taking @xmath257 with a random unitary @xmath34 as explained in sec .",
    "[ sec3 ] instead of ( [ is4 ] ) .",
    "note that in fig .",
    "[ fig8 ] , we generate a new random @xmath34 at each cycle , although a fixed set of dim@xmath258 ( 4 in our case ) such @xmath34 is sufficient as mentioned in sec .",
    "[ sec3 ] .",
    "( a ) reward @xmath41 , ( b ) fidelity @xmath259 defined in ( [ f ] ) , and ( c ) squared distance @xmath260 defined in ( [ d ] ) , where the overline denotes the ensemble average over @xmath230 agents for the setup as in fig .",
    "[ fig7](a ) ( i.e. , 4 percepts and 4 actions ) with 16 controls but without the reversal of meaning .",
    "the initial memory states are drawn from either a single or multiple orthonormal bases . , title=\"fig:\",width=158 ]   ( a ) reward @xmath41 , ( b ) fidelity @xmath259 defined in ( [ f ] ) , and ( c ) squared distance @xmath260 defined in ( [ d ] ) , where the overline denotes the ensemble average over @xmath230 agents for the setup as in fig .",
    "[ fig7](a ) ( i.e. , 4 percepts and 4 actions ) with 16 controls but without the reversal of meaning .",
    "the initial memory states are drawn from either a single or multiple orthonormal bases . , title=\"fig:\",width=158 ]   ( a ) reward @xmath41 , ( b ) fidelity @xmath259 defined in ( [ f ] ) , and ( c ) squared distance @xmath260 defined in ( [ d ] ) , where the overline denotes the ensemble average over @xmath230 agents for the setup as in fig .",
    "[ fig7](a ) ( i.e. , 4 percepts and 4 actions ) with 16 controls but without the reversal of meaning .",
    "the initial memory states are drawn from either a single or multiple orthonormal bases . , title=\"fig:\",width=158 ]    fidelity @xmath259 and squared distance @xmath260 are defined in ( [ f ] ) and ( [ d ] ) , where @xmath0 represents the agent memory and @xmath27 the target unitary . each cycle",
    "s update constitutes a single navigation step in the unitary group [ u(4 ) in our example ] .",
    "if , for a single onb , after a number of cycles , the average reward has approached unity , @xmath0 has reached a close neighbourhood of any unitary of the form @xmath30 , where @xmath32 @xmath7 @xmath261 with @xmath31 and @xmath262 being undetermined 4-rowed unitary matrices diagonal in the common eigenbasis of the @xmath248 ( i.e. , the `` computational basis '' ) .",
    "[ fig8](a ) shows that for a solution of the invasion game , a fixed onb is sufficient .",
    "drawing the percept states randomly , so that the set of all percept states is linearly dependent , does not affect the agent s ability to achieve perfect blocking efficiency , but slows down the learning process .",
    "the single onb case allows for a larger set of @xmath0 @xmath7 @xmath263 with respect to @xmath27 , as becomes evident in fig .",
    "[ fig8](b ) , so that navigation of @xmath0 from the identity to a member of this set takes less time ( as measured in cycles ) .",
    "the only freedom left in the case of multiple onbs is a global phase of @xmath0 , which remains undefined : navigation of @xmath0 towards @xmath27 with respect to the squared euclidean distance @xmath260 is not required for the learning tasks discussed , as evidenced by fig .",
    "[ fig8](c ) .      in sec .",
    "[ sec : ig44 ] we considered the case , where the symbols are presented in two different colors , as depicted in fig .  [ fig6](b ) . the original motivation for introducing colors as an additional percept category was to demonstrate the agent s ability to learn that they are irrelevant @xcite .",
    "in contrast , @xcite present a `` neverending - color scenario '' , where at each cycle , the respective symbol is presented in a new color .",
    "it is shown that while the basic ps - agent is in this case unable to learn at all , it becomes able to _ generalize _ ( abstract ) from the colors , if it is enhanced by a wildcard mechanism .",
    "the latter consists in adding an additional ( wildcard `` @xmath264 '' ) value to each percept category , and inserting between the input layer of percept clips and the output layer of action clips hidden layers of wildcard percept clips , in which some of the percept categories attain the wildcard value .",
    "the creation of these wildcard clips follows predefined deterministic rules , and the transitions from percept to action clips take then place via the hidden layers . (",
    "the notion of layers in the general ecm clip network fig .",
    "2 in @xcite follows from restricting to clips of length @xmath119 @xmath7 @xmath103 ) .",
    "since the use of wildcard clips is an integrated mechanism within ps ( inspired by learning classifier systems ) , the question is raised how similar ideas could be implemented in our context . for a memory fig .",
    "[ fig2](c ) , we could , e.g. , attribute one of the levels ( such as the respective ground state ) of the quantum system of each percept category @xmath265 to the wildcard - level @xmath266 , so that the percept space @xmath267 @xmath7 @xmath268 is enlarged to @xmath267 @xmath7 @xmath269 @xmath270 @xmath271 , where the @xmath272 are one - dimensional .    instead of this ,",
    "let us simply make use of the _ built - in _ generalization capacity of a quantum agent resulting from its coding of percepts as quantum states , which is much in the sense of sec . 8 in @xcite , where the percepts can be arbitrarily real - valued rather than being drawn from a countable or finite value set .",
    "consider the setup shown in fig .",
    "[ fig6](c ) , whose percept system includes a symbol and a color category and refers to a memory structure fig .  [ fig2](c ) . to allow for infinite colors",
    ", we could apply a color quantum system with infinite levels @xmath273 ( such as an oscillator - type system ) , which is initialized at each cycle in a new state drawn from a fixed onb ( such as a higher number state for an oscillator - type system ) .",
    "while such a scheme becomes more challenging to control , because the control vector @xmath14 has an infinite number of components [ we may replace it with a continuous control function @xmath274 , it still ignores the fact that colors ( as most percept categories in general ) are not countable . with this in mind",
    ", we can take the notion of colors literally and , to put it simply , code them in some appropriate color space such as rgb , where three parameters correspond to the red- , green- , and blue - signals of the agent s eye sensors .",
    "this suggests to encode a color as a mixed state of a two - level system , which is also given by three real - valued parameters ( determining its location in the bloch ball ) .",
    "the generalization from two colors to all rgb - colors then corresponds to the generalization from a classical to a quantum bit . in our setup , it is hence sufficient to apply a two - level system for the color category and initialize it at each cycle in a randomly chosen _ mixed",
    "_ state @xmath275 ( for neverending colors ) rather than a ( pure ) state randomly drawn from a single onb ( for two colors ) , whereas no changes are required on the agent s memory configuration itself .",
    "[ fig9 ] demonstrates the learning process .",
    "( a ) reward @xmath41 and ( b ) length @xmath276 of the control vector as a function of the number of cycles for a single agent fig .",
    "[ fig6](c ) playing an invasion game with 2 symbols , presented in a continuum of neverending colors , and 2 moves .",
    "a reward of + 1 ( -10 ) is given for each correct ( false ) move .",
    "the agent applies 64 controls with a learning rate of @xmath48 @xmath7 @xmath241 in an alternating layer scheme app .",
    "[ app : fl ] defined by two ( schmidt - orthonormalized ) 8-rowed random hamiltonians . ,",
    "title=\"fig:\",width=158 ]   ( a ) reward @xmath41 and ( b ) length @xmath276 of the control vector as a function of the number of cycles for a single agent fig .",
    "[ fig6](c ) playing an invasion game with 2 symbols , presented in a continuum of neverending colors , and 2 moves .",
    "a reward of + 1 ( -10 ) is given for each correct ( false ) move .",
    "the agent applies 64 controls with a learning rate of @xmath48 @xmath7 @xmath241 in an alternating layer scheme app .",
    "[ app : fl ] defined by two ( schmidt - orthonormalized ) 8-rowed random hamiltonians .",
    ", title=\"fig:\",width=158 ]    similar to fig .",
    "[ fig8 ] , random initialization slows down the learning process , so that we restrict to a single agent in fig .",
    "[ fig9 ] , rather than an ensemble average . as illustrated in fig .",
    "[ fig9](a ) , the agent s response becomes near - deterministic after about @xmath277 cycles , irrespective of the color .",
    "[ fig9](b ) illustrates in the example of the euclidean length of the control vector @xmath276 @xmath7 @xmath278 , that the navigation , which starts at @xmath55 @xmath7 @xmath56 , eventually comes to rest . while the random @xmath275 are drawn such that a positive probability is attributed to every volume element in the bloch ball , we did not care about drawing them with a uniform probability density , since mapping of an rgb - space of color ( as a perceptual property ) to the bloch ball is not uniquely defined .",
    "the ability to learn to distinguish between relevant @xmath279 and an arbitrary number of irrelevant percept categories @xmath280 as discussed in @xcite is of particular relevance for a quantum agent , where the irrelevant percept categories can be understood as adopting the role of a bath as shown in figs .",
    "[ fig2](b ) and ( d ) . here , a formal solution consists in a decoupled @xmath281 @xmath7 @xmath282 @xmath283 @xmath284 .      in what follows",
    ", we consider an arrangement of 8 grid cells as shown in fig .",
    "[ fig10 ] .",
    "@xmath285-grid world with an obstacle ( black cell ) .",
    "the arrows show the optimal policy for the shortest path to g , and the numbers present the policy numerically obtained with the agent fig .",
    "[ fig11](d ) after @xmath286 episodes .",
    "the red numbers in parentheses are the relevant different values obtained after @xmath287 episodes , if the agent starts each episode from a random cell , rather than always at s. , width=226 ]    the agent s task is to find the shortest route to a goal cell g , where at each step , only moves to an adjacent cell in four directions ( left , right , up , down ) are allowed .",
    "if the agent hits a boundary of the grid or the black cell , which is considered an obstacle , its location remains unchanged .",
    "this external classical navigation task constitutes a learning problem , because situations / percepts ( present location ) must be mapped to decisions / actions ( direction to go ) .",
    "the agent only perceives whether or not it has arrived at the goal cell .",
    "it has no access to a `` bird s perspective '' which would allow immediate exact location of the goal .",
    "it also has no access to a measure of goal distance or fidelity ( as in the case of the internal no - based loop regarding its own quantum memory in fig .",
    "[ fig1 ] ) , which prevents the use of external gradient information that could be obtained by testing the nearest neighbourhood of the present location .",
    "one can thus distinguish two objectives : ( a ) locating the goal and ( b ) finding a shortest route to it .",
    "this task constitutes a rl type problem , whose composite `` two - objective '' structure is approached by nesting iterations .",
    "the individual action selections , i.e. , choices of moves , correspond to the cycles in fig .",
    "sequences of cycles form episodes , which are terminated only once objective ( a ) has been solved .",
    "objective ( b ) is solved by sequences of episodes , which allow the agent to gradually solve objective ( a ) more efficiently and find an optimal policy . in fig .",
    "[ fig10 ] , the policy consists of a set of four probabilities for each cell , with which a corresponding move should be made from there .",
    "the optimal policy corresponding to the shortest route to g is indicated by the arrows in fig .",
    "[ fig10 ] .",
    "this grid world extends the above decision game in two aspects : ( a ) the optimal policy is in contrast to the decision game not deterministic , as indicated by the double arrows in the upper left and middle cell in fig .",
    "[ fig10 ] .",
    "( b ) depending on where the agent starts , more than a single move is required to reach g in general , preventing the agent from obtaining an immediate environmental feedback on the correctness of each individual move it makes .",
    "this second aspect leads to the mentioned notion of episodes . in what follows",
    ", we always place the agent at a fixed start cell s at the beginning of each episode , which is sufficient for learning the shortest path from s to g. while in the invasion game , episodes and cycles are synonyms , here , an episode is longer than a single cycle , since at least four moves are required to reach g from s.    when designing the agent s memory structure in the sense of fig .",
    "[ fig2 ] , we must take into account that the unitarity of the state transformation @xmath288 in fig .",
    "[ fig2](a ) places restrictions on the percept - encodings and the action - measurements , since @xmath288 maps an onb into another one . if we encode in fig .",
    "[ fig10 ] each cell location as a member of a given onb in an 8-dimensional system hilbert space @xmath289 and perform a naive symmetric @xmath289 @xmath7 @xmath290 @xmath270 @xmath290 @xmath270 @xmath290 @xmath270 @xmath290 -measurement for action selection , where the four 2-dimensional subspaces correspond to right , down , left and up moves , we can not properly ascribe the upper left and upper middle cells , because the right and downward pointing actions are already ascribed to the remaining 4 white cells .",
    "one may either try to construct a learning algorithm that exploits the fact that the two mentioned cells are off the optimal path from s to g so that the agent quickly ceases to visit them or construct a new povm such as a @xmath289 @xmath7 @xmath291 @xmath270 @xmath292 @xmath270 @xmath292 @xmath270 @xmath291-measurement , where two 3-dimensional subspaces correspond to right and down , and two 1-dimensional subspaces correspond to left and up moves .",
    "these possibilities require insight into the specifics of this problem and are not generalizable .",
    "in addition to that , fig .",
    "[ fig2](a ) requires initialisation of the agent memory in a random unitary to ensure it starts with a policy that does not give exclusive preference to certain actions that follow from symmetries of the initial @xmath288 ( such as the identity @xmath288 @xmath7 @xmath293 ) . if we want to avoid invoking a bath as in fig .",
    "[ fig2](b ) , we hence must resort to fig .",
    "[ fig2](c ) , which here implies a factorisation @xmath258 @xmath7 @xmath294 @xmath283 @xmath295 of @xmath258 into an 8-dimensional @xmath267 for encoding the grid cells and a 4-dimensional @xmath296 for encoding the four actions .",
    "if we encode the cells and actions as members of some onb in s and a , then initialising the agent s memory as identity , @xmath281 @xmath7 @xmath297 , and the initial action states as in ( [ ia ] ) ensures that the agent starts at the beginning of the first episode with a policy that assigns the same probability to all possible actions .    in figs .  [ fig11 ] and [ fig12 ]",
    "we investigate the episode length which is defined as the number of cycles per episode . rather than performing an ensemble average , we consider individual agents .",
    "these agents are described by ( [ ur3 ] ) with a learning rate of @xmath48 @xmath7 @xmath298 , absence of relaxation ( @xmath51 @xmath7 @xmath50 ) , and varying amounts of gradient glow ( @xmath76 @xmath299 @xmath103 ) .",
    "the number of episodes equals the number of times the agent is allowed to restart from s , whereas the time passed equals the sum of episode lengths .",
    "the episode length can be infinite but not smaller than four , the length of the shortest path from s to g.    fig .",
    "[ fig11 ] shows evolutions of episode lengths with the number of episodes , where we have set a maximum of @xmath300 episodes . as explained ,",
    "each episode starts at s and ends only when g has been reached .",
    "episode lengths as a function of the number of episodes for the @xmath285-grid world as shown in fig .",
    "[ fig10 ] .",
    "the plots illustrate the effect of gradient glow for single histories of individual agents .",
    "( a ) the agent receives a reward @xmath41 @xmath7 @xmath103 when it has found the goal , without gradient glow ( @xmath76 @xmath7 @xmath103 ) .",
    "( b ) as in ( a ) , but with gradient glow enabled ( @xmath76 @xmath7 @xmath301 ) .",
    "( c ) in addition to receiving a reward @xmath41 @xmath7 @xmath103 when it has found the goal , the agent is punished with a reward @xmath41 @xmath7 @xmath243 , when it has hit a boundary , without gradient glow ( @xmath76 @xmath7 @xmath103 ) .",
    "( d ) as in ( c ) , but with gradient glow enabled ( @xmath76 @xmath7 @xmath302 ) .",
    "( e ) as in ( d ) , but with gradient glow prolonged further ( @xmath76 @xmath7 @xmath303 ) .",
    "( f ) learning is disabled by always setting the reward to 0 .",
    "the agent performs a random walk through the grid with average length 54.1 which is included as dashed line in figs .  [ fig11 ] and [ fig12 ] .",
    ", title=\"fig:\",width=158 ]   episode lengths as a function of the number of episodes for the @xmath285-grid world as shown in fig .",
    "[ fig10 ] .",
    "the plots illustrate the effect of gradient glow for single histories of individual agents .",
    "( a ) the agent receives a reward @xmath41 @xmath7 @xmath103 when it has found the goal , without gradient glow ( @xmath76 @xmath7 @xmath103 ) .",
    "( b ) as in ( a ) , but with gradient glow enabled ( @xmath76 @xmath7 @xmath301 ) .",
    "( c ) in addition to receiving a reward @xmath41 @xmath7 @xmath103 when it has found the goal , the agent is punished with a reward @xmath41 @xmath7 @xmath243 , when it has hit a boundary , without gradient glow ( @xmath76 @xmath7 @xmath103 ) .",
    "( d ) as in ( c ) , but with gradient glow enabled ( @xmath76 @xmath7 @xmath302 ) .",
    "( e ) as in ( d ) , but with gradient glow prolonged further ( @xmath76 @xmath7 @xmath303 ) .",
    "( f ) learning is disabled by always setting the reward to 0 .",
    "the agent performs a random walk through the grid with average length 54.1 which is included as dashed line in figs .  [ fig11 ] and [ fig12 ] .",
    ", title=\"fig:\",width=158 ]   episode lengths as a function of the number of episodes for the @xmath285-grid world as shown in fig .",
    "[ fig10 ] .",
    "the plots illustrate the effect of gradient glow for single histories of individual agents .",
    "( a ) the agent receives a reward @xmath41 @xmath7 @xmath103 when it has found the goal , without gradient glow ( @xmath76 @xmath7 @xmath103 ) .",
    "( b ) as in ( a ) , but with gradient glow enabled ( @xmath76 @xmath7 @xmath301 ) .",
    "( c ) in addition to receiving a reward @xmath41 @xmath7 @xmath103 when it has found the goal , the agent is punished with a reward @xmath41 @xmath7 @xmath243 , when it has hit a boundary , without gradient glow ( @xmath76 @xmath7 @xmath103 ) .",
    "( d ) as in ( c ) , but with gradient glow enabled ( @xmath76 @xmath7 @xmath302 ) .",
    "( e ) as in ( d ) , but with gradient glow prolonged further ( @xmath76 @xmath7 @xmath303 ) .",
    "( f ) learning is disabled by always setting the reward to 0 .",
    "the agent performs a random walk through the grid with average length 54.1 which is included as dashed line in figs .  [ fig11 ] and [ fig12 ] .",
    ", title=\"fig:\",width=158 ]   episode lengths as a function of the number of episodes for the @xmath285-grid world as shown in fig .",
    "[ fig10 ] .",
    "the plots illustrate the effect of gradient glow for single histories of individual agents .",
    "( a ) the agent receives a reward @xmath41 @xmath7 @xmath103 when it has found the goal , without gradient glow ( @xmath76 @xmath7 @xmath103 ) .",
    "( b ) as in ( a ) , but with gradient glow enabled ( @xmath76 @xmath7 @xmath301 ) .",
    "( c ) in addition to receiving a reward @xmath41 @xmath7 @xmath103 when it has found the goal , the agent is punished with a reward @xmath41 @xmath7 @xmath243 , when it has hit a boundary , without gradient glow ( @xmath76 @xmath7 @xmath103 ) .",
    "( d ) as in ( c ) , but with gradient glow enabled ( @xmath76 @xmath7 @xmath302 ) .",
    "( e ) as in ( d ) , but with gradient glow prolonged further ( @xmath76 @xmath7 @xmath303 ) .",
    "( f ) learning is disabled by always setting the reward to 0 .",
    "the agent performs a random walk through the grid with average length 54.1 which is included as dashed line in figs .  [ fig11 ] and [ fig12 ] .",
    ", title=\"fig:\",width=158 ]   episode lengths as a function of the number of episodes for the @xmath285-grid world as shown in fig .",
    "[ fig10 ] .",
    "the plots illustrate the effect of gradient glow for single histories of individual agents .",
    "( a ) the agent receives a reward @xmath41 @xmath7 @xmath103 when it has found the goal , without gradient glow ( @xmath76 @xmath7 @xmath103 ) .",
    "( b ) as in ( a ) , but with gradient glow enabled ( @xmath76 @xmath7 @xmath301 ) .",
    "( c ) in addition to receiving a reward @xmath41 @xmath7 @xmath103 when it has found the goal , the agent is punished with a reward @xmath41 @xmath7 @xmath243 , when it has hit a boundary , without gradient glow ( @xmath76 @xmath7 @xmath103 ) .",
    "( d ) as in ( c ) , but with gradient glow enabled ( @xmath76 @xmath7 @xmath302 ) .",
    "( e ) as in ( d ) , but with gradient glow prolonged further ( @xmath76 @xmath7 @xmath303 ) .",
    "( f ) learning is disabled by always setting the reward to 0 .",
    "the agent performs a random walk through the grid with average length 54.1 which is included as dashed line in figs .  [ fig11 ] and [ fig12 ] .",
    ", title=\"fig:\",width=158 ]   episode lengths as a function of the number of episodes for the @xmath285-grid world as shown in fig .",
    "[ fig10 ] .",
    "the plots illustrate the effect of gradient glow for single histories of individual agents .",
    "( a ) the agent receives a reward @xmath41 @xmath7 @xmath103 when it has found the goal , without gradient glow ( @xmath76 @xmath7 @xmath103 ) .",
    "( b ) as in ( a ) , but with gradient glow enabled ( @xmath76 @xmath7 @xmath301 ) .",
    "( c ) in addition to receiving a reward @xmath41 @xmath7 @xmath103 when it has found the goal , the agent is punished with a reward @xmath41 @xmath7 @xmath243 , when it has hit a boundary , without gradient glow ( @xmath76 @xmath7 @xmath103 ) .",
    "( d ) as in ( c ) , but with gradient glow enabled ( @xmath76 @xmath7 @xmath302 ) .",
    "( e ) as in ( d ) , but with gradient glow prolonged further ( @xmath76 @xmath7 @xmath303 ) .",
    "( f ) learning is disabled by always setting the reward to 0 .",
    "the agent performs a random walk through the grid with average length 54.1 which is included as dashed line in figs .  [ fig11 ] and [ fig12 ] .",
    ", title=\"fig:\",width=158 ]    fig .",
    "[ fig11](f ) shows for comparison the lengths of @xmath300 random walks through the grid of an agent whose learning has been disabled by always setting the reward to 0 .",
    "the average number of 54.1 steps to reach g from s is shown in figs .",
    "[ fig11 ] and [ fig12 ] as a dashed line for comparison . in figs .",
    "[ fig11](a - e ) , a positive reward of @xmath41 @xmath7 @xmath103 is given for hitting g. while in fig .",
    "[ fig11](a ) , the reward is always zero before g has been hit , in fig .",
    "[ fig11](c ) hitting a boundary is punished with a negative reward of @xmath41 @xmath7 @xmath243 , which slightly improves the agent s performance . [",
    "note that all plots are specific to the respective learning rate ( here @xmath48 @xmath7 @xmath298 ) , which has been chosen by hand to observe an improvement within our @xmath300 episode - window and at the same time minimising the risk of oversized learning steps . while in general , the learning rate is gradually decreased ( cf .",
    "the conditions eq . ( 2.8 ) in @xcite to ensure convergence ) , this is not strictly necessary . in our numerical examples",
    "we have kept @xmath48 constant for simplicity .",
    "implementation of a dynamic adaptation of the learning rate as was done in @xcite and @xcite in the present context is left for future work . ]",
    "the transitions fig .",
    "[ fig11](a@xmath2b ) and fig .",
    "[ fig11](c@xmath2d ) show the effect of enabling gradient glow , i.e. @xmath304 @xmath190 @xmath305 in ( [ ur3 ] ) .",
    "gradient glow provides a mechanism of gradual backpropagation of the policy change from the nearest neighbourhood of g to cells more distant from g as the number of episodes increases . in fig .",
    "[ fig11 ] , the agent settles in the optimal policy in cases ( b ) , ( d ) and ( e ) .",
    "the policy resulting after @xmath306 episodes in case fig .",
    "[ fig11](d ) is given in fig .",
    "[ fig10 ] , where the numbers in each cell present the probability to move in the respective direction .",
    "while the agent finds the optimal policy for all cells forming the shortest path , it remains ignorant for the remaining cells .",
    "as the agent finds and consolidates the shortest path , then episode over episode , it soon visits the off - path cells less frequently , so that the transition probabilities from these cells do not accumulate enough iterations and are `` frozen '' in suboptimal values .",
    "this is characteristic of rl and can also be observed in learning to play games such as backgammon @xcite , where it is sufficient to play well only in typical rather than all possible constellations of the game . since for large games ,",
    "the former often form a small subset of the latter , this can be seen as a strategy to combat with large state spaces ( such as number of possible game constellations ) .",
    "to find an optimal policy for _ all _ cells in fig .",
    "[ fig10 ] , we may start each episode from a random cell , analogous to initialising the agent in an overcomplete basis as explained in fig .",
    "the red numbers in parentheses shown in fig .",
    "[ fig10 ] present a new policy obtained after @xmath307 episodes in this way .",
    "in contrast to the old policy , it is optimal or nearly optimal for all cells , with the difference between 1 and the sum of these numbers quantifying the deviation from optimality for each cell @xmath308 . since on average",
    ", the agent starts from a given cell only in 1/7-th of all episodes , the learning is slowed down , analogous to fig .",
    "[ fig8](a ) .",
    "[ fig12 ] summarises the effect of gradient glow illustrated in fig .",
    "[ fig11 ] for the two rewarding strategies .",
    "episode lengths @xmath309 averaged over the last 500 episodes in fig .",
    "[ fig11 ] , i.e. , episodes @xmath310",
    "@xmath74 @xmath311 of single histories of individual agents in the @xmath285-grid world as shown in fig .",
    "[ fig10 ] .",
    "the data points distinguish various rewarding strategies and values of gradient glow @xmath76 as explained in fig .",
    "[ fig11 ] .",
    "the upper and lower dashed lines reflect a random walk and the shortest path , respectively , and the letters ( a)-(f ) correspond to the respective plots in fig .",
    "[ fig11 ] .",
    "the optimum value of @xmath76 depends on the details of the rewarding strategy . , width=325 ]    to limit the numerical effort , we have averaged the episode lengths over the last 500 episodes in fig .  [ fig11 ] for individual agents as a `` rule of thumb''-measure of the agent s performance for the strategy chosen . for a deterministic calculation we must instead average the length of each episode ( and for each @xmath76 ) over a sufficiently large ensemble of independent agents for as many episodes as needed to reach convergence .",
    "despite these shortcomings , the results indicate a qualitatively similar behaviour as fig .",
    "4(a ) in @xcite . figs .",
    "[ fig11 ] and [ fig12 ] demonstrate that gradient glow improves the agent performance , irrespective of whether or not it receives information on false intermediate moves by means of negative rewards , although the latter reduce the required length of glow .",
    "it is expected that for an ensemble average , an optimal value of @xmath76 can be found , with which the fastest convergence to the shortest path can be achieved .",
    "[ fig11 ] distinguishes two qualitatively different modes of convergence .",
    "if @xmath76 is larger than optimal , a gradual improvement is observed , as seen by the damping of spikes in fig .",
    "[ fig11](d )",
    ". if @xmath76 is smaller than optimal , then an abrupt collapse to the optimal policy without visible evidence in the preceding statistics that would provide an indication is observed , cf .",
    "[ fig11](e ) .",
    "if @xmath76 is decreased further , this transition is likely to happen later , to the point it will not be observed within a fixed number of episodes .",
    "this results in the steep increase in episode length shown in fig .",
    "[ fig12 ] , which would be absent if the ensemble average was used instead .",
    "this sudden transition as shown in fig .",
    "[ fig11](e ) can also be observed for individual agents in @xcite ( not shown there ) , which applies a softmax - policy function along with edge glow .",
    "it is surprising that the quadratic measurement - based policy simulated here exhibits the same phenomenon .",
    "note however , that convergence does not imply optimality . in tabular rl and ps",
    ", such an abrupt transition can be observed if the @xmath115-parameter and hence the `` correlation length '' is too large ( in rl ) or if the @xmath76-parameter is too small , so that the glow lasts too long ( in ps ) .",
    "the policies obtained in this way are typically sub - optimal , especially in larger scale tasks such as bigger grid worlds , for which the agent learns `` fast but bad '' in this case .",
    "it is hence expected that a similar behaviour can be observed for our method if we increased the size of the grid .",
    "this work s numerical experiments rely on a symbolic expression ( [ gradcomps ] ) for the gradient @xmath312 in ( [ grad ] ) for simplicity , which is usually not available in practice , also keeping in mind the variety of compositions fig .",
    "[ fig2 ] , so that the agent s memory @xmath13 is generally unknown . as explained in the discussion of fig .",
    "[ fig1 ] , the agent may then apply a measurement - based internal loop by repeatedly preparing its memory in a state that corresponds to the last percept @xmath313 , and register whether or how often the last measurement outcome @xmath314 can be recovered .",
    "this approach can be done with either infinitesimal or finite changes in the control vector @xmath14 , where we can distinguish between expectation value- and sample - based updates , depending on how many internal cycles are performed between consecutive external cycles .",
    "it should be stressed that the external cycles in fig .",
    "[ fig1 ] represent the agent - environment interaction , resulting in sequences of state - action pairs and corresponding rewards . while in an elementary optimal control problem ,",
    "a given objective is to be optimized , here the environment poses at each external cycle a separate and generally unpredictable control problem , all of which must be addressed by the agent simultaneously .    due to the small learning rate @xmath48 , the update rule ( [ ur3 ] ) is in all cases local in parameter space , which reflects the assumption , that a physical agent can not completely reconfigure its `` hardware '' in a single instant .",
    "while it is then consistent to apply a gradient @xmath315 @xmath7 @xmath316 as a local quantity in ( [ ur3 ] ) , from a computational perspective , it has a few drawbacks , however .",
    "one is that the direction of steepest accent at the current control vector @xmath317 does not need to coincide with the direction @xmath315 @xmath7",
    "@xmath318 @xmath74 @xmath319 towards the optimum @xmath320 , as illustrated in fig .  [ fig13 ]",
    ".     following the direction @xmath321 of steepest ascent ( dashed line ) does not necessarily lead to the shortest route @xmath315 @xmath7 @xmath318 @xmath74 @xmath319 from a given control vector @xmath317 at cycle @xmath16 to a location @xmath320 , for which @xmath322 becomes maximum .",
    ", width=264 ]    another aspect is the vanishing of the gradient .",
    "consider for example the initialisation of the action system in a mixed state ( [ pcoh ] ) as done in fig .",
    "[ fig5](b ) .",
    "in particular , the graph with @xmath232 @xmath7 @xmath50 does not display any learning ability . substituting the corresponding @xmath235 @xmath7 @xmath323 in ( [ pcoh ] ) and @xmath0 @xmath7 @xmath59 into ( [ gradcomps ] ) ,",
    "we see that the reason is the vanishing gradient , @xmath324 @xmath7 @xmath325 $ ] @xmath7 @xmath50 . on the other hand , the corresponding setup fig .",
    "[ fig6](a ) reveals that in this case , substituting a swap - gate between s and a for @xmath0 provides an optimal solution ( along with an x - gate if the meaning of the symbols is reversed ) for any @xmath235 , that is obviously not found in fig .",
    "[ fig5](b ) .",
    "this failure occurs despite the fact that the agents explore , as indicated by the fluctuations in fig .",
    "[ fig5](b ) . to understand the difference , note that we may generate an @xmath1-greedy policy function by replacing in ( [ grada ] ) an ( arbitrarily given ) state @xmath38 with @xmath326 , where @xmath327 @xmath328 @xmath329 @xmath330 @xmath103 and @xmath331 @xmath7 @xmath332 .",
    "the term with @xmath218 then gives to ( [ grada ] ) a contribution @xmath333 , that is independent of @xmath37 . at the same time",
    ", it does not contribute in ( [ gradcomps ] ) to the gradient , @xmath324 @xmath7 @xmath50 . if @xmath315 @xmath7 @xmath316 @xmath7 @xmath50 for all @xmath16 in ( [ ur3 ] ) , the agent s learning comes to rest , however .",
    "finite difference and sample - based updates here offer a possibility to explore _ in parameter space _ the neighbourhood of the present location @xmath317 ( or , colloquially , the `` state '' ) of the agent s memory , as a consequence of asymmetries in the control landscape or statistical fluctuations in the samples .    of particular relevance is a final fixpoint ( [ task ] ) . intuitively , one would assume that ( despite the compactness of the ( s)u(@xmath33)-groups , that is in contrast to the potentially unbounded values of @xmath136 in rl or @xmath145 in ps ) once an agent has settled in a point ( [ task ] ) , due to the vanishing gradient , it wo nt be able to react quickly , if the environment suddenly changes its allocation of rewards ( without confronting the agent with percepts it has not perceived before ) . however , the learning curves for controllable memories ( 16 and 32 controls ) in fig .",
    "[ fig7](a ) demonstrate that relearning after @xmath242 cycles is not affected . a study of individual agents with 32 controls in fig .",
    "[ fig7](a ) reveals that the euclidean length of the numerical gradient rises from @xmath334 at cycle 5000 to a value @xmath335 in only 15 cycles .",
    "better understanding of this is left for future study . in",
    "what follows , we outline the mentioned alternatives in some more detail .",
    "if the time consumed by the internal cycles is uncritical with respect to the external cycles , the agent can obtain estimates of @xmath322 from a sufficiently large number of internal binary measurements . with these , it can either approximate the components @xmath336 @xmath337 @xmath338 @xmath74 @xmath339/\\delta{h}_k$ ] of the local gradient @xmath340 @xmath7 @xmath341 , which is then substituted as @xmath315 @xmath342 into ( [ ur3 ] ) . alternatively , it can perform a global search for the location @xmath320 of the maximum of @xmath322 . a possible algorithm for the latter",
    "is differential evolution , which relies on deterministic values @xmath343 rather than noisy samples .",
    "once an estimate for @xmath320 has been found , the difference @xmath315",
    "@xmath7 @xmath318 @xmath74 @xmath319 is used in ( [ ur3 ] ) .",
    "reliance on expectation values may give away potential speed gains offered by a quantum memory , which poses the question , whether a finite number of sample measurements is sufficient . since individual updates in ( [ ur3 ] )",
    "are made with a small fraction @xmath48 of the whole @xmath315 , the assumption is that the individual statistical errors in the sampled @xmath315 cancel out in the long run .",
    "as for the expectation value - based updates discussed above , samples can be used to either create _ discrete _ estimates @xmath336 @xmath337 @xmath344 @xmath74 @xmath345/2 $ ] for the components @xmath102 of the local gradient @xmath340 @xmath7 @xmath341 , where @xmath37 @xmath7 @xmath346 @xmath7 @xmath347 depending on whether the outcome of the binary measurement @xmath348 is positive or not .",
    "alternatively , for finite difference updates , one may consider a neural gas @xcite inspired approach depicted in fig .",
    "[ fig14 ] .     internally generated random cloud of sample controls @xmath349 around a given control vector @xmath317 at cycle @xmath16 for which binary measurements `` given @xmath313 , detect @xmath314 or not '' are carried out between external cycles , yielding positive ( @xmath350 ) or negative ( @xmath351 ) outcomes . ,",
    "width=188 ]    in this approach , the differences @xmath352 between the sampled centers of positive [ @xmath175 @xmath7 @xmath353 @xmath7 @xmath227 , i.e. , @xmath349 @xmath7 @xmath354 and negative outcomes ( @xmath175 @xmath7 @xmath353 @xmath7 @xmath228 , i.e. , @xmath349 @xmath7 @xmath355 ) of the binary measurements @xmath356 are then applied in ( [ ur3 ] ) .",
    "although one could store a current estimate @xmath357 for each observed state - action pair @xmath358 and merely update it according to ( [ ng ] ) with each new measurement point @xmath359 , this would give away the generalization capability described in sec .",
    "[ sec : nec ] .",
    "one would hence need to perform @xmath33 internal cycles with the povm - based internal loop between each external cycle .",
    "the @xmath349 could be drawn , e.g. , from a gaussian centered around the respective @xmath317 .",
    "the variance of this gaussian could be gradually decreased with the number of external cycles to increase the locality ( local resolution ) of the cloud .",
    "[ fig13 ] gives the misleading impression that finite difference updates are superior to gradient - based methods . to give an illustrative counterexample ,",
    "one could think of a two - dimensional @xmath14 @xmath7 @xmath360 and a control landscape @xmath361 modelled by the monotonically increasing height @xmath362 along the length @xmath199 of a tape of paper bent into a spiral and placed onto the dashed line in fig .",
    "[ fig13 ] , such that one end with @xmath363 @xmath7 @xmath50 is located at @xmath317 and the other one at @xmath320 . here",
    ", a gradient - based method would safely follow the long path on the tape s upper edge , whereas a finite difference method would trade a potential speedup with the risk of missing the paper at all trials .",
    "since a comparison of state of the art optimal control methods based on noisy samples for the agent s learning would go beyond the scope of this work , we here restrict ourselves to these sketchy lines of thought , whose numerical study is pending , and leave open the question of what the best method is for a given task .    a characteristic shared by the loops of fig .",
    "[ fig1 ] and optimal control setups is the need of an experimental `` mastermind '' who controls the controls .",
    "an agent which is supposed to act autonomously would be required to accomplish this by itself , ideally in a `` natural '' or `` organic computing '' sense .",
    "an elementary example from everyday life are `` desire paths '' which form or dissipate , depending on their usage and without a designated planner .",
    "in summary , we have adopted an update rule from the basic ps scheme , equipped it with gradient glow , and applied it to small - scale invasion game and grid world tasks .",
    "the numerical results show that similar results can be obtained for a quantum agent , as long as the memory is not underactuated .",
    "this is not obvious , because of the fundamental difference in the number of free parameters .",
    "if @xmath364 and @xmath365 denote the number of possible percepts and actions , respectively , then in classical tabular action value rl - methods , the estimated values of all percept - action pairs are combined to a @xmath366-matrix , i.e. , we have @xmath367 real parameters .",
    "if we encoded in our scheme each percept and action category by a separate subsystem , whose dimensionalities correspond to the number of values , the respective category can adopt , then @xmath0 is an at least @xmath368-matrix for which we are faced with @xmath369 real parameters .",
    "note that this work is unrelated to the reflecting ps agents , which are discussed in @xcite . while the scheme @xcite allows a proof of quantum speedup",
    ", our approach complements the latter in that it is simple , flexible in its construction , and does not involve specific analytic quantum gates .",
    "the learning of a good policy only for percepts which are `` typical '' and have thus been encountered sufficiently often in the past shares features with `` soft computing '' , where it is sufficient to find a good rather than an exact solution , which would here consist in a policy that is optimal for all possible percepts .",
    "one may think of , e.g. , simplifying a symbolic mathematical expression : while all transformation steps themselves must be exact , there are no strict rules , as far as the best way of its formulation is concerned . in future work",
    ", it may be worth to incorporate recent extensions of the classical ps scheme such as generalization @xcite .",
    "classical ps in its general form is based on a discrete network of clips that form its ecm . in the examples considered in @xcite , after each discrete time step ( external cycle ) ,",
    "a local edge is updated according to @xmath370 where the instantaneous excitation @xmath160 @xmath7 @xmath161 is the product of the edge s current glow value @xmath371 and the respective non - negative reward @xmath162 given at this time step .",
    "the glow values dampen according to @xmath372 @xmath7 @xmath373 with @xmath374 @xmath7 @xmath50 , where @xmath375 $ ] is a glow parameter , and are reset to 1 if the edge was visited during a cycle . @xmath107 $ ]",
    "is a damping parameter towards an equilibrium value @xmath376 in the absence of rewards ( e.g. , 1 ) .",
    "starting from some given @xmath377 ( e.g. , @xmath376 ) , this gives @xmath378h^{\\mathrm{eq } }    + \\sum_{k=0}^{t-1}(1-\\gamma)^kr_{t - k-1 }    \\nonumber\\\\    & = & h_0+\\sum_{k=0}^{t-1}r_{t - k-1}\\quad(\\gamma=0 ) , \\label{hdiv }    \\\\",
    "h_t&\\approx&h^{\\mathrm{eq}}+\\sum_{k=0}^{t-1}(1-\\gamma)^kr_{t - k-1 }    \\quad(\\gamma>0 ) , \\label{happ}\\end{aligned}\\ ] ] where the approximation as a backward - discounted sum ( [ happ ] ) holds for times sufficiently large so that @xmath379 @xmath330 @xmath103 or exactly for @xmath377 @xmath7 @xmath380 .",
    "note that due to the absence of damping , ( [ hdiv ] ) diverges in general if @xmath16 grows without limit .",
    "in app .  [ app : a2 ] we focus on a model - based ( i.e. , symbolic ) determination of the gradient @xmath381 .",
    "the exact form of the gradient depends on the parametrization .",
    "for example , if @xmath13 @xmath7 @xmath382 is given by some hermitian @xmath85 , then @xmath383 for small @xmath85 , we can expand the exponentials in @xmath85 to lowest order , and the approximation @xmath44",
    "@xmath337 @xmath384 holds .    in case of a continuous time dependence",
    ", the vector @xmath14 can be replaced by a function @xmath385 , with which a unitary propagator from time @xmath386 to time @xmath387 is given as a positively time ordered integral @xmath388 and @xmath389 if @xmath390 @xmath7 @xmath391 is expanded in terms of some fixed hamiltonians @xmath392 , then with @xmath46 @xmath7 @xmath393 , ( [ x2 ] ) becomes @xmath394 @xmath7 @xmath395 , and ( [ x3 ] ) is replaced with @xmath396 @xmath7 @xmath397 .",
    "navigation on unitary groups becomes discretized if only a restricted ( finite ) set of hamiltonians @xmath392 can be implemented at a time rather than an analytically time - dependent hamiltonian @xmath390 , so that only one of the @xmath398 is non - zero for a given @xmath16 .",
    "a known example is the alternating application of two fixed hamiltonians @xcite , @xmath399 @xmath7 @xmath400 and @xmath401 @xmath7 @xmath402 @xmath403 , for a set of times to be determined from the target unitary @xcite . in this discrete case as defined by a piecewise constant normalized @xmath85 in ( [ x2 ] ) , the function @xmath385 can be replaced with a vector @xmath14 , and the functional derivatives with respect to @xmath385 reduce to gradients with respect to @xmath14 .",
    "we can update the present unitary @xmath0 by multiplying it from the left with a new layer @xmath404 after each cycle , @xmath405 if @xmath404 @xmath7 @xmath406 is a small modification close to the identity , then the mentioned approximation of ( [ x1 ] ) gives @xmath407 @xmath337 @xmath408 .",
    "this is of advantage if the agent or its simulation is able to store only the present @xmath0 and not the history of updates ( layers ) .",
    "the components of ( [ grad ] ) then become @xmath409      in our numerical examples we consider a discretized memory model , for which ( [ x2 ] ) reduces to a product of @xmath33 unitaries @xmath410 which simplifies the determination of the gradient , since @xmath407 @xmath7 @xmath411 , where @xmath412 @xmath7 @xmath413 , so that the components of ( [ grad ] ) now become @xmath414 in this work , we use alternating layers defined by two fixed hamiltonians @xmath211 and @xmath212 as mentioned at the beginning of this section .",
    "consider an @xmath33-level system and two unitary operators @xmath0 and @xmath27 , where @xmath0 @xmath7 @xmath415 depends on an ( unrestricted ) external control field @xmath385 , and @xmath416 is a desired target .",
    "their ( squared ) euclidean distance as induced by the hilbert - schmidt dot product is given by @xmath417    \\nonumber\\\\    & = & 2n-2\\mathrm{re}\\mathrm{tr}(\\hat{u}_{\\mathrm{t}}^\\dagger\\hat{u } )    \\in[0,4n ] .",
    "\\label{d}\\end{aligned}\\ ] ] if @xmath418 is controllable in the sense that at least one @xmath385 exists such that @xmath418 @xmath7 @xmath416 , then ( [ d ] ) has the set @xmath419 as possible extremal values ( i.e ,",
    "@xmath420 @xmath7 @xmath50 ) , where the values @xmath327 and @xmath421 are attained for @xmath0 @xmath7 @xmath422 , while the remaining extrema are saddle points @xcite . a measure insensitive to global phases @xmath0",
    "@xmath7 @xmath423 is the average fidelity defined by @xmath424 where the overline denotes uniform average over all @xmath425 @xcite .",
    "note that @xmath259 @xmath52 @xmath426 $ ] for @xmath33 @xmath144 @xmath103 , and @xmath259 @xmath7 @xmath103 for @xmath33 @xmath7 @xmath103 .",
    "both ( [ d ] ) and ( [ f ] ) are determined by the complex @xmath427 which is confined to the complex unit circle and whose expectation @xmath428 @xmath7 @xmath429 for uniform random @xmath0 drops to zero with growing @xmath33 .    if in ( [ f ] )",
    ", the averaging is restricted to a @xmath331-dimensional subspace p , we must replace ( [ f ] ) with @xmath430 where @xmath431 @xmath7 @xmath432 , with @xmath433 being the projector onto p. note that @xmath434 @xmath52 @xmath435 $ ] for @xmath33 @xmath144 @xmath103 [ since @xmath436 @xmath7 @xmath437 @xmath52 @xmath438 with @xmath439 @xmath7 @xmath440 , @xmath441 @xmath7 @xmath442 , and @xmath434 @xmath7 @xmath103 for @xmath33 @xmath7 @xmath103 . while for a one - dimensional @xmath433 @xmath7 @xmath443 , ( [ fp ] ) reduces to @xmath444 @xmath7 @xmath445 , the other limit @xmath331 @xmath7 @xmath446 recovers ( [ f ] ) .",
    "if in ( [ fp ] ) , @xmath0 @xmath7 @xmath447 couples the quantum system s to a bath b , then we define a projector @xmath40 @xmath7 @xmath448 and generalize ( [ fp ] ) to @xmath449}^{(\\mathrm{p } ) }    \\\\    & = & \\left\\langle    \\frac{\\mathrm{tr}_{\\mathrm{s}}(\\hat{m}^\\dagger\\hat{m } )    + ( \\mathrm{tr}_{\\mathrm{s}}\\hat{m})^\\dagger(\\mathrm{tr}_{\\mathrm{s}}\\hat{m } )    } { d(d+1)}\\right\\rangle_{\\mathrm{b}},\\end{aligned}\\ ] ] where @xmath450 @xmath70 @xmath451 $ ] with a fixed bath state @xmath452 .    replacing @xmath453 in ( [ f ] ) with the output @xmath454 of a quantum channel generalizes ( [ f ] ) to @xcite @xmath455 where @xmath456 are the kraus operators of the decomposition of the channel map @xmath457 @xmath7 @xmath458 .",
    "note that a change @xmath459 @xmath7 @xmath460 of the kraus operators as described by a unitary matrix @xmath99 leaves ( [ f1 ] ) invariant .",
    "the uniform average in ( [ f1 ] ) can be generalized to an arbitrary distribution @xmath461 of possible input states @xmath462 , @xmath463 that reflects the statistics of their occurrence in different instances of applications of the device ( external cycles in a control loop ) .",
    "this generalizes ( [ f1 ] ) to @xmath464 which is just the total probability @xmath465 of correctly detecting a @xmath27-transformed pure ( but unknown ) input state drawn from a distribution ( [ rhoin ] ) .",
    "once @xmath466 @xmath7 @xmath103 , the channel s effect is indistinguishable from that of @xmath27 for the set of possible inputs @xmath462 , ( i.e. those for which @xmath461 @xmath144 @xmath50 ) .",
    "the case @xmath466",
    "@xmath467 @xmath103 is relevant from a numerical and experimental point of view .",
    "rare inputs @xmath462 , for which @xmath327 @xmath328 @xmath468 @xmath330 @xmath103 , will hardly affect @xmath466 in a control loop , which relaxes the demands on the channel compared to the uniform average ( [ f1 ] ) .",
    "the channel optimization itself is thus economized in the sense that it is required to perform well only on _ typical _ rather than _ all _ inputs .    here , we consider a navigation of @xmath0 in the above - mentioned discretized case , @xmath0 @xmath7 @xmath469 , that starts at the identity @xmath470 @xmath7 @xmath471 @xmath7 @xmath59 and from there undertakes a gradient - based maximization of @xmath465 as defined in ( [ f2 ] ) to a point where @xmath472 the control vector @xmath14 typically represents system variables and not those of the bath . rather than solving for the parameters @xcite for which the scheme @xcite yields a desired given unitary , we search parameters , for which the unitary , whose specific form we are not interested in , solves a given task .      while here we have discussed and compared concrete types of algorithms , a more fundamental question concerns the optimality and derivation of general ( e.g. , speed ) limits of the learning process",
    "although the physical time is given as the sum of the agent and environment response times over each cycle , one may restrict to counting the number of ( a ) memory cycles in total , ( b ) external cycles only ( c ) episodes or ( d ) parameter updates @xmath43 of the memory @xmath13 , depending on what the most critical criterion is . not only should the navigation of @xmath0 from the identity to a point ( [ task ] ) follow an optimal trajectory , but also the navigation of the percept states by a given @xmath0 should be such that the individual physical memory evolution times become minimum . such demands may conflict with restrictions on the practical implementability and complexity of the memory . since these questions are beyond the scope of this work , in what follows we restrict ourselves to outline a connection to constrained optimisation as a possible formal approach .    assuming the schrdinger equation @xmath17 @xmath7 @xmath18 and a fixed energy - type constraint @xmath473 @xmath7 @xmath474 @xmath475 @xmath476 , the length of a curve in the space of unitaries becomes @xmath477 where t is the arrival ( or protocol ) time , @xmath478 @xmath7 @xmath416 @xcite .",
    "( a `` protocol '' refers to a prescription for the time dependence of @xmath479 , @xmath85 , or @xmath0 . )",
    "in addition to ( or instead of ) the protocol time @xmath480 , we may also consider @xmath481 as a measure of the complexity of the protocol that integrates the changes that have to be made on @xmath85 via the control fields .",
    "if the optimization problem comprises two objectives such as minimising a distance @xmath260 or maximising a fidelity @xmath259 with minimum amount of ( [ l ] ) or ( [ c ] ) , then an _ approximate _ approach consists in first finding an @xmath385 that optimizes an objective function @xmath482 under a fixed constraint @xmath483 . here , @xmath482 represents @xmath260 or @xmath259 , while @xmath483 may represent @xmath119 or @xmath484 .",
    "this can be formulated as an euler - lagrange equation @xmath485 where the lagrange multiplier @xmath115 must finally be substituted with the given constant such as @xmath119 or @xmath484 .",
    "this optimisation is then repeated with stepwise decreased @xmath119 or @xmath484 , until the deterioration of the achievable @xmath482 exceeds a certain threshold .",
    "equivalently , ( [ ele ] ) may also be thought of optimizing @xmath483 under the constraint of constant @xmath482 .",
    "( [ ele ] ) , which contains both derivatives in a symmetric way , merely states the linear dependence of the functional derivatives at an extremal point @xmath479 in the function space @xmath486 .    in the discrete case , the time integrals in eqs .",
    "( [ l ] ) and ( [ c ] ) reduce to sums over time intervals with constant @xmath85 , and in ( [ c ] ) we assume that each jump of @xmath85 gives a fixed finite contribution . to gain some intuition into the meaning of @xmath484",
    ", we may think of navigating through a classical rectangular grid .",
    "there is a set of shortest paths connecting the diagonal corners , but they are not equivalent with respect to the number of turns the navigator has to make along its way . in the quantum context , the number of switches equals the number of intervals with constant @xmath85 , which may be thought of `` gates '' .",
    "in contrast to an analytical design of quantum circuits , the circuit is here generated numerically , however . since each switch to a different @xmath85 changes the instantaneous eigenbasis",
    ", we may thus think rather of `` layers '' drawing an analogy to classical artificial neural networks @xcite .",
    "40ifxundefined [ 1 ] ifx#1 ifnum [ 1 ] # 1firstoftwo secondoftwo ifx [ 1 ] # 1firstoftwo secondoftwo `` `` # 1''''@noop [ 0]secondoftwosanitize@url [ 0 ]",
    " + 12$12  & 12#1212_12%12@startlink[1]@endlink[0]@bib@innerbibempty @noop _ _ ,  vol .",
    "( ,  ) @noop _ _ , vol .",
    "( ,  ) @noop _ _  ( , ,  ) @noop _ _  ( ,  ,  ) @noop * * ,   ( ) @noop * * ,   ( ) @noop _ _  ( ,  ,  ) @noop _ _  ( ,  ,  ) @noop * * ,   ( ) @noop `` , ''   ( ) ,   @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop _ _  ( , ,  ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop _ _  ( ,  ,  ) @noop _ _  ( ,  ,  ) @noop * * ,   ( ) @noop `` , ''   ( ) ,   @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop ( ) ,   @noop * * ,   ( ) in  @noop _ _  ( ,  )  pp .   @noop * * ,   ( ) @noop * * ,   ( ) @noop * * ,   ( ) _ _ ,  @noop master s thesis ,  , ( ) @noop * * ,   ( ) @noop * * ,   ( ) @noop _ _  ( , ,  )"
  ],
  "abstract_text": [
    "<S> we consider a general class of models , where a reinforcement learning ( rl ) agent learns from cyclic interactions with an external environment via classical signals . </S>",
    "<S> perceptual inputs are encoded as quantum states , which are subsequently transformed by a quantum channel representing the agent s memory , while the outcomes of measurements performed at the channel s output determine the agent s actions . </S>",
    "<S> the learning takes place via stepwise modifications of the channel properties . </S>",
    "<S> they are described by an update rule that is inspired by the projective simulation ( ps ) model and equipped with a glow mechanism that allows for a backpropagation of policy changes , analogous to the eligibility traces in rl and edge glow in ps . in this way </S>",
    "<S> , the model combines features of ps with the ability for generalization , offered by its physical embodiment as a quantum system . </S>",
    "<S> we apply the agent to various setups of an invasion game and a grid world , which serve as elementary model tasks allowing a direct comparison with a basic classical ps agent . </S>"
  ]
}