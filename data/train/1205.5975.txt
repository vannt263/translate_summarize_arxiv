{
  "article_text": [
    "in the past 30 years , the development of linear algebra libraries has been tremendously successful , resulting in a variety of reliable and efficient computational kernels .",
    "unfortunately these kernels are limited by a rigid interface that does not allow users to pass knowledge specific to the target problem .",
    "if available , such knowledge may lead to domain - specific algorithms that attain higher performance than any traditional library  @xcite .",
    "the difficulty does not lay so much in creating flexible interfaces , but in developing algorithms capable of taking advantage of the extra information .    in this paper , we present preliminary work on a linear algebra compiler , written in mathematica , that automatically exploits application - specific knowledge to generate high - performance algorithms .",
    "the compiler takes as input a target equation and information on the structure and properties of the operands , and returns as output algorithms that exploit the given information .",
    "in the same way that a traditional compiler breaks the program into assembly instructions directly supported by the processor , attempting different types of optimization , our linear algebra compiler breaks a target operation down to library - supported kernels , and generates not one but a family of viable algorithms .",
    "the decomposition process undergone by our compiler closely replicates the thinking process of a human expert .",
    "we show the potential of the compiler by means of a challenging operation arising in computational biology : the _ genome - wide association study _ ( gwas ) , an ubiquitous tool in the fields of genomics and medical genetics  @xcite . as part of gwas",
    ", one has to solve the following equation @xmath0 where @xmath1 , @xmath2 , and @xmath3 are known quantities , and @xmath4 is sought after .",
    "the size and properties of the operands are as follows : @xmath5 , @xmath6 is full rank , @xmath7 is symmetric positive definite ( spd ) , @xmath8 , @xmath9 , and @xmath10 ; @xmath11 , @xmath12 , @xmath13 , and @xmath14 is either @xmath15 or of the order of @xmath16 .    at the core of gwas",
    "lays a linear regression analysis with non - independent outcomes , carried out through the solution of a two - dimensional sequence of the generalized least - squares problem ( gls ) @xmath17 while gls may be directly solved , for instance , by matlab , or may be reduced to a form accepted by lapack  @xcite , none of these solutions can exploit the specific structure pertaining to gwas . the nature of the problem , a sequence of correlated glss , allows multiple ways to reuse computation . also , different sizes of the input operands demand different algorithms to attain high performance in all possible scenarios .",
    "the application of our compiler to gwas , eq .  [ eq : probdef ] , results in the automatic generation of dozens of algorithms , many of which outperform the current state of the art by a factor of four or more .",
    "the paper is organized as follows .",
    "related work is briefly described in section  [ sec : related ] .",
    "sections  [ sec : principles ]  and  [ sec : system - overview ] uncover the principles and mechanisms upon which the compiler is built . in section",
    "[ sec : generation - algs ] we carefully detail the automatic generation of multiple algorithms , and outline the code generation process . in section  [ sec : performance ]",
    "we report on the performance of the generated algorithms through numerical experiments .",
    "we draw conclusions in section  [ sec : conclusions ] .",
    "a number of research projects concentrate their efforts on domain - specific languages and compilers . among them , the spiral project  @xcite and the tensor contraction engine ( tce )  @xcite , focused on signal processing transforms and tensor contractions , respectively .",
    "as described throughout this paper , the main difference between our approach and spiral is the inference of properties .",
    "centered on general dense linear algebra operations , one of the goals of the flame project is the systematic generation of algorithms .",
    "the flame methodology , based on the partitioning of the operands and the automatic identification of loop - invariants  @xcite , has been successfully applied to a number of operations , originating hundreds of high - performance algorithms .",
    "the approach described in this paper is orthogonal to flame .",
    "no partitioning of the operands takes place . instead , the main idea is the mapping of operations onto high - performance kernels from available libraries , such as blas  @xcite and lapack .",
    "in this section we expose the human thinking process behind the generation of algorithms for a broad range of linear algebra equations . as an example",
    ", we derive an algorithm for the solution of the gls problem , eq .",
    "[ eq : fgls ] , as it would be done by an expert .",
    "together with the derivation , we describe the rationale for every step of the algorithm .",
    "the exposed rationale highlights the key ideas on top of which we founded the design of our compiler",
    ".    given eq .",
    "[ eq : fgls ] , the * first concern is the inverse operator * applied to the expression @xmath18 .",
    "since @xmath19 is not square , the inverse can not be distributed over the product and the expression needs to be processed first .",
    "the attention falls then on @xmath20 .",
    "the inversion of a matrix is costly and not recommended for numerical reasons ; therefore , since @xmath21 is a general matrix , we * factor * it . given the structure of @xmath21 ( spd ) , we choose a cholesky factorization , resulting in @xmath22 where @xmath23 is square and lower triangular .",
    "as @xmath23 is square , the inverse may now be distributed over the product @xmath24 , yielding @xmath25 .",
    "next , we process @xmath26 ; we observe that the quantity @xmath27 * appears multiple times * , and may be computed and reused to * save computation * : @xmath28 at this point , since @xmath29 is not square and the inverse can not be distributed , there are two * alternatives * : 1 ) multiply out @xmath30 ; or 2 ) factor @xmath29 , for instance through a qr factorization . in this example , we choose the former : @xmath31 one can prove that @xmath32 is spd , suggesting yet another factorization .",
    "we choose a cholesky factorization and distribute the inverse over the product : @xmath33 now that all the remaining inverses are applied to triangular matrices , we are left with a series of products to compute the final result . since all operands are matrices except the vector @xmath34 , we compute eq .  [",
    "eq : exalg1step4 ] from right to left to * minimize the number of flops*. the final algorithm is shown in alg .",
    "[ alg : exalg - chol ] , together with the names of the corresponding blas and lapack building blocks .    ....",
    "$ l l^t = m$                 ( ! \\sc potrf ! )    $ w : = l^{-1 } x$                 ( ! \\sc trsm ! )    $ s : = w^t w$                 ( ! \\sc syrk ! )    $ g g^t = s$                 ( ! \\sc potrf ! )    $ y : = l^{-1 } y$                 ( ! \\sc trsv ! )    $ b : = w^t y$                 ( ! \\sc gemv ! )    $ b : = g^{-1 } b$                 ( ! \\sc trsv ! )    $ b : = g^{-t } b$                 ( ! \\sc trsv ! ) ....    three ideas stand out as the guiding principles for the thinking process :    * the first concern is to deal , whenever it is not applied to diagonal or triangular matrices , with the inverse operator .",
    "two scenarios may arise : a ) it is applied to a single operand , @xmath35 . in this case",
    "the operand is factored with a suitable factorization according to its structure ; b ) the inverse is applied to an expression .",
    "this case is handled by either computing the expression and reducing it to the first case , or factoring one of the matrices and analyzing the resulting scenario .",
    "* when decomposing the equation , we give priority to a ) common segments , i.e. , common subexpressions , and b ) segments that minimize the number of flops ; this way we reduce the amount of computation performed . *",
    "if multiple alternatives leading to viable algorithms arise , we explore all of them .",
    "our compiler follows the above guiding principles to closely replicate the thinking process of a human expert . to support the application of these principles , the compiler incorporates a number of modules ranging from basic matrix algebra support to analysis of dependencies , including the identification of building blocks offered by available libraries . in the following ,",
    "we describe the core modules .",
    "matrix algebra : :    the compiler is written using mathematica from scratch .",
    "we implement    our own operators : addition ( plus ) , negation ( minus ) , multiplication    ( times ) , inversion ( inv ) , and transposition ( trans ) .",
    "together with the    operators , we define their precedence and properties , as    commutativity , to support matrices as well as vectors and scalars .",
    "we    also define a set of rewrite rules according to matrix algebra    properties to freely manipulate expressions and simplify them ,    allowing the compiler to work on multiple equivalent representations .",
    "inference of properties : :    in this module we define the set of supported matrix properties . as of    now : identity , diagonal , triangular , symmetric , symmetric positive    definite , and orthogonal . on top of these properties",
    ", we build an    inference engine that , given the properties of the operands , is able    to infer properties of complex expressions .",
    "this module is extensible    and facilitates incorporating additional properties . building blocks interface : :    this module contains an extensive list of patterns associated with the    desired building blocks onto which the algorithms will be mapped",
    ". it    also contains the corresponding cost functions to be used to construct    the cost analysis of the generated algorithms . as with the properties    module ,",
    "if a new library is to be used , the list of accepted building    blocks can be easily extended .",
    "analysis of dependencies : :    when considering a sequence of problems , as in gwas , this module    analyzes the dependencies among operations and between operations and    the dimensions of the sequence . through this analysis , the compiler    rearranges the operations in the algorithm , reducing redundant    computations .",
    "code generation : :    in addition to the automatic generation of algorithms , the compiler    includes a module to translate such algorithms into code .",
    "so far , we    support the generation of matlab code for one instance as well as    sequences of problems .    to complete the overview of our compiler",
    ", we provide a high - level description of the compiler s _ reasoning_. the main idea is to build a tree in which the root node contains the initial target equation ; each edge is labeled with a building block ; and each node contains intermediate equations yet to be mapped .",
    "the compiler progresses in a breadth - first fashion until all leaf nodes contain an expression directly mapped onto a building block .",
    "while processing a node s equation , the search space is constrained according to the following criteria :    1 .",
    "if the expression contains an inverse applied to a single ( non - diagonal , non - triangular ) matrix , for instance @xmath20 , then the compiler identifies a set of viable factorizations for @xmath21 based on its properties and structure ; 2 .",
    "if the expression contains an inverse applied to a sub - expression , for instance @xmath36 , then the compiler identifies both viable factorizations for the operands in the sub - expression ( e.g. , @xmath37 ) , and segments of the sub - expression that are directly mapped onto a building block ( e.g. , @xmath38 ) ; 3 .   if the expression contains no inverse to process ( as in @xmath39 , with @xmath40 and @xmath23 triangular )",
    ", then the compiler identifies segments with a mapping onto a building block .",
    "when inspecting expressions for segments , the compiler gives priority to common segments and segments that minimize the number of flops .",
    "all three cases may yield multiple building blocks . for each building block",
    "either a factorization or a segment both a new edge and a new children node are created",
    ". the edge is labeled with the corresponding building block , and the node contains the new resulting expression .",
    "for instance , the analysis of eq .",
    "[ eq : exalg1step2 ] creates the following sub - tree :        in addition , thanks to the _ inference of properties _ module , for each building block , properties of the output operands are inferred from those of the input operands .",
    "each path from the root node to a leaf represents one algorithm to solve the target equation . by assembling the building blocks attached to each edge in the path ,",
    "the compiler returns a collection of algorithms , one per leaf .",
    "our compiler has been successfully applied to equations such as pseudo - inverses , least - squares - like problems , and the automatic differentiation of blas and lapack operations .",
    "of special interest are the scenarios in which sequences of such problems arise ; for instance , the study case presented in this paper , genome - wide association studies , which consist of a two - dimensional sequence of correlated gls problems .",
    "the compiler is still in its early stages and the code is not yet available for a general release .",
    "however , we include along the paper details on the input and output of the system , as well as screenshots of the actual working prototype .",
    "we detail now the application to gwas of the process described above .",
    "box  [ box : input ] includes the input to the compiler : the target equation along with domain - specific knowledge arising from gwas , e.g , operands shape and properties . as a result ,",
    "dozens of algorithms are automatically generated ; we report on three selected ones .",
    ".... equation = {     equal[b ,        times [           inv[times [                  trans[x ] ,                  inv[plus [ times[h , phi ] , times[plus[1 , minus[h ] ] , i d ] ] ] ,                  x ]            ] ,           trans[x ] ,           inv[plus [ times[h , phi ] , times[plus[1 , minus[h ] ] , i d ] ] ] ,           y         ]     ] } ;    operandproperties = {     { x ,    { `` input '' ,   `` matrix '' , `` fullrank '' } } ,     { y ,    { `` input '' ,   `` vector '' } } ,     { phi , { `` input '' ,   `` matrix '' , `` symmetric '' } } ,     { h ,    { `` input '' ,   `` scalar '' } } ,     { b ,    { `` output '' , `` vector '' } } } ;    expressionproperties = {      inv[plus [ times[h , phi ] , times[plus[1 , minus[h ] ] , i d ] ] ] , `` spd '' } ;    sizeassumptions = { rows[x ] > cols[x ] } ; ....      to ease the reader , we describe the process towards the generation of an algorithm similar to alg .",
    "[ alg : exalg - chol ] .",
    "the starting point is eq .",
    "[ eq : probdef ] .",
    "since @xmath19 is not square , the inverse operator applied to @xmath41 can not be distributed over the product ; thus , the inner - most inverse is @xmath42 .",
    "the inverse is applied to an expression , which is inspected for viable factorizations and segments . among the identified alternatives",
    "are a ) the factorization of the operand @xmath43 according to its properties , and b ) the computation of the expression @xmath44 . here",
    "we concentrate on the second case .",
    "the segment @xmath44 is matched as the scal - add building block ( scaling and addition of matrices ) ; the operation is made explicit and replaced : @xmath45    now , the inner - most inverse is applied to a single operand , @xmath21 , and the compiler decides to factor it using multiple alternatives : cholesky ( @xmath46 ) , qr ( @xmath47 ) , eigendecomposition ( @xmath48 ) , and svd ( @xmath49 ) .",
    "all the alternatives are explored ; we focus now on the cholesky factorization ( potrf routine from lapack ) : @xmath50    after @xmath21 is factored and replaced by @xmath24 , the inference engine propagates a number of properties to @xmath23 based on the properties of @xmath21 and the factorization applied .",
    "concretely , @xmath23 is square , triangular and full - rank .",
    "next , since @xmath23 is triangular , the inner - most inverse to be processed in eq .",
    "[ eq : alg1step1 ] is @xmath51 . in this case",
    "two routes are explored : either factor @xmath19 ( @xmath23 is triangular and does not need further factorization ) , or map a segment of the expression onto a building block .",
    "we consider this second alternative .",
    "the compiler identifies the solution of a triangular system ( trsm routine from blas ) as a common segment appearing three times in eq .",
    "[ eq : alg1step1 ] , makes it explicit , and replaces it : @xmath52 since @xmath23 is square and full - rank , and x is also full - rank , @xmath29 inherits the shape of @xmath19 and is labelled as full - rank .",
    "as @xmath29 is not square , the inverse can not be distributed over the product yet .",
    "therefore , the compiler faces again two alternatives : either factoring @xmath29 or multiplying @xmath30 .",
    "we proceed describing the latter scenario while the former is analyzed in sec .",
    "[ subsec : alg - two ] .",
    "@xmath30 is identified as a building block ( syrk routine of blas ) , and made explicit : @xmath53 the inference engine plays an important role deducing properties of @xmath32 . during the previous steps , the engine has inferred that @xmath29 is full - rank and rows[w ] > cols[w ] ; therefore the following rule states that @xmath29 is spd .",
    "is spd if @xmath54 is full rank and has more rows than columns . ]    .... isspdq [ times [ trans [ a_?isfullrankq ] , a _ ] / ; rows[a ] > cols[a ]    : = true ; ....",
    "this knowledge is now used to determine possible factorizations for @xmath32 .",
    "we concentrate on the cholesky factorization : @xmath55 in eq .",
    "[ eq : alg1step4 ] , all inverses are applied to triangular matrices ; therefore , no more treatment of inverses is needed .",
    "the compiler proceeds with the final decomposition of the remaining series of products .",
    "since at every step the inference engine keeps track of the properties of the operands in the original equation as well as the intermediate temporary quantities , it knows that every operand in eq .",
    "[ eq : alg1step4 ] are matrices except for the vector @xmath34 .",
    "this knowledge is used to give matrix - vector products priority over matrix - matrix products , and eq .",
    "[ eq : alg1step4 ] is decomposed accordingly . in case",
    "the compiler can not find applicable heuristics to lead the decomposition , it explores the multiple viable mappings onto building blocks .",
    "the resulting algorithm , and the corresponding output from mathematica , are assembled in alg .",
    "[ alg : alg - chol ] , chol - gwas .    ....",
    "$ m : = h\\phi + ( 1-h)i$      ( ! \\sc scal - add ! )",
    "$ l l^t = m$                 ( ! \\sc potrf ! )    $ w : = l^{-1 } x$                 ( ! \\sc trsm ! )    $ s : = w^t w$                 ( ! \\sc syrk ! )    $ g g^t = s$                 ( ! \\sc potrf ! )    $ y : = l^{-1 } y$                 ( ! \\sc trsv ! )    $ b : = w^t y$                 ( ! \\sc gemv ! )    $ b : = g^{-1 } b$                 ( !",
    "\\sc trsv ! )    $ b : = g^{-t } b$                 ( ! \\sc trsv ! ) ....          in this subsection",
    "we display the capability of the compiler to analyze alternative paths , leading to multiple viable algorithms . at the same time",
    ", we expose more examples of algebraic manipulation carried out by the compiler .",
    "the presented algorithm results from the alternative path arising in eq .",
    "[ eq : alg1step3 ] , the factorization of @xmath29 .",
    "since @xmath29 is a full - rank column panel , the compiler analyzes the scenario where @xmath29 is factored using a qr factorization ( geqrf routine in lapack ) : @xmath56 at this point , the compiler exploits the capabilities of the _ matrix algebra _ module to perform a series of simplifications : @xmath57 first , it distributes the transpose operator over the product .",
    "then , it applies the rule    ....          times [ trans [ q_?isorthonormalq , q _ ] - > i d , ....    included as part of the knowledge - base of the module .",
    "the rule states that the product @xmath58 , when @xmath59 is orthogonal with normalized columns , may be rewritten ( - > ) as the identity matrix .",
    "next , since @xmath60 is square , the inverse is distributed over the product .",
    "more mathematical knowledge allows the compiler to rewrite the product @xmath61 as the identity .    in eq .",
    "[ eq : alg2step4 ] , the compiler does not need to process any more inverses ; hence , the last step is to decompose the remaining computation into a sequence of products .",
    "once more , @xmath34 is the only non - matrix operand .",
    "accordingly , the compiler decomposes the equation from right to left .",
    "the final algorithm is put together in alg .",
    "[ alg : alg - qr ] , qr - gwas .    ....",
    "$ m : = h\\phi + ( 1-h)i$    ( ! \\sc scal - add ! )",
    "$ l l^t$ = $ m$              ( ! { \\sc potrf } ! )    $ w$ : = $ l^{-1 } x$             ( ! { \\sc trsm } ! )    $ q r = w$                 ( ! { \\sc geqrf } ! )    $ y$ : = $ l^{-1 } y$             ( ! { \\sc trsv } ! )    $ b$ : = $ q^t y$             ( ! { \\sc gemv } ! )    $ b$ : = $ r^{-1 } b$             ( ! { \\sc trsv } ! ) ....          this third algorithm exploits further knowledge from gwas , concretely the structure of @xmath21 , in a manner that may be overlooked even by human experts .",
    "again , the starting point is eq .",
    "[ eq : probdef ] .",
    "the inner - most inverse is @xmath62 . instead of multiplying out the expression within the inverse operator",
    ", we now describe the alternative path also explored by the compiler : factoring one of the matrices in the expression .",
    "we concentrate in the case where an eigendecomposition of @xmath43 ( syevd or syevr from lapack ) is chosen : @xmath63 where @xmath64 is a square , orthogonal matrix with normalized columns , and @xmath29 is a square , diagonal matrix .    in this scenario ,",
    "the _ matrix algebra _",
    "module is essential ; it allows the compiler to work with alternative representations of eq .",
    "[ eq : alg3step1 ] .",
    "we already illustrated an example where the product @xmath58 , @xmath59 orthonormal , is replaced with the identity matrix .",
    "the freedom gained when defining its own operators , allows the compiler to perform also the opposite transformation :    ....         i d - > times [ q , trans [ q ] ] ;         i d - > times [ trans [ q ] , q ] ; ....    to apply these rules , the compiler inspects the expression @xmath65 for orthonormal matrices : @xmath64 is found to be orthonormal and used instead of @xmath59 in the right - hand side of the previous rules .",
    "the resulting expression is @xmath66    the algebraic manipulation capabilities of the compiler lead to the derivation of further multiple equivalent representations of eq .",
    "[ eq : alg3step2 ] .",
    "we recall that , although we focus on a concrete branch of the derivation , the compiler analyzes the many alternatives . in the branch under study ,",
    "the quantities @xmath64 and @xmath67 are grouped on the left- and right - hand sides of the inverse , respectively : @xmath68 then , since both @xmath64 and @xmath69 are square , the inverse is distributed : @xmath70 finally , by means of the rules :    ....          inv [ q_?isorthonormalq ] - > trans [ q ] ;          inv [ trans [ q_?isorthonormalq ] ] - > q ; ....    which state that the inverse of an orthonormal matrix is its transpose , the expression becomes : @xmath71 the resulting equation is @xmath72    the inner - most inverse in eq .",
    "[ eq : alg3step3 ] is applied to a diagonal object ( @xmath29 is diagonal and @xmath73 a scalar ) .",
    "no more factorizations are needed , @xmath69 is identified as a scal - add building block , and exposed : @xmath74    @xmath75 is a diagonal matrix ; hence only the inverse applied to @xmath76 remains to be processed . among the alternative steps",
    ", we consider the mapping of the common segment @xmath77 , that appears three times , onto the gemm building block ( matrix - matrix product ) : @xmath78 from this point on , the compiler proceeds as shown for the previous examples , and obtains , among others , alg .",
    "[ alg : alg - eigen ] , eig - gwas .    ....",
    "$ z w z^t$ = $ \\phi$              ( ! { \\sc syevx } ! )    $ d : = h w + ( 1 - h ) i$                ( ! { \\sc add - scal } ! )    $ k : = x^t z$                 ( ! { \\sc gemm } ! )    $ v : = k d^{-1}$                 ( ! { \\sc scal } ! )    $ s$ : = $ v k^t$             ( ! { \\sc gemm } ! )    $ q r$ = $ s$              ( ! { \\sc geqrf } ! )    $ y : = z^t y$                 ( ! { \\sc gemv } ! )",
    "$ b : = v y$                 ( ! { \\sc gemv } ! )    $ b : = q^t b$                 ( ! { \\sc gemv } ! )    $ b : = r^{-1 } b$                 ( ! { \\sc trsv } ! ) ....        at first sight , alg .",
    "[ alg : alg - eigen ] might seem to be a suboptimal approach .",
    "however , as we show in sec .",
    "[ sec : performance ] , it is representative of a family of algorithms that play a crucial role when solving a certain sequence of gls problems within gwas .      we have illustrated how our compiler , closely replicating the reasoning of a human expert , automatically generates algorithms for the solution of a single gls problem . as shown in eq .",
    "[ eq : probdef ] , in practice one has to solve one - dimensional ( @xmath79 ) or two - dimensional ( @xmath80 ) sequences of such problems . in this context",
    "we have developed a module that performs a loop dependence analysis to identify loop - independent operations and reduce redundant computations . for space reasons",
    ", we do not further describe the module , and limit to the automatically generated cost analysis .",
    "the list of patterns for the identification of building blocks included in the _ building blocks interface _ module also incorporates the corresponding computational cost associated to the operations .",
    "given a generated algorithm , the compiler composes the cost of the algorithm by combining the number of floating point operations performed by the individual building blocks , taking into account the loops over the problem dimensions .",
    "table  [ tab : cost ] includes the cost of the three presented algorithms , which attained the lowest complexities for one- and two - dimensional sequences . while qr - gwas and chol - gwas share the same cost for both types of sequences , suggesting a very similar behavior in practice , the cost of eig - gwas differs in both cases .",
    "for the one - dimensional sequence the cost of eig - gwas is not only greater in theory , the practical constants associated to its terms increase the gap . on the contrary , for the two - dimensional sequence , the cost of eig - gwas is lower than the cost of the other two .",
    "this analysis suggests that qr - gwas and chol - gwas are better suited for the one - dimensional case , while eig - gwas is better suited for the two - dimensional one . in sec .",
    "[ sec : performance ] we confirm these predictions through experimental results .    [ cols=\"<,^,^,^\",options=\"header \" , ]      the translation from algorithms to code is not a straightforward task ; in fact , when manually performed , it is tedious and error prone . to overcome this difficulty",
    ", we incorporate in our compiler a module for the automatic generation of code . as of now",
    ", we support matlab ; an extension to fortran , a much more challenging target language , is planned .",
    "we provide here a short overview of this module .",
    "given an algorithm as derived by the compiler , the code generator builds an _ abstract syntax tree _ ( ast ) mirroring the structure of the algorithm .",
    "then , for each node in the ast , the module generates the corresponding code statements .",
    "specifically , for the nodes corresponding to _ for _ loops , the module not only generates a for statement but also the specific statements to extract subparts of the operands according to their dimensionality ; as for the nodes representing the building blocks , the generator must map the operation to the specific matlab routine or matrix expression . as an example of automatically generated code , the matlab routine corresponding to the aforementioned eig - gwas algorithm for a two - dimensional sequence",
    "is illustrated in fig .",
    "[ fig : eig - code ] .",
    "we turn now the attention to numerical results . in the experiments ,",
    "we compare the algorithms automatically generated by our compiler with lapack and genabel  @xcite , a widely used package for gwas - like problems . for details on genabel",
    "s algorithm for gwas , gwfgls , we refer the reader to  @xcite .",
    "we present results for the two most representative scenarios in gwas : one - dimensional ( @xmath81 ) , and two - dimensional ( @xmath82 ) sequences of gls problems .",
    "the experiments were performed on an 12-core intel xeon x5675 processor running at 3.06 ghz , with 96 gb of memory .",
    "the algorithms were implemented in c , and linked to the multi - threaded gotoblas and the reference lapack libraries .",
    "the experiments were executed using 12 threads .",
    "we first study the scenario @xmath81 .",
    "we compare the performance of qr - gwas and chol - gwas , with genabel s gwfgls , and gels - gwas , based on lapack s gels routine .",
    "the results are displayed in fig .",
    "[ fig : oney ] .",
    "as expected , qr - gwas and chol - gwas attain the same performance and overlap .",
    "most interestingly , our algorithms clearly outperform gels - gwas and gwfgls , obtaining speedups of 4 and 8 , respectively .    , @xmath83 , @xmath81 .",
    "the improvement in the performance of our algorithms is due to a careful exploitation of both the properties of the operands and the sequence of gls problems . ]",
    "next , we present an even more interesting result . the current approach of all state - of - the - art libraries to the case @xmath82 is to repeat the experiment @xmath14 times with the same algorithm used for @xmath79 . on the contrary",
    ", our compiler generates the algorithm eig - gwas , which particularly suits such scenario . as fig .",
    "[ fig : manyy ] illustrates , eig - gwas outperforms the best algorithm for the case @xmath79 , chol - gwas , by a factor of 4 , and therefore outperforms gels - gwas and gwfgls by a factor of 16 and 32 respectively .    , @xmath83 , @xmath84 .",
    "chol - gwas is best suited for the scenario @xmath79 , while eig - gwas is best suited for the scenario @xmath85 . ]",
    "the results remark two significant facts : 1 ) the exploitation of domain - specific knowledge may lead to improvements in state - of - the - art algorithms ; and 2 ) the library user may benefit from the existence of multiple algorithms , each matching a given scenario better than the others . in the case of gwas",
    "our compiler achieves both , enabling computational biologists to target larger experiments while reducing the execution time .",
    "we presented a linear algebra compiler that automatically exploits domain - specific knowledge to generate high - performance algorithms .",
    "our linear algebra compiler mimics the reasoning of a human expert to , similar to a traditional compiler , decompose a target equation into a sequence of library - supported building blocks .",
    "the compiler builds on a number of modules to support the replication of human reasoning . among them ,",
    "the _ matrix algebra _",
    "module , which enables the compiler to freely manipulate and simplify algebraic expressions , and the _ properties inference _ module , which is able to infer properties of complex expressions from the properties of the operands .",
    "the potential of the compiler is shown by means of its application to the challenging _ genome - wide association study _ equation .",
    "several of the dozens of algorithms produced by our compiler , when compared to state - of - the - art ones , obtain n - fold speedups .    as future work",
    "we plan an extension to the _ code generation _ module to support fortran .",
    "also , the asymptotic operation count is only a preliminary approach to estimate the performance of the generated algorithms . there is the need for a more robust metric to suggest a `` best '' algorithm for a given scenario .",
    "the authors gratefully acknowledge the support received from the deutsche forschungsgemeinschaft ( german research association ) through grant gsc 111 .",
    "bientinesi , p. , eijkhout , v. , kim , k. , kurtz , j. , van  de geijn , r. : sparse direct factorizations through unassembled hyper - matrices .",
    "computer methods in applied mechanics and engineering * 199 * ( 2010 ) 430438          anderson , e. , bai , z. , bischof , c. , blackford , s. , demmel , j. , dongarra , j. , du  croz , j. , greenbaum , a. , hammarling , s. , mckenney , a. , sorensen , d. : users guide .",
    "third edn .",
    "society for industrial and applied mathematics , philadelphia , pa ( 1999 )    pschel , m. , moura , j.m.f . ,",
    "johnson , j. , padua , d. , veloso , m. , singer , b. , xiong , j. , franchetti , f. , gacic , a. , voronenko , y. , chen , k. , johnson , r.w .",
    ", rizzolo , n. : : code generation for dsp transforms . proceedings of the ieee , special issue on `` program generation , optimization , and adaptation '' * 93*(2 ) ( 2005 ) 232 275    baumgartner , g. , auer , a. , bernholdt , d.e . , bibireata , a. , choppella , v. , cociorva , d. , gao , x. , harrison , r.j . , hirata , s. , krishnamoorthy , s. , krishnan , s. , chung lam , c. , lu , q. , nooijen , m. , pitzer , r.m . ,",
    "ramanujam , j. , sadayappan , p. , sibiryakov , a. , bernholdt , d.e . , bibireata , a. , cociorva , d. , gao , x. , krishnamoorthy , s. , krishnan , s. : synthesis of high - performance parallel programs for a class of ab initio quantum chemistry models . in : proceedings of the ieee .",
    "( 2005 ) 2005    fabregat - traver , d. , bientinesi , p. : knowledge - based automatic generation of partitioned matrix expressions . in gerdt , v. , koepf , w. , mayr , e. , vorozhtsov , e. ,",
    "eds . : computer algebra in scientific computing .",
    "volume 6885 of lecture notes in computer science . ,",
    "springer berlin / heidelberg ( 2011 ) 144157    fabregat - traver , d. , bientinesi , p. : automatic generation of loop - invariants for matrix operations . in : computational science and its applications , international conference , los alamitos , ca , usa , ieee computer society ( 2011 ) 8292        fabregat - traver , d. , aulchenko , y.s . ,",
    "bientinesi , p. : fast and scalable algorithms for genome studies . technical report , aachen institute for advanced study in computational engineering science ( 2012 ) available at http://www.aices.rwth-aachen.de:8080/aices/preprint/documents/aices-2012-05-01.pdf ."
  ],
  "abstract_text": [
    "<S> we present a prototypical linear algebra compiler that automatically exploits domain - specific knowledge to generate high - performance algorithms . </S>",
    "<S> the input to the compiler is a target equation together with knowledge of both the structure of the problem and the properties of the operands . </S>",
    "<S> the output is a variety of high - performance algorithms , and the corresponding source code , to solve the target equation . </S>",
    "<S> our approach consists in the decomposition of the input equation into a sequence of library - supported kernels . since in general </S>",
    "<S> such a decomposition is not unique , our compiler returns not one but a number of algorithms . </S>",
    "<S> the potential of the compiler is shown by means of its application to a challenging equation arising within the _ genome - wide association study_. as a result , the compiler produces multiple `` best '' algorithms that outperform the best existing libraries . </S>"
  ]
}