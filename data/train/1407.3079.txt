{
  "article_text": [
    "covariance estimation has been a long existing problem in various signal processing related fields , including multiantenna communication systems , social networks , bioinformatics , as well as financial engineering .",
    "a well known and easy to implement estimator is the sample covariance matrix . under the assumption of clean samples ,",
    "the estimator is consistent by the law of large numbers .",
    "however , the performance of the sample covariance matrix is vulnerable to data corrupted by noise and outliers , which is often the case in real - world applications .    as a remedy ,",
    "robust estimators are proposed aimed at limiting the influence of erroneous observations so as to achieve better performance in non - gaussian scenarios @xcite .",
    "recently , tyler s scatter estimator @xcite has received considerable attention both theoretically and practically in signal processing related fields , e.g. , @xcite to name a few , see @xcite for a comprehensive overview .",
    "tyler s estimator estimates the normalized scatter matrix ( equivalently the normalized covariance matrix if the covariance exists ) assuming that the underlying distribution is elliptically symmetric .",
    "the estimator is shown to enjoy the following advantages against the others : it is distribution - free in the sense that its asymptotic variance does not depend on the parametric form of the underlying distribution , and it is also the most robust estimator in a min - max sense .",
    "in addition to non - gaussian observations , another problem we face in practice is the `` small n large p '' problem , which refers to high dimensional statistical inference with insufficient number of samples .",
    "it is obvious that the sample covariance matrix is singular when the number of samples is smaller than the dimension , and tyler s estimator has the same drawback . in order to handle this problem",
    ", @xcite borrowed the diagonal loading idea @xcite and proposed a regularized tyler s estimator that shrinks towards identity . a rigorous proof for the existence , uniqueness , and convergence properties is provided in @xcite , where a systematic way of choosing the regularization parameter was also proposed",
    "however , the estimator is criticized for not being derived from a meaningful cost function . to overcome this issue , a new scale - invariant shrinkage tyler s estimator , defined as a minimizer of a penalized cost function ,",
    "was recently proposed in @xcite . by showing that the objective function is geodesic convex",
    ", wiesel proved that any algorithm that converges to the local minimum of the objective function is actually the global minimum .",
    "numerical algorithms are provided for the estimator and simulation results demonstrate the estimator is robust and effective in the sample deficient scenario . despite the good properties , the existence and uniqueness properties of the estimator",
    "remains unclear .    in this paper , we study the shrinkage tyler s estimator and try to answer the unsolved problems mentioned above .",
    "first , we give a proof that states the sufficient condition for the existence of shrinkage tyler s estimator with penalized cost function taking a general form .",
    "second , we propose a kullback - leibler divergence ( kl divergence ) penalized cost function that results in a shrinkage tyler s estimator similar to the heuristic diagonal loading one considered in @xcite .",
    "we then move to these two specific estimators and show that under the condition @xmath0 and the shrinkage target matrix being positive definite , the estimators exist , where @xmath1 is the number of samples , @xmath2 is the dimension of the samples and @xmath3 controls the amount of penalty added to the cost function , @xmath4 stands for the proportion of samples contained in a proper subspace @xmath5 . in addition , we prove it is also a necessary condition , provided that @xmath6 . although derived from different cost functions , and also with different estimation equation , we prove that the two shrinkage estimators are actually equivalent . under the assumption that the underlying distribution is continuous , the condition simplifies to @xmath7 . comparing with the existence condition for tyler s estimator , which is @xmath8 , or @xmath9 under continuity assumption",
    ", this result clearly demonstrates that regularization can relax the requirement on the number of samples , hence shows its capability of handling large dimension estimation problems .",
    "algorithms for the shrinkage estimators based on majorization - minimization framework are provided , where the convergence can be analyzed systematically .",
    "it is worth mentioning that in the work @xcite , where the same condition @xmath7 is also independently derived for the kl penalty based shrinkage estimator that shrinks the covariance matrix to identity in the complex field , assuming the samples are linearly independent .",
    "@xcite refutes the additional trace normalization step in @xcite by showing that the trace of the inverse of the estimator is equal to @xmath2 , and propose dropping the normalization step .",
    "different from that approach , our work gives an interpretation of the estimator as the minimizer of a kl divergence penalized cost function .",
    "starting from the cost function , we establish the existence condition with a different proof from @xcite .",
    "in addition , we extend the result ( in the real field ) , since the condition @xmath0 implies @xmath7 if the samples are linearly independent , and we consider a general positive definite shrinkage target matrix as in @xcite .",
    "the paper is organized as follows : in section ii , we briefly review tyler s estimator for samples drawn from the elliptical family . in section iii ,",
    "the two types of shrinkage estimators , i.e. , one proposed in @xcite and another derived based on kl divergence are considered , and a rigorous proof for the existence and uniqueness of the estimators is provided .",
    "algorithms based on majorization - minimization are presented in section iv .",
    "numerical examples follow in section v , and we conclude in section vi .",
    "@xmath10 stands for @xmath11-dimensional real - valued vector space , @xmath12 stands for vector frobenius norm .",
    "@xmath13 stands for symmetric positive semidefinite @xmath14 matrices , which is a closed cone in @xmath15 , @xmath16 denotes symmetric positive definite @xmath14 matrices . @xmath17 and @xmath18 stand for the largest and smallest eigenvalue of a matrix @xmath19 respectively . @xmath20 and @xmath21 stand for matrix determinant and trace respectively .",
    "@xmath22 is the matrix frobenius norm .",
    "the boundary of the open set @xmath16 is conventionally defined as @xmath23 , which contains all rank deficient matrices in @xmath13 . with a slightly abuse of notation ,",
    "we also include matrices with all eigenvalues @xmath24 into the boundary of @xmath16 . therefore a sequence of matrices @xmath25 converges to the boundary of @xmath16 iff @xmath26 or @xmath27 . in the rest of the paper , we will use the statement `` @xmath19 converges '' equivalently as `` a sequence of matrices @xmath25 converges '' for notation simplicity .",
    "in this paper , we assume a number @xmath1 of @xmath2-dimensional samples @xmath28 are drawn from an elliptical population distribution with probability density function ( pdf ) of the form @xmath29 with location and scatter parameter @xmath30 in @xmath31 .",
    "the nonnegative function @xmath32 , which is called the density generator , determines the shape of the pdf . in most of the popularly used distributions ,",
    "e.g. , the gaussian and the student s _ t_-distribution , @xmath32 is a decreasing function and determines the decay of the tails of the distribution . given @xmath33 , our problem of interest is to estimate the covariance matrix .",
    "we can always center the pdf by defining @xmath34 , hence without loss of generality in the rest of the paper we assume @xmath35 we use the notation @xmath36 and @xmath37 for the empirical and the population distributions , respectively .",
    "it is known that the covariance matrix of elliptical distribution takes the form @xmath38 with @xmath39 being a constant that depends on @xmath32 @xcite , hence it is unlikely to have a good covariance estimator without prior knowledge of @xmath40 . in this paper , instead of trying to find the parametric form of @xmath40 and get an estimator of @xmath38 , we are interested in estimating the normalized covariance matrix @xmath41 .    the commonly used sample covariance matrix , which also happens to be the maximum likelihood estimator for the normal distribution , estimates @xmath38 asymptotically , however it is sensitive to outliers .",
    "this motivates the research for estimators robust to outliers in the data and , in fact , many researchers in the statistics literature have addressed this problem by proposing various robust covariance estimators like m - estimators @xcite , s - estimators @xcite , mve @xcite , and mcd @xcite to name a few , see @xcite for a complete overview .",
    "for example , in @xcite , maronna analyzed the properties of the m - estimators , which are given as the solution @xmath19 to the equation @xmath42 where the choice of function @xmath43 determines a whole family of different estimators . under some technical conditions on @xmath44 ( i.e. , @xmath45 for @xmath46 and nonincreasing , and @xmath47 is strictly increasing )",
    ", maronna proved that there exists a unique @xmath19 that solves ( [ eq : maronna scatter ] ) , and gave an iterative algorithm to arrive at that solution .",
    "he also established its consistency and robustness .",
    "a number of well known estimators take the form ( [ eq : maronna scatter ] ) and in @xcite maronna gave two examples , with one being the maximum likelihood estimator for multivariate student s _ t_-distribution , and the other being the huber s estimator @xcite . both of them are popular for handling heavy tails and outliers in the data .    for all the robust covariance estimators ,",
    "there is a tradeoff between their efficiency , which measures the variance ( estimation accuracy ) of the estimator , and robustness , which quantifies the sensitivity of the estimator to outliers . as these two quantities are opposed in nature , a considerable effort has to be put in designing estimators that achieve the right balance between these two quantities . in @xcite ,",
    "tyler dealt with this problem by proposing an estimator that is distribution - free and the `` most robust '' estimator in mini - max sense .",
    "tyler s estimator of @xmath19 is given as the solution of the following equation @xmath48 where the results of @xcite can not be applied since @xmath49 is not strictly increasing .",
    "tyler established the conditions for the existence of a solution to the fixed - point equation ( [ eq : tyler scatter ] ) , as well as the fact that the estimator is unique up to a positive scaling factor , in the sense that @xmath19 solves ( [ eq : tyler scatter ] ) if and only if @xmath50 solves ( [ eq : tyler scatter ] ) for some positive scalar @xmath51 .",
    "the estimator was shown to be strongly consistent and asymptotically normal with its asymptotic standard deviation independent of @xmath40 .",
    "tyler s fixed - point equation ( [ eq : tyler scatter ] ) can be alternatively interpreted as follows .",
    "consider the normalized samples defined as @xmath52 , it is known that the probability distribution of @xmath53 takes the form @xcite @xmath54 given @xmath1 samples from the normalized distribution @xmath55 , the maximum likelihood estimator of @xmath19 can be obtained by minimizing the negative log - likelihood function @xmath56 which is equivalent to minimizing @xmath57 if a minimum @xmath58 of the function @xmath59 exists , it needs to satisfy the stationary equation given in ( [ eq : tyler scatter ] ) , which was originally derived by tyler in @xcite . in @xcite",
    ", the authors provided the condition for existence of a nonsingular solution to ( [ eq : tyler scatter ] ) based on the following reasoning .",
    "notice that @xmath60 must be nonsingular , and the function @xmath59 is unbounded above on the boundary of positive definite matrices , implies the existence of a minimum .",
    "based on these observations , kent and tyler established the existence conditions by showing @xmath61 on the boundary .",
    "specifically , under the condition that : ( i ) no @xmath62 lies on the origin , and ( ii ) for any proper subspace @xmath63 , @xmath8 , where @xmath64 stands for the proportion of samples in @xmath5 , then a nonsingular minimum of the problem ( [ eq : loss function sigma ] ) exists , which is equivalent to equation ( [ eq : tyler scatter ] ) having a solution . in words , the above mentioned conditions require the number of samples to be sufficiently large , and the samples should be spread out in the whole space .",
    "to arrive at the estimator satisfying ( [ eq : tyler scatter ] ) , tyler proposed the following iterative algorithm : @xmath65 that converges to the unique ( up to a positive scaling factor ) solution of ( [ eq : tyler scatter ] ) .",
    "the robust property of tyler s estimator can be understood intuitively as follows : by normalizing the samples , i.e. , @xmath52 , the magnitude of an outlier is more unlikely to make the estimator break down . in other words ,",
    "the estimator is not sensitive to the magnitude of samples , only their direction can affect the performance .",
    "the regularity conditions for the existence of tyler s estimator leads to a condition on the number of samples that @xmath66 @xcite . in some practical applications",
    "the number of samples is not sufficient , in those cases tyler s iteration ( [ eq : alg - tyler scatter ] ) may not converge . in these scenarios ,",
    "a most sensible approach is to shrink tyler s estimator to some known a priori estimate of @xmath19 . in the literature of robust estimators ,",
    "there exists two different shrinkage based approaches .    in the first approach ,",
    "the authors in @xcite proposed the following estimator : @xmath67 which is a slightly modified version of the original tyler s iteration in ( [ eq : alg - tyler scatter ] ) , with the modification being including an identity matrix in the first step of the iteration that aims at shrinking the estimator towards the identity matrix .",
    "this resembles the idea of regularizing an estimator via diagonal loading @xcite . in @xcite , chen et al . proved the uniqueness of the estimator obtained by the iteration ( [ eq : heuristic diagonal loading ] ) based on concave perron - frobenius theory , and gave a method to choose the regularization weight @xmath3 .",
    "although this estimator is widely used and performs well in practice , it is still considered to be heuristic as it does not have an interpretation based on minimizing a cost function .    as a second approach , in @xcite ,",
    "the author took a different route and derived a new shrinkage - based tyler s estimator that has a clear interpretation based on minimizing the penalized negative log - likelihood function    @xmath68    where @xmath69 is a function with minimum at the desired target matrix @xmath70 , hence it will shrink the solution of ( [ eq : wiesel penalty general objective ] ) towards the target . by showing",
    "the cost function @xmath71 is geodesic convex , the author proved that any local minimum over the set of positive definite matrices is a global minimum @xcite .",
    "he then derived an iterative algorithm based on majorization - minimization that monotonically decreases the cost function at each iteration : @xmath72    even though the author in @xcite showed that the cost function is convex in geodesic space , the existence and uniqueness of the global minimizer remains unknown .",
    "moreover , it is mentioned in @xcite that for some values of @xmath3 the cost function becomes unbounded below and the iterations do not converge .    in this section ,",
    "we address the following points : ( i ) we give the missing interpretation based on minimizing a cost function for the estimator in ( [ eq : heuristic diagonal loading ] ) , and we also prove its existence and uniqueness ; ( ii ) we prove the iteration in ( [ eq : wiesel penality ] ) with an additional trace normalization step converges to a unique point and also establish the conditions on the regularization parameter @xmath3 to ensure the existence of the solution .",
    "for both cases , the cost function takes the form of penalized negative log - likelihood function with different penalizing functions .",
    "our methodology for the proofs hinges on techniques used by tyler in @xcite .",
    "we start with a proof of existence for a minimizer of a general penalized negative log - likelihood function in the following theorem , the proof of existence of the two aforementioned cases @xmath59 and @xmath71 are just special cases of the general result .    the idea of proving the existence is to establish the regularity conditions under which the cost function takes value @xmath73 on the boundary of the set @xmath16 , a minimum then exists by the continuity of the cost function .",
    "the main result is established in theorem [ thm : existence thm ] , and the following lemma is needed .",
    "[ lem : equivalence lem]for any continuous function @xmath74 defined on the set @xmath75 , there exists a @xmath58 such that @xmath76 if @xmath77 on the boundary of the set @xmath75 .",
    "[ def : als]for any continuous function @xmath78 defined on @xmath46 , define the quantities @xmath79 and @xmath80    in this paper we are particularly interested in the functions @xmath81 and @xmath82 with some positive scalar @xmath83 . for @xmath81 , @xmath84 and , for @xmath82 , @xmath85 , @xmath86 .",
    "we restrict our attention to the case @xmath87 .",
    "consider the penalized cost function takes the general form @xmath88 with original cost function @xmath89 where @xmath90 is a continuous function , and the penalty term @xmath91 where @xmath92 measures the difference between @xmath19 and the positive semidefinite matrix @xmath93 .",
    "@xmath94 is , in general , an increasing function that increases the penalty as @xmath19 deviates from @xmath93 , which is considered to be the prior target that we wish to shrink @xmath19 to .",
    "we first give an intuitive argument on the condition that ensures the existence of the estimator .",
    "since the estimator @xmath60 is defined as the minimizer to the penalized loss function , it exists if @xmath95 on the boundary of @xmath16 by lemma [ lem : equivalence lem ] , and clearly @xmath60 is nonsingular .",
    "we infer @xmath19 by the samples @xmath96 , if the samples are concentrated on some subspace , naturally we `` guess '' the distribution is degenerate , i.e. , @xmath60 is singular . therefore , the samples are required to be sufficiently spread out in the whole space so that the inference leads to a nonsingular @xmath60 . under the case when we have a prior information that @xmath19 should be close to the matrix @xmath93 , to ensure @xmath60 being nonsingular we need to distribute more @xmath62 s in the null space of @xmath93 and hence less in the range of @xmath93 . to formalize this intuition , we give the following theorem .",
    "[ thm : existence thm]for cost function @xmath97 defined on positive definite matrices @xmath98 with @xmath90 and @xmath99 being continuous functions , define @xmath100 and @xmath101 for @xmath102 , @xmath103 and @xmath104 for @xmath105 s according to ( [ eq : sill ] ) and ( [ eq : floor ] ) ,",
    "then @xmath95 on the boundary of the set @xmath16 if the following conditions are satisfied :    \\(i ) no @xmath62 lies on the origin ;    \\(ii ) for any proper subspace @xmath106 where sets @xmath107 and @xmath108 are defined as @xmath109 , @xmath110 ;    \\(iii ) @xmath111 and @xmath112 .",
    "see appendix a.    condition ( i ) avoids the scenario when @xmath113 takes value @xmath114 and @xmath115 is undefined at @xmath116 , for example @xmath117 for the log - likelihood function .",
    "the first part in condition ( ii ) , @xmath118 , ensures @xmath95 under the case that some but not all eigenvalues @xmath119 of @xmath19 tend to zero , and the second part in condition ( ii ) , @xmath120 , ensures @xmath95 under the case that some but not all eigenvalues @xmath119 of @xmath19 tend to positive infinity .",
    "together they force @xmath95 when @xmath121 .",
    "the first part of condition ( iii ) ensures @xmath95 when all @xmath24 and the second part ensures @xmath95 when all @xmath122 .",
    "[ cor : checkable condition]assuming the population distribution @xmath74 is continuous , and the matrices @xmath123 are full rank , condition ( ii ) in theorem [ thm : existence thm ] simplifies to : @xmath124    the conclusion follows easily from the following two facts : given that the population distribution @xmath74 is continuous , and no @xmath62 lies on the origin , any @xmath125 sample points define a proper subspace @xmath5 with @xmath126 with probability one ; and since @xmath123 s are full rank , the set @xmath127 .    under the regularity conditions provided in theorem [ thm : existence thm ] , lemma [ lem : equivalence lem ]",
    "implies a minimizer @xmath60 of @xmath128 exists and is positive definite , therefore it needs to satisfy the condition @xmath129 .",
    "we then show how theorem [ thm : existence thm ] works for tyler s estimator defined as the nonsingular minimizer of ( [ eq : loss function sigma ] ) .",
    "notice that the loss function @xmath59 is scale - invariant , we have @xmath130 for any positive definite @xmath131 .",
    "this implies that there are cases when @xmath19 goes to the boundary of @xmath16 and @xmath59 will not go to positive infinity . due to this reason , condition ( iii ) is violated in theorem [ thm : existence thm ] . to handle the scaling issue",
    ", we introduce a trace constraint @xmath132 .    for the tyler s problem of minimizing ( [ eq : loss function sigma ] ) , we seek for the condition that ensures @xmath61 when @xmath19 goes to the boundary of the set @xmath133 relative to @xmath134 .",
    "the condition implies that there is a unique minimizer @xmath60 that minimizes @xmath59 over the set @xmath133 , and by it is equivalent to the existence of a unique ( up to a positive scaling factor ) minimizer @xmath135 that minimizes @xmath59 over the set @xmath16 since @xmath59 is scale - invariant .    the constraint @xmath132 excludes the case that any of @xmath136 and the case all @xmath137 , hence we only need to let @xmath61 under the case that some but not all @xmath137 , which corresponds to the condition @xmath118 in theorem [ thm : existence thm ] . for tyler s cost function @xmath59",
    ", we have @xmath138 and @xmath139 , @xmath140 , therefore theorem [ thm : existence thm ] leads to the condition on the samples : @xmath8 , or @xmath66 if the population distribution @xmath74 is continuous , which reduces to the condition given in @xcite .      in @xcite ,",
    "wiesel proposed a regularization penalty @xmath141 that results in a shrinkage estimator .",
    "specifically , the penalty terms that encourage shrinkage towards an identity matrix and more generally towards an arbitrary prior matrix @xmath70 are defined as follows : @xmath142 as can be seen the penalty terms are scale - invariant .",
    "wiesel justified the choice of the above mentioned penalty functions by showing that the minimizer of the penalty functions would be some scaled multiple of @xmath143 ( or @xmath70 ) . thus adding this penalty terms to the tyler s cost function would yield estimators that are shrunk towards @xmath143 ( or @xmath70 ) . in the rest of this subsection",
    "we consider the general case @xmath144 only , where the penalty term shrinks @xmath19 to scalar multiples of @xmath70 , and we make the assumption that @xmath70 is positive definite , which is reasonable since @xmath19 must be a positive definite matrix .",
    "the cost function is restated below for convenience @xmath145 minimizing @xmath71 gives the fixed - point condition @xmath146    recall that in the absence of regularization ( i.e. , @xmath147 ) , a solution to the fixed - point equation exists under the condition @xmath148 . with the regularization , however , it is not clear .",
    "we start giving a result for the uniqueness and then come back to the existence .",
    "[ thm : unique wiesel scatter]if ( [ eq : eqn wiesel scatter ] ) has a solution , then it is unique up to a positive scaling factor .    it s easy to see if @xmath19 solves ( [ eq : eqn wiesel scatter ] ) , @xmath50 is also a solution for @xmath149 . without loss of generality",
    "assume @xmath150 is a solution , otherwise define @xmath151 and @xmath152 , and that there exists another solution @xmath153 .",
    "denote the eigenvalues of @xmath153 as @xmath154 with at least one strictly inequality , then under the condition that @xmath70 is positive definite @xmath155 where the inequality follows from the fact that @xmath156 for any positive definite matrix @xmath157 and the last equality follows from the assumption that @xmath158 is a solution to ( [ eq : eqn wiesel scatter ] ) .",
    "we have the contradiction @xmath159 , hence all the eigenvalues of @xmath153 should be equal , i.e. , @xmath160 .    before establishing the existence condition , we give an example when the solution to ( [ eq : eqn wiesel scatter ] ) does not exist for illustration .",
    "consider the case when all @xmath62 s are aligned in one direction .",
    "eigendecompose @xmath161 and choose @xmath162 to be aligned with the @xmath62 s , let @xmath163 while others @xmath164 .",
    "ignoring the constant terms , the boundedness of @xmath71 is equivalent to the boundedness of @xmath165 , hence it is unbounded below if @xmath166 .",
    "the example shows that @xmath71 can be unbounded below implying that ( [ eq : eqn wiesel scatter ] ) has no solution if the data are too concentrated and @xmath3 is small .",
    "the following theorems gives the exact tradeoff between data dispersion and the choice of @xmath3 .",
    "[ thm : existence - wiesel]a unique solution to ( [ eq : eqn wiesel scatter ] ) exists ( up to a positive scaling factor ) if the following conditions are satisfied :    \\(i ) no @xmath62 lies on the origin ;    \\(ii ) for any proper subspace @xmath63 , @xmath0 , + and they are the global minima of the loss function ( [ eq : wiesel penalized loss functionl scatter ] ) .",
    "we start by rewriting the function including a the scaling factor @xmath167 w.r.t .",
    "( [ eq : wiesel penalized loss functionl scatter ] ) for convenience : @xmath168 invoke theorem [ thm : existence thm ] with @xmath169 , @xmath170 , @xmath171 and @xmath172 , hence @xmath140 and @xmath173 . by the same reasoning as for the tyler s loss function",
    ", the condition @xmath118 , which is @xmath0 since @xmath70 is full rank , ensures the existence of a unique solution to ( [ eq : eqn wiesel scatter ] ) under the constraint @xmath98 and @xmath132 .",
    "hence a unique ( up to a positive scaling factor ) solution to ( [ eq : eqn wiesel scatter ] ) exists on the set of @xmath16 by the scale - invariant property of @xmath71 .",
    "to make the existence condition checkable , we use corollary [ cor : checkable condition ] , theorem [ thm : existence - wiesel ] then simplifies to @xmath174 or , equivalently @xmath7 , from which we can see that compared to the condition without regularization shrinkage allows less number of samples , and the minimum number depends on @xmath3 .    at last , we show that the condition @xmath0 is also necessary in the following proposition .",
    "[ prop : necessaity wiesel]if ( [ eq : eqn wiesel scatter ] ) admits a solution on @xmath16 , then for any proper subspace @xmath63 , @xmath0 , provided that @xmath70 is positive definite and @xmath6 .    for a proper subspace @xmath5 , define @xmath175 as the orthogonal projection matrix associated to @xmath5 , i.e. , @xmath176 , @xmath177 . assume the solution is @xmath158 .",
    "multiplying both sides of equation ( [ eq : eqn wiesel scatter ] ) by matrix @xmath178 and taking the trace we have @xmath179 if @xmath180 , then @xmath181 , and @xmath182 if @xmath183 .",
    "moreover , @xmath184 since @xmath70 is positive definite .",
    "this therefore implies @xmath185 rearranging the terms yields @xmath0 .",
    "an ideal penalty term should increase as @xmath19 deviates from the prior target @xmath70 .",
    "wiesel s penalty function discussed in the last subsection satisfies this property and , in this subsection , we propose another penalty that has this property .",
    "the penalty that we choose is the kl divergence between @xmath186 and @xmath187 , i.e. , two zero - mean gaussians with covariance matrices @xmath19 and @xmath70 , respectively .",
    "the formula for the kl divergence is as follows @xcite @xmath188 ignoring the constant terms results in the following loss function : @xmath189 with the following fixed - point condition : @xmath190 unlike the penalty function discussed in the last subsection , kl divergence penalty encourages shrinkage towards @xmath70 without scaling ambiguity .",
    "this can be easily seen , as the minimizer for the kl divergence penalty is just @xmath70 .",
    "notice that ( [ eq : kl - scatteronly ] ) is similar to the diagonal loading in ( [ eq : heuristic diagonal loading ] ) , but without the heuristic normalizing step .",
    "[ thm : uniqueness kl scatter]if ( [ eq : kl - scatteronly ] ) has a solution , then it is unique .    without loss of generality , we assume @xmath150 solves ( [ eq : kl - scatteronly ] ) .",
    "assume there is another matrix @xmath153 that solves ( [ eq : kl - scatteronly ] ) , and denote the largest eigenvalue of @xmath153 as @xmath191 and suppose @xmath192 .",
    "we then have the following contradiction : @xmath193 which gives contradiction @xmath159 , hence @xmath194 . similarly ,",
    "suppose the smallest eigenvalue of @xmath153 satisfies @xmath195 . we then have @xmath196 which is a contradiction and hence @xmath197 , from which @xmath198 follows .",
    "[ thm : exist - kl penalty]a unique solution to ( [ eq : kl - scatteronly ] ) exists , if    \\(i ) no @xmath62 lies on the origin ;    \\(ii ) @xmath0 , + and it is the global minimum of loss function ( [ eq : p - kl - scatter ] ) .",
    "equivalently , we can define @xmath199 invoke theorem [ thm : existence thm ] with @xmath169 , @xmath200 , @xmath171 and @xmath172 , hence @xmath140 , @xmath201 , @xmath202 .",
    "since @xmath70 is full rank and @xmath201 , condition ( ii ) reduces to @xmath0 . condition ( iii ) is satisfied , hence an interior minimum exists .",
    "furthermore , it is the unique minimum , hence it is global .",
    "the only difference between the regularized estimator discussed in this subsection and the heuristic estimator in ( [ eq : heuristic diagonal loading ] ) is the extra normalizing step in ( [ eq : heuristic diagonal loading ] ) . with the trace normalization",
    ", @xcite proved that the iteration implied by ( [ eq : heuristic diagonal loading ] ) converges to a unique solution without any assumption of the data .",
    "however , the iteration implied by ( [ eq : kl - scatteronly ] ) , which is based on minimizing a negative log - likelihood function penalized via the kl divergence function , requires some regularity conditions to be satisfied ( cf .",
    "theorem [ thm : exist - kl penalty ] ) . according to corollary [ cor : checkable condition ] , the condition simplifies to @xmath174 if the population distribution is continuous .",
    "if ( [ eq : kl - scatteronly ] ) admits a solution on @xmath16 , then for any proper subspace @xmath63 , @xmath0 , provided that @xmath70 is positive definite and @xmath6 .",
    "multiply both sides of equation ( [ eq : kl - scatteronly ] ) by @xmath203 and define @xmath204 , @xmath205 yields @xmath206 the rest of the proof follows the same token as proposition [ prop : necessaity wiesel ] .    finally , we show in the following proposition that the wiesel s shrinkage estimator defined as solution to ( [ eq : eqn wiesel scatter ] ) and kl shrinkage estimator defined as solution to ( [ eq : kl - scatteronly ] ) are equivalent .    the solution to fixed point equation ( [ eq : kl - scatteronly ] ) solves ( [ eq : eqn wiesel scatter ] ) and , conversely , any solution of ( [ eq : eqn wiesel scatter ] ) solves ( [ eq : kl - scatteronly ] ) with a proper scaling factor .",
    "if @xmath3 is zero , the statement is trivial .",
    "we consider the case @xmath207 .",
    "following the argument of previous proposition we arrive at equation ( [ eq : whitened kl fixed point eqn ] ) .",
    "it has been shown in @xcite that the unique solution @xmath208 to ( [ eq : whitened kl fixed point eqn ] ) satisfies @xmath209 given @xmath6 , hence @xmath210 .",
    "substitute it into equation ( [ eq : eqn wiesel scatter ] ) yields exactly equation ( [ eq : kl - scatteronly ] ) with solution @xmath19 , which indicates @xmath19 solves ( [ eq : eqn wiesel scatter ] ) .",
    "the second part of the proposition follows from the fact that wiesel s fixed - point equation ( [ eq : eqn wiesel scatter ] ) has a unique solution up to a positive scaling factor .",
    "before going the specific algorithms , we first briefly introduce the concepts of majorization - minimization @xcite .",
    "consider the following optimization problem @xmath211 where @xmath74 is assumed to be a continuous function , not necessarily convex , and @xmath212 is a closed convex set .    at a given point @xmath213",
    ", the majorization - minimization algorithm finds a surrogate function @xmath214 that satisfies the following properties : @xmath215 with @xmath216 stands for directional derivative .",
    "the surrogate function @xmath214 is assumed to be continuous in @xmath217 and @xmath213 and @xmath214 are continuously differentiable , then the first two conditions in ( [ eq:-7 ] ) imply the third . ] .",
    "the majorization - minimization algorithm updates @xmath217 as @xmath218 it is proved that every limit point of the sequence @xmath219 converges to a stationary point of problem ( [ eq:-6 ] ) , and under the assumption that the level set @xmath220 is compact , the distance between @xmath219 and the set of stationary points reduces to zero in the limit@xcite .    in the rest of this section , for any continuous differentiable function @xmath221",
    ", we define @xmath222 when @xmath223 .      in @xcite , wiesel derived tyler s iteration ( [ eq : alg - tyler scatter ] ) but without the trace normalization step , from the majorization - minimization perspective , with surrogate function @xmath224 for ( [ eq : loss function sigma ] ) defined as @xmath225 a positive definite stationary point of @xmath224 satisfies the first equation of ( [ eq : alg - tyler scatter ] ) . by the same technique , to solve the problem @xmath226 wiesel derived the iteration ( [ eq : wiesel penality ] ) by majorizing ( [ eq : wiesel penalized loss functionl scatter ] ) with function @xmath227",
    "it is worth pointing out that if we do the change of variable @xmath228 in @xmath71 and linearize the term @xmath229 , this also leads to the same iteration ( [ eq : wiesel penality ] ) .    in the rest of this subsection",
    ", we prove the convergence of the iteration ( [ eq : wiesel penality ] ) proposed by wiesel , but with an additional trace normalization step , i.e. , our modified iteration takes the form : @xmath230 denote the set @xmath231 .    1",
    ".   initialize @xmath131 as an arbitrary positive definite matrix .",
    "do iteration @xmath232 until convergence .",
    "the set @xmath233 is a compact set .",
    "@xmath132 implies the set @xmath234 is bounded .",
    "the set is closed follows easily from the fact that @xmath235 when @xmath19 tends to be singular .",
    "[ lem : unique mini surrogate]the @xmath236 given in ( [ eq : sigma partial majorize ] ) is the unique minimizer to surrogate function ( [ eq : partial majorize ] ) .",
    "for surrogate function ( [ eq : partial majorize ] ) , its value goes to positive infinity when @xmath237 , since it majorizes @xmath71 and @xmath235 in this case .",
    "now consider the case when @xmath238 .",
    "define @xmath239 , then function ( [ eq : partial majorize ] ) can be rewritten as    @xmath240    the terms @xmath241 , @xmath242 and @xmath243 are all constants bounded away from both @xmath114 and @xmath73 .",
    "it is easy to see that when @xmath244 or @xmath245 , ( [ eq : partial majorize - rewrite ] ) goes to @xmath73 .",
    "therefore we conclude that the value of wiesel s surrogate function ( [ eq : partial majorize ] ) goes to @xmath73 when @xmath19 approaches the boundary of @xmath13 .",
    "the fact that @xmath236 given in ( [ eq : sigma partial majorize ] ) is the unique solution to the stationary equation implies that it is the unique minimizer of ( [ eq : partial majorize ] ) on the set @xmath13 .",
    "the sequence @xmath246 generated by algorithm [ alg : wiesels - iteration ] converges to the global minimizer of problem ( [ eq : p - wiesel penalty ] ) .",
    "it is proved in theorem [ thm : existence - wiesel ] that under the conditions provided in theorem [ thm : existence - wiesel ] , the minimizer @xmath60 for problem @xmath247 exists and is unique , furthermore , it solves problem ( [ eq : p - wiesel penalty ] ) .",
    "it is also proved that the objective function @xmath235 on the boundary of the set @xmath248 .",
    "we now show that the sequence @xmath246 converges to unique minimizer of ( [ eq:-15 ] ) .",
    "denote the surrogate function in general as @xmath224 , by lemma [ lem : unique mini surrogate ] we therefore have the following inequality @xmath249 which means @xmath250 is a non - increasing sequence .",
    "assume that there exists converging subsequence @xmath251 , then @xmath252 letting @xmath253 results in @xmath254 which implies that the directional derivative @xmath255 .",
    "the limit @xmath256 is nonsingular since if @xmath256 is singular @xmath257 , but @xmath258 given that @xmath259 , which is a contradiction . since @xmath260 and the function is continuously differentiable , we have @xmath261 . since @xmath262 , @xmath263 .",
    "the set @xmath264 is a compact set , and @xmath246 lies in this set , hence @xmath246 converges to @xmath60 .",
    "following the same approach , for the kl divergence penalty problem : @xmath265 we can majorize @xmath266 at @xmath267 by function @xmath268 the stationary condition leads to the iteration @xmath269 algorithm [ alg : kl iteration ] summarizes the procedure for kl shrinkage estimator .    1",
    ".   initialize @xmath131 as an arbitrary positive definite matrix .",
    "do iteration @xmath270 until convergence .",
    "the sequence @xmath246 generated by algorithm [ alg : kl iteration ] converges to the global minimizer of problem ( [ eq : p - kl ] ) .",
    "we verify the assumptions required for the convergence of algorithm @xcite , namely ( [ eq:-7 ] ) and the compactness of initial level set @xmath271",
    ".    the first condition in ( [ eq:-7 ] ) is satisfied by construction . to verify the second condition , we see that the gradient of the surrogate function @xmath224 has a unique zero .",
    "since @xmath224 is a global upperbound for @xmath266 , @xmath272 as @xmath19 goes to the boundary of @xmath13 . by the continuity of @xmath224 , a minimizer @xmath273 exists and has to satisfy @xmath274 . therefore the unique zero has to be the global minimum , i.e. , @xmath275 .",
    "the last condition is satisfied since @xmath266 is continuously differentiable on @xmath16 .",
    "it is proved in theorems [ thm : uniqueness kl scatter ] and [ thm : exist - kl penalty ] that on set @xmath16 , @xmath266 has a unique stationary point and it is the global minimum .",
    "furthermore , the conditions in theorem [ thm : exist - kl penalty ] ensures @xmath276 when @xmath19 goes to the boundary of @xmath13 .",
    "the initial set @xmath271 is compact follows easily .",
    "therefore the sequence @xmath246 converges to the set of stationary points , hence the global minimum of problem ( [ eq : p - kl ] ) .      in this subsection",
    ", we briefly discuss the covariance estimation problem with structure constraints . in general , the uniqueness of the estimator can not be guaranteed .",
    "however , algorithms can still be derived based on majorization - minimization when the constraint set @xmath277 is convex . in this case",
    ", we can majorize the objective functions @xmath71 and @xmath266 by @xmath278 and @xmath279 respectively , ignoring the constant term . without any additional constraint , setting the gradient of @xmath32 to zero yields update @xmath280 where @xmath281 and @xmath282 notice that @xmath283 is exactly the update we derived by only majorizing the @xmath284 terms in the previous subsection , and @xmath285 is the geometric mean between matrices @xmath267 and @xmath283 @xcite .",
    "intuitively @xmath285 can be viewed as a smoothed update of @xmath267 .",
    "however , when constrained , a closed - form solution for @xmath285 can not be obtained in general .",
    "the surrogate function @xmath32 is convex since @xmath286 is linear and @xmath287 is convex , @xmath288 can be found numerically if @xmath277 is convex .",
    "we consider two such examples .",
    "toeplitz structure arises frequently in various signal processing related fields .",
    "for example , in time series analysis , the autocovariance matrix of a stationary process is toeplitz . imposing the toeplitz structure on @xmath19 we need to solve @xmath289 for each iteration .",
    "the additional constraint is linear .",
    "suppose @xmath19 can be decomposed as @xmath290 , where @xmath291 is signal covariance and @xmath292 with @xmath293 is noise covariance restricted to some interval .",
    "then , at each iteration we solve @xmath294 the additional constraint is convex .",
    "a crucial issue in regularized covariance estimator is to choose the penalty parameter @xmath3 .",
    "we have shown that if the population distribution is continuous , for both wiesel s penalty and kl divergence penalty , we require @xmath174 to guarantee the existence of the regularized estimator .    there is a rich literature discussing the rules of parameter tuning developed for specific estimators .",
    "a standard way is to select @xmath3 by cross - validation , method based on random matrix theory has also been investigated in a recent paper @xcite .",
    "in all of the simulations , the estimator performance is evaluated according to the criteria in @xcite , namely , the normalized mean - square error @xmath295 where all matrices @xmath19 are all normalized by their trace . the expected value is approximated by @xmath296 times monte - carlo simulations .",
    "the first two simulations aims at illustrating the existence conditions for both wiesel s shrinkage estimator and kl shrinkage estimator .",
    "we choose @xmath297 and @xmath298 with the samples drawn a gaussian distribution @xmath299 , where @xmath131 is a randomly generated positive definite covariance matrix .",
    "the shrinkage target @xmath70 is also an arbitrary positive definite matrix .",
    "according to the result in section iii , @xmath174 , i.e. , @xmath300 , is the necessary and sufficient condition for the existence of a positive definite estimator .",
    "we simulate two scenarios with @xmath301 and @xmath302 .",
    "1 plots @xmath303 and the inverse of the condition number , namely @xmath304 , as a function of the number of iterations in log - scale for wiesel s shrinkage estimator and with @xmath301 ( left ) and @xmath305 ( right ) respectively .",
    "1 shows that for wiesel s shrinkage estimator , when @xmath301 @xmath267 diverges , and when @xmath305 @xmath267 converges to a nonsingular limit .",
    "2 shows similar situation happens for kl shrinkage estimator .    for the rest of the simulations , the shrinkage parameter @xmath3 is selected by grid search .",
    "that is , we define @xmath306 and enumerate @xmath102 uniformly on interval @xmath307 $ ] , and select the @xmath102 ( equivalently @xmath3 ) that gives the smallest error .",
    "[ fig : small",
    "n large p ] demonstrates the performance of shrinkage tyler s estimator in the sample deficient case .",
    "the tuning parameter is selected to be the one that yields the smallest nmse for each estimator as proposed in @xcite .",
    "we choose the example @xmath308 with @xmath309 . in this simulation ,",
    "the underlying distribution is chosen to be a student s _ t_-distribution with parameters @xmath310 , @xmath311 , and @xmath312 , and the shrinkage target is set to be an identity matrix .",
    "the number of samples @xmath1 starts from @xmath313 to @xmath314 .",
    "the curve corresponding to tyler s estimator starts at @xmath315 since the condition for tyler s estimator to exist is @xmath9 , i.e. , @xmath316 in this case .",
    "the figure illustrates that both tyler s estimator and shrinkage tyler s estimator outperform the sample covariance matrix when all of them exist , shrinkage estimators exist even when @xmath317 and , moreover , achieve the best performance in all cases .",
    "illustration of the benefit of shrinkage estimators with @xmath309 and shrinkage target matrix @xmath158 . ]",
    "[ fig : performance - comparison - shrinkage - identity ] and [ fig : performance - comparison - shrinkage - target ] compare the performance of different shrinkage tyler s estimators following roughly one of the simulation set - up in @xcite for a fair comparison .",
    "the samples are drawn from a student s _ t_-distribution with parameters @xmath310 , @xmath311 and @xmath312 .",
    "the number of samples @xmath1 varies from @xmath318 to @xmath296 .",
    "[ fig : performance - comparison - shrinkage - identity ] shows the estimation error when setting @xmath319 and fig . [",
    "fig : performance - comparison - shrinkage - target ] shows that when setting @xmath320 , the searching step size of @xmath102 is set to be @xmath321 .",
    "the result indicates that estimation accuracy is increased due to shrinkage when the number of sample is not enough .",
    "wiesel s shrinkage estimator and kl shrinkage estimator yield the same nmse .",
    "interestingly , chen s shrinkage estimator gives roughly the same nmse , although with a different shrinkage parameter @xmath3 .",
    "chen s and kl shrinkage estimator thus find their advantage in practice since an easier way of choosing @xmath3 rather than cross - validation has been investigated in the literature @xcite , a detailed comparison of them from random matrix theory perspective has also been provided in @xcite .",
    "in both of the simulations , we include tyler s estimator with a toeplitz structure constraint as introduced in the previous section .",
    "the figures show that the structure constrained estimator achieves relatively better performance than all other estimators both when shrinking to @xmath158 and shrinking to @xmath70 .",
    "although structure constraint can be imposed on shrinkage estimators to achieve potentially even smaller estimation error , we leave out this simulation due to the heavy computational cost introduced both by a lack of a closed - form solution per iteration ( a sdp need to be solved numerically ) and grid searching for the best regularization parameter .",
    "the problem of accelerating the algorithm and investigating the effect of imposing structure constraint on shrinkage estimator are left for future work .",
    "illustration of the benefit of shrinkage estimators with @xmath298 and shrinkage target matrix @xmath158 . ]",
    "illustration of the benefit of shrinkage estimators with @xmath298 and a knowledge - aided shrinkage target matrix @xmath70 . ]",
    "finally , the performance of tyler s estimator is tested on a real financial data set .",
    "we choose daily close prices @xmath322 from jan 1 , 2008 to july 31 , 2011 , 720 days in total , of @xmath323 stocks from the hang seng index provided by yahoo finance .",
    "the samples are constructed as @xmath324 , i.e. , the daily log - returns .",
    "the process @xmath325 is assumed to be stationary",
    ". the vector @xmath326 is constructed by stacking the log - returns of all @xmath2 stocks . @xmath326",
    "that is close to @xmath327 ( all elements are less than @xmath328 ) is discarded .",
    "we compare the performance of different covariance estimators in the minimum variance portfolio set up , that is , we allocate the portfolio weights to minimize the overall risk .",
    "the problem can be formulated formally as @xmath329 with @xmath19 being the covariance matrix of @xmath326 .",
    "clearly the scaling of @xmath19 does not affect the solution to this problem .",
    "the simulation takes the following procedure . for nonshrinkage estimators , at day @xmath330 , we take the @xmath331 s with @xmath332 $ ] as samples to estimate the normalized covariance matrix @xmath19 . for a particular shrinkage estimator ,",
    "the target matrix is set to be @xmath158 and the tuning parameter @xmath102 is chosen as follows : for each value of @xmath333 , we calculate the shrinkage estimator @xmath334 with samples @xmath331 , @xmath335 $ ] and the corresponding @xmath336 by solving ( [ p : portfolio opt ] ) .",
    "we then take the @xmath331 s with @xmath337 $ ] as validation data and evaluate the variance of portfolio series @xmath338 in this period , the best @xmath339 is chosen to be the one that yields the smallest variance .",
    "finally the shrinkage estimator is obtained using samples @xmath331 with @xmath332 $ ] and tuning parameter @xmath339 . with the allocation strategy @xmath340 for each of the estimators as the solution to ( [ p : portfolio opt ] )",
    ", we construct portfolio for the next @xmath341 days and collect the returns .",
    "the procedure is repeated every @xmath341 days till the end and the variance of the portfolio constructed based on different estimators is calculated .    in the simulation ,",
    "we choose @xmath342 and vary @xmath343 from @xmath344 to @xmath296 .",
    "[ fig : comparison - of - portfolio ] compares the variance ( risk ) of portfolio constructed based on different estimators , with one additional baseline portfolio constructed by equal investment in each asset . from the figure we can see",
    "shrinkage estimators achieves relatively better performance than the nonshrinkage ones .",
    "comparison of portfolio risk constructed based on different covariance estimators . ]",
    "in this work , we have given a rigorous proof for the existence and uniqueness of the regularized tyler s estimator proposed in @xcite , and justified the heuristic diagonal loading shrinkage estimator in @xcite by kl divergence .",
    "under the condition that samples are reasonably spread out , i.e. , @xmath0 , or @xmath7 if the underlying distribution is continuous , the estimators have been shown to exist and unique ( up to a positive scaling factor for wiesel s estimator ) .",
    "algorithms based on the majorization - minimization framework have also been provided with guaranteed convergence .",
    "finally we have discussed structure constrained estimation and have shown in the simulation that imposing such constraint helps improving estimation accuracy .",
    "for the loss function @xmath345 where the regularization term is written in general as @xmath346 . define @xmath100 , @xmath101 for @xmath115 and @xmath103 , @xmath104 for @xmath105 as in definition [ def : als ] .",
    "define function @xmath347 where @xmath348 is defined as the @xmath349th row of @xmath350 with @xmath351 being the unitary matrix such that @xmath352 , @xmath353 , and @xmath354 , @xmath355 .",
    "the eigenvalues @xmath119 is arranged in descending order , i.e. , @xmath356 , and denote the inverse of @xmath357 as @xmath358 , hence @xmath359 .",
    "denote the eigenvectors corresponding to @xmath119 as @xmath360 , the subspace spanned by @xmath361 as @xmath362 and @xmath363 with @xmath364 and @xmath365 . by definition , @xmath366 partition the whole @xmath367 space .",
    "notice that @xmath368 by the assumption that no @xmath62 lies on the origin , we have @xmath369 and @xmath370",
    ".      for the @xmath123 s , denote @xmath375 $ ] . for each @xmath376",
    ", there exists some @xmath371 such that @xmath377 , since the @xmath371 s partition the whole space .",
    "define @xmath378 to be the maximum index of @xmath371 that the @xmath376 s belongs to .",
    "therefore we have @xmath379 and @xmath380 for @xmath381 .",
    "consider the general case that some of the @xmath119 s go to zero , some remains bounded away from both @xmath114 and positive infinity , and the rests tend to positive infinity .",
    "formally , define two integers @xmath385 and @xmath386 that @xmath387 , such that @xmath136 for @xmath388 $ ] , @xmath119 is bounded for @xmath389 $ ] and @xmath137 for @xmath390 $ ] .",
    "denote some arbitrary small positive quantity by @xmath391 .",
    "first we analyze the terms @xmath392 with @xmath137 .",
    "consider the samples @xmath393 for some @xmath394 $ ] , then @xmath395 , which is @xmath396 .",
    "since @xmath397 , we have @xmath398 . by definition @xmath399 which implies @xmath400 , i.e. , @xmath401 .",
    "therefore , if @xmath402 , @xmath403 . for each @xmath392",
    "we have @xmath404    in the second step , we analyze the terms @xmath392 with @xmath136 .",
    "consider the samples @xmath393 for some @xmath405 $ ] , we have shown that @xmath406 . since @xmath407 , @xmath408 .",
    "given that @xmath409 , @xmath410 by @xmath411 therefore for each @xmath392 we have @xmath412      now we have characterized the @xmath414 s , we move to the @xmath415 s .",
    "since @xmath379 and @xmath380 for @xmath381 , by the same reasoning above , @xmath416 if @xmath417 and @xmath418 if @xmath419 . therefore @xmath420 with @xmath421 defined to be @xmath422 if the set @xmath423 is empty , and the same for @xmath424 .        now we simplify the assumption ( [ eq : tmp condtion ] ) .",
    "since @xmath369 and @xmath370 , and @xmath429 can take any value that satisfies @xmath430 , we end up with the following condition : @xmath431 for all @xmath432 .",
    "define sets @xmath433 and @xmath434 , consider when @xmath435 , which means @xmath436 , by the definition of @xmath378 , is equivalent to @xmath437 , similarly for @xmath438 , which means @xmath439 , is equivalent to @xmath440 .",
    "the condition should be valid for any @xmath351 and @xmath432 , tidy up the expression and let @xmath441 results in : for any proper subspace @xmath5 @xmath106 where sets @xmath107 and @xmath108 are defined as @xmath442 , @xmath443 .    for the case @xmath444 , @xmath445 , which means no @xmath24 and some , not all @xmath122 , following the same reasoning gives condition @xmath446 and for @xmath447 , @xmath448 , which means no @xmath122 and some , not all @xmath24 , gives condition @xmath449 notice that the above two conditions are included in the first one .",
    "r.  maronna , d.  martin , and v.  yohai , _ robust statistics : theory and methods _ , ser .",
    "wiley series in probability and statistics.1em plus 0.5em minus 0.4emwiley , 2006 .",
    "[ online ] .",
    "available : http://books.google.com.hk/books?id=ifvjqgaacaaj    p.  huber , _ robust statistics _",
    "wiley series in probability and statistics - applied probability and statistics section series.1em plus 0.5em minus 0.4emwiley , 2004 . [ online ] .",
    "available : http://books.google.com.hk/books?id=e62rhdqidmkc    d.  e. tyler , `` a distribution - free @xmath453-estimator of multivariate scatter , '' _ the annals of statistics _",
    "15 , no .  1",
    "234251 , 03 1987 .",
    "[ online ] .",
    "available : http://dx.doi.org/10.1214/aos/1176350263      e.  ollila and v.  koivunen , `` influence function and asymptotic efficiency of scatter matrix based array processors : case mvdr beamformer , '' _ ieee transactions on signal processing _ ,",
    "57 , no .  1 ,",
    "247259 , jan 2009 .",
    "e.  ollila and d.  tyler , `` distribution - free detection under complex elliptically symmetric clutter distribution , '' in _ 2012 ieee 7th sensor array and multichannel signal processing workshop ( sam ) _ , june 2012 , pp .",
    "413416 .",
    "f.  pascal , y.  chitour , j.  ovarlez , p.  forster , and p.  larzabal , `` covariance structure maximum - likelihood estimates in compound gaussian noise : existence and algorithm analysis , '' _ ieee transactions on signal processing _",
    "56 , no .  1 ,",
    "3448 , jan 2008 .",
    "e.  ollila , d.  tyler , v.  koivunen , and h.  poor , `` complex elliptically symmetric distributions : survey , new results and applications , '' _ signal processing , ieee transactions on _ , vol .  60 , no .  11 , pp .",
    "55975625 , nov 2012 .",
    "y.  abramovich and n.  spencer , `` diagonally loaded normalised sample matrix inversion ( lnsmi ) for outlier - resistant adaptive filtering , '' in _ ieee international conference on acoustics , speech and signal processing , 2007 .",
    "icassp 2007 . _ , vol .  3 , 2007 , pp .",
    "iii1105iii1108 .",
    "o.  ledoit and m.  wolf , `` a well - conditioned estimator for large - dimensional covariance matrices , '' _ journal of multivariate analysis _ ,",
    "88 , no .  2 , pp . 365",
    " 411 , 2004 .",
    "[ online ] .",
    "available : http://www.sciencedirect.com/science/article/pii/s0047259x03000964                r.  w. butler , p.  l. davies , and m.  jhun , `` asymptotics for the minimum covariance determinant estimator , '' _ the annals of statistics _ , vol .",
    "21 , no .  3 , pp .",
    "13851400 , 09 1993 .",
    "[ online ] .",
    "available : http://dx.doi.org/10.1214/aos/1176349264      d.  e. tyler , `` statistical analysis for the angular central gaussian distribution on the sphere , '' _ biometrika _ , vol .",
    "74 , no .  3 , pp .",
    "579589 , 1987 .",
    "[ online ] .",
    "available : http://biomet.oxfordjournals.org/content/74/3/579.abstract    j.  t. kent and d.  e. tyler , `` maximum likelihood estimation for the wrapped cauchy distribution , '' _ journal of applied statistics _",
    "15 , no .  2 ,",
    "pp . 247254 , 1988 .",
    "[ online ] .",
    "available : http://www.tandfonline.com/doi/abs/10.1080/02664768800000029      j.  t. kent and d.  e. tyler , `` redescending @xmath453-estimates of multivariate location and scatter , '' _ the annals of statistics _ , vol .",
    "19 , no .  4 , pp . 21022119 , 12 1991 .",
    "[ online ] .",
    "available : http://dx.doi.org/10.1214/aos/1176348388    m.  zhang , f.  rubio , d.  palomar , and x.  mestre , `` finite - sample linear filter optimization in wireless communications and financial systems , '' _ ieee transactions on signal processing _",
    "61 , no .",
    "20 , pp . 50145025 , 2013 .        d.  r. hunter and k.  lange , `` a tutorial on mm algorithms , '' _ the american statistician _ , vol .  58 , no .  1 ,",
    "pp . 3037 , 2004 .",
    "[ online ] .",
    "available : http://amstat.tandfonline.com/doi/abs/10.1198/0003130042836    m.  razaviyayn , m.  hong , and z.  luo , `` a unified convergence analysis of block successive minimization methods for nonsmooth optimization , '' _ siam journal on optimization _",
    "23 , no .  2 ,",
    "11261153 , 2013 .",
    "[ online ] .",
    "available : http://epubs.siam.org/doi/abs/10.1137/120891009"
  ],
  "abstract_text": [
    "<S> this paper considers the regularized tyler s scatter estimator for elliptical distributions , which has received considerable attention recently . </S>",
    "<S> various types of shrinkage tyler s estimators have been proposed in the literature and proved work effectively in the `` small n large p '' scenario . </S>",
    "<S> nevertheless , the existence and uniqueness properties of the estimators are not thoroughly studied , and in certain cases the algorithms may fail to converge . in this work , we provide a general result that analyzes the sufficient condition for the existence of a family of shrinkage tyler s estimators , which quantitatively shows that regularization indeed reduces the number of required samples for estimation and the convergence of the algorithms for the estimators . for two specific shrinkage tyler s estimators , we also proved that the condition is necessary and the estimator is unique . </S>",
    "<S> finally , we show that the two estimators are actually equivalent . </S>",
    "<S> numerical algorithms are also derived based on the majorization - minimization framework , under which the convergence is analyzed systematically .    </S>",
    "<S> tyler s scatter estimator , shrinkage estimator , existence , uniqueness , majorization - minimization . </S>"
  ]
}