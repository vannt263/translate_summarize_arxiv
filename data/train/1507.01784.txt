{
  "article_text": [
    "topic models have emerged as flexible and important tools for the modelisation of text corpora .",
    "while early work has focused on graphical - model approximate inference techniques such as variational inference  @xcite or gibbs sampling  @xcite , tensor - based moment matching techniques have recently emerged as strong competitors due to their computational speed and theoretical guarantees  @xcite . in this paper",
    ", we draw explicit links with the independent component analysis ( ica ) literature ( e.g. ,  @xcite and references therein ) by showing a strong relationship between latent dirichlet allocation ( lda )  @xcite and ica  @xcite .",
    "we can then reuse standard ica techniques and results , and derive new tensors with better sample complexity and new algorithms based on joint diagonalization .",
    "* notation . * following the text modeling terminology",
    ", we define a corpus @xmath0 as a collection of @xmath1 documents .",
    "each document is a collection @xmath2 of @xmath3 tokens .",
    "it is convenient to represent the @xmath4-th token of the @xmath5-th document as a @xmath6-of-@xmath7 encoding with an indicator vector @xmath8 with only one non - zero , where @xmath7 is the vocabulary size , and each document as the count vector @xmath9 . in such representation ,",
    "the length @xmath3 of the @xmath5-th document is @xmath10 .",
    "we will always use the index @xmath11 to refer to topics , the index @xmath12 to refer to documents , the index @xmath13 to refer to words from the vocabulary , and the index @xmath14 to refer to tokens of the @xmath5-th document .",
    "the plate diagrams of the models from this section are presented in appendix  [ sec : pds ] .    * latent dirichlet allocation * @xcite is a generative probabilistic model for discrete data such as text corpora . in accordance to this model",
    ", the @xmath5-th document is modeled as an _ admixture _ over the vocabulary of @xmath7 words with @xmath15 latent topics .",
    "specifically , the latent variable @xmath16 , which is sampled from the dirichlet distribution , represents the topic mixture proportion over @xmath15 topics for the @xmath5-th document .",
    "given @xmath16 , the topic choice @xmath17 for the @xmath4-th token is sampled from the multinomial distribution with the probability vector @xmath18 .",
    "the token @xmath19 is then sampled from the multinomial distribution with the probability vector @xmath20 , or @xmath21 if @xmath22 is the index of the non - zero element in @xmath23 .",
    "this vector @xmath21 is the @xmath22-th topic , that is a vector of probabilities over the words from the vocabulary subject to the simplex constraint , i.e. , @xmath24 , where @xmath25 .",
    "this generative process of a document ( the index @xmath5 is omitted for simplicity ) can be summarized as @xmath26 one can think of the latent variables @xmath27 as auxiliary variables which were introduced for convenience of inference , but can in fact be marginalized out @xcite , which leads to the following model    [ lda ]    & &    & ~(c ) , + x|&~(l , d ) ,    & &    where @xmath28 is the topic matrix with the @xmath22-th column equal to the @xmath22-th topic @xmath21 , and @xmath29 is the vector of parameters for the dirichlet distribution . while a document is represented as a set of tokens @xmath30 in the formulation  , the formulation   instead compactly represents a document as the count vector @xmath31 . although the two representations are equivalent , we focus on the second one in this paper and therefore refer to it as the lda model .",
    "importantly , the lda model does not model the length of documents .",
    "indeed , although the original paper  @xcite proposes to model the document length as @xmath32 , this is never used in practice and , in particular , the parameter @xmath33 is not learned .",
    "therefore , in the way that the lda model is typically used , it does not provide a complete generative process of a document as there is no rule to sample @xmath34 . in this paper , this fact is important , as we need to model the document length in order to make the link with discrete ica .",
    "* discrete pca . * the lda model   can be seen as a discretization of principal component analysis ( pca ) via replacement of the normal likelihood with the multinomial one and adjusting the prior  @xcite in the following probabilistic pca model  @xcite : @xmath35 and @xmath36 , where @xmath28 is a transformation matrix and @xmath37 is a parameter .",
    "* discrete ica ( dica ) . *",
    "interestingly , a small extension of the lda model allows its interpretation as a discrete independent component analysis model .",
    "the extension naturally arises when the document length for the lda model is modeled as a random variable from the gamma - poisson mixture ( which is equivalent to a negative binomial random variable ) , i.e. , @xmath38 and @xmath39 , where @xmath40 is the shape parameter and @xmath41 is the rate parameter .",
    "the lda model   with such document length is equivalent ( see appendix  [ sec : ldaproof2 ] ) to    [ gp ]    & &    _ k & ~(c_k , b ) , + x_m|&~([d]_m ) ,    & &    where all @xmath42 are mutually independent , the parameters @xmath43 coincide with the ones of the lda model in  , and the free parameter @xmath44 can be seen ( see appendix  [ sec : l : gp ] ) as a scaling parameter for the document length when @xmath45 is already prescribed .",
    "this model was introduced by canny  @xcite and later named as a discrete ica model  @xcite .",
    "it is more natural , however , to name model   as the gamma - poisson ( gp ) model and the model    [ dica ]    & &    _ 1,  ,_k & ~ , + x_m| & ~([d]_m )    & &    as the discrete ica ( dica ) model .",
    "the only difference between   and the standard ica model  @xcite ( without additive noise ) is the presence of the poisson noise which enforces discrete , instead of continuous , values of @xmath46 .",
    "note also that ( a ) the discrete ica model is a _ semi - parametric _ model that can adapt to any distribution on the topic intensities @xmath47 and that ( b ) the gp model   is a particular case of both the lda model   and the dica model  .",
    "thanks to this close connection between lda and ica , we can reuse standard ica techniques to derive new efficient algorithms for topic modeling .",
    "the method of moments estimates latent parameters of a probabilistic model by matching theoretical expressions of its moments with their sample estimates .",
    "recently  @xcite , the method of moments was applied to different latent variable models including lda , resulting in computationally fast learning algorithms with theoretical guarantees . for lda ,",
    "they ( a ) construct _ lda moments _ with a particular diagonal structure and ( b ) develop algorithms for estimating the parameters of the model by exploiting this diagonal structure . in this paper , we introduce the novel _ gp / dica cumulants _ with a similar to the lda moments structure .",
    "this structure allows to reapply the algorithms of  @xcite for the estimation of the model parameters , with the same theoretical guarantees .",
    "we also consider another algorithm applicable to both the lda moments and the gp / dica cumulants .      in this section ,",
    "we derive and analyze the novel cumulants of the dica model . as",
    "the gp model is a particular case of the dica model , all results of this section extend to the gp model .",
    "the first three _ cumulant tensors _ for the random vector @xmath31 can be defined as follows @xmath48 } , \\\\",
    "\\label{cumx } { \\mathrm{cum}}(x , x , x ) & : = { \\mathbb{e}}{\\left [ ( x - { \\mathbb{e}}(x ) ) { \\otimes}(x - { \\mathbb{e}}(x ) ) { \\otimes}(x - { \\mathbb{e}}(x))\\right]},\\end{aligned}\\ ] ] where @xmath49 denotes the tensor product ( see some properties of cumulants in appendix  [ sec : cumulants ] ) .",
    "the essential property of the cumulants ( which does not hold for the moments ) that we use in this paper is that the cumulant tensor for a random vector with _",
    "independent _ components is _",
    "diagonal_.    let @xmath50 ; then for the poisson random variable @xmath51 , the expectation is @xmath52 .",
    "hence , by the law of total expectation and the linearity of expectation , the expectation in   has the following form @xmath53 further , the variance of the poisson random variable @xmath46 is @xmath54 and , as @xmath55 , @xmath56 ,  @xmath57 ,  @xmath58 are conditionally independent given @xmath59 , then their covariance matrix is diagonal , i.e. , @xmath60 .",
    "therefore , by the law of total covariance , the covariance in   has the form @xmath61 } + { \\mathrm{cov}}{\\left[{\\mathbb{e}}(x|y),{\\mathbb{e}}(x|y)\\right ] } \\\\         & = { \\mathrm{diag}}{\\left[{\\mathbb{e}}(y)\\right ] } + { \\mathrm{cov}}(y , y )           = { \\mathrm{diag}}{\\left[{\\mathbb{e}}(x)\\right ] } + d{\\mathrm{cov}}({\\alpha},{\\alpha})d^{\\top } , \\end{aligned}\\ ] ] where the last equality follows by the multilinearity property of cumulants ( see appendix  [ sec : cumulants ] ) .",
    "moving the first term from the rhs of   to the lhs , we define    [ s ]    & &    s : = ( x , x)- .    & &    from   and by the independence of @xmath62 , @xmath57 , @xmath63 ( see appendix  [ sec : app : dicacum2 ] ) , @xmath64 has the following diagonal structure @xmath65}d^{\\top}.\\ ] ]    by analogy with the second order case , using the law of total cumulance , the multilinearity property of cumulants , and the independence of @xmath62 , @xmath57 , @xmath63 , we derive in appendix  [ sec : app:3dicacum ] the expression  , similar to  , for the third cumulant  . moving the terms in this expression , we define a tensor @xmath66 with the following element @xmath67}_{m_1m_2m_3 } : = { \\mathrm{cum}}(x_{m_1},x_{m_2},x_{m_3 } ) + 2{\\delta}(m_1,m_2,m_3 ) { \\mathbb{e}}(x_{m_1 } ) \\qquad\\qquad\\ , \\text{dica t - cum . }",
    "\\label{t } \\\\ & - { \\delta}(m_2,m_3 ) { \\mathrm{cov}}(x_{m_1},x_{m_2 } ) - { \\delta}(m_1,m_3 ) { \\mathrm{cov}}(x_{m_1},x_{m_2 } ) - { \\delta}(m_1,m_2 ) { \\mathrm{cov}}(x_{m_1},x_{m_3 } ) , \\notag\\end{aligned}\\ ] ] where @xmath68 is the kronecker delta . by analogy with   ( appendix  [ sec : app : dicacum2 ] ) ,",
    "the diagonal structure of the tensor @xmath66 : @xmath69    in appendix  [ sec : lda : moms : notation ] , we recall ( in our notation ) the matrix @xmath64   and the tensor @xmath66   for the lda model  @xcite , which are analogues of the matrix @xmath64   and the tensor @xmath66   for the gp / dica models . slightly abusing terminology",
    ", we refer to the matrix @xmath64   and the tensor @xmath66   as the _ lda moments _ and to the matrix @xmath64   and the tensor @xmath66   as the _ gp / dica cumulants_. the diagonal structure  &   of the lda moments is similar to the diagonal structure  &   of the gp / dica cumulants , though arising through a slightly different argument , as discussed at the end of appendix  [ sec : lda : moms : notation ] .",
    "importantly , due to this similarity , the algorithmic frameworks for both the gp / dica cumulants and the lda moments coincide .",
    "the following sample complexity results apply to the sample estimates of the gp cumulants : . ]",
    "[ sample - complexity ] under the gp model , the expected error for the sample estimator @xmath70   for the gp cumulant @xmath64   is : @xmath71 }   \\leq   \\sqrt{{\\mathbb{e}}{\\left [ { \\| { \\widehat{s } } - s \\|}_f^2 \\right ] } }   \\le o{\\left ( \\frac{1}{\\sqrt{n } }   \\max{\\left [ \\delta \\bar{l}^2,\\,\\bar{c}_0 \\bar{l }   \\right ] } \\right ) } , \\ ] ] where @xmath72 , @xmath73 and @xmath74 .",
    "a high probability bound could be derived using concentration inequalities for poisson random variables  @xcite ; but the expectation already gives the right order of magnitude for the error ( for example via markov s inequality ) . the expression   for an unbiased finite sample estimate @xmath70 of @xmath64 and the expression   for an unbiased finite sample estimate @xmath75 of @xmath66 are defined   and @xmath75   of @xmath64   and @xmath66   for the lda moments ( which are consistent with the ones suggested in  @xcite ) in appendix  [ sec : lda : empirical ] . ] in appendix  [ sec : dica : fs ] .",
    "a sketch of a proof for proposition  [ sample - complexity ] can be found in appendix  [ sec : sample - complexity ] .    by following a similar analysis as in  @xcite",
    ", we can rephrase the topic recovery error in term of the error on the gp cumulant .",
    "importantly , the whitening transformation ( introduced in section  [ sec : diag ] ) redivides the error on @xmath64   by @xmath76 , which is the scale of @xmath64 ( see appendix  [ section - analysis - of - whitening ] for details ) .",
    "this means that the contribution from @xmath77 to the recovery error will scale as @xmath78 , where both @xmath79 and @xmath80 are smaller than @xmath6 and can be very small .",
    "we do not present the exact expression for the expected squared error for the estimator of @xmath66 , but due to a similar structure in the derivation , we expect the analogous bound of @xmath81 \\leq   { 1}/{\\sqrt{n}}\\max\\ { \\delta^{3/2 } \\bar{l}^3,\\,\\bar{c}_0^{3/2 } \\bar{l}^{3/2 }   \\}$ ] .",
    "current sample complexity results of the lda moments  @xcite can be summarized as @xmath82 .",
    "however , the proof ( which can be found in the supplementary material  @xcite ) analyzes only the case when finite sample estimates of the lda moments are constructed from _",
    "one _ triple per document , i.e. , @xmath83 only , and not from the u - statistics that average multiple ( dependent ) triples per document as in the practical expressions   and   ( appendix  [ sec : lda : empirical ] ) .",
    "moreover , one has to be careful when comparing upper bounds .",
    "nevertheless , comparing the bound   with the current theoretical results for the lda moments , we see that the gp / dica cumulants sample complexity contains the @xmath84-norm of the columns of the topic matrix @xmath85 in the numerator , as opposed to the @xmath86 coefficient for the lda moments",
    ". this norm can be significantly smaller than @xmath6 for vectors in the simplex ( e.g. , @xmath87 for sparse topics ) .",
    "this suggests that the gp / dica cumulants may have better finite sample convergence properties than the lda moments and our experimental results in section  [ sec : exp : mom ] are indeed consistent with this statement .",
    "the gp / dica cumulants have a somewhat more intuitive derivation than the lda moments as they are expressed via the count vectors @xmath31 ( which are the sufficient statistics for the model ) and not the tokens @xmath88 s .",
    "note also that the construction of the lda moments depend on the unknown parameter  @xmath45 .",
    "given that we are in an unsupervised setting and that moreover the evaluation of lda is a difficult task  @xcite , setting this parameter is non - trivial . in appendix",
    "[ sec : app : c0lda ] , we observe experimentally that the lda moments are somewhat sensitive to the choice of @xmath45 .",
    "how is the diagonal structure   of @xmath64 and   of @xmath66 going to be helpful for the estimation of the model parameters ?",
    "this question has already been thoroughly investigated in the signal processing ( see , e.g. ,  @xcite and references therein ) and machine learning ( see  @xcite and references therein ) literature .",
    "we review the approach in this section . due to similar diagonal structure ,",
    "the algorithms of this section apply to both the lda moments and the gp / dica cumulants .    for simplicity ,",
    "let us rewrite the expressions   and   for @xmath64 and @xmath66 as follows @xmath89 where @xmath90 and @xmath91 .",
    "introducing the rescaled topics @xmath92 , we can also rewrite @xmath93 . following the same assumption from  @xcite that the topic vectors are linearly independent ( @xmath94 is full rank ) , we can compute a whitening matrix @xmath95 of @xmath64 , i.e. , a matrix such that @xmath96 where @xmath97 is the @xmath15-by-@xmath15 identity matrix ( see appendix  [ sec : whitening ] for more details ) . as a result , the vectors @xmath98 form an orthonormal set of vectors .",
    "further , let us define a projection @xmath99 of a tensor @xmath100 onto a vector @xmath101 : @xmath102 applying the multilinear transformation ( see , e.g. ,  @xcite for the definition ) with @xmath103 to the tensor  @xmath66 from   and projecting the resulting tensor @xmath104 onto some vector @xmath101 , we obtain @xmath105 where @xmath106 is due to the rescaling of topics and @xmath107 stands for the inner product . as the vectors @xmath108 are orthonormal , the pairs @xmath108 and @xmath109 are the eigenpairs of the matrix @xmath110 , which are uniquely defined if the eigenvalues @xmath111 are all different",
    ". if they are unique , we can recover the gp / dica ( as well as lda ) model parameters via @xmath112 and @xmath113 .",
    "this procedure was referred to as the spectral algorithm for lda  @xcite and the fourth - order for a discussion on the orders . ]",
    "blind identification algorithm for ica  @xcite .",
    "indeed , one can expect that the finite sample estimates @xmath70   and @xmath75   possess approximately the diagonal structure   and   and , therefore , the reasoning from above can be applied , assuming that the effect of the sampling error is controlled .",
    "this spectral algorithm , however , is known to be quite unstable in practice ( see , e.g. ,  @xcite ) . to overcome this problem ,",
    "other algorithms were proposed . for ica ,",
    "the most notable ones are probably the fastica algorithm  @xcite and the jade algorithm  @xcite .",
    "the fastica algorithm , with appropriate choice of a contrast function , estimates iteratively the topics , making use of the orthonormal structure  , and performs the deflation procedure at every step .",
    "the recently introduced tensor power method ( tpm ) for the lda model  @xcite is close to the fastica algorithm .",
    "alternatively , the jade algorithm modifies the spectral algorithm by performing _ multiple _ projections for   and then jointly diagonalizing the resulting matrices with an orthogonal matrix .",
    "the spectral algorithm is a special case of this orthogonal joint diagonalization algorithm when only one projection is chosen .",
    "importantly , a fast implementation  @xcite of the orthogonal joint diagonalization algorithm from  @xcite was proposed , which is based on closed - form iterative jacobi updates ( see , e.g. ,  @xcite for the later ) .    in practice ,",
    "the orthogonal joint diagonalization ( jd ) algorithm is more robust than fastica ( see , e.g. ,  @xcite ) or the spectral algorithm .",
    "moreover , although the application of the jd algorithm for the learning of topic models was mentioned in the literature  @xcite , it was never implemented in practice . in this paper",
    ", we apply the jd algorithm for the diagonalization of the gp / dica cumulants as well as the lda moments , which is described in algorithm  [ alg : jd ] .",
    "note that the choice of a projection vector @xmath114 obtained as @xmath115 for some vector @xmath116 is important and corresponds to the multilinear transformation of @xmath75 with @xmath117 along the third mode .",
    "importantly , in algorithm  [ alg : jd ] , the joint diagonalization routine is performed over @xmath118 matrices of size @xmath119 , where the number of topics @xmath15 is usually not too big .",
    "this makes the algorithm computationally fast ( see appendix  [ sec : code - and - compleixty ] ) .",
    "the same is true for the spectral algorithm , but not for tpm .",
    "_ input : _",
    "@xmath120 , @xmath15 , @xmath121 ( number of random projections ) ; ( and @xmath45 for lda moments ) compute sample estimate @xmath122 ( for gp / dica /   for lda in appendix  [ sec : app : implementation ] ) estimate whitening matrix @xmath123 of @xmath70 ( see appendix  [ sec : whitening ] ) _ option ( a ) : _ choose vectors @xmath124 uniformly at random from the unit @xmath84-sphere and set @xmath125 for all @xmath126 _ option ( b ) : _ choose vectors @xmath124 as the canonical basis @xmath127 of @xmath128 and set @xmath125 for all @xmath129 for @xmath130 , compute @xmath131 ( for gp / dica /   for lda ; appendix  [ sec : app : implementation ] ) perform orthogonal joint diagonalization of matrices @xmath132 ( see  @xcite and  @xcite ) to find an orthogonal matrix @xmath133 and vectors @xmath134 such that @xmath135 estimate joint diagonalization matrix @xmath136 and values @xmath137 , @xmath126 _ output : _ estimate of @xmath85 and @xmath138 as described in appendix  [ estimation - model - parameters ]    in section  [ sec : diagcmp ] , we compare experimentally the performance of the spectral , jd , and tpm algorithms for the estimation of the parameters of the gp / dica as well as lda models .",
    "we are not aware of any experimental comparison of these algorithms in the lda context .",
    "while already working on this manuscript , the jd algorithm was also independently analyzed by  @xcite in the context of tensor factorization for general latent variable models .",
    "however ,  @xcite focused mostly on the comparison of approaches for tensor factorization and their stability properties , with brief experiments using a latent variable model related but not equivalent to lda for community detection . in contrast",
    ", we provide a detailed experimental comparison in the context of lda in this paper , as well as propose a novel cumulant - based estimator . due to the space restriction the estimation of the topic matrix @xmath85 and the ( gamma / dirichlet )",
    "parameter @xmath138 are moved to appendix  [ estimation - model - parameters ] .",
    "in this section , ( a ) we compare experimentally the gp / dica cumulants with the lda moments and ( b ) the spectral algorithm  @xcite , the tensor power method  @xcite ( tpm ) , the joint diagonalization ( jd ) algorithm from algorithm  [ alg : jd ] , and variational inference for lda  @xcite .    *",
    "real data : * the associated press ( ap ) dataset , from d.  blei s web page , with @xmath139 documents and @xmath140 vocabulary words and the average document length @xmath141 ; the nips papers dataset  @xcite of @xmath142 nips papers and @xmath143 words , and @xmath144 ; the kos dataset , from the uci repository , with @xmath145 documents and @xmath146 words , and @xmath147 .",
    "* semi - synthetic data * are constructed by analogy with  @xcite : ( 1 ) the lda parameters @xmath85 and  @xmath138 are learned from the real datasets with variational inference and ( 2 ) toy data are sampled from a model of interest with the given parameters @xmath85 and  @xmath138 .",
    "this provides the ground truth parameters @xmath85 and  @xmath138 . for each setting",
    ", data are sampled 5 times and the results are averaged .",
    "we plot error bars that are the minimum and maximum values . for the ap data , @xmath148 topics",
    "are learned and , for the nips data , @xmath149 topics are learned . for larger @xmath15 ,",
    "the obtained topic matrix is ill - conditioned , which violates the identifiability condition for topic recovery using moment matching techniques  @xcite .",
    "all the documents with less than @xmath150 tokens are resampled .",
    "* sampling techniques . *",
    "all the sampling models have the parameter @xmath138 which is set to @xmath151 , where @xmath152 is the learned @xmath138 from the real dataset with variational lda , and @xmath45 is a parameter that we can vary .",
    "gp _ data are sampled from the gamma - poisson model   with @xmath153 so that the expected document length is @xmath154 ( see appendix  [ sec : l : gp ] ) .",
    "the _ lda - fix(@xmath155 ) _ data are sampled from the lda model   with the document length being fixed to a given @xmath155 .",
    "the _ lda - fix2(@xmath156,@xmath157,@xmath158 ) _ data are sampled as follows : @xmath159-portion of the documents are sampled from the _ lda - fix(@xmath157 ) _ model with a given document length @xmath157 and @xmath156-portion of the documents are sampled from the _ lda - fix(@xmath158 ) _ model with a given document length @xmath158 .    *",
    "evaluation . * the evaluation of topic recovery for semi - synthetic data is performed with the @xmath160-error between the recovered @xmath161 and true @xmath85 topic matrices with the best permutation of columns : @xmath162 $ ] .",
    "the minimization is over the possible permutations @xmath163 of the columns of @xmath161 and can be efficiently obtained with the hungarian algorithm for bipartite matching . for the evaluation of topic recovery in the real data case , we use an approximation of the log - likelihood for held out documents as the metric  @xcite . see appendix  [ sec : evaluation - real ] for more details .",
    "we use our matlab implementation of the gp / dica cumulants , the lda moments , and the diagonalization algorithms .",
    "the datasets and the code for reproducing our experiments are available online . in appendix",
    "[ sec : code - and - compleixty ] , we discuss the complexity and implementation of the algorithms .",
    "we explain how we initialize the parameter @xmath45 for the lda moments in appendix  [ sec : initialization - c0 ] .      in figure",
    "[ plots : diag ] , we compare the diagonalization algorithms on the semi - synthetic ap dataset for @xmath164 using the gp sampling .",
    "we compare the tensor power method ( tpm )  @xcite , the spectral algorithm ( spec ) , the orthogonal joint diagonalization algorithm ( jd ) described in algorithm  [ alg : jd ] with different options to choose the random projections : jd(k ) takes @xmath165 vectors @xmath166 sampled uniformly from the unit @xmath84-sphere in @xmath128 and selects @xmath167 ( option ( a ) in algorithm  [ alg : jd ] ) ; jd selects the full basis @xmath168 in @xmath128 and sets @xmath169 ( as jade  @xcite ) ( option ( b ) in algorithm  [ alg : jd ] ) ; @xmath170 chooses the full canonical basis of @xmath171 as the projection vectors ( computationally expensive ) .",
    "importantly , we observed that vi when initialized with the output of the jd - gp is consistently better in terms of the predictive log - likelihood .",
    "therefore , the new algorithm can be used for more clever initialization of other lda / gp inference methods .",
    "we also observe that the joint diagonalization algorithm for the lda moments is worse than the spectral algorithm .",
    "this indicates that the diagonal structure   and   might not be present in the sample estimates   and   due to either model misspecification or to finite sample complexity issues ."
  ],
  "abstract_text": [
    "<S> we consider moment matching techniques for estimation in latent dirichlet allocation ( lda ) . by drawing explicit links between lda and discrete versions of independent component analysis ( ica ) , </S>",
    "<S> we first derive a new set of cumulant - based tensors , with an improved sample complexity . </S>",
    "<S> moreover , we reuse standard ica techniques such as joint diagonalization of tensors to improve over existing methods based on the tensor power method . in an extensive set of experiments on both synthetic and real datasets , </S>",
    "<S> we show that our new combination of tensors and orthogonal joint diagonalization techniques outperforms existing moment matching methods . </S>"
  ]
}