{
  "article_text": [
    "let us consider the following global optimization problem @xmath4\\},\\ ] ] where @xmath5 $ ] is a hyperinterval in @xmath2 .",
    "it is supposed that the objective function @xmath3 can be multiextremal , possibly non - differentiable and it satisfies the lipschitz condition @xmath6 , @xmath7 in the euclidean norm .",
    "this statement can very frequently be met in applications where each evaluation of @xmath3 can be very expensive from the computational point of view ( see , e.g. , , etc . ) . due to this reason , in the literature there",
    "exist numerous methods dedicated to the problem ( [ p ] ) , ( [ lip ] ) ( together with references indicated above we can mention such recent publications as ) .",
    "it should be also mentioned that methods for problems where the gradient of the objective function satisfies the lipschitz condition were also studied ( see , e.g. , , etc . ) .",
    "one of the main issues regarding the problem ( [ p ] ) , ( [ lip ] ) is related to the treatment of the lipschitz constant @xmath8 . in the literature , there exist several approaches for acquiring the lipschitz information that can be distinguished with respect to the way the lipschitz constant is estimated during the process of optimization .",
    "for instance , there exist algorithms ( see , e.g. , ) that use an a priori given estimate @xmath9 of @xmath8 ( it should be mentioned that usually in practice it is difficult to obtain valid estimates ) or an adaptive estimate @xmath10 that is recalculated at each iteration @xmath11 of the method ( see , e.g. , @xcite ) .    it is well known that estimates of the lipschitz constant have a significant influence on the convergence and the speed of the global optimization algorithms ( see @xcite ) . namely , tight estimates can accelerate the search , overestimates can slow it down , and underestimates can lead to losing the global solution .",
    "algorithms that use in their work a global estimate @xmath10 or an a priori given estimate @xmath9 do not take into account any local information about the behavior of the objective function over small subregions of the domain @xmath1 .",
    "it has been shown in that a smart usage of local estimates @xmath12 of the local lipschitz constants @xmath13 over subregions @xmath14 allows one to significantly accelerate the global search .",
    "naturally , a balancing between the local and global information must be performed in an appropriate way in order to avoid missing the global solution .",
    "another interesting approach that has been introduced in @xcite uses at each iteration several estimates of the lipschitz constant @xmath8 .",
    "this algorithm works by partioning the hyperinterval @xmath1 in subintervals ( called hereinafter also _ subboxes _ ) and due to this reason it has been called by its authors _ direct _ ( dividing rectangles ) .",
    "we can see therefore that in the literature there exist four practical strategies to obtain a lipschitz information for its subsequent usage in numerical methods : a )  to consider the real constant @xmath8 , if it is given , or to use its overestimate when it is possible ; b )  to calculate dynamically a global a global ( i.e. , the same for the whole search region ) adaptive estimate of @xmath8 ; c )  to consider estimates of local lipschitz constants related to subintervals of the search region @xmath1 ; d )  to take into consideration a set of estimates of @xmath8 selected among all possible values varying from zero to infinity . in this paper",
    ", our attention will be dedicated to this fourth alternative .    as was mentioned above , in the literature there",
    "exists a variety of numerical methods dedicated to the problem ( [ p ] ) , ( [ lip ] ) .",
    "one of non trivial views on the problem consists of the reduction of the dimension of ( [ p ] ) , ( [ lip ] ) with the help of space - filling curves .",
    "these curves , first introduced by peano ( 1890 ) and hilbert ( 1891 ) ( see @xcite ) , emerge as the limit objects generated by an iterative process .",
    "they are fractals constructed using the principle of self - similarity .",
    "peano - hilbert curves fill in the hypercube @xmath5 \\subset \\re^n$ ] , i.e. , they pass through every point of @xmath5 $ ] , and this gave rise to the term space - filling curves .",
    "examples of construction of these curves in two dimensions are given in fig .",
    "[ fig.1a ] .",
    "= 12.5 cm    it has been shown ( see @xcite,@xcite,@xcite ) that , by using space filling curves , the multidimensional global minimization problem ( [ p ] ) , ( [ lip ] ) can be turned into a one - dimensional problem .",
    "in particular , strongin has proved in @xcite that finding the global minimum of the lipschitz function @xmath15 over a hypercube is equivalent to determining the global minimum of the function @xmath16 in an interval , i.e. , it follows @xmath17,\\ ] ] where @xmath18 is the peano curve . moreover ,",
    "the hlder condition @xmath19,\\ ] ] holds ( see @xcite ) for the function @xmath16 with the constant @xmath20 where @xmath8 is the lipschitz constant of the original multidimensional function @xmath3 from ( [ p ] ) , ( [ lip ] ) .",
    "thus , one can try to solve the problem ( [ p ] ) , ( [ lip ] ) by using algorithms proposed for minimizing hlderian functions ( [ fun ] ) , ( [ hold ] ) in one dimension . to do this in practice",
    ", three main steps should be executed if one wishes to use methods that partition the search region and try to obtain and to use a lipschitz / hlder information ( see @xcite for a general description of this kind of methods , their convergence properties , etc . ) .",
    "first , in order to realize the passage from the multi - dimensional problem to the one - dimensional one , computable approximations to the peano curve should be employed in the numerical algorithms .",
    "second , the hlder constant @xmath21 from ( [ hold ] ) should be estimated .",
    "finally , a partition strategy of the search region should be chosen .",
    "we have already seen above that in the lipschitz global optimization there exist at least four ways to obtain estimates of the lipschitz constant .",
    "when we move to the hlder global optimization the situation is different . in the literature",
    "( see ) , there exist methods that use strategies a ) , b ) , and c ) discussed above . however , inventing for the hlder global optimization a strategy similar to the technique d ) was an open problem since 1993 when the article @xcite showing how to use simultaneously several estimates of @xmath8 has been published . in this paper , we close this gap and propose an algorithm that uses several estimates of the hlder constant at each iteration employing space - filling curves , central point based partition strategies , and hlderian minorants .",
    "the rest of the paper has the following structure . in section  2 , difficulties regarding the usage of the strategy",
    "d ) in the hlderian framework and the proposed solution are presented . in section  3 , a new algorithm for solving the problem ( [ p ] ) , ( [ lip ] ) and its convergence properties are described .",
    "the new method uses numerical approximations to peano space - filling curves and the scheme of representation of intervals with hlderian minorants from section  2 .",
    "section  4 presents results of numerical experiments that compare the new method with its competitors on 800 test functions randomly generated by the gkls - generator from @xcite .",
    "finally , section  5 contains a brief conclusion .",
    "let us consider a one - dimensional function @xmath16 satisfying the lipschitz condition with a constant @xmath8 over the interval @xmath22 $ ] .",
    "then it follows from the lipschitz condition that @xmath23 , \\,\\ , 1 \\le i \\le k,\\ ] ] @xmath24 , \\\\      c_i^+(x ) =   f(m_i ) -l ( x - m_i ) , \\hspace{1.5 cm }   x\\in [ m_i , b_i ] ,      \\end{array } \\right.\\ ] ] where @xmath25 is ( see fig .",
    "[ fig.1 ] , left ) a piece - wise linear discontinuous minorant ( called often also _ support function _ ) for @xmath16 over each subinterval @xmath26 $ ] , @xmath27 and @xmath28 the values @xmath29 , @xmath30 called _ characteristics _ and being lower bounds for the function @xmath16 over each interval @xmath31 , @xmath30 can easily be calculated .",
    "in fact , if we suppose that an overestimate @xmath32 of the lipschitz constant @xmath8 is given , then it follows @xmath33 } c_i(x ) = f(m_i ) - 0.5l_1 ( b_i - a_i).\\ ] ]    however , in order to solve the multidimensional problem ( [ p ] ) , ( [ lip ] ) by using space - filling curves , instead of working with lipschitz functions , we should focus our attention on the one - dimensional hlderian function @xmath16 from ( [ fun ] ) .",
    "it follows from ( [ hold ] ) that , for all @xmath34 $ ] we have @xmath35 if a point @xmath36 $ ] is fixed , then the function @xmath37 is a minorant for @xmath16 over @xmath22 $ ] .",
    "then , analogously to ( [ sup_lip_1])([m_i ] ) , we obtain that the function @xmath38 , \\,\\ , 1 \\le i \\le k,\\ ] ] @xmath39 , \\\\",
    "l_i^+(x ) =   f(m_i ) -h ( x - m_i)^{1/n } , \\hspace{1.5 cm }   x\\in [ m_i , b_i ] ,      \\end{array } \\right.\\ ] ] is a discontinuous nonlinear minorant for @xmath16 ( see fig .",
    "[ fig.1 ] , right ) .",
    "the values @xmath29 , @xmath30 are lower bounds for the function @xmath16 over each interval @xmath31 , @xmath27 and can be calculated as follows if an overestimate @xmath40 of the hlder constant @xmath21 is given @xmath41 } l_i(x ) = f(m_i ) -h_1 |(b_i - a_i)/2|^{1/n}.\\ ] ]    = 14.5 cm    as it was discussed above , the direct algorithm works simultaneously with several estimates of the lipschitz constant at each iteration .",
    "one of the key features that allow it to do this is a smart representation of intervals @xmath42 $ ] , @xmath30 in the two - dimensional diagram shown in fig .",
    "[ direct ] . in this figure",
    ", we have intervals @xmath43 , @xmath44 , @xmath45 , @xmath46 , and @xmath47 that are represented by the dots @xmath48 , @xmath49 , @xmath50 , @xmath51 , and @xmath52 , respectively .",
    "the horizontal coordinate of each dot is equal to @xmath53 , i.e. , half of the length of the respective interval @xmath42 $ ] , and the vertical coordinate is equal to @xmath54 where @xmath55 is from ( [ m_i ] ) ( see , e.g. , the dot @xmath49 and its coordinates ) . let us consider now the intersection of the vertical coordinate axis with the line having the slope @xmath56 and passing through each dot representing subintervals in the diagram shown in fig .  [ direct ] .",
    "it is possible to see that this intersection gives us exactly the characteristic @xmath57 from  ( [ lowb_lip ] ) , i.e. , the lower bound for @xmath16 over the corresponding subinterval @xmath42 $ ] if @xmath32 ( note that the points on the vertical axis ( @xmath58 ) do not represent any subinterval ) .",
    "it can immediately be seen that the diagram allows one to determine very easily an interval with the minimal characteristic ( in fig .",
    "[ direct ] this interval is represented by the dot @xmath52 , its characteristic is @xmath59 ) . in the lipschitz global optimization ( see ,",
    "e.g. , @xcite ) , the operation of finding an interval with the minimal characteristic ( that can be calculated in different ways in different algorithms ) is an important one .",
    "it is executed at each iteration , the found interval is then subdivided and @xmath16 is evaluated at new points belonging to this interval . moreover , since we do not know the exact value of the real lipschitz constant @xmath8 , the scheme presented in fig .",
    "[ direct ] allows one ( see @xcite ) to take into consideration all possible values of @xmath8 from zero to infinity and @xmath52 by a line and indicate its slope by @xmath60 then for all constants @xmath61 the interval @xmath43 will have the minimal characteristic and , therefore it should be subdivided .",
    "analogously , for all constants @xmath62 the interval @xmath47 will have the minimal characteristic . then , if we subdivide both intervals , @xmath43 and @xmath47 , then the interval corresponding to the real constant @xmath8 will be subdivided even though we are not know this value @xmath8 .",
    "thus , the diagram really allows one to consider all possible values of the lipschitz constant from zero to infinity and to choose a small group of intervals for the subsequent subdivision ensuring that the interval corresponding to the correct lipschitz constant belongs to this group . ] and to choose a set of promising intervals for partitioning ( this issue will be discussed in detail later ) .",
    "let us try now to construct a similar diagram in the framework of the hlder optimization where the nonlinear support functions @xmath63 from ( [ li ] ) shown in fig .",
    "[ fig.1 ] , right , are built over each subinterval . in fig .",
    "[ fig.2 ] , intervals @xmath43 , @xmath44 , @xmath45 , @xmath46 , and @xmath47 are again represented by dots @xmath48 , @xmath49 , @xmath50 , @xmath51 , and @xmath52 , respectively .",
    "if we take an estimate @xmath64 of the hlder constant @xmath21 , then characteristic @xmath65 of the interval @xmath44 represented by the dot @xmath49 is given by the vertical coordinate of the intersection point of the auxiliary curve ( [ li ] ) passed through the point @xmath49 and the vertical coordinate axis .",
    "= 14.5 cm    we can see in fig .",
    "[ fig.2 ] that the curves constructed using the estimate @xmath64 and representing the nonlinear support function ( [ li ] ) can intersect each other in different ways .",
    "the procedure of the selection of subintervals for producing new trials ( _ trial _ is an evaluation of @xmath16 at a point @xmath66 that is called _ trial point _ )",
    "is based on estimates of the lower bounds of the objective function over current subboxes .",
    "subintervals for the further subdivision are chosen taking into account _ all _ possible values of the hlder constant @xmath21 .",
    "due to numerous possible intersections of the curves at the representation shown in fig .",
    "[ fig.2 ] , it becomes unclear how to select , for all possible values of @xmath21 , the set of promising intervals that must be partitioned at each iteration  @xmath67 .",
    "this difficulty that seemed unsolvable since 1993 did not allow people to construct global optimization algorithms working in the framework of the hlder global optimization with all possible hlder constants .",
    "this problem is solved in the next section that proposes an algorithm that uses , on the one hand , peano space - filling curves to reduce the multi - dimensional problem ( [ p ] ) , ( [ lip ] ) to one dimension and , on the other hand , multiple estimates of the hlder ( lipschitz ) constant .",
    "the algorithm for solving one - dimensional hlder global optimization problems in the spirit of the direct method is a part of it .",
    "in order to construct a procedure allowing one to select , at each iteration @xmath67 of the algorithm , a set of intervals to be partitioned with respect to all possible values of the constant @xmath21 we proceed as follows .",
    "first , we introduce a new graphical representation of subintervals @xmath31 by using instead of euclidean metric the hlderian one .",
    "namely , we propose to represent each interval @xmath26 \\in\\ { d^k\\}$ ] by a dot @xmath68 with coordinates @xmath69 , where @xmath70 is the current partition of the one - dimensional search interval during the @xmath67th iteration and coordinates of the point @xmath68 are calculated as follows @xmath71 @xmath72 where @xmath55 is from ( [ m_i ] ) , i.e. , @xmath55 is the central point of the interval @xmath26 $ ] .",
    "= 14.5 cm    the introduction of the hlderian metric allows us ( see fig .",
    "[ fig.3 ] ) to avoid the non - linearity and the intersection of minorants giving as a result a diagram similar to that from fig .",
    "[ direct ] . in fig .",
    "[ fig.3 ] we can see the representation of the same intervals considered in fig .  [ fig.2 ] .",
    "notice that in fig .",
    "[ fig.3 ] the values in the horizontal axis are calculated in the hlderian metric , while the vertical axis values coincide with those of fig .",
    "[ fig.2 ] . in this new representation , the intersection of the line with the slope @xmath64 passing through any dot representing an interval in the diagram of fig .",
    "[ fig.3 ] and the vertical coordinate axis gives us exactly the characteristic ( [ lowb ] ) of the corresponding interval .",
    "we can proceed now with the development of the new one - dimensional global optimization method following the spirit of the direct algorithm and keeping in mind that we deal with the hlderian metric .",
    "each subinterval @xmath31 of a current partition @xmath70 is characterized by a lower bound of the objective function over @xmath31 .",
    "an interval @xmath31 is selected for a further partitioning if for some value @xmath73 of the possible hlder constant it has the smallest lower bound for @xmath16 with respect to the other intervals present at this iteration . by changing the value of @xmath74 from zero to infinity , at each iteration @xmath67 , we select a set of intervals that will be partitioned .",
    "let us consider how this set of intervals is chosen during each iteration @xmath67 .",
    "the following definitions state a _ relation of domination _ between every two subintervals of the current partition of  @xmath51 .",
    "[ def1 ] given an estimate @xmath73 of the hlder constant @xmath21 from ( [ hold ] ) , an interval @xmath75 dominates the interval @xmath76 with respect to @xmath74 if @xmath77    [ def2 ] an interval @xmath78 is said to be nondominated with respect to @xmath73 if for the chosen value @xmath74 there is no other interval in @xmath79 which dominates  @xmath80 .",
    "in order to be sure to subdivide the nondominated interval corresponding to the real constant @xmath21 , we can select the set of nondominated intervals with respect to all possible estimates @xmath81 . by using the new graphical representation shown in fig .",
    "[ fig.3 ] it is easy to determine whether an interval dominates , with respect to an estimate of the hlder constant @xmath21 , some other interval from the partition @xmath79 .",
    "for example , in fig .  [ fig.3 ] we can see that for the estimate @xmath64 we have @xmath82 so , the interval @xmath46 dominates both intervals @xmath45 and @xmath44 with respect to @xmath64 .",
    "furthermore , @xmath83 , i.e. , the interval @xmath43 dominates the interval @xmath46 . finally , the characteristic @xmath84 is the smallest one , and the interval @xmath47 dominates all others intervals with respect to @xmath64 , i.e. , it is nondominated with respect to @xmath64 ( see fig .",
    "[ fig.3 ] ) .    if a different value @xmath85 of the hlder constant is considered ( see fig .",
    "[ fig.3 ] ) , the interval @xmath46 still dominates the interval @xmath44 with respect to @xmath86 , because @xmath87 , but @xmath46 is dominated by the interval @xmath45 , since @xmath88 .",
    "moreover , we have that @xmath89 , thus , for the chosen estimate @xmath86 the unique nondominated interval is @xmath43 . as we can see from this example , some intervals ( @xmath44 , @xmath45 , and @xmath46 in fig .",
    "[ fig.3 ] ) are always dominated by other intervals , independently on the chosen estimate of the hlder constant .",
    "other intervals ( @xmath47 and @xmath43 in fig .",
    "[ fig.3 ] ) can be nondominated for one value and dominated for another one",
    ". the following definition will be very useful hereinafter .",
    "[ def3 ] an interval @xmath78 is called nondominated if there exists an estimate @xmath81 of the hlder constant @xmath21 such that @xmath80 is nondominated with respect to @xmath90 .    in other words , nondominated intervals",
    "are intervals over which the objective function @xmath16 has the smallest lower bound from ( [ lowb ] ) for some particular estimate of the hlder constant @xmath21 .",
    "note that in the two - dimensional diagram @xmath69 , where @xmath91 and @xmath92 are from ( [ xdot ] ) , ( [ dot ] ) , the nondominated intervals are located at the bottom of each group of points with the same horizontal coordinate .",
    "for example in fig .",
    "[ fig.4 ] these points are designated as @xmath48 , @xmath49 , @xmath50 , @xmath51 , @xmath52 , @xmath93 , and @xmath94 .",
    "not all of these intervals are nondominated : in fact , in figure  [ fig.4 ] the interval @xmath45 is dominated either by the interval @xmath44 ( for example , with respect to @xmath95 , where @xmath96 corresponds to the slope of the line passed through the points @xmath49 and @xmath51 ) , or by the interval @xmath46 ( with respect to @xmath97 ) .",
    "the interval @xmath98 is dominated by @xmath46 and @xmath47 , with respect to any positive estimate of the constant @xmath21 .",
    "the interval @xmath99 is dominated by @xmath98 . in figure",
    "[ fig.4 ] , dots @xmath48 , @xmath49 , @xmath51 , and @xmath52 represent nondominated intervals . the following theorem allow us to easily identify the set of nondominated intervals .",
    "[ th0 ] let each interval @xmath26 \\in \\ { d^k\\}$ ] be represented by a dot with horizontal coordinate @xmath91 and vertical coordinate @xmath92 defined in ( [ xdot ] ) , ( [ dot ] ) .",
    "then , intervals that are nondominated in the sense of definition [ def3 ] are located on the lower - right convex hull of the set of dots representing the intervals .",
    "* proof . *",
    "the proof of the theorem [ th0 ] is analogous to the proof of theorem 2.2 from @xcite .",
    "@xmath100    in practice , nondominated intervals can be found by applying algorithms for identifying the convex hull of the dots ( see , e.g. , the algorithm called jarvis march , or gift wrapping , see @xcite ) .",
    "= 14.5 cm    we describe now the partition strategy adopted by the new algorithm for dividing subintervals in order to produce new trial points .",
    "when , at the generic iteration @xmath67 , we identify the set of nondominated intervals , we proceed with the subdivision of each of these intervals only if a significant improvement on the function values with respect to the current minimal value @xmath101 is expected .",
    "once an interval @xmath102 becomes nondominated , it can be subdivided only if the following condition is satisfied : @xmath103 where the lower bound @xmath104 is from ( [ lowb ] ) .",
    "this condition prevents the algorithm from subdividing already well - explored small subintervals .",
    "let us suppose now that at the current iteration @xmath67 of the new algorithm a subinterval @xmath105 $ ] , represented in the two - dimensional diagram of fig .",
    "[ fig.4 ] by the dot @xmath106 from ( [ xdot ] ) , ( [ dot ] ) , has been chosen for partitioning .",
    "the subdivision of this interval is performed in such a way that three new equal subintervals of the length @xmath107 are created , i.e. , @xmath108=[a_t , p_t]\\cup [ p_t , q_t ] \\cup [ q_t , b_t],\\ ] ] @xmath109 the interval @xmath80 is removed from the two - dimensional diagram representing the current partition @xmath79 of the search interval , and the three newly generated subintervals are introduced both into @xmath110 and the diagram . finally ,",
    "two new trials @xmath111 and @xmath112 are performed at the points @xmath113 and @xmath114 of the intervals @xmath115 $ ] and @xmath116 $ ] , respectively , i.e. , @xmath117 the central interval @xmath118 $ ] inherits the point @xmath119 at which the objective function has been evaluated when the original interval @xmath105 $ ] has been created .    until now we have described the strategies assuming to work with a function in one dimension . as was stated above",
    ", strongin has shown that multidimensional optimization problems can be solved by using modified algorithms proposed for minimizing functions in one dimension , and therefore in order to solve the global optimization problem in @xmath120 dimensions ( [ p ] ) , ( [ lip ] ) we can use the above developed one - dimensional global optimization method together with the space - filling curves .",
    "for an effective use of the peano curve in our algorithm we need computable approximations of the curve ( see @xcite for a detailed discussion and a code allowing one to implement such approximations ) .",
    "hereinafter we denote by @xmath121 the approximation of level @xmath122 of the peano curve . in fig .",
    "[ fig.1a ] we can see examples of peano curve approximations of the levels @xmath123 in dimension @xmath124 .",
    "suppose now that a global optimization method uses an approximation @xmath121 of the peano curve to solve the multidimensional problem and provides a lower bound @xmath125 for the corresponding one - dimensional function @xmath16 .",
    "then the value @xmath125 will be a lower bound for the function @xmath3 in dimension @xmath120 only along the curve @xmath121 .",
    "the following theorem establishes a lower bound for the function @xmath3 over the entire multidimensional search region @xmath5 $ ] given the value @xmath125 .",
    "[ th1 ] let @xmath126 be a lower bound along the space - filling curve @xmath121 for a multidimensional function @xmath3 , @xmath127\\subset r^n$ ] , satisfying lipschitz condition with constant @xmath8 , i.e. , @xmath128.\\ ] ] then the value @xmath129 is a lower bound for @xmath3 over the entire region @xmath5 $ ] .",
    "* proof . *",
    "see or the recent monograph @xcite .",
    "@xmath100    by using the space - filling curves we are able to work with a one - dimensional function in the interval @xmath22 \\subset r^1 $ ] .",
    "the level @xmath122 of the approximation of the peano curve @xmath121 , is crucial for a good performance of the method .",
    "if @xmath122 is too small , the domain in @xmath120 dimensions may not be well `` filled '' and we risk losing the global solution . when @xmath122 grows , the reduced function in one dimension becomes more and more oscillating and the number of local minima increases when @xmath120 increases ( see for a detailed discussion on this topic ) .",
    "then , due to the facts that we are in @xmath22 $ ] and that we use the metric of hlder , it happens that the width of the nondominated interval @xmath78 to be partitioned at a generic iteration @xmath67 can become very small .",
    "when the dimension @xmath120 increases , the width of the subintervals can reach the computer precision . in order to avoid this situation another condition in addition to ( [ cond ] )",
    "is required .",
    "namely , when an interval @xmath105\\in \\",
    "{ d^k\\ } $ ] becomes nondominated , it can be subdivided only if the following condition is satisfied : @xmath130 where @xmath131 is a parameter of the method .",
    "now , let us present the new algorithm called @xmath132 ( @xmath122ultidimensional @xmath94lobal optimization @xmath48lgorithm working with a @xmath1et of estimates of the hlder constant ) .",
    "( initialization ) .",
    "set the current iteration number @xmath133 .    split the initial interval",
    "@xmath134 $ ] in three equal parts and set @xmath135 , @xmath136 , @xmath137 and compute the values of the function @xmath138 , @xmath139 , where @xmath121 is the @xmath122-approximation of the peano curve .",
    "set the current partition of the search interval @xmath140 , [ 1/3,2/3 ] , [ 2/3,1 ] \\}$ ] .",
    "set the current number of intervals @xmath141 and the current number of trials @xmath142 .",
    "set @xmath143 , and @xmath144 .    after executing @xmath67 iterations , the iteration @xmath145 consists of the following steps .",
    "( nondominated intervals ) identify both the set @xmath146 , @xmath147 , of nondominated intervals , according to definition [ def3 ] , that satisfy conditions ( [ cond ] ) and ( [ cond1 ] ) , and the corresponding set @xmath148 of their indices .",
    "@xmath70 denotes the partition of the search interval @xmath134 $ ] at iteration @xmath67 .",
    "( subdivision of nondominated intervals ) set @xmath149 , and perform the following steps 2.1 - 2.3 :    ( interval selection ) .",
    "select a new interval @xmath105 $ ] from @xmath146 such that @xmath150    ( subdivision and sampling ) . subdivide interval @xmath80 in three new equal subintervals , named @xmath151 , @xmath152 , @xmath153 , of the length @xmath107 , following ( [ partit1 ] ) , ( [ partit2 ] ) , and produce two new trial points accordingly to ( [ trials ] ) . + eliminate the interval @xmath80 from @xmath154 , i.e. , set @xmath155 , and update @xmath154 with the insertion of the three new intervals , i.e. , @xmath156 increase both the current number of intervals @xmath157 , and the current number of trials @xmath158 .",
    "+ update the current record @xmath159 and the current record point @xmath160 if necessary .",
    "( next interval ) . eliminate the interval @xmath80 from @xmath146 , i.e. , set @xmath161 and @xmath162 .",
    "if @xmath163 , then go to step 2.1 .",
    "otherwise go to step 3 .",
    "( end of the current iteration ) . increase the iteration counter @xmath164 .",
    "go to step 1 and start the next iteration .",
    "different stopping criteria can be used in the algorithm introduced above .",
    "one of them will be introduced in the next section presenting numerical experiments .",
    "we proceed now to the study of convergence properties of the algorithm .",
    "theorem [ th1 ] linking the multidimensional global optimization problem ( [ p ] ) , ( [ lip ] ) to the one - dimensional problem ( [ fun ] ) , ( [ hold ] ) allows us to concentrate our attention on the convergence properties of the one - dimensional method .",
    "we shall study properties of an infinite sequence @xmath165 of trial points generated by the algorithm mgas when we suppose that the number of iteration @xmath67 goes to infinity ( i.e. , in this case the algorithm does not stop ) .",
    "the following theorem establishes the so - called _ everywhere dense _",
    "convergence of the method , i.e. , convergence of the infinite sequence of trial points to any point of the one - dimensional search domain .",
    "[ th2 ] if @xmath166 in ( [ cond1 ] ) , then for any point @xmath167 $ ] and any @xmath168 there exist an iteration number @xmath169 and a point @xmath170 , @xmath171 , such that @xmath172 .",
    "_ proof_. the interval partition scheme ( [ partit1 ] ) , ( [ partit2 ] ) used for each subdivision of intervals produces three new subintervals of the length equal to a third of the length of the subdivided interval .",
    "since @xmath166 , to prove the theorem it is sufficient to prove that for a fixed value @xmath168 , after a finite number of iterations @xmath173 , the largest subinterval of the current partition @xmath174 of the domain @xmath51 will have the length smaller than  @xmath175 .",
    "in such a case , in the @xmath175-neighborhood of any point of @xmath51 there will exist at least one trial point generated by the algorithm .",
    "@xmath176    to see this , let us fix an iteration number @xmath177 and consider the group of the largest intervals of the partition @xmath178 having the horizontal coordinate @xmath179 ( in the diagram of fig .",
    "[ fig.4 ] this group consists of two points : the dot @xmath48 and the dot above it ) . as can be seen from the scheme of the algorithm mgas , for any @xmath180 this group is always taken into account when nondominated intervals are looked for .",
    "in particular , an interval @xmath181 from this group , having the smallest value @xmath182 , must be partitioned at each iteration of the algorithm .",
    "this happens because there always exists a sufficiently large estimate @xmath183 of the hlder constant @xmath21 for the function @xmath16 such that the interval @xmath80 is the nondominated interval with respect to @xmath183 and condition ( [ cond ] ) is satisfied for the lower bound @xmath184 .",
    "three new subintervals having the length equal to a third of the length of @xmath80 are then inserted into the group with a horizontal coordinate @xmath185 . since each group contains a finite number of intervals , after a sufficiently large number of iterations all the intervals of the group @xmath179",
    "will be divided and the group will become empty . as a consequence , the group of the largest intervals will now be identified by @xmath186 , where the difference @xmath187 is finite .",
    "the same procedure will be repeated with this new group of the largest intervals , and the next new group , etc .",
    "this means that there exists a finite iteration number @xmath173 such that after performing @xmath173 iterations of the algorithm mgas , the length of the largest interval of the current partition @xmath174 is smaller than @xmath175 and , therefore , in the @xmath175-neighborhood of any point of the search region there will exist at least one trial point generated by the algorithm .    in fig .",
    "[ fig.5 ] , an example of convergence of the sequence of trial points generated by the algorithm mgas in dimension @xmath124 using the approximation of the level @xmath188 to the peano curve is given .",
    "the zone with the high density of the trial points corresponds to the global minimizer .",
    "[ fig.51 ] shows how this problem was solved in the one - dimensional space . in the upper part of fig .",
    "[ fig.51 ] , the one - dimensional function corresponding to the curve shown in fig .",
    "[ fig.5 ] and the respective trial points produced by the mgas at the interval @xmath22 $ ] are presented .",
    "the lower part of the figure shows the dynamics ( from bottom to top ) of 40 iterations executed by the mgas .",
    "it can be seen that each iteration contains more than one trial .",
    "the piece - wise line connects points with the best function value obtained during that iteration .    in order to conclude this section",
    "it should be noticed that theorem [ th2 ] establishes convergence conditions of infinite sequences of trial points generated by the algorithm mgas to any point of the domain @xmath22 $ ] and therefore to the global minimum points @xmath189 of the one - dimensional function @xmath16 .",
    "the peano curves used for reduction of dimensionality establish a correspondence between subintervals of the curve and the @xmath120-dimensional subcubes of the domain @xmath5 \\subset r^n$ ] .",
    "every point on the curve approximates an @xmath190-neighborhood in @xmath5 $ ] , i.e. , the points in the @xmath120-dimensional domain may be approximated differently by the points on the curve in dependence on the mutual disposition between the curve and the point in @xmath5 $ ] to be approximated ( see @xcite ) .",
    "here by approximation of a point @xmath191 $ ] we mean the set of points ( called _ images _ ) on the curve minimizing the euclidean distance from @xmath192 .",
    "it was shown in @xcite that the number of the images ranges between @xmath193 and @xmath194 .",
    "these images can be located on the curve very far from each other despite their proximity in the @xmath120-dimensional space .",
    "thus , by using the space - filling peano curve @xmath121 , the global minimizer @xmath195 in the @xmath120-dimensional space can have up to @xmath194 images on the curve , i.e. , it is approximated by @xmath196 , @xmath197 , points @xmath198 such that @xmath199 where @xmath200 is defined by the space - filling curve . obviously , in the limiting case , when @xmath201 and the iteration number @xmath202 , all global minimizers will be found .",
    "but in practice we work with a finite @xmath203 and @xmath204 , i.e. , with a finite trial sequence , then to obtain an @xmath190-approximation @xmath198 of the solution @xmath195 it is sufficient to find _ only one _ of the images @xmath205 on the curve .",
    "this effect may result in a serious acceleration of the search ( see @xcite for a detailed discussion ) .",
    "in this section , we present results of numerical experiments performed to compare the new algorithm mgas with the original direct algorithm proposed in @xcite and its locally biased modification lbdirect introduced in @xcite .",
    "these methods have been chosen for comparison because they , just as the mgas method , do not require the knowledge of the objective function gradient and work with several lipschitz constants simultaneously .",
    "the fortran implementation of the two methods described in @xcite and downloadable from @xcite have been used in both methods .    to execute numerical experiments with the algorithm mgas , we should define its parameter @xmath206 from ( [ cond ] ) . in direct ( see @xcite ) , where a similar parameter is used , the value @xmath206 is related to the current minimal function value @xmath101 and is fixed as follows : @xmath207 the choice of @xmath208 between @xmath209 and @xmath210 has demonstrated good results for direct on a set of test functions ( see @xcite ) .",
    "since the value @xmath211 has produced the most robust results for direct ( see , e.g. , @xcite ) , exactly this value was used in ( [ csi ] ) for direct in our experiments . the same formula ( [ csi ] ) and the same value @xmath211 were used in the new algorithm , too .",
    "the series of experiments involves a total of 800 test functions in the dimensions @xmath212 generated by the gkls - generator described in @xcite and downloadable from http://wwwinfo.deis.unical.it/@xmath213yaro/gkls.html .",
    "more precisely , eight classes of 100 functions have been considered .",
    "the generator allows one to construct classes of randomly generated multidimensional and multiextremal test functions with _ known _ values of local and global minima and their locations .",
    "each test class contains 100 functions and only the following five parameters should be defined by the user : + @xmath120  problem dimension ; + @xmath214  number of local minima ; + @xmath215  value of the global minima ; + @xmath216  radius of the attraction region of the global minimizer ; + @xmath217  distance from the global minimizer to the vertex of the paraboloid .",
    "the generator works by constructing in @xmath2 a convex quadratic function , i.e. , a paraboloid , systematically distorted by polynomials . in our numerical experiments",
    "we have considered classes of continuously differentiable test functions with @xmath218 local minima .",
    "the global minimum value @xmath215 has been fixed equal to @xmath219 for all classes .",
    "an example of a function generated by the gkls can be seen in fig .",
    "[ fig.6 ] .        by changing the user - defined parameters , classes with different properties can be created .",
    "for example , a more difficult test class can be obtained either by decreasing the radius @xmath216 of the attraction region of the global minimizer or by increasing the distance , @xmath217 , from the global minimizer to the paraboloid vertex . in this paper , for each dimension @xmath212 , two test classes where considered : a simple one and a difficult one , see table [ table1 ] that describes the classes used in the experiments . since the gkls - generator provides functions with known locations of global minima , the experiments have been carried out by using the following stopping criteria .",
    ".[table1 ] _ description of 8 classes of test functions used in experiments _ [ cols=\"^,^,^,^,^,^,^\",options=\"header \" , ]     figure  [ fig.7 ] shows a comparison of the three methods using the so called _ operating characteristics _ introduced in 1978 in  @xcite ( see , e.g. ,  @xcite for their english - language description ) .",
    "these characteristics show very well the performance of algorithms under the comparison for each class of test functions . on the horizontal axis",
    "we have the number of function evaluations and the vertical coordinate of each curve shows how many problems have been solved by one or another method after executing the number of function evaluations corresponding to the horizontal coordinate .",
    "for instance , the first graph in the right - hand column ( @xmath124 , class 2 ) shows that after 1000 function evaluations the lbdirect has found the global solution at 33 problems , direct at 47 problems , and the mgas at 84 problems .",
    "thus , the behavior of an algorithm is better if its characteristic is higher than characteristics of its competitors . in figure  [ fig.7 ] ,",
    "the left - hand column of characteristics , the behavior of algorithms mgas , direct , and lbdirect on the classes 1 , 3 , 5 , and 7 is shown .",
    "the right - hand column presents the situation when the more difficult classes 2 , 4 , 6 , and 8 have been used .",
    "the global optimization problem of a multi - dimensional , non - differentiable , and multiextremal function has been considered in this paper . it was supposed that the objective function can be given as a ` black - box ' and the only available information is that it satisfies the lipschitz condition with an unknown lipschitz constant over the search region being a hyperinterval in @xmath2 .",
    "a new deterministic global optimization algorithm called mgas has been proposed .",
    "it uses the following two ideas : the mgas applies numerical approximations to space - filling curves to reduce the original lipschitz multi - dimensional problem to a univariate one satisfying the hlder condition ; the mgas at each iteration uses a new geometric technique working with a number of possible hlder constants chosen from a set of values varying from zero to infinity evolving so ideas of the popular direct method to the field of hlder global optimization .",
    "convergence conditions of the mgas have been established .",
    "numerical experiments carried out on 800 of test functions generated randomly have been executed .",
    "it can be seen from the numerical experiments that the new algorithm shows quite a promising performance in comparison with its competitors .",
    "moreover , the advantage of the new technique becomes more pronounced for harder problems .",
    "casado l.g . , garca i. , and sergeyev ya.d . ,",
    "_ interval algorithms for finding the minimal root in a set of multiextremal non - differentiable one - dimensional functions _ , siam j. on scientific computing , 24(2 ) , 359376 ( 2002 ) .",
    "gablonsky m.j . , _ direct v2.04 fortran code with documentation _ , 2001 , http://www4.ncsu.edu/  ctk / software / directv204.tar.gz .",
    "gablonsky m.j . , _ modifications of the direct algorithm _ , ph.d thesis , north carolina state university ,",
    "raleigh , nc , 2001 .",
    "gablonsky m.j . and",
    "kelley c.t . , _ a locally - biased form of the direct algorithm _ , j. of global optimization , 21 , 2737 ( 2001 ) .    gaviano m. , kvasov d.e . , lera d. , and sergeyev ya.d .",
    "software for generation of classes of test functions with known local and global minima for global optimization _",
    ", acm toms , 29(4 ) , 469480 ( 2003 ) .",
    "horst r. and pardalos p.m. , _ handbook of global optimization _ , kluwer , doordrecht ( 1995 ) .",
    "horst r. and tuy h. , _ global optimization : deterministic approaches _ , springer , berlin , ( 1996 ) .",
    "gourdin e. , jaumard b. , and ellaia r. , _ global optimization of hlder functions _ , j. of global optimization , 8 , 323 - 348 ( 1996 ) .",
    "jones d.r . , perttunen c.d . , and stuckman b.e . , _",
    "lipschitzian optimization without the lipschitz constant _",
    ", j. of optimization theory and applications , 79 , 157181 ( 1993 ) .",
    "kvasov d.e .",
    ", pizzuti c. , and sergeyev ya.d . ,",
    "_ local tuning and partition strategies for diagonal go methods _ , numerische mathematik , 94(1 ) , 93106 ( 2003 ) .",
    "kvasov d.e . and sergeyev ya.d . , _ a univariate global search working with a set of lipschitz constants for the first derivative _ , optim .",
    "letters , 3 , 303318 ( 2009 ) .",
    "lera d. and sergeyev ya.d .",
    ", _ global minimization algorithms for hlder functions _ , bit , 42(1 ) , 119133 ( 2002 ) .",
    "lera d. and sergeyev ya.d .",
    ", _ an information global minimization algorithm using the local improvement technique _ , j. of global optimization , 48 , 99112 ( 2010 ) .",
    "lera d. and sergeyev ya.d .",
    ", _ lipschitz and hlder global optimization using space - filling curves _ ,",
    "numer . maths . , 60 , 115129 ( 2010 ) .",
    "lera d. and sergeyev ya.d .",
    ", _ acceleration of univariate global optimization algorithms working with lipschitz functions and lipschitz first derivatives _ , siam j. optim . , 23(1 ) , 508529 ( 2013 ) .",
    "martnez j.a . ,",
    "casado l.g .",
    ", garca i. , sergeyev ya.d . , toth b. _ on an efficient use of gradient information for accelerating interval global optimization algorithms _ , numerical algorithms , 37 , 6169 ( 2004 ) .",
    "pintr j.d . , _",
    "global optimization in action _ , kluwer , dordrecht ( 1996 ) .",
    "piyavskii s.a . , _",
    "an algorithm for finding the absolute extremum of a function _ , ussr comput . math . and math .",
    ", 12 , 5767 ( 1972 ) .",
    "sergeyev ya.d . ,",
    "daponte p. , grimaldi d. , and molinaro a. , _ two methods for solving optimization problems arising in electronic measurements and electrical engineering _",
    ", siam j. optim . , 10(1 ) , 121 ( 1999 ) .",
    "sergeyev ya.d . and kvasov d.e .",
    "_ global search based on efficient diagonal partitions and a set of lipschitz constants _ , siam j. optim .",
    "16(3 ) , 910937 ( 2006 ) .",
    "sergeyev ya.d . and kvasov d.e . , _ diagonal global optimization methods",
    "_ , fizmatlit , moscow ( 2008 ) , ( in russian ) .",
    "sergeyev ya.d .",
    ", strongin r.g . , and lera d. , _ introduction to global optimization exploiting space - filling curves _ , springer , new york ( 2013 ) .",
    "strongin r.g . , _ numerical methods in multiextremal problems : information - statistical algorithms _ , nauka , moscow ( 1978 ) , ( in russian ) ."
  ],
  "abstract_text": [
    "<S> in this paper , the global optimization problem @xmath0 with @xmath1 being a hyperinterval in @xmath2 and @xmath3 satisfying the lipschitz condition with an unknown lipschitz constant is considered . </S>",
    "<S> it is supposed that the function @xmath3 can be multiextremal , non - differentiable , and given as a ` black - box ' . to attack the problem , a new global optimization algorithm based on the following two ideas </S>",
    "<S> is proposed and studied both theoretically and numerically . </S>",
    "<S> first , the new algorithm uses numerical approximations to space - filling curves to reduce the original lipschitz multi - dimensional problem to a univariate one satisfying the hlder condition . </S>",
    "<S> second , the algorithm at each iteration applies a new geometric technique working with a number of possible hlder constants chosen from a set of values varying from zero to infinity showing so that ideas introduced in a popular direct method can be used in the hlder global optimization . </S>",
    "<S> convergence conditions of the resulting deterministic global optimization method are established . </S>",
    "<S> numerical experiments carried out on several hundreds of test functions show quite a promising performance of the new algorithm in comparison with its direct competitors .    * </S>",
    "<S> key words*. global optimization , lipschitz functions , space - filling curves , hlder functions , deterministic numerical algorithms , direct , classes of test functions . </S>"
  ]
}