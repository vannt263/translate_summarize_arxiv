{
  "article_text": [
    "model selection is central to statistics , and the most popular statistical techniques for model selection are akaike information criterion ( aic )  @xcite , bayesian information criterion ( bic )  @xcite and minimum description length ( mdl )  @xcite . the basic idea behind mdl principle is to equate compression with finding regularities in the data .",
    "since learning often involves finding regularities in the data , hence learning can be equated with compression as well .",
    "hence , in mdl , we try to find the model that yields the maximum compression for the given observations .",
    "the first mdl code introduced was the two - part code @xcite .",
    "it was shown that the the two - part code generalizes maximum entropy principle  @xcite . however , in the past two decades , the normalized maximum likelihood ( nml ) code , which is a version of mdl , has gained popularity among statisticians .",
    "this is particularly because of its minimax properties , as stated in @xcite , which the earlier versions of mdl did not possess .",
    "efficient methods for computing nml codelengths for mixture models have been proposed in @xcite . in both these papers",
    ", the aim of model selection is to decide the optimum number of clusters in a clustering problem .    in a maximum entropy approach to density estimation one has to decide , _ a priori _ , on the amount of ` information ' ( for example , number of moments ) that should be used from the data to fix the model .",
    "the minimax entropy principle @xcite states that for given sets of features for the data , one should choose the set that minimizes the maximum entropy .",
    "however , it can easily be shown that if there are two feature subsets @xmath0 and @xmath1 and @xmath2 , the minimax entropy principle will always prefer @xmath1 over @xmath0 . hence , though the minimax entropy principle is a good technique for choosing among sets of features with same cardinality , it can not decide when the sets of features have varying cardinality .    in this paper",
    ", we study the nml codelength of maximum entropy models . towards this end , we formulate the problem of selecting a maximum entropy model given various feature subsets and their moments , as a model selection problem .",
    "we derive nml codelength for maximum entropy models and show that our approach is a generalization of minimax entropy principle .",
    "we also compute the nml codelength for discriminative maximum entropy models .",
    "we apply our approach to gene selection problem for leukemia data set and compare it with minimax entropy method .",
    "let @xmath3 be a family of probability distributions on the sample space @xmath4 .",
    "@xmath5 denotes the sample space of all data samples of size @xmath6 .",
    "@xmath7 denotes an element in @xmath5 , where @xmath8 is a vector in @xmath4 .",
    "a two - part code encodes the data sample @xmath9 by first encoding a distribution @xmath10 and then the data @xmath11 . the best hypothesis to explain data",
    "@xmath11 is then the one that minimizes the sum @xmath12 @xcite .",
    "according to mdl principle @xmath13 \\label{crude } , \\ ] ] where @xmath14 is the codelength of the data , @xmath15 is the codelength of distribution and @xmath16 is the codelength of data given the distribution .",
    "@xmath16 is a measure of the error of data @xmath11 with respect to distribution @xmath17 .",
    "hence , if @xmath17 approximates @xmath11 well enough , @xmath16 should be small and vice versa . by kraft s inequality",
    ", there exists a codelength function @xmath18 on @xmath5 given by @xmath19 .",
    "we an use @xmath20 as @xmath16 directly , since it is the unique minimizer of expected codelength , when @xmath17 is indeed the true distribution .",
    "the normalized maximum likelihood code is one of the several ways for constructing @xmath15 . to formalize the definition of normalized maximum likelihood , we need the following notion of ` regret ' @xcite",
    ".    let @xmath3 be a model on @xmath5 and let @xmath21 be a probability distribution on @xmath5 .",
    "the regret of @xmath21 with respect to @xmath3 for data sample @xmath7 is defined as @xmath22   \\enspace .\\ ] ]    the regret is nothing but the extra number of bits needed in encoding @xmath7 using @xmath21 instead of the optimal distribution for @xmath7 in @xmath3 .",
    "the worst case regret denoted by @xmath23 is defined as the maximum regret over all sequences in @xmath5 @xmath24\\right ] \\enspace .\\ ] ] our aim is to find the distribution @xmath21 that minimizes the maximum regret . to this end",
    ", we define the complexity of a model @xmath25 as @xmath26 where @xmath27 denote the maximum likelihood estimate ( mle ) of the parameter @xmath28 for the model @xmath3 for the data sample @xmath29 . in the above equation and subsequent sections , the integral",
    "is defined subject to existence .",
    "also , we assume that mle is well defined for the model @xmath3 .",
    "furthermore , the error of a model is defined as @xmath30 \\enspace .",
    "\\label{eq : error_def}\\ ] ] the following result is due to @xcite .    if the complexity of a model is finite , then the minimax regret is uniquely achieved by the normalized maximum likelihood distribution given by @xmath31    the corresponding codelength @xmath32 also known as the stochastic complexity of the data sample @xmath7 is given by @xmath33",
    "let @xmath34 be a random variable taking values in @xmath4 .",
    "let @xmath35 be a set of functions of @xmath34 . the resultant linear family @xmath36 is given by the set of all probability distributions that satisfy the constraints @xmath37 where @xmath38 is the empirical estimate of @xmath39 for the data @xmath7 .",
    "the resulting maximum entropy model @xmath40 contains @xmath41 such that @xmath42 where @xmath43 . here",
    ", @xmath44 is the normalizing constant .    given a set of maximum entropy models characterized by their function set @xmath45 , we use nml code to choose the model that best describes the data .",
    "the nml codelength of data @xmath7 for a given model @xmath3 is composed of two parts : ( i ) the error codelength and ( ii ) the complexity of the model .",
    "error codelength of data sequence @xmath7 for the maximum entropy model @xmath46 is n times the maximum entropy of the corresponding linear family @xmath47 @xmath48 where @xmath49 is the maximum entropy distribution of @xmath36 given by .    first , we compute the error codelength of the data @xmath7 for the model @xmath50 . by definition ,",
    "@xmath51 using definition of @xmath50 from equation , we get @xmath52\\notag \\\\               = & \\inf_{\\lambda } \\left [ n\\lambda_0 + \\sum_{k=1}^m \\lambda_k \\sum_{i=1}^n \\phi_k(\\mathbf{x}^{(i ) } ) \\right]\\notag \\\\               = & \\inf_{\\lambda}\\left [   n\\lambda_0 + \\sum_{k=1}^m \\lambda_k \\left(n\\bar{\\phi}_k(\\mathbf{x}^n)\\right ) \\right ] \\notag \\\\",
    "= & n\\left[\\inf_{\\lambda } \\left ( \\lambda_0 + \\sum_{k=1}^m \\lambda_k \\bar{\\phi}_k(\\mathbf{x}^n)\\right ) \\right ] \\label{eq : error_lambda } \\enspace ,    \\end{aligned}\\ ] ] where @xmath38 is the sample estimate of @xmath39 for the data @xmath7 and @xmath53 .    using lagrange multipliers , it is easy to see that the maximum entropy distribution for the linear family @xmath47 has the form @xmath54 for some @xmath55 .",
    "since maximum entropy distribution always exists for a linear family , the parameters @xmath56 can be obtained by maximizing the log likelihood function .",
    "@xmath57 \\notag   \\\\                          = & { \\operatornamewithlimits{argmin}}_{\\lambda } \\left [ \\lambda_0 + \\sum_{k=1}^m \\lambda_k \\bar{\\phi}_k(\\mathbf{x}^n ) \\right ]   \\label{eq : lambda_star } \\enspace .",
    "\\end{aligned}\\ ] ] where we remove the negative sign to change @xmath58 to @xmath59 .",
    "the notation @xmath60 is used to denote the empirical mean of @xmath61 for the data @xmath7 .",
    "the corresponding entropy is given by @xmath62 where the last equality follows from the definition of @xmath47 in equation . by combining equations and and using the fact that maximum entropy distribution always exists for a linear family",
    ", we get @xmath63    \\label{eq : entropy_lambda }   \\enspace .\\ ] ] by combining equations and , we get @xmath64 for fixed @xmath7 , the error depends on the function set @xmath65 through the above equation .",
    "as the no . of functions in the function set",
    "is increased , the size of the linear family decreases .",
    "hence , the entropy of the maximum entropy distribution also decreases , since we are restricted to search for the maximum entropy distribution in a smaller space .",
    "hence , error of the model decreases .",
    "complexity of the maximum entropy model @xmath46 is given by @xmath66 where @xmath67 is the maximum entropy distribution of @xmath68 .",
    "we have @xmath69 where we have used the definition of error in and its relationship with entropy in to get the result . by similar arguments as above , it is easy to see that the complexity of the model increases by increase in the number of constraints . by using and",
    "we get the desired result .",
    "hence , the nml codelength ( also known as stochastic complexity ) of @xmath7 for the model @xmath70 is given by @xmath71      in this section , we show that the presented nml formulation for maximum entropy is a generalization of the minimax entropy principle @xcite , where this principle has been used for feature selection in texture modeling .",
    "let @xmath72 be sets of functions from @xmath4 to the set of real numbers .",
    "corresponding to each set @xmath73 , there exists a maximum entropy model @xmath74 and vice - versa . the mdl principle states that given a set of models for the data",
    ", one should choose the model that minimizes the codelength of the data .",
    "here , the codelength that we are interested in is the nml codelength ( also known as stochastic complexity ) .",
    "since , there exists a one - one relationship between the maximum entropy models and the function sets @xmath73 , the model selection problem can be reframed as @xmath75 \\enspace .\\ ] ] if we assume that all our models have the same complexity , then the second term in r.h.s can be ignored . since n , the size of data is a constant , the model selection problem becomes @xmath76 this is the classical minimax entropy principle given in @xcite .",
    "hence , the minimax entropy principle is a special case of the mdl principle where the complexity of all the models are assumed to be the same and the models assumed are the maximum entropy models .",
    "discriminative methods for classification , model the conditional probability distribution @xmath77 , where @xmath78 is the class label for data @xmath79 .",
    "maximum entropy based discriminative classification tries to find the probability distribution with the maximum conditional entropy @xmath80 subject to some constraints @xcite , where @xmath81 is the class variable .",
    "initially , information is extracted from the data in the form of empirical means of functions .",
    "these empirical values are then equated to their expected values , thereby forming a set of constraints .",
    "the classification model is constructed by finding the maximum entropy distribution subject to these sets of constraints .",
    "we use mdl to decide the amount of information to extract from the data in the form of functions of features .",
    "a straightforward application of this technique is feature selection .",
    "the maximum entropy discriminative model @xmath46 , where @xmath82 is the set of all probability distributions of the form @xmath83 let us denote the denominator in above equation as @xmath84 .",
    "since , we are not interested in modelling @xmath85 , we use the empirical distribution @xmath86to approximate @xmath85  @xcite .",
    "the empirical distribution @xmath86 is given by @xmath87 and @xmath88 otherwise .",
    "hence , the constraints become @xmath89    as discussed in @xcite , the sender - receiver model assumed here is as follows .",
    "both sender and receiver have the data @xmath7 .",
    "the sender is interested in sending the class labels @xmath90 .",
    "if he sends the class labels without compression , he needs to send @xmath91 bits .",
    "if , however , he uses the data to compute a probability distribution over the class labels , and then compress @xmath90 using that distribution , he may get a shorter codelength for @xmath90 .",
    "his goal is to minimize this codelength , such that the receiver can recover the class labels from this code .",
    "the error codelength of @xmath92 for the conditional model @xmath46 is equal to n times the maximum conditional entropy of the model @xmath93 .",
    "error of the conditional model is given by @xmath94 \\label{eq : error_lambda_disc}\\!\\!\\ !",
    "\\enspace .",
    "\\end{aligned}\\ ] ] where we have used similar reasoning as in to get the last statement .",
    "also , the maximum conditional entropy distribution can be obtained by maximizing the corresponding log - likelihood function .",
    "hence , @xmath95 correspondingly , @xmath96    the corresponding conditional entropy is given by @xmath97 \\nonumber \\\\",
    "= & \\frac{1}{n}\\!\\ ! \\left[\\sum_{k=1}^m \\lambda_k^*\\!\\ ! \\left(\\sum_{i=1}^n",
    "\\phi_k(\\mathbf{x}^{(i ) } , c^{(i)})\\right ) \\right .",
    "\\!\\!\\!+\\!\\!\\ ! \\left .",
    "\\sum_{i=1}^n    \\log{z_\\lambda(\\mathbf{x}^{(i)})}\\right ] \\label{eq : entropy_lambda_disc } \\!\\!\\!\\enspace .",
    "\\end{aligned}\\ ] ]    here the first equality follows from the definition of conditional entropy as used in @xcite .",
    "we use the definition of @xmath98 to convert the integral to a summation . by using and the fact that @xmath99 must sum up to 1 ,",
    "we obtain the fourth equality .    using and",
    ", we obtain @xmath100 \\label{eq : entropy_lambda_disc1 } \\!\\!\\!\\!\\enspace .\\ ] ]    replacing the above equation in , we get the desired result .    the complexity of the conditional model @xmath46 is given by @xmath101",
    "we use gene selection as an example to illustrate discriminative model selection for maximum entropy models .",
    "the dataset used is leukemia dataset available publicly at http://www.genome.wi.mit.edu .",
    "the dataset was also used in @xcite to illustrate nml model selection for discrete regression .",
    "the data set consists of two classes : acute myeloid leukemia ( aml ) and acute lymphoblastic leukemia ( all ) .",
    "there are 38 training samples and 34 independent test samples in the data .",
    "the data consists of 7129 genes .",
    "the genes are preprocessed as recommended in @xcite .    assuming the sender - receiver model discussed above",
    ", the sender needs 38 bits or 26.34 nats in order to send the class labels of training data to receiver .",
    "if the nml code is used , the sender needs 24.99 nats .",
    "since the sender and receiver both contain the microarray data , the sender can use the microarray data to compress the class labels much more than can be obtained wihout the microarray data . specifically , we are interested in finding the genes which gives the best compression , or the minimum nml codelength .",
    "[ ht ]   genes , title=\"fig:\",scaledwidth=45.0%,scaledwidth=20.0% ]    [ ht ]   genes , title=\"fig:\",scaledwidth=45.0%,scaledwidth=20.0% ]    [ ht ]     [ ht ]     for the purpose of our algorithm , we quantize the genes to various levels . we claim that quantizing a gene reduces the risk of overfitting of the model to data .",
    "to support our claim , we have also plotted the change in accuracy with quantization level in figure  [ accuracy ] for the top 25 genes . as can be seen from the graph , increasing quantizaton level from @xmath102 to @xmath103 results in a decrease in accuracy .",
    "we have also plotted a graph for change in average nml codelength with quantization level for the top 25 genes in figure  [ nml_codelength ] .",
    "an interesting observation is that the minima of nml codelength coincides exactly with the maxima of accuracy .",
    "a similar trend was obsrved when the number of genes were changed .    hence , we quantize each gene to 5 levels .",
    "other than the advantages of quantization mentioned above , quantization is also necessary for the current problem as the problem of calculating complexity can become intractable even for moderate n. the constraints that we use are moment constraints , that is @xmath104 .",
    "we vary the value of m from 1 to 7 to get a sequence of maximum entropy models .",
    "the nml codelength of the class labels is calculated for each such model .",
    "the model that results in the minimum nml codelength is selected for each gene .",
    "it was observed that for most genes , the nml codelength decreased sharply when m was increased from 1 to 2 .",
    "the change in values of nml codelength was less noticeable for @xmath105 .",
    "the variation of nml codelength of class labels for a typical gene are shown in figure  [ nml_vs_m ] . in order to make the changes in nml codelength more visible",
    ", we skip the nml codelength for m=1 .",
    "our approach for ranking genes is as follows . for each gene , we select the value of m that gives the minimum nml codelength .",
    "we then sort the genes in increasing order of their minimum nml codelengths .",
    "the minimum codelength achieved is 8.35 nats , which is much smaller than 24.99 nats achieved without using the microarray data .",
    "since compression is equated with finding regularity according to minimum description length principle , hence , it can be stated that the topmost gene is able to discover a lot of regularity in the data .",
    "finally , we use mdl to build a classifier .",
    "the amount of information to use for each gene is decided , by using mdl to fix the number of moments .",
    "mdl is used to rank the features .",
    "then , we use class conditional independence among features to build a maximum entropy classifier .",
    "the number of genes used for the classifier are varied from 1 to 130 .",
    "the resultant graph is compared with other maximum entropy classifiers in figure  [ classifier ] , where the amount of information used per gene is the same for all genes .",
    "finding appropriate feature functions and the number of moments is important to any maximum entropy method . in this paper",
    ", we pose this problem as a model selection problem and develop an mdl based method to solve this problem .",
    "we showed that this approach generalizes minimax entropy principle of  @xcite .",
    "we derived nml codelength in this respect , and extended it to discriminative maximum entropy model selection .",
    "we tested our proposed method for gene selection problem to decide on the quantization level and number of moments for each gene .",
    "finally , we selected the genes based on the codelength of the class labels and compared the simulation results with minimax entropy method .",
    "the bottleneck for using mdl for model selection in discriminative classification is the computation of complexity .",
    "more efficient approximations to calculate the complexity need to be developed to employ this approach for problems involving larger data sets .",
    "p.  kontkanen , p.  myllymki , w.  buntine , j.  rissanen , and h.  tirri , `` an mdl framework for data clustering , '' in _ advances in minimum description length _ , p.  g. grnwald , i.  j. myung , and m.  a. pitt , eds .",
    "mit press , cambridge , ma , 2005 .    s.  hirai and k.  yamanishi , `` efficient computation of normalized maximum likelihood coding for gaussian mixtures with its applications to optimal clustering , '' in _ information theory proceedings ( isit ) , 2011 ieee international symposium on_. ieee , 2011 , pp . 10311035 ."
  ],
  "abstract_text": [
    "<S> in this paper , we treat the problem of selecting a maximum entropy model given various feature subsets and their moments , as a model selection problem , and present a minimum description length ( mdl ) formulation to solve this problem . for this , we derive normalized maximum likelihood ( nml ) codelength for these models . </S>",
    "<S> furthermore , we show that the minimax entropy method is a special case of maximum entropy model selection , where one assumes that complexity of all the models are equal . </S>",
    "<S> we extend our approach to discriminative maximum entropy models . </S>",
    "<S> we apply our approach to gene selection problem to select the number of moments for each gene for fixing the model . </S>"
  ]
}