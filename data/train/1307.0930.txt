{
  "article_text": [
    "we consider the classical linear least - squares problem @xmath0 where @xmath1 is a full - rank @xmath2 matrix , with @xmath3 , and @xmath4 .",
    "such issue is typically addressed in presence of an ill - posed inverse problem @xmath5 since the existence and uniqueness of the least - squares solution @xmath6 together with its continuously data dependence ( that is guaranteed in the discrete case ) lead to a well - posed problem . however , especially in the case in which the linear system arises from the discretization of a continuous ill - posed inverse problem , the switch to the least - squares problem does not avoid the ill - conditioning pathology , that amplifies the noise affecting the data with the result of a meaningless reconstructed solution .",
    "a classical example occurs in the image deblurring problem @xcite , in which @xmath7 is a blurred and noisy version of an unknown image @xmath8 and @xmath1 describes the transformation from the target to the measured data values .",
    "the numerical instability provided by the ill - conditioning can be countered by looking for a regularized solution of , obtained by means of an approximation of the original problem with a one - parameter family of better conditioned ones .",
    "a strategy for the choice of the `` best '' parameter is finally needed , according to some criteria based for example on the knowledge of the noise level or the analysis of the residuals @xcite . besides the classical direct regularization approaches as the truncated singular value decomposition or the tikhonov method ,",
    "an increasing interest has been devoted to iterative regularization strategies , that are in general more computationally effective for large scale problems and allow an easier introduction of constraints on the desired solution ( e.g. , non - negativity , flux conservation ) @xcite .",
    "although in general the regularization property of iterative approaches is not theoretically proved , most of them exhibits the so - called _ semiconvergence effect _ , i.e. , the reconstruction error decreases until one optimal iterate and then diverges .",
    "such behaviour has been analyzed e.g. by nagy & palmer @xcite and donatelli & serra capizzano @xcite in terms of _ filter factors _ @xcite .",
    "in particular , in @xcite some classical gradient methods have been reformulated as a linear combination of the singular vectors of @xmath1 , in which the coefficients form involves the inverse of the singular values @xmath9 multiplied for suitable factors that balance possible noise amplification effects due to small values of some @xmath9 . in this paper",
    "we want to go one step further , proving that a similar expansion holds true even if a scaling matrix multiplying the gradient is present .",
    "moreover , we investigate the effect on the filter factors of some scaling matrices , arising from both the numerical optimization and the imaging framework .",
    "in particular , we show that the apparently negative oscillating behaviour of the filters provided by some scaled schemes results to be a surplus value for a more detailed reconstruction of the true solution . finally , we extend our analysis to a particular class of projected gradient methods in the case of non - negative solutions",
    ". + the plan of the paper is the following : in section 2 we introduce the optimization methods for the solution of we decided to analyze , while in section 3 the state - of - the - art on such algorithms as filtering methods is summarized , and the extensions to the scaled and projected cases are provided .",
    "section 4 shows the behaviour of the filter factors for the considered regularization algorithms through some numerical experiments , while our conclusions are offered in section 5 .",
    "a gradient method for the solution of is an iterative algorithm whose @xmath10th element is defined by @xmath11 where :    * @xmath12 is the gradient vector ; * @xmath13 is a symmetric and positive definite scaling matrix ; * @xmath14 is the steplength parameter .    in the literature , a large variety of possibilities for choosing the steplength @xmath14 and",
    "the scaling matrix @xmath13 has been proposed .",
    "classical examples of steplength selection are the steepest descent ( sd ) @xcite and the minimal gradient ( mg ) @xcite methods , which minimize @xmath15 and @xmath16 , respectively : @xmath17 @xmath18 in order to accelerate the slow convergence exhibited in most cases by the standard formulas and , many other strategies for the steplength selection have been proposed , as the two barzilai and borwein ( bb ) rules @xcite @xmath19 where @xmath20 and @xmath21 .",
    "these two schemes have been adapted to account for the scaling matrix @xcite as follows @xmath22 further accelerations have been proposed hereafter by alternating different steplength rules by means of an adaptively controlled switching criterion .",
    "examples of such rules are the adaptive steepest descent ( asd ) method , the adaptive barzilai - borwein ( abb ) method @xcite and its generalizations @xmath23 and @xmath24 provided by frassoldati et al .",
    "@xcite . + as far as the scaling matrix concerns , the first example we considered is given by the well - known conjugate gradient algorithm for the least squares problem ( cgls ) .",
    "indeed , this method can be expressed in the form by choosing the sd steplength and the scaling matrix @xmath25 in our analysis , we also included two scaling matrices arising from the constrained optimization case .",
    "the first is the one provided by the iterative space reconstruction algorithm ( isra ) , proposed by daube - whiterspoon and muehllener @xcite for reducing the computational cost of the expectation - maximization ( em ) algorithm of shepp and vardi @xcite in emission tomography .",
    "the explicit expression of the scaling matrix is @xmath26 where the quotient is intended in the hadamard sense , and we obtain the positive definiteness by thresholding the diagonal elements in a prefixed interval @xmath27 . + the second scaling matrix is the one described in @xcite by hager , mair and zhang ( hmz ) , that exploits a gradient splitting strategy @xcite with a resulting scaling matrix given by @xmath28 where @xmath29 and @xmath30 is a cyclic version of the first bb steplength rule computed by reusing @xmath31 for @xmath32 consecutive iterations @xcite .",
    "a further thresholding step assures again the positive definiteness of the scaling matrix .",
    "+ we remark that here we are _ not _ considering the isra and hmz algorithms , but we only borrow the scaling matrices defined in the algorithms themselves and use them in our scaled gradient scheme .",
    "+ we point out that , while the convergence of the nonscaled gradient methods with the steplengths described before is guaranteed , the presence of a scaling matrix might require a thresholding of the steplength in a fixed range of positive values @xmath33 $ ] , followed by the introduction of a successive steplength reduction strategy .",
    "a classical example is the well - known armijo rule @xcite : for given scalars @xmath34 and @xmath35 , the steplength @xmath14 is set equal to @xmath36 , where @xmath37 is the first non - negative integer @xmath38 for which @xmath39",
    "when considering a linear inverse problem with noisy data @xmath40 it is well - known that the role played by a regularization method is to contrast the amplifying effect on the noise @xmath41 due to the small singular values of @xmath1",
    ". indeed , if @xmath42 is the singular value decomposition of the matrix @xmath1 , @xmath43 , @xmath44 are the columns of @xmath45 , @xmath46 and @xmath47 are the diagonal elements of @xmath48 , then the following relation for the solution @xmath6 of holds : @xmath49 the classical recipe to contain the error propagation on the regularized solution @xmath50 consists of adding some weights in the linear combination that filter out the last components : @xmath51 the truncated singular value decomposition ( tsvd ) and the tikhonov method are examples of this approach , and the corresponding regularized solutions can be written in the form with , respectively , @xmath52 where @xmath53 and @xmath54 .",
    "besides the classical `` direct '' approaches as tsvd and tikhonov , also iterative regularization strategies can be thought by means of their filtering effect .",
    "indeed , expression can be generalized to the regularized solution @xmath55 provided by any gradient method by writing @xmath56 where the filter factors @xmath57 are automatically defined during the iterative procedure .",
    "the easiest example of gradient method is the landweber algorithm , which generates a sequence @xmath58 by @xmath59 where the steplength @xmath60 is fixed during the iterations and must satisfy @xmath61 in order to guarantee the convergence .",
    "the iteration can be rewritten , by choosing the initial guess @xmath62 , as @xmath63 or , equivalently , by means of the svd as @xmath64 thus obtaining an expression for the filter factors given by @xmath65 the previous equation suggests an extension to the case of the steplength @xmath14 varying at each iteration , in the case of nonscaled gradient .",
    "if @xmath66 , indeed , the filter factors of a general gradient method can be written as @xmath67 since the sequence generated by the gradient method converges to the least - squares solution @xmath6 , the filter factors @xmath57 will tend to 1 as @xmath68 increases . moreover , from the form of the products in we can observe that the convergence rate of the sequence @xmath69 to its limit value depends on the steplength values @xmath70 . in figure",
    "[ fig : filtriheatnoscal ] , the filter factors obtained in a numerical experiment with different steplength rules have been reported as functions of the singular values index @xmath71 . in particular , we considered the ` heat ` dataset from hansen s _ regularization tools _ @xcite and we plotted the filter factors generated at iteration 10 , 30 and at the iteration corresponding to the minimum error for gradient methods with the mg , sd , bb1 , bb2 , abb and abb@xmath72 steplength rules ( in this very last case , we used the revised version of the abb@xmath72 scheme proposed in @xcite ) . in all cases",
    "the filter factors start from zero ( since we chose @xmath62 as initial point ) and converge to 1 , with a faster convergence clearly visible for the factors corresponding to the largest singular values .",
    "moreover , the regular and smooth filter trends achieved in all the figures show that any steplength selection rule is not able to generate filter factors that behave as erratically as the true ones given by @xmath73 .    [",
    "cols=\"^,^,^ \" , ]",
    "in this paper we analyzed the regularizing effect of several gradient methods for the solution of the linear least - squares problem , i.e. , the ability of the scheme to produce a sequence of vectors that , at a certain iteration , approximates as close as possible the true solution .",
    "the analysis we carried out has been made in terms of the ability of a given method to reproduce correctly the filter factors of the solution , accounting for the way in which the resulting sequence opposes the amplification effect of the noise on the data due to the presence of small singular values .",
    "the starting point of our work has been a paper of nagy & palmer , in which the filter factors for nonscaled gradient methods have been formally calculated and numerically analyzed . in our paper",
    "we extended this analysis to the presence of scaling matrices in defining the descent direction , showing the analytical form of the corresponding filter factors and the advantages that can be obtained not only in terms of efficiency in decreasing the least - squares functional , but also in providing a better approximations of the unknown solution ( as remarked e.g. in @xcite ) .",
    "moreover , we also considered the case of non - negative constraints , and we generalized the expression of the filter factors to the projected gradient methods whose projection can be performed by means of the multiplication with a suitable diagonal matrix .",
    "we showed on some numerical examples the positive effect of the projection in reconstructing the irregular filter profiles of the true solution , in both scaled and nonscaled cases .",
    "this work has been partially supported by the italian spinner2013 phd project `` high - complexity inverse problems in biomedical applications and social systems '' and by a grant of the italian gruppo nazionale per il calcolo scientifico ( gncs ) - istituto nazionale di alta matematica ( indam ) .",
    "barzilai , j. , borwein , j.m . :",
    "two point step size gradient methods .",
    "i m a j. numer .",
    "8*(1 ) , 141148 ( 1988 ) bardsley , j. , nagy , j. : covariance - preconditioned iterative methods for nonnegatively constrained astronomical imaging .",
    "siam j. matrix anal .",
    "a. * 27*(4 ) , 11841198 ( 2006 ) benvenuto , f. , zanella , r. , zanni , l. , bertero , m. : nonnegative least - squares image deblurring : improved gradient projection approaches .",
    "inverse probl . * 26*(2 ) , 025004 ( 2010 ) bertero , m. , boccacci , p. : introduction to inverse problems in imaging .",
    "institute of physics , bristol ( 1998 ) bertero , m. , lanteri , h. , zanni , l. : iterative image reconstruction : a point of view , in mathematical methods in biomedical imaging and intensity - modulated radiation therapy ( imrt ) .",
    "( y. censor , m. jiang , and a. k. louis eds . ) , edizioni della normale , pisa , italy , birkhauser - verlag , 3763 ( 2008 ) bertsekas , d.p . : nonlinear programming .",
    "athena scientific , belmont ( 1999 ) bonettini , s. , zanella , r. , zanni , l. : a scaled gradient projection method for constrained image deblurring .",
    "inverse probl .",
    "* 25*(1 ) , 015002 ( 2009 ) bonettini , s. , landi , g. , loli piccolomini , e. , zanni , l. : scaling techniques for gradient projection - type methods in astronomical image deblurring .",
    "j. comput .",
    "math . * 90*(1 ) , 929 ( 2013 ) cauchy , a. : mthode gnrale pour la rsolution des systmes dequations simultanes . c. r. acad .",
    "paris * 25 * , 536538 ( 1847 ) dai , y.h . ,",
    "yuan , y.x .",
    ": alternate minimization gradient method .",
    "i m a j. numer .",
    "23*(3 ) , 377393 ( 2003 ) dai , y.h . ,",
    "hager , w.w .",
    ", schittkowski , k. , zhang , h. : the cyclic barzilai - borwein method for unconstrained optimization .",
    "i m a j. numer .",
    "anal . * 26*(3 ) , 604627 ( 2006 ) daube - witherspoon , m.e . ,",
    "muehllener , g. : an iterative image space reconstruction algorithm suitable for volume ect .",
    "ieee t. med .",
    "5*(2 ) , 6166 ( 1986 ) donatelli , m. , serra capizzano , s. : filter factor analysis of an iterative multilevel regularizing method .",
    ". ana . * 29 * , 163177 ( 2008 ) engl , h.w . , hanke , m. , neubauer , a. : regularization of inverse problems .",
    "kluwer academic publisher , dordrecht ( 2000 ) frassoldati , g. , zanni , l. , zanghirati , g. : new adaptive stepsize selections in gradient methods . j. indust",
    ". manag . optim .",
    "* 4*(2 ) , 299312 ( 2008 ) hager , w.w . ,",
    "bernard , a.m. , zhang , h. : an affine - scaling interior - point cbb method for box - constrained optimization .",
    "math . program . * 119*(1 ) , 132 ( 2009 ) hansen , p.c .",
    ": regularization tools : a matlab package for the analysis and solution of discrete ill - posed problems .",
    "algorithms * 6*(1 ) , 135 ( 1994 ) hansen , p.c . : rank - deficient and discrete ill - posed problems .",
    "siam , philadelphia ( 1997 ) hansen , p.c . ,",
    "nagy , j.g .",
    ", oleary , d.p . : deblurring images : matrices , spectra and filtering .",
    "siam , philadelphia ( 2006 ) lantri , h. , roche , m. , cuevas , o. , aime , c. : a general method to devise maximum likelihood signal restoration multiplicative algorithms with non - negativity constraints .",
    "signal process .",
    "* 81*(5 ) , 945974 ( 2001 ) lantri , h. , roche , m. , aime , c. : penalized maximum likelihood image restoration with positivity constraints : multiplicative algorithms .",
    "inverse probl . * 18*(5 ) , 13971419 ( 2002 ) nagy , j. , palmer , k. : steepest descent , cg and iterative regularization of ill - posed problems .",
    "43*(5 ) , 10031017 ( 2003 ) nocedal , j. , wright , s.j . : numerical optimization ( 2nd edition ) .",
    "springer , new york ( 2006 ) prato m. , cavicchioli r. , zanni l. , boccacci p. , bertero m. 2012 : efficient deconvolution methods for astronomical imaging : algorithms and idl - gpu codes .",
    "astronomy & astrophysics * 539 * , a133 shepp , l.a . ,",
    "vardi , y. : maximum likelihood reconstruction for emission tomography .",
    "ieee t. med .",
    "1*(2 ) , 113122 ( 1982 ) vogel , c.r .",
    ": computational methods for inverse problems .",
    "siam , philadelphia ( 2002 ) zhou , b. , gao , l. , dai , y.h .",
    ": gradient methods with adaptive step - sizes .",
    "* 35*(1 ) , 6986 ( 2006 )"
  ],
  "abstract_text": [
    "<S> many real - world applications are addressed through a linear least - squares problem formulation , whose solution is calculated by means of an iterative approach . a huge amount of studies has been carried out in the optimization field to provide the fastest methods for the reconstruction of the solution , involving choices of adaptive parameters and scaling matrices . however , in presence of an ill - conditioned model and real data , the need of a regularized solution instead of the least - squares one changed the point of view in favour of iterative algorithms able to combine a fast execution with a stable behaviour with respect to the restoration error . in this paper </S>",
    "<S> we want to analyze some classical and recent gradient approaches for the linear least - squares problem by looking at their way of filtering the singular values , showing in particular the effects of scaling matrices and non - negative constraints in recovering the correct filters of the solution . </S>"
  ]
}