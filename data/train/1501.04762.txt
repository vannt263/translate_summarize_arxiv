{
  "article_text": [
    "compressed sensing is a recently emerged technique for signal sampling and data acquisition which enables to recover sparse signals from much fewer linear measurements @xmath0 where @xmath1 is the sampling matrix with @xmath2 , @xmath3 denotes an @xmath4-dimensional sparse signal , and @xmath5 denotes the additive noise . such a problem has been extensively studied and a variety of algorithms , e.g. the orthogonal matching pursuit ( omp ) algorithm @xcite , the basis pursuit ( bp ) method @xcite , and the iterative reweighted @xmath6 and @xmath7 algorithms @xcite , were proposed . besides these methods , another important class of compressed sensing techniques that have received significant attention are bayesian methods , among which sparse bayesian learning ( also referred to as bayesian compressed sensing ) is considered as one of the most popular compressed sensing methods . sparse bayesian learning ( sbl )",
    "was originally proposed by tipping in his pioneering work @xcite to address the regression and classification problems . later on in @xcite , sparse bayesian learning was adapted for sparse signal recovery , and demonstrated superiority over the greedy methods and the basis pursuit method in a series of experiments . despite its superior performance , a major drawback of the sparse bayesian learning method is that it requires to compute an inverse of an @xmath8 matrix at each iteration , and thus has computational complexity that grows exponentially with the dimension of the signal .",
    "this high computational cost prohibits its application to many practical problems with even moderately large data sets .    in this paper",
    ", we develop a computationally efficient generalized approximate message passing ( gamp ) algorithm for sparse bayesian learning .",
    "gamp , introduced by donoho _ et .",
    "_ @xcite and generalized by rangan @xcite , is a newly emerged bayesian iterative technique developed in a message passing - based framework for efficiently computing an approximation of the posterior distribution of @xmath3 , given a pre - specified prior distribution for @xmath3 and a distribution for @xmath5 . in many expectation - maximization",
    "( em)-based bayesian methods ( including sbl ) , the major computational task is to compute the posterior distribution of the hidden variable @xmath3 .",
    "gamp can therefore be embedded in the em framework to provide an approximation of the true posterior distribution of @xmath3 , thus resulting in a computationally efficient algorithm .",
    "for example , in @xcite , gamp was used to derive efficient sparse signal recovery algorithms , with a markov - tree prior or a gaussian - mixture prior placed on the sparse signal . in this work , by resorting to gamp , we develop an efficient sparse bayesian learning method for sparse signal recovery .",
    "simulation results show that the proposed method performs similarly as the em - based sparse bayesian learning method , meanwhile achieving a significant computational complexity reduction .",
    "we note that an efficient sparse bayesian learning algorithm was developed in @xcite via belief propagation .",
    "the work , however , requires a sparse dictionary @xmath9 to facilitate the algorithm design , which may not be satisfied in practical applications .",
    "we first provide a brief review of the sparse bayesian learning method . in the sparse bayesian learning framework ,",
    "a two - layer hierarchical prior model was proposed to promote the sparsity of the solution . in the first layer , @xmath3 is assigned a gaussian prior distribution @xmath10 where @xmath11 is a non - negative hyperparameter controlling the sparsity of the coefficient @xmath12 .",
    "the second layer specifies gamma distributions as hyperpriors over the hyperparameters @xmath13 , i.e. @xmath14 where @xmath15 is the gamma function . besides",
    ", @xmath5 is assumed gaussian noise with zero mean and covariance matrix @xmath16 .",
    "we place a gamma hyperprior over @xmath17 : @xmath18 .",
    "an expectation - maximization ( em ) algorithm can be developed for learning the sparse signal @xmath3 as well as the hyperparameters @xmath19 . in the em formulation ,",
    "the signal @xmath3 is treated as hidden variables , and we iteratively maximize a lower bound on the posterior probability @xmath20 ( this lower bound is also referred to as the q - function ) . briefly speaking , the algorithm alternates between an e - step and a m - step . in the e - step ,",
    "we need to compute the posterior distribution of @xmath3 conditioned on the observed data and the estimated hyperparameters , i.e. @xmath21 it can be readily verified that the posterior @xmath22 follows a gaussian distribution with its mean and covariance matrix given respectively by @xmath23 where @xmath24 .",
    "the q - function , i.e. @xmath25 $ ] , can then be computed , where the operator @xmath26 $ ] denotes the expectation with respect to the posterior distribution @xmath22 . in the m - step ,",
    "we maximize the q - function with respect to the hyperparameters @xmath19 , which leads to the following update rules @xmath27 where @xmath28 denotes the expectation with respect to the posterior distribution @xmath22 .",
    "it can be seen that the em algorithm , at each iteration , requires to update the posterior distribution @xmath22 , which involves computing an @xmath8 matrix inverse .",
    "thus the em - based sparse bayesian learning algorithm has a computational complexity of order @xmath29 flops , and therefore is not suitable for many real - world applications with increasingly large data sets and unprecedented dimensions .",
    "we , in the following , will develop a computationally efficient sparse bayesian learning algorithm via gamp .",
    "generalized approximate message passing ( gamp ) is a very - low - complexity bayesian iterative technique recently developed @xcite for providing an approximation of the posterior distribution @xmath22 , conditioned on that the prior distribution for @xmath3 the distribution for the additive noise @xmath5 are factorizable .",
    "it therefore can be naturally embedded within the em framework to provide an approximate posterior distribution of @xmath3 to replace the true posterior distribution . from the gamp s point of view ,",
    "the hyperparameters @xmath19 are considered as known and fixed .",
    "the hyperparameters can be updated in the m - step based on the approximate posterior distribution of @xmath3 .",
    "gamp was developed in a message passing - based framework . by using central - limit - theorem approximations , the message passing between variable nodes and factor nodes",
    "can be greatly simplified , and the loopy belief - propagation on the underlying factor graph can be efficiently performed . in general , in the gamp algorithm development , the following two important approximations are adopted .",
    "let @xmath30 denote the hyperparameters .",
    "firstly , gamp assumes posterior independence among hidden variables @xmath31 and approximates the true posterior distribution @xmath32 by @xmath33 where @xmath34 and @xmath35 are quantities iteratively updated during the iterative process of the gamp algorithm , here we have dropped their explicit dependence on the iteration number @xmath36 for simplicity . substituting ( [ hm-1 ] ) into ( [ eqn-1 ] ) , it can be easily verified that the approximate posterior @xmath37 follows a gaussian distribution with its mean and variance given respectively as @xmath38 the other approximation is made to the noiseless output @xmath39 , where @xmath40 denotes the @xmath41th row of @xmath9 .",
    "gamp approximates the true marginal posterior @xmath42 by @xmath43 where @xmath44 and @xmath45 are quantities iteratively updated during the iterative process of the gamp algorithm , again here we dropped their explicit dependence on the iteration number @xmath36 . under the additive white gaussian noise assumption",
    ", we have @xmath46 .",
    "thus @xmath47 also follows a gaussian distribution with its mean and variance given by @xmath48    with the above approximations , we can now define the following two important scalar functions : @xmath49 and @xmath50 that will be used in the gamp algorithm . in the minimum mean - squared error ( mmse )",
    "mode , the input scalar function @xmath49 is simply defined as the posterior mean @xmath51 @xcite , i.e. @xmath52 the scaled partial derivative of @xmath53 with respect to @xmath34 is the posterior variance @xmath54 , i.e. @xmath55 the output scalar function @xmath50 is related to the posterior mean @xmath56 as follows @xmath57 the partial derivative of @xmath58 is related to the posterior variance @xmath59 in the following way @xmath60 given definitions of @xmath49 and @xmath50 , the gamp algorithm can now be summarized as follows ( details of the derivation of the gamp algorithm can be found in @xcite ) , in which @xmath61 denotes the @xmath62th entry of @xmath9 , @xmath63 and @xmath64 denote the posterior mean and variance of @xmath12 at iteration @xmath36 , respectively .",
    "* gamp algorithm *    [ cols= \" < \" , ]     .,width=264 ]    we first examine the phase transition behavior of respective algorithms . the phase transition is used to illustrate how sparsity level ( @xmath65 ) and the oversampling ratio ( @xmath66 ) affect the success rate of each algorithm in exactly recovering sparse signals in noiseless scenarios . in particular , each point on the phase transition curve corresponds to a success rate equal to @xmath67 .",
    "the success rate is computed as the ratio of the number of successful trials to the total number of independent runs .",
    "a trial is considered successful if the normalized squared error @xmath68 is no greater than @xmath69 .",
    "[ fig1](a ) plots the phase transitions of respective algorithms , where we set @xmath70 , and the oversampling ratio @xmath66 varies from @xmath71 to @xmath72 . from fig .",
    "[ fig1](a ) , we see that , when @xmath73 , the proposed sbl - gamp algorithm achieves performance similar to sbl - em , and is superior to bp - amp .",
    "the proposed method is surpassed by bp - amp as the oversampling ratio increases . nevertheless , sbl - gamp is still more appealing since we usually prefer compressed sensing algorithms work under high compression rate regions .",
    "the average run times of respective algorithms as a function of the signal dimension @xmath4 is plotted in fig .",
    "[ fig1](b ) , where we set @xmath74 and @xmath75 .",
    "results are averaged over 10 independent runs .",
    "we see that the sbl - gamp consumes much less time than the sbl - em due to its easy computation of the posterior distribution of @xmath3 , particularly for a large signal dimension @xmath4 .",
    "also , it can be observed that the average run time of the sbl - em grows exponentially with @xmath4 , whereas the average run time of the sbl - gamp grows very slowly with an increasing @xmath4 .",
    "this observation coincides with our computational complexity analysis very well .",
    "lastly , we examine the recovery performance in a noisy scenario , where we set @xmath76 , @xmath77 , and the signal to noise ratio ( snr ) is set to 20db . fig .",
    "[ fig2 ] depicts the normalized mean square errors ( nmse ) of respective algorithms vs. @xmath66 .",
    "results are averaged over 1000 independent runs .",
    "we see that the sbl - gamp algorithm achieves a similar recovery accuracy as the sbl - em algorithm even with a much lower computational complexity .",
    "we developed a computationally efficient sparse bayesian learning ( sbl ) algorithm via the gamp technique .",
    "the proposed method has a much lower computational complexity ( of order @xmath78 ) than the conventional sbl method .",
    "simulation results show that the proposed method achieves recovery performance similar to the conventional sbl method in the low oversampling ratio regime .",
    "d.  wipf and s.  nagarajan , `` iterative reweighted @xmath6 and @xmath7 methods for finding sparse solutions , '' _ ieee journals of selected topics in signal processing _ , vol .  4 , no .  2 , pp . 317329 , apr ."
  ],
  "abstract_text": [
    "<S> the sparse beyesian learning ( also referred to as bayesian compressed sensing ) algorithm is one of the most popular approaches for sparse signal recovery , and has demonstrated superior performance in a series of experiments . </S>",
    "<S> nevertheless , the sparse bayesian learning algorithm has computational complexity that grows exponentially with the dimension of the signal , which hinders its application to many practical problems even with moderately large data sets . to address this issue , </S>",
    "<S> in this paper , we propose a computationally efficient sparse bayesian learning method via the generalized approximate message passing ( gamp ) technique . </S>",
    "<S> specifically , the algorithm is developed within an expectation - maximization ( em ) framework , using gamp to efficiently compute an approximation of the posterior distribution of hidden variables . </S>",
    "<S> the hyperparameters associated with the hierarchical gaussian prior are learned by iteratively maximizing the q - function which is calculated based on the posterior approximation obtained from the gamp . </S>",
    "<S> numerical results are provided to illustrate the computational efficacy and the effectiveness of the proposed algorithm .    </S>",
    "<S> sparse bayesian learning , generalized approximate message passing , expectation - maximization . </S>"
  ]
}