{
  "article_text": [
    "intel xeon server cpus dominate in the commodity hpc market .",
    "although the microarchitecture of those processors is ubiquitous and can also be found in mobile and desktop devices , the average developer of numerical software hardly cares about architectural details and relies on the compiler to produce `` decent '' code with `` good '' performance . if we actually want to know what `` good performance '' means we have to build analytic models that describe the interaction between software and hardware . despite the necessary simplifications , such models can give useful hints towards the relevant bottlenecks of code execution and thus point to viable optimization approaches .",
    "the roofline model @xcite and the execution - cache - memory ( ecm ) model @xcite are typical examples .",
    "analytic modeling requires simplified machine and execution models , with details about properties of execution units , caches , memory , etc .",
    "although much of this data is provided by manufacturers , many relevant features can only be understood via microbenchmarks , either because they are not documented or because the hardware can not leverage its full potential in practice .",
    "one simple example is the maximum memory bandwidth of a chip , which can be calculated from the number , frequency , and width of the dram channels but which , in practice , may be significantly lower than this absolute limit .",
    "hence , microbenchmarks such as stream @xcite or ` likwid - bench ` @xcite are used to measure the limits achievable in practice .",
    "although there has been some convergence in processor microarchitecures for high performance computing , the latest cpu models show interesting differences in their performance - relevant features . building good analytic performance models and , in general",
    ", making sense of performance data , requires intimate knowledge of such details .",
    "the main goal of this paper is to provide a coverage and critical discussion of those details on the latest four intel architecture generations for server cpus : sandy bridge ( snb ) , ivy bridge ( ivb ) , haswell ( hsw ) , and broadwell ( bdw ) .",
    "the actual cpu models used for the analysis are described in sect .",
    "[ sec : hardware ] below .      out of the many possible approaches to performance analysis and optimization ( coined _ performance engineering _ [ pe ] ) we favor concepts based on analytic performance models . for",
    "recent server multicore designs the ecm performance model allows for a very accurate description of single - core performance and scalability .",
    "in contrast to the roofline model it drops the assumption of a single bottleneck for the steady - state execution of a loop . instead , time contributions from in - core execution and data transfers through the memory hierarchy are calculated and then put together according to the properties of a particular processor architecture ; for instance , in intel x86 server cpus all time contributions from data transfers including loads and stores in the l1 cache must be added to get a prediction of single - core data transfer time @xcite . on the other hand ,",
    "the ibm power8 processor shows almost perfect overlap @xcite . a full introduction to the ecm model would exceed the scope of this paper , so we refer to the references given above .",
    "the model has been shown to work well for the analysis of implementations of several important computational kernels @xcite .    in order to construct analytic models accurately , data about the capabilities of the microarchitecture and how it interacts with the code at hand is needed . for floating - point centric code in scientific computing , maximum throughput and latency numbers for arithmetic and load / store instructions",
    "are most useful in all their vectorized and non - vectorized , single ( sp ) and double precision ( dp ) variants . on intel",
    "multicore cpus up to haswell , this encompasses scalar , , , and avx2 instructions . modeling the memory hierarchy in the ecm model requires the maximum data bandwidth between adjacent cache levels ( assuming that the hierarchy is inclusive ) and the maximum ( saturated ) memory bandwidth . as for the caches",
    "it is usually sufficient to assume the maximum documented theoretical bandwidth ( presupposing that all prefetchers work perfectly to hide latencies ) , although latency penalties might apply @xcite .",
    "the main memory bandwidth and latency may depend on the mode and cache snoop mode settings .",
    "finally , the latest intel cpus work with at least two clock speed domains : one for the core ( or even individual cores ) and one for the uncore , which includes the l3 cache and memory controllers .",
    "both are subject to automatic changes ; in case of code on haswell and later cpus the guaranteed baseline clock speed is lower than the standard speed rating of the chip .",
    "the performance and energy consumption of code depends crucially on the interplay between these clock speed domains . finally , especially when it comes to power dissipation and capping , considerable variations among the specimen of the same cpu model can be observed .",
    "all these intricate architectural details influence benchmark and application performance , and it is insufficient to look up the raw specs in a data sheet in order to understand this influence .",
    "there is a large number of papers dealing with details in the architecture of cpus and their impact on performance and energy consumption . in @xcite",
    "the authors assessed the capabilities of the then - new nehalem server processor for workloads in scientific computing and compared its capabilities with its predecessors and competing designs . in @xcite , tools and techniques for measuring and tuning power and energy consumption of hpc systems were discussed .",
    "the quickpath interconnect ( qpi ) snoop modes on the haswell ep processor were investigated in @xcite .",
    "energy efficiency features , including the avx and uncore clock speeds , on the same architecture were studied in @xcite and @xcite .",
    "our work differs from all those by systematically investigating relevant architectural features , from the core level down to memory , via microbenchmarks in view of analytic performance modeling as well as important benchmark workloads such as linpack , graph500 , and hpcg .",
    "apart from confirming or highlighting some documented or previously published findings , this paper makes the following new contributions :    * we present benchmark results showing the improvement in the performance of the vector gather instruction from hsw to bdw .",
    "on bdw it is now advantageous to actually use the gather instruction instead of `` emulating '' it .",
    "* we fathom the capabilities of the l2 cache on all four microarchitectures and establish practical limits for l2 bandwidth that can be used in analytic ecm modeling .",
    "these limits are far below the advertised 64b / cy on hsw and bdw .",
    "* we study the bandwidth scalability of the l3 cache depending on the cluster on die ( cod ) mode and show that , although the parallel efficiency for streaming code is never below 85% , cod has a measurable advantage over non - cod . * we present latency data for all caches and main memory under various cache snoop modes and cod / non - cod .",
    "we find that although cod is best for streaming and aware workloads in terms of latency and bandwidth , highly irregular , numa - unfriendly code such as the graph500 benchmark benefits dramatically from non - cod mode with home snoop and opportunistic snoop broadcast by as much as 50% on bdw .",
    "* we show how the uncore clock speed on hsw and bdw has considerable impact on the power consumption of bandwidth- and cache - bound code , opening new options for energy efficient and power - capped execution .",
    "all measurements were performed on standard two - socket intel xeon servers .",
    "a summary of key specifications of the four generations of processors is shown in table  [ tab : testbed ] .",
    "according to intel s `` tick - tock '' model , a `` tick '' represents a shrink of the manufacturing process technology ; however , it should be noted that `` ticks '' are often accompanied by minor microarchitectural improvements while a `` tock '' usually involves larger changes .",
    "snb ( a `` tock '' ) first introduced , doubling the width from s 128bit to 256bit .",
    "one major shortcoming of snb is directly related to avx : although the register width has doubled and a second load unit was added , data path widths between the l1 cache and individual load / store units were left at 16b / cy .",
    "this leads to avx stores requiring two cycles to retire on snb , and avx loads block both units .",
    "ivb , a `` tick '' , saw an increase in core count as well as a higher memory clock ; in addition , ivb brought speedups for several instructions , e.g. , divide and square root ; see table  [ tab : instructions ] for details .",
    "hsw , a `` tock '' , introduced avx2 , extending the existing 256bit simd vectorization from floating - point to integer data types .",
    "instructions introduced by the extension are handled by two new , avx - capable execution units .",
    "data path widths between the l1 cache and registers as well as the l1 and l2 caches were doubled .",
    "a vector gather instruction provides a simple means to fill simd registers with non - contiguous data , making it easier for the compiler to vectorize code with indirect accesses . to maintain scalability of the core interconnect , hsw chips with more than eight cores move from a single - ring core interconnect to a dual - ring design . at the same time",
    ", hsw introduced the new mode , in which a chip is optionally partitioned into two equally sized domains in order to reduce latencies and increase scalability .",
    "starting with hsw , the system s qpi snoop mode can also be configured .",
    "hsw no longer guarantees to run at the base frequency with avx code .",
    "the guaranteed frequency when running avx code on all cores is referred to as `` avx base frequency , '' which can be significantly lower than the nominal frequency @xcite .",
    "also there is a separation of frequency domains between cores and uncore .",
    "the uncore clock is now independent and can either be set automatically ( when is enabled ) or manually via . as a `` tick , '' bdw , the most recent xeon - ep processor , offers minor architectural improvements .",
    "floating - point and gather instruction latencies and throughput have partially improved .",
    "the dual - ring design was made symmetric and an additional qpi snoop mode is available .",
    "all high - level language benchmarks ( graph500 , hpcg ) were compiled using intel icc 16.0.3 .",
    "for graph500 we used the reference implementation in version 2.1.4 , and for linpack we ran the intel - provided binary contained in mkl 2017.1.013 , the most recent version available at the time of writing .",
    "the likwid tool suite in its current stable version 4.1.2 was employed heavily in many of our experiments .",
    "all low - level benchmarks consisted of hand - written assembly .",
    "when available ( e.g. , for streaming kernels auch as stream triad and others ) we used the assembly implementations in the ` likwid - bench ` microbenchmarking tool . latency measurements in the memory hierarchy were done with all prefetchers turned off ( via ` likwid - features ` ) and a pointer chasing code that ensures consecutive cache line accesses .",
    "energy consumption measurements were taken with the ` likwid - perfctr ` tool via the rapl ( running average power limit ) interface , and the clock speed of the cpus was controlled with ` likwid - setfrequencies ` .",
    "starting with hsw , intel chips offer different base and turbo frequencies for avx and sse or scalar instruction mixes .",
    "this is due to the higher power requirement of using all simd lanes in case of avx . to reflect this behavior , intel introduced a new frequency nomenclature for these chips .",
    "the `` base frequency , '' also known as the `` non - avx base frequency '' or `` nominal frequency '' is the minimum frequency that is guaranteed when running scalar or code on all cores .",
    "this is also the frequency the chip is advertised with , e.g. , 2.30ghz for the xeon e5 - 2695v3 in table  [ tab : testbed ] . the maximum frequency that can be achieved when running scalar or sse code on all cores",
    "is called `` max all core turbo frequency . ''",
    "the `` avx base frequency '' is the minimum frequency that is guaranteed when running avx code on all cores and is typically significantly lower than the ( non - avx ) base frequency .",
    "analogously , the maximum frequency that can be attained when running avx code is called `` avx max all core turbo frequency . ''    on hsw , at least core running avx code resulted in a chip - wide frequency restriction to the avx max all core turbo frequency .",
    "on bdw , cores running scalar or sse code are allowed to float between the non - avx base and max all core turbo frequencies even when other cores are running avx code .    attained chip frequency during linpack runs on all cores on ( a ) bdw and ( b ) hsw .",
    "( c ) variation of clock speed and package power among all 1456 xeon e5 - 2630v4 cpus in rrze s `` meggie '' cluster running linpack.,title=\"fig : \" ] attained chip frequency during linpack runs on all cores on ( a ) bdw and ( b ) hsw .",
    "( c ) variation of clock speed and package power among all 1456 xeon e5 - 2630v4 cpus in rrze s `` meggie '' cluster running linpack.,title=\"fig : \" ] attained chip frequency during linpack runs on all cores on ( a ) bdw and ( b ) hsw .",
    "( c ) variation of clock speed and package power among all 1456 xeon e5 - 2630v4 cpus in rrze s `` meggie '' cluster running linpack.,title=\"fig : \" ]    all relevant values for the hsw and bdw specimen used can be found in table  [ tab : testbed ] . according to official documentation the actually used frequency depends on the workload ; more specifically , it depends on the percentage of avx instructions in a certain instruction execution window . to get a better idea about what to expect for demanding workloads , linpack and firestarter @xcite were selected to determine those frequencies .",
    "the maximum frequency difference between both benchmarks was 20mhz , so figure  [ fig : core_freqs ] shows only results obtained with linpack .",
    "figure  [ fig : core_freqs]a shows that bdw can maintain a frequency well above the avx _ and _ the non - avx base frequency for workloads running at its tdp limit of 145w ( measured package power during stress tests was 144.8w ) .",
    "hsw , shown in figure  [ fig : core_freqs]b , drops below the non - avx base frequency of 2.3ghz , but stays well above the avx base frequency of 1.9ghz while consuming 119.4w out of a 120w tdp .",
    "when running sse linpack , bdw consumes 141.8w and manages to run at the max all core turbo frequency of 2.8ghz .",
    "on hsw , running linpack with sse instructions still keeps the chip at its tdp limit ( 119.7w out of 120w ) ; the attained frequency of 2.6ghz is slightly below the max all core turbo frequency of 2.7ghz .    while it might be tempting to generalize from these results",
    ", we must emphasize that statistical variations even between specimen of the same cpu type are very common  @xcite .",
    "when examining all 1456 xeon e5 - 2630v4 ( 10-core , 2.2ghz base frequency ) chips of rrze s new `` meggie '' cluster , we found significant variations across the individual cpus .",
    "the chip has a max all core turbo and avx max all core turbo frequency of 2.4ghz @xcite .",
    "figure  [ fig : core_freqs]c shows each chip s frequency and package power when running linpack with sse or avx on all cores . with sse code",
    ", each chip manages to attain the max all core turbo frequency of 2.4ghz .",
    "however , a variation in power consumption can be observed . when running avx code , not all chips reach the defined peak frequency but stay well above the avx base frequency of 1.8 ghz .",
    "some chips do hit the frequency ceiling ; for these , a strong variation can be observed in the power domain .",
    "accurate predictions of instruction execution ( i.e. , how many clock cycles it takes to execute a loop body assuming a steady state situation with all data coming from the l1 cache ) are notoriously difficult in all but the simplest cases , but they are needed as input for analytic models . as a `` lowest - order '' and most optimistic approximation one can assume full throughput , i.e. , all instructions can be executed independently and are dynamically fed to the execution ports ( and the pipelines connected to them ) by the out - of - order engine .",
    "the pipeline that takes the largest number of cycles to execute all its instructions determines the runtime .",
    "the worst - case assumption would be an execution fully determined by the critical path through the code , heeding all dependencies . in practice",
    ", the actual runtime will be between these limits unless other bottlenecks apply that are not covered by the in - core execution , such as data transfers from beyond the l1 cache , instruction cache misses , etc .",
    "even if a loop body contains strong dependencies the throughput assumption may still hold if there are no loop - carried dependencies .",
    "l @0.2 in c c c c @0.1 in c c c c & & + arch & bdw & hsw & ivb & snb & bdw & hsw & ivb & snb + ` vdivpd ` ( avx ) & 24 & 35 & 35 & 45 & 16 & 28 & 28 & 44 + ` divpd ` ( sse ) & 14 & 20 & 20 & 22 & 8 & 14 & 14 & 22 + ` divsd ` ( scalar ) & 14 & 20 & 20 & 22 & * 4.5 * & 14 & 14 & 22 + ` vdivps ` ( avx ) & 17 & 21 & 21 & 29 & 10 & 14 & 14 & 28 + ` divps ` ( sse ) & 11 & 13 & 13 & 14 & 5 & 7 & 7 & 14 + ` divss ` ( scalar ) & 11 & 13 & 13 & 14 & * 2.5 * & 7 & 7 & 14 + ` vsqrtpd ` ( avx ) & 35 & 35 & 35 & 44 & 28 & 28 & 28 & 43 + ` sqrtpd ` ( sse ) & 20 & 20 & 20 & 23 & 14 & 14 & 14 & 22 + ` sqrtsd ` ( scalar ) & 20 & 20 & 20 & 23 & * 7 * & 14 & 14 & 22 + ` vsqrtps ` ( avx ) & 21 & 21 & 21 & 23 & 14 & 14 & 14 & 22 + ` sqrtps ` ( sse ) & 13 & 13 & 13 & 15 & 7 & 7 & 7 & 14 + ` sqrtss ` ( scalar ) & 13 & 13 & 13 & 15 & * 4 * & 7 & 7 & 14 + ` vrcpps ` ( avx ) & 7 & 7 & 7 & 7 & 2 & 2 & 2 & 2 + ` rcpps ` ( sse , scalar ) & 5 & 5 & 5 & 5 & 1 & 1 & 1 & 1 + ` * add * ` & 3,4^^ & 3 & 3 & 3 & 1 & 1 & 1 & 1 + ` * mul * ` & 3 & 5 & 5 & 5 & 0.5 & 0.5 & 1 & 1 + ` * fma * ` & 5,6^^ & 5,6^^ &  &  & 0.5 & 0.5 &  &  +   +   +    calculating the throughput and critical path predictions requires information about the maximum throughput and latency of all relevant instructions as well as general limits such as decoder / retirement throughput , l1i bandwidth , and the number and types of address generation units .",
    "the intel architecture code analyzer ( iaca ) can help with this , but it is proprietary software with an unclear future development path and it does not always yield accurate predictions .",
    "moreover , it can only analyze object code and does not work on the high - level language constructs .",
    "thus one must often revert to manual analysis to get predictions for the best possible code , even if the compiler can not produce it . in table  [",
    "tab : instructions ] we give worst - case measured latency and inverse throughput numbers for arithmetic instructions in avx , sse , and scalar mode . in the following we point out some notable changes over the four processor generations .",
    "the most profound change happened in the performance of the divide units .",
    "from snb to bdw we observe a massive decrease in latency and an almost three - fold increase in throughput for avx and sse instructions , in single and double precision alike .",
    "divides are still slow compared to multiply and add instructions , of course . the fact that the divide throughput _ per operation _ is the same for avx and sse",
    "is well known , but with bdw we see a significant rise in scalar divide throughput , even beyond the documented limit of one instruction every five cycles .",
    "the scalar square root instruction shows a similar improvement , but is in line with the documentation .",
    "the standard multiply , add , and fused multiply - add instructions have not changed dramatically over four generations , with two exceptions : together with the introduction of fma instructions with hsw , it became possible to execute two plain multiply ( but not add ) instructions per cycle .",
    "the latency of the add instruction in scalar and sse mode on bdw has increased from three to four cycles ; this result is not documented by intel for bdw but announced for avx code in the upcoming skylake architecture .",
    "the fma instruction shows the same characteristic ( latency increase from 5 to 6 cycles when using sse or scalar mode ) .",
    "one architectural feature that is not directly evident from single - instruction measurements is the number of address generation units ( agus ) . up",
    "to ivb there are two such units , each paired with a load unit with which it shares a port . as a consequence ,",
    "only two addresses per cycle can be generated .",
    "hsw introduced a third agu on the new port 7 , but it can only handle simple addresses for store instructions , which may lead to some restrictions .",
    "see sect .",
    "[ sec : l1 ] for details .",
    "the cores of all four microarchitectures feature two load units and one store unit .",
    "the data paths between each unit and the l1 cache are 16b on snb and ivb , and 32b on hsw and bdw .",
    "the theoretical bandwidth is thus 48b / cy on snb and ivb and 96b / cy on hsw and bdw ; however , several restrictions apply .",
    "an vectorized stream triad benchmark uses two avx loads , one avx , and one avx store instruction to update four dp elements .",
    "on hsw and bdw , only two are capable of performing the necessary address computations , i.e. , ( base + scaled index + offset ) , typically used in streaming memory accesses ; hsw s newly introduced third store can only perform offset computations .",
    "this means that only two addresses per cycle can be calculated , limiting the l1 bandwidth to 64b / cy .",
    "stream triad performance using only two is shown in figure  [ fig : l1-bw]a .",
    "one can make use of the new by using one of the `` fast lea '' units ( which can perform only indexed and no offset addressing ) to pre - compute an intermediate address , which is then used by the simple to complete the address calculation . this way both avx load units and",
    "the avx store unit can be used simultaneously .",
    "when the store is paired with address generation on the new store agu , both micro - ops are fused into a single micro - op .",
    "this means that the four micro - op per cycle front end retirement constraint should not be a problem : in each cycle two avx load instructions , the micro - op fused avx store instruction , and one avx instruction is retired . with sufficient unrolling ,",
    "loop instruction overhead becomes negligible and the bandwidth should approach 96b / cy .",
    "figure  [ fig : l1-bw ] shows , however , that micro - op throughput still seems to be the bottleneck because bandwidth can be further increased by removing the instructions from the loop body .    ( a ) l1 bandwidth achieved with stream triad and various optimizations on bdw .",
    "( b ) comparison of achieved l1 bandwidths using stream triad on all microarchitectures.,title=\"fig : \" ] ( a ) l1 bandwidth achieved with stream triad and various optimizations on bdw .",
    "( b ) comparison of achieved l1 bandwidths using stream triad on all microarchitectures.,title=\"fig : \" ]    figure  [ fig : l1-bw]b compares the bandwidths achievable by different microarchitectures ( using no arithmetic instructions on hsw and bdw for the reasons described above ) .",
    "on snb and ivb a regular stream triad code can almost reach maximum theoretical l1 performance because it only requires half the number of address calculations per cycle , i.e. , two are sufficient to generate three addresses every two cycles .",
    "vector gather is a microcode solution for loading noncontinuous data into vector registers .",
    "the instruction was first implemented in intel multicore cpus with avx2 on hsw .",
    "the first implementation offered a poor latency ( i.e. , the time until all data was placed in the vector register ) and using hand - written assembly to manually load distributed data into vector registers proved to be faster than using the gather instruction in some cases @xcite .    [ cols=\"^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     table  [ tab : l2_bw ] shows the measured bandwidths for a dot product ( a load - only benchmark ) and the stream triad .",
    "both snb and ivb operate near the specified bandwidth of 32b / cy for both access patterns .",
    "although hsw and bdw offer bandwidth improvements , especially in case of the dot product , measured bandwidths are significantly below the advertised 64b / cy .",
    "the question arises of how this result may be incorporated into the ecm model .",
    "preliminary experiments indicate that the ecm predictions for in - l3 data are quite accurate when assuming theoretical l2 throughput .",
    "we could thus interpret the low l2 performance as a consequence of a latency penalty , which can be overlapped when the data is further out in the hierarchy .",
    "further experiments are needed to substantiate this conjecture .",
    "together with the dual - ring interconnect , hsw introduced the mode , in which a single chip can be partitioned into two equally - sized clusters .",
    "hsw features a so - called `` eight plus @xmath0 '' design , in which the first physical ring features eight cores and the second ring contains the remaining cores ( six for our hsw chip ) .",
    "this asymmetry leads to a scenario in which the seven cores in the first cluster domain are physically located on the first ring ; the second cluster domain contains the remaining core from the first and six cores from the second physical ring .",
    "the asymmetry was removed on bdw : here both physical rings are of equal size so both cluster domains contain cores from dedicated rings . is intended for -optimized code and impacts l3 scalability and latency and , implicitly , main memory bandwidth because it uses a dedicated snoop mode that makes use of a directory to avoid unnecessary snoop requests ( see section  [ sec : snoop ] for more details ) .",
    "( a ) l3 scalability on hsw and bdw depending on whether is used .",
    "( b ) comparison of microarchitectures regarding l3 scalability .",
    "( c ) absolute l3 bandwidth for stream triad as function of cores on different microarchitectures.,title=\"fig : \" ] ( a ) l3 scalability on hsw and bdw depending on whether is used .",
    "( b ) comparison of microarchitectures regarding l3 scalability .",
    "( c ) absolute l3 bandwidth for stream triad as function of cores on different microarchitectures.,title=\"fig : \" ] ( a ) l3 scalability on hsw and bdw depending on whether is used .",
    "( b ) comparison of microarchitectures regarding l3 scalability .",
    "( c ) absolute l3 bandwidth for stream triad as function of cores on different microarchitectures.,title=\"fig : \" ]    figure  [ fig : l3-bw - and - scalability]a shows the influence of on l3 bandwidth ( using stream triad ) for hsw and bdw .",
    "when data is distributed across both rings on hsw , the parallel efficiency of the l3 cache is 92% ; it can be raised to 98% by using .",
    "the higher core count of bdw results in a more pronounced effect ; here , parallel efficiency is only 86% in non - cod mode . using",
    "the efficiency goes above 95% .",
    "figure  [ fig : l3-bw - and - scalability]b shows that hsw and bdw with cod offer similar l3 scalability as snb and ivb .    assuming an @xmath1-core chip , the topological diameter ( and with it the average distance from a core to data in an l3 segment ) is smaller in each of the @xmath2-core cluster domains in comparison to the non- domain consisting of @xmath1 cores .",
    "shorter ways between cores and data result in lower latencies when using mode .",
    "on bdw , the l3 latency is 41 cycles when with and 47 cycles without ( see table  [ tab : mem_latency ] ) .        starting with hsw",
    ", the qpi snoop mode can be selected at boot time .",
    "hsw supports three snoop modes : , , and ( often only indirectly selectable by enabling in the bios ) @xcite .",
    "bdw introduced a fourth snoop mode called + @xcite .",
    "the remainder of this section discusses the differences among the modes and the immediate impact on memory latency and bandwidth .    on a l3",
    "miss inside a domain , in addition to fetching the containing the requested data from main memory , cache coherency mandates other domains be checked for modified copies of the . attached to each l3 segment",
    "is a responsible for sending and receiving snoop information .",
    "in addition to multiple , each domain features a , which plays a major role in snooping .",
    "in , snoop requests are sent directly from the of the l3 segment in which the l3 miss occurred to the respective in other domains .",
    "queried remote directly respond back to the requesting ; in addition , they report to the in the requesting s domain , so it can resolve potential conflicts .",
    "involves a lot of requests and replies , but offers low latencies .    in , forward snoop requests to their domain s .",
    "the proceeds to fetch the requested from memory but stalls snoop requests to remote domains until the is available . for each , so - called directory information is stored in its memory ecc bits .",
    "the bits indicate whether a copy of the exists in other domains .",
    "the directory bits only tell whether a is present or not in other domains ; they do not tell which domain to query , so snoops have to broadcast to all domains . by waiting for directory data ,",
    "unnecessary snoop requests are avoided at the cost of higher latency due to delayed snoops . by reducing snoop requests , overall bandwidth",
    "can be increased . as in , potentially queried remote respond to the initiating and , which resolves potential conflicts .    in , a two - step approach is used . starting with hsw ,",
    "each features a 14 kb directory cache ( also called `` hitme '' cache ) holding additional directory information for present in remote domains .",
    "in addition to the directory information recorded in the ecc bits , the directory cache stores the particular domain in which the copy of a cl resides ; this means that on a hit in the directory cache only a single snoop request has to be sent .",
    "this mechanism further reduces snoop traffic , potentially increasing bandwidth .",
    "when the directory cache is hit , latency is also improved in compared to , because snoops are not delayed until directory information stored in ecc bits from main memory becomes available . in case of a directory cache",
    "miss , mode proceeds similarly to .",
    "note , however , that mode is recommended only for -aware workloads .",
    "the directory cache can only hold data for a small number of .",
    "if the number of shared between both cluster domains exceeds the directory cache capacity , mode degrades to mode , resulting in high latencies .",
    "bdw s new + mode works similarly to .",
    "however , will send opportunistic snoop requests while waiting for directory information stored in the ecc bits under `` light '' traffic conditions .",
    "latency is reduced in case the directory information indicates snoop requests have to be sent , because they were already sent opportunistically .",
    "redundant snoop requests are not supposed to impact performance under `` light '' traffic conditions .",
    "c @0.4 in c @0.4 in c @0.4 in c @0.4 in c arch & l1 & l2 & l3 & mem + snb & 4 & 12 & 40 & 230 + ivb & 4 & 12 & 40 & 208 + hsw & 4 & 12 & 37 ^ 2^ & 168 ^ 6^ + bdw & 4 & 12 & 47 ^ 1^ , 41 ^ 2^ & 248 ^ 3^ , 280 ^ 4^ , 190 ^ 5^ , 178 ^ 6^ +    the impact of snoop modes is largest on main memory latency . as expected , produces the best results with 178cy ( see table  [ tab : mem_latency ] ) .",
    "pointer chasing in main memory does not generate a lot of traffic on the ring interconnect , which is why + will generate opportunistic snoops , achieving a latency of 190cy .",
    "the difference in latency of 12cy compared to can be explained through shorter paths inside a single cluster domain in mode .",
    "we measured an l3 latency of 41cy for and 47cy for non- mode .",
    "since memory accesses pass through the interconnect twice ( one to request the , once to deliver it ) the memory latency of non- mode is expected to be twice the l3 latency penalty of six cycles . in",
    ", the requesting has to wait for its to acknowledge that it received all snoop replies from the remote , which causes a latency penalty .",
    "on bdw , the measured memory latency is 248cy .",
    "as expected , offers the worst latency at 280cy , because necessary snoop broadcasts are delayed until directory information becomes available from main memory .",
    "( a ) graph500 performance in millions of traversed edges per second ( mtep / s ) as function of snoop mode on bdw .",
    "( b ) graph500 performance of all chips .",
    "( c ) hpcg performance and performance per watt as function of uncore frequency.,title=\"fig : \" ] ( a ) graph500 performance in millions of traversed edges per second ( mtep / s ) as function of snoop mode on bdw .",
    "( b ) graph500 performance of all chips .",
    "( c ) hpcg performance and performance per watt as function of uncore frequency.,title=\"fig : \" ] ( a ) graph500 performance in millions of traversed edges per second ( mtep / s ) as function of snoop mode on bdw .",
    "( b ) graph500 performance of all chips .",
    "( c ) hpcg performance and performance per watt as function of uncore frequency.,title=\"fig : \" ]    graph500 was chosen to evaluate the influence of snoop modes on the performance of latency - sensitive workloads .",
    "figure  [ fig : graph500_and_hpcg_ufs_perf]a shows graph500 performance for a single bdw chip . a direct correlation between latency and performance",
    "can be observed for hs , es , and hs+osb .",
    "dir mode performs worst despite offering the best memory latency .",
    "this can be explained by the non -- awareness of the graph500 benchmarks .",
    "too much data is shared between both cluster domains ; this means the directory cache can not hold information on all shared . as a result , snoops are delayed until directory information from main memory becomes available .",
    "figure  [ fig : graph500_and_hpcg_ufs_perf]b shows an overview of graph500 performance on all chips and the qualitative improvement offered by the new + snoop mode introduced with bdw .    sustained main memory bandwidth on bdw for various access patterns .",
    "nt = nontemporal stores.,scaledwidth=90.0% ]    the effect of snoop mode on memory bandwidth for bdw is shown in fig .",
    "[ fig : mem_bw_broadep2 ] .",
    "the data is roughly in line with the reasoning above .",
    "for -aware workloads , should produce the least snoop traffic due to snoop information stored in the directory cache .",
    "this is reflected in a slightly better bandwidth compared to other snoop modes ( with the exception of the store access pattern , which seems to be a toxic case for mode ) .",
    "offers up to 10gb / s more for load - only access patterns when compared to , which produces the most amount of snoop traffic .",
    "the effect is less pronounced but still observable when comparing to and + .",
    "figure  [ fig : mem_bw_evolution ] shows the evolution of sustained memory bandwidth for all examined microarchitectures , using the best snoop mode on hsw and bdw .",
    "increases in bandwidths over the generations is explained by new ddr standards as well as increased memory clock speeds ( see table  [ tab : testbed ] ) .",
    "comparison of sustained main memory bandwidth across microarchitectures for various access patterns.,scaledwidth=90.0% ]      before hsw , the uncore was clocked at the same frequency as the cores . starting with hsw",
    ", the uncore has its own clock frequency .",
    "the motivation for this lies in potential energy savings : when cores do not require much data via the uncore ( i.e. , from / to l3 cache and main memory ) the uncore can be slowed down to save power .",
    "this mode of operation is called . for our bdw chip",
    ", the uncore frequency can vary automatically between 1.22.8ghz , but one can also define custom minimum and maximum settings within this range via .",
    "we examine the default behavior for both extremes of the roofline spectrum and use hpcg as a bandwidth - bound and linpack as a compute - bound benchmark .",
    "our findings indicate that at both ends of the spectrum , tends to select higher than necessary frequencies , pointlessly boosting power and in the case of linpack even hurting performance",
    ".    figure  [ fig : graph500_and_hpcg_ufs_perf]c shows hpcg performance and energy efficiency versus uncore frequency for a fixed core clock of 2.3ghz on hsw .",
    "we find that the uncore is the performance bottleneck only for uncore frequencies below 2.0ghz .",
    "increasing it beyond this point does not improve performance , because main memory is now the bottleneck .",
    "using performance counters the uncore frequency was determined to be the maximum of 2.8ghz when running hpcg in mode .",
    "the energy efficiency of 64.7gflop / s / w at 2.8ghz is 26% lower than the 87.2gflop / s / w observed at 2.0ghz uncore frequency , at almost the same performance .",
    "energy efficiency can be increased even more by further lowering the uncore clock ; however , below 2.0ghz performance is degraded .",
    "linpack performance on bdw as a function of core and uncore frequency.,scaledwidth=90.0% ]    for linpack , we observe a particularly interesting side effect of varying uncore frequency .",
    "figure  [ fig : hpl_perf_heatmap ] shows linpack performance on bdw as a function of core and uncore clock .",
    "note that in turbo mode , the performance increases when going from the highest uncore frequencies towards 1.8ghz .",
    "this effect is caused by uncore and cores competing for the chip s tdp .",
    "when the uncore clovk speed is reduced , a larger part of the chip s power budget can be consumed by the cores , which in turn boost their frequency .",
    "the core frequency in turbo mode is 2479mhz when the uncore clock is set to 2.8ghz ( the uncore actually only achieves a clock rate of 2475mhz ) vs 2595mhz when the uncore clock is set to 1.8ghz .",
    "below 1.8ghz the cpu frequency increases further , e.g. , to 2617mhz at an uncore clock of 1.7ghz and up to 2720mhz at an uncore clock of 1.2ghz .",
    "linpack performance starts to degrade at this point despite an increasing core frequency due to the uncore becoming a data bottleneck .",
    "in mode , the uncore is clocked at 2489mhz and the cores run at 2491mhz .",
    "compared to the optimum , degrades performance by 3% .",
    "energy efficiency is reduced by 6% from 4.94gflop / s / w at an uncore clock of 1.8ghz to 4.65gflop / s / w in .",
    "the most energy - efficient operating point for linpack is 5.74gflop / s / w at a core clock of of 1.6ghz and an uncore clock of 1.2ghz .",
    "we have conducted an analysis of core- and chip - level performance features of four recent intel server cpu architectures .",
    "previous findings about the behavior of clock speed and its interaction with thermal design limits on haswell and broadwell cpus could be confirmed .",
    "overall the documented instruction latency and throughput numbers fit our measurements , with slight deviations in scalar dp divide throughput and sse / scalar add and fused multiply - add latency on broadwell .",
    "we could also demonstrate the consequences of limited instruction throughput and the special properties of haswell s and broadwell s address generation units for l1 cache bandwidth .",
    "our microbenchmark results have unveiled that the gather instruction , which was newly introduced with the avx2 instruction set , was finally implemented on broadwell in a way that makes it faster than hand - crafted assembly . the l2 cache on haswell and broadwell",
    "does not keep its promise of doubled bandwidth to l1 but only delivers between 32 and 43b / cy , as opposed to sandy bridge and ivy bridge , which get close to their architectural limit of 32b / cy .",
    "the scalable l3 cache was one of the major innovations in the sandy bridge architecture . on haswell and broadwell ,",
    "the bandwidth scalability of the l3 cache is substantially improved in cluster on die ( cod ) mode . even without cod the full - chip efficiency ( at up to 18 cores )",
    "is never worse than 85% . in the memory domain",
    "we find , unsurprisingly , that cod provides the lowest latency and highest memory bandwidth ( except with streaming stores ) , but the irregular graph500 benchmark shows a 50% speedup on broadwell when switching to non - cod and home snoop with opportunistic snoop broadcast .",
    "finally , our analysis of core and uncore clock speed domains has exhibited significant potential for saving energy in a sensible setting of the uncore frequency , without sacrificing execution performance .",
    "future work will include a thorough evaluation of the ecm performance model on all recent intel architectures , putting to use the insights generated in this study .",
    "additionally , existing analytic power and energy consumption models will be extended to account for the uncore power more accurately .",
    "significant changes in performance and power behavior are expected for the upcoming skylake architecture , such as ( among others ) an l3 victim cache and avx-512 on selected models , and will pose challenges of their own",
    ".      barker , k. , davis , k. , hoisie , a. , kerbyson , d.j . ,",
    "lang , m. , pakin , s. , sancho , j.c . : a performance evaluation of the nehalem quad - core processor for scientific computing .",
    "parallel processing letters 18(4 ) , 453469 ( december 2008 ) , http://dx.doi.org/10.1142/s012962640800351x    gasc , t. , vuyst , f.d .",
    ", peybernes , m. , poncet , r. , motte , r. : building a more efficient lagrange - remap scheme thanks to performance modeling . in : papadrakakis , m. , et  al .",
    "eccomas congress 2016 , the vii .",
    "european congress on computational methods in applied sciences and engineering , crete island , greece , 510 june 2016 ( 2016 ) , https://www.eccomas2016.org/proceedings/pdf/12210.pdf      hackenberg , d. , schne , r. , ilsche , t. , molka , d. , schuchart , j. , geyer , r. : an energy efficiency feature survey of the intel haswell processor . in : 2015 ieee international parallel and distributed processing symposium workshop .",
    ". 896904 ( may 2015 )    hager , g. , treibig , j. , habich , j. , wellein , g. : exploring performance and power properties of modern multicore chips via simple machine models .",
    "concurrency computat . :",
    "( 2013 ) , doi : 10.1002/cpe.3180      hofmann , j. , fey , d. : an ecm - based energy - efficiency optimization approach for bandwidth - limited streaming kernels on recent intel xeon processors . in : proceedings of the 4th international workshop on energy",
    "efficient supercomputing .",
    "e2sc 16 , ieee press , piscataway , nj , usa ( 2016 ) , https://doi.org/10.1109/e2sc.2016.16    hofmann , j. , fey , d. , eitzinger , j. , hager , g. , wellein , g. : analysis of intel s haswell microarchitecture using the ecm model and microbenchmarks , pp .",
    "springer international publishing , cham ( 2016 ) , http://dx.doi.org/10.1007/978-3-319-30695-7\\textunderscore16    hofmann , j. , fey , d. , riedmann , m. , eitzinger , j. , hager , g. , wellein , g. : performance analysis of the kahan - enhanced scalar product on current multi - core and many - core processors .",
    "concurrency and computation : practice and experience pp .",
    "n / a  n / a ( 2016 ) , http://dx.doi.org/10.1002/cpe.3921    hofmann , j. , treibig , j. , hager , g. , wellein , g. : comparing the performance of different x86 simd instruction sets for a medical imaging application on modern multi- and manycore chips . in : proceedings of the 2014 workshop on programming models for simd / vector processing .",
    "wpmvp 14 , acm , new york , ny , usa ( 2014 ) , http://doi.acm.org/10.1145/2568058.2568068    : intel xeon processor e5 - 1600 , e5 - 2400 , and e5 - 2600 v3 product families - volume 2 of 2 , registers , http://www.intel.com/content/dam/www/public/us/en/documents/datasheets/xeon-e5-v3-datasheet-vol-2.pdf          molka , d. , hackenberg , d. , schne , r. , nagel , w.e . : cache coherence protocol and memory performance of the intel haswell - ep architecture . in : proceedings of the 44th international conference on parallel processing ( icpp15 ) .",
    "ieee ( 2015 )      schne , r. , treibig , j. , dolz , m.f . ,",
    "guillen , c. , navarrete , c. , knobloch , m. , rountree , b. : tools and methods for measuring and tuning the energy efficiency of hpc systems . scientific programming 22(4 ) , 273283 ( 2014 ) , http://dx.doi.org/10.3233/spr-140393    stengel , h. , treibig , j. , hager , g. , wellein , g. : quantifying performance bottlenecks of stencil computations using the execution - cache - memory model . in : proceedings of the 29th acm international conference on supercomputing .",
    "ics 15 , acm , new york , ny , usa ( 2015 ) , http://doi.acm.org/10.1145/2751205.2751240    treibig , j. , hager , g. , hofmann , h.g . , hornegger , j. , wellein , g. : pushing the limits for medical image reconstruction on recent standard multicore processors . the international journal of high performance computing applications 27(2 ) , 162177 ( 2013 ) , http://dx.doi.org/10.1177/1094342012442424      wilde , t. , auweter , a. , shoukourian , h. , bode , a. : taking advantage of node power variation in homogenous hpc systems to save energy , pp .",
    "springer international publishing , cham ( 2015 ) , http://dx.doi.org/10.1007/978-3-319-20119-1_27      wittmann , m. , hager , g. , zeiser , t. , treibig , j. , wellein , g. : chip - level and multi - node analysis of energy - optimized lattice boltzmann cfd simulations .",
    "concurrency and computation : practice and experience 28(7 ) , 22952315 ( 2016 ) , http://dx.doi.org/10.1002/cpe.3489"
  ],
  "abstract_text": [
    "<S> this paper presents a survey of architectural features among four generations of intel server processors ( sandy bridge , ivy bridge , haswell , and broadwell ) with a focus on performance with floating point workloads . starting on the core level and going down the memory hierarchy we cover instruction throughput for floating - point instructions , l1 cache , address generation capabilities , core clock speed and its limitations , l2 and l3 cache bandwidth and latency , the impact of cluster on die ( cod ) and cache snoop modes , and the uncore clock speed . using microbenchmarks we study the influence of these factors on code performance . </S>",
    "<S> this insight can then serve as input for analytic performance models . </S>",
    "<S> we show that the energy efficiency of the linpack and hpcg benchmarks can be improved considerably by tuning the uncore clock speed without sacrificing performance , and that the graph500 benchmark performance may profit from a suitable choice of cache snoop mode settings . </S>"
  ]
}