{
  "article_text": [
    "the density functional theory ( dft )  @xcite and the kohn - sham ( ks ) equations  @xcite are efficient techniques reducing the costs of solving the original schrdinger equations without sacrificing the accuracy in many practically important cases .",
    "however , numerical modeling of such properties as large surface reconstructions  @xcite and melting  @xcite , which involves molecular dynamics and systems with hundreds atoms , remains too time consuming for single - processor computers even in the dft - ks framework .",
    "parallelization , i.e. ,   software implementation suitable for computers with many processors , of electronic structure codes has become one of the main software development tasks as tens of thousands processors are already available on massively parallel systems , see  http://www.top500.org . in order to use efficiently these supercomputers , various _ ab initio _",
    "codes are being deeply modified  @xcite .",
    "efficient eigensolvers for iterative diagonalization of the hamiltonian matrix  @xcite are necessary for large systems . in the framework of _ ab initio _",
    "calculations , the most commonly used diagonalization method is the band - by - band conjugate gradient ( cg ) algorithm proposed by teter _",
    "et al . _",
    "@xcite for direct minimization of the total energy .",
    "thereafter , it has been modified by bylander _",
    "et al . _",
    "@xcite to fit the iterative diagonalization framework .",
    "this algorithm is fast and robust for small matrices , but requires explicit orthogonalization of residual vectors to already resolved bands . for larger matrices ,",
    "the residual minimization method  a direct inversion in the iterative subspace ( rmm - diis )  @xcite  is more efficient .",
    "this latter scheme based on the original work of pulay  @xcite and modified by wood and zunger  @xcite avoids orthogonalizations .",
    "the popular preconditioned block davidson scheme  @xcite is reported to be overtaken by the preconditioned cg ( pcg ) method for small and by the rmm - diis for large systems  @xcite .",
    "other widely used methods for large systems are the lanczos algorithm  @xcite with partial reorthogonalization and the chebyshev - filtered subspace iterations  @xcite with one diagonalization at the first iteration , neither utilizes preconditioning .    in solid state physics , it is convenient to expand wave - functions , densities and potentials over a plane waves ( pw ) basis set , where the wave - functions identify themselves with bloch s functions and periodic boundary conditions are naturally treated .",
    "moreover , the dependency of the total energy functional on the pw basis set is variational in this case .",
    "long - range potentials , which are hard to compute in real space , can be easily evaluated in reciprocal ( fourier ) space of the pw expansion coefficients .",
    "depending on the computational costs , quantities are computed in real or reciprocal space  and we go from one space to the other using backward and forward three - dimensional ( 3d ) fast fourier transformations ( ffts ) .    as iterative eigensolvers and ffts",
    "dominate in the overall computational costs of pw - based electronic structure codes , such as abinit  @xcite , efficient coordinated parallelization is necessary . in the present paper , we suggest and implement a parallelization scheme based on an efficient multiband eigenvalue solver , called the locally optimal block preconditioned conjugate gradient ( lobpcg ) method , and using optimized 3d ffts in the _ ab initio _ pw code abinit .",
    "in addition to the standard data partitioning over processors corresponding to different * k*-points , we introduce data partitioning with respect to blocks of bands as well as spatial partitioning in the fourier space of pw coefficients .",
    "the eigensolver that we use in this work is the lobpcg method proposed by knyazev  @xcite .",
    "our choice of the lobpcg is determined by its simplicity , effectiveness , ease of parallelization , acceptance in other application areas , and public availability of codes  @xcite . in the framework of",
    "_ ab initio _ self - consistent ( sc ) calculations , the use of lobpcg is apparently novel , see also @xcite where the lobpcg is adapted for the total energy minimization",
    ".    for ffts , we use the 3d fft implemented by goedecker _",
    "et al . _",
    "@xcite , which has previously demonstrated its efficiency on massively parallel supercomputers .",
    "this algorithm is cache optimized , minimizes the number of collective communications , and allows ffts with zero padding .    in section [ sec : sc ]",
    "we describe the flow chart and bottlenecks of self - consistent calculations to motivate our work on parallelization .",
    "we introduce the lobpcg eigensolver and compare it to the standard cg eigensolver is section [ sec : lobpcg ] .",
    "our multiband - fft parallelization and the optimizations performed are described in section [ sec : bfft ] . in section [ sec : results ] we demonstrate our numerical results and report scalability of our two - level ( multiband - fft ) and three - level ( * k*-points - multiband - fft ) parallelization a large - size system .",
    "in order to keep the problem setup general , we use the term in the right - hand side of the ks equations , the so - called overlap operator @xmath0 , which comes from the non - orthogonality of the wave - functions .",
    "it appears for ultra - soft pseudo - potentials ( uspp )  @xcite or the projector augmented - wave ( paw ) method  @xcite and is reduced to the identity in norm - conserving calculations .",
    "the minimum of the total energy functional is calculated by carrying on a sc determination of the density and potential in the sc loop . in the following flow - chart",
    ", we display schematically the sc loop with its various components in the framework of paw calculations in abinit : @xmath1({\\mathbf{r } } ) \\mathrm{\\;\\;\\ ; and \\;\\;\\ ; } \\rho_{ij}&\\longleftarrow & \\rm     \\left\\{c_{nk}({{\\mathbf{g}}});\\epsilon_{nk}\\right\\}\\\\    \\big\\downarrow&&\\big\\uparrow\\\\ \\rm    v_{loc}({\\mathbf{r } } ) \\mathrm{\\;\\;\\ ; and \\;\\;\\ ; } v_{nl}({\\mathbf{r } } ) & & \\rm    \\mid\\tilde{\\mathcal{h}}-\\epsilon_{n}\\mathcal{o}\\mid=0\\\\    \\big\\downarrow&&\\big\\uparrow\\\\    \\multicolumn{3}{c}{\\rm { \\big\\langlee^{i({{\\mathbf{k}}}+{{\\mathbf{g}}}).{{\\mathbf{r}}}}\\big|}\\tilde{\\mathcal{h}}{\\big|\\tilde{\\psi}_{nk}\\big\\rangle}=\\epsilon_{nk }    { \\big\\langlee^{i({{\\mathbf{k}}}+{{\\mathbf{g}}}).{{\\mathbf{r}}}}\\big|}\\mathcal{o}{\\big|\\tilde{\\psi}_{nk}\\big\\rangle}}\\\\   \\end{array}\\ ] ] the initiation step is that the wave - function of the system @xmath2 , where n and k are the band and the * k*-point indexes , respectively , is expanded over the pw basis set with coefficients @xmath3 and vectors @xmath4 in the reciprocal space .",
    "now the sc loop starts : first , the wave - function is used to compute the pseudized charge density @xmath5 as well as paw specific quantities : the compensation charge @xmath6 , needed to recover the correct multipolar development of the real charge density @xmath7 , and the matrix density @xmath8 , which defines the occupancies of the partial waves within the paw spheres .",
    "second , these densities are used to compute the local @xmath9 and non - local @xmath10 parts of the potential .",
    "third , as the current hamiltonian is determined , the linear ( generalized if the overlap operator @xmath0 is present ) eigenvalue problem projected over pw is now solved iteratively to obtain new approximate wave - functions pw coefficients @xmath3 corresponding to the minimal states . the next step is the subspace diagonalization involving all bands to improve the accuracy of approximate eigenvectors from the previous step . finally , the input and output densities ( or potentials ) are mixed .",
    "the sc loop stops when the error is lower than a given tolerance .      as the size of the system increases , various parts of the sc loop become very time consuming .",
    "parallelization of the most computationally expensive parts is expected to remove or at least widen the existing bottlenecks .",
    "we focus here on parallel algorithms to address the following tasks , in the order of importance : ( i ) iterative solution of linear generalized eigenvalue problems ( step 3 of the sc loop ) ; ( ii ) calculations of eigenvalues and eigenvectors in the subspace ( step 4 of the sc loop ) ; ( iii ) fft routines where the local potential is applied to wave - functions or where the density is recomputed ( necessary to apply the hamiltonian in the eigensolver in step 3 of the sc loop ) ; and ( iv ) computation of three non - local - like terms : the non - local part of the potential v@xmath11 , the overlap operator @xmath0 , and the @xmath12 matrix ( step 2 of the sc loop ) .",
    "a brief review of our approaches to these problems follows : to solve approximately eigenproblems ( i ) in parallel , we use multiband ( block ) eigenvalue solvers , where energy minimization is iteratively performed in parallel for sets of bands combined into blocks .",
    "scalapack is used to solve ( ii ) .",
    "3d ffts are well suited to perform fft in parallel to address ( iii ) , distributing the calculations over processors by splitting the pw coefficients .",
    "computation of non - local - like terms ( iv ) can be efficiently parallelized in the reciprocal space .",
    "our eigensolver , the lobpcg method , is a cg - like algorithm , which in its single - band version  @xcite calls the rayleigh - ritz procedure to minimize the eigenvalue @xmath13 within the 3d subspace @xmath14 spanned by @xmath15 where the index @xmath16 represents the iteration step number during the minimization and @xmath17 is the gradient of the rayleigh quotient @xmath18 given by @xmath19 since scaling of basis vectors is irrelevant , we replace @xmath17 with @xmath20 , so the new ritz vector is @xmath21 with the scalar coefficients @xmath22 , @xmath23 and @xmath24 being obtained by the rayleigh - ritz minimization on the subspace @xmath14 .",
    "the use of the variational principal to compute the iterative parameters justifies the name `` locally optimal '' and makes this method distinctive , as in other pcg methods , e.g. , in  @xcite , formulas for iterative parameters are explicit .",
    "to avoid the instability in the rayleigh - ritz procedure arising when @xmath25 becomes close to @xmath26 as the method converges toward the minimum , a new basis vector @xmath27 where @xmath28 , has been introduced  @xcite .",
    "replacing @xmath26 with @xmath29 in the basis of the minimization subspace @xmath14 does not change it but gives a more numerically stable algorithm : @xmath30 in order to accelerate the convergence and thus to improve the performance , a pw optimized preconditioner k is introduced and the preconditioned residual vectors @xmath31 replace @xmath32 in ( [ lobpcg_no_prec ] ) . in the next subsection we describe a multiband , or block , version of the lobpcg method , which is especially suitable for parallel computations .",
    "the single - band method ( [ lobpcg_no_prec ] ) can be used in the standard band - by - band mode , where the currently iterated band is constrained to be @xmath0-orthogonal to the previously computed ones .",
    "alternatively , method ( [ lobpcg_no_prec ] ) can be easily generalized  @xcite to iterate a block of @xmath33 vectors to compute @xmath34 bands simultaneously .",
    "it requires diagonalization of a matrix of the size @xmath35 on every iteration .    in large - scale _",
    "ab initio _ calculations the total number @xmath36 of bands is too big to fit all @xmath36 bands into one block @xmath37 as the computational costs of the rayleigh - ritz procedure on the @xmath35 dimensional subspace @xmath14 becomes prohibitive .",
    "instead , @xmath36 eigenvectors and eigenvalues are split into blocks @xmath38 and @xmath39 of the size @xmath40 .",
    "the other quantities involved in this algorithm are also defined by blocks ; the small letters are then replaced by capital letters , e.g. ,   the scalars @xmath22 , @xmath23 and @xmath24 are replaced with @xmath34-by-@xmath34 matrices @xmath41 , @xmath42 and @xmath43 .",
    "the algorithm for the block - vector @xmath44 of @xmath34 bands that we use here is the same as that in the lobpcg code in the general purpose library blopex  @xcite : +    * require * : let @xmath45 be a block - vector and @xmath46 be a preconditioner ; the @xmath47 is initialized to 0 .",
    "+ * for * i=1,@xmath48,`nline ` * do *    1 .",
    "@xmath49 2 .",
    "@xmath50 3 .",
    "@xmath51 ) 4 .",
    "we apply the rayleigh - ritz method within the subspace @xmath14 spanned by the columns of @xmath52 and @xmath53 to find @xmath54 corresponding to the minimal @xmath34 states within @xmath14 5 .",
    "@xmath55    * end for *     + the lobpcg algorithm as formulated above computes only the @xmath34 lowest states , while we need to determine all @xmath56 states .",
    "so we perform lobpcg algorithm within the loop over the blocks , where the current lobpcg block is constrained to be @xmath0-orthogonal to the previous ones . in other words",
    ", we iterate in a multiband - by - multiband fashion .",
    "typically we perform a fixed number ` nline ` of iterations in each sweep on the eigensolver , so the total number of iterations to approximate the solution to the eigenvalue problem with the given fixed hamiltonian is ` nline`@xmath57    to improve the accuracy , we conclude these multiband - by - multiband iterations with a final rayleigh - ritz call for all @xmath36 bands , as pointed out in the description of the sc loop in section [ sec : sc ] , but this procedure is technically outside of the lobpcg routine in the abinit code .",
    "the lobpcg algorithm is much better supported theoretically  @xcite and can even outperform the traditional pcg method  @xcite as we now demonstrate .",
    "we test band - by - band lobpcg , full - band lobpcg and cg methods for two different systems : a simple one with two atoms of carbon within a diamond structure and a somewhat larger and rather complex system with sixteen atoms of plutonium in the @xmath58 monoclinic structure .",
    "the time per iteration is approximately the same for the single - band lobpcg and pcg methods for single - processor calculations , so the complexity of one self - consistent electronic step is roughly the same for both methods . for the full - band lobpcg",
    "the number of floating - point operations is higher than that in the band - by - band version because of the need to perform the rayleigh - ritz procedure on the @xmath59 dimensional subspace , but the extensive use of the high - level blas for matrix - matrix operations in lobpcg may compensate for this increase .",
    "thus , we display the variation of the total energy as a function of the number of self - consistent electronic steps and , as the efficiency criterion , we use the final number of these steps needed for the variation of the total energy to reach the given tolerance in the sc loop .",
    "the numbers @xmath34 and @xmath36 are called @xmath60 and @xmath61 , correspondingly , in the abinit code , so we use the latter notation in our figures below .",
    "we show the variation of the total energy as a function of the number of electronic steps for the simple system using the tolerance 10@xmath62 hartree in figure  [ fig : cdiamond ] . in this example , the number of iterations ` nline ` carried out by the lobpcg or pcg method at each electronic step is set to 4 , which is a typical value in abinit for many systems . increasing ` nline ` does not change the number of electronic steps needed to reach the 10@xmath62 hartree tolerance in this case and therefore is inefficient .",
    "convergence is slightly faster for the lobpcg method , which takes 7 electronic steps vs. 10 steps needed by the pcg .",
    "the full - band lobpcg method does not improve the convergence of the sc loop in this case compared to the band - by - band lobpcg .    for the plutonium in its @xmath58 phase system",
    ", the typical value ` nline`=4 appears too small , so we increase it to ` nline`=6 and display the results in figure  [ fig : pualpha ] .",
    "we observe that the number of electronic steps is 40 for both band - by - band lobpcg and pcg methods methods .",
    "but the full - band ( @xmath63 ) lobpcg method finishes first with a clear lead in just 30 electronic steps",
    ". further increase of ` nline ` does not noticeably affect the number of electronic steps .",
    "we conclude that the lobpcg method is competitive compared to the standard pcg method and that choosing a larger block size @xmath33 in lobpcg may help to further improve the performance in sequential ( one - processor ) computations",
    ". however , the main advantage of the lobpcg method is that it can be efficiently parallelized as we describe in the next section .",
    "the lobpcg algorithm works multiband - by - multiband , or block - by - block , so it can be easily parallelized over bands by distributing the coefficients of the block over the processors .",
    "the main challenge is to make the multiband data partitioning compatible with the data partitioning suitable for the 3d parallel fft . in this section",
    "we describe these partitioning schemes and an efficient transformation between them .",
    "we implement two levels of parallelization using virtual cartesian two dimensional mpi topology with the so - called `` fft - processors '' and `` band - processors '' along virtual horizontal and vertical directions , correspondingly .",
    "let @xmath64 be the number of planes along one direction of the 3d reciprocal lattice of vectors @xmath65 .",
    "we choose numbers @xmath66 and @xmath67 of band- and fft - processors to be divisors of @xmath68 and @xmath64 , respectively , so that the total number of processors is ` nproc`=@xmath69 .",
    "all the quantities expanded over the pw basis set : wave - functions , densities , potentials , etc .",
    ", undergo similar distributions .",
    "we focus below on the coefficients of the wave - functions , and make a comment about other data at the end of the section .",
    "we omit the index @xmath70 in the following .",
    "we assume that the pw coefficients @xmath71 of a given band @xmath72 are originally defined on the processor at the virtual corner and first the @xmath64 planes are successively distributed over the @xmath67 fft - processors , such that the pw coefficients of all the bands are distributed along the first row of the virtual two dimensional processor topology : @xmath73 there are now @xmath74 planes defined on each processor .",
    "the @xmath75 sub - sets for @xmath76 define a partition of the whole 3d reciprocal lattice of vectors @xmath65 .",
    "this distribution is typically used in conjunction with goedecker s parallel fft routine . to parallelize further , these planes are sub - divided again by distributing the pw coefficients over the @xmath66 band - processors as : @xmath77 the @xmath78 sub - sub - sets for @xmath79 partition the @xmath75 sub - set . when performing lobpcg calculations with blocks of size @xmath66 , the coefficients are grouped in @xmath80 blocks of size @xmath66 , e.g. ,",
    "for the first @xmath66 bands we have @xmath81 this data distribution is well suited to perform dot - products as well as matrix - vector or matrix - matrix operations needed in the lobpcg algorithm . in particular , blocks of the gram matrix are computed by the blas function ` zgemm ` on each processor using this distribution . in the next section",
    "we will show that the ` zgemm ` function plays the main role in superlinear scaling of the lobpcg algorithm .",
    "however , such a distribution is not appropriate to perform parallel 3d ffts and thus has to be modified .",
    "this is achieved by transposing the pw coefficients inside each column , so the following distribution of the first @xmath66 bands is thus obtained : @xmath82 where each band is distributed over the @xmath67 fft - processors .",
    "the distribution is now suitable to perform 3d parallel ffts .",
    "the efficient 3d fft of goedecker _ et al . _",
    "@xcite implemented in our code is then applied on each virtual row of the processor partition , so @xmath66 parallel 3d ffts are performed simultaneously .",
    "the transformation used to transpose the coefficients within a column corresponds to the mpi_alltoall communication using the `` band communicator . '' during the 3d fft , the mpi_alltoall communication is also performed within each row using the `` fft communicator . ''",
    "these two communications are not global , i.e. , they do not involve communications between all processors , but rather they are local , involving processors only within virtual rows or columns .",
    "after ` nline ` lobpcg iterations for this first block of bands , we apply the same strategy to the next block , with the constraint that all bands in the block are orthogonal to the bands previously computed .",
    "finally , this last distribution is used to compute by fft the contributions of a given band to the density , which are subsequently summed up over all bands - processors to calculate the electronic density distributed over the fft - processors .",
    "the electronic density is in its turn used to produce local potentials having the same data distribution .      here",
    "we address the issue ( ii ) brought up in section [ sec : sc ] that diagonalization of a matrix of the size @xmath83 is computationally expensive .",
    "the standard implementation of this calculation is performed in the sequential mode by calling lapack to diagonalize the same matrix on every processor .",
    "it becomes inefficient if the number of bands @xmath36 or the number of processors increases .",
    "to remove this bottleneck , we call the scalapack library , which is a parallel version of lapack .",
    "the tests performed using scalapack show that this bottleneck is essentially removed for systems up to 2000 bands .",
    "for instance , on 216 processors this diagonalization takes 23% of the total time for a system with 1296 bands using lapack on each processor , but only 3% using scalapack .      finally , we address bottleneck ( iv ) formulated in section [ sec : sc ] , i.e. ,   computation of the three non - local like terms ( the non - local part of the potential v@xmath11 , the overlap operator @xmath0 and the @xmath12 matrix ) , which is one of the most time consuming parts of the sc loop .",
    "parallelization is straightforward if these terms are computed in reciprocal space and the distribution of the pw is efficient .",
    "the data distribution here corresponds to the one used in the lobpcg .",
    "computation of these terms can be made more efficient in parallel if we utilize the data distribution used for the 3d parallel ffts based on one virtual line of processors @xcite .",
    "the cause of better efficiency here comes from merging two parallel features . on the one hand",
    ", we avoid any collective communication on the whole set of processors , thus reducing the data over each line independently rather than over the whole set of processors .",
    "on the other hand , we obtain better load - balancing of the processors since the fft data are not sub - divided in this case .",
    "calculations are performed on tera-10 novascale 5160 supercomputer , composed of 544 nodes ( with 8 dual - core 1.6 ghz intel montecito processors per node ) connected by quadrics at the `` comissariat  lnergie atomique . ''",
    "benchmarks are carried out on a supercell calculation with 108 atoms of gold set in their face centered cubic lattice positions and 648 bands .",
    "an energy cutoff of 24 ha is used in these calculations , leading to a three dimensional reciprocal grid with 108@xmath84 points . in all tests ` nline`=4 .",
    "calculations are performed on various numbers of processors ` nproc`=1 , 4 , 18 , 54 , 108 , 162 , and 216 . for each value of ` nproc ` , we consider all distributions @xmath85 which are allowed within this framework .",
    "we remind the reader that @xmath67 has to be smaller than 108/2=54 , in order to use the 3d parallel fft , whereas @xmath66 and @xmath67 have to be divisors of @xmath86 and @xmath87 , respectively . for instance , for ` nproc`=18 , we can choose @xmath8818@xmath891 , 9@xmath892 , 6@xmath893 , 3@xmath896 , 2@xmath899 and 1@xmath8918 .",
    "we focus on three kinds of benchmarks here : the first two are @xmath901 ( band - only ) and 1@xmath91 ( fft - only ) distributions .",
    "the virtual mpi topology is one dimensional in these cases .",
    "the third kind is the optimized m@xmath89p ( band - fft ) distribution .",
    "the speedups for these three parallelization schemes are shown in figure  [ fig : scaling-2d ] .",
    "the transposition of the data distribution within each column during the computation of @xmath92 and the use of collective communications inside lines for the fft , applying @xmath93 to the wave - functions , are the two most important communications used in the framework of the band - fft parallelization .",
    "the communications within the ffts increase to 17% of the total time on 216 processors using the 1@xmath91 ( fft - only ) .",
    "in contrast , the @xmath901 ( band - only ) distribution is adequate for up to 54 processors .",
    "however , further increase of the block size @xmath66 together with the number of processors is inefficient in this case for more than 100 processors .",
    "we remind the reader that the lobpcg solver performs the rayleigh - ritz procedure on subspaces of dimension 3@xmath66 on every iteration . for large @xmath66",
    "this starts dominating the computational costs even on @xmath66 processors .",
    "so we reduce @xmath66 and introduce our double parallelization . for the optimal band - fft parallelization",
    "we obtain a superlinear scaling up to 100 processors , and a speedup of 150 for 200 processors .    figure  [ fig : bandfft - publi ] displays scaling of different parts of the abinit code using a slightly different format : the vertical axis represents the scalability divided by the number of processors , so the linear scaling is now displayed by the horizontal line .",
    "we also zoom in the horizontal axis to better represent the scalability for small numbers of processors .",
    "the abinit curve shows exactly the same data as in figure  [ fig : scaling-2d ] .",
    "the lobpcg curve represents the cumulative timing for the main parts that the lobpcg function calls in abinit : ` zgemm ` , non - local ( @xmath92 ) and local ( @xmath93 using fft ) parts of the potential .",
    "figure  [ fig : bandfft - publi ] demonstrates that the super - linear behavior is due to the ` zgemm ` . in the sequential mode we run the band - by - band eigensolver that does not take much advantage of the blas function ` zgemm ` use , since only matrix - vector products are performed in this case .",
    "so the super - linear behavior is not directly related to the number of processors , but is rather explained by the known positive effect of vector blocking in lobpcg that allows calling more efficient blas level 3 functions for matrix - matrix operation rather then blas level 2 functions for matrix - vector operations .",
    "this effect is especially noticeable in lobpcg since it is intentionally written in such a way that the vast majority of linear algebra related operations are matrix - matrix operations ( performed in lobpcg using the ` zgemm ` ) , which represent 60% of the total time in band - by - band calculations on a single processor , but exhibits a strong speedup with the increase of the block size in the parallel mode .",
    "the acceleration effect is hardware - dependent and is a result on such subtle processor features as multi - level cache , pipelining , and internal parallelism .",
    "using the hardware - optimized blas is crucial : in one of our tests the time spent in the ` zgemm ` routine represents only 10% of the total time using block size @xmath94 if hardware - optimized blas is used , but 53% otherwise .",
    "all the computation results reported here are obtained by running the abinit code with the hardware - optimized blas .",
    "the effectiveness of the blas level 3 matrix - matrix ` zgemm ` also depends on the sizes of the matrices involved in the computations .",
    "this is the point where the parallel implementation that partitions the matrices may enter the scene and play its role in the acceleration effect .",
    "parallelization over * k*-points is commonly used in _ ab initio _ calculations and is also available in abinit .",
    "it is very efficient and easy to implement , and results in a roughly linear scaling with respect to the number of processors .",
    "this kind of parallelization is evidently limited by the number @xmath70 of * k*-points available in the system .",
    "it appears theoretically useless for large supercell calculations or disorder systems with only the @xmath95 point within the brillouin zone , but in practice sampling of the brillouin zone is sometimes needed , e.g.,for surfaces and interfaces in the direction perpendicular to the stacking sequence  @xcite , or for accurate thermodynamic integrations for metling  @xcite .",
    "parallelization over * k*-points is clearly well suited for metals where a large number @xmath70 of * k*-points is required .",
    "we now combine the * k*-point and the band - fft parallelization by constructing a virtual three - dimensional configuration of processors @xmath96 , where the band - fft parallelization is applied on each * k*-point in parallel .    in figure  [",
    "fig : scaling-3d ] we present the speedup obtained for this triple parallelization .",
    "we use the same system as previously except that 10 * k*-points rather than 1 are sampled in the brillouin zone .",
    "we use as the reference the limit of the super - linear scaling obtained in the framework of the band - fft parallelization : 108 processors with a 36@xmath893 distribution . as expected , adding * k*-point processors in the virtual third dimension of the 3d processor configuration results in the linear scaling of computational costs . using our three - level parallelization on 10 * k*-points",
    "the abinit scales linearly up to 1000 processors .",
    "our combined * k*-points - multiband - fft parallelization allows the material science community to perform large scale electronic structure calculations efficiently on massively parallel supercomputers up to thousand of processors using the publicly available software abinit .",
    "the next generation of petascale supercomputers will bring new challenges .",
    "novel numerical algorithms and advances in computer sciences , e.g. , introduction of asynchronous ( non - blocking ) collective communication standards  @xcite , would help us to minimize the communication load in future versions of abinit and make efficient use of the next generation hardware .",
    "x.  gonze , j .-",
    "beuken , r.  caracas , f.  detraux , m.  fuchs , g .-",
    "rignanese , l.  sindic , m.  verstraete , g.  zerah , f.  jollet , m.  torrent , a.  roy , m.  mikami , p.  ghosez , j .- y .",
    "raty , d.  allan , comput .",
    "sci 25 ( 2002 ) 478 .",
    "a.  v. knyazev , in international ser .",
    "numerical mathematics , v. 96 , birkhauser , basel , 1991 , pp . 143154 ."
  ],
  "abstract_text": [
    "<S> we suggest and implement a parallelization scheme based on an efficient multiband eigenvalue solver , called the locally optimal block preconditioned conjugate gradient ( lobpcg ) method , and using an optimized three - dimensional ( 3d ) fast fourier transform ( fft ) in the _ ab initio _ plane - wave code abinit . </S>",
    "<S> in addition to the standard data partitioning over processors corresponding to different * k*-points , we introduce data partitioning with respect to blocks of bands as well as spatial partitioning in the fourier space of coefficients over the plane waves basis set used in abinit . </S>",
    "<S> this * k*-points - multiband - fft parallelization avoids any collective communications on the whole set of processors relying instead on one - dimensional communications only . for a single * k*-point </S>",
    "<S> , super - linear scaling is achieved for up to 100 processors due to an extensive use of hardware optimized blas , lapack and scalapack routines , mainly in the lobpcg routine . </S>",
    "<S> we observe good performance up to 200 processors . with 10 * </S>",
    "<S> k*-points our three - way data partitioning results in linear scaling up to 1000 processors for a practical system used for testing . </S>",
    "<S> + 2007 bottin , leroux , knyazev , zrah . </S>",
    "<S> all rights reserved .    </S>",
    "<S> ,    ,    ,    received may 11 , 2007 ; received in the revised form july 11 , 2007 ; accepted july 22 , 2007    density functional theory , abinit , eigenvalue , lobpcg , scalapack , fft , parallelization , mpi , supercomputing 71.15.dx </S>"
  ]
}