{
  "article_text": [
    "in modern cosmology , many attempts have been made to determine the large - scale structure of the physical universe using constraints provided by cosmological observations and knowledge derived from local physical experiments .",
    "the most common approach is to adopt the postulate that the universe is spatially homogeneous on large scales  the friedmann - lematre - robertson - walker ( flrw ) model .",
    "hence using observational data to determine the few free parameters characteristic of such universe models has become the primary objective , and this overall framework has been presented in great detail in the literature .",
    "the cosmological principle ( see @xcite and @xcite ) expresses spatial homogeneity as a point of principle , whereas the copernican principle merely states that we are not privileged observers .",
    "as pointed out in @xcite , the cosmological principle determines a complete universe model , but we can not verify it fully due to the predictions it makes about parts of the universe far beyond our observations . although the copernican principle only has implications for the observable universe , its validity can potentially be proven with observations . despite this , it is the much stronger cosmological principle which is almost invariably assumed in practice . certainly , a good argument for homogeneity is provided by the ehler - geren - sachs ( egs ) theorem @xcite , which required exactly isotropic cmbr observations for all observers , and the ` almost egs theorem ' , or stoeger - maartens - ellis ( sme ) theorem @xcite , which allows small anisotropies in the cosmic microwave background radiation ( cmbr ) and obtains an ` almost flrw ' geometry @xcite .",
    "but our attempts to verify homogeneity should not stop there .    the general assumption",
    "that the universe has a robertson - walker metric on the very large scale has served cosmology well , and is implicit in many calculations . but this makes verifying homogeneity rather tricky as there is a distinct danger of a circular argument . to analyse the cosmological data consistently requires the use of a non - homogeneous metric .",
    "current and planned galaxy surveys are vastly increasing the amount of cosmological data available for analysis .",
    "already in recent years there has been a dramatic improvement in the number of measured cosmological parameters and the accuracy of their values .",
    "properties of the matter distribution have been well studied , but always with the assumption of a homogeneous background metric . as accurate",
    "cosmological data accumulates , the proper reduction and interpretation of the high redshift data will require knowledge of the cosmic geometry that is traversed by the light rays we observe .",
    "it will no longer be necessary to assume homogeneity , the data will make it possible to quantify the level of homogeneity on different scales .",
    "hence , being able to _ prove _ the homogeneity of the observable region of the universe rather than assuming it in principle is a long term objective of the current project .",
    "there are of course a variety of methods for checking homogeneity , such as the sunyaev - zeldovich effect , and it is important to pursue the full range of methods .",
    "it should be emphasised that radial homogeneity is far harder to prove clearly than isotropy .",
    "our cosmological observations are restricted to our past null cone , which is a 3-dimensional slice through our 4-dimensional spacetime , and the expected variation of observations with redshift is affected by the cosmic equation of state , the evolution of the observed sources , and the geometry of spacetime .",
    "disentangling these effects without assuming homogeneity is not a trivial exercise @xcite .",
    "we wish to determine the spacetime geometry as far as possible from astronomical observations with minimal a priori assumptions . in principle a set of observations of the redshifts , angular diameters , and apparent luminosities of galaxies , as well as their number counts , combined with knowledge of the cosmic equation of state and the true diameters , luminosities and masses of the sources ( and the evolution of these source properties ) , can be turned into metric information .",
    "the idea of reducing observed cosmological data to a metric was first explicitly discussed by kristian and sachs @xcite ; they examined how this could be done near our present spacetime position by deriving expressions in power series for some astronomical observations near the observer in a general metric , and demonstrated the difficulties faced in confirming homogeneity of the universe from observations .",
    "however , the problem of source evolution was barely addressed in their derivations . in the ideal observational cosmology program by ellis and stoeger and others @xcite , they took a slightly different approach to kristian and sachs as they aimed to determine what could and could not be decidable in cosmology on the basis of ideal astronomical observations , and so considered the limits of verification in cosmology .",
    "they worked with observational coordinates since all observational data are given , not on the usual spacelike surface of constant time , but rather on our past null cone , which is centred at our observational position on our worldline .",
    "hence , the observational data can be used with ease in the implementation of any algorithm developed through using the einstein field equations ( efes ) that are written in observational coordinates .",
    "thus there has been a fair bit of theoretical work on how to determine the cosmic metric from standard obervations , but implementation has not been attempted and the two key issues of choosing appropriate numerical methods and handling real observational data have not been properly addressed .",
    "however bishop and haines @xcite did make a numerical attack on the problem that was only partly successful .",
    "they treated the past null cone ( pnc ) as a time - reversed characteristic initial value problem ( civp ) .",
    "since the civp code is not intended to deal with a reconverging past null cone , their numerics blew up at the maximum in the diameter distance , and they were not able to extend past this point , civp code and tested it on spherically symmetric einstein - de sitter pnc data , finding it to be second order accurate . although @xmath0 and @xmath1 are called luminosity distances throughout , it is evident from their equations and figure 4 that they are diameter distances .",
    "this work has not been followed up . ] .",
    "as shown here , careful consideration of the nature of this maximum allows the integration to be continued to much higher redshifts .",
    "this is clearly a very big task , and will take years to develop into a rigorous algorithm generating believable results .",
    "we here describe the beginnings of such a procedure , necessarily simple at first .",
    "our focus here is on turning the theoretical algorithm outlined in @xcite into a workable numerical method , and thereby providing a demonstration of the viability of a key component of the problem . in tackling a problem of this magnitude",
    ", it is essential to start simply , which is the main reason for initially assuming spherical symmetry and zero cosmological constant . in the long term we envisage a much more general treatment .",
    "( we note however , that we are unavoidably at the centre of our past null cone , and spherical coordinates provide a natural description . )",
    "even this simple first step provides some interesting challenges , which are discussed below .",
    "our expectation is that the accuracy and especially the completeness of cosmological redshift surveys will be much enhanced in the coming years , and extracting the geometric implications of the observations will become possible .",
    "the general spherically symmetric metric for an irrotational dust matter source in synchronous comoving coordinates is the lematre - tolman - bondi ( ltb ) @xcite metric @xmath2^{2}\\ } { 1 + 2e(r)}\\ dr^{2 } + r^{2}(t , r)d \\omega^{2}\\ , \\label{ltbmetric}\\ ] ] where @xmath3 , and @xmath4 .",
    "the function @xmath5 is the areal radius , since the proper area of a sphere of coordinate radius @xmath6 on a time slice of constant @xmath7 is @xmath8 .",
    "the function @xmath9 is an arbitrary function of the ltb model representing the local geometry .",
    "solving the efes with @xmath10 gives us a generalised friedmann equation for @xmath11 , @xmath12 and an expression for the density @xmath13 where @xmath14 is another arbitrary function of the ltb model that gives the gravitational mass within comoving radius @xmath0 .",
    "here @xmath15 also plays a dynamical role , it determines the local energy per unit mass of the dust particles .",
    "equation ( [ rdot ] ) can be solved in terms of a parameter @xmath16 , and a third arbitrary function @xmath17 which is the time of the big bang locally : @xmath18 @xmath19 @xmath20 for hyperbolic , parabolic and elliptic solutions respectively rather than @xmath21 that determines the type of solution . ] .",
    "specification of the three arbitrary functions  @xmath14 , @xmath15 and @xmath17  fully determines the model .",
    "they constitute a radial coordinate choice , and two physical relationships .",
    "the notation and null cone solution used here were first developed in @xcite .",
    "however , they chose to work with the parabolic ltb model , and hence , their gauge choice which locates the null cone of the observer at one instant of time is simpler .",
    "this gauge choice was generalised to all spatial sections , i.e. for all values of @xmath21 , in @xcite . in this latter paper , which we will call mhe , they gave a complete outline of the observer s null cone in the ltb model , and how one can relate the ltb model to observables using this more general gauge choice .",
    "therefore , we follow the general outline given in mhe here .    on the one hand specification of the three arbitrary functions is what determines the ltb model , and on the other the angular diameter distance and the redshift space number density are what is given on the observer s past null cone .",
    "hence , we first need to locate the null cone , and then relate the ltb arbitrary functions to the given data .    human observations of the sky are essentially a single event on cosmological time scales , and as a result , being able to locate a single null cone is all we need here ; no general solution is needed .",
    "on radial null geodesics , we have @xmath22 . from ( [ ltbmetric ] ) , if the past null cone of the observation event ( @xmath23 ) ( here and now ) is given by @xmath24 , then @xmath25 satisfies @xmath26\\ } { \\sqrt{1 + 2e}}\\ dr = - \\frac{\\widehat{r^{'}}}{\\ \\sqrt{1 + 2e}\\ } \\ dr\\ . \\label{ltbmetricnc}\\ ] ] we will denote a quantity evaluated on the observer s null cone , @xmath27 , by a @xmath28  ; for example @xmath29 \\equiv \\widehat{r}$ ] , and we note that it is a function of @xmath0 only instead of @xmath0 and @xmath7 . if we choose coordinate @xmath0 in such a way that , on the past null cone of ( @xmath30 ) , we have @xmath31 then the incoming radial null geodesics are given by @xmath32 with our coordinate choice ( [ coordinatechoice ] ) , the density ( [ density ] ) and the friedmann equation ( [ rdot ] ) on the past null cone then become @xmath33 @xmath34 the gauge equation is then found from the total derivative of @xmath35 on the null cone , @xmath36 and this , together with ( [ coordinatechoice ] ) , ( [ mhe8 ] ) and ( [ mhe10 ] ) , it is fairly safe to assume that it is positive on our past null cone on the large scales that we are considering . from now on we take the positive sign for the right hand side of equation ( [ mhe10 ] ) .",
    "] , leads to @xmath37 we can then obtain an expression for @xmath38 , @xmath39 - \\frac{\\ m\\ } { \\hat{r } } \\right \\ } \\left / \\left ( \\frac{\\ d \\hat{r}\\ } { dr } \\right ) \\right.\\ , \\label{mhe13}\\ ] ] where a new variable @xmath40 is introduced .",
    "this expression tells us for which regions the spatial sections are hyperbolic @xmath41 , parabolic @xmath42 or elliptic @xmath43 , based on data obtained from the null cone .",
    "we substitute ( [ mhe13 ] ) into ( [ mhe9 ] ) and rearrange it into the form @xmath44\\ . \\label{mhe14}\\ ] ] the proper time from the bang surface to the past null cone along the particle worldlines is described by @xmath45      since the cosmological observations are given in terms of redshift rather than the unobservable coordinate @xmath0 , we need to express all the relevant quantities in terms of redshift @xmath46 . in order to do this ,",
    "the redshift formula is developed here .",
    "as shown in mhe and elsewhere , the redhsift in ltb models is @xmath47 for the central observer at @xmath48 , receiving signals from an emitter at @xmath49 .",
    "we need to find the redshift @xmath46 explicitly in terms of @xmath0 , @xmath50 and @xmath51 , which we will later relate to observables .",
    "we differentiate ( [ rdot ] ) with respect to @xmath0 , then evaluate it on the observer s past null cone , and we find @xmath52\\ .\\label{mhe26}\\ ] ] from ( [ mhe13 ] ) , we can get the derivative of @xmath53 . using equation ( [ mhe9 ] ) to eliminate @xmath54 , and combining with equations ( [ mhe12 ] ) and ( [ mhe24 ] )",
    ", it now follows that @xmath55 with @xmath56 .",
    "so theoretically we now have the redshift in terms of coordinate radius @xmath0 from @xmath57 and @xmath58 directly if we integrate ( [ mhe29 ] ) with respect to @xmath0 .",
    "as explained , we are assuming a spherical metric with a central observer purely for purely pragmatic reasons  one does not tackle the full generality of a complicated problem all at once . for simplicity ,",
    "we suppose there is only one type of cosmic source and we only consider bolometric luminosities as in mhe .",
    "see hellaby @xcite for a discussion of multiple source types and multicolour observations .",
    "it is assumed that the luminosity and the number density of each source can evolve with time ; with the former written as an absolute bolometric luminosity @xmath59 , and the latter as a mass per source , @xmath60 .",
    "isotropy about the earth is assumed , and we also assume that the universe is described by zero - pressure matter - ` dust ' , and galaxies or perhaps clusters of galaxies are taken as the particles of this dust",
    ".    the two source evolution functions might naturally be expressed as functions of local proper time since the big bang , @xmath61 and @xmath62 . however , one can not be sure of the age of the objects at redshift @xmath46 because the bang time is uncertain in a ltb model and also because the location of the null cone is uncertain . the proper time from bang to null cone",
    "will be a function of redshift , @xmath63 , and the projections of the evolution functions on the null cone are written as @xmath64 and @xmath65 .",
    "of course , @xmath63 is unknown until we have solved for the ltb model that fits the data . for the sake of simplicity",
    ", we will take @xmath64 and @xmath65 to be given as function of @xmath46 , and we use @xmath1 for the apparent luminosity and @xmath66 for the number count observations . in practice , many observational studies of evolution express their results in terms of @xmath46 .",
    "the area distance @xmath67 ( or equivalently the diameter distance @xmath68 ) is the true linear extent of the source over the measured angular size , which is by definition the same as the areal radius of the source at the time of emission , i.e. @xmath35 in the ltb model .",
    "it multiplies the angular displacements to give proper distance tangentially and its projection onto the observer s null cone gives the quantity @xmath50 .",
    "the luminosity distance is measurable if we know the true absolute luminosity of the source at the time of emission @xmath64 .",
    "if the observed apparent luminosity is @xmath69 , then from the reciprocity theorem @xcite gives the relationship between the diameter distance @xmath70 and the luninosity distance @xmath71 , @xmath72 where @xmath73  parsecs .",
    "let the observed number density of sources in redshift space be @xmath74 per steradian per unit redshift interval , hence the number observed in a given redshift interval over the whole sky is @xmath75 thus the total rest mass between @xmath46 and @xmath76 is @xmath77 where @xmath78 $ ] is the mean mass per source .",
    "given the local proper density on the null cone @xmath51 , the total rest mass between @xmath0 and @xmath79 evaluated on the null cone is @xmath80 where @xmath81 is the proper volume on a constant time slice .",
    "hence , from ( [ mhe34]),([mhe35 ] ) and ( [ coordinatechoice ] ) , we get @xmath82      most of the equations developed above are given as differential equations ( des ) , and numerically des are easy to work with .",
    "therefore , we need a set of des , that will generate the values of @xmath0 , @xmath83 , @xmath21 and hence @xmath84 from the observations .",
    "the ltb model implied by the observations is thus deduced .",
    "transforming ( [ mhe29 ] ) to be in terms of redshift @xmath46 instead of coordinate @xmath0 , we obtain the null raychaudhury equation @xmath85 \\left ( \\frac{\\ dz\\ } { dr } \\right ) ^2 = - 4\\pi \\hat{\\rho } \\hat{r}(1 + z)\\ .",
    "\\label{mhe37}\\ ] ] substituting ( [ mhe36 ] ) into ( [ mhe37 ] ) , using the facts that @xmath86 and @xmath87 , rewriting it so that all terms involving @xmath50 ( and hence @xmath88 and @xmath89 ) are on one side of the equation , we then have a second order de for @xmath90 : @xmath91^{-1 } \\left \\ { \\left [ \\frac{\\ d^2\\hat{r}\\ } { dz^2}\\ ( 1 + z ) + \\frac{\\ d\\hat{r}\\ } { dz } \\right ] \\left ( \\frac{\\ dz\\ } { dr } \\right ) \\right .",
    "\\hspace{10 mm } \\left .",
    "+ \\frac{\\ 4 \\pi\\hat{\\mu}n\\ } { \\hat{r}}\\ ( 1 + z ) \\right \\}\\left ( \\frac{\\ dr\\ } { dz } \\right)^2\\ .\\label{secondrofz}\\end{aligned}\\ ] ] since we want to solve all our des ( and hence get the values for our functions @xmath0 ,",
    "@xmath83 and @xmath21 ) in parallel , we need to introduce a new variable such that we can rewrite ( [ secondrofz ] ) as two first order des .",
    "we introduce a new variable @xmath92 , defined by @xmath93 and equation ( [ secondrofz ] ) then becomes @xmath94 if we transform ( [ mhe13 ] ) into a function of @xmath46 and take square root of both sides , using the inverse of ( [ de1 ] ) we then obtain @xmath95 we substitute ( [ mhe36 ] ) into ( [ mhe14 ] ) and rewrite it as @xmath96 instead of @xmath97 , so together with equation ( [ w ] ) we then get @xmath98    hence , equations ( [ de1])-([de3 ] ) give us a set of coupled first order des that we use in order to generate the values for @xmath90 , @xmath99 and @xmath100 ( or @xmath101 ) from the observational data .",
    "we can then obtain the values for @xmath102 , @xmath103 and then the third arbitrary function @xmath104 for the hyperbolic and elliptic cases by substituting these values into equations ( [ hyperbolic ] ) , ( [ elliptic ] ) and ( [ mhe17 ] ) , with ( [ hyperbolic ] ) and ( [ elliptic ] ) evaluated on the null cone .",
    "however , there is a borderline case ",
    "the near - parabolic case , which is needed where the exact expressions become numerically intractable .",
    "see [ nearpara ] for the details .",
    "note that from equation ( [ de2 ] ) , if we know the values for @xmath50 and @xmath105 , we can then solve for @xmath106 independently without knowing the values of @xmath0 , @xmath83 and @xmath53 ; while solving for @xmath0 , @xmath83 and @xmath53 depends on knowing @xmath106 .",
    "this property of the @xmath106 equation will be very useful later on .      at the origin of spherical coordinates , @xmath48 , we have @xmath107 and @xmath108 for all @xmath7 . hence , on the observer s past null cone equations ( [ mhe11 ] ) and ( [ mhe12 ] ) then become @xmath109 and thus @xmath110 to lowest order near @xmath48 . from ( [ mhe9 ] )",
    "we then find that @xmath111 and from ( [ mhe13 ] ) using a taylor series for @xmath50 , and working to second order in @xmath0 , we get @xmath112 r^2\\ , \\label{mhe22}\\ ] ] where @xmath113 is finite when @xmath48 . note that equations ( [ mhe21 ] ) and ( [ mhe22 ] ) . ]",
    "give us @xmath114 .",
    "we can get the origin limit for @xmath115 from equation ( [ mhe36 ] ) , which is @xmath116 also , if we substitute ( [ mhe20 ] ) into ( [ mhe29 ] ) , after rearranging the expression , we then get the origin condition for @xmath117 ( and hence @xmath46 ) @xmath118    the behaviour of the des ( [ de1])-([de3 ] ) needs to be checked near the origin , i.e. @xmath119 . since @xmath0 and @xmath46 have a linear relation , we know that near the origin , @xmath120 , @xmath121 , @xmath88 is finite , @xmath122 and @xmath123 . also we know that @xmath124 , so @xmath125",
    "hence , our des are well behaved as @xmath119 .      in the early universe ,",
    "the expansion is so rapid that the light rays that are headed towards us are actually getting further away .",
    "one can consider the set of photons that are all the same time away from observation to be an incoming wavefront .",
    "as the universe slows down , there comes a moment when the area of such a wavefront is stationary , and @xmath11 has reached its maximum value .",
    "the locus of such points for all incoming wavefronts is the apparent horizon .",
    "hence , for the ltb model , the maximum of the areal radius ( or diameter distance ) down the null cone is where the null cone crosses the apparent horizon .",
    "we locate this point by the calculation below .",
    "since the apparent horizon is the hypersurface in spacetime where @xmath50 is momentarily constant , we put @xmath126 into ( [ mhe11 ] ) and using ( [ coordinatechoice ] ) , ( [ mhe8 ] ) and ( [ mhe10 ] ) , we get @xmath127 and hence @xmath128 we will see that this locus presents us with a particular difficulty in our numerical reduction of null cone data . of course , in the case when the cosmological constant is not set to zero , and if we are considering both the future and the past horizon ; the calculation and the analysis will be more complicated @xcite .",
    "see also @xcite for the observational significance of this locus .",
    "there are a few things worth considering here  our des become singular when we reach the maximum in the areal radius ( diameter distance ) @xmath50 , i.e. @xmath129 . from equation ( [ rover2 m ] ) we know that at the maximum of @xmath50 , we have @xmath130 . if one looks at equations ( [ w ] ) and ( [ de3 ] ) , it actually contains zero over zero at this point , and any numerical method will break down here .",
    "further , from ( [ mhe29 ] ) and ( [ de2 ] ) we can see that @xmath131 where @xmath129 .",
    "there is no problematic behaviour of @xmath106 here , as can be verified in the flrw case .",
    "hence , in order to carry our numerics through the maximum of @xmath50 , we need to perform a series expansion near this point for @xmath132 , @xmath133 , @xmath134 , @xmath99 and @xmath100 , as given in ( [ seriesmn])-([seriesw ] ) of [ rmaxseries ] .    here",
    ", we use @xmath135 to denote the maximum in @xmath50 , and its corresponding @xmath46 value is called @xmath136 . the series",
    "are then written in powers of @xmath137 . from the @xmath50 and @xmath115 data",
    ", we can easily determine the values of @xmath135 , @xmath138 and @xmath136 , and thus the remaining @xmath132 and @xmath133 coefficients can be evaluated by simply performing a least squares fit using the data values near @xmath136 . in order to obtain the expressions for the coefficients in the @xmath134 , @xmath99 and @xmath100 series , we need to substitute ( [ seriesmn])-([seriesw ] ) from [ rmaxseries ] into our des ( [ de1])-([w ] ) .",
    "the detailed expressions can be found in [ rmaxseries ] .",
    "from ( [ phi0])-([phi3 ] ) we can see that all @xmath134 coefficients are determinable once we know the values of @xmath136 and all coefficients of the @xmath50 and @xmath139 fits . using ( [ de1 ] ) and ( [ seriesphi ] ) , the series expansion for @xmath0 is simply @xmath140 where @xmath141 is the integration constant .",
    "note that @xmath142 is obtained directly from @xmath135 without any further information .",
    "the only problem is that expressions ( [ m2 ] ) , ( [ m3 ] ) and ( [ w0])-([w2 ] ) in [ rmaxseries ] , all depend linearly on @xmath143 .",
    "unfortunately , no information about @xmath143 can be obtained when we carry out the series expansion , as one can see from ( [ m1 ] ) . despite this",
    ", it is still possible to obtain a value for @xmath143 by substituting a known value ( from numerical integration ) , say @xmath144 at @xmath145 , where @xmath145 is some distance away from @xmath136 , into ( [ seriesm ] ) as described in more detail below .",
    "the actual data we must use consists of redshift and apparent magnitude measurements for a large number of discrete sources , which must be sorted into redshift bins . in each bin we must calculate the total number of sources , @xmath146 , and the average value for the diameter distance , @xmath50 .",
    "now the above theory treats all physical and geometric quantities , such as the density and the metric , as continuous functions of position , while the available data is a discrete set of sources .",
    "therefore it might at first seem one should fit a smooth curve to the data in order to proceed with the integration . however , numerical methods are not continuous either , and any numerical method we might choose to solve our pnc equations with is based on discretisation of continuous des .",
    "furthermore , we must be careful not to hide any inhomogeneity by smoothing on too large a scale , or introduce unintended bias by inappropriate choice of smoothing function .",
    "we note that the process of calculating averages on the redshift bins already introduces a measure of smoothing and a basic smoothing scale .",
    "we also note that higher order integration methods that use data from several different @xmath46 values will also have a smoothing effect , and this is an option we are keeping open .",
    "so although statistical fluctuations in the real data may need a further degree of smoothing , we prefer to keep it to a minimum , only introducing as much as necessary .",
    "we also argue that any extra smoothing that is required should be closely tied to the numerical integration scheme , and not merely ad hoc .",
    "an important consideration in the choice of numerical method is that the right hand sides of the des ( [ de1])-([de3 ] ) contain not only the funtions being solved for , @xmath134 , @xmath90 , @xmath99 and @xmath101 , but also the given data derived from observations , @xmath132 and @xmath147 . since the latter are only known at discrete @xmath46 values ( the mid points of the @xmath46 bins ) , methods that allow adaptive step sizes are not appropriate , and similarly evaluations of @xmath134 , @xmath90 , @xmath99 and @xmath101 significantly above or below their correct values should be avoided because there is no way to find the corresponding values of the given data .",
    "a second consideration is that with real observations , there will be statistical fluctuations and measurement uncertainties , so there is a limit to how much improvement can be gained from using higher order methods . in line with our policy of not using a more complicated method than the situation demands",
    ", we found that , with bin size (= step size ) @xmath148 , a second order runge - kutta method gave entirely satisfactory results when very accurate fake data was given for the data functions .",
    "these choices may change in the future once real data is used and as more factors are included .",
    "once @xmath134 , @xmath90 , @xmath99 and @xmath101 are determined , then @xmath149 , @xmath63 and @xmath104 are easily obtained from the algebraic equations ( [ mhe8 ] ) and ( [ hyperbolic])-([elliptic ] ) . however , at each discrete position , we are required to determine , numerically , which type of evolution we have : hyperbolic , elliptic or near - parabolic .",
    "note that for the near - parabolic case , when @xmath21 is small but not zero , we use equation ( [ tauhyperbolicnearparabolic ] ) from [ nearpara ] . as one might have noticed , equation ( [ tauhyperbolicnearparabolic ] ) is in powers of @xmath150 , and this factor can be evaluated at each discrete position since we know the values for @xmath50 , @xmath21 and @xmath83 . of course , the error for this approximation of the series expansion for @xmath103 has to be small ,",
    "say about @xmath151 ; if we take ( [ tauhyperbolicnearparabolic ] ) up to order 3 , this will give us @xmath152 .",
    "hence , if @xmath153 , we use the hyperbolic case ( [ hyperbolic ] ) ; if @xmath154 , we use the elliptic case ( [ elliptic ] ) , and if @xmath155 we use the near - parabolic case .",
    "a set of computer programmes were developed that generate the values for the ltb functions @xmath83 , @xmath21 and @xmath156 .",
    "below we give a brief summary of the order of the steps followed in our programmes . to obtain the mass , energy and bang time functions ( @xmath83 , @xmath21 and @xmath156 respectively ) from observational data and source evolution , we proceed as follows :",
    "\\(i ) take the discrete observed data for @xmath157 and @xmath158 , divide it into redshift bins of chosen width @xmath159 , and in each bin average or sum it over all angles and the bin width to obtain @xmath69 and @xmath74 .",
    "we may wish first to correct the data for known distortions and selection effects due to proper motions , absorption , shot noise , image distortions , etc . ;",
    "\\(ii ) choose evolution functions @xmath160 and @xmath161 based on whatever observations and theoretical arguments may be mustered ;    \\(iii ) determine @xmath132 from @xmath160 and @xmath69 using ( [ mhe31 ] ) , this is then our first input data function and we have @xmath139 as our second input data function ;    \\(iv ) numerically integrate the des ( [ de1])-([de3 ] ) using the redshift bins as the basic step size , and the binned data for @xmath46 , @xmath50 and @xmath139 , thus obtaining @xmath90 , @xmath99 and @xmath101 ;    \\(v ) solve for @xmath102 , @xmath162 , and hence @xmath163 from ( [ hyperbolic])-([elliptic ] ) and ( [ mhe17 ] ) , with ( [ hyperbolic])-([elliptic ] ) evaluated on the null cone .",
    "notice that @xmath61 and @xmath164 could also be found in this step .    however , at this early stage of development , steps ( i)-(iii ) use test data generated from a variety of model assumptions .",
    "in fact , step ( iv ) has four components , which are summarised below :    * deduce the origin parameters and output for the first 3 data points , i.e. at @xmath165 , @xmath166 and @xmath167 .",
    "* use numerical de solvers - a second order runge - kutta method - for solving the des up to just before the maximum in @xmath50 is reached .",
    "* determine the point @xmath168 where the switch to the series expansion is made , evaluate all quantities to be matched such as @xmath169 , @xmath170 , etc , and extend the numerics through the maximum in @xmath50 by calculating the series expansions of [ rmaxseries ] for @xmath0 , @xmath106 , @xmath83 and @xmath21 .",
    "* evaluate another matching value for switching back from the series expansion to numerical integration , and continue to solve the des numerically up to the limit of the data , here set to @xmath171 .",
    "although we have already discussed how the des behave near the origin , from any available cosmological data that we might use , there is no data available at the origin itself , and very little in the first few redshift bins .",
    "therefore , a method of filling in this gap is necessary in order to provide the initial values needed by the numerical integration .",
    "if we average over all the data values within each bin , for example the @xmath50 values within a given @xmath46 bin , then the average @xmath50 values that we use are located roughly in the middle of each bin .",
    "thus we have the first value of @xmath50 at @xmath172 .",
    "we can then get the discretised versions of @xmath88 and @xmath173 from the first and second differences of @xmath50 .",
    "it takes two @xmath159 bins to get the first value of the first difference at @xmath174 and all the values are located at @xmath175 for any positive integer @xmath176 .",
    "however , it takes three @xmath159 bins to get the first value of the second difference and hence the first value of @xmath173 at @xmath177 , therefore , all the values are located in the middle of each bin .",
    "since we want to have a complete set of data at each @xmath46 value , we take the average of the two neighbouring first difference data points to get all our data at the half @xmath159 locations ( in the middle of each bin ) . in doing so",
    ", we will not have data values at the origin or at @xmath172 , since the first complete data set is at @xmath178 .",
    "but we know that ltb is rw like near the origin due to the fact that it assumes spherical symmetry . for that reason ,",
    "the series expansions of the rw expressions are used for finding the rw parameters that fit the data values at the origin and at @xmath172 , i.e. we determine the central values of @xmath179 ( hubble constant ) and @xmath180 ( deceleration parameter ) from the data near @xmath165 .",
    "so we take the standard rw expressions for @xmath132 and @xmath181 given by equations ( a.1 ) and ( a.2 ) in appendix a of mhe , and we do series expansions of them near the origin , as detailed in [ nearorigin ] .",
    "the origin limits are @xmath182 , @xmath183 , @xmath184 , @xmath185 , @xmath186 and @xmath187 .",
    "as mentioned before , a series approximation is required in the vicinity of @xmath135 , where the des become singular . here",
    "all @xmath134 coefficients are determinable once we know the values of @xmath136 and all coefficients of @xmath50 and @xmath139 .",
    "a set of @xmath106 values can be generated from the series expansion by substituting a set of @xmath188 values into ( [ seriesphi ] ) , for a @xmath46 interval that overlaps with our numerical results .",
    "we use the @xmath50 and @xmath115 values of 180 redshift bins on either side of @xmath136 ( this is a redshift interval of 0.361 which covers about 12% of the total redshift interval that we are considering here ) , and perform a least squares fit with these data to obtain all the coefficients for @xmath50 and @xmath115 , and hence obtain all coefficients for @xmath106 .",
    "there is good agreement between the numerical and series values over a range of @xmath46 values when plotted on the same graph , and there is one intersection point between the two curves before @xmath136 ( and also the closest to @xmath136 ) .",
    "the intersection points here are important since they are where we match values between the series expansion and the numerical integration for @xmath106 .",
    "this is what we choose for @xmath168 , and the numerically derived @xmath83 at @xmath168 becomes our @xmath169 .",
    "now that we know where @xmath168 is , we can get @xmath143 from @xmath168 and @xmath169 if we substitute them into equation ( [ seriesm ] ) , using @xmath189 @xmath190 where @xmath191 and @xmath192 are given by ( [ m2 ] ) and ( [ m3 ] ) .",
    "similarly , if we are matching the @xmath53 values @xmath193 where @xmath194 and @xmath195 are given by ( [ w1 ] ) and ( [ w2 ] ) . therefore ,",
    "if we match @xmath83 then @xmath196 alternatively , if @xmath53 is used for the matching , we have @xmath197 all our functions should connect the numerical integration ( @xmath198 ) and series expansion ( @xmath199 ) parts at @xmath168 ; and from @xmath168 , we can generate the corresponding @xmath200 and @xmath169 easily . with all @xmath106 coefficients known",
    ", a value for @xmath201 can be found from ( [ seriesr ] ) . using ( [ m1fromm ] ) or ( [ m1fromw ] ) , a value for @xmath143 can easily be determined .",
    "the purpose of doing a series expansion is to extend our numerics through @xmath135 , but once this is achieved , we need to switch back to numerical integration again .",
    "it is sensible if we connect at @xmath202 where @xmath203 .",
    "initially , we tried matching @xmath83 at the two connecting points , @xmath168 and @xmath204 .",
    "however , after we compared @xmath0 , @xmath106 , @xmath83 and @xmath53 from our numerics with the correct curves generated from the assumed model , the @xmath0 , @xmath106 and @xmath83 curves showed good agreement , but the @xmath53 curve had jumps at the two connecting points . as anticipated , @xmath53 is the least well - determined function .",
    "we then tried to match @xmath53 at both connecting points .",
    "although this removed the two jumps in the numerical @xmath53 curve , it also reduced the accuracy of the @xmath53 series expansion .",
    "a key consideration is that at @xmath205 we actually know the value of @xmath83 from ( [ m0 ] ) if we know @xmath135 . in order to maximise the accuracy of our series expansion and minimise the jumps that appear in our @xmath53 graph",
    ", we matched @xmath83 at the first connecting point , and @xmath53 at the second one .",
    "this approach does not leave any visible alteration in the @xmath83 curve , the jump in @xmath53 at the first connecting point is still present , but better accuracy for the series expansion is obtained and the second jump is avoided .",
    "one thing worth mentioning here is that we may need to shorten the @xmath46 interval for the series expansion , since with inhomogeneous data , fluctuations will be present , so if the interval is too wide compared with the fluctuations , the accuracy for our series expansion will be lower .",
    "however , this problem will only be dealt with when it has shown a significant effect on the numerics .",
    "in order to test our numerical procedure , we need fake  observational data \" for which the correct results are known . therefore we generated sets of  observational data \" that would be produced in a selection of ltb universes .",
    "although we did a full comparison of numerical output from our programme , @xmath83 , @xmath21 and @xmath84 , with the correct ltb functions for a variety of different models , both homogeneous and inhomogeneous , we can not present all our results here . therefore , we summarise the ones we did in table [ summary_for_all_cases ] below and only present a complete set of plots from one homogeneous and one inhomogeneous model in the two subsections below . in order to avoid confusion",
    ", we call the @xmath179 and @xmath180 used for generating fake data @xmath206 and @xmath207 ; and the ones our numerical procedure extracts from the data @xmath180 and @xmath179 from here on . where the model is inhomogeneous , both pairs are the values at the origin , as explained in section 3.2 .",
    "hyperbolic & near & elliptic + ( @xmath208 ) & parabolic & ( @xmath209 ) + @xmath210 & @xmath211 & @xmath212 + @xmath213 & @xmath213 & @xmath213 + @xmath214 & @xmath215 & + @xmath213 & @xmath213 & +   + hyperbolic & near & elliptic + & parabolic & + & & +  @xmath216 &  @xmath217 &  @xmath218 + & & +  @xmath219 & & +      amongst the homogeneous models we found that the near - parabolic models to have slightly lower accuracy .",
    "as noted above , we expect the output function with largest error to be @xmath220 .",
    "below we present the results of the comparison for a homogeneous model with @xmath211 and @xmath213 .",
    "this is the case when we have a negatively curved universe , but very close to the flat case . in figures",
    "[ home1_r]-[home1_m ] , the curves plotted from our numerical output using the runge - kutta method and the ones from the generated rw data are in very good agreement with each other , and there is only 0.02106 % difference between the curves in figure [ home1_w_rk ] . as can be seen in figure [ home1_w_rk ] ,",
    "there is still a jump in @xmath53 at @xmath168 although it is barely visible , while the jump at the second matching point is no longer visible .",
    "this justifies the earlier decisions to match first @xmath83 and then @xmath53 at the two connections between the numerical integration parts and the series expansion part .",
    "the @xmath103 curves in figure [ home1_tau ] , and the @xmath84 in figure [ home1_tb ] , show good agreement between the numerical output and the correct values .     vs. @xmath46 with @xmath221 , @xmath222 and @xmath148 .",
    "the grey curve is the correct rw expression and the dotted black one is our numerical output using runge - kutta as the integration method.,width=393,height=289 ]    [ home1_r ]     vs. @xmath46 with @xmath221 , @xmath223 and @xmath148 .",
    "the grey curve is the correct rw expression and the dotted black one is our numerical output using runge - kutta as the integration method.,width=393,height=289 ]    [ home1_phi ]     vs. @xmath46 with @xmath221 , @xmath222 and @xmath148 . matching the @xmath83 values at the first connection point and the @xmath53 values at the second connection point .",
    "the solid grey curve is the correct rw expression and the dotted black one is our numerical output using runge - kutta as the integration method.,width=393,height=289 ]    [ home1_m ]     vs. @xmath46 with @xmath221 , @xmath222 and @xmath148 . matching the @xmath83 values at the first connection point and the @xmath53 values at the second connection point .",
    "the solid curve is the correct rw expression and the dotted one is our numerical output using runge - kutta as the integration method.,width=393,height=289 ]    [ home1_w_rk ]     vs. @xmath46 with @xmath221 , @xmath223 and @xmath148 .",
    "the solid grey curve is the correct rw expression and the dotted black one is our numerical output using runge - kutta as the integration method.,width=393,height=289 ]    [ home1_tau ]     vs. @xmath46 with with @xmath221 , @xmath223 and @xmath148 .",
    "the thick solid grey curve is the correct rw expression and the dotted black one is our numerical output using runge - kutta as the integration method .",
    "the solid black line on the top is the current age of the universe.,width=393,height=289 ]    [ home1_tb ]      a complete set of comparison plots from one of the inhomogeneous models tested is given here  a model with varying geometry / energy .",
    "this model is one in which the two arbitrary functions @xmath83 and @xmath84 take a rw form , while we vary the third function @xmath21 .",
    "the correct origin parameters are @xmath213 , @xmath224 , which gives us a near - parabolic case .",
    "the extracted values are @xmath225 and @xmath226 .",
    "figure [ inhomo2 m ] shows that the @xmath83 curve plotted from our numerical output is slightly below the correct one ; in fact there is about 2.17@xmath227 error at @xmath171 .",
    "although this percentage error is bigger than the ones we had for the homogeneous cases , this is to be expected since we are working with inhomogeneous data that was numerically generated .",
    "the percentage error is a bit larger for @xmath53 , being about 26.6@xmath227 as shown in figure [ inhomo2w ] .",
    "however , this percentage error in @xmath53 is large mostly because @xmath53 is quite small , and we note that the absolute error in @xmath228 is about the same as before .    from figures [ inhomo2tau ] and [ inhomo2 tb ] we can see that the correct data and the numerical output are generally in good agreement for both @xmath103 and @xmath84 , except near the origin . however , this is due to insufficient accuracy in the @xmath179 and @xmath180 values deduced from the  observational \" data at @xmath167 .",
    "accurate values for @xmath103 and @xmath84 depend on an accurate @xmath180 value , which is particularly difficult to get at the low @xmath46 values near the origin .",
    "a least squares estimate of origin values , using a wider range of near - origin data may improve accuracy here .     vs. @xmath46 with @xmath229 , @xmath230 and @xmath148 .",
    "the solid grey curve is from the correct testing data and the dotted black curve is our numerical output using runge - kutta as the integration method.,width=393,height=289 ]    [ inhomo2 m ]     vs. @xmath46 with @xmath229 , @xmath230 and @xmath148 .",
    "the solid grey curve is from the correct testing data and the dotted black curve is our numerical output using runge - kutta as the integration method.,width=393,height=289 ]    [ inhomo2w ]     vs. @xmath46 with @xmath229 , @xmath226 and @xmath148 .",
    "the grey curve is from the correct testing data and the dotted black curve is our numerical output using runge - kutta as the integration method.,width=393,height=289 ]    [ inhomo2tau ]     vs. @xmath46 with @xmath229 , @xmath226 and @xmath148 .",
    "the thick solid grey curve is from the correct correct testing data and the dotted black one is our numerical output using runge - kutta as the integration method .",
    "the solid black line is the current time ( origin).,width=393,height=289 ]    [ inhomo2 tb ]",
    "we have developed a computer programme to implement the mhe algorithm . given ( spherically symmetric ) data from standard observations for redshift ,",
    "apparent diameter , apparent luminosity and galaxy number counts , as well as the associated evolution functions , true diameter , absolute luminosity and mass per source , it determines the metric of the ( observed ) universe .",
    "its ability to reproduce the correct metric information has been tested via artificial data generated from both homogeneous and inhomogeneous models .",
    "we have started with a very simple case , in order to understand the key elements of a numerical extraction of metric information from observations . obviously one does not wish to tackle the full complexity of the problem at the start .",
    "thus , there are still many improvements which can be made in both the theory and the numerical method used .",
    "many considerations and effects must be included , for example , source evolution theories , data set completeness , different populations of sources , and more . at some point",
    ", a non - zero @xmath231 should be considered .",
    "also , issues like a least squares fit for the data near the origin in order to obtain better accuracy for @xmath179 and @xmath180 , and a shorter @xmath46 range for the series expansion in order to carry our numerics through the point @xmath135 , also need to be dealt with for the future development .",
    "of course , a higher order integration method may also be needed in the future in order to sustain the accuracy we have so far in our numerical output out to larger @xmath46 values",
    ". however , any numerical method we use to solve the des must be able to handle both known data and unknown functions at a discrete set of positions .",
    "although higher order runge - kutta methods will have a natural smoothing effect , some other form of smoothing of the data may be needed when we tackle real data .    the bin size used for",
    "binning the observational data will affect the accuracy of the bin averages , and require attention when one works with the real data .",
    "using the same bin size for the whole redshift range , will leave the higher @xmath46 bins flooded with data , and low @xmath46 bins with very sparse data . on the other hand ,",
    "making nearby redshift bins too large may impose too much smoothing .",
    "a question for the future then is the optimum binning strategy , and the choice of binning versus smoothing , given that numerical integration is ultimately a discrete process .",
    "our initial attempt at a numerical implementation of the mhe procedure has successfully demonstrated the viability of the basic concept , and opened the way to developing a more general treatment .",
    "our current focus is on developing a workable numerical scheme .",
    "there are of course many relevant observational issues , such as luminosity functions , k corrections , different source populations , source evolution , bias , etc that must be incorporated in reducing the observations to the data that such a programme must use .",
    "these will be considered in the future .",
    "current redshift surveys do not have the accuracy or completeness to enable meaningful metric data to be extracted at present data .",
    "small fluctuations in @xmath50 will generate much exaggerated fluctuations in @xmath232 and @xmath233 . ] .",
    "however , the next generation of surveys is expected to provide considerable improvements in accuracy and completeness , as well as extending to much deeper @xmath46 values .",
    "type ia supernova measurements hold the promise of very good luminosity distance data in the near future , so the accuracy of luminosity functions and therefore number counts will be the limiting factor .",
    "current work involves analysing the stability of the des , ensuring the procedure can handle data with statistical scatter , estimating uncertainties in the output from uncertainties in the observational data , and using the properties of the maximum in the area distance as a means to check or correct the result .",
    "eventually , knowing the metric nearby will assist in analysing more distant observations in more than just a statistical sense , since the spacetime that the light rays we observe have travelled through , changes the size , brightness , frequency , position and shape of the images we measure .",
    "therefore , as we probe deeper into space , the knowledge of the geometry of the universe around us will certainly play a crucial role in the data reduction of any survey in the future . with more reliable observational data ,",
    "one may hope to achieve one of the long term objectives of the current project ",
    "being able to _ prove the homogeneity of the observable region of the universe rather than just assuming it in principle . _",
    "thcl thanks the university of cape town and the national research foundation for their financial support .",
    "ch thanks the national research foundation for a research grant .",
    "one may have noticed from the three evolution equations ( [ hyperbolic])-([elliptic ] ) that the parabolic evolution is actually the @xmath234 limit of the other two evolutions , which is obtained by writing the functions of @xmath235 as taylor expansions for small @xmath235 and noting that @xmath236 remains finite .",
    "one can see that as we are approaching this borderline case , the evolution equations ( [ hyperbolic])-([elliptic ] ) are not well - behaved numerically . also , in reality , it is very difficult , if not impossible , to obtain an exactly parabolic case numerically .",
    "hence , a series expansion is needed in order to have reasonable numerical results for the near - parabolic case .",
    "most of the series expansions for the near - parabolic case can be found in @xcite .",
    "however , here we will consider the hyperbolic case and give a detailed derivation following the approach in @xcite , but for obtaining the series expansion for @xmath237 only since this is the only one that is essential to us .",
    "let us first introduce two new variables @xmath238 , and @xmath239 .",
    "the parabolic limit now occurs when @xmath240 , while @xmath35 and @xmath103 remain finite . by ( [ hyperbolic ] )",
    ", this requires @xmath241 so that the new evolution parameter @xmath242 remains finite for finite @xmath103 .",
    "taylor series expansion expressions of @xmath103 and @xmath243 for the hyperbolic case using equation ( [ hyperbolic ] ) are just @xmath244 @xmath245 if we invert the series for @xmath243 by writing @xmath242 in series expansion form : @xmath246 then substituting into ( [ aparabolic ] ) , and solving for the coefficients @xmath247 , we get @xmath248 which we substitute into ( [ tauparabolic ] ) , and write it in terms of @xmath35 , @xmath83 and @xmath21 , giving @xmath249 @xmath250 equation ( [ tauhyperbolicnearparabolic ] ) is the @xmath103 series expansion expression for the near - parabolic case .",
    "one can do the derivation using the elliptic evolution equations similarly .",
    "let us say that @xmath135 occurs at @xmath136 , @xmath251 , and we define @xmath252 .",
    "so the series expansions for @xmath133 , @xmath132 , @xmath134 , @xmath99 and @xmath100 have the form @xmath253",
    "@xmath254 @xmath255 @xmath256 and @xmath257 hence , @xmath258 and @xmath259    and the expressions for the coefficients of the series expansion for @xmath99 , @xmath134 and @xmath100 are given below : @xmath260 @xmath261 @xmath262 @xmath263 @xmath264 @xmath265 @xmath266 @xmath267 and @xmath268 @xmath269 @xmath270",
    "we do series expansion of equations ( a.1 ) and ( a.2 ) in appendix a of mhe near the origin , obtaining @xmath271 and @xmath272    we test the accuracy of the generated @xmath179 and @xmath180 values using @xmath273 , and @xmath50 and @xmath139 values at this same @xmath46 since this is where we have the first complete set of data according to available observational data and the way we discretise our des , and therefore , find the combination with the most consistent accuracy for different @xmath180 values .",
    "we find that in general , using both equations ( [ seriesrhatrw ] ) and ( [ series4pmnrw ] ) with the same number of terms gives us better accuracy .",
    "we can then get expressions for @xmath179 and @xmath180 near the origin in terms of @xmath46 , @xmath50 and @xmath274 only .",
    "the results are    @xmath275    and @xmath276    now we have a way of determining the origin values for @xmath179 and @xmath180 from the data .",
    "if we need to generate values for @xmath0 , @xmath106 , @xmath83 and @xmath53 at @xmath277 and the origin numerically from the rw expressions given in appendix a in mhe , then one can perform series expansions of them too , since the values of @xmath46 are small .",
    "they are : @xmath278 @xmath279 @xmath280 @xmath281 and these expressions are valid for any @xmath180 . for @xmath282 and @xmath283 respectively , the @xmath103 series expansions for the rw equations take the form : @xmath284 @xmath285"
  ],
  "abstract_text": [
    "<S> recent galaxy redshift surveys have brought in a large amount of accurate cosmological data out to redshift 0.3 , and future surveys are expected to achieve a high degree of completeness out to a redshift exceeding 1 . </S>",
    "<S> consequently , a numerical programme for determining the metric of the universe from observational data will soon become practical ; and thereby realise the ultimate application of einstein s equations . </S>",
    "<S> apart from detailing the cosmic geometry , this would allow us to verify and quantify homogeneity , rather than assuming it , as has been necessary up to now , and to do that on a metric level , and not merely at the mass distribution level . </S>",
    "<S> this paper is the beginning of a project aimed at such a numerical implementation . </S>",
    "<S> the primary observational data from our past light cone consists of galaxy redshifts , apparent luminosities , angular diameters and number densities , together with source evolution functions , absolute luminosities , true diameters and masses of sources . </S>",
    "<S> here we start with the simplest case , that of spherical symmetry and a dust equation of state , and execute an algorithm that determines the unknown metric functions from this data . </S>",
    "<S> we discuss the challenges of turning the theoretical algorithm into a workable numerical procedure , particularly addressing the origin and the maximum in the area distance . </S>",
    "<S> our numerical method is tested with several artificial data sets for homogeneous and inhomogeneous models , successfully reproducing the original models . </S>",
    "<S> this demonstrates the basic viability of such a scheme . </S>",
    "<S> although current surveys do nt have sufficient completeness or accuracy , we expect this situation to change in the near future , and in the meantime there are many refinements and generalisations to be added . </S>"
  ]
}