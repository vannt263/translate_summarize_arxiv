{
  "article_text": [
    "in recent years , there has been a significant increase in size among large datasets which possess hundreds or even thousands of features per observation .",
    "this has given rise to increased observations of the phenomenon known as the `` curse of dimensionality . ''",
    "first identified by bellman in 1957 , an application of this phenomenon occurs as the distances between observations grow with the number of additional dimensions .",
    "[ 3 ] one of the side effects of this phenomenon is that spurious results are obtained due to the increase in variance produced by the introduction of extraneous features ( dimensions ) .",
    "these spurious results arise directly from the influence that additional distances have on the underlying metric used to calculate the nearest distance for classification examples in a training dataset .",
    "this effect is especially problematic in statistical learning algorithms which rely heavily on measures of closeness or similarity between observations .",
    "+ interest in the distances between randomly generated points was initially undertaken by trying to analyze the effects that additional data features ( dimensions ) have upon classification algorithms used for statistical learning with large datsets .",
    "hastie , et al .",
    "provide an in - depth analysis of this phenomenon in their text  the elements of statistical learning \" , which is a widely used textbook at the graduate level .",
    "[ 7 ] the nearest neighbor algorithm is one such common machine learning technique which is known to be sensitive to the effects of extraneous dimensions because additional features increase the distances between points and reduce the contrast between meaningful neighbors and those which may arise purely from statistical noise .",
    "aggarwal , hinneburg , and keim provide a very good discussion of this phenomena in their paper and term it  relative contrast \" . [ 1 ] they observe that as the number of dimensions increases , the distance to a the nearest query point using a nearest neighbor algorithm increases faster than the difference in distances between the furthest and nearest query points .",
    "their conclusion is that as the number of dimensions increases , proximity queries become meaningless due to proper discrimination regarding what constitutes a suitable neighbor for classification purposes .",
    "+ while significant research has considered this problem conceptually , the authors are aware of little research regarding a quantitative approach .",
    "several years ago , there were several successful attempts to determine the distribution of distances between randomly generated points uniformly distributed within a hypersphere [ 2 , 6 , 9 ] , randomly generated points distributed on the surface of a hypersphere [ 10 ] , or more general analysis of random points contained within some compact convex subset of points in euclidean space [ 4 ] .",
    "solomon provided additional extensive coverage of these concepts for uniformly distributed points in circles and spheres in his book titled  geometric probability . \"",
    "[ 11 ] interestingly , all of these observations use a uniform distribution of point density , either on the surface or interior of a sphere and arose from motivations in the field of geometric probability .",
    "the authors are unaware of any similar research that considers the distribution of distances between points which themselves are distributed according to a standard normal distribution in @xmath0-dimensional space without geometric boundaries .",
    "this is surprising given the frequent application of the standard normal distribution for modeling purposes and the standard practice of normalizing data prior to the application of statistical learning techniques in order to prevent dominance of one variable over another .",
    "in particular , the technique regarding normalization of data is highlighted in several of the key texts involving statistical learning .",
    "[ 8 , 12 ] this paper develops the distribution which defines the distances between randomly generated standard normal points in @xmath0-dimensional space without geometric boundaries , thus providing some statistical tools that support further refinement of machine learning and data mining applications .",
    "this paper provides an investigation into the effects of dimensionality on the distribution of distances between random points which are distributed standard normal in each of their respective dimensions .",
    "most of the datasets confronted in high - dimensional problems are usually standardized in each of the attributes for respective datapoints .",
    "[ 8 , 12 ] demonstrated below are the quantitative effects that dimensionality brings as the number of dimensions of the dataset increases . to the authors knowledge",
    "this subject has not been treated from a quantitative aspect in the literature .",
    "+ we begin by determining the absolute difference between two random variables ( @xmath1 and @xmath2 ) which are both drawn from the standardized gaussian distribution . in this case , the absolute difference corresponds to one dimensional distance between points .",
    "note that this distance can not be negative .",
    "the cumulative density function ( cdf ) which provides the distribution of the absolute difference between points @xmath1 and @xmath2 , or their distance , is shown below and diagrammed further in figure 1 .",
    "@xmath3        figure 1 : diagram of zone of integration which covers the probability that the distance between @xmath1 and @xmath2 is less than @xmath4    the inner term of the integral is the product of the gaussian probability density function ( pdf ) for each variable under consideration .",
    "integrating with respect to the innermost integral we arrive at the expression : @xmath5 to determine the pdf of the above function , take the derivative of the integral with respect to @xmath6 : @xmath7 \\\\ & = \\frac{e^{-\\frac{x^2}{4}}}{\\sqrt{\\pi } } .\\end{aligned}\\ ] ] thus , the above expression is the pdf for the distribution of the absolute distance between two standardized gaussian variables in one dimension .",
    "a plot of this distribution is shown in figure 2 .",
    "+ figure 2 : probability density of the absolute difference between points    in this section we have derived the pdf for the absolute difference ( i.e. the non - negative distance ) between any two random variables with a standard gaussian distribution . in the following sections , this concept is extended to an arbitrary number of @xmath0-dimensions .",
    "the previous section derived the pdf for the distribution of the absolute difference in standard normal random variables in one dimension .",
    "we will now extend the concept to an arbitrary @xmath0 number of dimensions and use the euclidean metric to determine the distance between points .",
    "let two arbitrary points of interest in @xmath0-dimensions be further defined as @xmath8 and @xmath9 .",
    "we are interested in the euclidean distance between the two points , which is defined as :    @xmath10{}^{1/2}\\ ] ] +   + we generalize to @xmath0 dimensions now and begin by constructing the cdf which measures the probability that two points are separated by at most a distance less than or equal to @xmath6 .",
    "we do this by making a change of coordinates from standard euclidan @xmath11 to those in a @xmath0-dimensional hypersphere and then concern ourselves only with the first @xmath12-tant where all of the values in the euclidean coordinate system have positive values .",
    "this is possible since each euclidean coordinate is strictly non - negative and it has a corresponding probability that the difference between the two randomly distributed components is less than or equal to its radial value . by approaching the problem in this manner",
    ", we need only concern ourselves with integration of all angles in the hypersphere from 0 to @xmath13 , which we will designate using @xmath14 for each of the respective angular dimensions .",
    "the radial component @xmath15 captures the probability that the cdf is less than or equal to @xmath15 .",
    "figure 3 : plot of region of iterated integration in the @xmath12-quadrant    we begin by making the following replacements of hyperspherical coordinates , as shown by cohl in [ 5 ] .",
    "@xmath16 after converting to hyperspherical coordinates and integrating , the spherical volume element @xmath17 is given by @xmath18 the iterated integral in the first @xmath12- tant which defines the cdf for the probability that the distance between two random gaussian points is no greater than @xmath15 in @xmath0",
    "dimensional space is then given by @xmath19 + @xmath20 + @xmath21 + however , since we have completed the change of coordinates to the first @xmath12-tant of a hypersphere , the above expansion simplifies to + @xmath22 @xmath23 rearranging the terms so that we are only integrating with respect to the variable in question at each iterated integral , we have @xmath24 @xmath25 beginning with the innermost integral , we make use of the solution to the integral @xmath26 since @xmath0 is a positive integer , the above result simplifies to + @xmath27 after integration , the result is a constant with respect to all subsequent iterated integrals .",
    "thus , we can move it to the outside of the iterated integrals as shown below .",
    "@xmath28 @xmath29 @xmath30 + now we have a series of iterated integrals for decreasing powers of the sine function for all integral values for @xmath31 to @xmath32 .",
    "we now use the following identity which holds for any @xmath33 such that the real component is greater than @xmath34 and can be verified using a computer algebra system , @xmath35 since all powers of @xmath33 in the integral are strictly positive integers , each integral in our iterated integral expression can be replaced with the closed form expression above .",
    "the result is strictly a constant depending only on the power that the sine function is raised to in the integrand . since integration occurs for every power of sine for @xmath36 , the results of each of these integrals can be gathered as constants to the outside of the integral and formed into a product extending from @xmath32 to @xmath31 which leaves us with the following result @xmath37 the final integral found on the far right evaluates to @xmath13 , which leaves us the simplified cdf shown below .",
    "@xmath38 after further simplification this reduces to @xmath39 closer examination of the iterated product by expansion of terms and cancellation produces the following identity : @xmath40 substituting this result back into the cdf , we have @xmath41 taking the derivative with respect to @xmath15 yields the pdf below .",
    "@xmath42 \\\\   & = \\frac{2^{1-k}e^{-\\frac{r^2}{4}}r^{k-1}}{\\gamma\\left(\\frac{k}{2}\\right)}.\\end{aligned}\\ ] ] this reduces to @xmath43 , where @xmath15 is the absolute difference between two random variables which have each of their @xmath0 components distributed gaussian .",
    "this distribution is strictly non - negative , and a quick integration over all possible values of @xmath15 from 0 to infinity equals 1 , thereby confirming the validity of the pdf .",
    "a plot of the pdf is shown in figure 4 for a variety of dimensions ( @xmath0 ) , where @xmath44 .",
    "this plot is interesting in that as @xmath45 , the distribution resembles a shifted normal .",
    "furthermore , due to the underlying calculation of euclidean distance , there is no asymptotic limit regarding shift occurence .",
    "+     figure 4 : series of overlaid plots of pdfs for various values of @xmath0    in summary , we evaluated the euclidean distance between the absolute differences of two @xmath0-dimensional random variables by examining them as the equivalent of a set of points in the first @xmath46-tant of a hypersphere . through this conversion , we are able to evaluate an iterated integral to a simple closed form expression for the cdf which reflects the distance between two such points .",
    "further differentiation with respect to the radial component @xmath4 produces the pdf of the distribution for the distance between two points of interest in @xmath0-dimensional space .",
    "anytime that a new distribution is derived , it is natural to discuss the moments .",
    "we will do this by evaluating the following integral to calculate the @xmath47 raw moment of the distribution .",
    "+ @xmath48 factoring out constants , we are left with @xmath49 the following identity holds , provided that @xmath50 is strictly greater than zero .",
    "this is always true in this analysis given that the dimensionality has @xmath51 and the raw moment has @xmath52 .",
    "@xmath53 this simplifies to the expression below for the @xmath47 raw moment in @xmath0-dimensions .",
    "@xmath54 therefore , the first four raw moments for @xmath0-dimensions are shown below .",
    "+   + @xmath55 ( first raw moment ) .",
    "+   + @xmath56 ( second raw moment ) .",
    "+   + @xmath57 ( third raw moment ) .",
    "+   + @xmath58 ( fourth raw moment ) .    calculation of the central moments begins with the consideration of the first raw moment identified above and is used to calculate the @xmath47 central moment in the integral @xmath59 substituting values for @xmath60 and @xmath61 and restricting the lower bound of the integral to reflect strictly non - negative values , we have the following expression .",
    "@xmath62 unfortunately , determining a closed form expression for the @xmath47 central moment is non - trivial , but this paper will consider the second , third , and fourth central moments .",
    "we begin by determining the second central moment @xmath63 .",
    "+ @xmath64 + now determine the third central moment @xmath65 : @xmath66 finally , determine the fourth central moment @xmath67 : @xmath68 + the skewness for this distribution is as indicated below using @xmath69 , where @xmath70 is the @xmath70-th central moment . @xmath71 + the kurtosis for this distribution is provided below using the expression for @xmath72 .",
    "@xmath73 + all of the above identities hold provided that @xmath74 , which holds for the problem under consideration in this paper .",
    "thus , the closed form expression for the @xmath47 raw moment of the distance distribution in @xmath0-dimensions is shown .",
    "while a closed form expression for the @xmath47 central moment is not derived , the second , third , and fourth central moments are directly calculated .",
    "in this paper we have developed the pdf for the distribution of euclidean distance between any two points with @xmath0 features or dimensions .",
    "our methodology utilized a change of coordinates and allowed for the derivation of the cdf for the distribution of distances between points in @xmath0-dimensions . subsequent derivation provided the pdf and we were pleasantly surprised with the elegant closed - form expression .",
    "the distribution of distances between random points is especially important to data mining applications since several important classification algorithms rely on the use of distance to nearest known examples to determine the classification of unknown instances .",
    "frequently it is the case that a significant portion of features or dimensions in a given dataset are completely irrelevant to the problem , or are themselves noise .",
    "the inclusion of variables which provide no additional accuracy actually serves to increase the distance between points , and the effect of the added distance is distributed as outlined in this paper , and is dependent upon the number of extraneous dimensions .",
    "additionally , spurious results may be given erroneous meaning when such results emerge from the inclusion of a large number of randomly distributed features in the data under consideration .",
    "since we have derived the distribution of distances between neighboring points , future research will evaluate the use of this distribution in the reduction of such spurious associations and the probability that these associations would arise from the number of underlying features .",
    "[ 1 ] aggarwal , charu c. , alexander hinneburg , and daniel a. keim . on the surprising behavior of distance metrics in high dimensional space .",
    "springer berlin heidelberg , 2001 .",
    "[ 3 ] bellman , richard ernest . rand corporation ( 1957 )",
    ". dynamic programming .",
    "princeton university press .",
    "isbn 978 - 0 - 691 - 07951 - 6 . , republished : richard ernest bellman ( 2003 ) .",
    "dynamic programming .",
    "courier dover publications .",
    "isbn 978 - 0 - 486 - 42809 - 3 ."
  ],
  "abstract_text": [
    "<S> the curse of dimensionality is a common phenomenon which affects analysis of datasets characterized by large numbers of variables associated with each point . </S>",
    "<S> problematic scenarios of this type frequently arise in classification algorithms which are heavily dependent upon distances between points , such as nearest - neighbor and @xmath0-means clustering . given that contributing variables follow gaussian distributions , this research derives the probability distribution that describes the distances between randomly generated points in n - space . </S>",
    "<S> the theoretical results are extended to examine additional properties of the distribution as the dimension becomes arbitrarily large . with this distribution of distances between </S>",
    "<S> randomly generated points in arbitrarily large dimensions , one can then determine the significance of distance measurements between any collection of individual points . </S>"
  ]
}