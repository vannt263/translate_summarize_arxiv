{
  "article_text": [
    "video on demand drives network traffic growth today , and this trend is expected to continue until the end of the decade  @xcite . indeed , some studies predict that video traffic will grow by a factor 12 between 2010 and 2020  @xcite .",
    "video traffic is time varying in nature , with a significant peak during the evenings .",
    "this characteristic burdens service providers with the cost of having to expand their networks to handle increasing peak traffic even though the network remains underutilized during off - peak times .",
    "caching is a key technique to alleviate this problem by smoothing traffic in time and across the network .",
    "caching uses memories distributed throughout the network to duplicate popular content during off - peak times . during peak traffic times , these memories are then used to deliver requested content with less stress on the network .",
    "the core idea behind conventional caching schemes @xcite is to deliver part of the content locally from nearby caches .",
    "recently @xcite proposed instead to use _ coded caching_. unlike conventional uncoded schemes , coded caching creates linear combinations of chunks of content requested by different users .",
    "the combinations are formed such that each user can recover his requested chunk using the content of a nearby cache .",
    "therefore the caches are used not only to deliver parts of the content locally , but also to create coded multicasting opportunities .",
    "@xcite shows that , for some basic network structures , coded caching can yield significantly better performance than uncoded caching .",
    "this result demonstrates that widely - used caching algorithms such as least - frequently used ( lfu ) or least - recently used ( lru ) can be significantly suboptimal for cache networks  @xcite .",
    "the substantial gain of coded caching is achieved at the cost of increased delay during content delivery",
    ". however , many applications , such as video on demand , are delay sensitive , and consequently this added delay resulting from coding is problematic .",
    "this raises the question of how much of the coded caching gain can be obtained within some fixed delay constraint .    in this paper",
    ", we initiate the investigation of this tradeoff between coded caching gain and delivery delay .",
    "we develop a computationally efficient caching scheme that effectively exploits coding opportunities while respecting delivery - delay constraints .",
    "the proposed scheme achieves the optimum performance suggested by  @xcite in the large - delay limit , while still offering a significant gain for delay - sensitive applications .",
    "a video - streaming prototype implementing these ideas confirms the practicality and gains of the proposed scheme .",
    "we consider an origin server connected through a network to @xmath0 edge caches .",
    "the server stores a collection of videos , split into a number of symbols .",
    "each symbol corresponds to a chunk of data of constant size , say between @xmath1 and @xmath2 kb , plus a header providing a unique identifier for the symbol . to each edge cache",
    "are attached a number of users , and for simplicity we assume that each user can access only a single edge cache .    in a _ placement phase _ , occurring during a time of low network load , each cache independently prefetches every symbol independently at random with some probability @xmath3 , with @xmath4 $ ] chosen such that memory constraints at the users are satisfied as proposed in  @xcite .",
    "therefore , approximately a fraction @xmath3 of each video is stored at each cache .",
    "we assume that the server knows which content symbols are stored in which cache ( see the discussion in section  [ sec : implementation ] for how this can be achieved in practice ) . in a later _ delivery phase _ , occurring during a time of high network load , the users attached to the caches issue a sequence of requests , each request for one content symbol .",
    "the server can represent each such request by a triple @xmath5 , where @xmath6 is the cache handling the user requesting the symbol , @xmath7 is the subset of caches that have stored the requested symbol , and @xmath8 is the deadline by which this request needs to be served . for ease of notation , we will assume that @xmath9 is an integer , measured in some arbitrary unit , say milliseconds , and that time starts at @xmath10 . for the same reason , we sometimes write @xmath11 for @xmath12 when the value of @xmath8 is immaterial .",
    "note that this notation suppresses the identifier of the actual symbol requested by the user , as this identifier is not needed for most of the subsequent analysis .",
    "observe further that each request @xmath13 satisfies the property @xmath14 , because otherwise this request could be served from the user s local cache without contacting the server .",
    "[ eg : requests ] consider a user attached to cache one requesting a video in a streaming application . at time @xmath10 , the user issues a sequence of requests for the first few consecutive symbols of the video . the first request might have a tight deadline , say of @xmath15 ; the second request might have a less stringent deadline of @xmath16 .",
    "assume that the first requested symbol is available at caches two and three and that the second requested symbol is available only at cache two .",
    "the two requests are then described by is used as notational shorthand for @xmath17 .",
    "similar simplified notation is used throughout . ]",
    "@xmath18 .",
    "the server responds to these requests by sending multicast packets to all the @xmath0 edge caches associated with this server , and our aim is to minimize the bandwidth required by the server .",
    "the key observation is that the cached symbols can be used to create coded multicasting opportunities , in which a single coded packet is simultaneously useful for several users with different demands  @xcite .",
    "[ eg : coding ] consider the sequence of requests @xmath19 issued by three distinct users .",
    "the first request is via cache one requesting a symbol available at caches two and three , the second request is via cache two for a symbol available at cache one , and so on .    for concreteness ,",
    "assume that the five requested symbols are called @xmath20 , @xmath21 , @xmath22 , @xmath23 , @xmath24 .",
    "the server could satisfy all five requests by transmitting @xmath25 .",
    "however , by making use of the cache contents , the server can do better .",
    "note that symbol @xmath21 requested via cache two is available at cache one .",
    "similarly , symbol @xmath24 requested via cache one is available at cache two .",
    "hence , from the single multicast transmission @xmath26 and using their stored symbols , both caches can recover their requested symbol . here",
    "@xmath27 denotes the bitwise xor operation . by an analogous argument ,",
    "caches one , two , and three can recover their respective requested symbols @xmath20 , @xmath22 , and @xmath23 from the single multicast transmission @xmath28 .",
    "thus , the server can satisfy all five requests by just two coded multicast transmissions , @xmath26 and @xmath28 . because the xor of several symbols has the same size as a single symbol",
    ", this approach reduces the required bandwidth out of the server by a factor @xmath29 .    from example",
    "[ eg : coding ] , we see that two requests @xmath30 and @xmath31 can be _ merged _ , meaning that they can both be satisfied by a single coded multicast transmission from the server , if @xmath32 and @xmath33 .",
    "we denote the merged request by @xmath34 .",
    "this notations indicates that this new request is to be sent to both caches @xmath35 , @xmath36 , has a deadline of @xmath37 , and the symbols stored in both caches are @xmath38 .",
    "consider next two such merged requests @xmath39 and @xmath40 .",
    "these merged requests can be further merged , if @xmath41 the corresponding merged request is @xmath42    whenever the current time is equal to the time @xmath8 of the merged request , the server needs to respond to this request by transmitting the corresponding coded multicast packet . in order to minimize the required bandwidth out of the server ,",
    "our goal is to merge requests as much as possible , thereby minimizing the number of transmitted coded multicast packets .",
    "the reduction in server bandwidth resulting from coded multicasting compared to uncoded transmission is the _ global coding gain _",
    ", defined as the ratio between the bandwidth of the uncoded scheme and the coded scheme .",
    "we adopt this global coding gain as our figure of merit in this paper .",
    "the goal is thus to design rules to merge user requests such as to maximize this global coding gain .",
    "because symbols are requested by the users sequentially and need to be served within a deadline , this merging needs to be performed online , i.e. , among the requests currently pending at the server .",
    "furthermore , because the queue of pending requests can be quite large as illustrated by the next example , the merging algorithm needs to be computationally efficient .",
    "[ eg : length ] for an http adaptive streaming application with @xmath43 users , video rate of @xmath44 kb / s , delay tolerance of @xmath45 s , and symbol size of @xmath2 kb , the server queue will have approximately @xmath46 unmerged requests at any given time . for a progressive download video on - demand",
    "application with @xmath47 users , video rate of @xmath48 kb / s , delay tolerance of @xmath49 s , and symbol size of @xmath50 kb , the server queue will have approximately @xmath51 unmerged requests at any given time .",
    "for a complete download video on - demand application with @xmath52 users , video rate of @xmath53 kb / s , delay tolerance of @xmath54 s , and symbol size of @xmath1 kb , the server queue will have approximately @xmath55 unmerged requests at any given time .",
    "we now present several request - merging rules and compare their performance .",
    "we start with two examples to motivate the new proposed rule introduced below .",
    "_ first - fit rule _ : consider an ordered sequence of ( potentially merged ) requests @xmath39 , @xmath56 , @xmath57 , @xmath58 with @xmath59 .",
    "assume that a new request for a single symbol @xmath60 arrives at the server .",
    "in the first - fit rule , the server traverses its queue starting from @xmath61 to find the first ( tightest deadline ) request @xmath62 that can be merged with the new request , i.e. , such that the two conditions in   hold .",
    "the newly merged request is @xmath63 if no such request can be found in the queue , the new request @xmath64 is appended to the queue .",
    "this type of first - fit rule has a long history ; most relevant to this work is that it was used in @xcite in the context of coloring random graphs .",
    "the first - fit rule falls into the class of _ sequential _ merging rules .",
    "these rules keep a list of merged requests at the server .",
    "whenever a new request arrives , these algorithms traverse this list sequentially starting from the request with the tightest deadline and try to merge the new request with one already in the queue . the decision to merge with a queued request @xmath65",
    "is made based only on the information in that request .",
    "sequential merging rules have several advantages .",
    "first , because each queued request is considered only once , and because each such consideration is based on only information in the request and hence takes @xmath66 time to evaluate , the computational complexity to insert one new request into the server queue is @xmath67 , where @xmath68 is the queue length .",
    "second , for the same reasons , such rules can be easily parallelized . indeed",
    ", several threads can sequentially traverse the queue in parallel , each handling the merging operation of one new request .",
    "thus , sequential merging rules are computationally efficient .",
    "we next introduce a new sequential merging rule .",
    "_ perfect - fit rule _ : this is a sequential merging rule , in which a new request @xmath60 is merged with the first queued request @xmath65 satisfying the following three conditions : @xmath69 the first two conditions ensure that the two requests under consideration can in fact be merged .",
    "the third condition stipulates that the merging should be optimal in terms of the size of the intersection of the cache availability @xmath70 of the merged request .",
    "the relevance of this third condition will be discussed in more detail below .",
    "the first - fit and perfect - fit rules can be seen to be approximately optimal in terms of minimizing server bandwidth in certain regimes . for the large - queue regime , @xmath71 ,",
    "it is shown in  @xcite that a nonsequential version of the perfect - fit rule is optimal .",
    "indeed , as @xmath71 , with high probability there will be a queued request that satisfies all three merging conditions , and as we will see below , the third condition ensures that the server bandwidth is minimized .",
    "for the small - queue regime , @xmath72 , and assuming a large enough number of caches @xmath0 , results on online coloring of random graphs  @xcite suggest that the first - fit rule is close to optimal .",
    "the regime of most interest is clearly when the queue length is in between those two extremes ( see example  [ eg : length ] in section  [ sec : problem ] ) .",
    "we next introduce a novel family of sequential merging rules for this intermediate regime . before we describe these rules in detail , we need some additional notation .",
    "let @xmath39 and @xmath40 be two mergeable requests , and consider their merged version .",
    "merging these two requests has two effects on the system , one beneficial and one detrimental .",
    "the beneficial effect is that both requests can now be served with a single coded multicast transmission .",
    "the detrimental effect is that it is now harder for other requests to further merge with this merged version .",
    "the following example highlights the fundamental tension between these two effects .",
    "[ eg : misfit1 ] consider the sequence of requests @xmath73 ,  @xmath74 , @xmath75 .",
    "the request @xmath73 could be merged with either @xmath74 , creating the merged request @xmath76 , or with @xmath77 , creating the merged request @xmath78 .",
    "both of these merged requests correspond to two users , however , the first one can not accept any further requests whereas the second one can accept an additional request such as @xmath79 .",
    "assume while traversing the queue to insert the new request @xmath80 we encounter the already queued request @xmath74 .",
    "we can either merge the two requests at the cost of destroying merging opportunities for future requests , or we can decide not to merge them at the cost of potentially being unable to merge the current request at all . resolving this fundamental tension is at the core of a good request - merging rule .     and @xmath81 .",
    "]    to capture this tension , we introduce the following _ misfit function _ @xmath82 . for two requests @xmath83 and @xmath81 , the misfit is given by @xmath84 if @xmath85 and @xmath86 ( ensuring that the requests are mergeable ) , and the misfit is equal to @xmath87 otherwise , as is depicted in fig .  [",
    "fig : misfit ] .",
    "the misfit between two requests takes values in the set @xmath88 , where @xmath0 is the number of caches .",
    "this definition is motivated as follows .",
    "assume we merge @xmath89 and @xmath81 .",
    "the cached symbols @xmath90 in @xmath91 and @xmath92 in @xmath93 are thus useful to enable the merging .",
    "furthermore , the symbols in @xmath94 will be useful to further merge the already merged requests .",
    "on the other hand , the symbols in @xmath95 are not used for either purpose and will be wasted by merging the two requests .",
    "the misfit function @xmath96 thus measures the number of these wasted symbols .",
    "[ eg : misfit2 ] consider the same sequence of requests as in example  [ eg : misfit1 ] .",
    "here we have the misfit values @xmath97 consistent with our intuition that the second merged request is a better fit than the first merged request .",
    "the perfect - fit rule can now be equivalently described as the sequential rule that merges the new request with the first queued request with zero misfit .",
    "this scheme performs well for long queues , when we are likely to find queued requests that can be merged with the new request without wasting any symbols .",
    "similarly , the first - fit rule can be equivalently described as the sequential rule that merges the new request with the first queued request with finite misfit .",
    "this rule performs well for short queues , where any chance to merge requests can not be missed , irrespective of the number of wasted symbols .",
    "this suggests the introduction of a general class of merging rules , which we term _ @xmath98-fit threshold rule_.    _ @xmath98-fit threshold rule _ : this is a sequential merging rule , in which a new request @xmath60 is merged with the first queued request @xmath65 such that @xmath99    we thus see that the first - fit rule is a @xmath100-fit threshold rule and that the perfect - fit rule is a @xmath101-fit threshold rule .",
    "as @xmath98 varies from @xmath100 to @xmath101 , the @xmath98-fit threshold rule describes a family of sequential merging rules .",
    "the optimal value @xmath102 of the threshold @xmath98 depends strongly on the parameters @xmath0 and @xmath68 , and to a lesser extent on the parameter @xmath3 .",
    "we expect the value of the optimal threshold @xmath102 to be decreasing with the queue length @xmath68 ; when @xmath103 , @xmath104 , when @xmath105 , @xmath106 .",
    "this is indeed the case , as is illustrated in fig .",
    "[ fig : scalingl ] .     for @xmath98-fit threshold merging rule for different values of @xmath98 . in the figure ,",
    "the number of caches is fixed to @xmath107 and the cache probability to @xmath108 . ]    more formally , in the limit as the queue length @xmath71 , a nonsequential algorithm is shown in  @xcite to achieve a global coding gain of @xmath109 the sequential perfect - fit rule presented here ( i.e. , @xmath110 ) achieves asymptotically the same rate .",
    "for example , fig .",
    "[ fig : scalingl ] shows that for @xmath107 , @xmath108 , and @xmath111 , the global coding gain achieved by the perfect - fit rule is more than @xmath112 , which is quite close to the limiting value of around @xmath113 resulting from  .     in the system for first - fit merging ( @xmath114 ) and different values of cache probability @xmath3 . in the figure ,",
    "the queue length @xmath68 is equal to @xmath0 , and @xmath115 , @xmath116 , @xmath117 . ]",
    "it is also instructive to consider the behavior of the global coding gain as the length of the queue increases .",
    "this behavior is depicted in fig .",
    "[ fig : scalingk ] for the scenario in which the queue length @xmath68 is equal to the number of caches @xmath0 .",
    "this scaling of @xmath68 with @xmath0 corresponds to each user tolerating only a delay of a single outstanding symbol .",
    "[ fig : scalingk ] shows that even in this setting with extremely tight deadline constraints , the global coding gain can be quite substantial , being as large as a factor @xmath118 for @xmath119 queue length under a first - fit merging rule ( i.e. , @xmath120 ) .",
    "results from coloring of large random graphs  @xcite suggest that we should expect the global coding gain to scale approximately as @xmath121 in this scenario .",
    "[ fig : scalingk ] indicates that this seems indeed to be the case .",
    "to test the ideas developed in this paper in practice , we developed a video - streaming prototype making use of the coded caching approach .",
    "the prototype consists of three components : a video player , a client , and a server , as depicted in fig .",
    "[ fig : schematic ] .",
    "the server implements the operations of the origin server of the system .",
    "the server reads requests for content symbols from the client and responds by sending coded content symbols back to the client .",
    "the client implements the operations of the cache .",
    "the client reads requests for video content from the video player , converts these video requests into requests for a sequence of content symbols , and requests from the server those content symbols not found in its local cache .",
    "the client then reads the coded content symbols sent by the server , decodes them with the help of its local cache , reorders the decoded content symbols , and forwards them in order to the video player .",
    "finally , the video player interacts with the human end user .",
    "the player transmits a video request to the client , reads the corresponding response , and displays it to the user . to simulate @xmath0 caches",
    ", we replicate the player and client processes @xmath0 times .",
    "conceptually , the video player , client , and server processes are placed at different points in the network ( at the user , the edge cache , and the origin server , respectively ) . however , for simplicity , two or more of them may run on the same machine . for the video player",
    ", we use the open - source vlc media player .",
    "the client and server are implemented in java ; the client is about @xmath122 lines of code , the server about @xmath52 , with an additional about @xmath122 lines of shared code .",
    "the content database consists of three open - source videos ( `` elephants dream '' , `` big buck bunny '' , and `` sintel '' ) from the blender foundation , transcoded into flash video ( flv ) format .",
    "the files have a size of around @xmath123 mb each , corresponding to a bit rate of approximately @xmath124 kb / s .",
    "each video stream is divided into content symbols of constant size @xmath2 kb .",
    "a uniquely identifying header is attached to each symbol .",
    "this header consists of a string identifying the video ( `` video1.flv '' , `` video2.flv '' , `` video3.flv '' ) together with a @xmath125-bit sequence number . for coded content symbols ,",
    "the header consists of the number of symbols combined into this coded symbol together with a list of corresponding symbol headers .",
    "compared to the symbol size of @xmath2 kb , the size of the headers is small .",
    "the client caches are populated offline .",
    "we simulate this procedure by reading the entire video database from disk , but keeping only a fraction @xmath3 of read content symbols in main memory .",
    "which symbols to keep is decided using a pseudo - random number generator initialized with a randomly chosen seed .",
    "the seed value is sent from the client to the server at the beginning of the communication protocol .",
    "the server recomputes the cache configuration of the client by using the same pseudo - random number generator initialized with identical seed .",
    "all communication between the three components of the video - streaming prototype uses tcp . to keep the number of components needed to a minimum",
    ", we simulate application - layer multicast from the server to the clients by using several parallel unicast connections over which identical content is sent .",
    "the most interesting aspect of the implementation ( and the focus of sections  [ sec : problem ] and  [ sec : main ] ) is the server , which consists of three main components as shown in fig .",
    "[ fig : schematic_server ] .",
    "a number of request - handler threads , one for each client , read symbol requests sent by the clients over the network .",
    "new requests are enqueued in the request queue ( implemented as a java linkedblockingqueue ) .",
    "a single encoder thread dequeues requests from the request queue .",
    "it then traverses the coded symbol queue ( implemented as a java delayqueue ) to find an existing request with which the new request can be merged using a @xmath98-fit threshold merging rule .",
    "if the new request can not be merged with any existing request , it is enqueued in the coded symbol queue ordered by time to deadline .",
    "the transmitter thread dequeues from the coded symbol queue elements that are close to their deadline and `` multicasts '' them over the network to the clients .",
    "[ fig : demo ] shows a screenshot of our video - streaming prototype , in which three users simultaneously request a different video each ( right of screen ) .",
    "the bottom - left window shows the server process handling the encoding of the video streams .",
    "the top - left window shows the client processes handling the decoding and reconstruction of the video streams .",
    "for example , in line @xmath50 , the client indicates that it received a coded symbol containing the xor of symbol @xmath126 of video stream two and symbol @xmath127 of video stream one , from which it decoded the symbol of stream two .",
    "similarly , line @xmath128 shows the decoding operation for a coded symbol consisting of the xor of three symbols from different video streams .",
    "the performance of our implementation as measured by the global coding gain is depicted in fig .",
    "[ fig : coding_gain ] .",
    "the figure shows a @xmath129 s trace for a number of concurrent users increasing by one every @xmath49 s from one to four .",
    "as can be seen from the figure , the global coding gain increases with the number of concurrent requests being handled . for the last @xmath49",
    "s of the trace , during which four videos are served simultaneously , the coding gain is almost @xmath45 , corresponding to a server bandwidth reduction of close to @xmath130 compared to uncoded transmission .",
    "the coding gain of the trace is close to the simulated values ( also shown in the figure ) . because the coding gain reported from the trace",
    "takes overhead into account , whereas the coding gain from the simulations does not , this result confirms that the overhead is negligible .",
    "the caching problem is related to the index - coding problem  @xcite ( or , equivalently  @xcite , to the network - coding problem  @xcite ) . in the index - coding problem",
    ", a server is connected to users through a shared link .",
    "the server has access to a database of files .",
    "each user has local access to a fixed subset of these files and is interested in one file not locally available .",
    "the objective is to deliver the requested files to the users with the minimum number of transmissions from the server . from this description",
    ", we see that for _ fixed _ cache content and for _ fixed _ requests , the caching problem induces an index - coding subproblem .    the caching problem considered in this paper and",
    "the index - coding problem differ as follows .",
    "first , in the index - coding problem the locally available files are fixed , whereas in the caching problem they need to be designed . in other words",
    ", choosing the side information is part of the problem statement in index coding , but part of the solution in the caching problem .",
    "second , in index - coding , the files requested by the users are fixed , whereas in caching any of exponentially many requests are possible . therefore , after having deciding on the cache content , we are faced with exponentially many index - coding subproblems in the caching problem .",
    "third , the notion of delay , which is a central quantity in this work , is absent in index coding .",
    "the authors thank s.  stolyar and m. carroll for helpful discussions .",
    "y.  birk and t.  kol , `` coding on demand by an informed source ( iscod ) for efficient broadcast of different supplemental data to caching clients , '' _ ieee trans .",
    "inf . theory _ ,",
    "52 , pp .",
    "28252830 , june 2006 ."
  ],
  "abstract_text": [
    "<S> coded caching is a recently proposed technique that achieves significant performance gains for cache networks compared to uncoded caching schemes . </S>",
    "<S> however , this substantial coding gain is attained at the cost of large delivery delay , which is not tolerable in delay - sensitive applications such as video streaming . in this paper , we identify and investigate the tradeoff between the performance gain of coded caching and the delivery delay . </S>",
    "<S> we propose a computationally efficient caching algorithm that provides the gains of coding and respects delay constraints . </S>",
    "<S> the proposed algorithm achieves the optimum performance for large delay , but still offers major gains for small delay . </S>",
    "<S> these gains are demonstrated in a practical setting with a video - streaming prototype . </S>"
  ]
}