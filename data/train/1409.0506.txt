{
  "article_text": [
    "directional data ( data on a general sphere of dimension @xmath0 ) appear in a variety of contexts being the simplest one provided by observations of angles on a circle ( circular data ) , for instance , from wind directions or animal orientation @xcite .",
    "stars positions could be seen as data on a two - dimensional sphere and quite recently , directional data in higher dimensions have been considered in text mining @xcite . in order to identify a statistical pattern within a certain collection of texts",
    ", these objects may be represented by a vector on a sphere where each vector component gives the relative frequency of a certain word .",
    "for instance , from this vector - space representation , text classification ( see @xcite ) can be performed , but other interesting problems such as popularity prediction could be tackled .",
    "such a characterization can be done , for instance , for articles in news aggregators , where popularity prediction in web texts ( news ) can be quantified by the number of comments or views ( see @xcite ) .",
    "in addition , in order to model or predict the popularity of a certain web entry based on its contents ( in a vector - space form ) , a linear - directional regression model could be used .",
    "+ when dealing with directional and linear variables at the same time , the joint behavior could be modeled by considering a flexible density estimator , as the one proposed by @xcite .",
    "nevertheless , a regression approach may be more useful , allowing at the same time for explaining a relation between the variables and for making predictions .",
    "nonparametric regression estimation methods for linear - directional models have been proposed by different authors .",
    "for instance , @xcite introduced a general local linear regression method on manifolds , and quite recently , @xcite presented a local polynomial method for the regression function when both the predictor and the response are defined on spheres . + despite the fact that these methods provide flexible estimators which may capture the regression shape , in terms of interpretation of the results , purely parametric models may be more convenient . in this context , goodness - of - fit testing methods can be designed , providing a tool for assessing a certain parametric linear - directional regression model .",
    "goodness - of - fit tests for directional data , or including a directional component in the data generating process , have not been deeply studied . for the density case , @xcite provide a nonparametric goodness - of - fit test for directional densities and similar ideas are used by @xcite for directional - linear densities . except for the exploratory tool and lack - of - fit test for linear - circular regression developed by @xcite",
    "there are no other works in the regression context .",
    "a goodness - of - fit test for parametric regression models will be presented in this paper .",
    "the test is based on a squared distance between the parametric fit and a nonparametric one .",
    "specifically , a modified local linear estimator , similar to the one proposed by @xcite will be introduced with this purpose .",
    "theoretical properties of the test statistic will be studied , and its effectiveness in practice will be confirmed by simulation results . + the paper is organized as follows .",
    "basic notation is introduced in section [ gofreg : sec : reg ] , where the projected local regression estimator is also analyzed .",
    "section [ gofreg : sec : gof ] includes the main results , regarding the asymptotic behavior of the test statistic .",
    "a consistent bootstrap strategy is also presented .",
    "the performance of the proposed method is assessed for finite samples in a simulation study , provided in section [ gofreg : sec : simu ] .",
    "section [ gofreg : sec : data ] shows a real data application on news popularity prediction .",
    "an appendix contains the proofs of the main results .",
    "proofs of technical lemmas and further information on the simulation study , jointly with more simulation results , are provided as supplementary material .",
    "some basic concepts in nonparametric directional density and linear - directional regression estimation will be provided in this section .",
    "basic notation will be introduced , jointly with some motivation for the regression estimator proposal that will be used in the testing procedure .",
    "+ let @xmath1 denote the @xmath0-sphere in @xmath2 , with associated lebesgue measure denoted by @xmath3 ( when there is no possible confusion , the surface area of @xmath4 will be denoted by @xmath5 , @xmath6 ) .",
    "a directional density @xmath7 on @xmath4 satisfies @xmath8 .",
    "consider a random sample @xmath9 in @xmath4 , from a directional random variable @xmath10 with density @xmath7 .",
    "a kernel density estimator for @xmath7 was introduced by @xcite and @xcite . for a given point @xmath11 ,",
    "the kernel density estimator is defined as @xmath12 where @xmath13 is a directional kernel , @xmath14 is the bandwidth parameter and the normalizing constant @xmath15 is given by @xmath16 with @xmath17 and @xmath18 .",
    "+ in many practical situations , the interest lies in the analysis of the directional variable @xmath10 jointly with a real random variable @xmath19 .",
    "the joint behavior of both variables @xmath20 may be studied by a density approach , as in @xcite . however , a regression approach may be more suitable in some situations .",
    "assume that the directional random variable @xmath10 with density @xmath7 may be the covariate in the following regression model @xmath21 where @xmath19 is a scalar random ( response ) variable , @xmath22 is the regression function given by the conditional mean ( @xmath23}}$ ] ) , and @xmath24 is the conditional variance ( @xmath25}}$ ] ) .",
    "errors are collected by @xmath26 , a random variable such that @xmath27=0 $ ] , @xmath28=1 $ ] and @xmath29 $ ] and @xmath30 $ ] are bounded random variables .",
    "+ the regression and density functions @xmath31 can be extended from @xmath4 to @xmath32 by considering a radial projection ( see @xcite for the density case ) .",
    "this allows for the consideration of derivatives of these functions and the use of taylor expansions .",
    "1 .   @xmath22 and @xmath7 are extended from @xmath4 to @xmath32 by @xmath33 and @xmath34 .",
    "@xmath22 is three times and @xmath7 is twice continuously differentiable and @xmath7 is bounded away from zero .",
    "[ gofreg : assump : a1 ]    the continuity up to the second derivatives of @xmath7 and up to the third derivatives of @xmath22 , together with the @xmath0-spherical compact support , guarantees that these functions are in fact uniformly bounded .",
    "as a consequence of the radial extension , the directional derivative of @xmath22 in the direction @xmath35 and evaluated at @xmath35 is zero , that is , @xmath36 .",
    "+ consider , from now on , that a random sample from model ( [ gofreg : model ] ) , namely @xmath37 independent and identically distributed ( iid ) vectors in @xmath38 , is available .",
    "given two points @xmath39 , under assumption [ gofreg : assump : a1 ] , the one - term taylor expansion of @xmath22 at @xmath40 , conditionally on @xmath9 , can be written as : @xmath41 with @xmath42 the _ projection matrix_. for a given @xmath43 , let @xmath44 be a collection of @xmath0 resulting vectors that complete @xmath35 to an orthonormal basis @xmath45 of @xmath2 ( given , for example , by gram - schmidt ) . the projection matrix @xmath46 is a @xmath47 semiorthogonal matrix , _",
    "i.e. _ @xmath48 , with @xmath49 the identity matrix of dimension @xmath0 . by the spectral decomposition theorem , @xmath50 .",
    "+ with this setting , the first coefficient @xmath51 captures the constant effect in @xmath52 while the second one , @xmath53 , contains the linear effects of the _ projected gradient _ of @xmath22 given by @xmath54 .",
    "it should be noted that @xmath55 has dimension @xmath0 ( an appropriate dimension in the @xmath0-sphere @xmath56 , instead of having dimension @xmath57 , the one that will arise from an usual taylor expansion in @xmath2 ) .",
    "the previous taylor expansion provides the motivation for the _ projected local estimator _ of the regression function @xmath22 at @xmath43 that will be introduced and analyzed in the next section .      as in the previous section ,",
    "consider @xmath37 a random sample from model ( [ gofreg : model ] ) and recall the expansion of the regression function derived under assumption [ gofreg : assump : a1 ] : @xmath58 the projected local estimator proposed in this work is obtained as a local fit by weighting the constants @xmath59 or the hyperplanes @xmath60 according to the influence of @xmath40 over @xmath35 .",
    "both situations can be formulated together as a weighted least squares problem : @xmath61 where @xmath62 is the kronecker delta , used to control both the local constant ( @xmath63 ) and local linear ( @xmath64 ) fits , and @xmath65 are the directional kernels , defined as in ( [ gofreg : kde ] ) .",
    "the solution to the minimization problem ( [ gofreg : wlsp ] ) is given by @xmath66 where @xmath67 is the vector of observed responses , @xmath68 is the weight matrix and @xmath69 is the design matrix . specifically : @xmath70 and @xmath71 , with @xmath72 denoting a column vector of length @xmath73 with all entries equal to one and whose dimension will be omitted and determined by the context .",
    "the projected local ( constant or linear ) estimator at @xmath35 is given by the estimated coefficient @xmath74 and is a weighted linear combination of the responses : @xmath75 where @xmath76 ( @xmath77 denotes a unit canonical vector ) .",
    "+ it should be noted that , for @xmath63 ( local constant ) , @xmath78 .",
    "this corresponds to the nadaraya - watson estimator for directional predictor and scalar response , introduced by @xcite .",
    "@xcite have recently proposed a local linear estimator for model ( [ gofreg : model ] ) , but from a different approach .",
    "specifically , these authors consider another alternative for developing the taylor expansions of @xmath22 based on the tangent - normal decomposition : @xmath79 , with @xmath80 and @xmath81 satisfying that @xmath82 .",
    "this approach leads to an",
    "_ overparametrized _ design matrix of @xmath83 columns which makes @xmath84 exactly singular .",
    "this problem can be avoided in practice by computing a pseudo - inverse .",
    "although both approaches come from different motivations , it can be seen that their asymptotic behavior is the same ( conditional bias , variance and asymptotic normality ) .",
    "this can be checked by considering the different parametrization used in @xcite , where the smoothing parameter is @xmath85 .",
    "it should be also noted that , for the circular case ( @xmath86 ) , the projected local estimator corresponds to the proposal by @xcite .",
    "see supplement for a detailed discussion of particular cases .",
    "asymptotic bias and variance for the estimator ( [ gofreg : betaest ] ) , jointly with its asymptotic normality , will be derived in this section .",
    "some further assumptions will be required :    1 .",
    "the conditional variance @xmath24 is uniformly continuous and bounded away from zero .",
    "[ gofreg : assump : a2 ] 2 .",
    "the directional kernel @xmath13 is a continuous and bounded function @xmath87 with exponential decay : @xmath88 , @xmath89 , with @xmath90.[gofreg : assump : a3 ] 3 .",
    "the sequence of bandwidths @xmath91 is positive and satisfies @xmath92 and @xmath93 .",
    "[ gofreg : assump : a4 ]    conditions [ gofreg : assump : a2 ] and [ gofreg : assump : a4 ] are the usual ones for the multivariate local linear estimator ( see @xcite ) .",
    "condition [ gofreg : assump : a3 ] allows for the use of non - compactly supported kernels , such as the _",
    "von mises kernel _ @xmath94 , and implies condition * a2 * in @xcite .",
    "[ gofreg : theo : biasvar ] under assumptions [ gofreg : assump : a1][gofreg : assump : a4 ] , the conditional bias and variance for the projected local estimator with @xmath95 are given by @xmath96}}&=\\frac{b_q(l)}{q } b_p({\\mathbf{x}})h^2+{\\mathpzc{o}_\\mathbb{p}\\lph^2{\\right)}}+\\delta_{p,0}{\\mathcal{o}_\\mathbb{p}{\\left(}\\frac{h}{\\sqrt{nh^q}}{\\right)}},\\\\      { \\mathbb{v}\\mathrm{ar}{\\left[}\\hat m_{h , p}({\\mathbf{x}})|{\\mathbf{x}}_1,\\ldots,{\\mathbf{x}}_n{\\right]}}&=\\frac{\\lambda_q(l^2)\\lambda_q(l)^{-2}}{nh^qf({\\mathbf{x}})}\\sigma^2({\\mathbf{x}})+{\\mathpzc{o}_\\mathbb{p}{\\left(}{\\left(nh^q\\right)}^{-1}{\\right ) } } ,      \\end{aligned}\\ ] ] uniformly for all @xmath97 , where the terms in the bias are given by ( @xmath98 stands for the trace ) : @xmath99 } , & p=0,\\\\      { \\mathrm{tr}\\left[{\\boldsymbol{\\mathcal{h}}}_m({\\mathbf{x}})\\right ] } , & p=1 ,      \\end{array}{\\right.}\\quad b_q(l)=\\frac{\\int_0^\\infty l(r ) r^{\\frac{q}{2}}\\,dr}{\\int_0^\\infty l(r ) r^{\\frac{q}{2}-1}\\,dr}.\\ ] ]    as it happens in the euclidean setting , the conditional bias is reduced from the local constant fit to the local linear one , whereas the variance remains the same for both estimators .",
    "the expressions and residual terms obtained in this setting agree with their euclidean analogues ( see @xcite ) , with the role of the kernel s second moment played by @xmath100 and the integral of the squared kernel by @xmath101 .    from theorem [ gofreg : theo",
    ": biasvar ] , an _ equivalent kernel _ expression can be obtained .",
    "such a result allows for an explicit form of the weights @xmath102 , resulting an estimator asymptotically equivalent ( in probability ) to ( [ gofreg : hatm ] ) .",
    "this formulation , for @xmath64 , provides a simpler form for the weighting kernel than the one given in ( [ gofreg : hatm ] ) . in addition , the asymptotic expression will only depend on the datum @xmath103 and not on the whole data sample",
    ". this feature will be crucial for developing the goodness - of - fit test in section [ gofreg : sec : gof ] and the asymptotic normality of the estimators , collected in the next result .",
    "[ gofreg : coro : equiv ] under assumptions [ gofreg : assump : a1][gofreg : assump : a4 ] , the projected local estimator @xmath104 for @xmath95 satisfies uniformly in @xmath43 : @xmath105    note that the equivalent kernel is the same for @xmath63 and @xmath64 , as it happens in the euclidean case when estimating the regression function ( see pages 6466 of @xcite with @xmath106 and @xmath95 ) .    [ gofreg : theo : asympnorm ] under assumptions [ gofreg : assump : a1][gofreg : assump : a4 ] and for @xmath95 , for every fixed point @xmath43 such that @xmath107<\\infty$ ] , for some @xmath108 , @xmath109",
    "in this section , a test statistic for assessing if the regression function @xmath22 belongs to a class of parametric functions @xmath110 will be introduced . assuming that model ( [ gofreg : model ] )",
    "holds , the goal is to test the null hypothesis @xmath111 with @xmath112 known ( simple hypothesis ) or unknown ( composite hypothesis ) and where the statement _ for all _ holds except for a set of probability zero and _ for some _ holds for a set of positiveprobability . + the proposed statistic to test @xmath113 compares the projected local estimator introduced in subsection [ gofreg : subsec : est ] with a parametric estimator in @xmath114 throughout a squared weighted norm : @xmath115 where @xmath116 represents the local smoothing of the function @xmath22 from measurements @xmath117 and @xmath118 denotes either the known parameter @xmath119 ( simple null hypothesis ) or a consistent estimator ( composite null hypothesis",
    "; see condition [ gofreg : assump : a6n ] below ) .",
    "this smoothing of the ( possibly estimated ) parametric regression function is included to reduce the asymptotic bias @xcite . in order to mitigate the effect of the difference between @xmath120 and @xmath121 in sparse areas of the covariate ,",
    "the squared difference is weighted by a kernel density estimate of @xmath10 , namely @xmath122 .",
    "furthermore , by the inclusion of @xmath122 , the effects of the unknown density both on the asymptotic bias and variance are removed .",
    "optionally , a weight function @xmath123 can also be considered for removing possible boundary effects .",
    "+ two additional assumptions regarding the smoothness of the parametric regression function and the estimation of @xmath119 in the composite hypothesis are required for deriving the distribution of @xmath124 under the null hypothesis :    1 .",
    "@xmath125 is continuously differentiable as a function of @xmath126 , and this derivative is also continuous for @xmath43 .",
    "[ gofreg : assump : a5n ] 2 .   under @xmath113",
    ", there exists an @xmath127-consistent estimator @xmath118 of @xmath119 , _",
    "i.e. _ @xmath128 and such that , under @xmath129 , @xmath130 for a certain @xmath131 .",
    "[ gofreg : assump : a6n ]    [ gofreg : theo : limdis ] under conditions [ gofreg : assump : a1][gofreg : assump : a6n ] and under the null hypothesis @xmath132 ( that is , @xmath133 ) , @xmath134 where @xmath135}}$ ] is the conditional variance under @xmath113 and @xmath136    it should be noted that the convergence rate as well as the asymptotic bias and variance agree with the results in the multivariate setting given by @xcite and @xcite , except for the cancellation of the design density effects in bias and variance , achieved by the inclusion of @xmath122 in the test statistic .",
    "the use of a local estimator with @xmath63 or @xmath64 ( local constant or local linear ) does not affect the limiting distribution , given that the equivalent kernel is the same ( as stated in corollary [ gofreg : coro : equiv ] ) .",
    "finally , the general complex structure of the asymptotic variance ( see also @xcite and @xcite for the density context ) turns much simpler if a von mises kernel ( see subsection [ gofreg : subsec : prop ] ) is used : @xmath137 the contribution of this kernel to the asymptotic bias is @xmath138 .",
    "+ the power of the proposed test statistic is also investigated for a family of local alternatives that is asymptotically close to @xmath113 .",
    "denote these local alternatives by @xmath139 : @xmath140 where @xmath141 , @xmath142 and @xmath143 is a positive sequence such that @xmath144 , for example @xmath145 . with this notation ,",
    "@xmath139 becomes @xmath113 when @xmath146 is such that @xmath147 ( @xmath148 , for example ) and @xmath129 when the previous statement does not hold for a set of positive probability .",
    "the following conditions are required for deriving the limiting distribution of @xmath124 under @xmath139 :    1 .",
    "the function @xmath146 is continuous .",
    "[ gofreg : assump : a7n ] 2 .   under @xmath139",
    ", the @xmath127-consistent estimator @xmath149 also satisfies @xmath150 .",
    "[ gofreg : assump : a8n ]    [ gofreg : theo : power ] under conditions [ gofreg : assump : a1][gofreg : assump : a5n ] , [ gofreg : assump : a7n][gofreg : assump : a8n ] and under the hypothesis @xmath139 , @xmath151    the critical sequence @xmath143 , which is of order @xmath152 , reflects the asymptotic power that is bounded away from @xmath153 ( first case ) and the level of the test ( third case ) . in this case",
    ", the effect of @xmath146 in the limiting distribution of the test statistic appears in the asymptotic bias and the test asymptotically detects all kinds of local alternatives from @xmath113 whose component @xmath146 has a positive squared weighted norm .",
    "+ to illustrate the effective convergence of the statistic to the asymptotic distribution , a simple numerical experiment is provided .",
    "the regression setting is the model @xmath154 , with @xmath155 , @xmath156 , @xmath157 and @xmath158 uniformly distributed on the circle ( @xmath86 ) .",
    "the composite hypothesis @xmath159 , for @xmath160 unknown ( test for no effect ) , is checked using the local constant estimator ( @xmath63 ) with von mises kernel and considering the weight function @xmath161 .",
    "figure [ gofreg : fig : asymp ] presents two qq - plots computed from samples @xmath162 obtained for different sample sizes @xmath163 .",
    "two bandwidth sequences @xmath164 , @xmath165 are chosen to illustrate the effect of the bandwidths in the convergence to the asymptotic distribution , and , specifically , that the effect of undersmoothing boosts the convergence since the bias is mitigated .",
    "the kolmogorov - smirnov ( k - s ) and shapiro - wilk ( s - w ) tests are applied on to measure how close the empirical distribution of the test statistic is to a @xmath166 and to normality , respectively .      as it usually happens in smoothed tests ( see @xcite or @xcite ) ,",
    "the asymptotic distribution can not be used to calibrate the test statistic for small or moderate sample sizes due to the slow convergence rate and due to the presence of unknown quantities depending on the design density and the error structure . in this situation",
    ", bootstrapcalibration is an alternative .",
    "+ the main idea is to approximate the distribution of @xmath124 under @xmath113 by one of its bootstrapped version @xmath167 , which can be arbitrarily well approximated by monte carlo by generating bootstrap samples @xmath168 . under @xmath113 , the bootstrap responses are obtained from the parametric fit and bootstrap errors that imitate the conditional variance by a wild bootstrap procedure : @xmath169 , where @xmath170 and the variables @xmath171 are independent from the observed sample and iid with @xmath172}}=0 $ ] , @xmath173}}=1 $ ] and finite third and fourth moments .",
    "a common choice is considering a binary variable with probabilities @xmath174 and @xmath175 , which corresponds to the _ golden section _ bootstrap .",
    "the bootstrap test statistic is @xmath176 where @xmath177 and @xmath178 are the analogues of @xmath120 and @xmath118 , respectively , obtained from the bootstrapped sample @xmath168 .     with the sample quantiles for @xmath162 with @xmath179 ( left ) and @xmath180 ( right).,title=\"fig:\",scaledwidth=50.0% ] with the sample quantiles for @xmath162 with @xmath179 ( left ) and @xmath180 ( right).,title=\"fig:\",scaledwidth=50.0% ]",
    "the testing procedure for calibrating the test is summarized in the next algorithm , stated for the composite hypothesis . if the simple hypothesis is considered , then set @xmath181 .",
    "[ gofreg : algo : boot ] consider @xmath182 a random sample from model ( [ gofreg : model ] ) . to test @xmath183 , set a bandwidth @xmath184 and a weight function @xmath185 and proceed as follows :    1 .",
    "compute @xmath186 and obtain the fitted residuals @xmath187 , @xmath188.[gofreg : algo : boot:1 ] 2 .",
    "compute @xmath189.[gofreg : algo : boot:2 ] 3 .   _ bootstrap resampling_. for @xmath190:[gofreg : algo : boot:3 ] 1 .   obtain a bootstrap random sample @xmath168 , where @xmath191 and @xmath192 are iid golden section binary variables , @xmath188.[gofreg : algo : boot : a ] 2 .",
    "compute @xmath193 as in [ gofreg : algo : boot:1 ] , but now from the bootstrap sample from [ gofreg : algo : boot : a].[gofreg : algo : boot : b ] 3 .",
    "compute @xmath194.[gofreg : algo : boot : c ] 4 .   approximate the @xmath195-value by @xmath196.[gofreg : algo : boot:4 ]",
    "bootstrap strategies may be computational expensive . in this case",
    ", it should be noted that the test statistic can be written as @xmath197 , using the equivalent kernel notation .",
    "the bootstrap test statistic @xmath167 is the same , just taking @xmath198 instead of @xmath199 , so there is no need to recompute the other elements of @xmath124 in the bootstrap .",
    "+ in order to prove the consistency of the resampling mechanism detailed in algorithm [ gofreg : algo : boot ] , that is , that the bootstrapped statistic @xmath167 has the same asymptotic distribution as the original statistic @xmath124 , a bootstrap analogue of assumption [ gofreg : assump : a6n ] is required :    1 .",
    "the estimator @xmath178 computed from @xmath168 is such that @xmath200 , where @xmath201 is the probability law conditional on @xmath182 .",
    "[ gofreg : assump : a9n ]    based on this assumption and on the previous ones , it is proved from theorem [ gofreg : theo : limdis ] that the probability distribution function ( pdf ) of @xmath167 conditional on the sample converges always in probability to a gaussian pdf , which is the same asymptotic pdf of @xmath124 if @xmath113 holds .",
    "[ gofreg : theo : boot ] under conditions [ gofreg : assump : a1][gofreg : assump : a6n ] and [ gofreg : assump : a9n ] and conditionally on @xmath182 , @xmath202 in probability .",
    "if the null hypothesis holds , then @xmath203 .",
    "the finite sample performance of the goodness - of - fit test is explored in four regression models , considering different sample sizes , dimensions and bandwidths . given the regression model ( [ gofreg : model ] ) and",
    "taking @xmath124 as test statistic , the following components must be specified : the density of the predictor @xmath10 , the regression function @xmath22 , the noise @xmath204 and the deviations from @xmath113 .    ,",
    "title=\"fig:\",scaledwidth=22.5% ] , title=\"fig:\",scaledwidth=22.5% ] , title=\"fig:\",scaledwidth=22.5% ] , title=\"fig:\",scaledwidth=22.5% ] + , title=\"fig:\",scaledwidth=22.5% ] , title=\"fig:\",scaledwidth=22.5% ] , title=\"fig:\",scaledwidth=22.5% ] , title=\"fig:\",scaledwidth=22.5% ]    the parametric regression functions with directional covariate and scalar response are shown in figure [ gofreg : fig : models ] , with the following codification : the radius from the origin represents the response @xmath52 for a @xmath35 direction , resulting in a distortion from a perfect circle or sphere .",
    "the noise considered is @xmath205 , with two different conditional standard deviations given by @xmath206 ( homocedastic , hom . ) and @xmath207 ( heteroskedastic , het . ) , with @xmath208 being the density of the m16 model in @xcite . in order to define the design densities ,",
    "some models introduced by @xcite have been considered : m1 ( uniform ) , m4 , m12 and m20 are used as single densities or as part of mixture distributions , as in s2 and s3 ( see table [ gofreg : tab : mods ] ) .",
    "the alternative hypothesis @xmath129 is obtained by adding the deviations @xmath209 and @xmath210 to the true regression function @xmath211 .",
    "the different combinations considered in s1 to s4 are given in table [ gofreg : tab : mods ] ( see supplementary material also for further details ) .",
    ".specification of simulation scenarios for model ( [ gofreg : model]).[gofreg : tab : mods ] [ cols=\"^,^,^,^,^,^\",options=\"header \" , ]     in order to construct a plausible linear model , a preliminary variable selection was performed using lasso regression with ( tuning ) parameter @xmath212 selected by an overpenalized _ three _ standard error rule ( see @xcite ) . after removing some extra variables by using a backward stepwise method with bic",
    ", we obtain a fitted vector @xmath213 with @xmath214 non - zero entries .",
    "the test is applied to check the null hypothesis of a candidate linear model with coefficient @xmath215 constrained to be zero except in these previously selected @xmath73 words , that is @xmath216 , with @xmath215 subject to @xmath217 for an adequate choice of the matrix @xmath218 .",
    "the significance trace in figure [ gofreg : fig : text ] shows no evidence to reject the linear model for a wide grid of bandwidths , using a local constant approach ( local linear was not implemented due to its higher cost and computational limitations ) .",
    "table [ gofreg : tab : model ] shows the fitted linear model under the null hypothesis . as it can be seen , news where stemmed words like `` kill '' , `` climat '' , `` polit '' appear have a strong positive impact on the number of comments , since these news are likely more controversial and generate broader discussions . on the other hand , scientific related words like `` mission '' ,",
    "`` abstract '' or `` lab '' have a negative impact , since they tend to raise more objective and higher specific discussions .",
    "we would like to thank professors david e. losada , from university of santiago de compostela , for his guidance in the data application and irne gijbels , from catholic university of leuven , for her useful theoretical comments .",
    "this research was supported by project mtm200803010 from the spanish ministry of science and innovation , by project 10mds207015pr from direccin xeral de i+d of the xunta de galicia , by iap research network grant nr .",
    "p7/06 of the belgian government ( belgian science policy ) , by the european research council under the european community s seventh framework programme ( fp7/20072013 ) / erc grant agreement no",
    ". 203650 , and by the contract `` projet dactions de recherche concertes '' ( arc ) 11/16039 of the `` communaut franaise de belgique '' ( granted by the `` acadmie universitaire louvain '' ) .",
    "work of e. garca - portugus has been supported by a grant from fundacin barri and fpu grant ap20100957 from the spanish ministry of education .",
    "authors gratefully acknowledge the computational resources used at the svg cluster of the cesga supercomputing center .",
    "three extra appendices are included as supplementary material , containing particular cases of the projected local estimator , the technical lemmas and further results for the simulation study .",
    "the proof is divided in three sections : the conditional bias is first obtained for @xmath64 , then the result for @xmath63 follows by restricting the computations to the first column of @xmath69 and the variance is proved to be common to both estimators .",
    "+ _ bias of @xmath219_. working conditionally , by ( [ gofreg : model ] ) and ( [ gofreg : betaest ] ) , @xmath220}}={\\mathbf{e}}_1^t{\\left({\\boldsymbol{\\mathcal{x}}_{{\\mathbf{x}},p}}^t{\\boldsymbol{\\mathcal{w}}_{{\\mathbf{x}}}}{\\boldsymbol{\\mathcal{x}}_{{\\mathbf{x}},p}}\\right)}^{-1}{\\boldsymbol{\\mathcal{x}}_{{\\mathbf{x}},p}}^t{\\boldsymbol{\\mathcal{w}}_{{\\mathbf{x}}}}{\\mathbf{m}},\\label{gofreg : theo : biasvar:0 }      \\end{aligned}\\ ] ] where @xmath221 . the proof is based on theorem 2.1 in @xcite but adapted to the projected local estimator .",
    "first of all , consider the taylor expansion of @xmath222 of second order around the point @xmath43 , which follows naturally by extending the one given in section [ gofreg : sec : reg ] ( since @xmath223 , where @xmath224 is the hessian of @xmath22 ) : @xmath225 the taylor expansion can be expressed componentwise ( also for the orders ) as @xmath226 with @xmath227 the vector with @xmath228-th entry given by @xmath229 and remainder term of order @xmath230 , uniform in @xmath43 since the third derivative of @xmath22 is bounded by assumption [ gofreg : assump : a1 ] .",
    "then , by ( [ gofreg : theo : biasvar:0 ] ) , the first term in the taylor expansion is @xmath231 which for @xmath64 equals @xmath52 and hence the conditional bias is given by @xmath232 times the remaining vector . by using the results [ gofreg : lem:1:d0 ] , [ gofreg : lem:1:d1b ] and [ gofreg : lem:1:d2b ] of lemma [ gofreg : lem:1 ] , it follows that , componentwise , @xmath233 this matrix can be inverted by the inversion formula of a block matrix , resulting in @xmath234 now the quadratic term of the taylor expansion yields by results [ gofreg : lem:1:d2h][gofreg : lem:1:d3 ] of lemma [ gofreg : lem:1 ] : @xmath235}f({\\mathbf{x}})h^2+{\\mathpzc{o}_\\mathbb{p}\\lph^2{\\right ) } } \\\\          { \\mathpzc{o}_\\mathbb{p}\\lph^3{\\mathbf{1}}{\\right ) } }          \\end{array}\\right)}.\\label{gofreg : theo : biasvar:5 }      \\end{aligned}\\ ] ] finally , the remaining order is @xmath236 , because @xmath237 by setting @xmath238 and using [ gofreg : lem:1:d2h][gofreg : lem:1:d3 ] from lemma [ gofreg : lem:1 ] . joining ( [ gofreg : theo : biasvar:4 ] ) and ( [ gofreg : theo : biasvar:5 ] ) , then @xmath239}}=\\frac{b_q(l)}{q}{\\mathrm{tr}\\left[{\\boldsymbol{\\mathcal{h}}}_m({\\mathbf{x}})\\right]}h^2+{\\mathpzc{o}_\\mathbb{p}\\lph^2{\\right)}}.\\ ] ]    _ bias of @xmath240_. for the case @xmath63 the product in ( [ gofreg : theo : biasvar:1 ] ) is not @xmath52 but slightly different . by ( [ gofreg : theo : biasvar:4 ] ) , @xmath241 and also by ( [ gofreg : theo : biasvar:3 ] ) and [ gofreg : lem:1:d0][gofreg : lem:1:d1b ] in lemma [ gofreg : lem:1 ] , @xmath242",
    "then , ( [ gofreg : theo : biasvar:1 ] ) turns into @xmath243 because the coefficient in @xmath52 is _ exactly _ one . adding this to the bias of @xmath219 , the result follows since the contribution of the linear part in ( [ gofreg : theo : biasvar:5 ] ) and in the remaining order is negligible .",
    "+ _ variance of @xmath120_. by the variance property for linear combinations , @xmath244}}={\\mathbf{e}}_1^t{\\left({\\boldsymbol{\\mathcal{x}}_{{\\mathbf{x}},p}}^t{\\boldsymbol{\\mathcal{w}}_{{\\mathbf{x}}}}{\\boldsymbol{\\mathcal{x}}_{{\\mathbf{x}},p}}\\right)}^{-1}{\\boldsymbol{\\mathcal{x}}_{{\\mathbf{x}},p}}^t{\\boldsymbol{\\mathcal{w}}_{{\\mathbf{x}}}}{\\mathbf{v}}{\\boldsymbol{\\mathcal{w}}_{{\\mathbf{x}}}}{\\boldsymbol{\\mathcal{x}}_{{\\mathbf{x}},p}}{\\left({\\boldsymbol{\\mathcal{x}}_{{\\mathbf{x}},p}}^t{\\boldsymbol{\\mathcal{w}}_{{\\mathbf{x}}}}{\\boldsymbol{\\mathcal{x}}_{{\\mathbf{x}},p}}\\right)}^{-1}{\\mathbf{e}}_{1},\\ ] ] where @xmath245 . by results [ gofreg : lem:1:v1][gofreg : lem:1:v3 ] of lemma [ gofreg : lem:1 ] , @xmath246 therefore , by ( [ gofreg : theo : biasvar:4 ] ) and ( [ gofreg : theo : biasvar:6 ] ) , the common variance expression follows .",
    "note that @xmath247 @xmath248",
    ". then , by expression ( [ gofreg : theo : biasvar:4 ] ) , uniformly in @xmath97 it follows that @xmath249 by ( [ gofreg : kde ] ) and ( [ gofreg : chq ] ) , the first addend is @xmath250 .",
    "the second term is @xmath251 ( see [ gofreg : lem:1:d1c ] in lemma [ gofreg : lem:1 ] ) and negligible in comparison with the first one , which is @xmath252 .",
    "then , it can be absorbed inside the factor @xmath253 , proving the corollary .    for a fixed @xmath43 ,",
    "the next decomposition is studied : @xmath254}}\\right)}\\\\      & + \\sqrt{nh^q}{\\left({\\mathbb{e}{\\left[}\\hat m_{h , p}({\\mathbf{x}})|{\\mathbf{x}}_1,\\ldots,{\\mathbf{x}}_n{\\right]}}-m({\\mathbf{x}})\\right)}\\\\      = & \\,n_1+n_2 .",
    "\\end{aligned}\\ ] ] _ term @xmath255_. from the proof of theorem [ gofreg : theo : biasvar ] , @xmath256 . by the cramr - wold device ,",
    "if @xmath257 is asymptotically normal for any @xmath258 , then @xmath259 is also asymptotically normal . to obtain the asymptotic normality of @xmath260 the lyapunov s central limit theorem ( clt ) for triangular arrays @xmath261",
    "is employed , this is : if for @xmath108 @xmath262}}^{1+\\frac{\\delta}{2}}\\right)}^{-1}{\\mathbb{e}{\\left[}{\\left| v_n-{\\mathbb{e}{\\left[}v_n{\\right]}}\\right|}^{2+\\delta}{\\right]}}=0,\\ ] ] where @xmath263 , then @xmath264}}}{\\sqrt{{\\mathbb{v}\\mathrm{ar}{\\left[}v_n{\\right]}}}}\\stackrel{d}{\\longrightarrow}\\mathcal{n}(0,1)$ ] . from @xmath265}}\\right|}^{2+\\delta}\\big]=\\mathcal{o}\\big(\\mathbb{e}\\big[{\\left| v_n\\right|}^{2+\\delta}\\big]\\big)$ ] and",
    "the use of lemma [ gofreg : lem:4 ] , it holds that : @xmath266 } }          = & \\,\\frac{c_{h , q}(l)^{2+\\delta}}{c_{h , q}(l^{2+\\delta})}{\\int_{{\\mathbb{r } } } { \\left((y - m({\\mathbf{x}})){\\mathbf{a}}^t{\\left(1,\\delta_{p,1}{\\mathbf{b}}_{{\\mathbf{x}}}^t({\\mathbf{x}}-{\\mathbf{x}})\\right ) } \\right)}^{2+\\delta } f_{{\\mathbf{x}},y}({\\mathbf{x}},y)\\,d y}{\\left(1+{\\mathpzc{o}{\\left(}1{\\right)}}\\right)}\\\\      = & \\,\\frac{\\lambda_q(l^{2+\\delta})a_1^{2+\\delta}}{\\lambda_q(l)^{2+\\delta}h^{(1+\\delta)q}}f({\\mathbf{x}}){\\mathbb{e}{\\left[}(y - m({\\mathbf{x}}))^{2+\\delta}|{\\mathbf{x}}={\\mathbf{x}}{\\right]}}{\\left(1+{\\mathpzc{o}{\\left(}1{\\right)}}\\right)}\\\\      = & \\,{\\mathcal{o}\\lph^{-(1+\\delta)q}{\\right)}}.      \\end{aligned}\\ ] ] note that @xmath107<\\infty$ ] is required for a @xmath108 and that by [ gofreg : assump : a3 ] the kernel @xmath267 plays the same role as @xmath13 . by using this result with @xmath268",
    ", it follows that @xmath269}}\\leq{\\mathbb{e}{\\left[}v_n^2{\\right]}}={\\mathcal{o}\\lph^{-q}{\\right)}}$ ] .",
    "therefore , @xmath270}}\\right|}^{2+\\delta}{\\right]}}}{n^\\frac{\\delta}{2}{\\mathbb{v}\\mathrm{ar}{\\left[}v_n{\\right]}}^{1+\\frac{\\delta}{2}}}={\\mathcal{o}{\\left(}\\frac{h^{-(1+\\delta)q}}{n^\\frac{\\delta}{2}h^{-(1+\\frac{\\delta}{2})q}}{\\right)}}={\\mathcal{o}{\\left(}{\\left(nh^q\\right)}^{-\\frac{\\delta}{2}}{\\right)}},\\ ] ] so by [ gofreg : assump : a4 ] and the cramr - wold device @xmath271 , with the covariance matrix arising from ( [ gofreg : theo : biasvar:6 ] ) : @xmath272 on the other hand , by ( [ gofreg : theo : biasvar:4 ] ) , @xmath273 converges in probability to @xmath274 if @xmath64 and to @xmath275 if @xmath63 .",
    "the desired result then follows by the use of slutsky s theorem : @xmath276 _ term @xmath277_. by the conditional bias expansion of theorem [ gofreg : theo : biasvar ] @xmath277 converges in probability as @xmath278 so adding this bias to @xmath255 the asymptotic normality is proved by slutsky s theorem .",
    "both theorems are proved at the same time by assuming that @xmath139 holds and considering @xmath113 a particular case with @xmath148 .",
    "the proof follows the steps of @xcite and @xcite and makes use of the equivalent kernel representation for simplifying the computations and applying @xcite s clt .",
    "the test statistic @xmath124 can be separated into three addends by adding and subtracting the true smoothed regression function : @xmath279 where , thanks to result [ gofreg : lem:1:d0 ] from lemma [ gofreg : lem:1 ] , the addends are : @xmath280 by slutsky s theorem , the asymptotic distribution of @xmath281 will be the one of @xmath282 , so the proof is divided in the examination of each addend . + _",
    "terms @xmath283 and @xmath284 .",
    "_ by a taylor expansion on @xmath285 as a function of @xmath286 ( see [ gofreg : assump : a5n ] ) , @xmath287 with @xmath288 .",
    "the second equality holds by the boundedness of @xmath289 for @xmath290 , where the last holds by [ gofreg : assump : a6n ] and [ gofreg : assump : a8n ] . on the other hand , @xmath291 because of the previous considerations used and [ gofreg : lem:5:4 ] from lemma [ gofreg : lem:5 ] . as a consequence , @xmath292 and @xmath293 ,",
    "so by [ gofreg : assump : a3 ] it happens that @xmath294    _ term @xmath295 .",
    "_ now , @xmath295 can be dealt with the equivalent kernel of corollary [ gofreg : coro : equiv ] : @xmath296 using again slutsky s theorem , the asymptotic distribution of @xmath295 , and hence of @xmath124 , will be the one of @xmath297 .",
    "now it is possible to split @xmath298 by recalling that @xmath299 by the model definition ( [ gofreg : model ] ) and hypothesis @xmath139 .",
    "specifically , under @xmath139 the conditional variance can be expressed as @xmath300=\\sigma_{\\boldsymbol{\\theta}_0}^2(\\mathbf{x})(1+\\mathpzc{o}\\left(1\\right))$ ] , uniformly in @xmath301 since @xmath146 and @xmath302 are continuous and bounded by [ gofreg : assump : a2 ] and [ gofreg : assump : a7n ] .",
    "therefore , @xmath303 by results [ gofreg : lem:5:1 ] and [ gofreg : lem:5:2 ] of lemma [ gofreg : lem:5 ] , the behavior of the two last terms is @xmath304 if @xmath305 , then @xmath306 , making the asymptotic distribution degenerate . if @xmath307 , then @xmath308 . for these reasons , we assume that @xmath145 on the rest of the proof . for the first addend , consider now @xmath309 from result [ gofreg : lem:5:3 ] of lemma [ gofreg : lem:5 ] and because @xmath310 uniformly , @xmath311    the asymptotic behavior of @xmath312 is obtained using theorem 2.1 in @xcite .",
    "this result states that the sum @xmath313 , with @xmath314 random variables depending on the sample size and on independent variables @xmath315 and @xmath316 , converges as @xmath317 under the following conditions :    1 .   the random variables @xmath314 are _ clean _ , _ i.e. _ @xmath318=0 $ ] for @xmath319,[gofreg : dj:1 ] 2 .",
    "@xmath320\\to v^2$],[gofreg : dj:2 ] 3 .",
    "@xmath321\\right)v^{-2}\\to 0$],[gofreg : dj:3 ] 4 .   @xmath322v^{-4}\\to3 $ ] .",
    "[ gofreg : dj:4 ]    in order to apply this result , let us denote first @xmath323 then , @xmath324 and the random variables on which @xmath314 depends are @xmath325 and @xmath326 .",
    "condition [ gofreg : dj:1 ] is easily seen to hold by @xmath27=0 $ ] and the tower property , which implies that @xmath327=0 $ ] .",
    "because of this , the fact that @xmath328 and lemma 2.1 in @xcite , we have for condition [ gofreg : dj:2 ] : @xmath329=\\mathbb{e}\\bigg[\\big(\\sum_{i\\neq j } w_{ijn}\\big)^2\\bigg]=2\\mathbb{e}\\bigg[\\sum_{i\\neq j } w_{ijn}^2\\bigg]=2n(n-1)\\mathbb{e}\\left[w_{ijn}^2\\right].\\label{gofreg : vwn }      \\end{aligned}\\ ] ] then , by [ gofreg : lem:5:5 ] in lemma [ gofreg : lem:5 ] and the fact that @xmath330 , @xmath331=n^{-2}\\nu_{\\boldsymbol{\\theta}_0}^2\\left(1+\\mathpzc{o}\\left(1\\right)\\right)$ ] and as a consequence @xmath320\\to 2\\nu_{\\boldsymbol{\\theta}_0}^2 $ ] .",
    "condition [ gofreg : dj:3 ] follows easily from the previous computation : @xmath332\\bigg)v^{-2}\\leq\\left(\\max_{1\\leq i\\leq n}n^{-1}\\nu_{\\boldsymbol{\\theta}_0}^2\\left(1+\\mathpzc{o}\\left(1\\right)\\right)\\right)(2\\nu_{\\boldsymbol{\\theta}_0}^{2})^{-1}=(2n)^{-1}(1+\\mathpzc{o}\\left(1\\right))\\to0.\\ ] ] to check condition [ gofreg : dj:4 ] , note that @xmath322 $ ] can be split in the following form in virtue of lemma 2.1 in @xcite , as @xcite stated : @xmath333=&\\,\\mathbb{e}\\bigg[\\sum_{i_1\\neq j_1}\\sum_{i_2\\neq j_2}\\sum_{i_3\\neq j_3}\\sum_{i_4\\neq j_4 } w_{i_1j_1n}w_{i_2j_2n}w_{i_3j_3n}w_{i_4j_4n}\\bigg]\\nonumber\\\\      = & \\,8\\sum_{i , j}\\!^{\\neq}\\,\\mathbb{e}\\left[w_{ijn}^4\\right]+12\\sum_{i , j , k , l}\\!^{\\neq}\\,\\mathbb{e}\\left[w_{ijn}^2w_{kln}^2\\right]+48\\sum_{i , j , k}\\!^{\\neq}\\,\\mathbb{e}\\left[w_{ijn}w_{ikn}^2w_{jkn}\\right]\\nonumber\\\\      & + 192\\sum_{i , j , k , l}\\!^{\\neq}\\,\\mathbb{e}\\left[w_{ijn}w_{jkn}w_{kln}w_{lin}\\right],\\label{gofreg : ewn4 }      \\end{aligned}\\ ] ] where the notation @xmath334 stands for the summation over all _ pairwise different _ indexes ( _ i.e. _ , indexes that satisfy @xmath335 for their associated @xmath314 ) . by the results given in [ gofreg : lem:5:5 ] of lemma [ gofreg : lem:5 ] , @xmath336=\\mathcal{o}\\left((n^4h^{q})^{-1}\\right)$ ] , @xmath337=\\mathcal{o}\\left(n^{-4}h^{2q}\\right)$ ] and @xmath338=\\mathcal{o}\\left(n^{-4}\\right)$ ] .",
    "therefore , by ( [ gofreg : vwn ] ) and ( [ gofreg : ewn4 ] ) , @xmath339=12\\sum_{i\\neq j}\\sum_{k\\neq l}\\mathbb{e}\\left[w_{ijn}^2w_{kln}^2\\right]+\\mathpzc{o}\\left(1\\right)=3\\big(2\\sum_{i\\neq j}\\mathbb{e}\\left[w_{ijn}^2\\right]\\big)^2+\\mathpzc{o}\\left(1\\right)=3\\mathbb{v}\\mathrm{ar}\\left[w_n\\right]^2+\\mathpzc{o}\\left(1\\right)\\ ] ] and by [ gofreg : assump : a4 ] , @xmath322=3\\mathbb{v}\\mathrm{ar}\\left[w_n\\right]^2+\\mathpzc{o}\\left(1\\right)$ ] , so condition [ gofreg : dj:4 ] is satisfied , having that @xmath340 finally , using decompositions ( [ gofreg : theo : limdis:1 ] ) and ( [ gofreg : theo : limdis:4 ] ) and results ( [ gofreg : theo : limdis:3b ] ) and ( [ gofreg : theo : limdis:5 ] ) , it holds @xmath341(1+\\mathpzc{o}_\\mathbb{p}\\left(1\\right))+t_{n,2}+2t_{n,3}\\right)\\\\      = & \\,\\bigg(\\frac{\\lambda_q(l^2)\\lambda_q(l)^{-2}}{h^\\frac{q}{2}}\\int_{\\omega_q}\\sigma_{\\boldsymbol{\\theta}_0}^2(\\mathbf{x})w(\\mathbf{x})\\,\\omega_q(d\\mathbf{x})+nh^\\frac{q}{2}\\widetilde{t}_{n,1}^{(1b)}\\\\      & + \\int_{\\omega_q}g(\\mathbf{x})^2 f(\\mathbf{x})w(\\mathbf{x})\\,\\omega_q(d\\mathbf{x})\\bigg)(1+\\mathpzc{o}_\\mathbb{p}\\left(1\\right ) )      \\end{aligned}\\ ] ] and the limit distribution follows by slutsky s theorem and result ( [ gofreg : theo : limdis:7 ] ) .",
    "the proof mimics the steps of the proof of theorem [ gofreg : theo : limdis ] .",
    "first of all , the bootstrap test statistic @xmath167 can be separated as @xmath342 where : @xmath343    _ terms @xmath344 and @xmath345_. by assumption [ gofreg : assump : a9n ] and analogous computations to the ones in the proof of theorem [ gofreg : theo : limdis ] , it follows that @xmath346 and @xmath347 , where the convergence is stated in the probability law @xmath201 that is conditional on the sample @xmath348 .",
    "term @xmath349_. by the resampling procedure of algorithm [ gofreg : algo : boot ] , @xmath350 and the dominant term can be split into @xmath351 from result [ gofreg : lem:6:1 ] of lemma [ gofreg : lem:6 ] , the first term is @xmath352 so the dominant term is @xmath353 , whose asymptotic behavior is obtained using theorem 2.1 in @xcite conditionally on the sample .",
    "let us denote now @xmath354 then , @xmath355 and the random variables on which @xmath356 depends are now @xmath192 and @xmath357 .",
    "condition [ gofreg : dj:1 ] of the theorem follows immediately by the properties of the @xmath192 s : @xmath358=0 $ ] . on the other hand , analogously to ( [ gofreg : vwn ] ) , @xmath359}=\\!2\\sum_{i\\neq j}\\mathbb{e}^*{\\left[w_{ijn}^{*2}\\right]}\\!=2n^2h^q\\sum_{i\\neq j } \\!{\\left[{\\int_{\\omega_{q } } \\!\\ ! w_n^p{\\left({\\mathbf{x}},{\\mathbf{x}}_i\\right)}w_n^p{\\left({\\mathbf{x}},{\\mathbf{x}}_j\\right)}\\hat\\varepsilon_i\\hat\\varepsilon_j",
    "\\hat f_h({\\mathbf{x}})w({\\mathbf{x}})\\,\\omega_{q}(d { \\mathbf{x}})}\\right]}^2\\ ] ] and by result [ gofreg : lem:6:2 ] of lemma [ gofreg : lem:6 ] , @xmath360}\\stackrel{p}{\\longrightarrow } 2\\nu_{{\\boldsymbol\\theta}_1}^2 $ ] , resulting in the verification of condition [ gofreg : dj:3 ] in probability .",
    "condition [ gofreg : dj:4 ] is checked using the same decomposition for @xmath361 $ ] and the results collected in [ gofreg : lem:6:2 ] of lemma [ gofreg : lem:6 ] .",
    "hence @xmath362=3\\mathbb{v}\\mathrm{ar}^*{\\left[w_n^*\\right]}^2+{\\mathpzc{o}_\\mathbb{p}{\\left(}1{\\right)}}$ ] and [ gofreg : dj:4 ] is satisfied in probability , from which it follows that , conditionally on @xmath348 the pdf of @xmath363 converges in probability to the pdf of @xmath364 , that is : @xmath365 joining ( [ gofreg : theo : boot:1 ] ) and ( [ gofreg : theo : boot:3 ] ) and applying slutsky s theorem conditionally on the sample , the theorem is proved : @xmath366    alcal , j.  t. , cristbal , j.  a. , and gonzlez - manteiga , w. ( 1999 ) . goodness - of - fit test for linear models based on local polynomials .",
    ", 42(1):3946 .",
    "asur , s. and huberman , b.  a. ( 2010 ) . predicting the future with social media . in huang , j.  x. , king , i. , raghavan , v. , and rueger , s. , editors ,",
    "_ proceedings of the 2010 ieee / wic / acm international conference on web intelligence and intelligent agent technology _ , pages 492499 .",
    "bai , z.  d. , rao , c.  r. , and zhao , l.  c. ( 1988 ) .",
    "kernel estimators of density function of directional data .",
    ", 27(1):2439 .",
    "banerjee , a. , dhillon , i.  s. , ghosh , j. , and sra , s. ( 2005 ) .",
    "clustering on the unit hypersphere using von mises - fisher distributions .",
    ", 6:13451382 .",
    "boente , g. , rodrguez , d. , and gonzlez - manteiga , w. ( 2013 ) . goodness - of - fit test for directional data .",
    ", 41(1):259275 .",
    "bowman , a.  w. and azzalini , a. ( 1997 ) . .",
    "oxford statistical science series .",
    "clarendon press , oxford .",
    "buchta , c. , kober , m. , feinerer , i. , and hornik , k. ( 2012 ) .",
    "spherical k - means clustering .",
    ", 50(10):122 .",
    "cheng , m .- y . and wu , h",
    "local linear regression on manifolds and its geometric interpretation .",
    ", 108(504):14211434 .",
    "de  jong , p. ( 1987 ) .",
    "a central limit theorem for generalized quadratic forms .",
    ", 75(2):261277 .",
    "deschepper , e. , thas , o. , and ottoy , j.  p. ( 2008 ) .",
    "tests and diagnostic plots for detecting lack - of - fit for circular - linear regression models . , 64(3):912920 .",
    "di  marzio , m. , panzera , a. , and taylor , c.  c. ( 2009 ) . local polynomial regression for circular predictors . ,",
    "79(19):20662075 .",
    "di  marzio , m. , panzera , a. , and taylor , c.  c. ( 2014 ) .",
    "nonparametric regression for spherical data .",
    ".    fan , j. and gijbels , i. ( 1996 ) .",
    ", volume  66 of _ monographs on statistics and applied probability_. chapman & hall , london .",
    "fan , j. , gijbels , i. , hu , t .- c . , and huang , l .- s .",
    "( 1996 ) . a study of variable bandwidth selection for local polynomial regression . ,",
    "6(1):113127 .",
    "garca - portugus , e. ( 2013 ) .",
    "exact risk improvement of bandwidth selectors for kernel density estimation with directional data .",
    ", 7:16551685 .",
    "garca - portugus , e. , crujeiras , r.  m. , and gonzlez - manteiga , w. ( 2013 ) .",
    "kernel density estimation for directional - linear data .",
    ", 121:152175 .",
    "garca - portugus , e. , crujeiras , r.  m. , and gonzlez - manteiga , w. ( 2014 ) .",
    "central limit theorems for directional and linear data with applications . , to appear .",
    "hall , p. , watson , g.  s. , and cabrera , j. ( 1987 ) .",
    "kernel density estimation with spherical data .",
    ", 74(4):751762 .",
    "hrdle , w. and mammen , e. ( 1993 ) . comparing nonparametric versus parametric regression fits .",
    ", 21(4):19261947 .",
    "hastie , t. , tibshirani , r. , and friedman , j. ( 2009 ) . .",
    "springer series in statistics .",
    "springer , new york , second edition .",
    "jennrich , r.  i. ( 1969 ) .",
    "asymptotic properties of non - linear least squares estimators . , 40(2):633643 .",
    "joachims , t. ( 2002 ) . , volume 668 of _ kluwer international series in engineering and computer science_. kluwer academic publishers , boston .",
    "mardia , k.  v. and jupp , p.  e. ( 2000 ) . .",
    "wiley series in probability and statistics .",
    "john wiley & sons , chichester , second edition .",
    "meyer , d. , hornik , k. , and feinerer , i. ( 2008 ) .",
    "text mining infrastructure in r. , 25(5):154 .",
    "ruppert , d. and wand , m.  p. ( 1994 ) .",
    "multivariate locally weighted least squares regression .",
    ", 22(3):13461370 .",
    "srivastava , a.  n. and sahami , m. , editors ( 2009 ) . .",
    "chapman & hall / crc data mining and knowledge discovery series .",
    "crc press , boca raton .",
    "surian , d. and chawla , s. ( 2013 ) .",
    "mining outlier participants : insights using directional distributions in latent models . in blockeel , h. ,",
    "kersting , k. , nijssen , s. , and ztelezn , f. , editors , _ machine learning and knowledge discovery in databases _ , volume 8188 of _ lecture notes in artificial intelligence _ ,",
    "pages 337352 .",
    "springer , heidelberg .",
    "tatar , a. , antoniadis , p. , de  amorim , m.  d. , and fdida , s. ( 2012 ) .",
    "ranking news articles based on popularity prediction . in _ proceedings of the 2012 international conference on advances in social networks analysis and mining ( asonam 2012 ) _ , pages 106110 .",
    "wang , x. , zhao , l. , and wu , y. ( 2000 ) .",
    "distribution free laws of the iterated logarithm for kernel estimator of regression function based on directional data . , 21(4):489498 .",
    "zhao , l. and wu , c. ( 2001 ) .",
    "central limit theorem for integrated square error of kernel estimators of spherical density .",
    ", 44(4):474483 .",
    "* keywords * : local linear regression ; goodness  of  fit test ; directional data ; bootstrap calibration .",
    "some interesting cases and relations of the projected estimator are the following ones .      if @xmath63 , then @xmath78 and the nadaraya - watson estimator for directional predictor and scalar response , firstly proposed by @xcite , is obtained : @xmath367 for @xmath86 , denoting @xmath368 , for @xmath369 the circular sample can be identified with a set of angles @xmath370 and the usual notation for circular statistics applies .",
    "then , the local constant estimator for circular data is given by @xmath371 where the second equality holds if @xmath13 is the von mises kernel .",
    "+ for @xmath372 , denoting @xmath373 , for @xmath369 and @xmath374 , the spherical sample can be identified as the pairs of angles @xmath375 .",
    "therefore , the local constant estimator for spherical data is given by @xmath376      let denote @xmath368 , for @xmath369 .",
    "the matrix @xmath377 is formed by the vector @xmath378 , which is the orthonormal vector to @xmath35 .",
    "then , by the sine subtraction formula @xmath379 and as a consequence ( [ gofreg : wlsp ] ) can be expressedas @xmath380 and the solution ( [ gofreg : betaest ] ) is given by the design matrix @xmath381 the resulting estimate is the local linear estimator proposed by @xcite for circular predictors and for circular kernels which are functions of @xmath382 ( the change in notation is @xmath85 ) .",
    "the equivalence of both estimators can be seen also from examining the equality of their design matrices and weights or from the taylor expansions that motivate them . by the chain rule",
    ", it can be seen that the derivative of the regression function in the circular argument , as considered in @xcite , is the same as the projected gradient of @xmath22 : @xmath383 finally , if @xmath384 is close to @xmath385 ( in modulo @xmath386 ) , then @xmath387 and the linear coefficient of the local estimator captures indeed a linear effect of close angles in the response .",
    "+ the circular case of the local linear estimator in @xcite is different from the circular projected local estimator and the one in @xcite . the minimum weighted squares problem , using the tangent - normal decomposition ant translated to this paper s notation , is stated as follows : @xmath388 where @xmath389 is such that @xmath390 and @xmath391 if @xmath392 and @xmath393 otherwise . after considering the polar coordinates and doing some trigonometric algebra , it results that @xmath394 , so after identifying @xmath395 , the minimization ( [ gofreg : dimarq:1 ] ) is equivalentto @xmath396 provided that @xmath397 for close angles , the practical difference between ( [ gofreg : wlsp:1 ] ) and ( [ gofreg : dimarq:1 ] ) relies only in small samples and for large bandwidths .",
    "+ finally , it is possible to compute the exact expression for the estimator using the exact inversion formula of the @xmath398 matrix @xmath84 ( as it is done in @xcite , among others ) .",
    "this yields @xmath399 where , for @xmath400 , @xmath401      let denote @xmath373 , for @xmath369 and @xmath374 .",
    "now the matrix @xmath377 is given by vectors @xmath402 and @xmath403 if @xmath404 ( if @xmath405 , then @xmath406 and @xmath407 complete the orthonormal basis ) .",
    "therefore , after some trigonometric identities , @xmath408 as a consequence , the solution ( [ gofreg : betaest ] ) is given by the design matrix @xmath409 the second and third columns are almost linear in the angles @xmath384 and @xmath410 , respectively : if @xmath384 is close to @xmath385 ( in modulo @xmath386 ) and @xmath411 is close to @xmath412 ( in modulo @xmath413 ) , then @xmath414 and hence @xmath415 , so @xmath416 furthermore , as happens with the circular case , the projected gradient of @xmath22 used in the projected local estimator comprises naturally the estimator that follows from considering the function @xmath22 defined throughout spherical coordinates and taking the derivatives on them : @xmath417 finally , the exact expression for the estimator can also be obtained using the exact inversion formula of the @xmath418 matrix @xmath84 . to that end ,",
    "recall that @xmath419 where , for @xmath420 , @xmath421 therefore , after some matrix algebra it turns out that @xmath422",
    "[ gofreg : lem:1 ] under assumptions [ gofreg : assump : a1][gofreg : assump : a4 ] , for a random sample @xmath348 the following statements hold with uniform orders for any point @xmath43 :    1 .",
    "@xmath423.[gofreg : lem:1:d0 ] 2 .",
    "@xmath424.[gofreg : lem:1:d1b ] 3 .",
    "@xmath425.[gofreg : lem:1:d1c ] 4 .",
    "@xmath426.[gofreg : lem:1:d2b ] 5 .   @xmath427}f({\\mathbf{x}})h^2 $ ] @xmath428.[gofreg : lem:1:d2h ] 6 .",
    "@xmath429.[gofreg : lem:1:d3 ] 7 .",
    "@xmath430.[gofreg : lem:1:v1 ] 8 .",
    "@xmath431.[gofreg : lem:1:v2 ] 9 .",
    "@xmath432.[gofreg : lem:1:v3 ]    chebychev s inequality , lemma [ gofreg : lem:4 ] and taylor expansions will be used for each statement in which the proof is divided .",
    "+ _ proof of [ gofreg : lem:1:d0]_. by chebychev s inequality , @xmath433+\\mathcal{o}_\\mathbb{p}\\big(\\sqrt{\\mathbb{v}\\mathrm{ar}\\big[\\hat f_h({\\mathbf{x}})\\big]}\\big)$ ] .",
    "it follows by lemma [ gofreg : lem:4 ] that @xmath434=f({\\mathbf{x}})+{\\mathpzc{o}{\\left(}1{\\right)}}$ ] and that @xmath435=\\frac{1}{nh^q\\lambda_q(l)}(f({\\mathbf{x}})+{\\mathpzc{o}{\\left(}1{\\right)}})$ ] , with the remaining orders being uniform in @xmath43 .",
    "then , as @xmath7 is continuous in @xmath4 by assumption [ gofreg : assump : a1 ] it is also bounded , so by [ gofreg : assump : a4 ] @xmath435={\\mathpzc{o}{\\left(}1{\\right)}}$ ] uniformly , which results in @xmath423 uniformly in @xmath43",
    ". + _ proof of [ gofreg : lem:1:d1b]_. applying lemma [ gofreg : lem:2 ] and the change of variables @xmath436 , @xmath437\\nonumber\\\\      = \\,&{\\mathbb{e}{\\left[}l_h({\\mathbf{x}},{\\mathbf{x}}){\\mathbf{b}}_{{\\mathbf{x}}}^t({\\mathbf{x}}-{\\mathbf{x}}){\\right]}}\\nonumber\\\\      = \\,&c_{h , q}(l){\\int_{\\omega_{q } } l{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{y}}}{h^2}\\right)}{\\mathbf{b}}_{{\\mathbf{x}}}^t({\\mathbf{y}}-{\\mathbf{x}})f({\\mathbf{y}})\\,\\omega_{q}(d { \\mathbf{y}})}\\nonumber\\\\      = \\,&c_{h , q}(l)\\int_{-1}^1\\int_{{\\omega_{q-1 } } } l{\\left(\\frac{1-t}{h^2}\\right)}{\\boldsymbol\\xi}f{\\left(t{\\mathbf{x}}+(1-t^2)^\\frac{1}{2}{\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi}\\right)}(1-t^2)^{\\frac{q-1}{2}}\\,\\omega_{q-1}(d{\\boldsymbol\\xi})\\,dt\\nonumber\\\\          = \\,&c_{h , q}(l)h^{2}\\int_{0}^{2h^{-2}}\\int_{{\\omega_{q-1 } } } l{\\left(r\\right)}{\\boldsymbol\\xi}f{\\left({\\mathbf{x}}+{\\boldsymbol\\alpha}_{{\\mathbf{x}},{\\boldsymbol\\xi}}\\right ) } { \\left[rh^2(2-rh^2)\\right]}^{\\frac{q-1}{2}}\\,{\\omega_{q-1}}(d{\\boldsymbol\\xi})\\,dr\\nonumber\\\\      = \\,&c_{h , q}(l)h^{q+1}\\int_{0}^{2h^{-2 } } l{\\left(r\\right)}r^{\\frac{q-1}{2}}(2-rh^2)^{\\frac{q-1}{2 } } \\int_{{\\omega_{q-1 } } } f{\\left({\\mathbf{x}}+{\\boldsymbol\\alpha}_{{\\mathbf{x}},{\\boldsymbol\\xi}}\\right ) } { \\boldsymbol\\xi}\\,{\\omega_{q-1}}(d{\\boldsymbol\\xi})\\,dr,\\label{gofreg : lem:1:1 }      \\end{aligned}\\ ] ] where @xmath438}^\\frac{1}{2}{\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi}$ ] .",
    "the inner integral in ( [ gofreg : lem:1:1 ] ) is computed by a taylor expansion @xmath439 where the remaining order involves the second derivative of @xmath7 , which is bounded , thus being the order uniform in @xmath35 .",
    "using lemma [ gofreg : lem:3 ] , the first and second addends are : @xmath440}^\\frac{1}{2}\\int_{{\\omega_{q-1}}}({\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi})^t{\\boldsymbol\\nabla}f{\\left({\\mathbf{x}}\\right)}{\\boldsymbol\\xi}\\,{\\omega_{q-1}}(d{\\boldsymbol\\xi})\\\\      = & \\,{\\left[rh^2(2-rh^2)\\right]}^\\frac{1}{2}\\int_{{\\omega_{q-1}}}\\sum_{i , j=1}^q \\xi_i{\\mathbf{b}}_{{\\mathbf{x}}}^t{\\boldsymbol\\nabla}f{\\left({\\mathbf{x}}\\right)}\\xi_j \\,{\\omega_{q-1}}(d{\\boldsymbol\\xi})\\\\      = & \\,\\frac{{\\omega_{q-1}}}{q}{\\left[rh^2(2-rh^2)\\right]}^\\frac{1}{2 } { \\mathbf{b}}_{{\\mathbf{x}}}^t{\\boldsymbol\\nabla}f({\\mathbf{x } } ) .",
    "\\end{aligned}\\ ] ] the third addend is @xmath441 , because @xmath442 and @xmath443 . therefore , ( [ gofreg : lem:1:1 ] ) becomes @xmath444 where the second last equality follows from applying the dominated convergence theorem ( dct ) , ( [ gofreg : chq ] ) and the definition of @xmath445 . see the proof of theorem 1 in @xcite for the technical details involved in a similar situation .",
    "+ as the chebychev inequality is going to be applied componentwise , the interest is now in the order of the variance vector . to that end",
    ", the square of a vector will denote the vector with correspondent squared components . by analogous computations , @xmath446\\nonumber\\\\      \\leq\\,&\\frac{1}{n}{\\mathbb{e}{\\left[}l_h({\\mathbf{x}},{\\mathbf{x}})^2({\\mathbf{b}}_{{\\mathbf{x}}}^t({\\mathbf{x}}-{\\mathbf{x}}))^2{\\right]}}\\nonumber\\\\      = \\,&\\frac{c_{h , q}(l)^2}{n}{\\int_{\\omega_{q } } l^2{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{y}}}{h^2}\\right)}({\\mathbf{b}}_{{\\mathbf{x}}}^t({\\mathbf{y}}-{\\mathbf{x}}))^2f({\\mathbf{y}})\\,\\omega_{q}(d { \\mathbf{y}})}\\nonumber\\\\      = \\,&\\frac{c_{h , q}(l)^2}{n}\\int_{-1}^1\\int_{{\\omega_{q-1 } } } l^2{\\left(\\frac{1-t}{h^2}\\right)}{\\boldsymbol\\xi}^2f{\\left(t{\\mathbf{x}}+(1-t^2)^\\frac{1}{2}{\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi}\\right ) } ( 1-t^2)^{\\frac{q}{2}}\\,\\omega_{q-1}(d{\\boldsymbol\\xi})\\,dt\\nonumber\\\\          = \\,&\\frac{c_{h , q}(l)^2h^{2}}{n}\\int_{0}^{2h^{-2}}\\int_{{\\omega_{q-1 } } } l^2{\\left(r\\right)}{\\boldsymbol\\xi}^2 f{\\left({\\mathbf{x}}+{\\boldsymbol\\alpha}_{{\\mathbf{x}},{\\boldsymbol\\xi}}\\right ) } { \\left[rh^2(2-rh^2)\\right]}^{\\frac{q}{2}}\\,{\\omega_{q-1}}(d{\\boldsymbol\\xi})\\,dr\\nonumber\\\\      = \\,&\\frac{c_{h , q}(l)^2h^{q+2}}{n}\\int_{0}^{2h^{-2 } } l^2{\\left(r\\right)}r^{\\frac{q}{2}}(2-rh^2)^{\\frac{q}{2 } } \\int_{{\\omega_{q-1 } } } f{\\left({\\mathbf{x}}+{\\boldsymbol\\alpha}_{{\\mathbf{x}},{\\boldsymbol\\xi}}\\right)}{\\boldsymbol\\xi}^2 \\,{\\omega_{q-1}}(d{\\boldsymbol\\xi})\\,dr\\nonumber\\\\      = \\,&\\frac{c_{h , q}(l)^2h^{q+2}}{n}\\int_{0}^{2h^{-2 } } l^2{\\left(r\\right)}r^{\\frac{q}{2}}(2-rh^2)^{\\frac{q}{2 } } { \\mathcal{o}{\\left(}{\\mathbf{1}}{\\right)}}\\,dr\\nonumber\\\\          = \\,&{\\mathcal{o}{\\left(}\\frac{h^2}{nh^q}{\\mathbf{1}}{\\right)}}.\\label{gofreg : lem:1:4 }      \\end{aligned}\\ ] ] the result follows from chebychev s inequality , ( [ gofreg : lem:1:3 ] ) and ( [ gofreg : lem:1:4 ] ) .",
    "+ _ proof of [ gofreg : lem:1:d1c]_. the result is proved form the previous proof and the tower property of the conditional expectation . the expectation can be expressed as @xmath447=\\,&{\\mathbb{e}{\\left[}{\\mathbb{e}{\\left[}l_h({\\mathbf{x}},{\\mathbf{x}}){\\mathbf{b}}_{{\\mathbf{x}}}^t({\\mathbf{x}}-{\\mathbf{x}})y\\big|{\\mathbf{x}}{\\right]}}{\\right]}}\\\\      = \\,&{\\mathbb{e}{\\left[}l_h({\\mathbf{x}},{\\mathbf{x}}){\\mathbf{b}}_{{\\mathbf{x}}}^t({\\mathbf{x}}-{\\mathbf{x}})m({\\mathbf{x}}){\\right]}}\\\\      = \\,&c_{h , q}(l){\\int_{\\omega_{q } } l{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{y}}}{h^2}\\right)}{\\mathbf{b}}_{{\\mathbf{x}}}^t({\\mathbf{y}}-{\\mathbf{x}})m({\\mathbf{y}})f({\\mathbf{y}})\\,\\omega_{q}(d { \\mathbf{y}})}.      \\end{aligned}\\ ] ] then , replicating the proof of [ gofreg : lem:1:d1b ] , it is easily seen that the order is @xmath448 .",
    "the order of the variance is obtained in the same way : @xmath449}}\\leq&\\,\\frac{1}{n}{\\mathbb{e}{\\left[}l^2_h({\\mathbf{x}},{\\mathbf{x}})({\\mathbf{b}}_{{\\mathbf{x}}}^t({\\mathbf{x}}-{\\mathbf{x}}))^2y^2{\\right]}}\\\\      = & \\,\\frac{1}{n}{\\mathbb{e}{\\left[}l_h({\\mathbf{x}},{\\mathbf{x}})({\\mathbf{b}}_{{\\mathbf{x}}}^t({\\mathbf{x}}-{\\mathbf{x}}))^2(\\sigma^2({\\mathbf{x}})+m({\\mathbf{x}})^2){\\right]}}\\\\      = & \\,{\\mathcal{o}{\\left(}\\frac{h^2}{nh^q}{\\mathbf{1}}{\\right)}}.      \\end{aligned}\\ ] ] as a consequence , @xmath425",
    ". + _ proof of [ gofreg : lem:1:d2b]_. the steps of the proof of [ gofreg : lem:1:d1b ] are replicated : @xmath450\\nonumber\\\\      = \\,&c_{h , q}(l){\\int_{\\omega_{q } } l{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{y}}}{h^2}\\right)}{\\mathbf{b}}_{{\\mathbf{x}}}^t({\\mathbf{y}}-{\\mathbf{x}})({\\mathbf{y}}-{\\mathbf{x}})^t{\\mathbf{b}}_{{\\mathbf{x } } } f({\\mathbf{y}})\\,\\omega_{q}(d { \\mathbf{y}})}\\nonumber\\\\      = \\,&c_{h , q}(l)\\int_{-1}^1\\int_{{\\omega_{q-1 } } } l{\\left(\\frac{1-t}{h^2}\\right)}{\\boldsymbol\\xi}{\\boldsymbol\\xi}^t f{\\left(t{\\mathbf{x}}+(1-t^2)^\\frac{1}{2}{\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi}\\right ) } ( 1-t^2)^{\\frac{q}{2}}\\,\\omega_{q-1}(d{\\boldsymbol\\xi})\\,dt\\nonumber\\\\      = \\,&c_{h , q}(l)h^{q+2}\\int_{0}^{2h^{-2 } } l{\\left(r\\right)}r^{\\frac{q}{2}}(2-rh^2)^{\\frac{q}{2 } } \\int_{{\\omega_{q-1 } } } f{\\left({\\mathbf{x}}+{\\boldsymbol\\alpha}_{{\\mathbf{x}},{\\boldsymbol\\xi}}\\right)}{\\boldsymbol\\xi}{\\boldsymbol\\xi}^t \\,{\\omega_{q-1}}(d{\\boldsymbol\\xi})\\,dr.\\label{gofreg : lem:1:5 }      \\end{aligned}\\ ] ] the second integral of ( [ gofreg : lem:1:5 ] ) is obtained by expansion ( [ gofreg : lem:1:2 ] ) and lemma [ gofreg : lem:3 ] : @xmath451 as the third addend given by expansion ( [ gofreg : lem:1:2 ] ) has order @xmath452 , it results that : @xmath453 using the same arguments as in [ gofreg : lem:1:d1b ] .",
    "the order of the variance is @xmath454\\nonumber\\\\      \\leq\\,&\\frac{1}{n}{\\mathbb{e}{\\left[}l_h({\\mathbf{x}},{\\mathbf{x}})^2{\\left({\\mathbf{b}}_{{\\mathbf{x}}}^t({\\mathbf{x}}-{\\mathbf{x}})({\\mathbf{x}}-{\\mathbf{x}})^t{\\mathbf{b}}_{{\\mathbf{x}}}\\right)}^2{\\right]}}\\nonumber\\\\      = \\,&\\frac{c_{h , q}(l)^2}{n}{\\int_{\\omega_{q } } l^2{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{y}}}{h^2}\\right)}{\\left({\\mathbf{b}}_{{\\mathbf{x}}}^t({\\mathbf{y}}-{\\mathbf{x}})({\\mathbf{y}}-{\\mathbf{x}})^t{\\mathbf{b}}_{{\\mathbf{x}}}\\right)}^2 f({\\mathbf{y}})\\,\\omega_{q}(d { \\mathbf{y}})}\\nonumber\\\\      = \\,&\\frac{c_{h , q}(l)^2}{n}\\int_{-1}^1\\int_{{\\omega_{q-1 } } } l^2{\\left(\\frac{1-t}{h^2}\\right)}{\\left({\\boldsymbol\\xi}{\\boldsymbol\\xi}^t\\right)}^2 f{\\left(t{\\mathbf{x}}+(1-t^2)^\\frac{1}{2}{\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi}\\right ) } ( 1-t^2)^{\\frac{q}{2}+1}\\,{\\omega_{q-1}}(d{\\boldsymbol\\xi})\\,dt\\nonumber\\\\      = \\,&\\frac{c_{h , q}(l)^2h^{q+4}}{n}\\int_{0}^{2h^{-2 } } l^2{\\left(r\\right)}r^{\\frac{q}{2}+1}(2-rh^2)^{\\frac{q}{2}+1 } \\int_{{\\omega_{q-1 } } } f{\\left({\\mathbf{x}}+{\\boldsymbol\\alpha}_{{\\mathbf{x}},{\\boldsymbol\\xi}}\\right)}{\\left({\\boldsymbol\\xi}{\\boldsymbol\\xi}^t\\right)}^2\\,{\\omega_{q-1}}(d{\\boldsymbol\\xi})\\,dt\\nonumber\\\\      = \\,&{\\mathcal{o}{\\left(}\\frac{h^4}{nh^q}{\\mathbf{1}}{\\mathbf{1}}^t{\\right)}}.\\label{gofreg : lem:1:7 }      \\end{aligned}\\ ] ] the desired result now holds by ( [ gofreg : lem:1:6 ] ) and ( [ gofreg : lem:1:7 ] ) , as @xmath455 by [ gofreg : assump : a4 ] .",
    "+ _ proof of [ gofreg : lem:1:d2h]_. this is one of the most important results since it determines the dominant term of the bias of the local projected estimator .",
    "the expectation is @xmath456\\nonumber\\\\          = \\,&c_{h , q}(l){\\int_{\\omega_{q } } l{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{y}}}{h^2}\\right)}({\\mathbf{y}}-{\\mathbf{x}})^t{\\mathbf{b}}_{{\\mathbf{x}}}{\\mathbf{b}}_{{\\mathbf{x}}}^t{\\boldsymbol{\\mathcal{h}}}_m({\\mathbf{x}}){\\mathbf{b}}_{{\\mathbf{x}}}{\\mathbf{b}}_{{\\mathbf{x}}}^t({\\mathbf{y}}-{\\mathbf{x } } ) f({\\mathbf{y}})\\,\\omega_{q}(d { \\mathbf{y}})}\\nonumber\\\\      = \\,&c_{h , q}(l)\\int_{-1}^1\\int_{{\\omega_{q-1 } } } l{\\left(\\frac{1-t}{h^2}\\right)}({\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi})^t{\\boldsymbol{\\mathcal{h}}}_m({\\mathbf{x}}){\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi}f{\\left(t{\\mathbf{x}}+(1-t^2)^\\frac{1}{2}{\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi}\\right ) } ( 1-t^2)^{\\frac{q}{2}}\\nonumber\\\\      & \\times\\,{\\omega_{q-1}}(d{\\boldsymbol\\xi})\\,dt\\nonumber\\\\              = \\,&c_{h , q}(l)h^{q+2}\\int_{0}^{2h^{-2 } } l{\\left(r\\right)}r^{\\frac{q}{2}}(2-rh^2)^{\\frac{q}{2 } } \\int_{{\\omega_{q-1 } } } f{\\left({\\mathbf{x}}+{\\boldsymbol\\alpha}_{{\\mathbf{x}},{\\boldsymbol\\xi}}\\right)}({\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi})^t{\\boldsymbol{\\mathcal{h}}}_m({\\mathbf{x}}){\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi}\\nonumber\\\\      & \\times \\,{\\omega_{q-1}}(d{\\boldsymbol\\xi})\\,dr.\\label{gofreg : lem:1:8 }      \\end{aligned}\\ ] ] the first addend of the taylor expansion ( [ gofreg : lem:1:2 ] ) is computed using lemma [ gofreg : lem:3 ] and the following relation of the trace operator : @xmath457}={\\mathrm{tr}\\left[{\\mathbf{x}}{\\mathbf{x}}^t\\mathbf{a}\\right]},\\quad\\text{for $ { \\mathbf{x}}$ a vector and $ \\mathbf{a}$ a matrix.}\\ ] ] recall also that by definition of @xmath458 , @xmath459 and @xmath223 by [ gofreg : assump : a1 ] .",
    "then : @xmath460}\\\\",
    "= & \\,f({\\mathbf{x}})\\frac{{\\omega_{q-1}}}{q}{\\mathrm{tr}\\left[{\\boldsymbol{\\mathcal{h}}}_m({\\mathbf{x}})\\right]}.      \\end{aligned}\\ ] ] the second and third addends have the same orders as in [ gofreg : lem:1:d2b ] , so @xmath461}f({\\mathbf{x}})+{\\mathcal{o}\\lph^2{\\right)}}\\bigg\\}\\,dr\\\\      = \\,&\\frac{2b_q(l)}{q}{\\mathrm{tr}\\left[{\\boldsymbol{\\mathcal{h}}}_m({\\mathbf{x}})\\right]}f({\\mathbf{x}})h^2+{\\mathpzc{o}\\lph^2{\\right)}}.      \\end{aligned}\\ ] ] using the square notation for vectors , the order of the variance is @xmath462\\nonumber\\\\      \\leq\\,&\\frac{c_{h , q}(l)^2}{n}{\\int_{\\omega_{q } } l^2{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{y}}}{h^2}\\right)}{\\left(({\\mathbf{y}}-{\\mathbf{x}})^t{\\mathbf{b}}_{{\\mathbf{x}}}{\\mathbf{b}}_{{\\mathbf{x}}}^t{\\boldsymbol{\\mathcal{h}}}_m({\\mathbf{x}}){\\mathbf{b}}_{{\\mathbf{x}}}{\\mathbf{b}}_{{\\mathbf{x}}}^t({\\mathbf{y}}-{\\mathbf{x}})\\right)}^2 f({\\mathbf{y}})\\,\\omega_{q}(d { \\mathbf{y}})}\\nonumber\\\\      = \\,&\\frac{c_{h , q}(l)^2h^{q+4}}{n}\\int_{0}^{2h^{-2 } } l^2{\\left(r\\right)}r^{\\frac{q}{2}+1}(2-rh^2)^{\\frac{q}{2}+1}\\nonumber\\\\ & \\times\\int_{{\\omega_{q-1}}}f{\\left({\\mathbf{x}}+{\\boldsymbol\\alpha}_{{\\mathbf{x}},{\\boldsymbol\\xi}}\\right)}{\\left(({\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi})^t{\\boldsymbol{\\mathcal{h}}}_m({\\mathbf{x}}){\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi}\\right)}^2\\,{\\omega_{q-1}}(d{\\boldsymbol\\xi } ) \\,dr\\nonumber\\\\      = \\,&\\frac{c_{h , q}(l)^2h^{q+4}}{n}\\int_{0}^{2h^{-2 } } l^2{\\left(r\\right)}r^{\\frac{q}{2}+1}(2-rh^2)^{\\frac{q}{2}+1}{\\mathcal{o}{\\left(}1{\\right ) } } \\,dr\\\\      = \\,&{\\mathcal{o}{\\left(}\\frac{h^4}{nh^q}{\\right ) } }      \\end{aligned}\\ ] ] and the square root of this order can be merged into @xmath236 .",
    "+ _ proof of [ gofreg : lem:1:d3]_. similarly to the previous proofs , the order of the bias is @xmath463\\\\      = \\,&c_{h , q}(l)h^{q+3}\\int_{0}^{2h^{-2 } } l{\\left(r\\right)}r^{\\frac{q+1}{2}}(2-rh^2)^{\\frac{q+1}{2 } } \\\\      & \\times\\int_{{\\omega_{q-1 } } } f{\\left({\\mathbf{x}}+{\\boldsymbol\\alpha}_{{\\mathbf{x}},{\\boldsymbol\\xi}}\\right)}{\\boldsymbol\\xi}{\\boldsymbol\\xi}^t { \\boldsymbol{\\mathcal{h}}}_m({\\mathbf{x}}){\\boldsymbol\\xi}\\,{\\omega_{q-1}}(d{\\boldsymbol\\xi})\\,dr\\\\          = \\,&{\\mathcal{o}\\lph^4{\\mathbf{1}}{\\right ) } }      \\end{aligned}\\ ] ] because by lemma [ gofreg : lem:3 ] the first element in the taylor expansion of the inner integral is exactly zero .",
    "the variance is @xmath464\\\\      \\leq\\,&\\frac{c_{h , q}(l)^2h^{q+6}}{n}\\int_{0}^{2h^{-2 } } l^2{\\left(r\\right)}r^{\\frac{q}{2}+2}(2-rh^2)^{\\frac{q}{2}+2}\\\\      & \\times \\int_{{\\omega_{q-1 } } } f{\\left({\\mathbf{x}}+{\\boldsymbol\\alpha}_{{\\mathbf{x}},{\\boldsymbol\\xi}}\\right)}{\\left({\\boldsymbol\\xi}{\\boldsymbol\\xi}^t { \\boldsymbol{\\mathcal{h}}}_m({\\mathbf{x}}){\\boldsymbol\\xi}\\right)}^2 \\,{\\omega_{q-1}}(d{\\boldsymbol\\xi})\\,dr\\\\          = \\,&{\\mathcal{o}{\\left(}\\frac{h^{6}}{nh^q}{\\mathbf{1}}{\\right)}}.      \\end{aligned}\\ ] ] since @xmath465 the result is proved .",
    "+ _ proof of [ gofreg : lem:1:v1]_. because of lemma [ gofreg : lem:4 ] and ( [ gofreg : chq ] ) : @xmath466 } }     = & \\,c_{h , q}(l)^2{\\int_{\\omega_{q } } l^2{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{y}}}{h^2}\\right)}\\sigma^2({\\mathbf{y } } ) f({\\mathbf{y}})\\,\\omega_{q}(d { \\mathbf{y}})}\\\\      = & \\,\\frac{c_{h , q}(l)^2}{c_{h , q}(l^2)}{\\left[\\sigma^2({\\mathbf{x}})f({\\mathbf{x}})+{\\mathpzc{o}{\\left(}1{\\right)}}\\right]}\\\\          = & \\,\\frac{\\lambda_q(l^2)\\lambda_q(l)^{-2}}{h^q}\\sigma^2({\\mathbf{x}})f({\\mathbf{x}})+{\\mathpzc{o}\\lph^{-q}{\\right)}},\\\\      { \\mathbb{v}\\mathrm{ar}{\\left[}\\frac{1}{n}\\sum_{i=1}^nl_h^2{\\left({\\mathbf{x}},{\\mathbf{x}}_i\\right)}\\sigma^2({\\mathbf{x}}_i){\\right]}}\\leq&\\,\\frac{1}{n}{\\mathbb{e}{\\left[}l_h^4{\\left({\\mathbf{x}},{\\mathbf{x}}\\right)}\\sigma^4({\\mathbf{x}}){\\right]}}\\\\      = & \\,\\frac{c_{h , q}(l)^4}{n}{\\int_{\\omega_{q } } l^4{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{y}}}{h^2}\\right)}\\sigma^4({\\mathbf{y } } ) f({\\mathbf{y}})\\,\\omega_{q}(d { \\mathbf{y}})}\\\\      = & \\,\\frac{c_{h , q}(l)^4}{nc_{h , q}(l^4)}{\\left[\\sigma^4({\\mathbf{x}})f({\\mathbf{x}})+{\\mathpzc{o}{\\left(}1{\\right)}}\\right]}\\\\      = & \\,{\\mathcal{o}{\\left(}(nh^{3q})^{-1}{\\right)}}.      \\end{aligned}\\ ] ] the remaining order is @xmath467 because @xmath468 by [ gofreg : assump : a4 ] . + _ proof of [ gofreg : lem:1:v2]_. by ( [ gofreg : chq ] ) and lemma [ gofreg : lem:4 ] applied componentwise , since the functions in the integrand are vector valued , it follows that @xmath469\\\\          = & \\,c_{h , q}(l)^2{\\int_{\\omega_{q } } l^2{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{y}}}{h^2}\\right)}{\\mathbf{b}}_{{\\mathbf{x}}}^t({\\mathbf{y}}-{\\mathbf{x}})\\sigma^2({\\mathbf{y } } ) f({\\mathbf{y}})\\,\\omega_{q}(d { \\mathbf{y}})}\\\\      = & \\,\\frac{c_{h , q}(l)^2}{c_{h , q}(l^2)}{\\left({\\mathbf{0}}+{\\mathpzc{o}{\\left(}{\\mathbf{1}}{\\right)}}\\right)}\\\\      = & \\,{\\mathpzc{o}\\lph^{-q}{\\mathbf{1}}{\\right)}},\\\\      \\mathbb{v}\\mathrm{ar}\\bigg[\\frac{1}{n}\\sum_{i=1}^nl_h^2{\\left({\\mathbf{x}},{\\mathbf{x}}_i\\right)}{\\mathbf{b}}_{{\\mathbf{x}}}^t&({\\mathbf{x}}_i-{\\mathbf{x}})\\sigma^2({\\mathbf{x}}_i)\\bigg]\\\\          \\leq&\\,c_{h , q}(l)^4{\\int_{\\omega_{q } } l^4{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{y}}}{h^2}\\right)}{\\left({\\mathbf{b}}_{{\\mathbf{x}}}^t({\\mathbf{y}}-{\\mathbf{x}})\\right)}^2\\sigma^4({\\mathbf{y } } ) f({\\mathbf{y}})\\,\\omega_{q}(d { \\mathbf{y}})}\\\\      = & \\,\\frac{c_{h , q}(l)^4}{nc_{h , q}(l^4)}{\\left({\\mathbf{0}}+{\\mathpzc{o}{\\left(}{\\mathbf{1}}{\\right)}}\\right)}\\\\      = & \\,{\\mathpzc{o}{\\left(}(nh^{3q})^{-1}{\\right)}}.      \\end{aligned}\\ ] ]    _ proof of [ gofreg : lem:1:v3]_. the proof is analogous to [ gofreg : lem:1:v2 ] : using ( [ gofreg : chq ] ) and lemma [ gofreg : lem:4 ] componentwise the statement is proved trivially .",
    "[ gofreg : lem:5 ] under assumptions [ gofreg : assump : a1][gofreg : assump : a4 ] and [ gofreg : assump : a7n ] , for a random sample @xmath348 the following statements hold :    1",
    ".   @xmath470 .",
    "[ gofreg : lem:5:4 ] 2 .",
    "@xmath471 + @xmath472.[gofreg : lem:5:1 ] 3 .",
    "@xmath473.[gofreg : lem:5:2 ] 4 .",
    "@xmath474 + @xmath475 .",
    "[ gofreg : lem:5:3 ] 5 .",
    "@xmath331=n^{-2}\\nu^2{\\left(1+{\\mathpzc{o}{\\left(}1{\\right)}}\\right)}$ ] , @xmath476}}={\\mathcal{o}\\lpn^{-4}h^{2q}{\\right)}}$ ] , @xmath336={\\mathcal{o}{\\left(}(n^4h^{q})^{-1}{\\right)}}$ ] , @xmath477}}={\\mathcal{o}\\lpn^{-4}{\\right)}}$ ] , where @xmath478 is given in theorem [ gofreg : theo : limdis].[gofreg : lem:5:5 ]    the proof is divided for each statement .",
    "as in lemma [ gofreg : lem:1 ] , chebychev s inequality and lemma [ gofreg : lem:4 ] are used repeatedly .",
    "+ _ proof of [ gofreg : lem:5:4]_. by corollary [ gofreg : coro : equiv ] , @xmath479 using the properties of the conditional expectation , fubini , relation ( [ gofreg : chq ] ) and lemma [ gofreg : lem:4 ] : @xmath480\\\\      = & \\,{\\int_{\\omega_{q } } \\mathbb{e}{\\left[l_h^*({\\mathbf{x}},{\\mathbf{x}})\\mathbb{e}{\\left[y - m({\\mathbf{x}})|{\\mathbf{x}}\\right]}\\right]}f({\\mathbf{x}})w({\\mathbf{x}})\\,\\omega_{q}(d { \\mathbf{x}})}\\\\          = & \\,0,\\\\          \\mathbb{v}\\mathrm{ar}\\bigg[{\\int_{\\omega_{q } } & \\sum_{i=1}^nl_h^*({\\mathbf{x}},{\\mathbf{x}}_i)(y_i - m({\\mathbf{x}}_i))f({\\mathbf{x}})w({\\mathbf{x}})\\,\\omega_{q}(d { \\mathbf{x}})}\\bigg]\\\\                              = & \\,\\frac{1}{nh^{q}\\lambda_q(l)}{\\int_{\\omega_{q } } { \\int_{\\omega_{q } } l{\\left(\\frac{1-{\\mathbf{y}}^t{\\mathbf{x}}}{h^2}\\right)}\\sigma^2{\\left({\\mathbf{x}}\\right)}f({\\mathbf{x}})w({\\mathbf{x}})w({\\mathbf{y}})\\,\\omega_{q}(d { \\mathbf{x}})}\\,\\omega_{q}(d { \\mathbf{y}})}{\\left(1+{\\mathpzc{o}{\\left(}1{\\right)}}\\right)}\\\\      = & \\,\\frac{1}{n}{\\int_{\\omega_{q } } \\sigma^2{\\left({\\mathbf{y}}\\right)}f({\\mathbf{y}})w({\\mathbf{y}})^2\\,\\omega_{q}(d { \\mathbf{y}})}{\\left(1+{\\mathpzc{o}{\\left(}1{\\right)}}\\right)}\\\\      = & \\,{\\mathcal{o}\\lpn^{-1}{\\right)}}.      \\end{aligned}\\ ] ] then @xmath481 .",
    "+ _ proof of [ gofreg : lem:5:1]_. the integral can be split in two addends : @xmath482 now , by applying fubini , ( [ gofreg : chq ] ) and lemma [ gofreg : lem:4 ] , @xmath483}}=&\\,\\frac{1}{nh^{2q}\\lambda_q(l)^2}{\\int_{\\omega_{q } } { \\mathbb{e}{\\left[}l^2{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{x}}}{h^2}\\right)}g({\\mathbf{x}})^2{\\right ] } } \\frac{w({\\mathbf{x}})}{f({\\mathbf{x}})}\\,\\omega_{q}(d { \\mathbf{x}})}\\\\      = & \\,\\frac{1}{nh^{2q}\\lambda_q(l)^2}{\\int_{\\omega_{q } } { \\int_{\\omega_{q } }   l^2{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{y}}}{h^2}\\right)}\\frac{g({\\mathbf{y}})^2 w({\\mathbf{x}})}{f({\\mathbf{x}})}f({\\mathbf{y}})\\,\\omega_{q}(d { \\mathbf{y}})}\\,\\omega_{q}(d { \\mathbf{x}})}\\\\          = & \\,\\frac{\\lambda_q(l^2)}{nh^{q}\\lambda_q(l)^2}{\\int_{\\omega_{q } } g({\\mathbf{x}})^2 w({\\mathbf{x}})\\,\\omega_{q}(d { \\mathbf{x}})}(1+{\\mathpzc{o}{\\left(}1{\\right)}})\\\\      = & \\,{\\mathcal{o}{\\left(}(nh^q)^{-1}{\\right)}},\\\\          { \\mathbb{v}\\mathrm{ar}{\\left[}i_1{\\right]}}\\leq&\\,\\frac{1}{n^3h^{4q}\\lambda_q(l)^4}{\\mathbb{e}{\\left[}\\bigg({\\int_{\\omega_{q } } l^2{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{x}}}{h^2}\\right)}\\frac{g({\\mathbf{x}})^2w({\\mathbf{x}})}{f({\\mathbf{x}})}\\,\\omega_{q}(d { \\mathbf{x}})}\\bigg)^2{\\right]}}\\\\          = & \\,\\frac{\\lambda_q(l^2)^2}{n^3h^{2q}\\lambda_q(l)^4}{\\int_{\\omega_{q } } \\frac{g({\\mathbf{y}})^2w({\\mathbf{y}})^2 } { f({\\mathbf{y}})}\\,\\omega_{q}(d { \\mathbf{y}})}(1+{\\mathpzc{o}{\\left(}1{\\right)}})\\\\      = & \\,{\\mathcal{o}{\\left(}(n^3h^{2q})^{-1}{\\right ) } }      \\end{aligned}\\ ] ] and therefore @xmath484 . on the other hand , by lemma [ gofreg : lem:4 ] and the independence of @xmath40 and @xmath485 if @xmath335 : @xmath486}}=&\\,\\frac{1-n^{-1}}{h^{2q}\\lambda_q(l)^2}{\\int_{\\omega_{q } } { \\mathbb{e}{\\left[}l{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{x}}}{h^2}\\right)}g({\\mathbf{x}}){\\right]}}^2 \\frac{w({\\mathbf{x}})}{f({\\mathbf{x}})}\\,\\omega_{q}(d { \\mathbf{x}})}\\\\              = & \\,{\\left(1-n^{-1}\\right)}{\\int_{\\omega_{q } } g({\\mathbf{x}})^2 f({\\mathbf{x}})w({\\mathbf{x}})\\,\\omega_{q}(d { \\mathbf{x}})}(1+{\\mathpzc{o}{\\left(}1{\\right)}})\\\\      = & \\,{\\int_{\\omega_{q } } g({\\mathbf{x}})^2 f({\\mathbf{x}})w({\\mathbf{x}})\\,\\omega_{q}(d { \\mathbf{x}})}(1+{\\mathpzc{o}{\\left(}1{\\right)}}),\\\\                  { \\mathbb{e}{\\left[}i_2 ^ 2{\\right]}}=&\\ , \\frac{1}{n^4h^{4q}\\lambda_q(l)^4}\\sum_{i\\neq",
    "j}\\sum_{k\\neq l } { \\int_{\\omega_{q } } { \\int_{\\omega_{q } } \\mathbb{e}\\bigg [ l{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{x}}_i}{h^2}\\right)}l{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{x}}_j}{h^2}\\right)}l{\\left(\\frac{1-{\\mathbf{y}}^t{\\mathbf{x}}_k}{h^2}\\right)}\\\\              & \\times l{\\left(\\frac{1-{\\mathbf{y}}^t{\\mathbf{x}}_l}{h^2}\\right ) } g({\\mathbf{x}}_i)g({\\mathbf{x}}_j)g({\\mathbf{x}}_k)g({\\mathbf{x}}_l ) \\bigg ] \\frac{w({\\mathbf{x}})w({\\mathbf{y}})}{f({\\mathbf{x}})f({\\mathbf{y } } ) } \\,\\omega_{q}(d { \\mathbf{x}})}\\,\\omega_{q}(d { \\mathbf{y}})}\\\\          = & \\,{\\mathcal{o}{\\left(}(n^2h^{4q})^{-1}{\\right ) } } { \\int_{\\omega_{q } } { \\int_{\\omega_{q } } \\mathbb{e}\\bigg [ l{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{x}}}{h^2}\\right)}l{\\left(\\frac{1-{\\mathbf{y}}^t{\\mathbf{x}}}{h^2}\\right)}g({\\mathbf{x}})^2 \\bigg]^2\\\\              & \\times\\frac{w({\\mathbf{x}})w({\\mathbf{y}})}{f({\\mathbf{x}})f({\\mathbf{y } } ) } \\,\\omega_{q}(d { \\mathbf{x}})}\\,\\omega_{q}(d { \\mathbf{y}})}\\\\      & + { \\mathcal{o}\\lpnh^{-4q}{\\right)}}{\\int_{\\omega_{q } } { \\int_{\\omega_{q } } \\mathbb{e}\\bigg [ l{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{x}}}{h^2}\\right)}l{\\left(\\frac{1-{\\mathbf{y}}^t{\\mathbf{x}}}{h^2}\\right)}g({\\mathbf{x}})^2 \\bigg]\\\\              & \\times \\mathbb{e}\\bigg[l{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{x}}}{h^2}\\right)}g({\\mathbf{x}})\\bigg]\\mathbb{e}\\bigg[l{\\left(\\frac{1-{\\mathbf{y}}^t{\\mathbf{x}}}{h^2}\\right)}g({\\mathbf{x}})\\bigg ] \\frac{w({\\mathbf{x}})w({\\mathbf{y}})}{f({\\mathbf{x}})f({\\mathbf{y } } ) } \\,\\omega_{q}(d { \\mathbf{x}})}\\,\\omega_{q}(d { \\mathbf{y}})}\\\\      & + \\frac{n^4-{\\mathcal{o}\\lpn^3{\\right)}}}{n^4h^{4q}\\lambda_q(l)^4}\\bigg({\\int_{\\omega_{q } } \\mathbb{e}\\bigg [ l{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{x}}}{h^2}\\right)}g({\\mathbf{x } } ) \\bigg]^2 \\frac{w({\\mathbf{x}})}{f({\\mathbf{x } } ) } \\,\\omega_{q}(d { \\mathbf{x}})}\\bigg)^2 \\\\          = & \\,{\\mathcal{o}{\\left(}(n^2h^{2q})^{-1}{\\right ) } } { \\int_{\\omega_{q } } { \\int_{\\omega_{q } } l^2{\\left(\\frac{1-{\\mathbf{y}}^t{\\mathbf{x}}}{h^2}\\right)}g({\\mathbf{x}})^4f({\\mathbf{x } } ) \\frac{w({\\mathbf{x}})w({\\mathbf{y}})}{f({\\mathbf{y}})}\\,\\omega_{q}(d { \\mathbf{x}})}\\,\\omega_{q}(d { \\mathbf{y}})}\\\\      & + { \\mathcal{o}{\\left(}(nh^{q})^{-1}{\\right)}}{\\int_{\\omega_{q } } { \\int_{\\omega_{q } } l{\\left(\\frac{1-{\\mathbf{y}}^t{\\mathbf{x}}}{h^2}\\right)}g({\\mathbf{x}})^3g({\\mathbf{y}})f({\\mathbf{x}})w({\\mathbf{x}})w({\\mathbf{y}})\\,\\omega_{q}(d { \\mathbf{x}})}\\,\\omega_{q}(d { \\mathbf{y}})}\\\\       & + { \\left(1-{\\mathcal{o}\\lpn^{-1}{\\right)}}\\right ) } { \\mathbb{e}{\\left[}i_2{\\right]}}^2\\\\      = & \\ , { \\mathcal{o}{\\left(}(n^2h^{q})^{-1}{\\right)}}+{\\mathcal{o}\\lpn^{-1}{\\right)}}+{\\left(1-{\\mathcal{o}\\lpn^{-1}{\\right)}}\\right ) } { \\mathbb{e}{\\left[}i_2{\\right]}}^2",
    ".      \\end{aligned}\\ ] ] then @xmath487}}={\\mathbb{e}{\\left[}i_2 ^ 2{\\right]}}-{\\mathbb{e}{\\left[}i_2{\\right]}}^2={\\mathcal{o}\\lpn^{-1}{\\right)}}$ ] and @xmath488 .",
    "finally , @xmath489    _ proof of [ gofreg : lem:5:2]_. by the tower property of the conditional expectation and @xmath27=0 $ ] , the expectation is zero . by the independece between @xmath490 s and @xmath28=1 $ ] , the variance is @xmath491\\\\          = & \\,\\frac{1}{n^4h^{4q}\\lambda_q(l)^4}\\sum_{i , j , k , l=1}^n\\int_{\\omega_q}\\int_{\\omega_q}\\mathbb{e}\\bigg [ l\\left(\\frac{1-\\mathbf{x}^t\\mathbf{x}_i}{h^2}\\right)l\\left(\\frac{1-\\mathbf{x}^t\\mathbf{x}_j}{h^2}\\right)l\\left(\\frac{1-\\mathbf{y}^t\\mathbf{x}_k}{h^2}\\right)\\\\                   & \\times l\\left(\\frac{1-\\mathbf{y}^t\\mathbf{x}_l}{h^2}\\right )                  \\mathbb{e}\\left[\\varepsilon_i\\varepsilon_k\\right|\\mathbf{x}_i,\\mathbf{x}_k]g(\\mathbf{x}_j)g(\\mathbf{x}_l)\\bigg ] \\frac{w(\\mathbf{x})w(\\mathbf{y})}{f(\\mathbf{x})f(\\mathbf{y})}\\,\\omega_q(d\\mathbf{x})\\,\\omega_q(d\\mathbf{y})\\\\          = & \\,\\frac{1}{n^4h^{4q}\\lambda_q(l)^4}\\sum_{i , j , l=1}^n\\int_{\\omega_q}\\int_{\\omega_q}\\mathbb{e}\\bigg [ l\\left(\\frac{1-\\mathbf{x}^t\\mathbf{x}_i}{h^2}\\right)l\\left(\\frac{1-\\mathbf{x}^t\\mathbf{x}_j}{h^2}\\right ) l\\left(\\frac{1-\\mathbf{y}^t\\mathbf{x}_i}{h^2}\\right)\\\\                  & \\times l\\left(\\frac{1-\\mathbf{y}^t\\mathbf{x}_l}{h^2}\\right )                  g(\\mathbf{x}_j)g(\\mathbf{x}_l)\\bigg ] \\frac{w(\\mathbf{x})w(\\mathbf{y})}{f(\\mathbf{x})f(\\mathbf{y})}\\,\\omega_q(d\\mathbf{x})\\,\\omega_q(d\\mathbf{y})\\\\          = & \\,\\frac{1}{n^4h^{4q}\\lambda_q(l)^4}\\left\\{i_1+i_2+i_3+i_4\\right\\ } ,          \\end{aligned}\\ ] ] where , by repeated use of lemma [ gofreg : lem:4 ] : @xmath492}}\\frac{w({\\mathbf{x}})w({\\mathbf{y}})}{f({\\mathbf{x}})f({\\mathbf{y}})}\\,\\omega_{q}(d { \\mathbf{x}})}\\,\\omega_{q}(d { \\mathbf{y}})}\\\\      = & \\,{\\mathcal{o}\\lpnh^{2q}{\\right)}},\\\\          i_2=&\\,{\\mathcal{o}\\lpn^2{\\right)}}{\\int_{\\omega_{q } } { \\int_{\\omega_{q } }   { \\mathbb{e}{\\left[}l{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{x}}}{h^2}\\right)}l{\\left(\\frac{1-{\\mathbf{y}}^t{\\mathbf{x}}}{h^2}\\right)}{\\right]}}\\\\              & \\times{\\mathbb{e}{\\left[}l{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{x}}}{h^2}\\right)}l{\\left(\\frac{1-{\\mathbf{y}}^t{\\mathbf{x}}}{h^2}\\right)}g({\\mathbf{x}}){\\right ] } } \\frac{w({\\mathbf{x}})w({\\mathbf{y}})}{f({\\mathbf{x}})f({\\mathbf{y } } ) } \\,\\omega_{q}(d { \\mathbf{x}})}\\,\\omega_{q}(d { \\mathbf{y}})}\\\\      = & \\,{\\mathcal{o}\\lpn^2h^{4q}{\\right)}},\\\\          i_3=&\\,{\\mathcal{o}\\lpn^2{\\right)}}{\\int_{\\omega_{q } } { \\int_{\\omega_{q } } { \\mathbb{e}{\\left[}l^2{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{x}}}{h^2}\\right)}l{\\left(\\frac{1-{\\mathbf{y}}^t{\\mathbf{x}}}{h^2}\\right)}g({\\mathbf{x } } ) { \\right]}}{\\mathbb{e}{\\left[}l{\\left(\\frac{1-{\\mathbf{y}}^t{\\mathbf{x}}}{h^2}\\right)}g({\\mathbf{x}}){\\right]}}\\\\               & \\times\\frac{w({\\mathbf{x}})w({\\mathbf{y}})}{f({\\mathbf{x}})f({\\mathbf{y}})}\\,\\omega_{q}(d { \\mathbf{x}})}\\,\\omega_{q}(d { \\mathbf{y}})}\\\\      = & \\,{\\mathcal{o}\\lpn^2h^{3q}{\\right)}},\\\\          i_4=&\\,{\\mathcal{o}\\lpn^3{\\right)}}{\\int_{\\omega_{q } } { \\int_{\\omega_{q } } { \\mathbb{e}{\\left[}l{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{x}}}{h^2}\\right)}l{\\left(\\frac{1-{\\mathbf{y}}^t{\\mathbf{x}}}{h^2}\\right)}{\\right]}}{\\mathbb{e}{\\left[}l{\\left(\\frac{1-{\\mathbf{y}}^t{\\mathbf{x}}}{h^2}\\right)}g({\\mathbf{x}}){\\right]}}^2\\\\              & \\times\\frac{w({\\mathbf{x}})w({\\mathbf{y}})}{f({\\mathbf{x}})f({\\mathbf{y}})}\\,\\omega_{q}(d { \\mathbf{x}})}\\,\\omega_{q}(d { \\mathbf{y}})}\\\\      = & \\,{\\mathcal{o}\\lpn^3h^{4q}{\\right)}}.      \\end{aligned}\\ ] ] because @xmath493 by [ gofreg : assump : a4 ] , we have that @xmath494    _ proof of [ gofreg : lem:5:3]_. let us denote @xmath495 . by the unit conditional variance of @xmath490 and the boundedness of @xmath30 $ ] , @xmath496=&\\,\\sum_{i=1}^n\\int_{\\omega_q}\\mathbb{e}\\left[\\left(l_h^*\\left(\\mathbf{x},\\mathbf{x}_i\\right)\\sigma(\\mathbf{x}_i)\\right)^2\\mathbb{e}\\left[\\varepsilon_i^2|\\mathbf{x}_i\\right]\\right]f(\\mathbf{x})w(\\mathbf{x})\\,\\omega_q(d\\mathbf{x})\\\\          = & \\,\\frac{1}{nh^{2q}\\lambda_q(l)^2}\\int_{\\omega_q}\\int_{\\omega_q } l^2\\left(\\frac{1-\\mathbf{x}^t\\mathbf{y}}{h^2}\\right ) \\frac{\\sigma^2(\\mathbf{y})w(\\mathbf{x})}{f(\\mathbf{x})}f(\\mathbf{y})\\,\\omega_q(d\\mathbf{y})\\,\\omega_q(d\\mathbf{x})\\\\",
    "= & \\,\\frac{1}{nh^{2q}\\lambda_q(l)^2}\\int_{\\omega_q}\\frac{1}{c_{h , q}(l^2 ) } \\sigma^2(\\mathbf{x})w(\\mathbf{x})\\,\\omega_q(d\\mathbf{x})(1+\\mathpzc{o}\\left(1\\right))\\\\          = & \\,\\frac{\\lambda_q(l^2)\\lambda_q(l)^{-2}}{nh^{q}}\\int_{\\omega_q } \\sigma^2(\\mathbf{x})w(\\mathbf{x})\\,\\omega_q(d\\mathbf{x})(1+\\mathpzc{o}\\left(1\\right)),\\\\                  \\mathbb{e}\\left[i^2\\right]=&\\,\\sum_{i=1}^n\\sum_{j=1}^n\\int_{\\omega_q}\\int_{\\omega_q}\\mathbb{e}\\left[\\left(l_h^*\\left(\\mathbf{x},\\mathbf{x}_i\\right)\\sigma(\\mathbf{x}_i)l_h^*\\left(\\mathbf{y},\\mathbf{x}_j\\right)\\sigma(\\mathbf{x}_j)\\right)^2\\mathbb{e}\\left[\\varepsilon_i^2\\varepsilon_j^2|\\mathbf{x}_i,\\mathbf{x}_j\\right]\\right]\\\\                  & \\times f(\\mathbf{x})f(\\mathbf{y})w(\\mathbf{x})w(\\mathbf{y})\\,\\omega_q(d\\mathbf{x})\\,\\omega_q(d\\mathbf{y})\\\\          = & \\,\\frac{1}{n^3h^{4q}\\lambda_q(l)^4}\\int_{\\omega_q}\\int_{\\omega_q}\\mathbb{e}\\left[l^2\\left(\\frac{1-\\mathbf{x}^t\\mathbf{x}}{h^2}\\right)l^2\\left(\\frac{1-\\mathbf{y}^t\\mathbf{x}}{h^2}\\right)\\sigma^4(\\mathbf{x})\\mathbb{e}\\left[\\varepsilon^4|\\mathbf{x}\\right]\\right ] \\\\                  & \\times\\frac{w(\\mathbf{x})w(\\mathbf{y})}{f(\\mathbf{x})f(\\mathbf{y})}\\,\\omega_q(d\\mathbf{x})\\,\\omega_q(d\\mathbf{y})\\\\          & + \\frac{1-n^{-1}}{n^2h^{4q}\\lambda_q(l)^4}\\bigg(\\int_{\\omega_q}\\int_{\\omega_q}l^2\\left(\\frac{1-\\mathbf{x}^t\\mathbf{y}}{h^2}\\right)\\frac{\\sigma^2(\\mathbf{y})w(\\mathbf{x})}{f(\\mathbf{x})}f(\\mathbf{y})\\,\\omega_q(d\\mathbf{x})\\,\\omega_q(d\\mathbf{y})\\bigg)^2\\\\          = & \\,\\mathcal{o}\\left((n^3h^{2q})^{-1}\\right)\\int_{\\omega_q}\\frac{\\sigma^4(\\mathbf{x})w(\\mathbf{x})^2}{f(\\mathbf{x})}\\,\\omega_q(d\\mathbf{x})+\\left(1-\\mathcal{o}\\left(n^{-1}\\right)\\right)\\mathbb{e}\\left[i\\right]^2\\\\          = & \\,\\mathcal{o}\\left((n^3h^{2q})^{-1}\\right)+\\left(1-\\mathcal{o}\\left(n^{-1}\\right)\\right)\\mathbb{e}\\left[i\\right]^2 .          \\end{aligned}\\ ] ] then @xmath497}}={\\mathcal{o}{\\left(}(n^3h^{2q})^{-1}{\\right)}}-{\\mathcal{o}\\lpn^{-1}{\\right)}}{\\mathbb{e}{\\left[}i{\\right]}}^2={\\mathcal{o}{\\left(}(n^3h^{2q})^{-1}{\\right)}}$ ] and as a consequence @xmath498    _ proof of [ gofreg : lem:5:5]_. the computation of @xmath499=&\\,\\frac{n^2h^q}{n^4h^{4q}\\lambda_q(l)^4}\\int_{\\omega_q}\\int_{\\omega_q } \\mathbb{e}\\bigg[l\\left(\\frac{1-\\mathbf{x}^t\\mathbf{x}}{h^2}\\right)l\\left(\\frac{1-\\mathbf{y}^t\\mathbf{x}}{h^2}\\right)\\sigma^2(\\mathbf{x})\\mathbb{e}\\left[\\varepsilon^2|\\mathbf{x}\\right]\\bigg]^2 \\nonumber\\\\                  & \\times\\frac{w(\\mathbf{x})w(\\mathbf{y})}{f(\\mathbf{x})f(\\mathbf{y})}\\,\\omega_q(d\\mathbf{x})\\,\\omega_q(d\\mathbf{y})\\nonumber\\\\                  = & \\,\\frac{1}{n^2h^{3q}\\lambda_q(l)^4}\\int_{\\omega_q}\\int_{\\omega_q}\\left[\\int_{\\omega_q } l\\left(\\frac{1-\\mathbf{x}^t\\mathbf{z}}{h^2}\\right)l\\left(\\frac{1-\\mathbf{y}^t\\mathbf{z}}{h^2}\\right)\\sigma^2(\\mathbf{z})f(\\mathbf{z})\\,\\omega_q(d\\mathbf{z})\\right]^2\\nonumber\\\\                  & \\times\\frac{w(\\mathbf{x})w(\\mathbf{y})}{f(\\mathbf{x})f(\\mathbf{y})}\\,\\omega_q(d\\mathbf{x})\\,\\omega_q(d\\mathbf{y})\\label{gofreg : lem:5:6:1 }          \\end{aligned}\\ ] ] is split in the cases where @xmath500 and @xmath86 . for the first one , the usual change of variables given by lemma [ gofreg : lem:2 ] is applied : @xmath501 because @xmath502 , it is possible also to consider an extra change of variables : @xmath503 where @xmath504 , @xmath505 , @xmath506 and @xmath507 is the semi - orthonormal matrix resulting from the completion of @xmath508 to the orthonormal basis @xmath509 of @xmath510 .",
    "this change of variables is obtained by a recursive use of lemma [ gofreg : lem:2 ] : @xmath511 where in the third equality a change of variables @xmath512 is used .",
    "the matrix @xmath513 of dimension @xmath514 can be interpreted as the one formed by the column vectors that complete the orthonormal set @xmath515 to an orthonormal basis in @xmath2 .",
    "+ if the changes of variables ( [ gofreg : lem:5:6:2 ] ) and ( [ gofreg : lem:5:6:3 ] ) is applied first , after that the changes @xmath516 and @xmath517}^{-1 } ,      \\end{array}{\\right.}\\quad { \\left| \\frac{\\partial(t,\\tau)}{\\partial(\\rho,\\theta)}\\right|}=h^3{\\left[\\rho(2-h^2\\rho)\\right]}^\\frac{1}{2}\\ ] ] are used and , denoting @xmath518}^\\frac{1}{2}{\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi},\\\\      { \\boldsymbol\\beta}_{{\\mathbf{x}},{\\boldsymbol\\xi}}&=-h^2\\rho{\\mathbf{x}}+h{\\left[\\rho(2-h^2\\rho)\\right]}^\\frac{1}{2}{\\left[\\theta { \\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi}+(1-\\theta^2)^\\frac{1}{2}{\\mathbf{b}}_{{\\mathbf{x}}}{\\mathbf{a}}_{{\\boldsymbol\\xi}}{\\boldsymbol\\eta}\\right ] } ,      \\end{aligned}\\ ] ] then the following result is obtained employing the dct ( see lemma 4 of @xcite for technical details in a similar situation ) : @xmath519 ^ 2\\nonumber\\\\              & \\times\\frac{w({\\mathbf{x}})w({\\mathbf{y}})}{f({\\mathbf{x}})f({\\mathbf{y}})}\\,\\omega_{q}(d { \\mathbf{x}})}\\,\\omega_{q}(d { \\mathbf{y}})}\\nonumber\\\\          = & \\,\\frac{1}{n^2h^{3q}\\lambda_q(l)^4}\\int_{-1}^1\\int_{{\\omega_{q-1}}}{\\int_{\\omega_{q } } \\bigg[\\iint_{t^2+\\tau^2<1}\\int_{{\\omega_{q-2 } } } l{\\left(\\frac{1-t}{h^2}\\right)}l{\\left(\\frac{1-st-\\tau(1-s^2)^\\frac{1}{2}}{h^2}\\right)}\\nonumber\\\\          & \\times\\sigma^2{\\left(t{\\mathbf{x}}+\\tau { \\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi}+(1-t^2-\\tau^2)^\\frac{1}{2}{\\mathbf{b}}_{{\\mathbf{x}}}{\\mathbf{a}}_{{\\boldsymbol\\xi}}{\\boldsymbol\\eta}\\right)}\\nonumber\\\\          & \\times f{\\left(t{\\mathbf{x}}+\\tau { \\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi}+(1-t^2-\\tau^2)^\\frac{1}{2}{\\mathbf{b}}_{{\\mathbf{x}}}{\\mathbf{a}}_{{\\boldsymbol\\xi}}{\\boldsymbol\\eta}\\right)}(1-t^2-\\tau^2)^\\frac{q-3}{2}\\,{\\omega_{q-2}}(d{\\boldsymbol\\eta})\\,dt\\,d\\tau\\bigg]^2\\nonumber\\\\          & \\times\\frac{w({\\mathbf{x}})w\\big(s{\\mathbf{x}}+(1-s^2)^\\frac{1}{2}{\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi}\\big)}{f({\\mathbf{x}})f\\big(s{\\mathbf{x}}+(1-s^2)^\\frac{1}{2}{\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi}\\big)}\\,\\omega_{q}(d { \\mathbf{x}})}(1-s^2)^{\\frac{q}{2}-1}\\,{\\omega_{q-1}}(d{\\boldsymbol\\xi})\\,ds \\nonumber\\\\          = & \\,\\frac{1}{n^2\\lambda_q(l)^4}\\int_{0}^{2h^{-2}}\\int_{{\\omega_{q-1}}}{\\int_{\\omega_{q } } \\bigg[\\int_0^{2h^{-2}}\\int_{-1}^1\\int_{{\\omega_{q-2 } } } l{\\left(\\rho\\right)}\\nonumber\\\\          & \\times l{\\left(r+\\rho - h^2r\\rho-\\theta{\\left[r\\rho(2-h^2r)(2-h^2\\rho)\\right]}^\\frac{1}{2}\\right)}\\sigma^2{\\left({\\mathbf{x}}+{\\boldsymbol\\beta}_{{\\mathbf{x}},{\\boldsymbol\\xi},{\\boldsymbol\\eta}}\\right)}f{\\left({\\mathbf{x}}+{\\boldsymbol\\beta}_{{\\mathbf{x}},{\\boldsymbol\\xi},{\\boldsymbol\\eta}}\\right)}\\nonumber\\\\          & \\times(1-\\theta^2)^\\frac{q-3}{2}\\rho^{\\frac{q}{2}-1}(2-h^2\\rho)^{\\frac{q}{2}-1 } \\,{\\omega_{q-2}}(d{\\boldsymbol\\eta})\\,dt\\,d\\tau\\bigg]^2\\frac{w({\\mathbf{x}})w\\big({\\mathbf{x}}+{\\boldsymbol\\alpha}_{{\\mathbf{x}},{\\boldsymbol\\xi}}\\big)}{f({\\mathbf{x}})f\\big({\\mathbf{x}}+{\\boldsymbol\\alpha}_{{\\mathbf{x}},{\\boldsymbol\\xi}}\\big)}\\nonumber\\\\          & \\times \\,\\omega_{q}(d { \\mathbf{x}})}\\,r^{\\frac{q}{2}-1}(2-h^2r)^{\\frac{q}{2}-1}\\,{\\omega_{q-1}}(d{\\boldsymbol\\xi})\\,dr\\nonumber\\\\          = & \\,\\frac{(1+{\\mathpzc{o}{\\left(}1{\\right)}})}{n^2\\lambda_q(l)^4}\\int_{0}^{\\infty}\\int_{{\\omega_{q-1}}}{\\int_{\\omega_{q } } \\bigg[\\int_0^{\\infty}\\int_{-1}^1\\int_{{\\omega_{q-2 } } } l{\\left(\\rho\\right)}l{\\left(r+\\rho-2\\theta(r\\rho)^\\frac{1}{2}\\right)}\\sigma^2{\\left({\\mathbf{x}}\\right)}f({\\mathbf{x}})\\\\          & \\times(1-\\theta^2)^\\frac{q-3}{2}\\rho^{\\frac{q}{2}-1}2^{\\frac{q}{2}-1}{\\omega_{q-2}}(d{\\boldsymbol\\eta})\\,dt\\,d\\tau\\bigg]^2\\frac{w{\\left({\\mathbf{x}}\\right)}^2}{f({\\mathbf{x}})^2}\\,\\omega_{q}(d { \\mathbf{x}})}\\,r^{\\frac{q}{2}-1}2^{\\frac{q}{2}-1}\\,{\\omega_{q-1}}(d{\\boldsymbol\\xi})\\,dr\\\\          = & \\,{\\left(1+{\\mathpzc{o}{\\left(}1{\\right)}}\\right)}\\frac{{\\omega_{q-1}}{\\omega_{q-2}}^22^{\\frac{3q}{2}-3}}{n^2\\lambda_q(l)^4}{\\int_{\\omega_{q } } \\sigma^4{\\left({\\mathbf{x}}\\right)}w({\\mathbf{x}})^2\\,\\omega_{q}(d { \\mathbf{x}})}\\\\      & \\times\\int_{0}^{\\infty}r^{\\frac{q}{2}-1}{\\left\\{\\int_0^{\\infty } \\rho^{\\frac{q}{2}-1}l{\\left(\\rho\\right)}\\int_{-1}^1 ( 1-\\theta^2)^\\frac{q-3}{2}l{\\left(r+\\rho-2\\theta(r\\rho)^\\frac{1}{2}\\right)}\\,d\\theta\\,d\\rho\\right\\}}^2\\,dr\\\\          = & \\,n^{-2}\\nu^2{\\left(1+{\\mathpzc{o}{\\left(}1{\\right)}}\\right)}.      \\end{aligned}\\ ] ]    for @xmath86 , define the change of variables : @xmath520 where @xmath521 . note that as @xmath86 and @xmath522 , then necessarily @xmath523 or @xmath524 . these changes of variables are applied first , later @xmath525 and finally @xmath516 , using that : @xmath526 finally , considering @xmath527}^\\frac{1}{2}{\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi},\\quad{\\boldsymbol\\beta}_{{\\mathbf{x}},{\\boldsymbol\\eta}}=-\\rho h^2{\\mathbf{x}}+{\\left[\\rho h^2(2-\\rho h^2)\\right]}^\\frac{1}{2}{\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\eta } ,      \\end{aligned}\\ ] ] it follows by the use of the dct : @xmath528 ^ 2\\nonumber\\\\      & \\times\\frac{w({\\mathbf{x}})w({\\mathbf{y}})}{f({\\mathbf{x}})f({\\mathbf{y}})}\\,{\\omega_{1}}(d{\\mathbf{x}})\\,{\\omega_{1}}(d{\\mathbf{y}})\\nonumber\\\\          = & \\,\\frac{1}{n^2h^{3}\\lambda_q(l)^4}\\int_{-1}^1\\int_{{\\omega_{0}}}\\int_{{\\omega_{1}}}\\bigg[\\int_{-1}^1\\int_{{\\omega_{0 } } } \\nonumber\\\\      & \\times l{\\left(\\frac{1-t}{h^2}\\right)}l{\\left(\\frac{1-st-(1-t^2)^\\frac{1}{2}(1-s^2)^\\frac{1}{2}({\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi})^t({\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\eta})}{h^2}\\right)}\\nonumber\\\\      & \\times \\sigma^2{\\left(t{\\mathbf{x}}+(1-t^2)^\\frac{1}{2}{\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi}\\right)}f{\\left(t{\\mathbf{x}}+(1-t^2)^\\frac{1}{2}{\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi}\\right)}(1-t^2)^{-\\frac{1}{2}}\\,{\\omega_{0}}(d{\\boldsymbol\\eta})\\,dt\\bigg]^2\\nonumber\\\\      & \\times\\frac{w({\\mathbf{x}})w\\big(s{\\mathbf{x}}+(1-s^2)^\\frac{1}{2}{\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi}\\big)}{f({\\mathbf{x}})f\\big(s{\\mathbf{x}}+(1-s^2)^\\frac{1}{2}{\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi}\\big ) } \\,{\\omega_{1}}(d{\\mathbf{x}})\\,(1-s^2)^{-\\frac{1}{2}}\\,{\\omega_{0}}(d{\\boldsymbol\\xi})\\,ds\\nonumber\\\\          = & \\,\\frac{1}{n^2\\lambda_q(l)^4}\\int_{0}^{2h^{-2}}\\int_{{\\omega_{0}}}\\int_{{\\omega_{1}}}\\bigg[\\int_{0}^{2h^{-2}}\\int_{{\\omega_{0}}}\\nonumber\\\\      & \\times l{\\left(\\rho\\right)}l{\\left(r+\\rho - h^2r\\rho-{\\left(r\\rho(2-h^2r)(2-h^2\\rho)\\right)}^\\frac{1}{2}({\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi})^t{\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\eta}\\right)}\\nonumber\\\\      & \\times \\sigma^2{\\left({\\mathbf{x}}+{\\boldsymbol\\beta}_{{\\mathbf{x}},{\\boldsymbol\\eta}}\\right)}f{\\left({\\mathbf{x}}+{\\boldsymbol\\beta}_{{\\mathbf{x}},{\\boldsymbol\\eta}}\\right)}\\rho^{-\\frac{1}{2}}(2-h^2\\rho)^{-\\frac{1}{2}}\\,{\\omega_{0}}(d{\\boldsymbol\\eta})\\,d\\rho\\bigg]^2\\frac{w({\\mathbf{x}})w\\big({\\mathbf{x}}+{\\boldsymbol\\alpha}_{{\\mathbf{x}},{\\boldsymbol\\xi}}\\big)}{f({\\mathbf{x}})f\\big({\\mathbf{x}}+{\\boldsymbol\\alpha}_{{\\mathbf{x}},{\\boldsymbol\\xi}}\\big)}\\nonumber\\\\      & \\times \\,{\\omega_{1}}(d{\\mathbf{x}})\\,r^{-\\frac{1}{2}}(2-h^2r)^{-\\frac{1}{2}}\\,{\\omega_{0}}(d{\\boldsymbol\\xi})\\,dr\\nonumber\\\\      = & \\,\\frac{2^{-1}{\\left(1+{\\mathpzc{o}{\\left(}1{\\right)}}\\right)}}{n^2\\lambda_q(l)^4}\\int_{0}^{\\infty}\\int_{{\\omega_{0}}}\\int_{{\\omega_{1}}}\\bigg[\\int_{0}^{\\infty}\\int_{{\\omega_{0 } } } l{\\left(\\rho\\right)}l{\\left(r+\\rho-2{\\left(r\\rho\\right)}^\\frac{1}{2}({\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\xi})^t{\\mathbf{b}}_{{\\mathbf{x}}}{\\boldsymbol\\eta}\\right)}\\nonumber\\\\      & \\times \\sigma^2{\\left({\\mathbf{x}}\\right)}f{\\left({\\mathbf{x}}\\right)}\\rho^{-\\frac{1}{2}}\\,{\\omega_{0}}(d{\\boldsymbol\\eta})\\,d\\rho\\bigg]^2\\frac{w({\\mathbf{x}})^2}{f({\\mathbf{x}})^2 } \\,{\\omega_{1}}(d{\\mathbf{x}})\\,r^{-\\frac{1}{2}}\\,{\\omega_{0}}(d{\\boldsymbol\\xi})\\,dr\\\\          = & \\,\\frac{{\\omega_{0}}2^{-\\frac{3}{2}}{\\left(1+{\\mathpzc{o}{\\left(}1{\\right)}}\\right)}}{n^2\\lambda_q(l)^4}\\int_{{\\omega_{1}}}\\sigma^4{\\left({\\mathbf{x}}\\right)}w({\\mathbf{x}})^2 \\,{\\omega_{1}}(d{\\mathbf{x}})\\\\      & \\times \\int_{0}^{\\infty}r^{-\\frac{1}{2}}\\bigg\\{\\int_{0}^{\\infty } \\rho^{-\\frac{1}{2}}l{\\left(\\rho\\right ) } { \\left[l{\\left(r+\\rho-2{\\left(r\\rho\\right)}^\\frac{1}{2}\\right)}+l{\\left(r+\\rho+2{\\left(r\\rho\\right)}^\\frac{1}{2}\\right)}\\right ] } \\,d\\rho\\bigg\\}^2\\,\\,dr\\\\          = & \\,n^{-2}\\nu^2{\\left(1+{\\mathpzc{o}{\\left(}1{\\right)}}\\right)}.      \\end{aligned}\\ ] ]    the rest of the results are provided by the recursive use of lemma [ gofreg : lem:4 ] , bearing in mind that the indexes are pairwise different : @xmath529\\\\          = & \\,\\frac{n^4h^{2q}}{n^8h^{8q}\\lambda_q(l)^8}\\int_{\\omega_q}\\times\\stackrel{4}{\\cdots}\\times\\int_{\\omega_q}\\mathbb{e}\\bigg[\\prod_{k=1}^4l\\left(\\frac{1-\\mathbf{x}_k^t\\mathbf{x}}{h^2}\\right)\\sigma^4(\\mathbf{x})\\mathbb{e}\\left[\\varepsilon^4|\\mathbf{x}\\right]\\bigg]^2\\prod_{k=1}^4\\frac{w(\\mathbf{x}_k)}{f(\\mathbf{x}_k)}\\,\\omega_q(d\\mathbf{x}_k)\\\\          = & \\,\\mathcal{o}\\left((n^{4}h^{4q})^{-1}\\right)\\int_{\\omega_q}\\times\\stackrel{4}{\\cdots}\\times\\int_{\\omega_q}\\prod_{k=2}^4l^2\\left(\\frac{1-\\mathbf{x}_k^t\\mathbf{x}}{h^2}\\right)\\sigma^8(\\mathbf{x}_1)f(\\mathbf{x}_1)\\prod_{k=1}^8\\frac{w(\\mathbf{x}_k)}{f(\\mathbf{x}_k)}\\,\\omega_q(d\\mathbf{x}_k)\\\\          = & \\,\\mathcal{o}\\left((n^{4}h^{q})^{-1}\\right),\\\\                  \\mathbb{e}\\big[w_{ijn}&w_{jkn}w_{kln}w_{lin}\\big]\\\\          = & \\,\\frac{n^4h^{2q}}{n^8h^{8q}\\lambda_q(l)^8}\\int_{\\omega_q}\\times\\stackrel{4}{\\cdots}\\times\\int_{\\omega_q }          \\mathbb{e}\\left[l\\left(\\frac{1-\\mathbf{x}_1^t\\mathbf{x}}{h^2}\\right)\\!l\\left(\\frac{1-\\mathbf{x}_4^t\\mathbf{x}}{h^2}\\right)\\!\\sigma^2(\\mathbf{x})\\right]\\\\          & \\times\\mathbb{e}\\left[l\\left(\\frac{1-\\mathbf{x}_1^t\\mathbf{x}}{h^2}\\right)\\!l\\left(\\frac{1-\\mathbf{x}_2^t\\mathbf{x}}{h^2}\\right)\\!\\sigma^2(\\mathbf{x})\\right]\\!\\mathbb{e}\\left[l\\left(\\frac{1-\\mathbf{x}_2^t\\mathbf{x}}{h^2}\\right)\\!l\\left(\\frac{1-\\mathbf{x}_3^t\\mathbf{x}}{h^2}\\right)\\!\\sigma^2(\\mathbf{x})\\right]\\\\          & \\times\\mathbb{e}\\left[l\\left(\\frac{1-\\mathbf{x}_3^t\\mathbf{x}}{h^2}\\right)\\!l\\left(\\frac{1-\\mathbf{x}_4^t\\mathbf{x}}{h^2}\\right)\\!\\sigma^2(\\mathbf{x})\\right]\\prod_{k=1}^8\\frac{w(\\mathbf{x}_k)}{f(\\mathbf{x}_k)}\\,\\omega_q(d\\mathbf{x}_k)\\\\          = & \\,\\mathcal{o}\\left((n^{4}h^{2q})^{-1}\\right)\\int_{\\omega_q}\\times\\stackrel{4}{\\cdots}\\times\\int_{\\omega_q}l\\left(\\frac{1-\\mathbf{x}_4^t\\mathbf{x}_1}{h^2}\\right)l\\left(\\frac{1-\\mathbf{x}_2^t\\mathbf{x}_1}{h^2}\\right)l\\left(\\frac{1-\\mathbf{x}_2^t\\mathbf{x}_3}{h^2}\\right)\\\\          & \\times l\\left(\\frac{1-\\mathbf{x}_4^t\\mathbf{x}_3}{h^2}\\right)\\sigma^4(\\mathbf{x}_1)\\sigma^4(\\mathbf{x}_3)\\frac{f(\\mathbf{x}_1)f(\\mathbf{x}_3 ) } { f(\\mathbf{x}_2)f(\\mathbf{x}_3)}\\prod_{k=1}^4w(\\mathbf{x}_k)\\,\\omega_q(d\\mathbf{x}_k)\\\\          = & \\,\\mathcal{o}\\left(n^{-4}h^{2q}\\right),\\\\                  \\mathbb{e}\\big[w_{ijn}&w^2_{ikn}w_{jkn}\\big]\\\\          = & \\,\\frac{n^4h^{2q}}{n^8h^{8q}\\lambda_q(l)^8}\\int_{\\omega_q}\\times\\stackrel{4}{\\cdots}\\times\\int_{\\omega_q }          \\mathbb{e}\\left[l\\left(\\frac{1-\\mathbf{x}_1^t\\mathbf{x}}{h^2}\\right)l\\left(\\frac{1-\\mathbf{x}_2^t\\mathbf{x}}{h^2}\\right)\\sigma^2(\\mathbf{x})\\right]\\\\          & \\times\\mathbb{e}\\left[l\\left(\\frac{1-\\mathbf{x}_1^t\\mathbf{x}}{h^2}\\right)l\\left(\\frac{1-\\mathbf{x}_3^t\\mathbf{x}}{h^2}\\right)l\\left(\\frac{1-\\mathbf{x}_4^t\\mathbf{x}}{h^2}\\right)\\sigma^3(\\mathbf{x})\\mathbb{e}\\left[\\varepsilon^3|\\mathbf{x}\\right]\\right]^2 \\prod_{k=1}^4\\frac{w(\\mathbf{x}_k)}{f(\\mathbf{x}_k)}\\,\\omega_q(d\\mathbf{x}_k)\\\\          = & \\,\\mathcal{o}\\left((n^{4}h^{3q})^{-1}\\right)\\int_{\\omega_q}\\times\\stackrel{4}{\\cdots}\\times\\int_{\\omega_q } l\\left(\\frac{1-\\mathbf{x}_2^t\\mathbf{x}_1}{h^2}\\right)l^2\\left(\\frac{1-\\mathbf{x}_3^t\\mathbf{x}_1}{h^2}\\right)l^2\\left(\\frac{1-\\mathbf{x}_4^t\\mathbf{x}_1}{h^2}\\right)\\\\          & \\times\\sigma^8(\\mathbf{x}_1)\\frac{f(\\mathbf{x}_1)^2}{f(\\mathbf{x}_2)f(\\mathbf{x}_3)f(\\mathbf{x}_4)}\\prod_{k=1}^4w(\\mathbf{x}_k)\\,\\omega_q(d\\mathbf{x}_k)\\\\          = & \\,\\mathcal{o}\\left(n^{-4}\\right ) .",
    "\\end{aligned}\\ ] ]    [ gofreg : lem:6 ] under assumptions [ gofreg : assump : a1][gofreg : assump : a6n ] and [ gofreg : assump : a9n ] , for a random sample @xmath348 the following statements hold :    1 .",
    "@xmath530 + @xmath531 .",
    "[ gofreg : lem:6:1 ] 2 .",
    "@xmath532 ^ 2=2\\nu_{{\\boldsymbol\\theta}_1}^2(1+{\\mathpzc{o}_\\mathbb{p}{\\left(}1{\\right)}})$ ] , + @xmath533={\\mathcal{o}_\\mathbb{p}\\lpn^{-4}h^{2q}{\\right)}}$ ] , @xmath534={\\mathcal{o}_\\mathbb{p}{\\left(}(n^4h^{q})^{-1}{\\right)}}$ ] and @xmath535 $ ] @xmath536 .",
    "[ gofreg : lem:6:2 ]    the proof is divided in the evaluation of each statement .",
    "+ _ proof of [ gofreg : lem:6:1]_. using that the @xmath192 s are iid and independent with respect to the sample , @xmath537\\nonumber\\\\      = & \\,{\\int_{\\omega_{q } } \\sum_{i=1}^nw_n^{p}({\\mathbf{x}},{\\mathbf{x}}_i)^2(y_i - m_{\\hat{\\boldsymbol\\theta}}({\\mathbf{x}}_i))^2\\hat f_h({\\mathbf{x}})w({\\mathbf{x}})\\,\\omega_{q}(d { \\mathbf{x}})}\\nonumber\\\\      = & \\,{\\int_{\\omega_{q } } \\sum_{i=1}^nl_h^*({\\mathbf{x}},{\\mathbf{x}}_i)^2(y_i - m_{{\\boldsymbol\\theta}_1}({\\mathbf{x}}_i))^2 f({\\mathbf{x}})w({\\mathbf{x}})\\,\\omega_{q}(d { \\mathbf{x}})}(1+{\\mathpzc{o}_\\mathbb{p}{\\left(}1{\\right)}})\\label{gofreg : lem:6:1:1 }      \\end{aligned}\\ ] ] where the last equality holds because by assumptions [ gofreg : assump : a5n ] and [ gofreg : assump : a6n ] , @xmath538 uniformly in @xmath43 . by applying the tower property of the conditional expectation as in [ gofreg : lem:1:d1c ] from lemma [ gofreg : lem:1 ] , it is easy to derive from [ gofreg : lem:5:3 ] in lemma [ gofreg : lem:5 ] that @xmath539 the order of the variance is obtained applying the same idea , i.e. , first deriving the variance with respect to the @xmath192 s and then applying the order computation given in the proof of [ gofreg : lem:5:3 ] in lemma [ gofreg : lem:5 ] ( adapted via the conditional expectation ) : @xmath540\\\\      = & \\,\\sum_{i=1}^n\\bigg({\\int_{\\omega_{q } } l_h^*({\\mathbf{x}},{\\mathbf{x}}_i)^2(y_i - m_{\\hat{\\boldsymbol\\theta}}({\\mathbf{x}}_i))^2f({\\mathbf{x}})w({\\mathbf{x}})\\,\\omega_{q}(d { \\mathbf{x}})}\\bigg)^2\\mathbb{v}\\mathrm{ar}^*{\\left[v_i^{*2}\\right]}\\\\      = & \\,{\\mathcal{o}{\\left(}(n^4h^{4q})^{-1}{\\right)}}\\sum_{i=1}^n\\bigg({\\int_{\\omega_{q } } l^2{\\left(\\frac{1-{\\mathbf{x}}^t{\\mathbf{x}}_i}{h^2}\\right)}(y_i - m_{{\\boldsymbol\\theta}_1}({\\mathbf{x}}_i))^2\\frac{w({\\mathbf{x}})}{f({\\mathbf{x}})}\\,\\omega_{q}(d { \\mathbf{x}})}\\bigg)^2\\\\      = & \\,{\\mathcal{o}_\\mathbb{p}{\\left(}(n^3h^{2q})^{-1}{\\right)}}.      \\end{aligned}\\ ] ] the statement holds by chebychev s inequality with respect to the probability law @xmath201 .",
    "+ _ proof of [ gofreg : lem:6:2]_. first , by corollary [ gofreg : coro : equiv ] , the expansion for the kernel density estimate and the fact @xmath538 uniformly in @xmath43 , we have @xmath541}^2\\\\      = & \\,2\\sum_{i\\neq j } i_{ijn}(1+{\\mathpzc{o}_\\mathbb{p}{\\left(}1{\\right ) } } ) ,      \\end{aligned}\\ ] ] where @xmath542 by the tower property of the conditional expectation and [ gofreg : lem:5:3 ] in lemma [ gofreg : lem:5 ] , @xmath543}}=\\mathbb{e}\\big[w^2_{ijn}\\big]=n^{-2}\\nu_{{\\boldsymbol\\theta}_1}^2(1+{\\mathpzc{o}{\\left(}1{\\right)}})$ ] ( considering that the @xmath314 s are defined with respect to @xmath131 instead of @xmath119 ) . to prove that @xmath544 , consider @xmath545 and , by ( [ gofreg : vwn ] ) and ( [ gofreg : ewn4 ] ) , @xmath546}}=&\\,\\mathbb{e}\\bigg[\\big(2\\sum_{i\\neq j}i_{ijn}\\big)^2\\bigg]-4n^2(n-1)^2{\\mathbb{e}{\\left[}i_{ijn}{\\right]}}^2\\\\      & \\,=4\\sum_{i\\neq j}\\sum_{k\\neq l}{\\mathbb{e}{\\left[}w_{ijn}^2w_{kln}^2{\\right]}}-4n^2(n-1)^2{\\mathbb{e}{\\left[}w_{ijn}^2{\\right]}}^2\\\\      & \\,=\\frac{1}{3}{\\mathbb{e}{\\left[}w_n^4{\\right]}}-{\\mathbb{v}\\mathrm{ar}{\\left[}w_{n}{\\right]}}^2+{\\mathpzc{o}{\\left(}1{\\right)}}\\\\      & \\,={\\mathbb{v}\\mathrm{ar}{\\left[}w_{n}{\\right]}}^2{\\left(\\frac{1}{3}{\\mathbb{v}\\mathrm{ar}{\\left[}w_{n}{\\right]}}^{-2}{\\mathbb{e}{\\left[}w_n^4{\\right]}}-1\\right)}+{\\mathpzc{o}{\\left(}1{\\right)}}\\\\      & \\,=2\\nu_{{\\boldsymbol\\theta}_1}^2(1+{\\mathpzc{o}{\\left(}1{\\right)}}){\\mathpzc{o}{\\left(}1{\\right)}}+{\\mathpzc{o}{\\left(}1{\\right)}}\\\\      & \\,={\\mathpzc{o}{\\left(}1{\\right ) } } ,      \\end{aligned}\\ ] ] because , as it was shown in the proof of theorem [ gofreg : theo : asympnorm ] , conditions [ gofreg : dj:2 ] and [ gofreg : dj:4 ] hold",
    ". then , @xmath547 $ ] converges to zero in squared mean , which implies that it converges in probability and therefore @xmath548 + 2\\nu^2_{{\\boldsymbol\\theta}_1}+{\\mathpzc{o}{\\left(}1{\\right)}}\\right)}(1+{\\mathpzc{o}_\\mathbb{p}{\\left(}1{\\right)}})=2\\nu^2_{{\\boldsymbol\\theta}_1}+{\\mathpzc{o}_\\mathbb{p}{\\left(}1{\\right)}},\\ ] ] which proofs the first statement .",
    "+ second , it follows straightforwardly that @xmath534=\\mathcal{o}_\\mathbb{p}\\big(w_{ijn}^{4}\\big)$ ] , @xmath533=$]@xmath549 and @xmath550=\\mathcal{o}_\\mathbb{p}\\left(w_{ijn}w_{ikn}^{2}w_{jkn}\\right)$ ] .",
    "the idea now is to use that , for a random variable @xmath551 and by the markov s inequality , @xmath552+\\mathcal{o}_\\mathbb{p}\\left(\\mathbb{e}\\left[|x_n|\\right]\\right)$ ] .",
    "the expectations of the variables are given in [ gofreg : lem:5:5 ] from lemma [ gofreg : lem:5 ] .",
    "the orders of the absolute expectations are the same : in the definition of @xmath314 the only factor with sign is @xmath553 , which is handled by the assumption of boundedness of @xmath29 $ ] .",
    "therefore , @xmath554 , @xmath555 and @xmath556 , so the statement is proved .",
    "[ gofreg : lem:2 ] let @xmath7 be a function defined in @xmath4 and @xmath43 .",
    "then @xmath557 where @xmath458 is the projection matrix given in section [ gofreg : sec : reg ] .",
    "see lemma 2 of @xcite .",
    "[ gofreg : lem:3 ] set @xmath558 . for all @xmath559 , @xmath560 , @xmath561 and @xmath562 .",
    "apply lemma [ gofreg : lem:2 ] considering @xmath563 .",
    "then @xmath564 as the integrand is an odd function . as a consequence , and applying the same change of variables , for @xmath335 : @xmath565 for @xmath566 , @xmath567 . for the trivariate case ,",
    "@xmath568 using that the integrand is odd and the first statement .",
    "[ gofreg : lem:4 ] let @xmath569 be a continuous function and denote @xmath570 . under assumptions [ gofreg : assump :",
    "a3][gofreg : assump : a4 ] , @xmath571 , where the remaining order is uniform for any @xmath43 .",
    "this corresponds to lemma 5 in @xcite , but with slightly different conditions and notation .",
    "assumptions [ gofreg : assump : a1 ] and [ gofreg : assump : a3 ] imply conditions ( a ) , ( b ) , ( c@xmath572 ) and ( d ) stated in theorem 1 of the aforementioned paper .",
    "some extra simulation results are given to provide a better understanding of the design of the simulation study presented in the paper and a deeper insight into the empirical performance of the goodness - of - fit tests for different significance levels and sample sizes .",
    "+ graphical representations of the densities considered for the directional predictor @xmath158 are shown in figure [ gofreg : fig : dens ] .",
    "these densities aim to capture simple designs like the uniform and more challenging ones with _",
    "holes _ in the support .",
    "the deviations from the null hypothesis , @xmath573 and @xmath574 , are shown in figure [ gofreg : fig : devs ] , jointly with the conditional standard deviation function used to generate data with heteroskedastic noise .",
    "+    , title=\"fig:\",scaledwidth=22.5% ] , title=\"fig:\",scaledwidth=22.5% ] , title=\"fig:\",scaledwidth=22.5% ] , title=\"fig:\",scaledwidth=22.5% ] + , title=\"fig:\",scaledwidth=22.5% ] , title=\"fig:\",scaledwidth=22.5% ] , title=\"fig:\",scaledwidth=22.5% ] , title=\"fig:\",scaledwidth=22.5% ]     and @xmath574 and conditional standard deviation function @xmath575 for circular and spherical cases .",
    "[ gofreg : fig : devs],title=\"fig:\",scaledwidth=22.5% ]   and @xmath574 and conditional standard deviation function @xmath575 for circular and spherical cases .",
    "[ gofreg : fig : devs],title=\"fig:\",scaledwidth=22.5% ]        and @xmath574 and conditional standard deviation function @xmath575 for circular and spherical cases .",
    "[ gofreg : fig : devs],title=\"fig:\",scaledwidth=22.5% ]   and @xmath574 and conditional standard deviation function @xmath575 for circular and spherical cases .",
    "[ gofreg : fig : devs],title=\"fig:\",scaledwidth=22.5% ]    the coefficients @xmath576 for obtaining deviations @xmath577 and @xmath578 in each scenario were chosen such that the density of the response @xmath579 under @xmath113 ( @xmath268 ) and under @xmath129 ( @xmath580 ) were similar .",
    "figure [ gofreg : fig : densdevs ] shows the densities of @xmath19 under the null and the alternative for the four scenarios and dimensions considered . this is a graphical way of ensuring that the deviation is not trivial to detect and",
    "hence is not straightforward to reject @xmath113 .",
    "note that , due to the design of the deviations and its pairing with the regression functions , design densities and kind of noises , it may be harder to detect them on a particular situation .",
    "this is what happens for example in s4 for @xmath372 : due to the design density , most of the observations happen close to the north pole , where the shape of the parametric model and of @xmath574 are similar , resulting in a harder detectable deviation for that dimension .",
    "a different situation happens for s1 , where the heteroskedastic noise masks the deviation @xmath573 for moderate and large values of the smoothing parameter @xmath184 .",
    "+ the empirical sizes of the test for significance levels @xmath581 are given in figures [ gofreg : fig : size:1 ] , [ gofreg : fig : size:2 ] and [ gofreg : fig : size:3 ] , corresponding to sample sizes @xmath582 and @xmath583 .",
    "nominal levels are respected in most scenarios .",
    "finally , the empirical powers for @xmath582 and @xmath583 are given in figure [ gofreg : fig : pow:2 ] and , as it can be seen , the rejection rates increase with @xmath163 .     under the null ( solid line ) and under the alternative ( dashed line ) for scenarios s1 to s4 ( columns , from left to right ) and dimensions @xmath584 ( rows , from top to bottom).[gofreg : fig : densdevs],title=\"fig:\",scaledwidth=24.0% ]   under the null ( solid line ) and under the alternative ( dashed line ) for scenarios s1 to s4 ( columns , from left to right ) and dimensions @xmath584 ( rows , from top to bottom).[gofreg : fig : densdevs],title=\"fig:\",scaledwidth=24.0% ]   under the null ( solid line ) and under the alternative ( dashed line ) for scenarios s1 to s4 ( columns , from left to right ) and dimensions @xmath584 ( rows , from top to bottom).[gofreg : fig : densdevs],title=\"fig:\",scaledwidth=24.0% ]   under the null ( solid line ) and under the alternative ( dashed line ) for scenarios s1 to s4 ( columns , from left to right ) and dimensions @xmath584 ( rows , from top to bottom).[gofreg : fig : densdevs],title=\"fig:\",scaledwidth=24.0% ] +   under the null ( solid line ) and under the alternative ( dashed line ) for scenarios s1 to s4 ( columns , from left to right ) and dimensions @xmath584 ( rows , from top to bottom).[gofreg : fig : densdevs],title=\"fig:\",scaledwidth=24.0% ]   under the null ( solid line ) and under the alternative ( dashed line ) for scenarios s1 to s4 ( columns , from left to right ) and dimensions @xmath584 ( rows , from top to bottom).[gofreg : fig : densdevs],title=\"fig:\",scaledwidth=24.0% ]   under the null ( solid line ) and under the alternative ( dashed line ) for scenarios s1 to s4 ( columns , from left to right ) and dimensions @xmath584 ( rows , from top to bottom).[gofreg : fig : densdevs],title=\"fig:\",scaledwidth=24.0% ]   under the null ( solid line ) and under the alternative ( dashed line ) for scenarios s1 to s4 ( columns , from left to right ) and dimensions @xmath584 ( rows , from top to bottom).[gofreg : fig : densdevs],title=\"fig:\",scaledwidth=24.0% ] +   under the null ( solid line ) and under the alternative ( dashed line ) for scenarios s1 to s4 ( columns , from left to right ) and dimensions @xmath584 ( rows , from top to bottom).[gofreg : fig : densdevs],title=\"fig:\",scaledwidth=24.0% ]   under the null ( solid line ) and under the alternative ( dashed line ) for scenarios s1 to s4 ( columns , from left to right ) and dimensions @xmath584 ( rows , from top to bottom).[gofreg : fig : densdevs],title=\"fig:\",scaledwidth=24.0% ]   under the null ( solid line ) and under the alternative ( dashed line ) for scenarios s1 to s4 ( columns , from left to right ) and dimensions @xmath584 ( rows , from top to bottom).[gofreg : fig : densdevs],title=\"fig:\",scaledwidth=24.0% ]   under the null ( solid line ) and under the alternative ( dashed line ) for scenarios s1 to s4 ( columns , from left to right ) and dimensions @xmath584 ( rows , from top to bottom).[gofreg : fig : densdevs],title=\"fig:\",scaledwidth=24.0% ]     ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath587 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:1],title=\"fig:\",scaledwidth=32.0% ]   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath587 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:1],title=\"fig:\",scaledwidth=32.0% ]   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath587 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:1],title=\"fig:\",scaledwidth=32.0% ] +   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath587 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:1],title=\"fig:\",scaledwidth=32.0% ]   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath587 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:1],title=\"fig:\",scaledwidth=32.0% ]   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath587 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:1],title=\"fig:\",scaledwidth=32.0% ] +   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right ,",
    "columns represent dimensions @xmath584 with sample size @xmath587 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:1],title=\"fig:\",scaledwidth=32.0% ]   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath587 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:1],title=\"fig:\",scaledwidth=32.0% ]   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath587 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:1],title=\"fig:\",scaledwidth=32.0% ]     ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left",
    "to right , columns represent dimensions @xmath584 with sample size @xmath588 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:2],title=\"fig:\",scaledwidth=32.0% ]   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left",
    "to right , columns represent dimensions @xmath584 with sample size @xmath588 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:2],title=\"fig:\",scaledwidth=32.0% ]   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath588 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:2],title=\"fig:\",scaledwidth=32.0% ] +   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath588 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:2],title=\"fig:\",scaledwidth=32.0% ]   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath588 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:2],title=\"fig:\",scaledwidth=32.0% ]   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath588 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:2],title=\"fig:\",scaledwidth=32.0% ] +   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath588 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:2],title=\"fig:\",scaledwidth=32.0% ]   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath588 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:2],title=\"fig:\",scaledwidth=32.0% ]   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath588 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:2],title=\"fig:\",scaledwidth=32.0% ]     ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath589 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:3],title=\"fig:\",scaledwidth=32.0% ]   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath589 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:3],title=\"fig:\",scaledwidth=32.0% ]   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath589 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:3],title=\"fig:\",scaledwidth=32.0% ] +   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath589 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:3],title=\"fig:\",scaledwidth=32.0% ]   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath589 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:3],title=\"fig:\",scaledwidth=32.0% ]   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath589 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:3],title=\"fig:\",scaledwidth=32.0% ] +   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath589 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:3],title=\"fig:\",scaledwidth=32.0% ]   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath589 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:3],title=\"fig:\",scaledwidth=32.0% ]   ( first row ) , @xmath585 ( second row ) and @xmath586 ( third row ) for the different scenarios , with @xmath63 ( solid line ) and @xmath64 ( dashed line ) . from left to right , columns represent dimensions @xmath584 with sample size @xmath589 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : size:3],title=\"fig:\",scaledwidth=32.0% ]     ( solid line ) and @xmath64 ( dashed line ) . from top to bottom , rows represent sample sizes @xmath590 and from left to right , columns represent dimensions @xmath584 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : pow:2],title=\"fig:\",scaledwidth=32.0% ]   ( solid line ) and @xmath64 ( dashed line ) . from top to bottom , rows represent sample sizes @xmath590 and from left to right , columns represent dimensions @xmath584 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : pow:2],title=\"fig:\",scaledwidth=32.0% ]   ( solid line ) and @xmath64 ( dashed line ) . from top to bottom , rows represent sample sizes @xmath590 and from left to right , columns represent dimensions @xmath584 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : pow:2],title=\"fig:\",scaledwidth=32.0% ] +   ( solid line ) and @xmath64 ( dashed line ) . from top to bottom , rows represent sample sizes @xmath590 and from left to right , columns represent dimensions @xmath584 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : pow:2],title=\"fig:\",scaledwidth=32.0% ]   ( solid line ) and @xmath64 ( dashed line ) . from top to bottom , rows represent sample sizes @xmath590 and from left to right , columns represent dimensions @xmath584 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : pow:2],title=\"fig:\",scaledwidth=32.0% ]   ( solid line ) and @xmath64 ( dashed line ) . from top to bottom , rows represent sample sizes @xmath590 and from left to right , columns represent dimensions @xmath584 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : pow:2],title=\"fig:\",scaledwidth=32.0% ] +   ( solid line ) and @xmath64 ( dashed line ) . from top to bottom , rows represent sample sizes @xmath590 and from left to right , columns represent dimensions @xmath584 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : pow:2],title=\"fig:\",scaledwidth=32.0% ]   ( solid line ) and @xmath64 ( dashed line ) . from top to bottom , rows represent sample sizes @xmath590 and from left to right , columns represent dimensions @xmath584 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : pow:2],title=\"fig:\",scaledwidth=32.0% ]   ( solid line ) and @xmath64 ( dashed line ) . from top to bottom , rows represent sample sizes @xmath590 and from left to right , columns represent dimensions @xmath584 .",
    "green , blue , red and orange colors correspond to scenarios s1 to s4 , respectively .",
    "[ gofreg : fig : pow:2],title=\"fig:\",scaledwidth=32.0% ]"
  ],
  "abstract_text": [
    "<S> this paper presents a goodness - of - fit test for parametric regression models with scalar response and directional predictor , that is , vectors in a sphere of arbitrary dimension . </S>",
    "<S> the testing procedure is based on the weighted squared distance between a smooth and a parametric regression estimator , where the smooth regression estimator is obtained by a projected local approach . </S>",
    "<S> asymptotic behavior of the test statistic under the null hypothesis and local alternatives is provided , jointly with a consistent bootstrap algorithm for application in practice . </S>",
    "<S> a simulation study illustrates the performance of the test in finite samples . </S>",
    "<S> the procedure is also applied to a real data example from text mining .    </S>",
    "<S> * keywords : * local linear regression ; goodness - of - fit test ; directional data ; bootstrap calibration . </S>"
  ]
}