{
  "article_text": [
    "statistical - relational learning ( srl ) models have mostly been developed for discrete data ( see  @xcite for general overviews ) .",
    "an important reason for this lies in the fact that inference for hybrid models combining discrete and continuous variables quickly lead to inference problems that consist of integration problems for which no closed - form solutions are available . among the relatively few proposals for srl frameworks with continuous variables",
    "are hybrid markov logic networks  @xcite , hybrid problog  @xcite , and hybrid dependency networks  @xcite . in these",
    "works the complexity of the inference problem is addressed by focussing on approximate , sampling based methods  @xcite , or by imposing significant restrictions on the models , so that the required integration tasks for exact inference become solvable  @xcite .    in the first part of this paper",
    "we first take a closer look at the semantic and statistical roles that continuous variables can play in a probabilistic relational model .",
    "we arrive at a main distinction between numeric input relations , and numeric probabilistic relations , and we argue that for many modeling and learning problems involving numeric data , only numeric input relations are needed . we then proceed to show how numeric input relations can be integrated into the relational bayesian network ( rbn ) language  @xcite , with little or no cost in terms of algorithmic developments or computational complexity .",
    "the second part of the paper demonstrates by several examples and applications the usefulness of modeling with numeric input relations , and the feasibility of the associated learning problems .",
    "first , a synthetic environmental modeling example shows how rbns with numeric input relations support natural and interpretable models that provide a relational extension for traditional statistical models ( section  [ sec : water ] ) .",
    "we then turn to community detection in ( social ) networks as our main application .",
    "utilizing a general srl modeling language allows us to encode a variety of probabilistic network models on a single platform with a single generic inference and learning engine .",
    "we use rbns with numeric input relations to encode probabilistic models with continuous latent features representing community structure .",
    "the srl framework makes it easy to develop models for multi - relational networks ( a.k.a multiplex or multi - layer networks ) , where nodes are connected by more than one type of link . in such networks",
    ", it will usually no longer be possible to reduce community structure detection to a form of graph partitioning  @xcite , because different relations may define a multitude of different , overlapping , and partly conflicting community structures .",
    "we therefore propose a latent feature model that allows us to identify a number of communities with no restrictions on how communities are related in terms of inclusion or disjointness .",
    "furthermore , for each community we obtain a characterization of how they are defined in terms of the given network relations , and we are able to define a probabilistic significance measure that ranks the detected communities in terms of their explanatory value .",
    "last but not least , we obtain for each node in the network , and each community , a _ community centrality degree ( ccd)_. unlike most previously defined soft or fuzzy community membership degrees , these _ ccd _ values are not normalized to sum up to one over the different communities .",
    "they thereby allow us , for example , to identify influential _ hub _",
    "nodes  @xcite between communities ( nodes with high centrality degree for multiple communities ) .",
    "a srl model defines a probability distribution over relational structures .",
    "a model can be instantiated over different input domains , which may consist only of a set of objects , or , more generally , a set of objects together with a set of known input relations . thus",
    ", a srl model defines conditional probability distributions @xmath0 where @xmath1  ranges over a class of domains ( usually the class of all finite sets ) , @xmath2  ranges over interpretations over @xmath1  of a set of input relations @xmath3 , and @xmath4  ranges over interpretations of a set of probabilistic ( or output ) relations @xmath5 .",
    "interpretations @xmath2  and @xmath4  are given as value assignment to ground atoms @xmath6   ( @xmath7 ) . in the discrete case , each relation @xmath8  has an associated finite range of possible values .",
    "the distinction between input and probabilistic relation need not be explicitly defined in a given model .",
    "input relations can also be seen as probabilistic relations that are fully instantiated as evidence in a given inference or learning problem .",
    "markov logic networks ( mlns )  @xcite are one prominent framework in which there is only such an implicit distinction between input and probabilistic relations .",
    "hybrid srl models allow the introduction of numeric relations , so that atoms @xmath6  become real - valued variables .",
    "based on , one can distinguish numerical input and numerical probabilistic relations . to obtain a clearer view of the implications of this distinction ,",
    "consider a purely continuous , classical linear regression model : @xmath9 this model contains three different types of numerical variates : @xmath10 , the _ response variable _ , is a random variable with a gaussian distribution .",
    "@xmath11 , the _ predictor variables _ may be random variables themselves , or they may be non - probabilistic inputs whose values have to be instantiated before inferences about @xmath10  can be made . finally , @xmath12  are _ parameters _ of the model .",
    "the functional specification is completely symmetric for the predictor variables @xmath11  and the parameters @xmath13 .",
    "the conceptual difference between the two only becomes apparent when one considers repeated random samples @xmath14 .",
    "these samples would usually be drawn with varying values @xmath15  for the predictor variables , whereas the parameters @xmath13  remain constant .    in srl",
    ", data does not usually consist of iid samples , and one learns from a single observed pair @xmath16 .",
    "the distinction we can make in between @xmath11  and @xmath13 , therefore , is no longer supported .",
    "that means that in numeric input atoms @xmath6  can be equally interpreted as predictor variables , or as object - specific parameters .",
    "neither of these views requires to define @xmath6  as a random variable with an associated probability distribution : as long as is used purely as a conditional model , no prior distribution for numeric input relations is needed .",
    "such a model will not define ( posterior ) probability distributions for numerical atoms , but still support maximum likelihood inference for the numerical atoms , which depending on the interpretation of the input relations can be seen as mpe inference for unobserved predictor variables , or as estimation of object - specific parameters .    the clear focus of hybrid problog  @xcite is to introduce numeric probabilistic relations .",
    "the language provides constructs to explicitly define distributions of numeric atoms as gaussian with specified mean and standard deviation , for example .",
    "the nature of hybrid mlns  @xcite is a little less clear - cut , due to the only implicit distinction between input and output relations .",
    "hybrid mlns extend standard mlns by _ numeric properties _ ( which we can identify with numeric relations in our terminology ) from which weighted numeric features can be constructed .",
    "examples of weighted features that can be included in a hybrid mln then are @xmath17 a ground instance of a weighted feature contributes a weight to a possible world @xmath18  ( i.e. an interpretation of all discrete and numerical relations over a given domain ) that is equal to the value of the ground feature multiplied with the weight of the feature .",
    "the probability of @xmath18  then is given by @xmath19 , where @xmath20  is the sum of weights from all groundings of all features , and @xmath21  a normalization constant  @xcite .",
    "this definition , however , requires that a finite normalization constant @xmath21  can be found , which means that @xmath22  must be finite , where the integral represents integration over all numeric properties , and summation over all discrete relations .",
    "this normalization is not possible , for example , for an mln only consisting of the weighted feature with @xmath23 .",
    "no probability distribution for the _ distance _ property then is defined . for an mln consisting of , on the other hand , normalization is possible , and a gaussian distribution for the ground _ length",
    "_ atoms is defined .",
    "when a hybrid mln contains numeric properties that prevent normalization , then no probabilistic inference for these properties is possible , and they either have to be instantiated to perform probabilistic inference for other properties and relations , or one has to use the model for mpe inference tasks . in summary ,",
    "hybrid mlns support numeric probabilistic relations under the condition that a finite normalization constant can be computed ; otherwise they support numeric input relations for which no distribution is defined .",
    "the rbn language is based on _ probability formulas _ that define the probability @xmath24  for ground relational atoms @xmath25 .",
    "the language of probability formulas is defined by a parsimonious grammar that is based on the two main constructs of _ convex combinations _ and _ combination functions_. the following are two examples of the convex combination construct . to improve the readability and understandability of the formulas , we here use a modification of the original very compact syntax of  @xcite , and write convex combinations in the form of `` wif - then - else '' statements ( `` wif '' stands for `` weighted - if '' ) .",
    "@xmath26    formula defines the probability of a coin - toss to come up heads as 0.5 if a fair coin is tossed , and 0.7 otherwise . here",
    "the formula in the wif - clause is an ordinary boolean condition . in formula",
    "the wif - clause is a numerical mixture coefficient .",
    "the formula thereby defines the probability that @xmath27  has cancer as a mixture of a contribution coming from a genetic predisposition ( mixture weight 0.3 ) , and a base rate of 0.1 ( mixture weight 1 - 0.3 ) .",
    "generally a formula wif @xmath28  then @xmath29  else @xmath30  is evaluated over a concrete input domain to a probability value @xmath31a@xmath32b@xmath33c@xmath34  as @xmath35 .",
    "there are two features in this modeling approach that make an integration of numerical input relations extremely easy : first , logical input relations already are interpreted numerically : for example , @xmath36  is defined as 0 or 1 , depending on whether @xmath37  is false or true .",
    "second , according to the grammar of probability formulas , numerical constants and logical atoms are just different base cases for probability ( sub- ) formulas , which can be used interchangeably in the construction of more complex formulas .    the generalization from boolean to numerical relations , thus , is almost trivial : one can just allow relational atoms @xmath25  to evaluate to real values @xmath38  in any range @xmath39 $ ] , where @xmath40  depend on the intended meaning of @xmath8 .",
    "the only additional modification one has to make is to ensure that in the end probability formulas defining the probability for a boolean response variable return values in the interval @xmath41 $ ] .",
    "we do this by using the rbn combination function construct , which generally take multisets of ( probability ) values as inputs , and return a single probability value .",
    "in particular , we can define _ logistic regression _ as a rbn combination function .",
    "an example of a rbn model with numeric input relations and logistic regression combination function then is :    @xmath42    again we here use a slightly more verbose version of the combination function syntax than the original one .",
    "formula defines for a person @xmath28  the probability of getting cancer using the logistic regression function applied to the set of all intensity values of radiation sources @xmath43  that @xmath28  was exposed to .",
    "thus , assuming that the numerical attribute _ intensity _ , and the logical relation _ exposed _ are known , the probability _",
    "p(cancer(a ) ) _ evaluates to @xmath44 , where @xmath45 .",
    "probabilistic inference for rbns with numerical input relations is no different from inference in a purely boolean setting .",
    "all inference approaches that have previously been used for rbns ( i.e. , compilation to bayesian networks or arithmetic circuits , importance sampling ) can still be used without modifications .",
    "[ c]@xmath46        for learning the values of numerical relations , we use a slightly generalized version of the gradient graph that was introduced in  @xcite for parameter learning in rbns .",
    "the resulting _ likelihood graph _ data structure is illustrated in figure  [ fig : likgraph ] .",
    "the likelihood graph is a computational structure related to arithmetic circuits .",
    "each node of the graph represents a function of the inputs in the bottom layer of the graph : model parameters ( e ) , values of ground atoms in the numerical input relations ( f ) , and truth values of ground probabilistic atoms that are unobserved in the data ( g ) .",
    "the topmost layer of nodes in the graph corresponds to ground probabilistic atoms that are instantiated in the data ( a ) , or that are unobserved and need to be marginalized out for the computation of the likelihood ( b ) ( there is a one - to - one correspondence between the nodes in ( b ) and ( g ) ) .",
    "the function associated with the ground atom nodes of this layer is the probability of the atom , given the current parameter settings , and instantiations of the ground atoms in ( g ) .",
    "the nodes in the intermediate layers ( c ) represent sub - formulas of the probability formulas for the ground atoms in ( a ) and ( b ) .",
    "finally , the root node represents the product over all nodes in ( a ) and ( b ) , and thus represents the likelihood of the joint configuration of probabilistic atoms consisting of the observed values for ( a ) , and the current setting at the nodes ( g ) for the atoms in ( b ) .",
    "the likelihood graph is constructed in a top - down manner by a recursive decomposition of the probability formulas . in this decomposition",
    "also sub - formulas will be encountered that have a constant value , and do not depend on any parameters or unobserved atoms .",
    "these sub - formulas are not represented explicitly by nodes in the graph and are not decomposed further .",
    "their constant value is directly assimilated into the function computation at their parent nodes .",
    "the likelihood graph supports computation of the likelihood values and the gradient of the likelihood function with respect to the numerical parameters ( e ) and ( f ) .",
    "these computation are linear in the number of edges of the graph . based on these elementary computations ,",
    "the likelihood graph can be used for parameter learning via gradient ascend , marginalization over unobserved atoms via mcmc sampling ( mainly used when learning from incomplete data ) , and map inference for unobserved probabilistic atoms . for parameter learning ,",
    "we perform multiple restarts of gradient ascend with random initializations for the nodes ( e ) , ( f ) , ( g ) .",
    "we now demonstrate the usefulness of rbn models with numerical input relations for practical modeling and learning problems , and the feasibility of learning via likelihood graph based gradient ascent . in this section",
    "we present examples that demonstrate the use of the logistic regression model in our relational framework for constructing models that closely follow conventional and interpretable statistical modeling approaches .      in a first experiment we test whether standard `` propositional '' logistic regression",
    "is properly embedded in our relational framework . for this",
    "we use a very small dataset containing data on 27 cancer patients that was originally introduced in  @xcite , and which is often used as a standard example for logistic regression .",
    "we use a simplified version of the dataset given in  ( * ? ? ? * table 5.10 ) , which contains a single numerical predictor variable _ li _ , and a binary response variable indicating whether the cancer is in _ remission _ after treatment .",
    "a standard logistic regression model for predicting _ remission _ is represented by the probability formula    @xmath47    the combination function construct in this formula is somewhat degenerate , since it here effects no combination over a multiset of values , and simply reduces to the application of the logistic regression function to the single number @xmath48 .        figure  [ fig : remission ] shows the probability of the response variable as a function of the predictor variable for the parameters @xmath49  learned from the rbn encoding , and for the parameters given in  @xcite ( which were fitted using the sas statistics toolbox ) . clearly , our gradient ascent approach using the likelihood graph here yields results that are compatible with standard approaches to logistic regression .      in this section",
    "we consider a toy model for the propagation of pollution in a river network .",
    "this example demonstrates the ability to integrate into our relational modeling framework standard logistic response models based on meaningful and interpretable predictor relations .",
    "input domains for this model consist of measuring stations in a river network that measure whether the river is _ polluted _ or not .",
    "stations are related by the boolean _ upstream _ relation , denoting that one station is directly upstream of another ( i.e. , without any other stations in - between ) . for any pair of stations in the _ upstream _ relation",
    ", there also is a numerical relation _ invdistance _ containing the inverse of the distance between the two stations .",
    "[ c ] [ c ] [ c ] [ c ] [ c ] [ c ] [ c ] [ c ] [ c ] [ c ] [ c ] [ c ]        the outermost wif - then - else construct in lines 1,2,10 of the model of table  [ tab : river ] defines the _ polluted _ attributed as a mixture of two factors : first , there is a base probability of @xmath50  for pollution to occur regardless of pollution already being measured upstream .",
    "second , lines 2.-9 .",
    "contain a propagation model of pollution that is measured at one or several upstream stations .",
    "this probability sub - formula computes the expression    @xmath51{l } \\scriptstyle \\emph{upstream}(v , s ) \\ & \\\\ [ -1 mm ]                                 \\scriptstyle \\emph{polluted}(v )               \\end{array } }    \\hspace{-8 mm } \\emph{invdistance}(v , s)\\ , ) .\\ ] ]    \\1 .",
    "_ polluted _ @xmath52 wif 0.6 + 2 .",
    "combine = @xmath53 , + 3 .",
    "polluted(v ) _ + 4 . @xmath54 * _ invdistance(v , s ) _ + 5 . 0.0 + 6 .",
    ". _ upstream(v , s ) _",
    "l - reg + 10 . 0.2 ; +",
    "generalizing from the baseline example of section  [ sec : classic ] we investigate whether the parameters of the model can still be identified from independent samples of the _ polluted _ attribute . to this end",
    "we sample @xmath55  independent joint instantiations of the _ polluted _ attribute for the 12 measuring stations of the domain in figure  [ fig : waternetw ] with parameters @xmath56  and @xmath57 , and the values of the _ invdistance _ relation as shown in figure  [ fig : waternetw ] .",
    "all experiments are performed using 20 random restarts . in the first experiment",
    "the values of the _ invdistance _ relation are fixed at their true values of figure  [ fig : waternetw ] , and we only learn the values of @xmath49 .",
    "figure  [ fig : waterparams ] ( a ) shows the learned values for increasing sample sizes @xmath58 .",
    "clearly , quite accurate estimates for @xmath49  are already obtained from relatively small sample sizes .    in a second experiment , @xmath49",
    "are fixed at their true values , and the values of the _ invdistance _ relation are learned .",
    "figure  [ fig : waterparams ] ( b ) shows the convergence of the estimates for the _ invdistance _ values of five different pairs of neighboring measuring stations .",
    "again , the true values are consistently learned .",
    "the required sample size is much larger than for @xmath49 , because a single sample only contains relevant information for the estimation of @xmath59  when @xmath60  is true in that sample .",
    "( a )        ( b )        ( c )    in a third experiment , both @xmath49  and the _ invdistance _ relations are learned . in this setup",
    "no convergence to the true values can be expected , since the parameters are not jointly identifiable : for any given setting of the parameters @xmath61 , one obtains equivalent solutions in the form @xmath62 . for this reason , we compare the products @xmath63  for both the parameters in the generating and learned model .",
    "figure  [ fig : waterparams ] ( c ) shows these products for the same pairs of stations as in ( b ) .",
    "the convergence here shows that even if the exact values of the parameters can not be learned , a probabilistically equivalent model is learned ( the learned value of the parameter @xmath53  also converges to the true value -3.0 ) .",
    ".size of likelihood graph and learning times [ cols=\"^ , > , > , > , > , > \" , ]     table  [ tab : complwater ] shows the size of the likelihood graph , the time for construction , and the average time per restart for the gradient ascent optimization . for different values of the sample size @xmath55 ,",
    "these numbers are given for the case where we only learn the relation _ invdistance _",
    "( ) ( top entry in each cell of the table ) , and the case where we learn @xmath49  and _ invdistance _ ( ) ( bottom entry ) .",
    "the likelihood graph is significantly larger when also learning @xmath49 , because here more sub - formulas of the instantiated model depend on unknown parameters , and therefore can not be pruned in the construction .",
    "we will now apply learning of numeric input relations in rbns for community structure analysis in multi - relational networks .",
    "figure  [ fig : introex1 ] shows a small network with 6 individuals connected by two different types of ( undirected ) links . considering only the green ( solid ) link relation ,",
    "one would identify \\{1 , 3 , 5 } and \\{2 , 4 , 6 } as communities , whereas the red ( dashed ) link relation points to communities \\{1 , 2 } and \\{3 , 4 , 5 , 6}. moreover , the community structure \\{1 , 3 , 5 } , \\{2 , 4 , 6 } , would indicate that the red links are representing an antagonistic relationship that is more likely to exist in between communities , than within communities . considering both links simultaneously , and assuming both are positive indicators of communities , one may also consider \\{3,5 } and \\{4,6 } as the most clearly defined communities , to which 1 and 2 are more loosely connected .",
    "this tiny example illustrates how multiple relations can lead to a rather complex community landscape , with multiple possible views and interpretations .",
    "even though multi - relational networks occur naturally , research on community detection has very much focused on the single - relational case .",
    "proposals for dealing with multi - relational networks often consist of reductions to the single - relational setting , either by aggregating all relations into a single weighted relations  @xcite , or by aggregating results from community detection performed for each relation separately  @xcite .    in section  [ sec :",
    "commmultrel ] we will propose a latent feature model that takes all relations as input in a non - aggregated form , and returns multiple communities along with a characterization of how the different communities are correlated with the relations .",
    "figure  [ fig : introex2 ] shows a single - relational network with a relatively clear two - community structure .",
    "the two nodes 3 and 4 , however , are perfectly ambiguous with regard to their community membership .",
    "most existing soft clustering methods would give both nodes equal membership degrees of 0.5 for both communities .",
    "however , clearly it is desirable to be able to distinguish node 3 , which is well connected to both communities , and which for information diffusion purposes would be the most influential node in the network  @xcite , from node 4 , which is completely isolated .",
    "nodes 1 and 2 are both very strongly associated with the community on the left",
    ". however , instead of assigning a membership degree close to 1.0 to both of them , it will be more informative to assign a higher membership degree to node 2 than to node 1 , so that the membership degree also reflects the centrality of the nodes for the communities . in our model , the learned values of latent numeric relations can be interpreted as _ community centrality degrees _ , that reflect the degree of connectivity of a node with all communities .          in this section",
    "we first consider the single - relational case to explore models for learning community membership degrees that satisfy the desiderata outlined in the preceding section . we introduce a numerical binary relation @xmath64 , whose arguments are a node @xmath65 , and a community @xmath30 . the relation @xmath66  is constrained to be non - negative .",
    "we can then define the following probabilistic model : @xmath67 where @xmath68 and @xmath53  is a real - valued constant ( the intercept , in the language of log - linear models ) .",
    "this model is quite straightforward , and closely related to other models for link prediction ( e.g. for recommender systems ) in which the affinity of objects to be connected by a link is measured by the inner product of latent feature vectors associated with the objects .",
    "we note that in contrast to structurally similar probabilistic latent semantic models  @xcite the variables @xmath69  have no semantics as ( conditional ) probabilities , and , is not a mixture model with the communities as hidden mixture components .",
    "the model is readily encoded as a rbn .",
    "the observed links in a network with node set @xmath70  then define the likelihood function @xmath71 the generic learning method described in section  [ sec : inflearn ] can be used to fit the model parameters @xmath72 .",
    "we applied this model to the well known zachary karate club network depicted in figure  [ fig : zachary ] ( a ) , where the node colors represent the known `` ground truth '' communities in this network  @xcite .",
    "figure  [ fig : zachary ] ( b ) illustrates the learned @xmath66-values when the model is instantiated for 2 communities @xmath73 .",
    "nodes @xmath65  are plotted in 2-dimensional space according to their @xmath74  values .",
    "node colors still represent the ground truth . some individual nodes , and groups of nodes ,",
    "are marked correspondingly in figure  [ fig : zachary ] ( a ) and ( b ) .",
    "we first observe that the nodes 1 and 34 with maximal @xmath75  and @xmath76-values are central nodes of their respective communities .",
    "in contrast , the node groups @xmath28  and @xmath29  are well - connected with their own communities , but separated from the other community .",
    "@xmath30  is a large group of nodes with @xmath75  and @xmath76-values of similar magnitude .",
    "all nodes in this group can be seen as potential hub nodes between the two communities , but node 3 with the highest sum @xmath77  is most clearly identified as a well - connected hub between the communities .",
    "two further observations are worthwhile making : the fact that some @xmath66-values of zero have been learned indicates that allowing negative @xmath66-values could lead to higher likelihood scores . however , for the purpose of interpretability of the results , imposing the non - negativity constraint for @xmath66still seems beneficial .",
    "second , for nodes that are pairwise structurally indistinguishable ( all nodes other than node 27 in group @xmath29 ) identical @xmath66-values were learned .",
    "this , obviously , is highly desirable , and supports both the adequacy of the probabilistic model , and the effectiveness of the optimization procedure ( which , starting from random initial values , could be feared to get stuck in local optima with non - identical values ) .",
    "we compare the results obtained with model , with a slight modification of the _ distance model _ proposed in  @xcite .",
    "this model is given by in conjunction with @xmath78 thus , the log - odds of the _ link _ probability now depend on the squared euclidean distance between the latent feature vectors .",
    "this model , too , is readily encoded by a rbn , and the learned @xmath66-values are visualized in figure  [ fig : zachary ] ( c ) .",
    "the positions of the nodes in the latent space here are not interpretable as community centrality degrees , and ( in line with the motivation given by the authors for this model ) rather are suitable as node - coordinates for graph visualization and plotting .",
    "the model , also achieves a much lower log - likelihood of -245 than the model , , which achieves a log - likelihood of -157 .",
    "the baseline model that does not contain any latent feature vectors @xmath66 , and only the @xmath53  parameter is fitted ( i.e. , a fitted erds - rnyi model ) , achieves a log - likelihood score of -452 .",
    "the likelihood graphs for the models , and , contain 5679 and 10235 nodes , respectively .",
    "the construction times for the graphs are around 0.1s and 0.8s , respectively .",
    "the times per restart of the learning procedure was around 31s for both models .",
    "the increased computation time per gradient computation in the larger graph for the second model was offset by a smaller number of iterations required until convergence .",
    "the obtained results are quite robust : solutions with very similar likelihood scores and structures as the ones shown in figure  [ fig : zachary ] are usually obtained as the highest - scoring solutions within 3 - 5 restarts .",
    "we now generalize the model , to multi - relational networks .",
    "for a network containing @xmath79  relations @xmath80  ( @xmath81 ) , we introduce @xmath79  new numeric attributes @xmath82  on the cluster objects of the domain .",
    "the values of the @xmath83  are unconstrained .",
    "the intention is that @xmath82  measures whether the existence of links of type @xmath84  is positively ( @xmath85 ) or negatively ( @xmath86 ) correlated with membership in cluster @xmath30 .",
    "we now define the probability @xmath87  using in conjunction with @xmath88 given an observed network with @xmath79  different _ link _ relations , we have to fit the model parameters @xmath89 .",
    "this model is clearly not identifiable : for a given parameterization , multiplying all @xmath83-values with a factor @xmath90 , and dividing all @xmath66-values by @xmath91  leads to an equivalent parameterization .",
    "absolute numeric values of the fitted parameters are therefore not significant , but relative magnitudes of values can still identify community structure .",
    "[ c ] [ c ] [ c ] [ c ] [ c ] [ c ] [ c ] [ c ] [ c ] [ c ] [ c ] [ c ] [ c ] [ c ] [ c ]    we apply the model to the multi - relational _ wiring room _",
    "network  @xcite depicted in figure  [ fig : wiring ] .",
    "the network consists of 14 nodes connected by 5 distinct relations . for better visibility ,",
    "the relations here are displayed in two groups .",
    "out of the 5 relations , 3 represent positive relationships , 1 is antagonistic , and 1 ( `` arguments about opening a window '' ) potentially ambivalent .",
    "relation 5 is directed , the others undirected .",
    "the coloring of the nodes represent a community structure found in  @xcite for this network .    using 4 clusters , we learn @xmath92-values for the 14 nodes as shown in figure  [ fig : wiringplotsus ] , and @xmath93-values for the 4 communities as shown in figure  [ fig : wiringplotsthetas ] .",
    "the values @xmath75  ( light blue in figure  [ fig : wiringplotsus ] ) identify nodes 9,10,11 as central nodes of community @xmath94 , to which 8 and 14 also are strongly associated .",
    "this very much coincides with the original green community . according to the @xmath95-values of figure  [ fig : wiringplotsthetas ] ,",
    "membership in this community is most positively associated with relations 2 and 3 , and to a lesser extent 1 and 5 .",
    "relation 4 is clearly negatively associated with this community .",
    "similarly , there is a good correspondence between community @xmath96 , and the original yellow community . according to the @xmath83-values ,",
    "this community is most clearly associated with relations 1 and 3 .",
    "community @xmath97  considers relation 4 as strongly positive , and thereby provides a non - standard view on the community structure of this network , with nodes 7,8 the centers of this community .",
    "finally , community @xmath98  also considers relations 4 as positive , but unlike for @xmath97 , there is a negative association with relation 2 .",
    "the likelihood graph here contained 7361 nodes .",
    "the reported result is the best obtained in 10 random restarts of the learning procedure , where one restart took about 1 minute to compute .",
    "it is highly desirable that a method not only returns the requested number of communities , but also provides a measure of the significance , or validity , of each community . on the basis of our probabilistic model , we obtain such a measure in terms of the explanatory value that a community provides for the observed network structure , where explanatory value is formalized by the likelihood gain obtained by including community information into the model .    specifically , to measure the explanatory value of community @xmath99  defined by the @xmath100-values of all nodes , we consider the model given by in conjunction @xmath101 in this model , we now keep the previously learned @xmath100  values fixed , and re - learn the parameters @xmath102 . as a baseline",
    "we take the erds - rnyi log - likelihood obtained when only fitting the @xmath103-values in a model without communities .",
    "we then define the _ likelihood gain _ obtained from community @xmath99  as the log - likelihood obtained by , minus the erds - rnyi baseline .    for the communities identified for the wiring room network we obtain the following likelihood gain values : @xmath104 , @xmath105 , @xmath106 , @xmath107 .",
    "thus , the ranking obtained by the likelihood gain scores reflects quite well the intuitive evaluation of the communities in terms of interpretability .",
    "an important benefit of using probabilistic models for network analysis is the ability to handle incomplete information : for the likelihood function it is not required that for every pair of nodes @xmath108  the true / false status of the _ link _ relation is known . unlike many other graph partitioning and community detection methods , probabilistic approaches can therefore easily handle incomplete graph data , where _ link(v , w ) _ atoms can also have an _ unknown _ status .",
    "apart from dealing with such potentially 3-valued graph data , we can also exploit this robustness of the likelihood function to improve scalability to larger networks by sub - sampling the _ false_-link data .",
    "assuming complete network data , the number of factors in , and hence the number of nodes in the likelihood graph , is quadratic in the number of nodes of the network .",
    "since networks tend to be sparse , the number of _ true _ links are usually greatly outnumbered by the _ false _",
    "links , and one may expect that the community structure is already well identified by the _ true _ links , and a random sub - sample of",
    "the _ false _ links .    to investigate the effects of learning from",
    "randomly sub - sampled data , we consider a multi - relational social network described in  @xcite .",
    "this network , which we call the _",
    "aarhus _ network , contains 61 nodes and 5 different relations .",
    "we apply our model , with 5 communities to data consisting of all the _ true _ links ( of all 5 relations ) , and a random sub - sample of @xmath109 ( @xmath110 ) of the _ false _ links . having learned @xmath111  and @xmath66  parameters from sub - sampled data , we fix the learned @xmath83  and @xmath66  parameters , and re - learn the @xmath103  parameters using the full data . in conjunction with these adjusted @xmath103  parameters ,",
    "we evaluate the likelihood score of the learned @xmath83  and @xmath66  parameters on the complete data . in this manner",
    "we can assess how well the @xmath83,@xmath66  parameters learned from sub - sampled data fit the complete data ( since the learned @xmath103   essentially reflect estimates for the densities of the @xmath112 , these can not fit the complete data when learned from a sub - sample in which _ false _ links are under - represented ) .",
    "figure  [ fig : subsample ] shows the best log - likelihood score achieved in 20 restarts each for the different percentages of sampled _ false _ links .",
    "surprisingly , the likelihood score first even improves when data is sub - sampled . a possible explanation for this can be a higher variance of the scores achieved in different restarts for the larger datasets , and the best out of 20 restarts being further from a global optimum .",
    "figure  [ fig : subsample ] also shows the time per restart for the different data sets .",
    "these times follow very closely the number of atoms in the data .",
    "we next try to determine how closely the communities identified from the sub - sampled data resemble the communities found from the full data . for this",
    "we consider as a reference the communities found from the 100% data in an extended sequence of 38 restarts , where the best solution then obtained a likelihood score of -3174 .",
    "let @xmath113  be the @xmath66-values for community @xmath99   in this reference solution , and @xmath114  the @xmath66-values learned within 20 restarts from the @xmath109  data ( @xmath110 ) . for all @xmath115 ,",
    "we compute pearson s correlation between the vectors @xmath113  and @xmath114  for @xmath116 , and then ( manually ) re - index the communities in @xmath117  to obtain the best pairwise matches ( according to pearson s correlation ) between the @xmath113  and @xmath114 .",
    "figure  [ fig : pearson ] shows a coarse heat - map visualization of all pearson correlations after the re - indexing .",
    "rows in this figure correspond to the reference communities @xmath113  ( @xmath116 ) .",
    "the 5 main columns correspond to the communities @xmath114 .",
    "an element at row @xmath118  and column @xmath119  consists of 5 colored rectangles , representing the correlation between @xmath113   and @xmath120 , for @xmath110  ( in this order ) .",
    "dark red stands for a correlation @xmath121 , medium red for @xmath122 , and yellow for @xmath123 .",
    "the result shows that reference communities 1 and 4 are usually also identified from sub - sampled data .",
    "these two communities are also the ones which are identified as the most significant ones according to our significance measure , which evaluates to 292 , 183 , 130 , 330 , 147 for communities 1,2,3,4,5 , respectively .      probabilistic latent feature models for social networks ( in the single - relational setting ) have been proposed in  @xcite .",
    "the focus there , however , is more on obtaining interpretable , visual embeddings of the nodes in latent space , than on community analysis .    to apply srl modeling tools for node clustering in social network analysis",
    "has already been suggested in  @xcite .",
    "clusters here consist of nodes with similar properties , however , not of connected communities of nodes . in  @xcite a nonparametric bayesian model with discrete latent variables is proposed , that induces a hard partitioning of the nodes .",
    "that model is formulated for multi - relational networks , but only applied to single - relational ones in  @xcite .",
    "similarly , an rbn model with discrete latent variables for standard partitioning - based community detection was presented in  @xcite .",
    "we have identified two distinct ways in which support for numerical data can be added to statistical relational models : as numerical probabilistic relations with an associated distribution model , or as numeric input relations , which can also be understood as object - specific model parameters .",
    "we have extended the rbn framework to allow for numeric input relations .",
    "such an extension is particularly well supported by rbns , because here relational ( logical ) atoms always have been treated syntactically and semantically as interchangeable with numeric parameters , and only minimal adjustments to the language and its inference and learning algorithms are required . by also introducing a logistic regression combination function , we obtain a framework that supports standard modelling techniques for categorical data in a relational setting , where both models and learned parameters are interpretable .",
    "we here have focused on logistic regression for conditioning binary response variables on numeric inputs , but other models could be integrated by adding additional combination functions to the language .",
    "the only requirement is that the combination functions are differentiable .",
    "the second part of the paper applies the extended rbns to develop new models for community structure analysis in social networks .",
    "specifically , we address the challenges of communities in multi - relational networks , and of assigning community centrality degrees for node - community pairs . unlike most kinds of community membership degrees that are obtained by existing soft clustering methods , these _ ccd _ s are not fractional membership assignments , but measures for how well a node is connected with each community . at the same time , when applied to multi - relational networks , the proposed model provides an explanation of how communities relate to different relations , and a validity measure that for the significance of each detected community .",
    "the rbn modeling tool provides a platform on which one can easily implement different network models for community detection , and which are all supported by a single generic learning algorithm .",
    "like for all general purpose modeling and inference tools , this generality comes at the price that for any particular model more efficient inference and learning techniques could probably be developed by dedicated implementations that can incorporate numerous problem - specific optimizations .",
    "thus , should at any point in time more `` industrial strength '' applications be desired with the community structure model we proposed , then a new model - specific implementation may be needed .    with the network models we have investigated in this paper we have stayed close to established models , and only scratched the surface of the modeling capabilities provided within the rbn language .",
    "one line of future work is to integrate these structural network models with dynamic models for information diffusion within the networks .",
    "the computational bottleneck in our implementation at this point is the size of the likelihood graph .",
    "we have already shown that to some extent this problem can be reduced by sub - sampling the false edges of the network .",
    "other techniques we are currently exploring are optimization strategies in which in an iterative manner the likelihood function is only partially optimized based on smaller , partial likelihood graphs .",
    "such iterative partial optimizations can either follow a block gradient descent strategy , in which only subsets of parameters are optimized in each iteration , stochastic gradient descent strategies , in which only the likelihood function of a part of the data is optimized , or a combination of both these approaches .",
    "the challenge is to develop generic strategies that are widely applicable to a broad range of models , and that do not require the user to perform model - specific tuning of the learning strategy in each case .    in principle , it would also be quite straightforward to add models for numeric probabilistic relations to the rbn framework .",
    "the language of probability formulas can directly be used also to define mean and variance of a gaussian distribution ( for example ) , and thereby define gaussians that are in complex ways conditioned on continuous and categorical predictors . however",
    ", this will come at the price of loosing the tools for exact inference , and one would have to rely on sampling - based inference methods ."
  ],
  "abstract_text": [
    "<S> most work in the area of statistical relational learning ( srl ) is focussed on discrete data , even though a few approaches for hybrid srl models have been proposed that combine numerical and discrete variables . in this paper </S>",
    "<S> we distinguish numerical random variables for which a probability distribution is defined by the model from numerical input variables that are only used for conditioning the distribution of discrete response variables . </S>",
    "<S> we show how numerical input relations can very easily be used in the relational bayesian network framework , and that existing inference and learning methods need only minor adjustments to be applied in this generalized setting . </S>",
    "<S> the resulting framework provides natural relational extensions of classical probabilistic models for categorical data . </S>",
    "<S> we demonstrate the usefulness of rbn models with numeric input relations by several examples .    </S>",
    "<S> in particular , we use the augmented rbn framework to define probabilistic models for multi - relational ( social ) networks in which the probability of a link between two nodes depends on numeric latent feature vectors associated with the nodes . a generic learning procedure </S>",
    "<S> can be used to obtain a maximum - likelihood fit of model parameters and latent feature values for a variety of models that can be expressed in the high - level rbn representation . </S>",
    "<S> specifically , we propose a model that allows us to interpret learned latent feature values as community centrality degrees by which we can identify nodes that are central for one community , that are hubs between communities , or that are isolated nodes . in a multi - relational setting , the model also provides a characterization of how different relations are associated with each community . </S>"
  ]
}