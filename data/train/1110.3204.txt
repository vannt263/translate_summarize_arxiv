{
  "article_text": [
    "factor analysis ( fa ) is one of the cornerstones of classical data analysis .",
    "it explains a multivariate data set @xmath0 in terms of @xmath1 factors that capture joint variability or dependencies between the @xmath2 observed samples of dimensionality @xmath3 .",
    "each factor or component has a weight for each dimension , and joint variation of different dimensions can be studied by inspecting these weights , collected in the loading matrix @xmath4 .",
    "interpretation is made easier in the sparse variants of fa @xcite which favor solutions with only a few non - zero weights for each factor .",
    "we introduce a novel extension of factor analysis , coined _ group factor analysis _ ( gfa ) , for finding factors that capture joint variability between _ data sets _ instead of individual variables . given a collection @xmath5 of @xmath6 data sets of dimensionalities @xmath7 ,",
    "the task is to find @xmath8 factors that describe the collection and in particular the dependencies between the data sets or views @xmath9 .",
    "now every factor should provide weights over the data sets , preferably again in a sparse manner , to enable analyzing the factors in the same way as in traditional fa .",
    "the challenge in moving from fa to gfa is in how to make the factors focus on dependencies between the data sets . for regular fa",
    "it is sufficient to include a separate variance parameter for each dimension .",
    "since the variation independent of all other dimensions can be modeled as noise , the factors will then model only the dependencies .",
    "for gfa that would not be sufficient , since the variation specific to a multi - variate data set can be more complex . to enforce the factors to model only dependencies , gfa hence needs to explicitly model the independent variation , or structured noise , within each data set .",
    "we use linear factors or components for that as well , effectively using a principal component analyzer ( pca ) as a noise model within each data set .",
    "the solution to the gfa problem is described as a set of @xmath10 factors that each contain a projection vector for each of the data sets having a non - zero weight for that factor .",
    "a fully non - sparse solution would hence have @xmath11 projection vectors or , equivalently , @xmath10 projection vectors over the @xmath12-dimensional concatenation of the data sources .",
    "that would , in fact , correspond to regular fa of the feature - wise concatenated data sources .",
    "they key in learning the gfa solution is then in correctly fixing the sparsity structure , so that some of the components will start modeling the variation specific to individual data sets while some focus on different kinds of dependencies .",
    "an efficient way of solving the bayesian gfa problem can be constructed by extending ( sparse ) bayesian canonical correlation analysis @xcite from two to multiple sets and by replacing variable - wise sparsity by group - wise sparsity as was recently done by @xcite .",
    "the model builds on insights from these bayesian cca models and recent non - bayesian group sparsity works @xcite .",
    "the resulting model will operate on the concatenation @xmath13 $ ] of the data sets , where the groups correspond to the data sets .",
    "then the factors in the gfa ( projection vectors over the dimensions of @xmath14 ) become sparse in the sense that the elements corresponding to some subset of the data sets become zero , separately for each factor .",
    "the critical question is to which extent is the model able to extract the correct factors amongst the exponentially many alternatives that are active in any given subset of data sets .",
    "we empirically demonstrate that our bayesian model for group - wise sparse factor analysis finds the true factors even from fairly large number of data sets .",
    "the main advantages of the model are that ( i ) it is conceptually very simple , essentially a regular bayesian fa model with group - wise sparsity , and ( ii ) it enables tackling completely new kinds of data analysis problems . in this paper",
    "we apply the model to two real - world example scenarios specifically requiring the gfa model , demonstrating how the gfa solutions can be interpreted .",
    "the model is additionally applicable to various other tasks , such as learning of the subspace of multi - view data predictive of some of the views , a problem addressed by @xcite .",
    "the first application is analysis of fmri measurements of brain activity .",
    "encouraged by the recent success in discovering latent brain activity components in complex data setups @xcite , we study a novel kind of an analysis setup where the same subject has been exposed to several different representations of the same musical piece .",
    "the brain activity measurements done under these different conditions are considered as different views , and gfa reveals brain activity patterns shared by subsets of different conditions .",
    "for example , the model reveals `` speech '' activity shared by conditions where the subject listened to a recitation of the song lyrics instead of an actual musical performance .    in the second application drug responses",
    "are studied by modeling four data sets , three of which contain gene expression measurements of responses of different cell lines ( proxies for three diseases ; @xcite ) and one contains chemical descriptors of the drugs .",
    "joint analysis of these four data sets gives a handle on which drug descriptors are predictive of responses in a specific disease , for instance .",
    "the group factor analysis problem , introduced in this work , is as follows : given a collection @xmath15 of data sets ( or views ) with @xmath2 co - occurring observations , find a set of @xmath10 factors that describe the joint data set @xmath13 $ ] .",
    "each factor is a sparse binary vector @xmath16 over the data sets , and the non - zero elements indicate that the factor describes dependency between the corresponding views . furthermore , each active pair ( factor @xmath17 , data set @xmath18 ) is associated with a weight vector @xmath19 that describes how that dependency is manifested in the corresponding data set @xmath18 .",
    "the @xmath20 correspond to the factor loadings of regular fa , which are now multivariate ; their norm reveals the strength of the factor and the vector itself gives more detailed picture on the nature of the dependency .",
    "the @xmath21 s have been introduced to make the problem formulation and the interpretations simpler ; in the specific model we introduce next they will not be represented explicitly . instead",
    ", the weight vectors @xmath20 are instantiated for all possible factor - view pairs and collected into a single loading matrix @xmath22 , which is then made group - wise sparse .",
    "we solve the gfa problem with a group - wise sparse matrix factorization of @xmath14 , illustrated in figure  [ fig : factorization ] .",
    "the variable groups correspond to the views @xmath23 and the factorization is given by @xmath24 where we have assumed zero - mean data for simplicity .",
    "the factorization gives a group - wise sparse weight matrix @xmath25 and the latent components @xmath26 .",
    "the weight matrix @xmath22 collects the factor- and view - specific projection vectors @xmath20 for all pairs for which @xmath27 .",
    "the rest of the elements in @xmath22 are filled with zeroes .",
    "we solve the problem in the bayesian framework , providing a generative model that extracts the correct factors by modeling explicitly the structured noise on top of the factors explaining dependencies .",
    "we assume the observation model @xmath28 where each row of the gaussian noise @xmath29 has diagonal noise covariance with the diagonal of @xmath30 $ ] where @xmath31 has been repeated @xmath32 times .",
    "that is , every dimension within the same view has the same residual variance , but the views may have different variances . to complete the generative formulation , we assume the rows of @xmath33 to have a zero - mean gaussian distribution with unit covariance .",
    "that is , each factor is represented with a gaussian latent variable and all factors are _ a priori _ independent .",
    "the weight matrix @xmath22 is made sparse by assuming a group - wise automatic relevance determination ( ard ) prior , @xmath34 where @xmath35 denotes the @xmath36th element in the projection vector @xmath20 , the vector corresponding to the @xmath18th view and @xmath17th factor .",
    "the inverse variance of each vector is governed by the parameter @xmath37 which has a gamma prior with a small @xmath38 and @xmath39 ( we used @xmath40 ) .",
    "the ard makes groups of variables inactive for specific factors by driving their @xmath41 to zero .",
    "the components used as modeling the structured noise within each data set are automatically produced as factors active in only one view .",
    "since the model is formulated through a sparsity prior we do not explicitly need to represent @xmath42 in the model .",
    "it can , however , be created based on the factor - specific relative contributions to the total variance of each view , obtained by integrating out both @xmath43 and @xmath22 .",
    "we set @xmath27 if @xmath44 where @xmath45 is the total variance of the @xmath18th view and @xmath46 is a small threshold constant .",
    "the inference is based on a variational approximation .",
    "we build on the approximation provided for bayesian fa by @xcite , re - utilizing the em - style update rules for all of the parameters of the model .",
    "the only differences are that the posterior approximation for @xmath47 needs to be updated for each factor - view pair separately , @xmath31 are view - specific instead of feature - specific , and the parts of @xmath22 corresponding to different views are updated one at a time ; the detailed update formulas are not repeated here due to the close similarity .    to solve the difficult problem of fixing the rotation in factor analysis models , we borrow ideas from the recent solution by @xcite for cca models . between each round of the em",
    "updates we maximize the variational lower bound with respect to a linear transformation @xmath48 of the latent subspace , which is applied to both @xmath22 and @xmath33 so that the product @xmath49 remains unchanged .",
    "that is , @xmath50 and @xmath51 .",
    "given the fixed likelihood , the optimal @xmath48 corresponds to a solution best matching the prior that assumes independent latent components , hence resulting in a posterior with maximally uncorrelated components .",
    "we optimize for @xmath48 by maximizing @xmath52 with the l - bfgs algorithm for unconstrained optimization . here",
    "@xmath53 , and @xmath54 is the @xmath17th column of @xmath48 , and the @xmath55 collects the second moments of the factorization .",
    "similar notation is used for @xmath56 , which indicates the part of @xmath22 corresponding to view @xmath18 .",
    "when @xmath57 for all @xmath18 the problem reduces to regular factor analysis .",
    "then the @xmath20 are scalars and can be incorporated into @xmath58 to reveal the factor loadings .",
    "when @xmath59 , the problem reduces to probabilistic principal component analysis ( pca ) , since all the factors are active in the same view and they need to describe all of the variation in the single - view data set with linear components .",
    "when @xmath60 , the problem becomes canonical correlation analysis ( cca ) as formulated by @xcite and @xcite .",
    "this is because then there are only three types of factors .",
    "factors active in both data sets correspond to the canonical components , whereas factors active in only one of the data sets describe the residual variation in each view .",
    "note that most multi - set extensions of cca applicable for @xmath61 data sets , such as those by @xcite , do not solve the gfa problem .",
    "this is because they do not consider components that could be active in subsets of size @xmath62 where @xmath63 , but instead restrict every component to be shared by all data sets or to be specific to one of them .",
    "a related problem formulation has been studied in statistics under the name multi - block data analysis .",
    "the goal there is to analyze connections between blocks of variables , but again the solutions typically assume factors shared by all blocks @xcite .",
    "the model recently proposed by @xcite can find factors shared by only a subset of blocks by studying correlations between block pairs , but the approach requires specifying the subsets in advance .",
    "recently @xcite proposed a multi - view learning model that seeks components shared by any subset of views , by searching for a sparse matrix factorization with convex optimization .",
    "however , they did not attempt to interpret the factors and only considered applications with at most three views .",
    "@xcite suggested that regular sparse fa @xcite could be useful in a gfa - type setting .",
    "they applied sparse fa to analyzing biological data with five tissue types concatenated in one feature representation .",
    "in gfa analysis the tissues would be considered as different views , revealing automatically the sparse factor - view associations that can only be obtained from sparse fa after a separate post - processing stage . in the next section",
    "we show that directly solving the gfa problem outperforms the choice of thresholding sparse fa results .",
    "for technical validation of the model , we applied it to simulated data that has all types of factors : factors specific to just one view , factors shared by a small subset of views , and factors common to most views .",
    "we show that the proposed model can correctly discover the structure already with limited data , while demonstrating that possible alternative methods that could be adapted to the scenario do not find the correct result .",
    "we sampled @xmath64 data sets with dimensionalities @xmath32 ranging between 5 and 10 ( @xmath65 ) , using a manually constructed set of @xmath66 factors of various types .    for comparing our bayesian gfa model with alternative methods that could potentially find the same structure , we consider the following constructs :    * fa : regular factor analysis for the concatenated data @xmath14 .",
    "the model assumes the correct number of factors , @xmath66 .",
    "* bfa : fa with an ard prior for columns of @xmath22 , resulting in a bayesian fa model that infers the number of factors automatically but assumes each factor to be shared by all views . *",
    "nsfa : fully sparse factor analysis for @xmath14 .",
    "we use the nonparametric sparse fa method by @xcite which has an indian buffet process formulation for inferring the number of factors .    with the exception of the simple fa model , the alternatives are comparable in the sense that they attempt to automatically infer the number of factors , which is a necessary prerequisite for modeling collections of several datasets , and that they are based on bayesian inference .    the solution for the gfa problem is correct",
    "if the model ( i ) discovers the correct sparsity structure @xmath42 and ( ii ) the weights @xmath20 mapping the latent variables into the observations are correct .",
    "since the methods provide solutions of a varying number of factors and do not preserve the order of factors , we use a similarity measure @xcite that chooses an optimal re - ordering and sign for the factors found by the models , and then measures the mean - square error .    [",
    "cols=\"^ \" , ]     we inspected some of the factors more carefully , and more detailed biological analysis is on - going .",
    "the first factor of type ( i ) , shared by one cell line and the chemical descriptors , activates genes linked to inflammatory processes on the biological side , and is active in particular for non - steroidal anti - inflammatory drugs ( nsaids ) , especially ibuprofen - like compounds , which are known to have anti - cancer effects @xcite . of the factors shared by all cell lines ( type iii ) ,",
    "the one with the highest norm of the weight vectors shows strong toxic effects on all cell lines , being linked to stopping of cell growth and apoptosis . in summary , these first findings are well in line with established knowledge , and digging into the further components is on - going .",
    "we next validated quantitatively the ability of the model to discover biologically meaningful factors .",
    "we evaluated the performance of the found set of factors in representing drugs in the task of retrieving drugs known to have similar effects ( having the same targets ) .",
    "we represented each drug with the corresponding vector in the latent variable matrix @xmath33 , and used correlation along the vectors as a similarity measure when retrieving the drugs most similar to a query drug .",
    "retrieval performance was measured as the mean average precision of retrieving drugs having similar effects ( having the same targets ) . as baselines we computed the distances in only the biological views or in only the chemical view .",
    "representation by the gfa factors outperforms significantly ( t - test , @xmath67 ) using either space separately ( fig .",
    "[ fig : chembiof ] , bottom ) .",
    "the experiment was completely data driven except for one bit of prior knowledge : as the chemical space is considered to be the most informative about drug similarity , the factors were pre - sorted by decreasing euclidean norm of the weight vectors @xmath68 in the chemical space .",
    "we introduced a novel problem formulation of finding factors describing dependencies between data sets or views , extending classical factor analysis which does the same for variables .",
    "the task is to provide a set of factors explaining dependencies between all possible subsets of the views . for solving the problem , coined group factor analysis ( gfa ) , we provided a group - wise sparse bayesian factor analysis model by extending a recent cca formulation by @xcite to multiple views .",
    "the model was demonstrated to be able to find factors of different types , including those specific to just one view and those shared by all views , equally well even for high numbers of views .",
    "we applied the model to data analysis in new kinds of application setups in neuroimaging and chemical systems biology .",
    "the variational approximation used for solving the problem is computationally reasonably efficient and is directly applicable to data sets of thousands of samples and several high - dimensional views , with the main computational cost coming from a high number of factors slowing down the search for an optimal rotation of the factors .",
    "it would be fruitful to develop ( approximative ) analytical solutions for optimizing eq .",
    "[ eq : rotation ] necessary for the model to converge to the correct sparsity structure , which would speed up the algorithm to the level of standard bayesian pca / fa .    the primary challenge in solving the gfa problem is in correctly detecting the sparsity structure .",
    "our solution was demonstrated to be very accurate at least for simulated data , but it would be fruitful to study how well the method fares in comparison with alternative modeling frameworks that could be adapted to solve the gfa problem , such as the structured sparse matrix factorization by @xcite or extensions of nonparametric sparse factor analysis @xcite modified to support group sparsity",
    ". it could also be useful to consider models that are group - wise sparse but allow sparsity also within the active factor - view groups or sparse deviations from zero for the inactive ones , with model structures along the lines @xcite proposed for multi - task learning .",
    "we are deeply grateful to dr .",
    "krister wennerberg ( fimm at university of helsinki ) for helping us in interpretation of discovered factors for the drug structure - response application .",
    "we also thank dr .",
    "juha salmitaival , prof .",
    "mikko sams , and msc .",
    "enrico glerean ( becs at aalto university ) for providing the data used in the neuroscientific experiment .",
    "r.  jenatton , g.  obozinski , and f.  bach .",
    "structured sparse principal component analysis . in _ proceedings of the 13th international conference on artificial intelligence and statistics ( aistats )",
    "_ , pages 366373 , 2010 .",
    "d.  knowles and z.  ghahramani .",
    "infinite sparse factor analysis and infinite independent component analysis . in _",
    "7th international conference on independent component analysis and signal separation _ , pages 381388 , 2007 .",
    "d.  lashkari , r.  sridharan , and p.  golland . categories and functional units : an infinite hierarchical model for brain activations . in _ advances in neural information processing systems",
    "23 _ , pages 12521260 , 2010 .",
    "m.  morup , k.h .",
    "madsen , a.m. dogonowski , h.  siebner , and l.k.hansen .",
    "infinite relational modeling of functional connectivity in resting state fmri . in _ advances in neural information processing systems",
    "23 _ , pages 17501758 , 2010 ."
  ],
  "abstract_text": [
    "<S> we introduce a factor analysis model that summarizes the dependencies between observed variable _ groups _ , instead of dependencies between individual variables as standard factor analysis does . </S>",
    "<S> a group may correspond to one view of the same set of objects , one of many data sets tied by co - occurrence , or a set of alternative variables collected from statistics tables to measure one property of interest . </S>",
    "<S> we show that by assuming group - wise sparse factors , active in a subset of the sets , the variation can be decomposed into factors explaining relationships between the sets and factors explaining away set - specific variation . </S>",
    "<S> we formulate the assumptions in a bayesian model which provides the factors , and apply the model to two data analysis tasks , in neuroimaging and chemical systems biology . </S>"
  ]
}