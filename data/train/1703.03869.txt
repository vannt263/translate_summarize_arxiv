{
  "article_text": [
    "various markets across the world are becoming increasingly more saturated , with more and more customers swapping their registered services between competing companies .",
    "thus companies have realized that they should focus their marketing efforts in customer retention rather than customer acquisition .",
    "in fact , studies have shown that the funds a company spends in attempting to gain new customers is far greater than the funds it would spend if it were to attempt to retain its customers @xcite .",
    "customer retention strategies can be targeted on high - risk customers that are intending to discontinue their custom or move their custom to another service competitor .",
    "this effect of customer loss is better known as customer churn .",
    "thus accurate and early identification of these customers is critical in minimizing the cost of a company s overall retention marketing strategy .    through the use of machine learning , framed are able to identify high - risk customers before they churn .",
    "this churn assessment is performed monthly for a specific company so that the company can subsequently apply a targeted marketing strategy in order to retain these customers .",
    "this high - risk customer identification methodology is sold as a service to various companies that are interested in forming more advanced customer retention strategies .      at the moment , framed are using a very modern and advanced classification machine learning algorithm knows as the random forest algorithm .",
    "this is an ensemble classifier and thus has the distinct advantage of not over - fitting its generated model parameters due the law of large numbers @xcite . like most conventional machine learning algorithms , random forests performance in predicting",
    "is highly dependent on the features that it is given . without the capability of engineering its own features to be able to better capture variance present in the data ( which would ultimately increase prediction accuracies ) ,",
    "a lot of time is spent by framed in generating secondary features that can do just that .",
    "this derivation and generation of meaningful secondary features becomes a struggle when this needs to happen for each and every company framed provides its service to .",
    "this is because each company has its own unique features that exhibit their own variances and dependencies .",
    "fig 1 shows a very basic overview of the operations that take place within frameds machine learning pipeline .",
    "studies have shown that the performance of almost all machine learning algorithms are severely affected by the representation that is used to describe the data they are processing ( features ) @xcite .",
    "this is because different features can get entangled with other features and thus would hide some explanatory factors that would describe some of the variation within the data inputs .",
    "this phenomenon is also known as the curse of dimensionality @xcite .",
    "for that reason most of the effort when designing machine learning systems goes into the feature engineering phase , where machine learning practitioners need to generate new features from the data that would allow the machine learning algorithms to produce adequate results @xcite .",
    "thus companies like framed will spend a lot of time and effort in feature engineering in order to optimize their machine learning classifiers for a specific problem .",
    "this is especially important when dealing with high - dimensional data .",
    "furthermore , most human generated features usually end up being sub - optimal as most of the time they are either over - specified or incomplete @xcite .",
    "advances in the field of neural networks and recent increases in computing performance , have allowed for the development of large scale neural networks with more than a single hidden layer .",
    "this allowed deep neural networks to propagate the weights of each of their layers to the next .",
    "the effects of this ability was that the networks were able to decompose the complexities within the given data by generating abstract data features in an unsupervised manner in each of their hidden layers @xcite .",
    "this gave new life to predictive modelling on high - dimensional datasets with very noisy data ( image recognition , automatic speech recognition etc . ) as the unsupervised abstract features were able to capture the most important variances within the data and thus ignore any variance that did not affect the result variable @xcite .",
    "this inherent ability have made deep neural networks ( dnn s ) excellent tools in pattern recognition . since churn prediction is the analysis of user behavioural patterns , the application of dnn s in this domain",
    "could definitely be beneficial not only in terms of prediction accuracies but also in eliminating manual feature engineering as a required step .      from the presented limitations of human feature engineering in conventional machine learning algorithms which are currently employed at framed",
    ", the project will seek to investigate and apply a deep learning architecture in frameds machine learning pipeline .",
    "the deep architecture would allow for unsupervised feature learning which in practice should allow the company to bypass the feature engineering step for any company data they receive .",
    "ideally the deep architecture should also increase the company s prediction accuracy .",
    "most machine learning implementations are performed and tuned on specific company data based on the company s business model .",
    "thus input vectors and output labels are pre- processed based on these factors and therefore the representations generated are very task specific . in order to be able to apply a deep learning architecture for any kind of data framed deals with , the representations need to be abstracted and simplified while also capturing user behavioural patterns .",
    "the research conducted will give insight to how well the inherent unsupervised feature engineering ability of dnn s performs across companies when presented with abstract user behavioural input vectors .",
    "the general project aim can be broken down into objectives .",
    "the objectives describe the aim of the project in further detail and the collective completion of these objectives should provide reference as to how successfully the projects aim has been satisfied during evaluation .",
    "[ [ generate - an - encompassing - data - representation - architecture - for - deep - learning - prediction ] ] generate an encompassing data representation architecture for deep learning prediction + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    one of the core issues with applying deep learning architectures in any problem scenario is to generate a specialized data representation architecture .",
    "this data representation architecture should structure the data in such a way as to reduce dimensionality while upholding a high - resolution representation of the underlying data features .",
    "due to the fact that features change according to the problem scenario ( i.e. features change according to the service a company carries out ) , an encompassing data representation architecture needs to be developed that can be applied across different companies regardless of the data features each company uses . creating a generic data representation to encompass different company features is quite novel and its success",
    "could be quantified by how well the deep learning architecture performs across companies .",
    "[ [ implement - an - appropriate - deep - learning - architecture - for - churn - prediction ] ] implement an appropriate deep learning architecture for churn prediction + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    the deep learning architecture implemented should be able to employ the unsupervised feature learning ability of deep neural networks .",
    "this is critical as this would ensure that the general aim of the project , of avoiding human feature engineering , is satisfied .",
    "furthermore the deep architecture should employ techniques to generalize well across different months , without a lot of variance in prediction accuracy across months .",
    "ideally the deep learning architecture should perform better in terms of prediction accuracy against the currently employed machine learning algorithm at framed for the specific companies that will be investigated .",
    "this is not a requirement as the general aim of this research is to avoid manual feature engineering but it will definitely be a positive result if this is achieved .",
    "the remainder of the report is structured as follows . in section ii ,",
    "an overview of churn prediction and its applications are presented as well as an in depth overview of the research that was conducted in understanding deep learning and the spark computational cluster .",
    "the methodology that was followed to develop the proposed data representation algorithm and the deep learning architecture are presented in section iii .",
    "section iv covers the steps taken in evaluating and analysing the prediction results of the proposed deep learning architecture .",
    "the prediction results are discussed in section v and in section vi we draw our conclusions .",
    "companies are becoming increasingly more aware of the fact that retaining existing customers is the best marketing strategy to follow in order survive in industry @xcite . in order to be able to apply these marketing strategies , customers that are likely to move their custom to a competitor",
    "need to be identified .",
    "the effect of customer abandoning their custom with a service provider is better known as churn . applying retention strategies",
    "becomes even more important in the case of mature businesses whose customer base has reached its peak and thus retaining customers is of upmost importance .    the reasons behind why customers might want to discontinue their custom with a company can vary .",
    "this can be divided into two types of churn : incidental and deliberate churn @xcite .",
    "sometimes customers are forced into dropping their service with a company due to life circumstances .",
    "this is known as incidental churn .",
    "some examples include customer relocation to areas where the company does not provide service to , or even changes in a customer s financial status such that he / she can no longer afford to stay with a company .",
    "deliberate churn describes the effect of a customer churning due to the customer deciding to move their custom to a competitor .",
    "reasons behind this can range from a competitor offering a latest product , a competitor having better prices for the same service or even the customer s bad experience with technical support ( call centres ) .    from the reasons presented above",
    "it becomes clear that it is of great importance for a company to understand its customers in order for it to evolve its business strategy .",
    "thus identifying customers who are about to churn becomes not just important in terms of retaining customers but also in terms of gathering business intelligence .",
    "as a response in tackling this problem companies have turned to predictive modelling techniques to assist in the identification of these customers .",
    "numerous different machine learning techniques have been applied for churn prediction in the past decade .",
    "this section will cover some of these techniques and how well they performed when applied in the context of churn prediction .",
    "support vector machines were first introduced by vapnik during 1995 which were included in his studies in statistical learning theory .",
    "the main concept of svm is to take known labelled data observations and map them in a linear feature space where the separation between the classes is maximized .",
    "this is done through an optimization algorithm which aims to maximize the separation margin between the classes @xcite . furthermore with the introduction of slack variables ( usually denoted as c ) in the optimization function , a `` hard '' or a `` soft '' margin",
    "can be achieved between classes .",
    "a `` hard '' margin will have a lower separation between the classes but will tend to misclassify less than a `` soft '' margin , which will be lenient towards misclassification but will allow for a larger separation between classes",
    ". in real world situations employing a `` soft '' margin might be preferable as not to overfit the model .    in practice",
    "however the data is not linearly separable .",
    "a way around this is to perform non- linear mapping of the input feature space into a high - dimensional feature space using of what is now popularly known as the `` kernel trick '' @xcite .",
    "this allows the support vector machine algorithm to generalize across different non - linearly separable data depending on the kernel function used .",
    "the most recent application of support vector machines in the context of churn prediction was identified to be used to predict churn in subscription services @xcite .",
    "the paper noted that the use of svm s was not well documented in published research and that previous implementations were based on unrealistic data with small sample sizes without much noise in their samples .",
    "motivated by these reasons , svm was applied to real data which was gathered from a subscription oriented belgian newspaper , and its performance was compared to logistic regression and random forests techniques . furthermore the paper noted that using svms has distinct advantages .",
    "* support vector machines only require two parameters to be chosen in order for them to generate predictions .",
    "the kernel parameter and slack variable ` c ' . *",
    "the model generated by svms is always optimal and global .",
    "this is extremely advantageous as other methods might fall into local minima during their parameter optimization .",
    "the results of the approach presented , showed that svms perform very well in the application of churn prediction even on realistic , noisy datasets .",
    "the applied svm was able to beat logistic regression but under performed when compared to random forests .",
    "furthermore the paper notes that the performance of an svm is greatly dependent on the parameters ( kernel function and ` c ' ) that it is given and in turn the parameters are depended on the data features . it was also noted that svms take significantly more time to train than logistic regression and random forests .",
    "this is definitely the biggest drawback of svms , as companies usually deal with very large , high - dimensional datasets .",
    "even though the work done in the paper was a based on real data , svm s could prove to be unscalable in the world of big data .",
    "decision tress have been used extensively in the context of churn prediction throughout the years",
    ". a decision tree can be thought of as a tree structure representation of a given classification or regression problem .",
    "it is composed of nodes which are also known as ` non- leafs ' that represent explanatory variables .",
    "subsequently a set of decisions to be made are ` grown ' from these nodes , based on a subset of values of the explanatory variable the node depicts .",
    "this is repeated until the hierarchical representation generated has all of its end nodes linked to a value from the target variable @xcite .",
    "this is more easily understood in figure 2 from @xcite which illustrates a decision tree grown from explanatory variables ` outlook ' , ` humidity ' and ` wind ' in order to predict the categorical target variable ` play ? ' .",
    "a lot of algorithms have been developed in the last decade in order to build efficient and effective decision trees for machine learning applications ( cart , c5.0 etc . )",
    "however single decision trees have proved to underperform when compared to other methods .",
    "furthermore decision trees have a tendency to focus their growing on the majority class when presented with imbalanced datasets .",
    "thus ensemble methods ( usually bagging ) were developed in order to address the poor performance of decision trees .",
    "this is performed by generating a lot of different decision trees that are able to work as a single classifier through majority voting on their predictions .",
    "one of the most popular ensemble classifier for decision trees is known as the random forests algorithm .",
    "this is an ensemble classifier and thus has the distinct advantage of not over - fitting its generated models due to the law of large numbers .",
    "the algorithm works by splitting the dataset into random subsets of samples and subsequently generating decision trees on each subset . during the prediction",
    "phase each tree is allowed to report its predictions and the majority prediction is the one returned by the model @xcite . by applying the right amount of randomness in their configurations , random forests can become extremely accurate classifiers . furthermore due to their inherent process of creating multiple decision trees during the model generation ,",
    "the algorithm is perfectly suited for being deployed in distributed systems ( each node can build a distinct tree ) which can dramatically decrease computation time in training and validation @xcite .",
    "even though the effect of bagging allows the random forest algorithm to avoid overfitting , they still do not perform as well on datasets where there is extreme class imbalance ; for example churn prediction datasets .",
    "this inherent flaw is what motivated the development of improved balanced random forests @xcite .",
    "the proposed algorithm combined two previous attempts on tackling this issue , balanced forests and weighted forests .",
    "balanced forests work by sub sampling a dataset while balancing the samples in terms of class distribution for each tree .",
    "this is repeated until all trees generated have covered the majority class .",
    "weighted random forests assign weights to each class , such that the weight of the majority class has a lower weight than the minority class in order to penalize on misclassification accordingly .    the paper states that both these previous attempts have their limitations and continues by saying that the two previous attempts can be combined in order to make an extremely efficient and accurate classifier .",
    "the proposed algorithm was evaluated on real - world banking data provided by a chinese bank and feature selection was done in order to select optimal features for the model . in order to compare the proposed algorithms performance , training and testing",
    "were performed on artificial neural networks , decision trees , svms , and on both balanced forests and weighted forests .",
    "results showed that the proposed algorithm outperformed both previous attempts as well as the other traditional approaches .    from the results it is easy for one to conclude that the improved balanced random forests is the state - of - the - art algorithm for churn prediction . in terms of predictive performance",
    "they outclass other methods , but also due to their effective scalability , fast training and fast predictive speeds they offer great potential in the problem of churn prediction .",
    "having that said , their performance is dependent on the features selected , and therefore the feature engineering stage can not be avoided when using this algorithm .",
    "it is argued that the only way to allow a machine to understand the world around it ( ai ) is by first being able to untangle hidden features from the data without needing a human to intervene . in order to address the issues that occur due to the effects of high- dimensionality , an unsupervised representation algorithm is required .",
    "the algorithm should be able to decompose complexities within datasets and consequently generate new , more effective features .",
    "this is what motivated the development of deep learning algorithms @xcite .",
    "the key findings that propelled the ingenuity behind the deep learning algorithms was the proposal of how the human brain lets visual information flow through a hierarchical neural network in its visual cortex in order to learn what is being observed by the patterns the information exhibits @xcite . thus assuming that an algorithm could mimic this process , even at a very crude level , the algorithm could be applied over large datasets and consequently at every step of the hierarchy produce abstracted data features without the need for human supervision .    since the introduction of deep learning algorithms ,",
    "a number of different models have been created that have been able to simulate the effect that occurs in the human visual cortex using artificial neural networks .",
    "these models can be further generalized into three different types of deep architectures that have different types of applications @xcite :    * _ generative deep architectures _ - used to describe the higher - level correlation properties of the observed data for pattern analysis , and consequently describe the joint statistical distributions of the observed data with their associated classes . * _ discriminative deep architectures _ - used to directly classify patterns by describing previous distributions of classes given by the observed data . * _ hybrid deep architectures _ - used for when the goal is to classify but is supported by the outcomes of a generative architecture .",
    "usually these architectures have the highest prediction accuracy .",
    "most of these architectures are variants of other models and some are simply combinations of models especially in the case of hybrid architectures @xcite .",
    "the main concept is that they have some sort of hierarchy that is able to take inputs at the networks input layer and at every level in their hidden layer , create more abstract data features .",
    "this is done by having less artificial neurons at each step up the hierarchy @xcite .",
    "thus at the output layer of the network , extremely high level abstractions of the data are produced which are constructed by a high - resolution of previous data features . in the case of discriminative deep architectures",
    "better predictions can be made on classes based off these high level abstractions rather than the direct input features .",
    "deep learning architectures have been successfully applied in various pattern recognition scenarios : image recognition , natural language processing and signal processing ( mostly audio ) @xcite .",
    "thus there should be no reason why deep learning could not be applied in churn prediction as it is simply the analysis of user behavioural patterns .",
    "having that said , there are not a lot of scientific papers looking into the application of deep learning in churn prediction .",
    "in fact research was only able to identify one published paper describing this scenario , which discussed the application of deep learning in customer churn prediction regarding a mobile telecommunication network @xcite .",
    "the paper proposes a discriminative deep architecture using a four - layer feedforward neural network which acts as binary classifier which distinguishes churners and non- churners according to a users call patterns . the main motivation behind the use of deep learning architectures was to investigate the possibility of avoiding the time consuming feature engineering step in the company s pipeline while at the same time beating their previous predicting performance .",
    "due to the high underlying complexities of user call interactions they needed to introduce a data representation architecture that could efficiently describe user behaviour across multiple layers while keeping the representation as detailed as possible .",
    "this was essential as deep architectures require a high - resolution input so that they can successfully unravel the underlying interactions and generate secondary features that can increase the separation between classes .",
    "this data representation was used to train and test the deep feed - forward network with a sigmoid activation function in its hidden layers .",
    "results have shown that the model is stable across most of the months which suggests that the model generalizes well and does not overfit the data . furthermore the company was able to significantly increase their prediction accuracy from 73.2% to 77.9% auc .",
    "therefore it can be concluded that multi- layer feed forward models are effective in churn prediction .",
    "the paper also noted that there are some possible enhancements and further research that can be attempted to further improve the models accuracy .",
    "location data of calls could be included in the generation of their data representation architecture .",
    "furthermore the paper hinted that deep belief networks ( generative architecture ) could be applied as a pre - training step which could also increase performance of their multi - layer feedforward network .",
    "the papers overall goal of using a deep learning architecture to avoid the feature engineering stage is very close to the goal of this project .",
    "however , the developed data representation is very domain specific ( telecommunications industry ) and moreover , it does not completely avoid feature engineering ( a few features were engineered to compose the final segment of the data representation ) . even though the papers secondary goal of beating the previous prediction accuracy was achieved",
    ", it did not prove that the feature engineering step could be avoided completely .",
    "furthermore the data representation proposed can not be applied to any company other than a telecommunications company .",
    "this section covers in depth information regarding the inner mechanics of deep neural networks",
    ". it will provide context as to how deep architectures are able to learn , as well as the techniques that were used for implementing the final architecture proposed .      in the case of",
    "supervised learning on a specific training set @xmath0 , he goal is find a hypothesis function @xmath1 that approximates the function @xmath2 , where @xmath3 is the feature set in @xmath0 , @xmath4 is the output label ( target variable ) in @xmath0 and @xmath0 is of the form @xmath5 @xcite . as all instances of @xmath4 are known for all instances of @xmath3 , it can be said that @xmath0 is of the form @xmath6 . through the use of an appropriate cost function @xmath7",
    ", all the points of @xmath0 can be used to find the parameters that fit @xmath1 .",
    "thus supervised learning consists of finding the minimum of the arguments of the cost function @xmath7 , on training set @xmath0 : @xmath8    it has to be noted that simply finding the approximation to the function which fits the training set is not enough for true supervised learning .",
    "the approximation computed needs to generalize well not to just the samples in the training set but also to new samples .",
    "thus for optimized supervised learning , the function should be tested on subsequent validation and test sets in order to quantify its efficacy ( how well it generalizes ) .      as mentioned",
    "previously the motivation behind the development of deep learning architectures was the hierarchical neural network in the human visual cortex @xcite . in order to simulate this ,",
    "artificial neurons were used to form this hierarchy for computational simulations .",
    "artificial neurons are simply computational units that take an arbitrary number of inputs ( including a bias input ) and through a specific activation function are able to return a single output .",
    "this is can be understood easier through the example shown in figure 3 .",
    "as shown in the figure , an artificial neuron takes inputs @xmath9 and a bias term @xmath10 .",
    "in fact this is the simplest form of a neural network , which in this example allows for the representation of a hypothesis @xmath11 .",
    "this can be generalized through an activation function @xmath12 such that : @xmath13    there are mainly two types of activation functions that are used in neural networks , a sigmoid function ( logistic or hyperbolic tangent ) or the more recently developed rectified linear function .",
    "the use of sigmoid functions in deep neural networks stems from the fact that they introduce non - linearity to the model @xcite .",
    "the logistic sigmoid neuron generates a linear combination of its input values and weights ( pre - activation z ) and applies the logistic regression function to the result .",
    "@xmath14 thus the output of the neuron is bounded between 0 and 1 .",
    "intuitively the larger the value of the neurons pre - activation function , the closer the output will be to 1 . furthermore due to its easily calculated derivative @xmath15",
    ", it allows the possibility for a network , composed of these neurons , to be trained using greedy optimization learning algorithms like gradient descent .    instead of the logistic function ,",
    "a hyperbolic tangent ( tanh ) activation function can be used : @xmath16 this is simply a rescaled logistic activation function with its lower limit set to -1 .",
    "studies have shown that when presented with normalized data ( between 0 and 1 ) , hyperbolic tangent activation functions seem to generate stronger gradients during backpropagation ( data is cantered on 0 thus derivatives are larger ) @xcite .",
    "the rectified linear activation function has no upper limit above 0 and any negative pre - activation value computed will be set to 0 .",
    "@xmath17the effects of this is that a rectified linear function can only have two possible derivatives and thus the output values can only be 0 or @xmath18 @xcite.this makes rectified linear neurons extremely computationally efficient .",
    "moreover the general effect of employing rectified linear neurons in a network , is the fact that they allow the network to form sparse propagation paths as neurons will either be active or not and thus computations become linear along these paths . due to this linearity , gradients do not `` vanish '' during backpropagation as can be noticed in sigmoid or @xmath19 activation functions",
    ".      in theory stacking artificial neurons in various different combinations can allow such computational units to solve ever more complicated functions .",
    "more formally any function can be represented by a set of computational units configured in certain way @xcite .",
    ", width=336 ]    the configuration and connections between elements can be represented by a graph . for example",
    "the expression of the function @xmath20 can be considered as the composition of a set of operations which is illustrated in figure 4 ( from @xcite ) .    through this intuitive example one can recognize that a complex function , such as @xmath21 can not be expressed through a single computational unit of a specific type .",
    "this is similar to how complex non - linear functions can be approximated by stacking artificial neurons and by subsequently training each neurons weights .",
    "a feed - forward neural network or mlp is made up of an arrangement of interconnected neurons with a simple activation function .",
    "the arrangement of an mlp can be seen in figure 5 ( from @xcite ) and simulates the non - linear mapping of an input vector to an output value .",
    "initially an mlp has no inherent ability to solve a highly complex non - linear function as its weights are initialized randomly upon instantiation .",
    "thus by allowing each layer of neurons to propagate their activations to forward layers ( feed - forward architecture ) and iteratively ` fix ' their weights during backpropagation , the composition of weights of the architecture will begin to give increasingly better approximations to any function in every iteration .",
    "furthermore , after successful training , each neuron in the network can be thought of as a feature detector @xcite .",
    "thus by allowing information to flow forward between hidden layers , more complex , abstracted features will be generated within the network .",
    "this is because top level hidden layer neurons assign their weights based on the activations of previous hidden layers , which in theory are a combination of complex feature detections .    in the case of classification",
    ", a @xmath22 function can be used as the activation function in each artificial neuron located in the output layer .",
    "this is a generalization of logistic regression so that multiple classes can be predicted by having a neuron for each class that exists ( can also be used for binary classification by having 2 neurons ) @xcite .",
    "@xmath23 each neuron in the softmax layer will make a prediction @xmath24 based on its inputs @xmath25 , its weights @xmath26 and its bias value @xmath10 .",
    "the @xmath22 layer differs from other layers in an mlp as its neurons work collectively to put into effect @xmath27 . in other words",
    "each neuron will return a probability as to how likely the propagated inputs belong to its class , such that all the probabilities returned sum up to 1 .",
    "thus the predicted class can be found by the position of the neuron in the output layer which returned the highest probability.@xmath28 consequently the final prediction may not be equal to the actual value of the target variable .",
    "the difference between the output predictions and the actual outputs can be quantified as an error signal , or otherwise the cost @xcite .",
    "the magnitude of the cost is what determines how the weights will be adjusted during backpropagation .",
    "the regularized cost function @xmath29 for an mlp demonstrating classification through @xmath22 is given by the following function @xcite .",
    "@xmath30 + \\frac{\\lambda}{2}\\sum_{i=1}^{k}\\sum_{j=0}^{n}w_{i , j}^2$}\\ ] ] this is equivalent to the negative log likelihood with an added `` weight decay '' regularization technique which essentially limits overfitting of the weight parameters ( l2 regularization ) . from the above function we can see that due to the indicator function @xmath31 , only the activation neuron at position @xmath32 will contribute to the cost . intuitively by looking at the plot of @xmath33 , one can recognize that as the activation of a particular neuron gets closer to 0 , the cost increases exponentially .",
    "thus by forcing the model to minimize this error would essentially force the weights of the network to promote an activation close to 1 for that particular neuron .    by varying the weights across all possible values and passing it through the cost function , an error surface",
    "could be generated @xcite .",
    "it is difficult to visualize this for all the weights as a plot of all the weights in a normal mlp would most likely exceed three dimensions . in order to give context to gradient descent and backpropagation , an error surface visualization for an mlp with only two weights",
    "is shown in figure 6 ( from @xcite ) .",
    "the error surface depicted from the two weights shows how the error changes with respect to the values of the weights .",
    "essentially the absolute goal of a backpropagation algorithm is to locate the global minimum of the errors surface .",
    "it is able to do this through a technique known as gradient descent .",
    "as mentioned previously the weights of an mlp are instantiated randomly which technically means that that a random point in the errors surface is selected .",
    "in order for it to decide on how to alter the values of the two weights , the gradient of that locally selected point is calculated .",
    "this is done by differentiating the cost function with respect to each weight . by taking derivatives",
    "it can be shown that the gradient can be calculated using the following function @xcite .",
    "@xmath34 + \\lambda w_j$}\\ ] ] thus by using the resultant partial derivative vector @xmath35 , the weights can be updated according to their partial derivative and a constant @xmath36 , which is more formally known as the learning rate.@xmath37 the whole process is repeated and the weights are updated iteratively until the backpropagation algorithm is satisfied that it has reached the global minimum of the error surface .",
    "of course almost all backpropagation algorithms that use gradient descent in order to optimize a networks weights , can not be certain that they have reached a global minimum . as can be seen in figure 6 ,",
    "the error surface is composed of multiple local minima and thus a simple backpropagation algorithm could get stuck in a suboptimal local minimum .",
    "this is where the learning rate plays a big role in assisting the backpropagation algorithm to determine if it has reached a reasonable error minimum .",
    "the learning rate can be thought of as the ` step - size ' the algorithm takes as it goes down an error slope .",
    "a very large learning rate will cause the backpropagation algorithm to repetitively miss a global minimum due to the erratic weight changes . similarly",
    "if the learning rate is too small , backpropagation will be very slow and may never reach the best local minimum , as it might get caught in a different local minimum . to address this issue",
    "the momentum parameter can be introduced in the backpropagation algorithm which can help `` push '' the descent over local suboptimal minima @xcite .",
    "it does this by incorporating a proportion of the previous weight update in the current weight update .",
    "thus the weight update would happen as follows : @xmath38    the backpropagation algorithm described above is known as the batch backpropagation algorithm as for every iteration in its process , all training samples are used to generate the gradients , which in turn update the weights @xcite .",
    "studies have shown that in order to get more accurate weight updates , the gradients are computed over `` mini - batches '' ( subsets ) of the complete training set @xcite .",
    "subsequently the average of these gradients is taken to be the final update to be performed on the weights .",
    "this is more formally known as stochastic gradient descent ( sgd ) or as mini - batch gradient descent .      in supervised learning applications of deep neural networks , as in the architecture discussed above ,",
    "optimal predictive performance is not a measure of how well the architecture can fit the given output values ( training set ) .",
    "success in any supervised learning application is quantified as to how well the model performs on data it has not been trained on . due to the fact that deep learning architectures generate more complex , abstract features ( usually higher order features ) in each hidden layer , they tend to overfit the training data .",
    "this can be demonstrated through the following example of a linear model versus a high order polynomial model in figure 7 ( from @xcite ) .        as can be seen in figure 7 ,",
    "a linear and a polynomial model were fit on the initial training data ( left plot ) .",
    "one could come to the conclusion that the polynomial model fits the data perfectly and thus is the `` better '' model .",
    "however , when presented with additional data from a test set ( right plot ) , the polynomial model performs significantly worse than the linear model which in fact was the best suited model or in other words was the model that best generalized the variances within the data . in order to address this intrinsic issue of deep learning architectures ,",
    "regularization parameters are introduced within a cost function so that large computed weights are penalized .",
    "the most recognized regularization parameters that are commonly used in deep neural networks are the l1 regularization and the l2 regularization @xcite .",
    "@xmath39 l1 regularization is basically the sum of the weights multiplied by an l1 constant @xmath40 .",
    "this type of regularization has the interesting effect of making the weight vector very sparse . in other words",
    "most of the weights will be close to zero with only a few large weights .",
    "this is important as it forces neurons to compute their activations on the most important inputs , making them more resistant to noise .",
    "@xmath41 l2 regularization or `` weight decay '' is the sum of the square of the weights multiplied by half the l2 constant @xmath40 .",
    "the regularizer has the intuitive effect of forcing the cost function to penalize high variance weight vectors , thus forcing the selection of weight vectors with less variance in their weight values . by doing this ,",
    "the network is forced to use all of its inputs , rather than using specific inputs over and over .    additionally to these regularization techniques",
    ", an early stopping mechanic can be implemented within a backpropagation algorithm .",
    "this mechanic controls the overall run time of the algorithm . in order to be able to apply it ,",
    "the data needs to be split into training , validation and test data .",
    "its main purpose is to allow training to continue on the training data as long as the validation error decreases @xcite .",
    "this ensures that the architecture does not begin to overfit the training data .",
    "furthermore this allows the training time to be optimal where further training would not produce any significant increase in prediction accuracy .",
    "another very recent technique for preventing overfitting is dropout @xcite .",
    "the technique works by randomly dropping ( turning off ) a proportion of neurons from a specific layer in every training iteration .",
    "it does this by giving each neuron in a layer a probability @xmath42 that it will be active . applying dropout during training",
    "has the effect of running training on a subsample of the actual network .",
    "this is because any neural networks of @xmath43 artificial neurons can be seen as a group of @xmath44 possible subsamples .",
    "thus throughout the whole training process of a network with dropout , can be thought of as the training of @xmath44 subsamples . at test time the average prediction of all subsamples",
    "this brings distinct advancements to any deep learning architecture as the initial architecture inherits advantages seen in ensemble machine learning algorithms .",
    "this includes the generation of a much more generalized non - linear function with less overfitting on unseen data .",
    "due to the extremely massive amounts of data that framed handles through company event logs , generation of the encompassing data representation architecture on a single machine proved to be impossible .",
    "thus alternative technologies were investigated in order to address this issue .",
    "this section will cover the technologies used in order to allow the generation of the data representation to be realized .",
    "cluster computing refers to the process of performing parallel data computations on a cluster of computers .",
    "this is done through the use of a system that can defragment the complexities of a single computation and assign fragments of the computation to be performed on different computers in the cluster @xcite.thus the overall completion of the main computation can be collected from each computer and combined as a single object .",
    "this model was realized through the now extremely popular mapreduce @xcite system .",
    "the system provides a user with a programming model where he / she can pass data through a set of operations in a created acyclic data flow graph .",
    "an example of a directed acyclic graph ( dag ) can be seen in figure 8",
    ". therefore a complex computation can be thought of as a set of simple operations that have to be completed in a specific order so that the result satisfies the result of the original computation .",
    "the order of operations of the example in figure 8 would be \\{5 , 3 , 1 , 4 , 2}.        even though this data flow programming model can be applied to satisfy a lot of different applications , applications that reuse intermediate results of the computation in a lot of parallel operations , can not be efficiently expressed through such a model @xcite .",
    "an example of such an application is an iterative application where each iteration is expressed as set of operations to be performed .",
    "this inherently would mean that each iteration would invoke data reloading from storage , which would significantly decrease computation performance .",
    "spark is a relatively new cluster computing framework that tackles this issue through its novel abstracted programming model of the two stage mapreduce model .",
    "this novel programming model allows spark to support any arbitrary acyclic graph of operations .",
    "spark introduces the concept of resilient distributed datasets ( rdds ) @xcite which are a representation of the effect of certain operations / transformations on data in storage or in other rdds .",
    "this means that spark applications can be written in a sequential format through a chain of rdds which is a much more intuitive way for a programmer to describe the flow of data operations ; and in effect be able to reuse previous rdds in iterative computations .",
    "as rdds can be cached to memory , reusing a single rdd ( stored in memory ) for iterative operations , allows spark to be 100x faster than hadoop mapreduce .",
    "furthermore these transformations are applied `` lazily '' such that they can be stacked in a sequence without any computation occurring in the background . essentially through a stack of transformations ,",
    "a directed acyclic graph is generated in the background .",
    "only the application of an `` action '' such as @xmath45 or @xmath46 will trigger the initialization of a computation .",
    "after a computation is initialized , the graph is passed to the dagscheduler where the graph is split into stages of tasks .",
    "these stages are comprised of a set of optimized operations to be performed on the data .",
    "this optimization ability of the dagscheduler is what grants spark the ability to be 10x faster than hadoop mapreduce on disk operations .",
    "for example , multiple @xmath47 transformations can be scheduled within a single stage .",
    "once all stages are optimized , they are flagged as ready and subsequently passed to the taskscheduler which initiates tasks via the employed cluster manager which could be simply spark standalone or even yarn or mesos .",
    "each task in a stage is executed by a worker in the cluster and its computation results ( blocks ) are stored in a workers memory , which in turn can be returned for computations on subsequent stages .",
    "it has to be mentioned that the taskscheduler is not aware of any stage dependencies .",
    "thus stages to be computed that have dependencies on previous completed stages , whose task results have been discarded from memory , are recomputed @xcite .",
    "this allows spark applications to be resilient to faults but comes as a cost of reduced performance if there is nt enough cluster memory to store all task results .      in order for all of spark workers to have access to the data so that they can perform their individual tasks ,",
    "the data needs to be distributed .",
    "furthermore workers need to have access to data that might be stored on different workers .",
    "this can be achieved through the implementation of a distributed file system such as the hadoop distributed file system ( hdfs ) @xcite .",
    "hdfs allows for highly scalable distributed storage of data and it is the basis for all hadoop applications .",
    "it is able to do this by separating file system metadata and application data into namenode and datanode servers respectively .",
    "file system metadata take the form of @xmath48 objects which contain file and directory information such as permissions , modification times and access times .",
    "file content is split into blocks which are large size chunks of a files data .",
    "the blocks are subsequently distributed and reproduced in a number of datanode servers ( most commonly three ) .",
    "this can be seen in figure 9 which depicts an hdfs architecture with client interactions .",
    "the diagram above assumes that file data can be held in single blocks ( block numbering ) .",
    "this was done in the hopes of demonstrating the effect of block replication . in reality",
    "a single data file will be split into multiple blocks depending on the chunk size selected and the size of the actual data file .",
    "furthermore the figure demonstrates how a client interacts with the hdfs architecture .",
    "regardless of whether the client wishes to write or read data , client interactions are initiated on the namenode server where file metadata and namespace tree information are held .",
    "if a client wants to read particular file s data , the namenode server is contacted and the locations of the blocks regarding that particular file are returned by the namenode .",
    "subsequently the client then reads the block data directly from the datanodes that the blocks are stored in .",
    "similarly if a client wants to write data to the hdfs , the namenode is contacted with a request of nominating three suitable datanodes where file data blocks can be stored and replicated .",
    "after the namenode returns this information from its namespace tree , the client then proceeds to writing and replicating the blocks directly on the three datanode servers in a sequential fashion .    in order to keep the overall system integrity ,",
    "datanodes send @xmath49 to the namenode which contain information about a datanodes status and the blocks hosted on that datanode .",
    "this is usually done every three seconds and a datanode failing to do so for ten minutes will be regarded as out of service by the namenode .",
    "this in turn will initiate block replications of the blocks contained in the faulty datanode on other `` alive '' datanodes . additionally heartbeats play an important role in order for a namenode to perform efficient space allocation tasks and load balancing decisions .",
    "these actions are performed as response to heartbeats as a namenode will not directly contact a datanode .",
    "thus it is critical that heartbeats from datanodes to a namenode are performed as frequently as possible .",
    "this section will cover the steps undertaken in implementing the previously described objectives .",
    "it will describe the reasoning behind the proposed data representation architecture as well as how it was realized through the use of a spark computation cluster .",
    "furthermore this section will cover the development progression of the proposed deep neural network through the addition of the more advanced deep learning mechanics described in the background research section .",
    "each company that framed deals with , tracks system events and is able to log them as json objects . consequently daily event logs are supplied to framed as `` raw - dumps '' , which are made up of these json objects , such that each json object is separated by a new line .",
    "an example of the general structure of a json object can be seen in figure 10 .        as can be seen in the example ,",
    "the _ `` event '' _ key in the json object exists in every event logged and is independent of company type . in other words ,",
    "any event from any company , will always contain this initial key in their json event objects . even though the _",
    "`` properties '' _ key also exists in all json event object regardless of company , the value ( object ) of the _ `` properties '' _ key changes for different types of events and also different companies will have different property value objects which depend on their system .    since the data representation to be developed is concerned with user event data only , a key in the _ `` properties '' _ value object needed to be identified that would indicate this .",
    "this was identified to be the _",
    "`` distinct_id '' _ key .",
    "the _ `` distinct_id '' _ is only present in the _ `` properties '' _ value object if the event logged was triggered by a user in the system .",
    "furthermore the value of this key is independent of whether the user is a registered user in the company or a general / anonymous user . having that said ,",
    "there is a distinct difference between the values of registered and general user .",
    "registered users have numerical distinct ids , while general users have long alphanumeric ids usually corresponding to system cookie ids .",
    "lastly the _ `` time '' _ key in the _ `` properties '' _ value object was found to exist in all json event objects regardless of event type and company .",
    "the value of the _ `` time '' _ key contains a numerical unix timestamp .",
    "all other keys in the `` properties '' value object were found to be company and event type specific .    by having realized what information was available in user event data across different companies ,",
    "it was decided that the encompassing data representation architecture needed to be formed from these persisting key - value pairs .",
    "thus it was essential that the values of the `` event '' , _",
    "`` distinct_id '' _ and _ `` time '' _ keys were scrapped from the daily _ `` raw - dump '' _ files in a reasonable data structure .",
    "the values from these keys were scrapped from json event objects and stored in tuples of the form @xmath50 . after collecting all the tuples formed from each json event object , tuples containing user ids of non - registered users needed to be removed from the collection .",
    "this was done by validating that the _ userid _ was completely numerical , as it was known that non - registered users would have long alphanumeric values .",
    "having gathered the selected values , the question at hand was how these values could be structured in a way as to express differences between user behavioural patterns . inspired by the representation used to mine user development signals in online community platforms @xcite , it was decided a user event vector needed to be generated for each user across a specific timeframe .",
    "figure 11 depicts the proposed structure of a users event vector for a specific time frame .        in the context of churn prediction ,",
    "user behavioural patterns need to analysed in order to predict whether based on those patterns , the user will churn or not .",
    "since framed provides results of this analysis on a monthly basis , the generated user event vectors needed to be confined within a specific @xmath51 of the complete input data timeframe .",
    "the timeframe of a split would be further subdivided into one hundred periods .",
    "the periods could intuitively be thought of as percentage positions of a splits time interval .",
    "based on those period time intervals , user vectors could be generated for each user with one hundred dimensions .",
    "each dimension in the vector represents a count of events that occurred in a period of that splits timeframe by a specific user .",
    "this is repeated for each customer so that after collecting all the user vectors , the end result would be a matrix of @xmath52 where @xmath53 represents the number of users .",
    "the heat map in figure 12 depicts such a matrix .        even though the proposed representation is using very simplistic features to form user event vectors ,",
    "intuitively this representation is able to capture the differences between user behaviours as event counts are essentially compared on percentages of time across users . as can be seen in the above figure , the generated user event vectors are sparse , but by having the split length parametrized , the sparsity of the vectors can be adjusted , which ultimately removes sparsity in the vector .",
    "of course this is greatly dependent on the rate of user events of a particular company .",
    "event vectors , with adequate sparsity , can be generated from a company with a very high rate of events , by using a small split length .",
    "similarly extending the split length can benefit user event vectors of a company with not a lot of user activity .",
    "thus it can be said that the split length parameter needs to be selected through trial and error , so that denser user event vectors can be generated with more pattern information .",
    "now that the proposed representation has been defined , the following step was to decide how to label these representations . through their experience",
    ", framed has realized that almost 81% of the companies they deal with have no event implemented to signify the churning of a user @xcite .",
    "thus output values needed to be generated through some kind of logic that would determine if a user has churned based on the given data . using the logic described in @xcite ,",
    "a user would be deemed as a churner if there were no events triggered for a specific number of consecutive days . due to the inherent business model of subscription companies to bill their customers on a monthly basis ,",
    "it was decided that 30 days of inactivity would be a good threshold to signify user churn .",
    "this is because a whole month of inactivity means a loss in revenue for these companies , therefore a churner would have to be identified before they become inactive .",
    "the concept of time split s used to generate the user event input vectors was also used to implement this logic . since data from a company was divided into splits of a selected split length , data from subsequent splits could be used to determine the output value of a previous splits user event vector .",
    "this logic can be better demonstrated through figure 13 which shows the complete development of training and validation / test sets .",
    "the above figure shows the overall logic of using the proposed representation architecture to generate training , validation and test sets across 90 days worth of company data for a set split length of 30 days ( split length can be varied ) .",
    "let @xmath54 indicate consecutive 30 day splits .",
    "initially @xmath55 data is used to generate user event vectors for each user present within the time interval of @xmath55 .",
    "then the combined data from @xmath55 and @xmath56 is used to compute churn output values for every user in the combined time interval of @xmath55 and @xmath56 .",
    "the output values are calculated based on the time difference between the time of a users last triggered event and the final time of @xmath56 .",
    "if this time difference is greater than 30 days , then a 1 is returned as an output value ( indicating churn ) , otherwise a 0 is returned ( indicating an active user ) .",
    "furthermore , in the case that a users overall event time span throughout @xmath55 and @xmath56 is found to be less than 30 days ( which would indicate a new user ) , a -1 is returned .",
    "this is important as a company would not be interested in keeping new users but rather retain long active users .",
    "therefore any users with a value of -1 are filtered out .",
    "the user event vectors generated for @xmath55 are joined with the generated output values based on their user ids .",
    "this is done as we are only interested in the output values of users in the time interval of @xmath55 and thus any other users that have registered after @xmath55 are not included .",
    "therefore the end result will be a complete dataset for @xmath55 with user event input vectors and churn output values .",
    "since the overall aim of generating these representations is to effectively train deep neural networks to predict churn rather than just identifying what input vectors indicate churn , the deep neural network should be validated and tested on a following splits dataset .",
    "this is also demonstrated in figure 13 , as the whole process of generating a dataset for @xmath55 is repeated for @xmath56 , using @xmath57 to generate its output values .",
    "therefore the final result will be a dataset for @xmath55 and a dataset for @xmath56 .",
    "the dataset generated for @xmath55 would be used to train a deep neural network while the dataset generated for @xmath56 would be further sub divided into validation and test sets ( 50% - 50% random split ) .",
    "thus this ultimately would force the deep neural network to find a function that could predict churn on @xmath56 data representations based on @xmath55 data representations .",
    "this is key in enforcing prediction rather than just identification of data representations .    due to the inherent nature of the problem of churn prediction ,",
    "generated datasets will be imbalanced in terms of samples available for their respective output value classes .",
    "this is because customer churn will usually be a rare event .",
    "this can cause serious issues in prediction performance @xcite , as a model will adjust its parameters to fit the majority class while disregarding the minority class .",
    "in order to address this issue , generated training , validation and test sets were balanced using random under - sampling .",
    "this method randomly removes samples from the majority class until the samples of each class are balanced .",
    "naturally this may cause the loss of a considerable amount of majority samples that can contribute to better separation between the two classes , but this was the only identified technique that would not cause model overfitting .",
    "initially the generation of datasets was attempted using python and various scipy ecosystem packages such as numpy and pandas . even after boosting the performance of certain iterations in the developed script by using multithreading python techniques , dataset generation based on the proposed data representation architecture ,",
    "could not be realized on a single machine .",
    "this was mainly due to the massive sizes of the raw - dump daily json event files which in turn caused extremely long iteration times . therefore other technologies needed to be investigated that could generate these datasets as fast as possible and irrespective of how many days of data were selected .",
    "it was decided that a spark computational cluster would be used as the literature stated that it could be up to 100x faster than a hadoop mapreduce system ( if there is enough system memory ) .",
    "framed graciously provided access to a compute engine project on the google cloud platform in order build the spark cluster . compute engine projects allow a user to create high performance virtual machines of various computational and memory specifications . by initializing a number of such virtual machines and by consequently installing spark and hadoop ( hdfs ) proprietary software on each one",
    ", they could be configured to work together as a computational cluster .    before any building of the cluster could commence , the general architecture of the cluster needed to be considered . since",
    "a compute engine project will only allow a maximum of 24 computational cores to be utilized across all virtual machines , it was decided that the spark driver ( master ) would be based on a virtual machine with 8 cores and 16 spark workers ( slaves ) will be based on single core virtual machines .",
    "this setup would allow for maximum computational performance while also allowing for a very powerful master server to perform any non - cluster data operations .",
    "furthermore it was decided that an hdfs architecture should be incorporated with sparks architecture so that no additional virtual machines would need to be created .",
    "this was done by having the master virtual machine be both a spark driver and an hdfs namenode .",
    "thus the remaining 16 slave virtual machines would also serve a dual purpose , as they would be both a spark worker and an hdfs datanode .",
    "the final architecture can be visualized in figure 14 .        before any virtual machine instantiation , a virtual network needed to be created that would allow all the machines to operate under the same ip range .",
    "furthermore this was important as the computational cluster needed special firewall entries to allow tcp , udp and icmp protocols to be used for communication between them .",
    "thus by creating a virtual network these firewall rules could be applied internally within the compute engine without any concerns for outside security threats that could arise by enabling them .",
    "therefore each virtual machine instance was created on this virtual network following the specifications described in the architecture .",
    "it was important that all instances were working on the same operating system so that software installations could be carried out the same way across all machines .",
    "the chosen operating system was ubuntu 14.04 lts as it was a stable release of the popular linux distribution .",
    "after all the virtual machines were instantiated , java and the java development kit were installed on the all the machines , as both spark and hadoop hdfs require java to operate .",
    "spark allows for the possibility to operate in standalone mode .",
    "this means that it does not require the installation of third party cluster managers ( yarn or mesos ) in order for it to function .",
    "this can be achieved by installing a compiled version of spark on each machine .",
    "spark installations provide scripts that can be run in order to configure a cluster .",
    "a master server can be launched using the _",
    "`` start-master.sh '' _",
    "script and a slave server can be assigned to a master server using the _",
    "`` start-slave.sh ''",
    "_ script followed by the ip of the master server .",
    "this process can be accelerated by adding the ips of all slave servers to a _",
    "`` slaves '' _ file in the master servers configuration and consequently running the _",
    "`` start-all.sh '' _ script on the master server .",
    "in order for any of these scripts to work , password - less secure shell ( ssh ) access needed to be established between a master server and all slave servers .",
    "this was done by generating private and public ssh keys on the master server and by sequentially transferring these ssh keys to all slave servers .",
    "this allowed two - way communication between the master server and slave servers .",
    "finally , after further configuration in the master servers spark configuration files regarding environment variables , spark was initialized using the _ `` start-all.sh '' _ script on the master server .    to extend the capacity of the proposed architecture , additional 500 gb drives were attached on each of the slave virtual machines",
    "this was done using the google cloud sdk which allows for quick access and control of all projects in the google cloud platform with simple shell commands . after attaching the drives to the slave virtual machines ,",
    "the drives were mounted on each slaves operating system in identical directories .",
    "the directories would be used to hold the blocks of split data files within the hdfs .    having everything set up , the next stage was to tailor the proposed data representations script so that it can utilize the spark computational cluster .",
    "essentially the script was re- written using the pyspark api which exposes the spark programming model to python .",
    "in other words rdd transformations could be executed through this api by supplying python functions to the transformation methods , which would subsequently return python collection types if a spark `` action '' is executed on a transformation .",
    "thus most of the scripts logic was split into functions that could be easily passed into rdd transformations , by following the respective transformation arguments and return prerequisites .",
    "the example shown in figure 15 , depicts an algorithm that can be implemented in python and subsequently passed into a spark @xmath47 transformation .",
    "after a `` raw - dump '' files data has been loaded as text elements in an rdd , the `` getselecteddata '' function can be performed through the @xmath47 transformation .",
    "this will return a new rdd where every text element in the original rdd , has been transformed into a tuple of @xmath58 .",
    "this is in fact is the first step discussed in the encompassing data representations architecture , where required information was scrapped json event objects .",
    "the above methodology was applied to all procedures required to generate datasets of the discussed representation architecture .",
    "intuitively a directed acyclic graph could be visualized from the performed transformations which would give better context to the overall way the spark script works .",
    "figure 16 , illustrates how all the procedures needed to retrieve event vector to churn output mappings of the proposed data representation architecture , can be performed through spark transformations in a directed acyclic graph .",
    "transformations are illustrated as ovals and actions are illustrated as rectangles .",
    "the mapping for @xmath55 is consequently used to generate the training set , while the mapping for @xmath56 will be further subdivided to generate validation and test sets .",
    "of course further procedures are performed in order to generate the final dataset from the two resulting mappings ( like dataset balancing ) , but due to the fact that most of the processing is performed on the spark cluster the complete process is extremely fast .",
    "implementing a deep network architecture can be impossible if attempted to be done without a way of expressing and computing mathematical expressions programmatically .",
    "furthermore the programming language needs to be extremely fast in its computations as during the training phase of a neural network , a lot of different computations need to be performed ( cost function calculations , gradient estimations etc . ) .",
    "theano has long been recognised as an effective python library in implementing deep neural networks , especially in research @xcite .",
    "it allows the definition , optimization and evaluation of mathematical expressions of arbitrary complexity .",
    "once a mathematical function has be expressed and evaluation is initiated , theano will compile the function into c code and automatically optimize the generated c code so that when it is evaluated , the computation is extremely fast .",
    "furthermore theano allows for gpu acceleration for its computations using the nvidia cuda api .",
    "the advantage that ultimately led to the adoption of theano in this project , was that the library can automatically perform differentiation on a function .",
    "this simplified the backpropagation algorithm greatly .",
    "theano uses special objects in order to effectively express any function .",
    "the most basic object is the tensor object which essentially is a representation of the type of an expected input .",
    "for example let s assume that the function @xmath59 needed to be expressed in theano .",
    "variables @xmath60 and @xmath61 are defined as tensor objects of type scalar . in other words",
    "@xmath60 and @xmath61 are expected to have integers as inputs .",
    "subsequently variable @xmath62 can be defined as the operation to be performed on @xmath60 and @xmath61 .",
    "finally the complete function is expressed as having @xmath60 and @xmath61 as inputs and @xmath62 as the expected operation to be performed .",
    "thus any function can be easily expressed by a combination of input tensors and a single expression of the expected operation .    in order to better understand how",
    "theano can be used to implement deep neural networks , several deep learning tutorials from the university of montreal were implemented as practice ( found here @xcite ) .",
    "these included logistic regression using a single artificial neuron up to the implementation of a multilayer perceptron ( deep feed - forward architecture ) .",
    "the tutorials covered how theano could be used to implement layers in a deep feed - forward network and how these layer objects could be easily stacked and trained using stochastic gradient descent on the popular mnist dataset @xcite .",
    "having realised the basic concepts of how various deep neural network mechanics could be implemented , it was decided that the deep learning architecture to be implemented would be based on the tutorial examples .    since the only paper describing the application of deep learning in churn prediction ,",
    "had proposed a deep feed - forward architecture , it was decided that such an architecture should be adopted .",
    "as mentioned in the paper , hyperbolic tangent activation neurons were used in their architecture .",
    "having that said , background research suggested that the recently developed rectified linear activation neuron could allow for better backpropagation gradient estimations .",
    "furthermore background research suggested the use of dropout could allow for better generalization in deep architectures .",
    "thus it was decided that both these mechanics would have to be implemented so that the final architecture could benefit from these techniques .        after following the theano tutorials ,",
    "it was decided that four different classes needed to be created .",
    "the classes are depicted in the uml class diagram in figure 17 .",
    "the most basic class is the _ linearhiddenlayer _ , its main function is to generate the basic structure of a layer in an architecture using the _",
    "`` n_in '' _ and _ `` n_out '' _ ( neurons in , neurons out ) arguments to generate an array of shape @xmath63 and then assign the array values to a _",
    "`` theano.shared '' _ variable .",
    "shared variables allow information to be copied onto the gpu and provide access to their contents to all theano functions , so that information is not constantly copied on the gpu in order to perform computations ( severe decrease in performance ) .",
    "the array is randomly instantiated based on a random uniform distribution and essentially represents the weight values of a layer .",
    "the bias vector is instantiated in a similar way but instead , it is instantiated with zeros .",
    "lastly the output parameter is expressed as the dot product between the input argument and the weights plus the bias .",
    "the _ linearhiddenlayer _ class is extended into the reludropoutlayer class whose basic function is to apply the rectified linear activation on the output parameter of the _ linearhiddenlayer _ subclass .",
    "the output of the _ reludropoutlayer _ class is expressed as a dropout function applied on the now activated layer ( dropout function is discussed later on ) .",
    "in addition to the dropout hidden layers , the output layer of the architecture required a @xmath22 layer as its output in order to be able to act as a classifier . a _ softmaxlayer _",
    "class was implemented to serve this purpose .",
    "its weights and bias parameters are instantiated with zeros and similar to the _ linearhiddenlayer _ class are assigned to shared variables . the parameter _",
    "`` probability_of_class_given_input '' _ parameter is expressed through a _",
    "`` theano.softmax '' _ function which takes the dot product of its weights and its input and during computation will return the probability of an input belonging to a certain class . in order for the architecture to be able to make predictions , the _",
    "`` predict_y '' _ parameter is instantiated as an expression through the use of the _",
    "`` theano.argmax '' _ function which during computation will return the index of the neuron which has the highest probability .",
    "finally the _ softmaxlayer _ class has two methods .",
    "the _ negative_log_likelihood _ returns an expression of the architectures cost function based on the classes parameters and a given label vector @xmath61 .",
    "the errors method returns an expression to compute the zero - one - loss of the layers prediction against a given label vector @xmath61 .",
    "this is all brought together under the _ dropoutmlp _ class .",
    "the class example in figure 17 demonstrates a _ dropoutmlp _ class of 4 layers .",
    "the main function of the _ dropoutmlp _ is to stack _ reludropoutlayer _ classes and at the end apply a _ softmaxlayer_. the first hidden layer takes as input the input vectors from the data representation architecture .",
    "subsequently the second hidden layer takes the first hidden layers output as input and finally the output layer takes as input the second hidden layers output .",
    "it has to be noticed that the size of the hidden layers is intuitively assigned as a parameter by controlling the dimensions of each layers weight matrices .",
    "since dropout is used in this architecture , l1 and l2 regularizations are applied only in the output layer and the values are respectfully computed as separate parameters . finally the _ dropoutmlp _ class takes the output layers negative log likelihood method and assigns the expression as the cost parameter .",
    "similarly the errors parameter takes the output layers error method expression .",
    "the _ dropoutmlp _ object can now be effectively be trained by creating theano functions that can train , validate and test the objects architecture using its public parameters in a backpropagation algorithm .",
    "the implemented backpropagation algorithm was essentially an altered stochastic gradient descent algorithm from the tutorial with an added momentum technique . having that said ,",
    "the theano tutorials did not show how the dropout technique could be implemented or how the activation function for rectified linear neurons could be implemented .",
    "furthermore the tutorials only demonstrated stochastic gradient descent without a momentum parameter . thus these techniques needed to be implemented based on the research paper descriptions of each technique .",
    "the rectified linear unit activation was implemented by expressing a function in theano that could be applied on a layers pre - activation matrix ( the dot product of the weight matrix and the input values ) such that only the maximum of a theano tensor object would be returned . to implement dropout , a function",
    "was created that would allow only a proportion of the activations of a layer to be passed on to the next layer .",
    "the function can do this by generating an array of a randomized binomial distribution of 1 trial , of the same size of a layers activation matrix .",
    "the result of the binomial distribution is controlled by a parameter @xmath42 which corresponds to the probability of a neuron not dropping out ( indicated by a 1 in the array ) .",
    "thus the end result would be an array of ones and zeros of the exact same size as the activation matrix . by multiplying the array with the activation matrix ,",
    "effectively only the results where a one is present will be returned .",
    "this function could be applied in any layers activation matrix and could demonstrate dropout .",
    "having that said , dropout needs to be only applied during the training phase of a deep architecture . simply adding this function to a layers class would not work as dropout will be constantly performed . therefore a modification needed to be made in a dropout layers class such that dropout is only performed during training .",
    "this was done by using the theano _",
    "`` tensor.switch '' _ method which returns a variable depending on a conditions validity . by having the condition being _",
    "`` is_training '' _ ( boolean variable ) and by having that condition altered during the run time of the backpropagation algorithm , dropout could be switched on and off based on what phase the deep learning architecture was performing .    in order to implement momentum the expression of the _ `` updates '' _",
    "parameter of the theano train function needed to be altered .",
    "the _ `` updates '' _",
    "parameter of the theano train function essentially supplies the train function with an expression based on the weights of the architecture , which describes how the weights will be updated .",
    "thus every time the train function is run , the weights of the architecture will be updated based on the result of the train function ( cost ) and the update expression supplied .",
    "momentum can be incorporated in the update expression by firstly creating a theano shared variable which essentially keeps track of each weight update across every iteration .",
    "then the update expression was altered by incorporating a momentum parameter and having that parameter multiplied with the previous weight updates .",
    "this was subsequently added to the normal weight update expression where the learning rate is multiplied with the derivatives of the cost function with respect to each weight .",
    "as mentioned previously gradient derivation can be done automatically in theano using the _",
    "`` tensor.grad '' _ method by supplying it with the cost function and the weights .",
    "furthermore during the end of every iteration the momentum parameter would need to be increased and kept under one while the learning rate would need to be decayed , so that correct gradient descent can be performed .",
    "this was done by instantiating two more theano shared variables with the initial momentum and learning rate values . therefore at the end of every iteration these shared variables were updated with their respective updated values . momentum was increased by 2% after every iteration until it reached 0.99 , while the learning rate was decayed by 1.5% after every iteration .",
    "this is known as learning rate annealing in gradient descent algorithms with momentum , and it is said to guarantee convergence to a minimum @xcite .    therefore the final implemented architecture is a deep feed - forward neural network with rectified linear activations in its hidden layers . furthermore the architecture has a @xmath22 implementation as its output layer composed of two neurons ( one for each class ) .",
    "this layer is regularized using l1 and l2 regularization .",
    "the architecture also has a dropout technique implemented during its training which should allow for better generalization .",
    "lastly the architecture is trained using an implementation of stochastic gradient descent with momentum and an early stopping technique to further fight against overfitting .",
    "before any evaluation could commence , data from different companies needed to be gathered .",
    "three companies were randomly selected and 390 days worth of _ `` raw - dump '' _ event files were collected from each .",
    "it was necessary to take this much data as the proposed architecture effectiveness needed to be tested across different months .",
    "due to confidentiality reasons , these companies will not be named and will be denoted as @xmath64 , @xmath65 and @xmath66 .",
    "each company s data was passed through the proposed spark data representation algorithm with the split length parameter assigned to 30 days .",
    "it was decided that the input vectors of each company should be based on 60 days instead of just 30 ( less sparsity in input vectors ) , therefore two splits were taken to build the input vectors .",
    "the churn calculations that generate the churn output values were performed on subsequent 30 days ( 1 split ) and 60 days ( 2 splits ) .",
    "this effectively allowed for different churn projection windows as companies had varying churn rates .",
    "a company with a low churn rate would not be able to produce enough churn positive samples in a small churn projection window .",
    "generated datasets for projecting different company splits were collected from the spark cluster and were evaluated .    in order to effectively evaluate",
    "how well the proposed architecture performed , it was compared against a simple feed forward architecture on all generated datasets .",
    "the simple feed - forward architecture employed hyperbolic tangent activation neurons in its hidden layers , without dropout implemented and without momentum implemented in its backpropagation algorithm .",
    "this was done in order to evaluate the effects , if any of these newly introduced techniques .",
    "furthermore the number of layers for both architectures was increased during tests from 4 to 6 , to evaluate the effect of adding more hidden layers .",
    "both architectures had their learning rates incremented from 0.0001 to 0.01 across all datasets in order to adjust for different error surfaces .",
    "different datasets would have different error surfaces and because learning rate plays such an important role in determining an optimal minimum , it would not be wise to keep it constant for all datasets .",
    "the momentum parameter in the proposed architecture was kept constant across all tests with an initial value of 0.5 and was decayed at a constant rate .",
    "finally the proposed architecture s performance was compared against framed s current random forest algorithm across the same split intervals of the generated datasets .        through the first company s data",
    ", it was possible to generate two sets of datasets with different churn prediction window timeframes , as enough churn samples were generated in both cases . due to the fact that two splits ( 60 days ) were used to generate the input vectors ,",
    "the first split that could be predicted was the third one .",
    "in total ten splits could be predicted when the churn projection window was set to 30 days and nine splits when it was set to 60 days .",
    "splits , width=336 ]    the simple feed - forward architecture was tested across all generated datasets with varied learning rates and number of layers .",
    "the results can be seen in the box plot depicted in figure 18 .",
    "the length of each box can be expressed as the variance between prediction errors in terms of learning rate and layer numbers .",
    "it can be seen that when comparing the results between the two churn projection windows , the architecture performs much better in 30 day churn prediction timeframes . as more information exists in the input vectors of the larger prediction window , the results seem counterintuitive ( more information should lead to less sparse input vectors and therefore better results ) .",
    "this might be an indication that too much information might `` hide '' the variances between the input vector positions and thus ultimately produce less separable data .",
    "furthermore , when comparing the variance between the smaller churn prediction timeframe and the longer one , the results of the smaller timeframe have a much larger variance between the boxes across splits , while the longer one seems to be more stable .",
    "the lowest point of the box plot can be thought of as the architecture setup with the best result .",
    "thus it can be said that the simple architecture was able to generate adequate prediction results on split 4 , 6 , 7 and 9 with its best results being 70% , 68% , 68% and 69% accuracy respectively @xmath67 .",
    "splits , width=336 ]    the proposed feed - forward architecture was subsequently tested on the same datasets .",
    "it was able to overall perform much better across all splits , with even some splits having prediction errors lower than 0.3 ( figure 19 ) .",
    "therefore it can be said that the proposed architectures activations could better decompose the variance between the feature vectors .",
    "the most interesting result is how the variance between the 60 day churn prediction window boxes has all but diminished .",
    "it could be said that this effect proves that the architecture demonstrates better generalization when compared to the simple architecture on the same splits .",
    "even though the simple architecture performed better on split _",
    "`` 8 & 9 '' _ it could be that it was overfitting the hypothesis function as other 60 day splits performed much poorly .",
    "overall the introduced techniques in the proposed architecture seem to have a beneficial effect on the prediction results as the architecture demonstrates better generalization and much better accuracies across splits .",
    "its best results where on splits 3 , 7 , 9 , 10 with accuracies of 74% , 78% , 72% and 75% respectively .      similarly as with @xmath64 , it was possible to generate enough churn samples for the two different sized churn prediction windows .",
    "splits , width=336 ]    overall the results seem to be significantly worse when compared to the results obtained by the simple architecture across @xmath64 splits ( figure 20 ) .",
    "the only reasonable prediction results were obtained on spits 6 and 10 and even then , the results were nt particularly great with the best accuracies being 68% and 65% respectively .",
    "variances between the boxes of the smaller churn prediction window are extremely high , showing that the architecture has difficulty generalizing across different months .",
    "contrary to @xmath64 results it can not be said that the difference in churn prediction windows had an immediate effect on the results , as most of the smaller prediction window results have similar results to the larger ones .",
    "splits , width=336 ]    testing the proposed architecture on @xmath65 datasets produced similar results as the simple architecture , with no significant increases in prediction accuracies across splits .",
    "having that said , figure 21 demonstrates how effective the techniques employed in the architecture are at creating more generalized hypothesis functions .",
    "variances between the smaller projection window results have significantly decreased and variances between the larger projection window results have almost diminished .",
    "although this does not explain why predictions on all datasets for both the simple architecture and the proposed one are so poor .",
    "this may be due to the proposed data representation , failing to capture any significant differences in patterns between churners and non - churners ( non - separable data ) .",
    "thus further investigation would have to be carried out to prove that this is the cause and to subsequently understand what might be the reason ( discussed in subsection d ) .",
    "data from @xmath66 could not produce an adequate amount of churn samples for a single split ( 30 day ) churn prediction window .",
    "this was due to the low churn rate of the selected company and therefore only datasets based on two split churn prediction windows were generated .",
    "splits , width=336 ]    as can be seen in figure 22 , the results of the simple feed - forward architecture again show high variances between splits , proving again that the architecture is not stable across different months . having that said ,",
    "the results overall are extremely better than the prediction results obtained in data from @xmath64 and @xmath65 .",
    "more than half of the splits ( at a specific architecture layer setup ) , produced accuracies higher than 70% and in splits _",
    "`` 4 & 5 '' _ and _ `` 11 & 12 '' _ , accuracies higher than 80% were attained .",
    "splits , width=336 ]    the results become even better when the proposed architecture is tested on the generated datasets of @xmath66 ( figure 23 ) . at specific number of layer setups the architecture produces accuracies higher than 70% across all of the splits . furthermore four splits produced accuracies greater or equal to 80% which is a significantly better result when compared to the results obtained from the simple feed - forward architecture .",
    "most importantly the variances between split results have decreased in comparison to the variances in the simple architecture .",
    "although it has to be noted that the individual prediction results of splits _",
    "`` 5 & 6 '' _ and _ `` 10 & 11 '' _ show extreme fluctuations ( length of the boxes ) . further investigation ( see subsection b ) showed that these large variances were caused by the learning rate parameter assignment rather than the number of layers of a specific architectures setup .",
    "this of course demonstrates how important the learning rate parameter is to avoiding falling into suboptimal local minima , even when momentum is implemented in the gradient descent algorithm .",
    "the results shown in the previous section were based on cumulative prediction performances of architecture setups with varying number of layers and varying learning rate assignments . in order to identify which of the two varying criterions played a more significant role on the prediction performances and to evaluate",
    "if increasing the number of layers produced an effect on prediction performance , the following plots were generated .",
    ", width=336 ]    the plot in figure 24 depicts the effects the learning rate and the number of layers have in terms of prediction accuracies across all splits of @xmath64 between each of the architectures .",
    "the proposed architectures prediction errors are plotted in a red line and the errors of the simple feed - forward architecture are plotted in blue .",
    "the plot is split in sections with the learning rate varied from 0.0001 to 0.01 across the x - axis and the number of layers varied from 4 to 6 down the y - axis .",
    "immediately it can be seen that adding more layers has little to no effect on the simple feed - forward architectures prediction results . on the other hand",
    "an increase in layers in the proposed architecture seems to allow the architecture to take advantage of the extra layers in order to generate more abstract features .",
    "this is clearly seen in the plot depicting both architectures with 6 layers and trained on the very small learning rate of 0.0001 .",
    "viewing the figure from left to right will demonstrate the effect the learning rate has on each architecture setup .",
    "while comparing the two architectures this way , it can be seen that altering the learning rate has little to no effect on the employed stochastic gradient descent algorithm in the simple architecture .",
    "the proposed architectures added momentum parameter seems to be very dependent on what learning rate is chosen . from the plot",
    "it can be concluded that the employed backpropagation algorithm with momentum , overall seems to perform much better when small learning rates are used ( 0.0001 and 0.001 ) .",
    "lastly if an optimal setup would have to be suggested for predicting @xmath64 data , it would have to be a six layer proposed architecture with 0.0001 learning rate and 0.5 momentum as its hyper parameters .",
    "this is because it is the setup that produced the best results across most of the months .",
    ", width=336 ]    a similar plot was generated for @xmath65 in figure 25 .",
    "compared to the plot of @xmath64 , the effects of both varying the learning rate and varying the number of layers are negligible .",
    "the results are counter intuitive which further promotes the concept that the problem with @xmath65 s results lie within the proposed representation architectures ability of effectively capturing the differences between churn and non - churn user event patterns .",
    "although if an optimal setup would have to be suggested , it would be a proposed architecture of 6 layers trained using 0.0001 learning rate and 0.5 momentum as its hyper parameters , as it produced the best results across most of the splits .",
    ", width=336 ]    as mentioned previously the boxplots generated for the proposed architecture in the previous subsection for @xmath66 , showed high variance between prediction results on splits _ `` 5 & 6 '' _ and _ `` 10 & 11''_. this effect was attributed to learning rate selection and this can clearly be seen in the plot in figure 26 . viewing the plot from left to right , in every proposed architecture setup ( number of layers )",
    "there seems to be a direct dependency on the selected learning rate and the prediction results .",
    "the very low learning rate of 0.0001 produced consistently poor results across most of the splits . furthermore the highest learning rate seems to miss the minimum in most splits at the highest number of layer configuration .",
    "this could mean that the error surfaces of most of @xmath66 splits are riddled with a lot of local minima .",
    "due to the early stopping mechanism employed , the lowest learning rate could have never got the chance to reach a minimum .",
    "the largest learning rate could have consistently missed the minimum due to its large step size on the higher dimensional error space caused by the larger amount of layers .",
    "furthermore considering the increase in layer numbers , @xmath66 s data seems to not benefit substantially from a larger amount layers , as was noticed with @xmath64 .",
    "this might be due to the fact that the minimum number of layers was sufficient to defragment the complexities within the input vectors .",
    "overall if an optimal architecture configuration would have to be suggested for predicting @xmath66 splits , it would have to be a proposed architecture of 4 layers trained using 0.001 learning rate and 0.5 momentum as its hyper parameters .      even though the proposed representation architecture and the proposed deep learning architecture have shown promise in their applications , it is essential to see how well they perform against frameds current random forest model .",
    "as mentioned in the objectives section , beating framed s model is not a requirement due to the fact that the general aim of the project is to demonstrate how the feature engineering step could be skipped . having that said ,",
    "if the proposed pipeline performs significantly worse than the random forests model , even if the feature engineering step is skipped , framed would not look into integrating it in their pipeline .",
    "the metric that was used to keep the comparisons between the two models as fair as possible , was the zero - one - loss metric .",
    "this is the same metric that the prediction results of the proposed deep architecture were based on .",
    "furthermore it also has to be mentioned that framed s model is an implementation of balanced random forests which means that it does not require any balancing in its datasets in order to correctly classify between classes of an imbalanced dataset .",
    "thus frameds model would essentially have more samples to work with than the proposed deep architecture , as dataset balancing is performed using random under - sampling .",
    "lastly frameds model would be compared to the `` optimal '' deep learning architecture configurations for each company ( discussed previously ) .",
    "the plot shown in figure 27 , shows the prediction results of both frameds model and the best performing proposed deep learning architecture configuration across all splits of each company .",
    "the random forests model prediction accuracies are depicted in red and the proposed deep architecture is depicted in blue .",
    "the prediction results are overlaid so that their differences can be clearly seen over each split .    in @xmath64 s prediction",
    "results it can be clearly seen that the proposed deep learning pipeline outperforms the random forests algorithm over almost every split ( except three ) .",
    "furthermore in some of the splits the proposed deep learning pipeline exceeds the random forests performance with an increase of almost 15% , which is a remarkable increase in predictive performance .",
    "framed s random forests algorithm outperforms the proposed architecture substantially on split 8 .",
    "the reason behind this was identified to be that the particular splits dataset only had 22 samples in its validation set .",
    "this was caused by an exceedingly low number of churners during the splits timeframe , thus when the dataset was randomly under sampled for balancing , most of the splits samples were removed .",
    "therefore the proposed deep architecture was not able to correctly train its weights on such a small validation set .",
    "contrary to what was observed in @xmath64 s predictions , the prediction results for @xmath65 showed the proposed deep learning pipeline was not consistently beating the performance of the random forests model .",
    "interestingly almost all the splits that the random forests model was able to beat the proposed deep architecture , were splits of 60 day churn prediction windows .",
    "this could be caused by the way the input vectors of the proposed data representation could `` lose '' variance between each vector position such that discriminatory patterns are lost .",
    "the most promising results for the proposed deep learning pipeline , are the result comparisons on @xmath66 throughout all the splits , the deep learning pipeline outperforms the random forests model by an exceptional average prediction accuracy difference of 20% .",
    "furthermore , considering the fact that the deep learning architecture was trained on datasets with no manual feature engineering , the results of @xmath66 overall show great potential of an adoption of a deep learning pipeline at framed could significantly increase prediction performance and while excluding the feature engineering stage .",
    "the prediction results of @xmath65 demonstrated consistent poor performance across almost all of its splits when compared to @xmath64 and @xmath66 results .",
    "it was previously mentioned that the suspected reason behind this was that the proposed data representation architecture could not effectively capture the differences between user event vectors . in order to test this theory ,",
    "training set samples were taken from each company and passed through a nonlinear dimensionality reduction technique known as t- distributed stochastic neighbour embedding ( t - sne ) @xcite , in order to determine if the data of each company was able to separated .",
    "the technique has been extensively used to visualize the structure of high dimensional datasets .",
    "top right : @xmath65 , bottom : @xmath66,width=336 ]    examples of the dimensionality reduction results on different datasets , can be seen in figure 28 .",
    "the results are based on training sets from each company that had the largest amount of samples .",
    "immediately , it can be noticed that @xmath66 training data is easily separable which in turn would explain the remarkable prediction results .",
    "when looking at the data separation of @xmath64 , a slight separation can be seen on the left hand side of the plot .",
    "even though the separation is not clear , through the training of the proposed deep learning architecture more abstract features were created such that the architecture was able to separate the data much better .",
    "this would explain why the performance of @xmath64 data seemed to perform better with the maximum amount of layers in the proposed architecture .",
    "unfortunately the results of the dimensionality reduction on @xmath65 proved that the data has no apparent separation between its classes , which confirms the initial theory that the data representation was not able to capture pattern differences between the classes .",
    "this also explains why the best performance across the splits was found to be through the use of the 6 layer proposed architecture , as more layers would increase the performance if adequate abstract features are created during training .",
    "framed was approached with the evaluation results and they subsequently suggested that the only difference between @xmath65 and the other two companies was that it was not a subscription based company .",
    "in other words its registered users would not be paying on a monthly basis for a particular service .",
    "this was interesting information , as a non- subscription company would generally not have continuous customer event triggers , as customers would not feel the need to use the service frequently as they are not paying for it . furthermore due to the logic behind the proposed data representation in measuring even counts differences over time",
    ", this would explain why it was not effective at capturing differences between churners and non - churners .",
    "the project investigated the hypothesis that an abstract , company independent data representation could be developed and used to train deep learning architecture in the problem of churn prediction . through a deep learning architectures inherent ability of creating more abstract features though it s hidden layers it was hoped that the data representation could provide adequate prediction results .",
    "looking back at the results of the evaluation and analysis section , the results show the great potential of the overall proposed deep learning pipeline to pose as a solution to the stated hypothesis .",
    "the proposed data representation architecture is able to capture customer event patterns through the use of very abstract information that should be available in any company data that logs user events .",
    "furthermore the data representation architecture applies company independent logic to ascertain whether a user has churned based on a user s inactivity .",
    "this was proved by visualizing the generated representations through the nonlinear dimensionality reduction technique t - sne on @xmath66 and @xmath64 .",
    "results revealed that the developed representation does not work well on non- subscription based companies .",
    "the developed data representation was designed with the assumption that once a user becomes inactive for 30 consecutive days , he becomes a churner . in subscription based companies",
    "this works particularly well as users of a service will feel less inclined to pay for a service if they are not using it , which in turn causes them to churn .",
    "this effect is demonstrated through the t - sne visualization of @xmath65 where the data representation could not capture the differences between churners and non- churners .",
    "thus the key mistake was to assume that all of frameds customers were subscription based .    even with this set back , the proposed deep feed - forward architecture performed exceptionally well even on data representations where the data was nt particularly separable .",
    "this has been demonstrated by the prediction results of @xmath64 whose t - sne visualization showed a very mild separation between classes .",
    "the proposed architecture was able to generate more abstract data features across its hidden layers which in turn allowed the architecture to better distinguish the differences between churner and non- churner input vectors .",
    "furthermore this effect was shown to get better as the layer numbers were increased .",
    "in addition to its ability of generating abstract features , the proposed deep architecture employed techniques that allowed it to generalize its hypothesis functions better across different splits .",
    "this was established by comparing it against a simple feed - forward architecture with hyperbolic tangent activations and no generalization techniques apart from l1 and l2 regularizations . through the use of dropout",
    "the proposed architecture was able to inherit ensemble classifier traits that allowed it to produce less varied results across its months .",
    "furthermore by employing rectified linear activations and momentum in its backpropagation algorithm , the proposed architecture demonstrated much better prediction accuracies than the simple feed - forward architecture .",
    "lastly the complete proposed pipeline ( data representation architecture and deep feed- forward architecture ) , overall produced better prediction results than the currently employed machine learning algorithm at framed .",
    "this was shown by comparing both models by the same metric and on the same split timeframes . having that said ,",
    "the employed dataset balancing technique proved to produce problems on splits with low churn samples .",
    "this in effect caused the proposed deep architecture to significantly underperform against framed s random forest algorithm .",
    "therefore it can be said that the proposed pipeline is vulnerable on months with low churn rates .",
    "taking everything into account it can be said that the general aim of the project has been achieved . the project investigated and developed a representation architecture that could be applied to an arbitrary company that is able to log user events .",
    "furthermore through prediction results it has been proved that it is effective at reducing the dimensionality of incoming data while being able capture a pattern representations of the underlying data features . even if its effectiveness depends on what business model a company employs ( subscription based etc . )",
    "the overall prediction results showed that through a deep learning architecture , the data representation does not underperform when compared to the currently employed feature engineering methods at framed .",
    "in retrospect further companies should have been tested in order to better understand its effectiveness .",
    "furthermore through the development of the representation architecture , a cluster computing technology was implemented so that the generation of the proposed data representations could be realised .",
    "this took up a considerable amount of time away from exploring the data representation architecture further and other deep learning architectures . having that said , the development of these technologies made learning and understanding of these technologies",
    "moreover the utilization of these technologies have allowed framed to realize their potential and have expressed an interest in their adoption . as this was a research project for framed",
    ", the development of these technologies can be thought of as an extension to the overall research performed .",
    "the project also investigated and implemented an appropriate deep learning architecture that can demonstrate unsupervised feature learning and can be applied in the problem of churn prediction .",
    "in addition to the above objectives , the project incorporated modern deep learning concepts which greatly benefited the overall performance of the model .",
    "being newly introduced to deep learning , in depth research and comprehension of the underlying mechanics needed to be covered before any implementation could be performed .",
    "initially further architecture types were planned to be investigated ( recurrent neural networks , deep belief networks etc . ) , but due to the time constraints and the background understanding that needed to be covered , this proved to be unrealistic .",
    "the addition of an unsupervised generative pre - process architecture , like deep belief networks , behind the proposed architecture , might have greatly improved the prediction results . furthermore recurrent neural networks are excellent in generating predictions based on temporal data , which is exactly what churn prediction is .",
    "thus both of these architectures could be investigated further in future work .",
    "the authors would like to acknowledge elliot block , andrew berls , dan evans , and the rest of the framed data team for their engineering and data infrastructure assistance and technical guidance .",
    "simon tomlinson from the data science institute at the university of lancaster also provided additional guidance and advice .",
    "f.  castanedo , g.  valverde , j.  zaratiegui and a.  vazquez , _ using deep learning to predict customer churn in a mobile telecommunication networks _ , wise athena llc , 2014 .",
    "[ online ] .",
    "available : http://wiseathena.com/pdf/wa_dl.pdf [ accessed 01 05 2015 ] .",
    "l.  deng , r.  w.  u.  microsoft res .",
    ", g.  hinton and b.  kingsbury , _ new types of deep neural network learning for speech recognition and related applications : an overview _ ieee international conference on acoustics , speech and signal processing , vancouver , 2013 .",
    "m.  k.  kim , m.  c.  park and d.  h.  jeong , _ the effects of customer satisfaction and switching barrier on customer loyalty in korean mobile telecommunications _ , telecommunications policy : growth in mobile communications , vol .",
    "2 , pp . 145- 169 , 2004 .",
    "k.  coussement and d.  van  den  poel , _ churn prediction in subscription services : an application of support vector machines while comparing two parameter - selection techniques _ , expert systems with applications , vol . 34 , no",
    ". 1 , pp . 313 - 327 , 2008 .",
    "j.  bloemer , t.  brijs , k.  vanhoof and g.  swinnen , _ comparing complete and partial classification for identifying customers at risk _ , international journal of research in marketing , vol .",
    "117 - 131 , 2003 .",
    "a.  ng , j.  ngiam , c.  foo , c.  suen , a.  coates , a.  maas , a.  hannun , b.  huval , t.  wang and s.  tandon , _ unsupervised feature learning deep learning tutorial _ , stanford university , 2015 .",
    "[ online ] .",
    "available : http://ufldl.stanford.edu/tutorial/ [ accessed 25 8 2015 ] .",
    "t.  kocak , _ sigmoid functions and their usage in artificial neural networks _ , university of central florida , spring 2007 .",
    "[ online ] .",
    "available : https://excel.ucf.edu/classes/2007/spring/appsii/chapter1.pdf [ accessed 25 8 2015 ] .",
    "g.  m.  w. and s.  dorling , _ artificial neural networks ( the multilayer perceptron ) - a review of applications in the atmospheric sciences _ , atmospheric environment , vol .",
    "14 - 15 , pp .",
    "2627 - 2636 , 1998 .",
    "n.  buduma , _ data science 101 : preventing overfitting in neural networks _ , kdnuggets , 04 2015 .",
    "[ online ] .",
    "available : http://www.kdnuggets.com/2015/04/preventing-overfitting-neural-networks.html [ accessed 25 08 2015 ] .",
    "n.  srivastava , g.  hinton , a.  krizhevsky , i.  sutskever and r.  salakhutdinov , _ dropout : a simple way to prevent neural networks from overfitting _ , journal of machine learning research , vol .",
    "1929 - 1958 , 2014 .    g.  l.  valentini , w.  lassonde , s.  u.  khan , n.  min - allah , s.  a.  madani , j.  li , l.  zhang , l.  wang , n.  ghani , j.  kolodziej , h.  li , a.  y.  zomaya , c.  z.  xu , p.  balaji , a.  vishnu , f.  pinel , j.  pecero , d.  kliazovich and p.  bouvry , _ an overview of energy efficiency techniques in cluster computing _",
    ", cluster computing , vol .",
    "1 , pp . 3 - 15 , 2013 .",
    "m.  zaharia , m.  chowdhury , t.  das , a.  dave , j.  ma , m.  mccauley , m.  j.  franklin , s.  shenker and i.  stoica , _ resilient distributed datasets : a fault - tolerant abstraction for in - memory cluster computing _ , in 9th usenix conference on networked systems design and implementation , berkeley , 2012 .        framed data , _ 7 takeaways on analytics and churn : what we learned in beta _ , 2015 .",
    "[ online ] .",
    "available : http://blog.framed.io/7-takeaways - on - analytics - and - churn- what - we - learned - in - beta/[http://blog.framed.io/7-takeaways - on - analytics - and - churn- what - we - learned - in - beta/ ] [ accessed 30 8 2015 ] .",
    "f.  bastien , p.  lamblin , r.  pascanu , j.  bergstra , i.  goodfellow , a.  bergeron , n.  bouchard , d.  wade - farley and y.  bengio , _ theano : new features and speed improvements _ , in nips 2012 deep learning workshop , 2012 .",
    "g.  orr , _ momentum and learning rate adaptation _ , willamette university , [ online ] .",
    "available : http://www.willamette.edu/~gorr/classes/cs449/momrate.html[available : http://www.willamette.edu/~gorr/classes/cs449/momrate.html ] [ accessed 31 08 2015 ] ."
  ],
  "abstract_text": [
    "<S> as companies increase their efforts in retaining customers , being able to predict accurately ahead of time , whether a customer will churn in the foreseeable future is an extremely powerful tool for any marketing team . </S>",
    "<S> the paper describes in depth the application of deep learning in the problem of churn prediction . using abstract feature vectors , that can generated on any subscription based company s user event logs </S>",
    "<S> , the paper proves that through the use of the intrinsic property of deep neural networks ( learning secondary features in an unsupervised manner ) , the complete pipeline can be applied to any subscription based company with extremely good churn predictive performance . </S>",
    "<S> furthermore the research documented in the paper was performed for framed data ( a company that sells churn prediction as a service for other companies ) in conjunction with the data science institute at lancaster university , uk . </S>",
    "<S> this paper is the intellectual property of framed data .    </S>",
    "<S> churn prediction , deep learning , neural networks , feed forward , spark , hdfs </S>"
  ]
}