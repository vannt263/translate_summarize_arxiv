{
  "article_text": [
    "the generic problem in information processing is the transmission of information over a noisy communication channel @xcite .",
    "the transmission can be mathematically described by two random variables @xmath3 and @xmath4 representing the desired information and its noisy replica , respectively .",
    "a schematic figure of a communication channel is depicted in fig .",
    "[ communication channel ] .",
    "the basic properties of a communication system are : @xmath5 which is the probability of transmitting a symbol @xmath3 taken from the input alphabet , and @xmath6 which stands for the probability of receiving a symbol @xmath4 ( taken from the output alphabet ) following the transmission of a symbol @xmath3 .",
    "noisy transmission can occur either via space from one geographical point to another , as happens in communications , or in time , for example , when sequentially writing and reading files from a hard disk in the computer .",
    "mutual information , @xmath7 , is a principle quantity in information theory which quantifies the amount of information in common between two random variables .",
    "it is used to upper bound the attainable rate of information transferred across a channel .",
    "a basic definition of the mutual information is @xmath8 where @xmath9 is the shannon s information entropy ( in nats ) @xcite .",
    "the mutual information measures the amount of uncertainty in a random variable , indicating how easily data can be losslessly compressed . hence knowing @xmath4",
    ", we can save an average of @xmath7 bits in encoding @xmath3 compared to not knowing @xmath4 @xcite .",
    "a fundamental link between information theory and thermodynamics was first established five decades ago by jaynes @xcite .",
    "however , his work did not include an explicit relation between mutual information and thermodynamics .",
    "[ communication channel ]        recently , it has been proven that the mutual information can be reformulated as a consequence of the laws of thermodynamic , where the corollary was exemplified for the gaussian noisy channel and for the binary symmetric channel .",
    "the modeling of the communication channels as a thermal system required the generalization of thermodynamics to include t - dependent hamiltonians and the generalization of the second law of thermodynamic which was proved to have the following form @xmath10 where @xmath11 denotes averaging over the standard boltzmann distribution .    in communication channel the goal is to estimate the transmitted symbol @xmath3 from the received symbol @xmath4 ( fig .",
    "[ communication channel ] ) , hence the main quantity of interest is @xmath12 . a physical system with equivalent properties",
    "as the communication channel has to obey the following @xmath13    this new bridge between mutual information and thermodynamics requires the extension of the traditional physical framework and the following two questions are at the center of the first part of our work .",
    "the first one is whether the mapping between mutual information and thermodynamics as well as the physical energy governing a given communication channel is uniquely defined . in case",
    "the energy function is not uniquely defined , the question is whether the required extension of the physical framework is a necessary ingredient , or there is a physical way to express the mutual information using the traditional physical framework without altering the second law of thermodynamics .    the answers to the above questions are that the mapping between mutual information and thermodynamics as well as the physical energy governing a given communication channel is not uniquely defined and requires the extension of the physical framework .",
    "in the following we present three primary approaches , followed by a discussion of a possible physical system with an effective t - dependent hamiltonian and a possible realization of an information - heat engine .",
    "details of the derivations are left for [ derivations ] , whereas [ applications ] exemplifies the calculation of the mutual information of a few archetypal communication channels via thermodynamics .",
    "this approach takes the joint probability @xmath15 to be the boltzmann factor and defines a t - dependent energy , @xmath16.\\ ] ]    this form of the energy is a naive physical energy definition , since it adequately describes a physical system consisting of two degrees of freedom , @xmath3 and @xmath4 , in a contact with a macroscopic heat reservoir . at equilibrium",
    "the expectation properties of @xmath3 and @xmath4 are determined following the partition function @xmath17 @xcite . using the t - dependent hamiltonian ( [ energy definition p(x , y ) ] ) , to describe a communication channel ( e.g. @xcite , eq .",
    "44 ) enforces the generalization of the second law of thermodynamics ( [ generalized second law ] ) , and the mutual information takes the following form @xmath18 where @xmath19 denotes expectation of the random object within the bracket with respect to the subscript random variable @xmath4 , and for a given temperature @xmath20 .",
    "this approach refers to @xmath6 as the boltzmann factor @xcite and the resulted energy is @xmath22.\\ ] ]    the definition of the energy , ( [ energy definition p(y|x ) ] ) , is based on the interpretation of the prior probability of the inputs of the channel , @xmath5 , as the degeneracy of the energy level @xmath23 @xcite .",
    "this approach was recently adopted also by @xcite .",
    "it depicts a scenario of communication channels where the output , @xmath4 , is estimated by the input , @xmath3 . since the degeneracy of the input of the channel can be designed arbitrary , the degeneracy of the physical energy function ( [ energy definition p(y|x ) ] ) may decrease while the energy increases , in contrast to physical systems .",
    "the emission of heat to the reservoir decreases the energy and increases the entropy .",
    "hence , both terms of the free energy identity , @xmath24 , decrease and the system is unstable to thermal fluctuations .",
    "this situation demands a modification of both the free energy and the second law of thermodynamics ( [ free_energy_with dkl],[second law_with dkl ] ) .    the mutual information for energy ( [ energy definition p(y|x ) ] )",
    "is given by @xmath25 where the free energy ( as implicitly suggested in ) and the second law are effectively modified to be @xmath26 and @xmath27 respectively .",
    "for a clarification , @xmath28 is the free energy , @xmath29 denotes the kullback - leibler divergence@xcite and the boltzmann constant is arbitrarily taken to be unity .",
    "the derivation of ( [ mutual p(y|x)],[free_energy_with dkl],[second law_with dkl ] ) is detailed in [ derivations ] .",
    "note that when @xmath5 is uniformly distributed , the conventional identity of the free energy , @xmath24 , and the second thermodynamic law , @xmath30 , are restored .",
    "we propose a new approach , where we define the boltzmann factor to be @xmath12 . as a result ,",
    "the t - dependent energy is @xmath32.\\ ] ]    the 3rd approach describes a typical communication system where the input , @xmath3 , is estimated by the output , @xmath4 .",
    "nevertheless , in this kind of energy functions , the partition function is normalized to @xmath33 , independent of the temperature .",
    "yet also this approach requires the generalization of the second law ( [ generalized second law ] ) , however , the mutual information has a simple form of the internal energy only @xmath34    note that the proposed approach encompasses the other two approaches . on the one hand , the mutual information ( [ new formation of i ] ) can easily be deduced from the 2nd approach using @xmath35 in ( [ mutual p(y|x ) ] ) . on the other hand",
    ", the energy function ( [ energy definition p(x|y ) ] ) explicitly indicates that the second term of eq .",
    "( [ eq . mutual information shental&kanter ] ) is identically zero .",
    "a comprehensive derivation of the 3rd approach is exhibited in [ derivations ] .",
    "a synopsis of a comparison between the three approaches is depicted in table [ tab1 ] .",
    ".a comparison between the three approaches to connect the mutual information via thermodynamics .",
    "each approach requires the extension of the traditional physical framework and yields modified definitions for the free energy and/or for the second law of thermodynamics.[tab1 ] [ cols=\"^,^,^,^ \" , ]",
    "the extension of the physical framework to include t - dependent hamiltonians and the generalized second law of thermodynamics might also refresh our viewpoint on traditional physical systems .",
    "a prototypical physical system governed by an effective t - dependent hamiltonian is a spring where the spring constant is a function of the temperature , @xmath36 @xcite .",
    "the energy of the spring is @xmath37 where @xmath38 denotes the extension of the spring from a reference position with the lack of force on the spring .",
    "note that the common scenario is that the free energy is an explicit function of the temperature .",
    "however , in our case , the ( effective ) * hamiltonian * is a function of the temperature .",
    "this dependence calls for an explanation , since the fundamental potentials ( gravitation , electromagnetic etc . )",
    "governing the known physical laws are independent of the temperature .",
    "the solution of this mystery , a t - dependent hamiltonian , is that the spring is represented by one macroscopic degree of freedom and its property is a consequential of a coarse grained over the microscopic many degrees of freedoms and the nonlinear forces among them .",
    "a mass , @xmath39 , is connected to one end of the spring and the spring with the connected mass is hanged in a container which is vacuumed ( fig . [ spring ] ) .",
    "the container is connected to a heat reservoir at a temperature @xmath40 and for the simplicity of the following discussion we assume that the spring constant monotonically decreases with the temperature .",
    "the equilibrium situation at two different temperatures , @xmath41 is depicted in fig . [ basic_two ] .",
    "we turn now to describe a possible information - heat engine based on such t - dependent hamiltonian .",
    ", connected to a mass @xmath39 hangs in a container which is vacuumed . [ spring ] ]    , ( left panel ) and for a heat - reservoir at cold temperature , @xmath42 , ( right panel ) .",
    "we assume that the spring constant monotonically decreases with the temperature , hence the extension of the spring at @xmath43 is greater than for @xmath42 .",
    "[ basic_two ] ]     and a cold reservoir at temperature @xmath42 .",
    "[ carnot_cycle ] ]    a carnot cycle acting as a heat engine , is illustrated by the black - cycle in the temperature - entropy diagram in fig .",
    "[ carnot_cycle ] .",
    "the carnot cycle consists of @xmath44-steps , alternating isothermal and adiabatic processes @xcite .",
    "the cycle of the information - heat engine consists of two steps only and is illustrated by the red lines in fig .",
    "[ carnot_cycle ] .",
    "the first step , from d to b , describes a quasi - static process where the temperature increases from @xmath42 to @xmath43 and both the temperature and the entropy increase , since the hamiltonian is an explicit function of the temperature . in the reversed process , from b to d",
    ", the temperature decreases in a quasi - static manner back to @xmath42 and the cycle is completed .",
    "no work is done in the entire cycle , d - b - d , since the container is vacuumed , and mathematically the area formed by the cycle d - b - d in the @xmath45 plane is zero .",
    "the heat absorbed / emmited by the c / h reservoirs is responsible for the following two main changes of the system ( spring+mass ) placed in the container : ( a ) the kinetic energy of the microscopic degrees of freedom is modified .",
    "( b ) the hamiltonian of the spring is modified via the t - dependent spring constant .",
    "this process was named as `` channel work '' in , since it reduces the effective heat contributing to the change in the entropy and it resembles work . however",
    ", no actual work is done .",
    "note that in principle at equilibrium the macroscopic mass , m , oscillates as an harmonic oscillator too , since each degree of freedom has on the average a kinetic energy equals to @xmath46 , however , these microscopic vibrations are neglected .",
    "the information - heat engine depicted in fig .",
    "[ carnot_cycle ] describes a way to generate bits in a way resembling a traditional heat engine , but with the lack of work .",
    "the height of the mass m represents the generated bit : the cold position represents `` 0 '' whereas the hot position represents `` 1 '' .",
    "a generation of a sequence of bits can be done by using a predetermined protocol indicating the frequency ( bandwidth ) for the generation of bits .",
    "for instance , in the event that the current bit is `` 0 '' and the successor bit is `` 0 '' too , the contact to the cold reservoir remains , but in case of a successor `` 1 '' , the container is brought to a contact with the hot reservoir .",
    "the proposed information - heat engine describes a way to generate the information , a sequence of bits , in a 2-steps cycle and with the lack of work .",
    "the generation of the communication channel requires a fundamental physical mechanism to transmit the bits and with minimal work in order to enhance the efficiency of the process .",
    "all such mechanisms have to `` read '' and to estimate the height of the mass in the container .",
    "note that the framework of noisy communication channel enables a distortion of the information , however , the encoder represents a noise - free process where the noise is added during the transmission only .",
    "hence , the information - heat engine has a lack of inherent noise .",
    "there are many possible mechanisms to estimate the height of the mass using , for instance , reflected / transmitted photons from the mass / lack - of - mass at a given height , however , it is beyond the scope of our work .    in the above , we presented a possible mechanism which resembles the carnot engine , but with the lack of work .",
    "there are many alternative physical ways to generate an information - heat engine .",
    "for instance , using a material which undergoes a ferromagnetic / paramagnetic transition in between @xmath47 .",
    "however , the essence of such an information - heat engine is a t - dependent hamiltonian .",
    "the minimal mutual information for a given expected distortion , @xmath48 , can be found by minimizing the functional @xmath49 over all normalized distributions @xmath12 @xcite .",
    "the solution of the variational problem is the normalized probability @xmath50 where @xmath51 . @xmath52 and @xmath20 are the lagrange multipliers of the normalization and the expected distortion constraints , respectively .",
    "moreover , @xmath20 is positive and satisfies@xcite @xmath53    in order to satisfy the energy definition ( [ energy definition p(y|x ) ] ) and using the bayes law , a comparison of ( [ p(x|y ) ] ) with the boltzmann distribution law yields the following mapping : @xmath54 , @xmath55 , @xmath5 is used as the degeneracy of the energy level @xmath56 and @xmath57 is the partition function for a given @xmath4 .",
    "the internal energy of the system , @xmath58 , is the expectation value of the energy , @xmath56 . by equating the lagrange multiplier , @xmath20 ( [ lagrange multiplier beta ] ) to the second law of thermodynamics , @xmath59 ,",
    "it is easy to see that this system obeys the following mapping @xmath60    a verification of ( [ mapping s to i ] ) can be observed using the relation @xmath61 followed by comparing the free energy identity , @xmath24 , to @xmath62 ( see eq .",
    "12 in @xcite ) .    substituting @xmath63 @xcite into eqs .",
    "( [ lagrange multiplier beta ] , [ free energy with i ] ) and based on the first law of thermodynamics with the lack of work , @xmath64 , we obtain eqs .",
    "( [ free_energy_with dkl],[second law_with dkl ] ) .      the definitions of the marginal and conditional entropies , consisting the mutual information ( [ def .",
    "mutual infromation ] ) , are @xmath65 and @xmath66 , respectively .",
    "note that when @xmath3 and @xmath4 are independent random variables , @xmath67 becomes @xmath68 .",
    "we introduce a new variable @xmath20 which represents the noise in the channel and has the following properties @xmath69 where @xmath70 is the probability of receiving @xmath4 for a given noise @xmath20 . as a result",
    ", the conditional entropy becomes an explicit function of @xmath20 .",
    "using bayes law and defining @xmath71 we can write the entropies as , @xmath72 note that a noiseless channel is represented by the limit @xmath73 . taking into account that @xmath68 is independent of @xmath4 , we can write , @xmath74 substituting eqs .",
    "( [ entopies via s],[eq .",
    "hx as hxy0 ] ) into eq .",
    "mutual infromation ] ) , we achieve a new form of the mutual information @xmath75 following ( [ energy definition p(x|y ) ] ) , it is clear that the partition function of the equivalent thermodynamic system is @xmath76 , hence eq .",
    "( [ s_definition ] ) is @xmath77 where @xmath58 is the thermodynamic average of @xmath23 , divided by the partition function , or in other words , the internal energy .",
    "the free energy obeys @xmath78 , hence @xmath79 . substituting ( [ eq .",
    "diffrent energy - s ] ) into ( [ eq .",
    "mutual information ] ) , we finally receive a much simpler thermodynamic form of the mutual information , as the difference of the internal energies of the system , @xmath80",
    "the new description of the mutual information ( [ new formation of i ] , [ eq .",
    "diffrent energy - i ] ) is exemplified over several archetypal communication channels .",
    "the sketch of the calculations for the gaussian channel with gaussian input , the gaussian channel with bernoulli-1/2 input and finally the binary symmetric channel with a biased input ( biased bsc ) are presented . for the examples we shall use the following notations : @xmath81 .",
    "the input and the a - posteriori probabilities of this channel are @xmath83 hence , the energy , according to bayes law and ( [ energy definition p(x|y ) ] ) is @xmath84 using ( [ eq .",
    "diffrent energy - i ] ) one can easily find the formula for the shannon capacity @xcite , @xmath85 which is identical to the mutual information derived from the 1st approach , eqs .",
    "( [ energy definition p(x , y)],[eq . mutual information shental&kanter ] ) .",
    "this case is characterized by equiprobable binary inputs a - posteriori probabilities as following @xmath86 implementing the baye s law , while dropping the elements which are independent of @xmath87 , yields a simple expression for the energy ( [ energy definition p(x|y ) ] ) , @xmath88 which eventually gives us the known shannon - theoretic result @xcite , @xmath89      in this case the prior distribution of the biased input and the probability for a symbol to flip during the transmission are denoted as @xmath90 and @xmath91 respectively .",
    "hence , the probabilities are defined as following , @xmath92 the energy ( [ energy definition p(x|y ) ] ) is now given by @xmath93 where the inverse temperature , @xmath20 , is defined as @xmath94 applying ( [ eq .",
    "diffrent energy - i ] ) , we finally receive the mutual information , @xmath95 for @xmath96 the mutual information for the bsc is restored @xcite @xmath97"
  ],
  "abstract_text": [
    "<S> three different approaches to derive mutual information via thermodynamics are presented where the temperature - dependent energy is given by : ( a ) @xmath0 $ ] , ( b ) @xmath1 $ ] or ( c ) @xmath2 $ ] . </S>",
    "<S> all approaches require the extension of the traditional physical framework and the modification of the 2nd law of thermodynamics . </S>",
    "<S> a realization of a physical system with an effective temperature - dependent hamiltonian is discussed followed by a suggestion of a physical information - heat engine . </S>"
  ]
}