{
  "article_text": [
    "systems are traditionally analyzed assuming continuous transmission of encoded symbols through the channel .",
    "however , in many practical applications such an assumption is not appropriate , and transmitting a codeword can be intermittent due to lack of synchronization , shortage of transmission energy , or burstiness of the system .",
    "the challenge is that the receiver does not explicitly know whether a given output symbol of the channel is the result of sending a symbol of the codeword or is simply a noise symbol containing no information about the message .",
    "intermittent communication introduced in this paper provides one model for non - contiguous transmission of codewords in such settings .    if the intermittent process is considered to be part of the channel behavior , then intermittent communication models a sporadically available channel in which at some time slots a symbol from the codeword is sent , and at others the receiver observes only noise .",
    "the model can be interpreted as an insertion channel in which some number of noise symbols are inserted between the codeword symbols . as another application ,",
    "if the intermittent process is considered to be part of the transmitter , then we say that the transmitter is intermittent .",
    "practical examples include energy harvesting systems in which the transmitter harvests energy usually from a natural source and uses it for transmission .",
    "assuming that there is a special input that can be transmitted with zero energy , the transmitter sends the symbols of the codeword if there is enough energy for transmission , and sends the zero - cost symbol otherwise .",
    "conceptually , two sets of related work are asynchronous communication and insertion / deletion channels .",
    "the former corresponds to contiguous transmission of codeword symbols in which the receiver observes noise before and after transmission , and the latter corresponds to a channel model in which some number of symbols are inserted at the output of the channel , or some of the input symbols of the channel are deleted .",
    "a key assumption of these communication models is that the receiver does not know a priori the positions of codeword / noise / deleted symbols , capturing certain kinds of asynchronism .",
    "generally , this asynchronism makes the task of the decoder more difficult since , in addition to the uncertainty about the message of interest , there is also uncertainty about the positions of the symbols .    an information theoretic model for asynchronous communication is developed in  @xcite with a single block transmission of length @xmath0 that starts at a random time @xmath1 unknown to the receiver , within an exponentially large window @xmath2 known to the receiver , where @xmath3 is called the asynchronism exponent . in this model ,",
    "the transmission is contiguous ; once it begins , the whole codeword is transmitted , and the receiver observes noise only both before and after transmission",
    ". originally , the model appears in  @xcite with the goal being to locate a sync pattern at the output of a memoryless channel using a sequential detector without communicating a message to the receiver .",
    "the same author extend the framework for modeling asynchronous communication in  @xcite , where communication rate is defined with respect to the decoding delay , and give bounds on the capacity of the system .",
    "capacity per unit cost for asynchronous communication is studied in  @xcite .",
    "the capacity of asynchronous communication with a simpler notion of communication rate , i.e. , with respect to the codeword length , is a special case of the result in  @xcite . in  @xcite",
    ", it is shown that if the decoder is required to both decode the message and locate the codeword exactly , the capacity remains unchanged , and can be universally achieved .",
    "also , it is shown that the channel dispersion is not affected because of asynchronism . in a recent work",
    "@xcite , the authors study the capacity per unit cost for asynchronous communication if the receiver is constrained to sample only a fraction of the channel outputs , and they show that the capacity remains unchanged under this constraint .    in  @xcite , a slotted asynchronous channel model is investigated and fundamental limits of asynchronous communication in terms of miss detection and false alarm error exponents are studied . a more general result in the context of single - message unequal error protection ( uep )",
    "is given in  @xcite .",
    "although we formally introduce the system model of intermittent communication in section  [ sec - system - model ] , we would like to briefly compare asynchronous communication with intermittent communication .",
    "first , as contrasted in figure  [ fig - asynch - inter - compare ] , the transmission of codeword symbols is contiguous in asynchronous communication , whereas it is bursty in intermittent communication .",
    "second , the receive window @xmath4 is exponential with respect to the codeword length @xmath0 in asynchronous communication , but as we will see it is linear in intermittent communication . finally , in both models , we assume that the receiver does not know a priori the positions of codeword symbols .",
    ".8     .8     non - contiguous transmission of codeword symbols can be described by the following insertion channel : after the @xmath5 symbol of the codeword , @xmath6 fixed noise symbols @xmath7 are inserted , where @xmath6 , @xmath8 are random variables , possibly independent and identically distributed ( iid ) .",
    "the resulting sequence passes through a discrete memoryless channel , and the receiver should decode the message based on the output of the channel without knowing the positions of the codeword symbols . as will see later , if @xmath9 is the random variable denoting the total number of received symbols , the intermittency rate @xmath10 as @xmath11 will be an important parameter of the system .",
    "to the best of our knowledge , this insertion channel model has not been studied before .",
    "a more general class of channels with synchronization errors is studied in  @xcite in which every transmitted symbol is independently replaced with a random number of the same symbol , possibly empty string to model a deletion event .",
    "dobrushin  @xcite proved the following characterization of the capacity for such iid synchronization error channels .",
    "[ dobrushin ] ( @xcite ) : for iid synchronization error channels , let @xmath12 denote the channel input sequence of length @xmath0 , and @xmath13 denote the corresponding output sequence at the decoder , where length @xmath14 is a random variable determined by the channel realization .",
    "the channel capacity is @xmath15    where @xmath16 denotes the average mutual information  @xcite .",
    "theorem  [ dobrushin ] demonstrates that iid synchronization error channels are information stable . however , there are two difficulties related to computing the capacity through this characterization .",
    "first , it is challenging to compute the mutual information because of the memory inherent in the joint distribution of the input and output sequences .",
    "second , the optimization over all the input distributions is computationally involved . a single - letter characterization for the capacity of the general class of synchronization error channels is still an open problem , even though there are many papers deriving bounds on the capacity of the insertion / deletion channels  @xcite .",
    "focusing on the lower bounds , gallager  @xcite considers a channel model with substitution and insertion / deletion errors and derives a lower bound on the channel capacity . in  @xcite , codebooks from first - order markov chains are used to improve the achievability results for deletion channels .",
    "the intuition is that it is helpful to put some memory in the codewords if the channel has some inherent memory .",
    "improved achievability results are obtained in  @xcite , with the latter directly lower bounds the information capacity given by   for channels with iid deletions and duplications with an input distribution following a symmetric , first - order markov chain .",
    "upper bounds on the capacity of iid insertion / deletion channels are obtained in  @xcite , in which the capacity of an axillary channel obtained by a genie - aided decoder ( and encoder ) with access to side - information about the insertion / deletion process is numerically evaluated .",
    "although our insertion channel model is different , we can apply some of these ideas and techniques to upper bound the capacity of intermittent communication .",
    "finally , bounds on the capacity per unit cost for a noisy channel with synchronization errors are given in a recent work  @xcite , in which an encoding / decoding scheme with only loose transmitter - receiver synchronization is proposed .",
    "we use the same approach for obtaining a lower bound on the capacity per unit cost of intermittent communication .      after introducing a model for intermittent communication in section  [ sec - system - model ] ,",
    "we develop two coding theorems for achievable rates to lower bound the capacity in section  [ sec - achv ] . toward this end , we introduce the notion of partial divergence and its properties . we show that , as long as the ratio of the receive window to the codeword length is finite and the capacity of the dmc is not zero , rate @xmath17 is achievable for intermittent communication ; i.e. , if there are only two messages , then the probability of decoding error vanishes as the codeword length becomes sufficiently large . by using decoding from exhaustive search and decoding from pattern detection , we obtain two achievable rates that are also valid for arbitrary intermittent processes .",
    "focusing on the binary - input binary - output noiseless case , we obtain upper bounds on the capacity of intermittent communication in section  [ sec - upper ] by providing the encoder and the decoder with various amounts of side - information , and calculating or upper bounding the capacity of this genie - aided system .",
    "although the gap between the achievable rates and upper bounds is fairly large , especially for large values of intermittency rate , the results suggest that linear scaling of the receive window with respect to the codeword length considered in the system model is relevant since the upper bounds imply a tradeoff between the capacity and the intermittency rate .",
    "finally , in section  [ sec - cap - per - unit - cost ] we obtain lower and upper bounds on the capacity per unit cost of intermittent communication . to obtain the lower bound",
    ", we use a similar approach to the one in  @xcite , in which pulse - position modulation codewords are used at the encoder , and the decoder searches for the position of the pulse in the output sequence .",
    "the upper bound is the capacity per unit cost of the dmc .",
    "we consider a communication scenario in which a transmitter communicates a single message @xmath18 to a receiver over a discrete memoryless channel ( dmc ) with probability transition matrix @xmath19 and input and output alphabets @xmath20 and @xmath21 , respectively .",
    "let @xmath22 denote the capacity of the dmc .",
    "also , let @xmath23 denote the `` noise '' symbol , which corresponds to the input of the channel when the transmitter is `` silent '' .",
    "the transmitter encodes the message as a codeword @xmath24 of length @xmath0 , which is the input sequence of intermittent process as shown in figure  [ fig - system - model ] .",
    "the intermittent process captures the burstiness of the channel or the transmitter and can be described as follows : after the @xmath5 symbol from the codeword , @xmath6 noise symbols @xmath7 are inserted , where @xmath6 s are iid geometric random variables with mean @xmath25 , where @xmath26 is the intermittency rate . as will see later , if @xmath9 is the random variable denoting the total number of received symbols , the intermittency rate @xmath10 as @xmath11 will be an important parameter of the system .",
    "in fact , the larger the value of @xmath3 , the larger the receive window , and therefore , the more intermittent the system becomes with more uncertainty about the positions of the codeword symbols ; if @xmath27 , the system is not intermittent and corresponds to contiguous communication .",
    "we call this scenario _ intermittent communication _ and denote it by the tuple @xmath28",
    ".    this model corresponds to an iid insertion channel model in which at each time slot a codeword symbol is sent with probability @xmath29 and the noise symbol @xmath7 is sent with probability @xmath30 until the whole codeword is transmitted .",
    "the output of intermittent process then goes through a dmc . at the decoder , there are @xmath14 symbols , where @xmath14 is a random variable having a negative binomial distribution with parameters @xmath0 and @xmath31 : @xmath32 with @xmath33}=\\alpha k$ ] , and we have @xmath34 therefore , the receive window @xmath14 scales linearly with the codeword length @xmath0 , as opposed to the exponential scaling in asynchronous communication .",
    "intermittent communication model represents bursty communication in which either the transmitter or the channel is bursty . in a bursty communication scenario , the receiver usually does not know the realization of the bursts .",
    "therefore , we assume that the receiver does not know the positions of the codeword symbols , making the decoder s task more involved .    assuming that the decoded message is denoted by @xmath35 , which is a function of the random sequence @xmath36 , and defining a code as in  @xcite",
    ", we say that rate @xmath37 is achievable if there exists a sequence of length @xmath0 codes of size @xmath38 with @xmath39 as @xmath11 .",
    "note that the communication rate is defined as @xmath40 .",
    "the capacity is the supremum of all the achievable rates .",
    "rate region @xmath41 is said to be achievable if the rate @xmath37 is achievable for the corresponding scenario with the intermittency rate @xmath3 .",
    "it can be seen that the result of theorem  [ dobrushin ] for iid synchronization error channels applies to intermittent communication model , and therefore , the capacity equals @xmath42 . denting the binary entropy function by @xmath43 , we have the following theorem .",
    "[ theorem - simple - lower ] for intermittent communication @xmath28 , rates not exceeding + @xmath44 are achievable .    we show that @xmath45 is a lower bound for the capacity of intermittent communication by lower bounding the mutual information .",
    "let vector @xmath46 denote the number of noise insertions in between the codeword symbols , where the @xmath6 s are iid geometric random variables with mean @xmath25 .",
    "we have @xmath47 where   follows from the fact that @xmath48 is independent of the insertion process @xmath49 , and conditioned on the positions of noise symbols @xmath49 , the mutual information between @xmath48 and @xmath36 equals the mutual information between input and output sequences of the dmc without considering the noise insertions ; where   follows by considering iid codewords and by the fact that conditioning can not increase the entropy ; and   and   follow from the fact that @xmath6 s are iid geometric random variables .",
    "finally , the result follows after dividing both sides by @xmath0 and considering the capacity achieving input distribution of the dmc .",
    "although the lower bound on the capacity of intermittent communication in theorem  [ theorem - simple - lower ] is valid for the specific intermittent process described above , our achievability results in section  [ sec - achv ] apply to an arbitrary insertion process as long as @xmath10 as @xmath11 .    at this point",
    ", we summarize several notations that are used throughout the sequel .",
    "we use @xmath50 and @xmath51 to denote quantities that grow strictly slower than their arguments and are polynomial in their arguments , respectively . by @xmath52 , we mean @xmath53 is distributed according to @xmath54 . for convenience ,",
    "we define @xmath55 , and more generally , @xmath56 . in this paper",
    ", we use the convention that @xmath57 if @xmath58 or @xmath59 , and the entropy @xmath60 if @xmath54 is not a probability mass function , i.e. , one of its elements is negative or the sum of its elements is larger than one .",
    "we also use the conventional definitions @xmath61 , and if @xmath62 , then @xmath63 .",
    "additionally , @xmath64 denotes an equality in exponential sense as @xmath11 , i.e. , @xmath65 of both sides are equal .",
    "we will make frequent use of the notations and results from the method of types , a powerful technique in large deviation theory developed by csiszr and krner  @xcite .",
    "let @xmath66 denote the set of distributions over the finite alphabet @xmath20 .",
    "the empirical distribution ( or type ) of a sequence @xmath67 is denoted by @xmath68 . a sequence @xmath69 is said to have a type @xmath54 if @xmath70 . the set of all sequences that have type @xmath54",
    "is denoted @xmath71 , or more simply @xmath72 .",
    "joint empirical distributions are denoted similarly .",
    "the set of sequences @xmath73 that have a conditional type @xmath19 given @xmath69 is denoted @xmath74 .",
    "a sequence @xmath67 is called @xmath54-typical with constant @xmath75 , denoted @xmath76_\\mu}$ ] , if @xmath77 is called @xmath19-typical conditioned on @xmath67 with constant @xmath75 , denoted @xmath78_\\mu}$ ] , if @xmath79 and @xmath80 , the kullback - leibler divergence between @xmath54 and @xmath81 is defined as @xmath82 and the conditional information divergence between @xmath19 and @xmath83 conditioned on @xmath54 is defined as @xmath84 the average mutual information between @xmath85 and @xmath86 and coupled via @xmath87 is denoted by @xmath88 . with these definitions , we now state the following lemmas , which are used throughout the paper",
    ".    [ fact - typ1 ] ( ( * ? ? ? * lemma 1.2.6 ) ) : if @xmath89 is an iid sequence according to @xmath81 , then the probability that it has a type @xmath54 is bounded by @xmath90 also , if the input @xmath67 to a memoryless channel @xmath91 has type @xmath54 , then the probability that the observed channel output sequence @xmath92 has a conditional type @xmath19 given @xmath69 is bounded by @xmath93    [ fact - typ2 ] ( ( * ? ? ? * lemma 1.2.12 ) ) : if @xmath89 is an iid sequence according to @xmath54 , then @xmath94_\\mu } ) \\ge 1- \\frac{|\\mathcal{x}|}{4n\\mu^2}.\\ ] ] also , if the input @xmath67 to a memoryless channel @xmath95 , and @xmath92 is the output , then @xmath96_\\mu}(x^n ) ) \\ge 1- \\frac{|\\mathcal{x}\\|\\mathcal{y}|}{4n\\mu^2},\\ ] ] and here the terms subtracted from @xmath97 could be replaced by exponentially small terms @xmath98 and @xmath99 , respectively .",
    "finally , we state a stronger version of the packing lemma  ( * ? ? ?",
    "* lemma 3.1 ) that will be useful in typicality decoding , and is proved in  ( * ? ? ?",
    "* equations ( 24 ) and ( 25 ) ) based on the method of types .    [ fact1 ]",
    "assume that @xmath100 and @xmath101 are independent , @xmath100 is generated iid according to @xmath54 , then @xmath102_\\mu}(c^n(m ) ) ) \\le \\text{poly}(n)e^{-n(\\mathbb{i}(p , w)-\\epsilon)}\\ ] ] for all @xmath4 sufficiently large , where @xmath103 can be made arbitrarily small by choosing a small enough typicality parameter @xmath75 .",
    "in this section , we obtain achievability results for intermittent communication based upon two decoding structures : _ decoding from exhaustive search _ , which attempts to decode the transmitted codeword from a selected set of output symbols without any attempt to first locate or detect the codeword symbols ; and _ decoding from pattern detection _ , which attempts to decode the transmitted codeword only if the selected outputs appear to be a pattern of codeword symbols .",
    "in order to analyze the probability of error for the second decoding structure , which gives a larger achievable rate , we use a generalization of the method of types that leads to the notion of _ partial divergence _ and its properties described in section  [ subsec - partial - divergence ] .",
    "we also show that rate @xmath17 is achievable for intermittent communication with finite intermittency rate , using the properties of partial divergence .",
    "although the system model in section  [ sec - system - model ] assumes iid geometric insertions , the results of this section apply to a general intermittent process , as we point out in remark  [ remark - achv - general ] .",
    "we will see in section  [ subsec - achv - rates ] that a relevant function is @xmath104 , which we call partial divergence and view as a generalization of the kullback - leibler divergence . in this section",
    ", we examine some of the interesting properties of partial divergence that provide insights about the achievable rates in section  [ subsec - achv - rates ] . loosely speaking ,",
    "partial divergence is the normalized exponent of the probability that a sequence with independent elements generated partially according to one distribution and partially according to another distribution has a specific type .",
    "this exponent is useful in characterizing a decoder s ability to distinguish a sequence obtained partially from the codewords and partially from the noise from a codeword sequence or a noise sequence .",
    "the following lemma is a generalization of the method of types  @xcite .",
    "a different characterization and proof of lemma  [ lemma1 ] can be found in  @xcite .",
    "[ lemma1 ] consider the distributions @xmath105 on a finite alphabet @xmath20 .",
    "a random sequence @xmath106 is generated as follows : @xmath107 symbols iid according to @xmath108 and @xmath109 symbols iid according to @xmath110 , where @xmath62 .",
    "the normalized exponent of the probability that @xmath106 has type @xmath54 is @xmath111    with a little abuse of notation , let @xmath112 and @xmath113 be the sequence of symbols in @xmath106 that are iid according to @xmath108 , @xmath110 , respectively . if these sequences have types @xmath114 and @xmath115 , respectively , then the whole sequence @xmath106 has type @xmath116 .",
    "therefore , we have @xmath117 where :   follows from the disjointness of the events in   since a sequence has a unique type ;   follows from the independence of the events in   and obtaining the probability of each of them according to lemma  [ fact - typ1 ] to first order in the exponent ; and   follows from the fact that the number of different types is polynomial in the length of the sequence  @xcite , which makes the total number of terms in the summation   polynomial in @xmath0 , and therefore , the exponent equals the largest exponent of the terms in the summation  .    specializing lemma  [ lemma1 ] for @xmath118 results in lemma  [ fact - typ1 ] , and we have @xmath119 .",
    "however , we will be interested in the special case of lemma  [ lemma1 ] for which @xmath120 .",
    "in other words , we need to upper bound the probability that a sequence has a type @xmath54 if its elements are generated independently with a fraction @xmath121 according to @xmath108 and the remaining fraction @xmath122 according to @xmath54 . for this case",
    ", we call @xmath123 the partial divergence between @xmath54 and @xmath108 with mismatch ratio @xmath62 . proposition  [ prop1 ] gives an explicit expression for the partial divergence by solving the optimization problem in   for the special case of @xmath120 .",
    "[ prop1 ] if @xmath124 and @xmath125 , where @xmath126 and @xmath127 and we assume that all values of the pmf @xmath108 are nonzero , then partial divergence can be written as @xmath128 where @xmath129 is a function of @xmath121 , @xmath54 , and @xmath108 , and can be uniquely determined from @xmath130    a proof of the proposition uses karush - kuhn - tucker ( kkt ) conditions since the optimization problem in   is convex , but is omitted due to space considerations .",
    "the next proposition states some of the properties of the partial divergence , which will be applied in the sequel .",
    "[ prop2 ] the partial divergence @xmath131 , where all of the elements of the pmf @xmath108 are nonzero , has the following properties :    a.   @xmath132 .",
    "[ a ] b.   @xmath133 .",
    "[ b ] c.   partial divergence is zero if @xmath134 , i.e. , @xmath135 .",
    "d.   let @xmath136 denote the derivative of the partial divergence with respect to @xmath121 , then @xmath137 .",
    "[ c ] e.   if @xmath138 , then @xmath139 , for all @xmath140 , i.e. , partial divergence is increasing in @xmath121 .",
    "[ d ] f.   if @xmath138 , then @xmath141 , for all @xmath62 , i.e. , partial divergence is convex in @xmath121 .",
    "[ e ] g.   @xmath142 .",
    "[ f ]    see appendix  [ app - proof - prop2 ] .",
    "figure  [ f : partial diver ] shows examples of the partial divergence for pmf s with alphabets of size 4 .",
    "specifically , @xmath104 versus @xmath121 is sketched for @xmath143 , and two different @xmath108 s , @xmath144 and @xmath145 .",
    "the properties of proposition  [ prop2 ] are apparent in the figure for these examples .",
    "[ prop2 - 2 ] the partial divergence @xmath131 , satisfies @xmath146    from the definition of the partial divergence and  , we have @xmath147 where   follows from the convexity of the kullback - leibler divergence ; and   follows from the constraint @xmath148 in the minimization .",
    "the interpretation of proposition  [ prop2 - 2 ] is that if all the elements of a sequence are generated independently according to a mixture probability @xmath149 , then it is more probable that this sequence has type @xmath54 than in the case that a fraction @xmath121 of its elements are generated independently according to @xmath108 and the remaining fraction @xmath122 are generated independently according to @xmath54 . since the partial divergence @xmath104 is used to obtain achievability results in theorem  [ th - achv2-basic ] , it can be substituted with the less complicated function @xmath150 in   with the expense of loosening the bound according to proposition  [ prop2 - 2 ] .    using the results on partial divergence",
    ", we now state a result about the achievability at rate @xmath17 in the following theorem .",
    "the idea is that no matter how large the intermittency threshold becomes , as long as it is finite , the receiver can distinguish between two messages with vanishing probability of error .",
    "[ theorem - rate - zero ] if the intermittency rate is finite and the capacity of the dmc , @xmath22 , is non - zero , then rate @xmath17 is achievable .",
    "we need to show that the transmission of a message @xmath151 is reliable for intermittent communication @xmath28 as @xmath11 .",
    "encoding : if @xmath152 , then transmit symbol @xmath7 at all the times , i.e. , @xmath153 . if @xmath154 , then transmit symbol @xmath155 at all the times , i.e. , @xmath156 , where we consider the symbol @xmath157 . if the capacity of the dmc is nonzero , then @xmath158 .",
    "decoding : for arbitrarily small @xmath159 , if @xmath160 , then the decoder declares an error ; otherwise , if the sequence @xmath161 has type @xmath162 with a fixed typicality parameter @xmath163 , i.e. , @xmath164_\\mu}$ ] , then @xmath165 ; otherwise @xmath166 .    analysis of the probability of error : the average probability of error can be bounded as @xmath167_\\mu}|m\\!=\\!1,n\\!=\\!n ) \\!+\\!\\mathbb{p } ( y^n \\!\\in\\ ! t_{[w_\\star]_\\mu}|m\\!=\\!2,n\\!=\\!n)\\right ) \\label{eq - proof - the - rate - zero-1 } \\\\ &",
    "\\le o(1)+\\max_{n:|n / k-\\alpha|<\\epsilon}(o(1)+e^{-nd_{k / n}(w_\\star\\|w_{x^ * } ) } ) \\label{eq - proof - the - rate - zero-2 } \\\\ & \\le o(1)+ e^{-k(\\alpha-\\epsilon ) d_{1/(\\alpha+\\epsilon)}(w_\\star\\|w_{x^ * } ) } \\to 0 \\text { as } k \\to \\infty , \\label{eq - proof - the - rate - zero-3}\\end{aligned}\\ ] ] where :   follows from the union bound ;   follows from the fact that @xmath168 as @xmath11 , lemma  [ fact - typ2 ] and lemma  [ lemma1 ] ; and   follows from the fact that @xmath169 according to proposition  [ prop2 ] since @xmath158 and @xmath170 .    in order to prove achievability results for the case of an exponential number of messages , i.e. , @xmath171 , we introduce two decoding algorithms in the following section .      in this section ,",
    "two decoding structures are introduced .",
    "the encoding structure is identical for both : given an input distribution @xmath54 , the codebook is randomly and independently generated , i.e. , all @xmath172 are iid according to @xmath54 .",
    "although we focus on typicality for detection and decoding for ease of analyzing the probability of error , other algorithms such as maximum likelihood decoding could in principle be used in the context of these decoding structures .",
    "however , detailed specification and analysis of such structures and algorithms are beyond the scope of this paper .",
    "note that the number of received symbols at the decoder , @xmath14 , is a random variable .",
    "however , using the same procedure as in the proof of theorem  [ theorem - rate - zero ] , we can focus on the case that @xmath173 , and essentially assume that the receive window is of length @xmath174 , which makes the analysis of the probability of error for the decoding algorithms more concise .    * decoding from exhaustive search : * in this structure , the decoder observes the @xmath4 symbols of the output sequence @xmath73 , chooses @xmath0 of them , resulting in a subsequence denoted by @xmath175 , and performs joint typicality decoding with a fixed typicality parameter @xmath163 , i.e. , checks if @xmath176_\\mu}(c^k(m))$ ] for a unique index @xmath177 . in words",
    ", this condition corresponds to the joint type for codeword @xmath24 and selected outputs @xmath175 being close to the joint distribution induced by @xmath24 and the channel @xmath178 .",
    "if the decoder finds a unique @xmath177 satisfying this condition , it declares @xmath177 as the transmitted message .",
    "otherwise , it makes another choice for the @xmath0 symbols from symbols of the sequence @xmath73 and again attempts typicality decoding .",
    "if at the end of all @xmath179 choices the typicality decoding procedure does not declare any message as being transmitted , then the decoder declares an error .    *",
    "decoding from pattern detection : * this structure involves two stages for each choice of the output symbols . as in decoding from exhaustive search , the decoder chooses @xmath0 of the @xmath4 symbols from the output sequence @xmath73 .",
    "let @xmath175 denote the subsequence of the chosen symbols , and @xmath180 denote the subsequence of the other symbols .",
    "for each choice , the first stage checks if this choice of the output symbols is a good one , which consists of checking if @xmath175 is induced by a codeword , i.e. , if @xmath181 , and if @xmath180 is induced by noise , i.e. , if @xmath182 .",
    "if both of these conditions are satisfied , then we perform typicality decoding with @xmath175 over the codebook as in the decoding from exhaustive search , which is called the second stage here .",
    "otherwise , we make another choice for the @xmath0 symbols and repeat the two - stage decoding procedure . at any step that we run the second stage ,",
    "if the typicality decoding declares a message as being sent , then decoding ends .",
    "if the decoder does not declare any message as being sent by the end of all @xmath179 choices , then the decoder declares an error . in this structure ,",
    "we constrain the search domain for the typicality decoding ( the second stage ) only to typical patterns by checking that our choice of codeword symbols satisfies the conditions in the first stage .    in decoding from pattern detection , the first stage essentially distinguishes a sequence obtained partially from the codewords and partially from the noise from a codeword sequence or a noise sequence . as a result , in the analysis of the probability of error , partial divergence and its properties described in section  [ subsec - partial - divergence ]",
    "play a role .",
    "this structure always outperforms the decoding from exhaustive search structure , and their difference in performance indicates how much the results on the partial divergence improve the achievable rates .      in this section , we obtain two achievable rates for intermittent communication using the decoding algorithms introduced in section  [ subsec - decode - algorithm ] .    [ th - achv1-basic ] using decoding from exhaustive search for intermittent communication with @xmath28 , rates not exceeding @xmath183 are achievable .",
    "the proof of the theorem is removed since the analysis of the probability of error is similar to that of theorem  [ th - achv2-basic ] , except that instead of breaking down the first term in   as in  , we use the union bound over all the @xmath179 choices without trying to distinguish the output symbols based on their empirical distributions .    note that @xmath45 is the same as the lower bound in theorem  [ theorem - simple - lower ] , but here we introduced an explicit decoding structure which is also valid for a general intermittent process .",
    "the form of the achievable rate is reminiscent of communications overhead as the cost of constraints  @xcite , where the constraint is the system s burstiness or intermittency , and the overhead cost is @xmath184 .",
    "note that the overhead cost is increasing in the intermittency rate @xmath3 , is equal to zero at @xmath27 , and approaches infinity as @xmath185 .",
    "these observations suggest that increasing the receive window makes the decoder s task more difficult .",
    "[ th - achv2-basic ] using decoding from pattern detection for intermittent communication with @xmath28 , rates not exceeding @xmath186 are achievable , where @xmath187    see appendix  [ app - proof - th - achv2-basic ] .",
    "[ remark - achv - general ] the results of theorems  [ theorem - rate - zero ] ,  [ th - achv1-basic ] , and  [ th - achv2-basic ] are valid for an arbitrary intermittent process in figure  [ fig - system - model ] as long as @xmath10 as @xmath11 .",
    "[ remark - interpretation ] the achievable rate in theorem  [ th - achv2-basic ] can be expressed as follows : rate @xmath37 is achievable if for any mismatch @xmath188 , we have @xmath189 the interpretation is that the total amount of uncertainty should be smaller than the total amount of information .",
    "specifically , @xmath37 and @xmath190 are the amount of uncertainty in codewords and patterns , respectively , and @xmath88 and @xmath191 are the amount of information about the codewords and patterns , respectively .    the achievable rate in theorem  [ th - achv2-basic ] is larger than the one in theorem  [ th - achv1-basic ] , because decoding from pattern detection utilizes the fact that the choice of the codeword symbols at the receiver might not be a good one , and therefore , restricts the typicality decoding only to the typical patterns and decreases the search domain . in theorem",
    "[ th - achv2-basic ] , the overhead cost for a fixed input distribution is @xmath192 .",
    "using the properties of partial divergence , we state some properties of this overhead cost in the next proposition .    [ prop : overheadf - basic ] the overhead cost @xmath192 in   has the following properties :    a.   the maximum of the term in   occurs in the interval @xmath193 $ ] , i.e. , instead of the maximization over @xmath188 , @xmath192 can be found by the same maximization problem over @xmath194 .",
    "[ prop3-a - basic ] b.   @xmath192 is increasing in @xmath3 .",
    "[ prop3-b - basic ] c.   @xmath195 .",
    "[ prop3-bb - basic ] d.   if @xmath196 is finite , then @xmath197 as @xmath185",
    ". [ prop3-bbb - basic ]    see appendix  [ app - proof - prop : overheadf - basic ] .",
    "note that part ( [ prop3-b - basic ] ) in proposition  [ prop : overheadf - basic ] indicates that increasing the intermittency rate or the receive window increases the overhead cost , resulting in a smaller achievable rate .",
    "parts ( [ prop3-bb - basic ] ) and ( [ prop3-bbb - basic ] ) show that the achievable rate is equal to the capacity of the channel for @xmath27 and approaches zero as @xmath185 .    the results in this section can be extended to packet - level intermittent communication in which the intermittency is modeled at the packet level , and regimes that lie between the non - contiguous transmission of codeword symbols in intermittent communication , and the contiguous transmission of codeword symbols in asynchronous communication are explored .",
    "specifically , the system model in this paper can be generalized to small packet , medium packet , and large packet intermittent communication .",
    "see , for example ,  @xcite .    now consider a binary symmetric channel ( bsc ) for the dmc in figure  [ fig - system - model ] with the crossover probability @xmath198 , and the noise symbol @xmath199 .",
    "figure  [ f : bsc-2 ] shows the value of the achievable rates for different @xmath200 , versus @xmath3 .",
    "@xmath201 denotes the achievable rate obtained from theorem  [ th - achv2-basic ] if the channel is noiseless ( @xmath202 ) , and can be proven to be equal to @xmath203 .    as we can see from the plot , the achievable rate in theorem  [ th - achv2-basic ] ( indicated by",
    " @xmath204 \" ) is always larger than the one in theorem  [ th - achv1-basic ] ( indicated by `` @xmath45 '' ) since decoding from pattern detection takes advantage of the fact that the choice of the @xmath0 output symbols might not be a good one .",
    "specifically , the exponent obtained in lemma  [ lemma1 ] in terms of the partial divergence helps the decoder detect the right symbols , and therefore , achieve a larger rate .",
    "the arrows in figure  [ f : bsc-2 ] show this difference and suggest that the benefit of using decoding from pattern detection is larger for increasing @xmath3 .",
    "note that the larger @xmath3 is , the smaller the achievable rate would be for a fixed @xmath200 .",
    "not surprisingly , as @xmath205 , the capacity of the bsc is approached for both of the achievable rates . in this example",
    ", we can not achieve a positive rate if @xmath206 , even for the case of a noiseless channel ( @xmath202 ) .",
    "however , this is not true in general , because even the first achievable rate can be positive for a large @xmath3 , if the capacity of the channel @xmath22 is sufficiently large .",
    "the results suggest that , as communication becomes more intermittent and @xmath3 becomes larger , the achievable rate is decreased due to the additional uncertainty about the positions of the codeword symbols at the decoder .",
    "in this section , we focus on obtaining upper bounds on the capacity of a special case of intermittent communication in which the dmc in figure  [ fig - system - model ] is binary - input binary - output noiseless with the noise symbol @xmath199 . the achievable rate for this case is denoted by @xmath201 in section  [ subsec - achv - rates ] , and is shown by the blue curve in figure  [ f : bsc-2 ] .",
    "similar to  @xcite , upper bounds are obtained by providing the encoder and the decoder with various amounts of side - information , and calculating or upper bounding the capacity of this genie - aided system . after introducing a useful function @xmath207 in section  [ subsec - auxiliary ] ,",
    "we obtain upper bounds in section  [ subsec - genie ] .",
    "the techniques of in this section can in principle be applied to non - binary and noisy channels as well ; however , the computational complexity for numerical evaluation of the genie - aided system grows very rapidly in the size of the alphabets .",
    "let @xmath208 and @xmath209 be two integer numbers such that @xmath210 , and consider a discrete memoryless channel for which at each channel use the input consists of a sequence of @xmath208 bits and the output consists of a sequence of @xmath209 bits , i.e. , the input and output of this channel are @xmath211 and @xmath212 , respectively . at each channel use ,",
    "@xmath213 zeroes are inserted randomly and uniformly among the input symbols .",
    "the positions at which the insertions occur randomly takes on each of the possible @xmath214 realizations with equal probability , and is unknown to the transmitter and the receiver . as an example , the transition probability matrix of this channel for the case of @xmath215 and @xmath216 is reported in table  [ tab-1 ] .",
    ".transition probabilities @xmath217 for the auxiliary channel with @xmath215 and @xmath216 . [ cols=\"<,^,^,^,^,^,^,^,^\",options=\"header \" , ]     the capacity of the auxiliary channel",
    "is defined as @xmath218 where @xmath219 is the input distribution .",
    "the exact value of the function @xmath207 for finite @xmath208 and @xmath209 can be numerically computed by evaluating the transition probabilities @xmath217 and using the blahut algorithm  @xcite to maximize the mutual information .",
    "the computational complexity of the blahut algorithm increases exponentially for large values of @xmath208 and @xmath209 since the transition probability matrix is of size @xmath220 . in order to partially overcome this issue",
    ", we recall the following lemma .",
    "[ lemma - sep - cap ] ( ( * ? ? ?",
    "* problem 7.28 ) ) : consider a channel that is the union of @xmath221 memoryless channels @xmath222 with capacities @xmath223 , where at each time one can send a symbol over exactly one of the channels .",
    "if the output alphabets are distinct and do not intersect , then the capacity @xmath224 of this channel can be characterized in terms of @xmath223 in bits per channel use as @xmath225    now , notice that the function @xmath207 can be evaluated by considering the union of @xmath226 memoryless channels with distinct input and output alphabets , where the input of the @xmath5 channel is binary sequences with length @xmath208 and weight @xmath227 , @xmath228 , and the output is binary sequences with length @xmath209 obtained from the input sequence by inserting @xmath213 zeroes uniformly .",
    "the weight of the sequences remains fixed after zero insertions , and therefore , the output alphabets are also distinct and do not intersect .",
    "assuming that the capacity of the @xmath5 channel is @xmath229 and applying lemma  [ lemma - sep - cap ] , we have @xmath230 it is easy to see that @xmath231 and @xmath232 . for other values of @xmath221 , the capacity @xmath233 can be evaluated numerically using the blahut algorithm , where input and output alphabets have sizes @xmath234 and @xmath235 , respectively , which are considerably less than those of the original alphabet sizes .",
    "this reduction allows us to obtain the function @xmath207 for larger values of @xmath208 and @xmath209 , which will be useful in section  [ subsec - genie ] .",
    "the largest value of @xmath209 for which we are able to evaluate the function @xmath207 for all values of @xmath236 is @xmath237 .",
    "although we can not obtain a closed - form expression for the function @xmath207 , we can find a closed - form upper bound by expanding the mutual information in   and bounding some of its terms . as a result",
    ", we can find upper bounds on the function @xmath207 for larger values of @xmath208 and @xmath209 .",
    "first , we introduce some notation .",
    "for a binary sequence @xmath238 , let @xmath239 denote the weight of the sequence , i.e. , number of @xmath97 s .",
    "also , let the vector @xmath240 of length @xmath241 denote the length of consecutive @xmath242 s in the sequence @xmath243 such that @xmath244 if @xmath245 , i.e. , the binary sequence starts with @xmath97 , and @xmath246 if @xmath247 , i.e. , the binary sequence ends with @xmath97 , and all the other elements of the vector @xmath248 are positive integers .",
    "in addition , let the vector @xmath249 of length @xmath250 denote the length of consecutive @xmath97 s in the sequence @xmath243 with length larger than one , i.e. , runs of @xmath97 s with length one not counted .",
    "finally , let @xmath251 . if it is clear from the context , we drop the argument @xmath243 from these functions .",
    "for example , if @xmath252 , then @xmath253 , @xmath254 , @xmath255 , and @xmath256 . as another example , if @xmath257 , then @xmath258 , @xmath259 , @xmath260 , and @xmath261 .",
    "now , we define the following function , which will be used for expressing the upper bound , @xmath262 where @xmath263 , @xmath264 , and @xmath265 are a function of @xmath243 as defined before , and we have @xmath266    [ prop - lower - upper - bound ] the function @xmath207 in   satisfies @xmath267 where @xmath268 is defined in",
    ".    see appendix  [ app - proof - prop - lower - upper - bound ] .    similarly , it is possible to obtain a lower bound on the function @xmath207 .",
    "the lower bound and a numerical comparison between the lower bound , the exact value , and the upper bound on the function @xmath207 can be found in  @xcite",
    ". the following definition will be useful in expressing the upper bounds in section  [ subsec - genie ] .",
    "@xmath269 note that the function @xmath270 quantifies the loss in capacity due to the uncertainty about the positions of the insertions , and can not be negative .",
    "the following proposition characterizes some of the properties of the functions @xmath207 and @xmath270 , which will be used later .",
    "[ prop - auxil1 ] the functions @xmath207 and @xmath270 have the following properties :    a.   @xmath271 , @xmath272 .",
    "[ prop - aux - a ] b.   @xmath273 , @xmath274 .",
    "[ prop - aux - b ] c.   @xmath275 , @xmath276 .",
    "[ prop - aux - c ] d.   @xmath277 , @xmath278 .",
    "[ prop - aux - d ] e.   @xmath279 , @xmath280 .",
    "[ prop - aux - e ]    see appendix  [ app - proof - prop - auxil1 ] .",
    "in this section , we focus on upper bounds on the capacity of binary - input binary - output noiseless intermittent communication .",
    "the procedure is similar to  @xcite .",
    "specifically , we obtain upper bounds by giving some kind of side - information to the encoder and decoder , and calculating or upper bounding the capacity of this genie - aided channel .",
    "now we introduce one form of side - information .",
    "assume that the position of the @xmath281^{th}$ ] codeword symbol in the output sequence is given to the encoder and decoder for all @xmath282 and a fixed integer number @xmath283 .",
    "we assume that the codeword length is a multiple of @xmath284 , so that @xmath285 is an integer , and is equal to the total number of positions that are provided as side - information .",
    "this assumption does not impact the asymptotic behavior of the channel as @xmath11 .",
    "we define the random sequence @xmath286 as follows : @xmath287 is equal to the position of the @xmath288^{th}$ ] codeword symbol in the output sequence , and for @xmath289 , @xmath290 is equal to the difference between the positions of the @xmath281^{th}$ ] codeword symbol and @xmath291^{th}$ ] codeword symbol in the output sequence .",
    "since we assumed iid insertions , the random sequence @xmath286 is iid as well with negative binomial distribution : @xmath292 with mean @xmath293}=(s+1)/p_t$ ] .",
    "also , note that as @xmath11 , by the law of large numbers , we have @xmath294}=\\frac{s+1}{p_t}.\\ ] ]    let @xmath295 denote the capacity of the channel if we provide the encoder and decoder with side - information on the random sequence @xmath286 , which is clearly an upper bound on the capacity of the original channel . with this side - information , we essentially partition the transmitted and received sequences into @xmath296 contiguous blocks that are independent from each other . in the @xmath5 block the place of the @xmath288^{th}$ ] codeword symbol",
    "is given , which can convey one bit of information .",
    "other than that , the @xmath5 block has @xmath297 input bits and @xmath298 output bits with uniform @xmath242 insertions .",
    "therefore , the information that can be conveyed through the @xmath5 block equals @xmath299 .",
    "thus , we have @xmath300 } \\label{cap1-h2 } \\\\",
    "= & \\frac{1}{s+1 } \\left [ 1+\\sum_{b = s}^{\\infty } \\binom{b}{s}(1-p_t)^{b - s } p_t^{s+1 } g(s , b ) \\right ] \\label{cap1-h3 } \\\\ = & 1- \\frac{1}{s+1 } \\sum_{b = s}^{\\infty } \\binom{b}{s}(1-p_t)^{b - s } p_t^{s+1 } \\phi(s , b ) , \\label{cap1-h4}\\end{aligned}\\ ] ] where :   follows from  ;   follows from the law of large numbers ;   follows from the distribution of @xmath290 s given in  ; and   follows from the definition  .",
    "note that the capacity @xmath295 can not be larger than @xmath97 , since the coefficients @xmath301 can not be negative .",
    "the negative term in   can be interpreted as a lower bound on the communication overhead as the cost of intermittency in the context of  @xcite .",
    "the expression in   gives an upper bound on the capacity of the original channel with @xmath302 .",
    "however , it is infeasible to numerically evaluate the coefficients @xmath303 for large values of @xmath209 . as we discussed before",
    ", the largest value of @xmath209 for which we are able to evaluate the function @xmath303 is @xmath304 . the following upper bound on @xmath295 results by truncating the summation in   and using part ( [ prop - aux - d ] ) of proposition  [ prop - auxil1 ] .",
    "@xmath305 the expression  , which we denote by @xmath306 , gives a nontrivial and computable upper bound for each value of @xmath307 on @xmath295 , and therefore , an upper bound on the capacity of the original channel with @xmath302 .",
    "figure  [ f : outer-2 ] shows the upper bounds for @xmath304 and @xmath308 versus the intermittency rate @xmath3 , along with the the achievability result .",
    "next , we introduce a second form of side - information .",
    "assume that for consecutive blocks of length @xmath297 of the output sequence , the number of codeword symbols within that block is given to the encoder and decoder as side - information , i.e. , the number of codeword symbols in the sequence @xmath309 , @xmath282 for a fixed integer number @xmath310 .",
    "let @xmath311 denote the capacity of the channel if we provide the encoder and decoder with this side - information .",
    "using a similar procedure , we obtain @xmath312 note that the summation in   is finite , and we do not need to upper bound @xmath311 as we did for @xmath295 .",
    "the value of @xmath311 gives nontrivial and computable upper bounds on the capacity of the original channel .",
    "figure  [ f : outer-3 ] shows the upper bounds for @xmath313 versus the intermittency rate @xmath3 , along with the the achievability result .",
    "the upper bound corresponding to @xmath314 is tighter than others for all ranges of @xmath3 , i.e. ,   is decreasing in @xmath297 .",
    "intuitively , this is because by decreasing @xmath297 , we provide the side - information more frequently , and therefore , the capacity of the resulting genie - aided system becomes larger .",
    "it seems that   gives better upper bounds for the range of @xmath3 shown in the figures ( @xmath315 ) .",
    "however , the other upper bound @xmath306 can give better results for the limiting values of @xmath185 or @xmath316 .",
    "we have @xmath317 this is because of the fact that by increasing @xmath3 , and thus decreasing @xmath31 , we have more zero insertions and the first kind of genie - aided system provides side - information less frequently leading to tighter upper bounds .",
    "the best upper bound for the limiting case of @xmath185 found by   is @xmath318 bits / s . in principle",
    ", we can use the upper bound on @xmath207 in proposition  [ prop - lower - upper - bound ] to upper bound @xmath295 and @xmath311 . by doing so",
    ", we can find the bounds for larger values of @xmath297 and @xmath319 , because we can calculate the upper bound   for larger arguments .",
    "it seems that this does not improve the upper bounds significantly for the range of @xmath3 shown in the figures .",
    "however , by upper bounding   via  , we can tighten the upper bound for the limiting case of @xmath185 to @xmath320 bits / s .",
    "although the gap between the achievable rates and upper bounds is not particularly tight , especially for large values of intermittency rate @xmath3 , the upper bounds suggest that the linear scaling of the receive window with respect to the codeword length considered in the system model is natural since there is a tradeoff between the capacity of the channel and the intermittency rate . by contrast , in asynchronous communication  @xcite , where the transmission of the codeword is contiguous , only exponential scaling @xmath2 induces a tradeoff between capacity and asynchronism .",
    "in this section , we obtain bounds on the capacity per unit cost of intermittent communication . let @xmath321 $ ] be a cost function that assigns a non - negative value to each channel input .",
    "we assume that the noise symbol has zero cost , i.e. , @xmath322 .",
    "the cost of a codeword is defined as @xmath323    a @xmath324 code consists of @xmath325 codewords of length @xmath0 , @xmath326 , each having cost at most @xmath54 with average probability of decoding error at most @xmath327 , where the intermittent process is the same as in section  [ sec - system - model ] .",
    "note that the cost of the input and output sequences of the intermittent process shown in figure  [ fig - system - model ] is the same since the cost of the noise symbols is zero .",
    "we say rate @xmath328 bits per unit cost is achievable if for every @xmath329 and large enough @xmath325 there exists a @xmath324 code with @xmath330 . for intermittent communication @xmath28 ,",
    "the capacity per unit cost @xmath331 is the supremum of achievable rates per unit cost .",
    "it is shown in  @xcite that the capacity per unit cost of a general dmc is @xmath332 where we assumed that @xmath322 , and the optimization is over the input alphabet instead of over the set of all input distributions . the asynchronous capacity per unit cost for asynchronous communication with timing uncertainty per information",
    "bit @xmath333 has been shown to be  @xcite @xmath334 therefore , comparing to the capacity per unit cost of a dmc , the rate is penalized by a factor of @xmath335 due to asynchronism . for a channel with iid synchronization errors with average number of duplications equal to @xmath75 concatenated with a dmc",
    ", bounds on the capacity per unit cost , @xmath336 , have been obtained in  @xcite as @xmath337 where the lower bound is obtained by using a type of pulse position modulation at the encoder and searching for the position of the pulse at the decoder .",
    "using similar encoding and decoding schemes , we obtain a lower bound for the capacity per unit cost of intermittent communication .",
    "[ theorem - bound - cap - per - unit ] the capacity per unit cost @xmath331 for intermittent communication @xmath28 satisfies @xmath338    the upper bound in   is the capacity per unit cost of the dmc @xmath19 , and follows by providing the decoder with side - information about the positions of inserted noise symbols @xmath7 .",
    "the derivation of the lower bound is similar to the one in  ( * ? ? ?",
    "* theorem 3 ) .",
    "essentially , the encoder uses pulse position modulation , i.e. , to transmit message @xmath177 , it transmits a burst of symbols @xmath339 of length @xmath340 at a position corresponding to this message and transmits the zero - cost noise symbol @xmath7 at the other @xmath341 positions before and after this burst , so that each codeword has cost @xmath342 . in order to decode the message",
    ", we search for the location of the pulse using a sliding window with an appropriate length looking for a subsequence that has a type equal to @xmath343 , because at the receiver , we expect to have approximately @xmath344 inserted noise symbols @xmath7 in between the @xmath345 burst symbols @xmath339 .",
    "similar to the analysis of  ( * ? ? ?",
    "* theorem 3 ) , it can be shown that the probability of decoding error vanishes as @xmath346 , and the rate per unit cost is @xmath347 finally , by choosing the optimum input symbol @xmath339 , rate per unit cost equal to the left - hand side of   can be achieved .    from the convexity of the kullback - leibler divergence",
    ", it can be seen that the lower bound is always smaller than half of the upper bound .",
    "consider the bsc example with crossover probability @xmath348 and input costs @xmath349 and @xmath350 .",
    "the upper bound in   equals @xmath351 bits per unit cost , and the lower bound in   is plotted in figure  [ f - cap - per - unit - cost ] versus the intermittency rate @xmath3 . as we would expect , the lower bound decreases as the intermittency increases .",
    "we formulated a model for intermittent communications that can capture bursty transmissions or a sporadically available channel by inserting a random number of silent symbols between each codeword symbol so that the receiver does not know a priori when the transmissions will occur .",
    "first , we specified two decoding schemes in order to develop achievable rates .",
    "interestingly , decoding from pattern detection , which achieves a larger rate , is based on a generalization of the method of types and properties of partial divergence .",
    "as the system becomes more intermittent , the achievable rates decrease due to the additional uncertainty about the positions of the codeword symbols at the decoder .",
    "we also showed that as long as the intermittency rate @xmath3 is finite and the capacity of the dmc is not zero , rate @xmath17 is achievable for intermittent communication . for the case of binary - input binary - output noiseless channel",
    ", we obtained upper bounds on the capacity of intermittent communication by providing the encoder and the decoder with various amounts of side - information , and calculating or upper bounding the capacity of this genie - aided system .",
    "the results suggest that the linear scaling of the receive window with respect to the codeword length considered in the system model is relevant since the upper bounds imply a tradeoff between the capacity and the intermittency rate . finally , we derived bounds on the capacity per unit cost of intermittent communication . to obtain the lower bound",
    ", we used pulse - position modulation at the encoder , and searched for the position of the pulse at the decoder .",
    "a.   from  , we observe that @xmath352 as @xmath353 . therefore , from  , we have @xmath354 for @xmath352 .",
    "b.   from  , we observe that @xmath355 as @xmath356 .",
    "therefore , from  , we have @xmath357 for @xmath355 .",
    "c.   if @xmath134 , then   simplifies to @xmath358 , and therefore @xmath359 . by substituting @xmath129 into   and because @xmath360 , we obtain @xmath361 d.   by taking the derivative of   with respect to @xmath121 , we obtain @xmath362 where   is obtained by using  .",
    "therefore , @xmath363 because from  , we have @xmath364 .",
    "e.   according to  , in order to prove @xmath365 , it is enough to show that @xmath366 : + @xmath367 + where   holds according to the cauchy - schwarz inequality , and   is true because @xmath368 where   comes from  .",
    "note that the cauchy - schwarz inequality in   can not hold with equality for @xmath369 ( and therefore , for @xmath370 ) , because otherwise , @xmath371 , and @xmath134 . from  , @xmath372 , which results in the desirable inequality @xmath373 .",
    "f.   by taking the derivative of   with respect to @xmath121 , it can easily be seen that @xmath374 also , by taking the derivative of   with respect to @xmath121 and after some calculation , we have @xmath375 therefore , @xmath376 where   holds according to the cauchy - schwarz inequality , which can not hold with equality since otherwise @xmath134 , and where   follows from  . from  , @xmath377 , which implies that @xmath378 according to  .",
    "g.   from part  , @xmath104 is convex in @xmath121 , and therefore , @xmath379 is increasing in @xmath121 .",
    "in addition , from part  , @xmath380 , and from part  , @xmath381 . consequently , @xmath142 .",
    "fix the input distribution @xmath54 , and consider decoding from pattern detection described in sections  [ subsec - decode - algorithm ] . for any @xmath382",
    ", we prove that if @xmath383 , then the average probability of error vanishes as @xmath11 .",
    "we have @xmath384 where   follows from the union bound in which the second term is the probability that the decoder declares an error ( does not find any message ) at the end of all @xmath179 choices , which implies that even if we pick the correct output symbols , the decoder either does not pass the first stage or does not declare @xmath152 in the second stage",
    ". therefore , @xmath385_\\mu } ) + \\mathbb{p}(y_\\star^{n - k } \\notin t_{[w_\\star]_\\mu } ) + \\mathbb{p}(y^k \\notin t_{[w]_\\mu}(c^k(1 ) ) ) \\label{e : secterm } \\\\ & \\to 0 , \\text { as } k \\to \\infty , \\label{e : secterm1}\\end{aligned}\\ ] ] where @xmath386 is the output of the channel if the input is @xmath387 , and @xmath388 is the output of the channel if the input is the noise symbol , and where we use the union bound to establish  .",
    "the limit   follows because all the three terms in   vanish as @xmath11 according to lemma  [ fact - typ2 ] .",
    "the first term in   is more challenging .",
    "it is the probability that for at least one choice of the output symbols , the decoder passes the first stage and then the typicality decoder declares an incorrect message .",
    "we characterize the @xmath179 choices based on the number of incorrectly chosen output symbols , i.e. , the number of symbols in @xmath175 that are in fact output symbols corresponding to a noise symbol , which is equal to the number of symbols in @xmath180 that are in fact output symbols corresponding to a codeword symbol . for any @xmath389 , there are @xmath390 choices .. ] using the union bound for all the choices and all the messages @xmath391 , we have @xmath392 where the index @xmath393 in   denotes the condition that the number of wrongly chosen output symbols is equal to @xmath393 .",
    "note that message @xmath394 is declared at the decoder only if it passes the first and the second stage .",
    "therefore , @xmath395_\\mu}\\ }   \\cap \\{\\hat{y}^{n - k } \\in t_{[w_\\star]_\\mu}\\ } \\cap",
    "\\{\\tilde{y}^k \\in t_{[w]_\\mu}(c^k(2))\\}|m=1\\bigg ) \\notag \\\\ & = \\mathbb{p}_{k_1}(\\tilde{y}^k \\in t_{[pw]_\\mu})\\cdot \\mathbb{p}_{k_1}(\\hat{y}^{n - k } \\in t_{[w_\\star]_\\mu } ) \\mathsmaller{\\cdot \\mathbb{p}(\\tilde{y}^k \\in t_{[w]_\\mu}(c^k(2))|m\\!=\\!1,\\ ! \\tilde{y}^k \\ !",
    "\\hat{y}^{n - k } \\ !",
    "\\in \\!t_{[w_\\star]_\\mu } ) } \\label{e : eq2 } \\\\ & \\le e^{o(k)}e^{-k d_{k_1 /k}(pw\\|w_\\star)}e^{-(n - k ) d_{k_1 /(n - k)}(w_\\star\\|pw ) }   e^{-k(\\mathbb{i}(p , w)-\\epsilon ) } , \\label{e : eq3}\\end{aligned}\\ ] ] where   follows from the independence of the events @xmath396_\\mu}\\}$ ] and @xmath397_\\mu}\\}$ ] conditioned on @xmath393 wrongly chosen output symbols , and   follows from using lemma  [ lemma1 ] for the first two terms in   with mismatch ratios @xmath398 and @xmath399 , respectively , and using lemma  [ fact1 ] for the last term in  , because conditioned on message @xmath152 being sent , @xmath400 and @xmath401 are independent regardless of the other conditions in the last term .",
    "substituting   into the summation in  , we have @xmath402 where   is obtained by substituting @xmath383 , and where   is obtained by finding the exponent of the sum in   as follows @xmath403 where   follows by using stirling s approximation for the binomial terms ; where   follows by noticing that the exponent of the summation is equal to the largest exponent of each term in the summation , since the number of terms is polynomial in @xmath0 ; where   is obtained by letting @xmath404 ( @xmath188 ) and substituting @xmath174 ; and where   follows from the definition  .",
    "now , combining  ,  , and  , we have @xmath405 as @xmath11 , which proves the theorem .",
    "a.   the term @xmath190 in   is maximized at @xmath406 , because it is concave in @xmath333 and its derivative with respect to @xmath333 is zero at @xmath407 .",
    "thus , this term is decreasing in @xmath333 in the interval @xmath408 $ ] . also , note that the partial divergence terms in   are increasing with respect to @xmath333 according to proposition  [ prop2 ]  ( [ d ] ) .",
    "therefore , the term in the max operator in   is decreasing in @xmath333 in the interval @xmath408 $ ] , and the maximum occurs in the interval @xmath193 $ ] .",
    "b.   the term in the max operator in   is concave in @xmath333 , because @xmath409 is concave in @xmath333 and @xmath410 is convex in @xmath333 according to proposition  [ prop2 ]  ( [ e ] ) .",
    "therefore , the term is maximized at point @xmath411 where the derivative with respect to @xmath333 is equal to zero .",
    "thus , we have @xmath412 where   is used to derive  , and where @xmath413 and @xmath414 are the corresponding @xmath129 s in   for the two partial divergence terms in  . taking derivative of   with respect to @xmath3 assuming that the maximum occurs at @xmath411",
    ", we obtain @xmath415 where @xmath416 in the first line is the left - side of  , which is equal to zero , and where   follows from  , and   follows from  .",
    "finally ,   follows from the fact that @xmath417 is always positive for @xmath418 and @xmath419 is also always positive , because the partial divergence @xmath420 is convex in @xmath411 according to proposition  [ prop2 ]  ( [ e ] ) . c.   substituting @xmath27 in",
    ", all the terms would be zero , because @xmath421 and @xmath132 according to proposition  [ prop2 ]  ( [ a ] ) .",
    "d.   consider that the maximum in   occurs at @xmath411 .",
    "according to part ( [ prop3-a - basic ] ) , @xmath422 , and therefore , @xmath423 as @xmath185 .",
    "using proposition  [ prop2 ]  ( [ c ] ) and  , we have @xmath424 as @xmath185 .",
    "substitiuting @xmath425 in  , we obtain @xmath426 where   follows from proposition  [ prop2 ]  ( [ b ] ) , and where   follows from proposition  [ prop2 ]  ( [ c ] ) , and where   follows from the definition of the binary entropy function and the assumption that @xmath196 is finite .",
    "e.   this part follows directly from the definition  .",
    "let @xmath427 denote the random vector describing the positions of the insertions in the output sequence @xmath428 , such that @xmath429 if and only if @xmath430 is one of the @xmath213 inserted @xmath242 s .",
    "we have @xmath431 where   follows by the general identity @xmath432 and noticing that for this choice of @xmath427 , we have @xmath433 . for the term @xmath434 in  , we have @xmath435 where @xmath268 is defined in  ; and where   is because instead of the summation over @xmath436 , we can sum over the possible @xmath242 insertions in between the runs of a fixed input sequence @xmath243 such that there are total of @xmath213 insertions .",
    "if we denote the number of insertions in between the runs of zeros by @xmath437 , and the number of insertions in between the runs of ones by @xmath438 , then we have @xmath439 . given these number of insertions , it is easy to see that @xmath440 in   is equal to @xmath441 in  . also , @xmath442 is equal to @xmath443 in  , because given the input and output sequences , the only uncertainty about the position sequence is where there is a run of zeros in the input sequence , i.e. , for a run of ones , we know that all the zeros in between them are insertions .",
    "also , the uncertainty is uniformly distributed over all the possible choices .",
    "note that from  , we have    @xmath444    where @xmath445 denotes the mutual information if the input sequence , and therefore , the output sequence have weight @xmath446 , and the maximization is over the distribution of all such input sequences . using the chain rule , we have @xmath447 where   is because the entropy of the output sequence given the insertion positions equals the entropy of the input sequence , and because the entropy of the position sequence equals @xmath448 due to the uniform insertions . combining  ,  , and",
    ", we have @xmath449 where @xmath450 denotes the entropy of the output sequence if it has weight @xmath446 ; and where   follows from the fact that the uniform distribution maximizes the entropy and by maximizing @xmath451 over all input sequences with weight @xmath446 . finally , by combining   and  , we get the upper bound  .",
    "we prove the properties for the capacity function @xmath207 .",
    "the corresponding properties for the function @xmath270 easily follows from  .",
    "a.   since the cardinality of the input alphabet of this channel is @xmath452 , the capacity of this channel is at most @xmath208 bits / s .",
    "b.   there are no insertions .",
    "therefore , it is a noiseless channel with input and output alphabets of sizes @xmath452 and capacity @xmath208 bits / s . c.   the input alphabet is @xmath453 , and the output consists of binary sequences with length @xmath209 and weight @xmath242 or @xmath97 , because only @xmath242 s can be inserted in the sequence .",
    "considering all the output sequences with weight @xmath97 as a super - symbol , the channel becomes binary noiseless with capacity @xmath97 bits / s .",
    "d.   the capacity @xmath454 can not decrease if , at each channel use , the decoder knows exactly one of the positions at which an insertion occurs , and the capacity of the channel with this genie - aided encoder and decoder becomes @xmath207 .",
    "therefore , @xmath277 .",
    "e.   the capacity @xmath455 can not decrease if , at each channel use , the encoder and decoder know exactly one of the positions at which an input bit remains unchanged , so that it can be transmitted uncoded and the capacity of the channel with this genie - aided encoder and decoder becomes @xmath456 .",
    "therefore , @xmath279 .",
    "the authors wish to thank dr .",
    "aslan tchamkerten and shyam kumar for the useful discussion , which leaded to a more compact expression and a shorter proof for the result in lemma  [ lemma1 ] .",
    "a. tchamkerten , v. chandar , and g. caire , `` energy and sampling constrained asynchronous communication , '' _ ieee trans . on inf .",
    "theory _ , submitted 2013 .",
    "[ online ] .",
    "available : http://arxiv.org/pdf/1302.6574v2.pdf      d. wang , v. chandar , s .- y .",
    "chung , and g. wornell , `` on reliability functions for single - message unequal error protection , '' in _ proc .",
    "information theory ( isit ) _ , cambridge , ma , usa , july 2012 .",
    "r. venkataramanan , s. tatikonda , and k. ramchandran , `` achievable rates for channels with deletions and insertions , '' in _ proc .",
    "information theory ( isit ) , _ st .",
    "petersburg , russia , aug .",
    "2011 .",
    "d. fertonani , t. m. duman , and m. f. erden , `` bounds on the capacity of channels with insertions , deletions and substitutions , '' _ ieee transactions on communications , _ vol .",
    "1 , pp . 2 - 6 , jan . 2011 .",
    "huang , u. niesen , and p. gupta , `` energy - efficient communication in the presence of synchronization errors , '' _ ieee trans . on inf .",
    "theory _ , submitted 2013 .",
    "[ online ] .",
    "available : http://arxiv.org/pdf/1301.6589v1.pdf"
  ],
  "abstract_text": [
    "<S> we formulate a model for intermittent communications that can capture bursty transmissions or a sporadically available channel , where in either case the receiver does not know a priori when the transmissions will occur . </S>",
    "<S> focusing on the point - to - point case , we develop two decoding structures and their achievable rates for such communication scenarios . </S>",
    "<S> one structure determines the transmitted codeword , and another scheme first detects the locations of codeword symbols and then uses them to decode . </S>",
    "<S> we introduce the concept of partial divergence and study some of its properties in order to obtain stronger achievability results . </S>",
    "<S> as the system becomes more intermittent , the achievable rates decrease due to the additional uncertainty about the positions of the codeword symbols at the decoder . </S>",
    "<S> additionally , we provide upper bounds on the capacity of binary noiseless intermittent communication with the help of a genie - aided encoder and decoder . </S>",
    "<S> the upper bounds imply a tradeoff between the capacity and the intermittency rate of the communication system . finally , we obtain lower and upper bounds on the capacity per unit cost of intermittent communication . </S>"
  ]
}