{
  "article_text": [
    "the general goal of statistical inference is to summarize a multivariate probability distribution by a set of parameters of interest such as the normalization constant ( if unknown ) , the mean , the variance matrix ... in many cases , such a summary takes the form of a vector - valued integral : @xmath0 where @xmath1 is a feature function , and @xmath2 is the target distribution representing , for instance , a bayesian posterior given some data ( omitted from the notation ) or a model evidence . in the frequent situation where ( [ eq : moment ] ) is analytically intractable , one has to resort to numerical methods , which may be classified as being either sampling - based or fitting - based .",
    "traditional fitting methods such as newton - cotes and gaussian quadrature rules are computationally prohibitive in any but the smallest dimensions .",
    "sampling , or monte carlo integration methods were developed from the 40 s to overcome this curse of dimensionality @xcite .",
    "the basic idea is to sample random points @xmath3 independently from an instrumental distribution @xmath4 , and estimate the desired integral by : @xmath5 where the weights @xmath6 guarantee that the resulting estimator is unbiased .",
    "this simple method , known as importance sampling ( is ) , may however be statistically inefficient if high density regions of @xmath7 are not sampled .",
    "this problem has motivated a number of refinements that may be understood as variance reduction devices , such as defensive and multiple is @xcite , adaptive is @xcite , path sampling @xcite , annealed is @xcite , and nested sampling @xcite .",
    "methods from a more radical trend are explicitly aimed to sample from the target distribution , and include both independence sampling methods such as rejection sampling @xcite or population monte carlo @xcite , and markov chain monte carlo ( mcmc ) methods @xcite .    in the meantime , fitting - based methods have been developed or revisited for computational statistics .",
    "for instance , the laplace method @xcite fits a gaussian using the second - order taylor expansion of the distribution logarithm at the mode .",
    "bayesian quadrature @xcite interpolates the target distribution ( or part of the integrand ) using gaussian processes .",
    "along with the development of graphical models , `` message - passing '' methods have emerged , such as the variational bayes algorithm @xcite , belief propagation and expectation propagation @xcite .",
    "the common thread of message - passing methods is to work on factor graph - structured distributions , by adjusting factors in a sequential fashion rather than optimizing a global ( and generally intractable ) fitting measure @xcite .    generally speaking ,",
    "fitting - based methods tend to be fast but rely on restrictive assumptions regarding the target distribution , while sampling - based methods are widely applicable but may require many function evaluations to reach acceptable accuracy in practice .",
    "combining both approaches gives rise to a class of hybrid methods that we call _ variational sampling _ in this paper .",
    "a widespread implementation of variational sampling involves fitting a parametric model by maximum weighted likelihood such as , e.g. , in the monte carlo em algorithm @xcite , the cross - entropy method @xcite , or stochastic approximation @xcite .",
    "this natural approach generalizes is , as shown in section  [ sec : weighted_likelihood ] .",
    "another previous variational sampling method is bayesian monte carlo ( bmc ) @xcite , a simplification of bayesian quadrature where control points are selected randomly .",
    "we here propose a new objective function for variational sampling , which adds an important consistency property to the weighted likelihood objective , namely that the integral estimator is exact whenever the target distribution is among the fitting distributions .",
    "it will also turn out numerically more stable than bmc .",
    "our starting point is a well - known variational argument : computing the integral ( [ eq : moment ] ) is equivalent , under some existence conditions , to substituting @xmath8 with the distribution @xmath9 that minimizes the inclusive kullback - leibler ( kl ) divergence @xcite : @xmath10 over the exponential family : @xmath11 where @xmath12 is the normalization constant of  @xmath8 , assumed known for now .",
    "the substitution may yield a closed - form expression for the integral if the exponential family is , e.g. , a subset of gaussian distributions , discrete distributions , or products of coordinate - wise univariate distributions . while the difficulty is then to minimize ( [ eq : kullback ] ) because the kl  divergence may be just as intractable as @xmath13 , it becomes natural to use a monte carlo approximation to @xmath14 as a surrogate cost function .",
    "an obvious such approximation is @xcite : @xmath15 where @xmath16 are drawn independently from an instrumental distribution @xmath4 with support included in the support of  @xmath7 , and @xmath17 are strictly positive importance weights that warrant that ( [ eq : naive_cost ] ) approaches @xmath14 in the limit of large samples . minimizing ( [ eq : naive_cost ] ) boils down to a familiar maximum likelihood estimation problem , yielding a similar moment matching condition as in the continuous kl  divergence case : @xmath18 which is nothing but the classical is estimate if @xmath19 is estimated by is , as is natural , i.e. @xmath20 .",
    "we have thus recast is as a variational problem , which brings little practical value but stresses a major flaw of is .",
    "arguably , ( [ eq : naive_cost ] ) is a poor fitting measure since it is generally possible to find a distribution @xmath9 , even in the exponential family , for which @xmath21 , meaning that there may exist distributions that fit @xmath8 better than @xmath8 itself ...      to work around this inconsistency ,",
    "we reformulate the fitting problem using the _ generalized _ kl  divergence @xcite : @xmath22 which is a positive - valued convex function of @xmath23 for any pair of _ unnormalized _ distributions , and vanishes if and only if @xmath24 .",
    "it is easy to show that minimizing @xmath25 subject to @xmath26 is equivalent to minimizing @xmath27 without constraint . in other words ,",
    "the constraint is automatically satisfied when minimizing the generalized kl  divergence .",
    "let us now remark that @xmath28 is an expected loss : @xmath29 where the `` pointwise '' loss @xmath30 is a function of the ratio @xmath31 .",
    "this strongly suggests the following monte carlo approximation as an alternative to  ( [ eq : naive_cost ] ) : @xmath32.\\ ] ]    as depicted in figure  [ fig : loss_comparison ] , the loss @xmath33 at any point @xmath34 is minimized and vanishes iff @xmath35 .",
    "it follows that @xmath36 for any unnormalized distribution @xmath9 with equality iff @xmath9 coincides with @xmath8 at the sampling points .",
    "hence , if @xmath8 lies in the approximation space , then it is a global minimizer of @xmath37 regardless of which points are sampled and how many ( we show below that the minimizer is unique provided that the sample is large enough ) . therefore , as illustrated in figure  [ fig : motivation ] , minimizing ( [ eq : cost ] ) is expected to produce more accurate integral estimates than is if the target distribution is reasonably `` close '' to the approximation space .         for details .",
    "note that the laplace approximation is undefined on the left , and flat on the right.,title=\"fig : \" ]   for details .",
    "note that the laplace approximation is undefined on the left , and flat on the right.,title=\"fig : \" ]    when choosing the approximation space as the exponential family ( [ eq : exp_family ] ) , each pointwise loss turns out to be a smoothly convex function of @xmath38 , therefore @xmath39 is a smoothly convex function of @xmath38 with gradient and hessian matrix : @xmath40 where @xmath41 is the @xmath42 matrix with general element @xmath43 , while @xmath44 and @xmath45 denote the vectors of importance weights and fitted importance weights , respectively , i.e. @xmath46 and @xmath47 .",
    "the hessian may be interpreted as the gram matrix of the rows of @xmath41 for the dot product : @xmath48 .",
    "while there is generally no closed - form solution to the minimization of  ( [ eq : cost ] ) , the following theorem provides a simple sufficient condition for the existence and uniqueness of a minimum .",
    "[ th : uniqueness ] if the matrix @xmath41 has rank @xmath49 , then @xmath50 admits a unique minimizer in @xmath51 .",
    "if @xmath41 has rank @xmath49 , then @xmath52 is fully ranked , hence positive definite for any  @xmath38 .",
    "this implies that @xmath50 is strictly convex and guarantees uniqueness .",
    "existence then follows from the fact that @xmath50 is coercive in the sense that @xmath53 . to prove that , we decompose @xmath38 via @xmath54 where @xmath55 is a scalar and @xmath56 a fixed unit - norm vector .",
    "@xmath41 being fully ranked means that @xmath57 , hence there exists at least one  @xmath58 for which @xmath59 , implying that @xmath60 therefore @xmath61 , which completes the proof .    for many choices of @xmath62 functions , such as a set of linearly independent polynomials , the condition @xmath63 is verified as soon as @xmath64 distinct points are sampled .",
    "we may then track the unique minimizer of @xmath50 using a numerical optimization scheme such as the newton method with adaptive step size @xcite .      the cost function ( [ eq : cost ] )",
    "is a special case of monte carlo approximations in decision - making previously studied by shao @xcite , which enjoy stochastic convergence properties when the loss is smoothly convex .",
    "in particular , we have a central limit theorem analogous to the case of importance sampling ( to avoid technicalities , we will assume here that the sample space is finite , which is only a conceptual restriction since @xmath34-values are computer - encoded in practice ) .",
    "[ th : central_limit ] under the conditions of theorem  [ th : uniqueness ] , let @xmath65 and let the corresponding integral estimator @xmath66 . if @xmath27 admits a unique minimizer @xmath67 , then @xmath68 converges in distribution to @xmath13 : @xmath69 \\stackrel{d}{\\to } { \\cal n}(0 , \\sigma ) ,",
    "\\qquad { \\rm with } \\quad \\sigma = \\int \\frac{[p(\\x)-q_{\\theta_\\star}(\\x)]^2}{\\pi(\\x)}\\phi(\\x)\\phi(\\x)^\\top d\\x .\\ ] ]    most of the proof rests upon shao @xcite , theorem  3 , implying that @xmath70 converges in distribution to the minimizer @xmath67 of @xmath27 .",
    "the result then follows from a taylor expansion of @xmath71 around @xmath67 .",
    "this asymptotic behavior is to be compared with the is estimator , which is known to be unbiased with variance @xmath72 , where : @xmath73    while both estimation variances decrease in @xmath74 , the most noticeable difference between ( [ eq : asymptotic_variance ] ) and ( [ eq : is_variance ] ) is that @xmath75 vanishes regardless of the sampling distribution @xmath4 whenever @xmath7 lies in the approximation space , so that @xmath76 .",
    "in contrast , at most one eigenvalue of @xmath77 can cancel out by an appropriate choice for @xmath4 , meaning that an exact estimation of @xmath13 is impossible using is from the same sample if @xmath78 . in this regard , the proposed method , simply referred to as variational sampling ( vs ) in the sequel , appears as yet another variance reduction device for  is .",
    "as shown by ( [ eq : asymptotic_variance ] ) , the accuracy of vs is dependent upon the sampling distribution @xmath4 if the target distribution is outside the exponential family .",
    "while optimizing @xmath4 is out of the scope of this paper , a strategy that proves useful in practice is to adapt neal s annealed is method @xcite .",
    "the basic idea is to obtain each point @xmath79 as the final state of a markov chain @xmath80 having some initial distribution @xmath4 and intermediate distributions of the form @xmath81 as successive transition kernels , for a decreasing `` inverse temperature '' parameter @xmath82 .",
    "neal realized that the importance weights corresponding to this sampling scheme are remarkably easy to compute in the extended state space @xcite .",
    "the vs method may be generalized by simply replacing @xmath6 in ( [ eq : cost ] ) with neal s extended - state importance weights @xmath83 .",
    "all the properties established in the case of simple sampling ( convexity , uniqueness , central limit theorem ) are maintained , and the asymptotic variance formula generalizes by substituting @xmath4 in ( [ eq : asymptotic_variance ] ) with the `` apparent '' sampling distribution : @xmath84 where @xmath85 is the distribution of extended states induced by the annealed sampling procedure .",
    "this method may overcome a poorly specified initial sampling distribution at the cost of multiplying the sampling time by the number of intermediate steps .",
    "the purpose of this section is to evaluate vs in the problem of estimating the moments of order 0 , 1 and 2 of a multivariate distribution , in which case the number of free parameters depends on the sample space dimension  @xmath86 through @xmath87 .",
    "the approximation space then contains the set of unnormalized gaussian distributions , but also improper distributions since there are no explicit constraints on the @xmath38-parameters associated with second - order polynomials .",
    "all the code used in these experiments was written in python language based on the scientific python package ( www.scipy.org ) , and is in open - source access at https://github.com/alexis-roche/infer .    in the simulations presented below ,",
    "several target distributions are picked among exponential power distributions : @xmath88^d   \\exp \\big ( - \\sum_{i=1}^d |x_i / \\alpha|^{\\beta } \\big ) , \\ ] ] where @xmath89 is a positive - valued shape parameter , and @xmath90 is a scale parameter conventionally tuned depending on @xmath89 so that @xmath91 has identity variance matrix  @xmath92 .",
    "such distributions conveniently model different types of unimodal distributions with either sharp ( @xmath93 ) or flat ( @xmath94 ) peaks .",
    "normalized gaussian distributions are obtained in the case @xmath95 .",
    "we compare vs with both is and bmc in different dimensions  @xmath86 , for different values of  @xmath89 and different sample sizes  @xmath96 .",
    "also , two distinct sampling strategies are considered to mimic situations where either an accurate or a crude initial approximation to the target is available . in one case ,",
    "the sampling distribution is the gaussian with same moments as the target distribution , @xmath97 . in another case",
    ", we use annealing sampling @xcite , as outlined in section  [ sec : annealed_vs ] , starting from   @xmath98 and increasing the @xmath99 parameter geometrically from @xmath100 to @xmath101 in @xmath102 steps , each step comprising one metropolis update using a gaussian proposal with variance @xmath103 .",
    "sampling time is thus about @xmath102 times larger in annealed sampling than in `` matched '' sampling .",
    "using both strategies , is was found to perform at least as well as all mcmc variants we tested , hence those are not reported for the sake of clarity .",
    "our implementation of bmc uses an isotropic gaussian correlation function , meaning that the target distribution is fitted with a mixture of gaussian distributions with scalar variances @xmath104 . to save computation time ,",
    "@xmath105 is estimated by  is ( assuming isotropic variance ) , rather than using the conventional marginal likelihood method @xcite , which seemed to have little impact on the results . a rather large damping value @xmath106 is used to regularize the linear system underlying bmc , meaning that the fit is not a strict interpolation .",
    "this amount of damping was tuned to roughly optimize performance in the presented examples .    for each combination of parameters @xmath107 and sampling strategy ( matched or annealed )",
    ", @xmath108 samples were simulated .",
    "all methods were run once on each sample , and the error was defined as the generalized kl  divergence ( [ eq : gen_kullback ] ) between the kl - optimal gaussian fit @xmath109 and the gaussian fit @xmath110 output by the method : @xmath111 , which also reads @xmath112 .",
    "hence , @xmath113 is the excess value of the ideal kl  divergence objective .",
    "the question is whether is or bmc can do a better job than vs in minimizing it .",
    "note that @xmath113 is infinite whenever @xmath110 is improper , which occasionally happens on small samples using vs , and more frequently using bmc .",
    "figure  [ fig : simu_dim1 ] and  [ fig : simu_dim10 ] display the simulated median errors as functions of the sample size for different values of the shape parameter  @xmath89 , respectively in dimensions @xmath114 and @xmath115 .",
    "each point on the curves corresponds to a series of  100 simulations where the sample size is a multiple @xmath116 of the number of free parameters , for @xmath117 ( @xmath118 if @xmath114 and @xmath119 if @xmath115 ) . recall that",
    ", according to theorem  [ th : uniqueness ] , the vs  estimator is well - defined as soon as @xmath64 .     from left to right .",
    "top row : matched sampling , bottom row : annealed sampling.,title=\"fig : \" ]   from left to right .",
    "top row : matched sampling , bottom row : annealed sampling.,title=\"fig : \" ]   from left to right .",
    "top row : matched sampling , bottom row : annealed sampling.,title=\"fig : \" ]   from left to right .",
    "top row : matched sampling , bottom row : annealed sampling.,title=\"fig : \" ]   from left to right .",
    "top row : matched sampling , bottom row : annealed sampling.,title=\"fig : \" ]   from left to right .",
    "top row : matched sampling , bottom row : annealed sampling.,title=\"fig : \" ]     in dimension  10 .",
    "errors larger than the @xmath120-axis range are not represented ( vs errors vanish for @xmath95).,title=\"fig : \" ]   in dimension  10 .",
    "errors larger than the @xmath120-axis range are not represented ( vs errors vanish for @xmath95).,title=\"fig : \" ]   in dimension  10 .",
    "errors larger than the @xmath120-axis range are not represented ( vs errors vanish for @xmath95).,title=\"fig : \" ]   in dimension  10 .",
    "errors larger than the @xmath120-axis range are not represented ( vs errors vanish for @xmath95).,title=\"fig : \" ]   in dimension  10 .",
    "errors larger than the @xmath120-axis range are not represented ( vs errors vanish for @xmath95).,title=\"fig : \" ]   in dimension  10 .",
    "errors larger than the @xmath120-axis range are not represented ( vs errors vanish for @xmath95).,title=\"fig : \" ]    the results confirm that vs is exact for normal distributions , but also show that it has the potential to outperform both is and bmc for distributions that depart significantly from normality ( @xmath121 and @xmath122 ) provided that sufficiently large samples are used . similarly",
    "to is , the vs error is seen to decrease as a function of the sample size . in contrast , bmc breaks down beyond a certain sample size depending on the dimension , and tends to perform poorly in multiple dimension , as already noted in @xcite .",
    "this is explained by the fact that the linear system underlying bmc gets bigger as samples increase in size , and may end up ill - conditioned for large samples .",
    "this issue is avoided in both vs and is ( if interpreted as a variational method , see section   [ sec : weighted_likelihood ] ) , since the number of free parameters is determined by the dimension as opposed to the sample size .    in the idealistic case of matched sampling ,",
    "vs is significantly slower than is since evaluating the target distribution is relatively cheap in our examples , so that sampling time is small compared to fitting time . a key observation , however , is that the computational overhead of vs decreases with sample size , as shown in tables  [ tab : time_dim1 ] and  [ tab : time_dim10 ] , while bmc shows the opposite trend . in annealed sampling ,",
    "sampling time becomes predominant and differences in timing between methods are dramatically reduced .",
    ".computation times of vs and bmc relative to is in dimension  1 . on a single processor intel",
    "core i7 720qm running at 1.60ghz , is took about 1ms per sample with matched sampling ( sampling time was then negligible ) , and between 100ms and 1s with annealed sampling depending on sample size . [ cols=\"^,^,^,^,^,^,^,^\",options=\"header \" , ]",
    "in summary , the proposed vs method is an approximate moment matching technique that relies on few assumptions regarding the target distribution and can serve as an efficient alternative to conventional monte carlo methods . at the theoretical level",
    ", we established that the vs estimator is both well - defined under mild conditions ( theorem  [ th : uniqueness ] ) , and converges in distribution towards the actual moments in the limit of large samples ( theorem  [ th : central_limit ] ) . at the practical level",
    ", our experiments showed that vs can significantly outperform existing methods in the problem of estimating moments of order up to  2 of unimodal distributions in moderate dimension .",
    "in realistic scenarios where no `` good '' initial sampling scheme is available , vs can be fruitfully combined with neal s annealed importance sampling idea @xcite at low computational cost .",
    "an important remaining challenge for machine learning applications is to extend vs in high dimension .",
    "one easy way to go , in problems where the target distribution factorizes as a product of low dimensional factors , is to use vs in conjunction with message - passing algorithms @xcite to repeatedly perform factor - wise approximations .",
    "local _ applications of vs may be useful whenever messages lack closed - form expressions , hence vs has the potential to widen the application field of message - passing . in this approach , however , message - passing is used as a dimension reduction device that converts the globally convex minimization problem of vs into a series of simpler convex minimizations . while the computational advantage is clear",
    ", this might have a negative impact on estimation quality .",
    "implementing the direct , globally convex , vs approach in high dimension first raises engineering issues as the number of free parameters is bound to increase with dimension  for instance , it increases in @xmath123 in the gaussian fitting problem .",
    "these can be tackled by software design solutions such as block - alternating optimization or related techniques that can reduce memory load without distorting the global minimum ( as long as the problem is convex ) . on the other hand ,",
    "there is the more fundamental `` curse of dimensionality '' issue of how large samples need to be for vs to reach acceptable estimation errors .",
    "theorem  [ th : uniqueness ] gives some insight into this question as it states that the vs estimator is unique provided that the sample is larger than the parameter dimension , itself dependent on the sample space dimension .",
    "however , it is yet unclear whether the uniqueness condition implies that the minimization is sensible in that the solution is reasonably close to the ideal kl  divergence minimization ( [ eq : gen_kullback ] ) .",
    "we see this as an open research issue related with the problem of diagnostics in mcmc methods @xcite ."
  ],
  "abstract_text": [
    "<S> we propose a new method to approximately integrate a function with respect to a given probability distribution when an exact computation is intractable . </S>",
    "<S> the method is called `` variational sampling '' as it involves fitting a simplified distribution for which the integral has a closed - form expression , and using a set of randomly sampled control points to optimize the fit . </S>",
    "<S> the novelty lies in the chosen objective function , namely a monte carlo approximation to the generalized kullback - leibler divergence , which differs from classical methods that implement a similar idea , such as bayesian monte carlo and importance sampling . </S>",
    "<S> we review several attractive mathematical properties of variational sampling , including well - posedness under a simple condition on the sample size , and a central limit theorem analogous to the case of importance sampling . </S>",
    "<S> we then report various simulations that essentially show that variational sampling has the potential to outperform existing methods within comparable computation time in estimating moments of order up to  2 . </S>",
    "<S> we conclude with a brief discussion of desirable enhancements . </S>"
  ]
}