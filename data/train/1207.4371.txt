{
  "article_text": [
    "applications in various fields including information retrieval  @xcite and natural language processing  @xcite rely on statistics about @xmath0-grams ( i.e. , sequences of contiguous words in text documents or other string data ) as an important building block .",
    "google and microsoft have made available @xmath0-gram statistics computed on parts of the web . while certainly a valuable resource",
    ", one limitation of these datasets is that they only consider @xmath0-grams consisting of up to five words . with this limitation",
    ", there is no way to capture idioms , quotations , poetry , lyrics , and other types of named entities ( e.g. , products , books , songs , or movies ) that typically consist of more than five words and are crucial to applications including plagiarism detection , opinion mining , and social media analytics .",
    "mapreduce has gained popularity in recent years both as a programming model and in its open - source implementation hadoop .",
    "it provides a platform for distributed data processing , for instance , on web - scale document collections .",
    "mapreduce imposes a rigid programming model , but treats its users with features such as handling of node failures and an automatic distribution of the computation . to make most effective use of it",
    ", problems need to be cast into its programming model , taking into account its particularities .    in this work",
    ", we address the problem of efficiently computing @xmath0-gram statistics on mapreduce platforms .",
    "we allow for a restriction of the @xmath0-gram statistics to be computed by a maximum length @xmath1 and a minimum collection frequency @xmath2 .",
    "only @xmath0-grams consisting of up to @xmath1 words and occurring at least @xmath2 times in the document collection are thus considered .    while this can be seen as a special case of frequent sequence mining , our experiments on two real - world datasets",
    "show that mapreduce adaptations of apriori - based methods  @xcite do not perform well  in particular when long and/or less frequent @xmath0-grams are of interest . in this light",
    ", we develop our novel method suffix-@xmath1 that is based on ideas from string processing .",
    "our method makes thoughtful use of mapreduce s grouping and sorting functionality .",
    "it keeps the number of records that have to be sorted by mapreduce low and exploits their order to achieve a compact main - memory footprint , when determining collection frequencies of all @xmath0-grams considered .",
    "we also describe possible extensions of our method .",
    "this includes the notions of maximality / closedness , known from frequent sequence mining , that can drastically reduce the amount of @xmath0-gram statistics computed . in addition , we investigate to what extent our method can support aggregations beyond occurrence counting , using @xmath0-gram time series , recently made popular by michel et al .",
    "@xcite , as an example .    *",
    "contributions * made in this work include :    * a _ novel method _ suffix-@xmath1 to compute @xmath0-gram statistics that has been specifically designed for mapreduce ; * a detailed account on _ efficient implementation _ and _ possible extensions _ of suffix-@xmath1 ( e.g. , to consider maximal / closed @xmath0-grams or support other aggregations ) ; * a _ comprehensive experimental evaluation _ on the new york times annotated corpus ( 1.8 million news articles from 19872007 ) and clueweb09-b ( 50 million web pages crawled in 2009 ) , as two large - scale real - world datasets , comparing our method against state - of - the - art competitors and investigating their trade - offs .",
    "suffix-@xmath1 outperforms its best competitor in our experiments by up to a _",
    "factor 12x _ when long and/or less frequent @xmath0-grams are of interest . otherwise , it performs at least on par with the best competitor .    * organization . *",
    "section  [ sec : model  background ] introduces our model .",
    "section  [ sec : methods - based - prior ] details on methods to compute @xmath0-gram statistics based on prior ideas .",
    "section  [ sec : suffix - sorting ] introduces our method suffix-@xmath1 .",
    "aspects of efficient implementation are addressed in section  [ sec : impl  extens ] .",
    "possible extensions of suffix-@xmath1 are sketched in section  [ sec : extensions ] .",
    "our experiments are the subject of section  [ sec : exper - eval ] . in section  [ sec : related - work ] , we put our work into context , before concluding in section  [ sec : conclusions ] .",
    "we now introduce our model , establish our notation , and provide some technical background on mapreduce .",
    "our methods operate on sequences of terms ( i.e. , words or other textual tokens ) drawn from a vocabulary @xmath3 .",
    "we let @xmath4 denote the universe of all sequences over @xmath3 . given a sequence @xmath5 with @xmath6",
    ", we refer to its length as @xmath7 , write @xmath8 $ ] for the subsequence @xmath9 , and let @xmath10 $ ] refer to the element @xmath11 . for two sequences @xmath12 and @xmath13 , we let @xmath14 denote their concatenation .",
    "we say that    * @xmath12 is a _ prefix _ of @xmath13 ( @xmath15 ) iff @xmath16 =    \\mathbf{s}[i]\\ ] ] * @xmath12 is a _ suffix _ of @xmath13 ( @xmath17 )",
    "iff @xmath18 = \\mathbf{s}[|\\mathbf{s}| - |\\mathbf{r}| + i]\\ ] ] * @xmath12 is a _ subsequence _ of @xmath13 ( @xmath19 ) iff @xmath20 = \\mathbf{s}[i + j]\\ ] ]    and capture how often @xmath12 occurs in @xmath13 as @xmath21 = \\mathbf{s}[i + j ]      \\,\\right\\}\\right|\\;.\\ ] ]    to avoid confusion , we use the following convention : when referring to sequences of terms having a specific length @xmath22 , we will use the notion @xmath22-gram or indicate the considered length by alluding to , for instance , @xmath23-grams .",
    "the notion @xmath0-gram , as found in the title , will be used when referring to variable - length sequences of terms .    as an input , all methods considered in this work",
    "receive a document collection @xmath24 consisting of sequences of terms as documents .",
    "our focus is on determining how often @xmath0-grams occur in the document collection .",
    "formally , the _ collection frequency _ of an @xmath0-gram @xmath13 is defined as as @xmath25 alternatively , one could consider the document frequency of @xmath0-grams as the total number of documents that contain a specific @xmath0-gram . while this corresponds to the notion of _ support _ typically used in frequent sequence mining , it is less common for natural language applications .",
    "however , all methods presented below can easily be modified to produce document frequencies instead .",
    "mapreduce , as described by dean and ghemawat  @xcite , is a programming model and an associated runtime system at google . while originally proprietary , the mapreduce programming model has been widely adopted in practice and several implementations exist . in this work ,",
    "we rely on hadoop  @xcite as a popular open - source mapreduce platform .",
    "the objective of mapreduce is to facilitate distributed data processing on large - scale clusters of commodity computers .",
    "mapreduce enforces a functional style of programming and lets users express their tasks as two functions    [ cols=\"^,^,^ \" , ]      we use two publicly - available real - world datasets for our experiments , namely :    * * the new york times annotated corpus *  @xcite consisting of more than 1.8 million newspaper articles from the period 19872007 ( nyt ) ; * * clueweb09-b *  @xcite , as a well - defined subset of the clueweb09 corpus of web documents , consisting of more than 50 million web documents in english language that were crawled in 2009 ( cw ) .",
    "these two are extremes : nyt is a well - curated , relatively clean , longitudinal corpus , i.e. , documents therein have a clear structure , use proper language with few typos , and cover a long time period .",
    "cw is a `` world wild web '' corpus , i.e. , documents therein are highly heterogeneous in structure , content , and language .    for nyt a document",
    "consists of the newspaper article s title and body . to make cw more handleable",
    ", we use boilerplate detection as described by kohlschtter et al .",
    "@xcite and implemented in boilerpipe s  @xcite default extractor , to identify the core content of documents . on both datasets",
    ", we use opennlp  @xcite to detect sentence boundaries in documents .",
    "sentence boundaries act as barriers , i.e. , we do not consider @xmath0-grams that span across sentences in our experiments . as described in section  [ sec : impl  extens ] , in a pre - processing step , we convert both datasets into sequences of integer term - identifiers .",
    "the term dictionary is kept as a single text file ; documents are spread as key - value pairs of 64-bit document identifier and content integer array over a total of 256 binary files .",
    "table  [ tab : datasets ] summarizes characteristics of the two datasets .",
    "let us first look at the @xmath0-gram statistics that ( or , parts of which ) we expect as output from all methods . to this end , for both document collections , we determine all @xmath0-grams that occur at least five times ( i.e. , @xmath26 and @xmath27 )",
    ". we bin @xmath0-grams into 2-dimensional buckets of exponential width , i.e. , the @xmath0-gram @xmath13 with collection frequency @xmath28 goes into bucket @xmath29 where @xmath30 and @xmath31 .",
    "figure  [ fig : output ] reports the number of @xmath0-grams per bucket .",
    "* nytcw * +        the figure reveals that the distribution is biased toward short and less frequent @xmath0-grams .",
    "consequently , as we lower the value of @xmath2 , all methods have to deal with a drastically increasing number of @xmath0-grams .",
    "what can also be seen from figure  [ fig : output ] is that , in both datasets , @xmath0-grams exist that are very long , containing hundred or more terms , and occur more than ten times in the document collection .",
    "examples of long @xmath0-grams that we see in the output include ingredient lists of recipes ( e.g. , `` ) and chess openings ( e.g. , `` ) in nyt ; in cw they include web spam ( e.g. , `` ) as well as error messages and stack traces from web servers and other software ( e.g. , `` ) that also occur within user discussions in forums .",
    "for the apriori - based methods , such long @xmath0-grams are unfavorable , since they require many iterations to identify them .      as a first experiment",
    ", we investigate how the methods perform for parameter settings chosen to reflect two typical use cases , namely , _",
    "training a language model _ and _ text analytics_. for the first use case , we set @xmath32 on nyt and @xmath33 on cw , as relatively low minimum collection frequencies , in combination with @xmath34 .",
    "the @xmath0-gram statistics made public by google  @xcite , as a comparison , were computed with parameter settings @xmath35 and @xmath34 on parts of the web . for the second use case ,",
    "we choose @xmath36 , as a relatively high maximum sequence length , combined with @xmath33 on nyt and @xmath37 on cw .",
    "the idea in the analytics use case is to identify recurring fragments of text ( e.g. , quotations or idioms ) to be analyzed further ( e.g. , their spread over time ) .",
    "[ fig : usecases ]    figure  [ fig : usecases ] reports wallclock - time measurements obtained for these two use cases with 64  map / reduce slots . for our language - model use case ,",
    "suffix-@xmath1 outperforms apriori - scan as the best competitor by a _ factor 3x _ on both datasets . for our analytics",
    "use case , we see a _",
    "factor 12x _ improvement over apriori - index as the best competitor on nyt ; on cw suffix-@xmath1 still outperforms the next best apriori - scan by a _ factor  1.5x_. measurements for nave on cw in are missing , since the method did not complete in reasonable time .      our second experiment studies how the methods behave as we vary the minimum collection frequency @xmath2 .",
    "we use a maximum length @xmath34 and apply all methods to the entire datasets .",
    "measurements are performed using 64  map / reduce slots and reported in figure  [ fig : varyingminimumsupport ] .    * nyt * +    * cw * +    we observe that for high minimum collection frequencies , suffix-@xmath1 performs as well as the best competitor apriori - scan . for low minimum collection frequencies",
    ", it significantly outperforms the other methods .",
    "both apriori - based method show steep increases in wallclock time as we lower the minimum collection frequency  especially when we reach the lowest value of @xmath2 on each document collection .",
    "this is natural , because for both methods the work that has to be done in the @xmath22-th iteration depends on the number of @xmath38-grams output in the previous iteration , which have to be joined or kept in a dictionary , as described in section  [ sec : methods - based - prior ] .",
    "as observed in figure  [ fig : output ] above , the number of @xmath22-grams grows drastically as we decrease the value of @xmath2 .",
    "when looking at the number of bytes and the number of records transferred , we see analogous behavior . for low values of @xmath2 , suffix-@xmath1 transfers significantly less data than its competitors .      in this third experiment , we study the methods behavior as we vary the maximum length @xmath1 . the minimum collection frequency is set as @xmath33 for nyt and @xmath37 for cw to reflect their different scale .",
    "measurements are performed on the entire datasets with 64  map / reduce slots and reported in figure  [ fig : varyingmaximumlength ] .",
    "measurements for @xmath39 are missing for nave on cw , since the method did not finish within reasonable time for those parameter settings .    *",
    "nyt * +    * cw * +    suffix-@xmath1 is on par with the best - performing competitor on cw , when considering @xmath0-grams of length up to @xmath40 . for @xmath41",
    ", it outperforms the next best apriori - scan by a _",
    "factor 1.5x_. on nyt , suffix-@xmath1 consistently outperforms all competitors by a wide margin .",
    "when we increase the value of @xmath1 , the apriori - based methods need to run more hadoop jobs , so that their wallclock times keep increasing . for nave and suffix-@xmath1 ,",
    "on the other hand , we observe a saturation of wallclock times .",
    "this is expected , since these methods have to do additional work only for input sequences longer than @xmath1 consisting of terms that occur at least @xmath2 times in the document collection .",
    "when looking at the number of bytes and the number of records transferred , we observe a saturation for nave for the reason mentioned above . for suffix-@xmath1 only the number of bytes saturates , the number of records transferred is constant , since it depends only on the minimum collection frequency @xmath2 .",
    "further , we see that suffix-@xmath1 consistently transfers fewest records .      next , we investigate how the methods react to changes in the scale of the datasets . to this end , both from nyt and cw , we extract smaller datasets that contain a random @xmath42 , @xmath43 , or @xmath44 subset of the documents .",
    "again , the minimum collection frequency is set as @xmath33 for nyt and @xmath37 for cw . the maximum length is set as @xmath34 .",
    "wallclock times are measured using 64  map / reduce slots .",
    "* nytcw * +    from figure  [ fig : scalingdataset ] , we observe that nave handles additional data equally well on both datasets .",
    "the other methods scalability is comparable to that of nave on cw , as can be seen from their almost - identical slopes .",
    "on nyt , in contrast , apriori - scan , apriori - index , and suffix-@xmath1 cope slightly better with additional data than nave .",
    "this is due to the different characteristics of the two datasets .",
    "our final experiment explores how the methods behave as we scale computational resources .",
    "again , we set @xmath33 for nyt and @xmath37 for cw .",
    "all methods are applied to the @xmath43 samples of documents from the collections .",
    "we vary the number of map / reduce slots as 16 , 32 , 48 , and 64 .",
    "the number of cluster nodes remains constant in this experiment , since we can not add / remove machines to / from the cluster due to organizational restrictions .",
    "we thus only vary the amount of parallel work every machine can do ; their total number remains constant throughout this experiment .",
    "* nytcw * +    we observe from figure  [ fig : scalingcluster ] that all methods show comparable behavior as we make additional computational resources available .",
    "or , put differently , all methods make equally effective use of them . what can also be observed across all methods",
    "is that the gains of adding more computational resources are diminishing  because of mappers and reducers competing for shared devices such as hard disks and network interfaces .",
    "this phenomenon is more pronounced on nyt than cw , since methods take generally less time on the smaller dataset , so that competition for shared devices is fiercer and has no chance to level out over time .",
    "what we see in our experiments is that suffix-@xmath1 outperforms its competitors when long and/or less frequent @xmath0-grams are considered . even otherwise , when the focus is on short and/or very frequent @xmath0-grams , suffix-@xmath1 performs never significantly worse than the other methods .",
    "it is hence robust and can handle a wide variety of parameter choices . to substantiate this ,",
    "consider that suffix-@xmath1 could compute statistics about arbitrary - length @xmath0-grams that occur at least five times ( i.e. , @xmath26 and @xmath27 ) , as reported in figure  [ fig : output ] , in less than six minutes on nyt and six hours on cw .",
    "we now discuss the connection between this work and existing literature , which can broadly be categorized into :    * frequent pattern mining * goes back to the seminal work by agrawal et al .",
    "@xcite on identifying frequent itemsets in customer transactions . while the apriori algorithm described therein follows a candidate generation & pruning approach , han et al .",
    "@xcite have advocated pattern growth as an alternative approach . to identify frequent sequences , which is a problem closer to our work , the same kinds of approaches can be used .",
    "agrawal and srikant  @xcite describe candidate generation & pruning approaches ; pei et al .",
    "@xcite propose a pattern - growth approach .",
    "spade by zaki  @xcite also generates and prunes candidates but operates on an index structure as opposed to the original data .",
    "parallel methods for frequent pattern mining have been devised both for distributed - memory  @xcite and shared - memory machines  @xcite .",
    "little work exists that assumes mapreduce as a model of computation .",
    "li et al .",
    "@xcite describe a pattern - growth approach to mine frequent itemsets in mapreduce .",
    "huang et al .",
    "@xcite sketch an approach to maintain frequent sequences while sequences in the database evolve .",
    "their approach is not applicable in our setting , since it expects input sequences to be aligned ( e.g , based on time ) and only supports document frequency . for more detailed discussions , we refer to ceglar and roddick  @xcite for frequent itemset mining , mabroukeh and ezeife  @xcite for frequent sequence mining , and han et al .",
    "@xcite for frequent pattern mining in general .",
    "* natural language processing & information retrieval . * given their role in nlp , multiple efforts  @xcite have looked into @xmath0-gram statistics computation . while these approaches typically consider document collections of modest size , recently lin et al .",
    "@xcite and nguyen et al .",
    "@xcite targeted web - scale data . among the aforementioned work , huston et al .",
    "@xcite is closest to ours , also focusing on less frequent @xmath0-grams and using a cluster of machines .",
    "however , they only consider @xmath0-grams consisting of up to eleven words and do not provide details on how their methods can be adapted to mapreduce . yamamoto and church  @xcite augment suffix arrays , so that the collection frequency of substrings in a document collection can be determined efficiently .",
    "bernstein and zobel  @xcite identify long @xmath0-grams as a means to spot co - derivative documents",
    ". brants et al .",
    "@xcite and wang et al .",
    "@xcite describe the @xmath0-gram statistics made available by google and microsoft , respectively .",
    "zhai  @xcite gives details on the use of @xmath0-gram statistics in language models .",
    "michel et al .",
    "@xcite demonstrated recently that @xmath0-gram time series are powerful tools to understand the evolution of culture and language .",
    "* mapreduce algorithms . *",
    "several efforts have looked into how specific problems can be solved using mapreduce , including all - pairs document similarity  @xcite , processing relational joins  @xcite , coverage problems  @xcite , content matching  @xcite .",
    "however , no existing work has specifically addressed computing @xmath0-gram statistics in mapreduce .",
    "in this work , we have presented suffix-@xmath1 , a novel method to compute @xmath0-gram statistics using mapreduce as a platform for distributed data processing .",
    "our evaluation on two real - world datasets demonstrated that suffix-@xmath1 outperforms mapreduce adaptations of apriori - based methods significantly , in particular when long and/or less frequent @xmath0-grams are considered . otherwise , suffix-@xmath1 is robust , performing at least on par with the best competitor .",
    "we also argued that our method is easier to implement than its competitors , having been designed with mapreduce in mind .",
    "finally , we established our method s versatility by showing that it can be extended to produce maximal / closed @xmath0-grams and perform aggregations beyond occurrence counting ."
  ],
  "abstract_text": [
    "<S> statistics about @xmath0-grams ( i.e. , sequences of contiguous words or other tokens in text documents or other string data ) are an important building block in information retrieval and natural language processing . in this work , </S>",
    "<S> we study how @xmath0-gram statistics , optionally restricted by a maximum @xmath0-gram length and minimum collection frequency , can be computed efficiently harnessing mapreduce for distributed data processing . </S>",
    "<S> we describe different algorithms , ranging from an extension of word counting , via methods based on the apriori principle , to a novel method suffix-@xmath1 that relies on sorting and aggregating suffixes . we examine possible extensions of our method to support the notions of maximality / closedness and to perform aggregations beyond occurrence counting . assuming hadoop as a concrete mapreduce implementation </S>",
    "<S> , we provide insights on an efficient implementation of the methods . </S>",
    "<S> extensive experiments on the new york times annotated corpus and clueweb09 expose the relative benefits and trade - offs of the methods . </S>"
  ]
}