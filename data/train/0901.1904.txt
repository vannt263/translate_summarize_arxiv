{
  "article_text": [
    "it is well known that lossless source coding and statistical modeling are complementary objectives .",
    "this fact is captured by the kraft inequality ( see section  5.2 in cover and thomas @xcite ) , which provides a correspondence between uniquely decodable codes and probability distributions on a discrete alphabet . if one has full knowledge of the source statistics , then one can design an optimal lossless code for the source , and _",
    "vice versa_. however , in practice it is unreasonable to expect that the source statistics are known precisely , so one has to design _ universal _ schemes that perform asymptotically optimally within a given class of sources . in universal coding , too , as rissanen has shown in @xcite , the coding and modeling objectives can be accomplished jointly : given a sufficiently regular parametric family of discrete - alphabet sources , the encoder can acquire the source statistics via maximum - likelihood estimation on a sufficiently long data sequence and use this knowledge to select an appropriate coding scheme . even in nonparametric settings ( e.g. , the class of all stationary ergodic discrete - alphabet sources ) , universal schemes such as ziv  lempel @xcite amount to constructing a probabilistic model for the source . in the reverse direction ,",
    "kieffer @xcite and merhav @xcite , among others , have addressed the problem of statistical modeling ( parameter estimation or model identification ) via universal lossless coding .",
    "once we consider _ lossy _ coding , though , the relationship between coding and modeling is no longer so simple . on the one hand , having full knowledge of the source statistics",
    "is certainly helpful for designing optimal rate - distortion codebooks . on the other hand , apart from some special cases ( e.g. , for i.i.d .",
    "bernoulli sources and the hamming distortion measure or for i.i.d .",
    "gaussian sources and the squared - error distortion measure ) , it is not at all clear how to extract a reliable statistical model of the source from its reproduction via a rate - distortion code ( although , as shown recently by weissman and ordentlich @xcite , the joint empirical distribution of the source realization and the corresponding codeword of a  good \" rate - distortion code converges to the distribution solving the rate - distortion problem for the source ) .",
    "this is not a problem when the emphasis is on compression , but there are situations in which one would like to compress the source and identify its statistics at the same time .",
    "for instance , in _ indirect adaptive control _",
    "( see , e.g. , chapter  7 of tao @xcite ) the parameters of the plant ( the controlled system ) are estimated on the basis of observation , and the controller is modified accordingly .",
    "consider the discrete - time stochastic setting , in which the plant state sequence is a random process whose statistics are governed by a finite set of parameters .",
    "suppose that the controller is geographically separated from the plant and connected to it via a noiseless digital channel whose capacity is @xmath5 bits per use .",
    "then , given the time horizon @xmath6 , the objective is to design an encoder and a decoder for the controller to obtain reliable estimates of both the plant parameters and the plant state sequence from the @xmath7 possible outputs of the decoder .",
    "to state the problem in general terms , consider an information source emitting a sequence @xmath8 of random variables taking values in an alphabet @xmath9 .",
    "suppose that the process distribution of @xmath10 is not specified completely , but it is known to be a member of some parametric class @xmath11 .",
    "we wish to answer the following two questions :    1 .",
    "is the class @xmath12 universally encodable with respect to a given single - letter distortion measure @xmath13 , by codes with a given structure ( e.g. , all fixed - rate block codes with a given per - letter rate , all variable - rate block codes , etc . ) ?",
    "in other words , does there exist a scheme that is asymptotically optimal for each @xmath14 , @xmath15 ?",
    "if the answer to question 1 ) is positive , can the codes be constructed in such a way that the decoder can not only reconstruct the source , but also identify its process distribution @xmath14 , in an asymptotically optimal fashion ?    in previous work @xcite , we have addressed these two questions in the context of fixed - rate lossy block coding of stationary memoryless ( i.i.d . )",
    "continuous - alphabet sources with parameter space @xmath16 a bounded subset of @xmath17 for some finite @xmath18 .",
    "we have shown that , under appropriate regularity conditions on the distortion measure and on the source models , there exist joint universal schemes for lossy coding and source identification whose redundancies ( that is , the gap between the actual performance and the theoretical optimum given by the shannon distortion - rate function ) and source estimation fidelity both converge to zero as @xmath19 , as the block length @xmath2 tends to infinity .",
    "the code operates by coding each block with the code matched to the source with the parameters estimated from the preceding block . comparing this convergence rate to the @xmath20 convergence rate , which is optimal for redundancies of fixed - rate lossy block codes @xcite",
    ", we see that there is , in general , a price to be paid for doing compression and identification simultaneously .",
    "furthermore , the constant hidden in the @xmath21 notation increases with the  richness \" of the model class @xmath12 , as measured by the vapnik ",
    "chervonenkis ( vc ) dimension @xcite of a certain class of measurable subsets of the source alphabet associated with the sources .",
    "the main limitation of the results of @xcite is the i.i.d .",
    "assumption , which is rather restrictive as it excludes many practically relevant model classes ( e.g. , autoregressive sources , or markov and hidden markov processes ) .",
    "furthermore , the assumption that the parameter space @xmath16 is bounded may not always hold , at least in the sense that we may not know the diameter of @xmath16 _ a priori_. in this paper we relax both of these assumptions and study the existence and the performance of universal schemes for joint lossy coding and identification of stationary sources satisfying a mixing condition , when the sources are assumed to belong to a parametric model class @xmath12 , @xmath16 being an open subset of @xmath17 for some finite @xmath18 .",
    "because the parameter space is not bounded , we have to use variable - rate codes with countably infinite codebooks , and the performance of the code is assessed by a composite lagrangian functional @xcite which captures the trade - off between the expected distortion and the expected rate of the code .",
    "our result is that , under certain regularity conditions on the distortion measure and on the model class , there exist universal schemes for joint lossy source coding and identification such that , as the block length @xmath2 tends to infinity , the gap between the actual lagrangian performance and the optimal lagrangian performance achievable by variable - rate codes at that block length , as well as the source estimation fidelity at the decoder , converge to zero as @xmath22 , where @xmath3 is the vc dimension of a certain class of decision regions induced by the collection @xmath23 of the @xmath2-dimensional marginals of the source process distributions .",
    "this result shows very clearly that the price to be paid for universality , in terms of both compression and identification , grows with the richness of the underlying model class , as captured by the vc dimension sequence @xmath24 .",
    "the richer the model class , the harder it is to learn , which affects the compression performance of our scheme because we use the source parameters learned from past data to decide how to encode the current block .",
    "furthermore , comparing the rate at which the lagrangian redundancy decays to zero under our scheme with the @xmath25 result of chou , effros and gray @xcite , whose universal scheme is not aimed at identification , we immediately see that , in ensuring to satisfy the twin objectives of compression and modeling , we inevitably sacrifice some compression performance .    the paper is organized as follows .",
    "section  [ sec : prelims ] introduces notation and basic concepts related to sources , codes and vapnik ",
    "chervonenkis classes .",
    "section  [ sec : results ] lists and discusses the regularity conditions that have to be satisfied by the source model class , and contains the statement of our result .",
    "the result is proved in section  [ sec : proof ] .",
    "next , in section  [ sec : examples ] we give three examples of parametric source families ( namely , i.i.d .",
    "gaussian sources , gaussian autoregressive sources and hidden markov processes ) which fit the framework of this paper under suitable regularity conditions .",
    "we conclude in section  [ sec : conclusion ] and outline directions for future research .",
    "finally , the appendix contains some technical results on lagrange - optimal variable - rate quantizers .",
    "in this paper , a _ source _ is a discrete - time stationary ergodic random process @xmath8 with alphabet @xmath9 .",
    "we assume that @xmath9 is a polish space ( i.e. , a complete separable metric space for some @xmath26 . ] ) and equip @xmath9 with its borel @xmath27-field . for",
    "any pair of indices @xmath28 with @xmath29 , let @xmath30 denote the segment @xmath31 of @xmath10 .",
    "if @xmath32 is the process distribution of @xmath10 , then we let @xmath33 denote expectation with respect to @xmath32 , and let @xmath34 denote the marginal distribution of @xmath35 . whenever @xmath32 carries a subscript , e.g. , @xmath36",
    ", we write @xmath37 instead .",
    "we assume that there exists a fixed @xmath27-finite measure @xmath38 on @xmath9 , such that the @xmath2-dimensional marginal of any process distribution of interest is absolutely continuous with respect to the product measure @xmath39 , for all @xmath40 .",
    "we denote the corresponding densities @xmath41 by @xmath42 . to avoid notational clutter",
    ", we omit the superscript @xmath2 from @xmath39 , @xmath34 and @xmath42 whenever it is clear from the argument , as in @xmath43 , @xmath44 or @xmath45 .    given two probability measures @xmath46 on a measurable space @xmath47 , the _",
    "variational distance _ between them is defined by @xmath48 where the supremum is over all finite @xmath49-measurable partitions of @xmath50 ( see , e.g. , section  5.2 of gray @xcite ) .",
    "if @xmath51 and @xmath52 are the densities of @xmath32 and @xmath53 , respectively , with respect to a dominating measure @xmath54 , then we can write @xmath55 a useful property of the variational distance is that , for any measurable function @xmath56}$ ] , @xmath57 .",
    "when @xmath32 and @xmath53 are @xmath2-dimensional marginals of @xmath14 and @xmath58 , respectively , i.e. , @xmath59 and @xmath60 , we write @xmath61 for @xmath62 . if @xmath63 is a @xmath27-subfield of @xmath49 , we define the variational distance @xmath64 between @xmath32 and @xmath53 with respect to @xmath63 by @xmath65 where the supermum is over all finite @xmath63-measurable partitions of @xmath50 .",
    "given a @xmath66 and a probability measure @xmath32 , the _ variational ball _ of radius @xmath67 around @xmath32 is the set of all probability measures @xmath53 with @xmath68 .",
    "given a source @xmath10 with process distribution @xmath32 , let @xmath69 and @xmath70 denote the marginal distributions of @xmath32 on @xmath71 and @xmath72 , respectively .",
    "for each @xmath73 , the _ @xmath18th - order absolute regularity coefficient _ ( or _ @xmath0-mixing coefficient _ ) of @xmath32 is defined as @xcite : @xmath74 where the supremum is over all finite @xmath75-measurable partitions @xmath76 and all finite @xmath77-measurable partitions @xmath78 .",
    "observe that @xmath79 the variational distance between @xmath32 and the product distribution @xmath80 with respect to the @xmath27-algebra @xmath81 .",
    "since @xmath10 is stationary , we can  split \" its process distribution at any point @xmath82 and define @xmath83 equivalently by @xmath84 again , if @xmath32 is subscripted by some @xmath85 , @xmath36 , then we write @xmath86 .",
    "the class of codes we consider here is the collection of all finite - memory variable - rate vector quantizers .",
    "let @xmath87 be a _ reproduction alphabet _ , also assumed to be polish .",
    "we assume that @xmath88 is a subset of a polish metric space @xmath89 with a bounded metric @xmath90 : there exists some @xmath91 , such that @xmath92 for all @xmath93 .",
    "we take @xmath94}$ ] , @xmath95 , as our ( single - letter ) _",
    "distortion function_. a variable - rate vector quantizer with block length @xmath2 and memory length @xmath96 is a pair @xmath97 , where @xmath98 is the _ encoder _",
    ", @xmath99 is the _ decoder _ , and @xmath100 is a countable collection of binary strings satisfying the prefix condition or , equivalently , the kraft inequality @xmath101 where @xmath102 denotes the length of @xmath103 in bits .",
    "the mapping of the source @xmath10 into the reproduction process @xmath104 is defined by @xmath105 that is , the encoding is done in blocks of length @xmath2 , but the encoder is also allowed to observe the @xmath96 symbols immediately preceding each block .",
    "the _ effective memory _ of @xmath106 is defined as the set @xmath107 , such that @xmath108 the size @xmath109 of @xmath110 is called the _ effective memory length _ of @xmath106 .",
    "we shall often use @xmath106 to also denote the composite mapping @xmath111 : @xmath112 .",
    "when the code has zero memory ( @xmath113 ) , we shall denote it more compactly by @xmath114 .",
    "the performance of the code on the source with process distribution @xmath32 is measured by its expected distortion @xmath115 where for @xmath116 and @xmath117 , @xmath118 is the per - letter distortion incurred in reproducing @xmath119 by @xmath120 , and by its expected rate @xmath121 where @xmath122 denotes the length of a binary string @xmath103 in bits , normalized by @xmath2 .",
    "( we follow neuhoff and gilbert @xcite and normalize the distortion and the rate by the length @xmath2 of the _ reproduction _ block , not by the combined length @xmath123 of the source block plus the memory input . ) when working with variable - rate quantizers , it is convenient @xcite to absorb the distortion and the rate into a single performance measure , the _ lagrangian distortion _",
    "@xmath124 where @xmath125 is the _ lagrange multiplier _ which controls the distortion - rate trade - off .",
    "geometrically , @xmath126 is the @xmath127-intercept of the line with slope @xmath128 , passing through the point @xmath129 in the rate - distortion plane @xcite .",
    "if @xmath32 carries a subscript , @xmath36 , then we write @xmath130 , @xmath131 and @xmath132 .      in this paper ,",
    "we make heavy use of vapnik  chervonenkis theory ( see devroye , gyrfi and lugosi @xcite , vapnik @xcite , devroye and lugosi @xcite or vidyasagar @xcite for detailed treatments ) .",
    "this section contains a brief summary of the needed concepts and results .",
    "let @xmath47 be a measurable space .",
    "for any collection @xmath133 of measurable subsets of @xmath50 and any @xmath2-tuple @xmath134 , define the set @xmath135 consisting of all distinct binary strings of the form @xmath136 , @xmath137 . then @xmath138 is called the _",
    "@xmath2th shatter coefficient _ of @xmath139 .",
    "the _ vapnik  chervonenkis dimension _",
    "( or vc - dimension ) of @xmath139 , denoted by @xmath140 , is defined as the largest @xmath2 for which @xmath141 ( if @xmath142 for all @xmath143 , then we set @xmath144 ) . if @xmath145 , then @xmath139 is called a _ vapnik ",
    "chervonenkis class _ ( or vc class ) .",
    "if @xmath139 is a vc class with @xmath146 , then it follows from the results of vapnik and chervonenkis @xcite and sauer @xcite that @xmath147 .    for a vc class @xmath139 , the so - called _ vapnik  chervonenkis inequalities _",
    "( see lemma  [ lm : vc ] below ) relate its vc dimension @xmath140 to maximal deviations of the probabilities of the events in @xmath139 from their relative frequencies with respect to an i.i.d .",
    "sample of size @xmath2 . for any @xmath134 ,",
    "let @xmath148 denote the induced empirical distribution , where @xmath149 is the dirac measure ( point mass ) concentrated at @xmath150 .",
    "we then have the following :    [ lm : vc ]    let @xmath32 be a probability measure on @xmath47 , and @xmath151 an @xmath2-tuple of independent random variables with @xmath152 , @xmath153 .",
    "let @xmath139 be a vapnik ",
    "chervonenkis class with @xmath146 .",
    "then for every @xmath66 , @xmath154 and @xmath155 where @xmath156 is a universal constant .",
    "the probabilities and expectations are with respect to the product measure @xmath34 on @xmath157 .",
    "a more refined technique involving metric entropies and empirical covering numbers , due to dudley @xcite , can yield a much better @xmath158 bound on the expected maximal deviation between the true and the empirical probabilities .",
    "this improvement , however , comes at the expense of a much larger constant hidden in the @xmath21 notation .",
    "finally , we shall need the following lemma , which is a simple corollary of the results of karpinski and macintyre @xcite ( see also section  10.3.5 of vidyasagar @xcite ) :    [ lm : karpinski_macintyre ] let @xmath159 be a collection of measurable subsets of @xmath160 , such that @xmath161 where for each @xmath162 , @xmath163 is a polynomial of degree @xmath103 in the components of @xmath164 .",
    "then @xmath139 is a vc class with @xmath165 .",
    "in this section we state our result concerning universal schemes for joint lossy compression and identification of stationary sources under certain regularity conditions .",
    "we work in the usual setting of universal source coding : we are given a source @xmath8 whose process distribution is known to be a member of some parametric class @xmath12 .",
    "the parameter space @xmath16 is an open subset of the euclidean space @xmath17 for some finite @xmath18 , and we assume that @xmath16 has nonempty interior .",
    "we wish to design a sequence of variable - rate vector quantizers , such that the decoder can reliably reconstruct the original source sequence @xmath10 and reliably identify the active source in an asymptotically optimal manner for all @xmath15 .",
    "we begin by listing the regularity conditions .",
    "_ condition 1 .",
    "_ the sources in @xmath166 are _ algebraically @xmath0-mixing _ : there exists a constant @xmath167 , such that @xmath168 where the constant implicit in the @xmath21 notation may depend on @xmath85 .",
    "this condition ensures that certain finite - block functions of the source @xmath10 can be approximated in distribution by i.i.d .",
    "processes , so that we can invoke the vapnik  chervonenkis machinery of section  [ ssec : vc ] .",
    "this  blocking \" technique , which we exploit in the proof of our theorem  [ thm : main ] , dates back to bernstein @xcite , and was used by yu @xcite to derive rates of convergence in the uniform laws of large numbers for stationary mixing processes , and by meir @xcite in the context of nonparametric adaptive prediction of stationary time series . as an example of when an even stronger decay condition holds , let @xmath8 be a finite - order autoregressive moving - average ( arma ) process driven by a zero - mean i.i.d .",
    "process @xmath169 , i.e. , there exist poisitive integers @xmath170 and @xmath171 real constants @xmath172 such that @xmath173 mokkadem @xcite has shown that , provided the common distribution of the @xmath174 is absolutely continuous and the roots of the polynomial @xmath175 lie outside the unit circle in the complex plane , the @xmath0-mixing coefficients of @xmath10 decay to zero exponentially .",
    "_ condition 2 .",
    "_ for each @xmath15 , there exist constants @xmath176 and @xmath177 , such that @xmath178 for all @xmath179 in the open ball of radius @xmath180 centered at @xmath85 , where @xmath181 is the euclidean norm on @xmath16 .",
    "this condition guarantees that , for any sequence @xmath182 of positive reals such that @xmath183 and any sequence @xmath184 in @xmath16 satisfying @xmath185 for a given @xmath15 , we have @xmath186 it is weaker ( i.e. , more general ) than the conditions of rissanen @xcite which control the behavior of the relative entropy ( information divergence ) as a function of the source parameters in terms of the fisher information and related quantities . indeed , for each @xmath2 let @xmath187 be the normalized @xmath2th - order relative entropy ( information divergence ) between @xmath14 and @xmath58 .",
    "suppose that , for each @xmath85 , @xmath188 is twice continuously differentiable as a function of @xmath179",
    ". let @xmath179 lie in an open ball of radius @xmath67 around @xmath85 . since @xmath189 attains its minimum at @xmath190 , the gradient @xmath191 evaluated at @xmath190 is zero , and we can write the second - order taylor expansion of @xmath192 about @xmath85 as @xmath193 where the hessian matrix @xmath194_{i , j=1,\\ldots , k},\\ ] ] under additional regularity conditions , is equal to the fisher information matrix @xmath195_{i , j = 1,\\ldots , k}\\ ] ] ( see clarke and barron @xcite ) .",
    "assume now , following rissanen @xcite , that the sequence of matrix norms @xmath196 is bounded ( by a constant depending on @xmath85 ) .",
    "then we can write @xmath197 i.e. , the normalized relative entropies @xmath188 are locally quadratic in @xmath179",
    ". then pinsker s inequality ( see , e.g. , lemma  5.2.8 of gray @xcite ) implies that @xmath198 , and we recover our condition 2 .",
    "rissanen s condition , while stronger than our condition 2 , is easier to check , the fact which we exploit in our discussion of examples of section  [ sec : examples ] .",
    "_ condition 3 .",
    "_ for each @xmath2 , let @xmath199 be the collection of all sets of the form @xmath200 then we require that , for each @xmath2 , @xmath199 is a vc class , and @xmath201 .    this condition is satisfied , for example , when @xmath202 independently of @xmath2 , or when @xmath203 .",
    "the use of the class @xmath199 dates back to the work of yatracos @xcite on minimum - distance density estimation .",
    "the ideas of yatracos were further developed by devroye and lugosi @xcite , who dubbed @xmath199 the _ yatracos class _ ( associated with the densities @xmath204 ) .",
    "we shall adhere to this terminology . to give an intuitive interpretation to @xmath199 ,",
    "let us consider a pair @xmath205 of distinct parameter vectors and note that the set @xmath206 consists of all @xmath119 for which the simple hypothesis test @xmath207 is passed by the null hypothesis @xmath208 under the likelihood - ratio decision rule . now",
    ", suppose that @xmath209 are drawn _ independently _ from @xmath210 .",
    "to each @xmath211 we can associate a _ classifier _",
    "@xmath212 defined by @xmath213 .",
    "call two sets @xmath214 _ equivalent _ with respect to the sample @xmath215 , and write @xmath216 , if their associated classifiers yield identical classification patterns : @xmath217 it is easy to see that @xmath218 is an equivalence relation . from the definitions of the shatter coefficients @xmath219 and the vc dimension @xmath220 ( cf .  section  [ ssec : vc ] )",
    ", we see that the cardinality of the quotient set @xmath221 is equal to @xmath222 for all sample sizes @xmath223 , whereas for @xmath224 , it is bounded from above by @xmath225 , which is strictly less than @xmath222 .",
    "thus , the fact that the yatracos class @xmath199 has finite vc dimension implies that the problem of estimating the density @xmath204 from a large i.i.d .",
    "sample reduces , in a sense , to a finite number ( in fact , polynomial in the sample size @xmath96 , for @xmath224 ) of simple hypothesis tests of the type ( [ eq : hyptest ] ) . our condition 1",
    "will then allow us to transfer this intuition to ( weakly ) dependent samples .",
    "now that we have listed the regularity conditions that must hold for the sources in @xmath12 , we can state our main result .",
    "[ thm : main ] let @xmath11 be a parametric class of sources satisfying conditions 13 .",
    "then for every @xmath226 and every @xmath227 , there exists a sequence @xmath228 of variable - rate vector quantizers with memory length @xmath229 and effective memory length @xmath230 , such that , for all @xmath15 , @xmath231 where the constants implicit in the @xmath21 notation depend on @xmath85 . furthermore , for each @xmath2 , the binary description produced by the encoder is such that the decoder can identify the @xmath2-dimensional marginal of the active source up to a variational ball of radius @xmath232 with probability one .",
    "+    what ( [ eq : universality ] ) says is that , for each block length @xmath2 and each @xmath15 , the code @xmath233 , which is _ independent of @xmath85 _ , performs almost as well as the best finite - memory quantizer with block length @xmath2 that can be designed with full _ a priori _ knowledge of the @xmath2-dimensional marginal @xmath210 .",
    "thus , as far as compression goes , our scheme can compete with all finite - memory variable - rate lossy block codes ( vector quantizers ) , with the additional bonus of allowing the decoder to identify the active source in an asymptotically optimal manner .",
    "it is not hard to see that the double infimum in ( [ eq : universality ] ) is achieved already in the zero - memory case , @xmath234 .",
    "indeed , it is immediate that having nonzero memory can only improve the lagrangian performance , i.e. , @xmath235 on the other hand , given any code @xmath97 , we can construct a zero - memory code @xmath236 , such that @xmath237 for all @xmath15 . to see this ,",
    "define for each @xmath116 the set @xmath238 and let @xmath239 and @xmath240 .",
    "then , given any @xmath241 , let @xmath242 .",
    "we then have @xmath243 taking expectations , we see that @xmath237 for all @xmath15 , which proves that @xmath244 the infimum of @xmath245 over all zero - memory variable - rate quantizers @xmath114 with block length @xmath2 is the _",
    "operational @xmath2th - order distortion - rate lagrangian _",
    "@xmath246 @xcite .",
    "because each @xmath14 is ergodic , @xmath246 converges to the _ distortion - rate lagrangian _",
    "@xmath247 where @xmath248 is the shannon distortion - rate function of @xmath14 ( see lemma  2 in the appendix to chou , effros and gray @xcite ) . thus , our scheme is universal not only in the @xmath2th - order sense of ( [ eq : universality ] ) , but also in the distortion - rate sense , i.e. , @xmath249 for every @xmath15 .",
    "thus , in the terminology of @xcite , our scheme is _ weakly minimax universal _ for @xmath12 .",
    "in this section , we describe the main idea behind the proof and fix some notation .",
    "we have already seen that it suffices to construct a universal scheme that can compete with all _ zero - memory _ variable - rate quantizers .",
    "that is , it suffices to show that there exists a sequence @xmath250 of codes , such that @xmath251 this is what we shall prove .",
    "we assume throughout that the  true \" source is @xmath252 for some @xmath253 .",
    "our code operates as follows .",
    "suppose that :    * both the encoder and the decoder have access to a countably infinite  database \" @xmath254 , where each @xmath255 .",
    "using elias universal representation of the integers @xcite , we can associate to each @xmath256 a unique binary string @xmath257 with @xmath258 bits . * a sequence @xmath259 of positive reals is given , such that @xmath183 ( we shall specify the sequence @xmath260 later in the proof ) .",
    "* for each @xmath261 and each @xmath15 , there exists a zero - memory @xmath2-block code @xmath262 that achieves ( or comes arbitrarily close to ) the @xmath2th - order lagrangian optimum for @xmath14 : @xmath263 .",
    "fix the block length @xmath2 . because the source is stationary",
    ", it suffices to describe the mapping of @xmath35 into @xmath264 .",
    "the encoding is done as follows :    1 .",
    "the encoder estimates @xmath265 from the @xmath266-block @xmath267 as @xmath268 , where @xmath269 .",
    "the encoder then computes the _ waiting time _",
    "@xmath270 with the standard convention that the infimum of the empty set is equal to @xmath271 .",
    "that is , the encoder looks through the database @xmath272 and finds the first @xmath256 , such that the @xmath2-dimensional distribution @xmath273 is in the variational ball of radius @xmath274 around @xmath268 .",
    "if @xmath275 , the encoder sets @xmath276 ; otherwise , the encoder sets @xmath277 , where @xmath278 is some default parameter vector , say , @xmath279 .",
    "the binary description of @xmath35 is a concatenation of the following three binary strings : ( i ) a 1-bit flag @xmath280 to tell whether @xmath281 is finite @xmath282 or infinite @xmath283 ; ( ii ) a binary string @xmath284 which is equal to @xmath285 if @xmath286 or to an empty string if @xmath287 ; ( iii ) @xmath288 .",
    "the string @xmath289 is called the _ first - stage description _ , while @xmath290 is called the _ second - stage description_.    the decoder receives @xmath291 , determines @xmath292 from @xmath293 , and produces the reproduction @xmath294 .",
    "note that when @xmath295 ( which , as we shall show , will happen eventually almost surely ) , @xmath296 lies in the variational ball of radius @xmath274 around the estimated source @xmath268 .",
    "if the latter is a good estimate of @xmath265 , i.e. , @xmath297 as @xmath298 a.s .",
    ", then the estimate of the true source computed by the decoder is only slightly worse . furthermore , as we shall show , the almost - sure convergence of @xmath299 to zero as @xmath298 implies that the lagrangian performance of @xmath300 on @xmath252 is close to the optimum @xmath301 .    formally , the code @xmath233 is comprised by the following maps :    * the _ parameter estimator _ @xmath302 ; * the _ parameter encoder _",
    "@xmath303 , where @xmath304 ; * the _ parameter decoder _ @xmath305 .",
    "let @xmath306 denote the composition @xmath307 of the parameter estimator and the parameter encoder , which we refer to as the _ first - stage encoder _ , and let @xmath292 denote the composition @xmath308 of the parameter decoder and the first - stage encoder .",
    "the decoder @xmath309 is the _ first - stage decoder_. the collection @xmath310 defines the _ second - stage codes_. the encoder @xmath311 and the decoder @xmath312 of @xmath233 are defined as @xmath313 and @xmath314 respectively . to assess the performance of @xmath233 ,",
    "consider the function @xmath315.\\end{aligned}\\ ] ] the expectation @xmath316 of @xmath317 with respect to @xmath252 is precisely the lagrangian performance of @xmath233 , at lagrange multiplier @xmath318 , on the source @xmath252 .",
    "we consider separately the contributions of the first - stage and the second - stage codes .",
    "define another function @xmath319 by @xmath320 so that @xmath321 is the ( random ) lagrangian performance of the code @xmath322 on @xmath252 .",
    "hence , @xmath323 so , taking expectations , we get @xmath324 our goal is to show that the first term in eq .",
    "( [ eq : overall_lagrange ] ) converges to the @xmath2th - order optimum @xmath325 , and that the second term is @xmath326 .",
    "the proof itself is organized as follows .",
    "first we motivate the choice of the memory lengths @xmath266 in section  [ ssec : memory ] .",
    "then we indicate how to select the database @xmath327 ( section  [ ssec : database ] ) and how to implement the parameter estimator @xmath328 ( section  [ ssec : param_estimation ] ) and the parameter encoder / decoder pair @xmath329 ( section  [ ssec:1st_stage ] ) . the proof is concluded by estimating the lagrangian performance of the resulting code ( section  [ ssec : performance ] ) and the fidelity of the source identification at the decoder ( section  [ ssec : src_ident ] ) . in the following , ( in)equalities involving the relevant random variables are assumed to hold for all realizations and not just a.s . , unless specified otherwise .",
    "let @xmath330 , where @xmath331 is the common decay exponent of the @xmath0-mixing coefficients @xmath86 in condition  1 , and let @xmath332 .",
    "we divide the @xmath266-block @xmath267 into @xmath2 blocks @xmath333 of length @xmath2 interleaved by @xmath2 blocks @xmath334 of length @xmath335 ( see figure  [ fig : code_structure ] ) .",
    "the parameter estimator @xmath328 , although defined as acting on the entire @xmath267 , effectively will make use only of @xmath336 .",
    "the @xmath337 s are each distributed according to @xmath265 , but they are not independent . thus , the set @xmath338 is the effective memory of @xmath233 , and the effective memory length is @xmath230 .",
    "let @xmath339 denote the marginal distribution of @xmath340 , and let @xmath341 denote the product of @xmath2 copies of @xmath265 .",
    "we now show that we can approximate @xmath339 by @xmath341 in variational distance , increasingly finely with @xmath2 .",
    "note that both @xmath339 and @xmath341 are defined on the @xmath27-algebra @xmath342 , generated by all @xmath343 except those in @xmath334 , so that @xmath344 .",
    "therefore , using induction and the definition of the @xmath0-mixing coefficient ( cf .",
    "section  [ ssec : sources ] ) , we have @xmath345 where the last equality follows from condition 1 and from our choice of @xmath335 .",
    "this in turn implies the following useful fact ( see also lemma  4.1 of yu @xcite ) , which we shall heavily use in the proof : for any measurable function @xmath346}$ ] with @xmath347 , @xmath348 where the constant hidden in the @xmath21 notation depends on @xmath349 and on @xmath350 .",
    "the database , or the first - stage codebook , @xmath272 is constructed by random selection .",
    "let @xmath351 be a probability distribution on @xmath16 which is absolutely continuous with respect to the lebesgue measure and has an everywhere positive and continuous density @xmath352 .",
    "let @xmath353 be a collection of independent random vectors taking values in @xmath16 , each generated according to @xmath351 independently of @xmath10 .",
    "we use @xmath354 to denote the process distribution of @xmath327 .",
    "note that the first - stage codebook is countably infinite , which means that , in principle , both the encoder and the decoder must have unbounded memory in order to store it .",
    "this difficulty can be circumvented by using synchronized random number generators at the encoder and at the decoder , so that the entries of @xmath327 can be generated as needed .",
    "thus , by construction , the encoder will generate @xmath281 samples ( where @xmath281 is the waiting time ) and then communicate ( a binary encoding of ) @xmath281 to the decoder .",
    "since the decoder s random number generator is synchronized with that of the encoder s , the decoder will be able to recover the required entry of @xmath327 .",
    "the parameter estimator @xmath302 is constructed as follows .",
    "because the source @xmath10 is stationary , it suffices to describe the action of @xmath328 on @xmath267 . in the notation of section  [ ssec : main ] ,",
    "let @xmath355 be the empirical distribution of @xmath336 .",
    "for every @xmath15 , define @xmath356 where @xmath199 is the yatracos class defined by the @xmath2th - order densities @xmath357 ( see section  [ sec : results ] ) .",
    "finally , define @xmath358 as any @xmath359 satisfying @xmath360 where the extra @xmath361 term is there to ensure that at least one such @xmath362 exists .",
    "this is the so - called _ minimum - distance ( md ) density estimator _ of devroye and lugosi @xcite ( see also devroye and gyrfi @xcite ) , adapted to the dependent - process setting of the present paper .",
    "the key property of the md estimator is that @xmath363 ( see , e.g. , theorem  5.1 of devroye and gyrfi @xcite ) .",
    "this holds regardless of whether the samples @xmath333 are independent or not .",
    "next we construct the parameter encoder - decoder pair @xmath329 .",
    "given a @xmath15 , define the _ waiting time _",
    "@xmath364 with the standard convention that the infimum of the empty set is equal to @xmath271 .",
    "that is , given a @xmath15 , the parameter encoder looks through the codebook @xmath327 and finds the position of the first @xmath365 such that the variational distance between the @xmath2th - order distributions @xmath366 and @xmath367 is at most @xmath368 .",
    "if no such @xmath365 is found , the encoder sets @xmath369 .",
    "we then define the maps @xmath370 and @xmath309 by @xmath371 and @xmath372 respectively .",
    "thus , @xmath373 , and the bound @xmath374 holds for every @xmath15 , regardless of whether @xmath281 is finite or infinite .      given the random codebook @xmath327 , the expected lagrangian performance of our code on the source @xmath252 , is @xmath375 we now upper - bound the two terms in ( [ eq : overall_lagrange_random ] ) .",
    "we start with the second term .",
    "we need to bound the expectation of the waiting time @xmath376 .",
    "our strategy borrows some elements from the paper of kontoyiannis and zhang @xcite .",
    "consider the probability @xmath377 which is a random function of @xmath267 .",
    "from condition 2 , it follows for @xmath2 sufficiently large that @xmath378 where @xmath379 .",
    "because the density @xmath352 is everywhere positive , the latter probability is strictly positive for almost all @xmath267 , and so @xmath380 eventually almost surely .",
    "thus , the waiting times @xmath281 will be finite eventually almost surely ( with respect to both the source @xmath10 and the first - stage codebook @xmath327 ) . now , if @xmath380 , then , conditioned on @xmath381 , the waiting time @xmath281 is a geometric random variable with parameter @xmath382 , and it is not hard to show ( see , e.g. , lemma  3 of kontoyiannis and zhang @xcite ) that for any @xmath383 @xmath384 \\ge \\epsilon \\big| x^0_{-m_n+1 } = x^0_{-m_n + 1 } \\big ) \\le e^{-2^\\epsilon}.\\ ] ] setting @xmath385 , we have , for almost all @xmath386 , that @xmath387 \\ge \\log(2 \\log n ) \\big| x^0_{-m_n+1 } = x^0_{-m_n+1 } \\big ) \\\\ & & \\qquad \\qquad \\le e^{-2\\log n } \\le n^{-2}.\\end{aligned}\\ ] ] then , by the borel  cantelli lemma , @xmath388 eventually almost surely , so that @xmath389 for almost every realization of the random codebook @xmath327 and for sufficiently large @xmath2 .",
    "we now obtain an asymptotic lower bound on @xmath390 .",
    "define the events @xmath391 then by the triangle inequality we have @xmath392 and , for @xmath2 sufficiently large , we can write @xmath393 where ( a ) follows from the independence of @xmath10 and @xmath327 , ( b ) follows from the fact that the parameter estimator @xmath394 depends only on @xmath340 , and ( c ) follows from condition 2 and the fact that @xmath395 . since the density @xmath396 is everywhere positive and continuous at @xmath350 , @xmath397 for all @xmath398 for @xmath2 sufficiently large , so @xmath399 where @xmath400 is the volume of the unit sphere in @xmath17 . next , the fact that the minimum - distance estimate @xmath394 depends only on @xmath340 implies that the event @xmath401 belongs to the @xmath27-algebra @xmath342 , and from ( [ eq : blocking_2 ] ) we get @xmath402 under @xmath341 , the @xmath2-blocks @xmath333 are i.i.d . according to @xmath265 , and we can invoke the vapnik ",
    "chervonenkis machinery to lower - bound @xmath403 . in the notation of sec .",
    "[ ssec : param_estimation ] , define the event @xmath404 then @xmath405 implies @xmath401 by ( [ eq : mde_property ] ) , and @xmath406 where the second bound is by the vapnik  chervonenkis inequality ( [ eq : vcbound_1 ] ) of lemma  [ lm : vc ] . combining the bounds ( [ eq : mde_bound_1 ] ) and ( [ eq : mde_bound_2 ] ) and using condition 1 , we obtain @xmath407 now , if we choose @xmath408 then the right - hand side of ( [ eq : prob_bound_2 ] ) can be further lower - bounded by @xmath409 . combining this with ( [ eq : prob_bound_1 ] ) , taking logarithms , and then taking expectations ,",
    "we obtain @xmath410 \\\\   & & \\quad   + \\frac{3k}{2}\\log \\frac{1}{n } + c(k,\\theta_0 ) \\\\ & \\ge & \\log(1- o(1/n ) ) + \\frac{3k}{2}\\log \\frac{1}{n } + c(k,\\theta_0),\\end{aligned}\\ ] ] where @xmath411 is a constant that depends only on @xmath18 and @xmath350 . using this and ( [ eq : expected_waittime ] ) , we get that @xmath412 for @xmath354-almost every realization of the random codebook @xmath327 , for @xmath2 sufficiently large . together with ( [ eq:1st_stage_length ] )",
    ", this implies that @xmath413 for @xmath354-almost all realizations of the first - stage codebook .",
    "we now turn to the first term in ( [ eq : overall_lagrange_random ] ) .",
    "recall that , for each @xmath15 , the code @xmath414 is @xmath2th - order optimal for @xmath14 . using this fact together with the boundedness of the distortion measure @xmath13 , we can invoke lemma  [ lm : finite_codebook ] in the appendix and assume without loss of generality that each @xmath414 has a finite codebook ( of size not exceeding @xmath415 ) , and each codevector can be described by a binary string of no more than @xmath416 bits .",
    "hence , @xmath417 . let @xmath418 and @xmath419 be the marginal distributions of @xmath252 on @xmath75 and @xmath420 , respectively . note that @xmath421 does not depend on @xmath422 .",
    "this , together with condition 1 and the choice of @xmath335 , implies that @xmath423 furthermore , @xmath424 where ( a ) follows by fubini s theorem and the boundedness of @xmath425 , while ( b ) follows from the definition of @xmath425 .",
    "the lagrangian performance of the code @xmath300 , where @xmath426 , can be further bounded as @xmath427,\\end{aligned}\\ ] ] where ( a ) follows from lemma  [ lm : finite_codebook ] in the appendix , ( b ) follows from the @xmath2th - order optimality of @xmath300 for @xmath296 , ( c ) follows , overbounding slightly , from the lagrangian mismatch bound of lemma  [ lm : mismatch ] in the appendix , and ( d ) follows from the triangle inequality . taking expectations ,",
    "we obtain @xmath428 the second @xmath429 term in ( [ eq : lagrange_bound ] ) can be interpreted as the estimation error due to estimating @xmath265 by @xmath268 , while the first @xmath429 is the approximation error due to quantization of the parameter estimate @xmath328 .",
    "we examine the estimation error first . using ( [ eq : mde_property ] ) , we can write @xmath430 now , each @xmath337 is distributed according to @xmath265 , and we can approximate the expectation of @xmath431 with respect to @xmath339 by the expectation of @xmath431 with respect to the product measure @xmath341 : @xmath432 where the second estimate follows from the vapnik  chervonenkis inequality ( [ eq : vcbound_2 ] ) and from the choice of @xmath335 .",
    "this , together with ( [ eq : mde_error ] ) , yields @xmath433 as for the first @xmath429 term in ( [ eq : lagrange_bound ] ) , we have , by construction of the first - stage encoder , that @xmath434 eventually almost surely , so the corresponding expectation is @xmath435 as well .",
    "summing the estimates ( [ eq : estimation_error ] ) and ( [ eq : approximation_error ] ) , we obtain @xmath436 finally , putting everything together , we see that , eventually , @xmath437\\end{aligned}\\ ] ] for @xmath354-almost every realization of the first - stage codebook @xmath327 .",
    "this proves ( [ eq : zeromem_universality ] ) , and hence ( [ eq : universality ] ) .",
    "we have seen that the expected variational distance @xmath438 between the @xmath2-dimensional marginals of the true source @xmath252 and the estimated source @xmath439 converges to zero as @xmath440 .",
    "we wish to show that this convergence also holds eventually with probability one , i.e. , @xmath441 @xmath252-almost surely .    given an @xmath383",
    ", we have by the triangle inequality that @xmath442 implies @xmath443 where @xmath394 is the minimum - distance estimate of @xmath265 from @xmath267 ( cf .  section  [ ssec:1st_stage ] ) . recalling our construction of the first - stage encoder",
    ", we see that this further implies @xmath444 finally , using the property ( [ eq : mde_property ] ) of the minimum - distance estimator , we obtain that @xmath445 implies @xmath446 therefore , @xmath447 where ( a ) follows , as before , from the definition of the @xmath0-mixing coefficient and ( b ) follows by the vapnik  chervonenkis inequality .",
    "now , if we choose @xmath448 for an arbitrary small @xmath66 , then ( [ eq : prob_bound ] ) can be further upper - bounded by @xmath449 , which , owing to condition 1 and the choice @xmath330 , is summable in @xmath2 .",
    "thus , @xmath450 and we obtain ( [ eq : as_bound ] ) by the borel  cantelli lemma .",
    "as a basic check , let us see how theorem  [ thm : main ] applies to stationary memoryless ( i.i.d . ) sources .",
    "let @xmath451 , and let @xmath12 be the collection of all gaussian i.i.d .",
    "processes , where @xmath452 then the @xmath2-dimensional marginal for a given @xmath453 has the gaussian density @xmath454 with respect to the lebesgue measure .",
    "this class of sources trivially satisfies condition 1 with @xmath455 , and it remains to check conditions 2 and 3 .    to check condition 2 ,",
    "let us examine the normalized @xmath2th - order relative entropy between @xmath14 and @xmath58 , with @xmath453 and @xmath456 . because the sources are i.i.d .",
    ", @xmath457 applying the inequality @xmath458 and some straightforward algebra , we get the bound @xmath459 now fix a small @xmath460 , and suppose that @xmath461",
    ". then @xmath462 , so we can further upper - bound @xmath188 by @xmath463 thus , for a given @xmath15 , we see that @xmath464 for all @xmath179 in the open ball of radius @xmath67 around @xmath85 , with @xmath465 . using pinsker s inequality",
    ", we have @xmath466 for all @xmath2 .",
    "thus , condition 2 holds .    to check condition 3 , note that , for each @xmath2 , the yatracos class @xmath199 consists of all sets of the form @xmath467 for all @xmath468 .",
    "let @xmath469 and @xmath470 .",
    "then we can rewrite ( [ eq : yatracos_gauss ] ) as @xmath471 this is the set of all @xmath472 such that @xmath473 where @xmath474 is a third - degree polynomial in the six parameters @xmath475 .",
    "it then follows from lemma  [ lm : karpinski_macintyre ] that @xmath199 is a vc class with @xmath476 .",
    "therefore , condition 3 holds as well .",
    "again , let @xmath451 and consider the case when @xmath10 is a gaussian autoregressive source of order @xmath51 , i.e. , it is the output of an autoregressive filter of order @xmath51 driven by white gaussian noise . then there exist @xmath51 real parameters @xmath477 ( the filter coefficients ) , such that @xmath478 where @xmath479 is an i.i.d .",
    "gaussian process with zero mean and unit variance .",
    "let @xmath480 be the set of all @xmath477 , such that the roots of the polynomial @xmath481 , where @xmath482 , lie outside the unit circle in the complex plane .",
    "this ensures that @xmath10 is a stationary process .",
    "we now proceed to check that conditions 13 of section  [ sec : results ] are satisfied .",
    "the distribution of each @xmath174 is absolutely continuous , and we can invoke the result of mokkadem @xcite to conclude that , for each @xmath15 , the process @xmath10 is _ geometrically mixing _ , i.e. , for every @xmath15 , there exists some @xmath483 , such that @xmath484 .",
    "now , for any fixed @xmath167 , @xmath485 for @xmath18 sufficiently large , so condition 1 holds .",
    "to check condition 2 , note that , for each @xmath15 , the fisher information matrix @xmath486 is independent of @xmath2 ( see , e.g. , section  6 of klein and spreij @xcite ) . thus , the asymptotic fisher information matrix @xmath487 exists and is nonsingular ( * ? ? ?",
    "* theorem  6.1 ) , so , recalling the discussion in section  [ sec : results ] , we conclude that condition 2 holds also .    to verify condition 3 , consider the @xmath2-dimensional marginal @xmath488 , which has the gaussian density @xmath489 where @xmath490 is the @xmath2th - order autocorrelation matrix of @xmath10 .",
    "thus , the yatracos class @xmath199 consists of sets of the form @xmath491 for all @xmath205 .",
    "now , for every @xmath15 , let @xmath492 .",
    "since @xmath493 is uniquely determined by @xmath85 , we have @xmath494 for all @xmath495 . using this fact , as well as the easily established fact that the entries of the inverse covariance matrix @xmath496 are second - degree polynomials in the filter coefficients @xmath477",
    ", we see that , for each @xmath119 , the condition @xmath497 can be expressed as @xmath498 , where @xmath474 is quadratic in the @xmath499 real variables @xmath500 .",
    "thus , we can apply lemma  [ lm : karpinski_macintyre ] to conclude that @xmath501 .",
    "therefore , condition 3 is satisfied as well .",
    "a hidden markov process ( or a hidden markov model , see , e.g. , @xcite ) is a discrete - time bivariate random process @xmath502 , where @xmath503 is a homogeneous markov chain and @xmath504 is a sequence of random variables which are conditionally independent given @xmath505 , and the conditional distribution of @xmath506 is time - invariant and depends on @xmath505 only through @xmath507 .",
    "the markov chain @xmath505 , also called the _",
    ", is not available for observation .",
    "the observable component @xmath10 is the source of interest . in information theory",
    "( see , e.g. , @xcite and references therein ) , a hidden markov process is a discrete - time finite - state homogeneous markov chain @xmath505 , observed through a discrete - time memoryless channel , so that @xmath504 is the observation sequence at the output of the channel .",
    "let @xmath349 denote the number of states of @xmath505 .",
    "we assume without loss of generality that the state space @xmath508 of @xmath505 is the set @xmath509 .",
    "let @xmath510_{i , j = 1,\\ldots , m}$ ] denote the @xmath511 transition matrix of @xmath505 , where @xmath512 .",
    "if @xmath513 is ergodic ( i.e. , irreducible and aperiodic ) , then there exists a unique probability distribution @xmath514 on @xmath508 such that @xmath515 ( the _ stationary distribution _ of @xmath505 ) , see , e.g. , section  8 of billingsley @xcite . because in this paper we deal with two - sided random processes , we assume that @xmath505 has been initialized with its stationary distribution at some time sufficiently far away in the past , and can therefore be thought of as a two - sided stationary process .",
    "now consider a discrete - time memoryless channel with input alphabet @xmath508 and output ( observation ) alphabet @xmath516 for some @xmath26 .",
    "it is specified by a set @xmath517 of transition densities ( with respect to @xmath38 , the lebesgue measure on @xmath160 ) .",
    "the channel output sequence @xmath10 is the source of interest .",
    "let us take as the parameter space @xmath518 the set of all @xmath519 transition matrices @xmath520 $ ] , such that all @xmath521 for some fixed @xmath522 .",
    "for each @xmath523 \\in \\lambda$ ] and each @xmath261 , the density @xmath524 is given by @xmath525 where @xmath526 for every @xmath527 .",
    "we assume that the channel transition densities @xmath528 , are fixed _ a priori _ , and do not include them in the parametric description of the sources .",
    "we do require , though , that @xmath529 and @xmath530 we now proceed to verify that conditions 13 of section  [ sec : results ] are met .",
    "let @xmath531 denote the @xmath2-step transition probability for states @xmath532 .",
    "the positivity of @xmath513 implies that the markov chain @xmath505 is _ geometrically ergodic _",
    ", i.e. , @xmath533 where @xmath534 and @xmath535 , see theorem  8.9 of billingsley @xcite .",
    "note that ( [ eq : geometric_ergodicity ] ) implies that @xmath536 this in turn implies that the sequence @xmath503 is exponentially @xmath0-mixing , see theorem  3.10 of vidyasagar @xcite .",
    "now , one can show ( see section  3.5.3 of vidyasagar @xcite ) that there exists a measurable mapping @xmath537}{\\cx}$ ] , such that @xmath538 , where @xmath539 is an i.i.d .",
    "sequence of random variables distributed uniformly on @xmath540 $ ] , independently of @xmath505 .",
    "it is not hard to show that , if @xmath505 is exponentially @xmath0-mixing , then so is the bivariate process @xmath541 .",
    "finally , because @xmath343 is given by a time - invariant deterministic function of @xmath542 , the @xmath0-mixing coefficients of @xmath10 are bounded by the corresponding @xmath0-mixing coefficients of @xmath543 , and so @xmath10 is exponentially @xmath0-mixing as well .",
    "thus , for each @xmath15 , there exists a @xmath544 , such that @xmath484 , and consequently condition  1 holds .    to show that condition 2 holds , we again examine the asymptotic behavior of the fisher information matrix @xmath486 as @xmath298 . under our assumptions on the state transition matrices in @xmath16 and on the channel transition densities",
    "@xmath545 , we can invoke the results of section  6.2 in douc , moulines and rydn @xcite to conclude that the asymptotic fisher information matrix @xmath487 exists ( though it is not necessarily nonsingular ) .",
    "thus , condition  2 is satisfied .",
    "finally we check condition 3 .",
    "the yatracos class @xmath199 consists of all sets of the form @xmath546 for all @xmath523 , \\theta ' = [ a'_{ij } ] \\in \\lambda$ ] .",
    "the condition @xmath497 can be written as @xmath547 , where for each @xmath119 , @xmath548 is a polynomial of degree @xmath2 in the @xmath549 parameters @xmath550 , @xmath551 .",
    "thus , lemma  [ lm : karpinski_macintyre ] implies that @xmath552 , so condition  3 is satisfied as well .",
    "we have shown that , given a parametric family of stationary mixing sources satisfying some regularity conditions , there exists a universal scheme for joint lossy compression and source identification , with the @xmath2th - order lagrangian redundancy and the variational distance between @xmath2-dimensional marginals of the true and the estimated source both converging to zero as @xmath553 , as the block length @xmath2 tends to infinity .",
    "the sequence @xmath24 quantifies the learnability of the @xmath2-dimensional marginals .",
    "this generalizes our previous results from @xcite for i.i.d .",
    "sources .",
    "we can outline some directions for future research .    * both in our earlier work @xcite and in the present paper",
    ", we assume that the dimension of the parameter space is known _ a priori_. it would be of interest to consider the case when the parameter space is finite - dimensional , but its dimension is not known .",
    "thus , we would have a hierarchical model class @xmath554 , where , for each @xmath18 , @xmath555 is an open subset of @xmath17 , and we could use a complexity regularization technique , such as  structural risk minimization \" ( see , e.g. , lugosi and zeger @xcite or chapter 6 of vapnik @xcite ) , to adaptively trade off the estimation and the approximation errors . * the minimum - distance density estimator of devroye and lugosi @xcite , which plays the key role in our scheme both here and in @xcite , is not easy to implement in practice , especially for multidimensional alphabets . on the other hand , there are two - stage universal schemes , such as that of chou , effros and gray @xcite , which do not require memory and select the second - stage code based on pointwise , rather than average , behavior of the source . these schemes , however , are geared toward compression , and do not emphasize identification .",
    "it would be worthwhile to devise practically implementable universal schemes that strike a reasonable compromise between these two objectives .",
    "* finally , neither here nor in our earlier work @xcite have we considered the issues of optimality .",
    "it would be of interest to obtain lower bounds on the performance of any universal scheme for joint lossy compression and identification , say , in the spirit of minimax lower bounds in statistical learning theory ( cf . ,",
    "e.g. , chapter 14 of devroye , gyrfi and lugosi @xcite ) .",
    "conceptually , our results indicate that links between statistical modeling ( parameter estimation ) and universal source coding , exploited in the lossless case by rissanen @xcite , are present in the domain of lossy coding as well",
    ". we should also mention that another modeling - based approach to universal lossy source coding , due to kontoyiannis and others ( see , e.g. , madiman and kontoyiannis @xcite and references therein ) , treats code selection as a statistical estimation problem over a class of model distributions in the _ reproduction space_. this approach , while closer in spirit to rissanen s minimum description length ( mdl ) principle @xcite , does not address the problem of joint source coding and identification , but it provides a complementary perspective on the connections between lossy source coding and statistical modeling .",
    "in this appendix , we detail some properties of lagrange - optimal variable - rate vector quantizers . our exposition is patterned on the work of linder @xcite , with appropriate modifications .",
    "as elsewhere in the paper , let @xmath9 be the source alphabet and @xmath87 the reproduction alphabet , both assumed to be polish spaces .",
    "as before , let the distortion function @xmath13 be induced by a @xmath556-bounded metric on a polish metric space @xmath89 containing @xmath88 . for every @xmath557 ,",
    "define the metric @xmath558 on @xmath559 by @xmath560 for any pair @xmath561 of probability measures on @xmath562 , let @xmath563 be the set of all probability measures on @xmath564 having @xmath565 and @xmath566 as marginals , and define the _",
    "wasserstein metric _ @xmath567 ( see gray , neuhoff and shields @xcite for more details and applications . ) note that , because @xmath13 is a bounded metric , @xmath568 for all @xmath569 .",
    "taking the infimum of both sides over all @xmath569 and observing that @xmath570 see , e.g. , section  i.5 of lindvall @xcite , we get the useful bound @xmath571    now , for each @xmath2 , let @xmath572 denote the set of all discrete probability distributions on @xmath573 with finite entropy .",
    "that is , @xmath574 if and only if it is concentrated on a finite or a countable set @xmath575 , and @xmath576 for every @xmath574 , consider the set @xmath577 of all one - to - one maps @xmath578 , such that , for each @xmath579 , the collection @xmath580 satisfies the kraft inequality , and let @xmath581 be the minimum expected code length .",
    "since the entropy of @xmath53 is finite , there is always a minimizing @xmath582 , and the shannon  fano bound ( see section  5.4 of cover and thomas @xcite ) guarantees that @xmath583 .",
    "now , for any @xmath125 , any probability distribution @xmath32 on @xmath562 , and any @xmath574 , define @xmath584 to give an intuitive meaning to @xmath585 , let @xmath586 and @xmath587 be jointly distributed random variables with @xmath588 and @xmath589 , such that their joint distribution @xmath590 achieves @xmath591 .",
    "then @xmath585 is the expected lagrangian performance , at lagrange multiplier @xmath318 , of a _ stochastic _",
    "variable - rate quantizer which encodes each point @xmath116 as a binary codeword with length @xmath592 and decodes it to @xmath593 in the support of @xmath53 with probability @xmath594 .",
    "the following lemma shows that deterministic quantizers are as good as random ones :    [ lm : ran_det ]    let @xmath595 be the expected lagrangian performance of an @xmath2-block variable rate quantizer operating on @xmath588 , and let @xmath596 be the expected lagrangian performance , with respect to @xmath32 , of the best @xmath2-block variable - rate quantizer",
    ". then @xmath597    consider any quantizer @xmath598 with @xmath599 .",
    "let @xmath600 be the distribution of @xmath601 . clearly , @xmath602 , and @xmath603 hence , @xmath604 . to prove the reverse inequality , suppose that @xmath588 and @xmath589 achieve @xmath591 for some @xmath574 .",
    "let @xmath605 be their joint distribution .",
    "let @xmath575 be the support of @xmath53 , let @xmath606 achieve @xmath607 , and let @xmath608 be the associated binary code .",
    "define the quantizer @xmath598 by @xmath609 and @xmath610 then @xmath611 on the other hand , @xmath612 so that @xmath613 , and the lemma is proved .",
    "the following lemma gives a useful upper bound on the lagrangian mismatch :    [ lm : mismatch ] let @xmath614 be probability distributions on @xmath562 .",
    "then @xmath615    suppose @xmath616 .",
    "let @xmath617 achieve @xmath618 ( or be arbitrarily close ) .",
    "then @xmath619 where in ( a ) we used lemma  [ lm : ran_det ] ) in ( b ) we used the definition of @xmath620 , in ( c ) we used the fact that @xmath621 is a metric and the triangle inequality , and in ( d ) we used the bound ( [ eq : wasser_var ] ) .",
    "finally , the lemma below shows that , for bounded distortion functions , lagrange - optimal quantizers have finite codebooks :    [ lm : finite_codebook ] for positive integers @xmath622 , let @xmath623 denote the set of all zero - memory variable - rate quantizers with block length @xmath2 , such that for every @xmath624 , the associated binary code @xmath508 of @xmath114 satisfies @xmath625 and @xmath626 for every @xmath527 .",
    "let @xmath32 be a probability distribution on @xmath562 .",
    "then @xmath627 with @xmath628 and @xmath629 .",
    "let @xmath630 with encoder @xmath631 and decoder @xmath632 achieve the @xmath2th - order optimum @xmath596 for @xmath32 .",
    "let @xmath633 be the shortest binary string in @xmath508 , i.e. , @xmath634 without loss of generality , we can take @xmath635 as the minimum - distortion encoder , i.e. , @xmath636 thus , for any @xmath527 and any @xmath637 , @xmath638 hence , @xmath639 for all @xmath527 .",
    "furthermore , @xmath640 .",
    "now pick an arbitrary reproduction string @xmath641 , let @xmath642 be the empty binary string ( of length zero ) , and let @xmath643 be the zero - rate quantizer with the constant encoder @xmath644 and the decoder @xmath645 . then @xmath646 .",
    "on the other hand , @xmath647 . therefore , @xmath648 so that @xmath649 .",
    "hence , @xmath650 since the strings in @xmath508 must satisfy kraft s inequality , we have @xmath651 which implies that @xmath652 .",
    "the author would like to thank andrew r.  barron , ioannis kontoyiannis and mokshay madiman for stimulating discussions , and the anonymous reviewers for several useful suggestions that helped improve the paper .",
    "m.  madiman and i.  kontoyiannis , `` second - order properties of lossy likelihoods and the mle / mdl dichotomy in lossy compression , '' brown university , appts report no .",
    "04 - 5 , may 2004 , available [ online ] at http://www.dam.brown.edu/ptg/reports/04-5.pdf ."
  ],
  "abstract_text": [
    "<S> we consider the problem of joint universal variable - rate lossy coding and identification for parametric classes of stationary @xmath0-mixing sources with general ( polish ) alphabets . </S>",
    "<S> compression performance is measured in terms of lagrangians , while identification performance is measured by the variational distance between the true source and the estimated source . </S>",
    "<S> provided that the sources are mixing at a sufficiently fast rate and satisfy certain smoothness and vapnik  chervonenkis learnability conditions , it is shown that , for bounded metric distortions , there exist universal schemes for joint lossy compression and identification whose lagrangian redundancies converge to zero as @xmath1 as the block length @xmath2 tends to infinity , where @xmath3 is the vapnik  </S>",
    "<S> chervonenkis dimension of a certain class of decision regions defined by the @xmath2-dimensional marginal distributions of the sources ; furthermore , for each @xmath2 , the decoder can identify @xmath2-dimensional marginal of the active source up to a ball of radius @xmath4 in variational distance , eventually with probability one . </S>",
    "<S> the results are supplemented by several examples of parametric sources satisfying the regularity conditions . </S>",
    "<S> +   + _ index terms_learning , minimum - distance density estimation , two - stage codes , universal vector quantization , vapnik  chervonenkis dimension . </S>"
  ]
}