{
  "article_text": [
    "networks of interacting agents play an important modelling role in fields as diverse as computer science , biology , ecology , economy and sociology .",
    "an important notion in these networks is the _ distance _ between two agents .",
    "depending on the circumstances , distance may be measured by the strength of interaction between the agents , by their spatial distance or by some other criterium expressing the existence of a link between the agents . based on this notion ,",
    "global parameters have been constructed to characterize the connectivity structure of the networks .",
    "two of them are the _ clustering coefficient _ and the _ characteristic path length_. the clustering coefficient measures the average probability that two agents , having a common neighbor , are themselves connected .",
    "the characteristic path length is the average length of the shortest path connecting each pair of agents .",
    "these coefficients are sufficient to distinguish randomly connected networks from ordered networks and from small world networks . in ordered networks ,",
    "the agents being connected as in a crystal lattice , clustering is high and the characteristic path length is large too . in randomly connected networks ,",
    "clustering and path length are low , whereas in small world networks@xcite @xcite @xcite clustering may be high while the path length is kept at a low level .",
    "an important question is to find out what the mechanisms are , that lead to each type of structure , when a network of interacting agents evolves in time .",
    "in general , networks of agents organize themselves for some purpose .",
    "for example , a country is organized to insure the survival and well - being of its inhabitants ( or of a subset thereof , anyway ) , supply networks are organized to bring food to a town every day and the network of neurons in the brain is organized to process the information that arrives through the sensorial organs . therefore one might be led to think that it is the function of the network that determines its form .",
    "a simple example shows that it is not necessarily so .",
    "restaurants and private homes in a large city do not keep more than a few days worth of food and without a continuous replenishment of their reserves the city would collapse in a few days .",
    "the supply problem of several million inhabitants is solved every day in most cities by a self - organized network of producers , transporters and retailers where clustering and a short path length are the rule .",
    "alternatively , in a centralized economy , a different , very structured system may be organized with producers delivering their goods to a local cooperative , where they are collected by a state - organized transportation agency , which then delivers it to a few centralized stores , where all consumers are supposed to acquire their goods . in this case one",
    "has a very regular structure .",
    "people might say that one system is more efficient than the other , but that is quite irrelevant as far as functionality is concerned . in both cases the city",
    "is supplied and the fact is , that the structure of the two networks is quite different .    that institutions used in different societies to achieve similar aims may be very different is a well known fact .",
    "this also casts considerable doubt on any attempts to characterize the uniqueness of optimal solutions .",
    "the solutions that are arrived at must be largely history - dependent .",
    "any optimality criterium should therefore not be based on the functionality of the solution , but on other factors like stability , resistance to change , adaptability to a changing environment , etc .    of course , if there is only one possible configuration of the network for each desired functionality then , whenever the functionality is achieved , the form is fixed . in that case function",
    "determines form and the form does not dependent on the method by which the functionality is achieved .",
    "however , this is not the most frequent situation in networks of many agents .",
    "what we call the _ function of the network _ is associated to a few collective variables , like survival of the group , making war on a neighboring country , maintaining a few simple myths , extracting global concepts like color or pain from a multitude of external stimuli , etc .",
    "that is , the function of the network is related to a number of variables much smaller than the number of agents or internal degrees of freedom of the network . in that case",
    "it is to be expected that several distinct configurations of the network will be associated to the same functionality .    in the space of all the configurations that realize a given function",
    ", an important question is to know what types of network structures do exist , concerning in particular their connectivity properties ( clustering , path length , etc . )",
    "this is the main problem we address in this paper . because general statements about networks tend to be vague and do not go a long way , we concentrate in neural network models that are learning to represent a given function .",
    "the use of neural networks as a paradigm for networks of interacting agents is not so restrictive as it may seem because , as shown by doyne farmer@xcite , they are largely equivalent to many other connectionist systems .",
    "the distance between the agents ( nodes ) in the network is defined by the inverse of the absolute value of the connection strength .",
    "nodes are considered to be connected if the connection strength exceeds a certain threshold .",
    "this threshold is not fixed a priori , but is determined by a clustering algorithm as the lowest value that insures connectivity of the whole network .",
    "two learning methods for the same function are tested and , using the distance defined by the connection strengths , clustering coefficients and characteristic path lengths are computed .",
    "the general conclusion is that , in fact , function does not determine form , very different structures being obtained ( on average ) by the two methodsspace of the network configurations compatible with a specified mapping or a given training set , determines what has been called the entropy of the network .",
    "an adequate control of this quantity is important for the generalization problem@xcite .",
    "the metric characterization of the @xmath0space given in this paper provides a refiniment of the residual entropy configurations after the learning process . ] .",
    "the first learning algorithm is in the class of reinforcement learning methods , of which several variants exist@xcite @xcite , whereas , in the second , the agents ( nodes ) are punished by mistakes but nothing happens when the answer is the desired one@xcite .",
    "we will denote the first method by _ reinforcement learning method _ ( rlm ) and the second by _ learning from mistakes _ ( lfm ) .",
    "the neural network , as a network of agents , is richer than the graphs that , in the past , have been used to study connectivity in networks .",
    "this arises from the fact that not only the connections between nodes may be positive ( excitatory ) or negative ( inhibitory ) , but also the connections are in general asymmetric , one node having an influence over other node different from the influence it receives from the latter . to characterize this additional information on the structure of the network we have introduced new quantities to measure these properties , namely  _ symmetry , cooperation , antagonism and residuality coefficients _ as well as _ directed path lengths . _",
    "as a paradigm for goal - oriented networks , we study a neural network which starts as randomly connected ( with small connection strengths ) and learns to reproduce a function by two different learning methods . a certain number of nodes are defined to be input nodes and some others output nodes . in the numerical experiments we report here ,",
    "we have taken two nodes as input , one as output and the function to be reproduced is a boolean function like the exclusive or , for example .",
    "similar results are obtained for other more complex functions .",
    "the only care to be taken is that the network should have a sufficiently large number of nodes to guarantee that the subspace of strength connections , compatible with implementation of that function , is large .",
    "otherwise , if there is only one possible configuration of connection strengths , function determines form and the dependence on the learning method can not be detected .",
    "it is also obvious that , even when the regions in function space , explored by different learning methods , are distinct , it may happen that , by chance , configurations obtained by different methods do coincide .",
    "this is borne out in our experiments , some overlap being observed between the configurations obtained by different learning methods . however , on average , different methods explore quite different regions of the function space .",
    "the results we obtain , indicate that the structures resulting from the _ reinforcement learning method _ ( rlm ) exhibit a high clustering coefficient together with an intermediate value for the characteristic path length .",
    "on the other hand , the structures obtained by the _ learning from mistakes method _ ( lfm ) have a low clustering coefficient with characteristic path lengths similar to those obtained from random structures , that is , similar to the structures used to initialize the network .",
    "rlm seems to be largely dependent on the establishment of a highly clustered configuration while lfm does not require the creation of such an ordered structure .",
    "one may think of the structures created by the latter method as being those of a highly adaptive system where specific tasks are performed without strongly committed configurations .",
    "the network we study is characterized by a non - layered architecture representing a fully connected system of twelve agents with connection strengths initially chosen at random in the interval @xmath1 .",
    "the absolute value of the connection strength @xmath2 corresponds to the inverse distance of the agent pair @xmath3,@xmath4 . from the initial , random , configuration ,",
    "two different learning algorithms were used to obtain an xor function .",
    "the first one is a hebbian - like method , while the second relies on the learning from mistakes approach , with reinforcement being replaced by a process of depressing the synaptic connections involved in mistakes@xcite .",
    "both have a biological inspiration , the first one corresponding to long term synaptic potentiation@xcite and the second to long term synaptic depression@xcite .    in both learning methods ,",
    "a sigmoid function @xmath5 is used as activation function and the computation of the output signal includes a bias @xmath6 , which operates as a regulatory mechanism for the overall activity of the network . if the network activity is too low , the bias @xmath6 is decreased until an appropriate number of firing neurons is obtained . on the other hand , if the activity exceeds a certain limit , @xmath6 is increased in order to keep a low level of activity in the network .",
    "a neuron is defined as _ firing _ if its output is above @xmath7 of the maximum output ( @xmath8 ) .",
    "* reinforcement learning method ( rlm ) *    the connections between firing neurons are strengthened or weakened according to whether the output is successful or not .",
    "the process affects all firing neurons in the same way .",
    "the reinforcement updating rule is :    if the output is the correct one @xmath9 otherwise @xmath10 @xmath11 is the output of neuron @xmath3 and @xmath12 .    also , as stated before",
    ", the bias is adjusted to keep the overall network activity at a low level .",
    "saturation is avoided by a global rescaling of all the coupling strengths , when one of them exceeds a fixed threshold .    * learning from mistakes ( lfm ) *    if the output happens to be the desired one , nothing is done , otherwise the connections between firing neurons are depressed by a fixed amount @xmath13 , which is redistributed among the connections between non - firing neurons , @xmath14 with @xmath12    as in rlm , there is a global regulatory mechanism to keep the overall activity at a low level .",
    "clustering coefficient ( cc ) and characteristic path length ( cpl ) are important statistical parameters used in graph theory .",
    "these two parameters have been the object of growing attention ever since the small - world phenomenon was identified as an interesting property of the structures found in many different fields .",
    "the small - world feature@xcite is characteristic of structures with cc similar to the one obtained in regular structures but with cpl s close to those of random networks .    in the past , graph modelling concerned itself mostly with completely random or completely regular structures .",
    "regular graphs combine high cc with large cpl while , in the opposite case , random graphs exhibit low cc and small cpl . starting from a completely regular structure and applying a random rewiring procedure to interpolate between regular and random networks",
    "it has been found@xcite that there is a broad interval of structures over which cpl is almost as small as in random graphs and yet cc is much greater than expected in the random case .",
    "this rewiring procedure helps to characterize the structural aspects of a network in the transition from order to disorder .    in the work presented here ,",
    "the networks move in the opposite direction , from completely random towards a goal - oriented structure .",
    "the basic intuition is that forcing a randomly weighted network to learn a function by different learning methods , may lead to different forms of organization even though the methods are both targeted at reaching the same functional goal .",
    "in particular , we want to find out to what extent the success of a learning method is dependent on the transition from disorder to order in the network structure .",
    "the connectivity of the ( starting ) randomly connected networks provides reference values that help to characterize the lack of order . in the other extreme ,",
    "the more regular structures that arise from learning are evaluated by finding out how much their cc and cpl differ from those that characterize randomness in the starting configurations .",
    "cc and the cpl usually apply to graph structures that are connected and sparse . since the networks we work with are fully - connected structures , a first step is targeted at obtaining a sparse representation of the network , with the _ degree of sparseness _ generated by the learning method itself , instead of an _ a priori _ specification .",
    "notice that , when looking for a suitable degree of sparseness , one must avoid disconnected graphs ( where the value of the cpl of any disconnected element would be infinite ) . for this purpose",
    "we construct of a graph structure from the connection strengths .    * the graph representation of the network *    from the @xmath15 matrix @xmath16 of connection strengths , @xmath17",
    ", we construct a @xmath15 distance matrix @xmath18 , with elements @xmath19 .",
    "based on the distances @xmath20 , a hierarchical clustering is then performed using the _",
    "nearest neighbor method .",
    "_ initially @xmath21 clusters corresponding to the @xmath21 agents are considered .",
    "then , at each step , two clusters @xmath22 and @xmath23 are clumped into a single cluster if @xmath24 with the distance between clusters being defined by @xmath25 with @xmath26 and @xmath27    this process is continued until there is a single cluster .",
    "this clustering process is also known as the _ single link method _ , being the method by which one obtains the minimal spanning tree ( mst ) of a graph . in a connected graph ,",
    "the mst is a tree of @xmath28 edges that minimizes the sum of the edge distances .    in a network with @xmath21 agents ,",
    "the hierarchical clustering process takes @xmath28 steps to be completed , and uses , at each step , a particular distance @xmath29 to clump two clusters into a single one .",
    "let @xmath30 , @xmath31 , be the set of distances @xmath32 used at each step of the clustering , and @xmath33 .",
    "it follows that @xmath34 .    at this point",
    "we are able to define a representation of @xmath18 with sparseness replacing fully - connectivity in a suitable way . for this purpose , a boolean graph @xmath35 ( with @xmath21 vertices being the network nodes )",
    "is defined setting @xmath36 if @xmath37 and @xmath38 if @xmath39 . as usual , _ null arcs _ of @xmath35 are those for which @xmath40 while for _ unit arcs _",
    "here we want to consider two nodes @xmath3 and @xmath4 to be connected if either @xmath41 or @xmath42 .",
    "therefore we take @xmath43 .",
    "later on ( sect .",
    "3 ) we will take into account directional effects .",
    "let @xmath44 be the matrix associated with @xmath35 .",
    "each element @xmath45 is the number of edges of @xmath35 that join the vertices @xmath3 and @xmath4 and , since @xmath35 is a simple graph , @xmath46 .",
    "the degree of @xmath35 , or its _ coordination number _ @xmath47 , represents the average number of unit arcs leaving each element of the graph .",
    "the coordination number characterizes the sparseness of the graph and has an important bearing on its properties@xcite . in our approach",
    "we obtain the value of @xmath47 from the network itself .",
    "therefore we avoid any _ a priori _ estimation and , by the hierarchical clustering method , we also avoid disconnectivity .",
    "we are also interested in defining @xmath48 , @xmath49 , as the set of distances @xmath32 whose values are less or equal to @xmath50 , and computing @xmath51 .",
    "clearly @xmath52 is the number of _ redundant _ elements in @xmath53 , that is , the number of distances @xmath41 that , although being smaller than @xmath54 , need not be considered in the hierarchical clustering process .",
    "later on , we will discuss the relation between the value of @xmath55 and the clustering coefficient of the graph .",
    "* *    * clustering coefficient *    the clustering coefficient ( cc ) of a graph @xmath56 ( averaged over all vertices @xmath57 of @xmath56 ) measures whether two vertices adjacent to another vertex @xmath58 are adjacent to each other .",
    "when cc=1 one has a group of disconnected but individually complete subgraphs , while cc=0 implies that no neighbor of any vertex @xmath58 is adjacent to any other neighbor of @xmath58 .    at the end of the learning process , we build an adjacency list from the matrix @xmath44 associated with @xmath35 .",
    "it is done by listing all vertices of @xmath35 and , next to each one , its neighboring vertices . from the adjacency list of @xmath35 the clustering coefficient of @xmath35",
    "may be found in two different ways .    1 .",
    "the first method computes the value of the clustering coefficient @xmath59 of each vertex @xmath58 by dividing the number of unit arcs in the neighborhood of @xmath58 by the total number of arcs in such a neighborhood , which is given by @xmath60 , @xmath61 being the size of the adjacency list of vertex @xmath58 .",
    "averaging over all vertices of @xmath35 we obtain a coefficient which we denote by @xmath62 .",
    "2 .   in the second method",
    "the calculation of the clustering of @xmath35 does not consider the vertices @xmath58 of @xmath56 that have just one vertex in its neighborhood .",
    "for each pair of unit arcs @xmath63 and @xmath64 of @xmath35 that share a common vertex @xmath65 we count one if @xmath66 corresponds to a unit arc , otherwise we count nothing .",
    "the total sum is then divided by @xmath67 where @xmath61 is the size of the adjacency list of each vertex @xmath58 of @xmath35 . in this way",
    ", vertices with a single vertex in its neighborhood do not contribute to the value computed by the above expression ( since @xmath68 ) , being those vertices consequently excluded from the computation of @xmath69 .",
    "notice that , for a typical network , the values of @xmath70 and @xmath71 tend to be very similar .",
    "a significant difference between these values indicates either that the network has many single - neighbor vertices ( @xmath72 ) or that the distribution of the clustering coefficients for each vertex is very heterogeneous ( @xmath73 ) .",
    "to control these effects we have , for our simulations , computed both @xmath62 and @xmath70 .",
    "above , we have defined @xmath74 @xmath75 as the set of distances @xmath32 with values less or equal to @xmath54 and @xmath51 as the number of _ redundant _ elements in @xmath53 , that is , the number of distances @xmath41 that , although being smaller than @xmath54 , are not used in the hierarchical clustering process .    in a connected graph",
    "@xmath55 provides the cardinality of the cycle basis of @xmath76 , or its _",
    "cyclomatic number_. being a cycle basis of a graph defined by the set of its elementary cycles that taken together yield the entire graph , itself a cycle . in the next section ,",
    "when discussing the simulation results , we notice that , depending on the learning method , cycles and trees ( i.e. , connected graphs without cycles ) may or not appear in the resulting network structures .",
    "the clustered networks have a high coordination number while in the opposite case the networks approach a tree - like structure and , consequently , a low clustering coefficient . * *    * characteristic path length *    the characteristic path length ( cpl ) of a weighted graph is the average length of the shortest path between any two vertices in the graph . from the @xmath77 matrix @xmath16 of connection strengths , @xmath78 , and its corresponding distance matrix @xmath18 , the weighted graph @xmath79 ( with @xmath21 vertices corresponding to the network nodes ) is defined by @xmath80    we compute the characteristic path length of a weighted graph @xmath79 , by taking for each pair @xmath81 in @xmath79 , with @xmath82 , the smallest distance @xmath83 between @xmath3 and @xmath4",
    "the @xmath84 edges @xmath85 are sequentially taken from a list where the @xmath85 were sorted in ascending order . in the first step ,",
    "the smallest @xmath85 in the list provides the shortest distance between @xmath3 and @xmath4 . in the next steps , the new edge @xmath86 provides the shortest @xmath87 distance between @xmath88 and @xmath89 and",
    "may also provide another smallest path length by @xmath90 if @xmath91 or @xmath92 , namely : @xmath93    in the following steps we check whether the edge being considered , composed with the previously established minimal paths , defines a path @xmath94 that is smallest than a previously computed @xmath83 .",
    "if it happens @xmath95 replaces @xmath83 .",
    "this computation is sequentially repeated until the shortest distance @xmath96 between each pair of nodes in the graph is obtained .",
    "averaging over the @xmath84 edges of @xmath79 , we obtain the cpl of @xmath79 .",
    "* results *    the results presented here were obtained from several simulations in networks which start as randomly connected .",
    "a typical random network is shown in fig .",
    "1 . in this figure",
    ", the absolute value of each connection strength ( @xmath2 ) of the network specifies the grey intensity of the corresponding patch in the image .",
    "white patches represent null connections ( null arcs ) .",
    "units 1 and 2 are taken to be inputs and unit 12 is the output .",
    "this is the reason why the other units do not connect back to 1 and 2 and unit 12 does not connect back to the others .",
    "the absolute values of the connection strengths corresponding to the inverse of distances , dark patches represent small distances . with connection strengths chosen in the interval @xmath1 ,",
    "an almost black patch means @xmath97 , while an white patch corresponds to @xmath98 0.5 .",
    "the connectivity of random networks provides reference values to characterize the goal - oriented structures that are obtained by the learning methods . for this purpose ,",
    "2 shows the image of the adjacency matrix of a typical random network .",
    "it was obtained by :    1 .",
    "taking the network structure represented in fig.1 2 .   _",
    "_ applying the hierarchical clustering process to obtain the distance @xmath99 used in the last step of hierarchical clustering 3 .   building the corresponding boolean graph with adjacency matrix shown in fig .",
    "unit arcs ( @xmath100 ) are represented as black patches while null arcs ( @xmath101 ) correspond to white ones .    as shown in fig .",
    "2 , the graphs that represent the random structures used to initialize the network are characterized by a high degree of sparseness ( a small number of black patches in the corresponding image ) .",
    "the degree @xmath47 of the graph is much smaller than the number of agents in the network .",
    "moreover , in these graphs the number of unit arcs @xmath102 is usually only slightly larger than @xmath28 , which is the minimum value that ensures connectivity . as a consequence ,",
    "the number of _ redundant _ elements in the graph is small and the graph approaches a tree - like structure , with a small clustering coefficient ( see table 1 ) .",
    "3 shows the typical final structure of a network that starts as randomly connected and is organized by learning from mistakes ( lfm ) .",
    "the image shows that lfm networks are similar to the typical random structure shown in fig .",
    "the distribution of connection strengths is not , in general , very different from those generated at random , suggesting that lfm does not require the creation of a very organized structure in order to reach its functional goal .",
    "the image shown in fig .",
    "4 was obtained following the same steps as used for the image in fig .",
    "it is built from the network shown in fig.3 .",
    "it represents the adjacency matrix of a typical lfm network .",
    "given that the typical lfm structures differ little from a random configuration , it exhibits a significant degree of sparseness .",
    "looking at fig .",
    "4 we see that the number of unit arcs ( @xmath102 ) remains close to @xmath28 , hence the number of _ redundant _ elements in the graph is almost as small as in random networks . in a significant number of simulations ,",
    "the final structures are even closer to tree - like structures , with a consequently low clustering coefficient .",
    "5 shows a typical configuration for a network that learned through rlm .",
    "small and large distances are not so well distributed as they were at random , showing that rlm networks move away from the initial configuration in order to reach its functional goal .",
    "the degree of sparseness of fig .",
    "6 confirms this fact .",
    "the degree of sparseness of a typical rlm structure is smaller than that of a random one and also smaller than the degree of sparseness of a typical lfm network .",
    "some of the connection intensities are strongly increased during the learning process and the final network very often presents a significant degree of symmetry .",
    "the number of black patches strongly increases in the structure represented in fig .",
    "6 showing that the number of unit arcs ( @xmath102 ) is much greater than @xmath103 . consequently ,",
    "the number of _ redundant _ elements in the graph is significantly greater than in random networks .",
    "as shown in this figure , the final rlm structures tend to contain cycles and move away from the tree - like structures that appear in random and lfm networks .",
    "table 1 shows typical values for the degree of the graphs ( @xmath47),the clustering coefficient ( cc and ca ) and the characteristic path length ( cpl ) for random , lfm and rlm networks . in each case",
    "we show the mean ( x ) and the standard deviation ( @xmath104 ) obtained in the simulations .    [ cols=\"^,^,^,^,^,^,^,^,^\",options=\"header \" , ]     table 3 :",
    "symmetry , cooperation , antagonism and residuality    the results show that , before learning , in the random networks the weight of the connections below the threshold @xmath105 is two to three times higher than the weight of the connections above the threshold . after learning the residuality coefficient decreases in both the lfm and rlm networks , with a very significant decrease in the rlm networks .",
    "this is due to the fact that rlm networks become less sparse after learning ( see @xmath106 in table 2 ) forcing several _ residual connections _ to leave this category . for the lfm networks , although sparseness does not change much after learning , the decrease of @xmath107 happens because , the connection strengths above @xmath105 tend to be stronger than those that remain below the threshold .",
    "cooperation ( and antagonism ) behaves quite differently depending on the learning method . in lfm networks , @xmath108 approaches @xmath8 after learning , while , in a typical rlm network , the value of the cooperation coefficient stays around @xmath109 .",
    "antagonism seems to disappear on lfm learning . on the other hand ,",
    "rlm learning keeps a reasonable degree of antagonism @xmath110 in the network structure .",
    "the networks we have studied acquire a structure while learning a function . while clustering and path length bring information on the connectivity of the structures , the characterization of the mechanisms leading to each type of structure raises a few other questions , namely :    \\(i ) how easily will the acquired structure adapt itself to the representation of another function ?",
    "\\(ii ) to what extent do the structures succeed in keeping the same functionality when some of their connections are suppressed ?    as a first step to answer these questions we have measured the _ adaptability _ of rlm and lfm networks as follows :    1 .   a network with connection strengths chosen at random in the interval @xmath111 , learns to reproduce the exclusive or function 2 .   after learning",
    "the matrix @xmath112 keeps the resulting normalized @xmath113 connection strengths 3 .",
    "the network with connection strengths @xmath112 learns to reproduce the and function 4 .   after",
    "learning we obtain the matrix @xmath45 of the resulting normalized connection strengths 5 .   the network _ adaptability coefficient _",
    "@xmath114 is computed by @xmath115    averaging @xmath116 over the results of several simulations we have obtained @xmath117 , yields an average change @xmath118 when lfm is the method that is chosen .",
    "following the same set of steps as above in order to compute the adaptability of rlm networks turns out to be quite difficult because step 3 frequently fails . in contrasts with lfm structures ,",
    "adaptation in rlm networks is almost absent and new learning is efficient only if one starts from scratch , i.e. , from a randomly connected network structure .",
    "these results indicate that the configurations obtained by different learning methods behave quite differently as far as adaptability is concerned .",
    "the structures created by the lfm method are those of a highly adaptive system whereas for rlm the structures that are created seem to become highly specialized for its purpose .    to evaluate the _ robustness _ of the structures the following algorithm is applied :    \\(i ) a vector @xmath119 of @xmath120 components is defined , corresponding each components to a particular connection in the network .",
    "the vector is initialized with zeros .",
    "\\(ii ) after the learning process , one cuts each one of the connections in turn and tests whether the learned function is still reproduced .",
    "if the test fails one adds a one to the corresponding component of the vector @xmath121 .",
    "\\(iii ) the test is repeated for all the connections and for a certain number of different networks ( 60 different networks in our simulations )    \\(iv ) the distribution @xmath122 of the values stored in the vector @xmath121 is plotted .    figs .",
    "10 and 11 show the results corresponding to lfm and rlm networks with the same number of trials in each case .",
    "the results indicate that rlm networks are more robust than those resulting from the lfm method . the former exhibit , on average , a smaller number of errors , suggesting that , individually , the role of any specific connection is less important for reaching the desired functionality than in rlm networks .",
    "moreover , in the case of rlm networks the distribution of failures ( fig .",
    "11 ) shows that some connections are much more important than others ( those that contribute to the fat tail ) , while a large amount of connections play a smaller role .",
    "in multi - agent networks the overall functionality or collective behavior does not uniquely determine the interaction topology and the graph structure of the network .",
    "this happens because , in general , many different configurations are associated to the same ( small number ) of relevant collective variables . then , the organizing method , that is , the evolution history of the network , is the main determining factor on the establishment of a particular type of structure on the network .",
    "these general conclusions are borne out by our study of networks that , starting from a random configuration , learn to represent a function by two different learning methods .",
    "_ clustering coefficients _ and ( non - directed ) _ characteristic path lengths _ turn out to be appropriate to discriminate the two organizing methods that were used . in particular , a striking confirmation of the ",
    "_ function does not determine form _ ",
    "assertion is obtained from the fact that the high clustering and intermediate path length of rlm networks indicate that reinforcement learning establishes a highly ordered configuration , whereas the same functionality is obtained in lfm networks with low clustering and path lengths similar to random networks .    the idea that learning something or reaching some goal requires some degree of order is well accepted .",
    "so is the knowledge that regular structures - in opposition to those generated at random - exhibit high clustering and large path length .",
    "recent work has shown that there is a multitude of cases where the structures of interest lie in a broad interval between regular and random . in this paper",
    "we have shown that there are cases where the same goal may be achieved by structures near both extremes .",
    "achieving a goal does not necessarily require very organized structures .",
    "moreover when the method followed to achieve the goal implies the establishment of a high degree of order , the resulting structures tend to be hard to adapt to any different goal .",
    "as shown in the last section of the paper , algorithms may be developed to characterize , in a quantitive manner , the degree of _ robustness _ and _ adaptability _ of the networks .    in the neural - like networks that we have been using ( and in most natural occurring networks ) the interactions between the agents",
    "are not symmetric and may have positive or negative signs .",
    "this in contrast to the simple graph structures used in the past to study interaction topologies . _ directed path lengths _ , as well as _ symmetry _ ,",
    "_ cooperation _ , _ antagonism _ and _ residuality _ coefficients were defined , which provide a refined characterization of the network structures .",
    "relevant differences were also found between the learning methods when these new coefficients are measured .",
    "for example , starting from a random network , lfm seems to strongly improve cooperation , whereas in rlm cooperation increases only slightly .",
    "99 j. w. clark , g. c. littlewort and j. rafelski ; _ topology , structure and distance in quasirandom neural networks _ in _ computer simulation in brain science _ , r. m. j. cotterill ( ed . ) cambridge univ .",
    "press , cambridge 1988 .",
    "j. s. denker , d. b. schwartz , b. s. winter , s. a. solla , r. e. howard , l. d. jackel and j. j. hopfield ; _ large automatic learning , rule extraction and generalization _ , complex systems 1 ( 1987 ) 877 - 922 ."
  ],
  "abstract_text": [
    "<S> the main problem we address in this paper is whether function determines form when a society of agents organizes itself for some purpose or whether the organizing method is more important than the functionality in determining the structure of the ensemble .    as an example </S>",
    "<S> , we use a neural network that learns the same function by two different learning methods . for sufficiently large networks , very different structures </S>",
    "<S> may indeed be obtained for the same functionality . </S>",
    "<S> clustering , characteristic path length and hierarchy are structural differences , which in turn have implications on the robustness and adaptability of the networks .    in networks , as opposed to simple graphs , the connections between the agents are not necessarily symmetric and may have positive or negative signs . </S>",
    "<S> new characteristic coefficients are introduced to characterize this richer connectivity structure .    * keywords * : networks , agents , clustering , path length . </S>"
  ]
}