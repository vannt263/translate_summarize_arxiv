{
  "article_text": [
    "stationary points are the most interesting and most important points of potential energy surfaces .",
    "the relative energies of local minima and their associated configuration space volumes determine thermodynamic equilibrium properties.@xcite according to transition state theory , dynamical properties can be deduced from the energies and the connectivity of minima and transition states.@xcite therefore , the efficient determination of stationary points of potential energy surfaces is of great interest to the communities of computational chemistry , physics , and biology .",
    "clearly , optimization and in particular minimization problems are present in virtually any field .",
    "this explains why the development and mathematical characterization of iterative optimization techniques are important and longstanding research topics , which resulted in a number of highly sophisticated methods like for example direct inversion of the iterative subspace ( diis),@xcite conjugate gradient ( cg),@xcite or quasi - newton methods like the broyden - fletcher - goldfarb - shanno ( bfgs ) algorithm@xcite and its limited memory variant ( l - bfgs).@xcite since for a quadratic function newton s method is guaranteed to converge within a single iteration , it is not surprising that the bfgs and l - bfgs algorithms belong to the most efficient methods for minimizations of atomic systems.@xcite    if the potential energy surface can be computed with an accuracy on the order of the machine precision , the above mentioned algorithms usually work extremely well . in practice , however , computing the energy surface at this high precision is not possible for physically accurate but computationally demanding levels of theory like for example density functional theory ( dft ) . at dft level",
    ", this is due the finitely spaced integration grids and self consistency cycles that have to be stopped at small , but non - vanishing thresholds .",
    "therefore , optimization algorithms that are used at these accurate levels of theory must not only be computationally efficient but also tolerant to noise in forces and energies .",
    "unfortunately , the very efficient l - bfgs algorithm is known to be noise - sensitive and therefore , frequently fails to converge on noisy potential energy surfaces . for this reason",
    ", the fast inertial relaxation engine ( fire ) has been developed.@xcite fire is a method of the damped molecular dynamics ( md ) class of optimizers.@xcite it accelerates convergence by mixing the velocity at every md step with a fraction of the current steepest descent direction .",
    "a great advantage of fire is its simplicity .",
    "however , fire does not make use of any curvature information and therefore usually is significantly less efficient than the newton or quasi - newton methods .",
    "potential energy surfaces are bounded from below and therefore descent directions guarantee that a local minimum will finally be found .",
    "furthermore , the curvature at a minimum is positive in all directions .",
    "this means , all directions can be treated on the same footing during a minimization .",
    "the situation is different for saddle point optimizations .",
    "a saddle point is a stationary point at which the potential energy surface is at a maximum with respect to one or more particular directions and at a minimum with respect to all other directions .",
    "close to a saddle point it is therefore not possible to treat all directions on the same footing . instead one has to single out the directions that have to be maximized .",
    "furthermore , far away from a saddle point it is usually impossible to tell , which search direction guarantees to finally end up in a saddle point .",
    "therefore , saddle point optimizations typically are more demanding and significantly less reliable than minimizations .    in this contribution",
    "we present a technique that allows to extract curvature information from noisy potential energy surfaces .",
    "we explain how to use this technique to construct a stabilized quasi - newton minimizer ( sqnm ) and a stabilized quasi - newton saddle finding method ( sqns ) . using benchmarks",
    ", we demonstrate that both optimizers are robust and efficient .",
    "the comparison of sqnm to l - bfgs and fire and of sqns to an improved dimer method@xcite reveals that sqnm and sqns are superior to their existing alternatives .",
    "the potential energy surface of an @xmath0-atomic system is a map @xmath1 that assigns to each atomic configuration @xmath2 a potential energy .",
    "it is assumed that a second order expansion of @xmath3 about a point @xmath4 is possible : @xmath5^{t}{\\boldsymbol{\\mathbf{\\nabla } } }                           e\\left({\\boldsymbol{\\mathbf{r}}}^{i}\\right)\\notag\\\\ & \\phantom{{}=e\\left({\\boldsymbol{\\mathbf{r}}}^{i}\\right)}+ \\frac{1}{2 } \\left[{\\boldsymbol{\\mathbf{r}}}-               { \\boldsymbol{\\mathbf{r}}}^{i}\\right]^{t } h_{{\\boldsymbol{\\mathbf{r}}}_{i } } \\left[{\\boldsymbol{\\mathbf{r}}}-               { \\boldsymbol{\\mathbf{r}}}^i\\right]\\\\ { \\boldsymbol{\\mathbf{\\nabla}}}e\\left({\\boldsymbol{\\mathbf{r}}}\\right ) & \\approx { \\boldsymbol{\\mathbf{\\nabla } } }             e\\left({\\boldsymbol{\\mathbf{r}}}^{i}\\right ) + h_{{\\boldsymbol{\\mathbf{r}}}_{i}}\\left[{\\boldsymbol{\\mathbf{r}}}-             { \\boldsymbol{\\mathbf{r}}}^i\\right ] , \\label{eq : secant}\\end{aligned}\\ ] ] here , @xmath6 is the hessian of the potential energy surface evaluated at @xmath4 . if @xmath2 is a stationary point , the left hand side gradient of eq .",
    "[ eq : secant ] vanishes and newton s optimization method follows : @xmath7 in the previous equation @xmath2 was renamed to @xmath8 in order to emphasize the iterative character of newton s method for non - quadratic potential energy surfaces .    in practice , it is in most cases either impossible to calculate an analytic hessian or it is too time consuming to compute it numerically by means of finite differences at every iteration",
    ". therefore , quasi - newton methods use an approximation to the exact hessian that is computationally less demanding . using a constant multiple of the identity matrix as an approximation to the hessian results in the simple steepest descent method . in most cases ,",
    "such a choice is a very poor approximation to the true hessian .",
    "however , improved approximations can be generated from local curvature information which is obtained from the history of the last @xmath9 displacements @xmath10 and gradient differences @xmath11 , where @xmath12 .      in noisy optimization problems ,",
    "the noisy components of the gradients can lead to displacement components that correspond to erratic movements on the potential energy surface .",
    "consequently , curvature information that comes from the subspace spanned by these displacement components must not be used for the construction of an approximate hessian .",
    "in contrast to this , the non - noisy gradient components promote locally systematic net - movements , which do not tend to cancel each other . in this sense the displacement components that correspond to these well defined net - movement span a significant subspace from which meaningful curvature information can be extracted and used for building an approximate hessian .    ]",
    "the situation is depicted in fig .",
    "[ fig : removenoise ] where the red solid vectors represent the history of normalized displacements and the blue dashed vectors constitute a basis of the significant subspace .",
    "all the red solid vectors in fig .",
    "[ fig : removenoise]a point into similar directions .",
    "therefore , curvature information should only be extracted from a one - dimensional subspace , as , for example , is given by the blue dashed vector .",
    "displacement components perpendicular to this blue dashed vector come from the noise in the gradients .",
    "in contrast to fig .",
    "[ fig : removenoise]a , fig .",
    "[ fig : removenoise]b shows a displacement that points into a considerably different direction than all the other displacements . for this reason , significant curvature information can be extracted in the full two - dimensional space .    to define the significant subspace",
    "more rigorously , we first introduce the set of normalized displacements @xmath13 where @xmath12 . with @xmath14 , linear combinations @xmath15 of the normalized displacements",
    "are defined as : @xmath16 furthermore , we define a real symmetric overlap matrix @xmath17 as @xmath18 it can be seen from , @xmath19 that @xmath20 is made stationary by coefficient vectors @xmath21 that are eigenvectors of the overlap matrix . in particular the longest and shortest vectors that can be generated by linear combinations with normalized coefficient vectors",
    "@xmath22 correspond to those eigenvectors of the overlap matrix that have the largest and smallest eigenvalues . as motivated above",
    ", the shortest linear combinations of the normalized displacements correspond to noise .    from now on , let the @xmath23 be eigenvectors of @xmath24 and let @xmath25 be the corresponding eigenvalues .",
    "with @xmath26 we finally define the _ significant subspace _ @xmath27 as @xmath28 where @xmath29 . in all applications presented in this work",
    ", @xmath30 has proven to work well .",
    "henceforth , we will refer to the dimension of @xmath27 as @xmath31 . by construction",
    "it is guaranteed that @xmath32 .",
    "it should be noted that at each iteration of the optimization algorithms that are introduced below , the significant subspace and its dimension @xmath31 can change .",
    "the history length @xmath9 usually lies between 5 and 20 .",
    "our procedure is analogous to lwdins canonical orthogonalization,@xcite which is used in the electronic structure community to remove linear dependencies from chemical basis sets .",
    "we define the projection @xmath33 of the hessian @xmath34 onto @xmath27 as @xmath35 where for all @xmath36 @xmath37 and @xmath38 using eq .",
    "[ eq : secant ] and defining @xmath39 where @xmath40 , one obtains an approximation for each matrix element @xmath41 : @xmath42 in practice , we explicitly symmetrize @xmath41 in order to avoid asymmetries introduced by anharmonic effects : @xmath43 because the projection @xmath44 is the identity operator on @xmath27 , the curvature @xmath45 on the potential energy surface along a normalized @xmath46 is given by @xmath47 given the normalized eigenvectors @xmath48 and corresponding eigenvalues @xmath49 of the @xmath50 matrix @xmath51 , one can write normalized eigenvectors @xmath52 of @xmath33 with eigenvalues @xmath49 as @xmath53 where @xmath54 is the k - th element of @xmath48 . as can be seen from eq .",
    "[ eq : curv ] , the @xmath49 give the curvatures of the potential energy surface along the directions @xmath55 .",
    "the gradient @xmath57 can be decomposed into a component lying in @xmath27 and a component lying in its orthogonal complement : @xmath58 where @xmath59 , @xmath60 and @xmath61 . in this section",
    "we motivate how the @xmath49 can be used to precondition @xmath62 .",
    "furthermore , we explain how @xmath63 can be scaled appropriately with the help of a feedback that is based on the angle between two consecutive gradients .",
    "let us assume that the hessian @xmath34 at the current point of the potential energy surface is non - singular and let @xmath64 and @xmath65 be its eigenvalues and normalized eigenvectors . in newton s method ( eq .  [ eq : newton ] ) , the gradients are conditioned by the inverse hessian . for the significant subspace component",
    "@xmath62 it follows : @xmath66\\label{eq : firstidea}\\end{aligned}\\ ] ] as outlined in the previous section , we know the curvature @xmath67 along @xmath68 .",
    "therefore , at a first thought , eq .",
    "[ eq : firstidea ] suggests to simply replace @xmath64 by @xmath67 where @xmath69 and @xmath70 .",
    "indeed , if the optimization was restricted to the subspace @xmath27 this choice would be appropriate",
    ". however , with respect to the complete domain of the potential energy surface , one is at risk to underestimate the curvature @xmath64 if the overlap @xmath71 is non - vanishing .",
    "in particular , if @xmath72 is far from being negligible , underestimating the curvature @xmath64 can be particularly problematic because coordinate changes in the direction of @xmath65 might be too large .",
    "this can render convergence difficult to obtain in practice .",
    "we therefore replace @xmath64 in eq .",
    "[ eq : firstidea ] by @xmath73 where @xmath74 is chosen in analogy to the residue of weinstein s criterion@xcite as @xmath75 using equations  [ eq : deltag ] ,  [ eq : curv ] and  [ eq : eigenvec ] , this residue can be approximated by @xmath76 - \\kappa_{j } \\overset{\\sim}{{\\boldsymbol{\\mathbf{v}}}}^{j}\\right|.\\end{aligned}\\ ] ] with this choice for @xmath77 , the preconditioned gradient @xmath78 is finally given by : @xmath79     for @xmath80 and @xmath70 .",
    "@xmath81 is a measure for the quality of the estimation of the eigenvalue @xmath64 of the exact hessian .",
    "panel b ) shows the bin - averaged overlap @xmath72 .",
    "the frequency of severe curvature underestimation drops quickly in the region @xmath82 .",
    "the histogram in panel a ) peaks in the region of good estimation ( @xmath83 ) which coincidences with the region of large overlap @xmath72 , shown in panel b ) . the data for this figure come from 100 minimizations of a @xmath84 system described by the lenosky - silicon@xcite force field .",
    "]    clearly , the residue @xmath74 can only alleviate the problem of curvature underestimation , but it does not rigorously guarantee that every single @xmath64 is estimated appropriately .",
    "however , in practice this choice works very well .",
    "the reason for this can be seen from fig .",
    "[ fig : weinstein ] . in fig .",
    "[ fig : weinstein]a , a histogram of the quality and safety measure @xmath85 is shown . if @xmath86 , the curvature @xmath64 is underestimated , if @xmath87 the curvature @xmath64 is well estimated and finally , if @xmath88 , the curvature is overestimated .",
    "overestimation leads to too small step sizes , and therefore to a more stable algorithm , albeit at the cost of a performance loss .",
    "critical underestimation of the curvature ( @xmath89 ) is rare .",
    "[ fig : weinstein]b shows the averages of the overlap @xmath72 in the corresponding bins .",
    "if @xmath68 has on average a large overlap with @xmath65 , the curvature along @xmath65 is estimated accurately ( histogram in fig .",
    "[ fig : weinstein]a peaks at @xmath90 ) .",
    "what remains to discuss is how the gradient component @xmath63 should be scaled . by construction",
    ", @xmath63 lies in the subspace for which no curvature information is available .",
    "we therefore treat this gradient component by a simple steepest descent approach that adjusts the step size @xmath91 at each iteration . for the minimizer that is outlined in section  [ sec : findmin ]",
    ", the adjustment is based on the angle between the complete gradient @xmath56 and the preconditioned gradient @xmath92 .",
    "if the cosine of this intermediate angle is larger than @xmath93 , @xmath94 is increased by a factor of @xmath95 , otherwise @xmath94 is decreased by a factor of @xmath96 . for the saddle search algorithm",
    "the feedback is slightly different and will be explained in section  [ sec : findsad ] .    in conclusion , the total preconditioned gradient @xmath92 is given by @xmath97 in the next section , we explain how this preconditioned gradient can be further improved for biomolecules .",
    "the preconditioned subspace gradient @xmath78 was obtained under the assumption of a quadratic potential energy surface . however , if the gradients at the current iteration are large , this assumption is probably not satisfied .",
    "displacing along @xmath78 in these cases can reduce the stability of the optimization . hence ,",
    "if the @xmath98 exceeds a certain threshold , it can be useful to set the dimension of @xmath27 to zero for a certain number of iterations .",
    "this means that @xmath99 and therefore @xmath100 . in that case , @xmath94 is also adjusted according to the above described gradient feedback .",
    "however , as this fallback to steepest descent is intended as a last final fallback , it should have the ability to deal with arbitrarily large forces .",
    "therefore , we also check that @xmath101 does not displace any atom by more than a user - defined trust radius .",
    "however , to our experience , this fallback is not necessary in most cases .",
    "indeed , all the benchmarks presented in section  [ sec : bench ] were performed without this fallback .      many large molecules like biomolecules or polymers are floppy systems in which the largest and smallest curvatures can be very different from each other .",
    "steepest descent optimizers are very inefficient for these ill - conditioned systems , because the high curvature directions force to use step sizes that are far too small for an efficient optimization in the directions of small curvatures .",
    "put more formally , the optimization is inefficient for those systems , because the condition number , which is the fraction of largest and smallest curvature , is large.@xcite for biomolecules , the high - curvature directions usually correspond to bond stretchings , that is , movements along inter - atomic displacement vectors of bonded atoms . for the current purpose",
    "we regard two atoms to be bonded if their inter - atomic distance is smaller than or equal to @xmath102 times the sum of their covalent radii . for @xmath103 ,",
    "let @xmath104 be the coordinate vector of the i - th atom .",
    "for a system with @xmath105 bonds we define for each bond a bond vector @xmath106 , @xmath107 @xmath108 where the @xmath109 , @xmath110 are defined as @xmath111 the @xmath112 are sparse vectors with six non - zero elements .",
    "we separate the total gradient @xmath56 into its bond - stretching components @xmath113 and all the remaining components @xmath114 : @xmath115 let @xmath116 be coefficients that allow the bond - stretching components to be expanded in terms of the bond vectors @xmath117 using definition eq .",
    "[ eq : stretchgrad ] , left - multiplying eq .  [ eq : stretchsep ] with a bond vector @xmath118 and requiring the @xmath114 to be orthogonal to all the bond vectors , one obtains the following linear system of equations , which determines the coefficients @xmath119 and , with it , the bond stretching gradient defined in eq .",
    "[ eq : stretchgrad ] : @xmath120    for the optimization of a biomolecule , the bond - stretching components are minimized in a simple steepest descent fashion .",
    "the atoms are displaced by @xmath121 .",
    "the bond - stretching step size @xmath122 is a positive constant , which is adjusted in each iteration of the optimization by simply counting the number of projections @xmath123 that have not changed signs since the last iteration .",
    "if more than two thirds of the signs of the projections have remained unchanged , the bond - stretching step size @xmath122 is increased by 10 percent .",
    "otherwise , @xmath122 is decreased by a factor of @xmath124 .",
    "the non - bond - stretching gradients @xmath114 are preconditioned using the stabilized quasi - newton approach presented in sections  [ sec : sigsub ] to  [ sec : precondsiggrad ] .",
    "it is important to note that in sections  [ sec : sigsub ] to  [ sec : precondsiggrad ] all @xmath56 have to be replaced by @xmath114 when using this biomolecule preconditioner . in particular , this is also true for the gradient feedbacks that are described in sections  [ sec : precondsiggrad ] and  [ sec : findsad ] .",
    "the pseudo code below demonstrates how the above presented techniques can be assembled into an efficient and stabilized quasi - newton minimizer ( sqnm ) .",
    "the pseudo code contains 4 parameters explicitly .",
    "@xmath125 and @xmath126 are initial step sizes that scale @xmath127 and @xmath128 , respectively .",
    "@xmath129 is the maximum length of the history list from which the significant subspace @xmath27 is constructed .",
    "@xmath130 is an energy - threshold that is used to determine whether a minimization step is accepted or not .",
    "it should be adapted to the noise level of the energies and forces .",
    "the history list is discarded if the energy increases , because an increase in energy is an indication for inaccurate curvature information . in this case , the dimension of the significant subspace is considered to be zero . furthermore , line 17 implicitly contains the parameter @xmath131 , which is described in section  [ sec : sigsub ] .",
    "the optimization is considered to be converged if the norm of the gradient is smaller than a certain threshold value . of course ,",
    "other force criteria , like for example using the maximum force component instead of the force norm , are possible .    & _ ; _ s _ ; & + & ; & + & k 1 ; & + & _ k & + & e_k e(_k ) ; & + & & + & & + & & + & & & + & & & + & _ k e(_k ) - e _ ; & + & _ k _ k - _ s e _ ; & + & & + & & + & _ k e(_k ) ; & + & & + & & + & _ k+1 _ k - e^ ; & + & & + & & + & & + & & + & & + & & + & & + & & + & k > m & + & & + & & + & k k + 1 ; & + & & + & & +      in this section we describe a stabilized quasi - newton saddle finding method ( sqns ) that is based on the same principles as the minimizer in the previous section .",
    "sqns belongs to the class of the minimum mode following methods.@xcite    for simplicity , we will denote the hessian eigenvector corresponding to the smallest eigenvalue as minimum mode .",
    "broadly speaking , a minimum mode following method maximizes along the direction of the minimum mode and it minimzes in all other directions .",
    "the optimization is considered to be converged if the curvature along the minimum mode is negative and if the norm of the gradient is smaller than a certain threshold . as for the minimization , other force criteria are possible .",
    "the minimum mode of the hessian can be found by minimizing the curvature function @xmath132 @xmath133 where along with @xmath134 the following definitions were used : @xmath135 and @xmath136 .",
    "the vector @xmath2 is the position at which the hessian @xmath34 is evaluated at . for the minimization of @xmath137",
    ", we use the algorithm described in section  [ sec : findmin ] where the energy as objective function is replaced by @xmath137 . in the pseudocode",
    "below , the here discussed minimization is done at line 6 . under the constraint of normalization",
    ", the gradient @xmath138 is given by @xmath139 blindly using the biomolecule preconditioner of section  [ sec : bioprec ] for the minimization of @xmath137 would mean that the gradient of eq .",
    "[ eq : curvgraddiff ] was projected on the bond vectors of @xmath140 .",
    "obviously , the bond vector as defined in section  [ sec : bioprec ] has no meaning for @xmath140 .",
    "therefore , eq .  [ eq : curvgraddiff ] instead is projected onto the bond vectors of @xmath141 .    at a stationary point ,",
    "systems with free boundary conditions have six vanishing eigenvalues .",
    "the respective eigenvectors correspond to overall translations and rotations.@xcite instead of directly using eq .",
    "[ eq : curvgraddiff ] for the minimization of the curvature of those systems , it is advantageous to remove the translations and rotations from @xmath142 and @xmath138 in eq .",
    "[ eq : curvgraddiff].@xcite    the convergence criterion for the minimization of @xmath137 has a large influence on the total number of energy and force evaluations needed to obtain convergence .",
    "it therefore must be chosen carefully .",
    "the minimum mode is usually not computed at every iteration , but only if one of the following conditions is fulfilled :    1 .",
    "[ item : first ] at the first iteration of the optimization 2 .",
    "[ item : pathlength ] if the integrated length of the optimization path connecting the current point in coordinate space and the point at which the minimum mode has been calculated the last time exceeds a given threshold value @xmath143 3 .",
    "[ item : it ] if the curvature along the minimum mode is positive and the curvature has not been recomputed for at least @xmath144 iterations 4 .",
    "[ item : fnrm ] if the curvature along the minimum mode is positive and the norm of the gradient falls below the convergence criterion 5 .",
    "[ item : tighten ] at convergence ( optional )    in the pseudocode , these conditions are checked in line 5 . among these conditions ,",
    "condition no .",
    "[ item : pathlength ] is , with respect to the performance , the most important one .",
    "the number of energy and gradient evaluations needed for converging to a saddle point can be strongly reduced if a good value for @xmath143 is chosen .",
    "condition  [ item : it ] and  [ item : fnrm ] can be omitted for most cases . however , for some cases they can offer a slight reduction in the number of energy and gradient evaluations . for example for the alanine dipeptide system used in section  [ sec : bench ] , these two conditions offered a performance gain of almost 10% .",
    "although possible , we usually do not tune @xmath144 , but typically use @xmath145 . in our implementation ,",
    "condition  [ item : tighten ] is optional .",
    "it can be used if very accurate directions of the minimum mode at the saddle point are needed . in this case",
    ", this last minimum mode computation can also be done at a tighter convergence criterion .",
    "further energy and gradient computations are saved in our implementation by using the previously computed minimum mode as the starting mode for a new curvature minimization .",
    "as stated above , a saddle point is found by maximizing along the minimum mode and minimizing in all other directions .",
    "this is done by inverting the preconditioned gradient component that is parallel to the minimum mode .",
    "this is shown at line 19 of the pseudocode below .",
    "for the case of biomolecules , the component of the bond - stretching gradient that is parallel to the minimum mode is also inverted ( line 13 ) . as already mentioned in section  [ sec : precondsiggrad ] , the feedback that adjusts the stepsize of @xmath63 is slightly different in case of the saddle finding method . let @xmath146 be the normalized direction of the minimum mode . then , in contrast to minimizations , the stepsize that is used to scale @xmath63 is not based on the angle between the complete @xmath56 and @xmath92 , but only on the angle between @xmath147 and @xmath148 .",
    "these are the components that are responsible for the minimization in directions that are not the minimum mode direction . otherwise , the gradient feedback is absolutely identical to that described in section  [ sec : precondsiggrad ] .",
    "a saddle point can be higher in energy than the configuration at which the optimization is started at .",
    "therefore , in contrast to a minimization , it is not reasonable to discard the history , if the energy increases . as a replacement for this safeguard , we restore to a simple trust radius approach in which any atom must not be moved by more than a predefined trust radius @xmath149 . a displacement exceeding this trust radius",
    "is simply rescaled .",
    "if the curvature is positive and the norm of the gradient is below the convergence criterion , we also rescale displacements that do not come from bond - stretchings .",
    "the displacement is rescaled such that the displacement of the atom that moved furthest , is finally given by @xmath149 .",
    "this avoids arbitrarily small steps close to minima .    on very rare occasions",
    ", we could observe for some cluster systems that over the course of several iterations a few atoms sometimes detach from the main cluster . to avoid this problem",
    ", we identify the main fragment and move all neighboring fragments towards the nearest atom of the main fragment .",
    "below , the pseudocode for sqns is given .",
    "it contains 3 parameters explicitly .",
    "@xmath150 and @xmath151 are initial step sizes that scale @xmath127 and @xmath152 , respectively .",
    "@xmath153 is the maximum length of the history list from which the significant subspace is constructed .",
    "the path - length threshold @xmath154 that determines the recomputation frequency of the minimum mode is implicitly contained in line 5 .",
    "lines 14 and 21 imply the trust radius @xmath149 .",
    "besides all the parameters that are needed for the minimizer of section  [ sec : findmin ] , line 6 additionally implies the finite difference step size @xmath155 that is used to compute the curvature and its gradient .",
    "line 18 implicitly contains the parameter @xmath131 , which is described in section  [ sec : sigsub ]    &   _ ; _s  _ ; & + & l 1 ; & + & _ l & + & & + & & + & & + & & + & & + & & & + & & & + & _s e _ ; & + & _ l e(_l ) - e _ ; & + & _ l _ l - + 2 ( _ ) _ ; & + & & + & & + & _ l e(_l ) ; & + & & + & & + & _ l+1 _ l - e^ + 2(e^ _ ) _ ; & + & & + & & + & l > m & + & & + & & + & l l + 1 ; & + & & +",
    "[ cols=\"^,^,^ , > , > , > , > , > , > , > , > , > , > \" , ]     ( [ yshift=2ex]13.north ) ",
    "( 14 ) ; ( [ yshift=2ex]15.north )  ( 16 ) ; ( [ yshift=2ex]17.north )  ( 18 ) ; ( [ yshift=2ex]19.north )  ( 19 ) ; ( [ yshift=2ex]20.north )  ( 20 ) ;    [ tab : benchsad ]    the sqns method was compared to an improved version of the dimer method@xcite as described in ref .   and",
    "as implemented in the eon code.@xcite in this improved version , the l - bfgs@xcite algorithm is used for the rotations and translation of the dimer .",
    "furthermore , the rotational force and the dimer energy are evaluated by means of a first order forward finite difference of the gradients.@xcite the same force fields as for the minimization benchmarks were used . for the dft calculations , sqns was coupled to the bigdft code .",
    "the eon codes offers an interface to vasp,@xcite which consequently was used .",
    "the same test sets as for the minimizer benchmarks were used .",
    "in particular this means that the starting configurations are not close to a saddle point and therefore these test sets are comparatively difficult for saddle finding methods .",
    "again , parameters were only tuned for a subset of 100 configurations at force field level . with exception to the finite difference step size that is needed to calculate the curvature and its gradient",
    ", we used the same parameters at force filed and dft level .",
    "because of noise , the finite difference step size must be chosen larger at dft level . the same force norm convergence criteria as for the minimization benchmarks were used . in all sqns optimizations the minimum mode was recalculated at convergence ( condition 5 of section  [ sec : findsad ] ) .",
    "the test results are given in table  [ tab : benchsad ] .",
    "in contrast to the minimization benchmarks , we do not give averages for the number wavefunction optimization iterations , because the two saddle finding methods were coupled to two different electronic structure codes .",
    "therefore , the number of wavefunction optimizations is not comparable .    in particular in case of the @xmath84 system ,",
    "both methods converged only seldom to the same saddle points and therefore the statistical significance of the corresponding numbers given in table  [ tab : benchsad ] is limited .",
    "however , averages over large sets could be made in the case of convergence to an arbitrary saddle point .    in the cases we considered ,",
    "the dimer method needed between @xmath156 and @xmath157 times more energy and force evaluations than the new sqns method . in particular for alanine dipeptide , the sqns approach was far superior to the dimer method . due to its inefficiency",
    ", it was impossible to obtain a significant number of saddle points for alanine dipeptide at dft level when using the dimer method .",
    "for this reason , only benchmark results for the sqns method are given for alanine dipeptide at dft level .",
    "optimizations of atomic structures belong to the most important routine tasks in fields like computational physics , chemistry , or biology .",
    "although the energies and forces given by computationally demanding methods like dft are physically accurate , they are contaminated by noise .",
    "this computational noise comes from underlying integration grids and from self - consistency cycles that are stopped at non - vanishing thresholds .",
    "the availability of optimization methods that are not only efficient , but also noise - tolerant is therefore of great importance . in this contribution",
    "we have presented a technique to extract significant curvature information from noisy potential energy surfaces .",
    "we have used this technique to create a stabilized quasi - newton minimization ( sqnm ) and a stabilized quasi - newton saddle finding ( sqns ) algorithm .",
    "sqnm and sqns were demonstrated to be superior to existing efficient and well established methods .    until now",
    ", the sqnm and the sqns optimizers have been used over a period of several months within our group . during this time",
    "they have performed thousands of optimizations without failure at the dft level . because of their robustness with respect to computational noise and due to their efficiency",
    ", they have replaced the default optimizers that have previously been used in minima hopping@xcite and minima hopping guided path search@xcite runs .",
    "implementations of the minimizer and the saddle search method are made available via the bigdft electronic structure package .",
    "the code is distributed under the gnu general public license and can be downloaded free of charge from the bigdft website.@xcite      44ifxundefined [ 1 ] ifx#1 ifnum [ 1 ] # 1firstoftwo secondoftwo ifx [ 1 ] # 1firstoftwo secondoftwo `` `` # 1''''@noop [ 0]secondoftwosanitize@url [ 0 ]  + 12$12  & 12#1212_12%12@startlink[1]@endlink[0]@bib@innerbibempty @noop _ _  ( ,  ) link:\\doibase 10.1063/1.1749604 [ * * , ( ) ] link:\\doibase 10.1016/0009 - 2614(80)80396 - 4 [ * * ,   ( ) ] link:\\doibase 10.1002/jcc.540030413 [ * * , ( ) ] @noop * * ,   ( ) link:\\doibase 10.1093/imamat/6.1.76 [ * * , ( ) ] link:\\doibase 10.1093/comjnl/13.3.317 [ * * ,   ( ) ] link:\\doibase 10.1090/s0025 - 5718 - 1970 - 0258249 - 6 [ * * ,   ( ) ] link:\\doibase 10.1090/s0025 - 5718 - 1970 - 0274029-x [ * * ,   ( ) ] @noop * * ,   ( ) link:\\doibase 10.1007/bf01589116 [ * * , ( ) ] link:\\doibase 10.1103/physrevlett.97.170201 [ * * ( ) , 10.1103/physrevlett.97.170201 ] link:\\doibase 10.1103/physrevb.50.10561 [ * * ,   ( ) ] link:\\doibase 10.1016/s0021 - 9991(03)00308 - 5 [ * * ,   ( ) ] link:\\doibase 10.1063/1.480097 [ * * , ( ) ] link:\\doibase 10.1063/1.2815812 [ * * , ( ) ] link:\\doibase 10.1080/00018735600101155 [ * * ,   ( ) ] @noop _ _  ( ,  ,  ) @noop _ _  ( ,  ) @noop * * ,   ( ) @noop _ _  ( ,  ) link:\\doibase    10.1088/0965 - 0393/8/6/305 [ * * , ( ) ] link:\\doibase 10.1016/s0010 - 4655(02)00466 - 6 [ * * ,   ( ) ] link:\\doibase 10.1103/physrevb.64.161102 [ * * ,   ( ) ] link:\\doibase 10.1063/1.442352 [ * * , ( ) ] link:\\doibase 10.1039/ft9938901305 [ * * , ( ) ] link:\\doibase 10.1063/1.454172 [ * * , ( ) ] in  @noop _ _ ,  ( ,  )  p.   link:\\doibase    10.1063/1.2949547 [ * * ,   ( ) ] link:\\doibase 10.1063/1.4871876 [ * * , ( ) ] @noop _ _  ( ,  ) link:\\doibase 10.1063/1.4828704 [ * * ,   ( ) ] link:\\doibase    10.1088/0965 - 0393/22/5/055002 [ * * , ( ) ] link:\\doibase 10.1063/1.2104507 [ * * , ( ) ] link:\\doibase 10.1063/1.1809574 [ * * , ( ) ] link:\\doibase 10.1103/physrevb.47.558 [ * * , ( ) ] link:\\doibase 10.1103/physrevb.49.14251 [ * * ,   ( ) ] link:\\doibase 10.1016/0927 - 0256(96)00008 - 0 [ * * ,   ( ) ] link:\\doibase 10.1103/physrevb.54.11169 [ * * ,   ( ) ] link:\\doibase 10.1103/physrevb.59.1758 [ * * ,   ( ) ] link:\\doibase 10.1063/1.1724816 [ * * , ( ) ] link:\\doibase 10.1103/physrevlett.95.055501 [ * * ,   ( ) ] link:\\doibase 10.1063/1.4878944 [ * * ,   ( ) ] @noop"
  ],
  "abstract_text": [
    "<S> optimizations of atomic positions belong to the most commonly performed tasks in electronic structure calculations . </S>",
    "<S> many simulations like global minimum searches or characterizations of chemical reactions require performing hundreds or thousands of minimizations or saddle computations . to automatize these tasks , </S>",
    "<S> optimization algorithms must not only be efficient , but also very reliable . </S>",
    "<S> unfortunately computational noise in forces and energies is inherent to electronic structure codes . </S>",
    "<S> this computational noise poses a sever problem to the stability of efficient optimization methods like the limited - memory broyden  </S>",
    "<S> fletcher  </S>",
    "<S> goldfarb  </S>",
    "<S> shanno algorithm . </S>",
    "<S> we here present a technique that allows obtaining significant curvature information of noisy potential energy surfaces . </S>",
    "<S> we use this technique to construct both , a stabilized quasi - newton minimization method and a stabilized quasi - newton saddle finding approach . </S>",
    "<S> we demonstrate with the help of benchmarks that both the minimizer and the saddle finding approach are superior to comparable existing methods . </S>"
  ]
}