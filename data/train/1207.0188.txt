{
  "article_text": [
    "according to fisher [ ( @xcite ) , page 311 ] , `` the object of statistical methods is the reduction of data . ''",
    "the reduction of data is imperative in the case of discrete - valued networks that may have hundreds of thousands of nodes and billions of edge variables .",
    "the collection of such large networks is becoming more and more common , thanks to electronic devices such as cameras and computers .",
    "of special interest is the identification of influential subsets of nodes and high - density regions of the network with an eye to break down the large network into smaller , more manageable components .",
    "these smaller , more manageable components may be studied by more advanced statistical models , such as advanced exponential family models [ e.g. , @xcite ] .",
    "an example is given by signed networks , such as trust networks , which arise in world wide web applications .",
    "users of internet - based exchange networks are invited to classify other users as either @xmath0 ( untrustworthy ) or @xmath1 ( trustworthy ) .",
    "trust networks can be used to protect users and enhance collaboration among users [ @xcite ] .",
    "a second example is the spread of infectious disease through populations by way of contacts among individuals [ @xcite ] . in such applications , it may be of interest to identify potential super - spreaders  that is , individuals who are in contact with many other individuals and who could therefore spread the disease to many others  and dense regions of the network through which disease could spread rapidly .",
    "the current article advances the model - based clustering of large networks in at least four ways .",
    "first , we introduce a simple and flexible statistical framework for parameterizing models based on statistical exponential families [ e.g. , @xcite ] that advances existing model - based clustering techniques .",
    "model - based clustering of networks was pioneered by @xcite .",
    "the simple , unconstrained parameterizations employed by @xcite and others [ e.g. , @xcite ] make sense when networks are small , undirected and binary , and when there are no covariates .",
    "in general , though , such parameterizations may be unappealing from both a scientific point of view and a statistical point of view , as they may result in nonparsimonious models with hundreds or thousands of parameters .",
    "an important advantage of the statistical framework we introduce here is that it gives researchers a choice : they can choose interesting features of the data , specify a model capturing those features , and cluster nodes based on the specified model .",
    "the resulting models are therefore both parsimonious and scientifically interesting .",
    "second , we introduce approximate maximum likelihood estimates of parameters based on novel variational generalized em ( gem ) algorithms , which take advantage of minorization - maximization ( mm ) algorithms [ @xcite ] and have computational advantages . for unconstrained models ,",
    "tests suggest that the variational gem algorithms we propose can converge quicker and better avoid local maxima than alternative algorithms ; see sections [ seccomparison ] and [ secapp ] . in the presence of parameter constraints ,",
    "we facilitate computations by exploiting the properties of exponential families [ e.g. , @xcite ] . in addition",
    ", we sketch how the variational gem algorithm can be extended to obtain approximate bayesian estimates .",
    "third , we introduce bootstrap standard errors to quantify the uncertainty about the approximate maximum likelihood estimates of the parameters , whereas other work has ignored the uncertainty about the approximate maximum likelihood estimates . to facilitate these bootstrap procedures ,",
    "we introduce monte carlo simulation algorithms that generate sparse networks in much less time than conventional monte carlo simulation algorithms .",
    "in fact , without the more efficient monte carlo simulation algorithms , obtaining bootstrap standard errors would be infeasible .    finally ,",
    "while model - based clustering has been limited to networks with fewer than 13,000 nodes and 85 million edge variables [ see the largest data set handled to date , @xcite ] , we demonstrate that we can handle much larger , nonbinary networks by considering an internet - based data set with more than 131,000 nodes and 17 billion edge variables , where `` edge variables '' comprise all observations , including node pairs between which no edge exists .",
    "many internet - based companies and websites , such as http://amazon.com , http://netflix.com and http://epinions.com , allow users to review products and services . because most users of the world wide web do not know each other and thus can not be sure whether to trust each other , readers of reviews may be interested in an indication of the trustworthiness of the reviewers themselves .",
    "a convenient and inexpensive approach is based on evaluations of reviewers by readers .",
    "the data set we analyze in section [ secapp ] comes from the website http://epinions.com , which collects such data by allowing any user @xmath2 to evaluate any other user @xmath3 as either untrustworthy , coded as @xmath4 , or trustworthy , coded as @xmath5 , where @xmath6 means that user @xmath2 did not evaluate user @xmath3 [ @xcite ] .",
    "the resulting network consists of @xmath7,827 users and @xmath8,378,226,102 observations . since each user can only review a relatively small number of other users , the network is sparse : the vast majority of the observations @xmath9 are zero , with only 840,798 negative and positive evaluations .",
    "our modeling goal , broadly speaking , is both to cluster the users based on the patterns of trusts and distrusts in this network and to understand the features of the various clusters by examining model parameters .",
    "the rest of the article is structured as follows : a scalable model - based clustering framework based on finite mixture models is introduced in section [ secmodel ] .",
    "approximate maximum likelihood and bayesian estimation are discussed in sections [ secmle ] and [ secbay ] , respectively , and an algorithm for monte carlo simulation of large networks is described in section [ secsim ] . section [ seccomparison ] compares the variational gem algorithm to the variational em algorithm of @xcite .",
    "section [ secapp ] applies our methods to the trust network discussed above .",
    "we consider @xmath10 nodes , indexed by integers @xmath11 , and edges @xmath9 between pairs of nodes @xmath2 and @xmath3 , where @xmath9 can take values in a finite set of @xmath12 elements . by convention , @xmath13 for all  @xmath2 , where @xmath14 signifies `` no relationship . ''",
    "we call the set of all edges @xmath9 a discrete - valued network , which we denote by @xmath15 , and we let @xmath16 denote the set of possible values of  @xmath15",
    ". special cases of interest are ( a ) undirected binary networks @xmath15 , where @xmath17 is subject to the linear constraint @xmath18 for all @xmath19 ; ( b ) directed binary networks @xmath15 , where @xmath17 for all @xmath20 ; and ( c ) directed signed networks @xmath15 , where @xmath21 for all @xmath20 .    a general approach to modeling discrete - valued networks is based on exponential families of distributions [ @xcite ] : @xmath22,\\qquad \\mathbf{y}\\in{\\mathscr{y}},\\ ] ] where @xmath23 is the vector of canonical parameters and @xmath24 is the vector of canonical statistics depending on a matrix @xmath25 of covariates , measured on the nodes or the pairs of nodes , and the network @xmath15 , and @xmath26 is given by @xmath27,\\qquad { \\bolds{\\theta}}\\in \\mathbb{r}^p,\\ ] ] and ensures that @xmath28 sums to @xmath29 .",
    "a number of exponential family models have been proposed [ e.g. , @xcite ] . in general , though , exponential family models are not scalable : the computing time to evaluate the likelihood function is @xmath30 , where @xmath31 in the case of undirected edges and @xmath32 in the case of directed edges , which necessitates time - consuming estimation algorithms [ e.g. , @xcite ] .",
    "we therefore restrict attention to scalable exponential family models , which are characterized by dyadic independence : @xmath33 where @xmath34 corresponds to @xmath35 in the case of undirected edges and @xmath36 in the case of directed edges .",
    "the subscripted @xmath37 and superscripted @xmath10 mean that the product in ( [ dyadindependence ] ) should be taken over all pairs @xmath38 with @xmath39 ; the same is true for sums as in ( [ lb ] ) .",
    "dyadic independence has at least three advantages : ( a ) it facilitates estimation , because the computing time to evaluate the likelihood function scales linearly with  @xmath40 ; ( b ) it facilitates simulation , because dyads are independent ; and ( c ) by design it bypasses the so - called model degeneracy problem : if @xmath40 is large , some exponential family models without dyadic independence tend to be ill - defined and impractical for modeling networks [ @xcite ] .",
    "a disadvantage is that most exponential families with dyadic independence are either simplistic [ e.g. , models with identically distributed edges , @xcite ] or nonparsimonious [ e.g. , the @xmath41 model with @xmath42 parameters , @xcite ] .",
    "we therefore assume that the probability mass function has a @xmath43-component mixture form as follows : @xmath44\\\\[-8pt ] & = & \\sum_{\\mathbf{z}\\in{\\mathscr{z } } } \\prod_{i < j}^n p_{{\\bolds{\\theta}}}(d_{ij } = d_{ij } { \\mid}{\\mathbf{x } } , { \\mathbf{z}}= \\mathbf{z } ) p_{{\\bolds{\\gamma}}}({\\mathbf{z}}= \\mathbf{z } ) , \\nonumber\\end{aligned}\\ ] ] where @xmath45 denotes the membership indicators @xmath46 with distributions @xmath47 and @xmath48 denotes the support of @xmath45 . in some applications , it may be desired to model the membership indicators @xmath49 as functions of @xmath25 by using multinomial logit or probit models with @xmath49 as the outcome variables and @xmath25 as predictors [ e.g. , @xcite ] .",
    "we do not elaborate on such models here , but the variational gem algorithms discussed in sections [ secmle ] and [ secbay ] could be adapted to such models .",
    "mixture models represent a reasonable compromise between model parsimony and complexity . in particular , the assumption of conditional dyadic independence does _ not _ imply marginal dyadic independence , which means that the mixture model of ( [ mixturemodel ] ) captures some degree of dependence among the dyads .",
    "we give two specific examples of mixture models below .",
    "the @xmath41 model of @xcite for directed , binary - valued networks may be modified using a mixture model .",
    "the original @xmath41 models the sequence of in - degrees ( number of incoming edges of nodes ) and out - degrees ( number of outgoing edges of nodes ) as well as reciprociated edges , postulating that the dyads are independent and that the dyadic probabilities are of the form @xmath50,\\ ] ] where @xmath51 and @xmath52 is a normalizing constant . following @xcite , the parameters @xmath53 may be interpreted as activity or productivity parameters , representing the tendencies of nodes @xmath2 to `` send '' edges to other nodes ; the parameters @xmath54 may be interpreted as attractiveness or popularity parameters , representing the tendencies of nodes @xmath3 to `` receive '' edges from other nodes ; and the parameter @xmath55 may be interpreted as a mutuality or reciprocity parameter , representing the tendency of nodes @xmath2 and @xmath3 to reciprocate edges .",
    "a drawback of this model is that it requires @xmath56 parameters .",
    "here , we show how to extend it to a mixture model that is applicable to both directed and undirected networks as well as discrete - valued networks , that is much more parsimonious , and that allows identification of influential nodes .",
    "observe that the dyadic probabilities of ( [ dyadicprob ] ) are of the form @xmath57,\\ ] ] where @xmath58 is the reciprocity parameter and @xmath59 and @xmath60 are the sending and receiving propensities of nodes @xmath2 and @xmath3 , respectively .",
    "the corresponding statistics are the reciprocity indicator @xmath61 and the sending and receiving indicators @xmath62 and @xmath63 of nodes @xmath2 and @xmath3 , respectively .",
    "a mixture model modification of the @xmath41 model postulates that , conditional on @xmath45 , the dyadic probabilities are independent and of the form @xmath64\\\\[-8pt ] & & \\qquad \\propto\\exp\\bigl[{\\bolds{\\theta}}_1^\\top { \\mathbf{g}}_1(d_{ij } ) + { \\bolds{\\theta}}_{2k}^\\top { \\mathbf{g}}_{2k}(d_{ij } ) + { \\bolds{\\theta}}_{2l}^\\top { \\mathbf{g}}_{2l}(d_{ij})\\bigr],\\nonumber\\end{aligned}\\ ] ] where the parameter vectors @xmath65 and @xmath66 depend on the components @xmath67 and @xmath68 to which the nodes @xmath2 and @xmath3 belong , respectively .",
    "the mixture model version of the @xmath41 model is therefore much more parsimonious provided @xmath69 and was proposed by @xcite in the case of undirected , binary - valued networks . here",
    ", the probabilities of ( [ p1extension1 ] ) and ( [ p1extension2 ] ) are applicable to both undirected and directed networks as well as discrete - valued networks , because the functions @xmath70 and @xmath71 may be customized to fit the situation and may even depend on covariates @xmath25 , though we have suppressed this possibility in the notation .",
    "finally , the mixture model version of the @xmath41 model admits model - based clustering of nodes based on indegrees or outdegrees or both . a small number of nodes with high indegree or outdegree or both is considered to be influential : if the corresponding nodes were to be removed , the network structure would be impacted .",
    "the mixture model of @xcite assumes that , conditional on @xmath45 , the dyads are independent and the conditional dyadic probabilities are of the form @xmath72 in other words , conditional on @xmath45 , the dyad probabilities are constant across dyads and do not depend on covariates .",
    "it is straightforward to add covariates by writing the conditional dyad probabilities in canonical form : @xmath73,\\ ] ] where the canonical statistic vectors @xmath74 and @xmath75 may depend on the covariates @xmath25 . if the canonical parameter vectors @xmath76 are constrained by the linear constraints @xmath77 , where @xmath78 and @xmath79 are parameter vectors of the same dimension as @xmath76 , then the mixture model version of the @xmath41 model arises .",
    "in other words , the mixture model version of the @xmath41 model can be viewed as a constrained version of the @xcite model .",
    "while the constrained version can be used to cluster nodes based on degree , the unconstrained version can be used to identify , for instance , high - density regions of the network , corresponding to subsets of nodes with large numbers of within - subset edges .",
    "these regions may then be studied individually in more detail by using more advanced statistical models such as exponential family models without dyadic independence as proposed by , for example , @xcite , @xcite , @xcite , @xcite , @xcite or @xcite .",
    "other mixture models for networks have been proposed by @xcite , @xcite and @xcite . however , these models scale less well to large networks , so we confine attention here to examples 1 and 2 .",
    "a standard approach to maximum likelihood estimation of finite mixture models is based on the classical em algorithm , taking the complete data to be @xmath80 , where @xmath45 is unobserved [ @xcite ] .",
    "however , the e - step of an em algorithm requires the computation of the conditional expectation of the complete data log - likelihood function under the distribution of @xmath81 , which is intractable here even in the simplest cases [ @xcite ] .    as an alternative",
    ", we consider so - called variational em algorithms , which can be considered as generalizations of em algorithms .",
    "the basic idea of variational em algorithms is to construct a tractable lower bound on the intractable log - likelihood function and maximize the lower bound , yielding approximate maximum likelihood estimates . @xcite",
    "have shown that approximate maximum likelihood estimators along these lines are  at least in the absence of parameter constraints  consistent estimators .",
    "we assume that all modeling of @xmath82 can be conditional on covariates @xmath25 and define @xmath83however , for ease of presentation , we drop the notational dependence of @xmath84 on @xmath85 and make the homogeneity assumption @xmath86 which is satisfied by the models in examples 1 and 2 .",
    "exponential parameterizations of @xmath87 , as in ( [ dyadicprob ] ) and ( [ nowsni2 ] ) , may or may not be convenient .",
    "an attractive property of the variational em algorithm proposed here is that it can handle all possible parameterizations of @xmath88 . in some cases ( e.g. , example 1 ) , exponential parameterizations are more advantageous than others , while in other cases ( e.g. , example 2 ) , the reverse holds .",
    "let @xmath89 be an auxiliary distribution with support @xmath48 . using jensen s inequality ,",
    "the log - likelihood function can be bounded below as follows : @xmath90 a(\\mathbf{z } ) \\\\ & = & e_a\\bigl[\\log p_{{\\bolds{\\gamma } } , { \\bolds{\\theta}}}({\\mathbf{y}}= \\mathbf{y } , { \\mathbf{z}}= \\mathbf{z } ) \\bigr ] - e_a\\bigl[\\log a({\\mathbf{z}})\\bigr ] .",
    "\\nonumber\\end{aligned}\\ ] ] some choices of @xmath91 give rise to better lower bounds than others . to see which choice gives rise to the best lower bound ,",
    "observe that the difference between the log - likelihood function and the lower bound is equal to the kullback ",
    "leibler divergence from @xmath92 to @xmath93 : @xmath94 a(\\mathbf{z } ) \\nonumber \\\\ & & \\qquad = \\sum_{\\mathbf{z}\\in{\\mathscr{z } } } \\bigl[\\log p_{{\\bolds{\\gamma}},{\\bolds{\\theta}}}({\\mathbf{y}}= \\mathbf{y } ) \\bigr ] a(\\mathbf{z } ) - \\sum_{\\mathbf{z}\\in{\\mathscr{z } } } \\biggl[\\log \\frac{p_{{\\bolds{\\gamma } } , { \\bolds{\\theta}}}({\\mathbf{y}}= \\mathbf{y } , { \\mathbf{z}}= \\mathbf { z})}{a(\\mathbf{z } ) } \\biggr ] a(\\mathbf{z } ) \\\\ & & \\qquad = \\sum_{\\mathbf{z}\\in{\\mathscr{z } } } \\biggl[\\log\\frac{a(\\mathbf { z})}{p_{{\\bolds{\\gamma } } , { \\bolds{\\theta}}}({\\mathbf{z}}= \\mathbf{z}\\mid{\\mathbf{y}}= \\mathbf{y } ) } \\biggr ] a(\\mathbf{z } ) .",
    "\\nonumber\\end{aligned}\\ ] ] if the choice of @xmath91 were unconstrained in the sense that we could choose from the set of all distributions with support @xmath48 , then the best lower bound is obtained by the choice @xmath95 , which reduces the kullback ",
    "leibler divergence to @xmath14 and makes the lower bound tight . if the optimal choice is intractable , as is the case here , then it is convenient to constrain the choice to a subset of tractable choices and substitute a choice which , within the subset of tractable choices , is as close as possible to the optimal choice in terms of kullback  leibler divergence . a  natural subset of tractable choices is given by introducing the auxiliary parameters @xmath96 and setting @xmath97 where the marginal auxiliary distributions @xmath98 are multinomial@xmath99 . in this case , the lower bound may be written @xmath100 - e_{{\\bolds{\\alpha } } } \\bigl[\\log p_{{\\bolds{\\alpha}}}({\\mathbf{z}})\\bigr ] \\nonumber\\\\ & = & \\sum_{i",
    "< j}^n \\sum _ { k=1}^k \\sum_{l=1}^k \\alpha_{ik } \\alpha_{jl } \\log\\pi_{d_{ij};kl}({\\bolds{\\theta } } ) \\\\ & & { } + \\sum_{i=1}^n \\sum _ { k=1}^k \\alpha_{ik } ( \\log \\gamma_k - \\log\\alpha_{ik } ) .",
    "\\nonumber\\end{aligned}\\ ] ] because equation ( [ aux ] ) assumes independence , the kullback  leibler divergence between @xmath101 and @xmath93 , and thus the tightness of the lower bound , is determined by the dependence of the random variables @xmath46 conditional on @xmath82 .",
    "if the random variables @xmath46 are independent conditional on  @xmath82 , then , for each @xmath2 , there exists @xmath102 such that @xmath103 , which reduces the kullback  leibler divergence to @xmath14 and makes the lower bound tight . in general , the random variables @xmath46 are not independent conditional on @xmath82 and the kullback  leibler divergence ( [ kb ] ) is thus positive .",
    "approximate maximum likelihood estimates of @xmath104 and @xmath23 can be obtained by maximizing the lower bound in ( [ lb ] ) using variational em algorithms of the following form , where @xmath105 is the iteration number :    letting @xmath106 and @xmath107 denote the current values of @xmath104 and @xmath23 , maximize @xmath108 with respect to @xmath109 .",
    "let @xmath110 denote the optimal value of @xmath109 and compute @xmath111 $ ] .",
    "maximize @xmath111 $ ] with respect to @xmath104 and @xmath23 , which is equivalent to maximizing @xmath112 with respect to @xmath104 and @xmath23 .",
    "the method ensures that the lower bound is nondecreasing in the iteration number : @xmath113 where inequalities ( [ nondecreasing1 ] ) and ( [ nondecreasing2 ] ) follow from the e - step and m - step , respectively .",
    "it is instructive to compare the variational em algorithm to the classical em algorithm as applied to finite mixture models .",
    "the e - step of the variational em algorithm minimizes the kullback  leibler divergence between @xmath91 and @xmath114 .",
    "if the choice of @xmath91 were unconstrained , then the optimal choice would be @xmath115 .",
    "therefore , in the unconstrained case , the e - step of the variational em algorithm reduces to the e - step of the classical em algorithm , so the classical em algorithm can be considered to be the optimal variational em algorithm .      to implement the e - step",
    ", we exploit the fact that the lower bound is nondecreasing as long as the e - step and m - step increase the lower bound . in other words , we do not need to maximize the lower bound in the e - step and m - step .",
    "indeed , increasing rather than maximizing the lower bound in the e - step and m - step may have computational advantages when @xmath10 is large . in the literature on em algorithms , the advantages of incremental e - steps and incremental m - steps",
    "are discussed by @xcite and @xcite , respectively .",
    "we refer to the variational em algorithm with either an incremental e - step or an incremental m - step or both as a variational generalized em , or variational gem , algorithm .    direct maximization of @xmath108 is unattractive : equation ( [ lb ] ) shows that the lower bound depends on the products @xmath116 and , therefore , fixed - point updates of @xmath117 along the lines of [ @xcite ] depend on all other @xmath118 .",
    "we demonstrate in section [ seccomparison ] that the variational em algorithm with the fixed - point implementation of the e - step can be inferior to the variational gem algorithm when @xmath43 is large .",
    "to separate the parameters of the maximization problem , we increase @xmath108 via an mm algorithm [ @xcite ] .",
    "mm algorithms can be viewed as generalizations of em algorithms [ @xcite ] and are based on iteratively constructing and then optimizing surrogate ( minorizing ) functions to facilitate the maximization problem in certain situations .",
    "we consider here the surrogate function @xmath119\\\\[-8pt ] & & { } + \\sum_{i = 1}^n \\sum _ { k = 1}^k \\alpha_{ik } \\biggl(\\log \\gamma_k^{(t ) } - \\log\\alpha_{ik}^{(t ) } - \\frac{\\alpha_{ik}}{\\alpha _ { ik}^{(t ) } } + 1 \\biggr ) , \\nonumber\\end{aligned}\\ ] ] which we show in appendix [ minorizer ] to have the following two properties : @xmath120 in the language of mm algorithms , conditions ( [ p1 ] ) and ( [ p2 ] ) establish that @xmath121 is a _ minorizer _ of @xmath108 at @xmath122 .",
    "the theory of mm algorithms implies that maximizing the minorizer with respect to @xmath109 forces @xmath108 uphill [ @xcite ] .",
    "this maximization , involving @xmath10 separate quadratic programming problems of k variables @xmath102 under the constraints @xmath123 for all @xmath67 and @xmath124 , may be accomplished quickly using the method described by @xcite .",
    "when @xmath10 is large , it is much easier to update @xmath109 by maximizing the @xmath125 function , which is the sum of functions of the individual @xmath126 , than by maximizing the @xmath127 function , in which the @xmath109 parameters are not separated in this way",
    ". we therefore arrive at the following replacement for the e - step :    for @xmath128 , increase @xmath121 as a function of @xmath102 subject to @xmath123 for all @xmath67 and @xmath129 .",
    "let @xmath110 denote the new value of @xmath109 .      to maximize @xmath112 in the m - step , examination of ( [ lb ] )",
    "shows that maximization with respect to @xmath104 and @xmath23 may be accomplished separately .",
    "in fact , for @xmath104 , there is a simple , closed - form solution : @xmath130 concerning @xmath23 , if there are no constraints on @xmath131 other than @xmath132 , it is preferable to maximize with respect to @xmath133 rather than @xmath23 , because there are closed - form expressions for @xmath134 but not for @xmath135 .",
    "maximization with respect to @xmath136 is accomplished by setting @xmath137    if the homogeneity assumption ( [ homogeneity ] ) does not hold , then closed - form expressions for @xmath136 may not be available . in some cases , as in the presence of categorical covariates , closed form expressions for @xmath136 are available , but the dimension of @xmath136 , and thus computing time , increases with the number of categories .    if equations ( [ ergm ] ) and ( [ dyadindependence ] ) hold ,",
    "then the exponential parametrization @xmath131 may be inverted to obtain an approximate maximum likelihood estimate of @xmath23 after the approximate mle of @xmath136 is found using the variational gem algorithm .",
    "one method for accomplishing this inversion exploits the convex duality of exponential families [ @xcite ] and is explained in appendix [ convexduality ] .    if , in addition to the constraint @xmath138 , additional constraints on @xmath136 are present , the maximization with respect to @xmath136 may either decrease or increase computing time .",
    "linear constraints on @xmath136 can be enforced by lagrange multipliers and reduce the dimension of @xmath136 and thus computing time .",
    "nonlinear constraints on  @xmath136 , as in example 1 , may not admit closed form updates of @xmath136 and thus may require iterative methods .",
    "if so , and if the nonlinear constraints stem from exponential family parameterizations of @xmath131 with natural parameter vector @xmath23 as in example  1 , then it is convenient to translate the constrained maximization problem into an unconstrained problem by maximizing @xmath112 with respect to @xmath23 and exploiting the fact that @xmath139 is a concave function of @xmath23 owing to the exponential family membership of @xmath87 [ @xcite , page  150 ] . we show in appendix [ gradienthessian ] how the exponential family parameterization can be used to derive the gradient and hessian of the lower bound of @xmath112 with respect to @xmath23 , which we exploit in section [ secapp ] using a newton  raphson algorithm .",
    "although we maximize the lower bound @xmath140 of the log - likelihood function to obtain approximate maximum likelihood estimates , standard errors of the approximate maximum likelihood estimates @xmath141 and @xmath142 based on the curvature of the lower bound @xmath143 may be too small .",
    "the reason is that even when the lower bound is close to the log - likelihood function , the lower bound may be more curved than the log - likelihood function [ @xcite ] ; indeed , the higher curvature helps ensure that @xmath144 is a lower bound of the log - likelihood function @xmath145 in the first place . as an alternative ,",
    "we approximate the standard errors of the approximate maximum likelihood estimates of @xmath104 and @xmath23 by a parametric bootstrap method [ @xcite ] that can be described as follows :    given the approximate maximum likelihood estimates of @xmath104 and @xmath23 , sample @xmath146 data sets .    for each data set , compute the approximate maximum likelihood estimates of @xmath104 and @xmath23 .",
    "in addition to fast maximum likelihood algorithms , the parametric bootstrap method requires fast simulation algorithms .",
    "we propose such an algorithm in section [ secsim ] .      as usual with em - like algorithms",
    ", it is a good idea to use multiple different starting values with the variational em due to the existence of distinct local maxima .",
    "we find it easiest to use random starts in which we assign the values of @xmath147 and then commence with an m - step .",
    "this results in values @xmath148 and @xmath149 , then the algorithm continues with the first e - step , and so on .",
    "the initial @xmath150 are chosen independently uniformly randomly on @xmath151 , then each @xmath152 is multiplied by a normalizing constant chosen so that the elements of @xmath152 sum to one for every @xmath2 .",
    "the numerical experiments of section [ secapp ] use 100 random restarts each .",
    "ideally , more restarts would be used , yet the size of the data sets with which we work makes every run somewhat expensive .",
    "we chose the number 100 because we were able to parallelize on a fairly large scale , essentially running 100 separate copies of the algorithm .",
    "larger numbers of runs , such as 1000 , would have forced longer run times since we would have had to run some of the trials in series rather than in parallel .    as a convergence criterion",
    ", we stop the algorithm as soon as @xmath153 approximate bayesian estimation -------------------------------    the key to bayesian model estimation and model selection is the marginal likelihood , defined as @xmath154 where @xmath155 is the prior distribution of @xmath104 and @xmath23 . to ensure that the marginal likelihood is well - defined , we assume that the prior distribution is proper , which is common practice in mixture modeling [ @xcite , chapter 4 ]",
    ". a lower bound on the log marginal likelihood can be derived by introducing an auxiliary distribution with support @xmath156 , where @xmath157 is the parameter space of @xmath104 and @xmath158 is the parameter space of @xmath23 .",
    "a natural choice of auxiliary distributions is given by @xmath159 p_{{\\bolds{\\alpha}}_{{\\bolds{\\gamma}}}}({\\bolds{\\gamma } } ) \\biggl[\\prod _ { i = 1}^l p_{{\\bolds{\\alpha}}_{{\\bolds{\\theta}}}}(\\theta_i ) \\biggr],\\ ] ] where @xmath109 denotes the set of auxiliary parameters @xmath160 , @xmath161 and  @xmath162",
    ".    a lower bound on the log marginal likelihood can be derived by jensen s inequality : @xmath163\\\\[-8pt ] & \\geq & e_{{\\bolds{\\alpha}}}\\bigl[\\log p_{{\\bolds{\\gamma } } , { \\bolds{\\theta}}}({\\mathbf{y}}= \\mathbf{y } , { \\mathbf{z}}= \\mathbf { z } ) p ( { \\bolds{\\gamma } } , { \\bolds{\\theta}})\\bigr ] - e_{{\\bolds{\\alpha}}}\\bigl[\\log a_{{\\bolds{\\alpha}}}({\\mathbf{z } } , { \\bolds{\\gamma } } , { \\bolds{\\theta}})\\bigr ] , \\nonumber\\end{aligned}\\ ] ] where the expectations are taken with respect to the auxiliary distribution @xmath164 .",
    "we denote the right - hand side of ( [ lbbayesian ] ) by @xmath165 . by an argument along the lines of ( [ kb ] )",
    ", one can show that the difference between the log marginal likelihood and @xmath165 is equal to the kullback ",
    "leibler divergence from the auxiliary distribution @xmath164 to the posterior distribution @xmath166 : @xmath167 a_{{\\bolds{\\alpha}}}(\\mathbf{z } , { \\bolds{\\gamma } } , { \\bolds{\\theta } } ) \\,d{\\bolds{\\gamma}}\\,d { \\bolds{\\theta}}\\nonumber\\\\[-8pt]\\\\[-8pt ] & & \\qquad = \\int_{\\gamma } \\int_{\\theta } \\sum _ { \\mathbf{z}\\in{\\mathscr{z } } } \\biggl[\\log\\frac{a_{{\\bolds{\\alpha}}}(\\mathbf{z } , { \\bolds{\\gamma } } , { \\bolds{\\theta}})}{p({\\mathbf{z}}= \\mathbf{z } , { \\bolds{\\gamma } } , { \\bolds{\\theta}}\\mid{\\mathbf{y}}= \\mathbf{y } ) } \\biggr ] a_{{\\bolds{\\alpha } } } ( \\mathbf{z } , { \\bolds{\\gamma } } , { \\bolds{\\theta } } ) \\,d{\\bolds{\\gamma}}\\,d{\\bolds{\\theta}}. \\nonumber\\end{aligned}\\ ] ]    the kullback ",
    "leibler divergence between the auxiliary distribution and the posterior distribution can be minimized by a variational gem algorithm as follows , where @xmath105 is the iteration number :    letting @xmath168 and @xmath169 denote the current values of @xmath161 and @xmath162 , increase @xmath170 with respect to @xmath171 .",
    "let @xmath172 denote the new value of @xmath171 .",
    "choose new values @xmath173 and @xmath174 that increase @xmath175 with respect to @xmath161 and @xmath162 .",
    "by construction , iteration @xmath105 of a variational gem algorithm increases the lower bound @xmath176 : @xmath177 a variational gem algorithm approximates the marginal likelihood as well as the posterior distribution .",
    "therefore , it tackles bayesian model estimation and model selection at the same time .",
    "variational gem algorithms for approximate bayesian inference are only slightly more complicated to implement than the variational gem algorithms for approximate maximum likelihood estimation presented in section  [ secmle ] .",
    "to understand the difference , we examine the analogue of ( [ lb ] ) : @xmath178 + e_{{\\bolds{\\alpha}}}\\bigl[\\log p_{{\\bolds{\\gamma}}}({\\mathbf{z}}= \\mathbf{z})\\bigr ] \\\\ & & \\qquad\\quad{}+ e_{{\\bolds{\\alpha}}}\\bigl[\\log p({\\bolds{\\gamma } } , { \\bolds{\\theta}})\\bigr ] - e_{{\\bolds{\\alpha } } } \\bigl[\\log a({\\mathbf{z}}= \\mathbf{z } , { \\bolds{\\gamma } } , { \\bolds{\\theta}})\\bigr ] .",
    "\\nonumber\\end{aligned}\\ ] ] if the prior distributions of @xmath104 and @xmath23 are given by independent dirichlet and gaussian distributions and the auxiliary distributions of @xmath46 , @xmath104 and @xmath23 are given by independent multinomial , dirichlet and gaussian distributions , respectively , then the expectations on the right - hand side of ( [ lbbayesian2 ] ) are tractable , with the possible exception of the expectations @xmath179 $ ] . under the exponential parameterization @xmath180 \\biggr\\},\\ ] ]",
    "the expectations can be written as @xmath181 = e_{{\\bolds{\\alpha}}}[{\\bolds{\\theta}}]^\\top{\\mathbf{g}}(d ) - e_{{\\bolds{\\alpha } } } \\biggl \\{\\log\\sum_{d ' \\in{\\mathscr{d } } } \\exp\\bigl [ { \\bolds{\\theta}}^\\top { \\mathbf{g}}\\bigl(d'\\bigr ) \\bigr ] \\biggr\\}\\ ] ] and are intractable .",
    "we are not aware of parameterizations under which the expectations are tractable .",
    "we therefore use exponential parameterizations and deal with the intractable nature of the resulting expectations by invoking jensen s inequality : @xmath182 \\geq e_{{\\bolds{\\alpha}}}[{\\bolds{\\theta}}]^\\top{\\mathbf{g}}(d ) - \\log\\sum _ { d ' \\in{\\mathscr{d } } } e_{{\\bolds{\\alpha } } } \\bigl\\{\\exp\\bigl[{\\bolds{\\theta}}^\\top { \\mathbf{g}}\\bigl(d'\\bigr ) \\bigr ] \\bigr\\}.\\ ] ] the right - hand side of ( [ lblbbayesian ] ) involves expectations of independent log - normal random variables , which are tractable . we thus obtain a looser , yet tractable , lower bound by replacing @xmath183 $ ] in ( [ lbbayesian2 ] ) by the right - hand side of inequality  ( [ lblbbayesian ] ) .    to save space , we do not address the specific numerical techniques that may be used to implement the variational gem algorithm here .",
    "in short , the generalized e - step is based on an mm algorithm along the lines of section [ secmm ] . in the generalized m - step ,",
    "numerical gradient - based methods may be used .",
    "a  detailed treatment of this bayesian estimation method and its implementation , using a more complicated prior distribution , may be found in @xcite ; code related to this article is available at http://sites.stat.psu.edu/\\textasciitilde dhunter / code/[http://sites.stat.psu.edu/\\textasciitilde dhunter / code/ ] .",
    "monte carlo simulation of large , discrete - valued networks serves at least three purposes :    a.   to generate simulated data to be used in simulation studies ; b.   to approximate standard errors of the approximate maximum likelihood estimates by parametric bootstrap ; c.   to assess model goodness of fit by simulation .",
    "a crude monte carlo approach is based on sampling @xmath45 by cycling through all @xmath10 nodes and sampling @xmath184 by cycling through all @xmath185 dyads . however , the running time of such an algorithm is @xmath186 , which is too slow to be useful in practice , because each of the goals listed above tends to require numerous simulated data sets .",
    "we propose monte carlo simulation algorithms that exploit the fact that discrete - valued networks tend to be sparse in the sense that one element of @xmath187 is much more common than all other elements of @xmath187 .",
    "an example is given by directed , binary - valued networks , where @xmath188 is the sample space of dyads and @xmath189 tends to dominate all other elements of @xmath187 .",
    "assume there exists an element @xmath190 of @xmath187 , called the baseline , that dominates the other elements of @xmath187 in the sense that @xmath191 for all @xmath67 and @xmath68 .",
    "the monte carlo simulation algorithm exploiting the sparsity of large , discrete - valued networks can be described as follows :    1 .",
    "sample @xmath45 by sampling @xmath192 and assigning nodes @xmath193 to component @xmath29 , nodes @xmath194 to component @xmath195 , etc .",
    "sample @xmath196 as follows : for each @xmath197 , a.   sample the number of dyads @xmath198 with nonbaseline values , @xmath199 , where @xmath200 is the number of pairs of nodes belonging to components @xmath67 and @xmath68 ; b.   sample @xmath198 out of @xmath200 pairs of nodes @xmath37 without replacement ; c.   for each of the @xmath198 sampled pairs of nodes @xmath19 , sample the nonbaseline value @xmath201 according to the probabilities @xmath202 , @xmath203 , @xmath204 .    in general ,",
    "if the degree of any node ( i.e. , the number of nonbaseline values for all dyad variables incident on that node ) has a bounded expectation , then the expected number of nonbaseline values @xmath205 in the network scales with @xmath10 and the expected running time of the monte carlo simulation algorithm scales with @xmath206 . if @xmath43 is small and @xmath10 is large , then the monte carlo approach that exploits the sparsity of large , discrete - valued networks is superior to the crude monte carlo approach .",
    "we compare the variational em algorithm based on the fixed - point ( fp ) implementation of the e - step along the lines of @xcite to the variational gem algorithm based on the mm implementation of the e - step by applying them to two data sets .",
    "the first data set comes from the study on political blogs by @xcite . we convert the binary network of political blogs with two labels , liberal ( @xmath2071 ) and conservative ( @xmath2081 ) , into a signed network by assigning labels of receivers to the corresponding directed edges .",
    "the resulting network has 1490 nodes and 2,218,610 edge variables .",
    "the second data set is the epinions data set described in section [ secintroduction ] with more than 131,000 nodes and more than 17 billion edge variables .",
    "we compare the two algorithms using the unconstrained network mixture model of ( [ nowsni ] ) with @xmath209 and @xmath210 components .",
    "for the first data set , we allow up to 1 hour for @xmath209 components and up to 6 hours for @xmath211 components . for the second data set , we allow up to 12 hours for @xmath212 components and up to 24 hours for @xmath210 components . for each data set , for each number of components and for each algorithm , we carried out 100 runs using random starting values as described in section [ secstartingstopping ] .",
    "figure [ figfpvsmmconvergencetraces ] shows trace plots of the lower bound @xmath213 of the log - likelihood function , where red lines refer to the lower bound of the variational em algorithm with fp implementation and blue lines refer to the lower bound of the variational gem algorithm with mm implementation .",
    "the variational em algorithm seems to outperform the variational gem algorithm in terms of computing time when @xmath43 and @xmath10 are small .",
    "however , when @xmath43 or @xmath10 are large , the variational gem algorithm appears far superior to the variational em algorithm in terms of the lower bounds .",
    "the contrast is most striking when @xmath43 is large , though the variational gem seems to outperform the variational em algorithm even when @xmath43 is small and @xmath10 is large .",
    "we believe that the superior performance of the variational gem algorithm stems from the fact that it separates the parameters of the maximization problem and reduces the dependence of the updates of the variational parameters @xmath117 , as discussed in section [ secmm ] , while the variational em algorithm tends to be trapped in local maxima .    @c@c@     of the log - likelihood function for 100 runs each of the variational em algorithm with fp implementation ( red ) and variational gem algorithm with mm implementation ( blue ) , applied to the unconstrained network mixture model of ( [ nowsni ] ) for two different data sets.,title=\"fig : \" ] &   of the log - likelihood function for 100 runs each of the variational em algorithm with fp implementation ( red ) and variational gem algorithm with mm implementation ( blue ) , applied to the unconstrained network mixture model of ( [ nowsni ] ) for two different data sets.,title=\"fig : \" ] + ( a ) political blogs data set with @xmath209 & ( b ) political blogs data set with @xmath210 +   of the log - likelihood function for 100 runs each of the variational em algorithm with fp implementation ( red ) and variational gem algorithm with mm implementation ( blue ) , applied to the unconstrained network mixture model of ( [ nowsni ] ) for two different data sets.,title=\"fig : \" ] &   of the log - likelihood function for 100 runs each of the variational em algorithm with fp implementation ( red ) and variational gem algorithm with mm implementation ( blue ) , applied to the unconstrained network mixture model of ( [ nowsni ] ) for two different data sets.,title=\"fig : \" ] + ( c ) epinions data set with @xmath209 & ( d ) epinions data set with @xmath210    thus , if @xmath43 and @xmath10 are small and a computing cluster is available , it seems preferable to carry out a large number of runs using the variational em algorithm in parallel , using random starting values as described in section [ secstartingstopping ] .",
    "however , if either @xmath43 or @xmath10 is large , it is preferable to use the variational gem algorithm .",
    "since the variational gem algorithm is not prone to be trapped in local maxima , a small number of long runs may be all that is needed .",
    "here , we address the problem of clustering the @xmath214,000 users of the data set introduced in section [ secintroduction ] according to their levels of trustworthiness , as indicated by the network of @xmath2071 and @xmath0 ratings given by fellow users . to this end , we first introduce the individual `` excess trust '' statistics @xmath215 since @xmath216 is the number of positive ratings received by user @xmath2 in excess of the number of negative ratings , it is a natural measure of a user s individual trustworthiness .",
    "our contention is that consideration of the overall pattern of network connections results in a more revealing clustering pattern than a mere consideration of the @xmath216 statistics , and we support this claim by considering three different clustering methods : a parsimonious network model using the @xmath216 statistics , the fully unconstrained network model of ( [ nowsni ] ) , and a mixture model that considers only the @xmath216 statistics while ignoring the other network structure .    for each method , we assume that the number of categories , @xmath43 , is five . partly , this choice is motivated by the fact that formal model selection methods such as the icl criterion suggested by @xcite , which we discuss in section 9 , suggest dozens if not hundreds of categories , which complicate summary and interpretation .",
    "since the reduction of data is the primary task of statistics [ @xcite ] , we want to keep the number of categories small and follow the standard practice of internet - based companies and websites , such as http://amazon.com and http://netflix.com , which use five categories to classify the trustworthiness of reviewers , sellers and service providers .    our parsimonious model , which enjoys benefits over the other two alternatives as we shall see , is based on @xmath217 , \\nonumber\\end{aligned}\\ ] ] where @xmath218 and @xmath219 are indicators of negative and positive edges , respectively .",
    "the parameters in model ( [ excessdyadmodel1 ] ) are not identifiable , because @xmath220 and @xmath221 .",
    "we therefore constrain the positive edge parameter @xmath222 to be @xmath14 .",
    "model ( [ excessdyadmodel1 ] ) assumes in the interest of model parsimony that the propensities to form negative and positive edges and to reciprocate negative and positive edges do not vary across clusters ; however , the flexibility afforded by this modeling framework enables us to define cluster - specific parameters for any of these propensities if we wish .",
    "the conditional probability mass function of the whole network is given by @xmath223,\\nonumber\\end{aligned}\\ ] ] where @xmath224 is the total excess trust for all nodes in the @xmath67th category .",
    "the @xmath225 parameters are therefore measures of the trustworthiness of each of the categories .",
    "furthermore , these parameters are estimated in the presence of  that is , after correcting for  the reciprocity effects as measured by the parameters @xmath226 and @xmath227 , which summarize the overall tendencies of users to reciprocate negative and positive ratings , respectively .",
    "thus , @xmath226 and @xmath227 may be considered to measure overall tendencies toward _ lex talionis _ and _ quid pro quo _ behaviors .",
    "one alternative model we consider is the unconstrained network model obtained from ( [ nowsni ] ) . with five components , this model comprises four mixing parameters @xmath228 in addition to the @xmath229 parameters , of which there are 105",
    ": there are nine types of dyads @xmath230 whenever @xmath231 , contributing @xmath232 parameters , and six types of dyads @xmath230 whenever @xmath233 , contributing an additional @xmath234 parameters . despite the increased flexibility afforded by model ( [ nowsni ] )",
    ", we view the loss of interpretability due to the large number of parameters as a detriment .",
    "furthermore , more parameters opens up the possibility of overfitting and , as we discuss below , appears to make the lower bound of the log - likelihood function highly multi - modal .",
    "our other alternative model is a univariate mixture model applied to the @xmath216 statistics directly , which assumes that the individual excesses @xmath216 are independent random variables sampled from a distribution with density @xmath235 where @xmath236 , @xmath237 and @xmath238 are component - specific mixing proportions , means and standard deviations , respectively , and @xmath239 is the standard normal density .",
    "traditional univariate approaches like this are less suitable than network - based clustering approaches not only because by design they neither consider nor inform us about the topology of the network , which may be relevant , but also because the individual excesses are not independent : these @xmath216 are functions of edges , and edges may be dependent owing to reciprocity ( and other forms of dependence not modeled here ) , which decades of research [ e.g. , @xcite ] have shown to be important in shaping social networks .",
    "unlike the univariate mixture model of ( [ gaussianmixture ] ) , the mixture model we employ for networks allows for such dependence .",
    "we use a variational gem algorithm to estimate the network model ( [ excessdyadmodel1full ] ) , where the m - step is executed by a newton  raphson algorithm using the gradient and hessian derived in appendix [ gradienthessian ] with a maximum of 100 iterations .",
    "it stops earlier if the largest absolute value in the gradient vector is less than @xmath240 .",
    "by contrast , the unconstrained network model following from ( [ nowsni ] ) employs a variational gem algorithm using the exact m - step update ( [ piupdate ] ) .",
    "the variational gem algorithm stops when either the relative change in the objective function is less than @xmath240 or 6000 iterations are performed .",
    "most runs require the full 6000 iterations . to estimate the normal mixture model ( [ gaussianmixture ] )",
    ", we use the ` r ` package ` mixtools ` [ @xcite ] .    [ cols=\"^,^ \" , ]     to diagnose convergence of the algorithm for fitting the model ( [ excessdyadmodel1full ] ) , we present the trace plot of the lower bound of the log - likelihood function @xmath241 in figure [ figconvergence](a ) and the trace plot of the cluster - specific excess parameters @xmath242 in figure [ figconvergence](b ) . both figures are based on @xmath243 runs , where the starting values are obtained by the procedure described in section [ secstartingstopping ] .",
    "the results suggest that all @xmath243 runs seem to converge to roughly the same solution .",
    "this fact is somewhat remarkable , since many variational algorithms appear very sensitive to their starting values , converging to multiple distinct local optima [ e.g. , @xcite ] .",
    "for instance , the 100 runs for the unconstrained network model ( [ nowsni ] ) produced essentially a unique set of estimates for each set of random starting values .",
    "similarly , the normal mixture model algorithm produces many different local maxima , even after we try to correct for label - switching by choosing random starting values fairly tightly clustered by their mean values .",
    "@cc@    , grouped by highest - probability component of @xmath2 , for parsimonious network mixture model ( [ excessdyadmodel1full ] ) with 12 parameters , normal mixture model ( [ gaussianmixture ] ) with 14 parameters , and unconstrained network mixture model ( [ nowsni ] ) with 109 parameters.,title=\"fig : \" ] & , grouped by highest - probability component of @xmath2 , for parsimonious network mixture model ( [ excessdyadmodel1full ] ) with 12 parameters , normal mixture model ( [ gaussianmixture ] ) with 14 parameters , and unconstrained network mixture model ( [ nowsni ] ) with 109 parameters.,title=\"fig : \" ] + ( a ) network mixture model ( [ excessdyadmodel1full ] ) & ( b ) normal mixture model ( [ gaussianmixture ] ) +   +    figure [ figepinionsexcesstrust ] shows the observed excesses @xmath244 grouped by clusters for the best solutions , as measured by likelihood or approximate likelihood , found for each of the three clustering methods .",
    "it appears that the clustering based on the parsimonious network model does a better job of separating the @xmath216 statistics into distinct subgroups  though this is not the sole criterion used  than the clusterings for the other two models , which are similar to each other .",
    "in addition , if we use a normal mixture model in which the variances are restricted to be constant across components , the results are even worse , with one large cluster and multiple clusters with few nodes .    in figure [ figepinionsratings ] ,",
    "we `` ground truth '' the clustering solutions using external information : the average ratings of 659,290 articles , grouped according to the highest - probability category of the article s author . while in figure [ figepinionsexcesstrust ] the size of each cluster is the number of users in that cluster , in figure [ figepinionsratings ] the size of each cluster is the number of articles written by users in that cluster .",
    "the widths of the boxes in figures [ figepinionsexcesstrust ] and [ figepinionsratings ] are proportional to the square roots of the cluster sizes .    as an objective criterion to compare the three models , we fit one - way anova models where responses are article ratings and fixed effects are the group indicators of the articles authors .",
    "the adjusted r@xmath245 values are @xmath246 , @xmath247 and @xmath248 for the network mixture model , the normal mixture model and the unconstrained network mixture model , respectively . in other words , the latent structure detected by the 12-component network mixture model of ( [ excessdyadmodel1full ] ) explains the variation in article ratings better than the 14-parameter univariate mixture model or the 109-parameter unconstrained network model .",
    "@cc@    ) with 12 parameters ,  normal mixture model ( [ gaussianmixture ] ) with 14 parameters , and unconstrained network mixture model ( [ nowsni ] ) with 109 parameters .",
    "the ordering of the five categories , which is the same as in figure [ figepinionsexcesstrust ] , indicates that the unconstrained network mixture model does not even preserve the correct ordering of the median average ratings.,title=\"fig : \" ] & ) with 12 parameters ,  normal mixture model ( [ gaussianmixture ] ) with 14 parameters , and unconstrained network mixture model ( [ nowsni ] ) with 109 parameters .",
    "the ordering of the five categories , which is the same as in figure [ figepinionsexcesstrust ] , indicates that the unconstrained network mixture model does not even preserve the correct ordering of the median average ratings.,title=\"fig : \" ] + ( a ) network mixture model ( [ excessdyadmodel1full ] ) & ( b ) normal mixture model ( [ gaussianmixture ] ) +   +    @lcd3.3c@ & & & * confidence * + * parameter * & * statistic * & & * interval * + negative edges ( @xmath249 ) & @xmath250 & -24.020 & @xmath251 + positive edges ( @xmath222 ) & @xmath252 & 0 &  + negative reciprocity ( @xmath226 ) & @xmath253 & 8.660 & @xmath254 + positive reciprocity ( @xmath227 ) & @xmath255 & 9.899 & @xmath256 + cluster 1 trustworthiness ( @xmath257 ) & @xmath258 & -6.256 & @xmath259 + cluster 2 trustworthiness ( @xmath260 ) & @xmath261 & -7.658 & @xmath262 + cluster 3 trustworthiness ( @xmath263 ) & @xmath264 & -9.343 & @xmath265 + cluster 4 trustworthiness ( @xmath266 ) & @xmath267 & -11.914 & @xmath268 + cluster 5 trustworthiness ( @xmath269 ) & @xmath270 & -15.212 & @xmath271 +    table [ tableepinions ] reports estimates of the @xmath272 parameters from model ( [ excessdyadmodel1full ] ) along with 95% confidence intervals reported in that table obtained by simulating 500 networks using the method of section [ secsim ] and the parameter estimates obtained via our algorithm . for each network",
    ", we run our algorithm for 1000 iterations starting at the m - step , where the @xmath109 parameters are initialized to reflect the `` true '' component to which each node is assigned by the simulation algorithm by setting @xmath273 for @xmath67 not equal to the true component and @xmath274 otherwise .",
    "this is done to eliminate the so - called label - switching problem , which is rooted in the invariance of the likelihood function to switching the labels of the @xmath275 components and which can affect bootstrap samples in the same way it can affect markov chain monte carlo samples from the posterior of finite mixture models [ @xcite ] .",
    "the sample 2.5% and 97.5% quantiles form the confidence intervals shown .",
    "in addition , we give density estimates of the five trustworthiness bootstrap samples in figure [ figbootstrap1 ] .        table [ tableepinions ]",
    "shows that some clusters of users are much more trustworthy than others .",
    "in addition , there is statistically significant evidence that users rate others in accordance with both _",
    "lex talionis _ and _ quid pro quo _ , since both @xmath276 and @xmath227 are positive .",
    "these findings suggest that the ratings of pairs of users @xmath2 and @xmath3 are , perhaps unsurprisingly , dependent and not free of self - interest .",
    "finally , a few remarks concerning the parametric bootstrap are appropriate .",
    "while we are encouraged by the fact that bootstrapping is even feasible for problems of this size , there are aspects of our investigation that will need to be addressed with further research .",
    "first , the bootstrapping is so time - consuming that we were forced to rely on computing clusters with multiple computing nodes to generate a bootstrap sample in reasonable time .",
    "future work could focus on more efficient bootstrapping .",
    "some work on efficient bootstrapping was done by @xcite , but it is restricted to simple models and not applicable here .",
    "second , when the variational gem algorithm is initialized at random locations , it may converge to local maxima whose @xmath277 values are inferior to the solutions attained when the algorithm is initialized at the `` true '' values used to simulate the networks .",
    "while it is not surprising that variational gem algorithms converge to local maxima , it is surprising that the issue shows up in some of the simulated data sets but not in the observed data set .",
    "one possible explanation is that the structure of the observed data set is clear cut , but that the components of the estimated model are not sufficiently separated .",
    "therefore , the estimated model may place nonnegligible probability mass on networks where two or more subsets of nodes are hard to distinguish and the variational gem algorithm may be attracted to local maxima .",
    "third , some groups of confidence intervals , such as the first four trustworthiness parameter intervals , have more or less the same width .",
    "we do not have a fully satisfying explanation for this result ; it may be a coincidence or it may have some deeper cause related to the difficulty of the computational problem .    in summary , we find that the clustering framework we introduce here provides useful results for a very large network .",
    "most importantly , the sensible application of statistical modeling ideas , which reduces the unconstrained 109-parameter model to a constrained 12-parameter model , produces vastly superior results in terms of interpretability , numerical stability and predictive performance .",
    "the model - based clustering framework outlined here represents several advances . an attention to standard statistical modeling ideas relevant in the network context improves model parsimony and interpretability relative to fully unconstrained clustering models , while also suggesting a viable method for assessing precision of estimates obtained .",
    "algorithmically , our advances allow us to apply a variational em idea , recently applied to network clustering models in numerous publications [ e.g. , @xcite ] , to networks far larger than any that have been considered to date .",
    "we have applied our methods to networks with over a hundred thousand nodes and signed edges , indicating how they extend to categorical - valued edges generally or models that incorporate other covariate information . in practice , these methods could have myriad uses , from identifying high - density regions of large networks to selecting among competing models for a single network to testing specific network effects of scientific interest when clustering is present .",
    "to achieve these advances , we have focused exclusively on models exhibiting dyadic independence conditional on the cluster memberships of nodes .",
    "it is important to remember that these models are _ not _ dyadic independence models overall , since the clustering itself introduces dependence .",
    "however , to more fully capture network effects such as transitivity , more complicated models may be needed , such as the latent space models of @xcite , @xcite or @xcite .",
    "a major drawback of latent space models is that they tend to be less scalable than the models considered here .",
    "an example is given by the variational bayesian algorithm developed by @xcite to estimate the latent space model of @xcite .",
    "the running time of the algorithm is @xmath186 and it has therefore not been applied to networks with more than @xmath278 nodes and @xmath279,700 edge variables .",
    "an alternative to the variational bayesian algorithm of @xcite based on case - control sampling was proposed by @xcite . however , while the computing time of this alternative algorithm is @xmath42 , the suggested preprocessing step , which requires determining the shortest path length between pairs of nodes , is @xmath186 . as a result , the largest network @xcite analyze",
    "is an undirected network with @xmath280 nodes and @xmath281,686,970 edge variables .",
    "in contrast , the running time of the variational gem algorithm proposed here is @xmath42 in the constrained and @xmath282 in the unconstrained version of the @xcite model , where @xmath283 is the number of edge variables whose value is not equal to the baseline value .",
    "it is worth noting that @xmath283 is @xmath42 in the case of sparse graphs and , therefore , the running time of the variational gem algorithm is @xmath42 in the case of sparse graphs . indeed , even in the presence of the covariates , the running time of the variational gem algorithm is @xmath284 with categorical covariates , where @xmath285 is the number of covariates and @xmath286 is the number of categories of the @xmath2th covariate .",
    "we have demonstrated that the variational gem algorithm can be applied to networks with more than @xmath7,000 nodes and @xmath287 billion edge variables .",
    "while the running time of @xmath42 shows that the variational gem algorithm scales well with @xmath10 , in practice , the `` g '' in `` gem '' is an important contributor to the speed of the variational gem algorithm : merely increasing the lower bound using an mm algorithm rather than actually maximizing it using a fixed - point algorithm along the lines of @xcite appears to save much computing time for large networks , though an exhaustive comparison of these two methods is a topic for further investigation.=-1    an additional increase in speed might be gained by exploiting acceleration methods such as quasi - newton methods [ @xcite , section 10.7 ] , which have shown promise in the case of mm algorithms [ @xcite ] and which might accelerate the mm algorithm in the e - step of the variational gem algorithm",
    ". however , application of these methods is complicated in the current modeling framework because of the exceptionally large number of auxiliary parameters introduced by the variational augmentation .",
    "we have neglected here the problem of selecting the number of clusters . @xcite",
    "propose making this selection based on the so - called icl criterion , but it is not known how the icl criterion behaves when the intractable incomplete - data log - likelihood function in the icl criterion is replaced by a variational - method lower bound . in our experience ,",
    "the magnitude of the changes in the maximum lower bound value achieved with multiple random starting parameters is at least as large as the magnitude of the penalization imposed on the log - likelihood by the icl criterion .",
    "thus , we have been unsuccessful in obtaining reliable icl - based results for very large networks .",
    "more investigation of this question , and of the selection of the number of clusters in general , seems warranted .    by demonstrating that scientifically interesting clustering models can be applied to very large networks by extending the variational - method ideas developed for network data sets recently in the statistical literature",
    ", we hope to encourage further investigation of the possibilities of these and related clustering methods .",
    "the source code , written in ` c++ ` , and data files used in sections [ seccomparison ] and [ secapp ] are publicly available at http://sites.stat.psu.edu/\\textasciitilde dhunter / code[http://sites.stat.psu.edu/\\textasciitilde dhunter / code ] .",
    "the lower bound @xmath143 of the log - likelihood function can be written as @xmath288\\\\[-8pt ] & & { } + \\sum _ { i=1}^n \\sum_{k=1}^k \\alpha_{ik } ( \\log\\gamma_k - \\log\\alpha_{ik } ) .\\nonumber\\end{aligned}\\ ] ] since @xmath289 for all @xmath23 , the arithmetic - geometric mean inequality implies that @xmath290 [ @xcite ] , with equality if @xmath291 and @xmath292 .",
    "in addition , the concavity of the logarithm function gives @xmath293 with equality if @xmath291 .",
    "therefore , function @xmath294 as defined in ( [ qdefn ] ) possesses properties ( [ p1 ] ) and ( [ p2 ] ) .",
    "we show how closed - form expressions of @xmath23 in terms of @xmath136 can be obtained by exploiting the convex duality of exponential families .",
    "let @xmath295 be the legendre ",
    "fenchel transform of @xmath26 , where @xmath296 $ ] is the mean - value parameter vector and the subscripts @xmath67 and @xmath68 have been dropped . by barndorff - nielsen [ ( @xcite ) , page 140 ] and wainwright and jordan [ ( @xcite ) , pages 67 and 68 ] , the legendre ",
    "fenchel transform of @xmath26 is self - inverse and , thus , @xmath26 can be written as @xmath297 where @xmath298 and @xmath299 .",
    "therefore , closed - form expressions of @xmath23 in terms of @xmath136 may be found by maximizing @xmath300 with respect to @xmath136 .",
    "we are interested in the gradient and hessian with respect to the parameter vector @xmath23 of the lower bound in ( [ lbappendix ] ) .",
    "the two examples of models considered in section [ secmodel ] assume that the conditional dyad probabilities @xmath301 take the form @xmath302,\\ ] ] where @xmath303 is a linear function of parameter vector @xmath23 and @xmath304 is a matrix of suitable order depending on components @xmath67 and @xmath68 .",
    "it is convenient to absorb the matrix @xmath304 into the statistic vector @xmath305 and write @xmath306,\\ ] ] where @xmath307 .",
    "thus , we may write @xmath308 + \\mbox{const},\\ ] ] where `` const '' denotes terms which do not depend on @xmath23 and @xmath309.\\ ] ]    since the lower bound @xmath143 is a weighted sum of exponential family log - probabilities , it is straightforward to obtain the gradient and hessian of @xmath143 with respect to @xmath23 , which are given by @xmath310 \\bigr\\}\\ ] ] and @xmath311,\\ ] ] respectively .    in other words ,",
    "the gradient and hessian of @xmath143 with respect to @xmath23 are weighted sums of expectations  the means , variances and covariances of statistics .",
    "since the sample space of dyads @xmath187 is finite and , more often than not , small , these expectations may be computed by complete enumeration of all possible values of @xmath312 and their probabilities .",
    "we are grateful to paolo massa and kasper souren of http://www.trustlet.org[trustlet.org ] for sharing the http://www.epinions.com[epinion.com ] data"
  ],
  "abstract_text": [
    "<S> we describe a network clustering framework , based on finite mixture models , that can be applied to discrete - valued networks with hundreds of thousands of nodes and billions of edge variables . </S>",
    "<S> relative to other recent model - based clustering work for networks , we introduce a more flexible modeling framework , improve the variational - approximation estimation algorithm , discuss and implement standard error estimation via a parametric bootstrap approach , and apply these methods to much larger data sets than those seen elsewhere in the literature . </S>",
    "<S> the more flexible framework is achieved through introducing novel parameterizations of the model , giving varying degrees of parsimony , using exponential family models whose structure may be exploited in various theoretical and algorithmic ways . </S>",
    "<S> the algorithms are based on variational generalized em algorithms , where the e - steps are augmented by a minorization - maximization ( mm ) idea . </S>",
    "<S> the bootstrapped standard error estimates are based on an efficient monte carlo network simulation idea . </S>",
    "<S> last , we demonstrate the usefulness of the model - based clustering framework by applying it to a discrete - valued network with more than 131,000 nodes and 17 billion edge variables .    , </S>"
  ]
}