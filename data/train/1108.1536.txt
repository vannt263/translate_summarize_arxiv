{
  "article_text": [
    "let @xmath0 be the sequence generated by the recursion @xmath1 where @xmath2 is a random process with known distribution , @xmath3 and @xmath4 are known constants and @xmath5 is the unknown threshold parameter to be estimated from the sample @xmath6 .",
    "the equation is a basic instance of the threshold autoregression ( tar ) models , which play a considerable role in the theory and practice of time series .",
    "this type of models have been studied by statisticians for already more than three decades , producing interesting theory and finding many important applications , some of which can be traced in the early and more recent surveys @xcite and @xcite , @xcite , @xcite , @xcite , @xcite ( see also , e.g. , @xcite , @xcite for the analysis of the related moving average threshold ( tma ) models ) .    when it comes to the large sample asymptotic analysis of the estimators , the standard conditions imposed on the models such as are    1 .",
    "[ ci ] strong ergodicity of the observed process @xmath7 2 .   [ cii ] independence of the driving random variables @xmath8 s    departure from these assumptions poses challenging problems . for the model",
    ", the condition fails if the absolute value of either @xmath3 or @xmath4 is greater or equal to 1 .",
    "if the process @xmath7 is null recurrent ( e.g. @xmath9 and @xmath10 ) , characterization of the exact large sample asymptotic distribution of the likelihood based estimators of the threshold parameter @xmath5 remains an open problem ( see remark below ) .",
    "if @xmath11 is assumed to be known , the asymptotic distribution of the coefficients estimators in the non - ergodic case has been studied in @xcite , @xcite , @xcite .",
    "tar models beyond the independence assumption of the driving noise sequence has not yet been addressed .",
    "as we shall shortly see , in the dependent case the problem falls into the framework of statistical inference of hidden markov models ( hmm ) , where the driving noise plays the role of the hidden signal ( see ch .",
    "10 - 12 in @xcite and the references therein ) .",
    "however , most of the hmm literature deals with locally asymptotically normal ( lan ) experiments and , to the best of our knowledge , non - lan models with partial observations have not yet been studied systematically .    in this paper",
    ", we consider the model in which @xmath12 is a sequence with geometrically decaying correlation .",
    "more precisely , let @xmath13 be generated by the recursion @xmath14 subject to @xmath15 , where @xmath16 and the unknown parameter @xmath5 takes values in an open bounded subset @xmath17 .",
    "we shall consider the problem with discontinuous drift function @xmath18 , and thus assume @xmath19 and @xmath20 .",
    "the driving noises @xmath21 and @xmath22 are assumed independent : the _ white noise _",
    "component @xmath23 is a sequence of i.i.d .",
    "@xmath24 random variables and the _ colored noise _",
    "@xmath25 is the gaussian ar(1 ) process , generated by the linear recursion @xmath26 where @xmath27 are i.i.d .",
    "@xmath24 random variables and @xmath28 is a known constant @xmath29 , controlling the bandwidth of the noise .",
    "all the aforementioned random variables are defined on a measurable space @xmath30 , with the family of probabilities @xmath31 , indexed by the unknown parameter . for integers @xmath32 , we define @xmath33 and set @xmath34 and @xmath35 .",
    "all the processes in our problem are adapted to the filtration @xmath36 and we shall assume that @xmath37 .",
    "finally we define the observed filtration @xmath38 .    the recursions and form a conditionally gaussian system , which means that the conditional law of @xmath39 given @xmath40 is gaussian , and by theorem 13.5 in @xcite @xmath41 where @xmath42 is the innovation sequence of i.i.d .",
    "@xmath24 random variables . the process @xmath43 and the deterministic sequence @xmath44 satisfy the generalized kalman filter equations @xmath45 subject to @xmath46 and @xmath47 .    to avoid inessential technicalities",
    ", we shall assume that @xmath48 and @xmath49 are independent and @xmath50 , where @xmath51 is the unique positive root of the equation @xmath52 in this case , the conditional mean @xmath53 satisfies with constant coefficients : @xmath54 it can be seen that @xmath55 converges to @xmath51 exponentially fast and all the results claimed below hold for @xmath48 with an arbitrary gaussian distribution .",
    "let @xmath56 and @xmath57 , @xmath58 and note that @xmath59 for all @xmath60 . by the definition of the conditional expectation , @xmath61 is orthogonal to @xmath62 , and thus to @xmath63 .",
    "moreover , since the process @xmath64 is gaussian , @xmath61 is independent of @xmath63 and thus of @xmath65 as well .",
    "further , since @xmath66 and @xmath67 is independent of @xmath68 , independence of @xmath69 and @xmath65 follows and the representation implies that the likelihood of the data @xmath40 is given by @xmath70 the likelihood function is discontinuous in @xmath5 and hence we are faced with an irregular statistical experiment . in such problems ,",
    "the maximum likelihood estimator is often asymptotically inferior to the bayes estimator @xmath71 , while the latter is typically asymptotically efficient for arbitrary continuous positive prior densities in the following minimax sense ( see theorem 9.1 , @xcite ) : @xmath72 where @xmath73 s are @xmath74-measurable statistics .",
    "the bayes estimator for the problem at hand has relatively low computational complexity , since the likelihood function is piecewise constant in @xmath5 and has at most @xmath75 jumps at @xmath76 .",
    "more precisely , for a prior density @xmath77 , the bayes estimator with respect to the quadratic risk is given by @xmath78 where @xmath79 is the @xmath80-th order statistic of @xmath81 .",
    "if the prior @xmath77 is chosen so that numerical integration in the right hand side is avoided , the computation of @xmath71 can be carried out in polynomial time of order @xmath82 .",
    "the following property , whose proof is deferred to appendix [ app - sec ] ( see lemma [ lem - erg ] ) , plays a crucial role in the forthcoming analysis    [ prop1 ] assume @xmath83 is geometrically ergodic under @xmath84 , @xmath85 , with the unique invariant probability density @xmath86 .",
    "[ rem:1.2 ] obviously , recurrence of @xmath7 is necessary for consistent estimation of the threshold parameter .",
    "the condition guarantees positive recurrence of the process @xmath7 , which is an essential ingredient in derivation of the large sample asymptotic of theorem [ thm ] below . in the null recurrent case",
    ", i.e. when one of the coefficients have unit absolute value , consistent estimation of @xmath5 seems to be possible and some preliminary calculations show that the corresponding rate may depend on the distribution tail of the driving noise .",
    "the exact characterization of the large sample asymptotic in this setting remains an open problem , even for independent innovations .",
    "the main result of this paper is the following characterization of the asymptotic distribution of the sequence of bayes estimators :    [ thm ] let @xmath87 be the sequence of the bayes estimators with respect to the quadratic loss function and a prior with continuous positive density @xmath77 .",
    "then for any continuous function @xmath88 with at most polynomial growth @xmath89 uniformly on compacts from @xmath90 , where @xmath91 and @xmath92 , @xmath93 is the following two sided compound poisson process : @xmath94 here @xmath95 , @xmath96 are i.i.d poisson processes with the intensity @xmath97 @xmath98 is the unique invariant probability density of the markov process @xmath99 under @xmath100 , @xmath101 are i.i.d .",
    "@xmath24 random variables , independent of @xmath95 and @xmath96 and @xmath102      * 1 . *",
    "if @xmath103 is replaced in with @xmath104 , i.e. if the colored noise component enters without the one - step delay , the model @xmath105 is obtained . in this case , the kalman filter equations take a slightly different form and the asymptotic analysis can be carried out exactly as in our setting .    on the other hand , if the white noise component @xmath67 is omitted , the observed process satisfies the equation @xmath106 being a completely observed system , this model fits the setting of @xcite or @xcite after a straightforward modification .",
    "our method is directly applicable to the models , where the colored noise is generated by a linear multivariate recursion : @xmath107 where @xmath108 are i.i.d .",
    "standard gaussian vectors in @xmath109 and @xmath110 and @xmath111 are @xmath112 and @xmath113 matrices respectively . in this case",
    ", the observed process satisfies the scalar recursion @xmath114 where @xmath115 is a column vector of size @xmath116 . in this setting ,",
    "the kalman filter equations read ( cf . and ) @xmath117 subject to @xmath118 and @xmath119 .",
    "if @xmath110 is a stability matrix , i.e. the absolute values of its eigenvalues are strictly less than 1 , and the pair @xmath120 is controllable : @xmath121 then the solution of the riccati equation converges to the matrix @xmath122 , which is the unique strictly positive definite root of the corresponding algebraic riccati equation ( see e.g. @xcite ) @xmath123 the statement of theorem [ thm ] holds with the rate @xmath124 where @xmath98 is the invariant density of the process @xmath125 , and @xmath126 the latter formula emerges in the proof of the lemma [ lem-3 ] below , with the obvious adjustments to the multivariate setting .",
    "the model incorporates the case of the stationary arma@xmath127 noise : @xmath128 where @xmath129 and @xmath130 are constants , such that the roots of the polynomial @xmath131 lie in the open unit disk of the complex plain .",
    "the canonical state space representation is obtained through the usual state augmentation @xmath132    * 3 . *",
    "assuming noises with gaussian distribution is essential , since in this case the filtering equations for the conditional density of @xmath104 given @xmath133 are finite dimensional and , moreover , the conditional mean @xmath134 satisfies the linear recursion , whose explicit solution is used on several occasions through the proof and appears in the expression for @xmath135 .",
    "the result can be extended to more general conditionally gaussian models , such as , e.g. , higher order tar with possibly heteroscedastic driving noise .",
    "we expect that for non - gaussian noise , the limit likelihood will still be a two - sided compound poisson process , but no neat closed form expression for @xmath135 will be available .",
    "* 4 . * in principle , our technique is applicable to gaussian sequences with non - markov structure , such as fractional noises , etc .",
    "the analysis in this setting is more complicated , depending on the ergodic properties of the processes and the complexity of the filtering equations ( whose linearity will be intact ) .",
    "* joint asymptotic analysis of the likelihood based estimators of all the parameters in the model can be in principle carried out using the same weak convergence approach , used in the proof of theorem [ thm ] below ( see a brief outline in section [ sec-2.2 ] ) . in this case the likelihood is considered as a function of the four unknown parameters @xmath136 and @xmath137 and the corresponding normalized likelihood ratios read ( cf . below ) @xmath138 for a fixed value of the parameter vector @xmath139 and variables @xmath140 taking values in appropriate sets .",
    "note that the localizing scaling of @xmath5 differs from that of the other parameters , in which the likelihood function is smooth .",
    "it is possible to check the weak convergence of processes @xmath141 where @xmath142 is the same as in and @xmath143 with @xmath144 and the fisher information matrix @xmath145 , whose explicit expression is cumbersome .",
    "the convergence implies the weak convergence of errors for the corresponding bayes estimators @xmath146 with @xmath147 as in theorem [ thm ] and zero mean normal vector @xmath148 with covariance @xmath149 , independent of @xmath147 .",
    "since the lan property holds with respect to @xmath150 , the corresponding maximum likelihood estimators ( mles ) are also asymptotically efficient .",
    "the proof of theorem [ thm ] is given in the next section and some supplementary results , concerning the ergodic properties of the relevant processes , appear in appendix [ app - sec ] .",
    "some simulations , demonstrating the contributions of this paper , are gathered in section [ sec - sim ] .",
    "the actual unknown value of the parameter will be denoted by @xmath151 and will be assumed to belong to a generic compact @xmath152 .",
    "we shall use @xmath153 , @xmath154 to denote absolute constants , whose values depend only on the known parameters of the model and the compact @xmath155 and may change at each appearance . for random sequences",
    "@xmath156 , @xmath157 and a positive real decreasing sequence @xmath158 , @xmath159 means that @xmath160 is a random variable with moments , bounded uniformly over @xmath155 . throughout , we reserve @xmath161 for an integer @xmath75 , the quantities such as @xmath162 and @xmath163 , are understood to be rounded to the nearest integer . for @xmath164 , we set @xmath165 and @xmath166 . for a vector @xmath167 , @xmath168 stands for the @xmath169-norm .",
    "finally , @xmath170 and @xmath171 denote the probability on @xmath172 and the corresponding expectation , under which all the processes are stationary ( the unique existence of such probability is argued in appendix [ app - sec ] ) .",
    "consider the scaled sequence of likelihoods @xmath173 the bayes estimator of @xmath5 is given by @xmath174 and thus @xmath175 the right hand side is a functional of @xmath176 , @xmath177 , which under appropriate tightness conditions , converges weakly to the random variable @xmath178 if the finite dimensional distributions of @xmath176 converge to those of @xmath142 .",
    "more precisely , the result claimed in theorem [ thm ] follows from theorem i.10.2 in @xcite , whose assumptions we check in subsections [ sec - fdd ] and [ sec - tight ] below .",
    "we shall prove that the characteristic functions of the finite dimensional distributions of @xmath179-likelihoods @xmath180 converge to those of the compound poisson process in . to this end",
    ", we will show that for any @xmath181 and real numbers @xmath182 @xmath183 uniformly over compacts from @xmath90 , where @xmath184 and @xmath185 are constants , defined in theorem [ thm ] .",
    "without loss of generality , we shall assume that @xmath186 and consider only positive @xmath187 s .",
    "the symmetric case of negative @xmath187 s is treated similarly and independence of the emerging compound poisson processes @xmath188 and @xmath189 will be evident from the proof .",
    "let @xmath190 be the vector with the entries @xmath191 using the expression for the likelihood , we get @xmath192 where we set @xmath193 $ ] and used the identity @xmath194 if we define @xmath195 the expression takes the following form under @xmath100 @xmath196 with @xmath197 $ ] .",
    "the sequences @xmath198 and @xmath199 satisfy the recursions @xmath200 and @xmath201 where we set @xmath202 , @xmath203 . in",
    "what follows , both representations and will be useful .    to prove the convergence , we shall partition the terms in the sum or into @xmath162 consecutive blocks of size @xmath162 and discard @xmath163 first terms in each block . as shown in the lemma [ lem-1 ] below , discarding the total of @xmath204 terms does not alter the limit of the sum and , by lemma [ lem-2 ] , the remaining blocks become approximately independent due to the fast mixing of the process @xmath205 .",
    "moreover , in each remained block , the probability of having exactly one of the events @xmath206 occurred is of order @xmath162 .",
    "hence the sum of @xmath162 such nearly independent blocks yields the compound poisson limit of lemma [ lem-3 ] .",
    "this approach to poisson limits dates back to at least @xcite .",
    "denote by @xmath207 the summands in the right hand side of or .",
    "set @xmath208 and , for @xmath209 , define @xmath210 for @xmath211 and @xmath212 defined in , the triangle inequality yields the bound @xmath213 where @xmath171 stands for the expectation with respect to the probability @xmath170 on @xmath172 , under which the process @xmath214 is stationary ( see lemma [ lem - lip ] ) . in the following lemmas we show that all three terms in the right hand side of vanish as @xmath215 , uniformly over @xmath151 on compacts from @xmath90 .",
    "[ lem-1 ] for any @xmath216 and a compact @xmath152 , @xmath217 and consequently @xmath218 uniformly over @xmath219 .",
    "we shall assume that @xmath75 is large enough so that @xmath220 and hence on the events @xmath206 and @xmath221 , we have @xmath222 .",
    "using the representation , we get @xmath223 by jensen s inequality , it follows from , that @xmath224 which , in view of and @xmath225 , implies @xmath226 . further , since @xmath69 is independent of @xmath65 , @xmath227 .",
    "similarly @xmath228 .",
    "plugging these bounds into , we obtain @xmath229 with a constant @xmath230 , depending only on @xmath155 . the uniform convergence in follows , since @xmath231 .    [ lem-2 ] for any @xmath232 , @xmath233 uniformly over @xmath219 for any compact @xmath234 .",
    "we shall use the bound of lemma [ lem - lip ] and thus will need to establish the corresponding lipschitz property . to this end , for fixed @xmath235 and @xmath236 , let @xmath237 be the solution of the recursions , and ( cf . and )",
    "@xmath238 subject to the initial conditions @xmath239 , @xmath240 and @xmath241 respectively .",
    "the latter recursions give @xmath242 consider the random variable ( cf .",
    "the right hand side of ) @xmath243 where we dropped the dependence on @xmath239 and @xmath240 for brevity .",
    "define the function @xmath244 .",
    "we aim to show that for @xmath245 , @xmath246 for some constant @xmath247 , independent of @xmath248 .",
    "using the definition of @xmath249 and the explicit formula , a tedious but straightforward calculation gives @xmath250 taking the expectation of both sides , we get @xmath251 and applying the bound , the inequality follows .",
    "note that by the markov property , @xmath252 and for @xmath209 @xmath253 hence by the lemma [ lem - lip ] @xmath254 with a positive constant @xmath255 and @xmath256 , independent of @xmath151 . finally ,",
    "considering the telescopic series , we get @xmath257 as claimed .",
    "[ lem-3 ] for any @xmath232 , @xmath258 uniformly over @xmath219 for any compact @xmath234 .",
    "let @xmath259 and define the events @xmath260 and note that @xmath261 below we shall show that @xmath262 where @xmath263 and @xmath98 is the unique invariant density of the chain @xmath99 ( see lemma [ lem - erg ] ) . plugging these expressions into , we obtain @xmath264 the claimed result follows , once we check that , and hold uniformly in @xmath151 on compacts from @xmath90 .    to this end , note that on the event @xmath265 , the equations give @xmath266 and @xmath267 where @xmath268 , @xmath269 are bounded random variables under @xmath270 .",
    "similarly , since @xmath271 for @xmath272 , and @xmath273 for @xmath274 , @xmath275 and @xmath276 hence , for @xmath277 on the event @xmath265 , by we have @xmath278 where we defined the kernel @xmath279 by the triangle inequality @xmath280    by , for @xmath281 @xmath282 with a constant @xmath256 , independent of @xmath151 . similar bound holds for @xmath283 and hence , using the identity @xmath284 , we get @xmath285 further , since @xmath286 are i.i.d @xmath24 and @xmath287 is independent of @xmath65 for @xmath288 @xmath289 by , @xmath290 and thus we have @xmath291 plugging and into , we obtain @xmath292 and in turn : @xmath293 by setting all @xmath294 s to zero , we also get @xmath295 further , @xmath296 where in the equality @xmath297 we used . on the other hand , @xmath298 and",
    "the estimate follows from and and the asymptotic @xmath299 finally , follows since @xmath300 .      in this section",
    "we check the tightness conditions of theorem i.10.2 , @xcite .    for any compact @xmath152",
    ", there is a constant @xmath301 , such that @xmath302 for all @xmath219 and @xmath303 .",
    "suppose @xmath304 , then using the elementary inequality @xmath305 , @xmath306 we get @xmath307 similarly to , we find that under @xmath308 , @xmath309 where @xmath310 is defined in .",
    "note that @xmath69 is independent of @xmath65 under @xmath311 and , as in the proof of lemma [ lem-1 ] , @xmath312 .",
    "hence @xmath313 where @xmath314 depends only on @xmath155 .",
    "by symmetry , the same inequality holds when @xmath315 and follows .",
    "[ lem - ld ] for any @xmath316 , there is a constant @xmath317 , such that @xmath318    the proof is an adaptation of the analogous lemma 2.2 in @xcite .",
    "we shall assume @xmath319 and @xmath320 , omitting the similar complementary cases ( recall that @xmath20 ) .",
    "note that for a constant @xmath321 , @xmath322 and hence it is enough to check the large deviation bound @xmath323 for some positive constants @xmath324 and @xmath317 and all @xmath316 .",
    "for @xmath325 the formula gives @xmath326 where @xmath327 $ ] , the sequence @xmath328 is generated by with @xmath329 , and @xmath330 further , @xmath331 and since @xmath332 the bound holds , if we show that for some positive constant @xmath324 and all @xmath316 , @xmath333    to this end , we shall split the consideration to the cases @xmath334 and @xmath335 , where @xmath336 is a constant to be chosen later on , depending on @xmath337 in .      in this case ,",
    "@xmath339 $ ] and since @xmath340 subject to @xmath341 , it follows that @xmath342 and @xmath343 and the assumption @xmath29 .",
    "further , @xmath344 where the latter inequality holds for all @xmath75 large enough .",
    "consequently , @xmath345 by lemma [ lem - erg ] ,",
    "the process @xmath7 is geometric mixing and we have @xmath346 }   e^{-\\frac 1 2\\big(t-\\xi_{j-1}-f(x_{j-1},\\theta_0)\\big)^2}\\ge   c_2\\frac { { u } } n,\\end{gathered}\\ ] ] where the constant @xmath347 can be chosen for all @xmath219 to be independent of @xmath80 and @xmath75 by the ergodic properties of @xmath99 from lemma [ lem - erg ] . hence with @xmath348 , for an integer @xmath316 , @xmath349 where we defined @xmath350 .",
    "since @xmath351 , by lemma [ lem - erg ] , @xmath352 for all @xmath75 large enough .",
    "hence it is enough to check @xmath353 to estimate the latter expectation , we shall apply the covariance inequality ( 8.1 ) from @xcite : @xmath354 since @xmath355 , @xmath356 where the latter inequality holds by lemma [ lem - erg ] , since @xmath357 and @xmath358 . plugging the bound into",
    ", we obtain and consequently for @xmath338 .      for a fixed integer @xmath359 and all @xmath360 , define @xmath361 and note that on @xmath362 we have @xmath363 be such that , @xmath364 , then ( recall that both @xmath151 and @xmath365 are positive ) @xmath366 define @xmath367 and note that @xmath368 where the positive constant @xmath369 can be chosen independent of @xmath80 and @xmath75 due to the ergodic properties of @xmath99 from lemma [ lem - erg ] .",
    "hence with @xmath370 and any integer @xmath371 , @xmath372 for all sufficiently large @xmath75 . now follows if we show that for a positive constant @xmath317 , @xmath373 to this end , we shall use the marcinkiewicz - zygmund inequality from @xcite . for a sequence of random variables",
    "@xmath374 , the coefficient of weak dependence is defined @xmath375 where the supremum is taken over all @xmath376 , such that @xmath377 and @xmath378 , @xmath379 satisfy @xmath380 .",
    "let @xmath374 be a sequence of central random variables such that for a fixed integer @xmath381 , @xmath382 then there exists a positive constant @xmath111 , independent of @xmath75 , for which @xmath383    we shall apply this theorem to the bounded sequence @xmath384 . since @xmath385 is a function of @xmath386 ,",
    "it inherits the mixing property .",
    "more precisely , with @xmath387 by the markov property of @xmath99 , for @xmath388 @xmath389 which clearly satisfies . since @xmath390 , now gives @xmath391 and the bound follows , if we choose @xmath392",
    "the objective of this section is to illustrate the results of theorem [ thm ] by means of a simulation . to this end",
    ", we fixed the following values of the parameters @xmath393 and estimated the root mean square errors of the bayes estimator @xmath71 and the ml estimator @xmath394 by averaging over a large number of monte carlo trials .",
    "this has been done in two ways : by computing the estimators , based on simulated data , and computing the corresponding limit quantities , based on simulated process from the limit experiment .",
    "the practical advantage , offered by theorem [ thm ] , is that the latter simulation requires much less cpu time than the former .",
    "the normalized empirical root mean squares of the be , mle and pseudo mle versus the sample sizes , title=\"fig : \" ] +      using the recursions and , we generated a large number ( @xmath395 ) of sample paths .",
    "for each path we computed the bayes estimator @xmath71 , using the formula and the uniform prior on the interval @xmath396",
    ". then we calculated the normalized empirical root mean square error for a number of sample sizes @xmath397 where @xmath398 denotes averaging over the paths .",
    "similarly , we computed the ( central ) ml estimator and the _ pseudo _ ml estimator , which assumes independent innovations with the same variance @xmath399 .",
    "the results , depicted at figure [ fig1 ] , indicate that the errors converge as the sample size @xmath75 increases and that the bayes estimator performs better than the others for smaller sample sizes as well .      while the distribution of the random variable @xmath147 , defined in theorem [ thm ] , can not be computed in a closed form , it is easy to sample and the expectations can be approximated by averaging over monte carlo trials . to this end , we estimated the value of @xmath400 using standard kernel estimator , applied to a single long trajectory of @xmath7 .",
    "figure [ fig2 ] depicts the estimated marginal density @xmath401.\\ ] ] next we generated a large number ( @xmath402 ) of samples from the compound poisson process @xmath142 , defined in and computed the approximate root mean square errors @xmath403 where @xmath404 is the central maximizer of @xmath142 and @xmath405 denotes the empirical expectation .",
    "note that the obtained estimates are at good correspondence with the plots at figure [ fig1 ] .",
    "the typical realization of @xmath142 along with @xmath147 and @xmath404 are plotted at figure [ fig3 ] .",
    "the densities of @xmath404 and @xmath147 , whose kernel estimates are depicted at figure [ fig4 ] , appear to be heavy tailed .     the estimated stationary density of @xmath7 , title=\"fig : \" ] +     a typical sample path of the limit likelihood ratio process @xmath142 , @xmath93 .",
    "the marks are the positions of @xmath404 and @xmath147 , title=\"fig : \" ] +     the estimated probability densities of @xmath147 and @xmath404 , title=\"fig : \" ] +",
    "the proofs in section [ sec - pf ] use the ergodic properties of the processes , summarized in the following lemmas .",
    "our standing assumption is .",
    "[ lem - bnd ] for all integers @xmath60 and @xmath316 , @xmath406\\big ) \\le \\frac{|v|}{n},\\ ] ] and @xmath407 with a positive constant @xmath408 and constants @xmath409 and @xmath410 , independent of @xmath151 .    for @xmath58 , @xmath411\\big ) = { \\mathbb{e}}_{\\theta_0 } \\int_{\\theta_0}^{\\theta_0+v / n } \\frac 1 { \\sqrt{2\\pi } } e^{-\\frac 1 2 ( x - f(x_{j-1},\\theta_0)-\\xi_{j-1})^2}dx \\le",
    "\\frac { v } n.\\ ] ] further , by jensen s inequality @xmath412 and hence @xmath413 similarly , with @xmath414 @xmath415 and @xmath416 which gives .",
    "[ lem - erg ] the markov chain @xmath417 has the unique invariant measure under @xmath100 , with uniformly bounded probability density @xmath98 satisfying @xmath418\\big)= \\\\ \\int_{\\theta_0}^{\\theta_0+v / n}\\int_{{\\mathbb r } } p(x , y;\\theta_0)dxdy= \\frac{v}{n } \\int_{\\mathbb r}p(\\theta_0,y;\\theta_0)dy+ o(n^{-2}),\\end{gathered}\\ ] ] where @xmath170 is the corresponding stationary probability on @xmath172 .    moreover , the chain is geometrically ergodic , i.e. there exist positive constants @xmath115 and @xmath419 , such that for a measurable function @xmath420 and @xmath421 @xmath422 and consequently , for an @xmath423-measurable random variable @xmath424 @xmath425 finally , @xmath99 is geometrically mixing , i.e. for measurable functions @xmath426 , @xmath420 @xmath427 in particular , and hold with the stationary expectation @xmath171 .    the transition kernel of the process @xmath99 has a positive density with respect to the lebesgue measure : @xmath428 and hence in the terminology of @xcite , it is @xmath429-irreducible and aperiodic .",
    "further , a ball @xmath430 of radius @xmath431 around the origin is a small set with respect to e.g. the measure @xmath432 and @xmath433 satisfies the drift condition @xmath434 for sufficiently large @xmath435 . by theorem",
    "15.0.1 @xcite , it follows that there exists a unique invariant probability measure @xmath77 and for any measurable @xmath436 , @xmath437 with positive constants @xmath115 and @xmath419 , i.e. holds . since @xmath438 and @xmath439 with @xmath440 , the claim follows from and . since the transition kernel @xmath441 has a bounded continuously differentiable density with respect to the lebesgue measure , so does the invariant measure @xmath77 and follows .",
    "the mixing inequality follows from theorem 16.1.5 in @xcite .",
    "the theory , used in the proof of the previous lemma , does not directly apply to the markov chain @xmath442 ( see for the definition of @xmath190 ) , since it is generated by a @xmath443-dimensional recursion , driven by two dimensional noise .",
    "this typically excludes @xmath429-irreducibility .",
    "fortunately , for our purposes the following weaker properties are sufficient :    [ lem - lip ] the markov process @xmath442 has the unique invariant measure .",
    "let @xmath170 denote the corresponding stationary probability , introduced in lemmas [ lem - erg ] and [ lem - lip ] , coincide ] .",
    "then for a measurable function @xmath444 , satisfying @xmath445 and the lipschitz condition @xmath446 with a positive constant @xmath247 , @xmath447 for some positive constants @xmath115 and @xmath255 and all integers @xmath448 .    under the stationary measure @xmath170 from lemma [ lem - erg ] , we can extend the definition of @xmath99 to the negative integers and define @xmath449 where @xmath343 and @xmath450 .",
    "the distribution of @xmath451 is invariant . to establish uniqueness ,",
    "let @xmath452 and @xmath453 be two invariant measures and note that by lemma [ lem - erg ] their @xmath454 marginals coincide .",
    "hence @xmath455 where @xmath456 is the invariant measure of the process @xmath99 and @xmath457 and @xmath458 are corresponding regular conditional probabilities .",
    "let @xmath205 and @xmath459 be the solutions of the recursions , and with @xmath460 , @xmath461 subject to the initial conditions @xmath462 and @xmath463 , where @xmath464 is sampled from @xmath456 and @xmath465 and @xmath466 are sampled from @xmath467 and @xmath468 .",
    "note that @xmath469 and hence for any uniformly continuous function @xmath470 @xmath471 since uniformly continuous functions form a measure defining class , the uniqueness follows .    to derive the bound ,",
    "note that : @xmath472 using the bound , we get @xmath473 by the triangle inequality @xmath474    note that @xmath475 is measurable with respect to @xmath476 and by @xmath477 by the lipschitz property of @xmath478 and , we have @xmath479 similar bound holds for the last term in and the claim follows with @xmath480 .",
    "p.chigansky is supported by isf grant 314/09"
  ],
  "abstract_text": [
    "<S> large sample statistical analysis of threshold autoregressive ( tar ) models is usually based on the assumption that the underlying driving noise is uncorrelated . in this paper </S>",
    "<S> , we consider a model , driven by gaussian noise with geometric correlation tail and derive a complete characterization of the asymptotic distribution for the bayes estimator of the threshold parameter . </S>"
  ]
}