{
  "article_text": [
    "graph theory has recently reveived increasing attraction for applications to complex systems in various disciplines ( gernert 1997 , paton 2002a , b , bornholdt and schuster 2003 ) .",
    "the characterization of systems ( with interrelated constituents ) by graphs ( with linked vertices ) is comparably general as their characterization in terms of categories ( with elements related by morphisms ) . despite its generality ,",
    "graph theory has turned out to be a powerful tool for gaining very specific insight into structural and dynamical properties of complex systems ( see jost and joy 2002 , atmanspacher et al .",
    "2005 for examples ) .",
    "an area of particularly intense interest , in which complex systems abound , is biological information processing .",
    "this ranges from evolutionary biology over genetics to the study of neural systems .",
    "theoretical and computational neuroscience have become rapidly growing fields ( hertz et al .",
    "1991 , haykin 1999 , dayan and abbott 2001 ) in which graph theoretical methods have gained considerable significance ( cf .",
    "sejnowski 2001 ) .",
    "two basic classes of biological networks are feedforward and recurrent networks . in networks with purely feedforward ( directed ) connectivities , neuronal input is mapped onto neuronal output through a feedforward synaptic weight matrix . in recurrent networks , there are additional ( directed or bi - directed ) connectivities between outputs and other network elements , giving rise to a recurrent synaptic weight matrix . much recurrent modeling incorporates the theory of nonlinear and complex dynamical systems ( cf .",
    "smolensky 1988 , see also beim graben 2004 for discussion ) .",
    "hopfield networks are an example of a fully recurrent network in which all connectivities are bidirectional and the output is a deterministic function of the input .",
    "their stochastic generalizations are known as boltzmann machines .",
    "another important distinction with respect to the implementation of neural networks refers to the way in which the neuronal states are characterized : the two main options are firing rates and action potentials ( for more details see haykin 1999 ) .    a key topic of information processing in complex biological networks is learning , for which three basically different scenarios are distinguished in the literature ( see dayan and abbott 2001 , chap .",
    "iii ) : unsupervised , supervised and reinforcement learning . in unsupervised (",
    "also self - supervised ) learning a network responds to inputs solely on the basis of its intrinsic structure and dynamics .",
    "a network learns by evolving into a state that is constrained by its own properties and the given inputs , an important modelling strategy for implicit learning processes .",
    "in contrast , supervised learning presupposes the definiton of desired input - output relations , so the learned state of the network is additionally constrained by its outputs .",
    "usually , the learning process in this case develops by minimizing the difference between the actual output and the desired output .",
    "the corresponding optimization procedure is not intrinsic to the evolution of the system itself , but has to be externally arranged , hence the learning is called supervised .",
    "if the supervision is in some sense `` naturalized '' by coupling a network to an environment , which provides evaluative feedback , one speaks of reinforcement learning .    in this contribution",
    "we are interested in supervised learning ( see duda et al .",
    "2000 for a review ) on small , fully recurrent networks implemented on graphs ( cf .",
    "jordan 1998 ) .",
    "we start with a general formal characterization in terms of dynamical systems ( sec .",
    "2.1 ) , describe how they are implemented on graphs ( sec .",
    "2.2 ) , and show how it reaches asymptotically stable states ( attractors ) when the learning process is terminated , i.e.  is optimized for given inputs and ( random ) initial conditions with respect to predetermined outputs ( sec .",
    ".    we shall characterize the learning operations by a multiplicative structure characterizing successively presented inputs in sec .  3.1 . in this context",
    "we confirm and specify earlier conjectures ( e.g. , gernert 1997 ) about the non - commutativity of learning operations for a concrete model . in sec .",
    "3.2 , we study how the size of the set of attractors representing the derived structure changes during the process for perfectly and imperfectly optimized networks .",
    "the number of attractors is proposed to indicate the complexity of learning , and in sec .  4 this",
    "is tentatively related to pragmatic information as a particular measure of meaning .",
    "let @xmath0 be a set , and let @xmath1 , with @xmath2 , be a partition of @xmath0 into two disjoint subsets .",
    "if @xmath0 is some closed subset of @xmath3 , @xmath4 may be the boundary of @xmath0 .",
    "( later we will specify @xmath0 as the vertices of a graph , @xmath4 as a set of `` external '' or `` boundary '' vertices , and @xmath5 as a set of `` internal '' vertices . )",
    "we consider the dynamics of fields @xmath6 , where @xmath7 , @xmath8 , @xmath9 represents time as parametrized discretely or continuously , and @xmath10 is the space of admissible state values for the fields .",
    "the dynamics of @xmath11 can be described by an equation @xmath12=0   \\ , .\\ ] ] for a continuous time variable and @xmath13 , a typical example is the diffusion equation @xmath14= \\frac{\\partial u(x , t)}{\\partial t }            - \\lambda \\delta u(x , t)\\ ] ] where @xmath15 is the laplace operator and @xmath16 the diffusion constant . the only constraint on eq .",
    "[ eq1 ] is that a state @xmath17 at time @xmath18 determines uniquely the solution for any time @xmath19 .",
    "we now define a set of external conditions @xmath20 specifying field values @xmath21 on @xmath4 which will be kept fixed during the time evolution of the fields on @xmath0 . this is to say that the dynamics of fields is effectively restricted to @xmath5 : @xmath22=0\\ , .\\ ] ]    since the state of the system at time @xmath18 uniquely determines the states for all @xmath19 , we can define a mapping @xmath23 , the so - called time evolution operator , acting on the set of field states . for an initial state @xmath11 at @xmath18 , @xmath24 $ ] yields the state of the system at @xmath19 . taking into account",
    "that different external conditions initiate different evolutions , we have to specify the time evolution operator as a mapping @xmath25 , where @xmath26 is the set of states @xmath27 , by the following construction : let @xmath28 be the initial condition for eq .",
    "( [ eq2 ] ) , then @xmath29(x ) = u(x , b_i , t)\\ ] ] is the state of the corresponding solution at time @xmath9 under the external condition @xmath21 .    in principle",
    ", the state space of @xmath30 can be the entire set of states @xmath26 .",
    "however , for reasons which will become clear below , we are interested in dissipative systems evolving into attractors @xmath31 in the limit of large @xmath9 .",
    "if one of the states belonging to an attractor is chosen as an initial condition , the image of @xmath30 will again be one of the attractor states .",
    "this allows us to reduce the number of possible states on which the mappings @xmath30 close .    denoting the flow operator @xmath32 as the input under the external condition @xmath21",
    ", we now consider the set of states @xmath33 belonging to attractors after time @xmath9 .",
    "then all mappings @xmath34 , applied to an attractor @xmath35 , lead to images in @xmath36 : @xmath37 \\in a    \\hspace{1 cm }        \\mbox{for } a\\in a   \\ , .\\ ] ] in general , the set of all attractor states @xmath36 does not contain a proper subset which is mapped onto itself by the set of mappings @xmath38 ; otherwise @xmath36 can be reduced to such a subset .",
    "each single mapping @xmath34 may not be surjective , but the union of the images of all @xmath38 equals @xmath36 .    due to condition ( [ eq3 ] ) , we can define a composition of mappings @xmath34 . in this way ,",
    "the external conditions @xmath39 give rise to an associative multiplicative structure @xmath38 .",
    "this structure is represented on the set of attractors @xmath40 .",
    "we now implement the general notions developed so far on graphs ( see wilson 1985 for an introduction to graph theory ) and specify the set @xmath0 as the set of vertices @xmath41 of a graph . for simplicity",
    "we consider directed graphs with single connections for each direction between any two vertices and without self - loops .",
    "such a graph gives rise to non - reflexive relations on @xmath41 and can be represented by an adjacency matrix @xmath42 . for two vertices @xmath43 and @xmath44 we have : @xmath45 if @xmath42 is symmetric , the graph is undirected .",
    "the set of vertices @xmath41 is decomposed into a set of external vertices @xmath46 and a set of internal vertices @xmath47 .",
    "if @xmath48 is the total number of vertices , @xmath49 the number of external vertices and @xmath50 the number of internal vertices , we have @xmath51 .",
    "next we consider fields @xmath52 on a graph with vertices @xmath53 evolving in discrete time steps @xmath54 according to : @xmath55 the value of the field @xmath11 at vertex @xmath56 and time @xmath57 depends only on the sum of the field values at neighboring vertices @xmath58 at time @xmath9 .",
    "the fields @xmath52 assume integer values @xmath59 , and the function @xmath60 is defined as : @xmath61 where @xmath62 denotes the nearest integer - rounded @xmath63 .",
    "the function @xmath64 is shown in fig .",
    "[ fx ] .",
    "( 240,100)(0,0 ) ( 10,10)(1,0)200 ( 10,10)(0,1)80 ( 60,8)(0,1)4 ( 160,8)(0,1)4 ( 8,60)(1,0)4 ( 15,15)(5,5)10 ( 70,55)(10,-5)10 ( 65,55)(10,-5)10 ( 165,10)(5,0)5 ( 215,5)(0,0)@xmath63 ( 60,3)(0,0)@xmath65 ( 160,3)(0,0)@xmath66 ( 0,60)(0,0)@xmath67 ( 5,100)(0,0)@xmath64    the restriction of @xmath52 to integer values implies that there is only a finite number of states . starting from an arbitrary initial state in @xmath26 , the system runs into an attractor after a few time steps .",
    "in many cases , this attractor is a fixed point , i.e.  one single state that is asymptotically stable .",
    "sometimes the attractor is a limit cycle , i.e.  a periodic succession of several ( usually few ) states .",
    "strange attractors do not occur since the number of states is finite .",
    "the external conditions @xmath39 are defined as fixed states on the external vertices , i.e. , the state values on the external vertices remain unaffected by the dynamics . of course , the external conditions are supposed to influence the dynamics of the internal vertices .",
    "the graphs used in our investigations consist of a total of @xmath68 vertices with @xmath69 external vertices and @xmath70 internal vertices .",
    "the maximal value of @xmath52 is defined to be @xmath71 .",
    "we consider 11 different input patterns @xmath21 which are shown in fig .",
    "[ fig1 ] .",
    "( 390,120)(-20,0 ) ( 0,80 ) ( 10,80 ) ( 20,80 ) ( 30,80 ) ( 0,90 ) ( 10,90 ) ( 20,90 ) ( 30,90 ) ( 0,100 ) ( 10,100 ) ( 20,100 ) ( 30,100 ) ( 0,110 ) ( 10,110 ) ( 20,110 ) ( 30,110 ) ( 140,80 ) ( 150,80 ) ( 160,80 ) ( 170,80 ) ( 140,90 ) ( 150,90 ) ( 160,90 ) ( 170,90 ) ( 140,100 ) ( 150,100 ) ( 160,100 ) ( 170,100 ) ( 140,110 ) ( 150,110 ) ( 160,110 ) ( 170,110 ) ( 210,80 ) ( 220,80 ) ( 230,80 ) ( 240,80 ) ( 210,90 ) ( 220,90 ) ( 230,90 ) ( 240,90 ) ( 210,100 ) ( 220,100 ) ( 230,100 ) ( 240,100 ) ( 210,110 ) ( 220,110 ) ( 230,110 ) ( 240,110 ) ( 280,80 ) ( 290,80 ) ( 300,80 ) ( 310,80 ) ( 280,90 ) ( 290,90 ) ( 300,90 ) ( 310,90 ) ( 280,100 ) ( 290,100 ) ( 300,100 ) ( 310,100 ) ( 280,110 ) ( 290,110 ) ( 300,110 ) ( 310,110 ) ( 350,80 ) ( 360,80 ) ( 370,80 ) ( 380,80 ) ( 350,90 ) ( 360,90 ) ( 370,90 ) ( 380,90 ) ( 350,100 ) ( 360,100 ) ( 370,100 ) ( 380,100 ) ( 350,110 ) ( 360,110 ) ( 370,110 ) ( 380,110 ) ( 0,10 ) ( 10,10 ) ( 20,10 ) ( 30,10 ) ( 0,20 ) ( 10,20 ) ( 20,20 ) ( 30,20 ) ( 0,30 ) ( 10,30 ) ( 20,30 ) ( 30,30 ) ( 0,40 ) ( 10,40 ) ( 20,40 ) ( 30,40 ) ( 70,10 ) ( 80,10 ) ( 90,10 ) ( 100,10 ) ( 70,20 ) ( 80,20 ) ( 90,20 ) ( 100,20 ) ( 70,30 ) ( 80,30 ) ( 90,30 ) ( 100,30 ) ( 70,40 ) ( 80,40 ) ( 90,40 ) ( 100,40 ) ( 140,10 ) ( 150,10 ) ( 160,10 ) ( 170,10 ) ( 140,20 ) ( 150,20 ) ( 160,20 ) ( 170,20 ) ( 140,30 ) ( 150,30 ) ( 160,30 ) ( 170,30 ) ( 140,40 ) ( 150,40 ) ( 160,40 ) ( 170,40 ) ( 210,10 ) ( 220,10 ) ( 230,10 ) ( 240,10 ) ( 210,20 ) ( 220,20 ) ( 230,20 ) ( 240,20 ) ( 210,30 ) ( 220,30 ) ( 230,30 ) ( 240,30 ) ( 210,40 ) ( 220,40 ) ( 230,40 ) ( 240,40 ) ( 280,10 ) ( 290,10 ) ( 300,10 ) ( 310,10 ) ( 280,20 ) ( 290,20 ) ( 300,20 ) ( 310,20 ) ( 280,30 ) ( 290,30 ) ( 300,30 ) ( 310,30 ) ( 280,40 ) ( 290,40 ) ( 300,40 ) ( 310,40 ) ( 350,10 ) ( 360,10 ) ( 370,10 ) ( 380,10 ) ( 350,20 ) ( 360,20 ) ( 370,20 ) ( 380,20 ) ( 350,30 ) ( 360,30 ) ( 370,30 ) ( 380,30 ) ( 350,40 ) ( 360,40 ) ( 370,40 ) ( 380,40 ) ( 15,70)(0,0)1 ( 155,70)(0,0)2 ( 225,70)(0,0)3 ( 295,70)(0,0)4 ( 365,70)(0,0)5 ( 15,0)(0,0)6 ( 85,0)(0,0)7 ( 155,0)(0,0)8 ( 225,0)(0,0)9 ( 295,0)(0,0)10 ( 365,0)(0,0)11    .example of a mapping diagram for 11 inputs @xmath34 and a system with 10 different attractors @xmath31 .",
    "the entries show the number @xmath72 of the attractor state which is obtained by applying @xmath34 ( plotted vertically ) to @xmath31 ( plotted horizontally ) . [ cols= \" > ,",
    "> , > , > , > , > , > , > , > , > , > \" , ]     the multiplicative structure associated with tab .",
    "[ tab2 ] consists of the 11 elements @xmath34 which are idempotent , @xmath73 and satisfy the relation @xmath74 hence they are non - commutative , though associative : @xmath75    since the optimal reaction of a graph to an input is not uniquely related to that input , the attractor providing an optimal output can be identical for different inputs .",
    "therefore , the multiplicative structure of input operations can be even simpler in the sense that some of the attractors are identical .",
    "table  [ tab0 ] shows a corresponding example with less than 11 attractors .",
    "deviations from eq .",
    "( [ eqproj ] ) indicate a more complicated structure of learning operations .",
    "if the elements in the same row ( i.e.  for the same input ) of the mapping diagram differ from each other , the reaction of the graph with respect to an input depends on the previous input .",
    "this means that the result of a learning process depends on the sequence in which successive learning steps are carried out .",
    "this implies that the multiplicative structure of input operations deviates from eq .",
    "( [ eqproj ] ) .",
    "since the @xmath34 are mappings , associativity is valid trivially .",
    "however , the structure will generally be non - commutative , @xmath76 although it may happen that particular inputs commute , for instance when they project onto the same attractor , such as @xmath77 and @xmath78 , or @xmath79 and @xmath80 , or @xmath81 and @xmath82 in tab .",
    "[ tab0 ] .    we can now understand how an optimal learner differs from a perfect learner , which recognizes inputs independently of the sequence of their presentation . comparing tabs .",
    "[ tab0a ] and [ tab1 ] shows that attractor @xmath83 leads to the optimal output ( field values on the first two vertices ) for input @xmath84 , attractors @xmath85 and @xmath86 yield the optimal output for inputs @xmath87 , and attractors @xmath88 and @xmath89 yield the optimal output for inputs @xmath90 . in these cases ,",
    "optimal learning coincides with perfect learning .    from tab .",
    "[ tab0 ] we see that inputs @xmath91 are recognized independently of previous inputs .",
    "by contrast , inputs @xmath92 , @xmath93 and @xmath94 are recognized correctly only if the previous input is @xmath95 , @xmath92 and @xmath93 , respectively .",
    "table  [ tab0a ] shows that attractors @xmath96 lead to an `` almost '' correct output for inputs @xmath97 , and the output of @xmath98 differs considerably from any optimal output .",
    "although these situations represent optimal learning , they are different or even far from perfect learning .    if the attractor for a particular input does not consist of one single state ( fixed point ) , but of a perodic sequence of states ( limit cycle ) , idempotency [ eqidemp ] does no longer hold .",
    "( strictly speaking , this is only correct if the number of time steps @xmath9 in the mapping @xmath99 and the length of the cycle have no common denominator .",
    "otherwise , the attractor may consist of more states than can be detected by the mapping diagram or the set of inputs @xmath34 . )",
    "note that the structure of learning operations derived here is more general than an algebra ( as conjectured by gernert 2000 ) .",
    "there is no identity element , there is no neutral element , and no addition of the elements @xmath34 is defined .      in order to investigate the evolution of the set of attractors during the learning process",
    ", we focus on the number @xmath48 of attractor states as a function of learning steps for the entire sequence of graphs starting from a random graph until a graph with optimal learning is reached .",
    "since a large number of attractors intuitively relates to quite complex structures of the graph during the learning process , we propose to refer to the size of the set of attractors as a possible measure for the _ complexity of learning_. however , it should be emphasized that a rigorous definition of complexity ( cf .",
    "wackerbauer et al .",
    "1994 ) is not yet associated with this notion .",
    "initially , the graphs are ( almost ) random and exhibit large variances of the order of @xmath100 . for these graphs",
    "the number of attractor states with respect to the inputs varies over a large range ; typical are numbers between 30 and 50 .",
    "as learning begins , the variance decreases , but the number of attractor states increases , sometimes up to a few hundred .",
    "a further decrease in variance , below a value of 6000 , causes the number of attractor states to decrease again . for optimal learners ( graphs with vanishing variance ) the number of attractor states terminates at around @xmath101 .",
    "a typical example is shown in fig .",
    "[ fig3 ] .",
    "we now select a sample of 116 learning sequences starting from random graphs and terminating as ( almost ) optimal learners . for this sample",
    "we count the number of attractor states , i.e.  the complexity of learning , for those graphs which were accepted during the process , i.e. , for which the variance was always smaller than for any previous graph in the sequence .",
    "their behavior can be seen in fig .",
    "[ fig4 ] , where @xmath48 is plotted as a function of @xmath102 .",
    "it confirms the impression from fig .",
    "[ fig3 ] that , as learning proceeds , its complexity evolves non - monotonically .",
    "in about 50% of the cases the sequence started with less than @xmath103 attractor states .",
    "the final @xmath48 was much smaller , and for intermediate stages of learning @xmath48 reached a maximum during the learning process . in about 85% of all cases",
    "the final number of attractor configurations was smaller than 20 .",
    "the largest final number of attractor states for an optimal learner was 56 .",
    "exceptions from this behavior occur if the initial ( random ) graph has a number of attractor states that is extremely large , exceeding any other number of attractor states in the sequence .",
    "for this case we find a total number of 15 sequences . in 12 of these sequences",
    "the initial number of attractors is larger than 100 ( with a maximum of 747 ) .",
    "figure  [ fig5 ] shows a plot of number of attractors as a function of variance for 98 non - optimal learners whose final variance is @xmath104 . keeping in mind that decreasing variance corresponds to progressive learning , the general trend of figs .",
    "[ fig3 ] and [ fig4 ] reappears : the size of the set of attractors , i.e.  the complexity of learning , evolves non - monotonically as learning proceeds .    as the main observation of the present subsection",
    ", we can state that the number @xmath48 of attractors required to optimally map a given input onto a predetermined output evolves non - monotonically during the process of learning . while @xmath48 increases during the initial phase of learning , it decreases again until the learning process is terminated .",
    "we interpret this behavior as a non - monotonic complexity of the learning process .",
    "non - monotonic as opposed to monotonic measures of complexity have been developed and investigated for about two decades ; for a comparative overview see wackerbauer et al .",
    "the property of monotonicity is usually understood as a function of ( some measure of ) randomness of the pattern or process considered .",
    "monotonic complexity essentially increases as randomness increases : most random features are also most complex .",
    "non - monotonic complexity shows convex behavior as a function of increasing randomness : highest complexity is assigned to features with a mixture of random and non - random elements , while both very low and very high randomness yield minimal complexity .",
    "there is an interesting relationship between the two classes of complexity measures and measures of information ; for more details see atmanspacher ( 1994 ) or atmanspacher ( 2005 ) .",
    "it turns out that monotonic complexity usually corresponds to syntactic information , whereas non - monotonic ( convex ) complexity corresponds to semantic information or other measures of meaning ( see fig .",
    "[ fig6 ] ) .    as a particularly interesting approach ,",
    "pragmatic information has been proposed ( weizscker 1972 ) as an operationalized measure of meaning .",
    "its essence is that purely random messages keep providing complete novelty ( or primordiality ) as they are delivered , while purely non - random messages keep providing complete confirmation ( after initial transients ) .",
    "pragmatic information refers to meaning in terms of a mixture of confirmation and novelty . extracting meaning from a message depends on the capability to transform novel elements into knowledge using confirming elements .",
    "it has been speculated ( atmanspacher 1994 ) that systems having this capacity are able to reorganize themselves in order to flexibly modify their complexity relative to the task that they are supposed to solve .",
    "a learning process , in which insight is gained and meaning is understood , may start at low complexity ( high randomness , much novelty ) and terminate at low complexity ( high regularity , much confirmation ) , but it passes through an intermediate stage of maximal complexity .",
    "the notion of pragmatic information was earlier utilized in this sense for non - equilibrium phase transitions in multimode lasers ( atmanspacher and scheingraber 1990 ) .",
    "it could be shown that a particular well - defined type of pragmatic information , adapted to that case , behaves precisely as indicated above .",
    "pragmatic information is maximal at the unstable stage of the phase transition , and it is low in the preceding and successive stages .",
    "however , lasers are physical systems , and it is problematic to ascribe something like an `` understanding of meaning '' to their behavior .",
    "biological networks such as studied in this paper are more realistic systems for a concrete demonstration of the basic idea .",
    "the non - monotonic complexity of learning processes as indicated in sec .",
    "3.2 starts with random graphs and ends with graphs of minimized variance ( maximized fitness ) , which are as non - random as possible under the given conditions . in this sense",
    ", a scenario has been established in which the complexity of learning on graphs qualitatively satisfies the conditions required for relating it to a measure of pragmatic information . within this scenario ,",
    "our approach suggests that the actual `` release of meaning '' during learning does not occur when the output is optimized but rather when the complexity is maximized .",
    "it is a long - standing desideratum to identify meaning - related physiological features in the brain ( freeman 2003 ) .",
    "since learning is a key paradigm in which the emergence of meaning can be studied , we hope that our approach may offer a useful perspective for progress concerning this problem .",
    "in this contribution an example of supervised learning in recurrent networks of small size implemented on graphs is studied numerically .",
    "the elements of the network are treated as vertices of graphs and the connections among the elements are treated as links of graphs .",
    "eleven inputs and two outputs are predefined , and the learning process within the remaining six internal vertices is carried out such as to minimize the difference between the actual output and the predetermined output .",
    "optimization of outputs is achieved by stable configurations at the internal vertices that can be characterized as attractors .",
    "two particular features of the learning behavior of the network are investigated in detail .",
    "first , it is shown that , in general , the mapping from inputs to outputs depends on the sequence of inputs .",
    "thus , the associative multiplicative structure of input operations represented by sets of attractors is , in general , non - commutative .",
    "second , the size of the set of attractors changes as the learning process evolves .",
    "with increasing optimization ( fitness ) , the number of attractors increases up to a maximum and then decreases down to a usually small final set for optimal network performance .    assuming that the size of the set of attractors indicates the complexity of learning , its non - monotonic behavior is of special interest .",
    "since non - monotonic measures of complexity can be related to pragmatic information as a measure of meaning , it is tempting to consider the maximum of complexity as reflecting the release of meaning in learning processes .",
    "further work will be necessary to substantiate this speculation .",
    "atmanspacher , h. , 1994 , complexity and meaning as a bridge across the cartesian cut . journal of consciousness studies 1 , 168181 .",
    "freeman , w.j . , 2003 ,",
    "a neurobiological theory of meaning in perception , part i : information and meaning in nonconvergent and nonlocal brain dynamics .",
    "international journal of bifurcation and chaos 13 , 24932511 ."
  ],
  "abstract_text": [
    "<S> we present results from numerical studies of supervised learning operations in recurrent networks considered as graphs , leading from a given set of input conditions to predetermined outputs . </S>",
    "<S> graphs that have optimized their output for particular inputs with respect to predetermined outputs are asymptotically stable and can be characterized by attractors which form a representation space for an associative multiplicative structure of input operations . as the mapping from a series of inputs onto a series of such attractors generally depends on the sequence of inputs , </S>",
    "<S> this structure is generally non - commutative . </S>",
    "<S> moreover , the size of the set of attractors , indicating the complexity of learning , is found to behave non - monotonically as learning proceeds . a tentative relation between this complexity and the notion of pragmatic information </S>",
    "<S> is indicated . </S>"
  ]
}