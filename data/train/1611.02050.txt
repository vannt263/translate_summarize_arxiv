{
  "article_text": [
    "its inception in the 1960s , the kalman filter ( kf ) has been one of the most powerful tools in signal estimation and control engineering .",
    "it is a technique for estimating the unknown states of dynamical systems from a set of the resulting noisy measurements , see e.g. @xcite for an overview .",
    "it is well - known that the kf yields the minimum variance estimator ( mve ) if the noise terms are assumed to be zero - mean white gaussian processes .",
    "however , such gaussian assumption may limit the utility of the estimators in situations where the noise terms behave quite differently .",
    "if the gaussian assumption is discarded , the filter gives the linear minimum variance unbiased estimator ( lmvue ) .",
    "the problem of state - estimation in a stochastic setting with unknown distribution on the random terms has been studied ( e.g. , see @xcite , @xcite , @xcite , @xcite and references therein ) . the approach presented in @xcite ,",
    "@xcite is based on asymptotic distribution theory for state estimator in the absence of the usual gaussian assumptions on the noise terms . @xcite and @xcite",
    "show how inequalities such as chebyshev inequality from probability theory can be employed for characterizing the uncertainty bounds of the estimation error in a general distribution free setting . however , these methods assume an underlying stochastic dynamical model with known finite second order moments for the noise terms .",
    "moreover , they assume that the random terms ( the initial state , process noise and measurement noise ) are mutually independent . in many applications ,",
    "however , one is faced with lack of statistical knowledge of noise terms .",
    "a natural question to ask is what the performance of the kf will be with respect to disturbance variation and lack of statistical knowledge of the disturbance terms .",
    "some @xmath0 estimation methods @xcite , @xcite have been developed to study the performance of adaptive filtering without requiring statistical assumptions .",
    "such @xmath0 estimators are safeguard against the _ worst - case _ disturbance that maximizes the energy gain to the estimation errors .",
    "these techniques assume the existence of a linear model which together with additive noise generates the outputs .",
    "performance analysis of such approach does not address the situation where the generative models are incorrectly specified .    in this note , we aim to study the performance of the kf through analyzing its game - theoretic - based setting .",
    "this work is inspired by the worst - case , _ minimax _ setting in online machine learning as surveyed in @xcite .",
    "this approach is based on the _ universal prediction _ perspective which attempts to perform well on any sequence of observation without assuming a generative model of the data .",
    "our approach is essentially different from stochastic state - space analysis in that we abandon the basic assumption that the output sequence is generated by an underlying stochastic process . instead of making assumption on the data generating process , we ask : can we predict the sequence of outputs online almost as well as the best reference predictor ( comparator ) _ in hindsight _ ( in this case , in hindsight means that the reference predictor corresponds to the output of an offline algorithm with access to all the data simultaneously ) ?    the online algorithm is formulated as an iterated game between the _ player _ and the _ environment _    . at each round or _ trial _ of this game ,",
    "the player is challenged to guess an unknown output vector generated by the environment .",
    "the player computes its prediction for the output by combining the entries of a known input matrix with the information collected in the past trials .",
    "after this , the actual output is revealed , and the player incurs a loss computed according to a fixed _ convex loss function _ , measuring the discrepancy between the player s prediction and the observed output . as no assumption is made on how the environment generates the sequence of outputs , the player would accumulate an arbitrary high loss . to set a reasonable goal , a _ competitive _",
    "approach is adopted : the performance of the player is measured against the performance of the comparator from some comparison class @xmath1 . in the extreme case ,",
    "the sequence of observations could be completely random , in which it could be predicted by neither the player nor any reference predictor from the comparison class @xmath1 . on the other hand",
    ", the sequence of outputs might be completely predictable by a reference predictor from @xmath1 , in which case the player should incur only a small loss before learning to follow that predictor . in general ,",
    "the goal of the player is to achieve an average loss of prediction that is not too large compared to the average loss of the best offline reference predictor .",
    "recently , there has been an increasing interest in the machine learning community for analyzing the least mean square ( lms ) algorithm @xcite , @xcite and the recursive least square ( rls ) algorithm @xcite , @xcite in a worst - case setting , see also @xcite .",
    "the aforementioned work analyzes the performance of the algorithm in the stationary setting and guarantee that the algorithm performs almost as well as the best offline single reference predictor .",
    "however , in many real - world applications , particularly , in an adaptive context not accounting for the effect of parameter variations is insufficient .",
    "this motivates the study of the performance of algorithms that are able to compete with the best reference predictor that drifts over time .",
    "naturally , tracking worst - case bounds are , in general , much harder to prove than worst - case bounds for stationary targets .",
    "the tracking problem in the minimax setting has been considered previously : @xcite derived tracking bounds for general gradient descent algorithms and proposed a generic method for converting the stationary regret bounds into tracking regret bounds . for online regression algorithms ,",
    "tracking regret bounds were studied and analyzed in @xcite , @xcite , @xcite .",
    "@xcite uses a projection step to control the eigenvalues of a covariance - like matrix using scheduled resets , whereas the algorithm designed in @xcite is based on a last - step technique @xcite for which the eigenvalues are controlled implicitly .",
    "however , they lack a mechanism for incorporating dynamical models for effective prediction and tracking performance .",
    "we provide new tracking worst - case bounds for the kf by analysing its game - theoretic - based model .",
    "the proposed tracking bounds rely on a general notion of the drift or the comparator complexity as it is measured in terms of how much it deviates from a known fixed dynamical model .",
    "it is noteworthy that incorporating a predetermined dynamical model in the comparator s complexity , makes the problem more difficult and additional consideration is required for deriving tracking bounds for the kf compared to online regression algorithm @xcite .",
    "the precise setup is given in the next subsection .",
    "subsection b relates the current approach to an @xmath0 setting . in section ii",
    "the main results are given and technical comparison with the @xmath0 setting is given in section iii .",
    "section iv illustrates the theoretical results using simulation .",
    "finally , the paper is concluded with a discussion of the results in section v.       in this subsection , we study the game - theoretic - based formulation of the kf where an adversary ( environment ) generates the sequence of observations .",
    "to establish the worst - case bounds , we combine the state update mechanism from the classical kf with the online learning setting from universal prediction . the algorithm is defined by a 4-tuple @xmath2 where @xmath3 , @xmath4 are user - defined positive definite matrices , and @xmath5 , @xmath6 are time - invariant matrices assumed to be known and given to the algorithm .",
    "the game ( as implemented here by the kf ) proceeds in trials @xmath7 , where @xmath8 is any arbitrary value .",
    "the player maintains a parameter vector ( hypothesis ) , denoted by @xmath9 , and a positive definite matrix @xmath10 . in each trial",
    "@xmath11   the player makes a prediction ( here we concentrate on _ linear predictors _ ) @xmath12 .",
    "then , the environment reveals the actual output and the player incurs the corresponding loss @xmath13 , finally , the player updates its prediction rules as @xmath14 and proceeds to the next round .",
    "the update of the parameter vector @xmath15 is additive , with the error @xmath16 scales by the ( kalman ) gain @xmath17 and the matrix @xmath18 is updated according to the riccati recursion .",
    "the algorithm is initialised by a fixed user - defined @xmath19 and @xmath20 .",
    "algorithm [ algkf ] details the precise protocol .",
    "the total loss of the player on the sequence of observations @xmath21 is @xmath22[cumloss ]    according to the methodology of worst - case bounds as set out in online learning theory ( @xcite ) , this @xmath23 is compared to the total loss of any reference predictor @xmath24 where @xmath1 is the comparison class of predictors . to any arbitrary sequence of comparator @xmath25 , we associate a linear predictor , defined as @xmath26 with @xmath27 .",
    "then any set @xmath28 of vectors defines a comparison class @xmath1 of linear predictors by @xmath29 .",
    "given a sequence of observations @xmath30 , the total loss of the reference predictor @xmath31 is defined as    @xmath32    now , the aim of the player is to incur small loss relative to the reference predictor sequence in @xmath1 .",
    "hence , for any @xmath8 , the goal is to obtain an upper bound of the form @xmath33 where @xmath34 , @xmath35 , and @xmath36 are positive constants , and @xmath37 measures , intuitively , the complexity of the reference predictor in terms of how much it deviates from a _ given dynamical model _",
    "@xmath38 : @xmath39 also referred to as the total drift .",
    "note that the infimum ( minimum ) in ( [ worstbound ] ) depends on all the observations and can hence correspond to an offline solution computed after observing all the data .",
    "an important feature of the algorithm 1 is that a fixed and known dynamical model is incorporated in the learning process .",
    "the worst - case bound ( [ worstbound ] ) scales proportionally to the comparator complexity ( [ comparatorcomplexity ] ) , from a sequence evolving with known dynamics .",
    "if the reference predictor follows the dynamics well as described by the system matrix @xmath38 , then the @xmath37 would be small and one would get a small bound . if , however , @xmath37 is large ( e.g. because of misspecification ) , the resulting bound will become rather large .",
    "this insight is substantiated both theoretically ( section ii ) as well as experimentally ( see section iv ) .",
    "whereas our algorithm always predict according to a linear function as hypothesis using a fixed and known matrix @xmath40 , the resulting worst - case bound does not require the outputs @xmath41 to come from a linear system .",
    "it is worth mentioning that our bounds will be parametrized by the weights of the reference predictor , ( i.e. , @xmath42 ) .",
    "however , the algorithm for which we prove these bounds does not need the vectors @xmath42 as input parameter .",
    "[ rem2 ] in cases where the constant @xmath43 , one can rewrite ( [ worstbound ] ) in terms of the _ relative regret _ which is defined as the discrepency between the total loss of the player and that of the reference predictor @xmath44 @xmath45 the goal of the player is to minimize the supremum of the regret with respect to the comparison class @xmath1   and for an arbitrary sequence of observations @xmath46   chosen by the environment @xmath47 where @xmath35 and @xmath36 are positive constants .",
    "we restate the player s aim as having a _ low regret _ ,",
    "by which we mean that @xmath48 grows sublinearly with the number of rounds @xmath8 , i.e. , @xmath49 .",
    "intuitively , the regret measures how sorry the player is , _ after _ seeing all the data ( i.e. , in hindsight ) , for not having followed the predictions of the best reference predictor in the comparison class @xmath1 .",
    "[ ]    @xmath6 , @xmath5 . set @xmath50 ( identity matrix ) ,",
    "@xmath51 , @xmath52 , @xmath53 .",
    "+ ( 1 ) player predicts @xmath12 ( 2 ) environment reveals @xmath54 ( 3 ) player incurs loss @xmath55 ( 4 ) player updates estimate as @xmath56      the worst - case/_minimax _ analysis in the regret framework is closely related to the @xmath0 estimation theory , see e.g. @xcite and references therein : assume that there exists a linear time - invariant ( lti ) model explaining the data : @xmath58 where @xmath59 is an unknown true parameter state vector , @xmath60 is the measured output , @xmath61 is the process noise and @xmath62 is referred to as observation residual .",
    "let @xmath63 be any linear combination of the state @xmath64 where @xmath65 and let @xmath66 denote an estimate of @xmath67 given past observations @xmath68 .",
    "let @xmath69 denote the transfer operator that maps the unknown normalized disturbances @xmath70 to the normalized predicted estimation error @xmath71 where @xmath20 , @xmath72 , and @xmath73 are user - defined positive definite weighting matrices and @xmath19 denotes an initial guess of @xmath74 .",
    "the @xmath0 norm of @xmath69 denoted as @xmath75 is defined as the maximum energy gain from unknown disturbances to the predicted estimation error .",
    "an @xmath0 optimal strategy @xmath76 minimizes @xmath75 , and achieves the optimal attenuation factor @xmath77 @xmath78 where @xmath79 denotes the space of square - summable sequences , and the infimum is taken over all strictly causal estimators @xmath80 .",
    "note that in the above , and as done throughout the paper , we assumed @xmath51 .",
    "a simpler problem would be to relax the minimization problem , and obtain a sub - optimal solution , i.e. , given scalar @xmath81 , find estimation strategies @xmath76 that achieve @xmath82 for all @xmath8 .",
    "results from the @xmath0 setting state that the @xmath0 filter ensures the desired attenuation level , @xmath83 , provided that the solution of an associated riccati equation satisfies a suitable feasibility assumption @xcite .",
    "it is also shown in @xcite that the @xmath0 norm of the rls algorithm depends on the input - output data while for the lms and normalized lms algorithms the @xmath0 norm is simply unity @xcite .",
    "the results further show that for the filtered normalized estimation error @xmath84 , where @xmath85 denotes the estimate of @xmath64 , given @xmath86 , the @xmath0 norm of rls and kf is bounded by two .",
    "nevertheless , for the predicted estimation error @xmath87 , which does not have access to current observations , the @xmath0 performance of the rls and kf can be much larger @xcite .",
    "motivated by the similarity between the worst - case ( regret ) analysis in online machine learning and @xmath0 setting in adaptive filtering , kivinen _ et al .",
    "_ @xcite employed techniques of machine learning @xcite and applied them in filtering problem to analyze the lms algorithm by resorting to the study of bregman divergences .",
    "this connection formed also the basis for @xcite , which is related to the present study . to ensure a fair comparison ,",
    "a worst - case bound is derived based on the @xmath0 bound of the kf in @xcite and is compared with the proposed bounds in section iii .",
    "in this section , we analyse the game - theoretic - based setting of the kf .    in many cases ,",
    "@xmath18 ( [ riccati ] ) ( and hence the kalman gain @xmath17 ( [ kt ] ) ) converges to the steady - state value .",
    "the limiting solution @xmath88 will satisfy the following discrete algebraic riccati equation ( dare ) @xmath89 and @xmath90 is the steady - state kalman gain",
    ". we shall be particularly interested in real , symmetric , positive semi - definite solutions of the dare which gives a stable steady - state filter .",
    "[ lem1 ] @xcite provided that @xmath91 is detectable and @xmath92 is stabilizable and @xmath93 , then @xmath94 , @xmath95 exponentially fast where @xmath96 is the solution of the riccati recursion ( [ riccati ] ) and @xmath88 its the unique stabilizing solution of dare ( [ are ] ) .",
    "[ lem2 ] denote the instantaneous drift as @xmath97 for the steady - state kf , the state estimation error @xmath98 propagates according to the linear system @xmath99 with @xmath100 the closed - loop system , driven by the process @xmath101 . if @xmath91 is detectable and @xmath92 is controllable , this closed - loop system is stable .",
    "[ lem3 ] suppose @xmath102 is detectable and @xmath103 is controllable .",
    "let the kf algorithm [ algkf ] be run on the sequence @xmath104 with the initial values @xmath51 and @xmath50 , and generating the estimates @xmath105 .",
    "then for any @xmath106 and for all sequences of targets @xmath107 , one has @xmath108 where @xmath109 with @xmath88 the steady - state value of @xmath18 , and where @xmath110 denotes the largest singular value of its argument .",
    "we start by forming @xmath111 as follows @xmath112 using an upper bound from hassibi and kailath @xcite ( lemma [ lem2a ] in appendix ) , for all @xmath113 we have that @xmath114 now , using lemma [ lem1a ] in appendix we substitute @xmath115 by @xmath116 in the first term to get @xmath117 completing the squares yields @xmath118 next , summing over @xmath119 , and using the telescoping property we get that @xmath120 from which the claim follows .",
    "define @xmath121 where @xmath122 denotes the total drift ( cumulative squared norm of process residuals ) and @xmath123 indicates the cumulative state estimation error .",
    "we use next lemma to upper bound explicitly @xmath123 .    consider the linear system @xmath124 with @xmath125 .",
    "then @xmath126 where @xmath127    for any @xmath128 , one has @xmath129 using the triangular inequality , we get that @xmath130 hence @xmath131 which can be written as @xmath132 where @xmath133 .",
    "next , we use jensen s inequality to bound the second term in ( [ eqeq ] ) as @xmath134 summing over @xmath135 , yields @xmath136 which can be written as @xmath137 using the geometric series convergence in the first term , and change of indexing in the second term , we get @xmath138 the riemann zeta function and the polylogarithmic series are upper bounded as @xmath139 and @xmath140 respectively . substituting ( [ zeta ] ) and ( [ polylog ] ) in ( [ eq : xtilde_t ] )",
    "gives the result .    finally , using the above lemmas",
    ", we prove the main result of this section .",
    "suppose @xmath102 is detectable and @xmath103 is controllable .",
    "let the kf algorithm [ algkf ] be run on the sequence @xmath141 with initial values be chosen as @xmath51 and @xmath50 , generating the estimates @xmath142 .",
    "then for any sequence of targets @xmath143 , one has @xmath144    with @xmath145 is the steady - state kalman gain being given in ( [ kbar ] ) , and where constants @xmath146 , @xmath147 , @xmath148 , and @xmath149 are defined as in ( [ rbar ] ) and ( [ c ] ) , respectively .",
    "from lemma 2 we have @xmath150 . hence @xmath151 using ( [ wbar ] ) in ( [ xtildebound ] ) one obtains @xmath152 now , substituting ( [ xtildebound ] ) in ( [ reg ] ) and setting @xmath153 as ( see proposition 1 in appendix ) @xmath154 gives the result .",
    "one main conclusion is that the constant factor @xmath146 in ( [ b1 ] ) is strictly greater than 1 which implies that the absolute regret @xmath155 depends linearly on @xmath156 .",
    "note that as is implied by the @xmath0 norm lower bound obtained in @xcite , the case @xmath43 ( see remark [ rem2 ] ) is not achievable in the worst - case bound of the kf .",
    "if the cumulative loss @xmath156 is small and if the reference predictor sequence evolves approximately as @xmath157 , then @xmath158 would be small , leading to a vanishing regret bound .",
    "on the other hand , if the cumulative loss @xmath156 is large ( linear in @xmath8 ) and the offline reference predictor is not evolving as @xmath157 , then @xmath122 would be a large quantity and the bound would be @xmath159 .    a number of known / extreme cases can be considered .",
    "for instance , suppose that the process noise @xmath160 equals the state @xmath64 itself .",
    "then , from ( [ wt ] ) we get @xmath161 if @xmath162 is stable , this yields @xmath163 as @xmath164 .",
    "thus the total drift @xmath122 converges to zero and the bound becomes equivalent to the worst - case bound with stationary target .",
    "on the other hand , if @xmath162 is unstable , @xmath64 would grow unbounded , and the total drift @xmath122 will be quite large and one would get a rather large bound indicating a poor performance of the kf .    for the _ special _ case of the rls algorithm where @xmath165 the following worst - case bound obtained by vaits et al .",
    "@xcite @xmath166 with @xmath167 indicating the cumulative loss of any sequence of targets @xmath42 , and where @xmath168 denotes the total drift ( see @xcite , section vi for details ) .",
    "one can observe that the factor 1 multiplying the term @xmath167 , leading to a vanishing tracking regret bound when the total drift is sublinear . on the other hand ,",
    "the rls dependency to the total drift @xmath169  is worse than the kf bound dependency @xmath170 when the cumulative drift is sublinear in @xmath8 .",
    "this section provides a comparison with the @xmath0 setting used in adaptive filtering .",
    "motivated by the assumption that the data is close to the linear time - invariant state - space model ( [ sysdyn ] ) , analyses of @xmath57 estimation aims to bound the maximum energy gain from the unknown disturbances to the state estimation error .",
    "@xcite ( _ @xmath0 norm bound of the kalman filter _ ) consider the system dynamic ( [ sysdyn ] ) . run the kf algorithm 1 and let the initial values be chosen as @xmath51 and @xmath171 . then for any @xmath8 , one has @xmath172    where @xmath146 is defined as in ( [ rbar ] ) , and @xmath79 is the space of square - summable sequences .    for a fair comparison ,",
    "we derive a worst - case loss bound from the @xmath0 norm bound ( [ hinfnormbound ] ) of the kf .    consider the steady - state kf and let the initial values be chosen as @xmath51 and @xmath50 . then for all sequence of observations @xmath173 and for any sequence of targets @xmath174 , the total loss suffered by the algorithm is bounded by @xmath175    where @xmath122 is defined as in ( [ wt ] ) .",
    "we re - write the @xmath0 norm bound ( [ hinfnormbound ] ) as @xmath176 using lemma 5 in appendix for any @xmath113 , one has that @xmath177 summing over @xmath178 , and then substituting ( [ hinf1 ] ) in ( [ eq:40 ] ) , one gets @xmath179 now , setting @xmath153 as @xmath180 gives the result .    comparing the kf bounds in ( [ b1 ] ) and ( [ b3 ] )",
    ", one observes that the multiplicative factors @xmath181 and @xmath182 in the first two terms are worse for the bound ( [ b3 ] ) than for the bound ( [ b1 ] ) which equals @xmath146 .",
    "the kf bound ( [ b1 ] ) has dependency @xmath183 to the cumulative drift while the dependency of the bound ( [ b3 ] ) is @xmath184 .",
    "however , the constant multiplicative factor to the drift , is worse for the bound in ( [ b1 ] ) than the bound in ( [ b3 ] ) .",
    "on the other hand , the bound ( [ b3 ] ) is worse if the cumulative loss @xmath156   is linear in @xmath8 which implies that first and last terms of the bound ( [ b3 ] ) become dominant and hence the multiplicative factors make the bound ( [ b3 ] ) worse than the bound ( [ b1 ] ) .",
    "it is worth mentioning that both bounds derived from a _ minimax _ approach and small bounds are obtained in either case if @xmath158 and the cumulative loss @xmath156 is small .",
    "we finish the paper with two sets of experiments illustrating the use of the worst - case bounds of the kf algorithm . for both experiments the discrete system dynamic matrices @xmath5 and @xmath6",
    "are generated randomly using the _ drss _ command in _",
    "matlab_. we set @xmath185 , @xmath186 , @xmath187 .",
    "the weighting matrices are chosen as @xmath188 and @xmath189 .",
    "the system initial states are set to zero . for choosing @xmath160 , two different cases of linear and sublinear drift",
    "are considered . in the first experiment a sequence of vector @xmath190",
    "is generated from a gaussian distribution for which the instantaneous drift @xmath191 is constant ( linear drift ) , obeying @xmath192 , where @xmath193 .",
    "the second experiment is based on sublinear drift . here",
    ", we use a polynomial decay of the drift , @xmath194 for some @xmath195 . in this case @xmath196 for @xmath197 , and @xmath198 otherwise .",
    "we set @xmath199 so that the sequence of vectors @xmath190 rotating along a unit circle in a rate of @xmath200 ( sublinear drift ) .",
    "1 displays how the behavior of the worst - case loss bounds for the kf algorithm 1 in ( [ b1 ] ) ( kf bound 1 ) and the worst - case loss bound ( [ b3 ] ) obtained form the @xmath0 norm bound ( [ hinfnormbound ] ) ( kf bound 2 ) , when @xmath201 and the total drift is linear ( upper panel ) and sublinear ( lower panel ) .",
    "one can observe that the performance of the algorithm depends on the total amount of drift for which the algorithm performs worse in severe conditions ( linear drift ) .",
    "the worst - case kf - bound 1 has superior performance than kf bound 2 when the total drift is sublinear whereas it has worse performance in case where the total drift is linear .",
    "it should be stressed that the worst - case bounds depend on the specific choice of the design parameters and system dynamic matrices .",
    "the development of the empirical cumulative loss function of the kf algorithm , and empirical cumulative loss function of the sequence of targets is demonstrated in fig .",
    "it is observed that the performance of the kf algorithm converges to the performance of the best sequence of targets when @xmath158 .",
    "[ fig1 ]    [ fig2 ]",
    "this paper studied the worst - case performance of the kf algorithm using novel universal prediction perspective in online machine learning .",
    "tracking worst - case bounds were proved and compared with the bound derived from an @xmath0 setting .",
    "the proposed bounds hold for a wide range of observation models and noise distributions .",
    "the results do not require the data to be linearly related as opposed to @xmath0 setting which is based on the assumption that data obeys a fixed linear dynamical model .",
    "it was shown that the worst - case bounds scales proportional with the deviation of the comparator from a known predetermined dynamical model .",
    "future works will extend the results to more general problem where the target is any arbitrary linear combination of the state .",
    "an interesting open problem is to incorporate a time - varying dynamical model in which case the tracking bounds will scale with the deviation of a reference predictor from the best sequence of dynamical models .",
    "it would also be interesting to extend the results to the case where the dimensionality of the state vector @xmath27 is very high ( e.g. , @xmath202 ) .",
    "straightforward application of previous results will render the bound uninformative since the bound scales up linear in the state dimension .",
    "some recent work has examined the role of high - dimensionality in online rls algorithms @xcite and online mirror descent methods @xcite , under the assumption that the problem has a proper sparsity structure .",
    "a more general approach is to employ random projection technique and the celebrated johnson - lindenstrauss lemma which states roughly that , given an arbitrary set of @xmath203 points in a high - dimentional euclidean space , there exists a linear map of this points in a low - dimensional euclidean space such that all pairwise distances are preserved @xcite , @xcite . the goal would be to analyse the performance of the random projected kf algorithm relative to the performance of the computationaly intractable offline high - dimensional case .            from the riccati recursion ( [ riccati ] ) and using woodbury matrix inversion lemma",
    ", @xmath115 is obtained as @xmath209 where @xmath210 .",
    "since the second term in ( [ eq:11 ] ) is positive semidefinite , one gets @xmath211 this completes the proof .",
    "j. l. maryak , j. c. spall , b. d. heydon , `` use of the kalman filter for inference in state - space models with unknown noise distributions , '' _ ieee trans .",
    "87 - 90 , 2004 ."
  ],
  "abstract_text": [
    "<S> in this paper , we study the prediction performance of the kalman filter ( kf ) in a worst - case , _ minimax _ setting as studied in online machine learning , information - and game theory . </S>",
    "<S> the aim is to predict the sequence of observations almost as well as the best reference predictor ( comparator ) sequence in a comparison class . </S>",
    "<S> we prove worst - case bounds on the cumulative squared prediction errors using a priori knowledge about the complexity of reference predictor sequence . </S>",
    "<S> in fact , the performance of the kf is derived as a function of the performance of the best reference predictor and the total amount of drift occurs in the schedule of the best comparator .    </S>",
    "<S> kalman filter , @xmath0 estimation , online machine learning , tracking worst - case bounds . </S>"
  ]
}