{
  "article_text": [
    "intelligent control using adaptive / approximate dynamic programming ( adp ) , sometimes referred to by reinforcement learning ( rl ) or neuro - dynamic programming ( ndp ) , is a set of powerful tools for obtaining approximate solutions to difficult and mathematically intractable problems which seek optimum while sometimes even no knowledge of the system model / dynamics is available .",
    "the dramatic potential of the tools in practice has attracted many researchers within the last few decades , @xcite-@xcite .",
    "the multitude of appeared papers and success stories on applications of adp to different problems , however , has intensified the need for firm mathematical analyses for guaranteeing the convergence of the learning processes and the stability of the results .    besides the classifications of heuristic dynamic programming ( hdp ) , dual heuristic programming ( dhp ) , etc .",
    "@xcite , which are in terms of the variables subject to approximation and their dependencies , the learning algorithms are typically based on either value iteration ( vi ) or policy iteration ( pi ) , @xcite .",
    "these algorithms are well investigated both by computer scientists for machine learning @xcite and by control scientists for feedback control of dynamical systems @xcite .",
    "pi , despite having a higher computational load due to a ` full backup ' as opposed to a ` partial backup ' in vi @xcite , has the advantage that the control under evolution remains stabilizing , @xcite .",
    "hence , pi seems more suitable for online implementation , i.e. , adapting the control ` on the fly ' .",
    "however , the requirement that pi needs to start with an stabilizing initial control is one of its drawbacks .",
    "vi , on the other hand , does not require an stabilizing initial control and can be initiated arbitrarily .",
    "but , the closed loop system is not guaranteed to be stable during its learning process , if implemented online .",
    "considering optimal control of discrete - time problems with continuous state and action spaces and undiscounted cost functions using vi , which is the subject of this work , the convergence proof for linear systems was analyzed in @xcite .",
    "as for nonlinear systems , the convergence was established by different researchers including @xcite ( adapted in @xcite ) , @xcite , and @xcite through different approaches .",
    "all these convergence analyses are based on the assumption of perfect function reconstruction , i.e. , no error in the function approximation .",
    "while this assumption plays a major rule in deriving the results , it restricts their practical use severely , because , the approximation errors exist almost in every application when the system is nonlinear or when the cost function terms are non - quadratic and nonlinear .",
    "what makes their presence potentially problematic is the fact that the errors _ propagate _ throughout the iterations , hence , regardless of how small they are , a phenomenon similar to _ resonance _ might happen which could lead to the complete unreliability of the results .",
    "analyzing vi under the presence of approximation errors , i.e. _ approximate vi _ ( _ avi _ ) , is an open research problem with a few published results , including @xcite , to the best of the knowledge of the author",
    "@xcite investigated problems with _ discounted _ cost functions and the results are solely valid for such problems , prevalent in computer science .",
    "as a matter of fact , the ` forgetting ' nature of discounted problems is the backbone of the developments of the error bounds and if the discount factor approaches one , as in typical infinite - horizon optimal control problems , the bounds go to infinity .",
    "hence , the results do not cover this case . on the other hand ,",
    "the interesting results in @xcite provide some error analyses but with assumptions which are more restrictive and not easily verifiable , compared with this study . for example , the approximation error between the exact and approximate functions , respectively denoted with @xmath0 and @xmath1 , should be possible to be written in the _ multiplicative _ form of @xmath2 for some positive constant @xmath3 , instead of an _ additive _ form of @xmath4 , for some real valued function @xmath5 .",
    "moreover , the boundedness results are conditional upon @xmath6 being upper bounded with a term including a parameter which corresponds to the _ optimal _ value function .    based on this background , besides the stability issue during the online learning stage using vi , rigorous theoretical analyses of the consequences of the errors on the results are of great interest to the adp researchers and practitioners .",
    "the reasons are the scarcity of the available studies on avi , the prevalence of approximation errors , and the great potential of the tool in ( approximately ) solving optimal control problems in practice .",
    "the contributions of this study are multiple .",
    "initially , it is proved that vi also will be stabilizing for online control if , similar to pi , it is started using an initial stabilizing control .",
    "afterwards , it is shown that the start from an initial stabilizing control leads to an initial value function that does not satisfy the necessary conditions for any of the cited convergence proofs of vi . establishing this convergence ( to the optimal solution )",
    "is another contribution of this work .",
    "these results may not look substantially different from what the adp community assumes to hold intuitively or has already established , @xcite .",
    "what makes the abovementioned two results different is having two characteristics .",
    "the theoretical analyses in this study are simple and straight forward , both for optimality and stability analyses , compared to the existing developments in the literature .",
    "another feature is providing rigorous mathematical bases for the analyses . as an example , the use of a value function as a lyapunov function for stability analysis requires proof of continuity of the value function , @xcite .",
    "the firm proof of this continuity , presented in this work , is not as straight forward as it looks .",
    "the factors leading to the difficulty are the presence of the @xmath7 operator in calculation of the control at each iteration of vi , which may potentially lead to a discontinuity in the control policy , and also the concern of _ pointwise _ versus _ uniform _ convergence , for concluding the continuity of the limit function from the continuity of the elements of a converging sequence of functions , @xcite , @xcite .",
    "another contribution of this work is addressing the legitimate concern that any adp result is valid only when the state trajectory remains within the domain for which the controller is trained .",
    "this concern is resolved through establishing an _ estimation of the region of attraction _ ( eroa )",
    "@xcite for the controller , in this work , so that as long as the system s initial condition is within the region , it is guaranteed that the entire trajectory remains in the region . hence , the controller will remain valid and usable .    as the reader delves into the problem , it is discussed that the provided stability proof , whose main idea is not much different from @xcite , assumes applying a fixed ( time - invariant ) control policy on the system .",
    "but , this is rarely the case in online learning , since , as the learning proceeds , the control policy _ evolves _ , hence , the applied control policy is time - varying .",
    "therefore , another set of stability results for the time - varying and evolving control policy is developed with some ideas for establishing its respective eroa , as another contribution of this work .",
    "after providing the detailed analysis of the vi which is initiated with an admissible guess , called stabilizing vi throughout the paper , the case of presence of the approximation errors is investigated , leading to a new set of contributions which are of greater interests .",
    "they include boundedness / convergence analysis of the avi initiated with an admissible guess , stability and eroa analysis for the case of applying a fixed control policy , and stability and eroa analysis for the case of applying an evolving control policy .",
    "these theoretical analyses are the most important contributions of this work , as the assumptions leading to the results are verifiable and more straight forward , compared with the available studies . finally ,",
    "interested readers are referred to @xcite for some recent developments of this author on analyzing the effect of the approximation errors in regular value iteration , i.e. , the approximate vi which is initiated arbitrarily . and theorem [ thm_boundedness ] .",
    "]    the rest of this paper is organized as follows .",
    "the problem is formulated in section ii and the adp - based solutions are revisited in section iii .",
    "section iv presents the theoretical analyses on exact vi .",
    "the respective analyses for avi are presented in section v. finally , concluding remarks are given in section vi .",
    "let the system subject to control be given by discrete - time nonlinear dynamics @xmath8 where @xmath9 is a lipschitz continuous function versus its both inputs , i.e. , the state and control vectors , @xmath10 and @xmath11 , respectively , with @xmath12 .",
    "the set of non - negative integers is denoted with @xmath13 , and positive integers @xmath14 and @xmath15 denote the dimensions of the continuous state and control spaces . finally , sub - index @xmath16 represents the discrete time index .",
    "the performance index is given by @xmath17 where _ utility function _",
    "@xmath18 is of form @xmath19 for a continuous and positive semi - definite function @xmath20 and a positive definite @xmath21 real matrix @xmath22 .",
    "set @xmath23 denotes the non - negative reals . starting with any initial feedback _ control policy _ given by @xmath24 for control calculation , i.e. , @xmath25 , the problem is _ updating / adapting _ the control policy such that cost function ( [ costfunction ] ) is minimized . the control policy which leads to such a characteristic",
    "is called _ optimal control policy _ , denoted with @xmath26",
    ".    [ asymstability_definition ] a control policy is defined to be asymptotically stabilizing within a domain if @xmath27 using this control policy , for every initial state within the domain , @xcite .    [ def1 ]",
    "a control policy @xmath28 is defined to be _",
    "admissible _ within a compact set if a ) it is a lipschitz continuous function of @xmath10 in the set with @xmath29 , b ) it asymptotically stabilizes the system within the set , and c ) there exists a continuous positive definite function @xmath30 that puts an upper bound on the respective _ ` cost - to - go ' _ or _ ` value function ' _ , denoted with @xmath31 and defined by @xmath32 i.e. , @xmath33 . in eq .",
    "( [ valuefunction_of_h ] ) one has @xmath34 and @xmath35 . in other words , @xmath36 denotes the @xmath16th element on the state trajectory / history initiated from @xmath37 and propagated using control policy @xmath28 .",
    "the main difference between the defined admissibility and the ones typically utilized in the adp / rl literature , including @xcite , is the assumption of upper boundedness of the value function by a continuous ( positive definite ) function .",
    "this condition is trivially satisfied if the value function itself is continuous , i.e. , through selecting @xmath38 .",
    "however , instead of assuming continuity of the value function , the milder condition of being upper bounded by such a function is assumed .",
    "note that the continuity of the value function is required for _ uniform _ approximation of the function using parametric function approximators , @xcite . in this study",
    ", it will be shown that the upper boundedness will lead to the desired continuity .",
    "finally , it should be noted that continuous functions are bounded in a compact set @xcite , hence , the upper boundedness of the value function by the continuous function @xmath39 leads to the boundedness of the respective value function .",
    "this is an essential requirement for an admissible control , as a mere asymptotically stabilizing control policy may lead to an unbounded value function .",
    "[ assum_existingadmissiblecont ] there exists at least one admissible control policy for the given system within a connected and compact set @xmath40 containing the origin .    [ assum_invariantset ] the intersection of the set of n - vectors @xmath10 at which @xmath41 with the invariant set of @xmath42 only contains the origin .",
    "assumption [ assum_existingadmissiblecont ] guarantees that there is no state vector in @xmath43 for which the value function associated with the _ optimal _ control policy is infinite .",
    "assumption [ assum_invariantset ] assures that there is no set of states ( besides the set containing only the origin ) in which the state trajectory can _ hide _ forever , in the sense that the utility function evaluated at those states is zero without convergence of the states to the origin .",
    "note that , if such a set exists , then starting from an initial state within the set , the optimal solution would be @xmath44 .",
    "based on eq . ( [ valuefunction_of_h ] )",
    ", it can be seen that the value function satisfies the recursive relation given by @xmath45 defining the _ optimal value function _ , as the value function associated with the optimal control policy and denoting it with @xmath46 , the bellman equation @xcite , given below , provides the solution to the problem @xmath47 @xmath48 due to the _ curse of dimensionality _",
    "@xcite , however , the proposed solution is mathematically impracticable for general nonlinear systems .",
    "adp utilizes the idea of _ approximating _ the optimal value function , using either look - up tables or function approximators , e.g. , neural networks ( nns ) , for remedying the problem .",
    "the value function approximator is typically called the _",
    "critic _ in the adp / rl literature .",
    "the approximation is performed over a _",
    "compact _ and _ connected _ set containing the origin , called the _ domain of interest_. this domain , denoted with @xmath43 , has to be selected based on the specific problem at hand and it should be noted that the adp based results are valid only if the entire state trajectory initiated from the initial state vector remains within the domain for which the value function is approximated .",
    "the optimal value function approximation process is typically done through pi or vi . in pi ,",
    "one starts with an initial admissible control policy , denoted with @xmath49 , and iterates through the _ policy evaluation equation _ given by @xmath50 and the _ policy update equation _ given by @xmath51 for @xmath52 until the parameters converge .",
    "in other words , starting from policy @xmath49 , pointwise values for approximating value function @xmath53 can be calculated using ( [ pi_policyeval ] ) and once @xmath53 is used in ( [ pi_policyupdate ] ) pointwise values for approximating @xmath54 can be calculated , and so on .    on the other hand , in vi ,",
    "the iterative learning starts with an initial guess @xmath53 and iterates through the _ policy update equation _ given by @xmath55 and the _ value update equation _ @xmath56 or equivalently @xmath57 for @xmath52 until the iterations converge .",
    "pi requires an admissible policy @xmath49 to start the process with , otherwise , there may not exist a bounded ( and continuous ) function @xmath53 which satisfies eq .",
    "( [ pi_policyeval ] ) for @xmath58 .",
    "this can be observed by realizing that @xmath53 is actually the value function associated with policy @xmath49 . to confirm this one may compare eq .",
    "( [ pi_policyeval ] ) with eqs .",
    "( [ recursive_costtogo ] ) and ( [ valuefunction_of_h ] ) .",
    "an important feature of pi is the fact that @xmath59s remain lyapunov functions for the closed loop system , hence , each respective control policy will be stabilizing .",
    "this can be confirmed by noting that from eq .",
    "( [ pi_policyeval ] ) one has @xmath60 this feature leads to its suitability for _ online _ implementation , i.e. , for _ adaptive _ optimal control .",
    "it is an important point that _ online _ learning has the advantage of not requiring perfect knowledge of the internal dynamics of the system @xcite .",
    "however , stability of the system operated using the evolving control is of critical importance .    the vi scheme ,",
    "however , can be arbitrarily initiated using any @xmath53 . the convergence to the optimal solution is proved for @xmath61 in @xcite and for any smooth @xmath53 which satisfies @xmath62 in @xcite .",
    "moreover , the convergence is proved in @xcite for @xmath63 , \\forall x,$ ] assuming there exists a finite @xmath64 , independent of @xmath10 , such that @xmath65 .",
    "vi is more interesting for control of nonlinear systems as the convergence to optimal solution is guaranteed without requiring an initial admissible control , and also , instead of solving equation ( [ pi_policyeval ] ) , known as a ` full backup ' , the ` partial backup ' given by the simple recursion ( [ vi_valueupdate2 ] ) is needed , @xcite .",
    "however , these advantages come with the disadvantage that the ` immature ' control , i.e. , @xmath66s before the convergence of the solution , are not guaranteed to be stabilizing . this issue is generally considered as a shortcoming of vi as opposed to pi in the adp literature , see @xcite as an example . in this study",
    ", it will be shown that the credit for stabilizing feature of @xmath66s in pi is due to the initial admissible control and if vi also is initiated with such a control policy , the resulting control policies under iterations will be stabilizing .",
    "let the initial guess , @xmath53 , to be used in vi , be given by the value function of an admissible control policy .",
    "for the sake of brevity , such a vi is called _ stabilizing value iteration _ , throughout the paper , as defined in the next definition .",
    "[ stabilizingvi_definition ] the value iteration scheme given by recursive relation ( [ vi_valueupdate ] ) which is initiated using the value function of an admissible control policy is called stabilizing value iteration .    denoting the initial admissible policy with @xmath67 ( for notational compatibility ) ,",
    "its value function , denoted with @xmath53 , can be calculated through solving @xmath68 per ( [ recursive_costtogo ] ) . utilizing this @xmath53 in the vi as the initial guess ,",
    "the stability of @xmath66s can be guaranteed , as proved here , which requires some theoretical results given next .    [ lemma_nondecreasing ] sequence of functions @xmath69 resulting from stabilizing value iteration",
    "is a pointwise non - increasing sequence .",
    "_ proof _ : the proof is done by induction . considering ( [ vi_value_eq1 ] ) , which gives @xmath53 , and ( [ vi_valueupdate ] ) , which ( for @xmath58 ) gives @xmath70 , one has @xmath71 because @xmath70 is the result of minimization of the right hand side of ( [ vi_value_eq1 ] ) instead of being resulted from a given @xmath67 . now , assume that for some @xmath72 , we have @xmath73 define @xmath74 as @xmath75 comparing ( [ lemma1_eq3 ] ) with ( [ vi_valueupdate ] ) , one has @xmath76 because @xmath77 is the result of minimization of the right hand side of ( [ lemma1_eq3 ] ) .",
    "moreover , @xmath78 is given by @xmath79 based on ( [ vi_valueupdate2 ] ) .",
    "hence , @xmath80 because of ( [ lemma1_eq2 ] ) . considering ( [ lemma1_eq4 ] ) and ( [ lemma1_eq6 ] ) , one has @xmath81 which together with ( [ lemma1_eq2 ] ) , proves the lemma , by induction .    before proceeding to the stability theorem",
    ", it is needed to analyze the continuity of each @xmath59 within @xmath43 .",
    "note that even though functions @xmath82 and @xmath18 are assumed to be continuous versus all the inputs , the existence of @xmath7 operator in eq .",
    "( [ vi_policyupdate ] ) may lead to discontinuity in function @xmath66 , which may then lead to a discontinuous @xmath77 in eq .",
    "( [ vi_valueupdate2 ] ) . besides the @xmath7 issue",
    ", does the continuity of @xmath67 lead to the continuity of its value function , @xmath53 ?",
    "note that @xmath53 is going to be the initial guess in the stabilizing vi , hence , its continuity matters .",
    "moreover , how does one find the value function of a given @xmath67 ?",
    "the answer to these questions are given first .",
    "let @xmath83 ( respectively , @xmath84 ) denote that function @xmath0 is continuous at point @xmath85 ( respectively , within @xmath43 ) .",
    "[ lemma_conv_v0 ] if @xmath28 is an admissible control policy within @xmath43 , then selecting any @xmath86 which satisfies @xmath87 , the iterations given by @xmath88 converges monotonically to the value function of @xmath28 .",
    "_ proof _ : it is known that value function @xmath89 is the _ fixed point _ of iterations indexed by @xmath90 and given by ( [ pi_value_eq2 ] ) , @xcite .",
    "the reason is eq .",
    "( [ pi_value_eq2 ] ) is equivalent of @xmath91 comparing ( [ v0_conv_lem_eq1 ] ) with ( [ valuefunction_of_h ] ) and considering @xmath92 one has @xmath93 .",
    "therefore , sequence @xmath94 is upper bounded by @xmath95 .",
    "the limit function @xmath96 is equal to @xmath89 , since , the admissibility of @xmath28 leads to @xmath97 , and hence , @xmath98 as @xmath99 , due to @xmath92 .",
    "hence , ( [ v0_conv_lem_eq1 ] ) converges to ( [ valuefunction_of_h ] ) as @xmath99 .",
    "this proves _",
    "pointwise _ convergence of the sequence to @xmath89 .    as for monotonicity of this convergence , not that for any arbitrary positive integers",
    "@xmath100 and @xmath101 , if @xmath102 , then @xmath103 since @xmath104 , and the last term in the foregoing inequality is only one of the non - negative terms in the summation in the right hand side of ( [ pi_value_eq3_2 ] ) .",
    "therefore , sequence of functions @xmath105 is pointwise non - decreasing .",
    "while the foregoing lemma helps in finding the value function of a given control policy , it does not prove the possible continuity of the result .",
    "the reason is , even though @xmath106 is continuous for any @xmath107 as a finite sum of continuous functions given in ( [ v0_conv_lem_eq1 ] ) , the limit function may not be continuous , as only the _ pointwise _ convergence is proved . an idea for proof of uniform continuity can be adapted from @xcite , which is based on @xcite , and hence , requires @xmath108 to hold uniformly for a constant @xmath64 .",
    "however , the foregoing condition restricts the generality of the result .",
    "besides , it is not applicable to the assumed case of positive semi - definite @xmath109 , because , the value function is positive definite .",
    "the following lemma pursues another idea to this end .",
    "[ lemma_cont_v0 ] if @xmath28 is an admissible control policy within @xmath43 , then @xmath110 .",
    "_ proof _ : the proof is done by contradiction .",
    "assume that @xmath89 is discontinuous at some @xmath111 .",
    "then @xmath112 where @xmath113 denotes a vector norm and ` : ' denoted ` such that ' .",
    "the idea is showing that ( [ lem_cont_v0 ] ) is not possible . to this end , considering recursive relation ( [ pi_value_eq2 ] ) initiated with @xmath114 , from ( [ valuefunction_of_h ] ) and ( [ v0_conv_lem_eq1 ] ) one has @xmath115 therefore , @xmath116 which leads to @xmath117 by triangle inequality of absolute values .",
    "inequality ( [ lem_cont_v3 ] ) is the key to the solution , as it will be shown that the right hand side of the inequality can be made arbitrarily small if @xmath37 is close enough to @xmath118 and @xmath90 is large enough . by @xmath119 , for some @xmath120 , per the admissibility of @xmath28 , one has @xmath121 by admissibility of @xmath28 one has @xmath122 as @xmath99 . hence , by @xmath123 and @xmath124 , one has @xmath125 moreover , by @xmath123 @xmath126 hence , @xmath127 on the other hand , due to the lipschitz continuity of the closed loop system , the state trajectory at each finite time , for example @xmath90 , _ continuously _ depends on the initial conditions , @xcite .",
    "in other words , @xmath128 changes continuously as @xmath37 changes , hence , @xmath129 note that @xmath130 , for @xmath107 as mentioned before this lemma .",
    "hence , @xmath131 now we have enough inequalities to contradict ( [ lem_cont_v0 ] ) . for any point of discontinuity @xmath118 and @xmath132",
    "whose existence is guaranteed by ( [ lem_cont_v0 ] ) , find @xmath133 which leads to @xmath134 per ( [ lem_cont_v4 ] ) . then , select @xmath135 . per ( [ lem_cont_v6 ] ) and ( [ lem_cont_v9 ] )",
    "one has @xmath136 select @xmath137 to have @xmath138 per ( [ lem_cont_v7 ] ) . finally , set @xmath139 to have @xmath140 per ( [ lem_cont_v8 ] ) .",
    "select @xmath141 . using ( [ lem_cont_v9 ] ) , ( [ lem_cont_v10 ] ) , ( [ lem_cont_v11 ] ) , and ( [ lem_cont_v12 ] ) one has @xmath142 which contradicts ( [ lem_cont_v0 ] )",
    ". therefore , @xmath143 .",
    "even though we managed to skip the proof of uniform convergence for deriving the desired continuity result , the uniformness of the convergence of ( [ pi_value_eq2 ] ) , if established , will still be useful .",
    "the reason is , the uniform convergence can be used in finding the value function of an admissible control through guaranteeing that there exists a large enough iteration index such that one has @xmath144 for any selected _ constant _ tolerance @xmath145 .",
    "this condition can be used for terminating the iterations .",
    "the next lemma proves the desired uniform convergence .",
    "[ lemma_cont_w_vs_v ] if @xmath28 is an admissible control policy within compact set @xmath43 , then selecting any @xmath86 which satisfies @xmath87 , the iterations given by ( [ pi_value_eq2 ] ) converges uniformly in @xmath43 .",
    "_ proof _ : using dini s uniform convergence theorem ( ref .",
    "@xcite , theorem 7.13 ) , the pointwise monotonicity of @xmath105 ( lemma [ lemma_conv_v0 ] ) , the continuity of the elements of the foregoing sequence , the continuity of the limit function @xmath89 ( lemma [ lemma_cont_v0 ] ) , and the compactness of @xmath43 , lead to the uniform convergence of the iterations .",
    "now that the concern about continuity of @xmath89 is resolved , the next step is a lemma which proves that the @xmath7 operator will not cause discontinuity in the functions subject to investigation .",
    "[ lemma_cont_w_vs_v ] let @xmath146 and @xmath147 .",
    "if functions @xmath148 and @xmath59 are continuous within @xmath43 , then so is @xmath149 .",
    "_ proof _ : the proof is done by showing that the directional limit of @xmath149 at any selected point is equal to its evaluation at the point , and hence , it is continuous at that point ( motivated by @xcite ) .",
    "let @xmath150 be an arbitrary point in @xmath43 .",
    "set @xmath151 select an open set @xmath152 such that @xmath150 belongs to the boundary of @xmath153 and limit @xmath154 exists . if @xmath155 , for every such @xmath153 , then @xmath156 . in this case",
    "the continuity of @xmath149 at @xmath150 follows from the continuity of its forming functions , @xcite .",
    "now assume @xmath157 , for some @xmath153 denoted with @xmath158 .",
    "from @xmath159 for the given @xmath160 , one has @xmath161 if it can be shown that , for every selected @xmath158 , one has @xmath162 then the continuity of @xmath149 at @xmath150 follows , because from ( [ lem3_eq4 ] ) and ( [ lem3_eq5 ] ) one has @xmath163 and ( [ lem3_eq6 ] ) leads to the continuity by definition , @xcite .    the proof that ( [ lem3_eq5 ] ) holds is done by contradiction .",
    "assume that for some @xmath150 and some @xmath158 one has @xmath164 inequality ( [ lem3_eq9 ] ) leads to @xmath165 .",
    "but , this is against ( [ lem3_eq1 ] ) , hence , ( [ lem3_eq9 ] ) can not hold .",
    "now , assume @xmath166 hence there exists some @xmath167 such that @xmath168 then , due to the continuity of both sides of ( [ lem3_eq7_1 ] ) at @xmath150 for the fixed @xmath169 and @xmath160 , there exists an open set @xmath170 containing @xmath150 , see fig .",
    "[ fig_alpha_gamma_sets ] , and some @xmath171 , such that @xmath172    r0.15        given @xmath173 , inequality ( [ lem3_eq8 ] ) implies that at points which are _ close enough _ to @xmath150 , function @xmath174 is away from @xmath175 at least by a margin of @xmath176 .",
    "but , this contradicts eq .",
    "( [ lem3_eq2 ] ) which , implies that @xmath177 can be made arbitrarily close to @xmath160 as @xmath10 gets close to @xmath150 within @xmath158 .",
    "the reason is , the latter , given the continuity of @xmath178 versus both @xmath10 and @xmath11 , leads to the conclusion that function @xmath174 can be made arbitrarily close to @xmath175 if @xmath10 approaches @xmath150 from a certain direction .",
    "note that sets @xmath170 and @xmath158 are not disjoint , as @xmath150 is _ within _ @xmath170 and on the _ boundary _ of @xmath158 , as shown in fig .",
    "[ fig_alpha_gamma_sets ] .",
    "hence , inequality ( [ lem3_eq7 ] ) also can not hold .",
    "therefore , ( [ lem3_eq5 ] ) holds and hence , @xmath179 .",
    "finally , the continuity of the function subject to investigation at any arbitrary @xmath180 , leads to the continuity of the function in @xmath43 .",
    "now , we have all the required tools to make the following desired conclusion .",
    "[ thm_cont_vi ] the value functions at each iteration of stabilizing value iteration are continuous functions , i.e. , @xmath181 .",
    "_ proof _ : the theorem can be proved by induction .",
    "lemma [ lemma_cont_v0 ] proves @xmath182 .",
    "assume that @xmath183 .",
    "from lemma [ lemma_cont_w_vs_v ] it follows that @xmath184 , because , @xmath185 where @xmath186 is defined in lemma [ lemma_cont_w_vs_v ] .",
    "the proof of continuity is of interest for two reasons .",
    "1 ) to guarantee suitable approximation capability , especially in generalization , i.e. , approximating the function at the sample states which were not used during the training stage , @xcite , and 2 ) for using the value functions as lyapunov functions , for proof of stability , as given next .",
    "[ thm_stab_lyapfun ] let the compact domain @xmath187 for any @xmath188 be defined as @xmath189 and let @xmath190 be ( the largest @xmath191 ) such that @xmath192 . then , for every given @xmath193 , control policy @xmath194 resulting from stabilizing value iteration asymptotically stabilizes the system about the origin and @xmath195 will be an estimation of the region of attraction for the system .",
    "_ proof _ : the proof is done by showing that @xmath78 is a lyapunov function for @xmath194 , for each given @xmath72 . denoting the value function of the initial stabilizing control with @xmath53 , it is continuous ( lemma [ lemma_cont_v0 ] ) and positive definite , by positive semi - definiteness of @xmath18 and assumption [ assum_invariantset ] .",
    "note that , there is no @xmath196 with the value function of zero under any control policy .",
    "if @xmath59 for some @xmath72 is positive definite , it directly follows from ( [ vi_valueupdate2 ] ) that @xmath77 will also be positive definite , because , if @xmath197 for some @xmath196 , then @xmath198 by assumption [ assum_invariantset ] .",
    "hence , by induction , @xmath77 is positive definite for every @xmath193 .",
    "also , by theorem [ thm_cont_vi ] it is a continuous function in @xmath43",
    ".    by ( [ vi_valueupdate2 ] ) @xmath199 on the other hand , by lemma [ lemma_nondecreasing ] , inequality ( [ lemma1_eq7 ] ) holds for all @xmath72s .",
    "therefore , replacing @xmath200 in ( [ thm1_eq1 ] ) with @xmath201 leads to @xmath202 let @xmath203 . the right hand side of ( [ thm1_eq2 ] ) can be zero only if @xmath204 . since , by assumption [ assum_invariantset ] , no non - zero state trajectory can stay in @xmath205 , the asymptotic stability of @xmath66 follows from negative semi - definiteness of the difference between the value functions in ( [ thm1_eq2 ] ) , using lasalle s invariance theorem , @xcite .",
    "set @xmath195 is an eroa @xcite for the closed loop system , because , @xmath206 by ( [ thm1_eq2 ] ) , hence , @xmath207 leads to @xmath208 . finally ,",
    "since @xmath209 is contained in @xmath43 , it is bounded . also , the set is closed , because , it is the _ inverse image _ of a closed set , namely @xmath210 $ ] under a continuous function ( theorem [ thm_cont_vi ] ) , @xcite .",
    "hence , @xmath209 is compact .",
    "the origin is an _ interior _",
    "point of the eroa , because @xmath211 , @xmath212 , and @xmath213 .",
    "theorem [ thm_stab_lyapfun ] proves that each single @xmath66 if _ constantly _ applied on the system , will have the states converge to the origin .",
    "however , in online learning , the control will be subject to adaptation .",
    "in other words , if @xmath66 is applied at the current time , control policy @xmath214 will be applied at the next time - step .",
    "it is important to note that even though theorem [ thm_stab_lyapfun ] proves the asymptotic stability of the _ autonomous _ system @xmath215 for every fixed @xmath72 , it does not guarantee the asymptotic stability of the _ non - autonomous _",
    "system @xmath216 .",
    "therefore , it is required to have a separate stability analysis to show that the trajectory formed under the _ adapting / evolving _ control policy also will converge to zero .",
    "an idea for doing that is finding a _ single _ function , possibly @xmath53 , to be a lyapunov function for _ all _ the control policies .",
    "the proof of the following theorem , however , uses another approach .",
    "[ thm_stabil_evi_no_lyap ] if the system is operated using control policy @xmath217 at time @xmath16 , that is , the control subject to adaptation in the stabilizing value iteration , then , the origin will be asymptotically stable and every trajectory contained in @xmath43 will converge to the origin .",
    "_ proof _ : eq .",
    "( [ vi_valueupdate2 ] ) and the monotonicity feature established in lemma [ lemma_nondecreasing ] lead to @xmath218 and similarly @xmath219 let @xmath220 for @xmath221 and @xmath222 . evaluating ( [ thm_stab_nolyap_2 ] ) at @xmath223 and replacing the @xmath224 in the left hand side of the inequality in ( [ thm_stab_nolyap_1 ] ) with the left hand side of ( [ thm_stab_nolyap_2 ] ) , which is smaller per ( [ thm_stab_nolyap_2 ] ) , one has @xmath225 repeating this process by replacing @xmath226 in ( [ thm_stab_nolyap_3 ] ) using @xmath227 leads to @xmath228 similarly by repeating this process one has @xmath229 since @xmath230 , the foregoing equations leads @xmath231 that is , the sequence of partial sums of the left hand side is upper bounded and because of being non - decreasing , it converges , as @xmath232 , @xcite",
    ". therefore , @xmath233 as @xmath232 .",
    "considering assumption [ assum_invariantset ] , this leads to @xmath234 , as long as the entire state trajectory is contained in @xmath43 .",
    "it can be seen that theorem [ thm_stabil_evi_no_lyap ] does not provide an eroa .",
    "therefore , the training domain @xmath43 needs to be selected large enough to guarantee that states , on their way of traveling toward the origin ( with not a necessarily straight path ) do not exit @xmath43 .",
    "however , once the convergence of the value iteration is established , some analytical results regarding the desired eroa will be presented ( theorem [ thm_roa_evolving_vi ] ) .    besides stability , which is addressed",
    ", the convergence of the vi using the stabilizing initial guess also needs to be analyzed .",
    "the reason is , none of the cited existing convergence proofs is applicable , as they either require @xmath61 @xcite , or @xmath62 @xcite .",
    "for example , @xmath62 does not hold here because @xmath235 , which is greater than or equal to @xmath236 , is only one of the terms existing in the summation over infinite number of non - negative terms in the definition of @xmath237 as a value function of state @xmath10 . as for the convergence result in @xcite",
    "whose less restrictive version was presented in @xcite , it requires @xmath238 to hold uniformly .",
    "this condition is restrictive and not applicable to our analysis , as detailed before lemma [ lemma_cont_v0 ] .    the main idea for the convergence proof in here",
    "is adapted from @xcite , in which , an analogy was established between the _ iterations _ of the vi and the _ horizon length _ of a finite - horizon optimal control problem with a _",
    "fixed final time_. let the cost function for the finite - horizon problem be given by @xmath239 where @xmath240 is a continuous and positive semi - definite function representing the _ terminal cost_. the finite - horizon problem is defined as minimizing @xmath241 subject to the dynamics given by ( [ dynamics ] ) . once the final time is fixed , the value function and the control policy become time - dependent @xcite , i.e. , they may be denoted with @xmath242 and @xmath243 , respectively , where the second argument is the number of remaining time steps , or _",
    "time - to - go_. let the optimal finite - horizon value function given state @xmath37 and time - to - go @xmath244 be denoted by @xmath245 , where @xmath246 @xmath247 such that @xmath248 and @xmath249 .",
    "in other word , the summation is evaluated along the trajectory generated by applying the time varying control policy @xmath250 at time @xmath16 .",
    "clearly @xmath251 and by the bellman equation for fixed - final - time problems @xcite @xmath252 and @xmath253 if @xmath254 in the finite - horizon problem is selected equal to initial guess @xmath53 in the vi , then , comparing eq .",
    "( [ bellman_finhor_eq2 ] ) with ( [ vi_valueupdate ] ) it directly follows that @xmath255 in other words , the _ immature _ value function at the @xmath72th iteration of vi is identical to the _ optimal _ value function of the fixed - final - time problem of minimizing ( [ costfunction_finhor ] ) with the final time of @xmath72 , when @xmath256 . similarly comparing ( [ bellman_finhor_eq2_2 ] ) with ( [ vi_policyupdate ] )",
    "it can be seen that @xmath257 using this idea , it is proved in @xcite that if @xmath53 is smooth and @xmath258 then vi converges to @xmath46 .",
    "the following theorem generalizes the convergence proof to cover the case of @xmath53 being a value function .    before proceeding to the theorem , however , it is noteworthy that considering eq .",
    "( [ finhor_vs_vi_eq3 ] ) , the stability results given by theorem [ thm_stab_lyapfun ] resembles a method of stability proof in the model predictive control ( mpc ) literature @xcite , in which , a control lyapunov function is utilized as the terminal cost in the respective finite - horizon problems .",
    "using this idea , it is shown that the closed loop system , under each control calculated for the receding horizons , remains stable .",
    "[ theorem_vi_convergence ] the stabilizing value iteration converges to the optimal solution of the infinite - horizon problem within the selected compact domain .",
    "_ proof _ : considering the analogy between the iteration of vi and the horizon of a finite - horizon problem given by ( [ finhor_vs_vi_eq3 ] ) , it can be seen that each @xmath259 represents the cost - to - go of applying control sequence @xmath260 for the first @xmath72 steps , which are the optimal control sequence with respect to cost function ( [ costfunction_finhor ] ) , when @xmath256 and @xmath261 , and applying the stabilizing control sequence @xmath262 for the rest of the ( infinite ) horizon .",
    "the reason for this conclusion is the fact that @xmath263 , which is used as the terminal cost in the fixed - final - time problem itself represents the cost - to - go of applying admissible control policy @xmath67 for infinite number of times , starting from @xmath264 , per eq .",
    "( [ valuefunction_of_h ] ) .    on the other hand , the non - increasing ( cf .",
    "lemma [ lemma_nondecreasing ] ) and non - negative ( cf .",
    "proof of theorem [ thm_stab_lyapfun ] ) nature of value functions under vi , and hence , of the finite - horizon value function @xmath265 lead to the convergence of the sequence of value functions to a finite limit function , denoted with @xmath266 . because , every non - increasing and lower bounded sequence converges , @xcite .",
    "therefore , one has @xmath267 using control sequence @xmath260 .",
    "otherwise , @xmath268 becomes unbounded .",
    "this can also be concluded by noting that the tail of a convergent series can be made arbitrarily small ( cf .",
    "note that per assumption [ assum_invariantset ] the state trajectory can not hide in the invariant set of @xmath42 with zero utility function , to lead to a finite cost - to - go without convergence to the origin .",
    "due to the continuity and positive semi - definiteness of @xmath53 , one has @xmath269 as @xmath270 . therefore , by eq .",
    "( [ thm2_eq1 ] ) one has @xmath271 in calculation of the cost - to - go @xmath265 . comparing finite - horizon cost function ( [ costfunction_finhor ] ) with infinite - horizon cost function ( [ costfunction ] ) and considering ( [ thm2_eq2 ] )",
    ", one has @xmath272 otherwise , the smaller value among @xmath273 and @xmath274 will be both the optimal value function ( evaluated at @xmath10 ) for the infinite - horizon problem and the greatest lower bound of the sequence of value function of the fixed - final - time problems resulting from @xmath275 .    comparing the results",
    "given by theorems [ thm_stab_lyapfun ] and [ theorem_vi_convergence ] with the existing literature , the closest one is @xcite , in which a vi algorithm , called @xmath276-adp , was introduced .",
    "the point that @xmath276-adp requires to be initiated from a control lyapunov function ( clf ) of the respective system in order for the control under iterations to be stabilizing corresponds to the required initial admissible guess for vi in this study .",
    "the reason is , if the clf is known , an asymptotically stable control can be derived directly from the function , e.g. , using sontag s formula , @xcite .",
    "an asymptotically stable control law , however , is not required to lead to a finite cost - to - go . hence , at the first glance , the condition in @xmath276-adp seems to be less restrictive compared to the condition of using admissible initial control in this study .",
    "however , once the second requirement of @xmath276-adp , that is the existence and utilization of a scale factor @xmath276 using which the clf function evaluated along the state trajectory decays faster than a value function , is taken into account , the control resulting from the scaled clf will lead to a finite cost - to - go . therefore ,",
    "the results look similar in regards to the initial guess .",
    "however , the simplicity of the proofs especially for the convergence proof , including the intermediate steps for firm conclusions ( e.g. , continuity analysis ) , admitting a positive semi - definite running cost as opposed to the positive definite one in that work , and establishing an eroa are the main differences of the mentioned theorems compared with @xcite .",
    "besides addressing the convergence concern in vi , the foregoing theorem provides an idea for establishing an eroa for the system operated using evolving control policies , as presented next .",
    "[ thm_roa_evolving_vi ] let @xmath189 and @xmath277 for any @xmath188 . also , let the system be operated using control policy @xmath217 at time @xmath16 , that is , the control subject to adaptation in the stabilizing value iterations .",
    "if @xmath278 for an @xmath279 then @xmath280 is an estimation of the region of attraction of the closed loop system .",
    "_ proof _ : as the first step we show that for any given @xmath191 one has @xmath281 by inequality ( [ thm1_eq2 ] ) one has @xmath282 .",
    "therefore , @xmath283 by definition of @xmath284 if @xmath285 , which follows from lemma [ lemma_nondecreasing ] , then @xmath286 .",
    "therefore , @xmath287 finally ( [ eq_thm_roa_evolving_vi_2 ] ) and ( [ eq_thm_roa_evolving_vi_3 ] ) lead to ( [ eq_thm_roa_evolving_vi_1 ] ) . now",
    "that ( [ eq_thm_roa_evolving_vi_1 ] ) is proved , one may use mathematical induction to see @xmath288    the next step is noticing that @xmath289 , which follows from the monotonicity of @xmath290 and its convergence to @xmath46 , per theorem [ theorem_vi_convergence ] .",
    "the foregoing inequality leads to @xmath291 , by definition of @xmath284 and @xmath292 .",
    "therefore , ( [ eq_thm_roa_evolving_vi_3_1 ] ) leads to @xmath293 the result given by ( [ eq_thm_roa_evolving_vi_4 ] ) proves the theorem , because , if @xmath191 is such that @xmath278 then any trajectory initiated within @xmath280 will remain inside @xmath43 , and hence , by theorem [ thm_stabil_evi_no_lyap ] will converge to the origin .",
    "the problem with the exact vi is the issue that _ exact _ reconstruction of the right hand side of eq .",
    "( [ vi_valueupdate ] ) is not generally possible except for very simple problems . in general ,",
    "parametric function approximators are used for this purpose , which hence , give rise to function approximation errors . when the approximation errors are considered , eq .",
    "( [ vi_valueupdate ] ) reads @xmath294 where the _ approximate value function _ at the @xmath72th iteration is denoted with @xmath295 and the approximation error at this iteration is denoted with @xmath296 .",
    "note that the value function in the right hand side of eq .",
    "( [ avi_1 ] ) is also an approximate quantity , generated from the previous iteration .",
    "when @xmath297 , the convergence of the approximate vi ( avi ) does not follow from theorem [ theorem_vi_convergence ] .",
    "this convergence / boundedness is investigated in this section .    before proceeding to the convergence / boundedness analysis",
    "it is worth mentioning that one typically trains a control approximator ( actor ) to approximate the solution to the minimization problem given by ( [ bellman_eq2 ] ) based on the value function resulting from vi .",
    "the control approximator , will hence , lead to _ another _ approximation error term in the process , regardless of whether the value function reconstruction is exact or approximate .",
    "however , the effect of the actor s approximation error can be removed from both the convergence analysis of the avi and the stability analysis of the system during value iterations , as the control will be directly calculated from the minimization of the right hand side of eq .",
    "( [ avi_1 ] ) in _ online _ and _ adaptive _ optimal control and applied on the system . in other words , even though the actor will be updated simultaneously along with the critic in online learning , the critic training and the system s operation are independent of the actor s approximation accuracy .",
    "once the learning is concluded ( and if it is concluded ) , the operation of the system could be based on the control resulting from the trained actor , hence , the actor s approximation error can affect the stability of the system at that stage . the stability analysis _ after _ conclusion of avi is beyond the scope of this study and deserves to be investigated separately , as the focus in this work is analyzing the convergence / boundedness and stability _ during _ the online learning process through avi .    considering the above comment and denoting the minimizer of the right hand side of eq .",
    "( [ avi_1 ] ) by @xmath298 , one has @xmath299 therefore , eq . ( [ avi_1 ] ) can be written as @xmath300    assuming an upper bound for the approximation error @xmath301 the results given by theorem [ thm_boundedness ] can be obtained , in terms of boundedness of sequence @xmath302 resulting from the avi and its relation versus the optimal value function .",
    "this boundedness will later be used for stability analysis .    for our avi analyses ,",
    "i.e. , for the rest of this study , it is assumed that state penalizing function @xmath303 in @xmath304 only vanishes at the origin . in other words , instead of",
    "the positive semi - definiteness of @xmath109 , it is assumed that @xmath109 is positive definite hereafter .",
    "the reason for this modification is the point that @xmath236 is going to be used to put an upper bound on @xmath305 , which makes sense only if it does not vanish at any non - zero @xmath10 .",
    "[ thm_boundedness ] let @xmath306 for some @xmath307 . if the approximate value iteration is initiated using some @xmath308 which satisfies @xmath309 where @xmath310 and @xmath311 are , respectively , the initial guesses for the exact value iterations corresponding to cost functions @xmath312 and @xmath313 subject to dynamics ( [ dynamics ] ) , then , the result of the approximate value iteration at the @xmath72th iteration is bounded from below by the result of the exact value iteration corresponding to cost function ( [ cost_lower_1 ] ) and from above by the result of the exact value iteration corresponding to cost function ( [ cost_upper_1 ] ) .    _ proof _ : let @xmath314 and @xmath315 where @xmath316 and @xmath317 , be defined as sequences of functions initiated from some @xmath318 and @xmath319 and generated by @xmath320 @xmath321 considering recursive relations ( [ v_upper_1 ] ) and ( [ v_lower_1 ] ) it is seen that @xmath322 and @xmath323 are , respectively , the value functions at the @xmath72the iteration of _ exact vi _ for cost functions ( [ cost_lower_1 ] ) and ( [ cost_upper_1 ] ) .",
    "considering this point , the lemma can be proved using mathematical induction .",
    "initially @xmath324 by assumption .",
    "let @xmath325 hold for some @xmath72 .",
    "comparing eq .",
    "( [ v_upper_1 ] ) with eq .",
    "( [ avi_1 ] ) it follows that @xmath326 , since @xmath327 and @xmath328 .",
    "therefore , one has @xmath329 .",
    "the proof of @xmath330 is similar by induction , through comparing eq .",
    "( [ v_lower_1 ] ) with eq .",
    "( [ avi_1 ] ) and noting that @xmath331 .",
    "the result given by the foregoing theorem resembles the idea of relaxed dynamic programming presented in @xcite . however , the idea , the proof , and the applications of the boundedness result , presented in the rest of this study , are different .",
    "the exact vis given by ( [ v_lower_1 ] ) and ( [ v_upper_1 ] ) converge , based on theorem [ theorem_vi_convergence ] , when initiated using value functions ( defined based on the respective cost function ) of some admissible controls",
    ". therefore , considering theorem [ thm_boundedness ] , the boundedness of the avi results for all iterations follows , assuming the boundedness of the approximation errors by @xmath332 for some @xmath307 .    the actual convergence as well as stability of the system operated under avi are much more challenging compared to the respective analyses in exact vi , since the presence of the approximation error cancels the monotonicity feature presented in lemma [ lemma_nondecreasing ] .",
    "note that the monotonicity was the backbone of both the stability and the convergence results given in theorems [ thm_stab_lyapfun ] , [ thm_stabil_evi_no_lyap ] , and [ theorem_vi_convergence ] .",
    "as long as the boundedness of the functions during avi is guaranteed in a neighborhood of the optimal value function ( theorem [ thm_boundedness ] ) where the neighborhood shrinks if the approximation error decreases , the actual convergence of the iteration may not be of a critical importance in implementing the avi .",
    "but , the stability of the system operated under the avi is definitely critical .",
    "the following lemma develops a ` semi - monotonicity ' of the stabilizing avi to be used later for deriving some stability results .",
    "note that , following definition [ stabilizingvi_definition ] , _ stabilizing avi _ is defined as the avi which is initiated using the _ approximate _ value function of an admissible control policy , with an approximation error denoted with @xmath333 . in other words instead of the exact value function @xmath53 given by ( [ vi_value_eq1 ] ) one initiates the iterations using the approximate value function @xmath334 which satisfies @xmath335    [ lemma_avi_semimonotonicity ] let @xmath336 , where @xmath337 and @xmath338 . if the stabilizing approximate value iteration scheme is conducted using a function approximator that satisfies @xmath339 for some @xmath307 , then , @xmath340    _ proof _ : initially note that @xmath341 is the @xmath16th state vector on the state trajectory initiated from @xmath37 and propagated by applying control policy @xmath342 at time @xmath343 .",
    "the first iteration of avi leads to @xmath344 one has @xmath345 , because , per ( [ eq_lem_avi_semimonot_2 ] ) , @xmath346 is resulted from a minimization , as opposed to using a given policy @xmath67 in ( [ avi_v0_eq ] ) . the foregoing inequality along with @xmath347 lead to @xmath348 which confirms that inequality ( [ eq_lem_avi_semimonot_1 ] ) holds for @xmath58 .",
    "now , assume that @xmath349 if this assumption leads to ( [ eq_lem_avi_semimonot_1 ] ) the proof will be complete by induction .",
    "let @xmath350 since the minimizer of the right hand side of ( [ avi_1 ] ) is @xmath298 and not @xmath351 , one has @xmath352 on the other hand , by definition of @xmath353 , that is , @xmath354 and comparing it with ( [ eq_lem_avi_semimonot_5 ] ) and considering ( [ eq_lem_avi_semimonot_4 ] ) one has @xmath355 which because of @xmath347 leads to @xmath356 and along with ( [ eq_lem_avi_semimonot_6 ] ) leads to @xmath357 the next step is showing that @xmath358 note that , @xmath359 is the result of evaluating a finite sum of @xmath360 s along a ` trajectory ' initiated from @xmath37 .",
    "so , in order to show that ( [ eq_lem_avi_semimonot_10 ] ) holds , it suffices to show that the summations in both sides of ( [ eq_lem_avi_semimonot_10 ] ) , which each has @xmath361 elements , are along the same trajectory .",
    "the first summand in @xmath359 is @xmath362 which is matched by the same term existing in the left hand side of ( [ eq_lem_avi_semimonot_10 ] ) .",
    "the second summand of @xmath359 is @xmath109 evaluated at @xmath363 .",
    "the first summand of @xmath364 is @xmath109 evaluated at @xmath365 . since @xmath366 , one has @xmath367 , hence , the second summand of @xmath359 also will be matched by a term in the left hand side of ( [ eq_lem_avi_semimonot_10 ] ) .",
    "similarly , the third summand of @xmath359 is @xmath109 evaluated at @xmath368 .",
    "the second summand of @xmath364 is @xmath109 evaluated at @xmath369 , by definition . since @xmath370 one has @xmath371 . repeating this argument",
    "it is seen that the trajectories are identical and hence , ( [ eq_lem_avi_semimonot_10 ] ) holds , which along with ( [ eq_lem_avi_semimonot_9 ] ) proves ( [ eq_lem_avi_semimonot_1 ] ) .",
    "[ theorem_stability_avi ] let the stabilizing approximate value iteration be conducted using a continuous function approximator with the bounded approximation error @xmath372 for some @xmath307 .",
    "moreover , denoting the value function of @xmath67 with @xmath53 , let @xmath373 be such that @xmath374 .",
    "then , for every given @xmath193 , control policy @xmath375 asymptotically stabilizes the system about the origin if @xmath376 is such that @xmath377 moreover , let the compact domain @xmath378 for any @xmath188 be defined as @xmath379 and let @xmath190 be ( the largest @xmath191 ) such that @xmath380 .",
    "then , @xmath381 will be an estimation of the region of attraction for the system .",
    "_ proof _ : the idea , similar to theorem [ thm_stab_lyapfun ] , is using @xmath295 as a lyapunov function to prove the claim .",
    "the lower and upper boundedness of the function , established in theorem [ thm_boundedness ] , guarantees the positive definiteness of the function and the continuity of the parametric function approximator guarantees its continuity .",
    "the objective is showing negative definiteness of @xmath382 . by eq .",
    "( [ avi_3 ] ) @xmath383 lemma [ lemma_avi_semimonotonicity ] and inequality ( [ eq_lem_avi_semimonot_1 ] ) may be used in the foregoing equation to replace @xmath384 with @xmath385 in its left hand side . before that , considering the similarity between the exact and approximate vi , from eqs .",
    "( [ valfunction_finhor ] ) , ( [ finhor_vs_vi_eq3 ] ) , ( [ finhor_vs_vi_eq3_2 ] ) , and ( [ avi_1 ] ) , it is straight forward to see @xmath386 where @xmath337 and @xmath387 the point in concluding ( [ avi_stabil_thm_eq3 ] ) from the exact vi counterpart , given by ( [ valfunction_finhor ] ) considering ( [ finhor_vs_vi_eq3 ] ) and ( [ finhor_vs_vi_eq3_2 ] ) , is the fact that at time @xmath16 when the control policy is @xmath388 , the approximation error introduced in the summation will be @xmath389 , i.e. , the superscripts will the same per ( [ avi_3 ] ) and both functions will be evaluated at the current state @xmath341 .",
    "considering @xmath390 and comparing ( [ avi_stabil_thm_eq3 ] ) with @xmath391 defined in lemma [ lemma_avi_semimonotonicity ] one has @xmath392 moreover , by theorem [ thm_boundedness ] one has @xmath393 if @xmath322 is generated using the value function of @xmath67 as the initial guess .",
    "moreover , @xmath394 by lemma [ lemma_nondecreasing ] .",
    "therefore , @xmath395 the interesting point about inequality ( [ avi_stabil_thm_eq5 ] ) is showing the boundedness of the left hand side .",
    "this boundedness along with the results from lemma [ lemma_avi_semimonotonicity ] lead to @xmath396 utilizing ( [ avi_stabil_thm_eq6 ] ) in ( [ avi_stabil_thm_eq2 ] ) leads to @xmath397 in order to have @xmath398 one needs @xmath399 , which holds if @xmath400 note that @xmath401 by definition of @xmath311 which is the value function of @xmath67 with the utility of @xmath402 , while , @xmath237 is the value function of the same control policy with the utility of @xmath403 .",
    "therefore , from @xmath404 one has @xmath405 . hence , @xmath406 .",
    "therefore , if @xmath407 or equivalently if @xmath408 then inequality ( [ avi_stabil_thm_eq8 ] ) holds .",
    "the root of the left hand side of the foregoing inequality are real and given by @xmath409 inequality ( [ avi_stabil_thm_eq8_2 ] ) holds if @xmath410 or if @xmath411 by analysis of the sign of the quadratic equation on its left hand side .",
    "but , @xmath412 , hence , any @xmath376 which satisfies @xmath411 will be unacceptable for our purpose , because , such a @xmath376 does not belong to @xmath413 .",
    "as for @xmath410 it is required to make sure @xmath414 , otherwise no suitable @xmath376 will be resulted from this analysis . from @xmath415",
    "which along with the non - negativeness of both sides of the last inequality leads to @xmath416 , one has @xmath417 .",
    "therefore , @xmath418 is indeed positive and a non - negative @xmath376 smaller than @xmath418 leads to the desired stability . the first part of the theorem is proved by noticing that when ( [ avi_stabil_thm_eq1 ] ) holds , @xmath419 is strictly less than zero , considering the assumed positive definiteness of @xmath109 .    finally , considering the line of proof at the end of the proof of theorem [ thm_stab_lyapfun ] it is straight forward to see that @xmath398 leads to @xmath380 being an eroa for the system operated with @xmath298 .",
    "the reason is , any trajectory initiated within @xmath381 will remain inside the set and hence , within @xmath43 .",
    "note that the continuity of the function approximators leads to the desired continuity of @xmath295 for guaranteeing the compactness of the eroa , as detailed in the proof of theorem [ thm_stab_lyapfun ] .",
    "[ thm_stabil_avi_no_lyap ] let the stabilizing approximate value iteration be implemented using a continuous function approximator with the bounded approximation error @xmath372 for some @xmath307 .",
    "moreover , let @xmath373 be such that @xmath420 . then , the system operated using control policy @xmath421 at time @xmath16 , that is , the control subject to adaptation in the approximate value iterations scheme asymptotically stabilizes the system about the origin if @xmath376 is such that @xmath422    _ proof _ : the idea for proof of this theorem is similar to that of theorem [ thm_stabil_evi_no_lyap ] , except that the ` semi - monotonicity ' feature presented in lemma [ lemma_avi_semimonotonicity ] will be used , instead of the monotonicity given by lemma [ lemma_nondecreasing ] in exact vi .",
    "( [ avi_3 ] ) and lemma [ lemma_avi_semimonotonicity ] lead to @xmath423 and similarly @xmath424 which along with ( [ avi_stabil_no_lyap_thm_eq2 ] ) leads to @xmath425 or equivalently @xmath426 let @xmath427 for @xmath428 and @xmath429 . evaluating ( [ avi_stabil_no_lyap_thm_eq3_2 ] ) at @xmath430 and replacing the @xmath431 in the left hand side of the inequality in ( [ avi_stabil_no_lyap_thm_eq2 ] ) with the left hand side of ( [ avi_stabil_no_lyap_thm_eq3_2 ] ) , which is smaller per ( [ avi_stabil_no_lyap_thm_eq3_2 ] )",
    ", one has @xmath432 repeating this process by replacing @xmath433 in ( [ avi_stabil_no_lyap_thm_eq4 ] ) using @xmath434 which is resulted from @xmath435 leads to @xmath436 similarly by repeating this process one has @xmath437 from ( [ avi_stabil_thm_eq5 ] ) and @xmath438 , one has @xmath439 also , @xmath440 .",
    "therefore , from ( [ avi_stabil_no_lyap_thm_eq7 ] ) one has @xmath441 the last inequality holds even if we add the negative scalar value of @xmath442 to the last summation on its left hand side , which leads to @xmath443 since @xmath444 , it can be removed from the left hand side of the foregoing inequality while the inequality still holds .",
    "assume @xmath445 comparing ( [ avi_stabil_no_lyap_thm_eq7_4 ] ) , after removing @xmath446 , with ( [ thm_stab_nolyap_7 ] ) where the states were propagated using the exact vi in the latter , the same stability result can be obtained for the states propagated using the approximate vi , providing ( [ avi_stabil_no_lyap_thm_eq7_5 ] ) holds .",
    "that is , the sequence of partial sums of the left hand side of ( [ avi_stabil_no_lyap_thm_eq7_4 ] ) is upper bounded and because of being non - decreasing it converges , as @xmath232 , @xcite .",
    "considering the positive definiteness of @xmath109 , assumed in our avi analyses , this leads to @xmath447 , as long as the entire state trajectory is contained in @xmath43 . finally , in order to enforce ( [ avi_stabil_no_lyap_thm_eq7_5 ] )",
    ", one will need ( [ avi_stabil_no_lyap_thm_eq1 ] ) .",
    "the details are identical to the last stages of the proof of theorem [ theorem_stability_avi ] , as replacing @xmath170 in ( [ avi_stabil_thm_eq8_2 ] ) with @xmath448 , the left hand sides of inequalities ( [ avi_stabil_thm_eq8_2 ] ) and ( [ avi_stabil_no_lyap_thm_eq7_5 ] ) become identical .",
    "moreover , the proof of positiveness of the right hand side of ( [ avi_stabil_no_lyap_thm_eq1 ] ) follows from the same argument presented in that proof .",
    "finally , the last step of our analysis is presenting some results regarding the eroa for the system operated using evolving control policy during avi .",
    "[ thm_roa_evolving_avi ] let the conditions of theorem [ thm_stabil_avi_no_lyap ] hold",
    ". moreover , let @xmath379 and @xmath277 for any @xmath188 .",
    "if @xmath449 for a given @xmath279 , then compact set @xmath450 is an estimation of the region of attraction of the system operated using control policy @xmath421 at time @xmath16 , that is , the control subject to adaptation .",
    "_ proof _ : once the conditions of theorem [ thm_stabil_avi_no_lyap ] hold , from ( [ avi_stabil_no_lyap_thm_eq7_4 ] ) one has @xmath451 where @xmath427 for @xmath428 and @xmath429 , that is , @xmath452 denotes the state trajectory generated by the _ evolving _ control policy from the avi .",
    "therefore , @xmath453 from @xmath454 cf .",
    "theorem [ thm_boundedness ] , one has @xmath455 where @xmath456 and @xmath457 is defined in theorem [ thm_boundedness ] .",
    "moreover , @xmath458 is non - increasing and converges to @xmath459 , i.e. , the optimal value function corresponding to cost function ( [ cost_lower_1 ] ) , because it is resulted from an exact vi .",
    "therefore , @xmath460 which leads to @xmath461 from ( [ avi_thm_roa_evolving_vi_eq4 ] ) and ( [ avi_thm_roa_evolving_vi_eq5 ] ) one has @xmath462 on the other hand , it is not hard to see that @xmath463 which if holds leads to @xmath464 to verify @xmath465 , let the state trajectory generated from the exact vi of @xmath459 be denoted with @xmath466 . comparing cost function ( [ cost_lower_1 ] ) with ( [ costfunction ] ) one has @xmath467 the reason is , both sides of the foregoing inequality are infinite sums of similar summands , except that the the left hand side is evaluated along the optimal trajectory with respect to ( [ costfunction ] ) while the right hand side is evaluated along the optimal trajectory with respect to ( [ cost_lower_1 ] ) .",
    "moreover , by composition @xmath468 which along with @xmath469 and ( [ avi_thm_roa_evolving_vi_eq8 ] ) leads to @xmath465 .",
    "finally , from ( [ avi_thm_roa_evolving_vi_eq3 ] ) , ( [ avi_thm_roa_evolving_vi_eq6 ] ) , and ( [ avi_thm_roa_evolving_vi_eq7 ] ) one has @xmath470 therefore , any state trajectory initiated within @xmath450 remains inside @xmath471 and if the @xmath191 in the latter is such that @xmath449 , the state trajectory remains within @xmath43 and by theorem [ thm_stabil_avi_no_lyap ] converges to the origin .",
    "a set of theoretical analyses on convergence , stability , and regions of attraction for a value iteration based scheme which is initiated using an admissible guess were presented .",
    "afterwards , the results were extended to the more interesting but challenging case of incorporating the approximation errors in the iterations .",
    "simple and straight forward conditions for guaranteeing the boundedness of the learning results and the stability of the system under a fixed as well as an evolving control policy were developed .",
    "these results are expected to lay the foundation for improving the mathematical rigor of the popular field of intelligent control .",
    "g.  venayagamoorthy , r.  harley , and d.  wunsch , `` comparison of heuristic dynamic programming and dual heuristic programming adaptive critics for neurocontrol of a turbogenerator , '' _ ieee transactions on neural networks _ , vol .  13 , pp .  764773 , may 2002",
    ".      a.  al - tamimi , f.  lewis , and m.  abu - khalaf , `` discrete - time nonlinear hjb solution using approximate dynamic programming : convergence proof , '' _ ieee transactions on systems , man , and cybernetics , part b : cybernetics _ , vol .",
    "38 , pp .  943949 , aug 2008 .",
    "f.  lewis , d.  vrabie , and k.  vamvoudakis , `` reinforcement learning and feedback control : using natural decision methods to design optimal adaptive controllers , '' _ ieee control systems _ , vol .  32 , pp .  76105 , 2012 .",
    "d.  liu and q.  wei , `` policy iteration adaptive dynamic programming algorithm for discrete - time nonlinear systems , '' _ ieee transactions on neural networks and learning systems _ , vol .",
    "25 , pp .  621634 , 2014 .",
    "a.  farahmand , c.  szepesvri , and r.  munos , `` error propagation for approximate policy and value iteration , '' in _ advances in neural information processing systems _ ( j.  lafferty , c.  williams , j.  shawe - taylor , r.  zemel , and a.  culotta , eds . ) , pp .",
    "568576 , 2010 .",
    "q.  wei and d.  liu , `` a novel iterative @xmath276-adaptive dynamic programming for discrete - time nonlinear systems , '' _ ieee transactions on automation science and engineering _ , vol .  11 , no .  4 , pp .  11761190 , 2014 .",
    "ali heydari received his phd degree from the missouri university of science and technology in 2013 .",
    "he is currently an assistant professor of mechanical engineering at the south dakota school of mines and technology .",
    "he was the recipient of the outstanding m.sc .",
    "thesis award from the iranian aerospace society , the best student paper runner - up award from the aiaa guidance , navigation and control conference , and the outstanding graduate teaching award from the academy of mechanical and aerospace engineers at missouri s&t .",
    "his research interests include approximate dynamic programming and control of hybrid systems .",
    "he is a member of tau beta pi ."
  ],
  "abstract_text": [
    "<S> adaptive optimal control using value iteration ( vi ) initiated from a stabilizing policy is theoretically analyzed in various aspects including the continuity of the result , the stability of the system operated using any _ single / constant _ resulting control policy , the stability of the system operated using the _ evolving / time - varying _ control policy , the convergence of the algorithm , and the optimality of the limit function . </S>",
    "<S> afterwards , the effect of presence of approximation errors in the involved function approximation processes is incorporated and another set of results for boundedness of the approximate vi as well as stability of the system operated under the results for both cases of applying a single policy or an evolving policy are derived . </S>",
    "<S> a feature of the presented results is providing estimations of the _ region of attraction _ so that if the initial condition is within the region , the whole trajectory will remain inside it and hence , the function approximation results will be reliable . </S>"
  ]
}