{
  "article_text": [
    "we consider a system with a server and multiple users , where the server has access to a library of files ( e.g. , movies ) .",
    "each user is equipped with an isolated cache of certain size which can be used to store parts of the library .",
    "the server is connected to the users via an arbitrary memoryless communication network that could be consisting of multiple relays ( wireline or wireless ) .",
    "this represents a general cache network which encompasses the network configurations studied in several prior works , including bottleneck  @xcite , multiserver  @xcite , combinatorial  @xcite , and tree  @xcite cache networks .",
    "the system operates in two phases . in the first phase ,",
    "called the prefetching phase , each cache is populated up to its limited size from the contents of the library .",
    "this phase is followed by a delivery phase , where each user reveals its request for a file in the library .",
    "afterwards , the delivery strategy consists of generating a set of multicast messages at the server , one for each subset of users , and then delivering the generated multicast messages to the users through the intermediate communication network .",
    "our main result is to demonstrate that there exists a universal scheme for cache placement at the users and multicast message generation at the server , which is independent of the underlying communication network between the server and the users , and is approximately optimal for all cache network configurations ( in terms of the delivery delay ) . in particular , we demonstrate that the prefetching and multicast message generation which takes place assuming that there exists a bottleneck link ( see  @xcite ) is approximately optimal for all network configurations . note that even though we do not know how to use the communication network between the server and the users optimally in order to deliver the multicast messages , as opposed to the special case of shared bottleneck link , we still show that the corresponding prefetching and delivery is approximately optimal .",
    "the importance of our result lies in the fact that in our proposed universal scheme , caching and multicast message generation take place without the knowledge of the structure of the communication network between the server and the users , yet they are approximately optimal .",
    "this is particularly important since most cache networks are time - varying and dynamic .",
    "hence , it is critical to rely on a scheme which is oblivious to these variations .",
    "our result suggests that the proposed universal scheme performs almost as well as any other scheme with a priori knowledge of all network dynamics .",
    "moreover , our result essentially demonstrates that a _ separation _ between caching and multicast message generation on one hand , and delivering the multicast messages to the users on the other hand is approximately optimal . as mentioned before",
    ", this separation is practically important since it demonstrates that the design of caching and multicast message generation can be done without knowledge of the underlying communication network .    in order to prove the main result",
    ", we first propose a universal scheme for cache placement and multicast message generation , inspired by the decentralized coded caching scheme presented in @xcite for the case of bottleneck link networks , and characterize its achievable normalized delivery delay ( as a function of the mutlicast capacity region of the underlying network ) .",
    "we then provide an outer bound on the normalized delivery delay achievable by any scheme with full knowledge of the communication network between the server and the users , where we first bound the normalized delivery delay for any fixed set of user demands and then relax worst - case to average demands to obtain a lower bound on the optimal normalized delivery delay . finally ,",
    "through optimizing the rates over the multicast capacity region of the network , we show that the normalized delivery delay achievable by our proposed universal scheme is within a constant factor of the outer bound , hence establishing our main result .",
    "the rest of the paper is organized as follows .",
    "we describe the system model and provide two motivating examples in section [ sec : model ] .",
    "we formulate the problem and state our main result in section [ sec : problem_result ] .",
    "we prove our main result in section [ sec : proof ] .",
    "finally , we conclude the paper in section [ sec : conc ] .",
    "consider a system with a server @xmath0 and @xmath1 users @xmath2 , where the server has access to a library of @xmath3 files @xmath4 , each containing @xmath5 bits .",
    "more precisely , each file is uniformly selected from @xmath6 $ ] , where for any integer @xmath7 , @xmath8 $ ] is used to denote the set @xmath9 .",
    "also , we assume that each user is equipped with a cache memory of size @xmath10 bits which can be used to cache parts of the library at the users .",
    "the server is connected to the users through an arbitrary memoryless communication network consisting of @xmath11 relays @xmath12 , with the channel model given by an arbitrary transition probability @xmath13 , where for each node @xmath14 , @xmath15 represents the input alphabet and @xmath16 represents the output alphabet , @xmath17 and @xmath18 respectively denote the channel input by the server and the relays , and @xmath19 and @xmath20 respectively denote the channel output to the relays and the users .",
    "this represents a general cache network model where the caches are located at the edge of the network and it includes bottleneck  @xcite , multiserver  @xcite , combinatorial  @xcite , and tree  @xcite cache networks .",
    "an example of such a system model with @xmath21 users is illustrated in figure [ fig : network ] .",
    "the system operates in two phases :    * prefetching phase * : in this phase , each user @xmath22 $ ] , caches a subset of bits from the library , denoted by @xmath23 , such that @xmath24 .",
    "we restrict our attention to _ uncoded prefetching _ in which each user is allowed to cache any subset of the bits of the files in the library in an uncoded manner .",
    "* delivery phase * : in this phase , each user @xmath22 $ ] , will reveal its request for a file @xmath25 from the library .",
    "after the demand vector @xmath26^t$ ] is revealed , the delivery strategy consists of the following :    * _ multicast message generation _ : generating @xmath27 multicast messages @xmath28}$ ] and @xmath29 , @xmath30 is equivalent to @xmath31 .",
    "] at the server , and * _ multicast message delivery _ : implementing an encoder at the server and decoders at the users , and a relaying strategy for each relay , so that for each subset @xmath32 $ ] , the message @xmath33 is multicast to the users @xmath34 .",
    "the multicast messages are generated such that if they are successfully delivered , each user @xmath22 $ ] , will be able to decode its desired file @xmath25 based on its cache contents @xmath23 and its decoded multicast messages @xmath35 . in order to deliver the multicast messages",
    ", the server selects a multicast rate tuple @xmath36}$ ] from the _ multicast capacity region _",
    "@xmath37 , which is defined as follows .",
    "consider a communication network represented as @xmath38 a multicast rate tuple @xmath36}$ ] is said to be achievable if for each @xmath32 $ ] , the server can multicast an independent message @xmath33 to the users @xmath34 at rate @xmath39 with vanishing error probability as the blocklength increases .",
    "the multicast capacity region @xmath37 is defined as the closure of the set of all achievable multicast rate tuples .    the goal is , given a communication network between the server and the users and the cache capacity of each user , to design the cache placement and the delivery strategy in order to minimize the delivery delay .",
    "in this section , we first present two examples to motivate a universal scheme for cache networks .",
    "in particular , we consider a system with @xmath40 files @xmath41 , @xmath42 , and @xmath43 and @xmath21 users , each with a cache of size equal to @xmath44 file . as for the communication network between the server and the users , we consider two different scenarios in the following examples , for each of which we present prefetching and delivery schemes tailored to the communication network between the server and the users",
    ". we will then present a universal scheme which is applicable to both network configurations and achieves the optimal normalized delivery delay in both of the examples .",
    "[ ex : bottleneck ]    consider the communication network illustrated in figure [ fig : ex]-a , in which all the three users receive the same signal which is transmitted by the server through a shared bottleneck link .",
    "this is the same model considered in @xcite , which suggests that the prefetching and delivery phases can be done as follows :    * prefetching phase : we break each file @xmath45 , to three disjoint subfiles @xmath46 each of size @xmath47 bits .",
    "then , each user @xmath48 , caches @xmath49 * delivery phase : without loss of generality , suppose that users @xmath50 , @xmath51 and @xmath52 request files @xmath53 , @xmath54 and @xmath55 , respectively .",
    "the server multicasts the following messages over the shared bottleneck link .",
    "@xmath56 where @xmath57 denotes the bitwise xor operator    assuming a multicast capacity of 1 for the shared bottleneck link , this scheme achieves a normalized delivery delay of 1 .",
    "moreover , as demonstrated in @xcite , the aforementioned prefetching and delivery phases lead to the optimal normalized delivery delay in this communication network .",
    "[ ex : orth ]    consider the communication network illustrated in figure [ fig : ex]-b , in which the server can send distinct signals to each of the users through separate dedicated orthogonal links .",
    "the prefetching and delivery phases can be done as follows :    * prefetching phase : each user caches the first @xmath47 bits of each file in the library .",
    "this suggests that @xmath59 .",
    "* delivery phase : once the demands @xmath60 $ ] are revealed , for any user @xmath48 , the server sends the last @xmath61 bits of file @xmath25 to user @xmath62 through its dedicated link .",
    "therefore , we will have @xmath63    assuming a capacity of 1 for each of the orthogonal links , this scheme achieves a normalized delivery delay of @xmath64 .",
    "moreover , in this communication network , a simple cut - set outer bound would show that the aforementioned prefetching and delivery phases lead to the optimal normalized delivery delay .",
    "@xmath58    the schemes that we presented in examples [ ex : bottleneck ] and [ ex : orth ] were each tailored to the structure of the corresponding communication network .",
    "however , quite interestingly , we can use the same prefetching and multicast message generation that we presented in example [ ex : bottleneck ] for the case of orthogonal links communication network in example [ ex : orth ] as well .",
    "namely , the prefetching can be done similarly to and in the delivery phase , we send useful coded messages over each of the orthogonal links . in particular , we generate the multicast messages as @xmath65 the total size of the message delivered to each user is @xmath61 , hence this scheme also achieves the optimal normalized delivery delay of @xmath64 in example [ ex : orth ] .    motivated by the above examples , the question that we attempt to answer in this paper is whether the scheme that is optimal under the assumption of existence of a bottleneck link is also universally optimal for all network configurations in terms of the normalized delivery delay .",
    "our main result , which we state in the next section , is to provide an affirmative answer to this question .",
    "therefore , the optimal normalized delivery delay , denoted by @xmath72 , is achieved by the prefetching and delivery strategies which minimize the maximum delivery delay across all possible demands for sufficiently large @xmath5 ; i.e. , @xmath73.\\end{aligned}\\ ] ]    moreover , motivated by examples [ ex : bottleneck ] and [ ex : orth ] , we define a _",
    "universal scheme _ as follows .",
    "a universal scheme is a cache placement and multicast message generation scheme which is only a function of the number of files @xmath3 , the number of users @xmath1 and the cache size at each user @xmath74 , and is independent of the communication network between the server and the users .    as mentioned in section [ sec : examples ] , our main result is to show that there exists a universal scheme , based on the assumption of existence of a shared bottleneck link , which is approximately optimal for all cache network configurations ( i.e. , @xmath75 ) in terms of the normalizd delivery delay .",
    "we state this result formally in the following theorem .",
    "[ thm : main ]    there exists a universal scheme with an achievable normalized delivery delay denoted by @xmath76 which satisfies @xmath77 implying that the universal scheme is within a factor of 24 optimal in terms of the normalized delivery delay .    in order to prove theorem [ thm : main ] ,",
    "we demonstrate that the prefetching and multicast message generation which takes place assuming that there exists a bottleneck link is approximately optimal for all network configurations ; i.e. , its achievable normalized delivery delay is always within a factor of 24 of the optimal normalized delivery delay @xmath72 .",
    "the description of the scheme and its achievable delay will be presented in section [ sec : scheme ] .",
    "note that even though we do not know how to use the communication network between the server and the users optimally in order to deliver the multicast messages , as opposed to the special case of shared bottleneck link , we still show that the corresponding prefetching and delivery is approximately optimal .",
    "the importance of theorem [ thm : main ] lies in the fact that in our proposed universal scheme , caching and multicast message generation can take place without the knowledge of the structure of the communication network between the server and the users , yet it is approximately optimal .",
    "this is particularly important since most cache networks are time - varying and dynamic .",
    "hence , it is critical to rely on a scheme which is oblivious to these variations .",
    "theorem [ thm : main ] suggests that our proposed universal scheme performs almost as well as any other scheme with a priori knowledge of all network dynamics .",
    "theorem [ thm : main ] essentially demonstrates that a _ separation _ between caching and multicast message generation on one hand , and delivering the multicast messages to the users on the other hand is approximately optimal . as mentioned before",
    ", this separation is practically important since it demonstrates that the design of caching and multicast message generation can be done without knowledge of the underlying communication network .",
    "such a scheme admits a layered solution , where the caching and multicast message generation are done in , for example , the application layer and delivering the generated multicast messages is done in the network and link layers .",
    "our result shows that such a layered solution does not violate the optimality .",
    "there has been a surge of recent works on cache networks considering various topologies for the communication network between the server and the users , such as bottleneck  @xcite , multiserver  @xcite , combinatorial  @xcite , and tree  @xcite cache networks .",
    "theorem [ thm : main ] implies that under an uncoded prefetching and two - phase delivery assumption ( multicast message generation at the server and delivery to the users ) , there exists a universal scheme which is approximately optimal regardless of the structure of the communication network between the server and the users , hence providing an approximately optimal result for all the aforementioned network configurations .",
    "in order to prove theorem [ thm : main ] , our goal is to show that the universal scheme that is based on the assumption of existence of a shared bottleneck link is approximately optimal for all network configurations . to that end",
    ", we first describe the universal scheme and derive its achievable normalized delivery delay .",
    "we will then derive an outer bound on the optimal normalized delivery delay and we will show that the normalized delivery delay achieved by our universal scheme is within a factor of 24 of the outer bound .",
    "* prefetching phase : in this phase , each user caches @xmath78 bits of each file in the library uniformly at random .",
    "this clearly satisfies the cache size constraint at each user .",
    "* delivery phase : the aforementioned prefetching phase partitions each file @xmath79 $ ] in the library to @xmath80 subfiles @xmath81}$ ] , where for any subset @xmath82 $ ] , @xmath83 denotes the bits of file @xmath84 which are exclusively cached by the users @xmath85 .",
    "the law of large numbers implies that for large enough file size @xmath5 , size of each subfile @xmath83 is approximately equal to @xmath86 $ ] , the size of @xmath83 only depends on the size of @xmath87 . based on this observation ,",
    "once the demands @xmath88 $ ] are revealed , the server generates the multicast messages as follows .",
    "@xmath89.\\end{aligned}\\ ] ]            m. ji , m. f. wong , a. m. tulino , j. llorca , g. caire , m. effros , and m. langberg , `` on the fundamental limits of caching in combination networks , '' in _",
    "ieee 16th international workshop on signal processing advances in wireless communications ( spawc ) _ , pp .",
    "695 - 699 , june 2015 ."
  ],
  "abstract_text": [
    "<S> we consider a system , containing a library of multiple files and a general memoryless communication network through which a server is connected to multiple users , each equipped with a local isolated cache of certain size that can be used to store part of the library . </S>",
    "<S> each user will ask for one of the files in the library , which needs to be delivered by the server through the intermediate communication network . the objective is to design the cache placement ( without prior knowledge of users future requests ) and the delivery phase in order to minimize the ( normalized ) delivery delay . </S>",
    "<S> we assume that the delivery phase consists of two steps : ( 1 ) generation of a set of multicast messages at the server , one for each subset of users , and ( 2 ) delivery of the multicast messages to the users .    in this setting , we show that there exists a universal scheme for cache placement and multicast message generation , which is independent of the underlying communication network between the server and the users , and achieves the optimal delivery delay to within a constant factor for all memoryless networks . </S>",
    "<S> we prove this result , even though the capacity region of the underlying communication network is not known , even approximately . </S>",
    "<S> this result demonstrates that in the aforementioned setting , a separation between caching and multicast message generation on one hand , and delivering the multicast messages to the users on the other hand is approximately optimal . </S>",
    "<S> this result has the important practical implication that the prefetching can be done independent of network structure in the upcoming delivery phase . </S>"
  ]
}