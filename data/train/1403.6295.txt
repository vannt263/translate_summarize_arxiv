{
  "article_text": [
    "density - based minimum distance methods provide attractive alternatives to likelihood based methods in parametric inference .",
    "often these estimators combine strong robustness properties with full asymptotic efficiency .",
    "the estimators based on the family of power divergences ( cressie and read , 1984 ) is one such example .",
    "consider the class @xmath1 of all probability density functions on the @xmath2-field @xmath3 .",
    "usually , in practice @xmath4 and @xmath5 is the corresponding borel @xmath2-field .",
    "the power divergence measure between two densities @xmath6 and @xmath7 in @xmath1 , indexed by a parameter @xmath8 , is defined as @xmath9.\\ ] ] here , the integral is taken over the whole sample space @xmath10 and the same is to be understood in the rest of the paper also , unless mentioned otherwise . for values of @xmath11 and @xmath12",
    "the family generates the pearson s chi - square ( pcs ) , the likelihood disparity ( ld ) , the twice squared hellinger distance ( hd ) , the kullback - leibler divergence ( kld ) and the neyman s chi - square ( ncs ) respectively .",
    "the family is a subclass of the larger family of @xmath13-divergences ( csiszr , 1963 ) or disparities .",
    "the minimum disparity estimator of @xmath14 under the model @xmath15 is the minimizer of the divergence between @xmath16 ( a nonparametric estimate of the true density @xmath6 ) and the model density @xmath17 .",
    "all minimum distance estimators based on disparities have the same influence function as that of the maximum likelihood estimator ( mle ) at the model and hence have the same asymptotic model efficiency .",
    "the evaluation of a minimum distance estimator based on disparities requires kernel density estimation , and hence inherits all the complications of the latter method .",
    "basu et al .",
    "( 1998 ) developed a class of density - based divergence measures called the density power divergence ( dpd ) that produces robust parameter estimates but needs no nonparametric smoothing .",
    "the dpd measure between two densities @xmath6 and @xmath7 in @xmath1 is defined , depending on a nonnegative parameter @xmath18 , as @xmath19 the parameter @xmath18 provides a smooth bridge between the likelihood disparity ( @xmath20 ) and the @xmath21-divergence ( @xmath22 ) ; it also controls the trade - off between the robustness and efficiency with larger @xmath18 being associated with greater robustness but reduced efficiency .",
    "both the pd and dpd families provide outlier down weighting using powers of model densities .    combining the concepts of the power divergence and the density power divergence , ghosh et al .",
    "( 2013 ) developed a two parameter family of density - based divergences , named as  @xmath0-divergence \" , that connects the whole of the cressie - read family of power divergence smoothly to the @xmath21-divergence at the other end .",
    "this family contains both the pd and dpd families as special cases . through various numerical examples ,",
    "they illustrate that the minimum divergence estimators based on the @xmath0-divergence are also extremely robust and are also competitive in terms efficiency for most of the members of this family .    in this present article",
    ", we will develop the theoretical properties of the minimum @xmath0-divergence estimators . for simplicity",
    ", here we consider only the set up for the discrete model so that the true data generating probability mass functions can be estimated non - parametrically by just the relative frequencies of the observed sample  we do not need to consider any nonparametric smoothing .",
    "we will prove the consistency and asymptotic normality of the minimum @xmath0-divergence estimators .",
    "we will introduce the @xmath0-divergence and the minimum @xmath0-divergence estimator in section [ sec : s - div ] and [ sec : msde ] respectively .",
    "then section [ sec : asymptotics ] will contain the asymptotic properties of the minimum @xmath0-divergence estimators .",
    "we will present the application of the minimum @xmath0-divergence estimator in some interesting real data examples in section [ sec : example ] and relate the findings with the theoretical results in section [ sec : choice_par ] , that leads to some indication on the choice of the tuning parameters .",
    "finally we conclude the paper by an overall conclusion in section [ sec : conclusion ] . throughout the rest of the paper , we will use the term  density \" for the probability mass functions also .",
    "it is well - known that the estimating equation for the minimum density power divergence represents an interesting density power down - weighting , and hence robustification of the usual likelihood score equation ( basu et al . , 1998 ) .",
    "the usual estimating equations for the mle can be recovered from that estimating equation by the choice @xmath20 . within the given range of @xmath23",
    ", @xmath24 will lead to the maximum down - weighting for the score functions of the surprising observations corresponding to the @xmath21 divergence ; on the other extreme , the score functions will be subjected to no down - weighting at all for @xmath25 corresponding to the kullback - leibler divergence ( kullback and leibler , 1951 ) .",
    "intermediate values of @xmath18 provide a smooth bridge between these two estimating equations , and the degree of down weighting increases with increasing @xmath18 .",
    "noting that the kullback - leibler divergence is a particular case of cressie - read family of power divergence corresponding to @xmath26 , we see that the density power divergence gives us a smooth bridge between one particular member of the cressie - read family and the @xmath21 divergence with increasing robustness .",
    "ghosh et al .",
    "( 2013 ) constructed a family of divergences which connect , in a similar fashion , other members of the pd family with the @xmath21-divergence .",
    "that larger super - family , named as the @xmath0-divergence family , is defined as    @xmath27   \\nonumber \\\\ \\nonumber \\\\   & = &   \\frac{1}{a } ~ \\int ~ f^{1+\\alpha }   -    \\frac{1+\\alpha}{a b } ~ \\int ~~ f^{b } g^{a }    + \\frac{1}{b } ~ \\int ~~ g^{1+\\alpha } , \\label{eq : s_div_defn } \\\\",
    "\\nonumber \\end{aligned}\\ ] ]    where @xmath28 and @xmath29 .",
    "note that , @xmath30 .",
    "also the above form of divergence family is defined for those @xmath18 and @xmath31 for which @xmath32 and @xmath33 .",
    "if @xmath34 then the corresponding divergence measure is defined as the continuous limit of ( [ eq : s_div_defn ] ) as @xmath35 and is given by @xmath36 similarly , if @xmath37 then the divergence measure is defined to be @xmath38    note that for @xmath20 , the class of @xmath0-divergences reduces to the pd family with parameter @xmath31 ; for @xmath39 , @xmath40 equals the @xmath21 divergence irrespective of the value of @xmath31 . on the other hand",
    ", @xmath26 generates the dpd family as a function of @xmath18 . in ghosh",
    "et al .  ( 2013a ) , it was shown that the above @xmath0-divergence family defined in ( [ eq : s_div_defn ] ) , ( [ eq : s_div_defn_a0 ] ) and ( [ eq : s_div_defn_b0 ] ) indeed represent a family of genuine statistical divergence measures in the sense that @xmath41 for densities @xmath42 and all @xmath43 , @xmath8 , and @xmath44 is equal to zero if and only if @xmath45 identically .",
    "let us now consider the discrete set - up for parametric estimation . let @xmath46 , @xmath47 , @xmath48 denotes @xmath49 independent and identically distributed observations from the true distribution @xmath50 having a probability density function @xmath6 with respect to some counting measure . without loss of generality , we will assume that the support of @xmath6 is @xmath51 .",
    "let us denote the relative frequency at @xmath52 obtained from data by @xmath53 , where @xmath54 denotes the indicator function of the event @xmath55 .",
    "we model the true data generating distribution @xmath50 by the parametric model family @xmath15 .",
    "we will assume that both @xmath50 and @xmath56 belong to @xmath1 , the ( convex ) class of all distributions having densities with respect to the counting measure ( or the appropriate dominating measure in other cases ) .",
    "we are interested in the estimation of the parameter @xmath14 .",
    "note that , the minimum @xmath0-divergence estimator has to be obtained by minimizing the @xmath0-divergence measure between the data and the model distribution .",
    "however , in the discrete set - up , both the data - generating true distribution and the model distribution are characterized by the probability vectors @xmath57 and @xmath58 respectively .",
    "thus in this case , the minimum @xmath0-divergence estimator of @xmath14 can be obtained by just minimizing @xmath59 , the @xmath0-divergence measure between @xmath60 and @xmath61 , with respect to @xmath14 . the estimating equation",
    "is then given by @xmath62 = 0 , \\nonumber\\\\ \\mbox{or ,    } & &   \\frac{1+\\alpha}{a } \\sum_{x=0}^\\infty f_{\\theta}^{1+\\alpha}(x ) u_{\\theta}(x )   - \\frac{1+\\alpha}{a } \\sum_{x=0}^\\infty f_{\\theta}^{b}(x ) r_n^{a}(x ) u_{\\theta}(x ) = 0 , \\\\",
    "\\mbox{or ,    } & & \\sum_{x=0}^\\infty k(\\delta(x))f_{\\theta}^{1+\\alpha}(x ) u_{\\theta}(x ) = 0 , \\label{eq : s - est_eqn_discrete}\\end{aligned}\\ ] ] where @xmath63 , @xmath64 and @xmath65 is the likelihood score function .",
    "note that , @xmath66 represents the derivative with respect to @xmath14 and we will denote its @xmath67 component by @xmath68 .",
    "the model family @xmath83 is identifiable , i.e. , for any two @xmath84 and @xmath85 in the model family @xmath83 , @xmath86 2 .",
    "the probability density function @xmath17 of the model distribution have common support so that the set @xmath87 is independent of @xmath14 . also the true distribution @xmath6 is compatible with the model family .",
    "there exists an open subset @xmath88 for which the best fitting parameter @xmath73 is an interior point and for almost all x , the density @xmath89 admits all the third derivatives of the type @xmath90 @xmath91 . here , @xmath92 denotes the @xmath93 element of @xmath94 , the third order derivative with respect to @xmath14 4 .",
    "the matrix @xmath95 is positive definite .",
    "5 .   the quantities @xmath96 , @xmath97 and @xmath98 are bounded @xmath99 and @xmath91 . + here",
    ", @xmath100 denotes the @xmath101 element of @xmath102 and @xmath103 denotes the @xmath104 element of @xmath105 .",
    "6 .   for almost all @xmath52",
    ", there exists functions @xmath106 , @xmath107 , @xmath108 that dominate , in absolute value , @xmath109 , @xmath110 and + @xmath111 respectively @xmath112 and that are uniformly bounded in expectation with respect to @xmath6 and @xmath113 .",
    "+ here , @xmath114 denotes the @xmath93 element of @xmath115 . 7 .",
    "the function @xmath116 is uniformly bounded ( by , say , @xmath117 ) @xmath91 .",
    "+    to prove the consistency and asymptotic normality of the minimum @xmath0-divergence estimator , we will , now on , assume that the above @xmath118 conditions hold .",
    "we will first consider some lemmas .",
    "define @xmath119 .",
    "for any @xmath120 $ ] and any @xmath121 , we have    1 .   @xmath122 \\le n^{\\frac{k}{2}}e_g[|\\delta_n(x ) - \\delta_g(x)|]^k \\le   \\left [ \\frac{g(x)(1-g(x))}{f_{\\theta}^2(x)}\\right]^{\\frac{k}{2}}$ ] .",
    "@xmath123 \\le \\frac{2g(x)(1-g(x))}{f_{\\theta}(x)}$ ] .",
    "[ lem : lemma_3.1 ]    * proof :* the proof uses the same argument as in lemma 2.13 of basu et al .",
    "( 2013 , page 56 ) . for @xmath124",
    ", we have the inequality @xmath125 .",
    "so we get @xmath126 & = & n^{\\frac{k}{2 } } e_g\\left[\\left(\\sqrt{\\delta_n(x)}-\\sqrt{\\delta_g(x)}\\right)^{2k}\\right ] \\nonumber\\\\   & = & n^{\\frac{k}{2 } } e_g\\left[\\left(\\sqrt{\\delta_n(x)}-\\sqrt{\\delta_g(x)}\\right)^{2}\\right]^k\\nonumber \\\\ & \\le & n^{\\frac{k}{2 } } e_g[|\\delta_n(x ) - \\delta_g(x)|]^k .",
    "\\nonumber \\end{aligned}\\ ] ]    for the next part see that , under @xmath6 , @xmath127 .",
    "now , for any @xmath120 $ ] , we get by the lyapounov s inequality that @xmath128^k & \\le & \\left [ e_g(\\delta_n(x ) - \\delta_g(x))^2 \\right]^{\\frac{k}{2 } }   \\nonumber \\\\      & = & \\frac{1}{f_{\\theta}^k(x ) } \\left [ e_g(r_n(x ) - g(x))^2 \\right]^{\\frac{k}{2 } } \\nonumber \\\\          & = & \\frac{1}{f_{\\theta}^k(x ) } \\left [ \\frac{g(x)(1-g(x))}{n}\\right]^{\\frac{k}{2}}. \\nonumber \\\\   \\nonumber\\end{aligned}\\ ] ]    for the second part , note that @xmath128 & = & \\frac{1}{f_{\\theta}^k(x ) } \\left [ e_g|r_n(x ) - g(x)| \\right]^{\\frac{k}{2 } }   \\nonumber \\\\          & \\le & \\frac{2g(x)(1-g(x))}{f_{\\theta}(x ) } , \\nonumber \\end{aligned}\\ ] ] where the last inequality follows from the result about the mean - deviation of a binomial random variable .",
    "@xmath122 \\rightarrow 0 $ ] , as @xmath129 , for @xmath130 and @xmath121 .",
    "[ lem : lemma_3.2 ]    * proof :* this follows from theorem 4.5.2 of chung ( 1974 ) by noting that @xmath131 with probability one for each @xmath121 and by the lemma [ lem : lemma_3.1](1 ) , @xmath132 $ ] is bounded .",
    "let us now define , @xmath133 we will need the limiting distributions of +    @xmath134 and @xmath135 .    next two lemmas will help us to derive those distributions",
    ".    assume condition ( sa5 ) .",
    "then , @xmath136 and hence @xmath137 [ lem : lemma_3.3 ]      suppose @xmath145 is finite .",
    "then under @xmath6 , @xmath146 [ lem : lemma_3.4 ]      we will now consider the main theorem of this section about the consistency and asymptotic normality of the minimum @xmath0-divergence estimator . +    under assumptions ( sa1)-(sa7 ) , there exists a consistent sequence @xmath150 of roots to the minimum @xmath0-divergence estimating equation ( [ eq : s - est_eqn_discrete ] ) .",
    "+ also , the asymptotic distribution of @xmath151 is @xmath152dimensional normal with mean @xmath153 and variance @xmath154 .",
    "+    * proof of consistency : * consider the behavior of @xmath155 on a sphere @xmath156 which has radius @xmath157 and center at @xmath73 .",
    "we will show , for sufficiently small @xmath157 , the probability tends to one that @xmath158 so that the @xmath0-divergence has a local minimum with respect to @xmath14 in the interior of @xmath156 . at a local minimum",
    ", the estimating equations must be satisfied .",
    "therefore , for any @xmath159 sufficiently small , the minimum @xmath0-divergence estimating equation have a solution @xmath160 within @xmath156 with probability tending to one as @xmath129 .",
    "therefore , by lehmann ( 1983 , lemma 4.1 ) , it follows that @xmath224 has asymptotic distribution as @xmath225 .",
    "when the true distribution @xmath50 belongs to the model family , i.e. , @xmath226 for some @xmath75 , then @xmath227 has asymptotic distribution as @xmath228 , where @xmath229 =   \\int u_\\theta(x)u_\\theta^t(x)f_\\theta^{1+\\alpha}(x ) dx , \\\\",
    "v_\\alpha = v_\\alpha(f_\\theta ) & = & v_{f_\\theta}[u_\\theta(x)f_\\theta^{\\alpha}(x ) ] =   \\int u_\\theta(x)u_\\theta^t(x)f_\\theta^{1 + 2\\alpha}(x ) dx - \\xi \\xi^t , \\\\       \\xi = \\xi_\\alpha(f_\\theta ) & = & e_{f_\\theta}[u_\\theta(x)f_\\theta^{\\alpha}(x ) ] =   \\int u_\\theta(x)f_\\theta^{1+\\alpha}(x)dx.\\end{aligned}\\ ] ] note that , this asymptotic distribution is independent of the parameter @xmath31 in the @xmath0-divergence family . +    * proof : * note that , under @xmath226 for some @xmath75 , we get @xmath230 so that @xmath231 and @xmath232",
    ". thus the result follows from the above theorem by noting that @xmath233 and @xmath234 .",
    "note that , the asymptotic variance of the proposed minimum @xmath0-divergence estimator depends only on the parameter @xmath18 at the model family and hence coincides with that of the minimum density power divergence estimator ( which corresponds to the case @xmath235 ) of basu et al .",
    "further , interestingly , at the case @xmath236 , this asymptotic variance of the msde coincides with the inverse of the fisher information matrix @xmath237 $ ] ( @xmath238 and @xmath239 ) irrespectively of @xmath31 as expected ; note that the msde with @xmath240 is in fact the mle having the minimum asymptotic variance at the model .",
    "thus , the asymptotic relative efficiency ( are ) of the minimum @xmath0-divergence estimators @xmath241 can be calculated by comparing its asymptotic variance with that under the case @xmath240 .",
    "for example , when @xmath242 , we can define @xmath243 this measure is easy to calculate for the common parametric models .",
    "however , it is to be noted that the all the above asymptotic results hold under the assumptions ( sa1)(sa7 ) and one should check these conditions before applying the proposed msde to any parametric model .",
    "we have verified this conditions to hold for most common parametric models . in the following we will present the examples of two particular models to illustrate the validity of the assumptions ( sa1)(sa7 ) and the usefulness of the asymptotic results derived above .",
    "* example 1 : poisson model * + first let us consider the popular parametric model of poisson distribution with mean @xmath14 .",
    "we will verify that conditions ( sa1)(sa7 ) hold for this model assuming that the true density @xmath6 is also a poisson distribution with mean @xmath244 . clearly , the poisson model family is identifiable with the open parameter space @xmath245 and it has support @xmath246 , the set of all non - negative integers , which is independent of the mean parameter @xmath14 .",
    "further , the density of the poisson distribution is continuous in @xmath14 and is given by @xmath247 thus , clearly ( sa1)(sa3 ) holds for this model family .",
    "next , note that , in this case , we have @xmath248 so , using the boundedness of the functions @xmath249 , where @xmath179 is a positive integer , on the domain @xmath250 , one can easily show that the conditions ( sa5 ) and ( sa6 ) hold true .",
    "further , using the above forms , we have , for the poisson model , @xmath251 which is clearly a positive real number implying ( sa4 ) holds .",
    "finally , to show ( sa7 ) , note that @xmath252 which is clearly uniformly bounded in @xmath253 .",
    "hence all the assumptions ( sa1)(sa7 ) hold under the poisson model .    now , applying the above theorem 1 ,",
    "the msde of the poisson parameter @xmath14 is consistent and asymptotically normal with variance given by @xmath254 , where @xmath255 is as defined above and @xmath256 this asymptotic variance can be calculated by a simple numerical summation and can be compared with the corresponding fisher information matrix @xmath257 to examine the asymptotic relative efficiencies of the msdes .",
    "note that , for the poisson model , the fisher information matrix is given by @xmath258 table [ tab : are_msde ] presents the value of are for several msdes ; note that as seen in corollary 1 the asymptotic variance and hence the are of the msde is independent of the parameter @xmath31 .",
    "it can be seen from the table that the are of msde is maximum ( @xmath259 ) if @xmath236 ; as @xmath18 increases the efficiency decreases .",
    "l r| rrrrrrr & & + & & 0 & 0.05 & 0.1 & 0.3 & 0.5 & 0.7 & 1 + & 2 & 100 & 99.62 & 98.77 & 93.06 & 86.15 & 79.55 & 71.17 + & 3 & 100 & 99.66 & 98.82 & 92.86 & 85.18 & 77.42 & 68.22 + poisson & 5 & 100 & 99.61 & 98.80 & 92.38 & 84.19 & 76.96 & 66.47 + & 10 & 100 & 99.66 & 98.75 & 92.07 & 83.86 & 76.07 & 65.69 + & 15 & 100 & 99.66 & 98.83 & 92.09 & 83.76 & 75.71 & 65.59 + & 0.1 & 100 & 99.10 & 96.78 & 81.93 & 68.42 & 59.24 & 51.06 + & 0.2 & 100 & 99.10 & 96.79 & 82.01 & 68.59 & 59.49 & 51.45 + geometric & 0.5 & 100 & 99.14 & 96.92 & 82.90 & 70.37 & 62.19 & 55.64 + & 0.7 & 100 & 99.21 & 97.19 & 84.71 & 73.98 & 67.54 & 63.61 + & 0.9 & 100 & 99.43 & 98.03 & 90.04 & 84.07 & 81.56 & 82.15 +    [ tab : are_msde ]    * example 2 : geometric model * + now consider another popular parametric model family of geometric distribution with success probability @xmath14 .",
    "again we will verify conditions ( sa1)(sa7 ) assuming that the true density @xmath6 belongs to the model family with parameter value @xmath244 . clearly , the geometric family is identifiable with the open parameter space @xmath260 and support @xmath261 , the set of all positive integers , which is independent of the parameter @xmath14 .",
    "the geometric model also has a continuous density ( in @xmath14 ) given by @xmath262 so , assumptions ( sa1)(sa3 ) holds for the geometric model family .",
    "also , in this case , we have @xmath263 @xmath264 thus , one can easily prove the conditions ( sa5 ) and ( sa6 ) hold true for this geometric model .",
    "further , we get @xmath265 } { ( 1-\\theta)^2 t_\\alpha(\\theta)},\\ ] ] where @xmath266 . clearly , @xmath255 is a positive real number for all @xmath267 and so ( sa4 ) holds .",
    "finally , we have @xmath268 which is clearly uniformly bounded in @xmath267 by @xmath269 . hence all the assumptions ( sa1)(sa7 )",
    "holds under the geometric model also .",
    "now , applying theorem 1 , we have that the msde of parameter @xmath14 is consistent and asymptotically normal with variance @xmath254 , where @xmath270 } { ( 1-\\theta)^2 t_{2\\alpha}(\\theta ) } - \\left(\\frac{\\theta^{2\\alpha}(1-(1-\\theta)^\\alpha)^2}{t_\\alpha(\\theta)^4}\\right).\\ ] ] under the geometric model , the fisher information matrix @xmath257 has the simple form @xmath271 we can again compute the are of the msdes of the geometric parameter using the above expressions , which is reported in table [ tab : are_msde ] .",
    "clearly , the table shows that the asymptotic relative efficiency decreases as @xmath18 increases",
    ". however , there is no significant loss in efficiency at the smaller positive values of @xmath18 .",
    "here we consider a chemical mutagenicity experiment .",
    "these data were analyzed previously by simpson ( 1987 ) .",
    "the details of the experimental protocol are available in woodruff et al .",
    "( 1984 ) . in a sex linked recessive lethal test in drosophila ( fruit flies ) , the experimenter exposed groups of male flies to different doses of a chemical to be screened .",
    "each male was then mated with unexposed females . sampling 100 daughter flies from each male ( roughly ) , the number of daughters carrying a recessive lethal mutation on the @xmath77 chromosome was noted .",
    "the data set consisted of the observed frequencies of males having @xmath272 recessive lethal daughters . for our purpose , we consider two specific experimental runs  one on the day 28 and second on day 177 .",
    "the data of the first run consist of two small outliers with observed frequencies @xmath273 at @xmath274 and that of second run consists of observed frequencies @xmath275 at @xmath276 with a large outlier at @xmath277 .",
    "r| l l l l l l l l @xmath31 & @xmath278 0 & @xmath278 0.1 & @xmath278 0.25 & @xmath278 0.4 & @xmath278 0.5 & @xmath278 0.6 & @xmath278 0.8 & @xmath278 1 + @xmath279 &  & 0.08 & 0.11 & 0.12 & 0.12 & 0.12 & 0.13 & 0.13 + @xmath280 & 0.09 & 0.10 & 0.12 & 0.12 & 0.12 & 0.13 & 0.13 & 0.13 + @xmath281 & 0.10 & 0.11 & 0.12 & 0.12 & 0.12 & 0.13 & 0.13 & 0.13 + @xmath282 & 0.11 & 0.12 & 0.12 & 0.12 & 0.12 & 0.13 & 0.13 & 0.13 + @xmath283 & 0.11 & 0.12 & 0.12 & 0.12 & 0.13 & 0.13 & 0.13 & 0.13 + 0 & 0.12 & 0.12 & 0.12 & 0.12 & 0.13 & 0.13 & 0.13 & 0.13 + 0.5 & 0.12 & 0.12 & 0.12 & 0.13 & 0.13 & 0.13 & 0.13 & 0.13 + 1 & 0.12 & 0.12 & 0.13 & 0.13 & 0.13 & 0.13 & 0.13 & 0.13 + 1.3 & 0.12 & 0.12 & 0.13 & 0.13 & 0.13 & 0.13 & 0.13 & 0.13 + 1.5 & 0.12 & 0.12 & 0.13 & 0.13 & 0.13 & 0.13 & 0.13 & 0.13 + 2 & 0.12 & 0.13 & 0.13 & 0.13 & 0.13 & 0.13 & 0.13 & 0.13 +    [ tab : wo_d1 ]    r| l l l l l l l l @xmath31 & @xmath278 0 & @xmath278 0.1 & @xmath278 0.25 & @xmath278 0.4 & @xmath278 0.5 & @xmath278 0.6 & @xmath278 0.8 & @xmath278 1 + @xmath279 &  & 0.08 & 0.11 & 0.13 & 0.14 & 0.14 & 0.15 & 0.16 + @xmath280 & 0.10 & 0.11 & 0.13 & 0.14 & 0.14 & 0.15 & 0.16 & 0.16 + @xmath281 & 0.13 & 0.13 & 0.13 & 0.14 & 0.14 & 0.15 & 0.16 & 0.16 + @xmath282 & 0.18 & 0.15 & 0.14 & 0.14 & 0.14 & 0.15 & 0.16 & 0.16 + @xmath283 & 0.29 & 0.22 & 0.16 & 0.15 & 0.15 & 0.15 & 0.16 & 0.16 + 0 & 0.36 & 0.26 & 0.18 & 0.15 & 0.15 & 0.15 & 0.16 & 0.16 + 0.5 & 0.59 & 0.49 & 0.34 & 0.21 & 0.17 & 0.16 & 0.16 & 0.16 + 1 & 0.70 & 0.63 & 0.49 & 0.32 & 0.18 & 0.17 & 0.16 & 0.16 + 1.3 & 0.75 & 0.68 & 0.55 & 0.39 & 0.28 & 0.19 & 0.16 & 0.16 + 1.5 & 0.77 & 0.71 & 0.59 & 0.44 & 0.32 & 0.25 & 0.16 & 0.16 + 2 & 0.81 & 0.76 & 0.66 & 0.52 & 0.40 & 0.27 & 0.16 & 0.16 +    [ tab : w_d1 ]    poisson models are fitted to the data for this experimental runs by estimating the poisson parameter using minimum @xmath0-divergence estimation for several values of @xmath18 and @xmath31",
    ". a quick look at the observed frequencies for the experimental run reveals that there is an exceptionally large count  where one male is reported to have produced 91 daughters with the recessive lethal mutation .",
    "we estimate the poisson parameter from this data with the outlying observation and without that outlying observation .",
    "the difference in these two estimates gives an indication of the robust behavior ( or lack thereof ) of different minimum @xmath0-divergence estimators .",
    "our findings are reported in tables [ tab : wo_d1 ] to [ tab : w_d2 ] .",
    "the values of the minimum @xmath0-divergence estimators given in these tables clearly demonstrate their robustness with respect to the outlying value for all @xmath284 $ ] if @xmath285 . for @xmath26 the minimum @xmath0-divergence estimators are also robust for large values of @xmath18 , but smaller values of @xmath18 are highly non - robust ( note that @xmath20 and @xmath286 gives the mle ) . for @xmath287",
    "the msdes corresponding to small values of @xmath18 close to zero are highly sensitive to the outlier ; this sensitivity decreases with @xmath18 , and eventually the outlier has negligible effect on the estimator when @xmath18 is very close to 1 .",
    "the robustness of the estimators decrease sharply with increasing @xmath31 except when @xmath22 ; note that this particular case with @xmath288 gives the @xmath21 divergence irrespective of the value of @xmath31 which is highly robust but inefficient .",
    "r| l l l l l l l l @xmath31 & @xmath278 0 & @xmath278 0.1 & @xmath278 0.25 & @xmath278 0.4 & @xmath278 0.5 & @xmath278 0.6 & @xmath278 0.8 & @xmath278 1 + @xmath279 &  & 0.29 & 0.35 & 0.36 & 0.36 & 0.35 & 0.35 & 0.35 + @xmath280 & 0.34 & 0.35 & 0.36 & 0.36 & 0.36 & 0.36 & 0.35 & 0.35 + @xmath281 & 0.36 & 0.37 & 0.37 & 0.36 & 0.36 & 0.36 & 0.35 & 0.35 + @xmath282 & 0.38 & 0.38 & 0.37 & 0.37 & 0.36 & 0.36 & 0.35 & 0.35 + @xmath283 & 0.39 & 0.39 & 0.38 & 0.37 & 0.37 & 0.36 & 0.35 & 0.35 + 0 & 0.39 & 0.39 & 0.38 & 0.37 & 0.37 & 0.36 & 0.35 & 0.35 + 0.5 & 0.41 & 0.40 & 0.39 & 0.38 & 0.37 & 0.36 & 0.35 & 0.35 + 1 & 0.42 & 0.42 & 0.40 & 0.39 & 0.32 & 0.37 & 0.36 & 0.35 + 1.3 & 0.43 & 0.42 & 0.41 & 0.39 & 0.38 & 0.37 & 0.36 & 0.35 + 1.5 & 0.43 & 0.42 & 0.41 & 0.39 & 0.38 & 0.37 & 0.36 & 0.35 + 2 & 0.44 & 0.43 & 0.42 & 0.40 & 0.39 & 0.37 & 0.36 & 0.35 +    [ tab : wo_d2 ]    r| l l l l l l l l @xmath31 & @xmath278 0 & @xmath278 0.1 & @xmath278 0.25 & @xmath278 0.4 & @xmath278 0.5 & @xmath278 0.6 & @xmath278 0.8 & @xmath278 1 + @xmath279 &  & 0.30 & 0.35 & 0.36 & 0.36 & 0.36 & 0.36 & 0.36 + @xmath280 & 0.34 & 0.36 & 0.37 & 0.37 & 0.37 & 0.37 & 0.36 & 0.36 + @xmath281 & 0.36 & 0.37 & 0.37 & 0.37 & 0.37 & 0.37 & 0.37 & 0.36 + @xmath282 & 0.38 & 0.38 & 0.38 & 0.37 & 0.37 & 0.37 & 0.37 & 0.36 + @xmath283 & 0.39 & 0.39 & 0.38 & 0.38 & 0.37 & 0.37 & 0.37 & 0.36 + 0 & 3.03 & 0.39 & 0.39 & 0.38 & 0.37 & 0.37 & 0.37 & 0.36 + 0.5 & 31.31 & 30.28 & 25.12 & 0.39 & 0.38 & 0.37 & 0.37 & 0.36 + 1 & 32.20 & 31.84 & 30.79 & 27.08 & 0.99 & 0.38 & 0.37 & 0.36 + 1.3 & 32.40 & 32.15 & 31.48 & 29.71 & 24.93 & 0.38 & 0.37 & 0.36 + 1.5 & 32.50 & 32.29 & 31.76 & 30.48 & 27.78 & 22.54 & 0.37 & 0.36 + 2 & 33.22 & 32.50 & 32.15 & 31.43 & 30.28 & 26.24 & 0.37 & 0.36 +    [ tab : w_d2 ]      now we will consider another interesting real data example on the incidence of peritonitis for 390 kidney patients ( basu et al .",
    ", 2011 , table 2.4 ) .",
    "basu et al .",
    "( 2011 ) examined this data by fitting a geometric distribution with parameter @xmath14 ( success probability ) around 0.5 and observed that there are two mild to moderate outliers at the points @xmath289 and @xmath290 that moderately affect the non - robust estimators .",
    "the effect of outliers is not so dramatic here as in the previous example due to its large sample size .",
    "thus , this data set provides another interesting situation to examine the performance of any robust estimator .",
    "we will apply the proposed minimum @xmath0-divergence estimators to estimate the geometric parameter for this data set  once ignoring the two outlying observations and once considering the full data .",
    "the estimated values ae reported in tables [ tab : wo_pk ] and [ tab : w_pk ] respectively .",
    "again , we can see from the tables that the minimum @xmath0-divergence estimators differ significantly even in the presence of mild outliers for all smaller values of @xmath18 with @xmath287 ; but the msdes with @xmath291 or larger values of @xmath18 with @xmath292 remain more stable with respect to the outlying observations as seen in the previous example .    [",
    "tab : wo_pk ]    [ tab : w_pk ]",
    "in the last two sections we have observed the following : ( a ) the asymptotic distributions of the proposed minimum @xmath0-divergence estimators ( and hence their asymptotic relative efficiencies ) are independent of the parameter @xmath31 ; and ( b ) the behavior of the estimators with respect to robustness against outliers are widely different for different combinations of @xmath18 and @xmath31 , and sometimes even vary greatly over different values of @xmath31 for fixed @xmath18 .",
    "these observations indicate that a proper discussion of the role of the two tuning parameters are important in this context , and in this connection we record the following points .",
    "for part of this discussion we borrow from the ghosh et al .",
    "( 2013 ) paper , which describes the robustness issues related to the @xmath0-divergence , unlike the present paper which primarily concentrates on the asymptotic efficiency results of the corresponding estimators .    1 .",
    "the influence function of the minimum @xmath0-divergence estimators are independent of @xmath31 .",
    "this has , in fact , been directly observed by ghosh et al .",
    "( 2013 ) who evaluated the influence function of the minimum @xmath0-divergence estimators ; see ghosh et al .",
    "( 2013 ) , section 4.2 . 2 .",
    "our examples clearly show , however , that the true stability of our proposed estimators against outliers are not identical over @xmath31 for fixed values of @xmath18 . the estimators at @xmath20 ( or low values close to zero )",
    "are highly influenced by the choice of the value of @xmath31 under the presence of outliers .",
    "3 .   this indicates , further , that the influence function of the @xmath0-divergence estimators are not able to fully predict the robustness behavior of the minimum @xmath0-divergence estimators .",
    "ghosh et al .",
    "( 2013 ) , have , in fact , demonstrated that for different choices of the tuning parameters , the second order influence function prediction can be widely different from the first , and the discrepancy may be in either direction .",
    "we refer the reader to ghosh et al .  for an extensive discussion of this phenomenon , including theoretical calculations of the first and second order influence functions , extensive simulations and detailed graphical studies .",
    "another issue of importance that immediately presents itself on the basis of the above discussion is the choice of the tuning parameters which could be the most appropriate in a particular situation , where the experimenter is unaware of the purity of the data or about the nature of possible contaminations .",
    "this is clearly an issue which will require more research .",
    "however , on the basis of our empirical findings of section [ sec : example ] and theoretical efficiencies of section [ sec : asymptotics ] ( table [ tab : are_msde ] ) , it would appear that low values of @xmath18 ( say between 0.1 and 0.25 ) with moderately large negative values of @xmath31 ( say beteen @xmath282 and @xmath281 ) should be the more appropriate choices",
    ".    a further obvious application of the divergences considered in this paper would be in the case of testing of parametric hypothesis .",
    "some indications of the potential of the proposed divergence in this connection has been presented in ghosh et al .",
    "( 2014 ) , where one uses the form of the @xmath0-divergence to quantify the discrepancy between the null distribution and the empirical distribution .",
    "a simplifying application is to use the minimum dpd estimator in this connection in place of the actual minimum @xmath0-divergence estimator . as the distribution of our proposed estimators in section [ sec : asymptotics ] do not depend on @xmath31 , the test statistics proposed by ghosh et al .",
    "( 2014 ) have the same asymptotic distributions as one would get if the original minimum @xmath0-divergence estimators were used .",
    "the proposed @xmath0-divergence based inference has enough potential for application in several applied field where the observed data is supposed to contains outlying observations .",
    "one possible application of the minimum @xmath0-divergence estimator in the context of reliability is described in ghosh , maji and basu ( 2013 ) .",
    "further works are to be done in future to examine its properties in other applications .",
    "the @xmath0-divergence family generates a large class of divergence measures having several important properties .",
    "thus , the minimum divergence estimators obtained by minimizing these different members of the @xmath0-divergences family also have several interesting properties in terms of their efficiency and robustness . in this present paper",
    ", we have proved the asymptotic properties of the minimum @xmath0-divergence estimators under the discrete set - up .",
    "interestingly , we have seen that the asymptotic distributions of the minimum @xmath0-divergence estimators at the model is independent of one defining parameter @xmath31 , although their robustness depends on this parameter value . indeed , considering the minimum @xmath0-divergence estimators as members of a grid constructed based on its defining parameters @xmath31 and @xmath18",
    ", we can clearly observe a triangular region of non - robust estimators corresponding to the large @xmath293 and small @xmath18 values and a region of highly robust estimators corresponding to moderate @xmath18 and large negative @xmath31 values . as a future work",
    ", we need to prove all the properties of the minimum @xmath0-divergence estimators under the continuous models .",
    "however , under the continuous model , we need to use the kernel smoothing to estimate the true density @xmath6 and hence proving the asymptotic properties will inherit all the complications of the kernel estimation like bandwidth selection etc . we will try to solve these issues in our subsequent papers .    the author would like to thank his ph.d .",
    "supervisor prof .",
    "ayanendranath basu ( indian statistical institute , india ) for his sincere guidance and valuable comments about this work .",
    "the author also wishes to thank two anonymous referees for their comments that helped to improve the content and presentation of this paper .",
    "ghosh , a. , harris , i. r. , maji , a. , basu , a. and pardo , l. ( 2013 ) . a generalized divergence for statistical inference .",
    "_ technical report _ , * biru/2013/3 * , bayesian and interdisiplinary research unit , indian statistical institute , kolkata , india .",
    "ghosh , a. , a.  maji , and a.  basu ( 2013 ) .",
    "_ robust inference based on divergences in reliability systems_. _ applied reliability engineering and risk analysis .",
    "probabilistic models and statistical inference _ , ilia frenkel , alex , karagrigoriou , anatoly lisnianski & andre kleyner , eds , dedicated to the centennial of the birth of boris gnedenko , wiley , new york , usa .            woodruff , r. c. , j. m. mason , r. valencia , and a. zimmering ( 1984 ) .",
    "chemical mutagenesis testing in drosophila i : comparison of positive and negative control data for sex - linked recessive lethal mutations and reciprocal translocations in three laboratories . _",
    "environmental mutagenesis _ ,",
    "* 6 * , 189202 ."
  ],
  "abstract_text": [
    "<S> robust inference based on the minimization of statistical divergences has proved to be a useful alternative to the classical techniques based on maximum likelihood and related methods . recently </S>",
    "<S> ghosh et al .  ( </S>",
    "<S> 2013 ) proposed a general class of divergence measures , namely the @xmath0-divergence family and discussed its usefulness in robust parametric estimation through some numerical illustrations . in this present paper , we develop the asymptotic properties of the proposed minimum @xmath0-divergence estimators under discrete models . </S>"
  ]
}