{
  "article_text": [
    "policy gradient algorithms maximize the expectation of cumulative reward by following the gradient of this expectation with respect to the policy parameters . most existing algorithms estimate",
    "this gradient in a model - free manner by sampling returns from the real environment and rely on a likelihood ratio estimator @xcite .",
    "such estimates tend to have high variance and require large numbers of samples or , conversely , low - dimensional policy parameterizations .    a second approach to estimate a policy gradient relies on backpropagation instead of likelihood ratio methods .",
    "if a differentiable environment model is available , one can link together the policy , model , and reward function to compute an analytic policy gradient by backpropagation of reward along a trajectory @xcite . instead of using entire trajectories , one can estimate future rewards using a learned value function ( a critic ) and compute policy gradients from subsequences of trajectories .",
    "it is also possible to backpropagate analytic action derivatives from a q - function to compute the policy gradient without a model @xcite .",
    "following fairbank @xcite , we refer to methods that compute the policy gradient through backpropagation as _ value gradient _ methods .    in this paper , we address two limitations of prior value gradient algorithms .",
    "the first is that , in contrast to likelihood ratio methods , value gradient algorithms are only suitable for training deterministic policies .",
    "stochastic policies have several advantages : for example , they can be beneficial for partially observed problems @xcite ; they permit on - policy exploration ; and because stochastic policies can assign probability mass to off - policy trajectories , we can train a stochastic policy on samples from an experience database in a principled manner .",
    "when an environment model is used , value gradient algorithms have also been critically limited to operation in deterministic environments . by exploiting a mathematical tool known as `` re - parameterization '' that has found recent use for generative models @xcite , we extend the scope of value gradient algorithms to include the optimization of stochastic policies in stochastic environments .",
    "we thus describe our framework as _ stochastic value gradient _ ( svg ) methods .",
    "secondly , we show that an environment dynamics model , value function , and policy can be learned jointly with neural networks based only on environment interaction . learned dynamics models are often inaccurate , which we mitigate by computing value gradients along real system trajectories instead of planned ones , a feature shared by model - free methods @xcite .",
    "this substantially reduces the impact of model error because we only use models to compute policy gradients , not for prediction , combining advantages of model - based and model - free methods with fewer of their drawbacks .",
    "we present several algorithms that range from model - based to model - free methods , flexibly combining models of environment dynamics with value functions to optimize policies in stochastic or deterministic environments .",
    "experimentally , we demonstrate that svg methods can be applied using generic neural networks with tens of thousands of parameters while making minimal assumptions about plants or environments . by examining a simple stochastic control problem ,",
    "we show that svg algorithms can optimize policies where model - based planning and likelihood ratio methods can not .",
    "we provide evidence that value function approximation can compensate for degraded models , demonstrating the increased robustness of svg methods over model - based planning . finally , we use svg algorithms to solve a variety of challenging , under - actuated , physical control problems , including swimming of snakes , reaching , tracking , and grabbing with a robot arm , fall - recovery for a monoped , and locomotion for a planar cheetah and biped .",
    "we consider discrete - time markov decision processes ( mdps ) with continuous states and actions and denote the state and action at time step @xmath0 by @xmath1 and @xmath2 , respectively .",
    "the mdp has an initial state distribution @xmath3 , a transition distribution @xmath4 , and a ( potentially time - varying ) reward function @xmath5 .",
    "we consider time - invariant stochastic policies @xmath6 , parameterized by @xmath7 .",
    "the goal of policy optimization is to find policy parameters @xmath7 that maximize the expected sum of future rewards .",
    "we optimize either finite - horizon or infinite - horizon sums , i.e.,@xmath8 $ ] or @xmath9 $ ] where @xmath10 $ ] is a discount factor .",
    "for the infinite - horizon case .",
    "] when possible , we represent a variable at the next time step using the `` tick '' notation , e.g. , @xmath11 .    in what follows ,",
    "we make extensive use of the state - action - value q - function and state - value v - function . @xmath12 ; v^t({\\mathbf{s } } )   =   \\mathbb{e } \\left [ \\sum_{\\tau = t } \\gamma^{\\tau - t } r^{\\tau } \\big | { \\mathbf{s}}^t = { \\mathbf{s } } , \\theta \\right ] .\\end{aligned}\\ ] ] for finite - horizon problems , the value functions are time - dependent , e.g. , @xmath13 , and for infinite - horizon problems the value functions are stationary , @xmath14 .",
    "the relevant meaning should be clear from the context .",
    "the state - value function can be expressed recursively using the stochastic bellman equation @xmath15 p({\\mathbf{a}}|{\\mathbf{s } } ; \\theta ) d { \\mathbf{a}}. \\label{eq : v_recursive}\\end{aligned}\\ ] ] we abbreviate partial differentiation using subscripts , @xmath16 .",
    "the deterministic bellman equation takes the form @xmath17 for a deterministic model @xmath18 and deterministic policy @xmath19 . differentiating the equation with respect to the state and policy yields an expression for the value gradient @xmath20 in eq .",
    "[ eq : detvgradtheta ] , the term @xmath21 arises because the total derivative includes policy gradient contributions from subsequent time steps ( full derivation in appendix [ sec : appendix : recursive ] ) . for a purely model - based formalism",
    ", these equations are used as a pair of coupled recursions that , starting from the termination of a trajectory , proceed backward in time to compute the gradient of the value function with respect to the state and policy parameters .",
    "@xmath22 returns the total policy gradient . when a state - value function is used after one step in the recursion",
    ", @xmath23 directly expresses the contribution of the current time step to the policy gradient .",
    "summing these gradients over the trajectory gives the total policy gradient .",
    "when a q - function is used , the per - time step contribution to the policy gradient takes the form @xmath24 .",
    "one limitation of the gradient computation in eqs .",
    "[ eq : detvgrads ] and [ eq : detvgradtheta ] is that the model and policy must be deterministic .",
    "additionally , the accuracy of the policy gradient @xmath25 is highly sensitive to modeling errors .",
    "we introduce two critical changes : first , in section [ sec : maca : sbp ] , we transform the stochastic bellman equation ( eq . [ eq : v_recursive ] ) to permit backpropagating value information in a stochastic setting .",
    "this also enables us to compute gradients along real trajectories , not ones sampled from a model , making the approach robust to model error , leading to our first algorithm `` svg(@xmath26 ) , '' described in section [ sec : maca : trajectory ] .",
    "second , in section [ sec : maca : value ] , we show how value function critics can be integrated into this framework , leading to the algorithms `` svg(@xmath27 ) '' and `` svg(@xmath28 ) '' , which expand the bellman recursion for 1 and 0 steps , respectively .",
    "value functions further increase robustness to model error and extend our framework to infinite - horizon control .",
    "[ [ re - parameterization - of - distributions ] ] re - parameterization of distributions + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    our goal is to backpropagate through the stochastic bellman equation .",
    "to do so , we make use of a concept called `` re - parameterization '' , which permits us to compute derivatives of deterministic and stochastic models in the same way",
    ". a very simple example of re - parameterization is to write a conditional gaussian density @xmath29 as the function @xmath30 , where @xmath31 . from this point of view , one produces samples procedurally by first sampling @xmath32 , then deterministically constructing @xmath33 . here",
    ", we consider conditional densities whose samples are generated by a deterministic function of an input noise variable and other conditioning variables : @xmath34 , where @xmath35 , a fixed noise distribution .",
    "rich density models can be expressed in this form @xcite .",
    "expectations of a function @xmath36 become @xmath37 .",
    "the advantage of working with re - parameterized distributions is that we can now obtain a simple monte - carlo estimator of the derivative of an expectation with respect to @xmath38 : @xmath39 in contrast to likelihood ratio - based monte carlo estimators , @xmath40 , this formula makes direct use of the jacobian of @xmath41 .    [ [ sec : maca : reparameterization ] ] re - parameterization of the bellman equation + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we now re - parameterize the bellman equation . when re - parameterized , the stochastic policy takes the form @xmath42 , and the stochastic environment the form @xmath43 for noise variables @xmath44 and @xmath45 , respectively . inserting these functions into eq .",
    "( [ eq : v_recursive ] ) yields @xmath46 \\bigg ] .",
    "\\label{eq : v_recursivereparameterized}\\end{aligned}\\ ] ]    differentiating eq .",
    "[ eq : v_recursivereparameterized ] with respect to the current state @xmath47 and policy parameters @xmath7 gives @xmath48 ,    \\label{eq : vgrads } \\\\ v_\\theta & =   \\mathbb{e}_{\\rho(\\eta ) } \\bigg [ r_{\\mathbf{a}}\\pi_\\theta + \\gamma \\mathbb{e}_{\\rho(\\xi ) }    \\big [ v'_{{\\mathbf{s } } ' } { \\mathbf{f}}_{\\mathbf{a}}\\pi_\\theta + v'_\\theta   \\big ] \\bigg ] .",
    "\\label{eq : vgradtheta}\\end{aligned}\\ ] ] we are interested in controlling systems with _ a priori _ unknown dynamics .",
    "consequently , in the following , we replace instances of @xmath49 or its derivatives with a learned model @xmath50 .",
    "[ [ gradient - evaluation - by - planning ] ] gradient evaluation by planning + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    a planning method to compute a gradient estimate is to compute a trajectory by running the policy in loop with a model while sampling the associated noise variables , yielding a trajectory @xmath51 . on this sampled trajectory ,",
    "a monte - carlo estimate of the policy gradient can be computed by the backward recursions : @xmath52 \\big|_{\\eta , \\xi } ,   \\label{eq : vgradmcs } \\\\",
    "v_\\theta & = [ r_{\\mathbf{a}}\\pi_\\theta + \\gamma ( v'_{{\\mathbf{s } } ' } \\hat{{\\mathbf{f}}}_{\\mathbf{a}}\\pi_\\theta + v'_\\theta ) ] \\big|_{\\eta , \\xi } , \\label{eq : vgradmctheta}\\end{aligned}\\ ] ] where have written lower - case @xmath53 to emphasize that the quantities are one - sample estimates ) are @xmath54 . after the recursion ,",
    "the total derivative of the value function with respect to the policy parameters is given by @xmath55 , which is a one - sample estimate of @xmath56 .",
    "] , and `` @xmath57 '' means `` evaluated at @xmath58 '' .",
    "[ [ gradient - evaluation - on - real - trajectories ] ] gradient evaluation on real trajectories + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    an important advantage of stochastic over deterministic models is that they can assign probability mass to observations produced by the real environment . in a deterministic formulation",
    ", there is no principled way to account for mismatch between model predictions and observed trajectories . in this case , the policy and environment noise @xmath59 that produced the observed trajectory are considered unknown . by an application of bayes rule , which we explain in appendix [ sec :",
    "appendix : noise ] , we can rewrite the expectations in equations [ eq : vgrads ] and [ eq : vgradtheta ] given the observations @xmath60 as @xmath61 ,    \\label{eq : pvgrads } \\\\ v_\\theta & = \\mathbb{e}_{p({\\mathbf{a}}| { \\mathbf{s } } ) } \\mathbb{e}_{p({\\mathbf{s } } ' | { \\mathbf{s } } , { \\mathbf{a } } ) } \\mathbb{e}_{p(\\eta , \\xi | { \\mathbf{s } } , { \\mathbf{a } } , { \\mathbf{s } } ' ) } \\bigg [ r_{\\mathbf{a}}\\pi_\\theta   + \\gamma ( v'_{{\\mathbf{s } } ' } \\hat{{\\mathbf{f}}}_{\\mathbf{a}}\\pi_\\theta + v'_\\theta ) \\bigg ] ,   \\label{eq : pvgradtheta}\\end{aligned}\\ ] ] where we can now replace the two outer expectations with samples derived from interaction with the real environment . in the special case of additive noise ,",
    "@xmath62 , it is possible to use a deterministic model to compute the derivatives @xmath63 .",
    "the noise s influence is restricted to the gradient of the value of the next state , @xmath64 , and does not affect the model jacobian . if we consider it desirable to capture more complicated environment noise , we can use a re - parameterized generative model and infer the missing noise variables , possibly by sampling from @xmath65 .",
    "svg(@xmath26 ) computes value gradients by backward recursions on finite - horizon trajectories . after every episode , we train the model , @xmath50 , followed by the policy , @xmath66 . we provide pseudocode for this in algorithm [ alg : maca ] but discuss further implementation details in section [ sec : maca : modellearning ] and in the experiments .",
    "given empty experience database @xmath67 apply control @xmath42 , @xmath44 insert @xmath68 into @xmath67 train generative model @xmath50 using @xmath67 @xmath69 ( finite - horizon ) @xmath70 ( finite - horizon ) infer @xmath71 and @xmath72 @xmath73   \\big |_{\\eta , \\xi}$ ] @xmath74 \\big |_{\\eta , \\xi}$ ] apply gradient - based update using @xmath55    given empty experience database @xmath67 apply control @xmath75 , @xmath44 observe @xmath76 insert @xmath68 into @xmath67 //",
    "model and critic updates train generative model @xmath50 using @xmath67 train value function @xmath77 using @xmath67 ( alg .",
    "[ alg : fpe ] ) //",
    "policy update sample @xmath78 from @xmath67 ( @xmath79 ) @xmath80 infer @xmath81 and @xmath82 @xmath83 apply gradient - based update using @xmath84      in our framework , we may learn a parametric estimate of the expected value @xmath85 ( critic ) with parameters @xmath86 .",
    "the derivative of the critic value with respect to the state , @xmath87 , can be used in place of the sample gradient estimate given in eq .",
    "( [ eq : vgradmcs ] ) .",
    "the critic can reduce the variance of the gradient estimates because @xmath88 approximates the _ expectation _ of future rewards while eq .  ( [ eq : vgradmcs ] ) provides only a single - trajectory estimate .",
    "additionally , the value function can be used at the end of an episode to approximate the infinite - horizon policy gradient . finally , eq .  ( [ eq : vgradmcs ] ) involves the repeated multiplication of jacobians of the approximate model @xmath89 , @xmath90 . just as model error can compound in forward planning ,",
    "model gradient error can compound during backpropagation .",
    "furthermore , svg(@xmath26 ) is on - policy .",
    "that is , after each episode , a single gradient - based update is made to the policy , and the policy optimization does not revisit those trajectory data again . to increase data - efficiency ,",
    "we construct an off - policy , experience replay @xcite algorithm that uses models and value functions , svg(1 ) with experience replay ( svg(1)-er ) .",
    "this algorithm also has the advantage that it can perform an infinite - horizon computation .    to construct an off - policy estimator",
    ", we perform importance - weighting of the current policy distribution with respect to a proposal distribution , @xmath91 : @xmath92 .",
    "\\label{eq : deltathetaexpectation}\\end{aligned}\\ ] ]    specifically , we maintain a database with tuples of past state transitions @xmath78 .",
    "each proposal drawn from @xmath93 is a sample of a tuple from the database . at time @xmath0 , the importance - weight @xmath94 , where @xmath95 comprise the policy parameters in use at the historical time step @xmath96 .",
    "we do not importance - weight the marginal distribution over states @xmath97 generated by a policy ; this is widely considered to be intractable .",
    "similarly , we use experience replay for value function learning .",
    "details can be found in appendix [ sec : appendix : models ] .",
    "pseudocode for the svg(@xmath27 ) algorithm with experience replay is in algorithm [ alg : macazeroer ] .",
    "we also provide a model - free stochastic value gradient algorithm , svg(@xmath28 ) ( algorithm [ alg : svgzeroer ] in the appendix ) .",
    "this algorithm is very similar to svg(@xmath27 ) and is the stochastic analogue of the recently introduced deterministic policy gradient algorithm ( dpg ) @xcite . unlike dpg , instead of assuming a deterministic policy , svg(0 ) estimates the derivative around the policy noise @xmath98 $ ] .",
    "is a function of the state and noise variable . ]",
    "this , for example , permits learning policy noise variance .",
    "the relative merit of svg(1 ) versus svg(0 ) depends on whether the model or value function is easier to learn and is task - dependent .",
    "we expect that model - based algorithms such as svg(@xmath27 ) will show the strongest advantages in multitask settings where the system dynamics are fixed , but the reward function is variable .",
    "svg(1 ) performed well across all experiments , including ones introducing capacity constraints on the value function and model .",
    "svg(1)-er demonstrated a significant advantage over all other tested algorithms .",
    "we can use almost any kind of differentiable , generative model . in our work , we have parameterized the models as neural networks .",
    "our framework supports nonlinear state- and action - dependent noise , notable properties of biological actuators .",
    "for example , this can be described by the parametric form @xmath99 .",
    "model learning amounts to a purely supervised problem based on observed state transitions . our model and policy training occur",
    "jointly_. there is no `` motor - babbling '' period used to identify the model .",
    "as new transitions are observed , the model is trained first , followed by the value function ( for svg(@xmath27 ) ) , followed by the policy . to ensure that the model does not forget information about state transitions",
    ", we maintain an experience database and cull batches of examples from the database for every model update .",
    "additionally , we model the state - change by @xmath100 and have found that constructing models as separate sub - networks per predicted state dimension improved model quality significantly .",
    "our framework also permits a variety of means to learn the value function models .",
    "we can use temporal difference learning @xcite or regression to empirical episode returns . since svg(@xmath27 ) is model - based , we can also use bellman residual minimization @xcite . in practice",
    ", we used a version of `` fitted '' policy evaluation .",
    "pseudocode is available in appendix [ sec : appendix : models ] , algorithm [ alg : fpe ] .",
    "we found that the models exhibited the lowest per - step prediction error when the @xmath130 and @xmath131 vector components were computed by parallel subnetworks , producing one @xmath132 pair for each state dimension , i.e. , @xmath133 $ ] .",
    "( this was due to varied scaling of the dynamic range of the state dimensions . ) in the experiments in this paper , the @xmath131 components were parametrized as constant biases per dimension .",
    "( as remarked in the main text , this implies that they do not contribute to the gradient calculation .",
    "however , in the hand environment , the planner agent forward - samples based on the learned standard deviations . )    given empty experience database @xmath67 apply control @xmath75 , @xmath44 observe @xmath76 insert @xmath68 into @xmath67 //",
    "critic updates train value function @xmath134 using @xmath67 //",
    "policy update sample @xmath135 from @xmath67 ( @xmath136 ) infer @xmath82 @xmath137 apply gradient - based update using @xmath84    given experience database @xmath67 given value function @xmath138 , outer loop time @xmath0 @xmath139 sample @xmath78 from @xmath67 ( @xmath136 ) @xmath140 @xmath80 @xmath141 apply gradient - based update to @xmath142 using @xmath143 every @xmath144 updates , set @xmath145 ( @xmath146 )",
    "we tested the svg algorithms in two sets of experiments . in the first set of experiments ( section [ sec : experiments : analyzing ] )",
    ", we test whether evaluating gradients on real environment trajectories and value function approximation can reduce the impact of model error . in our second set ( section [ sec : experiments : scalingup ] ) , we show that svg(1 ) can be applied to several complicated , multidimensional physics environments involving contact dynamics ( figure [ fig : experiments : mujoco ] ) in the mujoco simulator @xcite .",
    "below we only briefly summarize the main properties of each environment : further details of the simulations can be found in appendix [ sec : appendix : expdetails ] and supplement . in all cases ,",
    "we use generic , 2 hidden - layer neural networks with _ tanh _ activation functions to represent models , value functions , and policies .",
    "a video montage is available at https://youtu.be/pydl7bcn_cm .",
    "[ [ gradient - evaluation - on - real - trajectories - vs .- planning ] ] gradient evaluation on real trajectories vs. planning + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    to demonstrate the difficulty of planning with a stochastic model , we first present a very simple control problem for which svg(@xmath26 ) easily learns a control policy but for which an otherwise identical planner fails entirely .",
    "our example is based on a problem due to @xcite .",
    "the policy directly controls the velocity of a point - mass `` hand '' on a 2d plane . by means of a spring - coupling ,",
    "the hand exerts a force on a ball mass ; the ball additionally experiences a gravitational force and random forces ( gaussian noise ) .",
    "the goal is to bring hand and ball into one of two randomly chosen target configurations with a relevant reward being provided only at the final time step .    with simulation time step @xmath101 , this demands controlling and backpropagating the distal reward along a trajectory of @xmath102 steps .",
    "because this experiment has a non - stationary , time - dependent value function , this problem also favors model - based value gradients over methods using value functions .",
    "svg(@xmath26 ) easily learns this task , but the planner , which uses trajectories from the model , shows little improvement .",
    "the planner simulates trajectories using the learned stochastic model and backpropagates along those simulated trajectories ( eqs .",
    "[ eq : vgradmcs ] and [ eq : vgradmctheta ] ) @xcite .",
    "the extremely long time - horizon lets prediction error accumulate and thus renders roll - outs highly inaccurate , leading to much worse final performance ( c.f .",
    "[ fig : hand_cartpole ] , _ left _ ) .",
    "[ [ robustness - to - degraded - models - and - value - functions ] ] robustness to degraded models and value functions + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    we investigated the sensitivity of svg(@xmath26 ) and svg(1 ) to the quality of the learned model on swimmer .",
    "swimmer is a chain body with multiple links immersed in a fluid environment with drag forces that allow the body to propel itself @xcite .",
    "we build chains of 3 , 5 , or 7 links , corresponding to 10 , 14 , or 18-dimensional state spaces with 2 , 4 , or 6-dimensional action spaces .",
    "the body is initialized in random configurations with respect to a central goal location .",
    "thus , to solve the task , the body must turn to re - orient and then produce an undulation to move to the goal .    to assess the impact of model quality",
    ", we learned to control a link-3 swimmer with svg(@xmath26 ) and svg(1 ) while varying the capacity of the network used to model the environment ( 5 , 10 , or 20 hidden units for each state dimension subnetwork ( appendix [ sec : appendix : expdetails ] ) ; i.e. , in this task we intentionally shrink the neural network model to investigate the sensitivity of our methods to model inaccuracy . while with a high capacity model ( 20 hidden units per state dimension ) , both svg(@xmath26 ) and svg(1 ) successfully learn to solve the task , the performance of svg(@xmath26 ) drops significantly as model capacity is reduced ( c.f .  fig .",
    "[ fig : swimmer ] , _ middle _ ) .",
    "svg(1 ) still works well for models with only 5 hidden units , and it also scales up to 5 and 7-link versions of the swimmer ( figs .",
    "[ fig : swimmer ] , _ right _ and [ fig : physics ] , _ left _ ) . to compare svg(1 ) to conventional model - free approaches",
    ", we also tested a state - of - the - art actor - critic algorithm that learns a @xmath103-function and updates the policy using the td - error @xmath104 as an estimate of the advantage , yielding the policy gradient @xmath105 @xcite .",
    "( svg(1 ) and the ac algorithm used the same code for learning @xmath103 . )",
    "svg(1 ) outperformed the model - free approach in the 3- , 5- , and 7-link swimmer tasks ( c.f .",
    "[ fig : swimmer ] , _ left _ , _ right _ ; fig .",
    "[ fig : physics ] , _ top left _ ) . in figure panels [ fig : hand_cartpole ] , _ middle _ , [ fig : swimmer ] , _ right _ , and [ fig : physics ] , _ left column _ , we show that experience replay for the policy can improve the data efficiency and performance of svg(1 ) .    similarly , we tested the impact of varying the capacity of the value function approximator ( fig .",
    "[ fig : hand_cartpole ] , _ right _ ) on a cart - pole .",
    "the v - function - based svg(1 ) degrades less severely than the q - function - based dpg presumably because it computes the policy gradient with the aid of the dynamics model .          ) .",
    "_ middle _ : however , as the environment model s capacity is reduced from 20 to 10 then to 5 hidden units per state - dimension subnetwork , svg(@xmath26 ) dramatically deteriorates , whereas svg(1 ) shows undisturbed performance .",
    ": for a 5-link swimmer , svg(1)-er learns faster and asymptotes at higher performance than the other tested algorithms.,scaledwidth=97.5% ]    [ fig : swimmer ]        [ fig : physics ]    in a second set of experiments we demonstrated that svg(1)-er can be applied to several challenging physical control problems with stochastic , non - linear , and discontinuous dynamics due to contacts .",
    "_ reacher _ is an arm stationed within a walled box with 6 state dimensions and 3 action dimensions and the @xmath106 coordinates of a target site , giving 8 state dimensions in total . in 4-target reacher ,",
    "the site was randomly placed at one of the four corners of the box , and the arm in a random configuration at the beginning of each trial . in moving - target reacher , the site moved at a randomized speed and heading in the box with reflections at the walls . solving this latter problem",
    "implies that the policy has generalized over the entire work space .",
    "_ gripper _ augments the reacher arm with a manipulator that can grab a ball in a randomized position and return it to a specified site .",
    "_ monoped _ has 14 state dimensions , 4 action dimensions , and ground contact dynamics .",
    "the monoped begins falling from a height and must remain standing .",
    "additionally , we apply gaussian random noise to the torques controlling the joints with a standard deviation of @xmath107 of the total possible actuator strength at all points in time , reducing the stability of upright postures . _",
    "half - cheetah _ is a planar cat robot designed to run based on @xcite with 18 state dimensions and 6 action dimensions .",
    "half - cheetah has a version with springs to aid balanced standing and a version without them .",
    "_ walker _ is a planar biped , based on the environment from @xcite .",
    "[ [ results ] ] results + + + + + + +    figure [ fig : physics ] shows learning curves for several repeats for each of the tasks .",
    "we found that in all cases svg(1 ) solved the problem well ; we provide videos of the learned policies in the supplemental material .",
    "the 4-target reacher reliably finished at the target site , and in the tracking task followed the moving target successfully .",
    "svg(1)-er has a clear advantage on this task as also borne out in the cart - pole and swimmer experiments .",
    "the cheetah gaits varied slightly from experiment to experiment but in all cases made good forward progress . for the monoped , the policies were able to balance well beyond the 200 time steps of training episodes and were able to resist significantly higher adversarial noise levels than used during training ( up to @xmath108 noise ) .",
    "we were able to learn gripping and walking behavior , although walking policies that achieved similar reward levels did not always exhibit equally good walking phenotypes .",
    "writing the noise variables as exogenous inputs to the system to allow direct differentiation with respect to the system state ( equation [ eq : vgrads ] ) is a known device in control theory @xcite where the model is given analytically .",
    "the idea of using a model to optimize a parametric policy around real trajectories is presented heuristically in @xcite and @xcite for deterministic policies and models .",
    "also in the limit of deterministic policies and models , the recursions we have derived in algorithm [ alg : maca ] reduce to those of @xcite .",
    "werbos defines an actor - critic algorithm called heuristic dynamic programming that uses a deterministic model to roll - forward one step to produce a state prediction that is evaluated by a value function @xcite .",
    "deisenroth et al .",
    "have used gaussian process models to compute policy gradients that are sensitive to model - uncertainty @xcite , and levine et al .",
    "have optimized impressive policies with the aid of a non - parametric trajectory optimizer and locally - linear models @xcite .",
    "our work in contrast has focused on using global , neural network models conjoined to value function approximators .",
    "we have shown that two potential problems with value gradient methods , their reliance on planning and restriction to deterministic models , can be exorcised , broadening their relevance to reinforcement learning .",
    "we have shown experimentally that the svg framework can train neural network policies in a robust manner to solve interesting continuous control problems .",
    "the framework includes algorithm variants beyond the ones tested in this paper , for example , ones that combine a value function with @xmath96 steps of back - propagation through a model ( svg(k ) ) . augmenting svg(1 ) with experience replay led to the best results , and a similar extension could be applied to any svg(k ) .",
    "furthermore , we did not harness sophisticated generative models of stochastic dynamics , but one could readily do so , presenting great room for growth .",
    "the use of derivatives in equation [ eq : detvgradtheta ] is subtle , so we expand on the logic here .",
    "we first note that a change to the policy parameters affects the immediate action as well as each future state and action .",
    "thus , the total derivative @xmath109 can be expanded to @xmath110 \\\\ & = \\bigg(\\frac{d { \\mathbf{a}}^{0}}{d \\theta } \\frac{\\partial}{\\partial { \\mathbf{a}}^{0 } } + \\frac{d { \\mathbf{s}}^{1}}{d \\theta } \\frac{\\partial}{\\partial { \\mathbf{s}}^{1 } } \\bigg ) + \\bigg [ \\sum_{t \\geq 1 } \\frac{d { \\mathbf{a}}^{t}}{d \\theta } \\frac{\\partial}{\\partial { \\mathbf{a}}^{t } } + \\sum_{t > 1 } \\frac{d { \\mathbf{s}}^{t}}{d \\theta } \\frac{\\partial}{\\partial { \\mathbf{s}}^{t } }   \\bigg].\\end{aligned}\\ ] ] let us define the operator @xmath111 $ ] .",
    "the operator obeys the recursive formula @xmath112 we can transform this to @xmath113 the value function depends on the policy parameters @xmath114 .",
    "the deterministic bellman equation can be specified as @xmath115 .",
    "now , we can apply the operator @xmath116 : @xmath117 \\\\ & = \\bigg[\\frac{d { \\mathbf{a}}^{t}}{d \\theta }   \\bigg(\\frac{\\partial}{\\partial { \\mathbf{a}}^{t } } + \\frac{d { \\mathbf{s}}^{t+1}}{d { \\mathbf{a}}^t }   \\frac{\\partial}{\\partial { \\mathbf{s}}^{t+1 } } \\bigg ) + \\nabla_{\\theta}^{t+1 } \\bigg ] \\bigg [ r({\\mathbf{s}}^t , { \\mathbf{a}}^t ) + \\gamma   v^{t+1}({\\mathbf{s}}^{t+1 } ; \\theta ) \\bigg ] \\\\ & = \\frac{d { \\mathbf{a}}^{t}}{d \\theta } \\frac{\\partial}{\\partial { \\mathbf{a}}^{t } } r({\\mathbf{s}}^t , { \\mathbf{a}}^t ) + \\frac{d { \\mathbf{a}}^{t}}{d \\theta } \\frac{d { \\mathbf{s}}^{t+1}}{d { \\mathbf{a}}^t }   \\frac{\\partial}{\\partial { \\mathbf{s}}^{t+1 } } \\gamma   v^{t+1}({\\mathbf{s}}^{t+1 } ; \\theta ) +   \\nabla_{\\theta}^{t+1 } \\gamma   v^{t+1}({\\mathbf{s}}^{t+1 } ; \\theta ) . \\end{aligned}\\ ] ] in the `` tick '' notation of the main text , this is equation [ eq : detvgradtheta ] .",
    "evaluating the jacobian terms in equations in equations [ eq : vgradmcs ] and [ eq : vgradmctheta ] ) may require knowledge of the noise variables @xmath118 and @xmath32 .",
    "this poses no difficulty when we obtain trajectory samples by forward - sampling @xmath119 and computing @xmath120 using the policy and learned system model .",
    "however , the same is not true when we sample trajectories from the real environment . here",
    ", the noise variables are unobserved and may need to be `` filled in '' to evaluate the jacobians around the right arguments .",
    "equations [ eq : pvgrads ] and [ eq : pvgradtheta ] arise from an application of bayes rule .",
    "we formally invert the forward sampling process that generates samples from the joint distribution @xmath121 . instead of sampling @xmath122 and then @xmath123 , @xmath124 , we first sample @xmath125 and @xmath126 using our policy and the real environment .",
    "given these data and a function @xmath41 of them , we sample @xmath127 to produce @xmath128 for example , in eq .",
    "[ eq : pvgrads ] , we plug in @xmath129 .",
    "for a direct comparison we ran svg(0 ) , svg(1 ) , svg(@xmath26 ) , and dpg without experience replay for the policy updates since replay is not possible for svg(@xmath26 ) .",
    "( all algorithms use replay for the value function updates . )",
    "we further implemented svg(1)-er such that policy updates are performed at the end of each episode , following the update of the value function , rather than following each step of interaction with the environment . for @xmath147 steps of replay for the policy",
    "we perform @xmath147 gradient steps according to lines 10 - 14 in algorithm [ alg : macazeroer ] , drawing new data from the database @xmath67 in each step . in some cases we found it helpful to apply an additional regularizer that penalized @xmath148   \\right ] } $ ] during each step of replay , where @xmath149 denotes the policy at the beginning of the update and @xmath150 the policy after @xmath151 steps of replay .",
    "the expectation with respect to @xmath47 is taken over the empirical distribution of states in @xmath67 .",
    "following @xcite we truncated importance weights ( using a maximum value of 5 for all experiments except for the 4-target reacher and gripper where we used 20 ) .",
    "computing the policy gradient with svg(@xmath26 ) involves backpropagation through what is effectively a recurrent network .",
    "( each time - step involves a concatenation of two multi - layer networks for policy and dynamics model .",
    "the full model is a chain of these pairs . ) in line with findings for training standard recurrent networks , we found that `` clipping '' the policy gradient improved stability .",
    "following @xcite we chose to limit the norm of the policy gradient ; i.e. , we performed backpropagation for an episode as described above and then renormalized @xmath55 if its norm exceeded a set threshold @xmath152 : @xmath153 .",
    "we used this approach for all algorithms although it was primarily necessary for svg(@xmath26 ) .",
    "we optimized hyper - parameters separately for each algorithm by first identifying a reasonable range for the relevant hyper - parameters and then performing a more systematic grid search .",
    "the main parameters optimized were : learning rates for policy , value function , and model ( as applicable ) ; the number of updates per episode for policy , value function , and model ( as applicable ) ; regularization as described above for svg(1)-er ; @xmath152 ; the standard deviation of the gaussian policy noise ."
  ],
  "abstract_text": [
    "<S> we present a unified framework for learning continuous control policies using backpropagation . </S>",
    "<S> it supports stochastic control by treating stochasticity in the bellman equation as a deterministic function of exogenous noise . </S>",
    "<S> the product is a spectrum of general policy gradient algorithms that range from model - free methods with value functions to model - based methods without value functions . </S>",
    "<S> we use learned models but only require observations from the environment instead of observations from model - predicted trajectories , minimizing the impact of compounded model errors . </S>",
    "<S> we apply these algorithms first to a toy stochastic control problem and then to several physics - based control problems in simulation . </S>",
    "<S> one of these variants , svg(1 ) , shows the effectiveness of learning models , value functions , and policies simultaneously in continuous domains . </S>"
  ]
}