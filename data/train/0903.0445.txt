{
  "article_text": [
    "we consider a distributed storage problem in a large - scale wireless sensor network with @xmath0 nodes among which @xmath1 sensor nodes acquire ( sense ) independent data . since sensors are usually vulnerable due to limited energy and hostile environment , it is desirable to disseminate the acquired information throughout the network so that each of the @xmath0 sensors stores one possibly coded packet and the original @xmath1 source packets can be recovered later in a computationally simple way from any @xmath2 of nodes for some small @xmath3 .",
    "no sensor knows locations of any other sensors except for their own neighbors , and they do not maintain any routing information ( e.g. , routing tables or network topology ) .",
    "algorithms that solve such problems using coding in a centralized way are well known and understood . in a sensor network , however , this is much more difficult , since we need to find a strategy to distribute the information from multiple sources throughout the network so that each sensor admits desired statistics of data . in  @xcite , lin",
    "_ et al . _",
    "proposed an algorithm that uses random walks with traps to disseminate the source packets in a wireless sensor network . to achieve desired code degree distribution , they employed the metropolis algorithm to specify transition probabilities of the random walks .",
    "while the proposed methods in  @xcite are promising , the knowledge of the total number of sensors @xmath0 and sources @xmath1 are required .",
    "another type of global information , the maximum node degree ( i.e. , the maximum number of neighbors ) of the graph , is also required to perform the metropolis algorithm .",
    "nevertheless , for a large - scale sensor network , these types of global information may not be easy to obtain by each individual sensor , especially when there is a possibility of change of topology .    in  @xcite",
    ", we proposed luby transform ( lt ) codes based distributed storage algorithms for large - scale wireless sensor networks to overcome these difficulties . in this paper , we extend this work to raptor codes and demonstrate their performance .",
    "particularly , we propose two new decentralized algorithms , raptor code distributed storage ( rcds - i ) and ( rcds - ii ) , that distribute information sensed by k source nodes to n nodes for storage based on raptor codes . in rcds - i , each node has limited global information ; while in rcds - ii , no global information is required .",
    "we compute the computational encoding and decoding complexity of these algorithms as well as evaluate their performance by simulation .",
    "suppose that the wireless sensor network consists of @xmath0 nodes that are uniformly distributed at random in a region @xmath4 ^ 2 $ ] .",
    "among these @xmath0 nodes , there are @xmath1 source nodes that have information to be disseminated throughout the network for storage .",
    "these @xmath1 nodes are uniformly and independently chosen at random among the @xmath0 nodes . usually , the fraction of source nodes .",
    "we assume that no node has knowledge about the locations of other nodes and no routing table is maintained , and thus that the algorithm proposed in  @xcite can not be applied . moreover , besides the neighbor nodes , we assume that each node has limited or no knowledge of global information .",
    "the limited global information refers to the total number of nodes @xmath0 , and the total number of sources @xmath1 .",
    "any further global information , for example , the maximal number of neighbors in the network , is not available .",
    "hence , the algorithms proposed in  @xcite are not applicable .",
    "( node degree ) consider a graph @xmath5 , where @xmath6 and @xmath7 denote the set of nodes and links , respectively . given @xmath8 , we say @xmath9 and @xmath10 are _ adjacent _ ( or @xmath9 is adjacent to @xmath10 , and vice versa ) if there exists a link between @xmath9 and @xmath10 , i.e. , @xmath11 .",
    "in this case , we also say that @xmath9 and @xmath10 are _",
    "neighbors_. denote by @xmath12 the set of neighbors of a node @xmath9 .",
    "the number of neighbors of a node @xmath9 is called the _ node degree _ of @xmath9 , and denoted by @xmath13 , i.e. , @xmath14 .",
    "the _ mean degree _ of a graph @xmath15 is then given by @xmath16 .",
    "( code degree ) for fountain codes , the number of source blocks used to generate an encoded output @xmath17 is called the code degree of @xmath17 , and denoted by @xmath18 .",
    "the code degree distribution @xmath19 is the probability distribution of @xmath18 .    for @xmath1 source blocks @xmath20 and",
    "a probability distribution @xmath19 with @xmath21 , a fountain code with parameters @xmath22 is a potentially limitless stream of output blocks @xmath23 .",
    "each output block is obtained by xoring @xmath24 randomly and independently chosen source blocks , where @xmath24 is drawn from a degree distribution @xmath19 .",
    "this is illustrated in fig .",
    "[ fig : fountaincodes ] .     source blocks chosen uniformly and independently at random from @xmath1 source inputs , where @xmath24 is drawn according to a probability distribution @xmath19 .",
    "]    raptor codes are a class of fountain codes with linear encoding and decoding complexity  @xcite .",
    "the key idea of raptor codes is to relax the condition that all input blocks need to be recovered .",
    "if an lt code needs to recover only a constant fraction of its input blocks , its decoding complexity is @xmath25 , i.e. , linear time decoding .",
    "then , we can recover all input blocks by concatenating a traditional erasure correcting code with an lt code .",
    "this is called pre - coding in raptor codes , and can be accomplished by a modern block code such as ldpc codes .",
    "this process is illustrated in fig .",
    "[ fig : raptorcodes ] .",
    "the pre - code @xmath26 used in this paper is the randomized ldpc ( low - density parity - check ) code that is studied as one type of pre - code in  @xcite . in this randomized ldpc code",
    ", we have @xmath1 source blocks and @xmath27 pre - coding output blocks .",
    "each source block chooses @xmath24 pre - coding output blocks uniformly independently at random , where @xmath24 is drawn from a distribution @xmath28 .",
    "each pre - coding output blocks combines the `` incoming '' source blocks and obtain the encoded output .",
    "the code degree distribution @xmath29 of raptor codes for lt coding is a modification of the ideal soliton distribution and given by @xmath30 where @xmath31 and @xmath32 .",
    "source blocks are first encoded to @xmath27 pre - coding output blocks by ldpc coding , and then the final encoded output blocks are obtained by applying lt codes with these @xmath27 pre - coding output blocks with degree distribution @xmath33 . ]    the following result provides the performance of the raptor codes  @xcite .",
    "[ lemma : decoding - raptor - codes ] let @xmath34 , and @xmath26 be the family of codes of rate @xmath35 .",
    "then , the raptor code with pre - code @xmath26 and lt codes with degree distribution @xmath36 has a linear time encoding algorithm . with @xmath2 encoded output blocks ,",
    "the bp decoding algorithm has a linear time complexity .",
    "more precisely , the average number of operations to produce an output symbol is @xmath37 , and the average number of operations to recover the @xmath1 source symbols is @xmath38 .",
    "as shown in  @xcite , distributed lt codes are relatively simple to implement .",
    "raptor codes take the advantage of lt codes to decode a major fraction of @xmath1 source packets within linear complexity , and use another error correcting code to decode the remaining minor fraction also within linear complexity by concatenating such an error correcting code and lt code together  @xcite .    nevertheless , it is not trivial to achieve this encoding mechanism in a distributed manner . in this section ,",
    "we propose two algorithms for distributed storage based on raptor codes .",
    "the first is called rcds - i , in which each node has knowledge of limited global information .",
    "the second is called rcds - ii , which is a fully distributed algorithm and does not require any global information .      in rcds - i",
    ", we assume that each node in the network knows the value of @xmath1the number of sources , and the value of @xmath0the number of nodes .",
    "we use simple random walk  @xcite for each source to disseminate its information to the whole network . at each round , each node @xmath9 that has packets to transmit chooses one node @xmath10 among its neighbors uniformly independently at random , and sends the packet to the node @xmath10 . in order to avoid local - cluster effect  each source packet is trapped most likely by its neighbor nodes at each node , we make acceptance of any a source packet equiprobable . to achieve this",
    ", we also need each source packet to visit each node in the network at least once .",
    "( cover time ) given a graph @xmath15 , let @xmath39 be the expected length of a random walk that starts at node @xmath9 and visits every node in @xmath15 at least once .",
    "the _ cover time _ of @xmath15 is defined by @xmath40  @xcite .",
    "[ lemma : cover - time ] given a random geometric graph with @xmath0 nodes , if it is a connected graph with high probability , then @xmath41 .",
    "therefore , we can set a counter for each source packet and increase the counter by one after each forward transmission until the counter reaches some threshold @xmath42 to guarantee that the source packet visits each node in the network at least once .    to perform the ldpc pre - coding mechanism for @xmath1 sources in a distributed manner",
    ", we again use simple random walks to disseminate the source packets .",
    "each source node generates @xmath43 copies of its own source packet , where @xmath43 follows distribution for randomized ldpc codes @xmath28 . after these @xmath43 copies are sent out and distributed uniformly in the network , each node among @xmath27 nodes chosen as pre - coding output nodes absorbs one copy of this source packet with some probability . in this way",
    ", we have @xmath27 pre - coding output nodes , each of which contains a combined version of a random number of source packets .",
    "then , the above method can be applied for these @xmath27 pre - coding output nodes as new sources to do distributed raptor encoding . in this way , we can achieve distributed storage packets based on raptor codes .",
    "the rcds - i algorithm is described in the following steps .",
    "initialization phase :    each node @xmath9 in the network draws a random number @xmath44 according to the distribution @xmath33 given by  .",
    "each source node @xmath45 draws a random number @xmath46 according to the distribution of @xmath28 and generates @xmath46 copies of its source packet @xmath47 with its i d and a counter @xmath48 with initial value zero in the packet header and sends each of them to one of @xmath49 s neighbors chosen uniformly at random .    pre - coding phase :    each node of the remaining @xmath50 non - source nodes chooses to serve as a redundant node with probability @xmath51 .",
    "we call these redundant nodes and the original source nodes as pre - coding output nodes",
    ". each pre - coding output node @xmath52 generates a random number @xmath53 according to distribution @xmath54 given by @xmath55}{m}\\right)^d \\left ( 1-\\frac{e[b]}{m}\\right)^{k - d}$ ] , where @xmath56=\\sum_b b\\omega_{l}(b)$ ] .",
    "each node that has packets in its forward queue before the current round sends the head of line packet to one of its neighbors chosen uniformly at random .",
    "when a node @xmath9 receives a packet @xmath57 with counter @xmath58 ( @xmath59 is a system parameter ) , the node @xmath9 puts the packet into its forward queue and update the counter as @xmath60 .",
    "each pre - coding output node @xmath61 accepts the first @xmath62 copies of different @xmath62 source packet with counters @xmath63 , and updates @xmath61 s pre - coding result each time as @xmath64 .",
    "if a copy of @xmath65 is accepted , the copy will not be forwarded any more , and @xmath61 will not accept any other copy of @xmath65 . when the node @xmath61 finishes @xmath62 updates ,",
    "@xmath66 is the pre - coding output of @xmath61    raptor - coding phase :    each pre - coding output node @xmath67 put its i d and a counter @xmath68 with initial value zero in the packet header , and sends out its pre - coding output packet @xmath69 to one of its neighbor @xmath9 , chosen uniformly at random among all its neighbors @xmath70 .",
    "the node @xmath9 accepts this pre - coding output packet @xmath69 with probability @xmath71 and updates its storage as @xmath72 .",
    "no matter the source packet is accepted or not , the node @xmath9 puts it into its forward queue and set the counter of @xmath69 as @xmath73 .    in each round ,",
    "when a node @xmath9 has at least one pre - coding output packet in its forward queue before the current round , @xmath9 forwards the head of line packet @xmath17 in its forward queue to one of its neighbor @xmath10 , chosen uniformly at random among all its neighbors @xmath12 .    depending on how many times",
    "@xmath17 has visited @xmath10 , the node @xmath10 makes its decisions :    if it is the first time that @xmath17 visits @xmath9 , then the node @xmath10 accepts this source packet with probability @xmath74 and updates its storage as @xmath75 .",
    "if @xmath17 has visited @xmath10 before and @xmath76 , then the node @xmath10 accepts this source packet with probability 0 .    no matter @xmath17 is accepted or not , the node @xmath10 puts it into its forward queue and increases the counter of @xmath17 by one @xmath77 .",
    "if @xmath17 has visited @xmath10 before and @xmath78 then the node @xmath10 discards packet @xmath17 forever .",
    "storage phase : when a node @xmath9 has made its decisions for all the pre - coding output packets @xmath79 , i.e. , all these packets have visited the node @xmath9 at least once , the node @xmath9 finishes its encoding process and @xmath80 is the storage packet of @xmath9 .",
    "the rcds - i algorithm achieves the same decoding performance as raptor codes .",
    "due to the space limitation , all the proofs for the theorems and lemmas are omitted .",
    "[ theorem : decoding - rcds - i ] suppose sensor networks have @xmath0 nodes and @xmath1 sources , and let @xmath81 .",
    "when @xmath0 and @xmath1 are sufficient large , the @xmath1 original source packets can be recovered from @xmath2 storage packets .",
    "the decoding complexity is @xmath38 .",
    "the price for the benefits we achieved in the rcds - i algorithm is the extra transmissions .",
    "the total number of transmissions ( the total number of steps of @xmath1 random walks ) is given in the following theorem .",
    "[ theorem : transmission - rcds - i ] denote by @xmath82 the total number of transmissions of the rcds - i algorithm , then we have @xmath83 where @xmath1 is the total number of sources before pre - coding , @xmath27 is the total number of outputs after pre - coding , and @xmath0 is the total number of nodes in the network .      in rcds",
    "- i algorithm , we assume that each node in the network knows @xmath0 and @xmath1the total number of nodes and sources .",
    "however , in many scenarios , especially , when changes of network topologies may occur due to node mobility or node failures , the exact value of @xmath0 may not be available for all nodes . on the other hand ,",
    "the number of sources @xmath1 usually depends on the environment measurements , or some events , and thus the exact value of @xmath1 may not be known by each node either . as a result , to design a fully distributed storage algorithm which does not require any global information is very important and useful . in this subsection",
    ", we propose such an algorithm based on raptor codes , called rcds - ii .",
    "the idea behind this algorithm is to utilize some features of simple random walks to do inference to obtain individual estimations of @xmath0 and @xmath1 for each node .    to begin",
    ", we introduce the definition of inter - visit time and inter - packet time . for a random walk on any graph , the _ inter - visit time _ is defined as follows  @xcite :    ( inter - visit time ) for a random walk on a graph , the _ inter - visit time _ of node @xmath9 , @xmath84 , is the amount of time between any two consecutive visits of the random walk to node @xmath9 .",
    "this inter - visit time is also called _ return time_.    for a simple random walk on random geometric graphs , the following lemma provides results on the expected inter - visit time of any node .",
    "[ lemma : inter - visit - time ] for a node @xmath9 with node degree @xmath13 in a random geometric graph , the mean inter - visit return time is given by @xmath85=\\frac{\\mu n}{d_n(u)},\\ ] ] where @xmath86 is the mean degree of the graph .    from lemma  [ lemma : inter - visit - time ]",
    ", we can see that if each node @xmath9 can measure the expected inter - visit time @xmath87 $ ] , then the total number of nodes @xmath0 can be estimated by @xmath88}{\\mu}.\\ ] ] however , the mean degree @xmath86 is a global information and may be hard to obtain .",
    "thus , we make a further approximation and let the estimation of @xmath0 by the node @xmath9 be @xmath89.\\ ] ]    in our distributed storage algorithms , each source packet follows a simple random walk . since there are @xmath1 sources , we have @xmath1 individual simple random walks in the network . for a particular random walk ,",
    "the behavior of the return time is characterized by lemma  [ lemma : inter - visit - time ] . nevertheless , lemma  [ lemma : inter - packet - time ] provides results on the inter - visit time among all @xmath1 random walks , which is called inter - packet time for our algorithm and defined as follows :    ( inter - packet time ) for @xmath1 random walks on a graph , the _ inter - packet time _ of node @xmath9 , @xmath90 , is the amount of time between any two consecutive visits of those @xmath1 random walks to node @xmath9 .",
    "[ lemma : inter - packet - time ] for a node @xmath9 with node degree @xmath13 in a random geometric graph with @xmath1 simple random walks , the mean inter - packet time is given by @xmath91=\\frac{e[t_{visit}(u)]}{k}=\\frac{\\mu n}{kd_n(u)},\\ ] ] where @xmath86 is the mean degree of the graph .    from lemma  [ lemma : inter - visit - time ] and lemma",
    "[ lemma : inter - packet - time ] , it is easy to see that for any node @xmath9 , an estimation of @xmath1 can be obtained by @xmath92}{e[t_{packet}(u)]}.\\ ] ]    after obtaining estimations for both @xmath0 and @xmath1 , we can employ similar techniques used in rcds - i to perform raptor coding and storage .",
    "we will only present details of the interference phase due to the space limitation . the initialization phase , pre - coding phase , raptor - coding phase and storage phase are the same as in rcds - i with replacements of @xmath1 by @xmath93 and @xmath0 by @xmath94 everywhere .",
    "inference phase :    for each node @xmath9 , suppose @xmath95 is the first source packet that visits @xmath9 , and denote by @xmath96 the time when @xmath95 has its @xmath97-th visit to the node @xmath9 .",
    "meanwhile , each node @xmath9 also maintains a record of visiting time for each other source packet @xmath98 that visited it .",
    "let @xmath99 be the time when source packet @xmath98 has its @xmath97-th visit to the node @xmath9 .",
    "after @xmath95 visiting the node @xmath9 @xmath100 times , where @xmath100 is system parameter which is a positive constant , the node @xmath9 stops this monitoring and recoding procedure .",
    "denote by @xmath101 the number of source packets that have visited at least once upon that time .    for each node @xmath9 ,",
    "let @xmath102 be the number of visits of source packet @xmath98 to the node @xmath9 and let @xmath103 .",
    "let @xmath104 , and @xmath105 . then , the average inter - visit time and inter - packet time for node @xmath9 are given by @xmath106 , and @xmath107,respectively .",
    "then the node @xmath9 can estimate the total number of nodes in the network and the total number of sources as @xmath108,and @xmath109 .",
    "in this phase , the counter @xmath48 of each source packet @xmath48 is incremented by one after each transmission .",
    "in this section , we study the performance of the proposed rcds - i and rcds - ii algorithms for distributed storage in wireless sensor networks through simulation .",
    "the main performance metric we investigate is the successful decoding probability versus the decoding ratio .",
    "( decoding ratio ) _ decoding ratio _",
    "@xmath110 is the ratio between the number of querying nodes @xmath111 and the number of sources @xmath1 , i.e. , @xmath112 .",
    "( successful decoding probability ) _ successful decoding probability _",
    "@xmath113 is the probability that the @xmath1 source packets are all recovered from the @xmath111 querying nodes .    in our simulation ,",
    "@xmath113 is evaluated as follows .",
    "suppose the network has @xmath0 nodes and @xmath1 sources , and we query @xmath111 nodes .",
    "there are @xmath114 ways to choose such @xmath111 nodes , and we choose @xmath115 uniformly randomly samples of the choices of query nodes .",
    "let @xmath116 be the number of samples of the choices of query nodes from which the @xmath1 source packets can be recovered .",
    "then , the successful decoding probability is evaluated as @xmath117 .",
    "our simulation results are shown in figures .",
    "[ fig : rcds - i-1 - 2 ] ,  [ fig : rcds - ii-1 - 2 ] and  [ fig : rcds - i - ii-3 ] . fig .",
    "[ fig : rcds - i-1 - 2 ] shows the decoding performance of rcds - i algorithm with different number of nodes and sources .",
    "the network is deployed in @xmath118 ^ 2 $ ] , and the system parameter @xmath59 is set as @xmath119 . from the simulation results we can see that when the decoding ratio is above 2 , the successful decoding probability is about @xmath120 .",
    "another observation is that when the total number of nodes increases but the ratio between @xmath1 and @xmath0 and the decoding ratio @xmath110 are kept as constants , the successful decoding probability @xmath113 increase when @xmath121 and decreases when @xmath122 .",
    "that is because the more nodes we have , the more likely each node has the desired degree distribution .",
    "[ fig : rcds - ii-1 - 2 ] compares the decoding performance of rcds - ii and rcds - i algorithms . to guarantee each node",
    "obtain accurate estimations of @xmath0 and @xmath1 , we set @xmath123",
    ". it can be seen that the decoding performance of the rcds - ii algorithm is a little bit worse than the rcds - i algorithm when decoding ratio @xmath110 is small , and almost the same when @xmath110 is large . to investigate how the system parameter @xmath59 and @xmath100 affects the decoding performance of the rcds - i and rcds - ii algorithms , we fix the decoding ratio @xmath110 and vary @xmath59 and @xmath100 .",
    "the simulation results are shown in fig .",
    "[ fig : rcds - i - ii-3 ] .",
    "it can be seen that when @xmath124 , @xmath113 keeps almost like a constant , which indicates that after @xmath125 steps , almost all source packet visit each node at least once .",
    "we can also see that when @xmath100 is chosen to be small , the performance of the rcds - ii algorithm is very poor .",
    "this is due to the inaccurate estimations of @xmath1 and @xmath0 of each node .",
    "when @xmath100 is large , for example , when @xmath126 , the performance is almost the same .",
    "in this paper , we studied raptor codes based distributed storage algorithms for large - scale wireless sensor networks .",
    "we proposed two new decentralized algorithms rcds - i and rcds - ii that distribute information sensed by @xmath1 source nodes to @xmath0 nodes for storage based on raptor codes . in rcds - i , each node has limited global information ; while in rcds - ii , no global information is required .",
    "we computed the computational encoding and decoding complexity , and transmission costs of these algorithms .",
    "we also evaluated their performance by simulation ."
  ],
  "abstract_text": [
    "<S> we consider a distributed storage problem in a large - scale wireless sensor network with @xmath0 nodes among which @xmath1 acquire ( sense ) independent data . the goal is to disseminate the acquired information throughout the network so that each of the @xmath0 sensors stores one possibly coded packet and the original @xmath1 data packets can be recovered later in a computationally simple way from any @xmath2 of nodes for some small @xmath3 . </S>",
    "<S> we propose two raptor codes based distributed storage algorithms for solving this problem . in the first algorithm , </S>",
    "<S> all the sensors have the knowledge of @xmath0 and @xmath1 . in the second one , </S>",
    "<S> we assume that no sensor has such global information . </S>"
  ]
}