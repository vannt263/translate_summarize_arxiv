{
  "article_text": [
    "in june of 2006 the first round of the mock lisa data challenges ( mldcs )  @xcite was released by the mldc taskforce of the lisa international science team .",
    "the hope of these challenges is to enhance the burgeoning field of gravitational wave data analysis with a focus on the laser interferometer space antenna ( lisa )  @xcite , to encourage the creation and sharing of ideas , methods , and techniques used by the various groups working on lisa data analysis , and to invite others to join in this interesting field of research .",
    "the mldc also provides a series of blind tests , of increasing complexity , for testing the performance of these various techniques .",
    "the first challenge was separated into three areas based on the types of sources whose gravitational waveforms were injected into mock lisa data streams .",
    "the first type of source ( used in challenge 1.1 ) was monochromatic galactic binaries , the second type of source ( used in challenge 1.2 ) was schwarzschild massive black hole binaries ( mbhbs ) , and the third type of source ( used in challenge 1.3 ) was extreme mass ratio inspirals ( emris ) .",
    "this work will cover the methods and techniques we used in preparing our entry for challenge 1.1 involving monochromatic source of gravitational waves .",
    "challenge 1.1.1 was divided into three parts , each with the signal of a single galactic binary system injected into a mock lisa data stream containing gaussian instrumental noise .",
    "this provided a simple test of algorithms across the lisa frequency band .",
    "challenge 1.1.2 contained 20 so - called `` verification binaries '' whose frequency and sky location were known .",
    "challenge 1.1.3 contained the signals of 20 galactic binary systems across the lisa band , with no other information provided to aid the search .",
    "challenge 1.1.4 was the first of the challenges to involve source confusion , where the signals from two or more binaries would overlap and thus possibly affect ( confuse ) data analysis techniques . here between @xmath0 and @xmath1 ( the actual number was unknown )",
    "signals were injected into a @xmath2 band , containing instrument noise .",
    "challenge 1.1.5 was designed to test an algorithm s ability to handle strong source confusion . in this challenge between @xmath3 and @xmath4 sources",
    "were injected into a @xmath5 band , containing instrument noise .",
    "our approach to these challenges was to use two algorithms that we have developed for lisa data analysis to search these various data streams , and extract the seven parameters that describe a monochromatic gravitational wave source : sky location ( @xmath6 ) , frequency ( @xmath7 ) , amplitude ( @xmath8 ) , polarization ( @xmath9 ) , inclination ( @xmath10 ) , and initial orbital phase ( @xmath11 ) .",
    "the first of these algorithms is the bam algorithm  @xcite . a brief overview of the algorithm is given in the next section .",
    "the bam algorithm was used on all levels of challenge 1.1 , though as it was optimized to handle areas where there is significant source overlap , we will focus our discussion on challenge 1.1.4 and 1.1.5 .",
    "the second algorithm we used in these challenges was a genetic algorithm ( ga )  @xcite .",
    "a brief overview of this algorithm is given in section [ ga_overview ] .",
    "as this algorithm has yet to be optimized for regions of high source confusion we used the ga on challenges 1.1.1 , 1.1.2 , and 1.1.3 .",
    "please note that these two methods of searching were conducted independently of each other .",
    "results garnered from the bam algorithm were not used to aid the ga searches or vice - versa .",
    "in this section we give a quick overview of the bam algorithm , for a more in depth description please see @xcite .",
    "the bam algorithm is a variant of the markov chain monte carlo ( mcmc ) approach  @xcite - a powerful set of techniques used to obtain the posterior distribution functions ( pdfs ) for a model that have been used in analyzing data in many different fields with excellent results .",
    "they were first brought to the study of gravitational wave data analysis by christensen and meyer  @xcite . since",
    "then the methods have been adapted to searching for sources in the lisa data stream  @xcite .",
    "the ability of mcmc techniques to explore large parameter spaces , with multiple sources makes them ideally suited to lisa data analysis . for a more complete review of mcmc methods",
    "see  @xcite .    in an mcmc approach",
    "one wants to generate a sample set , @xmath12 that corresponds to draws made from the posterior distribution of the system , @xmath13 . to do",
    "this one only needs to implement the following simple algorithm .",
    "beginning at a point , @xmath14 , in the parameter space for the system in question , propose a jump to a new point , @xmath15 , based on some proposal distribution , @xmath16 .",
    "the jump is accepted with probability @xmath17 , where @xmath18 is the hasting ratio .",
    "@xmath19 here @xmath20 is the prior of the parameters at @xmath14 , @xmath21 is the value of the proposal distribution for a jump from @xmath14 to @xmath15 , and @xmath22 is the likelihood at @xmath14 .",
    "if the noise is a normal process with zero mean , the likelihood is given by  @xcite : @xmath23\\ , , \\ ] ] where the normalization constant @xmath24 is independent of the signal , @xmath25 , and @xmath26 denotes the noise weighted inner product @xmath27 where @xmath28 and @xmath29 are the gravitational waveforms , and @xmath30 is the one - sided noise spectral density .",
    "repeated jumps using this algorithm will ensure convergence to the correct posterior for any ( non - trivial ) proposal distribution  @xcite . to decrease the time to reach convergence one wants use a proposal distribution ( or distributions ) that closely models the expected pdf .",
    "however , since we do not know the exact form of the posterior a priori , we choose to maintain maximum flexibility in our choice of proposal distributions .",
    "we achieve this by using a menu of various proposal distributions , including occasional `` bold '' proposals that attempt large variations in parameter values along with many `` timid '' proposals that attempt small variations in the parameter values of the chain ( for a detailed description of some of these proposals see  @xcite ) .",
    "this provides a simple means to both search the parameter space and develop the pdfs for the systems .",
    "the bam algorithm starts with a search phase that is followed by an exploration of the parameter posteriors . during the search phase simulated annealing",
    "is used to encourage exploration of the full parameter space .",
    "the annealing process effectively smooths the likelihood by the inclusion of a heating term .",
    "as the heat is removed the smoothed likelihood surface slowly anneals to the true surface . during this search phase the proposal distributions used to drive the metropolis - hastings sampling can be non - markovian .",
    "thus the search phase of the bam algorithm is not a true mcmc method .",
    "however , when the search phase is complete , and the sampling phase has begun , the proposal distributions that are used are purely markovian , and the subsequent portion of the chain can be used to properly explore the pdfs .    in our previous work",
    "@xcite the bam algorithm used the f - statistic  @xcite to limit the search to three variables per template , frequency and sky location ( @xmath7 , @xmath31 , @xmath32 ) .",
    "the extrinsic parameters - amplitude , polarization , inclination , and initial orbital phase ( @xmath8 , @xmath9 , @xmath10 , @xmath33 ) - were then recovered algebraically .",
    "while this method is still used as a first approximation for our searches in this work , we have extended the bam algorithm to a full @xmath34 parameter search so that we may match the pdfs for all seven parameters .",
    "another update is that we have replaced the low frequency approximation with the rigid adiabatic approximation  @xcite , using a fast new algorithm developed by cornish & littenberg  @xcite .",
    "the new waveforms give excellent matches to the full lisa simulator  @xcite output across the entire lisa band .",
    "the bam algorithm is optimized to search for multiple , densely packed monochromatic or mildly chirping signals .",
    "a key element of the bam algorithm is blocking .",
    "the blocks in the bam algorithm are small sub - units of the frequency range being searched . as can be seen in figure  [ block_mh_figure ] , which shows a schematic representation of a search region in our bam algorithm , the search region",
    "is broken up into equal sized blocks .",
    "the algorithm steps through these blocks sequentially , updating all sources within a given block simultaneously .",
    "after all blocks have been updated , they are shifted by one - half the width of a block for the next round of updates .",
    "this allows two correlated sources that might happen to be located on opposite sides of a border between two neighboring blocks to be updated together on every other update .",
    "it also lessens the number of parameters being updated , which greatly decreases the time needed to calculate non - linear proposal distributions that provide particularly good mixing of the chains .    a schematic representation of a search region in frequency space , showing the block structure and some features of the bam algorithm.,width=453 ]    this blocking allowed for quick and robust searching of isolated data snippets with up to @xmath35 templates .",
    "the limit on the number of templates is due to the computational cost of the ( non - linear ) multi - source f - statistic . in order to be able to handle the entire lisa band",
    ", we have to break the search up into sub - regions containing a manageable number of sources .",
    "this introduced the problem of edge effects from sources whose frequency lay just outside the chosen search region , but deposited power into the search region . to combat the edge effects we introduced `` wings '' and an `` acceptance window . ''",
    "the purpose of the wings is to create a buffer between the chosen search region and the regions beyond , so that sources in those outer regions would not adversely affect the `` searchers '' ( individual components of the multi - template ) in the acceptance window .",
    "the search would be run the same as before , but at the end only those searchers inside the acceptance window would be considered as having been found .",
    "searchers that ended up in the wings were discarded ( even though many might be perfectly good fits to actual sources ) .",
    "this would allow us step through frequency space , using multiple search regions .",
    "but that was not enough .",
    "we had to introduce a feature we call `` wing noise '' to attenuate the contribution from the wings in the noise weighted inner product .",
    "the wing noise is an increase in the noise spectral density in the wings increasing exponentially from the edge of the acceptance window to the edge of the search region .",
    "this keeps searchers from trying to fit to power bleeding into the search region from bright sources nearby .",
    "this problem which we call `` slamming '' is also lessened by hierarchically searching the lisa band , finding the brightest sources , and removing the signals of those sources who lay outside the search region , but bleed power into it .    as one can fit the noise in the system , continuing to add more searchers into the search region",
    "can steadily increase the value of the likelihood .",
    "thus some other means of deciding what is the best number of searchers must be used .",
    "the optimal method would be to calculate evidence ( @xmath36 ) for each model ( @xmath37 ) given the data ( @xmath25 ) , which is given by : @xmath38 however , computing this integral is prohibitively expensive for high dimension models .",
    "if we assume uniform priors we may use the laplace approximation estimate : @xmath39 where @xmath40 is the maximum likelihood for the model , @xmath41 is the volume of the model s parameter space , and @xmath42 is the volume of the uncertainty ellipsoid ( which can be estimated using a fisher information matrix ) .",
    "it is the @xmath43 term that will penalizes models with larger numbers of sources , thus serving as a built in occam factor . the model with the best ( approximate ) evidence",
    "is selected .",
    "thus one can divide up the lisa frequency band into multiple search regions , searching each for the resolvable sources that lie within .",
    "this allows for searches to be conducted in parallel ( after a few pilot searches to identify the brightest of sources that might lead to slamming ) on a distributed computing system , reducing the real - world time needed to search the entire lisa band to a matter of days .",
    "the ga is an algorithm that uses an evolving solution set whose breeding is based on fitness , as measured by ( the logarithm of ) the likelihood function , equation ( [ likely ] ) .",
    "the greater fitness a template ( organism ) has , the more likely it is to breed . through breeding and mutation",
    "the set of organisms evolves toward an optimal fit to the parameter values of the sources of the gravitational waves in the data stream . in our previous work",
    "@xcite the ga also used the f - statistic to limit the search space , and again we have extended the search to @xmath34 parameters for this work .",
    "the ga is used in this work to search only for the signals of the isolated binary systems ( challenges 1.1.1 - 1.1.3 ) .",
    "this was done because the ga has not yet been fully optimized for high density , multi - source searches .",
    "the ga that is used in these searches is the genetic - genetic algorithm ( gga ) discussed in our previous work .",
    "they are considered to be doubly genetic since the mutation rate of the parameters used when creating the next generation is controlled by the algorithm itself , thus optimizing the mutation rate along with the parameter values . in the previous work the parameter mutation rate ( pmr ) was one value ( per organism ) that controlled the mutation rate of all the parameters .",
    "here we have implemented distinct pmrs for each of the parameters being searched over ( i.e. each organism has three pmrs when an f - statistic based search is run , and seven pmrs when a full parameter search is run ) . as the gga is the only type of ga used in these searches , all will be referred to henceforth simply as gas .",
    "the searches conducted here consisted of @xmath44 organisms per generation with an elitism factor of @xmath45 ( i.e. the most fit organism was included in the next generation , and @xmath46 new organisms were bred to complete the new generation ) . also , mild simulated annealing was used to boost exploration of the likelihood surface during the initial phase of the search .",
    "the two breeding patterns used here are known as @xmath47-point crossover and @xmath48-point crossover . in the case of @xmath47-point crossover",
    ", breeding is accomplished by splicing the combination of complimentary sections of the binary strings used to represent the parameter values of two parent organisms . for @xmath48-point",
    "crossover each binary digit of the offspring s binary strings is randomly drawn from the corresponding locations in the parent strings .",
    "the breeding method was chosen at random for each breeding pair , though as the algorithm progressed the @xmath47-point crossover was increasingly preferred to the @xmath48-point crossover as @xmath48-point crossover breeding enhances exploration of the parameter space , while @xmath47-point crossover enhances stability after the algorithm has hopefully discovered the peak likelihood values of the space .",
    "in these three searches we are seeking the signals from a single binary system injected into a lisa data stream . in the first of the three challenges ( 1.1.1a )",
    "the source was known to have a frequency in the range @xmath49~{\\rm mhz}$ ] .",
    "the source for challenge 1.1.1b was in the frequency range @xmath50~{\\rm mhz}$ ] . while the source for challenge 1.1.1c was in the range @xmath51~{\\rm mhz}$ ] .    in challenges 1.1.1a and 1.1.1b",
    "the algorithms were allowed to search the entire range for the sources .",
    "exploratory searches of the entire ranges by both the ga and bam algorithms quickly localized down to the neighborhoods of @xmath52 and @xmath53 , respectively .",
    "results that were easily verified by viewing the fourier transform of the time series data . for challenge 1.1.1c",
    "the fourier transform was used to quickly localize the search to the region around @xmath54 .",
    "the first detailed searches of these localized ranges were performed using the f - statistic versions of the algorithms .",
    "the parameter values that were determined by these initial searches were then used as the starting points for the full @xmath34 parameter searches ( with the values obtained by the ga used for further ga searches , and the values obtained by the bam algorithm used for further bam searches ) .",
    "the values found by this @xmath34 parameter searching using ga are quoted in table  [ mldc_1.1.1_ga_table ] , along with the uncertainty estimations for each parameters as determined using a fisher information matrix ( fim ) calculated at the recovered parameter values .",
    "however , the bam algorithm included one more run per source to fully map out the pdf for the sources parameters .",
    "these searches used a highly efficient small uniform proposal , able to perform @xmath55 steps in less than ten minutes .",
    "the parameter values found by the bam algorithm are shown in table  [ mldc_1.1.1_bam_table ] , while figure  [ mldc_1.1.1a_histograms ] shows the histograms derived from the pdf exploration of challenge 1.1.1a ( note : this is the mode of marginalized pdf for each the parameter type , not the mode of the full @xmath56 dimensional posterior ) .",
    "also included in table  [ mldc_1.1.1_bam_table ] are two sets of uncertainty ranges determined a fim approach and a calculation of the standard deviations of the sources associated histograms .",
    "one issue to note in the results of this section are highlighted by the multi - modal nature of the histograms in figure  [ mldc_1.1.1a_histograms ] .",
    "there is a degeneracy in the polarization and initial phase parameters for monochromatic sources , such that a shift of @xmath57 in polarization with a corresponding shift of @xmath58 in initial phase represents the same physical system . with this taken into account nearly all parameters",
    "have been recovered to within @xmath59 as given by the fim ( and the chain standard deviations for the bam ) .",
    "the notable exceptions being the parameter values for the extrinsic parameters in challenge 1.1.1c .",
    "this difference is most likely due to a model mismatch between the coding of our algorithms and that of the mldc , possibly caused by implementing the new waveform generation method while the rushing to meet the december deadline .    [ cols=\"<,^,^,^,^,^,^,^\",options=\"header \" , ]     as challenges 1.1.2 and 1.1.3 were similar to challenge 1.1.1 , in that they featured essentially isolated sources , we will not give a review of those results here , but instead direct the interested reader to the evaluation of all of the entries to round 1 of the mldc in this volume @xcite .",
    "this challenge involved a search over a restricted frequency range ( @xmath60 ) containing a large number of injected signals ( @xmath61 ) .",
    "as the ga has not been optimized for searching such extended frequency ranges with high numbers of sources , only the bam algorithm was used in searching this data stream .    here",
    "the bam algorithm was used hierarchically , initially searching for a much smaller number of sources than was expected .",
    "the purpose of the initial run with few searchers is to pick off the brightest sources in the data stream so that in the next step , when the data is divided up in to separate frequency windows , the bright sources from one window can be accounted for in nearby windows .",
    "thus the initial run was performed searching the entire frequency range of the data looking for only @xmath62 sources . for the succeeding runs ,",
    "the data stream was separated into five snippets with windows of acceptance @xmath5 in width and wings ( acting as a buffer against edge effects ) @xmath63 in width ( please see @xcite for a full description on exactly how the wings provide this buffer and other aspects of multiple source searching using bam ) .",
    "sources that are discovered in the windows of acceptance are kept , those found in the wings are disregarded . in the hierarchical searches ,",
    "first one searcher is sent off to look for a source , and after a set number of steps in the chain the value of the evidence is recorded and a second searcher is added to the search . if , after a specified number of steps the evidence does not show that the fit has improved enough the search is ended and the results of the second searcher are discarded . if the evidence warrants keeping the second searcher , then a third searcher is added and the search continues ( up to a maximum of @xmath62 searchers in this particular run ) .    in the next step",
    "the searches were performed on the divided data stream , which was separated into @xmath62 search regions .",
    "there was a search for up to @xmath64 sources in each of the @xmath62 windows .",
    "after this step , the algorithm had isolated @xmath65 candidate sources .",
    "the next run over the divided data stream ( using the same @xmath62 windows ) , was a search for up to @xmath66 sources in each of the @xmath62 windows . to improve the chances of finding sources , @xmath62",
    "different starting seeds for each window were run in parallel on a supercomputer cluster .",
    "these runs consisted of up to @xmath67 steps in the chains ( all runs were stopped by the evidence criteria before reaching this mark ) .",
    "results from parallel chains were merged and duplicates discarded .",
    "after this step , the algorithm had isolated @xmath68 candidate sources , @xmath69 of which were considered to be recovered ( i.e. they had at most @xmath47 of the @xmath34 parameters",
    "greater than @xmath59 discrepant from the listed mldc parameters ) .",
    "the four false positives represent searchers whose parameters do not match this criteria .",
    "figure  [ mldc_1.1.4_figure ] gives a sampling of the results of this search .",
    "one thing to note is of the sources that were not recovered , the brightest ( snr @xmath70 ) shared an identical frequency with a brighter ( snr @xmath71 ) recovered source , and which was located only @xmath72 on the sky from the unrecovered source .",
    "this extreme similarity in sources represents one of the hardest challenges for lisa data analysis .",
    "this challenge involved a search over a smaller frequency range than challenge 1.1.4 , ( @xmath73 ) , yet it still contained a fairly large number of sources ( @xmath74 ) .",
    "it was designed to test an algorithm s ability to deal with very high source densities ( more than @xmath47 source every @xmath75 frequency bins ) . as in challenge 1.1.4 ,",
    "only the bam algorithm was used in searching this data stream .",
    "the first hierarchical search looked for @xmath66 sources over the entire data stream . in the next search ,",
    "the data stream was divided into six windows which were each searched for up to @xmath66 sources .",
    "these runs were performed using @xmath75 different starting seeds .",
    "the evidence was used to stop the search process if increasing the number of searchers did not provide enough benefit to the overall fit before that limit . while most stopped after reaching @xmath66 sources , many of these were found in the wings so that the number of candidates found in the acceptance windows of the searches only numbered @xmath76 .",
    "so at the end of this round there were @xmath76 candidate sources .",
    "another run was attempted in which the algorithm could search for up to @xmath77 sources per window .",
    "this run did not produce any more viable candidates , in part due to the analysis method used to study the chains .",
    "the candidate source parameters were chosen by taking the average values of the chains in that sampling phase of the search . however ,",
    "if there are other sources nearby the chain picks up secondary modes , and the average ends up being a blend of the two candidate sources .",
    "this is more of a problem for low snr sources , and presents a limit of the effectiveness of the bam algorithm when using chain averaging .",
    "we intend to move to choosing the candidate sources by either analyzing the modes of the histograms of the output chains or by using the maximum a posteriori ( map ) value to determine parameter values for candidate sources in the data stream .",
    "figure  [ mldc_1.1.5_figure ] gives a sampling of the results of this search .",
    "as with challenge 1.1.4 , this challenge also contained sources with identical frequencies . in this case",
    "there was a pair of binaries sharing one identical frequency ( both located within @xmath78 of each other on the sky ) as well as a triplet of binaries sharing another identical frequency ( all located within @xmath79 of each other on the sky ) .",
    "the bam algorithm was only able to recover @xmath80 of these @xmath62 sources before the deadline , and using averaging of the chains .",
    "two more of the unrecovered sources had a second source located less than one - quarter of one frequency bin from a recovered source .",
    "we hope to be able to separate such sources with the next version of the algorithm .",
    "the ga and bam algorithms performed well in these first mldcs . while the ga has yet to be optimized for multi - source searching , it was able to recover the parameters of the isolated sources quite well .",
    "the bam algorithm was especially suited to the multi - source cases in challenges 1.1.4 and 1.1.5 .",
    "it recovered more than @xmath81 of the sources present in challenge 1.1.4 , and more than @xmath82 of the sources present in the extremely source - dense data stream of challenge 1.1.5 .",
    "also , with its ability to partition of the frequency band , the bam algorithm appears to be ready to tackle the second round of mldcs  @xcite , which will include the gravitational wave signals from @xmath83 galactic binary systems . however , there is still room for improvement .",
    "some suggestions have been given , particularly moving away from using chain averaging for parameter determination .",
    "another improvement to the bam that will be paramount when attempting the next round of mldcs is introduction of the noise level as a searchable parameter",
    ". this should allow the algorithm to remove the signals to near the confusion limit .",
    "this work was supported at msu by nasa grant nng05gi69 g .",
    "the searches described herein were carried out using the computing facilities of the jet propulsion laboratory , california institute of technology , under a contract with the national aeronautics and space administration .",
    "99 arnaud k a _ et al _ 2006 _ laser interferometer space antenna : 6th int .",
    "lisa symp . _",
    "( 1923 jun 2006 greenbelt , md ) ed s m merkowitz and j c livas ( melville , ny : aip ) p 619 arnaud k a _ et al _ 2006 _ laser interferometer space antenna : 6th int .",
    "lisa symp . _",
    "( 1923 jun 2006 greenbelt , md ) ed s m merkowitz and j c livas ( melville , ny : aip ) p 625 bender p _",
    "_ 1998 _ lisa pre - phase a report _",
    "crowder j and cornish n j 2007 _ phys .",
    "d _ * 75 * 043008 crowder j , cornish n j and reddinger j l 2007 _ phys.rev . d _ * 73 * 063011 metropolis n",
    "_ et al . _",
    "1953 _ j. chem .",
    "* 21 * 1087 hastings w k 1970 _ biometrics _ * 57 * 97 gamerman d 1997 _ markov chain monte carlo : stochastic simulation of bayesian inference _ ( chapman & hall , london ) christensen n and meyer r _ phys . rev .",
    "d _ * 58 * 082001 cornish n j and crowder j 2005 _ phys .",
    "d _ * 72 * 043005 cornish n j and porter e k 2006 _ class .",
    "* 23 * s761 wickham e d l , stroeer a and vecchio a 2006 _ class .",
    ". grav . _ * 23 * s819 cornish n j and porter e k 2006 _ phys",
    "d _ * 75 * 021301 stroeer a , gair j and vecchio a 2006 _ laser interferometer space antenna : 6th int .",
    "lisa symp . _",
    "( 1923 jun 2006 greenbelt , md ) ed s m merkowitz and j c livas ( melville , ny : aip ) brown d a _ et al _ 2007 , p xxx in this volume cornish n j and porter e k _ et al _ 2007 , p xxx in this volume ( _ preprint _ gr - qc/0701167 ) andrieu c and doucet a 1999 _ ieee trans .",
    "signal process .",
    "* 2667 andrieu c _ et al _ 2003 _ machine learning _ * 50 * 5 finn l s 1992 _ phys .",
    "d _ * 46 * 5236 1996 _ markov chain monte carlo in practice _ ed w r gilks , s richardson and d j spiegelhalter , ( chapman & hall , london ) jaranowski p , krolak a and schutz b f 1998 _ phys .",
    "d _ * 58 * 063001 rubbo l j , cornish n j and poujade o 2004 _ phys .",
    "d _ * 69 * 082003 cornish n j and littenberg t 2007 _ in preparation _ lisa simulator v. 2.0 , www.physics.montana.edu/lisa arnaud k a _ et al _ 2007 , p xxx in this volume ( _ preprint _",
    "gr - qc/0701139 ) arnaud k a _ et al _ 2007 , p xxx in this volume ( _ preprint _ gr - qc/0701170 )"
  ],
  "abstract_text": [
    "<S> we report on the performance of an end - to - end bayesian analysis pipeline for detecting and characterizing galactic binary signals in simulated lisa data . </S>",
    "<S> our principal analysis tool is the blocked - annealed metropolis hasting ( bam ) algorithm , which has been optimized to search for tens of thousands of overlapping signals across the lisa band . </S>",
    "<S> the bam algorithm employs bayesian model selection to determine the number of resolvable sources , and provides posterior distribution functions for all the model parameters . </S>",
    "<S> the bam algorithm performed almost flawlessly on all the round 1 mock lisa data challenge data sets , including those with many highly overlapping sources . </S>",
    "<S> the only misses were later traced to a coding error that affected high frequency sources . </S>",
    "<S> in addition to the bam algorithm we also successfully tested a genetic algorithm ( ga ) , but only on data sets with isolated signals as the ga has yet to be optimized to handle large numbers of overlapping signals . </S>"
  ]
}