{
  "article_text": [
    "the elastic net @xcite was proposed for solving optimization problems like the famous travelling salesman problem . here",
    "we apply this concept to feature maps .",
    "the elastic net is defined as a gradient descent in the energy landscape @xmath11 with the input vectors denoted by @xmath12 . here",
    "@xmath13 is the index of the neurons in an one - dimensional array ( for the tsp : with periodic boundary conditions ) , and @xmath14 is the synaptic weight vector of that neuron . for @xmath15 ( [ eq : elnetenergie ] ) becomes @xmath16 here @xmath17 denotes the neuron with the smallest distance to the stimulus , the winning neuron , which is assumed to be nondegenerate .",
    "a gradient descent in the first term ( which can be interpreted as an entropy term @xcite ) leads for sufficiently small @xmath18 to the condensation of ( at least ) one weight vector to each input vector , if the input space is discrete .",
    "the second term is the potential energy of an elastic string between the weight vectors , and gradient descent in this term leads to a minimization of the ( squared ! ) distances between the weight vectors .",
    "depending on parameter adjustment @xcite a gradient descent in @xmath19 can provide near - optimal solutions to the tsp within polynomial processing time @xcite , similar as the kohonen algorithm @xcite .",
    "we remark that in the travelling salesman application ( if the numbers of neurons and cities are chosen to be equal ) both the elastic net and the kohonen algorithm share the same zero @xcite and first @xcite order terms and are therefore related for the final state of convergence , although their initial ordering process is different .",
    "the update rule of the elastic net algorithm is the gradient descent in  ( [ eq : elnetenergie ] ) : @xmath20 where @xmath21 denotes the discrete laplacian .",
    "if we apply this concept to feature maps , we have to replace the sum over all input vectors by an integral over @xmath22 , i.e. a probability density . if we interpret ( [ eq : elnetgrad ] ) as a neural feature mapping algorithm , it is a pattern parallel learning rule , or batch update rule , where contributions of all patterns are summed up to one update term . in the brain , hovever ,",
    "patterns are presented serially in a stochastic sequence .",
    "therefore we generalize this algorithm to serial presentation : @xmath23 asymptotic level density of the elastic net 941 +   +   in monte carlo simulations of this model , one chooses input vectors @xmath0 according to the probablility density function @xmath24 and updates @xmath5 for every neuron @xmath10 in the neural layer according to  ( [ eq : elnetseriell ] ) .",
    "the algorithm can be viewed as a stochastic approximation algorithm that converges if the conditions @xmath25 and @xmath26 for the time development of parameter @xmath9 are fulfilled @xcite the simultaneous adjustment of @xmath27 and @xmath18 has been discussed in @xcite for the special case of the tsp optimization problem . for the tsp it appears necessary to adjust @xmath28 to a system - size - dependent value to avoid spike defects for small @xmath28 and frozen bead defects for large @xmath28 when annealing @xmath29 @xcite .",
    "both defects are no defects in feature maps , the spike defects can only occur for delta - peaked stimuli ( cities ) together with a dimension - reduction .",
    "the aim in feature maps is different . using the kohonen algorithm ,",
    "one tries to start with large - ranged interaction in the neural layer to avoid global topological defects .",
    "this is not directly possible for the elastic net , as its learning cooperation is restricted to next - neighbour .",
    "only the strength of the elastic spring @xmath27 can be initialized with a high value and decreased after global ordering .",
    "the parameter @xmath18 is to be interpreted as a resolution length in feature space , e.  g. the distance between two receptors in skin or retina . for selectivity of the",
    "winner - take - all mechanism , one would choose @xmath18 smaller or alike the average or minimal distance between adjacent weight vectors .",
    "in this paper we consider the case of continuously distributed input spaces with same dimensionality as the neural layer , so there is no reduction of dimension .",
    "the magnification factor is defined as the density of neurons @xmath10 ( i.  e. the density of synaptic weight vectors @xmath5 ) per unit volume of input space , and therefore is given by the inverse jacobian of the mapping from input space to neural layer : @xmath30 .",
    "( in the following we consider the case of noninverting mappings , where @xmath31 is positive . )",
    "the magnification factor is a property of the networks response to a given probability density of stimuli @xmath32 . to evaluate @xmath33 in higher dimensions ,",
    "one in general has to compute the equilibrium state of the whole network using global knowledge on @xmath32 .    for one - dimensional mappings ( and possibly for special geometric cases in higher dimensions ) the magnification factor may follow an universal magnification law ,",
    "i.e. @xmath34 is a function only of the local probability density @xmath35 and independent of both location @xmath10 in the neural layer and @xmath36 in input space .",
    "an optimal map from the view of information theory would reproduce the input probability exactly ( @xmath37 with @xmath38 ) , according to a power law with exponent  1 , equivalent to all neurons in the layer fire with same probability .",
    "an algorithm of maximizing mutual information has been given by linsker @xcite .",
    "for the classical kohonen algorithm the magnification law ( for one - dimensional mappings ) is a power law @xmath39 with exponent @xmath40 @xcite . for a discrete neural layer and especially for neighborhood kernels with different shape and range",
    "there are corrections to the magnification law @xcite .",
    "claussen and h.g .",
    "the necessary condition for the final state of algorithm  ( [ eq : elnetseriell ] ) is that for all neurons @xmath10 the expectation value of the learning step vanishes : @xmath41 since this expectation value is equal to the learning step of the pattern parallel rule  ( [ eq : elnetseriell ] ) , equation  ( [ eq : notwend ] ) is the stationary state condition for _ both _ serial and parallel updating . inserting the learning rule  ( [ eq : elnetseriell ] ) to condition  ( [ eq : notwend ] ) , we obtain for the invariant density @xmath42 in the one - dimensional case : @xmath43    in the limit of a continuous neural layer for every stimulus @xmath44 there exists one unique center of excitation @xmath45 with @xmath46 thus we can substitute integration over @xmath47 by integration over @xmath48 . using the jacobian @xmath49",
    ", we have @xmath50 the second term becomes @xmath51 .",
    "the normalization integral is ( @xmath52 ) : +   @xmath53 + for the following equations , we define the abbreviation @xmath54 . using parametric differentiation , substitution @xmath55 , and saddlepoint expansion ( method of steepest descent ) for @xmath29 , the first integral becomes ( after simic @xcite ) : @xmath56 neglecting higher orders of @xmath18 , we obtain @xmath57 this is a first - order nonlinear differential equation for @xmath58 to a given input density @xmath59 .",
    "however , this can be expressed explicitly only if ( additional to @xmath60 ) the complete equilibrium state @xmath61 is known , and then one obtains @xmath58 directly by evaluating the first derivative .",
    "thus the differential equation  ( [ eq : elnet_pjr ] ) gives further insight only if @xmath58 follows an universal scaling law without explicit dependence on the location @xmath13 , that is , @xmath31 is a function of @xmath62 only .",
    "asymptotic level density of the elastic net 943 +   +   the ansatz @xmath63 leads for all @xmath13 , where @xmath64 , to the differential equation for the invariant state of the one - dimensional elastic net algorithm @xmath65 the first derivative depends only on @xmath66 . the gradient field of ( [ eq : elnetdgl ] ) has two regimes : for @xmath67 ( ` soft string tension ' ) @xmath68 therefore @xmath69 the magnification exponent is asymptotically 1 and cortical representation is near to the optimum given by information theory .",
    "for @xmath70 ( ` hard string tension ' ) @xmath71 therefore @xmath72 has a constant value . here",
    "all adaptation to the stimuli vanishes , equivalent to a magnification exponent of zero .    substituting @xmath73 , @xmath74 and @xmath75 ( [ eq : elnetdgl ] ) can be solved exactly ( see fig .  [",
    "fig : elnetloesungen ] ) @xmath76    thus the magnification exponent depends only on the local input probability density @xmath77 , and we have @xmath78 , where @xmath79 for limiting cases with @xmath80 . for @xmath81",
    "the magnification exponent shifts from @xmath82 to zero according to equation  ( [ eq : elnetdgl ] ) , rewritten as @xmath83    finally we remark that the decomposition ( [ eq : elnetseriell ] ) of the parallel update rule to update responses to the stimuli is not unique .",
    "especially the elastic term can be decomposed in a siutable stimulus - dependent manner so that elasticity is appended only in vicinity of the stimulus .",
    "this local elastic net reads @xmath84 where @xmath85 is a normalized gaussian function of distance , @xmath86 and @xmath87 .",
    "a small global elasticity ( e.g. @xmath88 ) smoothes fluctuations , but the `` forgetting '' due to global relaxation is reduced which improves convergence .",
    "the magnification law of the local elastic net is similar as for the elastic net @xcite .",
    "claussen and h.g .",
    "to calculate the asymptotic level density numerically , we considered the map of the unit interval to a onedimensional neural chain of 100 neurons with fixed first and last neuron .",
    "the learning rate was @xmath89 .",
    "the stimulus probability density was chosen exponentially as @xmath90 with @xmath91 .",
    "after an adaptation process of @xmath92 steps further @xmath93 of learning steps were used to calculate average slope and its fluctuation ( shown in brackets ) of @xmath94 as a function of @xmath95 ( the first and last @xmath93 of neurons were excluded to eliminate boundary effects ) . the ( local ) magnification exponents were obtained as +        + for the elastic net the parameter choice appeared crucial : same as in the tsp application @xcite the optimal choice of @xmath18 as the average distance ( in input space ) between two adjacent neurons seems to be appropriate . for larger @xmath18 clearly clustering phenomena",
    "appear due to the fact that too many neurons fall in the gaussian neighborhood of the stimulus . for large @xmath96",
    "the exponent decreases to zero , as given by the theory . for small @xmath96 the exponent first increases near to @xmath82 but",
    "simultaneously instability due to clustering arises ( last row ) .",
    "whereas the simulation validates the exact result , appropriate adjustment of @xmath96 between optimal mapping and stability remains difficult and becomes intractable for large - scale variations of the input probability density .",
    "16 t. kohonen 1982 .",
    "_ biological cybernetics _ * 43 * , 59 - 69 .",
    "+ k.  obermayer , g.  g.  blasdel , and k.  schulten 1992 .",
    "rev . a _ * 45 * , 7568 - 7589",
    ". + h. ritter , t. martinetz , and k. schulten 1992 .",
    "_ neural computation and self - organizing maps_. addison - wesley .",
    "+ r. durbin and d. willshaw 1987 .",
    "_ nature _ * 326 * ,  689 - 691 .",
    "+ p. d. simic 1990 .",
    "_ network _ * 1 * , 89 - 103 .",
    "+ r. durbin , r. szeliski , and a. yuille 1989 .",
    "_ neural computation _ * 1 * , 348 - 358 .",
    "+ m. w. simmen 1991 .",
    "_ neural computation _ * 3 * , 363 - 374 .",
    "+ j. hertz , a. krogh , and r. g. palmer 1991 .",
    "to the theory of neural comp . _",
    "addison - wesley , reading , ma .",
    "+ j. c. claussen ( born gruel ) 1992 .",
    "diploma thesis , kiel , + j. c. claussen ( born gruel ) and h. g. schuster 1994 .",
    "+ t. kohonen 1991 . in : _ artificial neural networks _",
    ", ed . t. kohonen et  al .",
    "north - holland , amsterdam .",
    "+ r. linsker 1989 .",
    "_ neural computation _ * 1 * , 402 - 411 .",
    "+ h. ritter and k. schulten 1986 .",
    "_ biological cybernetics _ * 54 * , 99 - 106 .",
    "+ d. r. dersch and p. tavan 1995 .",
    "_ ieee trans .",
    "_ * 6 * , 230 - 236 .",
    "+ h. ritter 1991 .",
    "_ ieee transactions on neural networks _ * 2 * , 173 - 175 .",
    "+ p. d. simic 1994 .",
    "private communication ."
  ],
  "abstract_text": [
    "<S> whileas the kohonen self organizing map shows an asymptotic level density following a power law with a magnification exponent 2/3 , it would be desired to have an exponent 1 in order to provide optimal mapping in the sense of information theory . in this paper , we study analytically and numerically the magnification behaviour of the elastic net algorithm as a model for self - organizing feature maps . </S>",
    "<S> in contrast to the kohonen map the elastic net shows no power law , but for onedimensional maps nevertheless the density follows an universal magnification law , i.e. depends on the local stimulus density only and is independent on position and decouples from the stimulus density at other positions .    </S>",
    "<S> self organizing feature maps map an input space , such as the retina or skin receptor fields , into a neural layer by feedforward structures with lateral inhibition . </S>",
    "<S> biological maps show as defining properties topology preservation , error tolerance , plasticity ( the ability of adaptation to changes in input space ) , and self - organized formation by a local process , since the global structure can not be coded genetically . </S>",
    "<S> the self - organizing feature map algorithm proposed by kohonen @xcite has become a successful model for topology preserving primary sensory processing in the cortex @xcite , and an useful tool in technical applications @xcite .    the kohonen algorithm for self organizing feature maps </S>",
    "<S> is defined as follows : every stimulus @xmath0 of an euclidian input space @xmath1 is mapped to the neuron with the position @xmath2 in the neural layer @xmath3 with the highest neural activity , given by the condition @xmath4 denotes the euclidian distance in input space . in the kohonen model the learning rule for each synaptic weight vector @xmath5 is given by @xmath6 with @xmath7 as a gaussian function of euclidian distance @xmath8 in the neural layer . </S>",
    "<S> the function @xmath7 describes the topology in the neural layer . </S>",
    "<S> the parameter @xmath9 determines the speed of learning and can be adjusted during the learning process . </S>",
    "<S> topology preservation is enforced by the common update of all weight vectors whose neuron @xmath10 is adjacent to the center of excitation @xmath2 . </S>",
    "<S> 940 j.c . </S>",
    "<S> claussen and h.g . </S>"
  ]
}