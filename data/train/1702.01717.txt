{
  "article_text": [
    "in recent years deep belief networks have achieved remarkable results in natural language processing @xcite , computer vision @xcite@xcite and speech recognition @xcite tasks . specifically , within natural language processing ,",
    "modeling information in search queries and documents has been a long - standing research topic @xcite@xcite .",
    "most of the work with deep learning has involved learning word vector representations through neural language models @xcite@xcite@xcite and performing composition over the learned word vectors for classification @xcite .",
    "the optimal transformation in our case was to map each query document to a single numeric vector by assigning a single numeric value to each unique word across all query documents .",
    "a second phase was then employed by mapping the numerically transformed query vectors to a random embedding space having a uniform distribution between -1 and 1 .",
    "this helped far more in reducing the distance between queries having similar words while further discriminating queries far on the data space having more dissimilar words .",
    "another suitable criteria that is applicable to our problem is proposed by johnson and zhang @xcite in 2014 , where they propose a similar model , but swapped in high dimensional ` one - hot ' vector representations of words as cnn inputs .",
    "convolutional neural networks ( cnn ) are biologically - inspired variants of multiple layer perceptrons ( mlp ) .",
    "they utilize layers with convolving filters that are applied to local features @xcite originally invented for computer vision .",
    "convolutional neural networks have also been shown to be highly effective for natural language processing and have achieved excellent results in information retrieval @xcite , semantic parsing @xcite , sentence modeling @xcite and other traditional natural language processing tasks @xcite .    before going into the details of our model architecture and results",
    ", we will first narrate the work we did to prepare our query data for modelling .",
    "the advertisements in ebay s classifieds platforms are classified according to a pre - defined hierarchy . the first level ( l1 ) of this hierarchy categorizes advertisements into general groupings like ` buy & sell ' , ` cars & vehicles ' , ` real state ' , ` pets ' , ` jobs ' , ` services ' , ` vacation rentals ' and ` community ' . the second level ( l2 )",
    "further classifies each l1-category with many subclasses with more specificity .",
    "the third level ( l3 ) further classifies and so on .",
    "most platforms terminate the hierarchy at a level of three or four . in this paper",
    "we will only demonstrate the results of our work related to l1-category query classification .    for each keyword search initiated within a user session at the all - advertisement level (",
    "all - advertisement level means a search across all inventory with no category restrictions employed ) , the chain of actions on that search is analysed .",
    "when that sequence of actions results in a view of an advertisement within a specific category , that particular category is scored with a dominance point for the given query .",
    "there are many noisy factors that must be accounted for when applying this technique . among them include factors like bots , redundant query actions , filtering out conversions to categories that no longer exist and filtering out queries without enough conversions .",
    "the dominance of category for each query document in the last 90 days is computed on the basis of the maximum number of collaborative clicks for each l1-category .",
    "the category with the highest number of clicks is considered the dominant category for that query .",
    "this also enabled us to produce the first highest , second highest and third highest dominant category and their respective conversion rates for each query . the conversion rate per query is calculated by counting the total number of clicks for each category divided by the total number of clicks for that query .",
    "finally all query documents for the last 90 days are standardized by transforming them to lower - case , removing duplicate queries , extra spaces , punctuations and all other noise factors .",
    "a single pattern from each l1-category of the final preprocessed data ready to be used for learning is shown in table [ preprocessed ] .    in table",
    "[ preprocessed ] the categoryid feature is used as a label for supervised learning using a deep convolutional neural network .",
    "the total distinct query patterns for most of the categories in the last 90 days ranges between 5000 to 7000 .    [ cols=\"^,^,^,^,^\",options=\"header \" , ]     [ model_result ]",
    "results of the proposed model for the dominant category prediction problem compared to other state - of - the - art methods are listed in table [ model_result ] .",
    "the proposed well - tuned deep convolutional neural network simply outperformed its variations and other models .",
    "we tested the predictive accuracy by first using few days different testing data from training shown in the first row and fourth column of table [ model_result ] for every model type .",
    "the cnn model produced a very high training and testing accuracy of 99.9 % and 98.5 % .",
    "secondly we tried testing completely different days testing data from training and the resulting outcomes are shown in the second row of table [ model_result ] for every model type .",
    "this is our worst case scenario where we have used a completely different testing data for dominant category prediction but still the cnn model has produced a very high testing accuracy of 95.8 % .",
    "the major advantage with cnn compared to other state - of - the - art approaches is its added capability to learn invariant features .",
    "this capability of cnn to make the convolution process invariant to translation , rotation and shifting helps in approximating to the same class even when there is a slight change in the input query document .",
    "the step by step training accuracy and loss of our convolutional neural network model are also shown in figure [ fig : sub1 ] and [ fig : sub2 ] .",
    "initially the accuracy was noted very low but gradually it improved at each training step and almost reached to one in the end as shown in figure [ fig : sub1 ] .",
    "similarly , the loss was very high in the beginning , but almost reached to zero in the end as shown in figure [ fig : sub2 ] .",
    "this clearly shows the convergence of the proposed well - tuned deep convolutional neural network .",
    "the multiple layer perceptron model with an empirically evaluated one and two hidden layers of size 200 did not perform effectively well and produced a predictive accuracy of 55.91 % and 54.98 % on both of the testing sets .",
    "we also further tried to increase the count of hidden layers to explicitly add the certain level of non - linearity but still the predictive accuracy more or less remained constant .",
    "furthermore we tried running long short term memory ( lstm ) recurrent neural networks which are shown to outperform other recurrent neural network algorithms specifically for language modelling @xcite .",
    "however , in our case there is no sequence to sequence connection between the current and previous activations of the sequential query patterns , the maximum predictive accuracy that lstm recurrent neural network could produce was 63.06 % and 65.19 % for both the testing datasets .",
    "the bi - directional recurrent neural network worked a little worse compared to lstm network and produced a predictive accuracy of 52.98 % and 50.05 % on both the testing datasets .",
    "in the present work we have described a tuned , fully connected cnn that outperformed its variants and other state - of - the art ml techniques .",
    "specifically , in query to category classification across several ebay classifieds platforms .",
    "our results integrate to evidence that numeric vector mapping to random uniformly distributed embedding spaces proves more suitable both computationally and performance wise in comparison to word2vec . specifically for datasets having a limited vocabulary corpus ( between 10,000 to 15,000 words ) and few words ( between 2 to 3 ) in each query document .",
    "the first and second authors are grateful to johann schweyer for his contribution in query normalization and aggregation .",
    "we are also extremely thankful to brent mclean vp , cto , ebay classifieds for his kind support and encouragement throughout this dominant category prediction project .",
    "g.  e. hinton , n.  srivastava , a.  krizhevsky , i.  sutskever , and r.  r. salakhutdinov , `` improving neural networks by preventing co - adaptation of feature detectors , '' _ arxiv preprint arxiv:1207.0580 _ , 2012 .",
    "s.  deerwester , s.  t. dumais , g.  w. furnas , t.  k. landauer , and r.  harshman , `` indexing by latent semantic analysis , '' _ journal of the american society for information science _",
    "41 , no .",
    ", 1990 .",
    "j.  gao , j .- y .",
    "nie , g.  wu , and g.  cao , `` dependence language model for information retrieval , '' in _ proceedings of the 27th annual international acm sigir conference on research and development in information retrieval_.1em plus 0.5em minus 0.4emacm , 2004 , pp . 170177 .",
    "r.  collobert , j.  weston , l.  bottou , m.  karlen , k.  kavukcuoglu , and p.  kuksa , `` natural language processing ( almost ) from scratch , '' _ journal of machine learning research _ , vol .",
    "aug , pp . 24932537 , 2011 .",
    "y.  shen , x.  he , j.  gao , l.  deng , and g.  mesnil , `` learning semantic representations using convolutional neural networks for web search , '' in _ proceedings of the 23rd international conference on world wide web_.1em plus 0.5em minus 0.4emacm , 2014 , pp .",
    "373374 .",
    "yih , k.  toutanova , j.  c. platt , and c.  meek , `` learning discriminative projections for text similarity measures , '' in _ proceedings of the fifteenth conference on computational natural language learning_.1em plus 0.5em minus 0.4emassociation for computational linguistics , 2011 , pp . 247256 ."
  ],
  "abstract_text": [
    "<S> deep neural networks , and specifically fully - connected convolutional neural networks are achieving remarkable results across a wide variety of domains . </S>",
    "<S> they have been trained to achieve state - of - the - art performance when applied to problems such as speech recognition , image classification , natural language processing and bioinformatics . </S>",
    "<S> most of these `` deep learning '' models when applied to classification employ the softmax activation function for prediction and aim to minimize cross - entropy loss . in this paper , </S>",
    "<S> we have proposed a supervised model for dominant category prediction to improve search recall across all ebay classifieds platforms . </S>",
    "<S> the dominant category label for each query in the last 90 days is first calculated by summing the total number of collaborative clicks among all categories . </S>",
    "<S> the category having the highest number of collaborative clicks for the given query will be considered its dominant category . </S>",
    "<S> second , each query is transformed to a numeric vector by mapping each unique word in the query document to a unique integer value ; all padded to equal length based on the maximum document length within the pre - defined vocabulary size . a fully - connected deep convolutional neural network ( cnn ) </S>",
    "<S> is then applied for classification . </S>",
    "<S> the proposed model achieves very high classification accuracy compared to other state - of - the - art machine learning techniques . </S>"
  ]
}