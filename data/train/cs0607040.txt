{
  "article_text": [
    "the literature on parallel logic programming ( see @xcite for a general discussion of parallel logic programming ) underscores the potential for achieving excellent speedups and performance improvements from execution of logic programs on parallel architectures , with little or no programmer intervention .",
    "particular attention has been devoted over the years to the design of technology for supporting _ or - parallel _ execution of prolog programs on shared - memory architectures .    or - parallelism ( op ) arises from the non - determinism implicit in the process of reducing a given subgoal using different clauses of the program .",
    "the non - determinism arising during the execution of a logic program is commonly depicted in the form of a _ search tree _",
    "( a.k.a .  _ or - tree _ ) .",
    "each internal node represents a _ choice - point _ ,",
    "i.e. , an execution point where multiple clauses are available to reduce the selected subgoal . leaves of the tree represent either failure points ( i.e. , resolvents where the selected subgoal does not have a matching clause ) or success points ( i.e. , solutions to the initial goal ) .",
    "a sequential computation boils down to traversal of this search tree according to some predefined search strategy  e.g .",
    ", prolog adopts a fixed strategy based on a left - to - right , depth - first traversal of the search tree .    while in a sequential execution the multiple clauses that match a subgoal",
    "are explored one at a time via backtracking , in or - parallel execution we allow different instances of prolog engines ( _ computing agents_)executing as separate processes  to concurrently explore these alternative clauses .",
    "different agents concurrently operate on different branches of the or - tree , each attempting to derive a solution to the original goal using a different sequence of derivation steps . in this work",
    "we will focus on or - parallel systems derived from the multi - sequential model originally proposed by d.h.d .",
    "warren @xcite . in this model ,",
    "the multiple agents traverse the or - tree looking for unexplored branches . if an unexplored branch ( i.e. , an untried clause to resolve a selected subgoal ) is found , the agent picks it up and begins execution",
    ". this agent will stop either if it fails ( reaches a failing leaf ) , or if it finds a solution . in case of failure , or",
    "if the solution found is not acceptable to the user , the agent will _ backtrack _ ,",
    "i.e. , move back up in the tree , looking for other choice - points with untried alternatives to explore .",
    "the agents need to synchronize if they access the same node in the tree  to avoid repetition of computations . in the rest of this work we will call _ parallel choice - points _ those choice - points",
    "from which we allow exploitation of parallelism .    intuitively , or - parallelism allows concurrent search for solution(s ) to the original goal .",
    "the importance of the research on efficient techniques for handling or - parallelism arises from the generality of the problem  technology originally developed for parallel execution of prolog programs has found application in contexts such as constraint programming ( e.g. , @xcite ) and non - monotonic reasoning ( e.g. , @xcite ) .",
    "efficient implementation of or - parallelism has also been extensively investigated in the context of ai systems @xcite .    in sequential implementations of search - based ai systems or prolog ,",
    "typically one branch of the tree resides on the inference engine s stacks at any given time .",
    "this simplifies implementation quite significantly .",
    "however , in case of parallel systems , multiple branches of the tree co - exist at the same time , making parallel implementation quite complex .",
    "efficient management of these co - existing branches is quite a difficult problem , and it is referred to as the _ environment management problem _ @xcite .",
    "most research in or - parallel execution of prolog so far has focused on techniques aimed at _ shared - memory multiprocessors ( smps)_. relatively fewer efforts @xcite have been devoted to implementing prolog systems on _ distributed - memory platforms ( dmps)_. out of these efforts only a small number have been implemented as working prototypes , and even fewer have produced acceptable speedups .",
    "existing techniques developed for smps are inadequate for the needs of dmp platforms . in fact , most implementation methods require sharing of data and control stacks in a smp context to allow for synchronization between agents with minimal communication . even in those models , such as _ stack copying _",
    "@xcite , where the different agents maintain independent copies of the various stacks ( i.e. , they do not physically share them ) , the requirement of sharing part of the control structure is still present .",
    "for example , in the muse implementation of stack copying , parts of each choice - point are maintained in a shared data structure , to ensure that the agents reproduce the same observable behavior as in a sequential execution ( e.g. , they do not duplicate computations already performed by another agent ) . in the case of recomputation - based methods",
    "@xcite , the sharing appears in the form of the use of a centralized controller ( as in the delphi model ) to handle the communication of the different branches of the tree to the computation agents .",
    "the presence of these forms of sharing are believed to lead to degradation of performance of these schemes on a distributed memory platform , as the lack of shared memory imposes the need for explicit communication between agents .",
    "experimental @xcite and theoretical studies @xcite have demonstrated that _ stack - copying _ , and in particular _",
    "incremental _ stack - copying , is one of the most effective implementation techniques devised for exploiting or - parallelism .",
    "stack - copying allows sharing of work between parallel agents by copying the state of one agent ( which owns unexploited tasks ) to another agent ( which is currently idle ) .",
    "the idea of _ incremental _ stack - copying is to only copy the _ difference _ between the state of two agents , instead of copying the entire state each time .",
    "incremental stack - copying has been used to implement or - parallel prolog efficiently in a variety of systems ( e.g. , muse @xcite , yap @xcite , penny @xcite ) , as well as to exploit parallelism from non - monotonic reasoning systems @xcite .",
    "in order to improve the performance of stack - copying and allow its efficient implementation on dmps , we propose a new technique , called _ stack - splitting _ @xcite .",
    "stack - splitting is a variation of stack - copying , aimed at solving the environment management problem and improving stack - copying by reducing the need for communication between agents during the execution of work .",
    "this is accomplished by making use of strategies that distribute the work available in a branch of the search tree between two processors during each scheduling operation . in this paper",
    ", we describe stack - splitting in detail , and provide results from the first ever concrete implementation of stack - splitting on both shared - memory multiprocessors ( smps ) and distributed - memory multiprocessors ( dmps)specifically , a pentium - based beowulf  along with a novel scheme to combine incremental copying with stack - splitting on dmps .",
    "incremental stack - splitting _ scheme is based on a procedure which labels parallel choice - points and then compares the labels to determine the fragments of data and control areas that need to be exchanged between agents .",
    "we also describe scheduling schemes suitable for our incremental stack - splitting scheme and variations of stack - splitting providing efficient handling of order - sensitive predicates ( e.g. , side - effects ) . both the incremental stack - splitting and the scheduling schemes described",
    "have been implemented in the _ pals _ system , a message - passing or - parallel implementation of prolog . in this paper",
    "we present performance results obtained from this implementation . to our knowledge ,",
    "pals is the first ever or - parallel implementation of prolog realized on a beowulf architecture ( built from off - the - shelf components ) .",
    "the techniques have already been embraced by other developers of parallel prolog systems @xcite .",
    "the techniques we propose are also immediately applicable to other systems based on similar underlying models , e.g. , non - monotonic reasoning @xcite systems .",
    "indeed , a distributed implementation of _ answer set programming _ based on incremental stack splitting has been reported in @xcite  note that the execution model of answer set programming relies on a search - tree exploration ( built using davis - putnam s procedure ) and is _ not _ a straightforward prolog implementation .",
    "the contributions of this paper can be summarized as follows :    * design of a novel methodology  stack splitting  to efficiently support or - parallelism on distributed memory systems ; * enhancement of the methodology to support incremental copying behavior ; * investigation of different splitting modalities , in particular , to facilitate the handling of side - effects ; * implementation of these methodologies in an industrial - strength prolog system ( als prolog ) and evaluation of its performance .    in the rest of this work",
    "we will focus on the execution of prolog programs ( unless explicitly stated otherwise ) ; this means that we will assume that programs are executed according to the computation and selection rules of prolog .",
    "we will also frequently use the term _ observable semantics _ to indicate the overall observable behavior of an execution  i.e . , the order in which all visible activities of a program execution take place ( order of input / output , order in which solutions are obtained , etc . ) .",
    "if a parallel computation respects the observable prolog semantics , then this means that the user does not see any difference between such computation and a sequential prolog execution of the same program  except for improved performance .",
    "our goal in this work is to develop parallel execution models that properly reproduce prolog s observable semantics and are still able to guarantee improved performance .",
    "a rich body of research has been developed to investigate methodologies for the exploitation of or - parallelism from prolog executions on smps .",
    "comprehensive surveys describing and comparing these methodologies have appeared , e.g. , @xcite .",
    "a theoretical analysis of the properties of different methodologies has been presented in  @xcite .",
    "these works provide an abstraction of the environment representation problem as a data structure problem on dynamic trees .",
    "these studies identify the presence of unavoidable overheads in the dynamic management of environments in a parallel setting , and recognize methods with constant - time environment creation and access as optimal methods for environment representation .",
    "methods such as stack - copying @xcite , binding arrays @xcite , and recomputation @xcite meet such requirements .",
    "distributed implementations of prolog have been proposed by several researchers @xcite . however ,",
    "none of these systems are very effective in producing speedups over a wide range of benchmarks .",
    "system @xcite and castro et al s system @xcite are based directly on stack - copying and generate communication overhead due to the shared choice - points ( no real implementation exist for the two of them ) .",
    "araujo s system uses recomputation @xcite rather than stack - copying .",
    "using recomputation for maintaining multiple environments is inherently inferior to stack - copying .",
    "the stack frames that are copied in the stack - copying technique capture the effect of a computation . in the recomputation technique",
    "these stack - frames are reproduced by re - running the computation .",
    "a computation may run for hours and yet produce only a single stack frame ( e.g. , a tail - recursive computation ) . distributed implementations of prolog have been developed on transputer systems ( the opera system @xcite and the system of benjumea and troya @xcite ) .",
    "of these , benjumea s system has produced quite good results .",
    "however , both the opera system and the benjumea s system have been developed on now - obsolete transputer hardware , and , additionally , both rely on a stack - copying mechanism which will produce poor performance in programs where the task - granularity is small . a different approach has been suggested by silva and watson with their dorpp model @xcite , which extends the binding array scheme @xcite to a distributed setting , relying on the _",
    "european declarative system ( eds ) _ platform to support distributed computation ( eds provides a limited form of distributed shared memory ) ; good results have been presented running dorpp on an eds simulator .",
    "finally , the idea of stack - splitting bears some similarities with some of the loop transformation techniques which are commonly adopted for parallelization of imperative programming languages , such as loop fission , loop tiling , and index set splitting @xcite .",
    "the rest of the paper is organized as follows .",
    "section [ lab - op ] provides an overview of the main issues related to or - parallel execution of prolog .",
    "section [ extension ] describes the stack - splitting scheme , while section [ ssimpl ] describes its implementation .",
    "section [ sched ] analyzes the problem of guaranteeing efficient distribution of work between idle agents .",
    "section [ sideff ] describes how stack - splitting can be adapted to provide efficient handling of order - sensitive predicates of prolog ( e.g. , control constructs , side - effects ) .",
    "section [ results ] analyzes the result obtained from the prototype implementation in the pals system .",
    "section [ costs ] offers a general discussion about possible optimizations of the implementation of stack - splitting . finally , section [ concl ] provides conclusions and directions for future research .",
    "the reader is assumed to be familiar with the basic notions of logic programming , prolog , and its execution model ( e.g. , a basic understanding of the warren abstract machine ) @xcite .",
    "in this section , we survey the main issues related to the exploitation of or - parallelism from prolog programs , and we discuss the main ideas behind the stack - copying method .",
    "parallelization of logic programs can be seen as a direct consequence of kowalski s principle @xcite +    _ algorithm = logic + control _    program development separates the control component from the logical specification of the problem , thus making the two orthogonal .",
    "the lack ( or , at least , the limited presence ) of knowledge about control in the program allows the run - time systems to adopt different execution strategies without affecting the declarative meaning of the program ( i.e. , the set of logical consequences of the program ) . the same is true of search - based systems , where the order of exploration of the branches of the search - tree is flexible ( within the limits imposed by the semantics of the search strategy  e.g . , search heuristics ) .",
    "apart from the separation between logic and control , from a programming languages perspective , logic programming offers two key features which make exploitation of parallelism more practical than in traditional imperative languages :    1 .   from an operational perspective ,",
    "logic programming languages are _ single - assignment _ languages ; variables are mathematical entities which can be assigned a value at most once during each derivation ( i.e. , along each branch of the or - tree)this relieves a parallel system from having to keep track of complex flow dependencies such as those needed in parallelization of traditional programming languages @xcite .",
    "2 .   the operational semantics of logic programming , unlike imperative languages , makes substantial use of _ non - determinism_i.e .",
    ", the operational semantics relies on the automatic exploration of a search tree .",
    "the alternative possible choices performed during such exploration ( _ points of non - determinism _ ) can be easily converted into parallelism without radically modifying the overall operational semantics .",
    "furthermore , control in most logic programming languages is largely implicit , thus limiting programmers influence on the development of the flow of execution .",
    "the second point is of particular importance : the ability to convert existing non - determinism ( and other `` choices '' performed during execution , such as the choice of the subgoal to resolve ) into parallelism leads to the possibility of extracting parallelism directly from the execution model , without requiring the programmer to perform any modifications of the original program and without requiring the introduction of ad - hoc parallelization constructs in the source language ( _ implicit parallelization _ ) .",
    "the typical strategy adopted in the development of parallel logic programming systems has been based on the translation of one ( or more ) of the choices present in the operational semantics ( see figure  [ interpreter ] ) into parallel computations .",
    "this leads to the three `` classical '' forms of parallelism @xcite :    * _ and - parallelism _ , which originates from parallelizing the selection of the next literal to be solved  thus allowing multiple literals to be solved concurrently .",
    "this can be visualized by imagining the operation select@xmath0 to return multiple literals that are concurrently processed by the rest of the algorithm . * _ or - parallelism _ , which originates from parallelizing the selection of the clause to be used in the computation of the resolvent  thus allowing multiple clauses to be tried in parallel .",
    "this can be visualized by having the select@xmath1 operation to select multiple clauses that are concurrently processed by the rest of the algorithm * _ unification parallelism _ , which arises from the parallelization of the unification process .",
    ", in the figure , we denote the most general unifier of @xmath2 and @xmath3 . ]    ' '' ''    ' '' ''    or - parallelism originates from the parallelization of the select@xmath1 phase in figure  [ interpreter ] .",
    "thus , _ or - parallelism _ arises when more than one rule defines a relation and a subgoal unifies with more than one rule head  the corresponding bodies can then be executed in parallel with each other , giving rise to or - parallelism . or - parallelism is thus a way of efficiently searching for solutions to the query , by exploring in parallel the search space generated by the presence of multiple clauses applicable at each resolution step .",
    "observe that each parallel computation is attempting to compute a distinct solution to the original goal .",
    "for example , consider the following simple logic program :    ....        f : - t(x , three ) , p(y ) , q(y ) .",
    "p(l ) : - s(l , m ) , t(m , l ) .",
    "p(k ) : - r(k ) .",
    "q(one ) .",
    "q(two ) .",
    "r(one ) .",
    "r(three ) .      s(two , three ) .",
    "s(four , five ) .",
    "t(three , three ) .      t(three , two ) .",
    "....    and the query ? - f. the calls to p , s , and r are non - deterministic and lead to the creation of choice - points  while the calls to t , f , and q are deterministic .",
    "the multiple alternatives in these choice - points can be executed in parallel .    ' '' ''    ' '' ''    a convenient way to visualize or - parallelism is through the _ or - tree_. informally , an or - tree ( sometimes referred to also as _ search tree _ ) for a query @xmath4 and logic program @xmath5 is a tree of nodes , each with an associated _ goal - list _ , such that :    1 .",
    "the root node of the tree has @xmath4 as its associated goal - list ; 2 .",
    "each internal node @xmath6 is created as a result of successful unification of the first goal in ( the goal - list of ) @xmath6 s parent node with the head of a clause in @xmath5 , @xmath7 the goal - list of node @xmath6 is @xmath8 , if the goal - list of the parent of @xmath6 is @xmath9 and @xmath10 .",
    "figure  [ ortree ] shows the or - tree for the simple program presented above . for the sake of readability ,",
    "we have also annotated the tree with the variables created and the description of the bindings performed .",
    "we have also introduced different notations ( empty nodes and filled nodes ) to distinguish deterministic reductions versus non - deterministic reductions .",
    "the boxes represent environments created for a clause ; the dotted lines are used to associate the segment of each branch to the corresponding resolvent existing during that part of the computation ; variable bindings are indicated next to the node where the binding is computed .    note",
    "that , since we are considering execution of prolog programs , the construction of the or - tree will follow the operational semantics of prolog  at each node we will consider clauses applicable to the first subgoal , and the children of a node will be considered ordered from left to right according to the order of the corresponding clauses in the program .",
    "i.e. , during sequential execution the or - tree of figure  [ ortree ] is built and explored in a left - to - right depth - first manner .",
    "however , if multiple agents are available , then multiple branches of the tree can be constructed and explored simultaneously ",
    "although , as mentioned later , we will aim at still constructing the same tree , i.e. , reproduce the same observable semantics as sequential prolog .",
    "observe also that , if a fragment of a branch of the or - tree contains multiple choice - points , and this is explored by a single agent , then the agent will employ traditional backtracking to search the various alternatives .    or - parallelism frequently arises in applications that explore a large search space via backtracking .",
    "this is the typical case in application areas such as expert systems , scheduling and optimization problems , and natural language processing . or - parallelism also arises during parallel execution of deductive database systems @xcite .",
    "despite the theoretical simplicity and results , in practice _ implementation _ of or - parallelism is difficult because keeping the run - time and parallelism - related overheads small is non - trivial due to the practical complications which emerge from the sharing of nodes in the or - tree .",
    "that is , given two nodes in two different branches of the or - tree , all nodes above ( and including ) the least common ancestor node of these two nodes are shared between the two branches . a variable created in one of these ancestor nodes might be bound differently in the two branches .",
    "thus , the environments of the two branches have to be organized in such a fashion that , in spite of the ancestor nodes being shared , the correct bindings applicable to each of the two branches are easily discernible .",
    "let us start by introducing some terminology .",
    "whenever a new clause is applied to resolve a selected subgoal , an _ environment _ is created . the environment plays a role analogous to that of the activation record in the implementation of imperative languages  it stores information to handle the execution of the clause ( e.g. , return address ) and it provides storage for the local variables introduced by the clause .",
    "the boxes containing variables shown in figure  [ ortree ] can be thought as representing a part of the environment of the clause .    during prolog execution",
    ", variables might receive bindings . if a variable is created before a choice - point but bound after the choice - point ( e.g. , variable l in figure  [ ortree])such a variable is refereed to as a _ conditional variable _ in the literature  then the variable might be bound differently in each branch of the choice - point . in a sequential execution ,",
    "conditional variables are handled using _ trailing _",
    ": whenever the conditional variable is bound , the address of the variable is pushed on a special stack ( the _ trail stack _ ) . during backtracking",
    ", the content of the trail stack is used to determine which bindings should be removed , thus clearing up ( _ untrailing _ ) conditional variables and preparing them for the new bindings they might receive in the alternative branches explored . this mechanism allows the use of a single memory location to store the value of the variable ( since the location can be reused across different branches of the or - tree , by repeatedly clearing it via untrailing )",
    ".    more generally , consider a variable v in node n@xmath11 , whose binding b has been created in node n@xmath12 .",
    "if there are no choice - points between n@xmath11 and n@xmath12 , then the variable v will have the binding b in every branch that is created below n@xmath12 .",
    "such a binding can be stored in - place in v  i.e .",
    ", it can be directly stored in the memory location allocated to v in n@xmath11 . however ,",
    "if there are choice - points between n@xmath11 and n@xmath12 , then the binding b can not be stored in - place , since other branches created between nodes n@xmath11 and n@xmath12 may impart different bindings to v. the binding b is applicable to only those nodes that are below n@xmath12 . such a binding to a conditional variable is known as a _",
    "conditional binding_. for example , variable y in figure  [ ortree ] is a conditional variable .",
    "a binding that is not conditional , i.e. , one that has no intervening choice - points between the node where this binding was generated and the node containing the corresponding variable , is termed _",
    "unconditional_. the corresponding variable is called an _ unconditional variable _ ( for example , variable x in figure  [ ortree ] ) .    if the different branches are searched in or - parallel , then the conditional variables ( e.g. , variable l ) receive different bindings in different branches of the tree , all of which will be active at the same time .",
    "storing and later accessing these bindings efficiently is a problem . in sequential execution",
    "the binding of a variable is stored in the memory location allotted to that variable . since branches are explored one at a time , and bindings are untrailed during backtracking , no problems arise . in parallel execution ,",
    "multiple bindings exist at the same time , hence they can not be stored in a single memory location allotted to the variable .",
    "this problem , known as the _ multiple environment representation problem _ in the literature , is a major problem in implementing or - parallelism @xcite .",
    "the main problem in implementing or - parallelism is the efficient representation of the multiple environments that co - exist simultaneously in the or - tree corresponding to a program s execution  i.e .",
    ", the development of an efficient way of associating the correct set of bindings to each branch of the or - tree . note that the main problem in management of multiple environments is that of efficiently representing and accessing the conditional bindings ; the unconditional bindings can be treated as in normal sequential execution of logic programs ( i.e. , they can be stored in - place ) .",
    "the naive approach of keeping a complete separate copy of the answer substitution for each separate branch is highly inefficient , since it requires the creation of complete copies of the substitution ( which can be arbitrarily large ) every time a choice - point is created @xcite .",
    "a large number of different methodologies have been proposed to address the environment representation problem in op @xcite",
    ".    variations of the same problem arise in many classes of search problems and paradigms relying on non - determinism .",
    "for example , in the context of non - monotonic reasoning under stable models semantics @xcite , the computation needs to determine the possible belief sets of a logical theory ; these are determined by guessing the truth values of selected logical atoms , and deriving the consequences of such guesses . in this case , the dynamic environment is represented by the truth values of the various atoms along each branch of the tree .",
    "a more abstract view of the problem has been presented in @xcite , where its theoretical properties have been investigated .",
    "the theoretical results show that methodologies like stack copying and stack recomputation are theoretically superior than other schemes  i.e .",
    ", in the formal abstraction of the environment representation problem , these methods have a computational complexity that is better than that of other proposed schemes .",
    "stack - copying @xcite is a successful approach for environment representation in op .",
    "in this approach , the environment representation problem is simply resolved by allowing each agent to have its own copy of all the environments present in the branch of the or - tree currently explored  this provides each agent with its own copy of each conditional variable .    in this approach ( originally developed in bc - machine @xcite and successfully implemented in systems like muse @xcite and yap @xcite )",
    ", agents maintain a _ separate _ but _ identical _ address space  i.e .",
    ", each agent is a process with its own address space , but separate agents maintain exactly the same organization of the data structures within their address space ( i.e. , they all locate data structures at the same logical addresses ) . whenever an agent @xmath13 becomes idle ( _ idle - agent _ ) , it will start looking for unexplored alternatives generated by another agent @xmath14 ( _ active - agent _ ) .",
    "once a choice - point @xmath15 is detected in the tree @xmath16 generated by @xmath14 , @xmath13 will create a local copy of @xmath16 and restart the computation by backtracking over @xmath15 . since all or - agents maintain an identical logical address space , the creation of a local copy of @xmath16 is reduced to a simple memory copying ( figure  [ stack_copy])without the need for any explicit pointer relocation . since each or - agent owns a separate copy of the environments , the environment representation problem is readily solved  each or - agent will store the locally produced bindings in the local copy of the environments . additionally , each or - agent performs prolog execution on a private copy of its tree branch , thus relieving the need for sharing memory .",
    "for this reason , stack - copying has been considered highly suitable for execution on dmps , where stack - copying can be simply implemented using message passing between agents .    in practice",
    ", the stack - copying operation is more involved than simple memory copying , as it is desirable to maintain a single copy of each choice - point , stored in a specialized area accessible to all agents .",
    "this is important because the set of untried alternatives is now shared between the two agents .",
    "if this set is not accessed in mutual exclusion , then two agents may execute the same alternative , leading to duplication of work .",
    "in addition , the duplicate execution of the same alternative will lead to an observable behavior which is different from that of a sequential prolog execution ( e.g. , if the duplicated alternative contains a side - effect , this will be seen repeated by the user ) .",
    "thus , after copying , parts of each choice - point in @xmath16 ( specifically , the parts related to the set of available alternatives ) will be transferred to a shared area  these will be called _ shared frames_. both active and idle agents will replace their choice - points with pointers to the corresponding shared frames .",
    "shared frames are accessed in mutual exclusion .",
    "this whole operation of obtaining work from another agent is usually termed _ sharing of or - parallel work_. this is illustrated in figure [ stack_copy ] .",
    "note that cp denotes the choice - point stack and env the environment stack in the figure . for illustration purposes",
    "we assume that the choice - points and environments are allocated space in separate stacks even though this is not always the case ; choice - points and environments may be allocated space in a single common stack . in figure [ stack_copy ]",
    "the part of the tree labeled as _ shared _ has been copied from agent p1 to agent p2 ; the choice - points lying in this part of the tree have been also moved to the shared space to avoid repetition of work . in particular",
    ", agent p2 picks an untried alternative from choice - point b , created by p1 . to begin execution along this alternative , p2 first transfers the choice - points between the root node and b ( inclusive ) in a global area ( accessible by all agents ) , and then copies p1 s local stacks from root node up to node b. it untrails the appropriate variables to restore the computation state that existed when b was first created , and it begins the execution of the alternative that was picked .",
    "a major reason for the success of muse and yap is that they effectively implement incremental stack copying with _",
    "scheduling on bottom - most choice - point_. each idle agent picks work from the bottom - most choice - point of an or - branch . during the sharing operation all the choice - points between the bottom - most and the top - most choice - points are shared between the two agents .",
    "this means that , in each sharing operation , we try to maximize the amount of work shared between the two agents .",
    "the stack segments upwards of this choice - point are copied before the exploration of this alternative is begun .",
    "the copied stack segments may contain other choice - points with untried alternatives  which are locally available without any further copying operation and with very limited synchronization between processors , i.e. , they become accessible via simple backtracking ( modulo simple use of locks for mutual exclusion ) .",
    "thus , a significant amount of work becomes available to the copying agent every time a sharing operation is performed .",
    "the cost of having to copy potentially larger fragments of the tree becomes relatively insignificant considering that this technique drastically reduces the _ number of sharing operations _ performed .",
    "it is important to observe that each sharing operation requires both the agents involved to stop the regular computation and cooperate in the sharing .",
    "furthermore , to reduce the amount of information transferred during the sharing operation , copying is done _ incrementally _ ,",
    "i.e. , only the _ difference _ between @xmath17 and @xmath16 is actually copied .",
    "traditional stack - copying requires agents which share work to transfer a complete copy of the data structures representing the status of the computation . in the case of a prolog computation",
    ", this may include transferring most of the choice - points along with copies of the other data areas ( trail , heap , environments ) .",
    "since prolog computations can make use of large quantities of memory ( e.g. , generate large structures on the heap ) , this copying operation can become quite expensive .",
    "muse introduced a variation of stack - copying , adopted by many other stack - copying systems , called _",
    "incremental stack - copying _",
    "@xcite , which allows to considerably reduce the amount of data transferred during a sharing operation .",
    "the idea is to compare the content of the data areas in the two agents involved in a sharing operation , and transfer only the difference between the state of the two agents .",
    "this is illustrated in figure  [ increm ] . in figure",
    "[ increm](i ) we have two agents ( p1 and p2 ) which have 3 choice - points in common ( e.g. , from a previous sharing operation ) .",
    "p1 owns two additional choice - points with unexplored alternatives while p2 is out of work .",
    "if p2 obtains work from p1 , then there is no need of copying again the 3 top choice - points ( figure  [ increm](ii ) ) .",
    "incremental stack - copying , in a shared - memory context , is relatively simple to realize  the shared frames can be used to identify which choice - points are common and which are not @xcite .",
    "this is primarily because all the information needed for performing incremental copying efficiently can be found in the shared frames  the use of shared frames is essential to determine the bottom - most _ common _ choice - point between the two agents .",
    "the determination of such choice - point is typically accomplished by analyzing the bitmaps stored in the various shared frames , which are used to keep track of the agents which currently maintain a copy of the associated choice - point ( each bit is associated to a different agent ) .",
    "an additional component required by incremental stack - copying is the need for _ binding installation_. as illustrated in figure  [ increm ] , the part of the environment stack corresponding to the three topmost choice points is not copied . on the other hand ,",
    "variables present in such environments might have received bindings during the execution of the bottom part of the computation ; these bindings need to be explicitely installed after copying , in order to reflect the proper computation state .",
    "in this section , we discuss the issues related to porting the stack - copying model to a dmp platform , and we present the basic idea behind the novel stack - splitting scheme .      as mentioned earlier , to avoid duplication of work and to guarantee effective scheduling , during the copying operation part of the content of each copied choice - point",
    "is transferred to a shared memory area ; the various agents access each shared frame in mutual exclusion , thus synchronizing and guaranteeing unique execution of each alternative .",
    "this solution works fine on smps  where mutual exclusion is easily implemented using _",
    "locks_. however , on a dmp this process is a source of significant overhead  access to the shared area becomes a bottleneck @xcite .",
    "this is because sharing of information in a dmp leads to frequent exchange of messages and hence considerable overhead .",
    "centralized data structures , such as the shared frames , are expensive to realize in a distributed setting . on the other hand , stack copying appears to be more suitable to support op in a distributed - memory setting @xcite , since , although the choice - points are shared , at least other data - structures representing the computation  such as , in the case of prolog , the environment , the trail , and the heap  are not .",
    "other environment representation schemes , e.g. , the popular binding arrays scheme @xcite , have been specifically designed for smps and share most of the computation ; the communication overhead produced by these alternative schemes on dmps is likely to be prohibitive . to avoid the problem of sharing choice - points in distributed implementations , many implementors have reverted back to the _ scheduling on top - most choice - point _",
    "strategy @xcite .",
    "the reason is that untried alternatives of a choice - point created higher up in the or - tree are more likely to generate large subtrees , and sharing work from the highest choice - point leads to smaller - sized stacks being copied .",
    "however , if the granularity does not turn out to be large , then another untried alternative has to be picked and a new copying operation has to be performed .",
    "in contrast , in scheduling on bottom - most , more work could be found via backtracking , since more choice - points are copied during the same sharing operation .",
    "additionally , scheduling on bottom - most is closer to the depth - first search strategy used by sequential systems , and facilitates support of prolog semantics ( e.g. , support of order sensitive predicates ) . indeed , comparative studies about scheduling strategies indicate that scheduling on bottom - most is superior to scheduling on top - most @xcite .",
    "this is especially true for the stack - copying technique because :    1 .",
    "the number of copying operations is minimized ; and , 2 .",
    "the alternatives in the choice - points copied are `` cheap '' sources of additional work , available via backtracking .",
    "however , the fact that these choice - points are shared is a major drawback for a distributed implementation of copying .",
    "the question we consider is : can we avoid sharing of choice - points while keeping scheduling on bottom - most ?",
    "the answer is affirmative , as is discussed next .",
    "in stack - copying , the primary reason why a choice - point has to be shared is because we want to serialize the selection of untried alternatives , so that no two agents can pick the same alternative .",
    "the shared frame is locked while the alternative is selected to achieve this effect . however , there are other simple ways of ensuring the same property : _ the untried alternatives of a choice - point can be split between the two copies of the choice - point stack_. we call this operation _",
    "choice - point stack - splitting _ or simply _ stack - splitting_. this will ensure that no two agents pick the same alternative .",
    "we can envision different schemes for splitting the set of alternatives between shared choice - points ",
    ", each choice - point receives half of the alternatives , or the partitioning can be guided by information regarding the unexplored computation , such as granularity and likelihood of failure . in addition , the need for a shared frame , as a critical section to protect the alternatives from multiple executions , has disappeared , as each stack copy has a choice - point with a different set of unexplored alternatives .",
    "all the choice - points can be evenly split in this way during the copying operation .",
    "the choice - point stack - splitting operation is illustrated in figure  [ stacksplit ] .",
    "the strategy adopted in this example is what we call _ horizontal splitting _ :",
    "the remaining alternatives in each of the shared choice - points are split between the two agents .    a variation of choice - point stack - splitting relies on splitting the content of the choice - point stack , instead of splitting the individual choice - points .",
    "this means that , during a sharing operation , the list of available choice - points is partitioned between the two agents .",
    "we will refer to this approach as _",
    "vertical splitting_. in this case , we can assume the availability of a partition function : @xmath18 where @xmath19 is the set of all possible choice - points and @xmath20 denotes a list of choice - points .",
    "the intuition is that , given the sequence @xmath21 of choice - points in the branch to be shared , @xmath22 will return a partition of @xmath21 in two subsets @xmath23 , where @xmath24 are the choice - points kept by the active agent and @xmath25 are the choice - points given to the idle agent .    in the rest of this work we will consider two main strategies for partitioning the choice - points :    * @xmath26 i.e. , the choice - points in the even positions",
    "are kept while those in the odd positions are given away ( see figure [ vert ] ) . *",
    "@xmath27 i.e. , the list of choice - points is cut in two segments , the first given to the idle agent , while the second is kept by the active agent ( see figure [ vert1 ] ) .",
    "observe that , in practice , all choice - points are copied  as it would be too expensive to selectively copy only the required ones  and the ones that are not needed are `` cleared '' of their alternatives ; this is explained in detail in the next section .",
    "the idea of splitting the list of choice - points is particularly useful when the search tree is _",
    "binary_which is a frequent situation in several prolog applications as well as in other search problems ( e.g. , non - monotonic reasoning where the choice - points represent choices of truth values ) . in these cases",
    "the use of horizontal splitting is rather ineffective .",
    "splitting of alternatives can be resorted to when very few choice - points with many alternatives are present in the stack .",
    "different mixes of splitting of the list of choice - points and choice - point splitting can be tried to achieve a good load balance  as discussed in @xcite . eventually , the user could also be given control regarding how the splitting is done ",
    "e.g . , by allowing the user to declare one of a set of splitting strategies for given predicates ",
    "although our system does not currently support this option .",
    "the major advantage of stack - splitting is that scheduling on bottom - most can still be used without incurring huge communication overheads .",
    "essentially , after splitting the different or - parallel agents become independent of each other , and hence communication is minimized during execution .",
    "this makes the stack - splitting technique highly suitable for dmps .",
    "the possibility of parameterizing the splitting of the alternatives based on additional semantic information ( granularity , non - failure , user annotations ) can further reduce the likelihood of additional communications due to scheduling .",
    "in the rest of the paper we describe the incremental stack - splitting scheme and its implementation issues on a message passing platform , analyzing in detail how the various problems mentioned earlier have been tackled .",
    "in addition to the basic stack - splitting scheme , we also    * analyze how stack - splitting can be extended to incorporate _",
    "incremental copying _ , an optimization which has been deemed essential to achieve speedups in various classes of applications , and * analyze how to handle order - sensitive predicates ( e.g. , side - effects ) in the presence of stack - splitting .",
    "the solution we describe has been developed in a concrete implementation , realized by modifying the engine of a commercial prolog system ( als prolog ) and making use of the message passing interface ( mpi ) as a communication platform .",
    "the als prolog system is based on an implementation of the warren abstract machine ( wam ) .",
    "the data structures employed by our distributed engine include all the data areas of a standard warren abstract machine ( e.g. , stack for the choice - points , stack for the environments , a heap for the dynamic creation of terms , a trail to support undoing of variable bindings during backtracking ) .",
    "we assume that the code - area is initially duplicated between all processors .    during stack - splitting , all wam areas , except for the code area ,",
    "are copied from the agent giving work to the idle one .",
    "next , the parallel choice - points are split between the two agents . blindly copying all the stacks every time an agent shares work with another idle agent can be wasteful , since frequently the two agents already have parts of the stacks in common due to previous copying .",
    "we can take advantage of this fact to reduce the amount of copying by performing _ incremental copying _",
    ", as discussed earlier . in our stack - splitting scheme , there are no shared frames , hence performing incremental stack - copying will incur more overhead due to the communication overhead involved . in order to figure out the incremental part that only needs to be copied during incremental stack - splitting , parallel choice - points will be _ labeled _ in a certain way .",
    "the goal of labeling is to uniquely identify the original `` source '' of each choice - point ( i.e. , which agent created it ) , to allow unambiguous detection of copies of common choice - points .",
    "thus , the labels effectively replace the bitmaps used in the shared memory implementations of stack - copying .",
    "the labeling procedure is described next .    to perform labeling ,",
    "each agent maintains a counter .",
    "initially , the counter in each agent is set to _ 1_. the counter is incremented each time the labeling procedure is performed . when a parallel choice - point is copied for the first time ,",
    "a label for it is created .",
    "the label is composed of three parts :    1 .",
    "agent rank , 2 .",
    "counter , and 3 .   choice - point address .",
    "the agent rank is the rank ( i.e. , i d ) of the agent which created the choice - point .",
    "the counter is the current value of the labeling counter for the agent generating the labels .",
    "the choice - point address is the address of the choice - point which is being labeled .",
    "the labels for the parallel choice - points are recorded in a separate _ label stack _ , in the order they are created  the choice - point address in the label maintains the connection between the label ( stored in the label stack ) and the corresponding choice - point ( stored in the choice - point stack ) .",
    "also , when a parallel choice - point is removed from the stack , its corresponding label is also removed from the label stack .",
    "initially , the label stack in each agent is set to _ empty_. the label stack keeps a record of the labels for the agent s shared choice - points .",
    "observe that the choice of maintaining labels in a stack  instead of associating them directly to the corresponding choice - points  has been dictated by efficiency reasons .",
    "let us illustrate stack - splitting accompanied by labeling with an example . in the rest of the discussion",
    "we assume the use of vertical splitting strategy .",
    "suppose agent a has just created two parallel choice - points and agent b is idle .",
    "agents a and b have their counters set to _ 1 _ and their label stacks set to _",
    "then agent b requests work from agent a. agent a first creates labels for its two parallel choice - points .",
    "these labels have their rank and counter parts as _",
    "a:1_. agent a then pushes these labels into its label stack .",
    "this is illustrated in figure  [ al ] ; for simplicity , in our figures , we do not show the label stack explicitly but show each label rank and counter parts inside the parallel choice - point being labeled .",
    "notice that agent a incremented its counter to 2 after the labeling procedure was over . in the figure",
    ", @xmath28 denotes the root of the tree .",
    "the next step requires the actual execution of stack - copying .",
    "agent b receives a message that contains all the parallel choice - points of agent a , along with agent a s label stack . at this point , it becomes possible to perform stack - splitting .",
    "agent a will keep the alternative _",
    "b2 _ but not _",
    "a2 _ and _ a3 _ , and agent b will get the alternatives _ a2 , a3 _ but not _ b2_. we have designed a new wam scheduling instruction ( _ schedule _ ) which is placed in the next alternative field of the choice - point above which there is no more parallel work .",
    "the execution of this instruction forces the agent to enter scheduling , and it implements the scheduling scheme described in section [ sched ] . agent a keeps the alternative _",
    "b2 _ of choice - point _",
    "b _ , changes the next alternative field of choice - point _ a _ to wam instruction _",
    "trust_fail _ to avoid taking the original alternative of this choice - point , and changes the next alternative field of the choice - point above _ a _ to the new wam instruction _ schedule _ which will take agent a into scheduling .",
    "trust_fail _ instruction will simply act as a filler to denote that the choice - point does not have any further alternatives .",
    "observe that in practice it is possible to optimize this scheme ( e.g. , in the example , we could have introduced the _ schedule _ instruction directly in the choice - point _ a _ ) .    in turn",
    ", agent b changes the next alternative field of choice - point _",
    "b _ to wam instruction _",
    "trust_fail _ , to avoid taking the original alternative of this choice - point , keeps the alternatives _ a2 , a3 _ of choice - point _ a _ , and changes the next alternative field of the choice - point above _ a _ to the _ schedule _ instruction",
    ". see figure [ agb ] .",
    "afterwards , agent b backtracks , removes choice - point _",
    "b _ along with its corresponding label in the label stack , and then takes alternative _ a2 _ of choice - point _",
    "suppose now that agent b creates two parallel choice - points and agent c is idle .",
    "agent c , with its counter set to _ 1 _ and its label stack set to _ empty _ , requests work from b. agent b first creates labels for its two new parallel choice - points .",
    "these labels have their rank and counter parts as _",
    "b:1_. agent b then pushes these labels into its label stack",
    ". see figure [ bl ] .",
    "notice that agent b incremented its counter to _",
    "2_.    at this point in time , stack - copying takes place .",
    "agent c gets all the parallel choice - points of agent b along with agent b label stack . the stack - copying phase is followed by the actual stack - splitting operation : agent b will keep alternatives _",
    "d2 _ and _ a3 _ but not _",
    "c2 _ , and agent c will keep alternative _ c2 _ but not _",
    "d2 _ nor _ a3_. notice that all three parallel choice - points of agent b have been split among b and c. agent b keeps the alternative _",
    "d2 _ of choice - point _",
    "d _ and changes the next alternative field of choice - point _",
    "c _ to wam instruction _",
    "trust_fail _ to avoid taking the original alternative of this choice - point , and keeps the alternative _",
    "_ of choice - point _",
    "a_. agent c changes the next alternative field of choice - point _",
    "d _ to wam instruction _",
    "trust_fail _ to avoid taking the original alternative of this choice - point , keeps the alternative _",
    "c2 _ of choice - point _",
    ", changes the next alternative field of choice - point _ a _ to wam instruction _",
    "trust_fail _ , and changes the next alternative field of the choice - point above _ a _ to _",
    "this is illustrated in figure  [ bgc ] .",
    "agent c backtracks , removes choice - point _",
    "d _ along with its corresponding label in the label stack , and then takes alternative _ c2 _ of choice - point _",
    "c_.      in this section we describe how the label stacks are used to compute the incremental part to be copied .",
    "let us assume that agent a is giving work to agent b. agent a will label all its parallel choice - points which have not been labeled before and will push them into its label stack .",
    "agent a then increments its counter .",
    "if the label stack of agent b is empty , then stack - copying will need to be performed followed by stack - splitting .",
    "agent a sends its complete choice - point stack and its complete label stack to agent b. then stack - splitting is performed on all the parallel choice - points of agent a. agent b then tries its new work via backtracking .",
    "however , if the label stack of agent b is not empty , then agent b will send its label stack to agent a. the objective is for agent a to locate the topmost label in common between a and b  and this is realized by comparing the content of the two stacks until a match is found .",
    "let us denote with _ ch _ the most recent choice - point with a common label between a and b. in this way , agents a and b are guaranteed to have the same computation _ above _ the choice - point _",
    "ch _ , while their computations will be different below such choice - point .",
    "if the choice - point _",
    "ch _ does not exist , then ( non - incremental ) stack - copying will need to be performed followed by stack - splitting just as described before . however , if choice - point _",
    "ch _ does exist , then agent b backtracks to choice - point _ ch _ , and performs incremental - copying .",
    "agent a sends its choice - point stack starting from choice - point _",
    "ch _ to the top of its choice - point stack .",
    "agent a also sends its label stack starting from the label corresponding to choice - point _",
    "ch _ to the top of its label stack .",
    "stack - splitting is then performed on all the parallel choice - points of agent a. afterwards , agent b tries its new work via backtracking .",
    "we illustrate the above procedure by the following example .",
    "suppose agent a has three parallel choice - points and agent c requests work from a. agent a first labels its last two parallel choice - points which have not been labeled before and then increments its counter .",
    "afterwards , agent c sends its label stack to agent a. agent a compares its label stack against the label stack of agent c and finds the last choice - point _ ch _ with a common label . above choice - point _ ch _",
    ", the prolog trees of agents a and c are equal .",
    "below choice - point _",
    "ch _ , the prolog trees of agents a and c differ .",
    "see figure [ al2 ] .    now , agent c backtracks to choice - point _",
    "ch_. incremental stack - copying can then take place .",
    "agent a sends its choice - point stack starting from choice - point _",
    "ch _ to the top of its choice - point stack .",
    "agent a also sends its label stack starting from the label corresponding to choice - point _",
    "ch _ to the top of its label stack",
    ". then , stack - splitting takes place on the three parallel choice - points of agent a. see figure [ agc ] .",
    "agent c backtracks to choice - point _",
    "i _ and takes alternative _",
    "i2_.      four issues that were not discussed above and which are fundamental for the correct implementation of the incremental stack - splitting scheme presented are discussed below .",
    "the first issue is related to the management of _ sequential choice - points_. typically , only a subset of the choice - points present during the execution are suitable to provide work that can be effectively parallelized .",
    "these choice - points are traditionally called _ parallel choice - points _ , to distinguish them from _ sequential choice - points _ , whose alternatives are meant to be explored by a single agent .",
    "systems like pals , muse , and aurora allow the user to explicitly declare predicates as parallel ( while , by default , the others are treated as sequential ) .",
    "the problem arises when sequential choice - points are located among the parallel choice - points that will be split between two agents .",
    "if the alternatives of these choice - points are kept in both agents , we may have repeated , useless or wrong computations .",
    "hence , the alternatives of these choice - points should only be kept in one agent ",
    "e.g . , the agent that is giving work . in our current approach , we keep the alternatives of sequential choice - points in the agent giving work ; as a consequence , the agent that is receiving work should change the next alternative field of all these choice - points to the wam instruction _",
    "trust_fail _ to avoid taking the original alternatives of these choice - points .",
    "the second issue has to do with the bindings of conditional variables ( i.e. , variables that may be bound differently in different or - parallel branches ) which need to be copied too as part of the incremental stack - splitting process .",
    "for example , suppose that in our last example , before agent a gives work to agent c , agent a created the variable _ x _ before choice - point _",
    "ch _ was created , and the variable _ x _ was instantiated after the creation of _",
    "is shown in figure [ aic ] .",
    "we can see that the binding for _ x _ was not copied during incremental stack - splitting .",
    "this is because _ x _ is a conditional variable which was created before choice - point _",
    "the incremental part of the heap or environment stack that was copied did not contain its binding .",
    "this means that the receiving agent does not see _ x _ becoming automatically instantiated thanks to the copying of the heap or environment stack .",
    "in order to solve the problem , we need to ensure that , during the sharing operation , also the bindings of the conditional variables created in the common part of the branch are transferred from the agent giving work to the idle agent . in the current implementation",
    ", we have tackled this problem by modifying the trail structure of the als wam engine .",
    "the _ trail _ is a stack , maintained by the wam , which records which conditional variables have been bound along the current branch of execution .",
    "the trail is used by the wam to support removal of bindings during backtracking . in our system",
    ", the trail has been modified to a _ value trail _ @xcite , thus maintaining with each bound conditional variable also a reference to its value .",
    "the value trail is employed by agent giving work to build a special message containing the values of the bound conditional variables , sent to the idle agent during the sharing operation .",
    "the idle agent will make use of this message and install the appropriate bindings for the conditional variables existing in the common segment of the search tree branch .",
    "observe that a similar problem appears also in shared - memory implementations of stack - copying @xcite  though they do not need to rely on value - trails , since each agent can directly retrieve the values of the bindings from the other agent s environments ( which are in shared memory ) .",
    "the third issue arises when garbage collection takes place . in the current implementation of the als system ( the underlying wam we modified for this project )",
    ", garbage collection occurs also on the choice - point stack , leading to possible shifting of choice - points .",
    "when this situation occurs , the labels in our label stack may no longer label the correct parallel choice - points  since labels are connected to choice - points by storing the address of the corresponding choice - points inside the label .",
    "therefore , we need to modify our labeling procedure so that when garbage collection on an agent takes place , the label stack of this agent is invalidated .",
    "this has been realized by just setting its label stack to empty .",
    "the next time this agent gives work , full stack - copying will have to take place .",
    "this solution is analogous to the one adopted in the muse system @xcite to address the similar problem in stack - copying .",
    "alternative solutions  e.g .",
    ", use of indirect labels  would introduce costs in each step of sharing , instead of an occasional additional cost during garbage collection , and have not been used in our system .",
    "the fourth issue arises when the next clause fields of the parallel choice - points between the first parallel choice - point _",
    "first cp _ and the last choice - point _ ch _ with a common label in the agent giving work are not the same compared to the ones in the agent receiving work .",
    "this situation occurs after several copying and splitting operations  that caused the next clause field of some choice - points to be changed to _",
    "trust_fail _ , while other agents still have active alternatives in such choice - points . in this case",
    ", it is not correct to just copy the part of the choice - point stack between choice - point _ ch _ and the top of the stack and then perform the splitting .",
    "this is because the splitting will not be performed correctly .",
    "for example , suppose that in our previous example ( see fig .",
    "[ aic ] ) , when agent c requests work from agent a , we have this situation , as illustrated in figure [ anc ] .",
    "let us assume that the scheduler decides to transfer the choice - point _",
    "g _ to agent c. but agent c does not have the right next clause field for this choice - point .",
    "hence , we need to modify our procedure once again . this can be done by having the agent giving work send all the next clause fields between its first parallel choice - point _ first cp _ and choice - point _",
    "ch _ to the agent receiving work .",
    "then the splitting of all parallel choice - points can take place correctly .",
    "see figure [ agnc ] .",
    "scheduling is an important aspect of any parallel system .",
    "the scheduling strategy adopted largely determines the level of speedup obtained for a particular parallel execution . the main objective of a scheduling strategy is to balance the amount of parallel work done by different agents",
    "additionally , work distribution among agents should be done with a minimum of communication overhead .",
    "these two goals are somewhat at odds with each other , since achieving perfect balance may result in a very complex scheduling strategy with considerable communication overhead , while a simple scheduling strategy which re - distributes work less often may incur a lower communication overhead but lead to a poorer balancing of work .",
    "therefore , it is obvious that there is an intrinsic contradiction between distributing parallel work as evenly as possible and minimizing the distribution overhead . thus our main goal is to find a trade - off point that results in a reasonable scheduling strategy .",
    "we adopt a simple and fair distributed algorithm to implement a scheduling strategy in the pals system . a new data structure  the _ load vector_is introduced to provide an _ approximated _ view of the work - loads of different agents .",
    "the work - load of an agent is approximated by the number of parallel choice - points with unexplored alternatives present in its local computation tree .",
    "this is analogous to the approach originally used by muse , and it can be efficiently implemented within als ; furthermore , the majority of examples we encountered offer parallel choice - points with a small number of alternatives ( often just two ) , thus making our approximated notion of work - load essentially equivalent to more refined versions .",
    "each agent keeps a work - load vector _",
    "v _ in its local memory , and the value of _",
    "v[i ] _ represents the estimated work - load of the agent with rank _",
    "i_. based on the work - load vector , an idle agent can request parallel work from other agent with the greatest work - load , so that parallel work can be fairly distributed .",
    "the load vector is updated at runtime .",
    "when stack - splitting is performed , a send_loadinfo message with updated load information will be broadcasted to all the agents so that each agent has the latest information of work - load distribution .",
    "additionally , load information is attached with each incoming message .",
    "for example : when a request_work message is received from agent @xmath29 , the value of @xmath29 s work - load , 0 , can be inferred .",
    "based on its work - load each agent can be in one of two states : _ scheduling _ state or _ running _ state . when an agent has some work to do , it is in a running state , otherwise , it is in a scheduling state .",
    "an agent that is running , occasionally checks whether there are incoming messages .",
    "two possible types of messages are checked by the running agent : one is request_work message sent by an idle agent , and the other is send_loadinfo message , which is sent when stack - splitting occurs . the idle agent in scheduling state",
    "is also called a scheduling agent .",
    "an idle agent wants to get work as soon as possible from another agent , preferably the one that has the largest amount of work .",
    "the scheduling agent searches through its local load vector for the agent with the greatest work - load , and then sends a request_work message to that agent asking for work .",
    "if all the other agents are idle ( in scheduling state ) , then the execution of the current query is finished and the agent halts .",
    "when a running agent receives a request_work message , stack - splitting will be performed if the running agent s work - load is greater than a predefined threshold ( the _ splitting threshold _ ) , otherwise , a reply_without_work message with a positive work - load value will be sent as a reply . if a scheduling agent receives a request_work message , a reply_without_work message with work - load 0 will be sent as a reply .",
    "the distributed scheduling algorithm mainly consists of two parts : one is for the scheduling agent , and the other is for the running agent .",
    "the running agent s algorithm can be briefly described as follows :    .... 1 :         while ( any incoming message ) { 2 :            get an incoming message ;     3 :            switch ( message type ) { 4 :            case send_loadinfo : 5 :                 update the corresponding agents ' work - load ; 6 :                 break ; 7 :            case request_work : 8 :                 if ( local work - load > splitting threshold ) { 9 :                    reply a message of type reply_with_work and perform                                                            stack - splitting ; 10 :                    broadcast the updated work - load to all the agents ; 11 :                 } 12 :                 else { 13 :                    reply a message of type reply_without_work 14 :                          and the value of its own work - load ; 15 :                       set work - load of the message source to 0 ; 16 :                    } 17 :                 break ; 18 :            } 19 :         } ....    at fixed time intervals ( which can be selected at initialization of the system ) the agent examines the content of its message queue for eventual pending messages .",
    "send_loadinfo messages are quickly processed ( lines 4 - 6 ) to update the local view of the overall load in the system .",
    "messages of the type request_work are handled as described above ( lines 7 - 17 ) .",
    "if stack - splitting is realized ( line 9 ) , then the agent will also notify the whole system of the new work - loads ( line 10 ) .",
    "we should remark that the implementation concretely checks for the presence of the two types of messages with different frequency  i.e .",
    ", request for work messages are considered less frequently than requests for load update .",
    "all messages are handled asynchronously ; send_loadinfo messages are given higher priority by the receiving agents ( i.e. , they are processed before any other types of messages ) , to ensure that the work - load vector remains as much up - to - date as possible .",
    "the reason of keeping work - load vector up - to - date as much as possible for each agent is that when a scheduling agent is looking for work , it is able to obtain work from the agent with the highest work - load immediately . we have observed worse performance by giving higher priority to other types of messages .",
    "this is because if work - loads are not up - to - date , an agent thought to have the highest work - load may turn out to have work - load lower than others , reducing the granularity of work obtained and increasing the number of splitting operations performed .",
    "the scheduling agent s algorithm can be briefly described as follows :    .... 1 :         while ( 1 ) { 2 :            d = the rank of the agent with the greatest work - load ; 3 :            if ( d 's work - load = = 0 ) and termination detection returns true                                       then halt ;    / * the whole work is done * / 4 :            send a request_work message to d ; 5 :            matched = false ; 6 :            while ( ! matched ) { 7 :               get an incoming message ; 8 :               switch ( message type ) { 9 :                  case reply_with_work : 10 :                     stack - splitting with the agent which sent the message ; 11 :                     update the corresponding work - load ; 12 :                     simulate failure and go to execute the split work ; 13 :                     return ; 14 :                  case reply_without_work : 15 :                     if ( source of message is d ) matched = true ; 16 :                     v[message sender i d ] = work - load of agent which sent                                                                    the message ; 17 :                     break ; 18 :                  case request_work : 19 :                     reply a message of type reply_without_work and 20 :                          its work - load 0 to the source of incoming message ; 21 :                     v[message sender i d ] = 0 ; 22 :                     break ; 23 :                  case send_loadinfo : 24 :                     update the corresponding agents ' work - load ; 25 :                     break ; 26 :               } 27 :           } ....    observe :    * a request_work message is sent to the agent with the greatest work - load according to the local load vector ( lines 2 and 4 ) ; an optimization to avoid some communication overhead is that if the greatest work - load is below the splitting threshold value , the request_work message can be delayed until there exists some agent that has work - load higher than the threshold ; in other words , if all the other agents have low work - load , no stack - splitting takes place in our strategy ; * the loop 6 - 27 is repeated until a reply is received from the agent contacted in line 4 ; * if a reply is positive , then the scheduling phase is left and execution restarted ; if the reply is negative , then another iteration of the outermost loop is performed ; * during scheduling , requests for work from other agents are denied ( and this is used to update to zero the work - load of the requesting agent ) , as shown in lines 18 - 22 ; * messages containing new work - load information are used to update the work - load vector ( lines 23 - 25 ) ; * if the work - load vector contains only zeros ( line 3 ) , then the scheduler initiates a procedure to verify global termination .",
    "the global termination process is based on a fairly standard black - white token ring scheme  @xcite .",
    "let us point out that the scheduling procedure bears some similarities with the argonne scheduler used by aurora @xcite .",
    "in our experiments on both shared - memory as well as distributed - memory platforms we did not perceive the problems noticed in other similar schedulers ( e.g. , see @xcite ) with this approach ( e.g. , the `` honey - pot '' problem , where every worker tries to grab the same piece of work ) .",
    "in this section , we discuss how the stack - splitting scheme can be adapted to support the correct semantics during parallel execution of programs containing side - effects and other order - sensitive predicates .      a parallel prolog system that maintains prolog semantics reproduces the behavior of a sequential system ( same solutions , in the same order , and with the same termination properties ) .",
    "sequential prolog systems include features that allow the programmer to introduce a component of sequentiality in the execution",
    ". these may be in the form of facilities to express side - effects ( e.g. , i / o ) or constructs to control the order of construction of the computation ( e.g. , pruning operations , user - defined search strategies ) . in a parallel system , such _ order sensitive components ( @xmath30)_i.e .",
    ", built - in predicates whose semantics is tied to the sequential operational semantics of prolog  need to be performed in the _ same order as in a sequential execution _ ; if this requirement is not met , the parallel computation may lead to an observable semantics different from the one indicated by the programmer @xcite .    in the context of prolog",
    ", there are three different classes of @xmath30 : _ side - effects _ predicates ( e.g. , i / o ) , _ meta - logical _ predicates ( e.g. , test the instantiation state of variables ) , and _ control _ predicates ( e.g. , for pruning branches of the search tree ) . in the context of or - parallelism only certain classes of @xmath31require sequentialization across parallel computations  only side - effects and control predicates .",
    "the presence of @xmath31does not require a sequentialization of the whole execution involved , only the @xmath31themselves need to be sequentialized .",
    "if the @xmath31are infrequent and spaced apart , good speedups can be obtained , even in a dmp .",
    "the correct order of execution of @xmath31corresponds to an in - order traversal of the computation tree .",
    "a specific @xmath31@xmath28 can be executed only if all the @xmath31that precede @xmath28 in the traversal have been completed ( this assumes also that we do not have infinite branches in the computation tree ) .",
    "detecting when all the @xmath31 to the left have been executed is an undecidable problem , thus requiring the use of approximations .",
    "the most commonly used approximation is to execute an @xmath31only when the branch containing it becomes the left - most branch in the tree @xcite .",
    "thus , we approximate the termination of the preceding @xmath31by verifying the termination of the _ branches _ that contain them .",
    "most of the schemes proposed @xcite rely on traversals of the tree , where the computation attempting an @xmath31walks up its branch verifying the termination of all the branches to its left .",
    "these approaches can be realized @xcite in presence of a shared representation of the computation tree  required to check the status of other executions without communication .",
    "these solutions do not scale to the case of dmp , where a shared representation of the computation tree is not available .",
    "simulation of a shared representation is infeasible , as it leads to unacceptable bottlenecks @xcite .",
    "some attempts to generalize mechanisms to handle @xmath31to dmps have been made @xcite , but only at the cost of sub - optimal scheduling mechanisms .",
    "it is unavoidable to introduce a communication component to handle @xmath31 in a distributed setting .",
    "we demonstrate that stack - splitting can be modified to solve this problem with minimal communication @xcite .",
    "the modification is inspired by the optimal algorithms for @xmath31studied in @xcite .",
    "in particular , in the context of this work we focus on side - effect predicates ; we believe these results can provide the foundations to handle also cut and pruning operators , but their effective management requires more significant changes , e.g. , to the scheduling strategies , and they are not addressed in the scope of this work .      the problem of efficiently handling @xmath31during parallel executions has been pragmatically tackled in a variety of proposals @xcite .",
    "nevertheless , only recently the problem has been formally studied , deriving solid theoretical foundations regarding the inherent complexity of testing for leftmostness in a dynamically changing tree @xcite .",
    "let @xmath32 be the computational tree ( where @xmath33 are its nodes and @xmath34 the current edges ) .",
    "the computation tree is dynamic ; the modifications to the tree can be described by two operations : expand which adds a ( bounded ) number of children to a leaf , and delete which removes a leaf from the tree .",
    "whenever a branch encounters a side - effect , it must check if it can execute it .",
    "this check boils down to verifying that the branch containing the side - effect is currently the leftmost active computation in the tree .",
    "if @xmath6 is the current leaf of the branch where the side - effect is encountered , its computation is allowed to continue only if @xmath35root , where @xmath36 indicates the highest node @xmath37 in the tree ( i.e. , closest to the root ) such that @xmath6 is in the leftmost branch of the subtree rooted at @xmath37 .",
    "@xmath36 is also known in the parallel logic programming community as the _ subroot node _ of @xmath6 @xcite .",
    "thus , checking if a side - effect can be executed requires the ability of performing the operation find_subroot@xmath38 which , given a leaf @xmath6 , computes the node @xmath36 .    the work presented in @xcite studies the data structure problem leading to the following result : any sequence of expand , delete , and find_subroot operations can be performed in @xmath39 time per operation on pure pointer machines  i.e . , without the need of complex arithmetic ( i.e.",
    ", the solution does not rely on the use of `` large '' labels ) .",
    "the data structure used to support this optimal solution is based on maintaining a dynamic list  i.e .",
    ", a list which allows arbitrary insertions and deletions to be performed at run - time  which represents the frontier of the tree ( the solid arrows in figure  [ optim - fig ] )",
    ". the dynamic list can be updated in @xmath39 time each time leaves are added or removed ( i.e. , when expanding a branch and performing backtracking ) .",
    "subroot nodes can be efficiently maintained for each leaf ( these are depicted by dotted lines in the figure)in particular , each delete operation affects the subroot node of at most one other leaf .",
    "identification of the computations an @xmath31@xmath28 depends on can be simply accomplished by traversing the list of leaves right - to - left from @xmath28 .",
    "executability ( i.e. , leftmostness ) can be verified in constant time by simply checking whether the subroot of the leaf points to the root of the tree @xcite .",
    "although the use of an explicit list to maintain the frontier of the computation tree has been suggested in other works ( e.g. , in the dharma scheduler @xcite ) , the data structure which allows its management in constant - time was proposed for the first time in @xcite .",
    "the reader is referred to @xcite for more details .",
    "this solution is feasible in a shared memory context but requires adjustment in a distributed - memory context . in the rest of this section",
    "we show how stack - splitting can incorporate a good solution to the problem , following the spirit of this optimal scheme .",
    "determining the executability of an @xmath31@xmath28 in a distributed - memory setting requires two coordinated activities : _ ( a ) _ determining _ what are _ the computations to the left of @xmath28 in the computation tree ",
    "i.e . , which agents have acquired work in branches to the left of @xmath28 ; _ ( b ) _ determining what is the _ status _ of the computations to the left of @xmath28 . on dmps ,",
    "both steps require exchange of messages between agents .",
    "the main difficulty is represented by step _",
    "( a)_without the help of a shared data structure , discovering the position of the different agents requires arbitrary localization messages exchanged between the agent in charge of @xmath28 and all the other agents .",
    "what we propose is a shift in perspective , directed from the ideas presented in section [ optimal ] : through a simple modification in the strategy for stack - splitting , we can guarantee that agents are aware of the position of their subroot nodes .",
    "thus , instead of having to locate the subroot nodes whenever an @xmath31occurs , these are implicitly located ( without added communication ) whenever a sharing operation is performed ( a very infrequent operation , compared to the frequency of @xmath30 steps ) .",
    "knowledge of the position of the subroot nodes allows agents to maintain an approximation of the ordering of the leaves of the tree , which in turn can be used to support the execution of step _",
    "( b ) _ above .    in the original stack - splitting procedure  using vertical splitting ( section [ split])during a sharing operation the parallel choice - points are alternatively split between two agents .",
    "the agent that is giving the work keeps the bottom - most choice - point , the third bottom - most choice - point , the fifth bottom - most choice - point , etc . the agent that receives the work keeps the second bottom - most choice - point , the fourth bottom - most choice - point , etc . in our previous works",
    "@xcite we have demonstrated that this splitting strategy is effective and leads to good speedups for large classes of representative benchmarks .",
    "the alternation in the distribution of choice - points is aimed at reducing the danger of focusing a particular agent on a set of fine - grained computations .",
    "this strategy for splitting a computation branch between two agents has a significant drawback w.r.t .",
    "execution of @xmath30 , since the two agents , through backtracking , may arbitrarily move left or right of each other .",
    "this makes it impossible to know a - priori whether one agent affects the position of the subroot node of other agents , preventing the detection of the position of agents in the frontier of the tree . from section",
    "[ optimal ] we learn that an agent operating on a leaf of the computation tree can affect other agents subroot nodes only in a limited fashion .",
    "the idea can be easily generalized : if an agent limits its activities to the bottom part of a branch , then the number of leaves affected by the agent is limited and well - defined .",
    "this observation leads to a modified splitting strategy , where the agent giving work keeps the lower segment of its branch as private , while the agent receiving work obtains the upper segment of the branch .",
    "this modification guarantees that the agent receiving work will be always to the right of the agent giving the work .",
    "since the result of a sharing operation is always broadcasted to all the agents  to allow agents to maintain an approximate view of the distribution of work  this method also allows each agent to have an approximate view of the composition of the frontier of the computation tree .",
    "observe that this modification to the splitting strategy leads to a scheduling strategy different from the traditional bottom - most scheduling mentioned earlier .",
    "nevertheless , as discussed in the experimental evaluation section , this modification does not harm parallel performance in applications with presence of @xmath30 , and it does not relevantly degrade performance in absence of @xmath30 .",
    "the next sections show how this new splitting strategy can be made effective to support @xmath31without losing parallel performance .",
    "[ [ data - structures ] ] data structures : + + + + + + + + + + + + + + + +    in order to support the new splitting strategy and use it to support @xmath30 steps , each agent will require only two additional data structures : _ ( 1 ) _ the _ linear vector _ and _ ( 2 ) _ the _ waiting queue_. each agent keeps and updates a _",
    "linear vector _ which consists of an array of agent ids that represents the linear ordering of the agents in the search tree  i.e . ,",
    "the respective position of the agents within the frontier of the computation tree ( section [ optimal ] ) .",
    "the idea behind this _ linear vector _ is that whenever an agent wants to execute an @xmath30 , it first waits until there are no agents ids to its left on the _ linear vector_. such a status indicates that all the agents that were operating to the left have completed their tasks and moved to the right side of the computation tree , and the subroot node has been pushed all the way to the root of the tree .",
    "once this happens , the agent can safely execute the @xmath30 , being left - most in the search tree .",
    "initially , the linear vector of all agents contains only the i d of the first running agent . in the original bottom - most scheduler developed for stack - splitting ( section [ ssimpl ] ) , every time a sharing operation is performed , a send_loadinfo message is broadcast to all agents ; this is used to inform all agents of the change in the workload and of the agents involved in the sharing . for every send_loadinfo message , each agent updates its linear vector by moving the i d of the agent that received work immediately to the right of the i d of the agent giving work .",
    "each agent also maintains a _ waiting queue _ of ids , representing all the agents that are waiting to execute an @xmath31but",
    "are located to the right of this agent . whenever an agent enters the _ scheduling _",
    "state to ask for work , it informs all agents in its waiting queue that they no longer need to wait on it to execute their @xmath30 .",
    "[ [ the - procedure ] ] the procedure : + + + + + + + + + + + + + +    in stack - splitting ( section [ ssimpl ] ) , an agent can only be in one of two states : _ running state _ or _ scheduling state_. in order to handle @xmath30 , we need another state : the _ order - sensitive _ state .",
    "all agents wanting to execute an @xmath31will enter this state until it is safe for them to execute their @xmath30 .",
    "the transition between the states requires the introduction of three types of messages : ( 1 ) request_osc , ( 2 ) osc_acknowledgment , and ( 3 ) reply_in_osc .",
    "their detailed explanations are shown in the following scheduling algorithms .",
    "we update the distributed scheduling algorithms as follows to support handling @xmath30 .",
    "only those parts related to handling osc are presented in the algorithms .",
    "the ignored parts ( denoted by ... ... ) can be found from the previous algorithms presented in section  [ sched ] .",
    "the scheduling algorithm for an agent in an order - sensitive state is described as follows :    ....      send a request_osc message to all the agents whose ids        are on the left of its own i d in the linear vector ;      while ( its own i d is not on the leftmost in the linear vector ) {         get an incoming message ;          switch ( message type ) {            case request_osc :                update the requesting agent 's work - load ;                consult the linear vector ;                if ( the requesting agent i d is on the right of its own i d )                    enqueue the requesting agent i d in the waiting queue ;                else                    reply a message of type osc_acknowledgment ;                break ;            case osc_acknowledgment :                update the sending agent 's work - load ;                remove the message sender i d from the linear vector ;                break ;            case send_loadinfo :                update the splitting agents ' work - load ;                update the linear vector by placing the i d of the agent                   who receives work to the right of the agent i d giving work ;                if ( the agent i d who receives work is on the left of its own i d )                   send a request_osc message to the agent ;                break ;            case request_work :                remove the requester i d from the linear vector ;                reply a message of type reply_in_osc ;                v[the requester i d ] = 0 ;                break ;           }        }        change to the running state to perform the osc ;     send a send_loadinfo message to all other agents ; ....    once an agent arrives to the order - sensitive state , it first sends a request_osc to all the agents to its left in its linear vector .",
    "it then waits for osc_acknowledgment messages from each of them .",
    "an osc_acknowledgment is sent by an agent when it is no longer to the left of the agent wanting to execute the @xmath30 .",
    "when this message is received , the i d of the agent sending it will be removed from the linear vector .",
    "the position of the sending agent will be re - acquired when such agent acquires more work in the successive scheduling phase .",
    "notice that when the agent is waiting for these messages , it may receive send_loadinfo messages .",
    "if this happens , the agent has to update its linear vector .",
    "in particular , if due to this sharing operation an agent moves to its left , a request_osc message needs to be sent to this agent as well .",
    "once the agent receives osc_acknowledgment messages from all these agents , it can safely perform the @xmath30 . and ,",
    "finally , after the osc is successfully performed , a send_loadinfo message will be broadcasted to all other agents with the precise work - load information .",
    "in addition , an agent in an order - sensitive state is not allowed to share work ; requests to share work are denied with the reply_in_osc message .",
    "its linear vector can be easily updated by removing the i d of the agent requesting work .",
    "just as we attach load information to messages in the traditional stack - splitting scheduling algorithm , we also attach updated load information to these three new messages .",
    "the updated scheduling algorithm for a running agent is described as follows :    ....          while ( any incoming message ) {             get an incoming message ;                switch ( message type ) {             case send_loadinfo :                  update the linear vector by placing the i d of the agent                     who receives work to the right of the agent i d giving work ;                  ... ...             case request_work :                  if ( local work - load > splitting threshold ) {                     update the linear vector by placing the requesting                       agent i d to the right of its own i d ;                     ... ...       % stack - splitting                   }                   else { % no stack - splitting                     remove the requester i d from the linear vector ;                     ... ...                   }                   break ;              case request_osc :                   consult the linear vector ;                   if ( the requesting agent i d is on the right of its own i d )                      enqueue the requesting agent i d in the waiting queue ;                   else                      reply a message of type osc_acknowledgment ;                   break ;              }           } ....    when an agent is in running state and receives a request_osc message , it consults its linear vector and reacts in the following way .",
    "if the i d of the agent wanting to execute an @xmath31is to its right in the linear vector , the i d of the requesting agent is inserted in the waiting queue .",
    "when the running agent runs out of work and moves to the scheduling state , an osc_acknowledgment message will be sent back to the agent wanting to execute the @xmath30 .",
    "if the i d of the agent wanting to execute the @xmath31is to its left , an osc_acknowledgment message is immediately sent back to the agent wanting to execute the @xmath30 .",
    "this means that the running agent is no longer to the left of the agent wanting to execute the @xmath30 .",
    "the updated scheduling agent s algorithm can be briefly described as follows :    ....          dequeue all the agent ids from the waiting queue and            send an osc_acknowledgment to all of them ;          while ( 1 ) {             ... ...             while ( ! matched ) {                get an incoming message ;                switch ( message type ) {                   case reply_with_work :                        update the linear vector by placing the own i d                          to the right of the message sender i d ;                        ... ...                   case reply_without_work :                        if ( the work - load of the message sender is 0 )                           remove the message sender i d from the linear vector ;                        ... ...                     case request_work :                        remove the requester i d from the linear vector ;                        ... ...                   case send_loadinfo :                        update the linear vector by placing the i d of                          the agent who receives work to the right of the                          agent i d giving work ;                        ... ...                   case reply_in_osc :                        update the work - load of the message sender to 1 ;                        break ;                          case request_osc :                        update the work - load of the message sender ;                        reply a message of type osc_acknowledgment ;                        break ;                 }             } ....    when an agent enters the scheduling state , it dequeues all the ids from its waiting queue and sends an osc_acknowledgment to all these agents , informing them that it is no longer to their left .",
    "when a scheduling agent receives a reply_in_osc , which means the current agent with the highest work - load is in an order - sensitive state , it then updates the work - load of that agent to 1 so that in the next round the agent will choose another agent with high work - load to request work from .",
    "the precise work - load will be updated later after the agent in the order - sensitive state becomes a running - state agent .",
    "[ [ partitioning - ratios ] ] partitioning ratios : + + + + + + + + + + + + + + + + + + + +    the stack - splitting modification divides the stack of parallel choice - points into two contiguous partitions , where the bottom partition is kept by the agent giving work and the upper partition is given away .",
    "this stack - splitting modification guarantees that the agent that receives work will be to the immediate right of the other agent .",
    "the question is what is the partitioning ratio that will produce the best results ?",
    "we first tried using a partition where the agent that is giving work keeps the bottom half of the branch and only gives away the top half . after experimenting with lots of different partition ratios",
    ", we found out that with a partition ratio of @xmath40 where the agent that is giving work keeps the bottom @xmath41 of the parallel choice - points and gives away the top @xmath42 of the parallel choice - points , our benchmarks without side - effects obtain excellent speedups  similar to our original alternating splitting @xcite .",
    "when we run our benchmarks with side - effects , the partition ratio of @xmath40 performed superior to the partition ratio of @xmath43 .",
    "one of the reasons is that it is common to have more side - effects towards the bottom part of the computation tree ; thus , using the proposed partition we assign smaller chunks of work , but with a greater probability of not encountering side - effects .",
    "additionally , keeping larger numbers of side - effects locally reduces the number of interactions .    [",
    "[ messages - out - of - order ] ] messages out of order : + + + + + + + + + + + + + + + + + + + + + +    send_loadinfo messages may arrive out of order and then the linear vectors may be outdated .",
    "e.g. , agent 2 receives from agent 0 a request_work message but decides not to share work .",
    "since agent 0 is requesting work , agent 2 removes 0 from its linear vector .",
    "later on , agent 0 gets work from agent 1 , and agent 1 broadcasts a send_loadinfo message . afterwards , agent 0 gives work to agent 3 and also broadcasts a send_loadinfo message .",
    "now , suppose that agent 2 receives the second send_loadinfo message first and the first send_loadinfo next .",
    "when agent 2 tries to insert 3 to the immediate right of 0 in the linear vector , 0 is not located and therefore 3 can not be inserted ( see figure  [ outof ] ) .",
    "mpi ( used in our system for agent communication ) does not guarantee that two messages sent from different agents at different times will arrive in the order that they were sent .",
    "the scenario presented above can be avoided if , in every sharing operation , both involved agents broadcast a send_loadinfo message to all the other agents . in this case every agent will be informed that a sharing operation occurred either by the giver or by the receiver of work .",
    "agent 2 in the above scenario will first know that agent 0 obtained work from agent 1 , and then will know that agent 0 gave work to agent 3 .",
    "duplication of send_loadinfo messages is handled through the use of two dimensional arrays @xmath44 and @xmath45 of size @xmath46 , where @xmath33 is the total number of agents ; @xmath47[j]$ ] ( @xmath48[j]$ ] ) is incremented when a sharing message from @xmath49 to @xmath50 is received from agent @xmath49 ( @xmath50 ) .",
    "thus , @xmath47[j]$ ] and @xmath48[j]$ ] keep track of how many times @xmath49 and @xmath50 have shared work ; @xmath44 records how many times @xmath49 notified of a sharing with @xmath50 and @xmath45 records how many times @xmath50 notified of a sharing with @xmath49 . the linear vector will be updated only if @xmath47[j ] > send2[i][j]$ ] ( @xmath48[j ] > send1[i][j]$ ] ) and the message comes from agent @xmath49 ( @xmath50 ) .",
    "the modified stack - splitting strategy described in this section provides a strategy to bias the exploration of the branches in the search tree in the left - to - right order .",
    "as far as exploring the search space with a bias towards exploring the branches to the left is concerned , it will depend on the choice - point splitting strategy used .",
    "consider the choice - point with alternatives a1 through a5 shown in figure  [ distr](i ) .",
    "two possible splittings are shown in figures  [ distr](ii ) and [ distr](iii ) . in the first one ( figure  [ distr](ii ) ) , the list of alternatives is split in the middle : agent p1 will be working on the left half of the tree rooted at this choice - point a , agent p2 on the right half .",
    "in contrast , in figure  [ distr](iii ) , the untried alternatives are distributed alternately between the two choice - points .",
    "this splitting strategy is more likely to produce a search that is biased to the left .",
    "this suggests that modifications similar to the one presented in this work can be extended to the case of horizontal splitting .",
    "in this section , we present experimental results and their evaluations obtained from two implementations of the proposed methodologies  one developed on a shared - memory platform and one on a beowulf platform .",
    "all the timings proposed have been obtained as an average over 10 consecutive runs ( excluding the lowest and highest times ) , executed on lightly loaded machines .",
    "the stack - splitting procedure has been implemented on top of the commercial als prolog system using the mpi library for message passing  specifically , the mpi-1 library natively provided by solaris 5.9 ( hpc 4.0 ) .",
    "the whole system runs on a sun enterprise 4500 with fourteen processors ( sparc 400mhz with 4 gb of memory ) .",
    "while the sun enterprise is a smp , it should be noted that all communication  during scheduling , copying , splitting , etc. is done using messages .",
    "this has enabled an easy migration of the system to a beowulf machine .",
    "the timing results in seconds from our incremental stack - splitting system on the 14 processor sun enterprise are presented in table  [ incre ] .",
    "this system is based on the scheduling strategy described in section [ sched ] .",
    "the benchmarks that we have used to test our system are the following .",
    "the _ 9 costas _ and _ 8 costas _",
    "benchmarks compute the costas sequences of length 9 and 8 respectively .",
    "the _ knight _ benchmark consists of finding a path of knight - moves on a chess - board of size 5 , starting at ( 1,1 ) and finishing at ( 1,5 ) , and visiting every square on the board just once .",
    "the _ stable _ benchmark is a simple engine to compute the models of a logic program with negation .",
    "the _ send more _",
    "benchmark consists of solving the classical crypto - arithmetic puzzle . the _ 8 puzzle _ benchmark is a solution to the puzzle involving a 3-by-3 board with 8 numbered tiles .",
    "the _ bart _",
    "benchmark is a simulator used to test the safety of the controller for a train .",
    "the _ solitaire _",
    "benchmark is a solution to the standard game involving a triangular board with pegs and one empty hole .",
    "the _ 10 queens _ and _ 8 queens _",
    "benchmarks consist of placing a number of queens on a chessboard so that no two queens attack each other .",
    "the _ hamilton _",
    "benchmark consists of finding a closed path through a graph such that all the nodes of the graph are visited once . the _ map coloring _ benchmark consists of coloring a planar map .",
    "the _ 9 costas_,_8 costas _ , _ knight _ , _ stable _ , _ 10 queens _ , _ 8 queens _ , _ hamilton _ , and _ map coloring _ benchmarks compute all the possible solutions .",
    "the _ send more _ ,",
    "_ 8 puzzle _ , _ bart _ , and _ solitaire _ benchmarks stop at the first solution ( observe that _",
    "bart _ actually has a unique solution ) .",
    "the _ 9 costas _ , _ 8 costas _ , and _ bart _",
    "benchmarks are fairly large programs , while the rest are simpler .",
    "however , all benchmarks provide sufficiently different program structures to extensively test the behavior of the parallel engine .",
    ".incremental stack - splitting on shared memory ( time in seconds and speedups ) [ cols=\"^,^,^,^,^,^ \" , ]     the results obtained are consistent with our belief that dmp implementations should be used for programs with coarse - grained parallelism and a modest number of @xmath30 .",
    "coarse - grained computations are even more important if we want to handle large numbers of side - effects where it is necessary that the @xmath31be spaced far apart . for programs with small - running times",
    "there is not enough work to offset the cost of exploiting parallelism and even less for handling @xmath30 . nevertheless , our system is reasonably efficient given that it produces good speedups for large and medium size benchmarks with even a considerable number of @xmath30 , and produces no slow downs except for benchmarks with huge numbers of side - effects and small running times . even in presence of @xmath30 ,",
    "the parallel overhead observed is substantially low  on average @xmath51 and seldomly over @xmath52 ( it is slightly higher than what described in the previous sections , due to some additional tests required for checking presence of messages related to @xmath30 ) .",
    "figure  [ comps ] compares with the speedups for some benchmarks obtained using a variant of the muse system @xcite on smp ( i.e. , a highly optimized stack - copying system on shared - memory platform ) .",
    "the results highlight the fact that , for benchmarks with significant running time , our methodology is capable of approximating the best behavior on smps .",
    "in this section we discuss some limitations of the current stack - splitting scheme and some possible optimizations .",
    "the adoption of stack - splitting releases the system from the need of keeping shared frames to support sharing of work .",
    "the shared frame used in the stack - copying technique on shared - memory platforms is also where global information related to scheduling is kept .",
    "the shared frames provide a globally accessible description of the or - tree , and each shared frame keeps information regarding which agent is working in which part of the tree .",
    "this last piece of information is needed to support the kind of scheduling typically used in stack - copying systems  work is taken from the agent that is `` closer '' in the computation tree , thus reducing the amount of information to be copied  since the difference between the stacks is minimized .",
    "the shared nature of the frames ensures accessibility of this information to all agents , providing a consistent picture of the computation .",
    "however , under stack - splitting the shared frames no longer exist ; scheduling and work - load information has to be maintained in some other way . while we have already described how to maintain work - load information in a distributed setting , through the use of work - load vectors , we did not discuss how to provide agents with knowledge of their relative positions in the computation tree .",
    "this type of information could be kept in a global shared area similar to the case of smps ",
    "e.g . , by building a centralized representation of the or - tree  or distributed over multiple agents and accessed by message passing in case of dmps .",
    "the maintenance of global scheduling information represents a problem which is orthogonal to the environment representation .",
    "this means that scheduling management in a dmp will anyway require communication between agents .",
    "shared frames are also employed in muse @xcite to detect the prolog order of choice - points , needed to execute order - sensitive predicates ( e.g. , side - effects , extra - logical predicates ) in the correct order . as in the case of scheduling , some information regarding global ordering of choice - points needs to be maintained to execute order - sensitive predicates in the correct order  see section [ sideff ] .",
    "thus , stack - splitting does not completely remove the need of a shared description of the or - tree .",
    "the use of stack - splitting can mitigate the impact of accessing shared resources ",
    ", stack - splitting allows scheduling on bottom - most which , in general , leads to a reduction of the number of calls to the scheduler .",
    "the stack - copying operation in stack - splitting is slightly more involved than in stack - copying on shared - memory platforms . in muse ,",
    "the original choice - point stack is traversed and the choice - points transferred to the shared area .",
    "this operation involves only those choice - points that have never been shared before  shared choice - points already reside in the global shared area .",
    "for this reason the actual _ sharing _ of the choice - points is performed by the _ active - agent _",
    "( i.e. , the agent that is providing work to the idle agent)which is forced to interrupt its regular computation to assist the sharing process .",
    "the actual copying of the stack takes place only after the choice - points have been copied to the shared memory area .    in the stack - splitting technique ,",
    "once the copying is completed , the actual sharing ( i.e. , transferring of choice - points to a shared area ) is replaced by a phase of splitting , performed by both agents , where they traverse the copied choice - points , completing the splitting of the untried alternatives . in the case of smp implementations",
    ", this operation is expected to be considerably cheaper than transferring the choice - points to the shared area  and",
    "indeed our experimental studies have highlighted this by denoting improved performance of stack - copying on smps .",
    "the actual splitting can be represented by a simple pair of indices that refer to the list of alternatives  which , in a smp system like muse , is static and shared by all the agents . in the case of dmp implementations ,",
    "the situation is similar : since each agent maintains a local copy of the code , the splitting can be performed by communicating to the copying agent which alternatives it can execute for each choice - point ( e.g. , a pair of pointers to the list of alternatives ) .",
    "it is simple to encode such information within the choice - point itself during copying .    in both cases",
    "we expect the sharing operation to have comparable complexity ; a slight delay may occur in stack - splitting , due to the traversal of the choice - point stack performed by each agent . on the other hand , in",
    "stack - splitting the two traversals  one in the _ idle - agent _ and one in the _ active - agent_can be overlapped .",
    "however , if the stack being copied , s@xmath53 , is itself a copy of some other stack , then unlike regular stack - copying ( where once a choice - point is shared  i.e .",
    ", moved to a shared area",
    " it will not have to be shared again ) , we may still need to traverse both the source and target stacks and split the choice - points ( even those that have been acquired through previous sharing operations ) .",
    "the presence of this additional step depends on the policy adopted for the partitioning of the alternatives between agents .",
    "it is , for example , required if we adopt a policy which assigns half of the alternatives to each of the agents . in such cases , the cost of sharing will be slightly more than the cost of regular stack - copying .",
    "once an agent selects new work , it will look for work again only after it finishes the exploration of all alternatives acquired via stack - splitting .",
    "as we mentioned earlier , different splitting modalities can be envisioned , e.g. , horizontal vs. vertical splitting .",
    "horizontal splitting , which is useful for programs having choice - points with many alternatives , incurs a linear cost due to the need of traversing a linear list of alternatives ( provided by the wam representation of procedures ) to perform the partition .",
    "the cost incurred in splitting the untried alternatives between the copied stack and the stack from which the copy is made , can be eliminated by amortizing it over the operation of picking untried alternatives .",
    "let us assume that the untried alternatives are evenly split using horizontal splitting ( as in figure [ stacksplit ] ) .    in the modified approach ,",
    "no traversal and modification of the choice - points is done during copying .",
    "the untried alternatives are organized as a binary tree ( see figure [ bintree ] ) .",
    "the binary alternatives can be efficiently maintained in an array , using standard techniques found in any data - structure textbook .",
    "in addition , each choice - point maintains the `` copying distance '' from the very first original choice - point as a bit string .",
    "this number is initially 0 when the computation begins .",
    "when stack - splitting takes place and a choice - point whose bit string is @xmath6 is copied from , then the new choice - point s bit string is @xmath54 ( @xmath55 appended to the bit string @xmath6 ) , while the old choice - point s bit string is changed to @xmath56 ( @xmath57 tagged to bit string @xmath6 ) .",
    "when an agent backtracks to a choice - point , it will use its bit string to navigate in the tree of untried alternatives , and find the alternatives that it is responsible for .",
    "for example , if the bit - string of an agent is 10 , then all the alternatives in the left subtree of the right subtree of the or - tree are to be executed by that agent .",
    "this scheme ( originally proposed in  @xcite ) has been introduced as part of the yapdss implementation @xcite .",
    "however , it is not very clear which of the two strategies  incurring cost of splitting at copying time _ vs _ amortizing the cost over the selection of untried alternatives  would be more efficient . in case of amortization , the cost of picking an alternative from a choice - point is now slightly higher , as the binary tree of choice - points needs to be traversed to find the right alternative .",
    "stack - splitting essentially performs semi - dynamic work distribution , as the untried alternatives are split at the time of picking work .",
    "if the choice - points that are split are balanced , then we can expect good performance .",
    "thus , we should expect to see good performance when the choice - points generated by the computation that are parallelized contain a large number of alternatives .",
    "this is the case for applications which fetch data from databases and for most generate & test type of applications .    for choice - points with a small number of alternatives ,",
    "stack - splitting is more susceptible to problems created by the semi - dynamic work distribution strategy that implicitly results from it : for example , in cases where op is extracted from choice - points with only two alternatives .",
    "such choice - points arise quite frequently , from the use of predicates like member and select :    member(x,[x@xmath58 _ ] ) .",
    "member(x,[_@xmath58y ] ) : - member(x , y ) .",
    "select(x,[x@xmath58y],y ) .",
    "select(x,[y@xmath58z],[y@xmath58r ] ) : - select(x , z , r ) .",
    "both these predicates generate choice - points with only two alternatives  thus , at the time of sharing , a single alternative is available in each choice - point .",
    "the different alternatives are spread across different choice - points .",
    "stack - splitting would assign all the alternatives to the copying agent , thus leaving the original agent without local work .",
    "however , the problems raised by such situations can be solved using a number of techniques :    * use knowledge about the inputs and partial evaluation , or automatic optimizations ( e.g. , _ last alternative optimization ( lao ) _",
    "@xcite ) to collapse the different choice - points into a single one .",
    "* use more complex splitting strategies , e.g. , if a choice - point has odd number of untried alternatives remaining ( @xmath59 ) , then one agent will be assigned @xmath6 alternatives and the other @xmath60 .",
    "the agent which gets @xmath6 and the agent which gets @xmath60 can be alternated for the different choice - points encountered in the stack , thus ensuring that no processor is left completely without work . *",
    "perform a _",
    "vertical _ splitting of the choice - points ;    additionally , observe that the splitting strategy adopted ( e.g. , horizontal splitting , vertical splitting ) can be changed depending on the specific structure of the computation .",
    "for example , along these lines rocha et al .",
    "@xcite have recently proposed a splitting strategy_diagonal splitting_that combines vertical and horizontal splitting and performs well for certain classes of benchmarks .",
    "or - parallelism is typically only one of the forms of parallelism that one can exploit from logic programming and other search - based systems .",
    "the development of a single branch of the search tree typically requires a large number of operations , that could themselves be executed in parallel , leading to what is typically indicated as _ and - parallelism_.    we can apply the basic ideas behind stack - splitting to the exploitation of and - parallelism .",
    "when an agent steals work , it gets a large grain of work .",
    "coupled with well - known run - time optimizations , such as the various parallel versions of the last call optimization ( e.g. , the _ last parallel call optimization _ used in logic programming @xcite ) which increases the size of the parallel conjunctions , larger grain - sized work can be obtained .",
    "also , if additional agents are not available , optimizations @xcite can keep the and - parallel computation as close to sequential execution as possible  just as in or - parallelism , where choice - point can be explored via backtracking .",
    "the stack - splitting technique is expected to be reasonably effective for distributed implementations of and - parallelism .",
    "copying of stacks may seem to be an inordinate amount of effort for and - parallel implementations , but given that implementations such as muse are quite efficient , a stack - copying based and - parallel system should also be efficient .",
    "in addition , the overhead in setting up the bindings while _ joining _ the multiple and - parallel agents , so that the continuation goal of the parallel conjunction can be correctly executed , may be quite high ; however , we hope that the use of and - trails will keep it to a minimum  where the and - trail is a data structure used to collect the bindings performed to variables that need to be exported .",
    "furthermore , the issue of joining will arise in any distributed implementation of and - parallelism , where bindings of commonly accessible variables have to be exchanged between agents to allow for a coherent execution .",
    "the use of and - trail is similar , but more efficient , than existing techniques such as _ environment closing _ @xcite .",
    "note that just as optimizations such as the last alternative optimization increase the applicability and the benefits of stack - splitting in the case of or - parallelism , so will optimizations such as the last parallel call optimization lead to increased applicability of and benefit from stack - splitting in the case of and - parallelism .",
    "in this paper , we presented a technique called stack - splitting for implementing op and discussed its advantages and disadvantages .",
    "we showed how stack - splitting can be extended to incremental stack - splitting which incrementally copies the difference of two stacks .",
    "implementations on both a shared memory multiprocessor and a distributed - memory multiprocessor were realized and reported .",
    "our dmp implementation is the first ever implementation of a prolog system on a beowulf architecture .",
    "stack - splitting is an extension of stack - copying .",
    "its main advantage , compared to other techniques for implementing op , is that it allows large grain - sized work to be picked up by idle agents and executed efficiently without incurring excessive communication overhead .",
    "the technique bears some similarity to the delphi model @xcite used in parallel execution of prolog ( the delphi model was not the inspiration for our stack - splitting technique ) , where computation leading to a goal with multiple alternatives is replicated in multiple agents , and each agent chooses a different alternative when that goal is reached . instead of recomputing we use stack - copying , which , we believe , is more efficient  and the existing literature has indicated this is the case for shared - memory implementations of prolog @xcite . in a separate work @xcite",
    ", we also showed how stack - splitting can be used for implementing non - monotonic reasoning systems under stable models semantics  by exploiting or - parallelism from a careful implementation of the davis - putnam procedure and using stack - splitting to transfer atom - split operations between processors .",
    "also in this case , copying with stack - splitting provides a superior performance than recomputation .",
    "the current implementation of stack - splitting in the pals system is stable , and work is in progress to evaluate its performance on larger applications . a number of issues are still open , and they will be addressed as future work .",
    "first of all , it is clear from our experience that the giving the ability to the programmer to supply information about the program can greatly affect parallel performance ; we are currently working in developing tools to analyze parallel executions of pals ( e.g. , through visualization of the parallel computation ) and support user - annotations to guide exploitation of parallelism .",
    "work is also in progress in supporting order - sensitive control predicates ( e.g. , pruning predicates ) in pals , and developing adaptive scheduling heuristics , which take advantage of knowledge of the structure of the computation to improve distribution of work ,",
    "thanks to c. geyer , l. castro , v. santos costa , f. silva , m. carro , and m. hermenegildo for discussions on implementation of distributed lp systems .",
    "pontelli and villaverde have been been supported by nsf grants cns-0220590 , cns-0454066 , and hrd-0420407 , guo by nsf nebraska epscor grant , and gupta by nsf grant cns-0130847 and grants from the us environmental protection agency .",
    "m. balduccini , e. pontelli , and f. bermudez .",
    "non - monotonic reasoning on beowulf platforms . in _ proceedings of the symposium on practicals aspects of declarative languages _ , springer verlag , pp . 3757 , 2003 .",
    "v.  benjumea and j.m .",
    "in m.  bruynooghe and j.  penjam , editors , _ international symposium on programming languages implementations and logic programming _ , pages 291301 , heidelberg , 1993 .",
    "springer verlag .",
    "r. butler , t. disz , e. lusk , r. olson , r. overbeek , and r. stevens . cheduling or - parallelism : an argonne perspective . in _ proceedings of the international conference and symposium on logic programming _ , mit press , pp . 15651577 , 1988 .",
    "castro , v.  santos costa , c.f.r .",
    "geyer , f.  silva , p.k .",
    "vargas , and m.e .",
    "correia . .",
    "in d.  pritchard and j.  reeve , editors , _ proceedings of europar _ , pages 899908 , heidelberg , 1999 .",
    "springer verlag .",
    "j.s . conery .",
    "inding environments for parallel logic programs in nonshared memory multiprocessors . in _ international symposium on logic programming _ , pages 457467 .",
    "san francisco , ieee computer society , august 1987 .",
    "r.  finkel , v.  marek , n.  moore , and m.  truszczyski . .",
    "in a.  provetti and s.c .",
    "tran , editors , _ proceedings of the aaai spring symposium on answer set programming _ , pages 7275 , cambridge , ma , 2001 .",
    "aaai / mit press .",
    "s.  ganguly , a.  silberschatz , and s.  tsur .",
    "ramework for the parallel processing of datalog queries . in h.",
    "garcia - molina and h.  jagadish , editors , _ proceedings of acm sigmod conference on management of data _ , new york , 1990 .",
    "acm press .          g.  gupta and e.  pontelli .",
    "last alternative optimization for or - parallel logic programming systems . in _",
    "eight international symposium on parallel and distributed processing_. ieee computer society , 1996 .",
    "g.  gupta and e.  pontelli .",
    "optimization schemas for parallel implementation of nondeterministic languages and systems . in _ international parallel processing symposium _",
    ", los alamitos , ca , 1997 .",
    "ieee computer society .",
    "b.  hausman , a.  ciepielewski , and a.  calderwood .",
    "ut and side - effects in or - parallel prolog . in icot staff , editor ,",
    "_ international conference on fifth generation computer systems _ , pages 831840 , tokyo , japan , november 1988 .",
    "springer verlag .",
    "e.  lusk , r.  butler , t.  disz , r.  olson , r.  stevens , d.  h.  d. warren , a.  calderwood , p.  szeredi , p.  brand , m.  carlsson , a.  ciepielewski , b.  hausman , and s.  haridi .",
    "he aurora or - parallel prolog system . , 7(2/3):243271 , 1990 .",
    "l.  perron . .",
    "in j.  jaffar , editor , _ proceedings of the international conference on principles and practice of constraint programming _ , volume 1713 of _ lncs _ , pages 346360 , heidelberg , 1999 .",
    "springer verlag .                  c.  schulte . . in n.  beldiceanu et al . , editor , _ proceedings of techniques for implementing constraint programming systems , post - conference workshop of cp 2000 _ , number tra9/00 , pages 4157 , university of singapore , 2000",
    "p.  szeredi .",
    "using dynamic predicates in an or - parallel prolog system . in v.  saraswat and k.  ueda , editors , _ proceedings of the international logic programming symposium _ , pages 355371 , cambridge , ma , october 1991 .",
    "mit press .",
    "o.  wolfson and a.  silberschatz . istributed processing of logic programs",
    ". in h.  boral and p.  larson , editors , _ proceedings of the sigmod international conference on management of data _ , pages 329336 , new york , 1988 .",
    "acm , acm press ."
  ],
  "abstract_text": [
    "<S> this paper describes the development of the _ pals _ system , an implementation of prolog capable of efficiently exploiting or - parallelism on _ distributed - memory _ platforms  specifically beowulf clusters . </S>",
    "<S> pals makes use of a novel technique , called _ incremental stack - splitting_. the technique proposed builds on the stack - splitting approach , previously described by the authors and experimentally validated on shared - memory systems , which in turn is an evolution of the stack - copying method used in a variety of parallel logic and constraint systems  </S>",
    "<S> e.g . , </S>",
    "<S> muse , yap , and penny . </S>",
    "<S> the pals system is the first distributed or - parallel implementation of prolog based on the stack - splitting method ever realized . </S>",
    "<S> the results presented confirm the superiority of this method as a simple yet effective technique to transition from shared - memory to distributed - memory systems . </S>",
    "<S> pals extends stack - splitting by combining it with incremental copying ; the paper provides a description of the implementation of pals , including details of how distributed scheduling is handled . </S>",
    "<S> we also investigate methodologies to effectively support order - sensitive predicates ( e.g. , side - effects ) in the context of the stack - splitting scheme . </S>",
    "<S> experimental results obtained from running pals on both shared memory and beowulf systems are presented and analyzed .    or - parallelism , beowulf clusters , order - sensitive predicates . </S>"
  ]
}