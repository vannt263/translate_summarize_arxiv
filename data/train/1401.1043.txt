{
  "article_text": [
    "frequent pattern mining is an important problem in the area of data mining that has diverse applications in a variety of domains  @xcite .",
    "even though many algorithms have been proposed for frequent pattern mining , most of these methods produce a large number of frequent patterns . in addition , the patterns found are often redundant in the sense that many patterns are very similar . the redundancy and the large volume of the patterns discovered makes it difficult to use the mined patterns to gain useful insights into the data or to use them to extract rules which are effective for prediction , classification etc .",
    ", in the application domain .",
    "thus , finding a small set of non - redundant , relevant and informative patterns that succinctly characterize the data , is an important problem of current interest .",
    "there are many methods that are proposed for reducing the number of extracted frequent patterns .",
    "many such methods concentrate on eliminating patterns that are deemed to be non - informative given the other frequent patterns .",
    "for example , in the context of transaction datasets , concepts such as closed  @xcite , non - derivable  @xcite and maximal  @xcite itemsets were suggested to reduce the number of frequent itemsets extracted .",
    "similarly , closed sequential patterns were proposed for sequence datasets  @xcite .",
    "even though such methods result in some reduction in the number of patterns returned by the algorithm , the number of patterns still remains substantial .",
    "also , the redundancy in the final set of patterns is , often , still large .",
    "recently , there have been other efforts for finding a small set of informative patterns that best describes the data . for example , @xcite proposes a method for summarization of transaction datasets based on some ideas from information theory .",
    "they propose a method of selecting a subset of frequent itemsets to achieve a good lossy summarization of the database . here",
    "each transaction is summarized by one itemset with as little loss of information as possible . in  @xcite , which also proposes a lossy summarization , each transaction is covered , partially , by the largest frequent itemset .",
    "in contrast to these methods ,  @xcite propose lossless summarization of transaction datasets using the minimum description length  ( mdl ) principle . a related approach called _",
    "tiling _ was used by  @xcite , again for a lossless summarization of the data .    in this paper , we address the problem of discovering a set of patterns that can achieve succinct lossless representation of temporal sequence data .",
    "we present algorithms that discover a small set of relevant patterns ( which are special forms of serial episodes ) which summarize the data well .",
    "we use the mdl principle @xcite to define what we mean by summarizing the data well .",
    "the basic idea is that a set of patterns characterizes or summarizes the data sequence well , if the set of patterns can be used as a model to encode the data to achieve good compression .",
    "as mentioned above , the mdl principle has been used earlier to obtain relevant and non - redundant subsets of frequent patterns .",
    "the idea was first explored by the krimp algorithm @xcite in the context of transaction data .",
    "this algorithm selects a subset of frequent itemsets which , when used for encoding the database , achieves good compression .",
    "each selected itemset is assigned a code with shorter code lengths assigned to higher frequency itemsets .",
    "the algorithm tries to encode each transaction with the codes of itemsets which have minimal code lengths and which cover maximum number of items .",
    "similar strategies have been proposed for sequence data also  @xcite . for sequential data , unlike in the case of transaction data , the temporal ordering is important and this presents additional complications while encoding the data .",
    "for example , consider a single transaction , @xmath0 from a transaction database and two itemsets @xmath1 and @xmath2 .",
    "the codewords for @xmath1 and @xmath2 can encode the transaction @xmath3 ( since the transaction is just a set of items ) . now consider a sequence @xmath4 and two serial episodes @xmath5 and @xmath6 . even though the occurrences of @xmath7 and @xmath8 would cover the sequence @xmath9 ,",
    "this information alone is insufficient for encoding the sequence .",
    "since the order of events is important in sequential data , in order to get back the exact sequence , one needs to specify where exactly the occurrences of the episodes happen in the sequence .",
    "for example , we need to know that the @xmath10 and @xmath11 in the occurrence of @xmath5 are not contiguous and that there is a @xmath12 in the gap between them .",
    "one needs to have some way of taking care of such gaps while encoding the data with the occurrences of some frequent episodes . in general",
    ", the events in a sequence constituting an episode occurrence need not be contiguous , and different occurrences can have arbitrary temporal overlaps .",
    "an encoding scheme should be able to properly take care of this .",
    "the previous approaches for using the mdl principle to summarize sequence data  @xcite , explicitly record such gaps while encoding data , thus significantly increasing the encoding length .",
    "while the methods presented in  @xcite consider only sequential data without time stamps , the method in  @xcite does encode event sequences with time stamps also ; but the encoding scheme needs to individually encode each event time stamp . in some cases , the resulting encoding may become even longer than the raw data  @xcite . for the problem of identifying a relevant subset of frequent patterns , we are using the encoded length ( of the data encoded with a subset of patterns ) , only as a figure of merit to compare different subsets . hence , the fact of the encoding length becoming more than the raw data is , per se , not disallowed .",
    "however , the underlying philosophy of mdl principle suggests that one needs a good level of data compression to have confidence in a model .",
    "for example , even if the sequence data is _ iid _ noise and has no temporal structure , there would be some subset of patterns that would achieve lower encoded data length than other subsets .",
    "however , one expects that even the best such subset here would not achieve any appreciable level of data compression , thus suggesting that there are no significant temporal regularities in the data .",
    "on the other hand , for a sequence with significant temporal regularities , one expects good compression of the data sequence , if the method is able to discover the best temporal patterns and encode the sequence with them . in general ,",
    "if we can discover some long episodes which occur many times , then their occurrences can encode many events in the data sequence thus giving rise to the possibility of data compression .    in this paper",
    ", we consider summarizing event sequences ( having time stamps on events ) using a pattern class consisting of serial episodes with fixed inter - event times .",
    "we present algorithms for discovering a small subset of relevant frequent episodes that result in good compression of the data sequence .",
    "the novelty of our approach is that , in contrast to the existing schemes in @xcite , our method does not need to explicitly encode gaps in episode occurrences and the encoding scheme is such that we can retrieve the full data sequence with the time stamps on events , from the encoded sequence .",
    "the encoding of the data consists of only the start times of occurrences of various episodes ; the gaps are determined from the fixed inter - event time constraints of the episodes .",
    "we show through simulations that our method results in better data compression .",
    "we also show , through empirical experiments , that the episodes that result in good data compression are also highly relevant for the dataset .",
    "we also illustrate the benefits of our algorithm using an application , where it is important to both find relevant patterns and achieve good data compression .",
    "we consider streams of sensor - data from a composable conveyor system ( ccs )  @xcite that is useful for materials handling . in this system ,",
    "several conveying units are dynamically composed to achieve the application objectives ; consequently , utilizing the data streams to diagnose or reconfigure the system is important .",
    "the data consists of a sequence of predefined events such as , _ package entered a unit _ , _ package exited a unit _ , _ package arrived at an input port _ , etc .",
    "such events occur at various units in the conveyor system during its routine operation . on this data stream ,",
    "frequent serial episodes represent the routes ( sequence of units ) over which packages were transported in the conveyor system .",
    "the inter - event times corresponds to the various physical constraints such as time required for a package to move through a specific unit , the time required for two adjacent units to complete a handshake protocol to transfer packages between them etc .",
    "thus a small set of relevant episodes can provide a good summary of the events in the conveyor system .",
    "we can use the discovered set of relevant episodes to achieve a lossless compression of the original temporal event sequence to support remote monitoring , diagnostics and visualization activities .",
    "we explain the system in more detail in section  [ sec : conveyor ] . using data obtained from a high - fidelity discrete event simulator of such conveyor systems ,",
    "we demonstrate that our algorithms : ( a ) unearth a small set of relevant episodes that capture the essence of the transport through the system , and ( b ) our scheme achieves good data compression .    even though our method is motivated by the above application",
    ", we show that our method is effective with other general sequential data as well .",
    "apart from conveyor system data streams , we show the effectiveness of our methods with text data as well as on a few other real data sequences .",
    "these are the data sets that are used to illustrate the effectiveness of the algorithms presented in  @xcite .",
    "we compare the performance of our algorithm with these methods on these data sets as well as on the composable conveyor system data .",
    "the rest of the paper is organized as follows . in section",
    "[ sec : prelims ] , we briefly review the formalism of episodes , introduce the new subclass of serial episodes and formally state the problem .",
    "section  [ sec : encoding - expln ] describes our encoding scheme for temporal data using our episodes .",
    "the various algorithms for mining and subset selection are explained in section  [ sec : algo ] and the experimental results are given in section  [ sec : expt ] .",
    "we conclude the paper in section  [ sec : conclude ] .",
    "the data we consider is a sequence of @xmath13 events denoted as @xmath14 , where @xmath15 , and if @xmath16 , then @xmath17 , where @xmath18 , is the event - type , @xmath19 is the alphabet and @xmath20 is the time stamp of the @xmath21 event . note that we can have multiple events ( of different types ) all occurring at the same time instant .",
    "a @xmath22-node serial episode @xmath23 is denoted as @xmath24 where @xmath25 , @xmath26 .",
    "an occurrence of @xmath23 in @xmath27 is a mapping @xmath28 , such that @xmath29 and @xmath30 , for @xmath31 .",
    "an occurrence can be denoted by @xmath32 , the event times of the events constituting the occurrence .",
    "we call the interval @xmath33 $ ] as the occurrence window of this occurrence .",
    "( if @xmath34 , then for the @xmath35-node episode , the occurrence window is essentially a number which is the event time of that event ) .",
    "consider an example event sequence @xmath36 in the data sequence given in  , a few occurrences of episode @xmath37 are @xmath38 .",
    "a _ fixed interval serial episode _ is a serial episode with fixed inter - event gaps .",
    "a fixed interval serial episode is denoted as @xmath39 .",
    "we will be considering the class of fixed interval serial episodes , where @xmath40 , with @xmath41 being a user specified upper bound on allowable gap .",
    "an occurrence of @xmath42 in @xmath27 is a mapping @xmath43 , such that @xmath29 and @xmath44 , for @xmath45 .",
    "for example , in sequence @xmath46 in , there are two occurrences of episode @xmath47 , namely @xmath48 and @xmath49 .",
    "note that the time of the first event of an occurrence completely specifies the entire occurrence .",
    "this property of the fixed interval serial episodes allows us to design a coding scheme that results in data compression .",
    "a @xmath22-node fixed interval serial episode @xmath50 is called if @xmath51 .    in the literature ,",
    "different notions of frequency are defined for episodes depending on the type of occurrences we count .",
    "( for a discussion on various frequencies see @xcite ) .",
    "an episode is said to be frequent if its frequency is above a given threshold . in this paper",
    ", we consider the number of distinct occurrences as the frequency .",
    "two occurrences are distinct if none of the events of one occurrence is among events of the other .",
    "more formally , a set of occurrences , @xmath52 of an episode @xmath23 are if for any @xmath53 , @xmath54 , @xmath55 .",
    "this is a natural notion of frequency for an injective fixed interval serial episode because any pair of its occurrences with different start times will always be distinct .    in this paper",
    ", we consider injective fixed interval serial episodes and from now on we refer to injective fixed interval serial episodes simply as episodes whenever there is no scope for confusion .",
    "pattern mining algorithms often output a large number of frequent episodes .",
    "our goal is to isolate a small subset of them which are non - redundant and are relevant for the data . to formalize this goal",
    ", we use the mdl principle which views learning as data compression .",
    "the idea is that if we can discover all the relevant regularities in the data , then an encoding based on these would result in data compression  @xcite .",
    "thus , the goal is to find a model which allows us to encode the data in a compact fashion .    given any model , @xmath56 ,",
    "let @xmath57 denote the length for encoding the model @xmath56 and let @xmath58 be the length of the data when encoded using the model @xmath56 .",
    "given an encoding scheme , under the mdl principle our goal is to find a model @xmath56 that minimizes total encoded length , @xmath59 . for us ,",
    "different models correspond to different subsets of the set of frequent fixed interval serial episodes .",
    "as mentioned earlier , an occurrence of such an episode is uniquely specified by its start time .",
    "hence , by giving the code for the identity of the episodes and a list of start times , we can code all the events constituting the occurrences of this episode .",
    "( we explain our encoding scheme in the next section ) .",
    "thus , large episodes with many occurrences would account for a large number of events in the data sequence thus decreasing @xmath58 .",
    "another advantage of our use of the mdl principle is that it inherently takes care of redundancy .",
    "selecting episodes with minimal overlap among their occurrences would help reduce the final encoded length .    under mdl",
    ", we are looking at lossless coding and hence the occurrences of the selected subset of episodes have to _ cover _ the entire dataset ; i.e. , every event in the data sequence should be part of an occurrence of ( at least ) one of the selected set of episodes .",
    "we can always ensure this by adding a few 1-node episodes , as needed .",
    "we will give details of our encoding in the next section .",
    "our main problem can now be stated as below    [ prob1 ] given a data sequence @xmath27 and a set of ( frequent ) fixed interval serial episodes , @xmath60 , find the subset @xmath61 such that @xmath62",
    "in this section , we explain our encoding scheme and derive the expression for encoded data length .",
    "each model @xmath56 is a set of some fixed interval serial episodes whose occurrences cover the data .",
    "given such an @xmath56 , which forms the dictionary , the data is then encoded by specifying the start times of selected occurrences of the episodes .",
    "@xmath63 +   +    .a data sequence and its encoding [ cols=\"^,^,^,^ \" , ]     for each dataset , the results were obtained by averaging 20 repetitions of 10-fold cross - validation .",
    "the results are shown in table  [ table : class - accuracy ] .",
    "the table gives the mean and standard deviation ( @xmath64 ) of the classification accuracy .",
    "we see that , sqs marginally outperforms the other methods for the _ aslgt _ and _ context _ datasets and the two csc-2 methods have higher accuracies for the _ aslbu _ and _ auslan2 _ data sets , respectively . in all the datasets , however , the accuracies are similar for sqs and csc-2 . in comparison ,",
    "the accuracies of the gokrimp method are slightly lower than both the sqs and csc-2 methods , except for the pioneer data , where none of the methods seem to misclassify .",
    "table  [ table : classify_numpatterns ] shows the number of patterns selected by each method as features and we see that the number of features selected by sqs is far higher than other methods .",
    "and we noticed that , even though the performance of sqs was only slightly better than csc-2 , the run time for the experiments was at least five times longer than that of csc-2 .",
    "on the other hand , gokrimp selects comparatively the lowest number of patterns ( except for the context data set ) and also has the lowest accuracy among the three methods . for the auslan2 dataset",
    ", gokrimp could nt extract any pattern and hence the classification accuracy is similar to @xmath65 ( the slight difference is due to difference in the cross validation splits for different runs ) .",
    "it is also interesting to see that the @xmath65 method , which consists of only the 1-node counts are always close to the best results .",
    "but nevertheless , the table shows that the selected patterns from different methods have contributed to the increase in accuracy .",
    "the results presented here show that csc-2 is a good method for finding a subset of patterns that achieve good compression .",
    "it is also seen that these patterns that achieve good compression are also highly relevant for the problem . for the conveyor system datasets",
    ", our method was shown to perform extremely well in pulling out patterns representing the stable flow of items and achieving great compression .",
    "both the aspects are of great importance for remote monitoring of such systems as discussed earlier .",
    "the other methods have failed in doing so . on these datasets , the other algorithms , namely sqs and gokrimp failed to find patterns that capture the package flow and they also could not achieve much data compression .",
    "we have also tested out method on some real world data sets .",
    "the csc-2 algorithm discovered subsets of episodes that result in better data compression compared to the other methods and our algorithm also seems faster than other algorithms for most of the datasets . the subset of patterns identified by csc-2 is also seen to be very effective in classification scenarios .",
    "recall that the patterns used by csc-2 are fixed interval serial episodes .",
    "such episodes , as we have seen , suited the conveyor system data sets , where sequential occurrences of events follow such a fixed gap mechanism . for the other sequential datasets used for classification , such constraints may not be really relevant .",
    "but even on these data sets , csc-2 identifies a subset of patterns that result in both data compression as well as better performance in classification .",
    "this shows that our pattern structure is not particularly restrictive and it is useful on a variety of data sets .",
    "frequent episodes discovered from sequential data are supposed to give us good insights into the characteristics of the data source . however , in practice , most mining algorithms output a large number of highly redundant episodes . isolating a small subset of episodes that succinctly characterize",
    "the data is a challenging problem . in this paper",
    ", we presented an mdl based approach for this problem . using the interesting class of fixed interval serial episodes and a novel data encoding scheme",
    ", we presented a method to discover a subset of highly relevant episodes .",
    "in contrast to methods in  @xcite , our method achieves good data compression , while being able to work with event sequences with time stamps .    we compared our method with sqs and gokrimp on text data and also on a number of real world data sets which were used earlier in temporal data mining . on all these data sets ,",
    "our method is good in comparison to others , both in terms of compression and run time . for the classification scenario ,",
    "our method was only slightly less effective than sqs but better than gokrimp .",
    "but we achieved it with far fewer patterns and very low run times .    in this paper",
    ", we also briefly discussed a novel application area for sequential pattern mining .",
    "this is the composable conveyor system .",
    "we presented empirical comparison of our method with that of others on three data sets from this problem domain to demonstrate both the effectiveness and efficiency of our method .    in this paper",
    ", we have not attempted any statistical analysis of our method so that we can relate the data compression to some measure of statistical significance of the pattern subset isolated by our method .",
    "this is an interesting and challenging direction to extend the work presented here .",
    "we would be exploring this in our future work .",
    "b.  archer , s.  shivakumar , a.  rowe , and r.  rajkumar .",
    "profiling primitives of networked embedded automation . in _ automation science and engineering , 2009 .",
    "case 2009 .",
    "ieee international conference on _ , pages 531536 .",
    "ieee , 2009 .",
    "s.  laxman , p.  s. sastry , and k.  p. unnikrishnan . a fast algorithm for finding frequent episodes in event streams . in _ proceedings of the 13th acm",
    "sigkdd international conference on knowledge discovery and data mining _ ,",
    "pages 410419 .",
    "acm , 2007 .",
    "n.  tatti and j.  vreeken .",
    "the long and the short of it : summarising event sequences with serial episodes . in _ proceedings of the 18th acm sigkdd international conference on knowledge discovery and data mining _",
    ", pages 462470 .",
    "acm , 2012 .",
    "j.  wang , j.  han , and j.  pei .",
    "closet+ : searching for the best strategies for mining frequent closed itemsets . in",
    "_ proceedings of the ninth acm sigkdd international conference on knowledge discovery and data mining _ ,",
    "pages 236245 .",
    "acm , 2003 .",
    "y.  xiang , r.  jin , d.  fuhry , and f.  f. dragan .",
    "succinct summarization of transactional databases : an overlapped hyperrectangle scheme . in _ proceedings of the 14th acm",
    "sigkdd international conference on knowledge discovery and data mining _ , pages 758766 .",
    "acm , 2008 ."
  ],
  "abstract_text": [
    "<S> most pattern mining methods yield a large number of frequent patterns and isolating a small , relevant subset of patterns is a challenging problem of current interest . in this paper </S>",
    "<S> we address this problem in the context of discovering frequent episodes from symbolic time series data . </S>",
    "<S> motivated by the minimum description length principle , we formulate the problem of selecting relevant subset of patterns as one of searching for a subset of patterns that achieves best data compression . </S>",
    "<S> we present algorithms for discovering small sets of relevant non - redundant episodes that achieve good data compression . </S>",
    "<S> the algorithms employ a novel encoding scheme and use serial episodes with inter - event constraints as the patterns . </S>",
    "<S> we present extensive simulation studies with both synthetic and real data , comparing our method with the existing schemes such as gokrimp and sqs . </S>",
    "<S> we also demonstrate the effectiveness of these algorithms on event sequences from a composable conveyor system ; this system represents a new application area where use of frequent patterns for compressing the event sequence is likely to be important for decision - support and control .    </S>",
    "<S> frequent episodes , serial episodes , mining event sequences , discovering compressing patterns , mdl , inter - event - time constraints . </S>"
  ]
}