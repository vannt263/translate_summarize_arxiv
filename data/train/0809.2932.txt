{
  "article_text": [
    "estimation of discrete structure , such as graphs or clusters , or variable selection is an age - old problem in statistics .",
    "it has enjoyed increased attention in recent years due to the massive growth of data across many scientific disciplines .",
    "these large datasets often make estimation of discrete structures or variable selection imperative for improved understanding and interpretation .",
    "most classical results do not cover the loosely defined case of high - dimensional data , and it is mainly in this area where we motivate the promising properties of our new stability selection .    in the context of regression , for example , an active area of research is to study the @xmath0 case , where the number of variables or covariates @xmath1 exceeds the number of observations @xmath2",
    "; for an early overview see for example @xcite . in a similar spirit , graphical modelling with many more nodes than sample size",
    "has been the focus of recent research , and cluster analysis is another widely used technique to infer a discrete structure from observed data .",
    "challenges with estimation of discrete structures include computational aspects , since corresponding optimisation problems are discrete , as well as determining the right amount of regularisation , for example in an asymptotic sense for consistent structure estimation .",
    "substantial progress has been made over the last years in developing computationally tractable methods which have provable statistical ( asymptotic ) properties , even for the high - dimensional setting with many more variables than samples .",
    "one interesting stream of research has focused on relaxations of some discrete optimisation problems , for example by @xmath3-penalty approaches @xcite or greedy algorithms @xcite .",
    "the practical usefulness of such procedures has been demonstrated in various applications .",
    "however , the general issue of selecting a proper amount of regularisation ( for the procedures mentioned above and for many others ) for getting a right - sized structure or model has largely remained a problem with unsatisfactory solutions .",
    "we address the problem of proper regularisation with a very generic subsampling approach ( bootstrapping would behave similarly ) .",
    "we show that subsampling can be used to determine the amount of regularisation such that a certain familywise type i error rate in multiple testing can be conservatively controlled for finite sample size . particularly for complex , high - dimensional problems ,",
    "a finite sample control is much more valuable than an asymptotic statement with the number of observations tending to infinity . beyond the issue of choosing the amount of regularisation",
    ", the subsampling approach yields a new structure estimation or variable selection scheme . for the more specialised case of high - dimensional linear models , we prove what we expect in greater generality : namely that subsampling in conjunction with @xmath3-penalised estimation requires much weaker assumptions on the design matrix for asymptotically consistent variable selection than what is needed for the ( non - subsampled ) @xmath3-penalty scheme .",
    "furthermore , we show that additional improvements can be achieved by randomising not only via subsampling but also in the selection process for the variables , bearing some resemblance to the successful tree - based random forest algorithm @xcite . subsampling ( and bootstrapping )",
    "has been primarily used so far for asymptotic statistical inference in terms of standard errors , confidence intervals and statistical testing .",
    "our work here is of a very different nature : the marriage of subsampling and high - dimensional selection algorithms yields finite sample familywise error control and markedly improved structure estimation or selection methods .      in general ,",
    "let @xmath4 be a @xmath1-dimensional vector , where @xmath4 is sparse in the sense that @xmath5 components are non - zero .",
    "in other words , @xmath6 . denote the set of non - zero values by @xmath7 and the set of variables with vanishing coefficient by @xmath8 .",
    "the goal of structure estimation is to infer the set @xmath9 from noisy observations .    as a first supervised example , consider data @xmath10 with univariate response variable @xmath11 and @xmath1-dimensional covariates @xmath12 .",
    "we typically assume that @xmath13 s are i.i . distributed .",
    "the vector @xmath4 could be the coefficient vector in a linear model @xmath14 where @xmath15 , @xmath12 is the @xmath16 design matrix and @xmath17 is the random noise whose components are independent , identically distributed .",
    "thus , inferring the set @xmath9 from data is the well - studied variable selection problem in linear regression . a main stream of classical methods proceeds to solve this problem by penalising the negative log - likelihood with the @xmath18-norm @xmath19 which equals the number of non - zero components of @xmath4 .",
    "the computational task to solve such an @xmath18-norm penalised optimisation problem becomes quickly unfeasible if @xmath1 is getting large , even when using efficient branch and bound techniques .",
    "alternatively , one can relax the @xmath18-norm by the @xmath3-norm penalty .",
    "this leads to the lasso estimator @xcite , @xmath20 where @xmath21 is a regularisation parameter and we typically assume that the covariates are on the same scale , i.e. @xmath22 .",
    "an attractive feature of lasso is its computational feasibility for large @xmath1 since the optimisation problem in ( [ lasso ] ) is convex .",
    "furthermore , the lasso is able to select variables by shrinking certain estimated coefficients exactly to 0 .",
    "we can then estimate the set @xmath9 of non - zero @xmath4 coefficients by @xmath23 which involves convex optimisation only .",
    "substantial understanding has been gained over the last few years about consistency of such lasso variable selection @xcite , and we present the details in section [ subsec.randomlasso ] . among the challenges",
    "are the issue of choosing a proper amount of regularisation @xmath24 for consistent variable selection and the fact that restrictive design conditions are needed for asymptotically recovering the true set @xmath9 of relevant covariates .",
    "a second example is on unsupervised gaussian graphical modelling .",
    "the data is assumed to be @xmath25 the goal is to infer conditional dependencies among the @xmath26 variables or components in @xmath27 .",
    "it is well - known that @xmath28 and @xmath29 are conditionally dependent given all other components @xmath30 if and only if @xmath31 , and we then draw an edge between nodes @xmath32 and @xmath33 in a corresponding graph @xcite .",
    "the structure estimation is thus on the index set @xmath34 which has cardinality @xmath35 ( and of course , we can represent @xmath36 as a @xmath37 vector ) and the set of relevant conditional dependencies is @xmath38 .",
    "similarly to the problem of variable selection in regression , @xmath18-norm methods are computationally very hard and become very quickly unfeasible for moderate or large values of @xmath26 .",
    "a relaxation with @xmath3-type penalties has also proven to be useful in this context @xcite .",
    "a recent proposal is the graphical lasso @xcite : @xmath39 this amounts to an @xmath3-penalised estimator of the gaussian log - likelihood , partially maximised over the mean vector @xmath40 , when minimising over all nonnegative definite symmetric matrices .",
    "the estimated graph structure is then @xmath41 which involves convex optimisation only and is computationally feasible for large values of @xmath26 .",
    "another potential area of application is clustering .",
    "choosing the correct number of cluster is a notoriously difficult problem . looking for clusters that are stable under perturbations or subsampling of the data can help to get a better sense of a meaningful number of clusters and to validate results .",
    "indeed , there has been some activity in this area , most notably in the context of _ consensus clustering _ @xcite . for an early application see @xcite .",
    "our proposed false discovery control can be applied to consensus clustering , yielding good estimates of the parameters of a suitable base clustering method for consensus clustering .",
    "the use of resampling for purposes of validation is certainly not new ; we merely try to put it into a more formal framework and to show certain empirical and theoretical advantages of doing so .",
    "it seems difficult to give a complete coverage of all previous work in the area , as notions of stability , resampling and perturbations are very natural in the context of structure estimation and variable selection .",
    "we reference and compare with previous work throughout the paper .",
    "the structure of the paper is as follows .",
    "the generic stability selection approach , its familywise type i multiple testing error control and some representative examples from high - dimensional linear models and gaussian graphical models are presented in section  [ sec.stable ] . a detailed asymptotic analysis of lasso and randomised lasso for high - dimensional linear models",
    "is given in section  [ sec.cons ] and more numerical results are described in section  [ sec.numeric ] . after a discussion in section  [ sec.disc ] ,",
    "we collect all the technical proofs in the appendix .",
    "stability selection is not a new variable selection technique .",
    "its aim is rather to enhance and improve existing methods .",
    "first , we give a general description of stability selection and we present specific examples and applications later .",
    "we assume throughout this section [ sec.stable ] that the data , denoted here by @xmath42 , are independent and identically distributed ( e.g. @xmath43 with covariate @xmath44 and response @xmath45 ) . for a generic structure estimation or variable selection technique , we have a tuning parameter @xmath46 that determines the amount of regularisation .",
    "this tuning parameter could be the penalty parameter in @xmath3-penalised regression , see ( [ lasso ] ) , or in gaussian graphical modelling , see ( [ glasso ] ) ; or it may be number of steps in forward variable selection or orthogonal matching pursuit @xcite or the number of iterations in matching pursuit @xcite or boosting @xcite ; a large number of steps of iterations would have an opposite meaning from a large penalty parameter , but this does not cause conceptual problems . for every value @xmath47 , we obtain a structure estimate @xmath48 .",
    "it is then of interest to determine whether there exists a @xmath47 such that @xmath49 is identical to @xmath9 with high probability and how to achieve that right amount of regularisation .",
    "we motivate the concept of stability paths in the following , first for regression .",
    "stability paths are derived from the concept of regularisation paths .",
    "a regularisation path is given by the coefficient value of each variable over all regularisation parameters : @xmath50 .",
    "stability paths ( defined below ) are , in contrast , the _ probability _ for each variable to be selected when randomly resampling from the data . for any given regularisation parameter @xmath47 , the selected set @xmath49 is implicitly a function of the samples @xmath51 .",
    "we write @xmath52 where necessary to express this dependence .",
    "let @xmath53 be a random subsample of @xmath54 of size @xmath55 , drawn without replacement .",
    "for every set @xmath56 , the probability of being in the selected set @xmath57 is @xmath58    the probability @xmath59 in ( [ pi ] ) is with respect to both the random subsampling ( and other sources of randomness if @xmath49 is a randomised algorithm , see section [ subsec.randomlasso ] ) .    the sample size of @xmath55 is chosen as it resembles most closely the bootstrap @xcite while allowing computationally efficient implementation .",
    "subsampling has also been advocated in a related context in @xcite .    for every variable @xmath60",
    ", the stability path is given by the selection probabilities @xmath61 , @xmath47 .",
    "it is a complement to the usual path - plots that show the coefficients of all variables @xmath60 as a function of the regularisation parameter .",
    "it can be seen in figure [ fig : stabpath ] that this simple path plot is potentially very useful for improved variable selection for high - dimensional data .    in the remainder of the manuscript , we look at the selection probabilities of individual variables .",
    "the definition above covers also sets of variables .",
    "we could monitor the selection probability of a set of functionally related variables , say , by asking how often _ at least one _ variable in this set is chosen or how often _ all _ variables in the set are chosen .",
    "left : the lasso path for the vitamin gene - expression dataset .",
    "the paths of the 6 non - permuted genes are plotted as solid , red lines , while the paths of the 4082 permuted genes are shown as broken , black lines . selecting a model with all 6 unpermuted genes invariably means selecting a large number of irrelevant noise variables .",
    "middle : the stability path of lasso .",
    "the first 4 variables chosen with stability selection are truly non - permuted variables .",
    "right : the stability path for the ` randomised lasso ' with weakness @xmath62 , introduced in section [ subsec.randomlasso ] .",
    "now all 6 non - permuted variables are chosen before any noise variable enters the model .",
    "_ , scaledwidth=95.0% ]    we apply stability selection to the lasso defined in ( [ lasso ] ) .",
    "we work with a gene expression dataset for illustration which is kindly provided by dsm nutritional products ( switzerland ) . for @xmath63 samples",
    ", there is a continuous response variable measuring the logarithm of riboflavin ( vitamin b2 ) production rate of bacillus subtilis , and we have @xmath64 continuous covariates measuring the logarithm of gene expressions from essentially the whole genome of bacillus subtilis .",
    "certain mutations of genes are thought to lead to higher vitamin concentrations and the challenge is to identify those relevant genes via a linear regression analysis .",
    "that is , we consider a linear model as in ( [ eq : linear ] ) and want to infer the set @xmath65 .",
    "instability of the selected set of genes has been noted before @xcite , if either using marginal association or variable selection in a regression or classification model .",
    "@xcite are close in spirit to our approach by arguing for ` consensus ' gene signatures which assess the stability of selection , while @xcite propose to measure stability of so - called ` molecular profiles ' by the jaccard index .",
    "to see how lasso and the related stability path cope with noise variables , we randomly permute all but 6 of the 4088 gene expression across the samples , using the same permutation to keep the dependence structure between the permuted gene expressions intact .",
    "the set of 6 unpermuted genes has been chosen randomly among the 200 genes with the highest marginal association with the response .",
    "the lasso path @xmath66 is shown in the left panel of figure  [ fig : stabpath ] , as a function of the regularisation parameter @xmath24 ( rescaled so that @xmath67 is the minimal @xmath68-value for which the null model is selected and @xmath69 amounts to the basis pursuit solution ) .",
    "three of the ` relevant ' ( unpermuted ) genes stand out , but all remaining three variables are hidden within the paths of noise ( permuted ) genes .",
    "the middle panel of figure  [ fig : stabpath ] shows the stability path .",
    "at least four relevant variables stand out much clearer now than they did in the regularisation path plot .",
    "the right panel shows the stability plot for randomised lasso which will be introduced in section [ subsec.randomlasso ] : now all 6 unpermuted variables stand above the permuted variables and the separation between ( potentially ) relevant variables and irrelevant variables is even better .",
    "choosing the right regularisation parameter is very difficult for the original path .",
    "the prediction optimal and cross - validated choice include too many variables @xcite and the same effect can be observed in this example , where 14 permuted variables are included in the model chosen by cross - validation . figure  [ fig : stabpath ] motivates that choosing the right regularisation parameter is much less critical for the stability path and that we have a better chance to select truly relevant variables .      in a traditional setting , variable selection would amount to choosing one element of the set of models @xmath70 where @xmath71 is again the set of considered regularisation parameters , which can be either continuous or discrete .",
    "there are typically two problems : first , the correct model @xmath9 might not be a member of ( [ list ] ) .",
    "second , even if it is a member , it is typically very hard for high - dimensional data to determine the right amount of regularisation @xmath24 to select exactly @xmath9 , or to select at least a close approximation .",
    "with stability selection , we do not simply select one model in the list ( [ list ] ) .",
    "instead the data are perturbed ( for example by subsampling ) many times and we choose all structures or variables that occur in a large fraction of the resulting selection sets .    for a cutoff @xmath72 with @xmath73 and a set of regularisation parameters @xmath71 , the set of stable variables",
    "is defined as @xmath74    we keep variables with a high selection probability and disregard those with low selection probabilities .",
    "the exact cutoff @xmath72 with @xmath73 is a tuning parameter but the results vary surprisingly little for sensible choices in a range of the cutoff .",
    "neither do results depend strongly on the choice of regularisation @xmath24 or the regularisation region @xmath71 .",
    "see figure [ fig : stabpath ] for an example .    before we present some guidance on how to choose the cutoff parameter and the regularisation region @xmath71 below , it is worthwhile pointing out that there have been related ideas in the literature on bayesian model selection .",
    "@xcite show certain predictive optimality results for the so - called _ median probability model _ , consisting of variables which have posterior probability of being in the model of 1/2 or greater ( as opposed to choosing the model with the highest posterior probability ) . @xcite or @xcite are examples of more applied papers considering bayesian variable selection in this context .",
    "when trying to recover the set @xmath9 , a natural goal is to include as few variables of the set @xmath75 of noise variables as possible .",
    "the choice of the regularisation parameter is hence crucial .",
    "an advantage of our stability selection is that the choice of the initial set of regularisation parameters @xmath71 has typically not a very strong influence on the results , as long as @xmath71 is varied with reason .",
    "another advantage , which we focus on below , is the ability to choose this set of regularisation parameters in a way that guarantees , under stronger assumptions , a certain bound on the expected number of false selections .",
    "[ def - not ] let @xmath76 be the set of selected structures or variables if varying the regularisation @xmath24 in the set @xmath71 .",
    "let @xmath77 be the average number of selected variables , @xmath78 .",
    "define @xmath79 to be the number of falsely selected variables with stability selection , @xmath80    in general , it is very hard to control @xmath81 , as the distribution of the underlying estimator @xmath82 depends on many unknown quantities .",
    "exact control is only possible under some simplifying assumptions .",
    "[ error control ] [ theo : error ] assume that the distribution of @xmath83 is exchangeable for all @xmath47 .",
    "also , assume that the original procedure is not worse than random guessing , i.e. for any @xmath84 , @xmath85 the expected number @xmath79 of falsely selected variables is then bounded by @xmath86    we will discuss below how to make constructive use of the value @xmath87 which is in general an unknown quantity .",
    "the expected number of falsely selected variables is sometimes called the per - family error rate ( _ pfer _ ) or , if divided by @xmath1 , the per - comparison error rate ( _ fcer _ ) in multiple testing @xcite . choosing less variables ( reducing @xmath88 ) or increasing the threshold @xmath72 for selection will , unsurprisingly , reduce the the expected number of falsely selected variables , with a minimal achievable non - trivial value of @xmath89 ( for @xmath90 and @xmath91 ) for the _",
    "pfer_. this seems low enough for all practical purposed as long as @xmath92 , say .",
    "the involved exchangeability assumption is perhaps stronger than one would wish , but there does not seem to be a way of getting error control in the same generality without making similar assumptions . for regression in ( [ eq : linear ] ) , the exchangeability assumption is fulfilled for all reasonable procedures @xmath93 if the design is random and the distribution of @xmath94 is exchangeable .",
    "independence of all variables in @xmath75 is a special case .",
    "more generally , the variables could have a joint normal distribution with @xmath95 for all @xmath96 with @xmath97 and @xmath98 .",
    "for real data , we have no guarantee that the assumption is fulfilled but the numerical examples in section [ sec.numeric ] show that the bound holds up very well for real data .    note also that the assumption of exchangeability is only needed to prove theorem  [ theo : error ] .",
    "all other benefits of stability selection shown in this paper do not rely on this assumption . besides exchangeability , we needed another , quite harmless , assumption , namely that the original procedure is not worse than random guessing .",
    "one would certainly hope that this assumption is fulfilled .",
    "if it is not , the results below are still valid with slightly weaker constants .",
    "the assumption seems so weak , however , that we do not pursue this further .",
    "the threshold value @xmath72 is a tuning parameter whose influence is very small . for sensible values in the range of , say , @xmath99 , results tend to be very similar . once the threshold is chosen at some default value , the regularisation region @xmath71 is determined by the desired error control .",
    "specifically , for a default cutoff value @xmath100 , choosing the regularisation parameters @xmath71 such that say @xmath101 will control @xmath102 ; or choosing @xmath71 such that @xmath103 controls the familywise error rate ( fwer ) at level @xmath104 , i.e. @xmath105 . of course , we can proceed the other way round by fixing the regularisation region @xmath71 and choosing @xmath72 such that @xmath81 is controlled at the desired level .    to do this ,",
    "we need knowledge about @xmath77 . this can be easily achieved by regularisation of the selection procedure @xmath106 in terms of the number of selected variables @xmath107 .",
    "that is , the domain @xmath108 for the regularisation parameter @xmath68 determines the number @xmath107 of selected variables , i.e. @xmath109 . for example , with @xmath3-norm penalisation as in ( [ lasso ] ) or ( [ glasso ] ) , the number @xmath107 is given by the variables which enter first in the regularisation path when varying from a maximal value @xmath110 to some minimal value @xmath111 .",
    "mathematically , @xmath112 is such that @xmath113 . without stability selection ,",
    "the regularisation parameter @xmath24 invariably has to depend on the unknown noise level of the observations .",
    "the advantage of stability selection is that ( a ) exact error control is possible , and ( b ) the method works fine even though the noise level is unknown . this is a real advantage in high - dimensional problems with @xmath0 , as it is very hard to estimate the noise level in these settings .",
    "[ [ pointwise - control . ] ] pointwise control .",
    "+ + + + + + + + + + + + + + + + + +    for some applications , evaluation of subsampling replicates of @xmath49 are already computationally very demanding for a single value of @xmath24 .",
    "if this single value @xmath24 is chosen such that some overfitting occurs and the set @xmath49 is rather too large , in the sense that it contains @xmath9 with high probability , the same approach as above can be used and is in our experience very successful .",
    "results typically do not depend strongly on the utilised regularisation @xmath24 .",
    "see the example below for graphical modelling . setting @xmath114",
    ", one can immediately transfer all results above to the case of what we call here pointwise control . for methods which select structures incrementally ,",
    "i.e. for which @xmath115 for all @xmath116 , pointwise control and control with @xmath117 are equivalent since @xmath118 is then monotonically increasing with decreasing @xmath24 for all @xmath60 .",
    "vitamin gene - expression dataset .",
    "the regularisation path of graphical lasso ( top row ) and the corresponding point - wise stability selected models ( bottom row ) .",
    "_ , scaledwidth=90.0% ]     the same plot as in figure  [ fig : eggs ] but with the variables ( expression values of each gene ) permuted independently .",
    "the empty graph is the true model . with stability selection ,",
    "only a few errors are made , as guaranteed by the made error control .",
    "_ , scaledwidth=90.0% ]    stability selection is also promising for graphical modelling . here",
    "we focus on gaussian graphical models as described in section [ subsec.prelim ] around formula ( [ ggm ] ) and ( [ glasso ] ) .",
    "the pattern of non - zero entries in the inverse covariance matrix @xmath119 corresponds to the edges between the corresponding pairs of variables in the associated graph and is equivalent to a non - zero partial correlation ( or conditional dependence ) between such pairs of variables @xcite .",
    "there has been interest recently in using @xmath3-penalties for model selection in gaussian graphical models due to their computational efficiency for moderate and large graphs @xcite .",
    "here we work with the graphical lasso @xcite , as applied to the data from 160 randomly selected genes from the vitamin gene - expression dataset ( without the response variable ) introduced in section [ subsec.examp1 ] .",
    "we want to infer the set of non - zero entries in the inverse covariance matrix @xmath119 .",
    "part of the resulting regularisation path of the graphical lasso showing graphs for various values of the regularisation parameter @xmath68 , i.e. @xmath120 where @xmath121 , are shown in the first row of figure  [ fig : eggs ] . for reasons of display , variables ( genes ) are ordered first using hierarchical clustering and are symbolised by nodes arranged in a circle .",
    "stability selection is shown in the bottom row of figure [ fig : eggs ] .",
    "we pursue a pointwise control approach . for each value of @xmath68 , we select the threshold @xmath72 so as to guarantee @xmath122 , that is we expect fewer than 30 wrong edges among the 12720 possible edges in the graph .",
    "the set @xmath123 varies remarkably little for the majority of the path and the choice of @xmath107 ( which is implied by @xmath68 ) does not seem to be critical , as already observed for variable selection in regression .",
    "next , we permute the variables ( expression values ) randomly , using a different permutation for each variable ( gene ) .",
    "the true graph is now the empty graph .",
    "as can be seen from figure  [ fig : eggsnull ] , stability selection selects now just very few edges or none at all ( as it should ) .",
    "the top row shows the corresponding graphs estimated with the graphical lasso which yields a much poorer selection of edges .",
    "stability selection demands to re - run @xmath124 multiple times . evaluating selection probabilities over 100 subsamples seems sufficient in practice .",
    "the algorithmic complexity of lasso in ( [ lasso ] ) or in ( [ randomisedlasso ] ) below is of the order @xmath125 , see @xcite . in the @xmath126 regime , running the full lasso path on subsamples of size",
    "@xmath127 is hence a quarter of the cost of running the algorithm on the full dataset and running 100 simulations is 25 times the cost of running a single fit on the full dataset .",
    "this cost could be compared with the cost of cross - validation , as this is what one has to resort to often in practice to select the regularisation parameter . running 10-fold",
    "cross - validation uses approximately @xmath128 as many computational resources as the single fit on the full dataset .",
    "stability selection is thus roughly three times more expensive than 10-fold cv .",
    "this analysis is based on the fact that the computational complexity scales like @xmath129 with the number of observations ( assuming @xmath130 ) .",
    "if computational costs would scale linearly with sample size ( e.g. for lasso with @xmath131 ) , this factor would increase to roughly 5.5 .",
    "stability selection with the lasso ( using 100 subsamples ) for a dataset with @xmath132 and @xmath133 takes about 10 seconds on a 2.2ghz processor , using the implementation of @xcite .",
    "computational costs of this order would often seem worthwhile , given the potential benefits .",
    "stability selection is a general technique , applicable to a wide range of applications , some of which we have discussed above . here , we want to discuss advantages and properties of stability selection for the specific application of variable selection in regression with high - dimensional data which is a well - studied topic nowadays @xcite .",
    "we consider a linear model as in ( [ eq : linear ] ) with gaussian noise , @xmath134 with fixed @xmath16 design matrix @xmath12 and @xmath135 i.i.d .",
    "the predictor variables are normalised with @xmath137 for all @xmath138 .",
    "we allow for high - dimensional settings where @xmath139 .",
    "stability selection is attractive for two reasons .",
    "first , the choice of a proper regularisation parameter for variable selection is crucial and notoriously difficult , especially because the noise level is unknown . with stability selection ,",
    "results are much less sensitive to the choice of the regularisation .",
    "second , we will show that stability selection makes variable selection consistent in settings where the original methods fail .",
    "we give general conditions under which consistent variable selection is achieved with stability selection .",
    "consistent variable selection for a procedure @xmath93 is understood to be equivalent to @xmath140 it is clearly of interest to know under which conditions consistent variable selection can be achieved . in the high - dimensional context , this places a restriction on the growth of the number @xmath1 of variables and sparsity @xmath141 , typically of the form @xmath142 @xcite . while this assumption is often realistic , there are stronger assumptions on the design matrix that need to be satisfied for consistent variable selection . for lasso",
    ", it amounts to the ` neighbourhood stability ' condition @xcite which is equivalent to the ` irrepresentable condition ' @xcite . for orthogonal matching pursuit ( which is essentially forward variable selection ) , the so - called ` exact recovery criterion ' @xcite is sufficient and necessary for consistent variable selection .    here",
    ", we show that these conditions can be circumvented more directly by using stability selection , also giving guidance on the proper amount of regularisation . due to the restricted length of the paper",
    ", we will only discuss in detail the case of lasso whereas the analysis of orthogonal matching pursuit is just indicated .",
    "an interesting aspect is that stability selection with the original procedures alone yields often very large improvements already .",
    "moreover , when adding some extra sort of randomness in the spirit of random forests @xcite weakens considerably the conditions needed for consistent variables selection as discussed next .",
    "the lasso @xcite estimator is given in ( [ lasso ] ) .",
    "for consistent variable selection using @xmath143 , it turns out that the design needs to satisfy the so - called ` neighbourhood stability ' condition @xcite which is equivalent to the ` irrepresentable condition ' @xcite : @xmath144 the condition in ( [ irc ] ) is sufficient and ( almost ) necessary ( the word ` almost ' refers to the fact that a necessary relation is using ` @xmath145 ' instead of ` @xmath146 ' ) .",
    "if this condition is violated , all one can hope for is recovery of the regression vector @xmath4 in an @xmath147-sense of convergence by achieving @xmath148 for @xmath149 .",
    "the main assumption here are bounds on the sparse eigenvalues as discussed below .",
    "this type of @xmath147-convergence can be used to achieve consistent variable selection in a two - stage procedure by thresholding or , preferably , the adaptive lasso @xcite .",
    "the disadvantage of such a two - step procedure is the need to choose several tuning parameters without proper guidance on how these parameters can be chosen in practice .",
    "we propose the randomised lasso as an alternative . despite its simplicity ,",
    "it is consistent for variable selection even though the ` irrepresentable condition ' in ( [ irc ] ) is violated .",
    "randomised lasso is a new generalisation of the lasso .",
    "while the lasso penalises the absolute value @xmath150 of every component with a penalty term proportional to @xmath24 , the randomised lasso changes the penalty @xmath24 to a randomly chosen value in the range @xmath151 $ ] .",
    "+    a proposal for the distribution of the weights @xmath152 is described below , just before theorem [ theo : randlasso ] .",
    "the word ` weakness ' is borrowed from the terminology of weak greedy algorithms @xcite which are loosely related to our randomised lasso .",
    "implementation of ( [ randomisedlasso ] ) is straightforward by appropriate re - scaling of the predictor variables ( with scale factor @xmath152 for the @xmath33-th variable ) .",
    "using these re - scaled variables , the standard lasso is solved , using for example the lars algorithm @xcite or fast coordinate wise approaches @xcite .",
    "the perturbation of the penalty weights is reminiscent of the re - weighting in the adaptive lasso @xcite . here",
    ", however , the re - weighting is not based on any previous estimate , but is simply chosen at random ! as such , it is very simple to implement .",
    "however , it seems nonsensical at first sight since one can surely not expect any improvement from such a random perturbation .",
    "if applied only with one random perturbation , randomised lasso is not very useful .",
    "however , applying randomised lasso many times and looking for variables that are chosen often will turn out to be a very powerful procedure .    [",
    "[ consistency - for - randomised - lasso - with - stability - selection . ] ] consistency for randomised lasso with stability selection .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    for stability selection with randomised lasso , we can do without the irrepresentable condition ( [ irc ] ) but need only a condition on the sparse eigenvalues of the design @xcite , also called the sparse riesz condition in @xcite .    for any @xmath153 , let @xmath154 be the restriction of @xmath12 to columns in @xmath155 .",
    "the minimal sparse eigenvalue @xmath156 is then defined for @xmath157 as @xmath158 and analogously for the maximal sparse eigenvalue @xmath159 .",
    "we have to constrain sparse eigenvalues to succeed .",
    "[ assum : sparse ] there exists some @xmath160 and some @xmath161 such that @xmath162    this assumption ( [ sparseassum ] ) is related to the sparse riesz condition in @xcite .",
    "the equivalent condition there requires the existence of some @xmath163 such that @xmath164 compare with remark 2 in @xcite .",
    "this assumption essentially requires that maximal and minimal eigenvalues , for a selection of order @xmath165 variables , are bounded away from 0 and @xmath166 respectively . in comparison ,",
    "our assumption is significantly stronger than ( [ condzhang ] ) , but at the same time typically much weaker than the standard assumption of the ` irrepresentable condition ' necessary to get results comparable to ours .",
    "we have not specified the exact form of perturbations we will be using for the randomised lasso in ( [ randomisedlasso ] ) . for the following , we consider the randomised lasso of ( [ randomisedlasso ] ) , where the weights @xmath152 are sampled independently as @xmath167 with probability @xmath168 and @xmath169 otherwise .",
    "other perturbations are certainly possible and work often just as well in practice .",
    "[ theo : randlasso ] consider the model in ( [ ymodel ] ) .",
    "for randomised lasso , let the weakness @xmath104 be given by @xmath170 , for any @xmath171 , and @xmath172 .",
    "let @xmath173 be a sequence with @xmath174 for @xmath149 .",
    "let @xmath175 .",
    "assume that @xmath92 and @xmath176 and that the sparse eigenvalue assumption [ assum : sparse ] is satisfied .",
    "then there exists some @xmath177 such that for all @xmath178 , stability selection with the randomised lasso satisfies on a set @xmath179 with @xmath180 that no noise variables are selected , @xmath181 where @xmath182 with @xmath183 . on the same set @xmath179 , @xmath184 where @xmath185 .",
    "this implies that all variables with sufficiently large regression coefficient are selected .    under the condition that the minimal non - zero regression coefficient is bounded from below by @xmath186 as a consequence of theorem  [ theo : randlasso ] , @xmath187 i.e.  consistent variable selection for @xmath188 ( @xmath189 or @xmath190 ) in the sense of ( [ consvar ] ) even if the irrepresentable condition ( [ irc ] ) is violated .",
    "if no such lower bound holds , the set of selected variables might miss variables with too small regression coefficients , which are , by definition , in the set @xmath191 .",
    "[ rem : conserv ] theorem [ theo : randlasso ] is valid for all @xmath192 .",
    "this is noteworthy as it means that even if the value of @xmath68 is chosen too large ( i.e. considerably larger than @xmath112 ) , no noise variables will be selected ( formula ( [ negative ] ) ) .",
    "only some important variables might be missed .",
    "this effect has been seen in the empirical examples as stability selection is very insensitive to the choice of @xmath68 .",
    "in contrast , a hard - thresholded solution of the lasso with a value of @xmath68 too large will lead to the inclusion of noise variables .",
    "thus , stability selection with the randomised lasso exhibits an important property of being conservative and guarding against false positive selections .",
    "theorem [ theo : randlasso ] is derived under random perturbations of the weights .",
    "while this achieves good empirical results , it seems more advantageous in combination with with subsampling of the data .",
    "the results extend directly to this case .",
    "let @xmath193 be the selection probability of variable @xmath194 , while doing both random weight perturbations and subsampling @xmath127 out of @xmath2 observations .",
    "the probability that @xmath193 is above the threshold @xmath195 is bounded by a markov - type inequality from below by @xmath196 having used that @xmath197 as a consequence of theorem  [ theo : randlasso ] .",
    "if @xmath198 is sufficiently small in comparison to @xmath199 , this elementary inequality implies that important variables in @xmath200 are still chosen by stability selection ( subsampling and random weights perturbation ) with very high probability .",
    "a similar argument shows that noise variables are also still not chosen with very high probability .",
    "empirically , combining random weight perturbations with subsampling yields very competitive results and this is what we recommend to use .",
    "there is an inherent tradeoff when choosing the weakness @xmath104 .",
    "a negative consequence of a low @xmath104 is that the design can get closer to singularity and can thus lead to unfavourable conditioning of the weighted design matrix . on the other hand ,",
    "a low value of @xmath104 makes it less likely that irrelevant variables are selected .",
    "this is a surprising result but rests on the fact that irrelevant variables can only be chosen if the corresponding irrepresentable condition ( [ irc ] ) is violated . by randomly perturbing the weights with a low @xmath104",
    ", this condition is bound to fail sometimes , lowering the selection probabilities for such variables .",
    "a low value of @xmath104 will thus help stability selection to avoid selecting noise variables with a violated irrepresentable condition ( [ irc ] ) . in practice ,",
    "choosing @xmath104 in the range of @xmath201 gives very useful results .",
    "[ [ relation - to - other - work . ] ] relation to other work .",
    "+ + + + + + + + + + + + + + + + + + + + + + +    in related and very interesting work , @xcite has proposed ` bolasso ' ( for bootstrapped enhanced lasso ) and shown that using a finite number of subsamples of the original lasso procedure and applying basically stability selection with @xmath90 yields consistent variables selection under the condition that the penalty parameter @xmath68 vanishes faster than typically assumed , at rate @xmath202 , and that the model dimension @xmath1 is fixed . while the latter condition could possibly be technical",
    "only , the first distinguishes it from our results .",
    "applying stability selection to randomised lasso , no false variable is selected for all sufficiently large values of @xmath68 , see remark [ rem : conserv ] . in other words , if @xmath68 is chosen ` too large ' with randomised lasso , only truly relevant variable are chosen ( though a few might be missed ) . if @xmath68 is chosen too large with bolasso , noise variables might be picked up .",
    "figure [ fig : ex3 ] is a good illustration .",
    "picking the regularisation in the left plot ( without extra randomness ) to select the correct model is much harder than in the right plot , where extra randomness is added .",
    "the same distinction can be made with two - stage procedures like adaptive lasso @xcite or hard - thresholding @xcite , where variables are thresholded .",
    "picking @xmath68 too large ( and @xmath68 is notoriously difficult to choose ) , false variables will invariably enter the model .",
    "in contrast , stability selection with randomised lasso is not picking wrong variables if @xmath68 is chosen too large .       the stability paths for randomised lasso with stability selection using weakness parameters @xmath203 ( left panel identical to the original lasso ) and @xmath204 ( middle ) and @xmath62 ( right ) .",
    "red solid lines are the coefficients of the first two ( relevant variables ) .",
    "the blue broken line is the coefficient of the third ( irrelevant ) variable and the dotted lines are the coefficients from all other ( irrelevant ) variables .",
    "introducing the randomised version helps avoid choosing the third ( irrelevant ) predictor variable .",
    "_ , title=\"fig:\",scaledwidth=32.0% ]   the stability paths for randomised lasso with stability selection using weakness parameters @xmath203 ( left panel identical to the original lasso ) and @xmath204 ( middle ) and @xmath62 ( right ) .",
    "red solid lines are the coefficients of the first two ( relevant variables ) .",
    "the blue broken line is the coefficient of the third ( irrelevant ) variable and the dotted lines are the coefficients from all other ( irrelevant ) variables .",
    "introducing the randomised version helps avoid choosing the third ( irrelevant ) predictor variable .",
    "_ , title=\"fig:\",scaledwidth=32.0% ]   the stability paths for randomised lasso with stability selection using weakness parameters @xmath203 ( left panel identical to the original lasso ) and @xmath204 ( middle ) and @xmath62 ( right ) .",
    "red solid lines are the coefficients of the first two ( relevant variables ) .",
    "the blue broken line is the coefficient of the third ( irrelevant ) variable and the dotted lines are the coefficients from all other ( irrelevant ) variables .",
    "introducing the randomised version helps avoid choosing the third ( irrelevant ) predictor variable .",
    "_ , title=\"fig:\",scaledwidth=32.0% ]    we illustrate the results on randomised lasso with a small simulation example : @xmath205 and the predictor variables are sampled from a @xmath206 distribution , where @xmath207 is the identity matrix , except for the entries @xmath208 and their symmetrical counterparts .",
    "we use a regression vector @xmath209 . the response @xmath11 is obtained from the linear model @xmath210 in ( [ eq : linear ] ) , where @xmath211 i.i.d .",
    "@xmath212 . for @xmath213 , the irrepresentable condition in ( [ irc ] )",
    "is violated and lasso is not able to correctly identify the first two variables as the truly important ones , since it always includes the third variable superfluously as well . using the randomised version for lasso , the two relevant variables are still chosen with probability close to 1 , while the irrelevant third variable is only chosen with much lower probability ; the corresponding probabilities are shown for randomised lasso in figure [ fig : ex3 ] .",
    "this allows to separate relevant and irrelevant variables . and",
    "indeed , the randomised lasso is consistent under stability selection .",
    "an interesting alternative to lasso or greedy forward search in this context are the recently proposed forward - backward search foba @xcite and the mc+ algorithm @xcite , which both provably lead to consistent variable selection under weak conditions on sparse eigenvalues , despite being greedy solutions to non - convex optimisation problems",
    ". it will be very interesting to explore the effect of stability selection on these algorithms , but this is beyond the scope of this paper .    here",
    ", we look instead at orthogonal matching pursuit , a greedy forward search in the variable space .",
    "the iterative sis procedure @xcite , entails orthogonal matching pursuit as a special case .",
    "we will examine the effect of stability selection under subsampling and additional randomisation . to have a clear definition of randomised orthogonal matching pursuit ( romp ) ,",
    "we define it as follows .",
    "+    a drawback of omp is clearly that conditions for consistent variable selection are quite strong .",
    "following @xcite , the exact recovery condition for omp is defined as @xmath214 this is a sufficient condition for consistent variable selection .",
    "if it is not fulfilled , there exist regression coefficients that cause omp or its weak variant to fail in recovery of the exact set @xmath9 of relevant variables .",
    "surprisingly , this condition is rather similar to the irrepresentable @xcite or neighbourhood stability condition @xcite .    in the spirit of theorem [ theo : randlasso ] , we have also a proof that stability selection for randomised orthogonal matching pursuit ( romp ) is asymptotically consistent for variable selection in linear models , even if the right hand side in ( [ erc ] ) is not bounded by 1 but instead by a possibly large constant ( assuming the weakness @xmath104 is low enough ) .",
    "this indicates that stability selection has a more general potential for improved structure estimation , beyond the case for the lasso presented in theorem  [ theo : randlasso ] .",
    "it is noteworthy that our proof involves artificial adding of noise covariates . in practice",
    ", this seems to help often but a more involved discussion is beyond the scope of this paper .",
    "we will give empirical evidence for the usefulness of stability selection under subsampling and additional randomisation for orthogonal matching pursuit in the numerical examples below .",
    "probability to select @xmath215 and @xmath216 important variables without selecting a noise variable with the lasso in the regression setting ( dark red bar ) and stability selection under subsampling ( light grey bar ) for the 64 different settings .",
    "black crosses mark the result for stability selection with additional randomisation ( @xmath204 ) .",
    "_ , scaledwidth=99.0% ]     the equivalent plot to fig .",
    "[ lasso ] for lasso applied to classification ( top two rows ) and omp applied to regression ( bottom two rows ) .",
    "_ , title=\"fig:\",scaledwidth=99.0% ]   the equivalent plot to fig .",
    "[ lasso ] for lasso applied to classification ( top two rows ) and omp applied to regression ( bottom two rows ) .",
    "_ , title=\"fig:\",scaledwidth=99.0% ]     comparison of stability selection with cross - validation for the real datasets ( f ) and ( g ) . the cross - validated solution ( for standard lasso )",
    "is indicated by a dot and the corresponding stability selection ( for randomised lasso , @xmath204 on the left and @xmath203 on the right ) by a red triangle , showing the average proportion of correctly identified relevant variables versus the average number of falsely selected variables .",
    "each pair consisting of a dot and triangle corresponds to a simulation setting ( some specified snr and @xmath165 ) .",
    "the broken vertical line indicates the value at which the number of wrongly selected variables is controlled , namely @xmath217 .",
    "looking at stability selection , the proportion of correctly identified relevant variables is very close to the cv - solution , while the number of falsely selected variables is reduced dramatically .",
    "_ , title=\"fig:\",scaledwidth=47.5% ]   comparison of stability selection with cross - validation for the real datasets ( f ) and ( g ) .",
    "the cross - validated solution ( for standard lasso ) is indicated by a dot and the corresponding stability selection ( for randomised lasso , @xmath204 on the left and @xmath203 on the right ) by a red triangle , showing the average proportion of correctly identified relevant variables versus the average number of falsely selected variables .",
    "each pair consisting of a dot and triangle corresponds to a simulation setting ( some specified snr and @xmath165 ) .",
    "the broken vertical line indicates the value at which the number of wrongly selected variables is controlled , namely @xmath217 .",
    "looking at stability selection , the proportion of correctly identified relevant variables is very close to the cv - solution , while the number of falsely selected variables is reduced dramatically .",
    "_ , title=\"fig:\",scaledwidth=47.5% ]    to investigate further the effects of stability selection , we focus here on the application of stability selection to lasso and randomised lasso for both regression and the natural extension to classification",
    ". the effect on omp and randomised omp will also be examined .    for regression ( lasso and omp ) , we generate observations by @xmath210 . for classification , we use the logistic linear model under the binomial family . to generate the design matrices @xmath12 ,",
    "we use two real and five simulated datasets ,    a.   independent predictor variables .",
    "all @xmath132 predictor variables are i.i.d .",
    "standard normal distributed .",
    "sample size @xmath133 and @xmath218 .",
    "b.   block structure with 10 blocks .",
    "the @xmath132-dimensional predictor variable follows a @xmath206 distribution , where @xmath219 for all pairs @xmath220 except if @xmath221 , for which @xmath222 .",
    "sample size @xmath223 and @xmath218 . c.   toeplitz design .",
    "the @xmath132-dimensional predictor variable follows a @xmath206 distribution , where @xmath224 and @xmath225 .",
    "sample size @xmath223 and @xmath218 .",
    "d.   [ d ] factor model with 2 factors .",
    "let @xmath226 be two latent variables following i.i.d .",
    "standard normal distributions .",
    "each predictor variable @xmath29 , for @xmath60 , is generated as @xmath227 , where @xmath228 have i.i.d .",
    "standard normal distributions for all @xmath60 .",
    "sample sizes are @xmath223 and @xmath218 , while @xmath132 .",
    "e.   identical to ( [ d ] ) but with 10 instead of 2 factors",
    ". f.   motif regression dataset . a dataset ( @xmath229 and @xmath230 ) about finding transcription factor binding sites ( motifs ) in dna sequences .",
    "the real - valued predictor variables are abundance scores for @xmath1 candidate motifs ( for each of the genes ) .",
    "our dataset is from a heat - shock experiment with yeast . for a general description and motivation about motif regression",
    "we refer to @xcite .",
    "g.   the already mentioned vitamin gene expression data ( with @xmath64 and @xmath231 ) described in section [ subsec.examp1 ] .",
    "we do not use the response values from the real datasets , however , as we need to know which variables are truly relevant or irrelevant . to this end",
    ", we create sparse regression vectors by setting @xmath232 for all @xmath60 , except for a randomly chosen set @xmath9 of coefficients , where @xmath233 is chosen independently and uniformly in @xmath234 $ ] for all @xmath235 .",
    "the size @xmath236 of the active set is varied between 4 and 50 , depending on the dataset . for regression ,",
    "the noise vector @xmath237 is chosen i.i.d .",
    "@xmath238 , where the rescaling of the variance with @xmath2 is due to the rescaling of the predictor variables to unit norm , i.e. @xmath239 . the noise level @xmath240 is chosen to achieve signal - to - noise ratios ( snr ) of @xmath241 and  @xmath242 . for classification , we scale the vector @xmath4 to achieve a given bayes misclassification rate , either @xmath243 or @xmath244 .",
    "each of the 64 scenarios is run 100 times , once using the standard procedure ( lasso or omp ) , once using stability selection with subsampling and once using stability selection with subsampling and additional randomisation ( @xmath204 for the randomised lasso and @xmath245 for randomised omp ) .",
    "the methods are thus in total evaluated on about 20.000 simulations each .",
    "the solution of stability selection can not be reproduced by simply selecting the right penalty with lasso , since stability selection provides a fundamentally new solution . to compare the power of both approaches , we look at the probability that @xmath246 of the @xmath165 relevant variables can be recovered without error , where @xmath247 . a set of @xmath248 variables",
    "is said to be recovered successfully for the lasso or omp selection , if there exists a regularisation parameter such that at least @xmath249 variables in @xmath9 have a non - zero regression coefficient and all variables in @xmath250 have a zero regression coefficient .",
    "for stability selection , recovery without error means that the @xmath251 variables with highest selection probability @xmath252 are all in @xmath9 .",
    "the value @xmath112 is chosen such that at most @xmath253 variables are selected in the whole path of solutions for @xmath254 .",
    "note that this notion neglects the fact that the most advantageous regularisation parameter is selected here automatically for lasso and omp but not for stability selection .",
    "results are shown in figure [ fig : lasso ] for lasso applied to regression , and in figure [ fig : class_omp ] for lasso applied to classification and omp applied to regression again . in figure",
    "[ fig : lasso ] , we also give the median number of variables violating the irrepresentable condition ( denoted by ` violations ' ) and the average of the maximal correlation between a randomly chosen variable and all other variables ( ` max cor ' ) as two measures of the difficulty of the problem .",
    "stability selection identifies as many or more correct variables than the underlying method itself in all cases except for scenario ( a ) , where it is about equivalent . that stability selection is not advantageous for scenario ( a ) is to be expected as the design is nearly orthogonal ( very weak empirical correlations between variables ) , thus almost decomposing into @xmath1 univariate decisions and we would not expect stability selection to help in a univariate framework",
    ".    often the gain of stability selection under subsampling is substantial , irrespective of the sparsity of the signal and the signal - to - noise - ratio .",
    "additional randomisation helps in cases where there are many variables violating the irrepresentable condition ; for example in setting ( e ) .",
    "this is in line with our theory .",
    "next , we test how well the error control of theorem [ theo : error ] holds up for these datasets . for the motif regression dataset ( f ) and the vitamin gene expression dataset ( g ) ,",
    "lasso is applied , with randomisation and without . for both datasets",
    ", the signal - to - noise ratio is varied between 0.5 , 1 and 2 .",
    "the number of non - zero coefficients @xmath165 is varied in steps of 1 between 1 and 12 , with a standard normal distribution for the randomly chosen non - zero coefficients .",
    "each of the 72 settings is run 20 times .",
    "we are interested in the comparison between the cross - validated solution and stability selection . for stability selection , we chose @xmath255 and thresholds of @xmath256 , corresponding to a control of @xmath217 , where @xmath79 is the number of wrongly selected variables .",
    "the control is mathematically derived under the assumption of exchangeability for the distribution of noise variables , see theorem [ theo : error ] .",
    "this assumption is most likely not fulfilled for the given dataset and it is of interest to see how well the bound holds up for real data .",
    "results are shown in figure [ fig : motifs ] .",
    "stability selection reduces the number of falsely selected variables dramatically , while maintaining almost the same power to detect relevant variables .",
    "the number of falsely chosen variables is remarkably well controlled at the desired level , giving empirical evidence that the derived error control is useful beyond the discussed setting of exchangeability .",
    "stability selection thus helps to select a useful amount of regularisation .",
    "stability selection addresses the notoriously difficult problem of structure estimation or variable selection , especially for high - dimensional problems .",
    "cross - validation fails often for high - dimensional data , sometimes spectacularly .",
    "stability selection is based on subsampling in combination with ( high - dimensional ) selection algorithms .",
    "the method is extremely general and we demonstrate its applicability for variable selection in regression and gaussian graphical modelling .",
    "stability selection provides finite sample familywise multiple testing error control ( or control of other error rates of false discoveries ) and hence a transparent principle to choose a proper amount of regularisation for structure estimation or variable selection .",
    "furthermore , the solution of stability selection depends surprisingly little on the chosen initial regularisation .",
    "this is an additional great benefit besides error control .",
    "another property of stability selection is the improvement over a pre - specified selection method .",
    "it is often the case that computationally efficient algorithms for high - dimensional selection are inconsistent , even in rather simple settings .",
    "we prove for randomised lasso that stability selection will be variable selection consistent even if the necessary conditions needed for consistency of the original method are violated . and thus , stability selection will asymptotically select the right model in scenarios where lasso fails .",
    "in short , stability selection is the marriage of subsampling and high - dimensional selection algorithms , yielding finite sample familywise error control and markedly improved structure estimation .",
    "both of these main properties are demonstrated on simulated and real data .",
    "an alternative to subsampling is sample splitting . instead of observing",
    "if a given variable is selected for a random subsample , one can look at a random split of the data into two non - overlapping samples of equal size @xmath55 and see if the variable is chosen in both sets simultaneously .",
    "let @xmath257 and @xmath258 be two random subsets of @xmath54 with @xmath259 for @xmath260 and @xmath261 .",
    "define the simultaneously selected set as the intersection of @xmath262 and @xmath263 , @xmath264    define the simultaneous selection probabilities @xmath265 for any set @xmath153 as @xmath266 where the probability @xmath59 is with respect to the random sample splitting ( and any additional randomness if @xmath49 is a randomised algorithm ) .",
    "we work with the selection probabilities based on subsampling but the following lemma lets us convert these probabilities easily into simultaneous selection probabilities based on sample splitting ; the latter is used for the proof of theorem [ theo : error ] .",
    "the bound is rather tight for selection probabilities close to 1 .",
    "[ lemma : bound ] for any set @xmath153 , a lower bound for the simultaneous selection probabilities is given by , for every @xmath267 , by @xmath268    _ proof .",
    "_ let @xmath257 and @xmath258 be the two random subsets in sample splitting of @xmath54 with @xmath259 for @xmath260 and @xmath261 . denote by @xmath269 the probability @xmath270 .",
    "note that the two events are not independent as the probability is only with respect to a random split of the fixed samples @xmath54 into @xmath257 and @xmath258 .",
    "the probabilities @xmath271 are defined equivalently by @xmath272 , @xmath273 , and @xmath274 .",
    "note that @xmath275 and @xmath276 it is obvious that @xmath277 . as @xmath278 , it also follows that @xmath279 .",
    "hence @xmath280 which completes the proof .",
    "@xmath281       the proof uses mainly lemma [ lemma : markov ] .",
    "we first show that @xmath282 for all @xmath283 , using the made definitions @xmath284 and @xmath285 .",
    "define furthermore @xmath286 to be the set of noise variables ( in @xmath75 ) which appear in @xmath287 and analogously @xmath288 .",
    "the expected number of falsely selected variables can be written as @xmath289 . using the assumption ( [ btrg ] ) ( which asserts that the method is not worse than random guessing )",
    ", it follows that @xmath290 .",
    "putting together , @xmath291 and hence @xmath292 . using the exchangeability assumption",
    ", we have @xmath293 for all @xmath283 and hence , for @xmath283 , it holds that @xmath294 , as desired .",
    "note that this result is independent of the sample size used in the construction of @xmath49 , @xmath47 .",
    "now using lemma [ lemma : markov ] below , it follows that @xmath295 for all @xmath296 and @xmath283 . using lemma [ lemma : bound ]",
    ", it follows that @xmath297 .",
    "hence @xmath298 , which completes the proof .",
    "@xmath281    [ lemma : markov ]",
    "let @xmath299 and @xmath49 the set of selected variables based on a sample size of @xmath300 . if @xmath301 , then @xmath302 if @xmath303 for some @xmath304 , then @xmath305     _ proof .",
    "_ let @xmath306 be , as above , the random split of the samples @xmath54 into two disjoint subsets , where both @xmath307 for @xmath260 .",
    "define the binary random variable @xmath308 for all subsets @xmath153 as @xmath309 denote the data ( the @xmath2 samples ) by @xmath310 .",
    "the simultaneous selection probability @xmath311 , as defined in ( [ simult ] ) , is then @xmath312 where the expectation @xmath313 is with respect to the random split of the @xmath2 samples into sets @xmath257 and @xmath258 ( and additional randomness if @xmath49 is a randomised algorithm ) . to prove the first part",
    ", the inequality @xmath314 ( for a sample size @xmath300 ) , implies that @xmath315 and hence @xmath316 therefore , @xmath317 using a markov - type inequality , @xmath318 thus @xmath319 , completing the proof of the first claim .",
    "the proof of the second part follows analogously .",
    "@xmath281      instead of working directly with form ( [ randomisedlasso ] ) of the randomised lasso estimator , we consider the equivalent formulation of the standard lasso estimator , where all variables have initially unit norm and are then rescaled by their random weights w.    [ def : additionala ] for weights @xmath320 as in ( [ randomisedlasso ] ) , let @xmath321 be the matrix of re - scaled variables , with @xmath322 for each @xmath60 .",
    "let @xmath323 and @xmath324 be the maximal and minimal eigenvalues analogous to ( [ phimin ] ) for @xmath321 instead of @xmath12 .",
    "the proof rests mainly on the two - fold effect a weakness @xmath325 has on the selection properties of the lasso .",
    "the first effect is that the singular values of the design can be distorted if working with the reweighted variables @xmath321 instead of @xmath12 itself .",
    "a bound on the ratio between largest and smallest eigenvalue is derived in lemma  [ lemma : trans ] , effectively yielding a lower bound for useful values of @xmath104 .",
    "the following lemma  [ lemma : boundedq ] then asserts , for such values of @xmath104 , that the relevant variables in @xmath9 are chosen with high probability under any random sampling of the weights .",
    "the next lemma  [ lemma:1/2 ] establishes the key advantage of randomised lasso as it shows that the ` irrepresentable condition ' ( [ irc ] ) is sometimes fulfilled under randomly sampled weights , even though its not fulfilled for the original data .",
    "variables which are wrongly chosen because condition ( [ irc ] ) is not satisfied for the original unweighted data will thus not be selected by stability selection .",
    "the final result is established in lemma  [ lemma : nofalse ] after a bound on the noise contribution in lemma  [ lemma : boundnoise ] .",
    "[ lemma : trans ] define @xmath326 by @xmath327 and assume @xmath176 .",
    "let @xmath320 be weights generated randomly in @xmath328 $ ] , as in ( [ randomisedlasso ] ) , and let @xmath321 be the corresponding rescaled predictor variables , as in definition  [ def : additionala ] .",
    "for @xmath329 , with @xmath330 , it holds under assumption [ assum : sparse ] for all random realisations @xmath320 that @xmath331    _ proof .",
    "_ using assumption  [ assum : sparse ] , @xmath332 where the first inequality follows by assumption [ assum : sparse ] , the equality by @xmath333 and the second inequality by @xmath334 .",
    "it follows that @xmath335 now , let @xmath336 be again the @xmath337-diagonal matrix with diagonal entries @xmath338 for all @xmath60 and 0 on the non - diagonal elements .",
    "then @xmath339 and , taking suprema over all @xmath336 with diagonal entries in @xmath328 $ ] , @xmath340 where the last step follows by a change of variable transform @xmath341 and the fact that @xmath342 as well as @xmath343 and thus @xmath344 for all @xmath345 with diagonal entries in @xmath328 $ ] . the corresponding argument for @xmath346 yields the bound @xmath347 for all @xmath348 .",
    "the claim ( [ boundphiw ] ) follows by observing that @xmath349 for @xmath176 , since @xmath350 by assumption  [ assum : sparse ] and hence @xmath351 .",
    "@xmath281    [ lemma : boundedq ] let @xmath352 be the set @xmath353 of selected variables of the randomised lasso with weakness @xmath354 $ ] and randomly sampled weights @xmath320 .",
    "suppose that the weakness @xmath355 .",
    "under the assumptions of theorem [ theo : randlasso ] , there exists a set @xmath356 in the sample space of @xmath11 with @xmath357 , such that for all realisations @xmath358 , for @xmath359 , if @xmath360 , @xmath361 where @xmath191 is defined as in theorem [ theo : randlasso ] .",
    "_ follows mostly from theorem 1 in @xcite . to this end ,",
    "set @xmath362 in their notation .",
    "we also have @xmath363 , as , by definition , @xmath333 , as in lemma [ lemma : trans ] .",
    "the quantity @xmath364 in @xcite is identical to our notation @xmath365 .",
    "it is bounded for all random realisations of @xmath358 , as long as @xmath355 , using lemma [ lemma : trans ] , by @xmath366 hence all assumptions of theorem 1 in @xcite are fulfilled , with @xmath367 , for any random realisation @xmath358 . using ( 2.20)-(2.24 ) in @xcite , it follows that there exists a set @xmath356 in the sample space of @xmath11 with @xmath368 for all @xmath359 , such that if @xmath369 , from ( 2.21 ) in @xcite , @xmath370 and , from ( 2.23 ) in @xcite , @xmath371 having used for the first inequality that , in the notation of @xcite , @xmath372 .",
    "the @xmath373 factor was omitted to account for our different normalisation . for the second inequality",
    ", we used @xmath374 .",
    "the last inequality implies , by definition of @xmath191 in theorem [ theo : randlasso ] , that @xmath375 , which completes the proof .",
    "@xmath281    [ lemma:1/2 ] set @xmath172 .",
    "let @xmath138 and let @xmath376 be a set which can depend on the random weight vector @xmath320 .",
    "suppose that @xmath377 satisfies @xmath378 and @xmath379 for all realisations @xmath358 .",
    "suppose furthermore that @xmath380 for some @xmath381 implies that @xmath382 for all pairs @xmath383 of weights that fulfill @xmath384 for all @xmath385 , with equality for all @xmath386 .",
    "then , for @xmath387 , @xmath388 where the probability @xmath389 is with respect to random sampling of the weights @xmath320 and @xmath390 is , as above , the probability of choosing weight @xmath104 for each variable and @xmath391 the probability of choosing weight  1 .",
    "_ let @xmath392 be the realisation of @xmath320 for which @xmath393 and @xmath394 for all other @xmath395 .",
    "the probability of @xmath396 is clearly @xmath397 under the used sampling scheme for the weights .",
    "let @xmath398 be the selected set of variables under these weights .",
    "let now @xmath399 be the set of all weights for which @xmath400 and @xmath401 for all @xmath386 , and arbitrary values in @xmath402 for all @xmath403 with @xmath404 .",
    "the probability for a random weight being in this set is @xmath405 . by the assumption on @xmath155",
    ", it holds that @xmath380 for all @xmath406 , since @xmath407 for all @xmath408 with equality for @xmath386 . for all weights @xmath406",
    ", it follows moreover that @xmath409 using the bound on @xmath104 , it hence only remains to be shown that , if @xmath410 for all @xmath411 , @xmath412 since @xmath413 for any vector @xmath414 , it is sufficient to show , for @xmath415 , @xmath416 as @xmath417 is the projection of @xmath29 into the space spanned by @xmath418 and @xmath419 , it holds that @xmath420 . using @xmath421",
    ", it follows that @xmath422 , which shows ( [ toshowww ] ) and thus completes the proof .",
    "@xmath281    [ lemma : boundnoise ]",
    "let @xmath423 be the projection into the space spanned by all variables in subset @xmath381 .",
    "suppose @xmath92 .",
    "then there exists a set @xmath424 with @xmath425 , such that for all @xmath426 , @xmath427    _ proof .",
    "_ let @xmath428 be the event that @xmath429 as entries in @xmath430 are i.i .",
    "@xmath136 distributed , @xmath431 for all @xmath432 .",
    "note that , for all @xmath433 and @xmath434 , @xmath435 .",
    "define @xmath436 as @xmath437 it is now sufficient to show that @xmath438 . showing this bound",
    "is related to a bound in @xcite and we repeat a similar argument .",
    "each term @xmath439 has a @xmath440 distribution as long as @xmath441 is of full rank @xmath442 .",
    "hence , using the same standard tail bound as in the proof of theorem 3 of @xcite , @xmath443 having used @xmath444 for all @xmath445 in the last step and thus , using @xmath446 , @xmath447 which completes the proof by setting @xmath448 and concluding that @xmath425 for all @xmath445 . @xmath281    [ lemma : nofalse]let @xmath449 and @xmath450 be again the probability for variable @xmath33 of being in the selected subset , with respect to random sampling of the weights @xmath320 .",
    "then , under the assumptions of theorem [ theo : randlasso ] , for all @xmath451 and @xmath92 , there exists a set @xmath179 with @xmath452 such that for all @xmath453 and @xmath454 , @xmath455 where @xmath191 is defined as in theorem [ theo : randlasso ] .",
    "_ we let @xmath456 , where @xmath356 is the event defined in lemma [ lemma : boundedq ] and event @xmath424 is defined in lemma [ lemma : boundnoise ] . since , using these two lemmas , @xmath457 it is sufficient to show ( [ toshowif1 ] ) and ( [ toshowif2 ] ) for all @xmath458 .",
    "we begin with ( [ toshowif1 ] ) .",
    "a variable @xmath451 is in the selected set @xmath352 only if @xmath459 where @xmath460 is the solution to ( [ randomisedlasso ] ) with the constraint that @xmath461 , comparable to the analysis in @xcite .",
    "let @xmath462 be the set of non - zero coefficients and @xmath463 be the set of regression coefficients which are either truly non - zero or estimated as non - zero ( or both ) .",
    "we will use @xmath464 as a short - hand notation for @xmath465 .",
    "let @xmath466 be the projection operator into the space spanned by all variables in the set @xmath464 . for all @xmath358 , this is identical to @xmath467 then , splitting the term @xmath468 in ( [ tmp1 ] ) into the two terms @xmath469 it holds for the right term in ( [ twoterms ] ) that @xmath470 looking at the left term in ( [ twoterms ] ) , since @xmath369 , we know by lemma [ lemma : boundedq ] that @xmath471 and , by definition of @xmath464 above , @xmath472 . thus the left term in ( [ twoterms ] ) is bounded from above by @xmath473 having used lemma [ lemma : boundnoise ] in the last step and @xmath474 .",
    "putting together , the two terms in ( [ twoterms ] ) are bounded , for all @xmath475 , by @xmath476 we now apply lemma [ lemma:1/2 ] to the rightmost term .",
    "the set @xmath464 is a function of the weight vector and satisfies for every realisation of the observations @xmath369 the conditions in lemma [ lemma:1/2 ] on the set @xmath377 .",
    "first , @xmath477 .",
    "second , by definition of @xmath464 above , @xmath478 for all weights @xmath479 .",
    "third , it follows by the kkt conditions for lasso that the set of non - zero coefficients of @xmath480 and @xmath481 is identical for two weight vectors @xmath479 and @xmath482 , as long @xmath483 for all @xmath484 and @xmath485 for all @xmath486 ( increasing the penalty on zero coefficients will leave them at zero , if the penalty for non - zero coefficients is kept constant ) .",
    "hence there exists a set @xmath487 in the sample space of @xmath320 with @xmath488 such that @xmath489 .",
    "moreover , for the same set @xmath487 , we have @xmath490 .",
    "hence , for all @xmath475 and , for all @xmath491 , the lhs of ( [ tmp1 ] ) is bounded from above by @xmath492 and variable @xmath451 is hence not part of the set @xmath493 .",
    "it follows that @xmath494 with @xmath495 for all @xmath451 .",
    "this completes the first part ( [ toshowif1 ] ) of the proof .    for the second part ( [ toshowif2 ] )",
    ", we need to show that , for all @xmath496 , all variables @xmath33 in @xmath9 are chosen with probability at least @xmath497 ( with respect to random sampling of the weights @xmath320 ) , except possibly for variables in @xmath498 , defined in theorem  [ theo : randlasso ] . for all @xmath499 , however , it follows directly from lemma  [ lemma : boundedq ] that @xmath500 .",
    "hence , for all @xmath194 , the selection probability satisfies @xmath501 for all @xmath369 , which completes the proof .",
    "@xmath281    since the statement in lemma  [ lemma : nofalse ] is a reformulation of the assertion of theorem  [ theo : randlasso ] , the proof of the latter is complete .",
    "both authors would like to thank anonymous referees for many helpful comments and suggestions which greatly helped to improve the manuscript .",
    "n.m . would like to thank fim ( forschungsinstitut fr mathematik ) at eth zrich for support and hospitality ."
  ],
  "abstract_text": [
    "<S> estimation of structure , such as in variable selection , graphical modelling or cluster analysis is notoriously difficult , especially for high - dimensional data . </S>",
    "<S> we introduce stability selection . </S>",
    "<S> it is based on subsampling in combination with ( high - dimensional ) selection algorithms . as such , </S>",
    "<S> the method is extremely general and has a very wide range of applicability . </S>",
    "<S> stability selection provides finite sample control for some error rates of false discoveries and hence a transparent principle to choose a proper amount of regularisation for structure estimation . </S>",
    "<S> variable selection and structure estimation improve markedly for a range of selection methods if stability selection is applied . </S>",
    "<S> we prove for randomised lasso that stability selection will be variable selection consistent even if the necessary conditions needed for consistency of the original lasso method are violated . </S>",
    "<S> we demonstrate stability selection for variable selection and gaussian graphical modelling , using real and simulated data . </S>"
  ]
}