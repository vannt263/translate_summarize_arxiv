{
  "article_text": [
    "modeling of collisionless plasmas are often done using fluid magnetohydrodynamic models ( mhd ) .",
    "the mhd fluid approximation is however questionable when the gyro radius of the ions are large compared to the spatial region that is studied .",
    "on the other hand , kinetic models that discretize the full velocity space , or full particle in cell ( pic ) models that treat ions and electrons as particles , are very computational expensive . for problems where the ion time- and spatial scales are of interest ,",
    "hybrid models provide a compromise . in such models ,",
    "the ions are treated as discrete particles , while the electrons are treated as a ( often massless ) fluid .",
    "this mean that the electron time- and spatial scales do not need to be resolved , and enables applications such as modeling of the solar wind interaction with planets . for a detailed discussion of different models , see @xcite .    here",
    "we present an finite difference implementation of a hybrid model in the flash parallel computational framework , along with test cases that show that the implementation scales well and conserve energy well .",
    "in the hybrid approximation , ions are treated as particles , and electrons as a massless fluid . in what follows we use si units .",
    "the trajectory of an ion , @xmath0 and @xmath1 , with charge @xmath2 and mass @xmath3 , is computed from the lorentz force , @xmath4 where @xmath5 is the electric field , and @xmath6 is the magnetic field .",
    "the electric field is given by @xmath7 where @xmath8 is the ion charge density , @xmath9 is the ion current , @xmath10 is the electron pressure , and @xmath11 is the magnetic constant . then faraday s law is used to advance the magnetic field in time , @xmath12",
    "we use a cell - centered representation of the magnetic field on a uniform grid .",
    "all spatial derivatives are discretized using standard second order stencils .",
    "time advancement is done by a predictor - corrector leapfrog method with subcycling of the field update , denoted cyclic leapfrog ( cl ) by @xcite .",
    "an advantage of the discretization is that the divergence of the magnetic field is zero , down to round off errors .",
    "the ion macroparticles ( each representing a large number of real particles ) are deposited on the grid by a cloud - in - cell method ( linear weighting ) , and interpolation of the fields to the particle positions are done by the corresponding linear interpolation .",
    "initial particle positions are drawn from a uniform distribution , and initial particle velocities from a maxwellian distribution .",
    "further details of the algorithm can be found in @xcite .",
    "we use an existing software framework , flash , developed at the university of chicago @xcite , that implements a block - structured adaptive ( or uniform ) cartesian grid and is parallelized using the message - passing interface ( mpi ) library for communication .",
    "it is written in fortran 90 , well structured into modules , and open source .",
    "output is handled by the hdf5 library , providing parallel i / o . although the flash framework has mostly been used for fluid modeling , it has support for particles which we have used to implement a hybrid solver using the latest version of the framework , flash3 .",
    "the advantage of using an existing framework when implementing a solver is that all grid operations , parallelization and file handling is done by standard software calls that have been well - tested .",
    "also , there is an existing infrastructure for parameter files and setup directories that simplifies code handling .",
    "the concept of a setup directory is that one can place modified versions of any routine in the directory , and this new version will be used during the build process .",
    "this is an easy way to handle different versions of code for different runs of the solver .    in particular , many of the basic operations needed for a pic code",
    "are provided as standard operations in flash :    * deposit charges onto the grid : ` call grid_mapparticlestomesh ( ) ` * interpolate fields to particle positions : ` call grid_mapmeshtoparticles ( ) ` * ghost cell update for all blocks : ` call grid_fillguardcells ( ) `    the advantage of a parallel solver is the ability to handle larger computational problems than on a single processor , both in terms of computational time and memory requirements .",
    "this is especially important for pic solvers which are computational intensive compared to fluid solvers . since we typically have 10100 particles per cell , the computational work will be dominated by operations on the particles : moving the particles and grid - particle operations .",
    "that a code works well in parallel is usually investigated by looking at how the code scales . for the case of strong scaling , a fixed size problem is run on different numbers of processors .",
    "ideally the execution time should decrease proportional to the number of processors ( linear scaling ) .",
    "this is however difficult to achieve in real world applications .",
    "sequential parts of the program quickly dominate the execution time .",
    "an alternative concept is that of weak scaling . here",
    "the problem size is increased at the same time as we add more processors . that this implementation of a hybrid solver exhibits weak scaling",
    "is shown in fig .",
    "[ fig : scaling ] . for this application , weak",
    "scaling is adequate , since we aim to solve larger problems using a constant wall clock time .",
    "it is not easy to check the correctness and accuracy of a hybrid solver .",
    "there are not many non - trivial test problems that have analytical solutions to compare with .",
    "one can of course check the results against published results but that only gives a crude sanity check of the code .",
    "one option is to investigate the conservation of total energy .",
    "this is a crucial quantity since the behavior of a plasma is a constant exchange of energy between the kinetic energy of the particles and the energy stored in the electromagnetic fields .",
    "it is also a quantity that easily can be measured for _ any _ simulation , especially if the boundary conditions are periodic .",
    "we therefore choose to perform numerical experiments with an ion beam , a test problem that strongly exhibits this exchange of energy between particles and fields , to study the conservation of total energy .",
    "a classic plasma model problem is that of an ion beam into a plasma  @xcite . @xcite",
    "describes a two - dimensional simulation of a low density ion beam through a background plasma .",
    "the initial condition has a uniform magnetic field , with a beam of ions , number density @xmath13 , propagating along the field with velocity @xmath14 , where @xmath15 is the alfvn velocity , through a background ( core ) plasma of number density @xmath16 .",
    "both the background and beam ions have thermal velocities @xmath17 . here",
    "we study what is denoted a",
    "_ resonant beam _ with @xmath18 and @xmath19 .",
    "electron temperature is assumed to be zero , so the electron pressure term in the electric field equation of state is zero .",
    "the weight of the macroparticles are chosen such that there is an equal number of core and beam macroparticles , each beam macroparticles thus represent fewer real ions than the core macroparticles .",
    "the number of magnetic field update subcycles is nine .",
    "the spatial extent of the domain is 22016  km along the @xmath20-axis , divided up in 256  cells with periodic boundary conditions .",
    "the core number density is @xmath21  @xmath22 .",
    "the magnetic field magnitude is @xmath23  nt directed along the @xmath20-axis , which give an alfvn velocity of 50  km / s and an ion inertial length of @xmath24=86  km , where @xmath25 and @xmath26 is the ion gyrofrequency .",
    "the time step is 0.0865  s = @xmath27 , and the cell size is @xmath24 .",
    "the number of particles per cell is 32 . in fig .",
    "[ fig : beam ] we show a velocity space plot of the macro - ions at time @xmath28  s @xmath29 .",
    "this can be compared to fig .  5 in @xcite .",
    "-axis as a function of position at time @xmath28  s @xmath29 .",
    "each gray dot is a core macro - ion , and each black dot is a beam macro - ion . ]",
    "the kinetic energy of the ions , the energy stored in the electromagnetic fields , and total energy , as a function of time , are shown in fig .",
    "[ fig:1de ] .",
    "the magnetic and electric field energies are computed as @xmath30 respectively . here",
    "@xmath31 is the vacuum permittivity , @xmath32 .",
    "note that the electric field energy is too small to be visible in the figure ( orders of magnitude smaller than the magnetic field energy ) .        at @xmath33  s @xmath34",
    "the relative energy error is less than 0.1% .",
    "this can be compared to the error of 11% given by @xcite .      in the two - dimensional case",
    "we have a square grid with sides of length 22016  km , and 128 cells in each direction with periodic boundary conditions .",
    "the time step is 0.0216  s = @xmath27 , and the cell widths are @xmath35 .",
    "the number of particles per cell is 16 .",
    "otherwise the setup is identical to the one - dimensional case . in fig .",
    "[ fig:2d ] we show the magnitude of the magnetic field @xmath36-component at time @xmath37  s @xmath38 .",
    "this can be compared to fig .  5 in @xcite .",
    "-component for a two - dimensional ion beam at time @xmath37  s @xmath38 . ]    the kinetic energy of the ions , the energy stored in the electromagnetic fields , and total energy , as a function of time , are shown in fig .",
    "[ fig:2de ] .",
    "we can note that the fluctuations in magnetic and kinetic energy is smaller than in the one - dimensional case , consistent with the observations by @xcite .",
    "the total relative energy gain at @xmath39  s @xmath40 is less than 0.004% .",
    "this can be compared to @xcite , where the stated energy gain was less than 2% and smoothing of the fields was needed for numerical stability .",
    "we have investigated the performance of a hybrid solver on the classical test case of an ion beam , in one and two dimensions .",
    "it is shown that the cell centered finite difference solver conserves energy very well . the change in total energy over time is more than an order of magnitude smaller than for two previously published solvers .",
    "also , the solver is numerically stable over long times without any smoothing of the fields .",
    "it is planned that this hybrid solver will be part of a future release of the flash code .",
    "this research was conducted using resources provided by the swedish national infrastructure for computing ( snic ) at the high performance computing center north ( hpc2n ) , ume  university , sweden .",
    "the software used in this work was in part developed by the doe - supported asc / alliance center for astrophysical thermonuclear flashes at the university of chicago ."
  ],
  "abstract_text": [
    "<S> we investigate the performance of a hybrid plasma solver on the test problem of an ion beam . </S>",
    "<S> the parallel solver is based on cell centered finite differences in space , and a predictor  corrector leapfrog scheme in time . </S>",
    "<S> the implementation is done in the flash software framework . </S>",
    "<S> it is shown that the solver conserves energy well over time , and that the parallelization is efficient ( it exhibits weak scaling ) . </S>"
  ]
}