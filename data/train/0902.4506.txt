{
  "article_text": [
    "our expectation that a quantum computer will one day outperform a classical computer is in part founded upon quantum error correction , which will be necessary to mitigate environmental decoherence and errors that arise from imprecise quantum control .",
    "various methods for error correction are known , including topological error correction @xcite , surface codes @xcite , and color codes @xcite .",
    "the method we are concerned with in this paper involves concatenating small quantum codes that are analogous to classical codes such as the repetition code and the hamming code .",
    "quantum codes such as the bacon - shor code @xcite and the steane code @xcite work by redundantly encoding the state of a single , physical qubit in the combined state of a few qubits , whereafter an error in the state of one or more of these qubits can be diagnosed and corrected using appropriately designed circuits that control the propagation of errors @xcite .",
    "this reduces the error rate of an encoded quantum circuit relative to an unencoded quantum circuit by at least a factor proportional to reciprocal of the physical error rate , as two or more concurrent errors are required to cause failure .",
    "however , because there are more places for errors to occur than before , the encoded error rate will only be lower than the physical error rate if the physical error rate is below some threshold @xcite .",
    "a single level of error correction can only reduce the encoded error rate so much .",
    "a greater reduction can be achieved by encoding a single physical qubit in the state of a few qubits , as before , and then encoding each of those qubits in the state of more qubits , and so on .",
    "this forms a concatenated quantum code , which allows , in theory , an arbitrarily long and large universal quantum computation to be undertaken reliably and efficiently .",
    "however , as both the number of qubits and the time required increase exponentially with the number of concatenation levels , it is likely that only a few levels will be practical . therefore with finite resources the fault tolerant threshold should not be thought of as the critical point for achieving arbitrary accuracy in quantum computing . to achieve a sufficiently low logical error rate with a practical amount of resources",
    "it is important to make each level as effective as possible at reducing the encoded error rate .",
    "typically , each level of a concatenated code operates independently of all other levels and implicitly assumes that errors on all qubits are equally likely . for a distance-@xmath0 code that is concatenated @xmath1 times , this method results in a concatenated code that can fail if there are @xmath2 errors .",
    "however , as the total distance of the concatenated code is @xmath3 , one might expect that a more sophisticated decoding method may exist which will never fail with fewer than @xmath4 errors .",
    "such a method is known for the case of quantum information being communicated over a noisy channel @xcite , but not for the case where errors can occur during error correction itself .",
    "recently we reported on a new method of concatenated error correction which makes use of classical information obtained during the first level of error correction to assist in the diagnosis of errors at the second level @xcite . when applied to the [ [ 9,1,3 ] ] subsystem code our method",
    "was shown to correct all cases with four physical errors whereas conventional error correction , where each level of error correction operates independently , can only guarantee success with up to three physical errors .",
    "our method was inspired by a similar method that has been applied to an error - detection code with the result that the minimum number of errors that cause failure scales with increasing levels of concatenation as the fibonacci sequence @xcite .",
    "here we extend the method of ref .",
    "@xcite so that it is applicable to any number of levels of concatenation . by using information gathered by the process of error correction itself",
    "we determine the relative likelihood of failure at each circuit location and thus maximize the probability of correctly interpreting high - level error syndromes .",
    "our method requires no additional quantum resources and increases the minimum number of errors that cause failure to half the total distance of the concatenated code . in the design of this method",
    "we assume a discrete error model in which the probability of error at each physical circuit location , @xmath5 , is constant and independent of the other locations .",
    "we also assume that the quantum computer will be operating in the low-@xmath5 limit , thus the exponent of the lowest order of @xmath5 is sufficient to describe probabilities .",
    "it may be possible to improve the method by relaxing these assumptions .",
    "the key idea for our method is that the process of error correction provides information that can be used to estimate the probability that an encoded qubit has failed .",
    "for example , whenever error correction measures a non - zero syndrome we know that at least one error has occurred , and so our confidence in the encoded information should be decreased . here",
    "we aim to use all of the information at our disposal to determine the most likely set of errors that could give rise to the measured syndrome so that we can maximize the probability of returning to the correct codeword state .",
    "we achieve this by tracking effect of each possible error through the error - correction circuitry , along with an associated confidence rating . whenever a syndrome measurement is made , the results are compared to predicted syndrome measurements for possible sets of errors .",
    "we apply corrections to the data qubits based on the most likely set of errors that would give rise to the syndrome .",
    "confidence ratings for the higher - level encoded qubits are calculated using the information from the syndrome measurements .",
    "for each location in the error - correction circuit we determine what effect a discrete @xmath6 and @xmath7 error occurring at that location would have on the data and ancilla at the end of syndrome extraction .",
    "taking into account that some errors are equivalent to other errors , we assign a _ flag _ to each unique error .",
    "in other words , there is a flag for each possible effect on the data and ancilla that can be caused by a single error in the circuit .",
    "each flag represents the possibility of a single error .",
    "the probability of any given flag representing an actual error is described by the _ weight _ of the flag such that @xmath8 .",
    "as the possibility of error at each location is independent of the other locations , the probability of multiple flags representing actual errors is @xmath9 , where the sum is over the set of flags being considered .",
    "error correction and flags in the @xmath6 and @xmath7 bases are treated separately .    at the physical level , each location in the error - correction circuit is assigned a confidence of one , indicating that physical - level locations all fail with probability @xmath5 . at the end of each error - correction cycle",
    "( in each basis ) , after measuring the syndrome and applying the appropriate correction , a confidence rating is calculated for the encoded qubit based on the syndrome and flags .",
    "this is the logical confidence rating of the higher - level circuit location , the calculation of which will be described in the following section .",
    "initially , all flags can be thought of as having infinite confidence ( as there can not be any errors before any operations have been done ) . as each circuit location",
    "is executed , the weight of the flag corresponding to an error at that location is updated to be the minimum of its current value and the confidence reported at that location .",
    "detection of errors at lower levels will lower our confidence of success at particular circuit locations all the way up to the highest level of error correction .",
    "two - qubit gates are special in that errors may be shared on both qubits .",
    "for example , errors occurring before a transversal @xmath10 may be copied and errors occurring during a @xmath10 may affect both the control and the target qubits . therefore , the confidence ratings reported by error correction immediately following a two - qubit gate are used to update the flag which corresponds to a correlated error affecting both qubits involved in the gate .",
    "this is in addition to updating both of the flags corresponding to the uncorrelated errors after gate .",
    "the flag weight corresponding to the two - qubit error is taken to be the maximum of the two single - qubit confidence ratings as opposed to the sum , which would correspond to the two uncorrelated errors .",
    "the possibility of a correlated error is thus represented by its own flag .",
    "any error on a data qubit will eventually have its effect transfered down to the ancilla .",
    "the location of the error determines which ancillary qubit will see its effect .",
    "an error which affects a data qubit only after it has interacted with the ancilla in its current error - correction cycle can not be detected until the following error - correction cycle .",
    "therefore , flags that represent errors that effect only data qubits should be put aside until the current error - correction cycle is complete , after which they will represent errors that will be detected in the following error - correction cycle .",
    "these flags can be called _ transitive flags_. following error correction , each transitive flag is converted into the its associated non - transitive flag for use in the next error - correction cycle .",
    "( see section [ sec : application ] for an example . )",
    "each logical qubit has its own set of flag weights .",
    "when two logical qubits interact ( during a logical operation ) certain errors within one logical qubit may propagate onto the other logical qubit .",
    "let us call the logical qubit from which errors may be copied the _ source _ and the logical qubit to which errors may be copied the _",
    "target_. flags representing the possibility of errors on the source are used to update the weights of flags on the target . for each possible error that may be copied from the source , the corresponding flag weight on the target is taken to be the minimum of its current weight and the weight of the flag on the source .",
    "although copying an error from one logical qubit to another would actually result in a correlated error affecting both qubits , we treat each error and its associated probability as if they were independent so that error correction can be performed on each logical qubit separately .",
    "the possibility of correlated errors is reintroduced at the logical level where the weight is taken to be the maximum of the weights of the individual logical qubits .      at the end of each error - correction cycle",
    "the ancillary qubits are measured to obtain the syndrome .",
    "each possible syndrome is consistent with a finite number of sets of errors .",
    "each of these sets of errors has a corresponding set of flags .",
    "we call each set that matches the measured syndrome a _ flag match_. the most likely cause of the syndrome is the set of errors that is represented by the flag match with the lowest weight , where the weight of a flag match is the sum of the weights of the flags in the set . with",
    "the lowest weight flag match identified , we apply corrections to whichever data qubits would be affected by the set of errors represented by the match .",
    "alternative flag matches will imply different data corrections .",
    "in particular , there will always be a flag match which implies a complement set of corrections to that of the lowest weight match @xmath11 that is , the corrections of both matches combine to form a logical operator .",
    "if we correct based on some match but the true set of errors are represented by its complement match then the result is a logical failure . to determine the logical confidence rating that will be used by error correction at the level above ,",
    "we consider the probability that our choice of correction will result in a logical error , @xmath12 pr(fail@xmath13s@xmath14flags ) is the probability of a logical failure given the current syndrome and set of flags .",
    "pr(s@xmath13flags ) is the probability of the current syndrome occurring given the current set of flags , regardless of success or failure .",
    "pr(fail@xmath14s@xmath13flags ) is the probability of the current syndrome occurring and error correction resulting in a logical error .",
    "in the low-@xmath5 limit these probabilities are dominated by the leading - order terms and the coefficients are not important . since our method applies corrections based on the most likely cause of the syndrome , to leading order pr(ss)@xmath15pr(success@xmath14ss ) .",
    "this implies @xmath16 where @xmath17 is the weight of the lowest weight flag match and @xmath18 is the weight of the complement match .",
    "therefore , @xmath19 is the logical confidence rating calculated during this error - correction cycle .",
    "the logical confidence rating represents the probability of a logical error , @xmath20 .",
    "however , there is also a possibility that the corrections applied will result in a state that is outside of the code space , so that it is neither the correct state nor the state affected by a logical error .",
    "just as we used the complement match to determine the probability of a logical error we can determine the probability of individual errors on each data qubit by considering the other flag matches . at the end of each error - correction cycle the confidence of each data qubit is updated to be the minimum of the confidence of its associated transitive flag and a confidence determined during the error correction of the encoded qubit , where this confidence is calculated in an analogous way to the logical confidence .",
    "in large codes there will also be probabilities of correlated sets of data errors that we are able to derive from the flag matches .",
    "let @xmath21 represent a particular set of a data errors after an error - correction cycle .",
    "then @xmath22 the weight of the flag corresponding to this set of data errors should , therefore , be updated to be the minimum of its current weight and the sum of each set of flags that match this outcome , taking into account any corrections that have been applied , minus the weight of the minimum weight flag match .",
    "( see section [ sec : application ] for an example . )    by the careful consideration of every possible cause of every syndrome we always have accurate confidence ratings for every qubit at every level .",
    "we use this information to maximize the probability of success by always correcting based on the most likely set of errors .",
    "here we apply our method to the [ [ 4,1,2 ] ] subsystem code @xcite .",
    "the stabilizer generators of this code are @xmath23 where @xmath24 and @xmath25 respectively represent the pauli operators @xmath26 and @xmath27 applied to the @xmath28 qubit .",
    "tensor products and identity operators are implicitly present .",
    "the gauge group of this code is generated by the operators @xmath29 the logical @xmath6 and @xmath7 operators are @xmath30 or equivalently any operators that can be generated by combining eq .  [ eq : logical ops ] with the gauge operators of eq .",
    "[ eq : gauge ] .    the syndrome - extraction circuit which we have chosen to use is shown in fig .",
    "[ fig : error map ] @xcite , along with the flags that will be used in our message - passing method .",
    "the circuit performs operator measurements of the gauge operators , two at a time . since the stabilizers are products of the gauge operators , the combined parity of each pair of ancilla measurements is effectively the result of an operator measurement of a stabilizer .",
    "individually , each gauge operator measurement tells us nothing about the syndrome .    since the logical operators of the [ [ 4,1,2 ] ] code are pairwise products of single - qubit operators , the code distance is equal to two .",
    "this means that it is only an error - detection code .",
    "the syndrome of the code consists of a single bit in each basis .",
    "a syndrome of zero indicates that no error was detected ; one indicates that an error was detected but provides no information on which qubits were affected by the error . when the syndrome is measured to be one , in the absence of any additional information",
    ", we must simply guess which data qubit was affected by the error .",
    "due to the gauge group of the code , each data qubit has a partner qubit in each basis for which errors are equivalent .",
    "for example , an @xmath6 error on the first qubit is the same as an @xmath6 error on the second qubit up to the application of the @xmath31 gauge operator .",
    "such an error can , therefore , be corrected by the application of an @xmath6 operator on either the first or second qubit . to simplify the analysis of errors , we refer to errors by which gauge operator that they are a part of , rather than by which data qubit they affect .",
    "an @xmath6 error on the first or second qubit is said to be an error on the _ first gauge _ in the @xmath6 basis , a @xmath7 error on the second or fourth qubit is said to be an error on the _ second gauge _ in the @xmath7 basis , and so on .",
    "when there is an error on one gauge and the syndrome is misinterpreted so that the correction operation is applied to the other gauge , the result is always a logical error .    in order to reliably correct errors using the [ [ 4,1,2 ] ] code , extra information is required to distinguish between errors affecting the first and second gauges .",
    "one method for obtaining this information is to pass classical messages describing the relative probabilities of errors from lower levels of error correction to higher levels .",
    "previously , it was shown that a message passing scheme such as this could increase the minimum number of errors that cause failure to a number that scales according to the fibonacci series with increasing levels of concatenation  @xcite .",
    "in contrast , the scaling achieved by our method is given by @xmath32 where @xmath1 is the number of levels of error correction .",
    "it is worth noting that we have chosen to use [ [ 4,1,2 ] ] code for several reasons .",
    "since it is a small code we can simulate more levels of error correction than if we had used a higher distance error - correction code . as our method involves an analysis of combinations of errors that can occur at every circuit location , it is easier to understand and test the operation of the method on a code that requires only small circuits with relatively few locations . finally , a similar code forms the basis of the post - selection scheme of knill , where ancillary states are prepared and verified before they are used to enact error correction by teleportation .",
    "the advantages of this scheme include a high threshold and fast logical gates , though unfortunately these come at a high resource cost .",
    "our method might be used to improve the performance and overhead of the scheme .",
    "for the [ [ 4,1,2 ] ] code , using the circuits we have chosen , there are five unique effects that can result from a single error .",
    "therefore there are five flags that we are required to track for each logical qubit at each level of error correction .",
    "these flags are defined in table i. note that two of the flags are transitive .",
    "the set of error locations belonging to each flag are shown in fig .",
    "[ fig : error map ] .",
    ".names assigned to the flags of the [ [ 4,1,2 ] ] code .",
    "there are five flags for each basis .",
    "_ first gauge _ refers to an error on either of the two qubits involved in the first gauge operator of the appropriate basis in eq .",
    "[ eq : gauge ] , similarly for _ second gauge_. _ ancilla _ refers to an error which flips the parity of the two gauge operator measurements @xmath11 that is , an error which changes the outcome of the syndrome measurement .",
    "flags @xmath33 and @xmath34 become @xmath35 and @xmath36 respectively at the end of the error - correction cycle as they will be seen on the ancilla of the following error - correction cycle . [ cols=\"^,^\",options=\"header \" , ]      our simulations are of a @xmath10 extended rectangle with concatenated error correction , including all physical locations .",
    "the message - passing scheme is simulated along side the concatenated error correction circuit so that it operates in the same way as it would in a real quantum computer .",
    "the logical @xmath10 extended rectangle is defined to succeed only if both logical qubits would , after decoding , produce the correct measurement results in both the @xmath6 and @xmath7 bases .",
    "the simulations use the method outlined in  @xcite , which involves keeping track of discrete pauli errors as they propagate through a circuit .",
    "we assume a stochastic error model , where memory , initialization , readout , single - qubit gate , and two - qubit gate errors are all equally likely to occur .",
    "errors are randomly selected one- and two - qubit discrete pauli errors .",
    "our simulator uses the simd - oriented mersenne twister pseudo random number generator  @xcite .",
    "we use the error - correction circuit shown in fig .",
    "[ fig : error map ] , which requires two ancilla qubits per logical qubit for a total block size of six qubits .",
    "we are primarily interested in the logical failure rate when the physical error rate , @xmath5 , is low . directly simulating the circuit in the low-@xmath5 limit",
    "requires too many runs to obtain statistically significant results .",
    "instead , we use the expansion of logical failure rate with respect to the physical error rate , @xmath37 where @xmath38 is the number of physical locations in the error - correction circuit , @xmath5 is the physical error rate , and @xmath39 is the probability of logical failure after error correction given exactly @xmath40 errors . to estimate values of @xmath39",
    "we simulate the circuit with exactly @xmath40 errors placed randomly , repeating many times for each @xmath40 .",
    "we approximate @xmath41 by truncating the series above @xmath42 .",
    "this approximation breaks down for values of @xmath5 for which the probability of having more than 29 errors becomes significant .",
    "the failure rate of the logical @xmath10 as a function of physical error rate is plotted in fig .",
    "[ fig : results ] for one , two , and three levels of error - correction . in the low-@xmath5 region",
    ", the gradient of each line on the log - log plot signifies the minimum number of errors that are required for the circuit to fail : one , two , and four errors for one , two , and three levels of error correction respectively .",
    "as described , our scheme assumes that each physical location is equally likely to fail . however , this need not be the case",
    ". relative probabilities of failure for individual physical locations are naturally represented in our scheme by the confidences at the physical level .",
    "that is , rather than setting the confidence of all physical locations to one , we can choose to assign different confidence ratings to each type of operation or each individual physical device . these confidence ratings could be based on data obtained by characterization of the gates .",
    "alternatively , data collected during error correction could be fed back to dynamically update physical confidence ratings in a similar way to how our method updates logical confidence ratings .",
    "our message - passing method could be applied to a post - selection based scheme instead of the fibonacci scheme .",
    "there are several ways this could be done , since we are free to choose the confidence threshold which determines whether ancillary states are accepted or rejected . to only accept states that have the maximum confidence would reduce the scheme to ordinary post - selection , as a state would be rejected if any errors are detected at any level regardless of whether or not these errors are correctable at higher levels . accepting all states that have a confidence greater than one would result in a high acceptance rate , but would not be optimal with respect to logical fidelity . in any case , the confidence of the states that are accepted could be used in higher - level error correction following post selection .",
    "there may also be some advantage to using the logical confidence ratings outputted by the highest level of error correction in some quantum algorithms .",
    "our method can be applied to higher distance bacon - shor codes and perhaps even to other css stabilizer codes .",
    "it would be interesting to investigate the pros and cons of using different codes with our method and also to rigorously establish the specific requirements on codes and circuits that enable us to achieve optimality with respect to the minimum number of errors that cause the code to fail ."
  ],
  "abstract_text": [
    "<S> here we present a new approach to concatenated quantum error correction in which additional classical processing is used with existing quantum codes and fault - tolerant circuits to more reliably correct errors . using this new scheme we are able to guarantee correction of a number of discrete errors up to half of the distance of the concatenated code . </S>",
    "<S> we simulate the application of the scheme to the [ [ 4,1,2 ] ] subsystem code with up to three levels of error correction . </S>"
  ]
}