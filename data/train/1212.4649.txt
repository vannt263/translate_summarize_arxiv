{
  "article_text": [
    "consider the problem of conveying the value of a parameter @xmath4 across a given discrete memoryless channel @xmath5 where @xmath6 and @xmath7 are the channel input and output vectors , respectively .",
    "our main interest , in this work , is in the following questions : how well can one estimate @xmath4 based on @xmath8 when one is allowed to optimize , not only the estimator , but also the modulator , that is , the function @xmath9 that maps @xmath4 into a channel input vector ?",
    "how fast does the estimation error decay as a function of @xmath10 when the best modulator and estimator are used ?    in principle , this problem , which is the discrete  time analogue of the classical problem of `` waveform communication '' ( in the terminology of ( * ? ? ?",
    "8) ) , can be viewed both from the information  theoretic and the estimation ",
    "theoretic perspectives .",
    "classical results in neither of these disciplines , however , seem to suggest satisfactory answers .    from the information ",
    "theoretic point of view , if the parameter is random , call it @xmath0 , this is actually a problem of joint source  channel coding , where the source emits a single variable @xmath0 ( or a fixed number of them when @xmath0 is a vector ) , whereas the channel is allowed to be used many times ( @xmath10 is large ) .",
    "the separation theorem of classical information theory asserts that asymptotic optimality of separate source and channel coding is guaranteed in the limit of long blocks .",
    "however , it refers to a regime of long blocks both in source coding and channel coding , whereas here the source block length is 1 , and so , there is no hope to compress the source with performance that comes close to the rate  distortion function .    in the realm of estimation theory ,",
    "on the other hand , there is a rich literature on bayesian and non ",
    "bayesian bounds , mostly concerning the mean square error ( mse ) in estimating parameters from signals corrupted by an additive white gaussian noise ( awgn ) channel , as well as other channels ( see , e.g. , @xcite and the introductions of @xcite , @xcite , and @xcite for overviews on these bounds ) .",
    "most of these bounds lend themselves to calculation for a _ given _ modulator @xmath11 and therefore they may give insights concerning optimum estimation for this specific modulator .",
    "they may not , however , be easy to use for the derivation of _ universal _ lower bounds , namely , lower bounds that depend neither on the modulator nor on the estimator , which are relevant when both optimum modulators and optimum estimators are sought .",
    "two exceptions to this rule ( although usually , not presented as such ) are families of bounds that stem from generalized data processing theorems ( dpt s ) @xcite , @xcite , @xcite , @xcite , @xcite , henceforth referred to as `` dpt bounds '' , and bounds based on hypothesis testing and channel coding considerations @xcite , @xcite , @xcite , henceforth called `` channel  coding bounds . ''    in this paper , we use both the channel  coding techniques and dpt techniques in order to derive lower bounds on general moments of the estimation error , @xmath1 , where @xmath0 is a random parameter , @xmath12 is its estimate , and the power @xmath3 is an arbitrary positive real ( not necessarily an integer ) .",
    "it turns out that when @xmath11 is subjected to optimization , @xmath1 can decay exponentially rapidly as a function of @xmath10 , and so , our focus is on the best achievable exponential rate of decay as a function of @xmath3 , which we shall denote by @xmath13 , that is , @xmath14 where the infimum is over all modulators and estimators .",
    "interestingly , both the upper and lower bounds on @xmath13 are intimately related to well  known exponential error bounds associated with channel coding , such as gallager s random coding exponent ( for small values of @xmath3 ) and the expurgated exponent function ( for large values of @xmath3 ) .",
    "in other words , we establish an estimation  theoretic meaning to these error exponent functions .",
    "in particular , under certain conditions , our channel  coding upper bound on @xmath13 ( corresponding to a lower bound on @xmath1 ) can be presented as @xmath15 where @xmath16 , @xmath17 being gallager s function , @xmath18 is the expurgated exponent at zero rate , and @xmath19 is value of @xmath3 for which @xmath20 ( so that @xmath21 is continuous ) .",
    "in addition , we derive a dpt bound and discuss its advantages and disadvantages compared to the above bound .    we also suggest a lower bound , @xmath22 , on @xmath13 ( associated with upper bounds on @xmath23 ) , which is achieved by a simple , separation  based modulation and estimation scheme . while there is a certain gap between @xmath21 and @xmath22 for every finite @xmath3 , it turns out that this gap disappears ( in the sense that the ratio @xmath24 tends to unity ) both for large @xmath3 and for small @xmath3 , and so , we have exact asymptotics of @xmath13 in these two extremes : for large @xmath3 , @xmath13 tends to @xmath18 and for small @xmath3 , @xmath25 , where @xmath26 is the channel capacity .",
    "our simple achievability scheme is then nearly optimum at both extremes , which means that a separation theorem essentially holds for very small and for very large values of @xmath3 , in spite of the earlier discussion ( see also ( * ? ? ? * section iii.d ) ) .",
    "the results are demonstrated for the example of a `` very noisy channel , '' ( * ? ? ?",
    "* example 3 , pp .",
    "147149 ) , @xcite , which is convenient to analyze , as it admits closed  form expressions .    finally , we suggest an extension of our results to the case of a multidimensional parameter vector @xmath27 .",
    "it turns out that the effect of the dimension @xmath28 is in reducing the effective value of @xmath3 by a factor of @xmath28 . in other words , @xmath21 is replaced by @xmath29 and the extension of the achievability result is straightforward .",
    "this means that for fixed @xmath3 , the limit of large @xmath28 ( where the effective value @xmath30 is very small ) also admits exact asymptotics , where @xmath31 .",
    "the outline of the paper is as follows . in section 2",
    ", we define the problem formally and we establish notation conventions . in section 3 , we derive our main upper and lower bounds based on channel coding considerations . in section 4 ,",
    "we derive our dpt bound and discuss it .",
    "section 5 is devoted to the example of the very noisy channel , and finally , in section 6 the multidimensional case is considered .",
    "throughout this paper , scalar random variables ( rv s ) will be denoted by capital letters , their sample values will be denoted by the respective lower case letters , and their alphabets will be denoted by the respective calligraphic letters .",
    "a similar convention will apply to random vectors and their sample values which will be denoted with same symbols in a bold face font . for example",
    ", @xmath32 is a realization of a random variable @xmath33 , whereas @xmath34 ( @xmath10 being a positive integer and @xmath35 being the @xmath10th cartesian power of @xmath36 ) is a realization of a random vector @xmath37 .",
    "let @xmath0 be a uniformly distributed and its support is made for convenience only .",
    "our results extend to more general densities . ]",
    "random variable over the interval @xmath38 $ ] , which we will also denote by @xmath39 .",
    "we refer to @xmath0 as the parameter to be conveyed from the source to the destination , via a given noisy channel . a given realization of @xmath0 will be denoted by @xmath4 .    a discrete memoryless channel ( dmc )",
    "is characterized by a matrix of conditional probabilities @xmath40 , where the channel input and output alphabets , @xmath41 and @xmath36 , are assumed finite . when a dmc @xmath40 is fed by an input vector @xmath42",
    ", it produces an output vector @xmath43 according to @xmath44 a modulator is a measurable mapping @xmath45 from @xmath46 $ ] to @xmath47 and an estimator is a mapping @xmath48 from @xmath35 back to @xmath39 .",
    "the random vector @xmath49 will also be denoted by @xmath50 .",
    "similarly , the random variable @xmath51 will also be denoted by @xmath12 .",
    "our basic figure of merit for communication systems is the expectation of @xmath3th power of the estimation error , i.e. , @xmath52 , where @xmath3 is a positive real ( not necessarily an integer ) and @xmath53 is the expectation operator with respect to ( w.r.t . )",
    "the randomness of @xmath0 and @xmath54 .",
    "the capability of attaining an exponential decay in @xmath52 by certain choices of a modulator @xmath55 and an estimator @xmath56 , motivates the definition of the following exponential rates @xmath57\\ ] ] and @xmath58.\\ ] ] this paper is basically about the derivation of upper bounds on @xmath59 and lower bounds on @xmath60 , with special interest in situations where these upper and lower bounds come close to each other .",
    "our first theorem ( see appendix a for the proof ) asserts that @xmath21 is an upper bound on the best achievable exponential decay rate of @xmath3th moment of the estimation error .",
    "let @xmath0 be uniformly distributed over @xmath46 $ ] and let @xmath78 be a given dmc .",
    "then , for every @xmath2 @xmath79    we now proceed to present a lower bound @xmath22 to @xmath60 .",
    "let @xmath80 be the smallest @xmath81 such that @xmath82 is attained with @xmath83 and let @xmath84 denote the largest @xmath81 such that @xmath85\\ ] ] is attained for @xmath83 .",
    ", @xmath86 , with @xmath87 , and @xmath88 , where @xmath89 @xcite . ]",
    "next , define @xmath90 and finally , @xmath91 our next theorem ( see appendix b for the proof ) tells us that @xmath22 is a lower bound on the best attainable exponential decay rate of @xmath52 .",
    "let @xmath0 be uniformly distributed over @xmath46 $ ] and let @xmath78 be a given dmc .",
    "then , for every @xmath2 @xmath92    the derivations of both @xmath21 and @xmath22 rely on channel coding considerations .",
    "in particular , the derivation of @xmath21 builds strongly on the method of @xcite , which extends the derivation of the ziv  zakai bound @xcite and the chazan ",
    "ziv bound @xcite . while the two latter bounds are based on considerations associated with binary hypotheses testing , here and in @xcite , the general idea is extended to exponentially many hypotheses pertaining to channel decoding .",
    "we see that both bounds exhibit different types of behavior in different ranges of @xmath3 ( i.e. , `` phase transitions '' ) , but in a different manner . for both @xmath21 and @xmath22 the behavior",
    "is related to the ordinary gallager function in some range of small @xmath3 , and to the expurgated exponent in a certain range of large @xmath3 .",
    "as can be seen in the proof of theorem 2 ( appendix b ) , the communication system that achieves @xmath22 works as follows ( see also @xcite , @xcite ) : define @xmath93 construct a uniform grid of @xmath94 evenly spaced points along @xmath39 , denoted @xmath95 .",
    "if @xmath96 assign to each grid point @xmath97 a codeword of a code of rate @xmath98 that achieves the expurgated exponent @xmath99 $ ] ( see ( * ? ? ?",
    "* theorem 5.7.1 ) or ( * ? ? ?",
    "* theorem 3.3.1 ) ) .",
    "if @xmath100 , do the same with a code that achieves @xmath101 $ ] ( see ( * ? ? ?",
    "* , corollary 1 ) or ( * ? ? ? * theorem 3.2.1 ) ) . given @xmath4 ,",
    "let @xmath102 be the codeword @xmath103 that is assigned to the grid point @xmath97 , which is closest to @xmath4 .",
    "given @xmath8 , let @xmath104 be the grid point @xmath105 that corresponds to the codeword @xmath106 that has been decoded based on @xmath8 using the ml decoder for the given dmc .",
    "let us examine the behavior of these bounds as @xmath107 and as @xmath108 .",
    "for very large values of @xmath109 , where the upper bound @xmath21 is obviously given by @xmath18 , the lower bound is given by @xmath110 which means that for large @xmath3 all the exponents asymptotically coincide : @xmath111 in the achievability scheme described above , @xmath98 is a very low coding rate . on the other hand , for very small values of @xmath3 , where @xmath112 , @xmath26 being the channel capacity , we have @xmath113 which means that for small @xmath3 all the exponents behave like @xmath114 , i.e. , @xmath115 it is then interesting to observe that not only channel  coding error exponents , but also channel capacity plays a role in the characterization of the best achievable modulation  estimation performance . in the achievability scheme described above",
    ", @xmath98 is a very high coding rate , very close to the capacity @xmath26 .",
    "we next derive an alternative upper bound on @xmath59 that is based on generalized data processing inequalities , following ziv and zakai @xcite and zakai and ziv @xcite .",
    "the idea behind these works is that it is possible to define generalized mutual information functionals satisfying a dpt , by replacing the negative logarithm function of the ordinary mutual information , by a general convex function .",
    "this enables to obtain tighter distortion bounds for communication systems with short block length .    in @xcite",
    "it was shown that the following generalized mutual information functional , between two generic random variables , @xmath116 and @xmath117 , admits a dpt for every positive integer @xmath118 and for every vector @xmath119 whose components are non  negative and sum to unity : @xmath120 in particular , since @xmath121 is a markov chain , then by the generalized dpt , @xmath122 the idea is to further upper bound @xmath123 and to further lower bound @xmath124 subject to the constraint @xmath125 , which leads to a generalized rate  distortion function , and thereby to obtain an inequality on @xmath1 . specifically , @xmath123 is upper bounded as follows : @xmath126_t)^{\\alpha_i}\\\\ & = & -\\prod_{t=1}^n\\sum_{y\\in{{\\cal y}}}\\prod_{i=1}^k\\int_{-1/2}^{+1/2 } \\mbox{d}u_ip(y_t|[f_n(u_i)]_t)^{\\alpha_i}\\\\ & \\le&-\\min_q\\prod_{t=1}^n\\sum_{y\\in{{\\cal y}}}\\prod_{i=1}^k\\sum_{x_i\\in{{\\cal x}}}q(x_i ) p(y_t|x_i)^{\\alpha_i}\\\\ & = & -\\min_q\\left[\\sum_{y\\in{{\\cal y}}}\\prod_{i=1}^k\\sum_{x_i\\in{{\\cal x}}}q(x_i ) p(y|x_i)^{\\alpha_i}\\right]^n\\\\ & = & -\\exp\\{-n\\max_qe(\\alpha_1,\\ldots,\\alpha_k , q)\\},\\end{aligned}\\ ] ] where @xmath127_t$ ] denotes the @xmath128th component of the vector @xmath129 and where @xmath130.\\ ] ] note that for @xmath131 ( @xmath73  integer ) , @xmath132 in appendix c we show that @xmath133 where @xmath134 is a constant that depends solely on @xmath3 , @xmath118 and @xmath135 , and where @xmath136 the function @xmath137 in eq .",
    "( [ rd ] ) is referred to as a `` generalized rate",
    " distortion function '' in the terminology of @xcite and @xcite .",
    "thus , from the generalized dpt , @xmath138 where @xmath139 is another constant and @xmath140 as an example , assume that the channel is such that the function @xmath141 is concave , so that @xmath142 .",
    "in this case , @xmath143 since @xmath144 and @xmath141 is monotonically increasing . now , let @xmath145 be an integer ( for example , @xmath146 is always a legitimate choice ) .",
    "then , @xmath147 thus , at least in this case , the dpt bound is guaranteed to be no worse than the channel  coding bound @xmath21 .",
    "nonetheless , in our numerical studies , we have not found an example where the dpt bound strictly improves on the channel  coding bound , i.e. , @xmath148 , and it remains an open question whether the dpt bound can offer improvement in any situation , thanks to its additional degrees of freedom .",
    "it should be pointed out that the vector @xmath119 that achieves @xmath149 is not always given by @xmath150 because the function @xmath151 is not convex in @xmath119 . at any rate , in all cases where the two bounds are equivalent , namely , @xmath152 ,",
    "this is interesting on its own right since the two bounds are obtained by two different techniques that are based on completely different considerations .",
    "one advantage of the dpt approach is that it seems to lend itself more comfortably to extensions that account for moments of more general functions of the estimation error , i.e. , @xmath153 , for a large class of monotonically increasing functions @xmath154 . on the other hand ,",
    "the optimization associated with calculation of the dpt bound is not trivial .",
    "( solid curve ) and the lower bound @xmath169 ( dashed curve ) for the example of the very noisy channel.,width=321,height=321 ]    as for the dpt bound , we have the following approximate analysis : @xmath170\\\\ & = & \\inf_q\\sum_{y\\in{{\\cal y}}}\\prod_{i=1}^k\\left\\{p(y)^{\\alpha_i } \\left[\\sum_{x_i\\in{{\\cal x}}}q(x_i)[1+\\epsilon(x_i , y)]^{\\alpha_i}\\right]\\right\\}\\\\ & = & \\inf_q\\sum_{y\\in{{\\cal y}}}p(y)\\prod_{i=1}^k\\left[\\sum_{x_i\\in{{\\cal x } } } q(x_i)[1+\\epsilon(x_i , y)]^{\\alpha_i}\\right]\\\\ & \\approx&\\inf_q \\sum_{y\\in{{\\cal y}}}p(y)\\prod_{i=1}^k\\left(\\sum_{x_i\\in{{\\cal x}}}q(x_i)\\left[1+\\alpha_i\\epsilon(x_i , y)- \\frac{1}{2}\\alpha_i(1-\\alpha_i)\\epsilon^2(x_i , y)\\right]\\right)\\\\ & = & \\inf_q\\sum_{y\\in{{\\cal y}}}p(y)\\prod_{i=1}^k\\left[1- \\frac{1}{2}\\alpha_i(1-\\alpha_i)\\sum_{x_i\\in{{\\cal x}}}q(x_i)\\epsilon^2(x_i , y)\\right]\\\\ & \\approx&\\inf_q\\sum_{y\\in{{\\cal y}}}p(y)\\left[1- \\frac{1}{2}\\sum_{i=1}^k\\alpha_i(1-\\alpha_i)\\sum_{x_i\\in{{\\cal x}}}q(x_i)\\epsilon^2(x_i , y)\\right]\\\\ & = & 1- \\frac{1}{2}\\sum_{i=1}^k\\alpha_i(1-\\alpha_i)\\sup_q\\sum_{x_i\\in{{\\cal x } } } \\sum_{y\\in{{\\cal y}}}q(x_i)p(y)\\epsilon^2(x_i , y)\\\\ & \\approx & 1-c\\sum_{i=1}^k\\alpha_i(1-\\alpha_i)\\\\ & = & 1-c\\left(1-\\sum_{i=1}^k\\alpha_i^2\\right).\\end{aligned}\\ ] ] where in the fifth line , we have used the identity @xmath171 for all @xmath172 with @xmath173 ( * ? ? ?",
    "* , eq .  ( 3.4.28 ) ) .",
    "thus , @xmath174 \\approx c\\left(1-\\sum_{i=1}^k\\alpha_i^2\\right),\\ ] ] and then @xmath175 the very same expressions are obtained for the continuous  time awgn channel with unlimited bandwidth , where @xmath176 , @xmath177 being the signal power and @xmath178 being the one  sided noise spectral density . for @xmath146 and @xmath179 , we have @xmath180 : @xmath181 which agrees with @xmath21 . for @xmath182 and @xmath179 ,",
    "the minimum is attained for @xmath183 , and the result is @xmath184 .",
    "however for @xmath185 , the bound improves to @xmath186 .",
    "consider now the case of a parameter vector @xmath187 , uniformly distributed across the unit hypercube @xmath188^d$ ] .",
    "a reasonable figure of merit in this case would be a linear combination of @xmath189 , @xmath190 .",
    "since each one of these terms is exponential in @xmath10 , it makes sense to let the coefficients of this linear combination also be exponential functions of @xmath10 , as otherwise , the results will be exponentially insensitive to the choice of the coefficients .",
    "this means that we consider the criterion @xmath191 where , without loss of generality , we take @xmath192 , @xmath193 .    the derivation below is an extension of the derivation of the channel coding bound , given in appendix a for the case @xmath194 .",
    "therefore , a reader who is interested in the details is advised to read appendix a first , or otherwise to skip directly to the final result in eq.([endresult ] ) and the discussion that follows .",
    "let us define @xmath195 for some constant @xmath196 .",
    "consider the following chain of inequalities : @xmath197\\right)\\right\\},\\end{aligned}\\ ] ] where the second line follows from chebychev s inequality , the fifth line follows from the union bound , and the last line follows from the same arguments as in ( * ? ? ?",
    "* sect.iv.a ) . maximizing over @xmath198",
    ", we get @xmath199\\right)\\right]\\right\\}.\\ ] ] defining @xmath200 , @xmath201 and @xmath202 , the above minimization at the exponent becomes equivalent to @xmath203\\\\ & = & \\min_{r\\ge r_{\\min}}\\left[\\frac{\\rho}{d}\\cdot r+e_{sl}(r)\\right]-\\rho\\bar{r}\\\\ & = & \\left\\{\\begin{array}{ll } e_{sp}(r_{\\rho / d})+\\frac{\\rho}{d}(r_{\\rho / d}-r_{\\min } ) & \\rho / d \\le \\rho_0\\\\ e_{ex}(0)-\\rho_0 r_{\\min } & \\rho / d > \\rho_0 \\end{array}\\right.\\end{aligned}\\ ] ] where @xmath204 is defined as the achiever of @xmath205 $ ] .",
    "thus , the extension of the channel  coding bound to the @xmath28dimensional case reads @xmath206 we see that when @xmath207 for all @xmath208 ( i.e. , all weights are 1 ) , it is the same channel  coding bound as before , except that @xmath3 is replaced by @xmath30 , that is , @xmath29 . for @xmath108 , the bound tends to @xmath18 , which can be approached again by a low  rate code for a cartesian grid in the parameter space . at the other extreme , when @xmath28 is very large compared to @xmath3 ,",
    "so @xmath30 is small , construct a grid of @xmath209 , quantize @xmath210 and assign to each grid point a codeword of a typical random code at rate @xmath211 .",
    "then the performance will be about @xmath212 .",
    "therefore , as a corollary of the above result , we have @xmath213}.\\ ] ]",
    "_ proof of theorem 1 . _",
    "we begin by using the markov / chebychev inequality : @xmath214 next we need to further lower bound @xmath215 and then maximize the r.h.s .  over @xmath216 .",
    "equivalently , similarly as in @xcite , we may set @xmath217 in the r.h.s .  and maximize the bound w.r.t .",
    "let @xmath218 be the reliability function of the channel .",
    "then , similarly in @xcite is replaced by the block length @xmath10 and the reliability function of the awgn channel is replaced by that of the dmc considered here . ] as in ( * ? ? ?",
    "* theorem 1 ) , we have : @xmath219}\\ ] ] and so , @xmath220}=e^{-n[\\rho r+e(r)+o(n)]}.\\ ] ] the best , with @xmath221}$ ] , would yield , after saddle  point integration , exactly the same exponential order as presented above .",
    "the weak link here is , therefore , not the chebychev inequality but the fact that there is no apparent single estimator , independent of @xmath81 , that minimizes @xmath222 uniformly for all @xmath81 . ]",
    "lower bound is obtained by maximizing the r.h.s .  over @xmath81 , yielding @xmath223}\\nonumber\\\\ & \\ge&e^{-n\\min_{r\\ge 0}[\\rho r+e_{sl}(r)+o(n)]}\\end{aligned}\\ ] ] where @xmath224 is the exponent associated with the _ straight line bound _ , which is well known to be an upper bound on the reliability function @xmath218 @xcite , @xcite , ( * ? ? ?",
    "3.8 ) , and which is given by @xmath225 where @xmath226\\ ] ] is the _ sphere  packing exponent _ , @xmath19 is as defined in theorem 1 and @xmath227 is the rate @xmath81 at which @xmath228 , or equivalently , the solution to the equation @xmath229 .",
    "thus , according to the second line of eq .",
    "( [ lb ] ) , @xmath230.\\ ] ] for @xmath231 , the minimum is obviously attained at @xmath232 , and so , @xmath233 for @xmath234 , we use @xmath235\\le \\min_{r\\ge 0}[\\rho r+e_{sp}(r)].\\ ] ] the right  most side of eq .",
    "( [ last ] ) is the legendre ",
    "fenchel transform ( lft ) of @xmath236 , which in turn ( according to ( [ esp ] ) ) , is the lft of @xmath68 .",
    "thus , the right  most side of ( [ last ] ) is given by the uce of @xmath68 , which is @xmath66 .",
    "thus , @xmath237 this completes the proof of theorem 1 .",
    "_ proof of theorem 2 .",
    "_ define @xmath93 consider a grid of @xmath94 evenly spaced points along @xmath39 , denoted @xmath95 , where @xmath238 and @xmath239 ( see also ( * ? ? ? * theorem 2 ) ) .",
    "if @xmath96 , assign to each point @xmath97 a codeword of a code of rate @xmath98 that achieves the expurgated exponent @xmath99 $ ] .",
    "otherwise , do the same with a code that achieves @xmath101 $ ] ( see ( * ? ? ?",
    "* , corollary 1 ) or ( * ? ? ?",
    "* theorem 3.2.1 ) ) . given @xmath4 ,",
    "let @xmath102 be the codeword @xmath103 that is assigned to the grid point @xmath97 , which is closest to @xmath4 .",
    "given @xmath8 , let @xmath104 be the grid point @xmath105 that corresponds to the codeword @xmath106 that has been decoded based on @xmath8 using the ml decoder for the given dmc . for every @xmath240 , we have : @xmath241^\\rho\\cdot 1 + 1^\\rho\\cdot \\mbox{pr}\\{|\\hat{u}-u| > e^{-nr}\\}\\nonumber\\\\ & = & e^{-n\\rho",
    "r}+ \\mbox{pr}\\{|\\hat{u}-u| > e^{-nr}\\}.\\end{aligned}\\ ] ] now , it follows from the construction of the proposed scheme that if @xmath81 is the coding rate and the spacing between each two consecutive grid points is @xmath242 , then the event @xmath243 occurs iff the ml decoder errs .",
    "thus , @xmath244 is exactly the probability of decoding error .",
    "considering the case @xmath245 , this code is assumed to achieve the expurgated exponent , and so , this probability of error is upper bounded by @xmath246-o(n)\\}}$ ] . since @xmath247 is an increasing function of @xmath81 and @xmath82 is a decreasing function , the best choice of @xmath81 is the solution to the equation @xmath248 or , equivalently @xmath249.\\ ] ] below we show that the solution to this equation is given by @xmath250 and for this choice of @xmath81 , both exponents in the last line of ( [ ub ] ) are given by @xmath251 which is exactly the expression of @xmath22 in the range @xmath96 . in the range @xmath252 , exactly the same arguments hold , except that @xmath82 and @xmath253 and @xmath254 are replaced by @xmath255 , @xmath141 , and @xmath256 , respectively . in the intermediate range , the same line of arguments hold once again , with @xmath83 and @xmath257 .",
    "it remains to show that @xmath98 in ( [ sol ] ) solves equation ( [ equ ] ) for @xmath96 , and then similar arguments will follow for the two other ranges .",
    "let @xmath98 be defined as in ( [ sol ] ) and let @xmath258 be defined as the solution to ( [ equ ] ) .",
    "we wish to prove that @xmath259 . to this end",
    ", we will prove that both @xmath260 and @xmath261 .",
    "to prove the first inequality , let @xmath262 denote the achiever of @xmath263 $ ] .",
    "then , by definition of @xmath258 , we obviously have @xmath264-\\varrho[r'(\\rho)]r'(\\rho)\\ ] ] i.e. , @xmath265}{\\varrho[r'(\\rho)]+\\rho}\\le \\sup_{\\varrho\\ge 1}\\frac{e_x(\\varrho)}{\\varrho+\\rho}\\equiv r(\\rho).\\ ] ] to prove the second ( opposite ) inequality , let @xmath266 be the achiever of @xmath98 , that is , @xmath267}{\\varrho(\\rho)+\\rho},\\ ] ] or , equivalently , @xmath268-\\varrho(\\rho)r(\\rho).\\ ] ] but the l.h.s",
    ".  can not exceed @xmath269=e_{ex}[r(\\rho)]$ ] , and so , @xmath270.\\ ] ] now , as mentioned earlier , the function @xmath247 is increasing in @xmath81 whereas the function @xmath82 is decreasing .",
    "thus , the value of @xmath81 for which there is equality @xmath271 , which is @xmath258 , can not be smaller than any value of @xmath81 , for which @xmath272 , like @xmath98 . hence , @xmath273 .",
    "this completex the proof of theorem 2 .",
    "_ derivation of a lower bound on the generalized rate  distortion function .",
    "_ consider the minimization of the generalized mutual information @xmath274 similarly as in ( * ? ? ?",
    "iv , example 2 ) and @xcite , since we are dealing with an exponentially small estimation error level ( small distortion ) , then for reasons of convenience , we approximate our distortion measure @xmath275 ( @xmath276 ) by @xmath277 where @xmath278 @xmath279 being the fractional part of @xmath280 , that is , @xmath281 .",
    "the justification is that for very small distortion ( the high  resolution limit ) , the modulo 1 operation has a negligible effect , and hence @xmath282 becomes essentially equivalent to the original distortion measure @xmath275 .",
    "using the same reasoning as in ( * ? ? ?",
    "iv , example 2 ) and @xcite , there is no loss of optimality by confining attention to channels @xmath283 of the form @xmath284 with @xmath285 .",
    "thus , the minimization of @xmath124 reduces to the maximization of @xmath286^{\\alpha_i}\\ ] ] subject to the constraints @xmath287 this optimization problem is not trivial , but we can find an upper bound on @xmath288 in terms of @xmath289 for small @xmath289 .",
    "we begin with the following bound for each one of the factors of @xmath288 : @xmath290^{\\alpha_i}&= & \\int_{-1/2}^{+1/2}\\mbox{d}w\\cdot [ f(w)]^{\\alpha_i}\\cdot\\left(\\frac{|w|^\\rho+d}{|w|^\\rho+d}\\right)^{\\alpha_i}\\\\ & = & \\int_{-1/2}^{+1/2}\\mbox{d}w\\cdot [ f(w)(|w|^\\rho+d)]^{\\alpha_i}\\cdot\\left[\\frac{1}{(|w|^\\rho+d)^{\\theta_i}}\\right]^{1-\\alpha_i}\\\\ & \\le&\\left[\\int_{-1/2}^{+1/2}\\mbox{d}w\\cdot f(w)(|w|^\\rho+d)\\right]^{\\alpha_i}\\cdot \\left[\\int_{-1/2}^{+1/2}\\frac{\\mbox{d}w}{(|w|^\\rho+d)^{\\theta_i}}\\right]^{1-\\alpha_i}\\\\ & = & ( 2d)^{\\alpha_i}\\cdot \\left[\\int_{-1/2}^{+1/2}\\frac{\\mbox{d}w}{(|w|^\\rho+d)^{\\theta_i}}\\right]^{1-\\alpha_i}.\\end{aligned}\\ ] ] where @xmath291 and the third line follows from hlder s inequality .",
    "it remains to evaluate the integral @xmath292 to this end , we have to distinguish between the cases @xmath293 and @xmath294 ( the case @xmath295 can be solved separately or approached as a limit of @xmath296 from either side ) . for the case",
    "@xmath297 , letting @xmath298 we can easily bound @xmath299 as follows : @xmath300 for @xmath294 , we proceed as follows : @xmath301\\\\ & = & 2d^{1/\\rho-\\theta_i}\\left[1+\\frac{t^{1-\\rho\\theta_i}}{1-\\rho\\theta_i}\\bigg|_1^{1/(2d^{1/\\rho})}\\right]\\\\ & = & 2d^{1/\\rho-\\theta_i}\\left[1+\\frac{2^{\\rho\\theta_i-1}d^{\\theta_i-1/\\rho}-1}{1-\\rho\\theta_i}\\right]\\\\ & \\le&\\frac{2^{\\rho\\theta_i}}{1-\\rho\\theta_i}.\\end{aligned}\\ ] ] thus , defining @xmath302 , we have @xmath290^{\\alpha_i}&\\le & ( 2d)^{\\alpha_i}i^{1-\\alpha_i}\\\\ & \\le & c_i'\\cdot d^{\\zeta_\\rho(\\alpha_i)},\\end{aligned}\\ ] ] where the function @xmath303 is defined as in ( [ zeta ] ) .",
    "thus , @xmath304 where @xmath305 .",
    "finally , it follows that @xmath306 as claimed .",
    "n.  merhav , `` data processing inequalities based on a certain structured class of information measures with application to estimation theory , '' _ ieee trans .  inform .",
    "theory _ , vol .",
    "58 , no .  8 , pp.52875301 , august 2012"
  ],
  "abstract_text": [
    "<S> we consider the problem of modulation and estimation of a random parameter @xmath0 to be conveyed across a discrete memoryless channel . </S>",
    "<S> upper and lower bounds are derived for the best achievable exponential decay rate of a general moment of the estimation error , @xmath1 , @xmath2 , when both the modulator and the estimator are subjected to optimization . </S>",
    "<S> these exponential error bounds turn out to be intimately related to error exponents of channel coding and to channel capacity . while </S>",
    "<S> in general , there is some gap between the upper and the lower bound , they asymptotically coincide both for very small and for very large values of the moment power @xmath3 . </S>",
    "<S> this means that our achievability scheme , which is based on simple quantization of @xmath0 followed by channel coding , is nearly optimum in both limits . </S>",
    "<S> some additional properties of the bounds are discussed and demonstrated , and finally , an extension to the case of a multidimensional parameter vector is outlined , with the principal conclusion that our upper and lower bound asymptotically coincide also for a high dimensionality .    </S>",
    "<S> * index terms : * parameter estimation , modulation , discrete memoryless channels , error exponents , random coding , data processing theorem .    </S>",
    "<S> department of electrical engineering + technion - israel institute of technology + technion city , haifa 32000 , israel + e  mail : merhav@ee.technion.ac.il + </S>"
  ]
}