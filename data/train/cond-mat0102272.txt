{
  "article_text": [
    "it is now a little more than ten years since studies of the dynamics of supervised learning in artificial neural networks started appearing in the statistical physics literature .",
    "early theoretical studies focussed on on - line learning using complete training sets where the probability of the same example appearing twice during training was zero , e.g. @xcite .",
    "this work enabled the evaluation of properties like convergence speed , generalization ability and optimal learning rates .",
    "however , such studies were still significantly removed from real - world scenarios .",
    "the most serious restriction was that one had to assume the availability of an infinite amount of training date , homogeneously distributed over the input space .",
    "in a recent article @xcite it was shown that even for very simple inhomogenuity the generalization error is no longer self - averaging and deterministic .",
    "the issue of repeating examples during training is technically a much harder problem and has received much attention recently .",
    "most of the work has focussed on simple or linear learning rules @xcite or different kinds of approximations , such as fokker - planck approaches @xcite and gaussian local field distributions @xcite .",
    "exact work on non - linear learning rules has drawn heavily on techniques from the spin glass and disordered systems community ( for an early overview of these techniques see e.g. @xcite ) .",
    "the generating functional technique was used to study the dynamics of gibbs learning in a perceptron with binary weights in @xcite .",
    "a dynamical version of the cavity method was employed in @xcite to study gradient descent batch learning and the methods of dynamical replica theory were applied to the problem of on - line learning in @xcite .",
    "the on - line learning scenario in this last sequence of papers is the one that we study here , but in the present paper we adapt the generating functional method  la de dominicis to deal with on - line learning .",
    "this paper might be the first to present exact macroscopic equations for on - line learning of restricted training sets for non - linear learning rules which are not of a gradient - descent type .",
    "precise definitions will be given in section [ sec : definitions ] , but the general setup is the following .",
    "the examples presented to the student perceptron are @xmath0 dimensional vectors chosen with equal probability from a fixed training set @xmath2 .",
    "the number of examples in @xmath2 is @xmath3 . at each presentation",
    "the student is given the teacher s classification of the pattern .",
    "the student can then decide to change its ` program ' , represented by the @xmath0 dimensional vector @xmath4 , in order to resemble more the teacher s program @xmath5 .",
    "the random choice of a pattern from the training set makes the evolution of the student weight vector @xmath6 a stochastic process . in section [ sec : genfun ] we write down a generating function for all the possible paths of @xmath6 .",
    "this function can be averaged over all possible realizations of the training set @xmath2 ( a quenched disorder average ) . at that point",
    "we will take the limit @xmath0 to infinity , to find saddle - point equations for a set of five order parameters and their conjugates .",
    "the reader who is mainly interested in results can skip section [ sec : genfun ] and go directly to section [ sec : eff ] , where the equations are reduced to a single exact set of three equations involving the student autocorrelation @xmath7 , the student - teacher overlap @xmath8 and the student response function @xmath9 .",
    "this set gives a surprisingly simple and intuitive picture of the evolution of the order parameters and the distribution of the local fields .",
    "from that point it is easy to establish links with earlier work on infinite training sets , batch learning and linear learning rules .",
    "numerical evidence is presented , showing that the present theory is in very good agreement with the simulations .    in section",
    "[ sec : statstate ] , the stationary state of a student with constant weight decay is studied . for the stationary state",
    "one can split all relevant order parameters into persistent and non - persistent parts .",
    "if we keep only the persistent parts and the single - time non - persistent parts , we find a closed set of equations containing just four scalar order parameters .",
    "the procedure is inspired by a similar method applied to the solution of detailed balance spin glass dynamics , where it can be shown to be exact .",
    "although the numerical evidence certainly seems to suggest that the procedure yields the correct results , we can not proof this fact rigorously here . at the moment , it remains an interesting open question .",
    "we study on - line learning in a student perceptron characterized by a vector @xmath10 .",
    "the student classifies patterns @xmath11 according to @xmath12 .",
    "the student tries to learn the task set by the teacher @xmath13 with @xmath5 , i.e. we only consider linear separable classifications .",
    "the components of the weight vectors of teacher and student are assumed not to scale with @xmath0 .",
    "the set @xmath2 contains only @xmath3 examples , independently chosen with equal probability from @xmath14 .",
    "patterns will be labeled by the greek index @xmath15 . at each iteration",
    "each pattern is equally likely to be chosen for presentation to the student , independently of previous rounds . if at step @xmath16",
    ", pattern @xmath17 is presented to the learning student , the student s weight vector is slightly adjusted to converge to the desired classification according to a recipe of the general form : @xmath18 the speed of the evolution is set by the learning rate @xmath19 .",
    "the function @xmath20 is the learning rule .",
    "popular learning rules are e.g. @xmath21 where @xmath22 is the stepfunction , @xmath23 for @xmath24 and @xmath25 for @xmath26 .",
    "the first three learning rules are all linear in @xmath27 , while the last two only alter the student s weights when student and teacher disagree",
    ".    a theoretical study of perceptrons can be useful for predicting learning times , for evaluating different learning rules or for finding optimal learning rates .",
    "for this purpose one is not so much interested in predicting the specific microscopic realizations of @xmath6 over time , but rather in the number of errors the perceptron makes in the classification of the training set ( training error , @xmath28 ) and the number of errors in the classification of the complete set of examples @xmath14 ( generalization error , @xmath29 ) : @xmath30 given @xmath6 , the generalization error is independent of the training set .",
    "it is in fact a standard result in perceptron theory that this error is only dependent on the angle between student and teacher vector , i.e. the norm of @xmath6 and its overlap with @xmath31 .",
    "the random choice of a pattern @xmath17 makes it more convenient to go to a description of an ensemble of students with a distribution of weight vectors , @xmath33 , than to study the stochastic evolution of @xmath6 directly . in this setting",
    "we can study the ( moment ) generating function @xmath34 for iteration times up to @xmath35 : @xmath36   =   \\int \\!d\\bsigma\\ , p(\\bsigma(0),\\bsigma(1),\\ldots,\\bsigma(m ) )     \\e^{i \\sum_{m=0}^m \\bpsi(m)\\cdot\\bsigma(m)},\\ ] ] where @xmath37 is an integral over all possible paths the students could take .",
    "derivation of @xmath34 with respect to @xmath38 generates all moments of the distribution @xmath39 .",
    "the microscopic dynamics of weight vectors at time @xmath16 can be written in the general form @xmath40 , with the transition probabilities @xmath41 to disentangle the double @xmath42 dependence of the transition rates , we employ the integral representation of the dirac delta - function and introduce @xmath43    \\widehat{w}({\\widehat{\\bsigma}}| { \\ensuremath{\\mathbf{x}}}',{\\ensuremath{\\mathbf{y}}},{\\ensuremath{\\mathbf{w}}}),\\end{aligned}\\ ] ] where we introduced three shorthands , called local fields ( in analogy with spin systems ) @xmath44 and the fourier transform of the transition rate @xmath45 . for large @xmath0 , @xmath45 will be of order @xmath46 and will therefore factorize over the patterns @xmath47 ,     \\quad ( n\\rightarrow\\infty ) \\nonumber\\end{aligned}\\ ] ] we can now rewrite the generating function : @xmath48    =    \\int   d\\bsigma p_0(\\bsigma(0 ) ) \\prod_{m=0}^{m-1}w(\\bsigma(m+1)|\\bsigma(m ) )    \\\\    \\lo = \\ !    \\int\\!\\!\\ ! \\frac{d\\bsigma d{\\widehat{\\bsigma}}}{(2\\pi)^n } d{\\ensuremath{\\mathbf{x}}}d{\\ensuremath{\\mathbf{y}}}d{\\ensuremath{\\mathbf{w}}}p_0(\\bsigma(0 ) )     \\gamma[{\\ensuremath{\\mathbf{y}}},{\\ensuremath{\\mathbf{x}}},{\\ensuremath{\\mathbf{w}}},\\bsigma ] \\prod_m { \\ensuremath{\\widehat{w}}}({\\widehat{\\bsigma}}(m)|{\\ensuremath{\\mathbf{x}}}(m),{\\ensuremath{\\mathbf{y}}},{\\ensuremath{\\mathbf{w}}}(m ) )   \\\\",
    "\\times    \\prod_m    \\exp\\left[i     { \\widehat{\\bsigma}}(m)\\cdot(\\bsigma(m+1)-\\bsigma(m ) ) + i\\bpsi(m)\\cdot\\bsigma(m ) \\right]\\end{aligned}\\ ] ] where the appearance of the training examples is restricted to the function @xmath49 , given by : @xmath50    & = &    \\prod_\\mu \\delta\\left[y^\\mu-\\frac{\\btau\\cdot\\bxi^\\mu}{\\sqrt{n}}\\right ]     \\prod_m \\delta\\left[x^\\mu(m)-\\frac{\\bsigma(m)\\cdot\\bxi^\\mu}{\\sqrt{n}}\\right ]     \\delta\\left[w^\\mu(m)-\\frac{{\\widehat{\\bsigma}}(m)\\cdot\\bxi^\\mu}{\\sqrt{n}}\\right ] \\end{aligned}\\ ] ] in the thermodynamic limit @xmath51 ) , all the macroscopic observables in this model are self - averaging with respect to the realization of the training set . to avoid the difficulty of choosing a typical training set",
    ", we can thus safely consider the disorder averaged generating function @xmath52_{dis}$ ] .",
    "the only term involving the actual patterns is @xmath49 .",
    "the quenched disorder average of @xmath49 is @xmath53_{dis }    =      \\int d{\\ensuremath{\\widehat{\\mathbf{y}}}}d{\\ensuremath{\\widehat{\\mathbf{x}}}}d{\\ensuremath{\\widehat{\\mathbf{w}}}}\\prod_\\mu    \\exp\\left [      i { \\ensuremath{\\widehat{y}}}^\\mu y^\\mu + \\sum_m i { \\ensuremath{\\widehat{x}}}^\\mu(m)x^\\mu(m)+ \\sum_m i { \\ensuremath{\\widehat{w}}}^\\mu(m)w^\\mu(m )    \\right ]    \\\\    \\times 2^{-n}\\!\\!\\!\\!\\!\\!\\!\\sum_{\\bxi^\\mu \\in \\{\\pm1\\}^n}\\!\\!\\!\\!\\!\\!\\ !    \\exp\\frac{-\\rmi}{\\sqrt{n}}\\bxi^\\mu \\!\\cdot \\!\\left [       { \\ensuremath{\\widehat{y}}}^\\mu \\btau      + \\sum_m { \\ensuremath{\\widehat{x}}}^\\mu(m)\\bsigma(m )      + \\sum_m \\!{\\ensuremath{\\widehat{w}}}^\\mu(m){\\widehat{\\bsigma}}(m )    \\right]\\end{aligned}\\ ] ] of the term on the second line , only the quadratic terms in @xmath31 , @xmath6 and @xmath54 survive in the thermodynamic limit .",
    "near this limit we find that this term containing the training patterns becomes @xmath55,\\end{aligned}\\ ] ] in the thermodynamic limit .",
    "we assume that the initial probability distribution @xmath56 factorizes over sites .",
    "full factorization of the generating function over patterns and input channels can then be achieved if we introduce the following order parameters and their conjugates via delta - functions : @xmath57 when changing @xmath16 to @xmath58 , the expectation of these order parameters can only can by a value of order @xmath59 .",
    "we thus rescale the time as @xmath60 . from here",
    ", one could go to a continuous time description by introducing @xmath61 and taking the limit @xmath62 to zero , but we delay this step in order to avoid technical difficulties in evaluating the path integrals .",
    "the generating function attains a form suitable for saddle - point integration : @xmath63\\right]_{dis }    \\propto    \\int \\ldots    \\exp \\left[n ( \\psi + \\phi+\\omega ) \\right]\\ ] ] there are three distinct leading order contributions to the exponent .",
    "the first is a ` bookkeeping ' term , linking the order parameters to their conjugates : @xmath64\\ ] ] the second term reflects the coupled dynamics of the local fields : @xmath65 ,   \\nonumber\\end{aligned}\\ ] ] where we have added additional sources @xmath66 and @xmath67 to couple to @xmath68 and @xmath69 .",
    "these sources act as biases of teacher and student .",
    "the third term describes the evolution of the now decoupled weight components : @xmath70 \\nonumber\\end{aligned}\\ ] ] where @xmath71_{tt'}=\\delta_{t+1,t'}-\\delta_{tt'}$ ] and where we have included an external driving force @xmath72 in the system . with a modest amount of foresight",
    "we write @xmath73 . upon taking derivatives with respect to the generating fields @xmath74",
    ", we find _ at _ the relevant saddle - point : @xmath75_{dis } , \\\\    c_{tt'}=\\lim_{n\\rightarrow\\infty}\\frac{1}{n}\\sum_i       \\left[\\langle \\sigma_{i}(t)\\sigma_{i}(t')\\rangle \\right]_{dis } , \\\\",
    "g_{tt'}=\\lim_{n\\rightarrow\\infty}\\frac{1}{n}\\sum_i        \\frac{\\partial}{\\partial \\theta_i(t ' ) }       \\left[\\langle \\sigma_{i}(t ) \\rangle\\right]_{dis}\\end{aligned}\\ ] ] using the built - in normalisation @xmath76_{dis}$ ] , we also find @xmath77_{dis}=0 , \\\\",
    "c_{tt'}=\\lim_{n\\rightarrow\\infty}\\frac{1}{n}\\sum_i       \\frac{\\partial^2}{\\partial",
    "\\theta_{i}(t )   \\partial \\theta_{i}(t ' ) }       \\left[z(0)\\right]_{dis}=0\\end{aligned}\\ ] ] if we perform the saddle - point integration , we find in addition that @xmath78_{dis}=0 , \\\\    i{\\ensuremath{\\widehat{c}}}_{tt'}=- \\lim_{n\\rightarrow\\infty } \\frac{1}{n}\\sum_\\mu      \\frac{\\partial^2}{\\partial \\theta_{x}(t ) \\partial \\theta_{x}(t ' ) }      \\left[z(0)\\right]_{dis}=0.\\end{aligned}\\ ] ] at this point we can already simplify ( or remove altogether ) the generating fields @xmath79 and @xmath80 . the external fields @xmath81 and @xmath82 can be interpreted as biases or thresholds of the student and teacher , respectively . without loss of generality we may set @xmath83 .",
    "the evolution of the local fields and the weight vector are now linked only via the remaining non - zero order parameters .",
    "we proceed to evaluate the two separate processes at the saddle - point .",
    "focussing on the evaluation of the pattern average @xmath84 we find that the terms involving @xmath85 can be interpreted as averages over a poisson - distribution : @xmath86    \\\\",
    "=    \\sum_{k_t=0}^\\infty \\int \\frac{dw_t}{2\\pi } \\exp\\left [      i{\\ensuremath{\\widehat{w}}}_t w_t",
    "-i\\eta k_t w_t f(x_t , y ) - \\frac{\\delta}{\\alpha }    \\right ]    \\frac{1}{k_t!}\\left (      \\frac{\\delta}{\\alpha }    \\right)^{k_t }    \\\\    = \\sum_{k_t=0}^\\infty \\delta({\\ensuremath{\\widehat{w}}}_t-\\eta k_t f(x_t , y ) ) { \\ensuremath{\\mathbb{p}}}(k_t),\\end{aligned}\\ ] ] where @xmath87 is a poisson distribution with average @xmath88 . for @xmath89 , @xmath87 gives the probability that a specific pattern is presented @xmath90 times to the student in time interval @xmath62 .",
    "the saddle - point equations of the remaining non - zero order parameters are found to be : @xmath91 with the shorthand @xmath92 .",
    "the average @xmath93 is using the measure implied by equation ( [ eq : phi ] ) . performing",
    "the disorder average has turned the @xmath94 integral into a gaussian one . evaluating this",
    "integration yields : @xmath95    \\label{eq : phi2}\\\\   & &   \\exp\\left [      -\\frac{1}{2}(y-\\theta_y)^2 - \\frac{1}{2}{\\ensuremath{\\widehat{x}}}d { \\ensuremath{\\widehat{x}}}+i{\\ensuremath{\\widehat{x}}}\\cdot(x-\\theta_x- gf - r(y-\\theta_y ) )    \\right],\\nonumber\\end{aligned}\\ ] ] where we have introduced the student autocovariance @xmath96 .",
    "we note the operator identity @xmath97 , which in turn implies using ( [ eq : hrhchg ] ) that @xmath98      the saddle - point equations involving the weight vectors are : @xmath99 where @xmath100 is an average with the measure induced by ( [ eq : omega ] ) .",
    "this measure can be generated by the stochastic process : @xmath101 where @xmath102 is a gaussian noise with zero mean and covariance @xmath103 . from this process",
    ", we find a simple expression for @xmath104 ( upon setting @xmath105 ) : @xmath106 with the response , student - teacher overlap and student autocovariance given by @xmath107^{-1}\\!\\!\\ !",
    ", \\qquad    r = g{\\ensuremath{\\widehat{r}}},\\qquad    d = g\\lambda g^t\\ ] ]",
    "[ cols=\"^,^ \" , ]     samples of the @xmath108 statistics for @xmath109 and @xmath110 of @xmath111 as a function of @xmath112 are shown in figure [ fig : f]a for @xmath113 ( top ) and @xmath114 ( bottom ) .",
    "the width of the sloping segment is @xmath115 , while the size of @xmath116 determines the rounding at the edges .",
    "the value of @xmath117 corresponding to @xmath118 as a function of the gaussian disorder @xmath112 is drawn in figure [ fig : f]b .",
    "for @xmath119 positive and roughly @xmath120 , one has @xmath121 , whereas for @xmath122 one finds @xmath123 . for @xmath112 in the range @xmath124 ,",
    "we find @xmath125 . in this particular example",
    "( using the same values for the order parameters as the graphs shown in figure [ fig : localfield]c ) @xmath126 so that the gaussian measure confines @xmath112 close to the origin .",
    "thus the resulting local field distribution is distinctly non - gaussian as shown in figure [ fig : localfield]c .",
    "in this paper , we have studied the statics and dynamics of an ensemble of students learning on - line the classification of a large number of examples .",
    "this problem boils down to solving a large number of coupled stochastic difference equations , each corresponding to a single input channel .",
    "the situation is complicated by the existence of disorder in the form of the composition of the training set . using the generating function method",
    "we have transformed this markovian system of @xmath0 coupled equations in the limit of @xmath0 to infinity into an effective single pattern process .",
    "the price paid for this reduction is that the new process has noise which is correlated in time and the presence of a retarded self - interaction in the system , which make the dynamics non - markovian . in principle it is possible to calculate the evolution of the system analytically , but in general it will be impossible to pursue this after the very first few time steps .",
    "however , the process can be solved numerically up to arbitrary precision .",
    "our calculation provides a solid basis for the further analytical study of linear rules . for non - linear rules",
    "the importance of our exact macroscopic dynamical equations is mainly in the insight they can give into the behaviour of different learning rules and the possibility they create to study and solve stationary states of both on - line and batch , gradient and non - gradient learning . until now",
    ", the stationary states of these kinds of learning processes have only been directly accessable with tools from equilibrium statistical mechanics , requiring detailed balance .",
    "this confined analyses to batch gradient - descent learning .",
    "this restriction has now been lifted . from our macroscopic evolution equations",
    "we can extract the stationary state equations very easily if we assume time translation invariance and the absence of anomalous response .",
    "we have not yet addressed the issue where this is likely to hold for on - line learning . to reduce the time - dependent order parameters like the student - autocorrelation and the student - response to a finite set of scalar order parameters , we apply a method we know from similar spin - glass problems based on the detachment of single - time and persistent order parameters from the non - persistent ones .",
    "the procedure consists of removing all non - persistent parts of the order parameters ( except for the single time quantities ) , retaining only a small closed set of equations containing just four ( q , r , d , g ) scalar macroscopic order parameters .",
    "whether this last procedure is indeed exact , remains to be seen and will be the subject of a future study , but the numerical evidence clearly suggests that the underlying assumption holds .",
    "24                                              c.w.h . mace and a.c.c .",
    "dynamics of supervised learning with restricted training sets and noisy teachers . in s.a .",
    "solla , t.k .",
    "leen , and k.  mller , editors , _ advances in neural information processing _ , volume  12 , page 237 . mit press , cambridge , ma , 2000 ."
  ],
  "abstract_text": [
    "<S> we study the dynamics of supervised on - line learning of realizable tasks in feed - forward neural networks . </S>",
    "<S> we focus on the regime where the number of examples used for training is proportional to the number of input channels @xmath0 . using generating function techniques from spin glass theory , we are able to average over the composition of the training set and transform the problem for @xmath1 to an effective single pattern system , described completely by the student autocovariance , the student - teacher overlap and the student response function , with exact closed equations . </S>",
    "<S> our method applies to arbitrary learning rules , i.e. not necessarily of a gradient - descent type . </S>",
    "<S> the resulting exact macroscopic dynamical equations can be integrated without finite - size effects up to any degree of accuracy , but their main value is in providing an exact and simple starting point for analytical approximation schemes . </S>",
    "<S> finally , we show how , in the region of absent anomalous response and using the hypothesis that ( as in detailed balance systems ) the short - time part of the various operators can be transformed away , one can describe the stationary state of the network succesfully by a set of coupled equations involving only four scalar order parameters . </S>"
  ]
}