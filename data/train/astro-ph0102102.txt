{
  "article_text": [
    "the last ten and the future ten years provide a large number of cmb ( cosmic microwave background ) experiments . their main goal is to estimate the cosmological parameters through sky temperature maps .",
    "the map - making is therefore a key question of these experiments , moreover with the advent of large time - ordered data ( tod ) being currently analysed or simulated .",
    "the `` brute force '' direct inversion of the linear map - making problem is beyond the reach of computing facilities of today .",
    "thus the data analysers need to find faster methods , which have to be as optimal in theory and as efficient in practice as possible .",
    "this is used to clean planck high frequency instrument ( bersanelli 1996 ) simulated data , to be published in giard ( 2000 ) .",
    "it uses a simple algorithm to destripe the data and make the map .",
    "the scanning strategy assumed to make the timeline is 1 round per minute of the beam on the sky , so that the timeline is a succession of circles on the sky .",
    "the cleaning algorithm follow the processing described below : first each circle is adjusted so that the measured signal fits a cosecant law ( galactic dust ) , plus dipole emission :    @xmath0    where k is the index of each circle of the data , i the data index along the circle , and @xmath1 the detector response .",
    "the free parameters are @xmath2 ( galactic emission for the circle k ) and @xmath3 ( constant to be subtracted ) . then we destripe the data using an algorithm derived from delabrouille ( 1998 ) which uses the scan intercepts .",
    "we adjust the constants to subtract to each circle k by minimising the spread between the measurements contributing to the same sky pixel .",
    "where a is the point - spread ( convolution ) matrix and n the random noise vector in the timeline . for cmb experiments , x would represent the pixelised sky map of the cmb temperature . the map - making problem is thus written as this :      where @xmath6 is the vector of the reconstructed sky map , and w the inversion matrix .",
    "the inversion matrix w depends on the method used .",
    "the simplest is the pixel averaging , but the optimal methods for estimating the map are the cobe method ( with no prior for x ) or the wiener filter ( with gaussian prior for x ) .",
    "see tegmark ( 1997 ) for a set of linear and non - linear map - making methods . as an example",
    ", the cobe method is written as this :          the cmb experiments now in analysis ( boomerang , maxima ... ) or to come ( archeops , map , planck ... ) provide several megabytes of time - ordered data .",
    "the noise covariance matrix , for instance , would be several terabytes of data , which is impossible to handle or even to write .",
    "how to solve optimally the map - making problem without writing any matrix anywhere ?    assuming no beam and the stationarity of the noise , it is possible to make the map iteratively handling only vectors . using the cobe method ( eq . [ cobe ] ) , the deconvolution equation ( eq . [ deconv_eq ] )",
    "can be written as this :      if the noise is stationary , the matrix multiplication @xmath10 y is equivalent to a convolution by a kernel , i.e. a multiplication in the fourier space .",
    "if the beam is not taken into account , a is making a timeline from a map , and @xmath11 is the pixel averaging of a timeline into a map .",
    "[ eqltm ] leads to iterative methods such as :      where @xmath13 is a free parameter of the iterative method .",
    "it is also possible to make converge the noise map instead of directly the sky map , by changing the variable @xmath14 into @xmath15^{-1 } a^t y$ ] .",
    "bersanelli , m. , bouchet , f. , efstathiou , g. , griffin , m. , lamarre , j.m . ,",
    "mandolesi , r. , nogaard - nielsen , h. , pace , o. , polny , j. , puget , j.l . ,",
    "tauber , j. , vittorio , n. , volont , s. : 1996 , cobras / samba phase a report"
  ],
  "abstract_text": [
    "<S> we present in this article two different ways to make cmb maps in practice , from large timelines . </S>",
    "<S> one is to make a simple destriping , fitting the data and using the scan intercepts to remove the low frequency noise ( stripes ) . </S>",
    "<S> the second , optimal , is to resolve linearly the map - making problem , which in case of big timelines must be simplified and changed from matrices to vectors for the calculations . assuming few conditions on the noise , it is possible to make fast map - making tools . </S>"
  ]
}