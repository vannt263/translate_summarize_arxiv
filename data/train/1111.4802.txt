{
  "article_text": [
    "we consider the problem of finding the global maxima of a function @xmath1 , where @xmath2 is assumed bounded , using the _ expected improvement _  ( ei ) criterion @xcite .",
    "many examples in the literature show that the ei algorithm is particularly interesting for dealing with the optimization of functions which are expensive to evaluate , as is often the case in design and analysis of computer experiments @xcite .",
    "however , going from the general framework expressed in @xcite to an actual computer implementation is a difficult issue .",
    "the main idea of an ei - based algorithm is a bayesian one : @xmath0 is viewed as a sample path of a random process @xmath3 defined on @xmath4 . for the sake of tractability , it is generally assumed that @xmath3 has a gaussian process distribution conditionally to a parameter @xmath5 , which tunes the mean and covariance functions of the process .",
    "then , given a prior distribution  @xmath6 on  @xmath7 and some initial evaluation results @xmath8 at @xmath9 , an ( idealized ) ei algorithm constructs a sequence of evaluations points @xmath10 such that , for each @xmath11 , @xmath12 where @xmath13 stands for the posterior distribution of @xmath7 , conditional on the @xmath14-algebra @xmath15 generated by @xmath16 , and @xmath17 is the ei at @xmath18 given @xmath7 , with @xmath19 and @xmath20 the conditional expectation given @xmath15 and @xmath7 . in practice ,",
    "the computation of @xmath21 is easily carried out ( see @xcite ) but the answers to the following two questions will probably have a direct impact on the performance and applicability of a particular implementation : a )  how  to deal with the integral in @xmath22 ?",
    "b )  how  to deal with the maximization of @xmath22 at each step ?",
    "we can safely say that most implementations  including the popular ego algorithm @xcite  deal with the first issue by using an _ empirical bayes _ ( or _ plug - in _ ) approach , which consists in approximating  @xmath13 by a dirac mass at the maximum likelihood estimate of @xmath7 . a plug - in approach using maximum",
    "a posteriori estimation has been used in  @xcite ; _ fully bayesian _ methods are more difficult to implement ( see @xcite and references therein ) . regarding the optimization of @xmath23 at each step , several strategies have been proposed ( see , e.g. , @xcite ) .",
    "this article addresses both questions simultaneously , using a sequential monte carlo ( smc ) approach @xcite and taking particular care to control the numerical complexity of the algorithm .",
    "the main ideas are the following .",
    "first , as in  @xcite , a weighted sample @xmath24 from  @xmath13 is used to approximate  @xmath22 ;  that is , @xmath25 . besides , at each step  @xmath26 , we attach to each @xmath27 a ( small ) population of candidate evaluation points @xmath28 which is expected to cover promising regions for that particular value of @xmath7 and such that @xmath29 .",
    "at each step @xmath11 of the algorithm , our objective is to construct a set of weighted particles @xmath30 so that @xmath31 , with @xmath32 where @xmath33 denotes the lebesgue measure , @xmath34 , @xmath35 is a criterion that reflects the interest of evaluating at @xmath18 ( given @xmath7 and past evaluation results ) , and @xmath36 is a normalizing term . for instance , a relevant choice for @xmath37 is to consider the probability that @xmath3 exceeds @xmath38 at @xmath18 , at step @xmath26 .",
    "( note that we consider less @xmath7s than @xmath18s in @xmath39 to keep the numerical complexity of the algorithm low . )    to initialize the algorithm , generate a weighted sample @xmath40 @xmath41 from the distribution @xmath42 , using for instance importance sampling with @xmath6 as the instrumental distribution , and pick a density @xmath43 over @xmath44 ( the uniform density , for example ) .",
    "then , for each @xmath11 : + _ step 1 : demarginalize _  using  @xmath45 and  @xmath46 , construct a weighted sample  @xmath47 of the form  , with @xmath48 , @xmath49 , and @xmath50 .",
    "+ _ step 2 : evaluate _  evaluate  @xmath3 at @xmath51 .",
    "+ _ step 3 : reweight / resample / move _  construct  @xmath52 from  @xmath45 as in @xcite : reweight the @xmath27s using @xmath53 , resample ( e.g. , by multinomial resampling ) , and move the @xmath27s to get @xmath54s using an independant metropolis - hastings kernel .",
    "step 4 : forge @xmath55 _  form an estimate  @xmath55 of the second marginal of  @xmath56 from the weighted sample @xmath57 .",
    "hopefully , such a choice of  @xmath55 will provide a good instrumental density for the next demarginalization step .",
    "any ( parametric or non - parametric ) density estimator can be used , as long as it is easy to sample from ; in this paper , a tree - based histogram estimator is used .      * experiments . * preliminary numerical results , showing the relevance of a fully bayesian approach with respect to empirical bayes approach , have been provided in @xcite .",
    "the scope of these results , however , was limited by a rather simplistic implementation ( involving a quadrature approximation for  @xmath22 and a non - adaptive grid - based optimization for the choice of  @xmath58 ) .",
    "we present here some results that demonstrate the capability of our new smc - based algorithm to overcome these limitations .",
    "the experimental setup is as follows .",
    "we compare our smc - based algorithm , with @xmath59 , to an ei algorithm in which : 1 ) we fix @xmath7 ( at a `` good '' value obtained using maximum likelihood estimation on a large dataset ) ; 2 ) @xmath58 is obtained by exhaustive search on a fixed lhs of size @xmath60 . in both cases ,",
    "we consider a gaussian process @xmath3 with a constant but unknown mean function ( with a uniform distribution on @xmath61 ) and an anisotropic matrn covariance function with regularity parameter @xmath62 .",
    "moreover , for the smc approach , the variance parameter of the matrn covariance function is integrated out using a jeffreys prior and the range parameters are endowed with independent lognormal priors .    * results .",
    "* figures  [ fig : branin ] and  [ fig : hart6 ] show the average error over @xmath63 runs of both algorithms , for the branin function ( @xmath64 ) and the log - transformed hartmann  6 function ( @xmath65 ) . for the branin function ,",
    "the reference algorithm performs better on the first iterations , probably thanks to the `` hand - tuned '' parameters , but soon stalls due to its non - adaptive search strategy .",
    "our smc - based algorithm , however , quickly catches up and eventually overtakes the reference algorithm . on the hartmann  6 function",
    ", we observe that the reference algorithm always lags behind our new algorithm .",
    "j.  mockus , v.  tiesis , and a.  zilinskas .",
    "the application of bayesian methods for seeking the extremum . in l.",
    "dixon and g.  szego , editors , _ towards global optimization _ ,",
    "volume  2 , pages 117129 .",
    "elsevier , 1978 ."
  ],
  "abstract_text": [
    "<S> we consider the problem of optimizing a real - valued continuous function @xmath0 using a bayesian approach , where the evaluations of @xmath0 are chosen sequentially by combining prior information about @xmath0 , which is described by a random process model , and past evaluation results . </S>",
    "<S> the main difficulty with this approach is to be able to compute the posterior distributions of quantities of interest which are used to choose evaluation points . in this article </S>",
    "<S> , we decide to use a sequential monte carlo ( smc ) approach . </S>"
  ]
}