{
  "article_text": [
    "most real complex systems are made up or modeled by elementary objects that locally interact with each other .",
    "graphical models  @xcite are formed by variables linked to each other by stochastic relationships .",
    "they enable to model dependencies in possibly high - dimensional heterogeneous data and to capture uncertainty .",
    "graphical models have been applied in a wide range of areas like image analysis , speech recognition , bioinformatics , ecology to name a few . in real applications",
    "a large number of random variables with a complex dependency structure are involved . as a consequence ,",
    "inference tasks such as the calculation of a normalization constant , a marginal distribution or the mode of the joint distribution are challenging .",
    "three main approaches exist to evaluate such quantities for a given distribution @xmath0 defining a graphical model : @xmath1 compute them in an exact manner ; @xmath2 use a stochastic algorithm to sample from the distribution @xmath0 to get ( unbiased ) estimates ; @xmath3 derive an approximation of @xmath0 for which the exact calculation is possible . even if appealing , exact computation on @xmath0 often leads to very time and memory consuming procedures , since the number of elements to store or elementary operations to perform increase exponentially with @xmath4 the number of random variables .",
    "the second approach is probably the most widely used by statisticians and modelers .",
    "stochastic algorithms such as monte - carlo markov chains ( mcmc , * ? ? ?",
    "* ) , gibbs sampling  @xcite and particle filtering  @xcite have become standard tools in many fields of application using statistical models .",
    "the last approach includes variational approximation techniques @xcite , which are starting to become common practice in computational statistics .",
    "in essence , approaches of type @xmath2 provide an approximate answer to an exact problem whereas approaches of type @xmath3 provide an exact answer to an approximate problem .",
    "in this paper we focus on approaches of type @xmath1 and @xmath3 , and we will review techniques for exact or approximate inference in graphical models borrowed from both optimization and computer science .",
    "they are computationally efficient , yet not standard in the statistician toolkit .",
    "our purpose is to show that the characterization of the structure of the graph @xmath5 associated to a graphical model ( precise definitions are given in section [ sec : gm ] ) enables both to determine if the exact calculation of the quantities of interest ( marginal distribution , normalization constant , mode ) can be implemented efficiently and to derive a class of operational algorithms .",
    "when the answer is no , the same analysis enables to design algorithms to compute an approximation of the desired quantities for which an acceptable complexity can be obtained .",
    "the central algorithmic tool is the variable elimination concept @xcite . in section [ sec : exactinference ]",
    "we adopt a unified algebraic presentation of the different inference tasks ( marginalization , normalizing constant or mode evaluation ) to emphasize that all of them can be solved as particular cases of variable elimination .",
    "this implies that if variable elimination is efficient for one task it will also be efficient for the other ones .",
    "the key ingredient to design efficient algorithms based on variable elimination is the clever use of distributivity between algebraic operators .",
    "for instance distributivity of the product ( @xmath6 ) over the sum ( @xmath7 ) enables to write @xmath8 and evaluating the left - hand side of this equality requires two multiplications and one addition while evaluating the right - hand side requires one multiplication and one addition .",
    "similarly since @xmath9 it is more efficient to compute the right - hand side from an algorithmic point of view .",
    "distributivity enables to minimize the number of operations .",
    "associativity and commutativity are also required and the algebra behind is the semi - ring category ( from which some notations will be borrowed ) .",
    "inference algorithms using the distributivity property have been known and published in the artificial intelligence and machine learning literature under different names , such as sum - prod , or max - sum  @xcite and are examples of variable elimination .",
    "variable elimination relies on the choice of an order of elimination of the variables ( either by marginalization or by maximization ) .",
    "this corresponds to the ordering calculations are performed when applying distributivity .",
    "the topology of the graph @xmath5 provides key information to organize the calculations for an optimal use of distributivity , i.e. to minimize the number of elementary operations to perform .",
    "for example , when the graph is a tree , the most efficient elimination order corresponds to eliminating recursively the vertices of degree one , starting from the leaves towards the root . for an arbitrary graph , the notion of an optimal elimination order for inference in a graphical model",
    "is closely linked to the notion of treewidth of the associated graph @xmath5 .",
    "we will see in section [ sec : exactinference ] the reason why inference algorithms based on variable elimination with the best elimination order are of complexity linear in @xmath4 but exponential in the treewidth .",
    "therefore treewidth is the key characterization of @xmath5 to determine if exact inference is possible in practice or not .",
    "the concept of treewidth has been proposed in parallel in computer science @xcite and in discrete mathematics and graph minor theory ( see * ? ? ?",
    "* ; * ? ? ?",
    "discrete mathematics existence theorems  @xcite establish that there exists an algorithm for computing the treewidth of any graph with complexity polynomial in @xmath4 ( but exponential in the treewidth ) , and even the degree of the polynomial is given .",
    "however this result does not tell how to derive and implement the algorithm , apart from some specific cases ( as trees , chordal graphs , and series - parallel graphs , see @xcite .",
    "so we will also present in section [ sec : tw ] several state - of - the - art algorithms for approximate evaluation of the treewidth and illustrate their behavior on benchmarks borrowed from optimization competitions .",
    "variable elimination has also lead to message passing algorithms  @xcite which are now common tools in computer science or machine learning .",
    "more recently these algorithms have been reinterpreted as re - parametrization tools  @xcite .",
    "we will explain in section [ sec : ve2mp ] how re - parametrization can be used as a pre - processing tool to transform the original graphical model into an equivalent one for which inference may been simpler .",
    "message passing is not the only way to perform re - parametrization and we will discuss alternative efficient algorithms that have been proposed in the context of constraint satisfaction problems ( csp , see @xcite ) and that have not yet been exploited in the context of graphical models .    as emphasized above , efficient exact inference algorithms can only be designed for graphical models with limited treewidth ( much less than the number of vertices ) , which is a far from being the general case .",
    "but the principle of variable elimination and message passing for a tree can still be applied to any graph leading then to heuristic inference algorithms .",
    "the most famous heuristics is the loopy belief propagation algorithm  @xcite we recall in section [ sec : variational ] the result that establishes lbp as a variational approximation method .",
    "variational methods rely on the choice of a distribution which renders inference easier , to approximate the original complex graphical model @xmath0 .",
    "the approximate distribution is chosen within a class of models for which efficient inference algorithms exist , that is models with small treewidth ( 0 , 1 or 2 in practice ) .",
    "we review some of the standard choices and we illustrate on the problem of parameter estimation in coupled hidden markov model  @xcite how variational methods have been applied in practice with different approximate distributions , each of them corresponding to a different underlying treewidth ( section [ sec : chmm ] ) .",
    "consider a a stochastic system defined by a set of random variables @xmath10 .",
    "each variable @xmath11 takes values in @xmath12 .",
    "then , a realization of @xmath13 is a set @xmath14 , with @xmath15 .",
    "the set of all possible realizations is called the state space , and is denoted @xmath16 .",
    "if @xmath17 is a subset of @xmath18 , @xmath19 , @xmath20 and @xmath21 are respectively the subset of random variables @xmath22 , the set of realizations @xmath23 and the state space of @xmath19 .",
    "if @xmath0 is the joint probability distribution of @xmath13 on @xmath24 , we denote @xmath25    note that we focus here on discrete variables ( we will discuss inference in the case of continuous variables on examples in section [ sec : conclu ] ) . a joint distribution @xmath0 on @xmath24",
    "is said to be a _",
    "probabilistic graphical model _",
    "@xcite indexed on a set @xmath26 of parts of @xmath27 if there exists a set @xmath28 of maps from @xmath29 to @xmath30 , called _ potential functions _ , indexed by @xmath26 such that @xmath0 can be expressed in the following factorized form : @xmath31 where @xmath32 is the normalizing constant , also called partition function . the elements @xmath33 are the scopes of the potential functions and @xmath34 is the arity of the potential function @xmath35 .",
    "the set of scopes of all the potential functions involving variable @xmath11 is denoted @xmath36 .",
    "one desirable property of graphical models is that of markov local independence : if @xmath37 can be expressed as ( [ eq : gm ] ) then a variable @xmath11 is ( stochastically ) independent of all others in @xmath13 conditionally to the set of variables @xmath38 .",
    "this set is called the markov blanket of @xmath11 , or its neighborhood .",
    "we will denote it @xmath39 .",
    "these conditional independences can be represented graphically , by a graph with one vertex per variable in @xmath13 .",
    "the question of encoding the independence properties associated with a given distribution into a graph structure is well described in  @xcite , and we will not discuss it here .",
    "we will consider the classical graph @xmath40 associated to a decomposition of the form ( [ eq : gm ] ) where an edge is drawn between two vertices @xmath41 and @xmath42 if there exists @xmath43 such that @xmath41 and @xmath42 are in @xmath44 .",
    "such a representation of a graphical model is actually not as rich as the representation ( [ eq : gm ] ) .",
    "for instance , if @xmath45 , the two cases @xmath46 and @xmath47 are represented by the same graph @xmath5 , namely a clique of size 3 .",
    "the factor graph representation goes beyond this limit : this graphical representation is a bipartite graph with one vertex per potential function and one vertex per variable .",
    "edges are only between functions and variables .",
    "an edge is present between a function vertex ( also called factor vertex ) and a variable vertex if the variable is in the scope of the potential function . figure  [ fig : graphical representation of gm ] displays examples of the two graphical representations .",
    "there exists several families of probabilistic graphical models  @xcite .",
    "they can be grouped into directed and undirected ones .",
    "the most classical directed framework is that of _ bayesian network _  @xcite . in a bayesian network ,",
    "an edge is directed from a parent vertex to a child vertex and potential functions are conditional probabilities of a variable given its parents in the graph ( see figure  [ fig : graphical representation of gm ] ( a ) ) . in such models ,",
    "trivially @xmath48 .",
    "undirected probabilistic graphical models ( see figure  [ fig : graphical representation of gm ] ( c ) ) are equivalent to _ markov random fields _",
    "@xcite as soon as the potential functions are in @xmath49 . in a markov random field ( mrf ) , a potential function is not necessarily a probability distribution : @xmath35 is not required to be normalized .",
    "= [ draw , circle , fill = gray!50,minimum size = 5pt , inner sep = 2pt , font= ] = [ ->,>=latex ] ( 0,0 ) node[node ] ( a ) 1 ( 0,-1 ) node[node ] ( b ) 2 ( 1,0 ) node[node ] ( c ) 3 ( 1,-1 ) node[node ] ( d ) 4 ( @xmath50 ) node[node ] ( e ) 5 ( @xmath51 ) node[node ] ( f ) 6 ( @xmath52 ) node[node ] ( g ) 7 ( @xmath53 ) node ( a ) ; ( a ) to ( b ) ; ( b ) to ( d ) ; ( d ) to ( e ) ; ( e ) to ( f ) ; ( a ) to ( c ) ; ( c ) to ( e ) ; ( e ) to ( g ) ;    = [ draw , circle , fill = gray!50,minimum size = 5pt , inner sep = 2pt ] = [ draw , minimum size = 5pt ] ( 0,0 ) node[node ] ( a ) 1 ( 0,-0.5 ) node[factor ] ( ab ) ( 0,-1 ) node[node ] ( b ) 2 ( 0.5,0 ) node[factor ] ( ac ) ( 1,0 ) node[node ] ( c ) 3 ( 0.5,-1 ) node[factor ] ( bd ) ( 1,-1 ) node[node ] ( d ) 4 ( @xmath50 ) node[node ] ( e ) 5 ( @xmath54 ) node[factor ] ( cde ) ( @xmath51 ) node[node ] ( f ) 6 ( @xmath55 ) node[factor , rotate=30 ] ( ef ) ( @xmath52 ) node[node ] ( g ) 7 ( @xmath56 ) node[factor , rotate=60 ] ( eg ) ( @xmath53 ) node ( b ) ; ( c )  ( ac )  ( a )  ( ab )  ( b )  ( bd ) ",
    "( d ) ; ( c )  ( cde )  ( d ) ; ( cde )  ( e ) ; ( e )  ( ef ) ",
    "( f ) ; ( e )  ( eg )  ( g ) ;    = [ draw , circle , fill = gray!50,minimum size = 5pt , inner sep = 2pt ] ( 0,0 ) node[node ] ( a ) 1 ( 0,-1 ) node[node ] ( b ) 2 ( 1,0 ) node[node ] ( c ) 3 ( 1,-1 ) node[node ] ( d ) 4 ( @xmath50 ) node[node ] ( e ) 5 ( @xmath51 ) node[node ] ( f ) 6 ( @xmath52 ) node[node ] ( g ) 7 ( @xmath53 ) node ( c ) ; ( c )  ( a )  ( b )  ( d )  ( e )  ( d ) (c )",
    "(e )  ( f )  ( e )  ( g ) ;    = [ draw , circle , fill = gray!50,minimum size = 5pt , inner sep = 2pt ] = [ draw , minimum size = 5pt ] ( 0,0 ) node[node ] ( a ) 1 ( 0,-0.5 ) node[factor ] ( ab ) ( 0,-1 ) node[node ] ( b ) 2 ( 0.5,0 ) node[factor ] ( ac ) ( 1,0 ) node[node ] ( c ) 3 ( 0.5,-1 ) node[factor ] ( bd ) ( 1,-1 ) node[node ] ( d ) 4 ( 1,-0.5 ) node[factor ] ( cd ) ( @xmath50 ) node[node ] ( e ) 5 ( @xmath57 ) node[factor , rotate=60 ] ( ce ) ( @xmath58 ) node[factor , rotate=30 ] ( de ) ( @xmath51 ) node[node ] ( f ) 6 ( @xmath55 ) node[factor , rotate=30 ] ( ef ) ( @xmath52 ) node[node ] ( g ) 7 ( @xmath56 ) node[factor , rotate=60 ] ( eg ) ( @xmath53 ) node ( d ) ; ( c ) ",
    "( a )  ( ab )  ( b )  ( bd )  ( d )  ( cd )  ( c )  ( ce )  ( e )  ( de )  ( d ) ; ( e )  ( ef ) ",
    "( f ) ; ( e )  ( eg )  ( g ) ;    [ [ deterministic - graphical - models . ] ] deterministic graphical models .",
    "although the terminology of graphical models is often used to refer to stochastic graphical models , the idea of describing a joint distribution on a set of variables through local functions has also been used in artificial intelligence to concisely describe boolean functions or cost functions , with no normalization constraint . in a graphical model with only boolean ( 0/1 ) potential functions , each potential function describes a constraint between variables . if the potential function takes value 1 , the corresponding realization is said to satisfy the constraint .",
    "the graphical model is known as a constraint network. it describes a joint boolean function on all variables that takes value 1 if and only if all constraints are satisfied .",
    "the problem of finding a realization that satisfies all the constraints , called a solution of the network , is the constraint satisfaction problem ( csp )  @xcite .",
    "this framework is used to model and solve combinatorial optimization problems and there is a variety of software tools to solve it .",
    "when variables are boolean too and when the boolean functions are described as disjunctions of variables or of their negation , the csp reduces to the boolean satisfiability problem ( or sat ) , the seminal np - complete problem  @xcite .",
    "csp have been extended to describe joint cost functions , decomposed as a sum of local cost functions in the ` weighted constraint network ' @xcite or ` cost function network ' . in this case",
    ", potential functions take finite or infinite integer or rational values : infinity enables to express hard constraints while finite values encode costs for unsatisfied soft constraints .",
    "the problem of finding a realization of minimum cost is the weighted constraint satisfaction problem ( wcsp ) , which is also np - hard .",
    "it is easy to observe that any stochastic graphical model can be translated in a weighted constraint network using a simple @xmath59 transformation . with this equivalence ,",
    "it becomes possible to use exact wcsp resolution algorithms that have been developed in this field for mode evaluation in stochastic graphical model .",
    "computations on probabilities and potentials rely on two fundamental types of operations .",
    "firstly , multiplication ( or addition in the @xmath60 domain ) is used to _ combine _ potentials to define a joint potential distribution .",
    "secondly , sum or @xmath61/@xmath62 can be used to _ eliminate _ variables and compute marginals or modes of the joint distribution on subsets of variables .",
    "the precise identity of these two basic operations is not crucial for the inference algorithms considered in this paper .",
    "we denote as @xmath63 the combination operator and as @xmath64 the elimination operator .",
    "the algorithms just require that @xmath65 defines a commutative semi - ring .",
    "specifically , the semi - ring algebra offers distributivity : @xmath66 .",
    "this corresponds to distributivity of product over sum since @xmath67 or distributivity of @xmath61 over sum since @xmath68 , or again distributivity of @xmath61 over product since @xmath69 .",
    "these two abstract operators can be defined to be applied to potential functions , as follows :    combine operator : : :    the combination of two potential functions @xmath70 and    @xmath35 is a new function    @xmath71 , from    @xmath72 to @xmath30 defined    as    @xmath73 .",
    "elimination operator : : :    the elimination of variable @xmath74 from a potential    function @xmath35 is a new function    @xmath75 from    @xmath76 to @xmath30    defined as    @xmath77 . for    @xmath78 ,",
    "@xmath79    represents @xmath80 .",
    "we can now describe classical counting and optimization tasks in graphical models in terms of these two operators . for simplicity , we denote by @xmath81 , where @xmath82 a sequence of eliminations @xmath83 for all @xmath84 , the result being insensitive to the order in a commutative semi - ring .",
    "similarly , @xmath85 represents the successive combination of all potential functions @xmath35 such that @xmath33 .",
    "[ [ counting - tasks . ] ] counting tasks .",
    "+ + + + + + + + + + + + + + +    under this name we group all tasks that involve summing over the state space of a subset of variables in @xmath13 .",
    "this includes the computation of the partition function @xmath86 or of any marginal distribution , as well as entropy evaluation . for @xmath87 and @xmath88 , the marginal distribution @xmath89 of @xmath19 associated to the joint distribution @xmath0",
    "is defined as : @xmath90 the function @xmath89 then satisfies : @xmath91 where @xmath63 combines functions using @xmath6 and @xmath64 eliminates variables using @xmath7 .",
    "marginal evaluation is also interesting in the case where some variables are observed .",
    "if the values of some variables @xmath92 ( @xmath93 ) have been observed , we can compute the marginal conditional distribution by restricting the domains of variables @xmath94 to the observed value .",
    "the entropy @xmath95 of a probabilistic graphical model @xmath0 is defined as @xmath96,\\ ] ] where @xmath97 $ ] denotes the mathematical expectation . in the case of a graphical model , by linearity of the expectation ,",
    "the entropy is equal to @xmath98 this expression is an alternation of use of @xmath63 and @xmath64 operators ( for @xmath99 evaluation , for each @xmath44 and @xmath100 ) .",
    "[ [ optimization - task ] ] optimization task + + + + + + + + + + + + + + + + +    an optimization task in a graphical model corresponds to the evaluation of the most probable state @xmath101 of the random vector @xmath13 , defined as @xmath102 the maximum itself is @xmath103 with @xmath64 set to @xmath61 and @xmath63 to @xmath7 . the computation of the mode @xmath104 does not require the computation of the normalizing constant @xmath86 , however computing the mode probability @xmath105 does .    therefore counting and optimization tasks can be interpreted as two instantiations of the same computational task expressed in terms of combination and elimination operators , namely @xmath106 where @xmath107 . when the combination operator @xmath63 and the elimination operator @xmath64 are respectively set to @xmath6 and @xmath7 , this computational problem is known as a sum - product problem in the artificial intelligence literature  @xcite,@xcite .",
    "when @xmath64 is set to @xmath61 and @xmath63 to the sum operator it is a max - sum problem  @xcite .",
    "we will see in section  [ sec : exactinference ] that there exists an exact algorithm solving this general task that exploits the distributivity of the combination and elimination operators to perform operations in a smart order . from this generic algorithm , known as variable elimination  @xcite or bucket elimination  @xcite",
    ", one can deduce exact algorithms to solve counting and optimization tasks in a graphical model , by instantiating the operators @xmath64 and @xmath63 .",
    "[ [ deterministic - graphical - models ] ] deterministic graphical models    : the constraint satisfaction problem is a @xmath108-@xmath109 problem as it can can be defined using @xmath108 ( logical or ) as the elimination operator and @xmath109 ( logical and ) as the combination operator over booleans .",
    "the weighted csp is a @xmath62-@xmath7 as it uses @xmath62 as the elimination operator and @xmath7 ( or bounded variants of @xmath7 ) as the combination operator .",
    "several other variants exist  @xcite , including generic algebraic variants  @xcite .",
    "we describe now the principle of variable elimination .",
    "we first recall the viterbi algorithm for hidden markov chains , a classical example of variable elimination for optimization .",
    "then we formally describe the variable elimination procedure in the general graphical model framework .",
    "the key element is the choice of an ordering for the sequential elimination of the variables .",
    "it is closely linked to the notion of treewidth of the graphical representation of the graphical model .",
    "as it will be shown , the complexity of the variable elimination is fully characterized by this notion .",
    "conversely , the treewidth can be bounded from above from a given variable elimination scheme .      as an introduction to exact inference on graphical models by variable elimination , we consider a well studied stochastic process : the discrete hidden markov chain model ( hmc ) .",
    "a hmc ( figure [ fig : hmm ] ) is defined by two sequences of random variables @xmath110 and @xmath95 of the same length , @xmath111 .",
    "a realization @xmath112 of the variables @xmath113 is observed , while the states of variables @xmath114 are unknown . in the hmc model",
    "the assumption is made that @xmath115 is independent of @xmath116 and @xmath117 given the hidden variable @xmath118 .",
    "these independences are modeled by pairwise potential functions @xmath119 .",
    "furthermore , hidden variable @xmath118 is independent of @xmath120 and @xmath121 given the hidden variable @xmath122 .",
    "these independences are modeled by pairwise potential functions @xmath123 .",
    "then the model is fully defined by specifying an additional potential function @xmath124 to model the initial distribution . in the classical hmc formulation @xcite , these potential functions are normalized conditional probability distributions i.e. , @xmath125 , @xmath126 and @xmath127 . as a consequence ,",
    "the normalizing constant @xmath86 is equal to @xmath128 , as in any bayesian network .",
    "= [ draw , circle , fill = gray!100,minimum size = 5pt , inner sep = 2pt , font= ] = [ draw , circle , fill = gray!50,minimum size = 5pt , inner sep = 2pt , font= ] = [ ->,>=latex ] ( 0,0 ) node[hiddennode ] ( a ) 1 ( 0,-1 ) node[obsnode ] ( b ) 2 ( 1,0 ) node[hiddennode ] ( c ) 3 ( 1,-1 ) node[obsnode ] ( d ) 4 ( 2,0 ) node[hiddennode ] ( e ) 5 ( 2,-1 ) node[obsnode ] ( f ) 6 ( 3,0 ) node[hiddennode ] ( g ) 7 ( 3,-1 ) node[obsnode ] ( h ) 8 ; ( a ) to ( b ) ; ( a ) to ( c ) ; ( c ) to ( d ) ; ( c ) to ( e ) ; ( e ) to ( f ) ; ( e ) to ( g ) ; ( g ) to ( h ) ;    a classical inference task for hmc is to identify the most likely value of the variables @xmath95 given a realization @xmath129 of the variables @xmath110 .",
    "the problem is to compute @xmath130 or equivalently the argument of : @xmath131\\ ] ] the number of possible realizations of @xmath95 is exponential in @xmath111 .",
    "nevertheless this optimization problem can be solved in a number of operations linear in @xmath111 using the well - known viterbi algorithm  @xcite .",
    "this algorithm , based on dynamic programming , performs successive eliminations ( by maximization ) of all hidden variables , starting with @xmath132 , then @xmath133 , and finishing by @xmath134 , to compute the most likely sequence of hidden variables . by using distributivity between the @xmath61 and the product operators ,",
    "the elimination of variable @xmath132 can be done by rewriting equation ( [ viterbi ] ) as :    @xmath135 \\nonumber\\end{aligned}\\ ] ]    the new potential function created by maximizing on @xmath132 depends only on variable @xmath133 , so that variables @xmath136 and potential functions involving them have been removed from the optimization problem .",
    "this is a simple application of the general variable elimination algorithm that we describe in the next section .      in section [ sec : gm ]",
    ", we have seen that counting and optimization tasks can be formalized by the same generic algebraic formulation @xmath137 where @xmath107 .",
    "the trick behind variable elimination  @xcite relies on a clever use of the distributivity property .",
    "indeed , evaluating @xmath138 as @xmath139 requires fewer operations .",
    "since distributivity applies both for counting and optimizing tasks , variable elimination can be applied to both tasks .",
    "it also means that if variable elimination is efficient for one task it will also be efficient for the other one . as in the hmc example",
    ", the principle of the variable elimination algorithm for counting or optimizing consists in eliminating variables one by one in expression ( [ elimeq ] ) .",
    "elimination of the first variable , say @xmath140 , is performed by merging all potential functions involving @xmath11 and applying operator @xmath83 to these potential functions . using commutativity and associativity equation ( [ elimeq ] ) can be rewritten as follows : @xmath141 then using distributivity we obtain @xmath142\\ ] ]    this shows that the elimination of @xmath11 results in a new graphical model where the variable @xmath11 and the potential functions @xmath143 have been removed and replaced by a new potential @xmath144 which does not involve @xmath11 , but its neighboring vertices .",
    "the graph associated to the new graphical model is similar to the graph of the original model except that vertex @xmath11 has been removed and that the neighbors @xmath39 of @xmath11 are now connected together in a clique .",
    "the new edges between the neighbors of @xmath11 are called _ fill - in _ edges .",
    "for instance , when eliminating variable @xmath145 in the graph of figure  [ fig : one step elim ] ( left ) , potential functions @xmath146 and @xmath147 are replaced by @xmath148 .",
    "the new graph is shown in figure  [ fig : one step elim ] , right part .    when the first elimination step is applied with @xmath78 and @xmath149 ,",
    "the probability distribution defined by this new graphical model is the marginal distribution @xmath150 of the original distribution @xmath0 .",
    "the complete elimination can be obtained by successively eliminating all variables in @xmath19 .",
    "the result is a graphical model over @xmath151 which is the marginal distribution @xmath152 .",
    "when @xmath153 , the result is a model with a single constant potential function with value @xmath86 .",
    "= [ draw , circle , fill = gray!50,minimum size = 5pt , inner sep=2pt , font= ] ( 0,0 ) node[vertexlight ] ( a ) 1 ( 4,8 ) node[vertexlight ] ( b ) 2 ( 8,4 ) node[vertexlight ] ( c ) 3 ( 4,-8)node[vertexlight ] ( d ) 5 ( 8,-4 ) node[vertexlight ] ( e ) 4 ; ( a )  ( b ) ; ( a)(c ) ; ( a)(d ) ; ( a)(e ) ; ( b)+(8,4 ) ; ( b)+(8,2 ) ; ( c)+(8,2 ) ; ( c)+(8,0 ) ; ( c)+(8,-2 ) ; ( e)+(8,2 ) ; ( e)+(8,0 ) ; ( e)+(8,-2 ) ; ( d)+(8,-4 ) ; ( d)+(8,-2 ) ;    = [ draw , circle , fill = gray!50,minimum size = 5pt , inner sep=2pt ] = [ draw , circle , fill = pink!100,minimum size = 5pt , inner sep=2pt ] ( 0,0 ) node[vertexdark ] ( a ) 1 ( 4,8 ) node[vertexlight ] ( b ) 2 ( 8,4 ) node[vertexlight ] ( c ) 3 ( 4,-8)node[vertexlight ] ( d ) 5 ( 8,-4 ) node[vertexlight ] ( e ) 4 ; ( a )  ( b ) ; ( a)(c ) ; ( a)(d ) ; ( a)(e ) ; ( b)(c ) ; ( b)(d ) ; ( b)(e ) ; ( c)(d ) ; ( c)(e ) ; ( d)(e ) ; ( b)+(8,4 ) ; ( b)+(8,2 ) ; ( c)+(8,2 ) ; ( c)+(8,0 ) ; ( c)+(8,-2 ) ; ( e)+(8,2 ) ; ( e)+(8,0 ) ; ( e)+(8,-2 ) ; ( d)+(8,-4 ) ; ( d)+(8,-2 ) ;    = [ draw , circle , fill = gray!50,minimum size = 5pt , inner sep=2pt ] ( 4,8 ) node[vertexlight ] ( b ) 2 ( 8,4 ) node[vertexlight ] ( c ) 3 ( 4,-8)node[vertexlight ] ( d ) 5 ( 8,-4 ) node[vertexlight ] ( e ) 4 ; ( b)(c ) ; ( b)(d ) ; ( b)(e ) ; ( c)(d ) ; ( c)(e ) ; ( d)(e ) ; ( b)+(8,4 ) ; ( b)+(8,2 ) ; ( c)+(8,2 ) ; ( c)+(8,0 ) ; ( c)+(8,-2 ) ; ( e)+(8,2 ) ; ( e)+(8,0 ) ; ( e)+(8,-2 ) ; ( d)+(8,-4 ) ; ( d)+(8,-2 ) ;    if instead @xmath64 is @xmath61 , and @xmath154 ( or @xmath7 with a log transformation of the potential functions ) and @xmath153 , the last potential function obtained after elimination of the last variable is equal to the maximum of the non normalized distribution . so evaluating @xmath86 or",
    "the maximal probability of a graphical model can be both obtained with the same variable elimination algorithm , just changing the meaning of @xmath64 and @xmath63 .",
    "+ however , if one is interested in the mode itself , an additional simple computation is required .",
    "the mode is obtained by induction : if @xmath155 is the mode of the graphical model obtained after the elimination of the first variable , @xmath11 , then the mode of @xmath0 can be defined as @xmath156 where @xmath157 is a value in @xmath12 that maximizes @xmath158 .",
    "this maximization is straightforward to derive because @xmath159 can take only @xmath160 values .",
    "@xmath155 itself is obtained by completing the mode of the graphical model obtained after elimination the second variable , and so on .",
    "we stress here that the procedure requires to keep the intermediary potential functions @xmath144 generated during the successive eliminations .",
    "when eliminating a variable @xmath11 , the task that can be computationally expensive is the computation of the intermediate @xmath144 .",
    "it requires to compute the product @xmath161 of several potential functions for all elements of @xmath162 , the state space of @xmath163 .",
    "the time and space complexity of the operation are entirely determined by the cardinality @xmath164 of the set of indices @xmath39 .",
    "if @xmath165 is the maximum domain size of a variable , the time complexity ( i.e. number of elementary operations performed ) is in @xmath166 and space complexity ( i.e. memory space needed ) is in @xmath167 .",
    "complexity is therefore exponential in @xmath164 , the number of neighbors of the eliminated variable in the current graphical model .",
    "the total complexity of the variable elimination is then exponential in the maximum cardinality @xmath164 over all successive eliminations ( but linear in @xmath4 ) . because the graphical model changes at elimination each step , this number usually depends on the order in which variables are eliminated .    as a consequence ,",
    "the prerequisite to apply variable elimination is to decide for an ordering of elimination of the variables .",
    "as illustrated in figure  [ fig : good and bad order ] two different orders can lead to two different @xmath39 subsets .",
    "the key message is that the choice of the order is crucial/ it dictates the efficiency of the variable elimination procedure .",
    "we will now illustrate and formalize this intuition .",
    "we can understand why the viterbi algorithm is an efficient algorithm for mode evaluation in a hmc .",
    "the graph associated to a hmc is comb - shaped : the hidden variables form a line and each observed variable is a leaf in the comb ( see figure [ fig : hmm ] ) .",
    "so it is possible to design an elimination order where the current variable to eliminate has a unique neighbor in the graphical representation of the current model : for instance @xmath168 ( the first eliminated variable is the largest according to this ordering ) . following this elimination order , when eliminating a variable using @xmath64 , the resulting graphical model has one fewer vertex than the previous one and _ no fill - in edge_. indeed , the new potential function @xmath144 is a function of a single variable since @xmath169 .",
    "more generally , variable elimination is very efficient , i.e. leads to intermediate @xmath39 sets of small cardinality , on graphical models whose graph representation is a tree , again because it is always possible to design an elimination order where the current variable to eliminate has only one neighbor in the graphical representation of the current model .",
    "another situation where variable elimination can be efficient is when the graph associated to the graphical model is chordal ( any cycle of length 4 or more has a chord i.e. , an edge connecting two non adjacent vertices in the cycle ) , the size of the largest clique being low .",
    "the reason is the following .",
    "in figure 2 , new edges are created between neighbors of the eliminated vertex . if this neighborhood is a clique , no new edge is added .",
    "a vertex whose neighborhood is a clique is called a simplicial vertex .",
    "chordal graphs have the property that there exists an elimination order of the vertices such that every vertex during elimination process is simplicial .",
    "then , there exists an elimination order such that no fill - in edges are created .",
    "thus , the largest size of @xmath39 is no more than the size of a clique , and is equal to or less than the size of the largest clique in the graph .",
    "let us note that a tree is a chordal graph in which all edges and only edges are cliques . hence ,",
    "for a tree , simplicial vertices are vertices of degree one .",
    "then , elimination of degree one vertices on a tree is an example of simplicial elimination on a chordal graph .    for arbitrary graphs ,",
    "if the maximal scope size of the intermediate @xmath144 functions created during variable elimination is too large , then memory and time required for the storage and computation quickly exceed computer capacities . depending on the chosen elimination order",
    ", this maximal scope can be reasonable from a computational point of view , or too large .",
    "so again , the choice of the elimination order is crucial .",
    "= [ draw , circle , fill = gray!50,minimum size = 5pt , inner sep = 2pt , font= ] ( 0,0 ) node[vertex ] ( a ) 1 ( 0,-1 ) node[vertex ] ( b ) 2 ( 1,0 ) node[vertex ] ( c ) 3 ( 1,-1 ) node[vertex ] ( d ) 4 ( @xmath50 ) node[vertex ] ( e ) 5 ( @xmath51 ) node[vertex ] ( f ) 6 ( @xmath52 ) node[vertex ] ( g ) 7 ; ( c )  ( a )",
    " ( b )  ( d ) ",
    "( e )  ( d ) (c )",
    "(e )  ( f )  ( e )  ( g ) ;    = [ draw , circle , fill = gray!50,minimum size = 5pt , inner sep = 2pt , font= ] = [ ->,>=latex ] ( 0,1 ) node[vertex ] ( g ) 7 ( 0,0 ) node[vertex ] ( f ) 6 ( 0,-1 ) node[vertex ] ( e ) 5 ( 0,-2 ) node[vertex ] ( d ) 4 ( 0,-3 ) node[vertex ] ( c ) 3 ( 0 , -4 ) node[vertex ] ( b ) 2 ( 0 , -5 ) node[vertex ] ( a ) 1 ;    \\(g ) to[bend left=60 ] ( e ) ; ( f ) to ( e ) ; ( e ) to ( d ) ; ( e ) to[bend right=60 ] ( c ) ; ( d ) to ( c ) ; ( d ) to[bend left=60 ] ( b ) ; ( c ) to[bend right=60 ] ( b ) ; ( c ) to[bend left=60 ] ( a ) ; ( b ) to ( a ) ;    = [ draw , circle , fill = gray!50,minimum size = 5pt , inner sep = 2pt , font= ] = [ ->,>=latex ] ( 0,1 ) node[vertex ] ( g ) 7 ( 0,0 ) node[vertex ] ( e ) 5 ( 0,-1 ) node[vertex ] ( c ) 3 ( 0,-2 ) node[vertex ] ( a ) 1 ( 0,-3 ) node[vertex ] ( f ) 6 ( 0 , -4 ) node[vertex ] ( d ) 4 ( 0 , -5 ) node[vertex ] ( b ) 2 ;    \\(g ) to ( e ) ; ( e ) to ( c ) ; ( e ) to[bend right=60 ] ( d ) ; ( e ) to[bend right=60 ] ( f ) ; ( c ) to[bend right=60 ] ( f ) ; ( f ) to[bend left=60 ] ( d ) ;    \\(c ) to[bend left=60 ] ( d ) ; ( c ) to ( a ) ; ( a ) to[bend right=60 ] ( f ) ; ( a ) to[bend left=60 ] ( d ) ; ( f ) to[bend left=60 ] ( d ) ;    \\(a ) to[bend left=60 ] ( b ) ; ( f ) to[bend right=60 ] ( b ) ;    \\(d ) to ( b ) ;      the lowest complexity achievable when performing variable elimination is characterized by a parameter called the treewidth of the graph associated to the original graphical model .",
    "this concept has been repeatedly discovered and redefined .",
    "the treewidth of a graph is sometimes called its induced width  @xcite , its minimum front size  @xcite , its @xmath170-tree number  @xcite , its dimension  @xcite and is also equal to the min - max clique number of @xmath5 minus one  @xcite .",
    "the treewidth is also a key notion in the theory of graph minors ( see * ? ? ?",
    "* ; * ? ? ?",
    "we insist here on two definitions .",
    "the first one from @xcite relies on the notion of induced graph and the link between fill - in edges and the intermediate @xmath39 sets created during variable elimination is straightforward .",
    "the second  @xcite is the commonly used characterization of the treewidth using so - called tree decompositions , also known as junction trees which are key tools to derive variable elimination algorithms .",
    "it underlies the block - by - block elimination procedure described in section [ sec : tdbbe ] .",
    "let @xmath171 be a graph defined by a set of vertices indexed on @xmath27 and a set @xmath172 of edges .",
    "given an ordering @xmath173 of the vertices of @xmath5 , the induced graph @xmath174 is obtained as follows .",
    "@xmath5 and @xmath174 have same vertices .",
    "then to each edge in @xmath172 corresponds an oriented edge in @xmath174 going from the first of the two nodes according to @xmath173 toward the second .",
    "then each vertex @xmath41 of @xmath27 is considered one after the other following the order defined by @xmath173 .",
    "when vertex @xmath41 is treated , an oriented edge is created between all pairs of neighbors of @xmath41 in @xmath5 that follows @xmath41 according to @xmath173 .",
    "again the edge is going from the first of the two nodes according to @xmath173 toward the second .",
    "the induced graph @xmath174 is also called the _ fill graph _ of @xmath5 and the process of computing it is sometimes referred to as `` playing the elimination game '' on @xmath5 , as it just simulates elimination on @xmath5 using the variable ordering @xmath173 .",
    "this graph is chordal  @xcite .",
    "it is known that every chordal graph @xmath5 has at least one vertex ordering @xmath173 such that @xmath175 , called a perfect elimination ordering  @xcite .    the second notion that enables to define",
    "the treewidth is the notion of tree decomposition .",
    "intuitively , a tree decomposition of a graph @xmath5 organizes the vertices of @xmath5 in clusters of vertices which are linked by edges such that the graph obtained is a tree .",
    "specific constraints on the way vertices of @xmath5 are associated to clusters in the decomposition tree are demanded .",
    "these contraints ensure properties to tree decomposition usefull for building variable elimination algorithms .    given a graph @xmath176 , a tree decomposition @xmath111 is a tree @xmath177 , where @xmath178 is a family of subsets of @xmath27 ( called clusters ) , and @xmath179 is a set of edges between the subsets @xmath180 , satisfying the following properties :    * the union of all clusters @xmath181 equals @xmath27 ( each vertex is associated with at least one vertex of @xmath111 ) . * for every edge @xmath182 in @xmath172 , there is at least one cluster @xmath181 that contains both @xmath41 and @xmath42 . *",
    "if clusters @xmath181 and @xmath183 both contain a vertex @xmath41 of @xmath5 , then all clusters @xmath184 of @xmath111 in the ( unique ) path between @xmath181 and @xmath183 contain @xmath41 as well : clusters containing vertex @xmath41 form a connected subset of @xmath111 .",
    "this is known as the running intersection property ) .",
    "the concept of tree decomposition is illustrated in figure [ fig : tree decomp ] .",
    "= [ draw , circle , fill = gray!50,minimum size = 5pt , inner sep = 2pt , font= ] ( 0,0 ) node[vertex ] ( a ) 1 ( 0,-1 ) node[vertex ] ( b ) 2 ( 1,0 ) node[vertex ] ( c ) 3 ( 1,-1 ) node[vertex ] ( d ) 4 ( @xmath50 ) node[vertex ] ( e ) 5 ( @xmath51 ) node[vertex ] ( f ) 6 ( @xmath52 ) node[vertex ] ( g ) 7 ; ( c )  ( a )  ( b )  ( d )  ( e )  ( d ) (c ) (e )  ( f )  ( e )  ( g ) ;    the two following definitions of the treewidth derived respectively from the notion of induce graph and from that of tree decomposition are equivalent ( but this is not trivial to establish ) :    * the treewidth @xmath185 of a graph @xmath5 for the ordering @xmath173 is the maximum number of outgoing edges of a vertex in the induced graph @xmath174 .",
    "the treewidth @xmath186 of a graph @xmath5 is the minimum treewidth over all possible orderings @xmath173 . *",
    "the width of a tree decomposition @xmath187 is the size of the largest @xmath188 .",
    "and the treewidth @xmath186 of a graph is the minimum width among all its tree decompositions .",
    "@xmath185 is exactly the cardinality of the largest set @xmath39 created during variable elimination with elimination order @xmath173 .",
    "for example , in figure  [ fig : good and bad order ] , the middle and right graphs are the two induced graphs for two different orderings .",
    "@xmath185 is equal to 2 with the first ordering and to 3 with the second .",
    "it is easy to see that in this example @xmath189 .",
    "the treewidth of the graph of the hmc model , and of any tree is equal to 1 .",
    "it has been established that finding a minimum treewidth ordering @xmath173 for a graph @xmath5 , finding a minimum treewidth tree decomposition or computing the treewidth of a graph are of equivalent complexity .",
    "for an arbitrary graph , computing the treewidth is not an easy task .",
    "section [ sec : tw ] is dedicated to this question , from a theoretical and a practical point of view .",
    "the treewidth is therefore a key indicator to answer the driving subject of this review : will variable elimination be efficient for a given graphical model ?",
    "for instance , the principle of variable elimination have been applied to the exact computation of the normalizing constant of a markov random field on a small @xmath190 by @xmath191 lattice in  @xcite . for this regular graph , it is known that the treewidth is equal to @xmath192 .",
    "so exact computation through variable elimination is only possible for lattices with a small value for @xmath192 .",
    "it is however well beyond computer capacities for real challenging problems in image analysis . in this case",
    "variable elimination can be used to define heuristic computational solutions , such as the algorithm of @xcite which relies on exact computations on small sub - lattices of the original lattice .      given",
    "a graphical model and a tree decomposition of its graph , a possible alternative to solve counting or optimization tasks is to eliminate variables in successive blocks instead of one after the other .",
    "to do so , the block by block elimination procedure  @xcite relies on the tree decomposition characterization of the treewidth .",
    "the underlying idea is to apply the variable elimination procedure on the tree decomposition , eliminating one cluster of the tree at each step .",
    "first a root cluster @xmath193 is chosen and used to define an order of elimination of the clusters , by progressing from the leaves toward the roots , such that every eliminated cluster corresponds to a leaf of the current intermediate tree .",
    "then each potential function @xmath35 is assigned to the cluster @xmath180 in @xmath194 such that @xmath195 which is the closest to the root .",
    "sucha cluster always exists from the properties of a tree decomposition and the fact that a potential function is associated to a clique in @xmath5 ) .",
    "the procedure starts with the elimination of any leaf cluster @xmath180 of @xmath111 , with parent @xmath196 in @xmath111 .",
    "let us note @xmath197 . here again , commutativity and distributivity are used to rewrite expression ( [ elimeq ] ) ( with @xmath153 ) as follows :    @xmath198\\ ] ] note that only variables with indices in @xmath199 are eliminated , even if it is common to say that the cluster has been eliminated . for instance , for the graph of figure [ fig : tree decomp ] ,",
    "if the first eliminated cluster is @xmath200 , the new potential function is @xmath201 , it depends only on variables @xmath145 and @xmath202 .",
    "cluster elimination continues until no cluster is left .",
    "the interest of this procedure is that the intermediate potential function created after each cluster elimination may have a scope much smaller than the treewidth , leading to better space complexity  @xcite .",
    "however , the time complexity is increased .    in summmary",
    ", the lowest complexity achievable when performing variable elimination is for elimination orders whose cardinalities of the intermediate @xmath39 sets are lower of equal to the treewidth of @xmath5 .",
    "this treewidth can be determined by considering clusters sizes in tree decompositions of @xmath5 .",
    "furthermore , a tree decomposition @xmath111 can be used to build an elimination order and vice versa .",
    "indeed , an elimination order can be defined by using a cluster elimination order based on @xmath111 and choosing an arbitrary order to eliminate variables with indices in the subsets @xmath203 .",
    "conversely , it is easy to build a tree decomposition from a given vertex ordering @xmath173 . since the induced graph @xmath174 is chordal ,",
    "its maximum cliques can be identified in polynomial time .",
    "each such clique defines a cluster @xmath180 of the tree decomposition .",
    "edges of @xmath111 can be identified as the edges of any maximum spanning tree in the graph with vertices @xmath180 and edges @xmath204 weighed by @xmath205 .",
    "[ [ deterministic - graphical - models-1 ] ] deterministic graphical models + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    : to our knowledge , the notion of treewidth and its properties have been first identified in combinatorial optimization in  @xcite where it was called `` dimension '' , a graph parameter which has been shown equivalent to the treewidth  @xcite .",
    "variable elimination itself is related to fourier - motzkin elimination  @xcite , a variable elimination algorithm that benefits from the linearity of the handled formulas .",
    "variable elimination has been repeatedly rediscovered , as non - serial dynamic programming  @xcite , in the david - putnam procedure for boolean satisfiability problems  ( sat , * ? ? ?",
    "* ) , as bucket elimination for the csp and wcsp  @xcite , in the viterbi and forward - backward algorithms for hmm  @xcite and many more .",
    "theree exists other situations where the choice of an elimination order has a deep impact on the complexity of the computations as in gauss elimination scheme for a system of linear equations , or choleski factorization of very large sparse matrices , and where equivalence between elimination and decomposition have been used ( see * ? ? ?",
    "as already mentioned , the complexity of the counting and the optimization tasks on graphical models is heavily linked to the treewidth @xmath186 of the underlying graph @xmath5 .",
    "if one could guess the optimal vertex ordering , @xmath206 , leading to @xmath207 , then , one would be able to achieve the `` optimal complexity '' @xmath208 for solving exactly these tasks ( we recall that @xmath209 is the maximal domain size of a variable in the graphical model ) . however , the problem is that one can not easily evaluate the treewidth of a given graph .",
    "the treewidth computation problem is known to be np - hard @xcite .    in the following subsections",
    "we provide a short presentation of the state - of - the - art theoretical and experimental results concerning the exact computation of the treewidth of a graph , and the computation of suboptimal vertex orderings providing approximations of the treewidth in the form of an upper bound .",
    "several exponential time exact algorithms have been proposed to compute the treewidth .",
    "these algorithms compute the treewidth in time exponential in @xmath4 .",
    "the algorithm with the best complexity bound has been proposed by @xcite .",
    "they provide an exact algorithm for computing the treewidth , which run in time @xmath210 ( using polynomial space ) , or in time @xmath211 , using exponential ( memory ) space .",
    "since the treewidth of a network can be quite small ( compared to @xmath4 ) in practice , there has been a great deal of interest in finding exact algorithms with time complexity exponential in @xmath186 and potentially only polynomial in @xmath4 .",
    "some of these algorithms even have complexity linear in @xmath4 @xcite . in @xcite ,",
    "an algorithm is proposed to compute the treewidth ( it also provides an associated tree decomposition ) of @xmath5 in time @xmath212 .",
    "if this algorithm is used to compute the treewidth of graphs in a family of graphs whose treewidth is uniformly bounded , then computing the treewidth would become of time complexity linear in @xmath4 ( however , even for a small bound on the treewidth , the constant can be huge ) .",
    "moreover , in the general case , there is no way to bound the treewidth a priori .",
    "now , recall that even though crucial , finding a `` good '' tree decomposition of the graph @xmath5 is only one element in the computation of quantities of interest in graphical models .",
    "if one has to spend more time on finding an optimal vertex ordering than on computing probabilities on the underlying graphical model using an easy - to - compute suboptimal ordering , the utility of exact treewidth computation becomes limited .",
    "therefore , an alternative line of search is to look for algorithms computing a vertex ordering @xmath173 leading to a suboptimal width , @xmath213 , but more efficient in terms of computational time .",
    "when defining such approximation algorithms , one is particularly interested in polynomial time ( in @xmath4 ) algorithms , finding a vertex ordering @xmath173 that approaches the optimal ordering within a constant multiplicative factor : @xmath214 .    however , the existence of such constant - factor approximation algorithms is not guaranteed for all np - hard problems .",
    "some np - hard problems are even known not to admit polynomial time approximation algorithms . as far as treewidth approximation is concerned , we are in the interesting case where it is not yet known whether or not a polynomial time approximation algorithm does exist @xcite .    finally , there have been a variety of proposed algorithms , trading off approximation quality and running time complexity @xcite .",
    "table [ tab : tw complexity ] ( extracted from * ? ? ?",
    "* ) summarizes the results in terms of approximation guarantee and time complexity for these algorithms .",
    ".approximation guarantee and time complexity of state - of - the - art treewidth approximation algorithms .",
    "each algorithm provides a vertex ordering @xmath173 such that @xmath215 is upper bounded by the approximation guarantee indicated in column 2 .",
    "the time complexity of these algorithms is is @xmath216 where @xmath4 is the number of vertices in @xmath5 . [",
    "cols=\"^,^,^,^ \" , ]     on the same benchmark , we also compared three exact methods for the task of mode evaluation that exploit either minimum fill - in ordering or its randomized iterative version : variable elimination ( elim ) , btd  @xcite and and/or search  @xcite .",
    "elim and btd exploit the minimum fill - in ordering while and/or search used its randomized iterative version .",
    "in addition , btd and and/or search exploit a tree decomposition during a depth first branch and bound method in order to get a good trade - off between memory space and search effort . as variable elimination",
    ", they have a worst - case time complexity exponential in the treewidth .",
    "all methods were allocated a maximum of 1 hour and 4 gb of ram on an amd operon 6176 at 2.3 ghz .",
    "the results , as reported in figure  [ fig : joint ] ( right ) shown that btd was able to solve more problems than the two other methods for a fixed cpu time .",
    "however , on a given problem , the best method heavily depends on the problem category . on paritylearning , elim was the fastest method , but it ran out of memory on @xmath217 of the total set of instances , while btd ( resp . and/or search ) used less than @xmath218 gb ( resp . 4 gb ) .",
    "the randomized iterative minimum fill - in heuristic used by and/or search in preprocessing consumed a fixed amount of time ( @xmath219 seconds , included in the cpu time measurements ) larger than the cost of a simple minimum fill - in heuristic .",
    "bdt was faster than and/or search to solve most of the instances except on two problem categories ( paritylearning and linkage ) .    to perform this comparison , we ran the followinf implementation of each method .",
    "the version of elim was the one implemented in the combinatorial optimization solver toolbar 2.3 ( options ` -i -t3 ` , available at mulcyber.toulouse.inra.fr/projects/toolbar ) .",
    "the version of btd was the one implemented in the combinatorial optimization solver toulbar2 0.9.7 ( options ` -b=1 -o=-3 -nopre ` .",
    "toulbar2 is available at mulcyber.toulouse.inra.fr/projects/toulbar2 .",
    "this software won the uai 2010  @xcite and 2014  @xcite inference competitions on the map task . and/or search was the version implemented in the open - source version  1.1.2 of daoopt  @xcite ( options ` -y -i 35 slsx=20 slst=10 lds 1 -m 4000 -t 30000 ordertime=180 ` for benchmarks from computer vision and ` -y -i 25 slsx=10 slst=6 lds 1 -m 4000 -t 10000 ordertime=60 ` for the other benchmarks ) which won the probabilistic inference challenge 2011  @xcite , albeit with a different closed - source version  @xcite .     left : comparison of treewidth bounds provided by ( red ) , ( green ) , ( blue ) and ( cyan ) for the 5 categories of problems right : mode evaluation by three exact methods exploiting minimum fill - in ordering or its randomized iterative version .",
    "number of instances solved ( @xmath220-axis ) within a given cpu time ( log10 scale @xmath221-axis ) of ( red ) , ( green ) , and ( blue).,title=\"fig : \" ]   left : comparison of treewidth bounds provided by ( red ) , ( green ) , ( blue ) and ( cyan ) for the 5 categories of problems right : mode evaluation by three exact methods exploiting minimum fill - in ordering or its randomized iterative version .",
    "number of instances solved ( @xmath220-axis ) within a given cpu time ( log10 scale @xmath221-axis ) of ( red ) , ( green ) , and ( blue).,title=\"fig : \" ]",
    "message passing algorithms make use of messages , which can be described as potential functions which are external to the definition of graphical models . on tree - structured graphical models message passing algorithms",
    "extend the variable elimination algorithm by efficiently computing every marginals ( or modes ) simultaneously , when variable elimination only computes one .",
    "on general graphical models , message passing algorithms can still be applied but either provide approximate results efficiently or have an exponential running cost .",
    "we present how it may be conceptually interesting to view these algorithms as performing a re - parametrization of the original graphical model _",
    "i.e. _ , modifications of the potentials , instead of producing external messages , which are not easy to interpret by themselves .      message passing algorithms over trees can be described as an extension of variable elimination , where the marginals of all variables are computed in a double pass of the algorithm ( instead of one variable in classical variable elimination ) . instead of eliminating a leaf @xmath11 and the potential functions involving @xmath11 , we just mark the leaf @xmath11 as  processed  and consider that the new potential @xmath144 is a `` message '' sent from @xmath11 to @xmath222 ( the parents of @xmath11 in the tree ) , denoted as @xmath223 .",
    "this message is a potential function over @xmath222 only .",
    "we can iterate this process , always applying it to a leaf in the subgraph defined by unmarked variables , handling already computed messages as unary potentials .    when only one variable remains unmarked ( defining the root of the tree )",
    ", the combination of all the functions on this variable ( messages and possibly original potential function ) will be equal to the marginal unnormalized distribution on this variable .",
    "this results directly from the fact that the operations are equivalent to variable elimination .",
    "the root of the tree defines a directed tree where the root is at the top , descendants are below and messages are flowing upwards , to the root .    to compute the marginal of another variable",
    ", one can redirect the tree using this new root . then some subtrees will remain unchanged ( in terms of direction from the root of the subtree to the leaves ) in this new tree and the messages in these subtrees",
    "do not need to be recomputed .",
    "it turns out that in a tree , one can organize all these computations cleverly so that only two messages are computed for each edge , one for each possible direction of the edge .",
    "formally , in the _ sum - product _ algorithm over a tree @xmath224 , messages @xmath225 are defined for each edge @xmath226 ( there are @xmath227 such messages , one for each edge direction ) in a _ leaves - to - root - to - leaves _ order .",
    "messages @xmath225 are functions of @xmath228 , which are computed iteratively , by the following algorithm :    1 .",
    "first , messages leaving the leaves of the tree are initialized : @xmath229 , where @xmath41 is a leaf of the tree , @xmath230 mark all leaves as processed .",
    ", messages are sent upward through all edges .",
    "message updates are performed iteratively , from marked nodes @xmath41 to their only unmarked neighbor @xmath42 through edge @xmath226 .",
    "message updates take the following form : @xmath231 where @xmath232 .",
    "+ _ mark _ node @xmath42 as _ processed_. see figure [ figmpt ] for an illustration .",
    "it remains to send the latter messages downward ( from root to leaves ) .",
    "this second phase of message updates takes the following form : * _ _ u__nmark root node . * while there remains a marked node , send update ( [ sumprod ] ) from an unmarked node to one of its marked neighbors , unmark the corresponding neighbor .",
    "4 .   after the two above steps",
    ", messages have been transmitted through all edges in both directions .",
    "finally , marginal distributions over variables and pairs of variables ( linked by an edge ) are computed as follows : @xmath233 @xmath234 and @xmath235 are suitable normalizing constants .",
    "\\(a ) at(0,0 )  ; ( s ) at(2,0)s ; ( w ) at(4,0)w ; ( t ) at(3,2)t ; ( b ) at(5,2 )  ; ( v ) at(4,4)v ; ( c ) at(6,0 )  ; ( d ) at(7,2 )  ; ( e ) at(8,4 )  ;    \\(a )  ( s ) ; ( t )  ( s ) node[midway , above , sloped]@xmath236 ; ( t )  ( w )",
    "node[midway , above , sloped]@xmath237 ; ( t ) ",
    "( v ) node[midway , above , sloped]@xmath238 ; ( v )  ( b ) ; ( b )  ( c ) ; ( b )  ( d ) ; ( d )  ( e ) ;    when the graph of the original graphical model is not a tree , the two - pass message passing algorithm can no more be applied . still , for general graphical models ,",
    "this message passing approach can be generalized in two different ways .",
    "* one can compute a tree decomposition , as previously shown .",
    "message passing can then be applied on the resulting cluster tree , handling each cluster as a cross - product of variables following a block - by - block approach .",
    "this yields an exact algorithm , for which computations can be expensive ( exponential in the treewidth ) and space intensive ( exponential in the separator size ) .",
    "a typical example of such algorithm is the algebraic exact message passing algorithm of  @xcite .",
    "* alternatively , the loopy belief propagation algorithm  @xcite is another extension of message passing in which messages updates are repeated , in arbitrary order through all edges ( possibly many times through each edge ) , until a termination condition is met .",
    "the algorithm returns approximations of the marginal probabilities ( over variables and pairs of variables ) .",
    "the quality of the approximation and the convergence to steady state messages are not guaranteed ( hence , the importance of the termination condition ) .",
    "however , it has been observed that lbp often provides good estimates of the marginals , in practice",
    ". a deeper analysis of message - passing algorithms will be provided in section  [ sec : variational ] .",
    "we have described above the sum - product algorithm .",
    "max - product , max - sum algorithms can be equivalently defined , for exact computation or approximation of the @xmath61-marginal of a joint distribution or its logarithm . in algebraic language , updates like defined in formula ( [ sumprod ] ) take the general form : @xmath239 as for sum - product , the resulting algorithm computes exact @xmath64-marginals on a tree - structured graphical model from which the mode of the distribution can be computed while on general graphical models , it provides only approximations .",
    "it is possible to use message passing on trees as a re - parametrization technique . instead of computing external messages , message passing can reformulate the original tree - structured graphical model in a new equivalent tree - structured graphical model . by `` equivalent '' we mean that the resulting tree defines exactly the same joint distribution as the original graphical model . in the re - parameterized problem ,",
    "information of interest ( marginals ) can be directly read in the potential functions  @xcite .",
    "the idea behind re - parametrization is conceptually very simple : when a message @xmath240 is computed , instead of keeping it as a message , it is possible to multiply any potential function involving @xmath241 by @xmath240 , using @xmath63 . to preserve the joint distribution defined by the graphical model",
    ", we need to divide another potential function involving @xmath241 by the same message @xmath240 using the inverse of @xmath63 .",
    "is not a group but only a semi - group or monoid , suitable pseudo inverses can often be defined .",
    "see  @xcite . ]",
    "one possibility is to incorporate the messages in the binary potentials : we replace @xmath242 by @xmath243 while @xmath244 is divided by @xmath245 and @xmath246 is divided by @xmath225 . in this case , each pairwise potential @xmath242 can be shown to be equal to the marginal of the joint potential on @xmath247 .",
    "the resulting tree - structured mrf is said to be _ calibrated _ to emphasize the fact that all pairs of binary potentials sharing a common variable agree on their marginals :    @xmath248    the main advantage of a calibrated re - parametrization is that it can be used instead for the original model for any further processing .",
    "this is useful in the context of incremental updates , where new evidence is introduced incrementally and each recalibration is simpler than a new calibration  @xcite .",
    "message passing based re - parameterizations can be generalized to cyclic graphs .",
    "if an exact approach using tree decompositions is followed , messages may have a size exponential in the intersection of pairs of clusters and the re - parametrization will create new potentials of this size . if these messages are multiplied inside the clusters , each resulting cluster will be the marginal of the joint distribution on the cluster variables .",
    "the tree - decomposition is calibrated and any two intersecting clusters agree on their marginals .",
    "this is exploited in the lauritzen - spiegelhalter and jensen sum - product - divide algorithms  @xcite . in this context , besides incremental updates , a calibrated tree decomposition allows also to locally compute exact marginals for any set of variables in the same cluster .    if a local `` loopy '' approach is used instead , re - parameterizations do not change scopes but provide a re - parameterized model where estimates of the marginals can be directly read . for map , such re - parameterizations can follow clever update rules to provide convergent re - parameterizations maximizing a well defined criterion .",
    "typical examples of this schema are the sequential version of the tree reweighted algorithm ( trws , * ? ? ?",
    "* ) , or the max product linear programming algorithm ( mplp , * ? ? ?",
    "* ) which try to optimize a bound on the non - normalized probability of the mode .",
    "a seminal reference , published in russian is  @xcite .",
    "these algorithms can be exact on graphical models with loops , provided the potential functions are all submodular ( often described as the discrete version of convexity ) .",
    "[ [ deterministic - graphical - models-2 ] ] deterministic graphical models + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    : message passing algorithms have also been used in deterministic graphical models where they are known as `` local consistency '' enforcing or constraint propagation algorithms .",
    "a local consistency property defines the targeted calibration property and the enforcing algorithm allows to transform the original network into an equivalent network ( defining the same joint function ) that satisfies the desired calibration / local consistency property . similar to lbp , arc consistency  @xcite is the most usual form of local consistency and is related to unit propagation in sat  @xcite .",
    "arc consistency is exact on trees and is usually incrementally maintained during an exact tree search , using re - parametrization . because of the idempotency of logical operators , local consistencies always converge to a unique fix - point .",
    "local consistency properties and algorithms for the weighted csp are very closely related to message passing for map .",
    "they however are always convergent , thanks to suitable calibration properties  @xcite and may also solve tree structured or fully submodular problems .",
    "we mainly discussed methods for exact inference in graphical models .",
    "they are useful if an order for variable elimination with small treewidth is available . in real life applications ,",
    "interaction network are seldom tree - shaped , and their treewidth can be large ( e.g. a grid of pixel in image analysis ) and exact methods can not be applied anymore . however , they are starting points to derive heuristic methods for inference that can be applied to any graphical model . by heuristic method , we mean an algorithm that is ( a priori ) not derived from the optimization of a particular criterion , as opposed to what we will call approximation methods .",
    "nevertheless , we shall alleviate this distinction and show that good performing heursitics can sometimes be interpreted as approximate methods .",
    "for the marginalization task , the most widespread heuristics derived from variable elimination and message passing principles is the loopy belief propagation algorithm ( lbp , * ? ? ?",
    "* ) described in section [ ref : lpb ] , and numerous extensions ( e.g. generalized bp , * ? ? ?",
    "* ) have been proposed since then . in the last decade ,",
    "a better understanding of these heuristics has been reached and they can now be reinterpreted as particular instances of variational approximation methods @xcite .",
    "a variational approximation of a distribution @xmath0 is defined as the best approximation of @xmath0 in a class @xmath249 of tractable distributions ( for inference ) , according to the kllback - leibler divergence .",
    "depending of the application ( e.g. discrete or continuous variables ) , several choices for @xmath249 have been considered .",
    "we are apparently far from variable elimination principles and treewidth issues . however , as we just emphasized , lbp can be cast in the variational framework .",
    "the treewidth of the chosen variational distribution depends on the nature of the variables : @xmath250 in the case of discrete variables the treewidth is low : the class @xmath249 is in the majority of cases that of independent variables ( mean field approximation ) , with associated treewidth of 0 , and some works consider a class @xmath249 with associated treewidth of 1 ; @xmath251 in the case of continuous variables , the treewidth of the variational distribution is the same as in the original model : @xmath249 is in general a class of multivariate gaussian distributions , for which numerous inference tools are available .",
    "we will illustrate these remarks in section [ sec : chmm ] . before that in the remainder of this section , we recall the two key component for a variational approximation method : the kllback - leibler divergence and the choice of a class of tractable distributions .",
    "we also explain how lbp can be interpreted as a variational approximation method .      the kllback - leibler divergence @xmath252 measures the dissimilarity between two probability distributions @xmath0 and @xmath253 .",
    "@xmath254 is positive , and it is null if and only if @xmath0 and @xmath253 are equal .",
    "let us consider now that @xmath253 is constrained to belong to a family @xmath249 which does not include @xmath0 .",
    "the solution @xmath255 of @xmath256 is then the best approximation of @xmath0 in @xmath249 according to divergence @xmath254 .",
    "if @xmath249 is a set of distributions tractable for inference , marginals , mode or normalizing constant of @xmath257 can be used as approximations of the same quantities on @xmath0 .",
    "for example , let us consider a binary potts model on @xmath4 vertices whose joint distribution is @xmath258 we can derive its so called mean field approximation , corresponding to the class @xmath259 of fully factorized distributions ( i.e. an associated graph of treewidth equal to 0 ) : @xmath260 .",
    "since variables are binary @xmath259 corresponds to joint distributions of independent bernoulli variables with respective parameters @xmath261 , namely for all @xmath253 in @xmath259 , @xmath262 the optimal approximation ( in terms of kllback - leibler divergence ) within this class of distributions is characterized by the set of @xmath263 s which minimize @xmath264 . denoting @xmath265 the expectation with respect to @xmath253 ,",
    "@xmath266 is @xmath267 - \\sum_i a_i x_i - \\sum_{(i , j ) \\in e } b_{ij } x_i x_j\\right ) \\\\   & & = \\sum_i \\left [ q_i \\log q_i + ( 1-q_i ) \\log ( 1-q_i ) \\right ] - \\sum_i a_i q_i - \\sum_{(i , j ) \\in e } b_{ij } q_i q_j.\\end{aligned}\\ ] ] this expectation has a simple form because of the specific structure of @xmath253 .",
    "minimizing it with respect to @xmath263 gives the fixed - point relation that each optimal @xmath268 s must satisfy : @xmath269 = a_i + \\sum_{j:(i , j ) \\in e } b_{ij } q_j^{mf}.\\ ] ] leading to @xmath270 @xmath268 is equal the conditional probability that @xmath271 given that all other variables are fixed to their mean field expected values under distribution @xmath253 , which explain the name of mean field approximation .",
    "note that in general @xmath263 is not equal to the marginal @xmath272 .",
    "the choice of the class @xmath273 is indeed a critical trade - off with opposite desirable properties : it must be large enough to guarantee a good approximation and small enough to contain only manageable distributions .",
    "we will focus in the next section on a particular choice for @xmath249 , the bethe class , that will enable to link the lbp heuristics to variational methods .",
    "other choices are possible and have been used .",
    "for instance , the chow - liu algorithm  @xcite computes the minimum of @xmath264 for @xmath253 a distribution whose associated graph is a spanning tree of the graph of @xmath0 .",
    "this amounts to computing the best approximation of @xmath0 among graphical models with treewidth equal to 1 . in the structured mean field approximation  @xcite the distribution of a factorial hidden markov model",
    "is approximated in a variational approach : the multivariate hidden state is decoupled and the variational distribution of the conditional distribution of hidden states is that of independent markov chains ( here again , the treewidth is equal to 1 ) . finally , an alternative to treewidth reduction is to choose the variational approximation in the class of exponential distributions this has been applied for gaussian process classification  @xcite using a multivariate gaussian approximation of the posterior distribution of the hidden field .",
    "this relies on the use of the ep algorithm  @xcite .",
    "note that in this algorithm , @xmath274 is minimized instead of @xmath264 .",
    "the mean field approximation is the most naive approximation among the so - called kikuchi approximations from statistical mechanics , also known as cluster variational methods ( cvm , * ? ? ?",
    "originally , they are not defined by a minimization of the kllback - leibler divergence , but as an approximation of the minimum of the free energy @xmath275 , @xmath276 the two problems are equivalent since @xmath275 is equal to @xmath277 and is minimum when @xmath278 . if @xmath0 and @xmath253 are pairwise mrf whose associated graph @xmath171 is the same and is a tree , then @xmath279 , where @xmath280 and @xmath281 coherent sets of order 1 and order 2 marginals of @xmath253 , and @xmath282 is the degree of vertex @xmath41 in the tree . in this particular case the bethe free energy , defined as @xmath275 is expressed as ( see * ? ? ?",
    "* ; * ? ? ?",
    "* ) @xmath283 the bethe approximation consists in applying to an arbitrary graphical model the same formula of the free energy than for a tree and minimizing it over the variables @xmath280 and @xmath281 under the constraint that they are probability distributions and that @xmath284 is the marginal of @xmath285 . by extension",
    "the bethe approximation can be interpreted as a variational method associated to the family @xmath286 of unnormalized distributions that can be expressed as @xmath287 with @xmath280 and @xmath281 coherent sets of order 1 and order 2 marginals .",
    "it has been established by @xcite that the fixed points of lbp ( when they exist , convergence is still not well understood , see @xcite and @xcite ) are stationary points of the problem of minimizing the bethe free energy , or equivalently @xmath288 with @xmath253 in the class @xmath286 of distributions .",
    "furthermore @xcite showed that for any class of distributions @xmath249 corresponding to a particular cvm method , it is possible to define a generalized bp algorithm whose fixed points are stationary points of the problem of minimizing @xmath288 in @xmath249 .    the drawback of the lbp algorithm and its extensions  @xcite it that they are not associated with any theoretical bound on the error made on the marginals approximations .",
    "nevertheless , this algorithm is increasingly used for inference in graphical models for its good behavior in practice  @xcite .",
    "it is implemented in several pieces of software for inference in graphical models like libdai  @xcite or opengm2  @xcite .",
    "= [ draw , circle , fill = gray!50 , minimum width=2em , inner sep=0 ] = [ draw , circle , minimum width=2em , inner sep=0 ] = [ draw = gray!80 , circle , minimum width=2em , inner sep=0 , text = gray!80 ] = [ draw , circle , minimum width=2em , color = gray!80 , inner sep=0 ] = [ ] = [ - > , > = latex , line width=1pt ] = [ - , line width=1pt ] = [ - > , > = latex , line width=1pt , fill = gray!80 , color = gray!80 ]    we now illustrate how the the procedures we have described have been used for parameter estimation in a elaborated example : coupled hmm .",
    "consider a set of @xmath289 signals observed at times @xmath290 and denote @xmath291 the variable corresponding to the observed signal @xmath41 at time @xmath292 .",
    "hmm models assume that the distribution of each @xmath291 is conditional to some hidden state @xmath293 , where the series @xmath294 is a markov chain .",
    "coupled hmm further assumes that the hidden states display a correlation at each time ( see * ? ? ?",
    "* ; * ? ? ?",
    "* ) , resulting in the graphical model displayed in  [ fig : coupledhmm ] .",
    "such models have been considered in a series of domains such as bioinformatics @xcite , electroencephalogram analysis @xcite or speech recognition @xcite .",
    "more complex versions are sometimes considered , assuming dependency between two times series at two consecutive time steps . for this model ,",
    "the joint distribution of the hidden variables @xmath295 and observed variables @xmath296 factorizes as @xmath297 where @xmath298 stands the vector of all hidden variables at time @xmath292 and where @xmath299 encodes the markovian dependency of the hidden variables within a series , @xmath300 encodes the coupling between the hidden variables of all series at a given time and @xmath301 encodes the emission of the observed signal given the corresponding hidden state .",
    "a fairly comprehensive exploration of these models can be found in @xcite .",
    "( e1tm2 ) at ( -1 * 3.5 * 2em , 0 ) @xmath302 ; ( h1tm1 ) at ( 0 , 0 ) @xmath303 ; ( h1 t ) at ( 3.5 * 2em , 0 ) @xmath304 ; ( h1tp1 ) at ( 2 * 3.5 * 2em , 0 ) @xmath305 ; ( e1tp2 ) at ( 3 * 3.5 * 2em , 0 ) @xmath302 ;    ( eitm2 ) at ( -1 * 3.5 * 2em , 2 * 2em ) @xmath302 ; ( hitm1 ) at ( 0 , 2 * 2em ) @xmath306 ; ( hit ) at ( 3.5 * 2em , 2 * 2em ) @xmath307 ; ( hitp1 ) at ( 2 * 3.5 * 2em , 2 * 2em ) @xmath308 ; ( eitp2 ) at ( 3 * 3.5 * 2em , 2 * 2em ) @xmath302 ;    ( eitm2 ) at ( -1 * 3.5 * 2em , 2 * 2 * 2em ) @xmath302 ; ( hitm1 ) at ( 0 , 2 * 2 * 2em ) @xmath309 ; ( hit ) at ( 3.5 * 2em , 2 * 2 * 2em ) @xmath310 ; ( hitp1 ) at ( 2 * 3.5 * 2em , 2 * 2 * 2em ) @xmath311 ; ( eitp2 ) at ( 3 * 3.5 * 2em , 2 * 2 * 2em ) @xmath302 ;    ( e1tm2 ) to ( h1tm1 ) ; ( h1tp1 ) to ( e1tp2 ) ; ( h1tm1 ) to ( h1 t ) ; ( h1 t ) to ( h1tp1 ) ; ( eitm2 ) to ( hitm1 ) ; ( hitp1 ) to ( eitp2 ) ; ( hitm1 ) to ( hit ) ; ( hit ) to ( hitp1 ) ; ( eitm2 ) to ( hitm1 ) ; ( hitp1 ) to ( eitp2 ) ; ( hitm1 ) to ( hit ) ; ( hit ) to ( hitp1 ) ;    ( h1tm1 ) to ( hitm1 ) ; ( h1tm1 ) to [ bend left](hitm1 ) ; ( hitm1 ) to ( hitm1 ) ; ( h1 t ) to ( hit ) ; ( h1 t ) to [ bend left](hit ) ; ( hit ) to ( hit ) ; ( h1tp1 ) to ( hitp1 ) ; ( h1tp1 ) to [ bend left](hitp1 ) ; ( hitp1 ) to ( hitp1 ) ;    ( o1tm1 ) at ( .5 * 3.5 * 2em , -.5 * 2 * 2em ) @xmath312 ; ( o1 t ) at ( 1.5 * 3.5 * 2em , -.5 * 2 * 2em ) @xmath313 ; ( o1tp1 ) at ( 2.5 * 3.5 * 2em , -.5 * 2 * 2em ) @xmath314 ; ( oitm1 ) at ( .5 * 3.5 * 2em , .5 * 2 * 2em ) @xmath315 ; ( oit ) at ( 1.5 * 3.5 * 2em , .5 * 2 * 2em ) @xmath316 ; ( oitp1 ) at ( 2.5 * 3.5 * 2em , .5 * 2 * 2em ) @xmath317 ; ( oitm1 ) at ( .5 * 3.5 * 2em , 1.5 * 2 * 2em ) @xmath318 ; ( oit ) at ( 1.5 * 3.5 * 2em , 1.5 * 2 * 2em ) @xmath319 ; ( oitp1 ) at ( 2.5 * 3.5 * 2em , 1.5 * 2 * 2em ) @xmath320 ;    ( h1tm1 ) to ( o1tm1 ) ; ( h1 t ) to ( o1 t ) ; ( h1tp1 ) to ( o1tp1 ) ; ( hitm1 ) to ( oitm1 ) ; ( hit ) to ( oit ) ; ( hitp1 ) to ( oitp1 ) ; ( hitm1 ) to ( oitm1 ) ; ( hit ) to ( oit ) ; ( hitp1 ) to ( oitp1 ) ;      coupled hmm are examples of incomplete data models , as they involve variables @xmath321 where only the variables @xmath110 are observed .",
    "maximum likelihood inference for such a model aims at finding the value of the parameter @xmath322 that maximizes the ( log-)likelihood of the observed data @xmath129 , that is to solve @xmath323 .",
    "the most popular algorithm to achieve this task is the em algorithm , @xcite , that can be rephrased in the following way .",
    "observe that @xmath324 where @xmath325 and @xmath253 stands for any distribution on the hidden variables @xmath95 , @xmath172 stands for the expectation under the true distribution @xmath0 and @xmath265 under the arbitrary distribution @xmath253 .",
    "the em algorithm consists in alternatively maximizing @xmath326 with respect to @xmath253 ( e - step ) and to @xmath322 ( m - step ) .",
    "the solution of the e - step is @xmath327 since the kullback - leibler divergence is minimal ( and null ) in this case .",
    "exact computation of @xmath328 can be performed by observing that ( [ eq : chmm - joint ] ) can be rewritten as @xmath329 where @xmath330 encodes both the markovian dependency and the coupling of the hidden variables within a given time step .",
    "this writing is equivalent to merging all hidden variables of a given time step and corresponds to the graphical model given in  [ fig : coupledhmm - merge ] .",
    "denoting @xmath209 the number of possible values for each hidden variables , we end up with a regular hidden markov model with @xmath331 possible hidden states .",
    "both @xmath328 ( and its mode ) can then be computed in a exact manner with either the forward - backward recursion or the viterbi algorithm , which have the same complexity : @xmath332 .",
    "the exact calculation can therefore be achieved provided that @xmath331 remains small enough , but becomes intractable when the number of signals @xmath289 exceeds few tens .",
    "( etm2 ) at ( -1 * 3.5 * 2em , 0 ) @xmath302 ; ( htm1 ) at ( 0 , 0 ) @xmath333 ; ( ht ) at ( 3.5 * 2em , 0 ) @xmath334 ; ( htp1 ) at ( 2 * 3.5 * 2em , 0 ) @xmath335 ; ( etp2 ) at ( 3 * 3.5 * 2em , 0 ) @xmath302 ;    ( etm2 ) to ( htm1 ) ; ( htp1 ) to ( etp2 ) ; ( htm1 ) to ( ht ) ; ( ht ) to ( htp1 ) ;    ( o1tm1 ) at ( .5 * 3.5 * 2em , -1.7 * 2 * 2em ) @xmath312 ; ( o1 t ) at ( 1.5 * 3.5 * 2em , -1.7 * 2 * 2em ) @xmath313 ; ( o1tp1 ) at ( 2.5 * 3.5 * 2em , -1.7 * 2 * 2em ) @xmath314 ; ( oitm1 ) at ( .5 * 3.5 * 2em , -1.1 * 2 * 2em ) @xmath315 ; ( oit ) at ( 1.5 * 3.5 * 2em , -1.1 * 2 * 2em ) @xmath316 ; ( oitp1 ) at ( 2.5 * 3.5 * 2em , -1.1 * 2 * 2em ) @xmath317 ; ( oitm1 ) at ( .5 * 3.5 * 2em , -.5 * 2 * 2em ) @xmath318 ; ( oit ) at ( 1.5 * 3.5 * 2em , -.5 * 2 * 2em ) @xmath319 ; ( oitp1 ) at ( 2.5 * 3.5 * 2em , -.5 * 2 * 2em ) @xmath320 ;    ( htm1 ) to ( o1tm1 ) ; ( ht ) to ( o1 t ) ; ( htp1 ) to ( o1tp1 ) ; ( htm1 ) to ( oitm1 ) ; ( ht ) to ( oit ) ; ( htp1 ) to ( oitp1 ) ; ( htm1 ) to ( oitm1 ) ; ( ht ) to ( oit ) ; ( htp1 ) to ( oitp1 ) ;      a first approach to derive an approximate e step it to seek for a variational approximation of @xmath328 assuming that @xmath336 is restricted to a family @xmath249 of tractable distributions , as described in section [ sec : variationapprox ] .",
    "this approach results in the maximization of a lower bound of the original log - likelihood .",
    "the choice of @xmath249 is critical and is a balance between approximation accuracy and computation efficiency .",
    "choosing @xmath249 typically amounts to breaking down some dependencies in the original distribution to end up with some tractable distribution . in the case of coupled hmm , the simplest distribution is the class of fully factorized distributions ( i.e. mean field approximation ) , that is @xmath337 such an approximation corresponds to the graphical model of  [ fig : coupledhmm - indep ] .",
    "intuitively , this approximation replaces the stochastic influence existing between the hidden variables by its mean value .",
    "( e1tm2 ) at ( -1 * 3.5 * 2em , 0 ) @xmath302 ; ( h1tm1 ) at ( 0 , 0 ) @xmath303 ; ( h1 t ) at ( 3.5 * 2em , 0 ) @xmath304 ; ( h1tp1 ) at ( 2 * 3.5 * 2em , 0 ) @xmath305 ; ( e1tp2 ) at ( 3 * 3.5 * 2em , 0 ) @xmath302 ;    ( eitm2 ) at ( -1 * 3.5 * 2em , 2 * 2em ) @xmath302 ; ( hitm1 ) at ( 0 , 2 * 2em ) @xmath306 ; ( hit ) at ( 3.5 * 2em , 2 * 2em ) @xmath307 ; ( hitp1 ) at ( 2 * 3.5 * 2em , 2 * 2em ) @xmath308 ; ( eitp2 ) at ( 3 * 3.5 * 2em , 2 * 2em ) @xmath302 ;    ( eitm2 ) at ( -1 * 3.5 * 2em , 2 * 2 * 2em ) @xmath302 ; ( hitm1 ) at ( 0 , 2 * 2 * 2em ) @xmath309 ; ( hit ) at ( 3.5 * 2em , 2 * 2 * 2em ) @xmath310 ; ( hitp1 ) at ( 2 * 3.5 * 2em , 2 * 2 * 2em ) @xmath311 ; ( eitp2 ) at ( 3 * 3.5 * 2em , 2 * 2 * 2em ) @xmath302 ;    ( o1tm1 ) at ( .5 * 3.5 * 2em , -.5 * 2 * 2em ) @xmath312 ; ( o1 t ) at ( 1.5 * 3.5 * 2em , -.5 * 2 * 2em ) @xmath313 ; ( o1tp1 ) at ( 2.5 * 3.5 * 2em , -.5 * 2 * 2em ) @xmath314 ; ( oitm1 ) at ( .5 * 3.5 * 2em , .5 * 2 * 2em ) @xmath315 ; ( oit ) at ( 1.5 * 3.5 * 2em , .5 * 2 * 2em ) @xmath316 ; ( oitp1 ) at ( 2.5 * 3.5 * 2em , .5 * 2 * 2em ) @xmath317 ; ( oitm1 ) at ( .5 *",
    "3.5 * 2em , 1.5 * 2 * 2em ) @xmath318 ; ( oit ) at ( 1.5 * 3.5 * 2em , 1.5 * 2 * 2em ) @xmath319 ; ( oitp1 ) at ( 2.5 * 3.5 * 2em , 1.5 * 2 * 2em ) @xmath320 ;    ( h1tm1 ) to ( o1tm1 ) ; ( h1 t ) to ( o1 t ) ; ( h1tp1 ) to ( o1tp1 ) ; ( hitm1 ) to ( oitm1 ) ; ( hit ) to ( oit ) ; ( hitp1 ) to ( oitp1 ) ; ( hitm1 ) to ( oitm1 ) ; ( hit ) to ( oit ) ; ( hitp1 ) to ( oitp1 ) ;    as suggested in @xcite , a less drastic approximation can be obtained using the distribution family of independent heterogeneous markov chains : @xmath338 which is consistent with the graphical representation of independent hmm , as depicted in  [ fig : coupledhmm - markov ] .",
    "( e1tm2 ) at ( -1 * 3.5 * 2em , 0 ) @xmath302 ; ( h1tm1 ) at ( 0 , 0 ) @xmath303 ; ( h1 t ) at ( 3.5 * 2em , 0 ) @xmath304 ; ( h1tp1 ) at ( 2 * 3.5 * 2em , 0 ) @xmath305 ; ( e1tp2 ) at ( 3 * 3.5 * 2em , 0 ) @xmath302 ;    ( eitm2 ) at ( -1 * 3.5 * 2em , 2 * 2em ) @xmath302 ; ( hitm1 ) at ( 0 , 2 * 2em ) @xmath306 ; ( hit ) at ( 3.5 * 2em , 2 * 2em ) @xmath307 ; ( hitp1 ) at ( 2 * 3.5 * 2em , 2 * 2em ) @xmath308 ; ( eitp2 ) at ( 3 * 3.5 * 2em , 2 * 2em ) @xmath302 ;    ( eitm2 ) at ( -1 * 3.5 * 2em , 2 * 2 * 2em ) @xmath302 ; ( hitm1 ) at ( 0 , 2 * 2 * 2em ) @xmath309 ; ( hit ) at ( 3.5 * 2em , 2 * 2 * 2em ) @xmath310 ; ( hitp1 ) at ( 2 * 3.5 * 2em , 2 * 2 * 2em ) @xmath311 ; ( eitp2 ) at ( 3 * 3.5 * 2em , 2 * 2 * 2em ) @xmath302 ;    ( e1tm2 ) to ( h1tm1 ) ; ( h1tp1 ) to ( e1tp2 ) ; ( h1tm1 ) to ( h1 t ) ; ( h1 t ) to ( h1tp1 ) ; ( eitm2 ) to ( hitm1 ) ; ( hitp1 ) to ( eitp2 ) ; ( hitm1 ) to ( hit ) ; ( hit ) to ( hitp1 ) ; ( eitm2 ) to ( hitm1 ) ; ( hitp1 ) to ( eitp2 ) ; ( hitm1 ) to ( hit ) ; ( hit ) to ( hitp1 ) ;    ( o1tm1 ) at ( .5 * 3.5 * 2em , -.5 * 2 * 2em ) @xmath312 ; ( o1 t ) at ( 1.5 * 3.5 * 2em , -.5 * 2 * 2em ) @xmath313 ; ( o1tp1 ) at ( 2.5 * 3.5 * 2em , -.5 * 2 * 2em ) @xmath314 ; ( oitm1 ) at ( .5 * 3.5 * 2em , .5 * 2 * 2em ) @xmath315 ; ( oit ) at ( 1.5 * 3.5 * 2em , .5 * 2 * 2em ) @xmath316 ; ( oitp1 ) at ( 2.5 * 3.5 * 2em , .5 * 2 * 2em ) @xmath317 ; ( oitm1 ) at ( .5 * 3.5 * 2em , 1.5 * 2 * 2em ) @xmath318 ; ( oit ) at ( 1.5 * 3.5 * 2em , 1.5 * 2 * 2em ) @xmath319 ; ( oitp1 ) at ( 2.5 * 3.5 * 2em , 1.5 * 2 * 2em ) @xmath320 ;    ( h1tm1 ) to ( o1tm1 ) ; ( h1 t ) to ( o1 t ) ; ( h1tp1 ) to ( o1tp1 ) ; ( hitm1 ) to ( oitm1 ) ; ( hit ) to ( oit ) ; ( hitp1 ) to ( oitp1 ) ; ( hitm1 ) to ( oitm1 ) ; ( hit ) to ( oit ) ; ( hitp1 ) to ( oitp1 ) ;    an alternative to the approximate maximization of @xmath339 consists in seeking for the maximum of an approximation @xmath340 of @xmath326 which involves only marginals of @xmath328 on subsets of variables in @xmath95 of limited size .",
    "then the lbp algorithm can be used to provide an approximation of these marginals .",
    "this approach has been proposed in @xcite where the authors approximated the negative entropy term @xmath341 in @xmath326 by its so - called bethe approximation as follows ( the first term in @xmath342 by definition depends only on marginals of variables involved in the potential functions @xmath343 ) .",
    "@xmath344 because each hidden variable @xmath293 has degree @xmath345 in the original graphical model given in figure [ fig : coupledhmm ] .",
    "the advantage of this approach compared to the variational approximations based on families @xmath346 and @xmath347 is that it provides an approximation of the joint conditional distribution of all hidden variables within the same time step .",
    "this tutorial on variable elimination for exact and approximate inference is an introduction to the basic concepts of variable elimination , message passing and their links with variational methods .",
    "it introduces these fields to statisticians confronted with inference in graphical models .",
    "the main message is that exact inference should not be systematically considered as out of reach . before looking for an efficient approximate method",
    ", a wise advice would be to know the treewidth of the graphical model . in practice",
    "this question is not easy to answer exactly but several implementations of the presented algorithms exist and provide an upper bound of the treewidth .",
    "obviously this tutorial is not exhaustive , since we have chosen to focus on the fundamental concepts . while many important results on treewidth and graphical models have several decades in age ,",
    "the area is still lively and we now broaden our discussion to a few recent works which tackle some challenges related to the computation of the treewidth .    because they offer efficient algorithms , graphical models with a bounded treewidth offer an attractive target when the aim is to learn a model that best represents some given sample . in  @xcite ,",
    "the problem of learning the structure of an undirected graphical model with bounded treewidth is approximated by a convex optimization problem .",
    "the resulting algorithm has a polynomial time complexity . as discussed in  @xcite , this algorithm is useful to derive tractable candidate distributions in a variational approach , enabling to go beyond the usual variational distributions with treewidth zero or 1 .    for optimization ( map ) ,",
    "other exact techniques are offered by tree search algorithms such as branch and bound  @xcite , that recursively consider possible conditioning of variables .",
    "these techniques often exploit limited variable elimination processing to prevent exhaustive search , either using message - passing like algorithms  @xcite to compute bounds that can be used for pruning , or by performing `` on - the - fly '' elimination of variables with small degree  @xcite .    beyond pairwise potential functions , the time needed for simple update rules of message passing becomes exponential in the size of the scope of the potential functions .",
    "however , for specific potential functions involving many ( or all ) variables , exact messages can be computed in reasonable time , even in the context of convergent message passing for optimization .",
    "this can be done using polytime graph optimization algorithms such as shortest path or mincost flow algorithms .",
    "such functions are known as global potential functions  @xcite in stochastic graphical models , and as global cost functions  @xcite in deterministic cost function networks .",
    "different problems appears with continuous variables , where counting requires integration of functions . here again , for specific families of distributions , exact ( analytic ) computations can be obtained for distributions with conjugate distributions . for message",
    "passing , several solutions have been proposed .",
    "for instance , a recent message passing scheme proposed by @xcite relies on the combination of orthogonal series approximation of the messages , and the use of stochastic updates .",
    "we refer the reader to references in  @xcite for a state - of - the - art of alternative methods dealing with continuous variables message passing .    finally , we have excluded monte - carlo methods from the scope of our review . but recent sampling algorithms have been proposed that use exact optimization algorithms to sample points with high probability in the context of estimating the partition function . additional control in the sampling method is needed to avoid biased estimations : this may be hashing functions enforcing a fair sampling  @xcite or randomly perturbed potential functions using a suitable noise distribution  @xcite .",
    "p.  austrin , t.  pitassi , and y.  wu . inapproximability of treewidth , one - shot pebbling , and related layout problems . in _",
    "international workshop on approximation algorithms for combinatorial optimization problems ( approx ) , _ , pages 1324 , boston , usa , 2012 .",
    "h.  l. bodlaender , p.  drange , m.  s. dregi , f.  v. fomin , d.  lokshtanov , and m.  pilipczuk . a @xmath348 5-approximation algorithm for treewidth . in _",
    "ieee symposium on foundations of computer science _ , pages 499508 , 2013 .",
    "s.  de  givry , t.  schiex , and g.  verfaillie . exploiting tree decomposition and soft local consistency in weighted csp . in _ proceedings of the national conference on artificial intelligence , aaai-2006 _ , pages 2227 , 2006 .",
    "r.  dechter and j.  pearl .",
    "network - based heuristics for constraint satisfaction problems . in l.",
    "kanal and v.  kumar , editors , _ search in artificial intelligence _",
    ", chapter  11 , pages 370425 .",
    "springer - verlag , 1988 .",
    "s.  ermon , c.  gomes , a.  sabharwal , and b.  selman .",
    "low - density parity constraints for hashing - based discrete integration . in _ proceedings of the 31st international conference on machine learning _ , pages 271279 , 2014 .",
    "n.  friel , a.  n. pettitt , r.  reeves , and e.  wit .",
    "bayesian inference in hidden markov random fields for binary data defined on large lattices .",
    "_ journal of computational and graphical statistics _ , 18:0 243261 , 2009 .",
    "t.  hazan , s.  maji , and t.  jaakkola . on sampling from the gibbs distribution with random maximum a - posteriori perturbations . in _ advances in neural information processing systems _ , pages 12681276 , 2013 .",
    "k.  s.  sesh kumar and f.  bach .",
    "convex relaxations for learning bounded treewidth decomposable graphs . in _ proceedings of the international conference on machine learning ( icml )",
    "_ , atlanta , united states , 2012 .          s.l .",
    "lauritzen and d.j .",
    "spiegelhalter . local computations with probabilities on graphical structures and their application to expert systems .",
    "_ journal of the royal statistical society  series b _ , 50:0 157224 , 1988 .",
    "j.  lee and k.  l. leung . towards efficient consistency enforcement for global constraints in weighted constraint satisfaction . in _ international conference on artificial intelligence _ ,",
    "volume  9 , pages 559565 , 2009 .",
    "j.  m. mooij .",
    "libdai : a free and open source c++ library for discrete approximate inference in graphical models .",
    "_ journal of machine learning research _",
    ", 11:0 21692173 , august 2010 .",
    "url https://staff.fnwi.uva.nl / j.m.mooij / libdai/.          k.  murphy , y.  weiss , and m.  jordan .",
    "loopy belief propagation for approximate inference : an empirical study . in",
    "_ proceedings of the 15th conference on uncertainty in artificial intelligence _ , pages 467475 , 1999 .",
    "n.  noorshams and m.  j. wainwright .",
    "belief propagation for continuous state spaces : stochastic message - passing with quantitative guarantees .",
    "_ journal of machine learning research _ , 140 ( 1):0 27992835 , 2013 .",
    "sintaksicheskiy analiz dvumernykh zritelnikh signalov v usloviyakh pomekh ( syntactic analysis of two - dimensional visual signals in noisy conditions ) .",
    "_ kibernetika _ , 4:0 113130 , 1976 .",
    "r.  tarjan and m.  yannakakis .",
    "simple linear - time algorithms to test chordality of graphs , test acyclicity of hypergraphs and selectively reduce acyclic hypergraphs .",
    "_ siam journal of computing _ , 130 ( 3):0 566579 , 1984",
    ".              t.  werner .",
    "high - arity interactions , polyhedral relaxations , and cutting plane algorithm for soft constraint optimisation ( map - mrf ) . in _",
    "computer vision and pattern recognition , cvpr 2008 _ , pages 18 , alaska , usa , 2008 .",
    "s.  zhong and j.  ghosh .",
    "s and coupled hmms for multi - channel eeg classification . in _ proceedings of the ieee international joint conference on neural networks _ , volume  2 , pages 12541159 , honolulu , hawai , 2002 ."
  ],
  "abstract_text": [
    "<S> probabilistic graphical models offer a powerful framework to account for the dependence structure between variables , which can be represented as a graph . the dependence between variables may render inference tasks such as computing normalizing constant , marginalization or optimization intractable . </S>",
    "<S> the objective of this paper is to review techniques exploiting the graph structure for exact inference borrowed from optimization and computer science . </S>",
    "<S> they are not yet standard in the statistician toolkit , and we specify under which conditions they are efficient in practice . </S>",
    "<S> they are built on the principle of variable elimination whose complexity is dictated in an intricate way by the order in which variables are eliminated in the graph . </S>",
    "<S> the so - called treewidth of the graph characterizes this algorithmic complexity : low - treewidth graphs can be processed efficiently . </S>",
    "<S> algorithmic solutions derived from variable elimination and the notion of treewidth are illustrated on problems of treewidth computation and inference in challenging benchmarks from optimization competitions . </S>",
    "<S> we also review how efficient techniques for approximate inference such as loopy belief propagation and variational approaches can be linked to variable elimination and we illustrate them in the context of expectation - maximisation procedures for parameter estimation in coupled hidden markov models .    * keywords : * graphical model , computational inference , treewidth , message passing , variational approximations </S>"
  ]
}