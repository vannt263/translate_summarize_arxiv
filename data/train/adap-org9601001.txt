{
  "article_text": [
    "william of ockham , a great lover of simple explanations , wrote that `` a plurality is never to be posited except where necessary.''@xcite the aim of this paper is to provide a geometric insight into this principle of economy of thought in the context of inference of parametric distributions .",
    "the task of inferring parametric models is often divided into two parts .",
    "first of all , a parametric family must be chosen and then parameters must be estimated from the available data . once a model family is specified , the problem of parameter estimation , although hard , is well understood - the typical difficulties involve the presence of misleading local minima in the error surfaces associated with different inference procedures . however , less is known about the task of picking a model family , and practitioners generally employ a judicious combination of folklore , intuition , and prior knowledge to arrive at suitable models .",
    "the most important principled techniques that are used for model selection are bayesian inference and the minimum description length principle . in this paper",
    "i will provide a geometric insight into both of these methods and i will show how they are related to each other . in section  [ sec : qual ]",
    "i give a qualitative discussion of the meaning of `` simplicity '' in the context of model inference and discuss why schemes that favour simple models are desirable . in section  [ sec : deriv ] i will analyze the typical behaviour of bayes rule to construct a quantity that will turn out to be a _ razor _ or an index of the simplicity and accuracy of a parametric distribution as a model of a given true distribution . in effect , the razor will be shown to be to be an ideal measure of  distance \" between a model family and a true distribution in the context of parsimonious model selection . in order to define this index",
    "it is necessary to have a notion of measure and of metric on a parameter manifold viewed as a subspace of the space of probability distributions .",
    "section  [ sec : geom ] is devoted to a derivation of a canonical metric and measure on a parameter manifold .",
    "i show that the natural distance on a parameter manifold in the context of model inference is the fisher information .",
    "the resulting integration measure on the parameters is equivalent to a choice of jeffreys prior in a bayesian interpretation of model selection .",
    "the derivation of jeffreys prior in this paper makes no reference to the minimum description length principle or to coding arguments and arises entirely from geometric considerations . in a certain novel sense jeffreys prior is seen to be the prior on a parameter manifold that is induced by a uniform prior on the space of distributions . some relationships with the work of amari et.al .",
    "in information geometry are described.(@xcite , @xcite ) in section  [ sec : largen ] the behaviour of the razor is analyzed to show that empirical approximations to this quantity will enable parsimonious inference schemes .",
    "i show in section  [ sec : meaning ] that bayesian inference and the minimum description length principle are empirical approximations of the razor .",
    "the analysis of this section also reveals corrections to mdl that become relevant when comparing models given a small amount of data .",
    "these corrections have the pleasing interpretation of being measures of the robustness of the model .",
    "examination of the behaviour of the razor also points the way towards certain geometric refinements to the information asymptotics of bayes rule derived by clarke and barron.(@xcite ) close connections with the index of resolvability introduced by barron and cover are also discussed.(@xcite )",
    "since the goal of this paper is to derive a geometric notion of simplicity of a model family it is useful to begin by asking why we would wish to bias our inference procedures towards simple models .",
    "we should also ask what the qualitative meaning of `` simplicity '' should be in the context of inference of parametric distributions so that we can see whether the precise results arrived at later are in accord with our intuitions . for concreteness",
    "let us suppose that we are given a set of @xmath0 outcomes @xmath1 generated i.i.d . from a true distribution  @xmath2 t . in some suitable sense , the empirical distribution of these events will fall with high probability within some ball around  @xmath2 t  in the space of distributions .",
    "( see figure  [ fig1 ] . )",
    "now let us suppose that we are trying to model  @xmath2 t  with one of two parametric families  _ 1@xmath3_1  or  _ 2@xmath4_2 . now  _ 1@xmath3_1  and  _ 2@xmath4_2   define manifolds embedded in the space of distributions ( see figure  [ fig1 ] ) and the inference task is to pick the distribution on  _ 1@xmath3_1  or  _",
    "2@xmath4_2  that best describes the true distribution .    if we had an infinite number of outcomes and an arbitrary amount of time with which to perform the inference , the question of simplicity would not arise .",
    "indeed , we would simply use a consistent parameter estimation procedure to pick the model distribution on  _ 1@xmath3_1   or  _",
    "2@xmath4_2  that gives the best description of the empirical data and that would be guaranteed to give the best model of the true distribution .",
    "however , since we only have finite computational resources and since the empirical distribution for finite @xmath0 only approximates the true , our inference procedure has to be more careful .",
    "indeed , we are naturally led to prefer models with fewer degrees of freedom .",
    "first of all , smaller models will require less computational time to manipulate .",
    "they will also be easier to optimize since they will generically have fewer misleading local minima in the error surfaces associated with the estimation .",
    "finally , a model with fewer degrees of freedom generically will be less able to fit statistical artifacts in small data sets and will therefore be less prone to so - called `` generalization error '' .",
    "another , more subtle , preference regarding models inferred from finite data sets has to do with the `` naturalness '' of the model .",
    "suppose we are using a family  @xmath5 m  to describe a set of @xmath0 outcomes drawn from  @xmath2 t .",
    "if the accuracy of the description depends very sensitively on the precise choice of parameters then it is likely that the true distribution will be poorly modelled by  @xmath5m.(see figure  [ fig2 ] . )",
    "this is for two reasons - 1 ) the optimal choice of parameters will be hard to find if the model is too sensitive to the choice , and 2 ) even if we succeed in getting a good description of one set of sample outcomes , the parameter sensitivity suggests that another sample will be poorly described . in geometric terms",
    ", we would prefer model families which describe a set of distributions all of which are close to the true .",
    "( see figure  [ fig2 ] . ) in a sense this property would make a family a more `` natural '' model of the true distribution  @xmath2 t  than another which approaches  @xmath2 t  very closely at an isolated point .",
    "the discussion above suggests that for practical reasons inference schemes operating with a finite number of sample outcomes should prefer models that give good descriptions of the empirical data , have fewer degrees of freedom and are `` natural '' in the sense discussed above .",
    "i will refer to the first property ( good description ) as _ accuracy _ and the latter two ( fewer degrees of freedom and naturalness ) as _ simplicity_. we will see that both accuracy and simplicity of parametric models can be understood in terms of the geometry of the model manifold in the space of distributions .",
    "this geometric understanding provides an interesting complement to the minimum description length approach , which gives an implicit definition of simplicity in terms of shortest description length of the data and model .",
    "the previous section has discussed the qualitative meaning of simplicity and its practical importance for inference of distributions from a finite amount of data . in this section",
    "we will construct a quantity that is an index of the accuracy and the simplicity of a model family as a description of a given true distribution .",
    "we will show in later sections that empirical approximations of this quantity which we call the _ razor _ of a model will enable consistent and parsimonious inference of parametric probability distributions .",
    "we will now motivate the definition of the razor via a construction from the bayesian approach to model inference .",
    "( in later sections we will conduct a more precise analysis of the relationship between the razor and bayes rule . )",
    "suppose we are given a collection of outcomes @xmath6 drawn independently from a true density  @xmath2 t , defined with respect to lebesgue measure on @xmath7 .",
    "suppose also that we are given two parametric families of distributions a and b and we wish to pick one of them as the model family that we will use .",
    "the bayesian approach to this problem consists of computing the posterior conditional probabilities @xmath8 and @xmath9 and picking the family with the higher probability .",
    "the conditional probabilities depend , of course , on the specific outcomes , and so in order to understand the most likely result of an application of bayes rule we should analyze the statistics of the posterior probabilities . let a be parametrized by a set of parameters @xmath10 .",
    "then bayes rule tells us that : @xmath11 in this expression @xmath12 is the prior probability of the model family , @xmath13 is a prior density with respect to lebesgue measure on the parameter space and @xmath14 is a prior density on the @xmath0 outcome sample space .",
    "the lebesgue measure induced by the parametrization of the @xmath15 dimensional parameter manifold is denoted @xmath16 .",
    "since we are interested in comparing @xmath8 with @xmath9 , the prior @xmath17 is a common factor that we may omit and for lack of any better choice we take the prior probabilities of a and b to be equal and omit them . in order to analyze the typical behaviour of equation  [ eq : bayes1 ]",
    "observe that @xmath18 $ ] .",
    "define @xmath19 and @xmath20 .",
    "we see that @xmath21 is the sum of @xmath0 identically distributed , independent random variables .",
    "consequently , as @xmath0 grows large the central limit theorem applies and we can write down the probability distribution for @xmath22 as : @xmath23 where @xmath24 and @xmath25 are the mean and standard deviation of the @xmath26 in the true distribution and are defined as @xmath27 and @xmath28 . the most likely value of @xmath21 is @xmath29 and @xmath24 can be written in the following pleasing form : @xmath30 where @xmath31 , the differential entropy of the true distribution , is assumed finite , and @xmath32 is the relative entropy or kullback - liebler distance between @xmath2 and the distribution indexed by @xmath33 .",
    "this suggests that the following quantity is worthy of investigation : @xmath34 ( the superscript @xmath35 is intended to indicate that equation  [ eq : cand ] is a _ candidate _",
    "razor that we will improve in the subsequent discussion . )",
    "we will see in section  [ sec : relbayes ] that @xmath36 is closely related to the typical asymptotics of @xmath37 . in the business of comparing @xmath38 and @xmath39",
    ", @xmath40 is a common factor .",
    "so we drop it and also note that in the absence of any prior information the most conservative choice for @xmath13 appears to be the uniform prior on the parameter manifold .",
    "( we will return to examine this point critically and we will find that the natural prior is not in fact uniform in the parameters . )",
    "so we finally write our candidate razor as : @xmath41 we have assumed a compact parameter manifold so that the uniform distribution on the surface can be written as one over the volume .",
    "we take the integration measure @xmath16 to be the lebesgue measure induced on the manifold by the atlas defined via the parametrization .",
    "these definitions can be extended to non - compact parameter manifolds with a little bit of care , but we will not do this here .",
    "the quantity @xmath36 defined in equation  [ eq : raz1 ] is our candidate for a natural meaure of the accuracy as well as the simplicity of a parametric model distribution .",
    "the construction of the razor in this section is intended to be motivational .",
    "we will see in section  [ sec : relbayes ] that the razor is closely related to the typical asymptotics of the logarithm of @xmath42 .",
    "note that the razor is not a quantity that is estimated from data - it is a theoretical measure of complexity like the `` index of resolvability '' introduced by barron and cover and discussed in section  [ sec : mincomp].(@xcite ) we will show that an accurate estimator of the razor can be used to implement consistent and parsimonious inference of probability distributions .",
    "there is a major difficulty with an interpretation of @xmath36 in equation  [ eq : raz1 ] as an intrinsic measure of qualities such as the simplicity of a parametric family .",
    "this difficulty arises because we have not defined the integration measure sufficiently carefully . to see this in pedestrian terms ,",
    "consider a family with two parameters , @xmath43 and @xmath44 , for which the naive integration measure in the razor would be @xmath45 .",
    "we could do the integration in polar coordinates ( @xmath46 and @xmath47 ) , in which case the measure would be @xmath48 .",
    "now the model could have been specified in the first place in terms of the coordinates @xmath46 and @xmath47 in which case the naive integration measure would have been @xmath49 which will clearly yield a different definition of the razor . in other words ,",
    "the razor as defined above is not reparametrization invariant and consequently measures something about both the model family and its parametrization . in order to define the razor as an intrinsic measure of the simplicity of a parametric distribution",
    "we need to have an invariant integration measure on the parameter manifold .",
    "this is easily achieved - if we know how to introduce a metric on the surface , the metric will induce a measure with required properties .",
    "but what is the correct metric on the parameter manifold ?",
    "since the model is embedded in the space of distributions , the metric on the manifold should be induced from a natural distance in the space of distributions .",
    "further insight into this issue is obtained by considering the bayesian construction of the razor .",
    "in the course of this construction we assumed a uniform prior on the parameter manifold .",
    "now the manifold itself is some parameter invariant object that lives in the space of distributions .",
    "let a be a set of distributions in the parameter manifold .",
    "the uniform prior associated with different parametrizations will assign different measures to the set a. on the one hand we could say that the choice of parametrization of a model involves an implicit choice of measure on the manifold and that a parameter dependence is therefore to be expected in bayesian methods",
    ". however , it seems more correct to say that we did not actually mean to say that all _ parameters _ are equally likely - our intention was to say that in the absence of any prior information , all _ distributions _ are equally likely . in other words , to apply bayesian methods properly to the task of model inference , we have to find a way of assigning a uniform prior in the space of distributions and induce from that a measure on parameter manifolds .    in the next section",
    "we will use the observations made in the previous paragraphs to derive a metric and a measure on the parameter manifold that make the razor a parameter - invariant measure of the simplicity of a model",
    ". we will find that the natural metric on the parameter manifold is the fisher information on the surface and the reparametrization invariant razor is consequently given by : @xmath50 where @xmath51 is the fisher information matrix .",
    "the work of rao , fisher , amari and others has previously suggested this choice of measure and metric.(@xcite , @xcite ) . however , as pointed out by these authors , there are many potential choices of metrics on parameter manifolds and the choice of a metric requires careful justification . once a metric is chosen the standard apparatus of differential geometry may be unfolded and statistical interpretations can be attached to geometric quantitites . in the following sections i provide justifications for the choice of the fisher information as the metric appropriate to model estimation .",
    "the choice of @xmath52 as the integration measure is equivalent to a choice of jeffreys prior in the bayesian interpretation of the razor.(@xcite , @xcite ) jeffreys recommends this choice of prior because , as we will note in later sections , its definition guarantees the reparametrization invariance of the bayesian posterior .",
    "however , the requirement of reparametrization invariance alone does not uniquely fix the prior - _ any _ prior which is defined to have suitable transformation properties under reparametrizations of a model will yield the desired invariance .",
    "indeed , jeffreys considers priors related to different distances on the space of distributions including the @xmath53 norms and the relative entropy distance .",
    "i will show that choosing a jeffreys prior is equivalent to assuming equal prior likelihood of all _ distributions _ as opposed to equal prior likelihood of parameters .    from the point of view of statistical mechanics",
    "the razor can be interpreted as a partition function with energies given by the relative entropy and temperature @xmath54 . since temperature regulates the size of fluctuations in physical systems as does the number of events in statistical systems , this analogy makes good sense . in section  [ sec : largen ] we exploit the techniques of statistical mechanics to develop a systematic series expansion for the razor .",
    "the geometrical interpretation will become more clear as the reader proceeds further .",
    "in this section i will derive a natural metric and measure on a parameter manifold .",
    "we will see that the fisher information is the natural metric and the natural measure is associated with this metric .",
    "the fisher information has a long history as a local measure of distance in the space of distributions starting with the cramer - rao bounds .",
    "the work of fisher , rao , amari and others has elucidated the role of geometry in statistics ( @xcite,@xcite ) and there is a sizable literature on the construction and interpretation of geometric quantities in information theory . however , since there are many potential metrics in the space of distributions the important issue is to determine which metric is appropriate to a given problem .",
    "once this is done , the theory of riemannian manifolds provides the necessary technology for manipulating parametric families in the space of distributions and the difficult task is to identify the geometric quantities of interest to statistics . in this section",
    "we will present two derivations of the natural integration measure on a parameter manifold in the context of density estimation .",
    "it is useful to start with a somewhat heuristic derivation that recapitulates arguments made in the `` information geometry '' literature .",
    "( @xcite , @xcite ) let us start by assuming that in the context of model inference , the natural distance on the space of distributions is the relative entropy @xmath55 .",
    "unfortunately , @xmath56 does not define a metric since it is not symmetric and does not obey triangle inequalities except in special cases .",
    "however , as we shall see , @xmath55 will induce a riemannian metric on a parameter manifold given suitable technical conditions",
    ". let @xmath57 be a manifold in the space of distributions with local coordinates @xmath58 .",
    "let @xmath59 be a fixed point on @xmath57 and @xmath60 be any other point .",
    "then the relative entropy between @xmath59 and @xmath60 , @xmath61 is a non - negative function of @xmath62 that attains its minimum at @xmath63 and the value of the minimum is zero .",
    "this means that the zeroth and first order terms in the taylor expansion of @xmath61 at @xmath59 vanish identically . letting @xmath64 , and assuming twice differentiability of the relative entropy in a neighbourhood of @xmath59",
    ", we can taylor expand to second order : @xmath65 \\delta\\theta^i \\delta\\theta^j                                  \\end{aligned}\\ ] ] in the above equation as in all future equations , repeated indices are implicitly summed over .",
    "so , for example , there is an implicit sum on @xmath66 and @xmath67 in equation  [ eq : taylor ] .",
    "the first term in this equation vanishes if the derivatives with respect to @xmath68 and @xmath69 commute with the integral ] .",
    "we assume this commutativity and recognize the remaining term as one - half times the fisher information on the parameter manifold @xmath70 . of the symmetrized relative entropy @xmath71 . in this case",
    "there is no need to make any further assumptions about commutativity of the derivatives and integral . ]",
    "we have found that if we accept that the relative entropy is the natural measure of distance between distributions in the context of model estimation , the induced distance between nearby points on a parameter manifold is @xmath72 to leading order in @xmath73 .",
    "since the fisher information appears in this expression as a quadratic form , it is tempting to interpret it as the natural metric on the surface .",
    "we will only consider consider models where the determinant of the fisher information is non - vanishing everwhere on the surface .",
    "this non - degeneracy condition essentially guarantees that nearby points on a model manifold describe sufficiently different distributions . since we derived the fisher information metric from a taylor expansion at the minimum of a function",
    "we conclude that for the non - degenerate models that are of interest to us , the fisher information is a positive definite metric on the model manifold .",
    "therefore , we can appeal to the standard theory of riemannian geometry to observe that the reparametrization invariant integration measure on the manifold is @xmath52 where @xmath51 is the fisher information . putting this measure into equation  [ eq : raz1 ] for the razor",
    ", we immediately get equation  [ eq : raz2 ] which is now coordinate - independent and a candidate for a measure of some intrinsic properties of a parametric distribution .",
    "the bayesian derivation of the razor of a model provides good intuitions for a more careful derivation of the integration measure .",
    "as we have discussed , in the bayesian interpretation we would like to say that all distributions are equally likely . if this is the case we should give equal weight to all distinguishable distributions on a model manifold .",
    "however , nearby parameters index very similar distributions .",
    "so let us ask the question , `` how do we count the number of distinct distributions in the neighbourhood of a point on a parameter manifold ? ''",
    "essentially , this is a question about the embedding of the parameter manifold in the space of distributions .",
    "points that are distinguishable as elements of @xmath74 may be mapped to indistinguishable points ( in some suitable sense ) of the embedding space .    to answer the question let us take @xmath59 and @xmath60 to be points on a parameter manifold .",
    "since we are working in the context of density estimation a suitable measure of the distinguishability of @xmath75 and @xmath62 should be derived by taking @xmath0 data points drawn from either @xmath59 or @xmath60 and asking how well we can guess which distribution produced the data .",
    "if @xmath59 and @xmath60 do not give very distinguishable distributions , they should not be counted separately in the razor since that would count the same distribution twice .",
    "precisely this question of distinguishability is addressed in the classical theory of hypothesis testing .",
    "suppose @xmath76 are drawn iid from one of @xmath77 and @xmath78 with @xmath79 .",
    "let @xmath80 be the acceptance region for the hypothesis that the distribution is @xmath77 and define the error probabilities @xmath81 and @xmath82 .",
    "( @xmath83 is the complement of @xmath84 in @xmath85 and @xmath86 denotes the product distribution on @xmath85 describing @xmath0 iid outcomes drawn from @xmath87 . ) in these definitions @xmath88 is the probability that @xmath77 was mistaken for @xmath78 and @xmath89 is the probability of the opposite error .",
    "stein s lemma tells us how low we can make @xmath89 given a particular value of @xmath88 .",
    "indeed , let us define : @xmath90 then stein s lemma tells us that : @xmath91 by examining the proof of stein s lemma ( @xcite ) we find that for fixed @xmath92 and sufficiently large @xmath0 the optimal choice of decision region places the following bound on @xmath93 : @xmath94 where @xmath95 for sufficiently large @xmath0 .",
    "the @xmath96 are any sequence of positive constants that satisfy the property that : @xmath97 for all sufficiently large @xmath0 .",
    "the strong law of large number numbers tells us that @xmath98 converges to @xmath99 almost surely since @xmath100 .",
    "almost sure convergence implies convergence in probability so that for any fixed @xmath101 we have : @xmath102 for all sufficiently large n. for a fixed @xmath92 and a fixed @xmath0 let @xmath103 be the collection of @xmath104 which satisfy equation  [ eq : deltas2 ] .",
    "let @xmath105 be the infimum of the set @xmath103 .",
    "equation  [ eq : deltas2 ] guarantees that for any @xmath106 , for any sufficiently large @xmath0 , @xmath107 .",
    "we conclude that @xmath105 chosen in this way is a sequence that converges to zero as @xmath108 while satisfying the condition in equation  [ eq : deltas ] which is necessary for proving stein s lemma .",
    "we will now apply these facts to the problem of distinguishability of points on a parameter manifold .",
    "let @xmath75 and @xmath62 index two distributions on a parameter manifold and suppose that we are given @xmath0 outcomes generated independently from one of them .",
    "we are interested in using stein s lemma to determine how distinguishable @xmath75 and @xmath62 are . by stein s lemma : @xmath109 where we have written @xmath110 and @xmath111 to emphasize that these quantities are functions of @xmath62 for a fixed @xmath75 .",
    "let @xmath112 be the average of the upper and lower bounds in equation  [ eq : st1 ] .",
    "then @xmath113 because the @xmath110 have been chosen to satisfy equation  [ eq : deltas ] .",
    "we now define the set of distributions @xmath114 where @xmath115 is some fixed constant .",
    "note that as @xmath116 , @xmath117 for @xmath118 .",
    "we want to show that @xmath119 is a set of distributions which can not be very well distinguished from @xmath75 .",
    "the first way to see this is to observe that the average of the upper and lower bounds on @xmath120 is greater than or equal to @xmath121 for @xmath118 .",
    "so , in this loose , average sense , the error probability @xmath93 exceeds @xmath122 for @xmath118 .",
    "more carefully , note that @xmath123 by choice of the @xmath110 .",
    "so , using equation  [ eq : st1 ] we see that @xmath124 .",
    "exponentiating this inequality we find that : @xmath125^{(1/n ) }          \\geq         ( \\beta^*)^{(1/n ) } \\ , e^{-\\delta_{\\epsilon n}(\\theta_q)}\\ ] ] the significance of this expression is best understood by considering parametric families in which , for every @xmath62 , @xmath126 is a random variable with finite mean and bounded variance , in the distribution indexed by @xmath75 . in that case , taking @xmath127 to be the bound on the variances , chebyshev s inequality says that : @xmath128 in order to satisy @xmath129 it suffices to choose @xmath130 .",
    "so , if the bounded variance condition is satisfied , @xmath131 for any @xmath62 and therefore we have the limit @xmath132 . applying this limit to equation  [ eq : expbound ] we find that : @xmath133^{(1/n ) } \\geq 1 \\times \\lim_{n \\rightarrow \\infty } \\inf_{\\theta_q \\in u_n }         e ^{-\\delta_{\\epsilon n}(\\theta_q ) } = 1\\ ] ] in summary we find that @xmath134^{(1/n ) } = 1 $ ] .",
    "this is to be contrasted with the behaviour of @xmath111 for any _ fixed _",
    "@xmath135 for which @xmath136^{(1/n ) } = \\exp{- d(\\theta_p\\|\\theta_q ) } < 1 $ ] .",
    "we have essentially shown that the sets @xmath119 contain distributions that are not very distinguishable from @xmath75 .",
    "the smallest one - sided error probability @xmath93 for distinguishing between @xmath75 and @xmath137 remains essentially constant leading to the asymptotics in equation  [ eq : asstein ] .",
    "define @xmath138 so that we can summarize the region @xmath119 of high probability of error @xmath122 at fixed @xmath92 as @xmath139 . , @xmath140 and @xmath141 in that order . ]",
    "as n grows large for fixed @xmath142 , the distributions @xmath75 and @xmath62 must be close in relative entropy sense and so we can write @xmath143 and taylor expand the relative entropy on the manifold near @xmath59 . by arguments identical to those made in section  [ sec : reldist ] we conclude that @xmath144 where , as before , we have used the index summation convention and defined the fisher information @xmath145 from the matrix of second derivatives of the relative entropy .",
    "for the assumptions concerning differentiability and commutation of derivatives and integrals .",
    "] so , the nearly indistinguishable region @xmath119 around @xmath75 is summarized by @xmath146 , which defines the interior of an ellipsoid on the parameter manifold . for large @xmath0 , @xmath147 is small , and so , since the manifold is locally euclidean , the volume of this ellipsoid is given by :    @xmath148    we refer to  _",
    ", ^ * , n@xmath149v_,^ * , n  as the volume of indistinguishability at levels @xmath92 , @xmath122 and @xmath0 .",
    "it measures the volume of parameter space in which the distributions are indistinguishable from @xmath75 with error probabilities @xmath129 and @xmath150 , given @xmath0 sample events .    if @xmath122 is very close to one , the distributions inside  _ , ^ * , n@xmath149v_,^ * , n  are not very distinguishable and should not be counted separately in the razor .",
    "( equivalently , the bayesian prior should not treat them as separate distributions . )",
    "we wish to construct a measure on the parameter manifold that reflects this indistinguishability .",
    "we will also assume a principle of  translation invariance \" in the space of distributions by supposing that volumes of indistinguishability at given values of @xmath0 , @xmath122 and @xmath92 should have the same measure regardless of where in the space of distributions they are centered .",
    "in what follows we will define a sequence of measures that reflect indistinguishability and translation invariance at each level @xmath122 , @xmath92 and @xmath0 in the space of distributions .",
    "the continuum measure on the manifold is obtained by considering the limits of integrals defined with respect to this sequence of measures .",
    "we begin with the lebesgue measure induced on the model manifold by the parameter embedding in @xmath151 .",
    "for convenience we will assume that the model manifold can be covered by a single parameter patch so that issues of consistent sewing of patches do not arise .",
    "a real function on the model manifold will be called a _ step _ map with respect to a finite , lebesgue measurable partition of the manifold if it is constant on each set in the partition . for any lebesgue measurable function @xmath87",
    "there is a sequence of step maps that converges pointwise to @xmath87 almost everywhere and in @xmath152.(@xcite ) we will assume that the fisher information matrix @xmath51 is non - singular everywhere and is component - wise lebesgue measurable",
    ". the determinant of @xmath51 will consequently be everywhere finite and also lebesgue measurable .",
    "let @xmath87 be a step map with respect to some partition @xmath153 of the model manifold and let @xmath145 be a fisher information matrix which is non - singular everywhere and a step map with respect to a partition @xmath154 .",
    "let @xmath155 be a partition of the manifold such that if @xmath156 then both @xmath87 and @xmath51 are constant on @xmath157 .",
    "consider fixed values of @xmath122 , @xmath92 and @xmath0 in the above definition of the volumes of indistinguishability . at fixed @xmath122 , @xmath92 and @xmath0",
    "we would like to define a measure @xmath158 by covering the sets @xmath159 economically with volumes of indistinguishability and placing delta functions at the center of each volume in the cover .",
    "such a definition would give each volume of indistinguishability equal weight in an integral over the model manifold and would ignore variations in an integrand on a scale smaller than these volumes . as such , the definition would reflect the properties of indistinguishability and translation invariance at fixed @xmath122 , @xmath92 and @xmath0 . as the volumes of indistinguishability",
    "shrink we could hope to define a continuum limit of this discrete sequence of measures .",
    "the following discussion gives a careful prescription for carrying out this agenda .",
    "the argument should be regarded as a `` construction '' consistent with the principles of indistinguishability and translation invariance rather than as a `` derivation '' .    since the program outlined above involves covering arbitrary measurable subsets of @xmath151 with volumes of indistinguishability , we begin by amassing some useful facts about covers of @xmath151 by spheres .",
    "( see  @xcite ) let @xmath160 be a cover of @xmath151 by spheres of radius @xmath46 .",
    "let @xmath161 have finite lebesgue measure and let @xmath162 be the number of sphere in @xmath160 that intersect @xmath163 .",
    "define the covering density of @xmath163 induced by @xmath160 , @xmath164 , to be : @xmath165 where @xmath166 is the volume of a @xmath15 dimensional sphere of radius @xmath46 and @xmath167 is the lebesgue measure of @xmath163 .",
    "let @xmath168 be a square of side @xmath169 centered at any point in @xmath151 .",
    "for a fixed covering radius @xmath46 , let @xmath170 and let @xmath171 be the number of spheres of @xmath160 that intersect @xmath168 . then define the covering density of @xmath151 induced by @xmath160 to be : @xmath172 so long as this limit exists .",
    "( usually the limit @xmath173 is taken , but we will find the current formulation easier to work with . ) let a _",
    "minimal _ cover of @xmath151 with spheres of radius @xmath46 be a cover that attains the minimum possible @xmath174 over all covers @xmath160 .",
    "this minimal density is independent of @xmath46 and so we will write it as @xmath175 . to show this independence ,",
    "suppose that there is an @xmath46 dependence and that the minimal densities for @xmath176 and @xmath177 have the relationship @xmath178 .",
    "then by rescaling the coordinates of @xmath151 by @xmath179 we can convert the cover by spheres of radius @xmath176 into a cover by spheres of radius @xmath177 .",
    "however , equation  [ eq : rdens ] shows that the density of the cover would remain unchanged since the sides of the squares @xmath169 would increase in length by @xmath179 .",
    "this would give a cover with radius @xmath177 whose density is less than the density @xmath180 implying that the latter density can not be minimal .",
    "it is well known that the minimal density for covering @xmath151 with spheres , @xmath175 , is greater than @xmath181 so that the volumes of indistinguishability used in covering parameter manifolds will necessarily intersect each other .",
    "we will pick the minimal cover using volumes of indistinguishability in order to minimize overcounting of distributions in the measure that will be derived via the construction presented in this paper .    the construction of a measure on a parameter manifold that respects indistinguishability and translation invariance requires the property that the density of the covering by @xmath160 of any disjoint union of sufficiently large squares approaches @xmath175 when the limit in equation  [ eq : rdens ] exists .",
    "indeed , the following lemma is easy to show :    [ lemma : constr ] let @xmath160 be a covering of @xmath151 by spheres of radius @xmath46 for which @xmath174 exists .",
    "then for any @xmath182 there is a @xmath183 such that if @xmath184 is a finite union of squares intersecting at most on their boundaries and each of whose sides exceeds @xmath185 satisfying @xmath186 , then @xmath187 .    * proof : * let @xmath168 be a square of side @xmath169 and let @xmath188 be the number of spheres in @xmath160 that intersect @xmath168 .",
    "let @xmath189 be the number of spheres that intersect the boundary of @xmath168 .",
    "take @xmath190 to be a square of side @xmath191 , centered at the same location as @xmath168",
    ". then @xmath192 . by equation  [ eq : rdens ] , for any @xmath193",
    ", we can pick @xmath194 to be small enough so that : @xmath195 where @xmath196 .",
    "writing @xmath197 and using the upper bound in equation  [ eq : asdf1 ] with the lower bound in equation  [ eq : asdf ] we find : @xmath198 solving for @xmath199 we find that : @xmath200   +    \\frac{\\epsilon^\\prime}{v_d(r ) } \\left [ 1 + ( 1 - 2r / l)^d\\right]\\ ] ] this tells us that @xmath199 can be made as small as desired by picking sufficiently small @xmath193 and @xmath170 .",
    "finally , let s be any finite union of squares @xmath201 of sides @xmath202 where every @xmath202 exceeds some given @xmath185 and the @xmath203 intersect at most on their boundaries .",
    "taking @xmath204 to be the number of spheres in @xmath160 intersecting @xmath203 , with @xmath205 the number of spheres intersecting the boundary , we have the following bound on the density of the cover of of @xmath184 : @xmath206 by picking @xmath207 to be small enough we can make @xmath208 as close as we want to @xmath174 and @xmath209 as close as we want to zero .",
    "consequently , since all the sums in equation  [ eq : lllk ] are finite we can see that for any choice of @xmath182 , for sufficiently small @xmath207 , @xmath210 .",
    "this proves the lemma .",
    "@xmath211    lemma  [ lemma : constr ] has given us some understanding of covers of finite unions of squares .",
    "the next lemma gives control over covers of arbitrary lebesgue measurable subsets of @xmath151 .",
    "the basic difficulty that we must confront is that there are subsets of @xmath151 of lebesgue measure zero for which the covering density is not well defined .",
    "since we are interested in integration on parameter manifolds it is natural that such sets of measure zero will not contribute to the integral over the manifold .",
    "the following lemma shows how to find well - behaved subsets of any lebesgue measurable set .",
    "[ lemma : seq ] let @xmath175 be the minimal density for covering @xmath151 by spheres .",
    "let @xmath212 be any sequence of covers of @xmath151 such that @xmath213 as @xmath214 and @xmath215 for every @xmath66 . take @xmath216 to have a finite lebesgue measure .",
    "then there exists a sequence @xmath217 such that a ) @xmath218 and b ) @xmath219 .",
    "* proof : * let @xmath216 have finite lebesgue measure .",
    "let @xmath220 be the closure of the interior of @xmath221 which differs from @xmath221 at most by a set of measure zero .",
    "then @xmath163 can be written as a countable union of squares @xmath203 each of which has finite measure and which intersect at most on their boundaries .",
    "let @xmath222 be the set of these squares that have side greater than @xmath223 .",
    "it is clear that @xmath224 and that @xmath225 .",
    "this proves the first part of the lemma .",
    "each @xmath226 is a finite union of squares of side greater than @xmath223 that intersect at most on their boundaries .",
    "so , by lemma  [ lemma : constr ] , for any @xmath182 there is a @xmath227 such that if @xmath228 , then @xmath229 since @xmath213 in the limit @xmath230 , @xmath231 .",
    "this proves the second part of the lemma .",
    "we have found that in the limit that the radius of covering spheres @xmath46 goes to zero , any subset of @xmath151 of finite lebesgue measure can be covered up to a set of measure zero with a minimal thickness @xmath175 .",
    "we will now use this lemma to construct a measure on a parameter manifold that reflects indistinguishability and translation invariance .",
    "define a _ regular sequence _ @xmath232 of a lebesgue measurable set @xmath221 to be one of the sequences @xmath232 whose existence was shown in lemma  [ lemma : seq ] .",
    "now consider one of the sets @xmath233 in which the function @xmath87 and the fisher information @xmath145 are constant . by rescaling the coordinates of @xmath233 by @xmath145 we transform the volumes of indistinguishability into spheres of volume : @xmath234 and change the measure of @xmath235 from @xmath236 to @xmath237 where @xmath24 is the lebesgue measure in the original coordinates .",
    "now suppose that we want to integrate the step function @xmath87 over the measurable domain @xmath238 .",
    "let @xmath239 and let @xmath240 be a regular sequence of @xmath241 .",
    "the transformed coordinates define an embedding of @xmath241 into @xmath151 and we consider a minimal covering of @xmath151 by transformed volumes of indistinguishability @xmath242 .",
    "this minimal covering induces a cover of @xmath233 and therefore of each @xmath243 .",
    "we define a measure @xmath244 at levels @xmath245 and @xmath246 for integration over @xmath238 by placing a delta function at some point in the intersection of each covering sphere and @xmath243 .",
    "this yields the following definition of integration of the step function @xmath87 :    [ def : def1 ] let @xmath247 be the sets on which the step maps @xmath87 and @xmath145 are both constant . then , at levels of indistinguishability @xmath92 , @xmath122 and @xmath0 , and at level @xmath246 in a regular sequence of each @xmath233 , we define the integral of @xmath87 over the measurable domain @xmath238 to be : @xmath248 where @xmath249 is the number of spheres that intersect @xmath250 in the cover of @xmath151 by the spheres @xmath242 .",
    "we are actually interested in a measure @xmath251 normalized so that the integral of @xmath181 over the entire manifold gives unity .",
    "the normalization is easily achieved by dividing equation  [ eq : int1 ] by the integral of @xmath181 over the manifold @xmath252 .",
    "[ def : def2 ] let @xmath253 be a regular sequence of @xmath233 and let @xmath254 be the number of spheres that intersect @xmath255 in the cover of @xmath151 by the spheres @xmath242 .",
    "the normalized integral of the step function @xmath87 over the domain @xmath238 is given by : @xmath256    the division by @xmath257 is motivated by our desire to take the limit @xmath258 .",
    "the definition in equation  [ eq : int2 ] reflects the properties of indistinguishability and translation invariance by ignoring variations on a scale smaller than the volumes @xmath259 and giving equal weight in the integral to all such volumes .",
    "we begin by taking the limit @xmath116 so that the definition of the integral reflects indistinguishability in the limit of an infinite amount of data .",
    "this is followed by the limit @xmath260 so that the entire domains @xmath241 are included in the integral up to a set of measure zero .",
    "then we will take the limits @xmath141 so that we are working with truly indistinguishable distributions .",
    "finally we will take @xmath152 completions of @xmath87 and @xmath145 to arrive at the defintion of integration of any lebesgue measurable @xmath87 over a parmeter manifold with lebesgue measurable and non - singular fisher information .",
    "the result of this sequence of limits is summarized in the following theorem .",
    "[ th : measure ] let @xmath24 be the lebesgue measure on a parameter manifold @xmath252 that is induced by the parametrization .",
    "let @xmath87 be any lebesgue measurable function on the manifold and let the fisher information @xmath145 be lebesgue measurable and non - singular everywhere on the manifold .",
    "let @xmath261 be the normalized measure on @xmath252 that measures the volume of distinguishable distributions indexed by the parameters . then @xmath261 is absolutely continuous with respect to @xmath24 and if @xmath238 is any lebesgue measurable set , then : @xmath262    * proof : * first of all , observe that the volumes @xmath242 used in covering the @xmath243 have radius @xmath263 .",
    "consequently , the sequence of covers by @xmath242 for increasing @xmath0 and the sets of the regular sequence @xmath264 satisfy the conditions of lemma  [ lemma : seq ] .",
    "therefore , applying the lemma and the definition of the density of a cover ( equation  [ eq : dendef ] ) , we find : @xmath265 where @xmath266 is the fisher information in the region @xmath233 .",
    "( we have used the fact that the measure of @xmath241 in the coordinates in which the volumes of indistinguishability are spheres is @xmath267 . ) therefore , both the numerator and denominator of the right hand side of equation  [ eq : int2 ] are finite sums of terms that approach finite limits as @xmath268 .",
    "so we can evaluate the limits of these terms to write : @xmath269    the right hand side is now independent of @xmath122 and @xmath92 permitting us to freely take the limits @xmath141 and @xmath270 which gives us the definition of a normalized integral over truly indistinguishable distributions which we write as @xmath271 .",
    "we now want to take the @xmath152 completion of the step maps @xmath145 and @xmath87 in order to arrive at the definition of integration of any function that is lebesgue measurable on the manifold .",
    "first we take the @xmath152 completion of the @xmath145 with respect to lebesgue measure . by the standard theory of integration ,",
    "the sums @xmath272 converge to integrals to give the following definition of the integrals of step maps @xmath87.(@xcite ) @xmath273 where the step function @xmath87 is constant on the sets @xmath274 .",
    "we have arrived at a new measure on the manifold @xmath275 where @xmath24 is the original lebesgue measure . since @xmath261 and @xmath24 are absolutely continuous with respect to each other , the @xmath152 completion of step maps with respect to @xmath261 describes the same class of the functions as the completion of step maps with respect to @xmath24 .",
    "we can therefore take the @xmath152 completion of @xmath87 with respect to @xmath261 to arrive at the following definition of integration of any lebesgue measurable function on a parameter manifold : @xmath276 where @xmath24 is the lebesgue measure induced by the parametrization .",
    "as discussed above , this construction accounts for indistinguishability and translation invariance in the space of probability distributions .",
    "@xmath211    in sum , the normalized measure on the manifold that accounts for the indistinguishability of neighbouring distributions is given by : @xmath277 where @xmath278 is the lebesgue measure on the manifold induced by its atlas which in simple cases is simply the product measure @xmath279 .",
    "in this expression we have taken the limits @xmath141 and @xmath116 .",
    "the meaning of this is that we are dividing out the volume of the parameter space which contains models that will be perfectly indistinguishable ( in a one - sided error ) given an arbitrary amount of data .",
    "as discussed earlier , equation  [ eq : appmeas ] is equivalent to a choice of jeffreys prior in the bayesian formulation of model inference .",
    "we stated earlier that jeffreys prior has the desirable property of being reparametrization invariant on account of the transformation properties of the fisher information that enters its definition , but that one could define many such quantities .",
    "norms.(@xcite ) ] it appears that the derivation in this paper may provide the first rigorous justification for a choice of jeffreys prior for bayesian inference that does not involve assumption of a minimum description length principle or a statement concerning compact coding of data .",
    "the derivation suggests that the fisher information is the reparametrization invariant prior on the parameter manifold that is induced by a uniform prior in the space of distributions .",
    "as such it would seem to be the natural prior for density estimation in a bayesian context .",
    "it is worthwhile to point out that the work of wallace and freeman and barron and cover ( among others ) has demonstrated that the optimal code derived from a parametric model should pick parameters from a grid distributed with a density inversely proportional to the determinant of the fisher information matrix .",
    "( @xcite,@xcite ) the continuum limit of these grids can be obtained in the fashion demonstrated here and would yield a jeffreys prior on the parameter manifold .",
    "the reader may worry that the asymmetric errors @xmath280 and @xmath281 are a little peculiar since they imply that @xmath59 can be distinguished from @xmath60 , but @xmath60 can not be distinguished from @xmath59 .",
    "a more symmetric analysis can be carried out in terms of the chernoff bound at the expense of a convexity assumption on the parameter manifold . since the derivation of the measure using the chernoff bound exactly parallels the derivation using stein s lemma and yields the same result , we will not present it here .",
    "although the derivation in this section has focussed on deriving the measure on a parameter manifold , future sections will take the metric on the manifold to be the fisher information .",
    "i do not have enough space in this paper to recapitulate the theory of riemannian geometry in the setting of the space of probability distributions .",
    "i will therefore assume that the reader has a rudimentary understanding of the notions of vectors , connection coefficients and covariant derivatives on manifolds .",
    "the necessary background can be gleaned from the early pages of any differential geometry or general relativity textbook .",
    "a discussion of geometry in a specifically statistical setting can be found in the works of amari , rao and others.(@xcite , @xcite ) in the next section i will assume a rudimentary knowledge of geometry , but since we do not need any sophisticated results , the reader who is unfamiliar with covariant derivatives should still be able to understand most of the results .",
    "table  [ table1 ] provides a few formulae that will be useful to such readers , but contains no explanations .",
    "in the previous sections we have constructed the razor from bayes rule and discussed measures and metrics on parameter manifolds .",
    "we are left with a candidate for a coordinate invariant index of simplicity and accuracy of a parametric family as a model of a true distribution  @xmath2 t : @xmath282 where the fisher information @xmath145 is the metric on the manifold . in this section",
    "i will demonstrate that the razor has the desired properties of measuring simplicity and accuracy . in order to make progress various technical assumptions are necessary .",
    "let  ^*@xmath283^ *   be the value of @xmath33 that globally minimizes @xmath284 .",
    "i will assume that  ^*@xmath283^ *  is a unique global minimum and that it lies in the interior of the compact parameter manifold .",
    "i will also assume that that @xmath284 and @xmath285 are smooth functions of @xmath33 in order that taylor expansions of these quantities are possible .",
    "( actually the degree of continuity required here depends on the accuracy of the approximation we seek and since we will only evaluate terms to @xmath286 we will only require the existence of derivatives up to the fourth order for our computations . ) finally , let the values of the local minima be bounded away from the global minimum by some @xmath127 .",
    "for any given @xmath127 , for sufficiently large @xmath0 , the value of the razor will be dominated by the neighbourhood of  ^*@xmath283^*. our strategy for evaluating the razor will be to taylor expand the exponent in the integrand around  ^*@xmath283^ *  and to develop a perturbation expansion in powers of @xmath54",
    ". we will omit mention of the @xmath287 terms arising from the local minima . in their analysis of the asymptotics of the bayesian marginal density clarke and barron introduce a notion of `` soundness of parametrization''.(@xcite )",
    "this condition is intended to guarantee that there is a one - to - one map between parameters and distributions and that distant parameters index distant distributions . in geometric terms",
    "this simply means that the parameter manifold is embedded in the space of distributions in such a way that no two separable points on the manifold are embedded inseparably in the space of distributions - i.e. , the manifold does not fold back on itself or intersect itself .",
    "the conditions stated for the following analysis are much weaker because we only need `` soundness '' at the point on the manifold that is closest to the true distribution in relative entropy .",
    "even this is merely a technical condition for ease of analysis - multiple global maxima of the integrand of the razor would simply contribute separately to the analysis and thereby increase the value of the razor .",
    "the most important conditions required in this paper are that taylor expansions of the relevant quantities should exist at @xmath288 .",
    "we can begin the evaluation of the razor by rewriting it as : @xmath289 } }                                                 { \\int d\\mu(\\theta ) \\sqrt{\\det j_{ij}}}\\ ] ] where @xmath290 denotes trace .",
    "define @xmath291 .",
    "let @xmath292 be the nth covariant derivative of the relative entropy with respect to @xmath293 evaluated at  ^*@xmath283^*. define the nth covariant derivatives of @xmath21 similarly .",
    "we can taylor expand the exponent in equation  [ eq : step1 ] in terms of these quantities .",
    "letting @xmath294 be the exponent , we find that : @xmath295   +                                         \\frac{1}{2 } f({\\ifmmode\\theta^*\\else \\ifhmode $ \\theta^*$\\else \\theta^*\\fi\\fi } ) +   \\sum_{i=1}^{\\infty } \\frac{1}{2 i ! }                                     f_{\\mu_1\\cdots\\mu_i } \\delta\\theta^{\\mu_1}\\cdots\\delta\\theta^{\\mu_i}\\ ] ] to proceed further shift the integration variable @xmath33 to @xmath296 and rescale to integrate with respect to @xmath297 . with this change of variables",
    "the razor becomes : @xmath298 where @xmath299 collects the terms in the exponent that are suppressed by powers of @xmath0 : @xmath300 \\nonumber \\\\                                   = & \\frac{1}{\\sqrt{n } } \\left [ \\frac{1}{3 ! } \\tilde{j}_{\\mu_1\\mu_2\\mu_3 }                                   \\phi^{\\mu_1 } \\phi^{\\mu_2 } \\phi^{\\mu_3 } - \\frac{1}{2 } f_{\\mu_1 }                                   \\phi^{\\mu_1 } \\right ] + \\nonumber \\\\                                       &         \\frac{1}{n}\\left [ \\frac{1}{4 ! } \\tilde{j}_{\\mu_1\\cdots\\mu_4 }                                   \\phi^{\\mu_1}\\cdots\\phi^{\\mu_4 } - \\frac{1}{2\\,2 ! } f_{\\mu_1\\mu_2 } \\phi^{\\mu_1 }                                   \\phi^{\\mu_2 } \\right ] +                                                 o(\\frac{1}{n^{3/2 } } )                                  \\end{aligned}\\ ] ] note that the leading term in @xmath299 is @xmath301 .",
    "the razor may now be evaluated in a series expansion using a standard trick from statistical mechanics .",
    "define a `` source '' @xmath302 as an auxiliary variable .",
    "then it is easy to verify that the razor can be written as : @xmath303 where the derivatives have been assumed to commute with the integral .",
    "the function @xmath299 has been removed from the integral and its argument ( @xmath304 ) has been replaced by @xmath305 .",
    "evaluating the derivatives and setting @xmath306 reproduces the original expression for the razor .",
    "but now the integral is a simple gaussian .",
    "the only further obstruction to doing the integral is that the parameter space is compact and consequently the integral is a complicated multi - dimensional error function . as our final simplifying assumption",
    "we will analyze a situation where  ^*@xmath283^ *  is sufficiently in the interior , or @xmath0 is sufficiently large as to give negligible error when the integration bounds are extended to infinity .",
    "the integral can now be done instantly .",
    "we find that : @xmath307_{h=0 } }                                                 { n^{d/2}\\ ; \\int d\\mu(\\theta ) \\sqrt{\\det{j_{ij}}}}\\ ] ] expanding @xmath308 and collecting terms gives : @xmath309\\ ] ] where we have defined @xmath310 to be the volume of the parameter manifold measured in the fisher information metric .",
    "integrate to zero because they are odd in @xmath47 while the gaussian integrand and the integration domain in our approximation are even in @xmath47 . ]",
    "the terms of order @xmath54 arise from the action of @xmath221 in equation  [ eq : temp2 ] .",
    "it turns out to be most useful to examine @xmath311 . in that case , we can write , to order @xmath54 : @xmath312 } + \\nonumber \\\\                                    & \\frac{1}{n }   \\left\\ { \\frac{{\\ifmmode\\tilde{j}\\else \\ifhmode $ \\tilde{j}$\\else \\tilde{j}\\fi\\fi}_{\\mu_1\\mu_2\\mu_3\\mu_4}}{4 ! } \\left [                                   ( { \\ifmmode\\tilde{j}\\else \\ifhmode $ \\tilde{j}$\\else \\tilde{j}\\fi\\fi}^{-1})^{\\mu_1\\mu_2 } ( { \\ifmmode\\tilde{j}\\else \\ifhmode $ \\tilde{j}$\\else \\tilde{j}\\fi\\fi}^{-1})^{\\mu_3\\mu_4 } + \\ldots \\right ] -                                        \\frac{f_{\\mu_1\\mu_2}}{2\\,2 ! }",
    "\\left [ ( { \\ifmmode\\tilde{j}\\else \\ifhmode $ \\tilde{j}$\\else \\tilde{j}\\fi\\fi}^{-1})^{\\mu_1\\mu_2 } +                                   ( { \\ifmmode\\tilde{j}\\else",
    "\\ifhmode $ \\tilde{j}$\\else \\tilde{j}\\fi\\fi}^{-1})^{\\mu_2\\mu_1 }                                   \\right ]                                    \\right .",
    "- \\nonumber \\\\                                    &     \\frac{\\tilde{j}_{\\mu_1\\mu_2\\mu_3 } { \\ifmmode\\tilde{j}\\else \\ifhmode $ \\tilde{j}$\\else \\tilde{j}\\fi\\fi}_{\\nu_1\\nu_2\\nu_3}}{2!\\,3!\\,3 ! }                                    \\left [                                   ( { \\ifmmode\\tilde{j}\\else \\ifhmode $ \\tilde{j}$\\else \\tilde{j}\\fi\\fi}^{-1})^{\\mu_1\\mu_2}({\\ifmmode\\tilde{j}\\else \\ifhmode $ \\tilde{j}$\\else \\tilde{j}\\fi\\fi}^{-1})^{\\mu_3\\nu_1}({\\ifmmode\\tilde{j}\\else \\ifhmode $ \\tilde{j}$\\else \\tilde{j}\\fi\\fi}^{-1})^{\\nu_2\\nu_3 }                                   + \\ldots \\right ] - \\nonumber \\\\                                    &   \\left . \\frac{f_{\\mu_1 } f_{\\mu_2}}{2!\\,4\\,2!\\,2 ! } \\left [                                   ( { \\ifmmode\\tilde{j}\\else \\ifhmode $ \\tilde{j}$\\else \\tilde{j}\\fi\\fi}^{-1})^{\\mu_1\\mu_2 } + \\ldots                                    \\right ]   + \\frac{f_{\\mu_1 } { \\ifmmode\\tilde{j}\\else \\ifhmode $ \\tilde{j}$\\else \\tilde{j}\\fi\\fi}_{\\mu_2\\mu_3\\mu_4}}{2!\\,2\\,2!\\,3 ! }                                   \\left[({\\ifmmode\\tilde{j}\\else \\ifhmode $ \\tilde{j}$\\else \\tilde{j}\\fi\\fi}^{-1})^{\\mu_1\\mu_2 } ( { \\ifmmode\\tilde{j}\\else \\ifhmode $ \\tilde{j}$\\else \\tilde{j}\\fi\\fi}^{-1})^{\\mu_3\\mu_4 } + \\ldots                                   \\right ]                                    \\right\\ }                                \\end{aligned}\\ ] ] the ellipses within the parentheses indicate further terms involving all permutations of the indices on the single term that has been indicated and we have omitted terms of @xmath313 and smaller . it is worthwhile to point out that the systematic series expansion above allows us to evaluate the razor to arbitrary accuracy for any relative entropy functions whose taylor expansion exists and whose derivatives grow sufficiently slowly with order . therefore , this method of analyzing the asymptotics circumvents the need to place bounds on the higher terms since they can be explicitly evaluated .",
    "the statistical mechanical idea of using such expansions around a saddlepoint of an integral could also find applications in other asymptotic analyses in information theory in which integrals are dominated by narrow maxima .",
    "in the next section we will discuss why this large @xmath0 analysis shows that the razor meaures simplicity and accuracy and we will analyze the geometric meaning of the terms in the above expansion .",
    "we will then discuss the connections between equation  [ eq : lograzor ] and the minimum description length principle .",
    "the various terms of equation  [ eq : lograzor ] tell us why the razor is a measure of the simplicity and accuracy of a parametric distribution .",
    "models with higher values of @xmath314 and therefore lower values of @xmath315 are considered to be better .",
    "the @xmath316 term , @xmath317 measures the relative entropy between the true distribution and the best model distribution on the manifold .",
    "this is a measure of the accuracy with which the model family @xmath318 will be able to describe @xmath2 .",
    "the geometric interpretation of this term is that it arises from the distance between the true distribution and the closest point on the model manifold in relative entropy sense .",
    "the @xmath319 term , @xmath320 , tells us that the value of the log razor increases linearly in the dimension of the parameter space .",
    "this penalizes models with many degrees of freedom .",
    "the geometric reason for the existence of this term is that the volume of a peak in the integrand of the razor measured relative to the volume of the manifold shrinks more rapidly as a function of @xmath0 in higher dimensions .",
    "the @xmath321 term , is even more interesting .",
    "the determinant of @xmath322 is proportional to the volume of the ellipsoid in parameter space around @xmath283 where the value of the integrand of the razor is significant . where @xmath87 is close to 1",
    ", the integrand of the razor will be greater that @xmath87 times the peak value in an elliptical region around the maximum . ]",
    "the scale for determining whether @xmath323 is large or small is set by the fisher information on the surface whose determinant defines the volume element .",
    "consequently the term @xmath324 can be understood as measuring the naturalness of the model in the sense discussed in section  [ sec : qual ] since it involves a preference for model families with distributions concentrated around the true .",
    "another way of understanding this point is to observe from the derivation of the integration measure in the razor that given a fixed number of data points @xmath0 , the volume of indistinguishability around @xmath283 is proportional to @xmath325 .",
    "so the factor @xmath326 is essentially proportional to the ratio @xmath327 , the ratio of the volume where the integrand of the razor is large to the volume of indistinguishability introduced earlier .",
    "essentially , a model is better ( more natural ) if there are many distinguishable models that are close to the true .",
    "the term @xmath328 can be understood as a preference for models that have a smaller invariant volume in the space of distributions and hence are more constrained .",
    "the terms proportional to @xmath54 are less easy to interpret .",
    "they involve higher derivatives of the metric on the parameter manifold and of the relative entropy distances between points on the manifold and the true distribution .",
    "this suggests that these terms essentially penalize high curvatures of the model manifold , but it is hard to extract such an interpretation in terms of components of the curvature tensor on the manifold .",
    "a consistent estimator of the razor can be used to implement parsimonious and consistent inference .",
    "suppose we are comparing two model families a and b. we evaluate the razor of each family and pick the one with the larger razor . to evaluate the behaviour of the razor we have consider several different cases .",
    "first suppose that a is d - dimensional and b is a more accurate k - dimensional model with @xmath329 . by saying that b is more accurate we mean that @xmath330 .",
    "we expect that for small @xmath0 the terms proportional to @xmath331 will dominate and that for large @xmath0 the terms proportional to @xmath0 will dominate .",
    "we can compute the crossover number of events beyond which accuracy is favoured over simplicity . ignoring the terms of @xmath321",
    "let us ask how large @xmath0 must be so that @xmath332 .",
    "the answer is easily seen to be the solution to the equation : @xmath333 up to terms of @xmath286 , this is the expected crossover point between a and b if the inference used the minimum description length principle based on stochastic complexity as introduced by rissanen.(@xcite,@xcite ) the @xmath321 terms in the razor are important for small @xmath0 and for cases where the models in question have parameter spaces of equal dimension . in that case , ignoring terms of @xmath286 in the razor , the crossover point where @xmath334 is given by : @xmath335\\ ] ] the terms within the parentheses have been interpreted above in terms of the relative volume of the parameter space that is close to the true distribution ( in other words , as a measure of robustness ) .",
    "so we see that if a is a more robust model than b then the crossover number of events is greater .",
    "the crossover point is inversely proportional to the difference in relative entopy distances between the true distribution and the best model and this too makes good intuitive sense .",
    "further examinations of this sort show that the razor has a preference for simple models , but is consistent in that the most accurate model in relative entropy sense will dominate for sufficiently large n. inference can be carried out with a countably large set of candidate families by placing the families in a list and examining the razor of the first @xmath0 families when @xmath0 events are provided .",
    "it is clear that this procedure is then guaranteed to be asymptotically consistent while remaining parsimonious at each stage of the inference .",
    "indeed the model which is closest to the true in relative entropy sense will eventually be chosen while simpler models may be be preferred for finite @xmath0 .",
    "the analysis of the previous section has shown that the razor is parsimonious , yet consistent in its preferences . unfortunately , in order to compute the razor one must already know the true distribution .",
    "it is certainly an index measuring the simplicity and accuracy of a model , but actual inference procedures must devise schemes to _ estimate _ the value of the razor from data .",
    "a good estimator of the razor will be guaranteed to pick accurate , yet simple models .",
    "so how do we estimate the razor of a model ?    given the bayesian derivation of the razor a natural candidate is the bayesian posterior probability of a parametric model given the data . as discussed before , this probability is given by : @xmath336 we have used a jeffreys prior which is the uniform prior on the space of probability distributions as discussed in previous sections .",
    "the relationship between the razor and the estimator in equation  [ eq : razest ] can be analyzed in various ways .",
    "the simplest relationship arises because the exponential is a convex function so that jensen s inequality gives us the following bound on the expectation value of @xmath337 in the true distribution @xmath2 : @xmath338 where @xmath339 is the differential entropy of the true distribution .",
    "so the razor times the exponential of the entropy of the true is a lower bound on the expected value of the bayesian posterior .",
    "a sharper analysis may be carried out to show that under certain regularity assumptions the razor reflects the typical behaviour of @xmath340 .",
    "the first assumption is that @xmath341 is a smooth function of @xmath33 for every set of outcomes @xmath342 in the @xmath0 outcome sample space .",
    "( in fact , this assumption can be weakened to smoothness only in a neighbourhood of @xmath283 . ) using this premise , and the already assumed smoothness of fisher information matrix @xmath285 , we can expand the exponent in equation  [ eq : razest ] around the maximum likelihood parameter @xmath343 to obtain : @xmath344 \\nonumber \\\\                              & & + \\frac{1}{2 } f(\\theta^ * ) +                              \\sum_{i=1}^{\\infty } \\frac{1}{2i ! } \\ ,                              f_{\\mu_1 \\cdots \\mu_i } \\ ,                              \\delta\\theta^{\\mu_1}\\cdots\\delta\\theta^{\\mu_i}\\end{aligned}\\ ] ] in this expression @xmath345 and @xmath346 are the same as in equation  [ eq : expon ] and we have defined @xmath347 .",
    "we will only consider models in which @xmath348 , the empirical fisher information related to relative entropy distances between model distributions and the true , is nonsingular everywhere for every set of outcomes @xmath294 .",
    "this is a condition ensuring that nearby parameters index sufficiently different models of the true distribution . by imitating the analysis of the razor ( under the same assumptions as those listed for that analysis )",
    ", we find that : @xmath349 } { n^{d/2 } \\ ; v } \\ ] ] where @xmath350 is the same as in equations  [ eq : gdef ] and  [ eq : gdef2 ] with the substitution of @xmath351 for every @xmath352 .",
    "defining @xmath353 we find that to @xmath286 : @xmath354 + + o(1/n ) \\label{eq : chie}\\end{aligned}\\ ] ] terms proportional to positive powers of @xmath54 may be computed as before , but we will not evaluate them explicitly here .",
    "it suffices to note that all terms of order @xmath355 in equation  [ eq : chie ] are identical to the corresponding terms in equation  [ eq : lograzor ] with @xmath351 substituted for @xmath352 .",
    "we will now prove a theorem showing that any finite collection of terms in @xmath356 converges with high probability to @xmath357 in the limit of a large number of samples . throughout the discussion we will assume _ consistency _ of the maximum likelihood estimator in the following sense .",
    "let @xmath358 be any neighbourhood of @xmath359 on the parameter manifold and let @xmath360 be any set of @xmath0 outcomes drawn independently from @xmath2 .",
    "then , for any @xmath361 , we shall assume that the maximum likelihood estimator @xmath362 falls inside @xmath358 with probability greater than @xmath363 for sufficiently large n. we also require that the log likelihood of a single outcome @xmath364 , @xmath365 , considered as a family of functions on @xmath33 indexed by the outcomes @xmath364 , is an _",
    "equicontinuous _ family at @xmath283.(@xcite ) in other words , given any @xmath182 , there is a neighbourhood @xmath5 of @xmath283 , such that for every @xmath364 and @xmath366 , @xmath367 .",
    "finally , we will require that all derivatives with respect to @xmath33 of the log likelihood of a single outcome should be equicontinuous at @xmath283 in the same sense .",
    "[ lemma : conv1 ] let @xmath0 be the number of iid outcomes @xmath368 arising from a distribution @xmath2 and take @xmath369 and @xmath370 . if the maximum likelihood estimator is consistent , @xmath371 for sufficiently large @xmath0 .",
    "( see above for definitions of @xmath283 and @xmath372 . )",
    "furthermore , if the log likehood of a single outcome is equicontinuous at @xmath283 ( see definition above ) then @xmath373 for sufficiently large @xmath0 . finally , if the derivatives with respect to @xmath33 of the log likelihood of a single outcome are equicontinuous at @xmath283 , then @xmath374 for sufficiently large @xmath0 .",
    "* proof : * we have assumed that the fisher information matrix @xmath285 is a smooth matrix valued function on the parameter manifold . by consistency of the maximum likelihood estimator @xmath375 in probability . since the entries of the matrix @xmath145 are continuous functions of @xmath33 we can conclude that @xmath376 in probability also .",
    "this proves the first claim . to prove the second and third claims consider any function of the form @xmath377 where @xmath378 is an equicontinuous family of functions of @xmath33 at @xmath283 .",
    "we want to show that @xmath379|$ ] approaches zero in probability where the expectation is taken in @xmath2 , the true distribution .",
    "to this end we write : latexmath:[\\[\\label{eq : bounder }                \\leq                 hand side is the absolute value of the difference between the sample average of an iid random variable and its mean value .",
    "this approaches zero almost surely by the strong law of large numbers and so for sufficiently large @xmath0 the first term is less than @xmath381 with probability greater than @xmath382 for any @xmath383 and @xmath370 . in order to show that the second term on the right hand side converges to zero in probability , note that since @xmath378 is equicontinuous at @xmath283 , given any @xmath182 there is a neighbourhood @xmath358 of @xmath283 within which @xmath384 for any @xmath364 and @xmath385 .",
    "therefore , for any set of outcomes @xmath294 and @xmath385 , @xmath386 . by consistency of the maximum likelihood estimator ,",
    "@xmath387 with probability greater than @xmath382 for sufficiently large @xmath0 . consequently , @xmath388 for sufficiently large @xmath0 . putting the bounds on the two terms on the right hand side of equation  [ eq : bounder ] together , and using the union of events bound we see that for sufficiently large @xmath0 : @xmath389 | >",
    "\\epsilon ) < \\delta\\ ] ] to complete the proof we can observe that by assumption @xmath365 and its derivatives with respect to @xmath33 are equicontinuous at @xmath283 and that @xmath390 and the various @xmath391 are therefore examples of the functions of @xmath22 . furthermore , @xmath392 = d(t\\|\\theta^ * ) + h(t)$ ] and @xmath393 = \\tilde{j}_{\\mu_1\\cdots\\mu_i}$ ] under the assumption that derivatives with respect to @xmath33 commute with expectations with respect to @xmath2 . on applying equation  [ eq : result ] to these observations ,",
    "the theorem is proved .",
    "note that lemma  [ lemma : conv1 ] shows that the two leading terms in the asymptotic expansions of @xmath394 and @xmath315 approach each other with high probability .",
    "we will now obtain control over the subleading terms in these expansions .",
    "define @xmath395 to be the coefficient of @xmath355 in the asymptotic expansion of @xmath356 so that we can write @xmath396 .",
    "let @xmath397 be the corresponding coefficients of @xmath355 in the expansion of @xmath315 .",
    "the @xmath397 are identical to the @xmath395 with each @xmath351 replaced by @xmath352",
    ". we can show that the @xmath395 approach the @xmath397 with high probability .",
    "[ lemma : conv2 ] let the assumptions made in lemma  [ lemma : conv1 ] hold and let @xmath369 and @xmath370 .",
    "then for every intger @xmath398 , there is an @xmath399 such that @xmath400 .    *",
    "proof : * the coefficient @xmath401 has been shown to approach @xmath402 in probability as an immediate consequence of lemma  [ lemma : conv1 ] . next we consider @xmath395 for @xmath403 .",
    "every term in every such @xmath395 can be shown to be a finite sum over finite products of constants and random variables of the form @xmath391 and @xmath404 .",
    "we have already seen that @xmath405 in probability .",
    "the @xmath406 are the entries of the inverse of the empirical fisher information @xmath407 . since the inverse is a continuous function , and since @xmath408 in probability , @xmath409 in probability also .",
    "as noted before , @xmath397 is identical to @xmath395 with each @xmath351 replaced by @xmath352 . since @xmath395 is finite sum of finite products of random variables @xmath351 that converge individually in probability to the @xmath352",
    ", we can conclude that @xmath410 in probability .",
    "finally , we consider @xmath411 .",
    "we have shown that @xmath376 and @xmath412 in probability .",
    "since the determinant and the logarithm are continuous functions we conclude that @xmath413 in probability .",
    "we have just shown that each term in the asymptotic expansion of @xmath414 approaches the corresponding term in @xmath315 with high probability for sufficiently large @xmath0 . as an easy corollary of this lemma",
    "we obtain the following theorem :    [ theorem : conv ] let the conditions necessary for lemmas  [ lemma : conv1 ] and  [ lemma : conv2 ] hold and take @xmath415 to be integers .",
    "then let @xmath416 consist of the terms in the asymptotic expansion of @xmath394 that are of orders @xmath355 to @xmath417 .",
    "for example , @xmath418 , using the coefficients @xmath395 defined above .",
    "let @xmath419 be the corresponding terms in the asymptotic expansion of @xmath315 .",
    "then for any @xmath246 and @xmath420 , and for any @xmath383 and @xmath370 , @xmath421 for sufficiently large @xmath0 .",
    "* proof : * by definition of @xmath422 and @xmath423 , @xmath424 . by lemma  [ lemma : conv2 ]",
    "@xmath425 in probability .",
    "therefore , @xmath426 is a postive number that is upper bounded by a finite sum of random variables that individually converge to zero in probability .",
    "since the sum is finite we can conclude that @xmath426 also converges to zero in probability thereby proving the theorem @xmath211    note that the multiplication by @xmath427 ensures that the convergence is not simply due to the fact that every partial sum @xmath416 is individually decreasing to zero as the number of outcomes increases .",
    "any finite series of terms in the asymptotic expansion of the logarithm of the bayesian posterior probability converges in probability to the corresponding series of terms in the expansion of the razor .",
    "theorem  [ theorem : conv ] precisely characterizes the sense in which the razor of a model reflects the typical asymptotic behaviour of the bayesian posterior probability of a model given the sample outcomes .",
    "we can also compare the razor to the expected behaviour of @xmath337 in the true distribution @xmath2 .",
    "clarke and barron have analyzed the expected asymptotics of the logarithm of @xmath428 where @xmath2 is the true distribution , under the assumption that @xmath2 belongs to the parametric family @xmath318.(@xcite ) with certain small modifications of their hypotheses their results can be extended to the situation studied in this paper where the true density need not be a member of the family under consideration .",
    "the first modification is that the expectation values evaluated in condition 1 of  @xcite should be taken in the true distribution @xmath2 which need not be a member of the parametric family .",
    "secondly the differentiability requirements in conditions 1 and 2 should be applied at @xmath283 which minimizes @xmath284 .",
    "( clarke and barron apply these requirements at the true parameter value since they assume that @xmath2 is in the family . ) finally , condition 3 is changed to require that the posterior distribution of @xmath33 given @xmath429 concentrates on a neighbourhood of @xmath283 except for @xmath429 in a set of probability @xmath430 . under these slightly modified hypotheses",
    "it is easy to rework the analysis of  @xcite to demonstrate the following asymptotics for the expected value of @xmath337 : @xmath431 we see that as @xmath116 , @xmath432 is equal to the razor up to a constant term @xmath433 .",
    "more careful analysis shows that this term arises from the statistical fluctuations of the maximum likelihood estimator of @xmath283 around @xmath283 .",
    "it is worth noting that while terms of @xmath321 and larger in @xmath434 depend depend at most on the measure ( prior distribution ) assigned to the parameter manifold , the terms of @xmath286 depend on the geometry via the connection coefficients in the covariant derivatives .",
    "for that reason , the @xmath286 terms are the leading probes of the effects that the geometry of the space of distributions has on statistical inference in a baysian setting and so it would be very interesting to analyze them",
    ". normally we do not include these terms because we are interested in asymptotics , but when the amount of data is small , these correction terms are potentially important in implementing parsimonious density estimation .",
    "unfortunately it turns out to be difficult to obtain sufficiently fine control over the probabilities of events to extend the expected asymptotics beyond the @xmath321 terms and so further analysis will be left to future publications .",
    "in the previous section we have seen that the bayesian conditional probability of a model given the data is an estimator of the razor . in this section",
    "we will consider the relationship of the razor to the minimum description length principle and the stochastic complexity inference criterion advocated by rissanen .",
    "the mdl approach to parameteric inference was pioneered by akaike who suggested choosing the model maximizing @xmath435 with @xmath15 the dimension of the model and @xmath372 the maximum likelihood estimator.(@xcite ) subsequently , schwarz studied the maximization of the bayesian posterior likelihood for densities in the koopman - darmois family and found that the bayesian decision procedure amounted to choosing the density that maximized @xmath436.(@xcite ) rissanen placed this criterion on a solid footing by showing that the model attaining @xmath437 gives the most efficient coding rate possible of the observed sequence amongst all universal codes.(@xcite,@xcite ) . in this paper we have shown that the razor of a model , which reflects the typical asymptotics of the logarithm of the bayesian posterior , has a geometric interpretation as an index of the simplicity and accuracy of a given model as a description of some true distribution . in the previous section we have shown that the logarithm of the bayesian posterior can be expanded as : @xmath438 + o(1/n ) \\label{eq : stoch}\\end{aligned}\\ ] ] with @xmath372 the maximum likelihood parameter and @xmath439 . the term of @xmath286 that we have not explicitly written is the same as the the corresponding term of the logarithm of the razor ( equation  [ eq : lograzor ] ) with every @xmath352 replaced by @xmath351 .",
    "we recognize the first two terms in this expansion to be exactly the stochastic complexity advocated by rissanen as a measure of the complexity of a string relative to a particular model family .",
    "we have given a geometric meaning to the term @xmath440 in terms of a measurement of the rate of shrinkage of the volume in parameter space in which the likelihood of the data is significant .",
    "given our results concerning the razor and the typical asymptotics of @xmath356 , this strongly suggests that the definition of stochastic complexity should be extended to include the subleading terms in equation  [ eq : stoch ] .",
    "indeed , rissanen has considered such an extension based on the work of clarke and barron and finds that the terms of @xmath321 in the expected value of equation  [ eq : stoch ] remove the redundancy in the class of codes that meet the bound on the expected coding rate represented by the earlier definition of stochastic complexity.(@xcite ) essentially , in coding short sequences we are less interested in the coding rate and more interested in the actual code _",
    "length_. this suggests that for small @xmath0 the @xmath286 terms can be important in determining the ideal expected codelength but it remains difficult to obtain sufficient control over the probabilities of rare events to extend the rissanen s result to this order . as mentioned earlier",
    ", the metric on the parameter manifold affects the terms of @xmath286 and therefore these corrections would be geometric in nature .",
    "another approach to stochastic complexity and learning that is related to the razor and its estimators has been taken recently by yamanishi.(@xcite ) let @xmath441 be a hypothesis class indexed by d - dimensional real vectors @xmath33 .",
    "then , in a general decision theoretic setting yamanishi defines the extended stochastic complexity of a model @xmath318 relative to the data @xmath294 , the class @xmath442 , and a loss function @xmath169 to be : @xmath443 where @xmath444 and @xmath445 is a prior . following the work described in this paper he defines the razor index of @xmath318 relative to @xmath169 , @xmath442 and a given true distribution @xmath59 to be : @xmath446}\\ ] ] for",
    "the case of a loss function @xmath447 , equations  [ eq : escrazor ] and  [ eq : esc ] reduce to the quantities @xmath315 and @xmath356 which are the logarithm of the razor and its estimator .",
    "yamanishi shows that if the class of functions @xmath448 has finite vapnik - chervonenkis dimension , then @xmath449 with high probability for sufficiently large @xmath0 .",
    "for the case of a logarithmic loss function this result applies to the razor and its estimator as defined in this paper .",
    "there is an interesting `` physical '' interpretation of the results regarding the razor and the asymptotics of bayes rule which identifies the terms in the razor with energies , temperatures and entropies in the physical sense .",
    "many techniques for model estimation involve picking a model that minimizes a loss function @xmath450 where @xmath294 is the data , @xmath33 are the parameters and @xmath169 is some empirical loss calculated from it .",
    "the typical behaviour of the loss function is that it grows as the amount of data grows . in the case of maximum likelihood model estimation",
    "we take @xmath451 where we expect @xmath452 to attain a finite positive limit as @xmath116 under suitable conditions on the process generating the data . in this case",
    "we can make an analogy with physical systems : @xmath0 is like the inverse temperature and the limit of @xmath452 is like the energy of the system .",
    "maximum likelihood estimation corresponds to minimization of the energy and in physical terms will be adequate to find the equilibrium of the system at zero temperature ( infinite @xmath0 ) .",
    "on the other hand we know that at finite temperature ( finite @xmath0 ) the physical state of the system is determined by minimizing the free energy @xmath453 where @xmath454 is the temperature and @xmath184 is the entropy .",
    "the entropy counts the volume of configurations that have energy @xmath294 and accounts for the fluctuations inherent in a finite temperature system .",
    "we have seen in the earlier sections that terms in the razor and in the asymptotics of bayes rule that account for the simplicity of a model arise exactly from such factors of volume .",
    "indeed , the subleading terms in the extended stochastic complexity advocated above can be identified with a  physical \" entropy associated with the statistical fluctuations that prevent us from knowing the  true \" parameters in estimation problems .",
    "the evaluation of the razor and the relationship to the asymptotics of bayes rule suggest how to pick the  natural \" parametrization of a model . in geometric terms , the  natural \" coordinates describing a surface in the neighbourhood of a given point make the metric locally flat .",
    "the corresponding statement for the manifolds in question here is that the natural parametrization of a model in the vicinity of @xmath455 reduces the fisher information @xmath145 at @xmath455 to the identity matrix .",
    "this choice can also be justified from the point of view of statistics by noting that for a wide class of parametric families the maximum likelihood estimator of @xmath455 is asymptotically distributed as a normal density with covariance matrix @xmath145 . if @xmath145 is the identity in some parametrization , then the various components of the maximum likelihood estimator are independent , identically distributed random variables .",
    "therefore , the geometric intuitions for `` naturalness '' are in accord with the statistical intuitions . in our context where the true density need not be a member of the family in question , there is another natural choice in the vicinity of @xmath283 that minimizes @xmath284 .",
    "we could also pick coordinates in which @xmath456 is reduced to the identity matrix .",
    "we have carried out an expansion of the bayesian posterior probability in terms of @xmath372 which maximizes @xmath457 .",
    "we expect that @xmath372 is asymptotically distributed as a normal density with covariance @xmath352 .",
    "the second choice of coordinates will therefore make the components of @xmath372 independent and identically distributed .",
    "there are numerous close relationships between the work described in this paper and previous results on minimum complexity density estimation .",
    "the seminal work of barron and cover introduced the notion of an `` index of resolvability '' which was shown to bound covergence rates of a very general class of minimum complexity density estimators .",
    "this class of estimators was constructed by considering densities which achieve the following minimization : @xmath458\\ ] ] where the @xmath459 are drawn iid from some distribution , @xmath60 belongs to some countable list of densities , and the set of @xmath460 satisfy kraft s inequality.(@xcite ) equation  [ eq : mincomp ] can be interpreted as minimizing a two stage code for the density @xmath60 and the data .",
    "the `` index of resolvability '' @xmath461 of @xmath59 is constructed from expectation value in @xmath59 of equation  [ eq : mincomp ] divided by @xmath0 , the number of samples : @xmath462\\ ] ] where the @xmath463 are description lengths of the densities and @xmath56 is the relative entropy .",
    "this quantity was shown to bound the rates of convergence of the minimum complexity estimators . in a sense",
    "the density achieving the minimization in equation  [ eq : index ] is a theoretical analog of the sample - based minimum complexity estimator arising from equation  [ eq : mincomp ] .",
    "the work of barron and cover starts from the assumption that description length is the correct measure of complexity in the context of density estimation and that minimizing this complexity is a good idea .",
    "they have demonstrated several very general and beautiful results concerning the consistency of the minimum description length principle in the context of density estimation .",
    "we also know that minimum description length principles lead to asymptotically optimal data compression schemes .",
    "the goal of this paper has been to develop some alternative intuitions for the practical meaning of simplicity and complexity in terms of geometry in the space of distributions .",
    "the _ razor _ defined in this paper , like the index of resolvability , is an idealized theoretical quantity which sample - based inference schemes will try to approximate .",
    "the razor reflects the typical order - by - order behaviour of bayes rule just as the index of resolvability reflects the _ expected _ behaviour of the minimum complexity criterion of barron and cover .    in order to compare the two quantities and their consequences we have to note that barron and cover do not work with families of distributions , but rather with a collection of densities .",
    "consequently , in order to carry out inference with a parametric family they must begin by discretizing the parameter manifold .",
    "the goal of this paper has been to develop a measure of the simplicity of a family as a whole and hence we do not carry out such a truncation of a parameter manifold . under the assumption that the true density is approximated by the parametric family barron and cover",
    "find that an optimal discretization of the parameter manifold ( see  @xcite and  @xcite ) yields a bound on the resolvability of a parametric model of : @xmath464 where @xmath465 is the true parameter value , @xmath466 is the fisher information at @xmath465 , @xmath467 is a prior density on the parameter manifold and @xmath468 arises from sphere - packing problems and is close to @xmath469 for large @xmath15 .",
    "the asymptotic minimax value of the bound is attained by choosing the prior @xmath13 to be jeffreys prior @xmath470 .",
    "we see that aside from the factor of @xmath54 , the leading terms reproduce the logarithm of the razor for the case when the true density is infinitesimally distant from the parameter manifold in relative entropy sense so that @xmath471 .",
    "barron and cover use this bound to evaluate convergence rates of minimum complexity estimators .",
    "in contrast , this paper has presented the leading terms in an asymptotically exact expansion of the razor as an abstract measure of the complexity of a parametric model relative to a true distribution .",
    "i begin by studying what the meaning of `` simplicity '' should be in the context of bayes rule and the geometry of the space of distributions , and arrive at results that are closely related to the minimum complexity scheme .",
    "saying the models with larger razors are preferred is asymptotically equivalent to saying the models with a lower resolvability ( given an optimal discretization ) are preferred .",
    "however , we see from a comparison of the logarithm of the razor and equation  [ eq : resolvbound ] , that the resolvability bound is a truncation of the series expansion of the log razor which therefore gives a finer classification of model families .",
    "the geometric formulation of this paper leads to interpretations of the various terms in the razor that give an alternative understanding of the terms in the index of resolvability that govern the rate of convergence of minimum complexity estimators .",
    "we have also given a systematic scheme for evaluating the razor to all orders in @xmath54 .",
    "this suggests that the results on optimal discretizations of parameter manifolds used in the index of resolvability should be extended to include such sub - leading terms.(@xcite,@xcite )",
    "in this paper we have set out to develop a measure of complexity of a parametric distribution as a description of a particular true distribution .",
    "we avoided appealing to the minimum description length principle or to results in coding theory in order to arrive at a more geometric understanding in terms of the embedding of the parametric model in the space of probability distributions .",
    "we constructed an index of complexity called the razor of a model whose asymptotic expansion was shown to reflect the accuracy and the simplicity of the model as a description of a given true distribution .",
    "the terms in the asymptotic expansion were given geometrical interpretations in terms of distances and volumes in the space of distributions .",
    "these distances and volumes were computed in a metric and measure given by the fisher information on the model manifold and the square root of its determinant .",
    "this metric and measure were justified from a statistical and geometrical point of view by demonstrating that in a certain sense a uniform prior in the space of distributions would induce a fisher information ( or jeffreys ) prior on a parameter manifold .",
    "more exactly , we assumed that indistinguishable distributions should not be counted separately in an integral over the model manifold and that there is a `` translation invariance '' in the space of distributions .",
    "we then showed that a jeffreys prior can be rigorously constructed as the continuum limit of a sequence of discrete priors consistent with these assumptions .",
    "a technique of integration common in statistical physics was introduced to facilitate the asymptotic analysis of the razor and it was also used to analyze the asymptotics of the logarithm of the bayesian posterior .",
    "we have found that the razor defined in this paper reflects the typical order - by - order asymptotics of the bayesian posterior probability just as the index of resolvability of barron and cover reflects the expected asymptotics of the minimum complexity criterion studied by those authors . in particular , any finite series of terms in the asymptotic expansion of the logarithm of the bayesian posterior converges in probability to the corresponding series of terms in the asymptotic expansion of the razor .",
    "examination of the logarithm of the bayesian posterior and its relationship to the razor also suggested certain subleading geometrical corrections to the expected asymptotics of bayes rule and corresponding corrections to stochastic complexity defined by rissanen .",
    "i would like to thank kenji yamanishi for several fruitful conversations .",
    "i have also had useful discussions and correspondence with steve omohundro , erik ordentlich , don kimber , phil chou and erhan cinlar .",
    "finally , i am grateful to curt callan for his support for this investigation and to phil anderson for helping with travel funds to the 1995 workshop on maximum entropy and bayesian methods .",
    "this work was supported in part by doe grant de - fg02 - 91er40671 .",
    "s.i.amari , o.e.barndorff-nielsen , r.e.kass , s.l.lauritzen , and c.r.rao , _ differential geometry in statistical inference _ , institute of mathematical statistics lecture note - monograph series , vol.10 , 1987 ."
  ],
  "abstract_text": [
    "<S> i define a natural measure of the complexity of a parametric distribution relative to a given true distribution called the _ razor _ of a model family . </S>",
    "<S> the minimum description length principle ( mdl ) and bayesian inference are shown to give empirical approximations of the razor via an analysis that significantly extends existing results on the asymptotics of bayesian model selection . </S>",
    "<S> i treat parametric families as manifolds embedded in the space of distributions and derive a canonical metric and a measure on the parameter manifold by appealing to the classical theory of hypothesis testing . </S>",
    "<S> i find that the fisher information is the natural measure of distance , and give a novel justification for a choice of jeffreys prior for bayesian inference . </S>",
    "<S> the results of this paper suggest corrections to mdl that can be important for model selection with a small amount of data . </S>",
    "<S> these corrections are interpreted as natural measures of the simplicity of a model family . </S>",
    "<S> i show that in a certain sense the logarithm of the bayesian posterior converges to the logarithm of the _ razor _ of a model family as defined here . </S>",
    "<S> close connections with known results on density estimation and `` information geometry '' are discussed as they arise .    </S>",
    "<S> [ section ] [ section ] [ section ] [ section ] [ section ] [ section ] [ section ] </S>"
  ]
}