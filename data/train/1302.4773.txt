{
  "article_text": [
    "modulation classification is the process of choosing the most likely scheme from a set of predefined candidate schemes that a received signal could belong to .",
    "various approaches have been proposed to address this problem .",
    "there has recently been growing interest in modulation classification for applications such as software defined radio , cognitive radio and interference identification @xcite .",
    "existing classification methods can generally be categorized into two main groups : feature based classifiers and likelihood based ( ml ) classifiers .",
    "the ml classifiers give the minimum possible classification error of all possible discriminant functions given perfect knowledge of the signal s probability distribution .",
    "however , this approach is very sensitive to modeling errors such as imperfect knowledge of the signal to noise ratio ( snr ) or phase offset .",
    "further , such approaches have very high computational complexity and are thus impractical in actual hardware implementation . to address these issue ,",
    "various feature based techniques such as cumulant - based classifiers @xcite and cylostationary - based classifiers have been proposed @xcite .",
    "recently , goodness - of - fit ( gof ) tests such as the kolmogorov - smirnov ( ks ) @xcite distribution distance have been proposed to identify the constellation used in qam modulation @xcite .",
    "based on the ks classifier , we proposed a new reduced complexity kuiper ( rck ) classifer in @xcite .",
    "the rck classifier only finds the empirical cumulative distribution function ( ecdf ) in a small set of predetermined testpoints that have the highest probability of giving the maximum distribution distance , effectively sampling the distribution function .",
    "the algorithm offered reduced computational complexity by removing the need to estimate the full ecdf while still providing better performance than the ks classifier .",
    "it also increased the robustness of the classifier to imperfect parameter estimates .",
    "the idea of improving the classification accuracy of the rck classifier by using more testpoints was proposed in @xcite .",
    "the method is referred to as variational distance ( vd ) classifier where testpoints are selected to be the pdf - crossings of two classes being recognized .",
    "the sum of the absolute distances is then used as the final discriminating statistic .",
    "we refer to methods such as rck and vd , that utilize the value of the ecdf at a small number of testpoints , as sampled distribution distance classifiers . in this work",
    "we derive the optimal discriminant functions for classification with the sampled distribution distance given a set of testpoint locations .",
    "we also provide a systematic way of finding testpoint locations that provide near optimal performance by maximizing the bhattacharyya distance between classes . finally , we present results that compare the performance of this approach with existing techniques .",
    "following  @xcite , we assume a sequence of @xmath0 discrete , complex , i.i.d . and sampled baseband symbols , @xmath1 $ ] , drawn from a constellation @xmath2 , transmitted over awgn channel . the received signal , under constellation @xmath3 , is given as @xmath4 $ ] , where @xmath5 , @xmath6 .",
    "we further define the snr as @xmath7/\\sigma^2 $ ] .",
    "the task of the modulation classifier is to find @xmath8 , from which @xmath9 is drawn from . without loss of generality",
    ", we consider unit power constellations .",
    "let @xmath10=f(\\mathbf{r})$ ] where @xmath11 is the chosen mapping from received symbols @xmath9 to the extracted feature vector @xmath12 , where @xmath13 is the length of the feature vector .",
    "possible feature maps include @xmath14 ( magnitude , @xmath15 ) , the concatenation of @xmath16 and @xmath17 ( quadrature , @xmath18 ) , the phase information @xmath19 ( angle , @xmath15 ) , among others . the theoretical cdf of @xmath20 given @xmath3 and @xmath21 , denoted as @xmath22 , is assumed to be known _ a priori _ ( methods of obtaining these distributions , both empirically and theoretically , are presented in  ( * ? ? ?",
    "iii - a ) ) .    in this paper",
    "we focus on algorithms that use the ecdf defined as @xmath23 as the discriminating feature for classification . here",
    ", @xmath24 is the indicator function whose value is 1 if the function argument is true , and 0 otherwise . if the complete ecdf resulting from the entire feature vector , @xmath12 , is used for classification , we get the conventional distribution distance measures such as kuiper , kolmogorov - smirnov , anderson - darling and others . details of these measures are discussed in @xcite .",
    "once the ecdf is found and the appropriate distribution distance is calculated , the candidate constellation with minimum distance is chosen . however , prior work in @xcite have shown that improved classification accuracy can be achieved at much lower computational complexity and with increased model robustness by finding the value of the ecdf at a small number of specific testpoints .",
    "we describe these methods formally by defining a set of @xmath25 testpoints : @xmath26 $ ] , with @xmath27 . for notational consistency",
    ", we also define the following virtual test points , @xmath28 and @xmath29 in addition to @xmath30 . evaluating the ecdf from ( [ eq : ecdf ] ) at @xmath30",
    "gives us @xmath31 $ ] , @xmath32 .",
    "we refer to any classifier that utilizes the feature vector @xmath33 as a _ sampled distribution distance - based classifier_. as an example , the variational distance ( vd ) classifier from @xcite proposed forming @xmath30 from ecdf points that give either a local maxima or minima of the difference between two theoretical cdfs of the candidate classes . instead of using the sampled ecdf directly , vd classifier finds the number of samples that fall between two consecutive testpoints , which is equivalent to taking the difference of the ecdf at consecutive testpoints , @xmath34 .    in this paper",
    "our goal is to optimize the classification accuracy of the sampled distribution distance classification approach defined as @xmath35 intuitively , there are two ways to improve @xmath36 . first , since different testpoints have varying distribution distance , it is expected that different weights should be assigned to each testpoint .",
    "second , the choice of the number and location of the points along the ecdf should also be investigated to find the proper balance between complexity and classification accuracy .",
    "both of these improvements are addressed in the following subsection .",
    "we first assume that @xmath30 has been selected _ a priori _ and our goal is to find the optimal classifier for the resulting feature vector @xmath33 .",
    "we want to find a discriminant function @xmath37 for each @xmath38 $ ] , for every candidate constellation @xmath3 .",
    "where we follow the rule : @xmath39 it is well established in decision theory that if the performance metric used is average classification error , the optimal classifier is based on the _ bayes decision procedure _ @xcite .",
    "this procedure can be stated as : @xmath40 using the prior probabilities @xmath41 , the posterior probabilities @xmath42 could be found from @xmath43 using bayes formula .",
    "thus , finding the pdf of the feature vector conditioned on the modulation scheme , @xmath44 , effectively gives us the optimal classifier in the minimum error rate sense .",
    "the testpoints partition @xmath12 into @xmath45 regions .",
    "an individual sample , @xmath46 , can be in region @xmath47 , such that @xmath48 , with a given probability , completely determined by the cdf , @xmath22 .",
    "the number of samples that fall into each of the regions , @xmath49 $ ] , where @xmath50 corresponds to region @xmath51 , @xmath52 , is jointly distributed according to a multinomial probability mass function ( pmf ) given as @xmath53 where @xmath54 $ ] , and @xmath55 is the probability of an individual sample being in region @xmath47 . given that @xmath12 is drawn from @xmath3 , @xmath56 , for @xmath57 .    given a particular @xmath33 ,",
    "the number of samples in each of the @xmath45 regions could be found as @xmath58 where @xmath59 and @xmath60 .",
    "this gives a mapping from any given @xmath33 to @xmath61 and therefore to the pmf @xmath62 as defined in ( [ eq : multinomial ] ) .",
    "therefore we have the complete class - conditional pdf , @xmath63 with @xmath64 in ( [ eq : multinomial ] ) determined by @xmath22 , the cdf of class @xmath3 .",
    "thus we have the optimal classifier",
    ". we will refer to @xmath33 and @xmath61 conditioned on class @xmath3 as @xmath65 and @xmath66 .",
    "although the multinomial pmf in ( [ eq : multinomial ] ) can be used for minimum error rate classification , its calculation is very computationally intensive . to address this issue we note that asymptotically the multinomial pmf , @xmath62 in ( [ eq : multinomial ] ) , approaches a multivariate gaussian distribution , @xmath67 as @xmath68 .",
    "where , @xmath69 since @xmath33 is simply the cumulative sum of @xmath61 ( i.e. @xmath70 ) , which is a linear operation , it follows that @xmath71 where , @xmath72 having shown that the feature vector @xmath33 is asymptotically gaussian distributed , we can proceed to apply the _",
    "bayes decision procedure _ in ( [ eq : bdp ] ) .",
    "however , the full multivariate pdfs are not required to perform classification because the optimal discriminant functions for gaussian feature vectors are known to be quadratic with the following form @xcite : @xmath73 where @xmath74 and @xmath75 in the following sections we will simply refer to this classifier as the bayesian approach .",
    "similar to rck @xcite and vd @xcite the bayesian approach only needs to store the testpoint locations for a fixed set of snrs since the theoretical cdf is dependent on snr . given a @xmath30 of size @xmath25 , vd and rck require both @xmath30 and @xmath76 for each class @xmath3 .",
    "in contrast , the bayesian approach requires the same vector @xmath30 , an @xmath77 matrix @xmath78 , a vector @xmath79 of size @xmath25 , and a scalar @xmath80 for each class @xmath3 . however , there are typically no more than 12 testpoints ( total number of pdf - crossings ) , so this additional storage requirements are negligible .",
    "the bayesian approach also requires the calculation of a quadratic form expression ( [ eq : discriminant ] ) .",
    "again , due to the fact that only a relatively small number of testpoints is used , the additional complexity is minimal .      in this subsection",
    "we present a method for choosing testpoint locations , @xmath30 , that provide good classification performance .",
    "the method of using the pdf - crossings make intuitive sense , since it tries to find the testpoints that provide the maximum difference in the theoretical cdf while providing some heuristic rule that the testpoints will be spaced apart .",
    "tespoints that are too close to each other are not as effective because the ecdf tends to be highly correlated and thus provide minimal additional information .",
    "another issue with using the pdf - crossing is that it does not factor in knowledge of the correlation between testpoints . as we have shown in section  [ subsec : optimal ] , the distribution @xmath33 follows an approximate multivariate gaussian with statistics given in ( [ eq : mu ] ) and ( [ eq : sigma ] ) . therefore , the class - conditional means @xmath76 and covariance matrices @xmath81 are sufficient to completely describe the distribution of the feature vectors conditioned on @xmath3 .",
    "thus , these statistics are also sufficient to find the optimal testpoint locations , @xmath82 .",
    "however , since @xmath81 are clearly not equal for all @xmath3 , a closed form expression for the classification accuracy for this problem does not exist . instead , a @xmath83-dimensional integration is required and the limits , determined by the decision boundaries defined by ( [ eq : discriminant ] ) , are non - trivial .",
    "as is typically done in this scenario , we replace exact @xmath36 with a sub - optimum distance metric that is easier to evaluate and does not require a @xmath83-dimensional integral .",
    "in particular we use the bhattacharyya distance first studied for signal selection in @xcite shown to be a very effective as a `` goodness '' criterion in the process of of selecting effective features to be used in classification .",
    "the metric is shown here for reference : @xmath84 note that the bhattacharyya distance is calculated between 2 classes . as a result",
    ", the search for testpoints can only be performed for the @xmath85 case . however , this could be done sequentially through all the possible pairs of @xmath3 . as @xmath86 is a function of @xmath76 and @xmath81 which are functions of our testpoint selection , @xmath30 , then we can express it as @xmath87 .",
    "we thus find the good candidate testpoint by @xmath88 under the constraint @xmath27 .",
    "as this is an @xmath25-dimensional optimization problem , a closed - form solution is beyond the scope of this letter paper .",
    "instead , we turn to numerical optimization methods ( gradient descent methods ) to find local maxima . the intial point of these procedures could be chosen to coincide with the pdf - crossings or equally spaced over some interval .",
    "for the results section we focus on the quadrature feature which is a concatenation of the i and q component of each symbol . in fig .  [",
    "fig : testpoints ] , we show the results of the testpoint selection procedure with @xmath89 , under 0 db snr , for varying number of testpoints with the two class being 4-qam and 16-qam .    .",
    "the solid line shows the cdf difference between the two classes ( 4-qam and 16-qam , under snr=0 db , @xmath90 ) ]    the solid line plot corresponds to the difference of the two theoretical cdfs .",
    "we note that in the vd classifier the local maxima and minima of this plot are used as the testpoints .",
    "however , we find that the numerical optimization finds `` good '' testpoints to be close , but not exactly at the local maxima and minima .",
    "this is due to the additional information provided by the covariance matrices .",
    "in contrast to vd classifier that has a fixed number of testpoints ( 4 for this particular problem ) corresponding to the number of local maxima and minima , the optimization procedure allows more flexibility in choosing the number of testpoints . in fig .",
    "[ fig : testpoints ] , we show the result of the optimization procedure for a range of 1 to 8 testpoints .",
    "it confirms our intuition that `` good '' testpoints tend to be 1 ) spaced apart to avoid high correlation , 2 ) concentrated around locations that have high cdf difference , and 3 ) are not necessarily the same for different values of @xmath25 .",
    "this result further confirms the need to jointly optimize the testpoint locations .      as mentioned in the previous section , the proposed approach has the flexibility of varying the number of testpoints .",
    "this effectively gives more flexibility to trade - off classification accuracy with computational complexity .",
    "this idea is illustrated in fig .",
    "[ fig : vary_tp ] . for @xmath89 and snr=0",
    "db , we show the classification accuracy of the proposed method as the number of testpoints is increased from 1 to 8 , for all possible pairs of @xmath3 .",
    "the dotted lines correspond to the accuracy of the ml classifier which serves as an upperbound to classification accuracy , while the dashed lines correspond to that of the vd classifier .",
    "note that both are plotted as horizontal lines because ml does not utilize testpoints , while vd has a fixed number of testpoints corresponding to the pdf - crossings .     for all possible pairs of constellations of interest.the classification accuracy of both ml and vd classifiers",
    "are also shown for comparison .",
    "( snr=0 db , @xmath0=200 ) ]    [ fig : vary_tp ]    we see that the proposed method is able to exceed the accuracy of the vd classifier with as low as 3 testpoints .",
    "further , the method s accuracy could be improved by adding more testpoints but at the cost of higher complexity .",
    "we also note that with additional testpoints , the bayesian classifier reaches classification accuracy close to the ml classifier .    finally , in fig .",
    "[ fig : vary_snr ] , we compare the performance of the proposed method with the existing techniques under varying snr with @xmath90 symbols used for classification . to have a fair comparison ,",
    "the same number of testpoints are used for both vd and bayesian . for the entire range of snr the proposed bayesian approach",
    "is shown to provide substantial gains over the vd classifier .",
    "we emphasize again that asymptotically , the proposed approach is the optimal classifier when using the sampled distribution distance as the discriminating feature . also shown in the plot are the classification accuracy of the ml classifier which acts as the upperbound , and the conventional kuiper classifier .",
    "= 200 symbols used for classification .",
    "the same number of testpoints are used for both vd and bayesian . ]",
    "[ fig : vary_snr ]",
    "in this letter we presented the optimal discriminant functions for classifying using the sampled distribution distance .",
    "this method was shown to provide substantial gains compared to other existing approaches .",
    "the performance of this method is also shown to be close to the ml classifier but at significantly lower computational complexity .",
    "although modulation classification is presented in this paper to illustrate the basic concept , the approach is not limited to this application .",
    "the same classifier can be generalized to any classification problem where the cdf of each class is available .",
    "p.  urriza , e.  rebeiz , p.  pawelczak , and d.  cabric , `` computationally efficient modulation level classification based on probability distribution distance functions , '' _ ieee commun .",
    "_ , vol .  15 , no .  5 , pp .",
    "476478 , may 2011 ."
  ],
  "abstract_text": [
    "<S> in this letter , we derive the optimal discriminant functions for modulation classification based on the sampled distribution distance . </S>",
    "<S> the proposed method classifies various candidate constellations using a low complexity approach based on the distribution distance at specific testpoints along the cumulative distribution function . </S>",
    "<S> this method , based on the bayesian decision criteria , asymptotically provides the minimum classification error possible given a set of testpoints . </S>",
    "<S> testpoint locations are also optimized to improve classification performance . </S>",
    "<S> the method provides significant gains over existing approaches that also use the distribution of the signal features . </S>"
  ]
}