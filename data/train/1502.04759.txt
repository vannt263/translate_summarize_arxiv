{
  "article_text": [
    "coordinate descent ( cd ) algorithms for optimization have a history that dates to the foundation of the discipline .",
    "they are iterative methods in which each iterate is obtained by fixing most components of the variable vector @xmath0 at their values from the current iteration , and approximately minimizing the objective with respect to the remaining components .",
    "each such subproblem is a lower - dimensional ( even scalar ) minimization problem , and thus can typically be solved more easily than the full problem .",
    "cd methods are the archetype of an almost universal approach to algorithmic optimization : solving an optimization problem by solving a sequence of simpler optimization problems .",
    "the obviousness of the cd approach and its acceptable performance in many situations probably account for its long - standing appeal among practitioners .",
    "paradoxically , the apparent lack of sophistication may also account for its unpopularity as a subject for investigation by optimization researchers , who have usually been quick to suggest alternative approaches in any given situation",
    ". there are some very notable exceptions .",
    "the 1970 text of ortega and rheinboldt  ( * ? ? ?",
    "* section  14.6 ) included a comprehensive discussion of `` univariate relaxation , '' and such optimization specialists as luo and tseng  @xcite , tseng  @xcite , and bertsekas and tsitsiklis  @xcite made important contributions to understanding the convergence properties of these methods in the 1980s and 1990s .",
    "the situation has changed in recent years .",
    "various applications ( including several in computational statistics and machine learning ) have yielded problems for which cd approaches are competitive in performance with more reputable alternatives .",
    "the properties of these problems ( for example , the low cost of calculating one component of the gradient , and the need for solutions of only modest accuracy ) lend themselves well to efficient implementations of cd , and cd methods can be adapted well to handle such special features of these applications as nonsmooth regularization terms and a small number of equality constraints . at the same time , there have been improvements in the algorithms themselves and in our understanding of them . besides their extension to handle the features just mentioned , new variants that make use of randomization and acceleration have been introduced .",
    "parallel implementations that lend themselves well to modern computer architectures have been implemented and analyzed . perhaps most surprisingly , these developments are relevant even to the most fundamental problem in numerical computation : solving the linear equations @xmath1 .    in the remainder of this section , we state the problem types for",
    "which cd methods have been developed , and sketch the most fundamental versions of cd .",
    "section  [ sec : applications ] surveys applications both historical and modern .",
    "section  [ sec : algs ] sketches the types of algorithms that have been implemented and analyzed , and presents several representative convergence results .",
    "section  [ sec : parallel ] focuses on parallel cd methods , describing the behavior of these methods under synchronous and asynchronous models of computation .",
    "our approach throughout is to describe the cd methods in their simplest forms , to illustrate the fundamentals of the applications , implementations , and analysis .",
    "we focus almost exclusively on methods that adjust just one coordinate on each iteration .",
    "most applications use _ block _ coordinate descent methods , which adjust groups of blocks of indices at each iteration , thus searching along a coordinate hyperplane rather than a single coordinate direction .",
    "most derivation and analysis of single - coordinate descent methods can be extended without great difficulty to the block - cd setting ; the concepts do not change fundamentally .",
    "we mention too that much effort has been devoted to developing more general forms of cd algorithms and analysis , involving weighted norms and other features , that allow more flexible implementation and allow the proof of stronger and more general ( though usually not qualitatively different ) convergence results .",
    "the problem considered in most of this paper is the following unconstrained minimization problem : @xmath2 where @xmath3 is continuous .",
    "different variants of cd make further assumptions about @xmath4 .",
    "sometimes it is assumed to be smooth and convex , sometimes smooth and possibly nonconvex , and sometimes smooth but with a restricted domain .",
    "( we will make such assumptions clear in each discussion of algorithmic variants and convergence results . )",
    "motivated by recent popular applications , it is common to consider the following structured formulation : @xmath5 where @xmath4 is smooth , @xmath6 is a regularization function that may be nonsmooth and extended - valued , and @xmath7 is a regularization parameter .",
    "@xmath6 is often convex and usually assumed to be separable or block - separable .",
    "when separable , @xmath6 has the form @xmath8 where @xmath9 for all @xmath10 .",
    "the best known examples of separability are the @xmath11-norm ( in which @xmath12 and hence @xmath13 ) and box constraints ( in which @xmath14}(x_i)$ ] is the indicator function for the interval @xmath15 $ ] ) .",
    "block separability means that the @xmath16 identity matrix can be partitioned into column submatrices @xmath17 , @xmath18 such that @xmath19 block - separable examples include group - sparse regularizers in which @xmath20 .",
    "formulations of the type , with separable or block - separable regularizers , arise in such applications as compressed sensing , statistical variable selection , and model selection .",
    "the class of problems known as empirical risk minimization ( erm ) gives rise to a formulation that is particularly amenable to coordinate descent ; see @xcite .",
    "these problems have the form @xmath21 for vectors @xmath22 , @xmath23 and convex functions @xmath24 , @xmath23 and @xmath25",
    ". we can express linear least - squares , logistic regression , support vector machines , and other problems in this framework .",
    "recalling the following definition of the conjugate @xmath26 of a convex function @xmath27 : @xmath28 we can write the fenchel dual  ( * ? ? ?",
    "* section  31 ) of as follows : @xmath29 where @xmath30 is the @xmath31 matrix whose columns are @xmath32 , @xmath23 .",
    "the dual formulation is has special appeal as a target for coordinate descent , because of separability of the summation term .",
    "one interesting case is the system of linear equations @xmath33 which we assume to be a feasible system .",
    "the least - norm solution is found by solving @xmath34 whose lagrangian dual is @xmath35 ( we recover the primal solution from by setting @xmath36 . ) we can see that is a special case of the fenchel dual obtained from if we set @xmath37 where @xmath38 denotes the indicator function for @xmath39 , which is zero at @xmath39 and infinite elsewhere .",
    "( its conjugate is @xmath40 . )",
    "the primal problem can be restated correspondingly as @xmath41 where @xmath42 denotes the @xmath10th row of the matrix @xmath43 in , which has the form .",
    "the basic coordinate descent framework for continuously differentiable minimization is shown in algorithm  [ alg : cd ] .",
    "each step consists of evaluation of a single component @xmath44 of the gradient @xmath45 at the current point , followed by adjustment of the @xmath44 component of @xmath0 , in the opposite direction to this gradient component .",
    "( here and throughout , we use @xmath46_i$ ] to denote the @xmath10th component of the gradient @xmath47 . )",
    "there is much scope for variation within this framework .",
    "the components can be selected in a cyclic fashion , in which @xmath48 and @xmath49 + 1 , \\quad k=0,1,2,\\dotsc.\\ ] ] they can be required to satisfy an `` essentially cyclic '' condition , in which for some @xmath50 , each component is modified at least once in every stretch of @xmath51 iterations , that is , @xmath52 alternatively , they can be selected randomly at each iteration ( though not necessarily with equal probability ) . turning to steplength @xmath53",
    ": we may perform exact minimization along the @xmath44 component , or choose a value of @xmath53 that satisfies traditional line - search conditions ( such as sufficient decrease ) , or make a predefined `` short - step '' choice of @xmath53 based on prior knowledge of the properties of @xmath4 .",
    "set @xmath54 and choose @xmath55 ; choose index @xmath56 ; @xmath57_{i_k } e_{i_k}$ ] for some @xmath58 ; @xmath59 ; termination test satisfied ;    the cd framework for the separable regularized problem , is shown in algorithm  [ alg : cdreg ] . at iteration @xmath60 ,",
    "a scalar subproblem is formed by making a linear approximation to @xmath4 along the @xmath44 coordinate direction at the current iterate @xmath61 , adding a quadratic damping term weighted by @xmath62 ( where @xmath53 plays the role of a steplength ) , and treating the relevant regularization term @xmath63 explicitly .",
    "note that when the regularizer @xmath63 is not present , the step is identical to the one taken in algorithm  [ alg : cd ] . for some interesting choices of @xmath63 ( for example @xmath64 ) ,",
    "it is possible to write down a closed - form solution of the subproblem ; no explicit search is needed . the operation of solving such subproblems",
    "is often referred to as a `` shrink operation , '' which we denote by @xmath65 and define as follows : @xmath66 by stating the subproblem in algorithm  [ alg : cdreg ] equivalently as @xmath67_{i_k } ) \\right\\|^2 + \\omega_i(\\chi),\\ ] ] we can express the cd update as @xmath68_{i_k})$ ] .",
    "set @xmath54 and choose @xmath55 ; choose index @xmath56 ; @xmath69_{i_k } + \\frac{1}{2\\alpha_k } \\|\\chi - x^k_{i_k}\\|_2 ^ 2 + \\lambda \\omega_i(\\chi)$ ] for some @xmath58 ; @xmath70 ; @xmath59 ; termination test satisfied ;    algorithms  [ alg : cd ] and [ alg : cdreg ] can be extended to block - cd algorithms in a straightforward way , by updating a block of coordinates ( denoted by the column submatrix @xmath71 of the identity matrix ) rather than a single coordinate . in algorithm",
    "[ alg : cdreg ] , it is assumed that the choice of block is consistent with the block - separable structure of the regularization function @xmath6 , that is , @xmath71 is a concatenation of several of the submatrices @xmath17 in .      for the formulation that arises from the linear system @xmath1 , let us assume that the rows of @xmath43 are normalized , that is , @xmath72 applying algorithm  [ alg : cd ] to with @xmath73 , each step has the form @xmath74",
    "if we maintain and update the estimate @xmath75 of the solution to the primal problem after each update of @xmath61 , according to @xmath76 , we obtain @xmath77 which is the update formula for the kaczmarz algorithm @xcite . following this update",
    ", we have using that @xmath78 so that the @xmath44 equation in the system @xmath1 is now satisfied .",
    "this method if sometimes known as the `` method of successive projections '' because it projects onto the feasible hyperplane for a single constraint at every iteration .",
    "stochastic gradient ( sg ) methods , also undergoing a revival of interest because of their usefulness in data analysis and machine learning applications , minimize a smooth function @xmath4 by taking a ( negative ) step along an estimate @xmath79 of the gradient @xmath80 at iteration @xmath60 .",
    "it is often assumed that @xmath79 is an unbiased estimate of @xmath81 , that is , @xmath82 , where the expectation is taken over whatever random variables were used in obtaining @xmath79 from the current iterate @xmath61 .",
    "randomized cd algorithms can be viewed as a special case of sg methods , in which @xmath83_{i_k } e_{i_k}$ ] , where @xmath44 is chosen uniformly at random from @xmath84 .",
    "here , @xmath44 is the random variable , and we have @xmath85_{i } e_{i } =   \\nabla f(x^k),\\ ] ] certifying unbiasedness .",
    "however , cd algorithms have the advantage over general sg methods that descent in @xmath4 can be guaranteed at every iteration . moreover , the variance of the gradient estimate @xmath79 shrinks to zero as the iterates converge to a solution @xmath86 , since every component of @xmath87 is zero . by contrast , in general sg methods , the gradient estimates @xmath79 may be nonzero even when @xmath61 is a solution .",
    "the relationship between cd and sg methods can also be discerned from the fenchel dual pair and .",
    "sg methods are quite popular for solving formulation , where the estimate @xmath79 is obtained by taking a single term @xmath44 from the summation and using @xmath88 as the estimate of the gradient of the _ full _ summation .",
    "this approach corresponds to applying cd to the dual , where the component @xmath44 of @xmath0 is selected for updating at iteration @xmath60 .",
    "this relationship is typified by the kaczmarz algorithm for @xmath1 , which can be derived either as cd applied to the dual formulation or as sg applied to the sum - of - squares problem @xmath89    cd is related in an obvious way to the gauss - seidel method for @xmath90 systems of linear equations , which adjusts the @xmath44 variable to ensure satisfaction of the @xmath44 equation , at iteration @xmath60 .",
    "( successive over - relaxation ( sor ) modifies this approach by scaling each gauss - seidel step by a factor @xmath91 for some constan @xmath92 , chosen so as to improve the convergence rate . )",
    "standard gauss - seidel and sor use the cyclic choice of coordinates , whereas a random choice of @xmath44 would correspond to `` randomized '' versions of these methods . to make the connections more explicit :",
    "the gauss - seidel method applied to the normal equations for  that is , @xmath93  is equivalent to applying algorithm  [ alg : cd ] to the least - squares problem , when the steplength @xmath53 is chosen to minimize the objective exactly along the given coordinate direction .",
    "sor also corresponds to algorithm  [ alg : cd ] , with @xmath53 chosen to be a factor @xmath91 times the exact minimum .",
    "these equivalences allow the results of section  [ sec : algs ] to be used to derive convergence rates for gauss - seidel applied to the normal equations , including linear convergence when @xmath94 is nonsingular .",
    "note that these results do not require feasibility of the original equations .",
    "we mention here several applications of cd methods to practical problems , some dating back decades and others relatively new .",
    "our list is necessarily incomplete , but it attests to the popularity of cd in a wide variety of application communities .",
    "bouman and sauer @xcite discuss an application to positron emission tomography ( pet ) in which the objective has the form where @xmath4 is smooth and convex and @xmath6 is a sum of terms of the form @xmath95 for some pairs of components @xmath96 of @xmath0 and some @xmath97 $ ] .",
    "ye et al .",
    "@xcite apply a similar method to a different objective arising from optical diffusion tomography .",
    "liu , paratucco , and zhang  @xcite describe a block cd approach for linear least squares plus a regularization function consisting of a sum of @xmath98 norms of subvectors of @xmath0 .",
    "the technique is applied to semantic basis discovery , which learns from data how to identify and classify the functional mri response of a person s brain when they hear certain english words .",
    "canutescu and dunbrack  @xcite describe a cyclic coordinate descent method for determining protein structure , adjusting the dihedral angles in a protein chain so that the atom at the end of the chain comes close to a specified position in space .",
    "florian and chen  @xcite recover origin - destination matrices from observed traffic flows by alternately solving a bilevel optimization problem over two blocks of variables : the origin - destination demands and the proportion of each origin - destination flow assigned to each arc in the network .",
    "breheny and huang  @xcite discuss coordinate descent for linear and logistic regression with nonconvex separable regularization terms , reporting results for genetic association and gene expression studies .",
    "the sparsenet algorithm  @xcite applied to problems with these same nonconvex separable regularizers uses warm - started cyclic coordinate descent as an inner loop to solve a sequence of problems in which the regularization parameter @xmath99 in and the parameters defining concavity of the regularization functions are varied .",
    "friedman , hastie , and tibshirani  @xcite propose a block cd algorithm for estimating a sparse inverse covariance matrix , given a sample covariance matrix @xmath100 and taking the variable in their formulation to be a modification @xmath101 of @xmath100 , such that @xmath102 is sparse .",
    "the resulting `` graphical lasso '' algorithm cycles through the rows / columns of @xmath101 ( in the style of block cd ) , solving a standard lasso problem to calculate each update . the same authors @xcite apply cd to generalized linear models such as linear least squares and logistic regression , with convex regularization terms .",
    "their framework include such formulations as lasso , graphical lasso , elastic net , and the dantzig selector , and is implemented in the package glmnet .",
    "chang , hsieh , and lin  @xcite use cyclic and stochastic cd to solve a squared - loss formulation of the support vector machine ( svm ) problem in machine learning , that is , @xmath103 where @xmath104 are feature vector / label pairs and @xmath99 is a regularization parameter .",
    "this problem is an important instance of the erm form . in the best known early application of coordinate descent to svm , platt  @xcite deals with a hinge - loss formulation of svm , which is identical to except that the square on each term of the summation is omitted .",
    "the dual of this problem has bounds on its variables along with a single linear constraint .",
    "platt s procedure smo ( for `` sequential minimal optimization '' ) , applied to the dual , changes two variables at a time , with the variable pair chosen according to a `` greedy '' criterion and the search direction chosen to maintain feasibility of the linear constraint .",
    "sardy , bruce , and tseng  @xcite consider the basis - pursuit formulation of wavelet denoising : @xmath105 this formulation is equivalent to the well known lasso of tibshirani  @xcite and has become famous because of its applicability to sparse recovery and compressed sensing .",
    "although this formulation fits the erm framework and could thus be dualized before applying cd , the approach of @xcite applies block cd directly to the primal formulation .",
    "applications of block cd approaches to transceiver design for cellular networks and to tensor factorization are discussed in razaviyayn  ( * ? ? ?",
    "* section  8) .",
    "finally , we mention several popular problem classes and algorithms that can be interpreted as cd algorithms , but for which such an interpretation may not be particularly helpful in understanding the performance of the algorithm .",
    "first , we consider low - rank matrix completion problems in which we are presented with limited information about a rectangular matrix @xmath106 and seek matrices @xmath107 and @xmath108 ( with @xmath109 small ) such that @xmath110 is consistent with the observations of @xmath111",
    ". when the observations satisfy a restricted isometry property ( an assumption commonly made in compressed sensing ; see ( * ? ? ?",
    "* definition  3.1 ) for a definition that applies to matrix completion ) , the block cd approach of jain , netrapalli , and sanghavi  ( * ? ? ?",
    "* algorithm  1 ) converges to a solution .",
    "this approach defines the objective to be the least - squares fit between the observations and their predicted values according to the product @xmath110  a function that is nonconvex with respect to @xmath112  and minimizes alternately over @xmath113 and @xmath114 , respectively .",
    "standard analysis of cd for nonconvex functions would yield at best stationarity of accumulation points , but much stronger results are attained in @xcite because of special assumptions that are made on the problem in this paper .",
    "second , we consider the `` alternating - direction method of multipliers '' ( admm ) @xcite , which has gained great currency in the past few years because of its usefulness in solving regularized problems in statistics and machine learning , and in designing parallel algorithms .",
    "each major iteration of admm consists of an ( approximate ) minimization of the augmented lagrangian function for a constrained optimization problem over each block of primal variables in turn , followed by an update to the lagrange multiplier estimates .",
    "it might seem appealing to do multiple cycles of updating the primal variable blocks , in the manner of cyclic block cd , thus finding a better approximation to the solution of each subproblem over _ all _ primal variables and moving the method closer to the standard augmented lagrangian approach .",
    "eckstein and yao  @xcite show , however , that this `` approximate augmented lagrangian '' approach has a fundamentally different theoretical interpretation from admm , and a computational comparison between the two approaches ( * ? ? ?",
    "* section  5 ) appears to show an advantage for admm .",
    "we now describe the most important variants of coordinate descent and present their convergence properties , including the proofs of some fundamental results .",
    "we also discuss the implementation of accelerated cd methods for problems of the form and for the kaczmarz algorithm for @xmath1 .",
    "as mentioned in the introduction , we deal with the most elementary framework possible , to expose the essential properties of the methods .",
    "we start with a simple but intriguing example due to powell  ( * ? ? ?",
    "* formula ( 2 ) ) of a function in @xmath115 for which cyclic cd fails to converge to a stationary point .",
    "the nonconvex , continuously differentiable function @xmath116 is defined as follows : @xmath117 it has minimizers at the corners @xmath118 and @xmath119 of the unit cube , but coordinate descent with exact minimization , started near ( but just outside of ) one of the other vertices of the cube cycles around the neighborhoods of six points that are close to the six non - optimal vertices .",
    "powell shows that the cyclic nonconvergence behavior is rather special and is destroyed by small perturbations on this particular example , and we can note that a randomized coordinate descent method applied to this example would be expected to converge to the vicinity of a solution within a few steps . still",
    ", this example and others in @xcite make it clear that we can not expect a general convergence result for nonconvex functions , of the type that are available for full - gradient descent .",
    "results are available for the nonconvex case under certain additional assumptions that still admit interesting applications .",
    "bertsekas  ( * ? ? ?",
    "* proposition  2.7.1 ) describes convergence of a cyclic approach applied to nonconvex problems , under the assumption that the minimizer along any coordinate direction from any point @xmath0 is unique .",
    "more recent work @xcite focuses on cd with two blocks of variables , applied to functions that satisfy the so - called kurdyka - ojasiewicz ( kl ) property , such as semi - algebraic functions . convergence of subsequences or the full sequence @xmath120 to stationary points can be proved in this setting .    , width=288 ]      for most of this section , we focus on the unconstrained problem , where the objective @xmath4 is _ convex _ and lipschitz continuously differentiable . in some places ,",
    "we assume strong convexity with respect to the euclidean norm , that is , existence of a modulus of convexity @xmath121 such that @xmath122 ( henceforth , we use @xmath123 to denote the euclidean norm @xmath124 , unless otherwise specified . ) we define lipschitz constants that are tied to the component directions , and are key to the algorithms and their analysis .",
    "the first set of such constants are the _ component lipschitz constants _ , which are positive quantities @xmath125 such that for all @xmath126 and all @xmath127 we have @xmath128 to be such that @xmath129 the standard lipschitz constant @xmath130 is such that @xmath131 for all @xmath0 and @xmath132 of interest . by referring to relationships between norm and trace of a symmetric matrix",
    ", we can assume that @xmath133 .",
    "( the upper bound is achieved when @xmath134 , for @xmath135 . )",
    "we also define the _ restricted lipschitz constant _",
    "@xmath136 such that the following property is true for all @xmath137 , all @xmath138 , and all @xmath23 : @xmath139 clearly , @xmath140 .",
    "the ratio @xmath141 is important in our analysis of asynchronous parallel algorithms in section  [ sec : parallel ] . in the case of @xmath4 convex and",
    "twice continuously differentiable , we have by positive semidefiniteness of the @xmath142 at all @xmath0 that @xmath143_{ij } | \\le \\left ( [ \\nabla^2 f(x)]_{ii}[\\nabla^2 f(x)]_{jj }   \\right)^{1/2},\\ ] ] from which we can deduce that @xmath144 however , we can derive stronger bounds on @xmath145 for functions @xmath4 in which the coupling between components of @xmath0 is weak . in the extreme case",
    "in which @xmath4 is separable , we have @xmath146 .",
    "the coordinate lipschitz constant corresponds @xmath147 to the maximal absolute value of the diagonal elements of the hessian @xmath148 , while the restricted lipschitz constant @xmath136 is related to the maximal column norm of the hessian .",
    "therefore , if the hessian is positive semidefinite and diagonally dominant , the ratio @xmath145 is at most @xmath149 .",
    "the following assumption is useful in the remainder of the paper .",
    "[ ass : fconv ] the function @xmath4 in is convex and uniformly lipschitz continuously differentiable , and attains its minimum value @xmath150 on a set @xmath151 .",
    "there is a finite @xmath152 such that the level set for @xmath4 defined by @xmath153 is bounded , that is , @xmath154      in randomized cd algorithms , the update component @xmath44 is chosen randomly at each iteration . in algorithm  [ alg : rcd ]",
    "we consider the simplest variant in which each @xmath44 is selected from @xmath84 with equal probability , independently of the selections made at previous iterations .",
    "( we can think of this scheme as `` sampling with replacement '' from the set @xmath84 . )",
    "choose @xmath55 ; set @xmath54 ; choose index @xmath44 with uniform probability from @xmath84 , independently of choices at prior iterations ; set @xmath57_{i_k } e_{i_k}$ ] for some @xmath58 ; @xmath59 ; termination test satisfied ;    we denote expectation with respect to a single random index @xmath44 by @xmath155 , while @xmath156 denotes expectation with respect to all random variables @xmath157 .",
    "we prove a convergence result for the randomized algorithm , for the simple steplength choice @xmath158 .",
    "( the proof is a simplified version of the analysis in nesterov  ( * ? ? ?",
    "* section  2 ) . a result similar",
    "to is proved by shalev - schwartz and tewari  @xcite for certain types of @xmath11-regularized problems . )",
    "[ th : rcd ] suppose that assumption  [ ass : fconv ] holds .",
    "suppose that @xmath159 in algorithm  [ alg : rcd ] .",
    "then for all @xmath160 we have @xmath161 when @xmath121 in , we have in addition that @xmath162    by application of taylor s theorem , and using and , we have @xmath163_{i_k } e_{i_k } \\right ) \\\\ \\nonumber & \\le f(x^k ) - \\alpha_k [ \\nabla f(x^k)]_{i_k}^2 + \\frac12 \\alpha_k^2 l_{i_k } [ \\nabla f(x^k)]_{i_k}^2",
    "\\\\   \\nonumber & \\le f(x^k ) - \\alpha_k \\left ( 1-\\frac{{l_{\\mbox{\\rm\\scriptsize max}}}}{2 } \\alpha_k",
    "\\right ) [ \\nabla f(x^k)]_{i_k}^2 \\\\",
    "\\label{eq : rcd.1 } & = f(x^k ) - \\frac{1}{2 { l_{\\mbox{\\rm\\scriptsize max } } } }   [ \\nabla f(x^k)]_{i_k}^2,\\end{aligned}\\ ] ] where we substituted the choice @xmath164 in the last equality .",
    "taking the expectation of both sides of this expression over the random index @xmath44 , we have @xmath165_{i}^2   \\\\ \\label{eq : rcd.2 } & = f(x^k ) - \\frac{1}{2n{l_{\\mbox{\\rm\\scriptsize max } } } } \\| \\nabla f(x^k ) \\|^2.\\end{aligned}\\ ] ] ( we used here the facts that @xmath61 does not depend on @xmath44 , and that @xmath44 was chosen from among @xmath84 with equal probability . )",
    "we now subtract @xmath166 from both sides this expression , take expectation of both sides with respect to _ all _ random variables @xmath167 , and use the notation @xmath168 to obtain @xmath169 ^ 2.\\ ] ] ( we used jensen s inequality in the second inequality . ) by convexity of @xmath4 we have for any @xmath170 that @xmath171 where the final inequality is because @xmath172 , so that @xmath61 is in the level set in . by taking expectations of both sides ,",
    "we obtain @xmath173 when we substitute this bound into , and rearrange , we obtain @xmath174 we thus have @xmath175 by applying this formula recursively , we obtain @xmath176 so that holds , as claimed .    in the case of @xmath4",
    "strongly convex with modulus @xmath121 , we have by taking the minimum of both sides with respect to @xmath177 in , and setting @xmath178 , that @xmath179 by using this expression to bound @xmath180 in , we obtain @xmath181 recursive application of this formula leads to .    note",
    "that the same convergence expressions can be obtained for more refined choices of steplength @xmath53 , by making minor adjustments to the logic in .",
    "for example , the choice @xmath182 leads to the same bounds and .",
    "the same bounds hold too when @xmath53 is the exact minimizer of @xmath4 along the coordinate search direction ; we modify the logic in for this case by taking the minimum of all expressions with respect to @xmath53 , and use the fact that @xmath164 is in general a suboptimal choice .",
    "we can compare with the corresponding result for full - gradient descent with constant steplength @xmath183 ( where @xmath130 is from ) .",
    "the iteration @xmath184 leads to a convergence expression @xmath185 ( see , for example , @xcite ) .",
    "since , as we have noted , @xmath130 can be as large as @xmath186 , the bound in this expression may be equivalent to in extreme cases . more typically , these two lipschitz constants are comparable in size , and the appearance of the additional factor @xmath187 in indicates that we pay a price in terms of slower convergence for using only one component of @xmath81 , rather than the full vector .",
    "expected linear convergence rates have been proved under assumptions weaker than strong convexity ; see for example the `` essential strong convexity '' property of @xcite , the `` optimal strong convexity '' property of @xcite , the `` generalized error bound '' property of @xcite , and ( * ? ? ?",
    "* assumption  2 ) , which concerns linear growth in a measure of the gradient with distance from the solution set .",
    "a variant on algorithm  [ alg : rcd ] uses `` sampling without replacement . '' here the computation proceeds in `` epochs '' of @xmath187 consecutive iterations . at the start of each epoch",
    ", the set @xmath188 is shuffled .",
    "the iterations then proceed by setting @xmath44 to each entry in turn from the ordered set .",
    "this kind of randomization has been shown in several contexts to be superior to the sampling - with - replacement scheme analyzed above , but a theoretical understanding of this phenomenon remains elusive .",
    "[ [ randomized - kaczmarz - algorithm . ] ] randomized kaczmarz algorithm .",
    "+ + + + + + + + + + + + + + + + + + + + + + + + + + + + + +    it is worth proving an expected linear convergence result for the kaczmarz iteration for linear equations @xmath1 as a separate , more elementary analysis . in one sense ,",
    "the result is a special case of theorem  [ th : rcd ] since , as we showed above , the iteration is obtained by applying algorithm  [ alg : rcd ] to the dual formulation . in another sense ,",
    "the result is stronger , since we obtain a linear rate of convergence without requiring strong convexity of the objective , that is , the system @xmath1 is allowed to have multiple solutions .",
    "we denote by @xmath189 the minimum nonzero eigenvalue of @xmath190 and let @xmath191 denote projection onto the solution set of @xmath1 .",
    "we have @xmath192 where we have used normalization of the rows and the fact that @xmath193 .",
    "by taking expectations of both sides with respect to @xmath44 , we have @xmath194 by taking expectations of both sides with respect to all random variables @xmath167 , and proceeding recursively , we obtain @xmath195 ( this analysis is slightly generalized from strohmer and vershynin  @xcite to allow for nonunique solutions of @xmath1 ; see also @xcite . )      the accelerated randomized algorithm , specified here as algorithm  [ alg : arcd ] , was proposed by nesterov  @xcite .",
    "it assumes that an estimate is available of modulus of strong convexity @xmath196 from , as well as estimates of the component - wise lipschitz constants @xmath125 from .",
    "( the algorithm remains valid if we simply use @xmath147 in place of @xmath197 for all @xmath60 . )",
    "choose @xmath55 ; set @xmath54 , @xmath198 , @xmath199 ; choose @xmath200 to be the larger root of @xmath201 set @xmath202 set @xmath203 ; choose index @xmath56 with uniform probability and set @xmath204_{i_k } e_{i_k}$ ] ; set @xmath205 ; set @xmath206 ; @xmath59 ; termination test satisfied ;    the approach is a close relative of the accelerated ( full-)gradient methods that have become extremely popular in recent years .",
    "these methods have their origin in a 1983 paper of nesterov  @xcite and owe much of their recent popularity to a recent incarnation known as fista @xcite and an exposition in nesterov s 2004 monograph  @xcite , as well as ease of implementation and good practical experience . in their use of momentum in the choice of step  the search direction combines new gradient information with the previous search direction  these methods are also related to such other classical techniques as the heavy - ball method ( see @xcite ) and conjugate gradient methods .",
    "nesterov  ( * ? ? ?",
    "* theorem  6 ) proves the following convergence result for algorithm  [ alg : arcd ] .",
    "[ th : arcd ] suppose that assumption  [ ass : fconv ] holds , and define @xmath207 then for all @xmath208 we have @xmath209^{-2 } \\\\ \\label{eq : arcd.2 } & \\le s_0 \\left ( \\frac{n}{k+1 } \\right)^2.\\end{aligned}\\ ] ]    in the strongly convex case @xmath121 , the term @xmath210 eventually dominates the second term in brackets in , so that the linear convergence rate suggested by this expression is significantly faster than the corresponding rate for algorithm  [ alg : rcd ] .",
    "essentially , the measure @xmath211 of conditioning in is replaced by its square root in , suggesting a decrease by a factor of @xmath212 in the number of iterations required to meet a specified error tolerance . in the sublinear rate",
    "bound , which holds even for weakly convex @xmath4 , the @xmath213 bound of is replaced by a @xmath214 factor , implying a reduction from @xmath215 to @xmath216 in the number of iterations required to meet a specified error tolerance .",
    "choose @xmath217 ; set @xmath54 , @xmath218 , @xmath199 ; choose @xmath200 to be the larger root of @xmath201 set @xmath219 set @xmath220 ; choose index @xmath221 with uniform probability and set @xmath222 ; set @xmath223 ; set @xmath224 ; @xmath59 ; termination test satisfied ;    one fact detracts from the appeal of accelerated cd methods over standard methods : the higher cost of each iteration of algorithm  [ alg : arcd ] .",
    "both standard and accelerated variants require calculation of one element of the gradient , but algorithm  [ alg : rcd ] requires an update of just a single component of @xmath0 , whereas algorithm  [ alg : arcd ] also requires manipulation of the generally dense vectors @xmath177 and @xmath225",
    ". moreover , the gradient is evaluated at @xmath61 in algorithm  [ alg : rcd ] , where the argument changes by only one component from the prior iteration , a fact that can be exploited in several contexts . in algorithm  [ alg :",
    "arcd ] , the argument @xmath226 for the gradient changes more extensively from one iteration to the next , making it less obvious whether such economies are available . however , by using a change of variables due to lee and sidford  @xcite , it is possible to implement the accelerated randomized cd approach efficiently for problems with certain structure , including the linear system @xmath1 and certain problems of the form .",
    "we explain the lee - sidford technique in the context of the kaczmarz algorithm for , assuming normalization of the rows of @xmath43 .",
    "as we explained in , the kaczmarz algorithm is obtained by applying cd to the dual formulation with variables @xmath0 , but operating in the space of `` primal '' variables @xmath227 using the transformation @xmath36 .",
    "if we apply the transformations @xmath228 and @xmath229 to the other vectors in algorithm  [ alg : arcd ] , and use the fact of normalization ( and hence @xmath230 for all @xmath231 ) to note that @xmath232 in , we obtain algorithm  [ alg : ark ] .    when the matrix @xmath43 is dense , there is only a small factor of difference between the per - iteration workload of the standard kaczmarz algorithm and its accelerated variant , algorithm  [ alg : ark ] .",
    "both require @xmath233 operations per iteration .",
    "however , when @xmath43 is sparse , the computational difference between the two algorithms becomes substantial . at iteration @xmath60 , the standard kaczmarz algorithm requires computation proportion to a small multiple of the number of nonzeros in row @xmath234 ( which we denote by @xmath235 ) .",
    "meanwhile , iteration @xmath60 of algorithm  [ alg : ark ] requires manipulation of the dense vectors @xmath236 and @xmath237  both @xmath238 processes  and the benefits of sparsity are lost .",
    "this apparent defect was partly remedied in @xcite by `` caching '' the updates to these vectors , resulting in a number of cycles within which updates gradually `` fill in . ''",
    "the more effective approach of @xcite performs a change of variables from @xmath236 and @xmath237 to two other vectors @xmath239 and @xmath240 that _ can _ be updated in @xmath241 operations . to describe this representation , we start by noting that if we substitute for @xmath75 and @xmath242 in the formulas of algorithm  [ alg : ark ] , we obtain the updates to @xmath236 and @xmath237 in the following form : @xmath243 = \\left [ \\begin{matrix } \\tilde{v}^{k } & \\tilde{y}^{k } \\end{matrix } \\right ] r_k   - s_k,\\ ] ] where @xmath244 , \\\\ s_k & : = ( a_{i_k } \\tilde{y}^k - b_{i_k } ) a_{i_k}^t \\left [ \\begin{matrix } \\gamma_k & ( 1-\\alpha_{k+1 } + \\alpha_{k+1 } \\gamma_k )   \\end{matrix } \\right].\\end{aligned}\\ ] ] note that @xmath245 is a @xmath246 matrix while @xmath247 is an @xmath248 matrix with nonzeros only in those rows for which @xmath249 has a nonzero entry .",
    "we define a change of variables based on another @xmath250 matrix @xmath251 , as follows : @xmath252 = \\left [ \\begin{matrix } \\hat{v}^k & \\hat{y}^k \\end{matrix } \\right ] b_k,\\ ] ] where we initialize with @xmath253 . by substituting this representation into",
    ", we obtain @xmath254 b_{k+1 } = \\left [ \\begin{matrix } \\hat{v}^{k } & \\hat{y}^{k } \\end{matrix } \\right ] b_k r_k   - s_k,\\ ] ] so we can maintain validity of the representation at iteration @xmath255 by setting @xmath256 : = \\left [ \\begin{matrix } \\hat{v}^{k } & \\hat{y}^{k } \\end{matrix } \\right ] - s_k b_{k+1}^{-1}.\\ ] ] the computations in can be performed in @xmath257 operations , and can replace the relatively expensive computations of @xmath237 and @xmath258 in algorithm  [ alg : ark ] .",
    "the only other operation of note in this algorithm  computation of @xmath259  can also be performed in @xmath257 operations using the @xmath260 representation , by noting from that @xmath261    this efficient implementation can be extended to the dual empirical risk minimization problem for certain choices of regularization function @xmath262 , for example , @xmath263 ; see @xcite . as pointed out in @xcite , the key requirement for the efficient scheme",
    "is that the gradient term @xmath264_{i_k}$ ] can be evaluated efficiently after an update to the two vectors in the alternative representation of @xmath226 , and to the two coefficients in this representation .",
    "another variant of this implementation technique appears in ( * ? ? ?",
    "* section  5 ) .",
    "we have the following result from @xcite for the cyclic variant of algorithm  [ alg : cd ] .",
    "[ th : cd.cyclic ] suppose that assumption  [ ass : fconv ] holds .",
    "suppose that @xmath159 in algorithm  [ alg : cd ] , with the index @xmath44 at iteration @xmath60 chosen according to the cyclic ordering ( with @xmath48 ) .",
    "then for @xmath265 , we have @xmath266 when @xmath121 in the strong convexity condition , we have in addition for @xmath265 that @xmath267    the result follows from theorems  3.6 and 3.9 in @xcite when we note that ( i ) each iteration of algorithm bcgd in @xcite corresponds to a `` cycle '' of @xmath187 iterations in algorithm  [ alg : cd ] ; ( ii ) we update coordinates rather than blocks , so that the parameter @xmath268 in @xcite is equal to @xmath187 ; ( iii ) we set @xmath269 and @xmath270 in @xcite both to @xmath147 .    comparing the complexity bounds for the cyclic variant with the corresponding bounds proved in theorem  [ th : rcd ] for the randomized variant",
    ", we see that since @xmath271 in general , the numerator in is @xmath272 , in contrast to @xmath238 term in .",
    "a similar factor of @xmath187 in seen in comparing to , when we note that @xmath273 for small values of @xmath274 .",
    "the bounds in theorem  [ th : cd.cyclic ] are deterministic , however , rather than being bounds on expected nonoptimality , as in theorem  [ th : rcd ] .",
    "we noted in subsection  [ sec : assumptions ] that the ratio @xmath275 lies in the interval @xmath276 $ ] when @xmath4 is a convex quadratic function and both parameters are set to their best values .",
    "lower values of this ratio are attained on functions that are `` more decoupled '' and larger values attained when there is a greater dependence between the coordinates .",
    "larger values lead to weaker bounds in theorem  [ th : cd.cyclic ] , which accords with our intuition ; we expect cd methods to require more iterations to resolve the coupling of the coordinates .    we are free to make other , larger choices of @xmath147 ; they need only satisfy the conditions and .",
    "larger values of @xmath147 lead to shorter steps @xmath277 and different complexity expressions . for @xmath278 ,",
    "for example , the bound in becomes @xmath279 which is worse by a factor of approximately @xmath280 than the bound for the full - step gradient descent approach . for @xmath281 , we obtain @xmath282 which still trails by a factor of @xmath283 .      in this section",
    "we consider the separable regularized formulation , where @xmath4 is smooth and strongly convex , and each @xmath63 , @xmath23 is convex .",
    "we prove a result similar to the second part of theorem  [ th : rcd ] for a randomized version of algorithm  [ alg : cdreg ] .",
    "the proof is a simplified version of the analysis from @xcite .",
    "it makes use of the following assumption .",
    "[ ass : hconv ] the function @xmath4 in is uniformly lipschitz continuously differentiable and strongly convex with modulus @xmath121 ( see ) .",
    "the functions @xmath63 , @xmath23 are convex .",
    "the function @xmath284 in attains its minimum value @xmath285 at a unique point @xmath86 .",
    "our result uses the coordinate lipschitz constant @xmath147 for @xmath4 , as defined in .",
    "note that the modulus of convexity @xmath286 for @xmath4 is also the modulus of convexity for @xmath284 . by elementary results for convex functions",
    ", we have @xmath287    [ th : rcd.reg ] suppose that assumption  [ ass : hconv ] holds .",
    "suppose that the indices @xmath44 in algorithm  [ alg : cdreg ] are chosen independently for each @xmath60 with uniform probability from @xmath84 , and that @xmath158 .",
    "then for all @xmath208 , we have @xmath288    define the function @xmath289 and note that this function is separable in the components of @xmath290 , and attains its minimum over @xmath290 at the vector @xmath291 whose @xmath44 component is defined in algorithm  [ alg : cdreg ] .",
    "note by strong convexity that @xmath292 we have by minimizing both sides over @xmath290 in this expression that @xmath293 } \\",
    ", h(\\alpha x^ * + ( 1-\\alpha ) x^k ) + \\frac12 ( { l_{\\mbox{\\rm\\scriptsize max}}}-\\sigma ) \\alpha^2 \\|x^k - x^ * \\|^2                                     \\\\",
    "\\nonumber   & \\le \\min_{\\alpha \\in [ 0,1 ] } \\ , \\alpha h^ * + ( 1-\\alpha ) h(x^k ) + \\frac12 \\left [ ( { l_{\\mbox{\\rm\\scriptsize max}}}-\\sigma ) \\alpha^2 - \\sigma \\alpha ( 1-\\alpha ) \\right ] \\| x^k - x^ * \\|^2 \\\\ \\label{eq : zig.2 }   & \\le \\frac{\\sigma}{{l_{\\mbox{\\rm\\scriptsize max } } } } h^ * + \\left ( 1-\\frac{\\sigma}{{l_{\\mbox{\\rm\\scriptsize max } } } } \\right ) h(x^k),\\end{aligned}\\ ] ] where we used for the first inequality , for the third inequality , and the particular value @xmath294 for the fourth inequality ( for which value the coefficient of @xmath295 vanishes ) . taking the expected value of @xmath296 over the index @xmath44",
    ", we have @xmath297 \\\\ & \\le \\frac{1}{n } \\sum_{i=1}^n \\left\\ { f(x^k ) + [ \\nabla f(x^k)]_i ( z^k_i - x^k_i ) + \\frac12 { l_{\\mbox{\\rm\\scriptsize max}}}(z^k_i - x^k_i)^2 \\right .",
    "\\\\ & \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad    \\left .",
    "+ \\lambda \\omega_i(z^k_i ) + \\lambda \\sum_{j \\neq i } \\omega_j(x^k_j ) \\right\\ } \\\\",
    "& = \\frac{n-1}{n } h(x^k ) + \\frac{1}{n } \\left [ f(x^k ) + \\nabla f(x^k)^t(z^k - x^k ) \\right . \\\\ & \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad   + \\left . \\frac12 { l_{\\mbox{\\rm\\scriptsize max}}}\\| z^k - x^k\\|^2 + \\lambda \\omega(z^k ) \\right ] \\\\ & = \\frac{n-1}{n } h(x^k ) + \\frac{1}{n } h(x^k , z^k).\\end{aligned}\\ ] ] by subtracting @xmath285 from both sides of this expression , and using to substitute for @xmath298 , we obtain @xmath299 by taking expectations of both sides of this expression with respect to the random indices @xmath300 , we obtain @xmath301 the result follows from a recursive application of this formula .    a result similar to can be proved for the case",
    "in which @xmath4 is convex but not strongly convex , but there are a few technical complications , and we refer the reader to @xcite for details .",
    "an extension of the fixed - step approach to separable composite objectives , with _ nonconvex _",
    "smooth part @xmath4 is discussed in @xcite , where it is shown that accumulation points of the sequence of iterates are stationary and that a measure of optimality decreases to zero at a sublinear ( @xmath213 ) rate .",
    "a full computational comparison between variants of cd ( and between cd and other methods ) is beyond the scope of this paper .",
    "nevertheless it is worth asking whether various aspects of the convergence analysis presented above  in particular , the distinction between cd variants  can be observed in practice . to this end , we used these methods to minimize a convex quadratic @xmath302 ( with @xmath303 symmetric and positive semidefinite ) for which @xmath304 and @xmath305 .",
    "we constructed @xmath303 by choosing an integer @xmath109 from @xmath306 and parameters @xmath307 $ ] and @xmath308 , and defining    @xmath309^t.\\end{aligned}\\ ] ]    where @xmath310 is a random matrix with @xmath311 orthogonal columns , @xmath312 is an @xmath313 positive diagonal matrix whose diagonal elements were chosen from a log - uniform distribution to have a specified condition number ( with maximum diagonal of @xmath314 ) , and @xmath315 is the vector @xmath316 . for convenience , we normalized @xmath303 so that its maximum diagonal  and thus @xmath147  is @xmath314 .    by choosing @xmath317 and @xmath318 appropriately ,",
    "we can obtain a range of values for the quantities described in subsection  [ sec : assumptions ] , which enter along with the smallest singular value into the convergence expression .",
    "for example , by setting @xmath319 and @xmath320 we obtain a randomly oriented matrix , possibly singular , with a specified range of nonzero eigenvalues .",
    "nonzero values of @xmath317 and @xmath318 induce different types of orientation bias . in particular , we see that @xmath145 increases toward its upper bound of @xmath321 as @xmath318 increases away from zero .",
    "we tested three cd variants .",
    "* cyclic : cyclic cd , described in subsection  [ sec : cyclic ] .",
    "* iid : randomized cd using sampling with replacement : algorithm  [ alg : rcd ] . * epochs : the `` sampling without replacement '' variant of algorithm  [ alg : rcd ] , described following the proof of theorem  [ th : rcd ] .    for each variant",
    ", we tried both a fixed steplength @xmath322 and the optimal steplength @xmath323 .",
    "thus , there were a total of six algorithmic variants tested .",
    "the starting point @xmath153 was chosen randomly , with all components from the unit normal distribution @xmath324 .",
    "the algorithms were terminated when the objective was reduced by a factor of @xmath325 over its initial value @xmath326 . the speed of convergence varied widely according to the problem construction parameters @xmath317 , @xmath99 , and @xmath327 , but we can make some general observations .",
    "first , on problems that are not well conditioned , the function values @xmath328 decreased rapidly at first , then settled into a _ linear _ rate of decrease .",
    "this linear rate held even for problems in which @xmath303 was singular  a significant improvement over the sublinear rates predicted by the theory .",
    "second , the epochs variant of randomized cd tended to converge faster than the iid version , though rarely more than twice as fast .",
    "third , the use of the optimal step was usually better than the fixed step ( with sometimes up to six times fewer iterations ) , but this was by no means always the case .",
    "fourth , while there were extensive regimes of parameter values in which all six variants performed similarly , there were numerous `` stressed '' settings in which the cyclic variants are much slower than the randomized variants , by factors of @xmath329 or more .",
    "cd methods lend themselves to different kinds of parallel implementation .",
    "even basic algorithm frameworks such as algorithm  [ alg : cd ] may be amenable to application - specific parallelism , when the computations involved in evaluating a single element of the gradient vector are substantial enough to be spread out across cores of a multicore computer .",
    "we concern ourselves here with more generic forms of parallelism , which involve multiple instances of the basic cd algorithm , running in parallel on multiple processors .",
    "we can distinguish different types of parallel cd algorithms .",
    "_ synchronous _ algorithms are those that partition the computation into pieces that can be executed in parallel on multiple processors ( or cores of a multicore machine ) , but that synchronize frequently across all processors , to ensure consistency of the information available to all processors at certain points in time . for example , each processor could update a subset of components of @xmath0 in parallel ( with the subsets being disjoint ) , and the synchronization step could ensure that the results of all updates are shared across all processors before further computation occurs .",
    "the synchronization step often detracts from the performance of algorithms , not only because some processors may be forced to idle while others complete their work , but also because the overheads associated with ( hardware and software ) locking of memory accesses can be high .",
    "thus , _ asynchronous",
    "_ methods , which weaken or eliminate the requirement of consistent information across processors , are preferred in practice .",
    "analysis of such methods is more difficult , but results have been obtained that accord with practical experience of such methods .",
    "indeed , it can be verified that in certain regimes , linear speedup can be expected across a modest number of processors .",
    "we mention several synchronous parallel variants of cd that appear in the recent literature .",
    "we note that in the some of these papers , the computational results were obtained by implementing the methods in an asynchronous fashion , disregarding the synchronization step required by the analysis .",
    "bradley at al .",
    "@xcite consider a bound - constrained problem that is a reformulation of the problem with specific choices of @xmath4 and with @xmath330 .",
    "their algorithm performs short - step updates of individual components of @xmath0 in parallel on @xmath331 processors , with synchronization after each round of parallel updating .",
    "this scheme essentially updates a randomly - chosen block of @xmath331 variables at each cycle . by modifying the analysis of @xcite ,",
    "they show that the @xmath213 sublinear convergence rate bound is not affected provided that @xmath331 is no larger than @xmath332 , where @xmath130 is the lipschitz constant from .",
    "jaggi et al .",
    "@xcite perform a synchronized cd method on the dual erm model for the case of @xmath333 , partitioning components of the dual variable @xmath0 between cores and sharing a copy of the vector @xmath334 across cores , updating this vector at each synchronization point .",
    "the approach can be thought of as a nonlinear block gauss - jacobi method ( by contrast with the coordinate gauss - seidel approaches discussed in section  [ sec : algs ] ) .",
    "richtarik and takac  @xcite describe a method for the separably regularized formulation , in which a subset of indices @xmath335 is updated according to the formula in algorithm  [ alg : cdreg ] .",
    "the work of updating the components in @xmath247 is divided between processors ; essentially , a synchronization step takes place at each iteration .",
    "this scheme is enhanced with an acceleration step in @xcite ; the extra computations associated with the acceleration step too are parallelized , using ideas from @xcite . in the scheme of marecek , richtarik , and takac  @xcite ,",
    "the variable vector @xmath0 is partitioned into subvectors , and each processor is assigned the responsibility for updating one of these subvectors . on each processor ,",
    "the updating scheme described in @xcite is applied , providing a second level of parallelism .",
    "synchronization takes place at each outer iteration .",
    "details of the information - sharing between processors required for accurate computation of gradients in different applications are described in ( * ? ? ?",
    "* section  6 ) .      in asynchronous variants of cd , the variable vector @xmath0 is assumed to be accessible to each processor , available for reading and updating .",
    "( for example , @xmath0 could be stored in the shared - memory space of a multicore computer , where each core is viewed as a processor . )",
    "each processor runs its own cd process , shown here as algorithm  [ alg : pcd.1 ] , without any attempt to coordinate or synchronize with other processors .",
    "each iteration on each processor chooses an index @xmath10 , loads the components of @xmath0 that are needed to compute the gradient component @xmath46_{i}$ ] , then updates the @xmath10th component @xmath336 .",
    "note that this evaluation may need only a small subset of the components of @xmath0 ; this is the case when the hessian @xmath337 is structurally sparse , for example . on some multicore architectures ( for example , the intel xeon ) ,",
    "the update of @xmath336 can be performed as a unitary operation ; no software or hardware locking is required to block access of other cores to the location @xmath336 .",
    "choose index @xmath338 ; evaluate @xmath46_{i}$ ] , reading components of @xmath0 from shared memory as necessary ; update @xmath339_{i}$ ] for some @xmath340 ; termination ;    we can take a global view of the entire parallel process , consisting of multiple processors each executing algorithm  [ alg : pcd.1 ] , by defining a global counter @xmath60 that is incremented whenever _ any _ processor updates an element of @xmath0 : see algorithm  [ alg : pcd.2 ] .",
    "note that the _ only _ difference with the basic framework of algorithm  [ alg : cd ] is in the argument of the gradient component : in algorithm  [ alg : cd ] this is the latest iterate @xmath61 whereas in algorithm  [ alg : pcd.2 ] it is a vector @xmath341 that is generally made up of components of vectors from previous iterations @xmath342 , @xmath343 .",
    "the reason for this discrepancy is that between the time at which a processor _ reads _ the vector @xmath0 from shared storage in order to calculate @xmath46_i$ ] , and the time at which it _ updates _",
    "component @xmath10 , _ other processors _ have generally made changes to @xmath0 . in consequence , each update step is using slightly stale information about @xmath0 . to prove convergence results ,",
    "we need to make assumptions on how much `` staleness '' can be tolerated , and to modify the convergence analysis quite substantially . indeed ,",
    "proofs of convergence even for the most basic asynchronous algorithms are quite technical .",
    "set @xmath54 and choose @xmath55 ; choose index @xmath56 ; @xmath344_{i_k } e_{i_k}$ ] for some @xmath58 ; @xmath59 ; termination test satisfied ;    asynchronous cd algorithms are distinguished from each other mostly by the assumptions they make on the the choice of update components @xmath44 and on the `` ages '' of the components of @xmath341 , that is , the iterations at which each component of this vector was last updated . in the terminology of bertsekas and tsitsiklis  @xcite , the algorithm is _ totally asynchronous _",
    "if    * each index @xmath345 of @xmath0 is updated at infinitely many iterations ; and * if @xmath346 denotes the iteration at which component @xmath347 of the vector @xmath341 was last updated , then @xmath348 as @xmath349 for all @xmath350 .    in other words , each component of @xmath0 is updated infinitely often , and all components used in successive evaluation vectors @xmath341 are also updated infinitely often .",
    "the following convergence result for totally asynchronous variants of algorithm  [ alg : pcd.2 ] is due to bertsekas and tsitsiklis ; see in particular ( * ? ? ?",
    "* sections  6.1 , 6.2 , and 6.3.3 ) .",
    "[ th : async ] suppose that the problem has a unique solution @xmath86 and that @xmath4 is convex and continuously differentiable .",
    "suppose that algorithm  [ alg : pcd.2 ] is implemented in a totally asynchronous fashion .",
    "suppose that the mapping @xmath51 defined by @xmath351 for some @xmath340 ( for which @xmath86 is the unique fixed point ) is strictly contractive in the @xmath98 norm , that is , @xmath352 then if we set @xmath353 in algorithm  [ alg : pcd.2 ] , the sequence @xmath120 converges to @xmath86 .",
    "we can not expect to obtain a convergence rate in this setting ( such as sublinear with rate @xmath213 ) , given that the assumptions on the ages of the components in @xmath341 are so weak .",
    "although this result can be generalized impressively and its proof is not too complex , we should note that the @xmath98 contraction assumption is quite strong .",
    "it is violated even by some strictly convex objectives @xmath4 .",
    "for example , when @xmath354 with @xmath355,\\ ] ] we have @xmath4 strictly convex with minimizer @xmath304",
    ". however the mapping @xmath356 is not contractive for any @xmath340 ; we have for example that @xmath357 when @xmath358 .",
    "we turn now to _ partly asynchronous",
    "_ variants of algorithm  [ alg : pcd.2 ] , in which we make stronger assumptions on the ages of the components of @xmath341 .",
    "liu and wright  @xcite consider a version of algorithm  [ alg : pcd.2 ] that is the parallel analog of algorithm  [ alg : rcd ] , in that each update component @xmath44 is chosen independently and randomly with equal probability from @xmath359 .",
    "they assume that no component of @xmath341 is older than a nonnegative integer @xmath360  the `` maximum delay ''  for any @xmath60 .",
    "specifically , they express the difference between @xmath61 and @xmath341 in terms of `` missed updates '' to @xmath0 , as follows : @xmath361 where @xmath362 is a set of iteration numbers drawn from the set @xmath363 .",
    "the value of @xmath360 is related to the number of processors @xmath331 involved in the computation .",
    "if all processors are performing their updates at approximately the same rates , we could expect @xmath360 to be a modest multiple of @xmath331  perhaps @xmath364 or @xmath365 , to allow a safety margin for occasional delays .",
    "hence the value of @xmath360 is an indicator of potential parallelism in the algorithm .    in @xcite , the steplengths in algorithm  [ alg : pcd.2 ]",
    "are fixed as follows : @xmath366 where @xmath367 is chosen to ensure that algorithm  [ alg : pcd.2 ] progresses steadily toward a solution , but not too rapidly .",
    "too - rapid convergence would cause the information in @xmath341 to become too stale too quickly , so the gradient component @xmath368_{i_k}$ ] would lose its relevance as a suitable update for the variable component @xmath369 at iteration @xmath60 .",
    "steady convergence is enforced by choosing some @xmath370 and requiring that @xmath371 where @xmath372 is the vector that would hypothetically be obtained if we were to apply the the update to _ all _ components , that is , @xmath373 and the expectations @xmath156 are taken over all random variables @xmath374 .",
    "condition ensures that the `` expected squared update norms '' decrease by at most a factor of @xmath375 at each iteration .",
    "the main results in @xcite apply to composite functions , , but for simplicity here we state the result in terms of the problem , where @xmath4 is convex and continuously differentiable , with nonempty solution set @xmath151 and optimal objective value @xmath150 .",
    "we use @xmath376 to denote projection onto @xmath151 , and recall the definition of the ratio @xmath145 between different varieties of lipschitz constants .",
    "the results also make use of an _ optimal strong convexity _ condition , which is that the following inequality holds for some @xmath121 : @xmath377    the following result is a modification of ( * ? ? ?",
    "* corollary  2 ) .",
    "[ th : ji ] suppose that assumption  [ ass : fconv ] holds , and that @xmath378 then by setting @xmath379 in ( that is , choosing steplengths @xmath380 ) , we have that @xmath381 assuming in addition that is satisfied for some @xmath121 , we obtain the following linear rate : @xmath382    a comparison with theorem  [ th : rcd ] , which shows convergence rates for serial randomized cd ( algorithm  [ alg : rcd ] ) shows a striking similarity in convergence bounds .",
    "the factor - of-@xmath149 difference in steplength between the serial and parallel variants accounts for most of the difference between the linear rates and , while there is an extra term @xmath187 in the denominator of the sublinear rate .",
    "we conclude that we do not pay q high overhead ( in terms of total workload ) for parallel implementation , and hence that near - linear speedup can be expected .",
    "( indeed , computational results in @xcite and @xcite observe near - linear speedup for multicore asynchronous implementations . )",
    "these encouraging conclusions depend critically on the condition , which is an upper bound on the allowable delay @xmath360 in terms of @xmath187 and the ratio @xmath145 from . for functions @xmath4 with weak coupling between the components of @xmath0 ( for example",
    ", when off - diagonals in the hessian @xmath142 are small relative to the diagonals ) , we have @xmath145 not much greater than @xmath314 , so the maximum delay can be of the order of @xmath383 before there is any attenuation of linear speedup . when stronger coupling exists , the restriction on @xmath360 may be quite tight , possibly not much greater than @xmath314 . a more general convergence result ( * ? ? ?",
    "* theorem  1 ) shows that in this case , we can choose smaller values of @xmath367 in , allowing graceful degradation of the convergence bounds while still obtaining fairly efficient parallel implementations .",
    "we note that an earlier analysis in @xcite made a stronger assumption on @xmath341  that it is equal to some earlier iterate @xmath342 of algorithm  [ alg : pcd.2 ] , where @xmath384 , that is , the earlier iterate is no more than @xmath360 cycles old .",
    "( a similar assumption was used to analyze convergence of as asynchronous stochastic gradient algorithm in @xcite . )",
    "this stronger assumption yields stronger convergence results , in that the bound on @xmath360 in can be loosened .",
    "however , the assumption may not always hold , since some parts of @xmath0 in memory may be altered by some cores as they are being read by another core , a phenomenon referred to in @xcite as `` inconsistent reading . ''",
    "we have surveyed the state of the art in convergence of coordinate descent methods , with a focus on the most elementary settings and the most fundamental algorithms .",
    "the recent literature contains many extensions , enhancements , and elaborations ; we refer interested readers to the bibliography of this paper , and note that new works are appearing at a rapid pace .",
    "coordinate descent method have become an important tool in the optimization toolbox that is used to solve problems that arise in machine learning and data analysis , particularly in `` big data '' settings .",
    "we expect to see further developments and extensions , further customization of the approach to specific problem structures , further adaptation to various computer platforms , and novel combinations with other optimization tools to produce effective `` solutions '' for key application areas .",
    "i thank ji liu for the pleasure of collaborating with him on this topic over the past two years .",
    "i am grateful to the editors and referees of the paper , whose expert and constructive comments led to numerous improvements .",
    "attouch , h. , bolte , j. , redont , p. , soubeyran , a. : proximal alternating minimization and projection methods for nonconvex problems : an approach based on the kurdyka - lojasiewicz inequality .",
    "mathematics of operations research * 35*(2 ) , 438457 ( 2010 )                boyd , s. , parikh , n. , chu , e. , peleato , b. , eckstein , j. : distributed optimization and statistical learning via the alternating direction methods of multipliers . foundations and trends in machine learning * 3*(1 ) , 1122 ( 2011 )    bradley , j.k . , kyrola , a. , bickson , d. , guestrin , c. : parallel coordunate descent for @xmath11-regularized loss minimization . in : proceedings of the 28 international conference on machine learning ( icml 2011 ) ( 2011 )                        jaggi , m. , smith , v. , takc , m. , terhorst , j. , krishnan , s. , hoffman , t. , jordan , m.i . :",
    "communication - efficient distributed dual coordinate ascent .",
    "advances in neural information processing systems * 27 * ( 2014 )        lee , y.t . , sidford , a. : efficient accelerated coordinate descent methods and faster algorihtms for solving linear systems . in : 54th annual symposium on foundations of computer science , pp . 147156 ( 2013 )",
    "liu , h. , palatucci , m. , zhang , j. : lockwise coordinate descent procedures for the multi - task lasso , with applications to neural semantic basis discovery . in : proceedings of the 26th annual international conference on machine learning , icml 09 , pp .",
    "acm , new york , ny , usa ( 2009 )    liu , j. , wright , s.j . :",
    "asynchronous stochastic coordinate descent : parallelism and convergence properties . technical report arxiv:1403.3862 , university of wisconsin , madison ( 2014 ) . to appear in _ siam journal on optimization _",
    "liu , j. , wright , s.j . ,",
    "r , c. , bittorf , v. , sridhar , s. : an asynchronous parallel stochastic coordinate descent algorithm .",
    "arxiv:1311.1873 , computer sciences department , university of wisconsin - madison ( 2013 ) . to appear in _ journal of machine learning research _    liu , j. , wright , s.j . , sridhar , s. : an accelerated randomized kaczmarz algorithm .",
    "technical report arxiv 1310.2887 , computer sciences department , university of wisconsin - madison ( 2013 ) . to appear in",
    "_ mathematics of computation _                            platt , j.c . : fast training of support vector machines using sequential minimal optimization . in : b.  schlkopf ,",
    "burges , a.j .",
    "smola ( eds . ) advances in kernel methods  support vector learning , pp .",
    "mit press , cambridge , ma ( 1999 )                                ye , j.c . ,",
    "webb , k.j .",
    ", bouman , c.a . ,",
    "millane , r.p . : optical diffusion tomography by iterative - coordinate - descent optimization in a bayesian framework .",
    "journal of the optical society of america a * 16*(10 ) , 24002412 ( 1999 )"
  ],
  "abstract_text": [
    "<S> coordinate descent algorithms solve optimization problems by successively performing approximate minimization along coordinate directions or coordinate hyperplanes . </S>",
    "<S> they have been used in applications for many years , and their popularity continues to grow because of their usefulness in data analysis , machine learning , and other areas of current interest . </S>",
    "<S> this paper describes the fundamentals of the coordinate descent approach , together with variants and extensions and their convergence properties , mostly with reference to convex objectives . </S>",
    "<S> we pay particular attention to a certain problem structure that arises frequently in machine learning applications , showing that efficient implementations of accelerated coordinate descent algorithms are possible for problems of this type . </S>",
    "<S> we also present some parallel variants and discuss their convergence properties under several models of parallel execution .    </S>",
    "<S> example.eps gsave newpath 20 20 moveto 20 220 lineto 220 220 lineto 220 20 lineto closepath 2 setlinewidth gsave .4 setgray fill grestore stroke grestore </S>"
  ]
}