{
  "article_text": [
    "the information theory realm , it is well - known that given the second - order moment ( or variance ) , a gaussian density function maximizes the differential entropy .",
    "similarly , given the second - order moment , the gaussian density function minimizes the fisher information , a result which is referred to as the cramr - rao inequality in the signal processing literature .",
    "surprisingly , the proofs proposed in the literature for these fundamental results are relatively quite diverse , and no unifying feature exists . since differential entropy or fisher information is a functional with respect to a probability density function , the most natural way to establish these results is by approaching them from the perspective of functional analysis .",
    "this paper presents a unifying variational framework to address these results as well as numerous other fundamental information theoretic results .",
    "a challenging information theoretic inequality , referred to as the extremal entropy inequality ( eei ) @xcite , can be dealt with successfully in the proposed framework of functionals .",
    "furthermore , the proposed variational calculus perspective is useful in establishing other novel results , applications and extensions of the existing information theoretic inequalities .",
    "the main theme of this paper is to illustrate how some tools from calculus of variations can be used successfully to prove some of the fundamental information theoretic inequalities , which have been widely used in information theory and other fields , and to establish some applications .",
    "the proposed variational approach provides alternative proofs for some of the fundamental information theoretic inequalities and enables finding novel extensions of the existing results .",
    "this statement is strengthened by the fact that the proposed variational framework is quite general and powerful , and it allows easy integration of various linear and inequality constraints into the functional that is to be optimized .",
    "therefore , we believe that a large number of applications could benefit of these tools .",
    "the proposed variational approach offers also a potential guideline for finding the optimal solution for many open problems .",
    "variational calculus techniques have been used with great success in solving important problems in image processing and computer vision @xcite such as image reconstruction ( denoising , deblurring ) , inverse problems , and image segmentation . recently",
    ", variational techniques were also advocated for optimization of multiuser communication systems @xcite , for deriving analytical wireless channel models using the maximum entropy principle when only limited information about the environment is available @xcite , and for designing optimal training sequences for radar and sonar applications @xcite-@xcite .",
    "maximum entropy principle found also applications in spectral estimation ( e.g. , burg s maximum entropy spectral density estimator @xcite ) and bayesian statistics @xcite .",
    "the major results of this paper are enumerated as follows .",
    "first , using calculus of variations , the maximizing differential entropy and minimizing fisher information theorems are proved under the classical ( standard ) assumptions found in the literature as well as under a different set of assumptions .",
    "it is shown that a gaussian density function maximizes the differential entropy but it minimizes the fisher information , given the second - order moment .",
    "it is also shown that a half normal density function maximizes the differential entropy over the set of non - negative random variables , given the second - order moment .",
    "furthermore , it is shown that a half normal density function minimizes the fisher information over the set of non - negative random variables , provided that the regularity condition and [ ef_thm6 ] . ]",
    "is ignored and the second - order moment is given .",
    "it is also shown that a chi density function minimizes the fisher information over the set of non - negative random variables , under the assumption that the regularity condition holds and the second - order moment is given .",
    "second , a novel proof of the worst additive noise lemma @xcite is provided in the proposed functional framework .",
    "previous proofs of the worst additive noise lemma were based on jensen s inequality or data processing inequality @xcite , @xcite . unlike the previous proofs , our approach is purely based on calculus of variations techniques , and the vector version of the lemma is treated .",
    "third , eei is studied from the perspective of a functional problem .",
    "the main advantage of the proposed new proof is that neither the channel enhancement technique and the entropy power inequality ( epi ) , adopted in @xcite , nor the equality condition of data processing inequality and the technique based on the moment generating functions , used in @xcite , are required . using a technique based on calculus of variations , an alternative proof of eei",
    "is provided .",
    "finally , several applications and extensions of the proposed results are discussed .",
    "the rest of this paper is organized as follows .",
    "some variational calculus preliminary results and their corollaries are first reviewed in section [ cal ] . maximizing differential entropy theorem and minimizing fisher information theorem ( cramr - rao inequality )",
    "are proved in section [ ef ] . in section [ wa ] ,",
    "the worst additive noise lemma is introduced and proved based on variational arguments .",
    "eei is proved in section [ ei ] . in section [ app ] ,",
    "some additional applications of the proposed variational techniques are briefly mentioned .",
    "finally , section [ con ] concludes this paper .",
    "in this section , we will review some of the fundamental results from variational calculus , and establish the concepts , notations and results that will be used constantly throughout the rest of the paper .",
    "these results are standard and therefore will be described briefly without further details .",
    "additional details can be found in calculus of variations books such as @xcite-@xcite .",
    "[ cal_def1 ] a functional @xmath0 $ ] might be defined as @xmath1 = \\int_{a}^{b } k(x , f(x ) , f'(x ) ) dx,\\end{aligned}\\ ] ] which is defined on the set of continuous functions @xmath2 with continuous first - order derivatives @xmath3 on the interval @xmath4 $ ] .",
    "the function @xmath5 is assumed to satisfy the boundary conditions @xmath6 and @xmath7 . the functional @xmath8 is also assumed to have continuous first - order and second - order ( partial ) derivatives with respect to ( wrt ) all of its arguments .    [ cal_def2 ] the increment of a functional @xmath0 $ ] is defined as @xmath9 = u[f + t ] - u[f],\\end{aligned}\\ ] ] where the function @xmath10 , that satisfies the boundary conditions @xmath11 , represents the admissible increment of @xmath5 , and it is assumed independent of the function @xmath5 and twice differentiable .    [ cal_def3 ] suppose that given @xmath5 , the increment in ( [ cal_eq2_1 ] ) is expressed as @xmath12 = \\varphi\\left[t\\right ] + \\epsilon \\|t\\|,\\end{aligned}\\ ] ] where @xmath13 $ ] is a linear functional , @xmath14 goes to zero as @xmath15 approaches zero , and @xmath16 denotes a norm defined in the case of a function @xmath5 as : @xmath17 where @xmath18 are assumed to exist and be continuous for @xmath19 on the interval @xmath4 $ ] , and the summation upper index @xmath20 might vary depending on the normed linear space considered ( e.g. , if the normed linear space consists of all continuous functions @xmath21 , which have continuous first - order derivative on the interval @xmath22 $ ] , @xmath23 , and in this case @xmath24 ; see e.g. , @xcite for further details ) . under the above assumptions , the functional @xmath25 $ ] is said to be differentiable , and the major part of the increment @xmath13 $ ] is called the ( first - order ) variation of the functional @xmath25 $ ] and it is expressed as @xmath26 $ ] .",
    "based on definitions [ cal_def1 ] , [ cal_def2 ] , [ cal_def3 ] and taylor s theorem ( see e.g. , @xcite-@xcite for additional justifications ) , the first - order and the second - order variations of a functional @xmath25 $ ] can be expressed as @xmath27 \\hspace*{-3 mm } & = & \\hspace*{-4mm}\\int \\hspace*{-1 mm } \\left[k'_{f } \\left(x , f , f ' \\right ) t ( x)+ k'_{f ' } \\left(x , f , f ' \\right ) t ' ( x)\\right ] dx\\\\ \\label{cal_eq5_2 } \\delta^2 u\\left[f \\right ] \\hspace*{-3 mm } & = & \\hspace*{-3mm}\\frac{1}{2 } \\int \\big[k''_{f   f } \\left(x , f , f ' \\right ) { t ( x)}^2 + 2 k''_{f   f ' } \\left(x , f , f ' \\right ) t ( x ) t ' ( x ) + k''_{f '   f ' } \\left(x , f , f ' \\right){t ' ( x)}^{2 } \\big ] dx\\nonumber\\\\ \\hspace*{-3 mm } & = & \\hspace*{-3mm}\\frac{1}{2 } \\int \\left[k''_{f '   f ' } { t ' } ^{2 }   + \\left ( k''_{f   f }   - \\frac{d}{dx } k''_{f   f ' } \\right ) { t } ^2 \\right ] dx,\\end{aligned}\\ ] ] where @xmath28 and @xmath29 stand for the first - order partial derivatives wrt @xmath30 and @xmath31 , respectively , @xmath32 denotes the second - order partial derivative wrt @xmath30 and @xmath31 , @xmath33 represents the second - order partial derivative wrt @xmath30 , and @xmath34 is the second - order partial derivative wrt @xmath31 . throughout the paper to simplify the exposition , the arguments of functionals or functions are omitted unless the arguments are ambiguous or confusing . also , the range of integration in various integrals will not be explicitly marked unless the range is ambiguous .",
    "[ cal_thm1 ] a necessary condition for the functional @xmath35 $ ] in ( [ cal_eq1_1 ] ) to have an extremum ( or local optimum ) for a given function @xmath36 is that its first variation vanishes at @xmath36 : @xmath37 = 0,\\end{aligned}\\ ] ] for all admissible increments.this implies @xmath38 a result which is known as euler s equation .",
    "when the functional in ( [ cal_eq1_1 ] ) includes multiple functions ( e.g. , @xmath39 ) and multiple integrals wrt @xmath40 , i.e. , @xmath41 then euler s equation in ( [ cal_eq7_1 ] ) takes the form of the system of equations : @xmath42 in particular , when the functional does not depend on the first - order derivative of the functions @xmath39 , the equations in ( [ cal_eq7_4 ] ) reduce to @xmath43    details of the proof of this theorem can be found , e.g. , in @xcite-@xcite .",
    "[ cal_thm2 ] a necessary condition for the functional @xmath0 $ ] in ( [ cal_eq1_1 ] ) to have a minimum for a given @xmath36 is that the second variation of functional @xmath0 $ ] be nonnegative : @xmath44 \\geq 0,\\end{aligned}\\ ] ] for all admissible increments .",
    "this implies @xmath45 in particular , when the functional in ( [ cal_eq1_1 ] ) does not depend on the first - order derivative of the function @xmath46 , ( [ cal_eq8_3 ] ) simplifies to @xmath47 when the functional in ( [ cal_eq1_1 ] ) includes multiple functions ( e.g. , @xmath39 ) and multiple integrals wrt @xmath40 ,",
    "i.e. , @xmath48 then the condition in ( [ cal_eq8_5 ] ) is expressed in terms of the positive semi - definiteness of the matrix : @xmath49   \\succeq   0 .\\end{aligned}\\ ] ]    the inequality in ( [ cal_eq8_5 ] ) is easily derived from the inequality in ( [ cal_eq8_3 ] ) since @xmath50 and @xmath51 are vanishing in ( [ cal_eq5_2 ] ) when the functional in ( [ cal_eq1_1 ] ) does not depend on the first - order derivative of the function @xmath52 .",
    "the remaining details of the proof can be tracked in @xcite .",
    "[ cal_thm3 ] given the functional @xmath53 = \\int_{a}^{b } k(x , f_{1 } , f_{2},f'_{1 } , f'_{2 } ) dx,\\end{aligned}\\ ] ] assume that the admissible functions satisfy the following boundary conditions : @xmath54 = \\int_{a}^{b } \\tilde{l}(x , f_{1 } , f_{2 } , f'_{1 } , f'_{2 } ) dx = l,\\end{aligned}\\ ] ] where @xmath55 , @xmath56 , @xmath57 , @xmath58 , @xmath59 , @xmath60 , and @xmath61 are constants , @xmath62 is a functional wrt @xmath63 and @xmath64 , and @xmath65 $ ] is assumed to have an extremum for @xmath66 and @xmath67 .",
    "if @xmath68 and @xmath69 are not extremals of @xmath70 $ ] , or @xmath71 and @xmath72 do not vanish simultaneously at any point in ( [ cal_eq10_2 ] ) , there exist a constant @xmath73 and a function @xmath74 such that @xmath68 and @xmath69 are extremals of the functional @xmath75    based on theorem [ cal_thm3 ] , the following corollary is derived .",
    "[ cal_cor1 ] given the functional @xmath76 = \\int_{a}^{b } \\int_{a}^{b } k(x , y , \\fx , \\fy ) dxdy,\\end{aligned}\\ ] ] assume that the admissible functions satisfy the following boundary conditions : @xmath77 = \\int_{a}^{b } \\int_{a}^{b } \\tilde{l}(x , y,\\fx , \\fy ) dx dy= l ,    \\label{cal_eq13_1}\\end{aligned}\\ ] ] where @xmath55 , @xmath56 , @xmath78 , @xmath79 , @xmath80 , and @xmath81 stand for some constants , @xmath82 is a function of @xmath83 , @xmath84 is a function of @xmath85 , @xmath86 is a function of @xmath87 , and @xmath88 is a function of @xmath89 .",
    "the functional @xmath90 $ ] is assumed to have an extremum at @xmath91 and @xmath92 .    unless @xmath93 and @xmath94 are extremals of @xmath95 $ ] , or @xmath96 and @xmath97 simultaneously vanish at any point of @xmath98 , there exist a constant @xmath73 and a function @xmath99 such that @xmath100 and @xmath92 is an extremal of the functional @xmath101 + \\lambda(y)g(y,\\fy ) \\big\\}dy . \\label{cal_eq15_1}\\end{aligned}\\ ] ]    this corollary is a simple extension of theorem [ cal_thm3 ] for multiple integrals .",
    "therefore , the detailed proof is omitted .",
    "based on theorems [ cal_thm1 ] , [ cal_thm2 ] and corollary [ cal_cor1 ] , we can derive the following corollary , which will be repeatedly used throughout this paper .",
    "[ cal_cor2 ] based on the functional defined in ( [ cal_eq15_1 ] ) , the following necessary conditions are derived for the optimal solutions @xmath93 and @xmath94 : @xmath102 and the matrix @xmath103,\\end{aligned}\\ ] ] is positive semi - definite .",
    "the functional @xmath104 is defined as @xmath105 and @xmath106 is a ( arbitrary but fixed ) function which satisfies @xmath107 , and it is introduced to homogenize the functional in ( [ cal_eq15_1 ] ) .",
    "the equations in ( [ cal_eq16_1 ] ) and ( [ cal_eq16_2 ] ) are derived from the first - order variation condition in theorem [ cal_thm1 ] .",
    "the equations in ( [ cal_eq16_1 ] ) and ( [ cal_eq16_2 ] ) are euler s equations for multiple integrals .",
    "the positive semi - definiteness of the matrix in ( [ cal_eq17_1 ] ) is derived from the second - order variation condition in theorem [ cal_thm2 ] .",
    "this condition is the same as the one in ( [ cal_eq8_7 ] ) .",
    "the details of the proof are omitted here .",
    "this simple but significant result  given the second - order moment ( or variance ) of a random vector , a gaussian random vector maximizes the differential entropy  is well - known . in this section , a completely rigorous and general derivation of the distribution achieving the maximum entropy will be first provided .",
    "this proof sets up the variational framework for establishing a second important result in this section , namely the cramr - rao bound , which states that for a given mean and correlation matrix , a normally distributed random vector minimizes the fisher information matrix .",
    "[ ef_thm2 ] given ( a mean vector @xmath108 and ) a correlation matrix @xmath109 , a gaussian random vector @xmath110 with the correlation matrix @xmath109 ( and the mean vector @xmath108 ) maximizes the differential entropy , i.e. , @xmath111 where @xmath112 denotes differential entropy , @xmath113 is an arbitrary ( but fixed ) random vector with the correlation matrix @xmath109 .",
    "we first construct a functional , which represents the inequality in ( [ ef_eq8_1 ] ) and required constraints , as follows : @xmath114 using theorem [ cal_thm3 ] , the functional in ( [ ef_eq30_1 ] ) is expressed as @xmath115,\\end{aligned}\\ ] ] where @xmath116=$ ] @xmath117 @xmath118 @xmath119 , @xmath120 is the lagrange multiplier associated with the constraint ( [ ef_eq30_2 ] ) , and @xmath121 stand for the lagrange multipliers corresponding to the constraints ( [ ef_eq30_3 ] ) .",
    "based on theorem [ cal_thm1 ] , by checking the first - order variation condition , we can find the optimal solution @xmath122 as follows : @xmath123 with the matrix @xmath124 $ ] , @xmath125 . considering the constraints in ( [ ef_eq30_2 ] ) and ( [ ef_eq30_3 ] ) , from ( [ ef_eq32_1 ] ) it turns out that @xmath126 where @xmath127    two remarks are now in order .",
    "first , the correlation matrix @xmath109 is assumed to be invertible .",
    "when the correlation matrix is non - invertible , similar to the method shown in @xcite , we can equivalently re - write the functional problem in ( [ ef_eq30_1 ] ) as @xmath128 where @xmath129 , and in the spectral factorization @xmath130 , @xmath131 is an orthogonal matrix , @xmath132 @xmath133 and @xmath134 denotes a diagonal matrix .    let @xmath135^{\\scriptscriptstyle t}$ ] , where the dimensions of @xmath136 and @xmath137 are @xmath138 and @xmath139 , respectively . since the correlation matrix ( or covariance matrix ) of @xmath140 , @xmath141 , is equal to the diagonal matrix @xmath142 , @xmath136 and @xmath137 are statistically independent .",
    "therefore , @xmath143 .",
    "since the correlation of @xmath144 is a zero matrix , we can consider @xmath144 as a deterministic vector , and we only need to consider @xmath145 .",
    "therefore , the equation in ( [ ef_add_eq1 ] ) and constraints in ( [ ef_eq30_2])-([ef_eq30_3 ] ) are equivalently re - written as @xmath146 where @xmath147 is a positive - definite matrix .",
    "therefore , without loss of generality , we assume that the correlation matrix @xmath109 is invertible .",
    "second , if an additional constraint , related to the mean vector of @xmath113 , @xmath108 , is enforced , the optimal solution is a multi - variate gaussian density function with mean @xmath108 , instead of the multi - variate gaussian density function with zero mean ( [ ef_eq33_1 ] ) .",
    "based on theorem [ cal_thm2 ] , since @xmath148 the second - order variation @xmath149 $ ] is positive , and the optimal solution @xmath150 is a minimal solution for the variational problem in ( [ ef_eq30_1 ] ) .",
    "therefore , the negative of differential entropy @xmath151 is minimized ( or equivalently @xmath152 is maximized ) when @xmath113 is a multi - variate gaussian random vector with zero mean and covariance matrix @xmath153 .",
    "even though theorems [ cal_thm1 ] , [ cal_thm2 ] are necessary conditions for the minimum , in this case , a multi - variate gaussian density function is the actual solution since there is only one solution , namely the multi - variate gaussian density function , in the feasible set .",
    "an alternative justification of global optimality of multi - variate gaussian pdf can be achieved by exploiting the convexity of @xmath154 wrt @xmath89 .",
    "the proposed proof is different from the ones mentioned in @xcite , @xcite in the sense that the proposed proof relies only on variational calculus techniques , and it turns out that the constraints related to the first - order moment or non - singularity of correlation matrix are not necessary .    the maximum entropy result can be extended in various ways .",
    "a simple variation consists in considering only non - negative random variables .",
    "then it turns out that a gaussian random variable is not anymore the solution which maximizes the differential entropy . the following theorem can be easily established and states that a half - normal random variable maximizes the differential entropy over the set of non - negative random variables .",
    "[ ef_thm3 ] within the class of non - negative random variables with given second - order moment @xmath155 , a half - normal random variable @xmath156 maximizes the differential entropy , i.e. , @xmath157 where @xmath158 is an arbitrary ( but fixed ) non - negative random variable with the second - order moment @xmath155 , and @xmath112 denotes differential entropy .    the proof is omitted since it can be established following similar steps to the proof of theorem [ ef_thm2 ] .    adopting a similar variational approach to the one in theorem [ ef_thm2 ]",
    ", we can also determine the probability density function that minimizes the fisher information matrix as shown by the following theorem .",
    "[ ef_thm5 ] given a mean vector @xmath108 and a correlation matrix @xmath109 , the gaussian density function with the mean vector @xmath108 and the correlation matrix @xmath109 minimizes the fisher information matrix , i.e. , @xmath159 where @xmath113 and @xmath110 stand for an arbitrary ( but fixed ) random vector and gaussian random vector , respectively , with given mean @xmath108 and correlation matrix @xmath109 , and @xmath160 denotes the fisher information matrix : @xmath161,\\end{aligned}\\ ] ] @xmath162 the following regularity condition is assumed to be satisfied : @xmath163 where @xmath164 denotes the gradient , and the covariance matrix is supposed to be non - singular .",
    "we first represent the inequality in ( [ ef_eq25_1 ] ) as a functional with the required constraints as follows : @xmath165 where @xmath166 is an arbitrary but fixed non - zero vector , and it is defined as @xmath167^{\\scriptscriptstyle t}$ ] .",
    "using theorem [ cal_thm3 ] , the functional problem in ( [ ef_eq36_1 ] ) is expressed as @xmath168,\\end{aligned}\\ ] ] where @xmath116= \\int k(\\mathbf{x},f_{\\scriptscriptstyle x } , \\nabla f_{\\scriptscriptstyle x } ) d\\mathbf{x}$ ] , @xmath169 , and @xmath120 , @xmath170 , and @xmath121 are the lagrange multipliers corresponding to the three constraints in ( [ ef_eq36_2 ] ) .",
    "based on theorem [ cal_thm1 ] , by confirming the first - order variation condition , i.e. , @xmath171=0 $ ] , we can find the optimal solution @xmath172 as follows : @xmath173 where @xmath174    therefore , the left - hand side of the equation in ( [ ef_eq38_1 ] ) is expressed as @xmath175    unlike theorem [ ef_thm2 ] , we can not directly calculate @xmath122 from ( [ ef_eq38_1 ] ) .",
    "fortunately , the first two parts in equation ( [ ef_eq40_1 ] ) are expressed as quadratic forms when @xmath122 is a multi - variate gaussian density function , and therefore , the multi - variate gaussian density function satisfies the equality in ( [ ef_eq40_2 ] ) .",
    "when @xmath122 is a multi - variate gaussian density function : @xmath176 with @xmath177 and @xmath178,\\end{aligned}\\ ] ] its partial derivatives can be expressed as follows : @xmath179 by substituting ( [ ef_eq43_2 ] ) into ( [ ef_eq40_1 ] ) , it turns out that @xmath180\\nonumber\\\\ % & &   + \\sum_{i=1}^n \\sum_{j=1}^n \\left(\\sigma_{\\scriptscriptstyle x_{ij}}^2 + \\sigma_{\\scriptscriptstyle x_{ji}}^2\\right ) \\xi_i \\xi_j   + \\alpha + \\sum_{i=1}^n \\zeta_i x_i + \\sum_{i=1}^n \\sum_{j=1}^n",
    "\\lambda_{ij}x_i x_j\\nonumber\\\\ % & = & \\sum_{l=1}^n \\sum_{m=1}^n \\left[\\left(x_l-\\mu_{\\scriptscriptstyle x_l}\\right)\\left(x_m - \\mu_{\\scriptscriptstyle x_m}\\right)\\boldsymbol{\\xi}^{\\scriptscriptstyle t}\\boldsymbol{\\sigma}_{\\scriptscriptstyle x_{lm } } \\boldsymbol{\\xi } \\right]+ \\sum_{i=1}^n \\sum_{j=1}^n \\left(\\sigma_{\\scriptscriptstyle x_{ij}}^2 + \\sigma_{\\scriptscriptstyle x_{ji}}^2\\right ) \\xi_i",
    "\\xi_j \\nonumber\\\\ % & & + \\alpha + \\sum_{i=1}^n \\zeta_i x_i + \\sum_{i=1}^n \\sum_{j=1}^n \\lambda_{ij}x_i x_j\\nonumber\\\\ \\hspace*{-2 mm } & = & \\hspace*{-2 mm } \\sum_{l=1}^n \\sum_{m=1}^n \\omega_{lm}\\left(x_l-\\mu_{\\scriptscriptstyle x_l}\\right)\\left(x_m-\\mu_{\\scriptscriptstyle x_m}\\right ) + \\sum_{i=1}^n \\sum_{j=1}^n \\left(\\sigma_{\\scriptscriptstyle x_{ij}}^2\\sigma_{\\scriptscriptstyle x_{ji}}^2\\right ) \\xi_i \\xi_j+\\alpha + \\sum_{i=1}^n",
    "\\zeta_i x_i + \\sum_{i=1}^n \\sum_{j=1}^n   \\lambda_{ij}x_i x_j\\nonumber\\\\ % & = & \\left(\\mathbf{x } - \\boldsymbol{\\mu}_{\\scriptscriptstyle x}\\right)^{\\scriptscriptstyle t } \\boldsymbol{\\omega } \\left(\\mathbf{x}- \\boldsymbol{\\mu}_{\\scriptscriptstyle x}\\right)+ \\boldsymbol{\\xi}^{\\scriptscriptstyle t } \\boldsymbol{\\psi }",
    "\\boldsymbol{\\xi } + \\alpha + \\boldsymbol{\\zeta}^{\\scriptscriptstyle t } \\mathbf{x } + \\mathbf{x}^{\\scriptscriptstyle t}\\boldsymbol{\\lambda } \\mathbf{x}\\nonumber\\\\ \\hspace*{-2 mm } & = & \\hspace*{-2 mm } \\mathbf{x}^{\\scriptscriptstyle t } \\boldsymbol{\\omega } \\mathbf{x } + \\mathbf{x}^{\\scriptscriptstyle t}\\boldsymbol{\\lambda } \\mathbf{x}+\\boldsymbol{\\zeta}^{\\scriptscriptstyle t } \\mathbf{x}- 2\\boldsymbol{\\mu}_{\\scriptscriptstyle x}^{\\scriptscriptstyle t } \\boldsymbol{\\omega } \\mathbf{x}+ \\boldsymbol{\\mu}_{\\scriptscriptstyle x}^{\\scriptscriptstyle t } \\boldsymbol{\\omega } \\boldsymbol{\\mu}_{\\scriptscriptstyle x } + \\boldsymbol{\\xi}^{\\scriptscriptstyle t } \\boldsymbol{\\psi } \\boldsymbol{\\xi } + \\alpha   \\nonumber \\\\",
    "\\hspace*{-2 mm } & = & \\hspace*{-2 mm } 0,\\end{aligned}\\ ] ] where @xmath181 , \\ ; \\boldsymbol{\\lambda } \\hspace*{-1mm}=\\hspace*{-1mm}\\left[\\begin{array}{ccc } \\lambda_{11 } & \\cdots & \\lambda_{1n}\\\\ \\vdots & \\ddots & \\vdots \\\\ \\lambda_{n1 } & \\cdots & \\lambda_{nn } \\end{array}\\right ] , \\nonumber\\\\ \\boldsymbol{\\psi } \\hspace*{-4mm}&=&\\hspace*{-4 mm } \\left[\\begin{array}{ccc } \\psi_{11 } & \\cdots & \\psi_{1n}\\\\ \\vdots & \\ddots & \\vdots \\\\ \\psi_{n1 } & \\cdots & \\psi_{nn } \\end{array}\\right ] , \\ ; \\boldsymbol{\\omega } \\hspace*{-1mm}=\\hspace*{-1 mm } \\left[\\begin{array}{ccc } \\omega_{11 } & \\cdots & \\omega_{1n}\\\\ \\vdots & \\ddots & \\vdots \\\\ \\omega_{n1 } & \\cdots & \\omega_{nn } \\end{array}\\right]\\nonumber\\\\ \\sigma_{\\scriptscriptstyle x_{ij}}^{\\scriptscriptstyle lm } \\hspace*{-3 mm } & = & \\hspace*{-3 mm } \\frac{1}{4 } \\left(\\sigma_{\\scriptscriptstyle x_{il}}^2+\\sigma_{\\scriptscriptstyle x_{li}}^2\\right)\\left(\\sigma_{\\scriptscriptstyle x_{jm}}^2+\\sigma_{\\scriptscriptstyle x_{mj}}^2\\right)\\nonumber\\\\ \\hspace*{-3 mm } & = & \\hspace*{-3 mm } \\sigma_{\\scriptscriptstyle x_{li}}^2 \\sigma_{\\scriptscriptstyle x_{jm}}^2,\\ ; i , j=1,\\ldots , n,\\quad l , m=1,\\ldots , n \\nonumber\\\\ \\psi_{ij } \\hspace*{-3 mm } & = & \\hspace*{-3 mm } 2\\sigma_{\\scriptscriptstyle x_{ij}}^2 , \\ ; i , j=1,\\ldots , n \\nonumber\\\\ \\omega_{lm } \\hspace*{-3 mm } & = & \\hspace*{-3 mm } \\boldsymbol{\\xi}^{\\scriptscriptstyle t}\\boldsymbol{\\sigma}_{\\scriptscriptstyle x_{lm } } \\boldsymbol{\\xi } , \\ ;   l , m=1,\\ldots , n.\\end{aligned}\\ ] ] therefore , the lagrange multipliers @xmath120 and @xmath121 must be selected as @xmath182 since the second - order variation is positive : @xmath183 based on theorem [ cal_thm2 ] , the gaussian distribution @xmath122 is necessary optimal for the variational problem in ( [ ef_eq36_1 ] ) .",
    "even though theorems [ cal_thm1 ] and [ cal_thm2 ] are necessary conditions for the minimum , in this case , the multi - variate gaussian density function is sufficiently the global minimum solution since this is a convex optimization problem ( the objective function is strictly convex and its constraint set is convex ) .    using similar variational arguments",
    ", one can show that a half - normal and a chi density function minimize the fisher information over the set of non - negative random variables as shown by the following two theorems .",
    "[ ef_thm6 ] within the class of non - negative continuous random variables with fixed second - order moment @xmath155 , the fisher information is minimized by a half - normal random variable @xmath156 : @xmath184 where @xmath158 is an arbitrary ( but fixed ) non - negative random variable with the second - order moment @xmath155 , and @xmath185 denotes the fisher information .",
    "theorem [ ef_thm6 ] does not assume the following regularity condition : @xmath186 for the fisher information .",
    "the following result establishes the counterpart of theorem [ ef_thm6 ] for the class of non - negative random variables with fixed second order moment and whose distribution satisfies the regularity condition in ( [ ef_eq28_1_1 ] ) .",
    "[ ef_thm7 ] within the class of non - negative continuous random variables @xmath158 with fixed second - order moment and whose distributions satisfy the regularity condition in ( [ ef_eq28_1_1 ] ) , the fisher information is minimized by a chi - distributed random variable @xmath187 : @xmath188 where @xmath185 stands for the fisher information .    unlike the proof in @xcite , by considering the first - order and the second - order moments instead of variance",
    ", we construct a variational problem and address the problem using the first - order and second - order necessary conditions , as well as the convexity property of the problem .",
    "the details of the proof are omitted because of the similar steps to those encountered in the proof of theorem [ ef_thm5 ] .",
    "worst additive noise lemma was introduced and exploited in several references @xcite , @xcite , @xcite , and it has been widely used in numerous other applications .",
    "one of the main applications of the worst additive noise lemma pertains to the capacity calculation of a wireless communication channel subject to different constraints such as gaussian mimo broadcasting , gaussian mimo wire - tap , etc . in this section , the worst additive noise lemma for random vectors",
    "will be proved solely based on variational arguments .",
    "[ wa_thm2 ] assume @xmath113 is an arbitrary but fixed random vector and @xmath110 is a gaussian random vector , whose correlation matrix is identical to that of @xmath113 , denoted as @xmath109 .",
    "given a gaussian random vector @xmath189 , assumed independent of both @xmath113 and @xmath110 and with the correlation matrix @xmath190 , then the following relation holds : @xmath191    our novel proof is entirely anchored in the variational calculus framework .",
    "a summary of our proof runs as follows .",
    "first , we construct a variational problem , which represents the inequality in ( [ wa_eq14_1 ] ) and required constraints in a functional form .",
    "second , using the first - order variation condition , we find necessary optimal solutions , which satisfy euler s equation .",
    "third , using the second - order variation condition , we show that the optimal solutions are necessarily local minima .",
    "finally , we justify that the local minimum is also global .    by setting @xmath192 , where @xmath113 and @xmath189 are independent of each other , in ( [ wa_eq14_1 ] )",
    ", the mutual information @xmath193 can be expressed as @xmath194 then , we consider the functional : @xmath195 the density function @xmath196 and conditional density function @xmath197 are expressed as @xmath198 respectively . therefore , by substituting @xmath196 for @xmath199 and @xmath200 for @xmath197 , respectively , and appropriately changing the constrains in ( [ wa_eq15_2 ] ) , the variational problem in ( [ wa_eq15_1 ] ) is expressed as @xmath201 d\\mathbf{x}d\\mathbf{y }      \\\\ \\label{wa_eq17_3_1 } \\hspace*{-4mm}&&\\hspace*{-4mm}\\text{s",
    ". t. } \\int \\hspace*{-2 mm } \\int \\hspace*{-2 mm } f_{\\scriptscriptstyle x}(\\mathbf{x})f_{\\scriptscriptstyle w}(\\mathbf{y}-\\mathbf{x } ) d\\mathbf{x } d\\mathbf{y } = 1,\\\\ \\label{wa_eq17_3_2 } \\hspace*{-3 mm } & & \\hspace*{+2mm}\\int \\hspace*{-2 mm } \\int \\hspace*{-1mm}\\mathbf{x } f_{\\scriptscriptstyle x}(\\mathbf{x})f_{\\scriptscriptstyle w}(\\mathbf{y}-\\mathbf{x } ) d\\mathbf{x } d\\mathbf{y } = \\boldsymbol{\\mu}_{\\scriptscriptstyle x},\\\\ \\label{wa_eq17_3_3 } \\hspace*{-3 mm } & & \\hspace*{+2mm}\\int   \\hspace*{-2 mm } \\int \\hspace*{-1 mm } \\mathbf{x}\\mathbf{x}^{\\scriptscriptstyle t } f_{\\scriptscriptstyle x}(\\mathbf{x})f_{\\scriptscriptstyle w}(\\mathbf{y}-\\mathbf{x } ) d\\mathbf{x } d\\mathbf{y } = \\boldsymbol{\\omega}_{\\scriptscriptstyle x},\\\\ \\label{wa_eq17_3_4 } \\hspace*{-3 mm } & & \\hspace*{+2 mm } \\int f_{\\scriptscriptstyle y}(\\mathbf{y } ) d\\mathbf{y } = 1,\\\\ \\label{wa_eq17_3_5 } \\hspace*{-3 mm } & & \\hspace*{+2 mm } \\int \\mathbf{y } f_{\\scriptscriptstyle y}(\\mathbf{y } ) d\\mathbf{y } = \\boldsymbol{\\mu}_{\\scriptscriptstyle y},\\\\ \\label{wa_eq17_3_6 } \\hspace*{-3 mm } & & \\hspace*{+2mm}\\int \\mathbf{y}\\mathbf{y}^{\\scriptscriptstyle t } f_{\\scriptscriptstyle y}(\\mathbf{x } ) d\\mathbf{y } = \\boldsymbol{\\omega}_{\\scriptscriptstyle y},\\\\ \\label{wa_eq17_3_7 } \\hspace*{-3 mm } & & \\hspace*{+2mm}f_{\\scriptscriptstyle y}(\\mathbf{y } ) = \\int f_{\\scriptscriptstyle x}(\\mathbf{x})f_{\\scriptscriptstyle w}(\\mathbf{y}-\\mathbf{x})d\\mathbf{x}.\\end{aligned}\\ ] ]    the functional problem in ( [ wa_eq17_1 ] ) can be re - cast into the following equivalent form : @xmath202 d\\mathbf{x})\\nonumber\\\\ \\hspace*{-4mm}&+&\\hspace*{-4 mm } f_{\\scriptscriptstyle y}(\\mathbf{y})\\left[\\alpha_1 + \\sum_{i=1}^n \\eta_{i } y_i + \\sum_{i=1}^n \\sum_{j=1}^n \\theta_{ij } y_i y_j + \\lambda(\\mathbf{y})\\right]d\\mathbf{y},\\end{aligned}\\ ] ] where @xmath203 $ ] , @xmath204 $ ] , and @xmath205 , @xmath206 , @xmath207 , @xmath208 , @xmath209 , @xmath210 , and @xmath211 stand for the lagrange multipliers corresponding to the constraints ( [ wa_eq17_3_1 ] ) , ( [ wa_eq17_3_2 ] ) , ( [ wa_eq17_3_3 ] ) , ( [ wa_eq17_3_4 ] ) , ( [ wa_eq17_3_5 ] ) , ( [ wa_eq17_3_6 ] ) , and ( [ wa_eq17_3_7 ] ) , respectively .",
    "define now the functional @xmath212 as @xmath213 \\hspace*{-2 mm } & = & \\hspace*{-2 mm } \\int \\left(\\int k(\\mathbf{x},\\mathbf{y},f_{\\scriptscriptstyle x},f_{\\scriptscriptstyle y } ) d\\mathbf{x}\\right ) + \\tilde{k}(\\mathbf{y},f_{\\scriptscriptstyle y } ) d\\mathbf{y},\\nonumber\\end{aligned}\\ ] ] where @xmath214,\\nonumber\\\\ \\hspace*{-4mm}&&\\hspace*{-4mm}\\tilde{k}(\\mathbf{y},f_{\\scriptscriptstyle y})=f_{\\scriptscriptstyle y}(\\mathbf{y})[\\alpha_1+\\hspace*{-1mm}\\sum_{i=1}^n \\hspace*{-1mm}\\eta_{i } y_i+\\hspace*{-1mm}\\sum_{i=1}^n \\sum_{j=1}^n \\hspace*{-1mm}\\theta_{ij}y_i y_j+\\lambda(\\mathbf{y } ) ]   .\\end{aligned}\\ ] ] based on the first - order variation condition , we can find the optimal solution @xmath150 and @xmath215 as follows : @xmath216 where @xmath217 , \\ ; \\boldsymbol{\\theta } = \\left[\\begin{array}{ccc } \\theta_{11 }   & \\cdots & \\theta_{1n}\\\\ \\vdots & \\ddots & \\vdots\\\\ \\theta_{n1 } & \\cdots & \\theta_{nn } \\end{array}\\right]\\end{aligned}\\ ] ] @xmath218^{\\scriptscriptstyle t}$ ] and @xmath219^{\\scriptscriptstyle t}$ ] .",
    "the following relationships satisfy the necessary conditions ( [ wa_eq21_2 ] ) and ( [ wa_eq21_3 ] ) : @xmath220 and hence : @xmath221 considering the constraints in ( [ wa_eq17_3_1])-([wa_eq17_3_7 ] ) , @xmath122 and @xmath222 in ( [ wa_eq24_1 ] ) are expressed as @xmath223 where @xmath224 , @xmath225 , and @xmath226 is the covariance matrix of @xmath189 .",
    "based on the equations in ( [ wa_eq25_1 ] ) , it turns out that @xmath227 therefore , @xmath150 and @xmath215 are multi - variate gaussian density functions ( without loss of generality , and we can assume that the covariance matrix @xmath153 is invertible due to the arguments mentioned in appendix [ non ] ) .",
    "now , by confirming the second - order variation condition , we will show that the optimal solutions @xmath150 and @xmath215 are necessarily local minima . using theorem [ cal_thm2 ]",
    ", we will show that the following matrix is positive semi - definite : @xmath228 \\bigg|_{\\fx=\\fxstar,\\fy=\\fystar } \\succeq { \\bf   0}.\\end{aligned}\\ ] ] since the elements of the matrix in ( [ wa_eq27_1 ] ) are defined as @xmath229 the matrix is a positive semi - definite matrix , and therefore @xmath230 . because of the convexity of functional @xmath231 wrt variables @xmath52 and @xmath232 , the optimal solutions @xmath150 and @xmath215 actually globally minimize the variational functional in ( [ wa_eq17_1 ] ) . even though these optimal solutions are necessarily optimal",
    ", there exists only one solution , which is the multi - variate gaussian density function and it satisfies euler s equation in ( [ wa_eq21_2 ] ) and ( [ wa_eq21_3 ] ) .",
    "therefore , @xmath150 and @xmath215 are also sufficient in this case .",
    "an alternative more detailed proof of the fact that @xmath150 and @xmath215 represent global optimal solutions is to show that @xmath233 \\geq u[\\fxstar,\\fystar]$ ] , where @xmath234 denote any arbitrary functions satisfying the boundary conditions and the constraints .",
    "first , the following functionals are defined : @xmath235 ,   \\\\",
    "f_0(\\x,\\y,\\fx ) & = & \\fx(\\x)\\fw(\\y-\\x ) ,   \\\\",
    "f_1(\\x,\\y,\\fx ) & = & x_i \\fx(\\x)\\fw(\\y-\\x ) ,   \\\\",
    "f_2(\\x,\\y,\\fx ) & = & x_i x_j \\fx(\\x)\\fw(\\y-\\x),\\end{aligned}\\ ] ] and thus @xmath236 can be expressed as @xmath237 since the hessian matrix of @xmath236 wrt @xmath89 and @xmath87 is given by @xmath238,\\ ] ] which is positive semi - definite , @xmath236 is convex wrt @xmath89 and @xmath87 , and the following inequality holds @xmath239\\big|_{\\fx=\\fxstar,\\fy=\\fystar},\\ ] ] due to the fact that the convex function lies above its tangents .",
    "therefore , it follows that @xmath240 - u[\\fxstar,\\fystar ] \\hspace{-3 mm } & = & \\hspace{-3 mm } \\iint \\hspace{-2 mm } f(\\x,\\y,\\fxhat,\\fyhat ) - f(\\x,\\y,\\fxstar,\\fystar ) d\\x d\\y   \\nonumber \\\\",
    "\\hspace{-3 mm } & = & \\hspace{-3 mm } \\iint \\hspace{-2 mm } f(\\x,\\y,\\fxhat,\\fyhat ) - f(\\x,\\y,\\fxstar,\\fystar ) d\\x d\\y + \\alpha_0 \\iint \\hspace{-2 mm } f_0(\\x,\\y,\\fxhat ) - f_0(\\x,\\y,\\fxstar ) d\\x d\\y \\nonumber",
    "\\\\ \\hspace{-3 mm } & + & \\hspace{-3 mm } \\sum\\limits_{i=1}^n \\hspace{-1mm}\\zeta_i \\iint \\hspace{-2 mm } f_1(\\x,\\y,\\fxhat ) - f_1(\\x,\\y,\\fxstar ) d\\x d\\y + \\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^n \\hspace{-1mm}\\gamma_{ij } \\iint \\hspace{-2 mm } f_2(\\x,\\y,\\fxhat ) - f_2(\\x,\\y,\\fxstar ) d\\x d\\y \\nonumber",
    "\\\\ \\hspace{-3 mm } & + & \\hspace{-3 mm } \\alpha_1   \\int \\hspace*{-1 mm } \\fyhat(\\y ) - \\fystar(\\y ) d\\y \\hspace*{-.1 cm } + \\hspace*{-.1 cm } \\sum\\limits_{i=1}^n \\eta_i \\hspace*{-1 mm } \\int \\hspace{-2 mm } y_i ( \\fyhat(\\y ) - \\fystar(\\y ) ) d\\y + \\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^n \\hspace{-2 mm } \\theta_{ij } \\int \\hspace{-2 mm } y_i y_j ( \\fyhat(\\y ) - \\fystar(\\y ) ) d\\y   \\nonumber \\\\ \\hspace{-3 mm } & + & \\hspace{-3 mm } \\int \\hspace{-2 mm } \\lambda(\\y ) \\bigg [ \\fyhat(\\y ) - \\int \\hspace{-2 mm } \\fxhat(\\x)\\fw(\\y-\\x ) d\\x \\bigg ] d\\y - \\int \\hspace{-2 mm } \\lambda(\\y ) \\bigg [ \\fystar(\\y ) - \\int \\fxstar(\\x)\\fw(\\y-\\x ) d\\x \\bigg ] d\\y \\nonumber \\\\ \\hspace{-3 mm } & = & \\hspace{-3 mm } \\iint \\hspace{-1 mm } k(\\x,\\y,\\fxhat,\\fyhat ) - k(\\x,\\y,\\fxstar,\\fystar ) d\\x d\\y + \\int \\hspace{-1 mm } ( \\fyhat - \\fystar ) ( \\alpha_1 + \\bm\\eta^{\\subt}\\y + \\y^{\\subt}\\bm\\theta\\y + \\lambda(\\y ) ) d\\y   \\nonumber \\\\\\end{aligned}\\ ] ]    based on ( [ eq : wanl - tangent ] ) , the righthand side of ( [ eq : wanl - suff ] ) can be lower bounded as follows : @xmath241 - u[\\fxstar,\\fystar ] & \\\\ \\geq & \\hspace*{-.1 cm } \\iint \\hspace*{-.1 cm } \\left [ ( \\fxhat \\hspace*{-.1 cm } - \\hspace*{-.1 cm } \\fxstar ) k'_{\\fx } \\hspace*{-.12 cm } + \\hspace*{-.1 cm } ( \\fyhat \\hspace*{-.1 cm } - \\hspace*{-.1 cm } \\fystar ) k'_{\\fy } \\right]\\big|_{\\fx=\\fxstar,\\fy=\\fystar } \\hspace*{-.15 cm } d\\x d\\y \\\\ & + \\int ( \\fyhat - \\fystar ) ( \\alpha_1 + \\bm\\eta^{\\subt}\\y + \\y^{\\subt}\\bm\\theta\\y + \\lambda(\\y ) ) d\\y \\\\ \\overset{(a)}{= } & \\int ( \\fxhat-\\fxstar ) \\left [ \\int k'_{\\fx } \\big|_{\\fx=\\fxstar,\\fy=\\fystar } d\\y \\right ] d\\x \\\\ & + \\int ( \\fyhat - \\fystar ) \\left [ \\int k'_{\\fy } d\\x + \\tilde{k}'_{\\fy }   \\right ] \\big|_{\\fx=\\fxstar,\\fy=\\fystar } d\\y \\\\ \\overset{(b)}{= } & 0 , \\end{split}\\ ] ] where ( a ) follows from the fact that @xmath242 and ( b ) is due to ( [ wa_eq21_2 ] ) and ( [ wa_eq21_3 ] ) .",
    "this proves the sufficiency of the gaussian distributions , and therefore , @xmath150 and @xmath215 minimize the variational problem .",
    "the constraints related to the vector means in ( [ wa_eq17_3_2 ] ) and ( [ wa_eq17_3_5 ] ) are unnecessary . without these constraints , the optimal solutions are still multi - variate gaussian density functions but the vector means are equal to zero .",
    "extremal entropy inequality , proposed by liu and viswanath @xcite , was motivated by multi - terminal information theoretic problems such as the vector gaussian broadcast channel and the distributed source coding with a single quadratic distortion constraint .",
    "eei is an entropy power inequality which includes a covariance constraint . because of the covariance constraint",
    ", the extremal entropy inequality could not be proved directly by using the classical entropy power inequality ( epi ) .",
    "therefore , new techniques ( @xcite , @xcite ) were adopted in the proofs reported in @xcite , @xcite . in this section",
    ", the extremal entropy inequality will be proved using a variational approach .",
    "[ ei_thm2 ] assume that @xmath243 is an arbitrary but fixed constant and @xmath244 is a positive semi - definite matrix . a gaussian random vector @xmath189 with positive definite covariance matrix @xmath226",
    "is assumed to be independent of an arbitrary random vector @xmath113 whose covariance matrix @xmath153 satisfies @xmath245 .",
    "then , there exists a gaussian random vector @xmath246 with covariance matrix @xmath247 which satisfies the following inequality : @xmath248 where @xmath249 .    by setting @xmath250 , we first consider the following variational problem ( without loss of generality , we assume that @xmath113 , @xmath251 , and @xmath252 have zero mean ) : @xmath253 where @xmath254 is a constant , and @xmath255 stands for the covariance matrix of the optimal solution @xmath252 .",
    "in addition , the term @xmath256 is added to the objective functional ( [ ei_eq20_1 ] ) , and being a constant , it does not affect the optimization problem . without loss of generality , the matrix @xmath244 is assumed to be a positive definite matrix due to the same arguments mentioned in @xcite .",
    "the optimization problem ( [ ei_eq20_1 ] ) can be better addressed by re - casting it as follows : @xmath257 d\\mathbf{x } d\\mathbf{y}\\\\ \\label{ei_eq21_5_1 } \\hspace*{-4mm}&&\\hspace*{-4mm}\\text{s.t.}\\ ; \\int \\hspace*{-2 mm } \\int f_{\\scriptscriptstyle x}(\\mathbf{x } ) f_{\\scriptscriptstyle w}(\\mathbf{y}-\\mathbf{x})d\\mathbf{x}d\\mathbf{y } = 1,\\\\ \\label{ei_eq21_5_2 } \\hspace*{-4mm}&&\\hspace*{-1 mm } \\int \\hspace*{-2 mm } \\int \\left(y_i y_j -x_i x_j - \\left(y - x\\right)_i \\left(y - x\\right)_j \\right ) f_{\\scriptscriptstyle x}(\\mathbf{x } ) f_{\\scriptscriptstyle w}(\\mathbf{y}-\\mathbf{x})d\\mathbf{x } d\\mathbf{y } = 0,\\\\ \\label{ei_eq21_5_3 } \\hspace*{-4mm}&&\\hspace*{-1mm}\\sum_{i=1}^{n}\\sum_{j=1}^n\\left ( \\int\\int x_i x_j \\xi_i \\xi_j f_{\\scriptscriptstyle x}(\\mathbf{x } ) f_{\\scriptscriptstyle w}(\\mathbf{y}-\\mathbf{x})d\\mathbf{x}d\\mathbf{y } \\right )   \\leq   \\sum_{i=1}^{n}\\sum_{j=1}^n\\sigma^2_{ij } \\xi_i \\xi_j,\\\\ \\label{ei_eq21_5_4 } \\hspace*{-4mm}&&\\hspace*{-1 mm } \\int \\hspace*{-2mm}\\int y_iy_j f_{\\scriptscriptstyle x}(\\mathbf{x } ) f_{\\scriptscriptstyle w}(\\mathbf{y}-\\mathbf{x})d\\mathbf{x}d\\mathbf{y } = { \\sigma}^2_{\\scriptscriptstyle y^*_{ij } } , \\\\",
    "\\label{ei_eq21_5_5 } \\hspace*{-4mm}&&\\hspace*{-1 mm } -\\int \\hspace*{-2mm}\\int f_{\\scriptscriptstyle x}(\\mathbf{x } ) f_{\\scriptscriptstyle w}(\\mathbf{y}-\\mathbf{x } ) \\log f_{\\scriptscriptstyle x}(\\mathbf{x } ) d\\mathbf{x } d\\mathbf{y } \\geq   p_{\\scriptscriptstyle x},\\\\ \\label{ei_eq21_5_6 } \\hspace*{-4mm}&&\\hspace*{-1 mm } f_{\\scriptscriptstyle y}(\\mathbf{y } ) = \\int f_{\\scriptscriptstyle x}(\\mathbf{x } ) f_{\\scriptscriptstyle w}(\\mathbf{y}-\\mathbf{x})d\\mathbf{x},\\end{aligned}\\ ] ] where the arbitrary deterministic non - zero vector @xmath166 is defined as @xmath258^t$ ] , @xmath259 denotes the @xmath260 row and @xmath261 column entry of @xmath255 , @xmath262 , and @xmath263 .    using lagrange multipliers , as shown in corollary [ cal_cor1 ]",
    ", the functional problem in ( [ ei_eq21_1 ] ) and the constraints in ( [ ei_eq21_5_1])-([ei_eq21_5_6 ] ) can be expressed in terms of the lagrangian : @xmath264 where @xmath265,\\nonumber\\\\ \\tilde{k}(\\mathbf{y},f_{\\scriptscriptstyle y } ) \\hspace*{-2 mm } & = & \\hspace*{-2 mm } \\lambda(\\mathbf{y})f_{\\scriptscriptstyle y}(\\mathbf{y}).\\end{aligned}\\ ] ] the lagrange multipliers @xmath205 , @xmath207 , @xmath266 , @xmath267 , @xmath208 , and @xmath211 correspond to the constraints in ( [ ei_eq21_5_1 ] ) , ( [ ei_eq21_5_2 ] ) , ( [ ei_eq21_5_3 ] ) , ( [ ei_eq21_5_4 ] ) , ( [ ei_eq21_5_5 ] ) , and ( [ ei_eq21_5_6 ] ) , respectively .    to find the optimal solutions , based on corollary [ cal_cor2 ] , the first - order variation condition is checked as follows : @xmath268 d\\y \\nonumber \\\\ \\hspace*{-2 mm } & = & \\hspace*{-2 mm }   0.\\\\ \\label{ei_eq24_2 } \\hspace*{-2mm}&&\\hspace*{-2 mm } \\int k'_{f_{\\scriptscriptstyle y } } d\\x + \\tilde{k}'_{\\fy } \\big|_{f_{\\scriptscriptstyle x}=f_{\\scriptscriptstyle x^ * } , f_{\\scriptscriptstyle y}=f_{\\scriptscriptstyle y^ * } } =   -\\frac{\\mu \\int f_{\\scriptscriptstyle x}(\\mathbf{x})f_{\\scriptscriptstyle w}(\\mathbf{y}-\\mathbf{x } ) d\\mathbf{x}}{f_{\\scriptscriptstyle y}(\\mathbf{y } ) } + \\lambda(\\mathbf{y } ) =   0.\\end{aligned}\\ ] ] the following expressions satisfy the equalities in ( [ ei_eq24_1 ] ) and ( [ ei_eq24_2 ] ) : @xmath269 where @xmath270 ,   \\quad \\boldsymbol{\\gamma }   =   \\left[\\begin{array}{ccc } \\gamma_{11 } & \\cdots & \\gamma_{1n}\\\\ \\vdots & \\ddots & \\vdots \\\\ \\gamma_{n1 } & \\cdots & \\gamma_{nn } \\end{array}\\right ] \\nonumber \\\\ \\boldsymbol{\\xi } \\hspace*{-3 mm } & = & \\hspace*{-3 mm }   \\left[\\begin{array}{ccc } \\xi_1 \\xi_1 & \\cdots & \\xi_1 \\xi_n\\\\ \\vdots & \\ddots & \\vdots \\\\ \\xi_n \\xi_1 & \\cdots & \\xi_n \\xi_n \\end{array}\\right ] , \\nonumber\\\\ \\mathbf{x } \\hspace*{-3 mm } & = & \\hspace*{-3 mm } \\left[x_1 , \\cdots , x_n\\right]^t,\\nonumber\\\\ \\mathbf{y } \\hspace*{-3 mm } & = & \\hspace*{-3 mm } \\left[y_1 , \\cdots , y_n\\right]^t.\\nonumber % \\theta \\hspace*{-3 mm } & \\geq & \\hspace*{-3 mm } 0.\\end{aligned}\\ ] ] now considering the constraints in ( [ ei_eq21_5_1])-([ei_eq21_5_6 ] ) ,",
    "the equations in ( [ ei_eq25_3 ] ) are further processed as follows : @xmath271 where @xmath272 the inequality in ( [ ei_eq28_6 ] ) is due to the second - order variation condition , which will be presented later in this proof .",
    "the inequality ( [ ei_eq28_x ] ) is based on the theory of kkt conditions since the multiplier associated with the inequality constraint is nonnegative .",
    "moreover , the complementary slackness condition in the kkt conditions leads to the following relationship : @xmath273 = 0.\\ ] ]    based on theorem [ cal_thm2 ] , to make the second variation nonnegative , the positive semi - definiteness of the following matrix is required : @xmath274,\\end{aligned}\\ ] ] which further reduces to the following condition : @xmath275 \\left[\\begin{array}{cc } k''_{f_{\\scriptscriptstyle x^ * } f_{\\scriptscriptstyle x^ * } } &   k''_{f_{\\scriptscriptstyle x^ * } f_{\\scriptscriptstyle y^*}}\\\\ k''_{f_{\\scriptscriptstyle y^ * } f_{\\scriptscriptstyle x^ * } } &   k''_{f_{\\scriptscriptstyle y^ * } f_{\\scriptscriptstyle y^ * } } \\end{array}\\right ] \\left[\\begin{array}{c } h_{\\scriptscriptstyle x } \\\\",
    "h_{\\scriptscriptstyle y}\\end{array } \\right ] \\nonumber\\\\ \\hspace*{-4mm}&=&\\hspace*{-4mm}k''_{f_{\\scriptscriptstyle x^ * } f_{\\scriptscriptstyle x^ * } } h_{\\scriptscriptstyle x}^2 + k''_{f_{\\scriptscriptstyle y^ * } f_{\\scriptscriptstyle y^ * } } h_{\\scriptscriptstyle y}^2 +   ( k''_{f_{\\scriptscriptstyle x^ * } f_{\\scriptscriptstyle y^*}}+k''_{f_{\\scriptscriptstyle y^ * } f_{\\scriptscriptstyle x^ * } } ) h_{\\scriptscriptstyle y } h_{\\scriptscriptstyle x}\\nonumber\\\\ \\hspace*{-4mm}&\\geq & \\hspace*{-2 mm } 0,\\end{aligned}\\ ] ] where @xmath276 and @xmath277 are arbitrary admissible functions .",
    "since @xmath278 , @xmath279 , @xmath280 , and @xmath281 are defined as @xmath282 the condition in ( [ ei_eq35_1 ] ) requires @xmath283 which holds true if @xmath284 ( i.e. , @xmath285 ) .",
    "condition @xmath286 is also imposed by the kkt complementary slackness condition corresponding to the constraint ( [ ei_eq21_5_5 ] ) .",
    "therefore , the optimal solutions @xmath150 and @xmath215 minimize the functional problem in ( [ ei_eq21_1 ] ) , and the proof is completed because of convexity of the functional @xmath231 wrt variables @xmath52 and @xmath232 .    a more detailed alternative justification of the fact the gaussian distributions @xmath150 and @xmath215 are global minima is next presented .",
    "we will prove the sufficiency of the gaussian distributions by showing @xmath233 \\geq u[\\fxstar,\\fystar]$ ] , where @xmath287 $ ] represents the objective functional in the problem and @xmath288 denote any arbitrary functions satisfying the boundary conditions and the constraints .",
    "first , the following functionals are defined : @xmath289 and thus @xmath290 it can be verified that the hessian matrix of @xmath236 w.r.t @xmath89 and @xmath87 is given by @xmath291,\\ ] ] which is positive semi - definite due to ( [ ei_eq28_6 ] ) .",
    "the convexity property of @xmath236 yields that @xmath292 \\big|_{\\fx=\\fxstar,\\fy=\\fystar},\\ ] ] and it follows that @xmath293 \\hspace{-1mm}- \\hspace{-1mm}u[\\fxstar,\\fystar ] \\hspace*{-4 mm } & = & \\hspace*{-4 mm } \\iint \\hspace*{-1mm}f(\\x,\\y,\\fxhat,\\fyhat ) - f(\\x,\\y,\\fxstar,\\fystar ) d\\x d\\y \\nonumber \\\\",
    "\\hspace*{-4 mm } & \\overset{(a)}{\\geq } & \\hspace*{-4 mm } \\iint \\hspace*{-1mm}f(\\x,\\y,\\fxhat,\\fyhat ) - f(\\x,\\y,\\fxstar,\\fystar ) d\\x d\\y   + \\alpha_0 \\left [ \\iint \\hspace*{-1 mm } f_0(\\x,\\y,\\fxhat ) - f_0(\\x,\\y,\\fxstar ) d\\x d\\y \\right ] \\nonumber \\\\ \\hspace*{-4 mm } & + & \\hspace*{-4 mm } \\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^n \\gamma_{ij } \\left [ \\iint \\hspace*{-1 mm } f_1(\\x,\\y,\\fxhat ) - f_1(\\x,\\y,\\fxstar ) d\\x d\\y \\right ]   + \\theta \\left [ \\iint \\hspace*{-1 mm } f_2(\\x,\\y,\\fxhat ) - f_2(\\x,\\y,\\fxstar ) d\\x d\\y \\right ] \\nonumber \\\\ \\hspace*{-4 mm } & + & \\hspace*{-4 mm } \\sum\\limits_{i=1}^n \\sum\\limits_{j=1}^n \\phi_{ij } \\left [ \\iint \\hspace*{-1 mm } f_3(\\x,\\y,\\fxhat ) - f_3(\\x,\\y,\\fxstar ) d\\x d\\y \\right ] + \\alpha_1 \\hspace*{-.1 cm } \\left [ \\iint \\hspace*{-1 mm } f_4(\\x,\\y,\\fxhat ) - f_4(\\x,\\y,\\fxstar ) d\\x d\\y   \\right ] \\nonumber \\\\ \\hspace*{-4 mm } & + & \\hspace*{-4 mm } \\int \\hspace*{-1 mm } \\lambda(\\y ) \\left [ \\fyhat(\\y ) - \\int \\hspace*{-1 mm } \\fxhat(\\x)\\fw(\\y-\\x ) d\\x \\right ] d\\y - \\int \\hspace*{-1 mm } \\lambda(\\y ) \\left [ \\fystar(\\y ) - \\int \\hspace*{-1 mm } \\fxstar(\\x)\\fw(\\y-\\x ) d\\x \\right ] d\\y \\\\ \\hspace*{-4 mm } & = & \\hspace*{-4 mm } \\iint \\hspace*{-1 mm } k(\\x,\\y,\\fxhat,\\fyhat ) - k(\\x,\\y,\\fxstar,\\fystar ) d\\x d\\y   + \\int \\hspace*{-1 mm } \\lambda(\\y ) \\left ( \\fyhat(\\y ) - \\fystar(\\y ) \\right ) d\\y \\nonumber",
    "\\\\ \\hspace*{-4 mm } & \\overset{(b)}{\\geq } & \\hspace*{-4 mm } \\iint \\hspace*{-1 mm } \\left [ ( \\fxhat - \\fxstar ) k'_{\\fx }   +   ( \\fyhat \\hspace*{-.1cm}-\\hspace*{-.1 cm } \\fystar ) k'_{\\fy } \\right ] \\big|_{\\fx=\\fxstar,\\fy=\\fystar }   d\\x d\\y   + \\int \\hspace*{-1 mm } \\lambda(\\y ) \\left ( \\fyhat(\\y ) - \\fystar(\\y ) \\right ) d\\y \\nonumber \\\\",
    "\\hspace*{-4 mm } & = & \\hspace*{-4 mm } \\int \\hspace*{-1 mm } ( \\fxhat-\\fxstar ) \\left[\\int \\hspace*{-1 mm } k'_{\\fx}\\big|_{\\fx=\\fxstar,\\fy=\\fystar } d\\y \\right ] d\\x   + \\int ( \\fyhat-\\fystar ) \\left [ \\int \\hspace*{-1 mm } k'_{\\fy}\\big|_{\\fx=\\fxstar,\\fy=\\fystar } d\\x + \\lambda(\\y ) \\right ] d\\y \\nonumber \\\\ \\hspace*{-4 mm } & \\overset{(c)}{= } & \\hspace*{-4 mm } 0,\\end{aligned}\\ ] ] where the inequality ( a ) follows from the complementary slackness condition in the kkt conditions ( [ eq : eei - vec - comple ] ) .",
    "indeed , since @xmath294 only represents an arbitrary feasible solution and @xmath295 , it follows that @xmath296 = 0,\\ ] ] and @xmath297 \\leq 0,\\ ] ] and therefore , @xmath298",
    "\\leq 0.$ ] similarly , the complementary slackness condition associated with ( [ ei_eq21_5_5 ] ) leads to @xmath299\\leq 0 $ ] .",
    "in addition , ( b ) is due to ( [ eq : eei - vec - tan ] ) , and ( c ) follows from ( [ ei_eq24_1 ] ) and ( [ ei_eq24_2 ] ) .",
    "this proves the sufficiency of gaussian distributions .",
    "the proposed proof only exploits calculus of variations tools . unlike the previous proofs",
    ", this proof does not adopt neither the channel enhancement technique and epi as in @xcite nor the epi and data processing inequality as in @xcite .",
    "[ ei_thm3 ] assume that @xmath243 is an arbitrary but fixed constant and @xmath244 is a positive semi - definite matrix .",
    "independent gaussian random vectors @xmath189 with covariance matrix @xmath226 and @xmath300 with covariance matrix @xmath301 are assumed to be independent of an arbitrary random vector @xmath113 with covariance matrix @xmath245 .",
    "both covariance matrices @xmath226 and @xmath301 are assumed to be positive definite .",
    "then , there exists a gaussian random vector @xmath246 with covariance matrix @xmath302 which satisfies the following inequality : @xmath303 where @xmath304",
    ".    see appendix [ ei_thm3_proof ] .",
    "the proposed proof does not borrow any techniques from @xcite . even though the proposed proof adopts the equality condition for the data processing inequality , a result which was also exploited in @xcite ,",
    "the proposed proof is different from the one in @xcite due to the following features .",
    "first , the proposed proof uses the equality condition of the data processing inequality only once while the proof in @xcite uses it twice . the proof in @xcite exploited the channel enhancement technique twice , which is equivalent to using the equality condition in the data processing inequality .",
    "second , the proposed proof does not use the moment generating function technique unlike the proof proposed in @xcite ; instead the current proof directly exploits a property of the conditional mutual information pertaining to a markov chain .",
    "because of the easiness to incorporate a broad class of constraints , the proposed variational framework finds applicability in a large number of applications",
    ". herein section , we will briefly illustrate some potential applications in this regard and state several open research problems which might be also addressed within the considered functional framework .",
    "the secrecy capacity of gaussian wire - tap channel has been studied by many researchers @xcite , @xcite .",
    "we will approach the gaussian wire - tap problem from the estimation viewpoint , rather than considering the secrecy capacity from an information theoretic perspective .",
    "the following scalar gaussian wire - tap channel is considered : @xmath305 where @xmath158 is an arbitrary but fixed random variable with zero mean and unit variance , @xmath55 is a constant , and @xmath306 and @xmath307 are gaussian random variables with variances @xmath308 and @xmath309 , respectively .",
    "the random variables @xmath306 and @xmath307 are independent of each other , and they have zero mean . in the channel model ( [ wire_eq1_1 ] ) ,",
    "@xmath310 and @xmath311 are considered as a legitimate receiver and as an eavesdropper , respectively .",
    "the goal of this problem is the following .",
    "assume that both receivers use minimum mean square error ( mmse ) estimators .",
    "given the value of the mean square error ( mse ) , which allows to correctly decode the legitimate receiver , what is the optimal distribution which maximizes the difference between the mse in the legitimate receiver and the mse in the eavesdropper ?",
    "the above mentioned problem adopts both practical and reasonable assumptions due to the following reasons .",
    "first , the mmse estimator is an optimal estimator in the sense that it minimizes the mse .",
    "therefore , it is reasonable to use such an optimal estimator .",
    "second , to prevent from eavesdropping , finding the signal distribution that maximizes the difference between the mses corresponding to the legitimate receiver and the eavesdropper , respectively , represents a legitimate design objective . to find the optimal distribution , the following functional problem",
    "is constructed : @xmath312 where @xmath313\\right)^2\\right]$ ] , @xmath314 $ ] denotes the expectation operator , and @xmath315 is a constant .    the optimization problem in ( [ wire_eq2_1 ] ) is expressed as @xmath316|y_2),\\\\ \\label{wire_eq3_2 } & & \\text{s.t.}\\quad \\mathbb{e}\\left [ \\mathbb{e } \\left[x|y_1\\right]^2\\right ] = 1- r.\\end{aligned}\\ ] ] the equation in ( [ wire_eq3_1 ] ) is due to the total law of variance and the markov chain @xmath317 . since @xmath318=1 $ ] , the equation ( [ wire_eq3_2 ] ) follows from the constraint in ( [ wire_eq2_1 ] ) .",
    "the objective function in ( [ wire_eq3_1 ] ) is further expressed as @xmath319|y_2\\right)=\\mathbb{e } \\left[\\mathbb{e}\\left[x|y_1\\right]^2\\right]-\\mathbb{e } \\left[\\mathbb{e } \\left[x|y_2\\right]^2\\right]\\end{aligned}\\ ] ] and using the equations ( [ wire_eq3_2 ] ) , ( [ wire_eq4_1 ] ) , the optimization problem in ( [ wire_eq3_1 ] ) is re - formulated in terms of the following variational problem : @xmath320 where @xmath321 and @xmath322 are the probability density functions of @xmath158 and @xmath311 , respectively , and @xmath323 stands for the second - order moment of @xmath311 .",
    "since the first term in ( [ wire_eq4_1 ] ) is given and @xmath324 ^ 2\\right ] = \\int f_{\\scriptscriptstyle y_2}(y)\\left(\\int x \\frac{f_{\\scriptscriptstyle y_2|x}(y|x ) f_{\\scriptscriptstyle x}(x)}{f_{\\scriptscriptstyle y_2}(y ) } dx\\right)^2dy,\\nonumber\\end{aligned}\\ ] ] the objective function in ( [ wire_eq5_1 ] ) is derived from the equation ( [ wire_eq3_1 ] ) . also , the additional constraint in ( [ wire_eq5_3 ] ) is required to solve this variational problem .",
    "considering the lagrange multipliers @xmath325 and @xmath99 to account for the constraints in ( [ wire_eq5_3 ] ) and ( [ wire_eq5_3 ] ) , respectively , the following variational problem is constructed : @xmath326 where @xmath327 in accordance with theorem [ cal_thm1 ] , we can determine @xmath328 and @xmath329 to enforce the first - order variation to be zero : @xmath330 taking into account ( [ wire_eq7_1 ] ) , it follows further that @xmath331 & = & \\frac{g^*(y)}{f_{\\scriptscriptstyle y_2}^*(y ) } = \\sqrt{\\lambda_1 } y.\\end{aligned}\\ ] ] since the equation in ( [ wire_eq8_1 ] ) is a linear function of @xmath85 and @xmath332 is a gaussian random variable , therefore , @xmath333 is also a gaussian random variable .",
    "based on theorem [ cal_thm2 ] , it can be verified that the second - order variation is nonnegative . moreover , due to the convexity of @xmath334 wrt @xmath335 and @xmath336 , we can confirm that the solution is optimal , and the proof is completed .",
    "the importance of the variational framework in establishing some fundamental information theoretic inequalities was already illustrated herein paper . at their turn",
    ", these information theoretic inequalities played a fundamental role in establishing other important results and applications .",
    "for example , the minimum fisher information theorem ( cramr - rao inequality ) and maximum entropy theorem were used for developing min - max robust estimation techniques @xcite , results which were recently further extended to the more general framework of noise with arbitrary distribution ( and correlation ) in @xcite and used to explain why the mimo channel estimation scheme proposed in @xcite exhibits a min - max robustness property . along the same line of potential applications , the extensions of the maximum entropy and minimum fisher information results to positive random variables , as stated in theorems [ ef_thm3 ] , [ ef_thm6 ] and [ ef_thm7 ] , play a fundamental role in developing robust clock synchronization algorithms for wireless sensor networks and other wireless networks that rely on message exchanges to acquire the timing information .",
    "a large class of clock synchronization protocols ( see e.g. , tpsn , internet , pbs @xcite ) rely on the two - way message exchange mechanism and for which the timing synchronization approach reduces to estimating a linear regression model for which the distribution of additive noise has positive support but it is otherwise arbitrary @xcite . designing robust timing synchronization algorithms for such protocols is difficult , because of the variability of delay distributions caused by the variable network traffic .",
    "however , this problem can now be resolved at the light of the results brought by theorems [ ef_thm3 ] , [ ef_thm6 ] and [ ef_thm7 ] . by",
    "optimizing the design of timing messages for the scenario of a chi or log - normal distributed delay , then min - max robust time synchronization algorithms could be developed .",
    "the extremal entropy inequality was used in the vector gaussian broadcast channel @xcite , the distributed source coding with a single quadratic distortion constraint problem @xcite , the gaussian wire - tap channel @xcite , and many other problems . even though these applications were traditionally addressed using the information theoretic inequalities",
    ", one can directly approach these applications by means of the proposed variational calculus techniques .",
    "one of the benefits of such a variational approach is the fact that it can cope with many types of constraints as opposed to the eei which is still quite rigid in its formulation . as prof .",
    "max costa suggested the authors of this paper in a private communication , in the context of z gaussian interference channels , such a variational approach might be helpful to develop novel entropy - power - like inequalities , where the limiting variables are gaussian and independent but not anymore identically distributed , and to assess the capacity of the z - gaussian interference channel .",
    "additional important extensions of maximum entropy theorem , minimum fisher information theorem , additive worst noise lemma , and extremal entropy inequality might be envisioned within the proposed variational framework by imposing various restrictions on the range of values assumed by random variables / vectors ( e.g. , random variables whose support is limited to a finite length interval or finite set of values ) or on their second or higher - order moments and correlations .",
    "for example , the problem of finding the worst additive noise under a covariance constraint @xcite as well as establishing multivariate extensions of costa s entropy power inequality @xcite along the lines mentioned by liu et al .",
    "@xcite and palomar @xcite , @xcite might be also addressed within the proposed variational framework .",
    "however , all these challenges together with finding a variational proof of epi remain open research problems for future study .",
    "in this paper , we derived several fundamental information theoretic inequalities using a functional analysis framework .",
    "the main benefit for employing calculus of variations for proving information theoretic inequalities is the fact that the global optimal solution is obtained from the necessary conditions for optimality .",
    "a brief summary of this paper contributions is the following .",
    "first , the entropy maximizing theorem and fisher information minimizing theorem were derived under different assumptions .",
    "second , the worst additive noise lemma was proved from the perspective of a functional problem .",
    "third , the extremal entropy inequality was derived using calculus of variations techniques .",
    "finally , applications and possible extensions that could be addressed within the proposed variational framework were briefly presented .",
    "many open research problems were also formulated .",
    "let @xmath337 and @xmath338 $ ] , where @xmath339 , @xmath109 is a singular matrix , @xmath340 is an orthogonal matrix , and @xmath341 denotes a diagonal matrix .",
    "the correlation matrix of @xmath342 is the zero matrix , and therefore , it is considered as a deterministic vector . without loss of generality , we can assume @xmath343 .",
    "the following matrices are also considered : @xmath344,\\nonumber\\\\ \\mathbf{d } & = & \\left[\\begin{array}{cc } \\mathbf{i } & -\\mathbf{b}^{\\scriptscriptstyle t } \\mathbf{c}^{-1 } \\\\ \\mathbf{0 } & \\mathbf{i } \\end{array}\\right],\\end{aligned}\\ ] ] where the dimensions of @xmath345 , @xmath346 , and @xmath347 are @xmath348 , @xmath349 , and @xmath350 , respectively .",
    "then , @xmath351\\left[\\begin{array}{c } \\bar{\\mathbf{x}}_a \\\\ \\mathbf{0 } \\end{array}\\right ] = \\left[\\begin{array}{c}\\bar{\\mathbf{x}}_a \\\\ \\mathbf{0 } \\end{array}\\right],\\nonumber\\\\ \\hspace*{-4 mm } & & \\hspace*{-4 mm } \\mathbf{d}\\mathbf{q}^{\\scriptscriptstyle t}_{\\scriptscriptstyle \\omega } \\mathbf{w}_{\\scriptscriptstyle g }   =   \\left[\\begin{array}{c } \\bar{\\mathbf{w}}_{\\scriptscriptstyle g_a } \\\\ \\bar{\\mathbf{w}}_{\\scriptscriptstyle g_b } \\end{array}\\right],\\nonumber\\\\ \\label{im_eq0_1 } \\hspace*{-4 mm } & & \\hspace*{-4 mm } \\mathbb{e } \\left[\\mathbf{d } \\mathbf{q}_{\\scriptscriptstyle \\omega}^{\\scriptscriptstyle t } \\mathbf{w}_{\\scriptscriptstyle g }   \\mathbf{w}_{\\scriptscriptstyle g}^{\\scriptscriptstyle t } \\mathbf{q}_{\\scriptscriptstyle \\omega } \\mathbf{d}^{\\scriptscriptstyle t } \\right ]   =   \\left[\\begin{array}{cc } \\mathbf{a } - \\mathbf{b}^{\\scriptscriptstyle t } \\mathbf{c}^{-1}\\mathbf{b } & \\mathbf{0 } \\\\",
    "\\mathbf{0 } & \\mathbf{c } \\end{array}\\right].\\end{aligned}\\ ] ] due to ( [ im_eq0_1 ] ) , the random vectors @xmath352 and",
    "@xmath353 are statistically independent of each other .    the left - hand side of the equation in ( [ wa_eq14_1 ] ) can be re - expressed as @xmath354 in ( [ im_eq1_1 ] ) , @xmath137 is considered as a deterministic variable , @xmath353 is given , the term @xmath355 can be ignored in the optimization , and the correlation matrix of @xmath356 is non - singular .",
    "therefore , we can always assume the correlation matrix to be invertible .",
    "first , choose a gaussian random vector @xmath357 whose covariance matrix @xmath358 satisfies @xmath359 and @xmath360 . since the gaussian random vectors @xmath300 and",
    "@xmath189 can be represented as the summation of two independent random vectors @xmath357 and @xmath361 , and the summation of two independent random vectors @xmath357 and @xmath362 , respectively , the left - hand side of the equation in ( [ ei_eq50_1 ] ) is written as follows : @xmath363    since the expression will be minimized over @xmath364 , the last two terms in ( [ ei_eq51_2 ] ) are ignored , and by substituting @xmath252 and @xmath365 for @xmath366 and @xmath367 , respectively , the inequality in ( [ ei_eq50_1 ] ) is equivalently expressed as the following variational problem : @xmath368 where @xmath369 , @xmath370 , @xmath371 , @xmath372 , @xmath373 , @xmath374 , and @xmath375 is the covariance matrix of the optimal solution @xmath376 .",
    "the variational problem in ( [ ei_eq52_1 ] ) is exactly the same as the one in ( [ ei_eq21_1 ] ) .",
    "therefore , using the same method as in the proof of theorem [ ei_thm2 ] , we obtain the following inequality ( see the details in the proof of theorem [ ei_thm2 ] ) : @xmath377    by appropriately choosing @xmath378 and @xmath357 , the right - hand side of the equation in ( [ ei_eq53_1 ] ) is expressed as @xmath379 the equality in ( [ ei_eq54_1 ] ) is due to the equality condition of the data processing inequality in @xcite . for the completeness of the proof , we introduce a technique , which is slightly different from the one in @xcite .",
    "[ ei_lem1 ] when three random vectors @xmath380 , @xmath381 , and @xmath382 represent a markov chain @xmath383 , the following inequality is satisfied : @xmath384 the equality holds if and only if @xmath385 .    in lemma [ ei_lem1 ] , @xmath380 , @xmath381 , and @xmath382 are defined as @xmath378 , @xmath386 , and @xmath387 , respectively .",
    "therefore , the equality condition , @xmath385 is expressed as @xmath388 if @xmath389 , the equality in ( [ ei_eq56_5 ] ) is satisfied , the equality condition in lemma [ ei_lem1 ] holds , and therefore , the equality in ( [ ei_eq54_1 ] ) is proved .",
    "the validity of @xmath389 is proved by lemma @xmath390 in @xcite .",
    "g. aubert and p. kornprobst , _ mathematical problems in image processing : partial differential equations and the calculus of variations_. applied mathematical sciences vol . 147 .",
    "springer verlag .",
    "new york , 2006 .",
    "h.  weingarten , y.  steinberg , and s.  shamai , `` the capacity region of the gaussian mutiple - input multiple - output broadcast channel , '' _ ieee trans .",
    "inform . theory _",
    "9 , pp . 3936 - 3964 , sep .",
    "2006 .",
    "m. payaro , m. gregori , and d. palomar ,  yet another power entropy inequality with an application , \" _ 2011 international conference on wireless communications and signal processing ( wcsp ) _ , nanjing , china , nov .",
    "2011 , pp . 1 - 5 .",
    "m. payaro and d. palomar ,  a multivariate generalization of costa s entropy power inequality , \" _ ieee international symposium in information theory 2008 ( isit 2008 ) _ , toronto , canada , jul .",
    "2008 , pp .",
    "1088 - 1092 .",
    "sangwoo park received the b.s .",
    "degree in electrical engineering from chung - ang university ( cau ) , seoul , korea , in 2004 , and the m.s . and ph.d .",
    "degrees in electrical engineering from texas a&m university , college station , in 2008 and 2012 , respectively . from 2004 to 2005 , he worked as a full - time assistant engineer for umts / wcdma projects in samsung electronics . currently , he is a research engineer at kt ( korea telecom ) in korea .",
    "his research interests lie in wireless communications , information theory , and statistical signal processing .",
    "erchin serpedin(f13 ) received the specialization degree in signal processing and transmission of information from ecole superieure delectricite ( supelec ) , paris , france , in 1992 , the m.sc .",
    "degree from the georgia institute of technology , atlanta , in 1992 , and the ph.d .",
    "degree in electrical engineering from the university of virginia , charlottesville , in january 1999 .",
    "he is currently a professor in the department of electrical and computer engineering at texas a&m university , college station .",
    "he is the author of two research monographs , one edited textbook , 100 journal papers and 150 conference papers , and has served as associate editor for about 10 journals such as ieee transactions on information theory , ieee transactions on communications , signal processing ( elsevier ) , ieee transactions on signal processing , ieee transactions on wireless communications , ieee communications letters , ieee signal processing letters , phycom , eurasip journal on advances in signal processing , and eurasip journal on bioinformatics and systems biology .",
    "his research interests include signal processing , wireless communications , computational statistics , bioinformatics and systems biology .",
    "khalid qaraqe(m97-s00 ) received with honors the b.s . degree in ee from the university of technology , baghdad , irak , in 1986 .",
    "he received the m.s .",
    "degree in ee from the university of jordan , jordan , in 1989 , and he earned his ph.d .",
    "degree in ee from texas a&m university , college station , tx , in 1997 . from 1989 to 2004 ,",
    "qaraqe held a variety of positions in many companies .",
    "he has over 15 years of experience in the telecommunications industry .",
    "qaraqe has worked for qualcomm , enad design systems , cadence design systems / tality corporation , stc , sbc and ericsson .",
    "he has worked on numerous gsm , cdma , wcdma projects and has experience in product development , design , deployment , testing and integration .",
    "qaraqe joined texas a&m university at qatar , in july 2004 , where he is now a professor .",
    "qaraqe research interests include communication theory and its application to design and performance analysis of cellular systems and indoor communication systems .",
    "particular interests are in the development of 3 g umts , cognitive radio systems , broadband wireless communications and diversity techniques ."
  ],
  "abstract_text": [
    "<S> this paper proposes a unifying variational approach for proving some fundamental information theoretic inequalities . </S>",
    "<S> fundamental information theory results such as maximization of differential entropy , minimization of fisher information ( cramr - rao inequality ) , worst additive noise lemma , and extremal entropy inequality ( eei ) are interpreted as functional problems and proved within the framework of calculus of variations . </S>",
    "<S> several applications and possible extensions of the proposed results are briefly mentioned .    </S>",
    "<S> maximizing entropy , minimizing fisher information , worst additive noise , extremal entropy inequality , calculus of variations </S>"
  ]
}